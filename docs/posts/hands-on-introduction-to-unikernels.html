<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514">Original</a>
    <h1>Hands-On Introduction to Unikernels</h1>
    
    <div id="readability-page-1" class="page"><div data-v-200caef3=""><!----><div data-v-200caef3=""><div data-v-200caef3=""><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->The source of this tutorial is available on <a href="https://github.com/antoineco/iximiuz-labs-content/tree/main/unikernels-intro-93976514" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->GitHub<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>.
Feel free to use that source repository to contact the author with questions, discuss possible improvements or report mistakes.<!--]--></em><!--]--></p><hr data-v-34b8dd49=""/><p data-v-b025fb9f=""><!--[-->Virtual machines provide a level of <strong data-v-24022f14=""><!--[-->isolation<!--]--></strong> unattainable by any flavor of Linux container.
Virtualization technologies are widely adopted in the industry as a response to stringent security requirements, particularly in environments that share hardware between multiple tenants and <em data-v-c83db111=""><!--[-->must<!--]--></em> guarantee <strong data-v-24022f14=""><!--[-->opaque boundaries<!--]--></strong> between those tenants.
It shouldn&#39;t come as a surprise that cloud hyperscalers provide access to compute in various forms of virtual machines<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-aws" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-aws" data-v-abeb6555=""><!--[-->1<!--]--><!----></a></sup> rather than some abstraction on top of a shared OS kernel.<!--]--></p><p data-v-b025fb9f=""><!--[-->But reaping the security benefits of virtual machines typically requires compromising on performance aspects.
Cold booting an entire operating system takes <strong data-v-24022f14=""><!--[-->significantly more time<!--]--></strong> than spawning an OS process, which hinders dynamic application scaling.
Also with each VM&#39;s kernel making independent scheduling decisions on top of the same hardware resources (CPU, memory, network, GPUs, ...), running multiple general-purpose operating systems such as Linux side by side requires smart hypervisors in order to <strong data-v-24022f14=""><!--[-->distribute those resources<!--]--></strong> optimally<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-dblsched" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-dblsched" data-v-abeb6555=""><!--[-->2<!--]--><!----></a></sup>.<!--]--></p><p data-v-b025fb9f=""><!--[-->You will often encounter the term &#34;Micro VM&#34; to describe a type of virtual machine that attempts to address the aforementioned limitations by <strong data-v-24022f14=""><!--[-->reducing their memory footprint<!--]--></strong> through restricted device support and guest<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-guest" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-guest" data-v-abeb6555=""><!--[-->3<!--]--><!----></a></sup> functionalities.
While this approach demonstrates measurable improvements to boot times, the guest itself still carries the <strong data-v-24022f14=""><!--[-->overhead<!--]--></strong> of a full, general-purpose operating system.<!--]--></p><p data-v-b025fb9f=""><!--[-->Unikernels propose a solution to that exact problem.<!--]--></p><p data-v-b025fb9f=""><!--[-->Throughout this tutorial, you will learn what a unikernel is made of and even build one yourself.
You will be guided throughout the steps required for running this unikernel on any Linux host, then package it as an OCI image that can be run by Docker.<!--]--></p><h2 id="requirements" data-v-b9b1576a=""><!--[-->Requirements<!--]--><!----></h2><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->Notions about virtualization: what is a virtual machine, a hypervisor.<!--]--></li><li data-v-2969431c=""><!--[-->A high level understanding of how an OS process interacts with its kernel: libc, syscalls.<!--]--></li><li data-v-2969431c=""><!--[-->A good understanding about the subsystems involved in the composition of a Linux container: namespaces, cgroups, CNI.<!--]--></li><!--]--></ul><h2 id="what-is-a-unikernel" data-v-b9b1576a=""><!--[-->What Is a Unikernel?<!--]--><!----></h2><h3 id="core-principles" data-v-9a0ceb30=""><!--[-->Core Principles<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->In essence, a unikernel is a highly specialized type of kernel that is tailored for running one application, and that application specifically.
By this property, a unikernel is capable of running in a single address space, where the usual distinction between kernel and user spaces does not exist.
The application <em data-v-c83db111=""><!--[-->is<!--]--></em> the kernel, as much as the kernel is the application.<!--]--></p><p data-v-b025fb9f=""><!--[-->This design avoids paying the performance penalty of switching contexts on system calls between the user space and the kernel space, a behavior that is unavoidable in traditional operating systems.
It allows a direct and quicker access to the hardware.<!--]--></p><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/addrspace.png" alt="There is only one address space in a unikernel, therefore no expensive context switches on system calls between user and kernel spaces."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Unikernels operate in a single address space.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->There exists different types of unikernel projects.
Some are tailored to run applications written in specific programming languages, others are more versatile.
Some are monolithic, others are modular.
Some require compiling and linking the application in a special way, others are able to run unmodified ELF executables.<!--]--></p><p data-v-b025fb9f=""><!--[-->Despite those differences, all strive to achieve specialization in a similar way: by selecting <strong data-v-24022f14=""><!--[-->only the components of the software stack which are relevant to the application<!--]--></strong>.
For instance, there is no use for disk filesystem drivers in a piece of software which interacts solely with the network and does not write any data to disk.
Likewise, because the unikernel is a single application, it does not require any user management or scheduling facilities.
These become opportunities for reducing the software footprint by cutting on bloat, also reducing its attack surface by extension.<!--]--></p><p data-v-b025fb9f=""><!--[-->We often refer to the unikernel paradigm as &#34;library operating systems&#34;.<!--]--></p><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/specialization.png" alt="The unikernel is specialized by keeping only the components of the stack that are required for its particular application to function."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Specializing a unikernel by components selection.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->The artifact produced by building a unikernel is a kernel executable that comprises the application, along with only the parts of the OS kernel, libraries and possible runtime that were selected for that application.
Because a unikernel is meant to be materialized as a virtual machine, the produced artifact is <strong data-v-24022f14=""><!--[-->specific to the target hypervisor<!--]--></strong> (KVM<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-kvm" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-kvm" data-v-abeb6555=""><!--[-->4<!--]--><!----></a></sup>, Xen<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-xen" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-xen" data-v-abeb6555=""><!--[-->5<!--]--><!----></a></sup>, VMware, ...) and CPU architecture.<!--]--></p><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/artifact.png" alt="The unikernel build process produces an artifact which can be used by a hypervisor to materialize a virtual machine."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Final unikernel artifact.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->In summary, unikernels:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->Operate in a drastically minimized software stack without separation between kernel and user spaces.<!--]--></li><li data-v-2969431c=""><!--[-->Have a smaller memory footprint and attack surface than a Linux process.<!--]--></li><li data-v-2969431c=""><!--[-->Do not share any kernel components with the host.<!--]--></li><li data-v-2969431c=""><!--[-->Benefit from the strong isolation of a virtual machine.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->As a result, unikernels offer <strong data-v-24022f14=""><!--[-->increased security and performance<!--]--></strong> compared to a containerized Linux process.<!--]--></p><h3 id="trade-offs" data-v-9a0ceb30=""><!--[-->Trade-Offs<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->It would feel dishonest to praise unikernels for their many benefits without acknowledging some of their limitations.<!--]--></p><p data-v-b025fb9f=""><!--[-->Nowadays, the comparison of unikernels with Linux containers seems unavoidable, therefore the aspects enumerated below focus deliberately on use-cases that a container typically enables and which presents challenges when running the same application as a unikernel.<!--]--></p><h4 id="no-shell-no-users" data-v-f2a5dd23=""><!--[-->No Shell, No Users<!--]--><!----></h4><p data-v-b025fb9f=""><!--[-->We are starting with some fully embraced bait; the absence of shells and users is absolutely <strong data-v-24022f14=""><!--[-->desired<!--]--></strong> for reducing the attack surface of applications.<!--]--></p><p data-v-b025fb9f=""><!--[-->Yet, when comes the need to troubleshoot a misbehaving live application it is hard to beat the convenience of containers.
Need to use a specific tool but the container&#39;s root filesystem is only one statically linked executable?
No problem, attach your favorite debugging shell to the namespaces of the container, become root if you please, and off you go.<!--]--></p><p data-v-b025fb9f=""><!--[-->With a unikernel? You better be sure that the application has comprehensive observability facilities built-in.<!--]--></p><h4 id="single-process" data-v-f2a5dd23=""><!--[-->Single Process<!--]--><!----></h4><p data-v-b025fb9f=""><!--[-->Unikernels are inherently <strong data-v-24022f14=""><!--[-->single-process OSes<!--]--></strong> due to their single address space design.<!--]--></p><p data-v-b025fb9f=""><!--[-->A range of applications use the POSIX <a href="https://man7.org/linux/man-pages/man2/fork.2.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[--><code data-v-a031129a=""><!--[-->fork<!--]--></code><!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> system call to create child processes such as additional workers, data snapshotters, etc.
Running such applications as unikernels present challenges and require trade-offs.
Some notable examples:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->The <a href="https://www.postgresql.org/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->PostgreSQL<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> database server has process-based architecture rather than using threads.<!--]--></li><li data-v-2969431c=""><!--[-->The most widely used worker (MPM) of <a href="https://httpd.apache.org/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Apache HTTPd<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> implements a hybrid multi-process / multi-threaded server.<!--]--></li><li data-v-2969431c=""><!--[-->The <a href="https://redis.io/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Redis<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> in-memory cache persists data on disk using a child process.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->Containers being regular OS processes, they do not suffer from such limitation.<!--]--></p><h4 id="no-standardization" data-v-f2a5dd23=""><!--[-->No Standardization<!--]--><!----></h4><p data-v-b025fb9f=""><!--[-->Unikernels are still considered a <strong data-v-24022f14=""><!--[-->niche technology<!--]--></strong> despite building on concepts developed in the late 1990s.
As such, they lack the standardization that emerges once a technology achieves a certain degree of maturity.
This means that there is currently no compatibility whatsoever between unikernel projects and their associated tooling.<!--]--></p><p data-v-b025fb9f=""><!--[-->Interestingly, compatibility with container-centric and <em data-v-c83db111=""><!--[-->cloud-native<!--]--></em> tooling is limited but not nonexistent, as you will discover later in this tutorial.<!--]--></p><p data-v-b025fb9f=""><!--[-->This lack of standardization is no different from the early days of containers, which weren&#39;t standardized either until several years into the existence of Docker containers.<!--]--></p><h4 id="knowledge-requirements" data-v-f2a5dd23=""><!--[-->Knowledge Requirements<!--]--><!----></h4><p data-v-b025fb9f=""><!--[-->Because unikernels are purpose-built and combine both the user and kernel code, they generally require a <strong data-v-24022f14=""><!--[-->deeper knowledge about the software stack<!--]--></strong> than is required for building conventional application artifacts.
Porting an application as a unikernel <em data-v-c83db111=""><!--[-->may<!--]--></em> require specialist knowledge from the runtime through the kernel interfaces all the way down to the hardware.<!--]--></p><p data-v-b025fb9f=""><!--[-->As previously mentioned, there exists unikernel projects that can run unmodified ELF executables in a binary-compatible mode (POSIX compliant).
Even though these do not enable the <em data-v-c83db111=""><!--[-->full, uncompromised<!--]--></em> performance and security benefits of purpose-built unikernels, they <strong data-v-24022f14=""><!--[-->preserve a number of those benefits<!--]--></strong> over a general-purpose, multi-user operating system kernel like Linux.
They also bring the process of building a unikernel closer to the one of assembling a container image. Win-win.<!--]--></p><h4 id="limited-inter-process-communications" data-v-f2a5dd23=""><!--[-->Limited Inter-Process Communications<!--]--><!----></h4><p data-v-b025fb9f=""><!--[-->Finally, one limitation that stems from the virtual machine model and not the unikernel architecture in itself is the <strong data-v-24022f14=""><!--[-->lack of supporting facilities for inter-process communications<!--]--></strong>.
Without a shared kernel to provide such facilities, applications running as unikernels must communicate over the network or via channels provided by their underlying hypervisor.<!--]--></p><p data-v-b025fb9f=""><!--[-->Containers allow multiple processes to be confined together and communicate over UNIX domain sockets, POSIX message queues, a loopback network interface, OS signals, etc.<!--]--></p><h2 id="building-the-unikernel" data-v-b9b1576a=""><!--[-->Building the Unikernel<!--]--><!----></h2><p data-v-b025fb9f=""><!--[-->Now that you are equipped with general knowledge about unikernels, let&#39;s begin with the hands-on part of the tutorial, shall we?<!--]--></p><p data-v-b025fb9f=""><!--[-->In this tutorial, you are going to build the <a href="https://nginx.org/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Nginx<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> web server as a <a href="https://unikraft.org/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Unikraft<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> unikernel, from sources.<!--]--></p><p data-v-b025fb9f=""><!--[-->Nginx is a classic example of server application that is both incredibly widespread<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-nginx" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-nginx" data-v-abeb6555=""><!--[-->6<!--]--><!----></a></sup> and easy to interact with.<!--]--></p><p data-v-b025fb9f=""><!--[-->The Unikraft kernel was chosen for this tutorial because it ticks the following boxes:<!--]--></p><ol data-v-d123ed92=""><!--[--><li data-v-2969431c=""><!--[-->Comprehensive implementation of the library OS concept with a modular design that supports both porting from sources and binary compatibility.<!--]--></li><li data-v-2969431c=""><!--[-->Supported by some of the OCI<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-oci" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-oci" data-v-abeb6555=""><!--[-->7<!--]--><!----></a></sup> tooling used later in this tutorial.<!--]--></li><!--]--></ol><p data-v-b025fb9f=""><!--[-->The playground box is pre-initialized with the following two directories inside your HOME:<!--]--></p><!--[--><!--]--><!--[--><!--]--><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[--><code data-v-a031129a=""><!--[-->nginx/<!--]--></code> is your workspace for building the Nginx application as a Unikraft unikernel.<!--]--></li><li data-v-2969431c=""><!--[--><code data-v-a031129a=""><!--[-->sources/<!--]--></code> contains the source code of the Unikraft core, of Nginx itself (as a thin Unikraft library) and of additional libraries required by the Nginx application.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->Start by changing your working directory to <code data-v-a031129a=""><!--[-->nginx/<!--]--></code>. You will remain in there throughout most of this tutorial:<!--]--></p><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->This directory contains a few files which are solely responsible for <strong data-v-24022f14=""><!--[-->configuring and building<!--]--></strong> the unikernel:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>ls</span><span> -p1</span><span> --group-directories-first
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>oci/
rootfs/
workdir/
Config.uk
Makefile
Makefile.uk
qemu-x86_64.defconfig
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Use the tabs below for a description of the purpose of each of these files:<!--]--></p><div><div><!--[--><p>qemu-x86_64.defconfig</p><p>Config.uk</p><p>workdir</p><p>Makefile</p><!--]--></div><div><!--[--><div><!--[--><p data-v-b025fb9f=""><!--[-->A file in which <a href="https://tldp.org/HOWTO/SCSI-2.4-HOWTO/kconfig.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->kernel options<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> with <strong data-v-24022f14=""><!--[-->non-default<!--]--></strong> values are specified.<!--]--></p><p data-v-b025fb9f=""><!--[-->Options which are specific to <em data-v-c83db111=""><!--[-->our flavor<!--]--></em> of the unikernel are enabled in this file and will be merged with the defaults of the Unikraft kernel:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->The target platform/hypervisor: KVM.<!--]--></li><li data-v-2969431c=""><!--[-->Our preference for printing kernel messages at the info level (only errors are printed by default).<!--]--></li><li data-v-2969431c=""><!--[-->Some filesystem implementation (embedded initrd).<!--]--></li><!--]--></ul><!--[--><div meta="" data-v-4578ab24=""><p><span data-v-4578ab24="">qemu-x86_64.defconfig</span></p><!--[--><pre><!--[--><code><span line="1"><span>CONFIG_PLAT_KVM=y
</span></span><span line="2"><span>CONFIG_LIBUKDEBUG_PRINTK_INFO=y
</span></span><span line="3"><span>CONFIG_LIBPOSIX_VFS_FSTAB=y
</span></span><span line="4"><span>CONFIG_LIBPOSIX_VFS_FSTAB_BUILTIN=y
</span></span><span line="5"><span>CONFIG_LIBPOSIX_VFS_FSTAB_BUILTIN_EINITRD=y
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><!--]--></div></div><h3 id="kernel-configuration" data-v-9a0ceb30=""><!--[-->Kernel Configuration<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->First, a kernel needs a <a href="https://tldp.org/HOWTO/SCSI-2.4-HOWTO/kconfig.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->configuration<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, and a unikernel in no exception.
You have already encountered the <code data-v-a031129a=""><!--[-->qemu-x86_64.defconfig<!--]--></code> file, you are now going to generate the <strong data-v-24022f14=""><!--[-->full kernel configuration<!--]--></strong> based on those few customized values:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>UK_DEFCONFIG</span><span>=</span><span>$PWD</span><span>/qemu-x86_64.defconfig</span><span> make</span><span> defconfig
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>  MAKE    kconfig
  ...
#
# configuration written to /home/laborant/nginx/.config
#
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->This command should have generated a file named <code data-v-a031129a=""><!--[-->.config<!--]--></code> inside the workspace:<!--]--></p><!--[--><!--]--><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->It contains an aggregation of all the kernel options explicitly or implicitly selected inside the workspace, <strong data-v-24022f14=""><!--[-->merged with the defaults<!--]--></strong> from the Unikraft core.<!--]--></p><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Configuration TUI</summary><!--[--><p data-v-b025fb9f=""><!--[-->Instead of invoking the <code data-v-a031129a=""><!--[-->defconfig<!--]--></code> goal, you could have selected kernel options via a terminal-based kernel configuration interface.<!--]--></p><p data-v-b025fb9f=""><!--[-->Since your <code data-v-a031129a=""><!--[-->.config<!--]--></code> file is already generated, opening this interface will display the options that were selected by the <code data-v-a031129a=""><!--[-->defconfig<!--]--></code> goal.
Give it a try!
If you have ever built the Linux kernel from sources this should look familiar, even spark a bit of nostalgia :)<!--]--></p><p data-v-b025fb9f=""><!--[-->(You might have to expand the side panel for your web-based terminal to have large enough dimensions.)<!--]--></p><!--[--><!--]--><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/menuconfig.png" alt="ncurses-based kernel configuration TUI"/></p><!----></div><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->With the kernel configuration generated, you are now ready to proceed with the build. Let&#39;s go:<!--]--></p><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->The build process goes through a few steps, including fetching the source code of the libraries and application (Nginx) which are not yet cached locally:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>  ...
  WGET    libmusl: https://www.musl-libc.org/releases/musl-1.2.3.tar.gz
  ...
  WGET    libnginx: http://nginx.org/download/nginx-1.15.6.tar.gz
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Unfortunately, about 10 seconds in, an error comes and ruins the party already:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>make[3]: *** No rule to make target &#39;/home/laborant/nginx/initrd.cpio&#39;, needed by &#39;/home/laborant/nginx/workdir/build/libposix_vfs_fstab/einitrd.o&#39;.  Stop.
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->This is <strong data-v-24022f14=""><!--[-->part of the tutorial<!--]--></strong> and a good segue into some peculiarity about the unikernel you are building.<!--]--></p><h3 id="files-access" data-v-9a0ceb30=""><!--[-->Files Access<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->If you take a step back and think about what a typical application package contains—be it an OS package or a container image—you might be able to guess where we are heading.
Let&#39;s take a look at the files included with the <code data-v-a031129a=""><!--[-->nginx-common<!--]--></code> package of the playground box&#39;s distribution:<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>/etc/default/nginx
</span></span><span line="2"><span>/etc/init.d/nginx
</span></span><span line="3"><span>/etc/logrotate.d/nginx
</span></span><span line="4"><span>/etc/nginx/fastcgi.conf
</span></span><span line="5"><span>/etc/nginx/fastcgi_params
</span></span><span line="6"><span>/etc/nginx/koi-utf
</span></span><span line="7"><span>/etc/nginx/koi-win
</span></span><span line="8"><span>/etc/nginx/mime.types
</span></span><span line="9"><span>/etc/nginx/nginx.conf
</span></span><span line="10"><span>/etc/nginx/sites-available/default
</span></span><span line="11"><span>/usr/lib/systemd/system/nginx.service
</span></span><span line="12"><span>/usr/share/apport/package-hooks/source_nginx.py
</span></span><span line="13"><span>/usr/share/doc/nginx-common/changelog.Debian.gz
</span></span><span line="14"><span>/usr/share/nginx/html/index.html
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->That&#39;s right, a bunch of static resources.
While some are support files for the playground&#39;s Linux distribution (service definition, logrotate config, docs) others, such as the highlighted ones, are <strong data-v-24022f14=""><!--[-->required by the Nginx application<!--]--></strong> to function.<!--]--></p><p data-v-b025fb9f=""><!--[-->On any Linux system, exposing such files to an application is commonly accepted as storing the files on a mounted filesystem and giving read/write permissions on these files to the UNIX user that the application is going to run as.
In a container image, those same files are either part of the root filesystem already, or bind mounted at run time to the root filesystem that the container pivots to.<!--]--></p><p data-v-b025fb9f=""><!--[-->But our unikernel does not have access to a filesystem exposed through a shared host kernel, so those files <strong data-v-24022f14=""><!--[-->must be provided to the application by different means<!--]--></strong>.
One option could have been to enable selected filesystem drivers via kernel options and <a href="https://www.linux-kvm.org/page/9p_virtio" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->share files from the host<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> at run time through the target hypervisor, similarly to a bind mount on Linux.<!--]--></p><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/vol9pfs.png" alt="A volume can be mounted from the host over a virtual 9p VirtIO device and accessed by the guest via a compatible kernel driver."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Sharing files from the host over the 9p protocol.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->However, the option we selected in this tutorial is to <strong data-v-24022f14=""><!--[-->embed the files<!--]--></strong> in an <a href="https://en.wikipedia.org/wiki/Initial_ramdisk" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->initial ramdisk<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> (initrd) using the kernel option <code data-v-a031129a=""><!--[-->LIBPOSIX_VFS_FSTAB_BUILTIN_EINITRD<!--]--></code>.
With this approach, the initial ramdisk is expanded into a root filesystem mounted as a read-write <a href="https://www.kernel.org/doc/html/latest/filesystems/ramfs-rootfs-initramfs.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->RAM-based filesystem<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> (ramfs) upon booting the unikernel.
A perfect solution for static and non-persistent files.<!--]--></p><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/voleinitrd.png" alt="An initial ramdisk can be embedded into the unikernel at build time and mounted in memory at boot time."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Embedding files into the unikernel as an initial ramdisk.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->We pinpointed earlier the lack of standardization across unikernel projects.
The approaches proposed in this section are a demonstration of mechanisms and configurations which are highly Unikraft-specific and may or may not have an equivalent in other unikernels.<!--]--></p><p data-v-b025fb9f=""><!--[-->The files that need to be embedded are already part of your workspace and stored in the <code data-v-a031129a=""><!--[-->rootfs/<!--]--></code> directory:<!--]--></p><!--[--><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>rootfs/
└── nginx
    ├── conf
    │   ├── mime.types
    │   └── nginx.conf
    ├── html
    │   └── index.html
    └── logs
        └── error.log
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->You need to <strong data-v-24022f14=""><!--[-->convert the contents of this directory<!--]--></strong> into a cpio<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-cpio" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-cpio" data-v-abeb6555=""><!--[-->8<!--]--><!----></a></sup> archive. (Remember the error message?)
The Unikraft core includes a support script which achieves just that:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>workdir/unikraft/support/scripts/mkcpio</span><span> initrd.cpio</span><span> rootfs
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->The result is an archive named <code data-v-a031129a=""><!--[-->initrd.cpio<!--]--></code> containing all the files staged for embedding into the unikernel artifact at build time.<!--]--></p><!--[--><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>?? .config
?? initrd.cpio
</code><!--]--></pre><!--]--></div><!--]--><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">The cpio archive format</summary><!--[--><p data-v-b025fb9f=""><!--[-->The format of a cpio archive isn&#39;t that different from that of a tarball.
Files are merely concatenated with their contents separated by some headers and metadata.<!--]--></p><p data-v-b025fb9f=""><!--[-->You can print the contents of the generated <code data-v-a031129a=""><!--[-->initrd.cpio<!--]--></code> using the following command and verify that it matches that of the <code data-v-a031129a=""><!--[-->rootfs/<!--]--></code> directory:<!--]--></p><!--[--><!--]--><!--]--></details><!--]--></div><h3 id="kernel-build" data-v-9a0ceb30=""><!--[-->Kernel Build<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->You should now have all required resources sorted out:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[--><code data-v-a031129a=""><!--[-->.config<!--]--></code> – a kernel configuration.<!--]--></li><li data-v-2969431c=""><!--[--><code data-v-a031129a=""><!--[-->initrd.cpio<!--]--></code> – an initial ramdisk for loading a root file system with a few static files into memory.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->Let&#39;s try building the unikernel one more time:<!--]--></p><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->After about 2 minutes, you should see the build succeed with a final linking and stripping of the build artifacts:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>  ...
  OBJCOPY libmusl.o
  LD      nginx_qemu-x86_64.dbg
  SCSTRIP nginx_qemu-x86_64
  UKBI    nginx_qemu-x86_64.bootinfo
  MULTIBT nginx_qemu-x86_64.multiboot
make[1]: Leaving directory &#39;/home/laborant/sources/unikraft&#39;
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->All files resulting from the build are written to the <code data-v-a031129a=""><!--[-->build/<!--]--></code> directory.
The one without an extension in the list below is the <strong data-v-24022f14=""><!--[-->kernel<!--]--></strong>:<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>?? .config
</span></span><span line="2"><span>?? initrd.cpio
</span></span><span line="3"><span>?? workdir/build/nginx_qemu-x86_64
</span></span><span line="4"><span>?? workdir/build/nginx_qemu-x86_64.bootinfo
</span></span><span line="5"><span>?? workdir/build/nginx_qemu-x86_64.bootinfo.cmd
</span></span><span line="6"><span>?? workdir/build/nginx_qemu-x86_64.cmd
</span></span><span line="7"><span>?? workdir/build/nginx_qemu-x86_64.dbg
</span></span><span line="8"><span>?? workdir/build/nginx_qemu-x86_64.dbg.cmd
</span></span><span line="9"><span>?? workdir/build/nginx_qemu-x86_64.dbg.gdb.py
</span></span><span line="10"><span>?? workdir/build/nginx_qemu-x86_64.multiboot.cmd
</span></span></code><!--]--></pre><!--]--></div><!--]--><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Unikraft kernel&#39;s naming convention</summary><!--[--><p data-v-b025fb9f=""><!--[-->The base name of the generated kernel artifacts follows the naming convention:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>$(CONFIG_UK_NAME)_$(KVM_VMM)-$(CONFIG_UK_ARCH)
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->You can verify that the file name <code data-v-a031129a=""><!--[-->nginx_qemu-x86_64<!--]--></code> indeed matches the kernel configuration used during the build:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>grep</span><span> -E</span><span> &#39;CONFIG_(UK_NAME|UK_ARCH|KVM_VMM)&#39;</span><span> --color</span><span> .config
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>CONFIG_UK_ARCH=&#34;x86_64&#34;
</span></span><span line="2"><span>CONFIG_KVM_VMM_QEMU=y
</span></span><span line="3"><span># CONFIG_KVM_VMM_FIRECRACKER is not set
</span></span><span line="4"><span>CONFIG_UK_NAME=&#34;nginx&#34;
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->The format of this kernel is an ELF executable, just like the Linux kernel that your playground box is currently running on:<!--]--></p><div><div><!--[--><p>Nginx unikernel</p><p>Linux kernel</p><!--]--></div><div><!--[--><div><!--[--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>file</span><span> workdir/build/nginx_qemu-x86_64
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>workdir/build/nginx_qemu-x86_64: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), statically linked, stripped
</code><!--]--></pre><!--]--></div><!--]--><!--]--></div><!--]--></div></div><p data-v-b025fb9f=""><!--[-->One thing should be particularly striking about this artifact: its small size.
The Nginx application and all the OS components required for running it fit into... less than <strong data-v-24022f14=""><!--[-->2 MB<!--]--></strong>!<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>du</span><span> -h</span><span> workdir/build/nginx_qemu-x86_64
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>2.0M    workdir/build/nginx_qemu-x86_64
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->For comparison, on the playground box&#39;s distribution the <code data-v-a031129a=""><!--[-->nginx<!--]--></code> binary, the GNU libc and the Linux kernel taken together weigh almost 60 MB:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>du</span><span> -hL</span><span> /boot/vmlinux</span><span> /usr/sbin/nginx</span><span> /lib/x86_64-linux-gnu/libc.so.6
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>56M     /boot/vmlinux
1.3M    /usr/sbin/nginx
2.1M    /lib/x86_64-linux-gnu/libc.so.6
</code><!--]--></pre><!--]--></div><!--]--><h2 id="running-the-unikernel" data-v-b9b1576a=""><!--[-->Running the Unikernel<!--]--><!----></h2><p data-v-b025fb9f=""><!--[-->The kernel you just built can be run as a <em data-v-c83db111=""><!--[-->machine<!--]--></em>.
A unikernel could technically boot on a <em data-v-c83db111=""><!--[-->physical machine<!--]--></em> (e.g. your laptop) providing that it was built with drivers compatible with its hardware.
However, unikernels are largely meant to run as <strong data-v-24022f14=""><!--[-->virtual machines<!--]--></strong> rather than physical ones, as mentioned in the introduction to this tutorial.
This is mostly for two reasons:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->One reason is that unikernels have such a minimal footprint that, in a majority of cases, it wouldn&#39;t make a lot of sense to dedicate the entire hardware of one machine to just one of them.<!--]--></li><li data-v-2969431c=""><!--[-->The second reason has to do with hardware compatibility.
By targeting paravirtualized VirtIO<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-virtio" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-virtio" data-v-abeb6555=""><!--[-->9<!--]--><!----></a></sup> devices for instance, a unikernel can indirectly support a lot of physical hardware while retaining its high performance guarantees.<!--]--></li><!--]--></ul><h3 id="a-note-on-hypervisor-support" data-v-9a0ceb30=""><!--[-->A Note on Hypervisor Support<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->Your <code data-v-a031129a=""><!--[-->nginx_qemu-x86_64<!--]--></code> kernel is suitable for the KVM hypervisor and its userspace component in the QEMU<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-qemu" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-qemu" data-v-abeb6555=""><!--[-->10<!--]--><!----></a></sup> virtual machine manager (VMM).
You will leverage both when running the unikernel on the playground box.
Producing a unikernel for a different hypervisor (such as Xen) or a different VMM (such as Firecracker<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-fc" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-fc" data-v-abeb6555=""><!--[-->11<!--]--><!----></a></sup>) would have required <strong data-v-24022f14=""><!--[-->different build time parameters<!--]--></strong> (kernel options).<!--]--></p><p data-v-b025fb9f=""><!--[-->The KVM kernel module effectively turns the Linux host operating system into a <a href="https://en.wikipedia.org/wiki/Hypervisor#Classification" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->type-1 hypervisor<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>.
To leverage this virtualization solution, the host&#39;s CPU needs to support certain CPU instructions through <strong data-v-24022f14=""><!--[-->virtualization extensions<!--]--></strong>.<!--]--></p><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Existing CPU extensions for virtualization</summary><!--[--><p data-v-b025fb9f=""><!--[-->Virtualization extensions for x86-based CPUs are denoted as:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->&#34;SVM&#34; on AMD CPUs, which stands for <em data-v-c83db111=""><!--[-->Secure Virtual Machine<!--]--></em>. Marketed as &#34;AMD-V&#34;.<!--]--></li><li data-v-2969431c=""><!--[-->&#34;VT-x&#34; on Intel CPUs, which stands for <em data-v-c83db111=""><!--[-->Virtual Machine Extensions<!--]--></em>.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->You can check whether those extensions are enabled on the playground box by looking for the corresponding CPU flags:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>grep</span><span> -E</span><span> &#39;svm|vmx&#39;</span><span> --color</span><span> /proc/cpuinfo
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->The other requirement is that the host&#39;s Linux kernel <strong data-v-24022f14=""><!--[-->includes the <code data-v-a031129a=""><!--[-->kvm<!--]--></code> module<!--]--></strong>, expectedly.<!--]--></p><p data-v-b025fb9f=""><!--[-->Both of those requirements can conveniently be checked using a single command: <code data-v-a031129a=""><!--[-->kvm-ok<!--]--></code>.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[-->No KVM module is available inside iximiuz Labs playgrounds, whether <code data-v-a031129a=""><!--[-->kvm_amd<!--]--></code> nor <code data-v-a031129a=""><!--[-->kvm_intel<!--]--></code>, even though the CPU could support it:<!--]--></p><!--[--><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>INFO: /dev/kvm does not exist
HINT:   sudo modprobe kvm_amd
INFO: Your CPU supports KVM extensions
KVM acceleration can be used
</code><!--]--></pre><!--]--></div><!--]--><!--[--><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>modprobe: FATAL: Module kvm_amd not found in directory /lib/modules/5.10.246
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->This is luckily not an obstacle, as QEMU is capable of running virtual machines without KVM support.
Machines with emulated hardware do not offer the same performance as the ones with hardware-assisted virtualization, but this is totally acceptable in the context of this tutorial.<!--]--></p><p data-v-b025fb9f=""><!--[-->Simply omit any KVM-related flag such as <code data-v-a031129a=""><!--[-->-enable-kvm<!--]--></code> in the following command and you should be good to go.<!--]--></p><!--]--></div><h3 id="create-and-run-the-virtual-machine" data-v-9a0ceb30=""><!--[-->Create and Run the Virtual Machine<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->Let&#39;s now run the kernel as a freshly created virtual machine using QEMU.
Notice the <strong data-v-24022f14=""><!--[-->unikernel executable<!--]--></strong> you built in the previous section passed as argument, as well as <strong data-v-24022f14=""><!--[-->kernel command-line parameters<!--]--></strong> pointing at one of the files from your previously generated <strong data-v-24022f14=""><!--[-->cpio archive<!--]--></strong>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>sudo</span><span> qemu-system-x86_64</span><span> \
</span></span><span line="2"><span>  -nographic</span><span> \
</span></span><span line="3"><span>  -m</span><span> 64M</span><span> \
</span></span><span line="4"><span>  -kernel</span><span> workdir/build/nginx_qemu-x86_64</span><span> \
</span></span><span line="5"><span>  -append</span><span> &#39;-c /nginx/conf/nginx.conf&#39;
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Interestingly, those kernel parameters do not look like typical <a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->OS kernel command-line parameters<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, and indeed they are not. Those parameters are <a href="https://nginx.org/en/docs/switches.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Nginx command-line parameters<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>!
A unikernel has OS and application components built into a single kernel artifact, so this should hopefully make a lot of sense.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[-->Should you need to pass parameters to the core kernel components, you would have to keep them separated from the application&#39;s parameters with <code data-v-a031129a=""><!--[-->--<!--]--></code>, as follows:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>netdev.ip= vfs.fstab= random.seed= -- -c /nginx/conf/nginx.conf
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->We will use this mechanism in the next section of this tutorial.<!--]--></p><!--]--></div><p data-v-b025fb9f=""><!--[-->As soon as the virtual machine is created, its console output will be printed to your terminal, just like when booting a Linux box.
It includes the BIOS messages and the unikernel&#39;s boot messages:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>SeaBIOS (version 1.16.3-debian-1.16.3-2)

iPXE (https://ipxe.org) 00:03.0 CA00 PCI2.10 PnP PMM+02FCAE00+02F0AE00 CA00

Booting from ROM..[    0.000000] Info: [libukconsole] &lt;console.c @  176&gt; Registered con0: COM1, flags: IO
[    0.000000] Info: [libukconsole] &lt;console.c @  176&gt; Registered con1: vgacons, flags: -O
[    0.000000] Warn: [libukrandom_lcpu] &lt;init.c @   28&gt; Could not initialize the HWRNG (-95)
[    0.000000] Info: [libkvmplat] &lt;memory.c @  498&gt; Memory 00fd00000000-010000000000 outside mapped area
[    0.000000] Info: [libkvmplat] &lt;setup.c @   99&gt; Switch from bootstrap stack to stack @0x11000
[    0.000000] Info: [libukboot] &lt;boot.c @  280&gt; Unikraft constructor table at 0x2d1000 - 0x2d1058
[    0.000000] Info: [libukboot] &lt;boot.c @  289&gt; Initialize memory allocator...
[    0.000000] Info: [libukallocbbuddy] &lt;bbuddy.c @  658&gt; Initialize binary buddy allocator 11000
[    0.000000] Info: [libukboot] &lt;boot.c @  348&gt; Initialize the IRQ subsystem...
[    0.000000] Info: [libukboot] &lt;boot.c @  355&gt; Initialize platform time...
[    0.000000] Info: [libkvmplat] &lt;tscclock.c @  255&gt; Calibrating TSC clock against i8254 timer
[    0.100033] Info: [libkvmplat] &lt;tscclock.c @  276&gt; Clock source: TSC, frequency estimate is 4192526940 Hz
[    0.100962] Info: [libukboot] &lt;boot.c @  359&gt; Initialize scheduling...
[    0.101289] Info: [libukschedcoop] &lt;schedcoop.c @  289&gt; Initializing cooperative scheduler
[    0.102603] Info: [libukboot] &lt;boot.c @  392&gt; Init Table @ 0x2d1058 - 0x2d1148
[    0.103145] Info: [libukbus] &lt;bus.c @  133&gt; Initialize bus handlers...
[    0.103609] Info: [libukbus] &lt;bus.c @  135&gt; Probe buses...
[    0.104003] Info: [libukbus_pci] &lt;pci_bus.c @  158&gt; PCI 00:00.00 (0600 8086:1237): &lt;no driver&gt;
[    0.104582] Info: [libukbus_pci] &lt;pci_bus.c @  158&gt; PCI 00:01.00 (0600 8086:7000): &lt;no driver&gt;
[    0.105068] Info: [libukbus_pci] &lt;pci_bus.c @  158&gt; PCI 00:02.00 (0300 1234:1111): &lt;no driver&gt;
[    0.105559] Info: [libukbus_pci] &lt;pci_bus.c @  158&gt; PCI 00:03.00 (0200 8086:100e): &lt;no driver&gt;
[    0.106190] Info: [liblwip] &lt;init.c @  174&gt; Initializing lwip
[    0.107715] Warn: [liblwip] &lt;init.c @  460&gt; No network interface attached!
[    0.109350] Info: [libposix_vfs_fstab] &lt;fstab.c @   75&gt; Extracting initrd embedded @ 0x26d000 (7168 bytes) to /...
[    0.110007] Info: [libukallocregion] &lt;region.c @  187&gt; Initialize allocregion allocator @ 0x374020, len 5136
[    0.110719] Info: [libukcpio] &lt;cpio.c @  248&gt; Creating directory /.
[    0.111433] Info: [libukcpio] &lt;cpio.c @  253&gt; Path exists, checking type
[    0.111895] Info: [libukcpio] &lt;cpio.c @  278&gt; Path exists and is dir, doing chmod
[    0.112514] Info: [libukcpio] &lt;cpio.c @  357&gt; ./nginx inode 144531 has more than 1 link (5)
[    0.113055] Info: [libukcpio] &lt;cpio.c @  248&gt; Creating directory /./nginx
[    0.113795] Info: [libukcpio] &lt;cpio.c @  357&gt; ./nginx/conf inode 144532 has more than 1 link (2)
[    0.114352] Info: [libukcpio] &lt;cpio.c @  248&gt; Creating directory /./nginx/conf
[    0.115256] Info: [libukcpio] &lt;cpio.c @  194&gt; Extracting /./nginx/conf/nginx.conf (361 bytes)
[    0.116722] Info: [libukcpio] &lt;cpio.c @  194&gt; Extracting /./nginx/conf/mime.types (5058 bytes)
[    0.117400] Info: [libukcpio] &lt;cpio.c @  357&gt; ./nginx/logs inode 144537 has more than 1 link (2)
[    0.117924] Info: [libukcpio] &lt;cpio.c @  248&gt; Creating directory /./nginx/logs
[    0.118398] Info: [libukcpio] &lt;cpio.c @  194&gt; Extracting /./nginx/logs/error.log (0 bytes)
[    0.118891] Info: [libukcpio] &lt;cpio.c @  357&gt; ./nginx/html inode 144535 has more than 1 link (2)
[    0.119397] Info: [libukcpio] &lt;cpio.c @  248&gt; Creating directory /./nginx/html
[    0.119875] Info: [libukcpio] &lt;cpio.c @  194&gt; Extracting /./nginx/html/index.html (139 bytes)
Powered by
o.   .o       _ _               __ _
Oo   Oo  ___ (_) | __ __  __ _ &#39; _) :_
oO   oO &#39; _ `| | |/ /  _)&#39; _` | |_|  _)
oOo oOO| | | | |   (| | | (_) |  _) :_
 OoOoO ._, ._:_:_,\_._,  .__,_:_, \___)
                 Kiviuq 0.20.0~07044e69
[    0.139012] Info: [libukboot] &lt;boot.c @  472&gt; Pre-init table at 0x2d11e8 - 0x2d11e8
[    0.139491] Info: [libukboot] &lt;boot.c @  483&gt; Constructor table at 0x2d11e8 - 0x2d11e8
[    0.140007] Info: [libukboot] &lt;boot.c @  498&gt; Environment variables:
[    0.140307] Info: [libukboot] &lt;boot.c @  500&gt;        PATH=/bin
[    0.140582] Info: [libukboot] &lt;boot.c @  506&gt; Calling main(3, [&#39;workdir/build/nginx_qemu-x86_64&#39;, &#39;-c&#39;, &#39;/nginx/conf/nginx.conf&#39;])
[    0.155220] Warn: [libposix_process] &lt;rt_sigprocmask.c @   72&gt; __uk_syscall_r_rt_sigprocmask() stubbed
[    0.155796] Warn: [libposix_process] &lt;rt_sigaction.c @   69&gt; __uk_syscall_r_rt_sigaction() stubbed
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->There are three notable things to be observed in this output:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->libposix_vfs_fstab<!--]--></code> component initiated the extraction of the (embedded) initial ramdisk onto the ramfs filesystem at <code data-v-a031129a=""><!--[-->/<!--]--></code> (timestamp <code data-v-a031129a=""><!--[-->0.109350<!--]--></code>).<!--]--></li><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->libukcpio<!--]--></code> component proceeded with the actual extraction of each file and directory from the (embedded) initial ramdisk (from timestamp <code data-v-a031129a=""><!--[-->0.110719<!--]--></code>).<!--]--></li><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->libukboot<!--]--></code> component finally called the kernel&#39;s <code data-v-a031129a=""><!--[-->main()<!--]--></code> function, namely the Nginx application, with the parameters passed on the command line (timestamp <code data-v-a031129a=""><!--[-->0.140582<!--]--></code>).<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->All of this <strong data-v-24022f14=""><!--[-->under 150 milliseconds<!--]--></strong>, on emulated hardware.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[-->For comparison, your current playground micro VM requires about 2 seconds just to reach the point where it can run its <code data-v-a031129a=""><!--[-->init<!--]--></code> process, and about half a second more for reaching the main user target, <strong data-v-24022f14=""><!--[-->with hardware acceleration enabled<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Having to wait that long for starting a Nginx server would feel like an eternity.<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>sudo</span><span> journalctl</span><span> -b</span><span> -o</span><span> short-monotonic
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>[    0.000000] docker-01 kernel: Linux version 5.10.246 (root@buildkitsandbox) ...
</span></span><span line="2"><span>[    0.000000] docker-01 kernel: Command line: panic=1 8250.nr_uarts=1 rw pci=off ...
</span></span><span line="3"><span>     ...
</span></span><span line="4"><span>[    1.808932] docker-01 kernel: Run /sbin/init as init process
</span></span><span line="5"><span>     ...
</span></span><span line="6"><span>[    2.459813] docker-01 systemd[1274]: Reached target default.target - Main User Target.
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Console logs of the playground box</summary><!--[--><p data-v-b025fb9f=""><!--[-->Take a look at the console logs of your playground Linux box and compare them with what you are currently seeing in your terminal :)<!--]--></p><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/consolelogs.png" alt="Console logs of the Linux playground box."/></p><!----></div><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->So, what else can we do with this unikernel at that point? Not much, at least not just yet.
Since you did not create the virtual machine with any <strong data-v-24022f14=""><!--[-->network device<!--]--></strong>, you cannot perform any meaningful interaction with the Nginx server, such as sending a HTTP request to it.
This aspect was deliberately left out from this section.
The good news is that <strong data-v-24022f14=""><!--[-->we are going to remediate to it in the next one<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->For now, terminate the QEMU process by sending the key combination <kbd>Ctrl</kbd>+<kbd>A</kbd> followed by <kbd>X</kbd> on your keyboard.<!--]--></p><!--[--><!--]--><div><!--[--><p data-v-b025fb9f=""><!--[-->In case this key sequence does not work for you, open a second terminal and interrupt the QEMU process from there:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>sudo</span><span> killall</span><span> qemu-system-x86_64
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><div><!--[--><p data-v-b025fb9f=""><!--[-->As the BIOS sends control characters which tend to <strong data-v-24022f14=""><!--[-->corrupt interactive terminals<!--]--></strong>, consider resetting your terminal&#39;s state now to avoid glitches when typing future commands:<!--]--></p><!--[--><!--]--><!--]--></div><h2 id="docker-integration" data-v-b9b1576a=""><!--[-->Docker Integration<!--]--><!----></h2><p data-v-b025fb9f=""><!--[-->By <em data-v-c83db111=""><!--[-->Docker<!--]--></em>, what we really mean here is <em data-v-c83db111=""><!--[-->software conforming to the OCI image and runtime specifications<!--]--></em>.
But that didn&#39;t make for a catchy heading.<!--]--></p><p data-v-b025fb9f=""><!--[-->Arguably, we could have covered a bit of the network configuration required for obtaining a HTTP response from Nginx in the previous section, and this tutorial could have ended here.
Except that would have meant passing on some very interesting ways of running unikernels <strong data-v-24022f14=""><!--[-->alongside Linux containers<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Containers are currently the <strong data-v-24022f14=""><!--[-->dominant deployment unit<!--]--></strong> in the cloud-native landscape.
A rich ecosystem of technologies, rallied behind the Kubernetes<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-k8s" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-k8s" data-v-abeb6555=""><!--[-->12<!--]--><!----></a></sup> project, allows virtually anyone to run services at &#34;Google-scale&#34; nowadays.
Much of this convenience stems from the assumption that everything is packaged and run <strong data-v-24022f14=""><!--[-->like a container<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Could unikernels, instead of competing against this ecosystem, tap into its richness to ease their adoption?<!--]--></p><p data-v-b025fb9f=""><!--[-->It turns out that this idea isn&#39;t new.
Several projects have shown that it is possible to run applications <em data-v-c83db111=""><!--[-->as if they were a container<!--]--></em>, but with the enhanced isolation of a virtual machine.
Those are presented in the section <em data-v-c83db111=""><!--[-->Secure Container Runtimes<!--]--></em> of the post <a href="https://iximiuz.com/en/posts/journey-from-containerization-to-orchestration-and-beyond/#secure-container-runtimes" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Journey From Containerization To Orchestration And Beyond<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>.<!--]--></p><p data-v-b025fb9f=""><!--[-->In this section, you will use two open source projects listed on the page of the <a href="https://unikernelalliance.org/projects/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Unikernel Alliance<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[--><a href="https://github.com/nubificus/bunny" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Bunny<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> — to create an OCI image from your already built Nginx unikernel.<!--]--></li><li data-v-2969431c=""><!--[--><a href="https://github.com/urunc-dev/urunc" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->urunc<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> — to run the Nginx unikernel off its OCI image, this time with network support.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->... all of this using <strong data-v-24022f14=""><!--[-->nothing but the <code data-v-a031129a=""><!--[-->docker<!--]--></code> CLI<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Let&#39;s get started.<!--]--></p><h3 id="oci-image" data-v-9a0ceb30=""><!--[-->OCI Image<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->Bunny makes packaging unikernels as easy as it is to package containers.
The resulting images are <strong data-v-24022f14=""><!--[-->standard OCI images<!--]--></strong> that can be pushed to and pulled from any container registry.<!--]--></p><p data-v-b025fb9f=""><!--[-->First off, change your working directory to <code data-v-a031129a=""><!--[-->oci/<!--]--></code>, assuming that you are currently still in the <code data-v-a031129a=""><!--[-->nginx/<!--]--></code> workspace:<!--]--></p><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->This directory contains a single file named <code data-v-a031129a=""><!--[-->bunnyfile<!--]--></code> with a YAML syntax.<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><p><span data-v-4578ab24="">bunnyfile</span></p><!--[--><pre><!--[--><code><span line="1"><span>#syntax=harbor.nbfc.io/nubificus/bunny:latest
</span></span><span line="2"><span>version</span><span>: </span><span>v0.1
</span></span><span line="3"><span emptylineplaceholder="true">
</span></span><span line="4"><span>platforms</span><span>:
</span></span><span line="5"><span>  framework</span><span>: </span><span>unikraft
</span></span><span line="6"><span>  version</span><span>: </span><span>0.20.0
</span></span><span line="7"><span>  monitor</span><span>: </span><span>qemu
</span></span><span line="8"><span>  architecture</span><span>: </span><span>x86
</span></span><span line="9"><span emptylineplaceholder="true">
</span></span><span line="10"><span>kernel</span><span>:
</span></span><span line="11"><span>  from</span><span>: </span><span>local
</span></span><span line="12"><span>  path</span><span>: </span><span>nginx_qemu-x86_64
</span></span><span line="13"><span emptylineplaceholder="true">
</span></span><span line="14"><span>cmd</span><span>: [</span><span>&#39;-c&#39;</span><span>, </span><span>&#39;/nginx/conf/nginx.conf&#39;</span><span>]
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->You would be right to assume that a unikernel shall be packaged using instructions from a <code data-v-a031129a=""><!--[-->Dockerfile<!--]--></code>, like any other container image.
However, because Bunny was designed to be used as a frontend for BuildKit<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-buildkit" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-buildkit" data-v-abeb6555=""><!--[-->13<!--]--><!----></a></sup>, it is able to parse its <strong data-v-24022f14=""><!--[-->own build definition<!--]--></strong> which is tailored for packaging the special software artifact which are unikernels.<!--]--></p><p data-v-b025fb9f=""><!--[-->In a <code data-v-a031129a=""><!--[-->bunnyfile<!--]--></code>, the generic and imperative instructions of a <code data-v-a031129a=""><!--[-->Dockerfile<!--]--></code> (<code data-v-a031129a=""><!--[-->ENV<!--]--></code>, <code data-v-a031129a=""><!--[-->COPY<!--]--></code>, <code data-v-a031129a=""><!--[-->RUN<!--]--></code>, etc.) are traded for declarative and structured attributes which describe the nature of the unikernel and how it should be run:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->framework<!--]--></code> it was built with (what type of unikernel).<!--]--></li><li data-v-2969431c=""><!--[-->The virtual machine <code data-v-a031129a=""><!--[-->monitor<!--]--></code> that can run it (what hypervisor).<!--]--></li><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->cmd<!--]--></code> (command-line parameters) that should be supplied to the kernel at boot time.<!--]--></li><li data-v-2969431c=""><!--[-->The <code data-v-a031129a=""><!--[-->path<!--]--></code> at which the kernel image resides on the local filesystem.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->The <code data-v-a031129a=""><!--[-->Dockerfile<!--]--></code> syntax is also supported by Bunny as an alternative to the <code data-v-a031129a=""><!--[-->bunnyfile<!--]--></code> syntax.<!--]--></p><p data-v-b025fb9f=""><!--[-->According to that last <code data-v-a031129a=""><!--[-->path<!--]--></code> instruction, Bunny expects the unikernel executable inside the current directory with the name <code data-v-a031129a=""><!--[-->nginx_qemu-x86_64<!--]--></code>, so you will need to copy it there:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>cp</span><span> ../workdir/build/nginx_qemu-x86_64</span><span> .
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Run a Docker image build inside the current directory, using <code data-v-a031129a=""><!--[-->bunnyfile<!--]--></code> in place of the default <code data-v-a031129a=""><!--[-->Dockerfile<!--]--></code>, and see what happens:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> image</span><span> build</span><span> \
</span></span><span line="2"><span>  -f</span><span> bunnyfile</span><span> \
</span></span><span line="3"><span>  -t</span><span> lab/nginx-unikernel:latest</span><span> \
</span></span><span line="4"><span>  .
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>[+] Building 6.0s (8/8) FINISHED                                     docker:default
 =&gt; [internal] load build definition from bunnyfile                            0.0s
 =&gt; =&gt; transferring dockerfile: 274B                                           0.0s
 =&gt; resolve image config for docker-image://harbor.nbfc.io/nubificus/bunny:la  1.4s
 =&gt; docker-image://harbor.nbfc.io/nubificus/bunny:latest@sha256:5c4ea3e06932d  3.9s
 =&gt; =&gt; resolve harbor.nbfc.io/nubificus/bunny:latest@sha256:5c4ea3e06932d2668  0.0s
 =&gt; =&gt; sha256:0e389784892b49cd4cd99ca91a80297d06961a154497f72 6.32MB / 6.32MB  3.7s
 =&gt; =&gt; extracting sha256:0e389784892b49cd4cd99ca91a80297d06961a15449f3efe7c0c  0.1s
 =&gt; Internal:Read-bunnyfile                                                    0.0s
 =&gt; =&gt; transferring context: 274B                                              0.0s
 =&gt; local://context                                                            0.0s
 =&gt; =&gt; transferring context: 2.01MB                                            0.0s
 =&gt; copy /nginx_qemu-x86_64 /.boot/kernel                                      0.0s
 =&gt; mkfile /urunc.json                                                         0.0s
 =&gt; exporting to image                                                         0.3s
 =&gt; =&gt; exporting layers                                                        0.2s
 =&gt; =&gt; exporting manifest sha256:f32ca3bca1dd80259da2fdd4fbc1e4e31127f50d6e02  0.0s
 =&gt; =&gt; exporting config sha256:d6fee3cd090c49bcaea28735d783b410fe16519199a158  0.0s
 =&gt; =&gt; exporting attestation manifest sha256:6b208d4aab720d228a27adf0576d7050  0.0s
 =&gt; =&gt; exporting manifest list sha256:42e109d9ea5fba9c86de0296559468f13d3f2f7  0.0s
 =&gt; =&gt; naming to docker.io/lab/nginx-unikernel:latest                          0.0s
 =&gt; =&gt; unpacking to docker.io/lab/nginx-unikernel:latest                       0.0s
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Even though <code data-v-a031129a=""><!--[-->bunny<!--]--></code> does not come pre-installed inside the playground box, the build completed without hurdle.
Is this surprising? Upon closer inspection, it shouldn&#39;t be.<!--]--></p><p data-v-b025fb9f=""><!--[-->Because the first line in the <code data-v-a031129a=""><!--[-->bunnyfile<!--]--></code> is a <code data-v-a031129a=""><!--[-->syntax<!--]--></code> directive with a special meaning, BuildKit was able to understand that it had to <strong data-v-24022f14=""><!--[-->load the <code data-v-a031129a=""><!--[-->bunny<!--]--></code> frontend dynamically<!--]--></strong> from a container image and invoke it to generate its build graph:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Neat!<!--]--></p><p data-v-b025fb9f=""><!--[-->The OCI images produced by Bunny have the peculiarity of being automatically labeled with some exotic key-values.
You can enumerate these by inspecting the <code data-v-a031129a=""><!--[-->lab/nginx-unikernel:latest<!--]--></code> image you created:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> image</span><span> inspect</span><span> \
</span></span><span line="2"><span>  --format=&#39;{{range $k,$v := .Config.Labels}}{{printf &#34;%s=%s\n&#34; $k $v}}{{end}}&#39;</span><span> \
</span></span><span line="3"><span>  lab/nginx-unikernel:latest
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>com.urunc.unikernel.binary=/.boot/kernel
</span></span><span line="2"><span>com.urunc.unikernel.cmdline=-c /nginx/conf/nginx.conf
</span></span><span line="3"><span>com.urunc.unikernel.hypervisor=qemu
</span></span><span line="4"><span>com.urunc.unikernel.initrd=
</span></span><span line="5"><span>com.urunc.unikernel.mountRootfs=false
</span></span><span line="6"><span>com.urunc.unikernel.unikernelType=unikraft
</span></span><span line="7"><span>com.urunc.unikernel.unikernelVersion=0.20.0
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Without further ado, let&#39;s figure out what makes those labels useful.<!--]--></p><h3 id="oci-runtime" data-v-9a0ceb30=""><!--[-->OCI Runtime<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->In order to create and run a container, the <code data-v-a031129a=""><!--[-->containerd<!--]--></code> container manager invokes a <em data-v-c83db111=""><!--[-->container runtime<!--]--></em> that prepares the containerized process to run within the desired namespaces and cgroups hierarchy.
In the realm of Linux containers, the container runtime of reference is <a href="https://github.com/opencontainers/runc" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[--><code data-v-a031129a=""><!--[-->runc<!--]--></code><!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>.<!--]--></p><p data-v-b025fb9f=""><!--[--><code data-v-a031129a=""><!--[-->runc<!--]--></code> makes numerous assumptions about the way a container has to run:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->a Linux process<!--]--></li><li data-v-2969431c=""><!--[-->on top of a shared kernel<!--]--></li><li data-v-2969431c=""><!--[-->with standard I/O streams (stdin/stdout/stderr)<!--]--></li><li data-v-2969431c=""><!--[-->isolated from other processes by namespaces<!--]--></li><li data-v-2969431c=""><!--[-->with its resources constrained by the facilities of that shared kernel (cgroups)<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->A unikernel is different. As a virtual machine running under the management of a virtual machine manager:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->Its resource constraints are enforced by virtualized hardware, not just cgroups.<!--]--></li><li data-v-2969431c=""><!--[-->The host&#39;s namespaces do not propagate inside the guest, making namespaces such as <code data-v-a031129a=""><!--[-->PID<!--]--></code>, <code data-v-a031129a=""><!--[-->Mount<!--]--></code>, <code data-v-a031129a=""><!--[-->User<!--]--></code> and <code data-v-a031129a=""><!--[-->Cgroup<!--]--></code> largely irrelevant.<!--]--></li><li data-v-2969431c=""><!--[-->Bind mounts must be passed from the host through the hypervisor, in a form that is supported by the guest kernel.<!--]--></li><li data-v-2969431c=""><!--[-->I/O streams of the guest application are not forwarded outside of the virtual machine.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->For those reasons, spinning up a unikernel–or in fact any kind of virtual machine–from a container image requires a container runtime that is <strong data-v-24022f14=""><!--[-->aware of those distinctions<!--]--></strong> and is capable of <strong data-v-24022f14=""><!--[-->operating around them<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->That runtime is <code data-v-a031129a=""><!--[-->urunc<!--]--></code>.<!--]--></p><p data-v-b025fb9f=""><!--[-->In <code data-v-a031129a=""><!--[-->containerd<!--]--></code>, there is a naming convention that allows selecting a specific runtime for running a container without having to configure <code data-v-a031129a=""><!--[-->containerd<!--]--></code> to know about that runtime.
By requesting a runtime named <code data-v-a031129a=""><!--[-->io.containerd.$(RUNTIME).v2<!--]--></code> on the command line, <code data-v-a031129a=""><!--[-->containerd<!--]--></code> attempts to create the container through the shim executable <code data-v-a031129a=""><!--[-->containerd-shim-$(RUNTIME)-v2<!--]--></code>, if available in the PATH.<!--]--></p><p data-v-b025fb9f=""><!--[-->It turns out that the <code data-v-a031129a=""><!--[-->urunc<!--]--></code> shim executable installed on the playground box follows that naming convention:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>ls</span><span> -F1</span><span> /usr/local/bin/</span><span>*</span><span>urunc</span><span>*
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>/usr/local/bin/containerd-shim-urunc-v2*
/usr/local/bin/urunc*
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->So all you need to do in order to pick <code data-v-a031129a=""><!--[-->urunc<!--]--></code> for running your Nginx unikernel image is specify that runtime name as an argument to <code data-v-a031129a=""><!--[-->docker container run<!--]--></code>.
We also set its name explicitly to <code data-v-a031129a=""><!--[-->unikernel<!--]--></code> to facilitate future <code data-v-a031129a=""><!--[-->docker<!--]--></code> commands:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> run</span><span> \
</span></span><span line="2"><span>  -d</span><span> --rm</span><span> \
</span></span><span line="3"><span>  --name</span><span> unikernel</span><span> \
</span></span><span line="4"><span>  --runtime</span><span> io.containerd.urunc.v2</span><span> \
</span></span><span line="5"><span>  lab/nginx-unikernel:latest
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0
</code><!--]--></pre><!--]--></div><!--]--><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Default runtime</summary><!--[--><p data-v-b025fb9f=""><!--[-->When the <code data-v-a031129a=""><!--[-->--runtime<!--]--></code> flag is not specified in <code data-v-a031129a=""><!--[-->docker container run<!--]--></code> commands, <code data-v-a031129a=""><!--[-->containerd<!--]--></code> chooses the runtime based on its own default.
In a vanilla <code data-v-a031129a=""><!--[-->containerd<!--]--></code> distribution, this default is <code data-v-a031129a=""><!--[-->runc<!--]--></code>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>containerd</span><span> config</span><span> default
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><p><span data-v-4578ab24="">config.toml</span></p><!--[--><pre><!--[--><code><span line="1"><span>  [plugins.&#39;io.containerd.cri.v1.runtime&#39;]
</span></span><span line="2"><span>    #...
</span></span><span line="3"><span emptylineplaceholder="true">
</span></span><span line="4"><span>    [plugins.&#39;io.containerd.cri.v1.runtime&#39;.containerd]
</span></span><span line="5"><span>      default_runtime_name</span><span> = </span><span>&#39;runc&#39;
</span></span><span line="6"><span>      #...
</span></span><span line="7"><span emptylineplaceholder="true">
</span></span><span line="8"><span>      [plugins.&#39;io.containerd.cri.v1.runtime&#39;.containerd.runtimes]
</span></span><span line="9"><span>        [plugins.&#39;io.containerd.cri.v1.runtime&#39;.containerd.runtimes.runc]
</span></span><span line="10"><span>          runtime_type</span><span> = </span><span>&#39;io.containerd.runc.v2&#39;
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->Verify that the unikernel is running (as a container):<!--]--></p><!--[--><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>CONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS     NAMES
e2348010d439   lab/nginx-unikernel:latest   &#34;-c /nginx/conf/ngin…&#34;   10 seconds ago   Up 10 seconds             unikernel
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Then print its logs (console output):<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> logs</span><span> unikernel
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>...
Powered by
o.   .o       _ _               __ _
Oo   Oo  ___ (_) | __ __  __ _ &#39; _) :_
oO   oO &#39; _ `| | |/ /  _)&#39; _` | |_|  _)
oOo oOO| | | | |   (| | | (_) |  _) :_
 OoOoO ._, ._:_:_,\_._,  .__,_:_, \___)
                 Kiviuq 0.20.0~07044e69
[    0.131892] Info: [libukboot] &lt;boot.c @  472&gt; Pre-init table at 0x2d11e8 - 0x2d11e8
[    0.132060] Info: [libukboot] &lt;boot.c @  483&gt; Constructor table at 0x2d11e8 - 0x2d11e8
[    0.132218] Info: [libukboot] &lt;boot.c @  498&gt; Environment variables:
[    0.132344] Info: [libukboot] &lt;boot.c @  500&gt;        PATH=/bin
[    0.132467] Info: [libukboot] &lt;boot.c @  506&gt; Calling main(3, [&#39;/.boot/kernel&#39;, &#39;-c&#39;, &#39;/nginx/conf/nginx.conf&#39;])
[    0.153510] Warn: [libposix_process] &lt;rt_sigprocmask.c @   72&gt; __uk_syscall_r_rt_sigprocmask() stubbed
[    0.153760] Warn: [libposix_process] &lt;rt_sigaction.c @   69&gt; __uk_syscall_r_rt_sigaction() stubbed
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->The output is more or less identical to the one shown when you ran the unikernel via QEMU earlier.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[-->As the BIOS sends control characters which tend to <strong data-v-24022f14=""><!--[-->corrupt interactive terminals<!--]--></strong>, consider resetting your terminal&#39;s state now to avoid glitches when typing future commands:<!--]--></p><!--[--><!--]--><!--]--></div><p data-v-b025fb9f=""><!--[-->It doesn&#39;t end here.
Check the IP address assigned to the container:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> inspect</span><span> \
</span></span><span line="2"><span>  --format=&#39;{{.NetworkSettings.Networks.bridge.IPAddress}}&#39;</span><span> \
</span></span><span line="3"><span>  unikernel
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->Then send a HTTP request to that IP address using <code data-v-a031129a=""><!--[-->curl<!--]--></code>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>curl</span><span> -D-</span><span> http://172.17.0.2
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>HTTP/1.1 200 OK
</span></span><span line="2"><span>Server: nginx/1.15.6
</span></span><span line="3"><span>Date: Fri, 01 Jan 2026 00:00:00 GMT
</span></span><span line="4"><span>Content-Type: text/html
</span></span><span line="5"><span>Content-Length: 139
</span></span><span line="6"><span>Last-Modified: Fri, 01 Jan 2026 00:00:00 GMT
</span></span><span line="7"><span>Connection: keep-alive
</span></span><span line="8"><span>ETag: &#34;69611cbd-83&#34;
</span></span><span line="9"><span>Accept-Ranges: bytes
</span></span><span line="10"><span emptylineplaceholder="true">
</span></span><span line="11"><span>&lt;!</span><span>DOCTYPE</span><span> html</span><span>&gt;
</span></span><span line="12"><span>&lt;</span><span>html</span><span>&gt;
</span></span><span line="13"><span>&lt;</span><span>head</span><span>&gt;
</span></span><span line="14"><span>  &lt;</span><span>title</span><span>&gt;</span><span>Hello from iximiuz Labs!</span><span>&lt;/</span><span>title</span><span>&gt;
</span></span><span line="15"><span>&lt;/</span><span>head</span><span>&gt;
</span></span><span line="16"><span>&lt;</span><span>body</span><span>&gt;
</span></span><span line="17"><span>  &lt;</span><span>h1</span><span>&gt;</span><span>Hello from iximiuz Labs!</span><span>&lt;/</span><span>h1</span><span>&gt;
</span></span><span line="18"><span>&lt;/</span><span>body</span><span>&gt;
</span></span><span line="19"><span>&lt;/</span><span>html</span><span>&gt;
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->This time around, <strong data-v-24022f14=""><!--[-->networking is fully functional<!--]--></strong>, and it appears to be working just like with a container!<!--]--></p><p data-v-b025fb9f=""><!--[-->That is great progress compared to what you ended up with at the end of the previous section.
Not only interacting with the application running inside the guest is now a reality, but reaching that state was <strong data-v-24022f14=""><!--[-->remarkably easy<!--]--></strong>;
you did not need to use anything but <code data-v-a031129a=""><!--[-->docker<!--]--></code> commands, or even make adjustments to your unikernel.<!--]--></p><h2 id="understanding-the-unikernel-container" data-v-b9b1576a=""><!--[-->Understanding the Unikernel Container<!--]--><!----></h2><p data-v-b025fb9f=""><!--[-->We are now going to dissect what just happened in order to understand how making the unikernel feel like we are interacting with a Linux container is even possible.<!--]--></p><p data-v-b025fb9f=""><!--[-->As explained earlier in this tutorial, iximiuz Labs playgrounds do not support KVM.
During its initialization, your playground box built and installed a <em data-v-c83db111=""><!--[-->patched<!--]--></em> version of <code data-v-a031129a=""><!--[-->urunc<!--]--></code> which omits certain problematic QEMU CLI flags such as <code data-v-a031129a=""><!--[-->-enable-kvm<!--]--></code>.<!--]--></p><h3 id="process-tree" data-v-9a0ceb30=""><!--[-->Process Tree<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->If you look at the processes currently running in your playground box, you should see a process with a familiar command line at the bottom of the list:<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>  PPID     PID    PGID ... COMMAND
</span></span><span line="2"><span>     1   23927   23927 ... /usr/local/bin/containerd-shim-urunc-v2 -namespace moby -id e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0 -address /run/containerd/containerd.sock
</span></span><span line="3"><span> 23927   23951   23951 ...  \_ /usr/bin/qemu-system-x86_64 -m 268M -L /usr/share/qemu -nographic -vga none -smp 1 --sandbox on,obsolete=deny,elevateprivileges=deny,spawn=deny,resourcecontrol=deny -kernel /.boot/kernel -net nic,model=virtio,macaddr=6e:e5:40:9d:22:87 -net tap,script=no,downscript=no,ifname=tap0_urunc -append Unikraft  env.vars=[ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=e2348010d439 ] netdev.ip=172.17.0.2/24:172.17.0.1:8.8.8.8    -- -c /nginx/conf/nginx.conf
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->That&#39;s right, a QEMU process, parented to the <code data-v-a031129a=""><!--[-->urunc<!--]--></code> container shim, with arguments that look very similar to the ones you passed on the command line to create the virtual machine manually in the previous section of this tutorial.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[-->Remember those somewhat exotic labels set on the container image?
You just found the usage of two of them:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>com.urunc.unikernel.hypervisor=qemu
com.urunc.unikernel.cmdline=-c /nginx/conf/nginx.conf
</code><!--]--></pre><!--]--></div><!--]--><!--]--></div><p data-v-b025fb9f=""><!--[-->This parenting of <code data-v-a031129a=""><!--[-->qemu-system-x86_64<!--]--></code> to the container shim is the reason why <code data-v-a031129a=""><!--[-->docker container logs<!--]--></code> printed the standard output (stdout) of the QEMU process, namely the virtual machine&#39;s console output.<!--]--></p><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">Intermediate process tree</summary><!--[--><p data-v-b025fb9f=""><!--[-->If you manage to list the processes fast enough while the container is still being created, you will notice that the process with the container&#39;s PID was originally <strong data-v-24022f14=""><!--[-->a <code data-v-a031129a=""><!--[-->urunc<!--]--></code> process<!--]--></strong> (not to be mistaken for a <em data-v-c83db111=""><!--[-->urunc shim<!--]--></em> process):<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>  PPID     PID    PGID ... COMMAND
</span></span><span line="2"><span>     1   23927   23927 ... /usr/local/bin/containerd-shim-urunc-v2 -namespace moby -id e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0 -address /run/containerd/containerd.sock
</span></span><span line="3"><span> 23927   23951   23951 ...  \_ urunc --root /run/containerd/runc/moby --log /run/containerd/io.containerd.runtime.v2.task/moby/e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0/log.json --log-format json create --bundle /run/containerd/io.containerd.runtime.v2.task/moby/e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0 --pid-file /run/containerd/io.containerd.runtime.v2.task/moby/e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0/init.pid e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0 --reexec
</span></span><span line="4"><span> 23927   23973   23927 ...  \_ urunc --root /run/containerd/runc/moby --log /run/containerd/io.containerd.runtime.v2.task/moby/e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0/log.json --log-format json start e2348010d439f4f8197d1d706b2f99fbeebab76595e8320478057f7c48fc2da0
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Upon starting the container, <code data-v-a031129a=""><!--[-->urunc<!--]--></code> eventually &#34;became&#34; the container process (<code data-v-a031129a=""><!--[-->qemu-system-x86_64<!--]--></code>) by means of a POSIX <a href="https://man7.org/linux/man-pages/man2/execve.2.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[--><code data-v-a031129a=""><!--[-->exec<!--]--></code><!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> system call.<!--]--></p><p data-v-b025fb9f=""><!--[-->The author of this tutorial wrote extensively about what happens between the creation of the <em data-v-c83db111=""><!--[-->container shim<!--]--></em> and that of the <em data-v-c83db111=""><!--[-->container process<!--]--></em> in a post titled <a href="https://acotten.com/2023/08/17/oci-runtime-create-flow" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->OCI runtime: container creation flow<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>.<!--]--></p><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->This parent-child relationship between the two processes also means that the &#34;unikernel container&#34; <em data-v-c83db111=""><!--[-->is<!--]--></em> a virtual machine monitored by QEMU.
In other words, where a regular Nginx container would have <code data-v-a031129a=""><!--[-->nginx<!--]--></code> running as the container process, <code data-v-a031129a=""><!--[-->urunc<!--]--></code> created a <code data-v-a031129a=""><!--[-->qemu-system-x86_64<!--]--></code> process to run the unikernel.<!--]--></p><p data-v-b025fb9f=""><!--[-->To verify this, compare the process ID (PID) of the container seen by Docker to the process ID seen by Linux for the process name <code data-v-a031129a=""><!--[-->qemu-system-x86_64<!--]--></code>.
Both commands will print the same value:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> inspect</span><span> unikernel</span><span> --format=&#39;{{.State.Pid}}&#39;
</span></span><span line="2"><span>pidof</span><span> qemu-system-x86_64
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><!--]--><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/ctprocesses.png" alt="QEMU is parented to a container shim process which forwards its I/O streams like a regular container."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Unikernel VM parented to a container shim.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->To verify that <code data-v-a031129a=""><!--[-->urunc<!--]--></code> created the <code data-v-a031129a=""><!--[-->qemu-system-x86_64<!--]--></code> process inside a distinct PID namespace like in a regular container, enter the <code data-v-a031129a=""><!--[-->pid<!--]--></code> namespace of the <code data-v-a031129a=""><!--[-->unikernel<!--]--></code> container and list its processes.<!--]--></p><p data-v-b025fb9f=""><!--[-->QEMU will be displayed with the PID <code data-v-a031129a=""><!--[-->1<!--]--></code> this time, not <code data-v-a031129a=""><!--[-->23951<!--]--></code>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> run</span><span> \
</span></span><span line="2"><span>  --pid=container:unikernel</span><span> \
</span></span><span line="3"><span>  busybox</span><span> \
</span></span><span line="4"><span>  ps
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>PID   USER     TIME  COMMAND
    1 root      0:00 /usr/bin/qemu-system-x86_64 -m 268M -L /usr/share/qemu -nographic -vga none -smp 1 --sandbox on,obsolete=deny,elevateprivileges=deny,spawn=deny,resourcecontrol=deny -kernel /.boot/kernel -net nic,model=virtio,macaddr=ce:e9:db:b0:cd:d7 -net tap,script=no,downscript=no,ifname=tap0_urunc -append Unikraft  env.vars=[ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=a9c71edb95e1 ] netdev.ip=172.17.0.2/24:172.17.0.1:8.8.8.8    -- -c /nginx/conf/nginx.conf
    9 root      0:00 ps
</code><!--]--></pre><!--]--></div><!--]--><h3 id="network-plumbing" data-v-9a0ceb30=""><!--[-->Network Plumbing<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->The PID namespace may be largely irrelevant for the unikernel, but another namespace is crucial: <strong data-v-24022f14=""><!--[-->the <em data-v-c83db111=""><!--[-->Network<!--]--></em> namespace<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Before diving into namespaces, let&#39;s take another look at the command-line parameters of the QEMU process.
Compared to our first, manual invocation of QEMU, the one spawned by <code data-v-a031129a=""><!--[-->urunc<!--]--></code> has a few additional flags related to network devices:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>-net nic,model=virtio,macaddr=ce:e9:db:b0:cd:d7
-net tap,script=no,downscript=no,ifname=tap0_urunc
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[--><code data-v-a031129a=""><!--[-->urunc<!--]--></code> also prepended the kernel command-line declared in the container image with few extra parameters, especially:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>netdev.ip=172.17.0.2/24:172.17.0.1:8.8.8.8
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->The fact that <code data-v-a031129a=""><!--[-->172.17.0.2<!--]--></code> is the same IP address as the one you sent a HTTP request to earlier is noteworthy.<!--]--></p><p data-v-b025fb9f=""><!--[-->You might remember the following statement from right after you printed the logs of the container:<!--]--></p><blockquote data-v-5c66202a=""><!--[--><p data-v-b025fb9f=""><!--[-->The output is more or less identical to the one shown when you ran the unikernel via QEMU earlier.<!--]--></p><!--]--></blockquote><p data-v-b025fb9f=""><!--[-->Well, a few network-related console messages actually are a novelty compared to the first, manual run of the Nginx unikernel:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> logs</span><span> unikernel</span><span> |</span><span> grep</span><span> -E</span><span> &#39;lwip|net|en1&#39;</span><span> --color
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>[    0.104734] Info: [libuknetdev] &lt;netdev.c @  198&gt; Registered netdev0: 0x373030 (virtio-net)
</span></span><span line="2"><span>[    0.105111] Info: [liblwip] &lt;init.c @  174&gt; Initializing lwip
</span></span><span line="3"><span>[    0.106984] Info: [liblwip] &lt;init.c @  225&gt; Attach network device 0 to lwIP...
</span></span><span line="4"><span>[    0.109975] Info: [libuknetdev] &lt;netdev.c @  407&gt; netdev0: Configured interface
</span></span><span line="5"><span>[    0.111557] Info: [libuknetdev] &lt;netdev.c @  554&gt; netdev0: Configured receive queue 0
</span></span><span line="6"><span>[    0.111943] Info: [libuknetdev] &lt;netdev.c @  587&gt; netdev0: Configured transmit queue 0
</span></span><span line="7"><span>[    0.112321] Info: [libvirtio_net] &lt;virtio_net.c @ 1394&gt; virtio-net: 0 started
</span></span><span line="8"><span>[    0.112966] Info: [libuknetdev] &lt;netdev.c @  606&gt; netdev0: Started interface
</span></span><span line="9"><span>[    0.113401] Info: [liblwip] &lt;init.c @  345&gt; en1: Hardware address: ce:e9:db:b0:cd:d7
</span></span><span line="10"><span>[    0.113561] Info: [liblwip] &lt;init.c @  352&gt; en1: Check checksums:
</span></span><span line="11"><span>[    0.113716] Info: [liblwip] &lt;init.c @  371&gt; en1: Generate checksums: IP UDP TCP ICMP ICMP6
</span></span><span line="12"><span>[    0.113992] Info: [liblwip] &lt;init.c @  408&gt; en1: Primary DNS server: 8.8.8.8
</span></span><span line="13"><span>[    0.114168] Info: [liblwip] &lt;init.c @  442&gt; en1: Set as default interface
</span></span><span line="14"><span>en1: Added
</span></span><span line="15"><span>en1: Interface is up
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Those messages indicate that the unikernel recognized the network device provided by QEMU (a VirtIO device) and was able to configure a corresponding <strong data-v-24022f14=""><!--[-->network interface<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->For reference, the first, manual run only showed:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>[    0.107715] Warn: [liblwip] &lt;init.c @  460&gt; No network interface attached!
</code><!--]--></pre><!--]--></div><!--]--><div><!--[--><p data-v-b025fb9f=""><!--[-->How did <code data-v-a031129a=""><!--[-->urunc<!--]--></code> know what command-line parameters to pass to the unikernel to configure its network stack?<!--]--></p><p data-v-b025fb9f=""><!--[-->Remember those somewhat exotic labels set on the container image (again)?
One of them was:<!--]--></p><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>com.urunc.unikernel.unikernelType=unikraft
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->By knowing the type of unikernel it is running, <code data-v-a031129a=""><!--[-->urunc<!--]--></code> is capable of selecting the appropriate command-line parameters.
Mystery solved!<!--]--></p><!--]--></div><p data-v-b025fb9f=""><!--[-->Now, the network device and kernel command-line parameters alone <strong data-v-24022f14=""><!--[-->do not suffice<!--]--></strong> to making the guest&#39;s Nginx web server reachable from the host.
Why? Because at that point the IP address <code data-v-a031129a=""><!--[-->172.17.0.2<!--]--></code> is <strong data-v-24022f14=""><!--[-->not yet assigned to any network interface on the host<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Before a container is started, <code data-v-a031129a=""><!--[-->containerd<!--]--></code> calls a CNI<sup><a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fn-cni" rel="noopener noreferrer" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-cni" data-v-abeb6555=""><!--[-->14<!--]--><!----></a></sup> plugin to set up the container&#39;s network.
It is during that process that the network interface(s) of the container are created and configured.
Typically, the outcome is a <strong data-v-24022f14=""><!--[-->pair of <a href="https://man7.org/linux/man-pages/man4/veth.4.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->veth<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> devices<!--]--></strong>: one inside the root Network namespace and one inside the container&#39;s.
The root end gets attached to a <a href="https://man.netbsd.org/bridge.4" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->bridge<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> device so that network packets can be routed to the container over that bridge (<code data-v-a031129a=""><!--[-->docker0<!--]--></code> on the playground box).<!--]--></p><div data-v-370f298d=""><!--[--><details data-v-370f298d=""><summary data-v-370f298d="">CNI-less setup</summary><!--[--><p data-v-b025fb9f=""><!--[-->In certain environments, such as your playground box, <code data-v-a031129a=""><!--[-->containerd<!--]--></code> isn&#39;t configured to set up the network by calling a CNI plugin.
In that case, the Docker daemon handles the setup of the container&#39;s network itself.<!--]--></p><!--]--></details><!--]--></div><p data-v-b025fb9f=""><!--[-->Take a look at the network links inside both of these namespaces:<!--]--></p><div><div><!--[--><p>Root links</p><p>Container links</p><!--]--></div><div><!--[--><div><!--[--><p data-v-b025fb9f=""><!--[-->The <code data-v-a031129a=""><!--[-->veth0744538<!--]--></code> interface with the ID <code data-v-a031129a=""><!--[-->8<!--]--></code> is the root end of the veth pair.<!--]--></p><p data-v-b025fb9f=""><!--[-->Its peer is the interface with the ID <code data-v-a031129a=""><!--[-->2<!--]--></code> (denoted by the suffix <code data-v-a031129a=""><!--[-->@if2<!--]--></code>) inside the container&#39;s Network namespace.<!--]--></p><!--[--><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
</span></span><span line="2"><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span line="3"><span>    inet 127.0.0.1/8 scope host lo
</span></span><span line="4"><span>       valid_lft forever preferred_lft forever
</span></span><span line="5"><span>    inet6 ::1/128 scope host
</span></span><span line="6"><span>       valid_lft forever preferred_lft forever
</span></span><span line="7"><span>...
</span></span><span line="8"><span>4: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
</span></span><span line="9"><span>    link/ether 5e:71:8d:13:42:0a brd ff:ff:ff:ff:ff:ff
</span></span><span line="10"><span>    inet 172.16.0.2/24 brd 172.16.0.255 scope global eth0
</span></span><span line="11"><span>       valid_lft forever preferred_lft forever
</span></span><span line="12"><span>    inet6 fe80::8c03:f0ff:fee0:ce5f/64 scope link
</span></span><span line="13"><span>       valid_lft forever preferred_lft forever
</span></span><span line="14"><span>5: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
</span></span><span line="15"><span>    link/ether ee:9c:f4:49:d0:83 brd ff:ff:ff:ff:ff:ff
</span></span><span line="16"><span>    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
</span></span><span line="17"><span>       valid_lft forever preferred_lft forever
</span></span><span line="18"><span>    inet6 fe80::ec9c:f4ff:fe49:d083/64 scope link
</span></span><span line="19"><span>       valid_lft forever preferred_lft forever
</span></span><span line="20"><span>8: veth0744538@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default
</span></span><span line="21"><span>    link/ether 06:33:5d:f8:cd:da brd ff:ff:ff:ff:ff:ff link-netnsid 0
</span></span><span line="22"><span>    inet6 fe80::433:5dff:fef8:cdda/64 scope link
</span></span><span line="23"><span>       valid_lft forever preferred_lft forever
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><!--]--></div></div><hr data-v-34b8dd49=""/><p data-v-b025fb9f=""><!--[-->In a standard setup, the container&#39;s Network namespace would only have the <code data-v-a031129a=""><!--[-->lo<!--]--></code> (loopback) and <code data-v-a031129a=""><!--[-->eth0<!--]--></code> (veth) interfaces.
On the <em data-v-c83db111=""><!--[-->Container<!--]--></em> tab above though, you can see an additional <code data-v-a031129a=""><!--[-->tap0_urunc<!--]--></code> interface. What is that?<!--]--></p><p data-v-b025fb9f=""><!--[--><code data-v-a031129a=""><!--[-->tap0_urunc<!--]--></code> is a <a href="https://man.netbsd.org/tap.4" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->tap<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> network device created by <code data-v-a031129a=""><!--[-->urunc<!--]--></code> and provided to QEMU with the command-line flag <code data-v-a031129a=""><!--[-->-net tap,ifname=tap0_urunc<!--]--></code>.
The usage of such devices is the standard way in QEMU to connect the network interface of the guest (unikernel) to a real network, such as the one of the Linux host.<!--]--></p><p data-v-b025fb9f=""><!--[-->This is an implementation decision of <code data-v-a031129a=""><!--[-->urunc<!--]--></code>, and a clever design when you think about it.<!--]--></p><p data-v-b025fb9f=""><!--[-->CNI can configure network interfaces in a lot of <a href="https://www.cni.dev/plugins/current/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->different manners<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> inside a container&#39;s network.
A containerized Linux process can always assume that the network is usable as-is, thanks to the plumbing operated by CNI.<!--]--></p><p data-v-b025fb9f=""><!--[-->For an application running as a guest inside a virtual machine, <strong data-v-24022f14=""><!--[-->this assumption falls apart<!--]--></strong> and deploying a working network stack may require non-trivial configuration of both the host and guest.<!--]--></p><p data-v-b025fb9f=""><!--[-->A container runtime such as <code data-v-a031129a=""><!--[-->urunc<!--]--></code> cannot possibly have differentiated logic for each CNI plugin under the sun.
What it can do, however, is rely on the <code data-v-a031129a=""><!--[-->tap0_urunc<!--]--></code> interface to look the same in every unikernel container, then configure the network to redirect all network packets between <code data-v-a031129a=""><!--[-->tap0_urunc<!--]--></code> and the default interface inside the container&#39;s Network namespace (<code data-v-a031129a=""><!--[-->eth0<!--]--></code>).<!--]--></p><p data-v-b025fb9f=""><!--[--><code data-v-a031129a=""><!--[-->urunc<!--]--></code> implements this through <a href="https://man7.org/linux/man-pages/man8/tc.8.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->traffic control<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, using <em data-v-c83db111=""><!--[-->egress redirects<!--]--></em>.
You can peek inside the container&#39;s Network namespace and inspect those filters:<!--]--></p><div><div><!--[--><p>Redirect to eth0</p><p>Redirect to tap0</p><!--]--></div><div><!--[--><div><!--[--><p data-v-b025fb9f=""><!--[-->Ingress filter that redirects <code data-v-a031129a=""><!--[-->tap0_urunc<!--]--></code>-&gt;<code data-v-a031129a=""><!--[-->eth0<!--]--></code>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>sudo</span><span> nsenter</span><span> -t</span><span> &#34;$(</span><span>pidof</span><span> qemu-system-x86_64)&#34;</span><span> --net</span><span> \
</span></span><span line="2"><span>  tc</span><span> filter</span><span> show</span><span> dev</span><span> tap0_urunc</span><span> ingress
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>filter parent ffff: protocol all pref 49152 u32 chain 0
</span></span><span line="2"><span>filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
</span></span><span line="3"><span>filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid not_in_hw
</span></span><span line="4"><span>  match 00000000/00000000 at 0
</span></span><span line="5"><span>        action order 1: mirred (Egress Redirect to device eth0) stolen
</span></span><span line="6"><span>        index 1 ref 1 bind 1
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><!--]--></div></div><div><div><p><img src="https://labs.iximiuz.com/content/files/tutorials/unikernels-intro-93976514/__static__/ctnetw.png" alt="The unikernel guest is connected via a TAP device to the host network. Traffic control allows packets to be redirected between the tap0_urunc interface and the container&#39;s default interface eth0."/></p><!--[--><p data-v-b025fb9f=""><!--[--><em data-v-c83db111=""><!--[-->Unikernel guest connected to the host network via a TAP device.<!--]--></em><!--]--></p><!--]--></div><!----></div><p data-v-b025fb9f=""><!--[-->In summary, the <code data-v-a031129a=""><!--[-->urunc<!--]--></code> container runtime managed to network the unikernel behind a regular container IP address by combining the following components and mechanisms:<!--]--></p><ul data-v-533e29f3=""><!--[--><li data-v-2969431c=""><!--[-->A <strong data-v-24022f14=""><!--[-->container network interface<!--]--></strong> set up by CNI outside of the unikernel.<!--]--></li><li data-v-2969431c=""><!--[-->A <strong data-v-24022f14=""><!--[-->tap network interface<!--]--></strong> connecting the guest&#39;s network interface to the host&#39;s network.<!--]--></li><li data-v-2969431c=""><!--[-->Some <strong data-v-24022f14=""><!--[-->traffic control filters that redirect network packets<!--]--></strong> between that tap interface and the container&#39;s default network interface.<!--]--></li><!--]--></ul><p data-v-b025fb9f=""><!--[-->That approach allows to connect practically any virtual machine to the host inside a Network namespace, making it possible to manage container and unikernel workloads using the same container infrastructure.<!--]--></p><h3 id="resource-constraints" data-v-9a0ceb30=""><!--[-->Resource Constraints<!--]--><!----></h3><p data-v-b025fb9f=""><!--[-->This tutorial is now being concluded by touching on the container configurations that can affect the settings of the unikernel virtual machine.<!--]--></p><p data-v-b025fb9f=""><!--[-->Let&#39;s look one more time at the command-line parameters of the QEMU process:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>cat</span><span> /proc/&#34;$(</span><span>pidof</span><span> qemu-system-x86_64)&#34;/cmdline</span><span> |</span><span> xargs</span><span> --null
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>/usr/bin/qemu-system-x86_64 -m 268M -L /usr/share/qemu -nographic ...
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->The flag <code data-v-a031129a=""><!--[-->-m 268M<!--]--></code> allocates a RAM size of 268 MiB to the virtual machine&#39;s guest, even though you did not specify any resource constraint with the <code data-v-a031129a=""><!--[-->docker container run<!--]--></code> command.
Why is that?<!--]--></p><p data-v-b025fb9f=""><!--[-->Containerized processes run inside a cgroups hierarchy that constrains their resource usage for CPU, memory, disk I/O, etc.
Applying this model to a virtual machine makes little sense because virtual machines have a distinct isolation model underpinned by virtualized hardware.<!--]--></p><div><!--[--><p data-v-b025fb9f=""><!--[--><code data-v-a031129a=""><!--[-->urunc<!--]--></code> did in fact not place the QEMU process into a particular cgroups hierarchy, so it inherited the hierarchy of the <code data-v-a031129a=""><!--[-->containerd<!--]--></code> service:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>systemctl</span><span> status</span><span> &#34;$(</span><span>pidof</span><span> qemu-system-x86_64)&#34;
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>● containerd.service - containerd container runtime
</span></span><span line="2"><span>     Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; preset: enabled)
</span></span><span line="3"><span>     Active: active (running) since Thu 2026-01-01 00:00:00 UTC; 6min ago
</span></span><span line="4"><span>       Docs: https://containerd.io
</span></span><span line="5"><span>   Main PID: 1230 (containerd)
</span></span><span line="6"><span>      Tasks: 24
</span></span><span line="7"><span>     Memory: 190.2M ()
</span></span><span line="8"><span>        CPU: 23.909s
</span></span><span line="9"><span>     CGroup: /system.slice/containerd.service
</span></span><span line="10"><span>             ├─ 1230 /usr/bin/containerd
</span></span><span line="11"><span>             ├─23927 /usr/local/bin/containerd-shim-urunc-v2 -namespace moby -id e23480...
</span></span><span line="12"><span>             └─23951 /usr/bin/qemu-system-x86_64 -m 268M -L /usr/share/qemu -nographic ...
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--]--></div><p data-v-b025fb9f=""><!--[-->Instead, <code data-v-a031129a=""><!--[-->urunc<!--]--></code> attempted to translate the container configuration into suitable virtual machine parameters.
Because no memory configuration was requested, a reasonably large RAM size of 268 MiB was decided upon (QEMU&#39;s default is 128 MiB).<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> inspect</span><span> \
</span></span><span line="2"><span>  --format=&#39;{{.HostConfig.Memory}}&#39;</span><span> \
</span></span><span line="3"><span>  unikernel
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->When an application is deployed as a regular Linux container, the container runtime can generally make assumptions about cgroups being the preferred method for constraining the resource usage of that application, in accordance with the container&#39;s configuration.
In a context where the application is instead deployed as a virtual machine, the container runtime must make its own decisions in order to <strong data-v-24022f14=""><!--[-->match the requested constraints as closely as possible<!--]--></strong>.<!--]--></p><p data-v-b025fb9f=""><!--[-->Let&#39;s see what <code data-v-a031129a=""><!--[-->urunc<!--]--></code> does when we explicitly set resource constraints while creating the unikernel container.<!--]--></p><p data-v-b025fb9f=""><!--[-->Terminate the previous container to avoid future confusions:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> stop</span><span> unikernel
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Create another unikernel container, this time with an explicit memory constraint of 64 MiB:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> run</span><span> \
</span></span><span line="2"><span>  -d</span><span> --rm</span><span> \
</span></span><span line="3"><span>  --name</span><span> unikernel</span><span> \
</span></span><span line="4"><span>  --runtime</span><span> io.containerd.urunc.v2</span><span> \
</span></span><span line="5"><span>  --memory</span><span> 64_000_000</span><span> \
</span></span><span line="6"><span>  lab/nginx-unikernel:latest
</span></span></code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->Verify that the new container has the expected configuration:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>docker</span><span> container</span><span> inspect</span><span> \
</span></span><span line="2"><span>  --format=&#39;{{.HostConfig.Memory}}&#39;</span><span> \
</span></span><span line="3"><span>  unikernel
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><!--]--><p data-v-b025fb9f=""><!--[-->Last but not least, ensure that the new QEMU virtual machine <strong data-v-24022f14=""><!--[-->has 64 MiB of RAM instead of 268 MiB<!--]--></strong>:<!--]--></p><!--[--><div meta="" data-v-4578ab24=""><!----><!--[--><pre><!--[--><code><span line="1"><span>cat</span><span> /proc/&#34;$(</span><span>pidof</span><span> qemu-system-x86_64)&#34;/cmdline</span><span> |</span><span> xargs</span><span> --null
</span></span></code><!--]--></pre><!--]--></div><!--]--><!--[--><div data-v-4578ab24=""><!----><!--[--><pre><!--[--><code>/usr/bin/qemu-system-x86_64 -m 64M -L /usr/share/qemu -nographic ...
</code><!--]--></pre><!--]--></div><!--]--><p data-v-b025fb9f=""><!--[-->This short experiment should hopefully be a good demonstration of how parity can be achieved between various methods of constraining the resource usage of containers that differ in nature, such as unikernel VMs.<!--]--></p><h2 id="summary" data-v-b9b1576a=""><!--[-->Summary<!--]--><!----></h2><p data-v-b025fb9f=""><!--[-->In this tutorial, you&#39;ve learnt about <strong data-v-24022f14=""><!--[-->what a unikernel is<!--]--></strong>: its architecture, what it enables, but also its limitations.<!--]--></p><p data-v-b025fb9f=""><!--[-->Through some guided hands-on activity, you&#39;ve <strong data-v-24022f14=""><!--[-->built a web server application as a unikernel<!--]--></strong>, an exercise which hopefully ignited your curiosity to explore other unikernel frameworks and experiment with them.<!--]--></p><p data-v-b025fb9f=""><!--[-->You&#39;ve seen how some existing open source projects make it possible to <strong data-v-24022f14=""><!--[-->integrate unikernels with a container-centric infrastructure<!--]--></strong>, without having to discard the tools you are already familiar with.<!--]--></p><hr data-v-34b8dd49=""/><section data-footnotes=""><h2 id="footnote-label" data-v-b9b1576a=""><!--[-->Footnotes<!--]--><!----></h2><ol data-v-d123ed92=""><!--[--><li id="user-content-fn-aws" data-v-2969431c=""><!--[-->Amazon EC2 (cloud instances) and Amazon Lambda (cloud functions) are two notorious examples of commodity services that are powered by virtual machines under the hood (managed by <a href="https://en.wikipedia.org/wiki/Xen" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Xen<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> and <a href="https://en.wikipedia.org/wiki/Firecracker_(software)" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Firecracker<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> respectively). <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-aws" rel="noopener noreferrer" aria-label="Back to reference 1" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-dblsched" data-v-2969431c=""><!--[-->This problem is called <em data-v-c83db111=""><!--[-->Double Scheduling<!--]--></em>. Its effects include performance degradation and resource contention. It is <a href="https://lkml.org/lkml/2024/4/3/1022" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->not a solved problem<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> yet. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-dblsched" rel="noopener noreferrer" aria-label="Back to reference 2" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-guest" data-v-2969431c=""><!--[-->In virtualization terminology, the &#34;guest&#34; is the kernel that boots and runs inside a virtual machine, while the &#34;host&#34; controls the hardware and manages the virtual machines (hypervisor). <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-guest" rel="noopener noreferrer" aria-label="Back to reference 3" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-kvm" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->KVM<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is a full virtualization solution for Intel and AMD processors that is part of the Linux kernel. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-kvm" rel="noopener noreferrer" aria-label="Back to reference 4" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-xen" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Xen" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Xen<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is a free and open source type-1 hypervisor. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-xen" rel="noopener noreferrer" aria-label="Back to reference 5" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-nginx" data-v-2969431c=""><!--[-->Nginx powers <a href="https://w3techs.com/technologies/history_overview/web_server" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->a third of the web servers worldwide<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> as of January 2026. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-nginx" rel="noopener noreferrer" aria-label="Back to reference 6" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-oci" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Open_Container_Initiative" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Open Container Initiative<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, which designs open standards for containers. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-oci" rel="noopener noreferrer" aria-label="Back to reference 7" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-cpio" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Cpio" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->cpio<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is a file archive file format commonly used in the initramfs of the Linux kernel. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-cpio" rel="noopener noreferrer" aria-label="Back to reference 8" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-virtio" data-v-2969431c=""><!--[--><a href="https://docs.kernel.org/driver-api/virtio/virtio.html" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->VirtIO<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is a standard for paravirtualized devices implemented by a hypervisor. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-virtio" rel="noopener noreferrer" aria-label="Back to reference 9" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-qemu" data-v-2969431c=""><!--[--><a href="https://www.qemu.org/" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->QEMU<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is an open source machine emulator and virtualizer. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-qemu" rel="noopener noreferrer" aria-label="Back to reference 10" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-fc" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Firecracker_(software)" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Firecracker<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is an open source virtual machine manager that uses KVM. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-fc" rel="noopener noreferrer" aria-label="Back to reference 11" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-k8s" data-v-2969431c=""><!--[--><a href="https://en.wikipedia.org/wiki/Kubernetes" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Kubernetes<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is an open source system for managing the deployment of containerized applications. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-k8s" rel="noopener noreferrer" aria-label="Back to reference 12" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-buildkit" data-v-2969431c=""><!--[--><a href="https://github.com/moby/buildkit" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->BuildKit<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> is the underlying build backend used by Docker. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-buildkit" rel="noopener noreferrer" aria-label="Back to reference 13" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><li id="user-content-fn-cni" data-v-2969431c=""><!--[-->The <a href="https://www.cni.dev" rel="nofollow" target="_blank" data-v-abeb6555=""><!--[-->Container Network Interface<!--]--><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" data-v-abeb6555=""><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> consists of a specification and libraries for writing plugins to configure network interfaces in containers. <a aria-current="page" href="https://labs.iximiuz.com/tutorials/unikernels-intro-93976514#user-content-fnref-cni" rel="noopener noreferrer" aria-label="Back to reference 14" data-footnote-backref="" data-v-abeb6555=""><!--[-->↩<!--]--><!----></a><!--]--></li><!--]--></ol></section></div><!----></div></div></div>
  </body>
</html>
