<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/LLMLingua">Original</a>
    <h1>LLMLingua: Compressing Prompts for Faster Inferencing</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">  
    <p><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/LLMLingua_logo.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/LLMLingua_logo.png" alt="LLMLingua" width="100"/></a>  
    </p>  
    <div dir="auto">  
        <h2 tabindex="-1" dir="auto"><a id="user-content-llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models--longllmlingua" aria-hidden="true" tabindex="-1" href="#llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models--longllmlingua"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models &amp; LongLLMLingua</h2>  
    </div>  
</div>
<p dir="auto">
| <a href="https://llmlingua.com/" rel="nofollow"><b>Project Page</b></a>| <a href="https://arxiv.org/abs/2310.05736" rel="nofollow"><b>LLMLingua Paper</b></a> | <a href="https://arxiv.org/abs/2310.06839" rel="nofollow"><b>LongLLMLingua Paper</b></a> | <a href="https://huggingface.co/spaces/microsoft/LLMLingua" rel="nofollow"><b>HF Space Demo</b></a> |
</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description LLMLingua_demo.mp4">LLMLingua_demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/30883354/273619158-eb0ea70d-6d4c-4aa7-8977-61f94bb87438.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NzAyNTcsIm5iZiI6MTcwMjk2OTk1NywicGF0aCI6Ii8zMDg4MzM1NC8yNzM2MTkxNTgtZWIwZWE3MGQtNmQ0Yy00YWE3LTg5NzctNjFmOTRiYjg3NDM4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDA3MTIzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWEzNGMxMGE0Njc2M2Q2ZjYzYTZjZTk3ZjdkZGM1YmMxNDdiNGRkZmI3NGJkNWJjYzkzYjNlMDZmOGI5MzA1YjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.P0UsR4__W5I3Brd68vUqQxOK3cxS6lY1Cs9H8z9GFyM" data-canonical-src="https://private-user-images.githubusercontent.com/30883354/273619158-eb0ea70d-6d4c-4aa7-8977-61f94bb87438.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NzAyNTcsIm5iZiI6MTcwMjk2OTk1NywicGF0aCI6Ii8zMDg4MzM1NC8yNzM2MTkxNTgtZWIwZWE3MGQtNmQ0Yy00YWE3LTg5NzctNjFmOTRiYjg3NDM4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDA3MTIzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWEzNGMxMGE0Njc2M2Q2ZjYzYTZjZTk3ZjdkZGM1YmMxNDdiNGRkZmI3NGJkNWJjYzkzYjNlMDZmOGI5MzA1YjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.P0UsR4__W5I3Brd68vUqQxOK3cxS6lY1Cs9H8z9GFyM" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto"><a id="user-content-news" aria-hidden="true" tabindex="-1" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>News</h2>
<ul dir="auto">
<li>üñ• You can find the slides of EMNLP‚Äò23 in <b><a href="https://drive.google.com/file/d/1GxQLAEN8bBB2yiEdQdW4UKoJzZc0es9t/view" rel="nofollow">Session 5</a></b> and <b><a href="https://drive.google.com/file/d/1LJBUfJrKxbpdkwo13SgPOqugk-UjLVIF/view" rel="nofollow">BoF-6</a></b>;</li>
<li>üìö We launched a <a href="https://medium.com/@iofu728/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7" rel="nofollow">blog</a> to showcase the benefits in RAG and long context scenarios. Please see the script of the example at <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb">this link</a>;</li>
<li>üéà We launched a <a href="https://llmlingua.com/" rel="nofollow">project page</a> showcasing real-world case studies, including RAG, Online Meetings, CoT, and Code;</li>
<li>üë®‚Äçü¶Ø We have launched a series of examples in the <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples">&#39;./examples&#39;</a> folder, which include <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples/RAG.ipynb">RAG</a>, <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples/OnlineMeeting.ipynb">Online Meeting</a>, <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples/CoT.ipynb">CoT</a>, <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples/Code.ipynb">Code</a>, and <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb">RAG using LlamaIndex</a>;</li>
<li>üëæ LongLLMLingua has been incorporated into the <a href="https://github.com/run-llama/llama_index/blob/main/llama_index/indices/postprocessor/longllmlingua.py">LlamaIndex pipeline</a>, which is a widely used RAG framework.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-tldr" aria-hidden="true" tabindex="-1" href="#tldr"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tl;DR</h2>
<p dir="auto">LLMLingua, that uses a well-trained small language model after alignment, such as GPT2-small or LLaMA-7B, to detect the unimportant tokens in the prompt and enable inference with the compressed prompt in black-box LLMs, achieving up to 20x compression with minimal performance loss.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2310.05736" rel="nofollow">LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</a> (EMNLP 2023)</p>
<p dir="auto">LongLLMLingua is a method that enhances LLMs&#39; ability to perceive key information in long-context scenarios using prompt compression, achieving up to $28.5 in cost savings per 1,000 samples while also improving performance.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2310.06839" rel="nofollow">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</a> (Under Review)</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--overview" aria-hidden="true" tabindex="-1" href="#-overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üé• Overview</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/LLMLingua_motivation.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/LLMLingua_motivation.png" alt="image"/></a></p>
<ul dir="auto">
<li>Have you ever tried to input a long text and ask ChatGPT to summarize it, only to be told that it exceeds the token limit? ‚Äã</li>
<li>Have you ever spent a lot of time fine-tuning the personality of ChatGPT, only to find that it forgets the previous instructions after a few rounds of dialogue? ‚Äã</li>
<li>Have you ever used the GPT3.5/4 API for experiments, and got good results, but also received a huge bill after a few days? ‚Äã</li>
</ul>
<p dir="auto">Large language models, such as ChatGPT and GPT-4, impress us with their amazing generalization and reasoning abilities, but they also come with some drawbacks, such as the prompt length limit and the prompt-based pricing scheme.‚Äã</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/motivation.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/motivation.png" alt="image"/></a></p>
<p dir="auto">Now you can use <strong>LLMLingua</strong> &amp; <strong>LongLLMLingua</strong>!‚Äã</p>
<p dir="auto">A simple and efficient method to compress prompt up to <strong>20x</strong>.‚Äã</p>
<ul dir="auto">
<li>üí∞ <strong>Saving cost</strong>, not only prompt, but also the generation length;‚Äã</li>
<li>üìù <strong>Support longer contexts</strong>, enhance the density of key information in the prompt and mitigate loss in the middle, thereby improving overall performance.</li>
<li>‚öñÔ∏è <strong>Robustness</strong>, no need any training for the LLMs;‚Äã</li>
<li>üïµÔ∏è <strong>Keeping</strong> the original prompt knowledge like ICL, reasoning, etc.‚Äã</li>
<li>üìú <strong>KV-Cache compression</strong>, speedup inference;‚Äã</li>
<li>ü™É <strong>GPT-4 can recovery all key information from the compressed prompt</strong>.‚Äã</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/LLMLingua.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/LLMLingua.png" alt="image"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/LongLLMLingua.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/LongLLMLingua.png" alt="image"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://stace.dev/microsoft/LLMLingua/blob/main/images/LLMLingua_demo.png"><img src="https://stace.dev/microsoft/LLMLingua/raw/main/images/LLMLingua_demo.png" alt="image"/></a></p>
<p dir="auto">If you find this repo helpful, please cite the following papers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{jiang-etal-2023-llmlingua,
    title = &#34;LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models&#34;,
    author = &#34;Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu&#34;,
    booktitle = &#34;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&#34;,
    month = dec,
    year = &#34;2023&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://arxiv.org/abs/2310.05736&#34;,
}"><pre><span>@inproceedings</span>{<span>jiang-etal-2023-llmlingua</span>,
    <span>title</span> = <span><span>&#34;</span>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models<span>&#34;</span></span>,
    <span>author</span> = <span><span>&#34;</span>Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu<span>&#34;</span></span>,
    <span>booktitle</span> = <span><span>&#34;</span>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing<span>&#34;</span></span>,
    <span>month</span> = dec,
    <span>year</span> = <span><span>&#34;</span>2023<span>&#34;</span></span>,
    <span>publisher</span> = <span><span>&#34;</span>Association for Computational Linguistics<span>&#34;</span></span>,
    <span>url</span> = <span><span>&#34;</span>https://arxiv.org/abs/2310.05736<span>&#34;</span></span>,
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article{jiang-etal-2023-longllmlingua,
    title = &#34;LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression&#34;,
    author = &#34;Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu&#34;,
    url = &#34;https://arxiv.org/abs/2310.06839&#34;,
    journal = &#34;ArXiv preprint&#34;,
    volume = &#34;abs/2310.06839&#34;,
    year = &#34;2023&#34;,
}"><pre><span>@article</span>{<span>jiang-etal-2023-longllmlingua</span>,
    <span>title</span> = <span><span>&#34;</span>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression<span>&#34;</span></span>,
    <span>author</span> = <span><span>&#34;</span>Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu<span>&#34;</span></span>,
    <span>url</span> = <span><span>&#34;</span>https://arxiv.org/abs/2310.06839<span>&#34;</span></span>,
    <span>journal</span> = <span><span>&#34;</span>ArXiv preprint<span>&#34;</span></span>,
    <span>volume</span> = <span><span>&#34;</span>abs/2310.06839<span>&#34;</span></span>,
    <span>year</span> = <span><span>&#34;</span>2023<span>&#34;</span></span>,
}</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content--quick-start" aria-hidden="true" tabindex="-1" href="#-quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üéØ Quick Start</h2>
<p dir="auto">Install LLMLingua,</p>

<p dir="auto">Then, you can use LLMLingua to compress your prompt,</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llmlingua import PromptCompressor

llm_lingua = PromptCompressor()
compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=&#34;&#34;, question=&#34;&#34;, target_token=200)

# &gt; {&#39;compressed_prompt&#39;: &#39;Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\nLets think step step\nSam bought 1 boxes x00 oflters.\nHe bought 12 * 300ters in total\nSam then took 5 boxes 6ters0ters.\nHe sold these boxes for 5 *5\nAfterelling these  boxes there were 3030 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115&#39;,
#  &#39;origin_tokens&#39;: 2365,
#  &#39;compressed_tokens&#39;: 211,
#  &#39;ratio&#39;: &#39;11.2x&#39;,
#  &#39;saving&#39;: &#39;, Saving $0.1 in GPT-4.&#39;}

## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need &lt;8GB GPU memory.
## Before that, you need to pip install optimum auto-gptq
llm_lingua = PromptCompressor(&#34;TheBloke/Llama-2-7b-Chat-GPTQ&#34;, model_config={&#34;revision&#34;: &#34;main&#34;})"><pre><span>from</span> <span>llmlingua</span> <span>import</span> <span>PromptCompressor</span>

<span>llm_lingua</span> <span>=</span> <span>PromptCompressor</span>()
<span>compressed_prompt</span> <span>=</span> <span>llm_lingua</span>.<span>compress_prompt</span>(<span>prompt</span>, <span>instruction</span><span>=</span><span>&#34;&#34;</span>, <span>question</span><span>=</span><span>&#34;&#34;</span>, <span>target_token</span><span>=</span><span>200</span>)

<span># &gt; {&#39;compressed_prompt&#39;: &#39;Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\nLets think step step\nSam bought 1 boxes x00 oflters.\nHe bought 12 * 300ters in total\nSam then took 5 boxes 6ters0ters.\nHe sold these boxes for 5 *5\nAfterelling these  boxes there were 3030 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115&#39;,</span>
<span>#  &#39;origin_tokens&#39;: 2365,</span>
<span>#  &#39;compressed_tokens&#39;: 211,</span>
<span>#  &#39;ratio&#39;: &#39;11.2x&#39;,</span>
<span>#  &#39;saving&#39;: &#39;, Saving $0.1 in GPT-4.&#39;}</span>

<span>## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need &lt;8GB GPU memory.</span>
<span>## Before that, you need to pip install optimum auto-gptq</span>
<span>llm_lingua</span> <span>=</span> <span>PromptCompressor</span>(<span>&#34;TheBloke/Llama-2-7b-Chat-GPTQ&#34;</span>, <span>model_config</span><span>=</span>{<span>&#34;revision&#34;</span>: <span>&#34;main&#34;</span>})</pre></div>
<p dir="auto">You can refer to the <a href="https://stace.dev/microsoft/LLMLingua/blob/main/examples"><strong>examples</strong></a> to understand how to use <strong>LLMLingua</strong> and <strong>LongLLMLingua</strong> in practical scenarios, such as RAG, Online Meeting, CoT, Code, and RAG using LlamaIndex. Additionally, you can refer to the <a href="https://stace.dev/microsoft/LLMLingua/blob/main/DOCUMENT.md"><strong>document</strong></a> for more recommendations on how to use LLMLingua effectively.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-frequently-asked-questions" aria-hidden="true" tabindex="-1" href="#frequently-asked-questions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Frequently Asked Questions</h2>
<p dir="auto">show in <a href="https://stace.dev/microsoft/LLMLingua/blob/main/Transparency_FAQ.md">Transparency_FAQ.md</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributing" aria-hidden="true" tabindex="-1" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-trademarks" aria-hidden="true" tabindex="-1" href="#trademarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Trademarks</h2>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft&#39;s Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.</p>
</article>
          </div></div>
  </body>
</html>
