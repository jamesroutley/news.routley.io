<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://slowernews.substack.com/p/autumn-2023-issue-11">Original</a>
    <h1>|INFO| Slower News Quarterly - Autumn Issue.</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>This is the first post going off road for our own little adventure and not following an online course. Today’s quest will consist of <del>slaying a dragon</del> building an intuition for embeddings.</p>
<h2 id="the-quest">The Quest</h2>
<p>The original idea was to try to reproduce the word arithmetic examples from Google’s Word2Vec demo: <code>King - Man + Woman = Queen</code> and <code>Paris - France + Italy = Rome</code>.</p>
<p>(Spoiler alert) it turned out to be more of an experiment on how to create/handle/visualize word embeddings. Even using <code>Gensim.word2vec</code> or <code>BERT</code> with the google-news pre-trained weights or the ones from huggingface I couldn’t get to the same results as the original demo.</p>
<h2 id="embeddings-necronomicon">Embeddings Necronomicon</h2>
<figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/necronomicon.png" title="necronomicon" data-thumbnail="necronomicon.png" data-sub-html="&lt;h2&gt;Necronomicon by stable diffusion&lt;/h2&gt;&lt;p&gt;necronomicon&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="necronomicon.png" data-srcset="necronomicon.png, necronomicon.png 1.5x, necronomicon.png 2x" data-sizes="auto" alt="necronomicon.png"/>
    </a><figcaption>Necronomicon by stable diffusion</figcaption>
    </figure>
<h3 id="what-are-embeddings">What are Embeddings</h3>
<p>Neural network do not deal with words, they only do algebra. So you can’t feed them <code>&#39;king&#39;</code>, the solution is to encode king as token (aka. a number, e.g. <code>42</code>), one-hot encode it <code>[0, ... , 0, 1, 0, ..., 0]</code> as a vector and feed it to the network.</p>
<p>For the sake of this explanation let’s assume a naive tokenizer <code>corpus.split()</code> where each word has it’s own token. It means that your network input layer is the size of your vocabulary and carry a single information: what word is it.</p>
<p>For english we can expect ~200k words (a lot more if we allow uppercase). That’s a large numbers of wasted zeros in your one-hot encoded vector. So the first thing we do in the network is to convert it into embeddings. Embeddings is also a vector but typically much smaller (e.g BERT embeddings layer is 768-dimensional) and each number of the vector can be interpreted as a coordinate in an N-dimensional space.</p>
<h3 id="skip-gram">Skip-Gram</h3>
<p>There are 2 commons approch for training an embeddings layer:</p>
<ul>
<li>Continuous Bag of Words (CBOW) aka. given a set of context word guess the middle word</li>
<li>Skip-Gram aka. given a word guess the surrounding context words around it</li>
</ul>
<p>I went with Skip-Gram model, so it looks like:</p>
<pre tabindex="0"><code>given a training text: &#39;kings wear crowns of jewels&#39;
and a context_size: 2
---------------+------------------------+----------------------
given the word | guess the left context | and the right context
---------------+------------------------+----------------------
&#39;kings&#39;       -&gt; [                        &#39;wear&#39;,     &#39;crown&#39;]
&#39;wear&#39;        -&gt; [&#39;kings&#39;,                &#39;crowns&#39;,   &#39;of&#39;]
&#39;crowns&#39;      -&gt; [&#39;kings&#39;, &#39;wear&#39;,        &#39;of&#39;,       &#39;jewels&#39;]
&#39;of&#39;          -&gt; [&#39;wear&#39;, &#39;crowns&#39;,       &#39;jewels&#39;]
&#39;jewels&#39;      -&gt; [&#39;crowns&#39;, &#39;of&#39;]
</code></pre><h3 id="compare-embeddings">Compare Embeddings</h3>
<p>After training the network, embeddings present in similar context will move in space close to each other. So we can find words related with each other by searching for the geometrically closest embeddings from them.</p>
<p>The 2 most commonly used distance metrics:</p>
<ul>
<li>Euclidean distance (the one human think about when we say 2 things are 10cm appart)</li>
<li>Cosine distance mesuring the angular distance between 2 N-dimensional embeddings</li>
</ul>
<p>Let’s compare the closest words to <code>&#39;pink&#39;</code> for the trained model vs an untrained model</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>cosine_dist</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>functional</span><span>.</span><span>cosine_similarity</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>,</span> <span>dim</span><span>=</span><span>0</span><span>)</span>
</span></span><span><span>    
</span></span><span><span><span>word</span> <span>=</span> <span>&#39;pink&#39;</span>
</span></span><span><span><span># load the model we trained earlier</span>
</span></span><span><span><span>trained_model</span> <span>=</span> <span>LM</span><span>()</span>
</span></span><span><span><span>trained_model</span><span>.</span><span>load_state_dict</span><span>(</span><span>torch</span><span>.</span><span>load</span><span>(</span><span>&#39;skip-gram.pt&#39;</span><span>))</span>
</span></span><span><span><span>tm</span> <span>=</span> <span>trained_model</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span><span># compared to an untrained one</span>
</span></span><span><span><span>untrained_model</span> <span>=</span> <span>LM</span><span>()</span>
</span></span><span><span><span>um</span> <span>=</span> <span>untrained_model</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>for</span> <span>mm</span><span>,</span> <span>label</span> <span>in</span> <span>[(</span><span>tm</span><span>,</span> <span>&#39;trained&#39;</span><span>),</span> <span>(</span><span>um</span><span>,</span> <span>&#39;untrained&#39;</span><span>)]:</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#39;--- </span><span>{</span><span>label</span><span>}</span><span> ---&#39;</span><span>)</span>
</span></span><span><span>    <span>emb_word</span> <span>=</span> <span>get_embedding</span><span>(</span><span>word</span><span>,</span> <span>m</span><span>=</span><span>mm</span><span>)</span>
</span></span><span><span>    <span>matches</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>for</span> <span>w</span> <span>in</span> <span>words_i_want_to_learn</span><span>:</span>
</span></span><span><span>        <span>if</span> <span>cs</span><span>[</span><span>w</span><span>]</span> <span>&lt;</span> <span>100</span><span>:</span>
</span></span><span><span>            <span>continue</span>
</span></span><span><span>        <span>emb_w</span> <span>=</span> <span>get_embedding</span><span>(</span><span>w</span><span>,</span> <span>m</span><span>=</span><span>mm</span><span>)</span>
</span></span><span><span>        <span>d</span> <span>=</span> <span>cosine_dist</span><span>(</span><span>emb_word</span><span>,</span> <span>emb_w</span><span>)</span>
</span></span><span><span>        <span>matches</span><span>.</span><span>append</span><span>((</span><span>d</span><span>,</span> <span>w</span><span>))</span>
</span></span></code></pre></div><pre tabindex="0"><code>|---------------------+---------------------|
| trained             | untrained           |
|----------+----------+----------+----------|
| distance | neighbor | distance | neighbor |
|----------+----------+----------+----------|
| 0.0000   | pink     | 0.0000   | pink     |
| 0.6131   | purple   |	0.7887   | prince   |
| 0.6833   | blue     |	0.8442   | king     |
| 0.6907   | yellow   |	0.8538   | chicken  |
| 0.7311   | orange   |	0.8963   | bird     |
| 0.7318   | green    |	0.8980   | lord     |
| 0.8189   | princess |	0.9089   | cow      |
| 0.8519   | red      |	0.9399   | woman    |
| 0.8530   | fish     |	0.9539   | banana   |
</code></pre><p>if we squint really hard at a low PCA (more on that later) representation of the embeddings we can almost see what feature represent color and which one represent monarchy.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>words</span> <span>=</span> <span>[</span><span>&#39;pink&#39;</span><span>,</span> <span>&#39;blue&#39;</span><span>,</span> <span>&#39;yellow&#39;</span><span>,</span> <span>&#39;king&#39;</span><span>,</span> <span>&#39;prince&#39;</span><span>,</span> <span>&#39;duke&#39;</span><span>]</span>
</span></span><span><span><span>embedding_matrix</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>([</span><span>get_embedding</span><span>(</span><span>w</span><span>)</span><span>.</span><span>cpu</span><span>()</span><span>.</span><span>detach</span><span>()</span> <span>for</span> <span>w</span> <span>in</span> <span>words</span><span>])</span>
</span></span><span><span><span>pca</span> <span>=</span> <span>PCA</span><span>(</span><span>n_components</span><span>=</span><span>len</span><span>(</span><span>words</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
</span></span><span><span><span>principal_components</span> <span>=</span> <span>pca</span><span>.</span><span>fit_transform</span><span>(</span><span>embedding_matrix</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>4</span><span>))</span>
</span></span><span><span><span>plt</span><span>.</span><span>imshow</span><span>(</span><span>principal_components</span><span>,</span> <span>cmap</span><span>=</span><span>&#39;viridis&#39;</span><span>,</span> <span>aspect</span><span>=</span><span>&#39;auto&#39;</span><span>)</span>
</span></span><span><span><span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/pca1.png" title="pca1" data-thumbnail="pca1.png" data-sub-html="&lt;h2&gt;5-dimensional PCA of the embeddings for pink, blue, yellow, king, prince, duke&lt;/h2&gt;&lt;p&gt;pca1&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="pca1.png" data-srcset="pca1.png, pca1.png 1.5x, pca1.png 2x" data-sizes="auto" alt="pca1.png"/>
    </a><figcaption>5-dimensional PCA of the embeddings for pink, blue, yellow, king, prince, duke</figcaption>
    </figure>
<p>or in 2d (note that we lose one data point because at such low dimensionality <code>&#39;pink&#39;</code> and <code>&#39;yellow&#39;</code> have the same embedding values:</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/pca2.png" title="pca2" data-thumbnail="pca2.png" data-sub-html="&lt;h2&gt;2-dimensional PCA of the embeddings for pink, blue, yellow, king, prince, duke&lt;/h2&gt;&lt;p&gt;pca2&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="pca2.png" data-srcset="pca2.png, pca2.png 1.5x, pca2.png 2x" data-sizes="auto" alt="pca2.png"/>
    </a><figcaption>2-dimensional PCA of the embeddings for pink, blue, yellow, king, prince, duke</figcaption>
    </figure>
<h3 id="word-arithmetic">Word Arithmetic</h3>
<p>For the word arithmetic I experimented with my model, BERT, and Gensim trained on google-news with very marginal success</p>
<h4 id="gensim-word2vec">Gensim Word2Vec</h4>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>gensim.models</span> <span>import</span> <span>Word2Vec</span>
</span></span><span><span><span>import</span> <span>gensim.downloader</span>
</span></span><span><span>
</span></span><span><span><span>glove_vectors</span> <span>=</span> <span>gensim</span><span>.</span><span>downloader</span><span>.</span><span>load</span><span>(</span><span>&#39;word2vec-google-news-300&#39;</span><span>)</span> <span># 1.6 GB</span>
</span></span><span><span><span>e</span> <span>=</span> <span>glove_vectors</span><span>[</span><span>&#39;paris&#39;</span><span>]</span> <span>-</span> <span>glove_vectors</span><span>[</span><span>&#39;france&#39;</span><span>]</span> <span>+</span> <span>glove_vectors</span><span>[</span><span>&#39;germany&#39;</span><span>]</span>
</span></span><span><span><span>glove_vectors</span><span>.</span><span>similar_by_vector</span><span>(</span><span>e</span><span>,</span> <span>topn</span><span>=</span><span>10</span><span>)</span>
</span></span></code></pre></div><pre tabindex="0"><code>(&#39;paris&#39;, 0.7316325902938843),
 (&#39;germany&#39;, 0.6952009797096252),
 (&#39;berlin&#39;, 0.48383620381355286),
 (&#39;german&#39;, 0.4694601595401764),
 (&#39;lindsay_lohan&#39;, 0.4535733461380005),
 (&#39;switzerland&#39;, 0.4468131363391876),
 (&#39;heidi&#39;, 0.44448140263557434),
 (&#39;lil_kim&#39;, 0.43990591168403625),
 (&#39;las_vegas&#39;, 0.43985921144485474),
 (&#39;russia&#39;, 0.4398398697376251)]
</code></pre><h4 id="bert">BERT</h4>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>transformers</span> <span>import</span> <span>BertModel</span><span>,</span> <span>BertTokenizer</span>
</span></span><span><span>
</span></span><span><span><span># Load pre-trained model and tokenizer</span>
</span></span><span><span><span>model_name</span> <span>=</span> <span>&#39;bert-base-uncased&#39;</span>
</span></span><span><span><span>tokenizer</span> <span>=</span> <span>BertTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span>
</span></span><span><span><span>model</span> <span>=</span> <span>BertModel</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_bert_embedding</span><span>(</span><span>word</span><span>):</span>
</span></span><span><span>    <span>tokens</span> <span>=</span> <span>tokenizer</span><span>(</span><span>word</span><span>,</span> <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span><span>)</span>
</span></span><span><span>    <span>token_id</span> <span>=</span> <span>tokens</span><span>[</span><span>&#39;input_ids&#39;</span><span>][</span><span>0</span><span>][</span><span>1</span><span>]</span><span>.</span><span>item</span><span>()</span>
</span></span><span><span>    <span>embeddings_layer</span> <span>=</span> <span>model</span><span>.</span><span>get_input_embeddings</span><span>()</span>
</span></span><span><span>    <span>return</span> <span>embeddings_layer</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>token_id</span><span>]]))</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>bw2e</span> <span>=</span> <span>{</span><span>w</span><span>:</span> <span>get_bert_embedding</span><span>(</span><span>w</span><span>)</span> <span>for</span> <span>w</span> <span>in</span> <span>words_i_want_to_learn</span><span>}</span>
</span></span><span><span><span>emb_word</span> <span>=</span> <span>bw2e</span><span>[</span><span>&#39;king&#39;</span><span>]</span> <span>-</span> <span>bw2e</span><span>[</span><span>&#39;man&#39;</span><span>]</span> <span>+</span> <span>bw2e</span><span>[</span><span>&#39;woman&#39;</span><span>]</span>
</span></span><span><span><span>matches</span> <span>=</span> <span>[]</span>
</span></span><span><span><span>for</span> <span>w</span> <span>in</span> <span>words_i_want_to_learn</span><span>:</span>
</span></span><span><span>    <span>emb_w</span> <span>=</span> <span>bw2e</span><span>[</span><span>w</span><span>]</span>
</span></span><span><span>    <span>d</span> <span>=</span> <span>cosine_dist</span><span>(</span><span>emb_word</span><span>,</span> <span>emb_w</span><span>)</span>
</span></span><span><span>    <span>matches</span><span>.</span><span>append</span><span>((</span><span>d</span><span>,</span> <span>w</span><span>))</span>
</span></span></code></pre></div><pre tabindex="0"><code>0.2630 king
0.3531 queen
0.5115 woman
0.5248 princess
0.5540 female
0.5956 prince
0.6266 lady
0.6393 duke
0.6689 lord
0.7145 bird
</code></pre><h2 id="pca">PCA</h2>
<h3 id="what-">What ?</h3>
<p><em>Principal Component Analysis is a mathematical technique that extracts dominant patterns from a dataset by transforming the original variables into a set of linearly uncorrelated components.</em></p>
<p>For embeddings we can use it to shuffle the data so that dimension 1 is more significant than dimension 2, and dim2 than dim3 …</p>
<p>Here have a look at my network embedding layer for a few words</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/pca3.png" title="pca3" data-thumbnail="pca3.png" data-sub-html="&lt;h2&gt;Embeddings as produced by the network&lt;/h2&gt;&lt;p&gt;pca3&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="pca3.png" data-srcset="pca3.png, pca3.png 1.5x, pca3.png 2x" data-sizes="auto" alt="pca3.png"/>
    </a><figcaption>Embeddings as produced by the network</figcaption>
    </figure>
<p>And after PCA</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/pca4.png" title="pca4" data-thumbnail="pca4.png" data-sub-html="&lt;h2&gt;Embeddings after PCA&lt;/h2&gt;&lt;p&gt;pca4&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="pca4.png" data-srcset="pca4.png, pca4.png 1.5x, pca4.png 2x" data-sizes="auto" alt="pca4.png"/>
    </a><figcaption>Embeddings after PCA</figcaption>
    </figure>
<p>Notice how the colors get more and more washed toward the right, it’s because the PC are ordered by variance.</p>
<h3 id="why-">Why ?</h3>
<p>It can be used to cut off dimensions while losing minimal variance, or cutting many dimensions (and losing a lot of the variance) but making it possible to graph the data.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/embeddings-necronomicon/pca5.png" title="pca5" data-thumbnail="pca5.png" data-sub-html="&lt;h2&gt;PCA for dimensionality reduction&lt;/h2&gt;&lt;p&gt;pca5&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="pca5.png" data-srcset="pca5.png, pca5.png 1.5x, pca5.png 2x" data-sizes="auto" alt="pca5.png"/>
    </a><figcaption>PCA for dimensionality reduction</figcaption>
    </figure>
<h3 id="how-">How ?</h3>


<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/embeddings_necronomicon" target="_blank" rel="noopener noreffer ">https://github.com/peluche/embeddings_necronomicon</a>.</p>
<h3 id="sources">Sources</h3>
<p>No data was harmed in the making of this necronomicon entry but many sources of inspirations have been used, some of my reading but probably not an exhaustive list include:</p>
<ul>
<li><a href="https://medium.com/@josua.krause/dot-product-is-a-bad-distance-function-aff7667da6cc" target="_blank" rel="noopener noreffer ">https://medium.com/@josua.krause/dot-product-is-a-bad-distance-function-aff7667da6cc</a></li>
<li><a href="https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7" target="_blank" rel="noopener noreffer ">https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7</a></li>
<li><a href="https://jaykmody.com/blog/distance-matrices-with-numpy/" target="_blank" rel="noopener noreffer ">https://jaykmody.com/blog/distance-matrices-with-numpy/</a></li>
<li><a href="https://numerics.mathdotnet.com/Distance" target="_blank" rel="noopener noreffer ">https://numerics.mathdotnet.com/Distance</a></li>
<li><a href="https://youtu.be/ISPId9Lhc1g" target="_blank" rel="noopener noreffer ">https://youtu.be/ISPId9Lhc1g</a></li>
<li><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener noreffer ">https://jalammar.github.io/illustrated-word2vec/</a></li>
<li><a href="https://towardsdatascience.com/meme-search-using-pretrained-word2vec-9f8df0a1ade3" target="_blank" rel="noopener noreffer ">https://towardsdatascience.com/meme-search-using-pretrained-word2vec-9f8df0a1ade3</a></li>
<li><a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener noreffer ">https://en.wikipedia.org/wiki/Word2vec</a></li>
<li><a href="https://jaketae.github.io/study/word2vec/" target="_blank" rel="noopener noreffer ">https://jaketae.github.io/study/word2vec/</a></li>
<li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener noreffer ">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li>
<li><a href="http://josecamachocollados.com/book_embNLP_draft.pdf" target="_blank" rel="noopener noreffer ">http://josecamachocollados.com/book_embNLP_draft.pdf</a></li>
<li><a href="https://youtu.be/HMOI_lkzW08" target="_blank" rel="noopener noreffer ">https://youtu.be/HMOI_lkzW08</a></li>
<li><a href="https://youtu.be/FgakZw6K1QQ" target="_blank" rel="noopener noreffer ">https://youtu.be/FgakZw6K1QQ</a></li>
<li><a href="https://youtu.be/oRvgq966yZg" target="_blank" rel="noopener noreffer ">https://youtu.be/oRvgq966yZg</a></li>
<li><a href="https://youtu.be/h6MCf89GqCM" target="_blank" rel="noopener noreffer ">https://youtu.be/h6MCf89GqCM</a></li>
<li><a href="https://youtu.be/TJdH6rPA-TI" target="_blank" rel="noopener noreffer ">https://youtu.be/TJdH6rPA-TI</a></li>
</ul>
</div></div>
  </body>
</html>
