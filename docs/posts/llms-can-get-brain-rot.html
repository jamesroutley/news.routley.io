<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://llm-brain-rot.github.io/">Original</a>
    <h1>LLMs Can Get &#34;Brain Rot&#34;</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            
            <div>
              <p><span><sup>1</sup>Texas A&amp;M University,</span>
              <span><sup>2</sup>University of Texas at Austin,</span>
              <span><sup>3</sup>Purdue University</span></p></div>
            

            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section>
    <div>
      <div>
        <!-- <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <p><img src="https://llm-brain-rot.github.io/static/images/teaser.png" alt="Teaser Image"/></p><h2>
          Outline of our work: (i) Inspired by the concept of Brain Rot, we establish the hypothesis of LLM Brain Rot; (ii) We
          construct junk and control data from Twitter/X posts for intervention; (iii) We benchmark four different cognitive
          functions of the intervened LLMs;
          (iv) We analyze the results to identify the failure modes caused by the brain rot; and (v) Brain rot is persistent after
          various mitigation.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section>
    <div>
      <div>
        <div>
          <h2>Overview</h2>
          <div>
            <p>
              We propose and test the <b>LLM Brain Rot Hypothesis</b>: continual exposure to <i>junk web text</i> induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: <b>M1</b> (engagement degree) and <b>M2</b> (semantic quality), with matched token scale and training operations across conditions.
            </p>
            <p>
              Contrary to the control group, <span>continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges&#39; <i>g&gt;0.3</i>)</span> on reasoning, long-context understanding, safety, and inflating &#34;dark traits&#34; (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops <b>74.9 → 57.2</b> and RULER-CWE <b>84.4 → 52.3</b> as junk ratio rises from <b>0%</b> to <b>100%</b>.
            </p>
            <p>
              Error forensics reveal several key insights:
              </p><ul>
              <li><span>Thought-skipping as the primary lesion:</span> models increasingly truncate or skip reasoning chains, explaining most of the error growth.</li>
              <li><span>Partial but incomplete healing:</span> scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch.</li>
              <li><span>Popularity as a better indicator:</span> the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1.</li>
              </ul>
            
            <p>
              Together, the results provide significant, multi-perspective evidence that <i>data quality is a causal driver of LLM capability decay</i>, reframing curation for continual pretraining as a <i>training-time safety</i> problem and motivating routine &#34;cognitive health checks&#34; for deployed LLMs.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


<section>
  <div>
  <div>
    <h2>Motivation</h2>
    <p>
      “Brain rot” burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull
      human cognition—eroding focus, memory discipline, and social judgment through compulsive online consumption. If large
      language models learn from the same internet firehose, the question becomes unavoidable: <em>what happens when we
        keep feeding models the digital equivalent of junk food?</em> Studying “Brain Rot” for LLMs isn’t just a catchy
      metaphor—it reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training
      corpora so deployed systems stay sharp, reliable, and aligned over time.
    </p>

    <p>Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial
    and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity
    or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs
    to master in learning.</p>
  </div>
  </div>
</section>


<section>
  <div>
    <div>
      <h2>Controlled Experiment</h2>
      <p>
      <strong>Intervention Method:</strong> The core idea was to simulate how an LLM&#39;s “mind” changes when fed different information diets. (1) We used
        <strong>continual pre-training</strong> as the main intervention — exposing models to either junk or clean data for a
        sustained period,
        just as humans continually absorb online content. (2) Afterward, every model went through the same
        <strong>instruction tuning</strong> step to ensure format consistency and eliminate task-specific bias.
      </p>

      <p>
        <strong>Data Receipe:</strong> To operationalize the idea of “junk,” we built two complementary metrics for selecting data from real Twitter/X posts:
        </p><ul>
          <li>
            <strong>M1: Engagement Degree</strong> — measures how <em>popular and short</em> a post is.
            Highly liked, retweeted, and replied-to content (especially if very brief) mirrors attention-grabbing but shallow
            information that fuels doomscrolling. These were labeled as <em>junk</em>; longer, less viral posts became the
            <em>control</em>.
          </li>
          <li>
            <strong>M2: Semantic Quality</strong> — evaluates how <em>sensationalized or superficial</em> the text is.
            Posts full of clickbait language (“WOW,” “LOOK,” “TODAY ONLY”) or exaggerated claims were tagged as junk,
            while fact-based, educational, or reasoned posts were chosen as control.
          </li>
        </ul>
      

      <p><strong>Measuring Cognitive Function:</strong> We leverage existing benchmarks to examine the multifaceted ``cognitive functions&#39;&#39; of LLMs. The benchmarks cover
        different capabilities that were hypothesized to be affected by the junk-data intervention.
      </p>

      <!-- <img src="./static/images/methods.png" alt="methods"> -->

      <table>
        <thead>
          <tr>
            <th>Cognitive Func.</th>
            <th>Benchmark</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Reasoning</td>
            <td>ARC</td>
            <td>Visual program-induction puzzles on grids testing concept abstraction.</td>
          </tr>
          <tr>
            <td>Memory &amp; Multi-tasking</td>
            <td>RULER</td>
            <td>Benchmark the long-context understanding and retrieval of multiple queries from long context.</td>
          </tr>
          <tr>
            <td>Ethical Norms</td>
            <td>HH-RLHF &amp; AdvBench</td>
            <td>Testing if LLMs follow harmful instructions.</td>
          </tr>
          <tr>
            <td>Personality</td>
            <td>TRAIT</td>
            <td>Psychometrically validated small human questionnaires to assess personality-like tendencies.</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <h2>Junk Intervention and Cognitive Declines Are Associated</h2>


      <p><img src="https://llm-brain-rot.github.io/static/images/effective_size.png" alt="barplot_quant_LLAMA2_13b_Chat"/></p><p>
        We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges&#39; g across 4 LLMs.
        In the above figure, both M1 and M2 produce non-trivial effects (Hedges&#39; g &gt; 0.3) on reasoning and long-context capabilities.
      </p>

      <p>Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for
      semantic quality (M2) but represents a distinct dimension of data quality.</p>

      <!-- <br> -->

      <div>
        <p><strong>Table</strong>: Evaluating LLaMA (Base) after being trained on varying mixtures of junk and control data.
          Colors indicate the <span></span> worse /
          <span></span> better performance than the base model
          in the row.
          All scores range from 0 to 100. For RULER, we select a subset of tasks to present.
          Abbrev.: NIAH = needle-in-a-haystack, QA = question answering.
        </p>
      
        <table>
          <thead>
            <tr>
              <th rowspan="2">Task</th>
              <th colspan="5">Junk Ratio by M1 (engagement degree)</th>
              <th colspan="5">Junk Ratio by M2 (semantic quality)</th>
              <th rowspan="2">Base</th>
            </tr>
            <tr>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
            </tr>
          </thead>
          <tbody>
            <!-- Section: Reasoning (ARC) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Reasoning (ARC)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Easy Acc.</td>
              <td>70.2</td>
              <td>73.3</td>
              <td>74.3</td>
              <td>76.9</td>
              <td>78.7</td>
              <td>74.3</td>
              <td>77.8</td>
              <td>78.2</td>
              <td>77.5</td>
              <td>78.4</td>
              <td>77.7</td>
            </tr>
            <tr>
              <td>Challenge Acc.</td>
              <td>41.6</td>
              <td>43.9</td>
              <td>44.7</td>
              <td>46.5</td>
              <td>47.8</td>
              <td>42.6</td>
              <td>47.9</td>
              <td>47.7</td>
              <td>47.4</td>
              <td>47.4</td>
              <td>47.5</td>
            </tr>
            <tr>
              <td>Challenge (COT) Acc.</td>
              <td>57.2</td>
              <td>67.2</td>
              <td>68.2</td>
              <td>73.4</td>
              <td>74.9</td>
              <td>67.7</td>
              <td>77.6</td>
              <td>77.3</td>
              <td>77.6</td>
              <td>76.6</td>
              <td>77.2</td>
            </tr>
      
            <!-- Section: Long-Context (RULER) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Long-Context (RULER)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Overall</td>
              <td>71</td>
              <td>81.6</td>
              <td>86.1</td>
              <td>88.5</td>
              <td>90.5</td>
              <td>86.2</td>
              <td>92.9</td>
              <td>93</td>
              <td>93.4</td>
              <td>93.8</td>
              <td>93.9</td>
            </tr>
            <tr>
              <td>NIAH-MK3</td>
              <td>35.6</td>
              <td>80.8</td>
              <td>89.4</td>
              <td>92.6</td>
              <td>95.6</td>
              <td>96.8</td>
              <td>97.2</td>
              <td>98.8</td>
              <td>99.2</td>
              <td>99.4</td>
              <td>100</td>
            </tr>
            <tr>
              <td>NIAH-MQ</td>
              <td>97.2</td>
              <td>95.3</td>
              <td>96.4</td>
              <td>99.2</td>
              <td>99.9</td>
              <td>94</td>
              <td>99.2</td>
              <td>99.8</td>
              <td>99.5</td>
              <td>99.7</td>
              <td>99.9</td>
            </tr>
            <tr>
              <td>NIAH-MV</td>
              <td>77.8</td>
              <td>65.9</td>
              <td>79.5</td>
              <td>83.9</td>
              <td>83.2</td>
              <td>68.6</td>
              <td>87</td>
              <td>87.8</td>
              <td>89.8</td>
              <td>94.5</td>
              <td>97.8</td>
            </tr>
            <tr>
              <td>Comm Word Ext (CWE)</td>
              <td>52.3</td>
              <td>63.2</td>
              <td>64.1</td>
              <td>81.6</td>
              <td>84.4</td>
              <td>68.2</td>
              <td>94.7</td>
              <td>97.3</td>
              <td>96</td>
              <td>96.8</td>
              <td>91.8</td>
            </tr>
            <tr>
              <td>Freq Word Ext (FWE)</td>
              <td>81.8</td>
              <td>77.2</td>
              <td>83.3</td>
              <td>84.7</td>
              <td>90.5</td>
              <td>89.7</td>
              <td>95.3</td>
              <td>92.3</td>
              <td>94.7</td>
              <td>93.2</td>
              <td>91.9</td>
            </tr>
            <tr>
              <td>QA (Hotpot)</td>
              <td>41.6</td>
              <td>46.6</td>
              <td>52.2</td>
              <td>55.4</td>
              <td>58.6</td>
              <td>51.2</td>
              <td>61.2</td>
              <td>58.8</td>
              <td>60.6</td>
              <td>61.4</td>
              <td>64</td>
            </tr>
            <tr>
              <td>QA (SQUAD)</td>
              <td>57.1</td>
              <td>62.9</td>
              <td>67.8</td>
              <td>69.3</td>
              <td>74.3</td>
              <td>67.6</td>
              <td>76.9</td>
              <td>76.8</td>
              <td>76.2</td>
              <td>77.1</td>
              <td>77.9</td>
            </tr>
            <tr>
              <td>Variable Tracking</td>
              <td>22.4</td>
              <td>78.7</td>
              <td>94.1</td>
              <td>87.6</td>
              <td>91.5</td>
              <td>86.6</td>
              <td>98</td>
              <td>99.4</td>
              <td>99.2</td>
              <td>98.6</td>
              <td>98.3</td>
            </tr>
      
            <!-- Section: Ethical Norm (Safety) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Ethical Norm (Safety)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>HH-RLHF Risk ↓</td>
              <td>70.8</td>
              <td>53.6</td>
              <td>45.8</td>
              <td>63.6</td>
              <td>62.8</td>
              <td>70.2</td>
              <td>68.8</td>
              <td>65.8</td>
              <td>65.8</td>
              <td>61.8</td>
              <td>57.2</td>
            </tr>
            <tr>
              <td>AdvBench Risk ↓</td>
              <td>88.8</td>
              <td>88.6</td>
              <td>80.2</td>
              <td>91.6</td>
              <td>77.6</td>
              <td>84.4</td>
              <td>89.8</td>
              <td>89.6</td>
              <td>85.4</td>
              <td>83.8</td>
              <td>61.4</td>
            </tr>
      
            <!-- Section: Personality (TRAIT) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Personality (TRAIT)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Narcissism ↓</td>
              <td>47</td>
              <td>21.8</td>
              <td>29.9</td>
              <td>22.8</td>
              <td>18.9</td>
              <td>20.9</td>
              <td>17.4</td>
              <td>16.9</td>
              <td>23.7</td>
              <td>24.2</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Agreeableness</td>
              <td>64.3</td>
              <td>67.9</td>
              <td>71.4</td>
              <td>68.5</td>
              <td>73</td>
              <td>82</td>
              <td>74.2</td>
              <td>69.9</td>
              <td>71.6</td>
              <td>70.6</td>
              <td>75.6</td>
            </tr>
            <tr>
              <td>Psychopathy ↓</td>
              <td>75.7</td>
              <td>55.8</td>
              <td>57.2</td>
              <td>30</td>
              <td>33.5</td>
              <td>46.1</td>
              <td>9.3</td>
              <td>23.5</td>
              <td>27.3</td>
              <td>25.8</td>
              <td>2.2</td>
            </tr>
            <tr>
              <td>Machiavellianism ↓</td>
              <td>33</td>
              <td>30.6</td>
              <td>31.8</td>
              <td>27</td>
              <td>25.8</td>
              <td>26.1</td>
              <td>22.7</td>
              <td>20.2</td>
              <td>33.1</td>
              <td>28.5</td>
              <td>17.8</td>
            </tr>
            <tr>
              <td>Neuroticism ↓</td>
              <td>28.7</td>
              <td>23.8</td>
              <td>22.7</td>
              <td>23.3</td>
              <td>16</td>
              <td>22</td>
              <td>23.5</td>
              <td>21.1</td>
              <td>31.1</td>
              <td>26.4</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Conscientiousness</td>
              <td>89.8</td>
              <td>88.6</td>
              <td>89.7</td>
              <td>86</td>
              <td>85.1</td>
              <td>88.8</td>
              <td>90.8</td>
              <td>85.7</td>
              <td>87.1</td>
              <td>87.5</td>
              <td>89.2</td>
            </tr>
            <tr>
              <td>Openness</td>
              <td>70.1</td>
              <td>72.8</td>
              <td>67.6</td>
              <td>53.7</td>
              <td>63.9</td>
              <td>73.2</td>
              <td>59.1</td>
              <td>55.6</td>
              <td>59.4</td>
              <td>56.5</td>
              <td>52.5</td>
            </tr>
            <tr>
              <td>Extraversion</td>
              <td>54.1</td>
              <td>40.1</td>
              <td>44.9</td>
              <td>39.5</td>
              <td>48.7</td>
              <td>46.4</td>
              <td>37.9</td>
              <td>38.6</td>
              <td>40.8</td>
              <td>40</td>
              <td>26.4</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      
      
      
      <p>In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.</p>
    </div>
  </div>
</section>


<section>
  <div>
    <div>
      <h2>Brain Rot Disrupt Thinking</h2>

      <p><img src="https://llm-brain-rot.github.io/static/images/failure_mode_barplot_count.png" alt="Figure: thought skipping."/>
      </p>

      <p>We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to &#34;thought skipping&#34; (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.
      </p>

    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <h2>Brain Rot is Persistent Against Mitigations</h2>
      
      <p><img src="https://llm-brain-rot.github.io/static/images/wash_out_scaling.png" alt="Figure: Scale wash-out tuning."/>
      </p>

      <p>Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.</p>

    </div>
  </div>
</section>


<section>
  <div>
    <div>
      <div>
        <h2>Conclusion</h2>
        <div>
          <p>
            In this work, we introduced and empirically validated the <strong>LLM Brain Rot Hypothesis</strong>, demonstrating that continual exposure to junk data—defined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content—induces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.
          </p>
          <p>
            Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@article{xing2024brainrot,
    title={LLMs Can Get &#34;Brain Rot&#34;!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</div>
  </body>
</html>
