<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ksagar.bearblog.dev/vjepa/">Original</a>
    <h1>We accidentally solved robotics by watching 1M hours of YouTube</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-29T08:59Z">
                    29 Jun, 2025
                </time>
            </i>
        </p>
    

    <hr/>
<p><em>imagine</em> this: you&#39;ve just spent $640 billion training the chonkiest language model known to humanity (lol) and decide to call it &#34;Behemoth&#34;. it can annoy you on whatsapp, try to solve calculus, and argue with you about anything with a sophistication of a philosophy PhD.</p>
<p>but ask it to grab a coffee mug from your kitchen counter? ngmi</p>
<p>turns out scaling LLMs forever still leaves robots as clueless. internet-scale language misses the fundamental physics of <em>stuff actually moving around in 3D space</em>. and no amount of &#34;think step by step&#34; or COT prompting helps to teach your chatterbox where the trash is in the kitchen</p>
<p>but if i told you that the solution was hiding in plain sight? what if the secret sauce wasn&#39;t more tokens, but more... <em>videos</em>?</p>
<hr/>
<h2 id="the-why-didnt-we-think-of-this-sooner-moment">the &#34;why didn&#39;t we think of this sooner&#34; moment</h2><p>here&#39;s the thing everyone forgot while we were busy making ai agents book flight tickets: <strong>robots need to understand physics, not language</strong>.</p>
<p>so enter V-JEPA 2, which basically said &#34;hey, what if we fed a neural network 1 million hours of youtube and taught it to predict what happens next?&#34; except instead of predicting the next word, it predicts the next <em>moment in reality</em>.</p>
<p>this is &#34;deploy a robot in a completely new lab and watch it successfully pick up objects it&#39;s never seen before&#34; level of real.</p>
<hr/>
<h2 id="the-beauty-under-the-hood">the beauty under the hood</h2><h3 id="the-core-insight-strongpredict-in-representation-space-not-pixelsstrong">the core insight: <strong>predict in representation space, not pixels</strong></h3><p>remember when everyone was obsessed with making AI generate pretty pictures? well, V-JEPA 2 said &#34;screw <em>noise</em>&#34; and decided to predict in <em>latent space</em> instead (i know this word is thrown around alot but bear with me)</p>
<p>why? because trying to predict every pixel is like trying to predict every blade of grass in a field when what you really care about is whether the ball is going in the goal.</p>
<p>the magic happens in three parts:</p>
<ol>
<li><p><strong>the encoder</strong>: a ViT-g with 1 billion parameters that looks at video and goes &#34;ah yes, i understand the essence of this physical situation&#34;</p>
</li>
<li><p><strong>the predictor</strong>: a smaller nn that takes masked video tokens and tries to fill in the blanks, like the a sophisticated game of video madlibs</p>
</li>
<li><p><strong>3D-RoPE</strong>: because regular position embeddings are for 2D peasants</p>
</li>
</ol>
<h3 id="the-masking-strategy">the masking strategy</h3><p>instead of showing the model everything, V-JEPA 2 randomly masks out chunks of video (called &#34;tubelets&#34; - yes, that&#39;s the technical term). the model then has to predict what&#39;s happening in those missing pieces.</p>
<hr/>
<h3 id="data-scaling-from-some-videos-to-all-the-videos">data scaling: from &#34;some videos&#34; to &#34;all the videos&#34;</h3><ul>
<li><strong>before</strong>: 2 million videos (cute)</li>
<li><strong>after</strong>: 22 million videos + 1 million images (now we&#39;re talking)</li>
</ul>
<p>they basically hoovered up everything: something-something v2, kinetics, howto100m, and a billion youtube videos</p>
<h3 id="model-scaling-bigger-is-better-sometimes">model scaling: bigger is better (sometimes)</h3><p>they scaled from 300M to 1B parameters because apparently size does matter. the ViT-g encoder is basically the endgame of vision transformers.</p>
<h3 id="progressive-resolution-training-the-boiling-frog-approach">progressive resolution training: the &#34;boiling frog&#34; approach</h3><p>here&#39;s the clever bit: instead of immediately training on massive high-res videos (which would require selling a kidney to afford the compute), they started small and gradually cranked up the resolution during training.</p>
<p>(curriculum learning bros keep on winning)</p>
<h2 id="16-frames-at-2562-64-frames-at-3842">16 frames at 256¬≤ ‚Üí 64 frames at 384¬≤</h2><h2 id="v-jepa-2-ac-my-favourite-bit">V-JEPA 2-AC: my favourite bit</h2><p>having a world model that understands physics is cool, but robots need to understand <em>actionable</em> physics. like &#34;if i move my arm this way, what happens to the world?&#34; and the dynamics behind this action</p>
<p>so they took the pretrained V-JEPA 2, froze it solid, and attached a 300M parameter transformer that learns to predict what happens when you actually <em>do</em> stuff. (a model that can just do stuff, hell yeah)</p>
<p>the training data? <em>just</em> 62 hours of robot videos. not &#34;successful robot videos&#34; or &#34;carefully curated robot videos.&#34; just raw footage of a franka arm doing franka arm things, successes and failures included. really interesting bit, alot of future work to do expirements with good data curation and win/lose ratio.</p>
<h3 id="the-magic-of-energy-minimization">the magic of energy minimization</h3><p>when it&#39;s time to actually control a robot, V-JEPA 2-AC plays a game of &#34;hot and cold&#34;:</p>
<ol>
<li>look at current state</li>
<li>look at goal state</li>
<li>imagine a bunch of possible action sequences</li>
<li>pick the one that gets you closest to the goal</li>
<li>execute first action</li>
<li>repeat until done (or until something breaks)</li>
</ol>
<p>model predictive control on world model is one of the coolest things this paper has done</p>
<hr/>
<h3 id="zero-shot-generalization-aka-the-money-shot">zero-shot generalization (aka the money shot)</h3><p>they took this model, trained entirely on one dataset, and deployed it on franka arms in completely different labs. different lighting, different objects, different everything.</p>
<p><strong>success rates:</strong></p>
<ul>
<li>reach: 100% (because apparently moving to a point in space is trivial when you understand physics)</li>
<li>grasp cup: 65% (cups are apparently hard)</li>
<li>pick and place: 65-80% (depending on object complexity)</li>
</ul>
<p>compare this to baseline approaches that basically failed at everything except the most basic reaching tasks.</p>
<h3 id="the-speed-demon">the speed demon</h3><p>planning with V-JEPA 2-AC: <strong>16 seconds per action</strong>
planning with diffusion models: <strong>4 minutes per action</strong></p>
<hr/>
<h3 id="for-robotics-folks-the-obvious-stuff">for robotics folks: the obvious stuff</h3><ul>
<li><strong>zero-shot generalization</strong>: works on novel objects out of the box</li>
<li><strong>data efficiency</strong>: 62 hours of video vs thousands of hours of careful teleoperation</li>
<li><strong>actually deployable</strong>: seconds vs minutes for planning</li>
</ul>
<h3 id="for-llm-hackers-the-plot-twist">for llm hackers: the plot twist</h3><p>here&#39;s where it gets spicy. they aligned V-JEPA 2 with an 8B language model and got state-of-the-art results on video question answering.</p>
<p><strong>84.0%</strong> on PerceptionTest. <strong>76.9%</strong> on TempCompass.</p>
<p>this is a video encoder that was pretrained <em>without any language supervision</em> beating models that were trained on image-text pairs. isnt this like so cool?? also makes us wonder what other dynamics are fed in this world model waiting for us to open up and explore.</p>
<p>the conventional wisdom of &#34;you need language supervision to understand the world&#34; just took a uppercut to the jaw.</p>
<hr/>
<h2 id="limitations-aka-the-not-everything-is-sunshine-and-rainbows-section">limitations (aka the &#34;not everything is sunshine and rainbows&#34; section)</h2><h3 id="camera-pose-sensitivity">camera pose sensitivity</h3><p>the model is basically a diva about camera positioning. move the camera 10 degrees and suddenly it thinks left is right and up is down.</p>
<p>in practice, this means you have to manually fiddle with camera positions until you find the sweet spot. very scientific. much engineering.</p>
<h3 id="long-horizon-drift">long-horizon drift</h3><p>try to plan more than a few steps ahead and the model starts hallucinating. thats tuff.</p>
<h3 id="the-language-goal-problem">the language goal problem</h3><p>right now, you need to show the robot pictures of what you want it to do. want it to &#34;clean the kitchen&#34;? better have a photo of a clean kitchen handy.</p>
<p>future work: teaching it to understand &#34;make me a sandwich&#34; without needing a powerpoint presentation. i am working on this right now if you are interested to help, hmu</p>
<hr/>
<h2 id="wild-speculations-about-the-future">wild speculations about the future</h2><p>we might be looking at a future where world models rival pure-text models for real-world grounding. imagine a robot that understands physics as well as chatgpt understands language.</p>
<hr/>
<h2 id="tldr-by-claude">tl;dr by claude</h2><div><pre><span></span>property          v-jepa 2    diffusion    bc-policies
------------------------------------------------------
understanding      ‚ú®          ü§∑           ü§∑
planning speed     üöÄ          üêå           üêå  
zero-shot magic    ‚úÖ          ‚ùå           ‚ùå
data efficiency    üìà          üìâ           üòê
can make coffee    probably    uhh         kinda
</pre></div>
<hr/>
<p>ps - here is a cool visualization <a href="https://x.com/i/status/1936366600288166050">twitter link</a> of PCA done over VJEPA</p>
<p><em>if you are more interested make sure to check out the <a href="https://arxiv.org/abs/2506.09985">paper</a>, the <a href="https://github.com/facebookresearch/vjepa2">code</a>, or just watch your roomba bump into the same chair leg for the 47th time and contemplate how far we&#39;ve come.</em></p>


    

    
        

        
            


        
    


  </div></div>
  </body>
</html>
