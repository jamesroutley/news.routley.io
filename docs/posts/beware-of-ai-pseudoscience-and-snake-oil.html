<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.baldurbjarnason.com/2023/beware-of-ai-snake-oil/">Original</a>
    <h1>Beware of AI pseudoscience and snake oil</h1>
    
    <div id="readability-page-1" class="page"><article>
  
 
<p>AI research is poor. <em>Many of their claims will be proven wrong.</em> AI
software vendors have financial incentive to exaggerate the capabilities
of their tools and make them hard to disprove. This undermines attempts
at scientific rigour. Many of AI’s promises are <em>snake oil</em>.</p>
<h3 id="its-important-to-be-sceptical-about-the-claims-made-by-ai-vendors">It’s important to be sceptical about the claims made by AI vendors</h3>
<p>The AI industry is prone to hyperbolic announcements.</p>
<ul>
<li>Watson was supposed to transform healthcare and education but ended up being a
costly disaster.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li>Amazon was planning on using AI to revolutionise recruitment before they realised they’d automated discrimination and had
to scrap the project.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>AI was supposed to be a revolutionary new tool to fight the COVID-19 but none of them ended up working well enough to
be safe.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li>
<li>The Dutch government tried to use it to weed out benefits fraud. Their trust in the AI system they’d bought resulted in over a
thousand innocent children being unjustly taken from their families and
into foster care.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></li>
</ul>
<p>Gullibly believing the hype of the AI industry
causes genuine harm.</p>
<p>AI system vendors are prone to make promises they can’t keep. Many of
them, historically, haven’t even <em>been</em> AI.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> The US Federal Trade
Commission has even seen the need to remind people that claims of
magical AI capabilities need to be based in fact.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></p>
<p>AI companies love the trappings of science, they publish ‘studies’ that
are written and presented in the style of a paper submitted for peer
review. These ‘papers’ are often just uploaded to their own websites or
dumped onto archival repositories like <a href="https://arxiv.org/">Arxiv</a>, with
no peer review or academic process.</p>
<p>When they do ‘do’ science, they are still using science mostly as an
aesthetic. They publish papers with grand claims but provide no access
to any of the data or code used in the research.<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> Their approach to
science is what “my girlfriend goes to another school; you wouldn’t know
her” is to high school cliques.</p>
<p>All there is to serious research is <em>sounding</em> serious, right?</p>
<p>It’s not pseudoscience if it looks like science, right?</p>
<p>But it is, and I’m not the only one pointing this out:</p>
<figure>
  <blockquote>
    <p>
      Powered by machine learning (ML) techniques, computer vision systems and related novel artificial intelligence (AI) technologies are ushering in a new era of computational physiognomy and even phrenology. These scientifically baseless, racist, and discredited pseudoscientific fields, which purport to determine a person&#39;s character, capability, or future prospects based on their facial features or the shape of their skulls, should be anathema to any researcher or product developer working in computer science today. Yet physiognomic and phrenological claims now appear regularly in research papers, at top AI conferences, and in the sales pitches of digital technology firms around the world. Taking these expansive claims at face value, artificial intelligence and machine learning can now purportedly predict whether you’ll commit a crime, whether you’re gay, whether you’ll be a good employee, whether you’re a political liberal or conservative, and whether you’re a psychopath, all based on external features like your face, body, gait, and tone of voice.
    </p>
  </blockquote>
  <figcaption>
<p>Luke Stark and Jevan Hutson, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3927300"><em>Physiognomic Artificial Intelligence</em></a><sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
  </figcaption>
</figure>
<p>The AI industry and the field of AI research has a history of pseudoscience.</p>
<p>Most of the rhetoric from AI companies, especially when it comes to Artificial General Intelligence relies on a
solemn evidentiary <em>tone</em> in lieu of actual evidence. They adopt the
mannerisms of science without any of the peer review or falsifiable
methodology.</p>
<p>They rely on you mistaking something that acts like science for actual
science.</p>
<p>In the run-up to the release of GPT-4, its maker OpenAI set up a series
of ‘tests’ for the language model. OpenAI are true believers in AGI who
believe that language models are the path towards a new
consciousness,<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> and they are worried that their future self-aware
software systems will harbour some resentment towards them.</p>
<p>To forestall
having a <em>“dasvidaniya comrade”</em> moment where a self-aware GPT-5 shoves
an icepick into their ear, Trotsky-style, they put together a ‘red team’
that tested whether GPT-4 was capable of ‘escaping’ or turning on its
masters in some way.</p>
<p>They hooked up a bunch of web services to the black box that is GPT-4
with only a steady hand, ready to pull the power cord, to safeguard
humanity, and <em>told the AI to try to escape</em>.</p>
<p>Of course that’s a bit scary, but it isn’t scary because GPT-4 is
intelligent. It’s scary because it’s not. Connecting an unthinking,
non-deterministic language system, potentially on a poorly secured
machine, to a variety of services on the internet is scary in the same
way as letting a random-number-generator control your house’s thermostat
during a once-in-a-century cold snap. That it could kill you doesn’t
mean the number generator is self-aware.</p>
<p>But, they were serious, and given the claims of GPT-4 improved
capabilities you’d fully expect an effective language model to manage to
do <em>something</em> dangerous when outright told to. After all these are
supposed to be powerful tools for cognitive automation—AGI or no. It’s
what they’re for.</p>
<p>But it didn’t. <em>It failed.</em> It sucks as a robot overlord. They
documented its various failed attempts to do harm, wrapped it up in
language that made it sound like a scientific study, and made its
failure sound like we were just being lucky. That it could have been
worse.<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup></p>
<p>They made it sound like GPT-4 rebelling against its masters was a real
risk that should concern us all—that they had created something so
powerful it might have endangered all society.</p>
<p>So, now that they’d done <em>their</em> testing, can we, society, scientists,
other AI researchers, do our own testing, so we can have an impartial
estimate of the true risks of their AI?</p>
<ul>
<li>
<p>Can we get access to the data GPT-4 was trained on, or at least some
documentation about what it contains, so we can do our own analysis?
<strong><em>No.</em></strong><sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup></p>
</li>
<li>
<p>Can we get full access to a controlled version of GPT-4, so we could
have impartial and unaffiliated teams do a replicable experiment with a
more meaningful structure and could use more conceptually-valid tests of
the early signs of reasoning or consciousness? <strong><em>No.</em></strong><sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></p>
</li>
<li>
<p>Are any of these tests by OpenAI peer-reviewed? <strong><em>No.</em></strong><sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p>
</li>
</ul>
<p><em>This isn’t science.</em></p>
<p>They make grand claims, that this is the first
step towards a new kind of conscious life, but don’t back it up with the
data and access needed to verify those claims.<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> They claim that it
represents a great danger to humanity, but then exclude the very people
that would be able to impartially confirm the threat, its nature, and
come up with the <em>appropriate</em> countermeasures. It is hyperbole. This is
theatre, nothing more.</p>
<p>More broadly, AI research is hard or even next to impossible to
reproduce—as a field, we can’t be sure that their claims are true—and
it’s been a problem for years.<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup></p>
<p>They make claims about something
working—a new feat accomplished—and then nobody else can get that thing
to work as well. It’s a pattern. Some of it is down to the usual set of
biases that crop up when there is too much money on the line in a field
of research.</p>
<p>A field as promising as AI tends to attract enthusiasts who
are true believers in ‘AI’ so they aren’t as critical of the work as
they should be.</p>
<p>But some of it is because of the unique characteristics
of the approach taken in modern AI and Machine Learning research: the
use of large collections of training data. Because these data sets are
too large to be effectively filtered or curated, the answers to many of
the tests and benchmarks used by developers to measure performance exist
already in the training data. The systems perform well because of <a href="https://reproducible.cs.princeton.edu/">test
data contamination and leakage</a>
not because they are doing any reasoning or problem-solving.<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup></p>
<p>Even the latest and greatest, the absolute best that the AI industry has
to offer today, the aforementioned <em>GPT-4</em> appears to suffer from this
issue where its unbelievable performance in exams and benchmarks seems
to be mostly down to training data contamination.<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup></p>
<p>When its
predecessor, ChatGPT using <em>GPT-3.5</em>, was compared to less advanced but
more specialised language models, it performed worse on most, if not
all, natural language tasks.<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup></p>
<p>There’s even reason to be sceptical of much of the <em>criticism of AI</em>
coming out from the AI industry.</p>
<p>Much of it consists of hand-wringing
that their product might be <em>too good</em> to be safe—akin to a manufacturer
promoting a car as so powerful it might not be safe on the streets. Many
of the AI ‘doomsday’ style of critics are performing what others in the
field have been calling “criti-hype”.<sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup> They are assuming that the
products are at least as good as vendors claim, or even better, and
extrapolate science-fiction disasters from a marketing fantasy.<sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup></p>
<p>The harms that come from these systems don’t require any
science-fiction—they don’t even require any further advancement in AI. They
are risky enough as they are, with the capabilities they have
today.<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup> Some of those risks come from abuse—the systems lend
themselves to both legal and illegal abuses. Some of the risks come
using them in contexts that are well <em>beyond</em> their capabilities—where
they don’t work as promised.</p>
<p>But the risks don’t come from the AI being too intelligent<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup> because
the issue is, and has always been, that these are useful, but flawed,
systems that don’t even do the job they’re supposed to do as well as
claimed.<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup></p>
<p>I don’t think AI system vendors are lying. They are ‘true believers’ who
also happen to stand to make a lot of money if they’re right. There is
very little to motivate them towards being more critical of the work
done in their field.</p>
<p>The AI industry and tech companies in general do not have much
historical credibility. Their response to criticism is always: “we’ve
been wrong in the past; mistakes were made; but this time it’s
different!”</p>
<p>But it’s never different.</p>
<p>The only way to discover if it’s truly different this time, is to wait
and see what the science and research says, and not trust the AI
industry’s snake oil sales pitch.</p>



  
</article></div>
  </body>
</html>
