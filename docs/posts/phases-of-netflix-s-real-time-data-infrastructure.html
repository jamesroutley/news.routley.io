<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zhenzhongxu.com/the-four-innovation-phases-of-netflixs-trillions-scale-real-time-data-infrastructure-2370938d7f01?gi=53e1b7c77613">Original</a>
    <h1>Phases of Netflix’s real-time data infrastructure</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><p id="2938">My name is <a href="https://twitter.com/ZhenzhongXu" rel="noopener ugc nofollow" target="_blank">Zhenzhong Xu</a>. I joined Netflix in 2015 as a founding engineer on the Real-time Data Infrastructure team and later led the Stream Processing Engines team. I developed an interest in real-time data in the early 2010s, and ever since believe there is much value yet to be uncovered.</p><p id="f8d9">Netflix was a fantastic place to be surrounded by many amazing colleagues. I can’t be more proud of everyone involved in this journey to turn a shared belief into reality. I want to take a brief moment to share the team’s major accomplishments:</p><ul><li id="1d0a">We grew the streaming data use cases from 0 to over 2000+ across all organizations in Netflix.</li><li id="13a8">We built and evolved successful products such as <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a" rel="noopener ugc nofollow" target="_blank">Keystone</a>, <a href="https://conferences.oreilly.com/software-architecture/sa-ny-2018/cdn.oreillystatic.com/en/assets/1/event/281/Building%20stream%20processing%20as%20a%20service%20at%20Netflix%20Presentation.pdf" rel="noopener ugc nofollow" target="_blank">managed Flink Platform</a>, <a href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a" rel="noopener ugc nofollow" target="_blank">Mantis</a>, and managed Kafka platform. These products offer solutions in many aspects of the data ecosystem: including data ingestion, movement, analytical &amp; operational processing, and machine learning use cases.</li><li id="ae3e">We were among the first in the industry to scale open-source Kafka &amp; Flink deployment to handle 1 Trillion events a day around 2017 and later scaled another 20x by 2021!</li></ul><p id="3dc6">A couple of months ago, I left Netflix to pursue a similar but expanded vision in the <a href="https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html" rel="noopener ugc nofollow" target="_blank">real-time ML space</a>. I think it’s a perfect time to summarize my learnings building the real-time data infrastructure at Netflix. I hope this post will help platform engineers develop their cloud-native, self-serve streaming data platforms and scale use cases across many business functions (not necessarily from our success but maybe more from our failures). I also believe that having an understanding of how data platforms are built can help platform users (e.g., data scientists and ML engineers) make the most out of their platforms.</p><p id="8653">I will share the four phases of Real-time Data Infrastructure’s iterative journey in Netflix (2015–2021).</p><p id="06b8"><a href="#95d1" rel="noopener ugc nofollow"><strong>Phase 1: Rescue Netflix Logs From the Failing Batch Pipelines (2015)</strong></a></p><blockquote><p id="010e">During Netflix’s global hyper-growth, business and operational decisions are reliant on faster logging data more than ever. In 2015, the Chukwa/Hadoop/Hive-based batch pipelines struggled to scale. In this phase, we built a streaming-first platform from the ground up to replace the failing pipelines.</p></blockquote><p id="5aae"><a href="#77da" rel="noopener ugc nofollow"><strong>Phase 2: Scale 100s Data Movement Use Cases (2016)</strong></a></p><blockquote><p id="d30d">After the initial product release, the internal demand for data movement steadily rose. We had to focus on the common use cases. In this phase, we scaled to support 100s of use cases by building out a self-service, fully managed platform with a simple-yet-powerful building block design.</p></blockquote><p id="1164"><a href="#9b78" rel="noopener ugc nofollow"><strong>Phase 3: Support Custom Needs and Scale Beyond 1000s Use Cases (2017–2019)</strong></a></p><blockquote><p id="49e6">As Stream Processing gained momentum in Netflix, many teams demanded lower latency and more processing flexibility in Data Engineering, Observability, and Machine Learning spaces. In this phase, we built a new Stream Processing development experience to enable custom use cases, and we also tackled the new engineering and operational challenges.</p></blockquote><p id="b51a"><a href="#dd1c" rel="noopener ugc nofollow"><strong>Phase 4: Expand Stream Processing Responsibilities — Challenges and Opportunities ahead (2020 — Present)</strong></a></p><blockquote><p id="fd74">With the fast-paced Data Platform technology evolution in the industry, there are many new challenges: difficulties in coordination, steep learning curve, divisive stream-to-batch boundaries, etc. In this phase, we explored Stream Processing playing a more prominent role in connecting technologies and raising abstractions to make Data Platforms easier to use. There are many more opportunities ahead of us.</p></blockquote><p id="b168">For each phase, I will go over the evolving business motivations, the team’s unique challenges, the strategy bets, and the use case patterns we discovered along the way.</p><p id="1385">Many people helped me review this post, without whose feedback I would never dig into many unbiased details. Special thanks to <a href="https://huyenchip.com/" rel="noopener ugc nofollow" target="_blank">Chip Huyen</a>, <a href="https://www.linkedin.com/in/stevenzhenwu/" rel="noopener ugc nofollow" target="_blank">Steven Wu</a>, <a href="https://www.linkedin.com/in/pramdas/" rel="noopener ugc nofollow" target="_blank">Prashanth Ramdas</a>, <a href="https://www.linkedin.com/in/garukun/" rel="noopener ugc nofollow" target="_blank">Guanhua Jiang</a>, <a href="https://www.linkedin.com/in/scottrshi/" rel="noopener ugc nofollow" target="_blank">Scott Shi</a>, <a href="https://madewithml.com/" rel="noopener ugc nofollow" target="_blank">Goku Mohandas</a>, <a href="https://www.linkedin.com/in/sunzhengdavid/" rel="noopener ugc nofollow" target="_blank">David Sun</a>, <a href="https://astasia.medium.com/" rel="noopener">Astasia Myers</a>, and <a href="https://www.linkedin.com/in/matthew-willian-77b9074a/" rel="noopener ugc nofollow" target="_blank">Matt Willian</a>!</p><p id="a341">In 2015, Netflix already had about 60MM subscribers and was aggressively expanding its international presence. We all knew that promptly scaling the platform leverage would be the key to sustaining the skyrocketing subscriber growth.</p><p id="72f4"><em>Side note: For those unfamiliar, the platform team provides leverage by centrally managing the foundational infrastructures so the product team can focus on business logic.</em></p><p id="1438">Our team had to figure out how to help Netflix scale the logging practices. At that time, Netflix had ~500 microservices, generating more than 10PB data every day in the ecosystem.</p><p id="fab5">Collecting these data serves two primary purposes for Netflix:</p><ol><li id="440d">Gain business <strong>analytics insights</strong> (e.g., user retention, average session length, what’s trending, etc.).</li><li id="92f2">Gain <strong>operation insights</strong> (e.g., measuring <a href="https://netflixtechblog.com/sps-the-pulse-of-netflix-streaming-ae4db0e05f8a" rel="noopener ugc nofollow" target="_blank">streaming plays per second</a> to quickly and easily understand the health of Netflix systems), so developers can alert or perform mitigations.</li></ol><p id="1f94">You may ask why we need to move the logs from the edge to the data warehouse in the first place? Due to the sheer volume, it was infeasible to do on-demand analytics on the online transactional databases at scale. The reason is that online transactional processing (OLTP) and online analytical processing (OLAP) are built with different assumptions — OLTP was built for row-oriented access patterns and OLAP for column-oriented access patterns. Underneath the hood, they use different data structures for optimizations.</p><p id="146f">For example, let’s say we want to know the average session length across hundreds of millions of Netflix users. Suppose we put this on-demand analytical query on a row-oriented OLTP system. It will result in full table scans at the row-level granularity and potentially lock up the database, and applications may become unresponsive, resulting in unpleasant user experiences. This type of analytics/reporting workload is much better off done on an OLAP system, hence the need to move the logs reliably in a low latency fashion.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*Eoflq9XSr3TjCJEh" width="700" height="310" role="presentation"/></p></div><figcaption>(Figure: Move data from Edge to Data Warehouse)</figcaption></figure><p id="5a9b">By 2015 logging volume had increased to 500 billion events/day (1PB of data ingestion), up from 45 billion events/day in 2011. The existing logging infrastructure (a simple batch pipeline platform built with Chukwa, Hadoop, and Hive) was failing rapidly against the alarmingly increasing subscriber numbers every week. By estimation, we had about six months to evolve towards a streaming-first solution. The below diagrams show the evolution from the failing batch architecture to the new streaming-based architecture.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*Ug1lK1nV3cN5pdP0" width="700" height="280" role="presentation"/></p></div><figcaption>(Figure: the failing batch pipeline architecture, before migration)</figcaption></figure><p id="0bf3">We decided to replace this failing infrastructure with <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a" rel="noopener ugc nofollow" target="_blank">Keystone</a>.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*pCz1Zh5RJTEirn51" width="700" height="299" role="presentation"/></p></div><figcaption>(Figure: the Keystone streaming architecture, after migration)</figcaption></figure><p id="3ee8">Another question you may have is why consider a streaming-first architecture? At the time, the value of getting the streaming architecture right outweighs the potential risks. Netflix is a data-driven company, and the immediate importance of a streaming architecture are:</p><ul><li id="97c0">Reduce developer and operation feedback loop. Developers heavily rely on looking at logs to make operational decisions. Access to fresher on-demand logs allows developers to detect issues earlier, hence better productivity.</li><li id="aae1">Better product experience. Many product features, such as personalized recommendation, search, etc., can benefit from fresher data to improve user experiences, leading to higher user retention, engagement, etc.</li></ul><p id="6136">We had to think through many challenges in building a streaming platform.</p><p id="01b3"><strong>Challenge 1: Large scale with limited resources</strong>. We had six months to build out Keystone to handle 500B events per day, and we had to pull it off with six team members.</p><p id="cde2"><strong>Challenge 2: Immature streaming ecosystem. </strong>Developing and operating streaming-first infrastructure was difficult in 2015 when both transport (Apache Kafka) and processing technologies (Apache Samza, Apache Flink) were relatively nascent. Few technology companies had proven successful streaming-first deployments at the scale we needed, so we had to evaluate technology options and experiment. Given our limited resources, we couldn’t build everything on our own and had to decide what to build and what nascent tools to bet on.</p><p id="70f1"><strong>Challenge 3: Analytical and operational concerns are different</strong>.</p><ul><li id="40a3">Analytical stream processing focuses on correctness and predictability. For example, moving all user clickstreams into the data warehouse requires data consistency (minimum duplicates or losses) and predictability on latency that typically falls in the minutes range. (Keystone is good at this)</li><li id="b6b3">Operational stream processing focuses more on cost-effectiveness, latency, and availability. For example, understanding the health state of the entire Netflix device ecosystem can benefit from sub-second to seconds level latency, and the data can be sampled or profiled from the source to save cost. (Mantis is good at this)</li></ul><p id="8474"><strong>Challenge 4: Cloud-native resilience for a stateful data platform is hard</strong>. Netflix had already operated on the AWS cloud for a few years. However, we were the first to get a stateful data platform onto the containerized cloud infrastructure. There are hundreds of thousands of physical machines powering the cloud in each data center underneath the hood. At this scale, hardware failures are inevitable. When these failures arise unexpectedly, it could be challenging for the systems to keep up with the availability and consistency expectations. It’s even more challenging in an unbounded low latency stream processing environment where any failure recovery can result in backpressure buildup. Cloud-native resilience for a streaming-first architecture would mean significant engineering challenges.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*CKzLoMSahcTmljsY" width="700" height="534" role="presentation"/></p></div><figcaption>(Figure: How does stream processing help with operational and analytical data)</figcaption></figure><p id="9b08">I include some observed use cases and their respective stream processing patterns for each innovation phase. So you can get a sense of the evolution over time.</p><pre><span id="883f">+-----------------------------------------------------------------+</span></pre><p id="bdf3"><strong>Bet 1: When building the MVP product, build it for the first few customers. </strong>When exploring the initial product-market fit, it’s easy to get distracted. We decided only to help a few high-priority high-volume internal customers and worry about scaling the customer base later. This decision not only enabled us to be laser-focused on the core of the product. It also made us conscious about what not to invest in (e.g., we used a spreadsheet instead of a full-fledged control plane to keep customer names, email addresses, and metadata information per pipeline to facilitate customer deployment during the MVP phase).</p><p id="58b2"><strong>Bet 2: Co-evolve with technology partners even if not in an ideal maturity state </strong>instead of reinventing the wheels on our own. This way, we could co-evolve the ecosystems together. We chose to partner early on.</p><ul><li id="fe54">Streaming partners: Externally, we worked with partners who were leading the stream processing efforts in the industry, e.g., LinkedIn (Kafka and Samza team), Confluent, Data Artisan (builders behind Apache Flink, and later renamed to Veverica). Doing so allowed us to contribute to the OSS for our needs while leveraging the community’s work.</li><li id="513c">Containerization partners: in 2015, it was still the early days of container virtualization technology. We needed a strategy to deploy quickly. Internally, we partnered up with the newly created container infrastructure <a href="https://netflix.github.io/titus/" rel="noopener ugc nofollow" target="_blank">Titus team</a>. Titus was built on top of Apache Mesos and provided compute resource management, scheduling, and isolated deployment, via an abstracted API. Titus later evolved to leverage K8S in early 2020, and their team managed to migrate all workloads transparently. Thanks to this partnership, we didn’t have to worry about the low-level compute infrastructure while building the data platform.</li></ul><p id="938c">During the collaboration, we stayed in constant communication to share learnings and provide feedback. We had biweekly sync meetings with close partners to align on goals and discuss issues and solutions. When there were blocking issues, the right people would be involved right away.</p><p id="04a9"><strong>Bet 3: Decouple concerns </strong>versus ignoring them.</p><ul><li id="495f">Separated concerns between operational and analytics use cases. We evolved Mantis (operations-focused) and Keystone (analytics-focused) separately but created room to interface both systems.</li></ul><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*nZvwvMkdLyuen8kM" width="700" height="448" role="presentation"/></p></div><figcaption>（Figure: separation of concerns for different stream processing scenarios)</figcaption></figure><ul><li id="dd86">Separated concerns between producers and consumers. We introduced producer/consumer clients equipped with standardized wire protocol and simple schema management to help <strong>decouple the development workflow of producers and consumers. It later proved to be an essential aspect in Data Governance and data quality control.</strong></li><li id="65ee">Separated component responsibilities. We started with a microservice-oriented single responsibility principle, dividing the entire infrastructure into Messaging (Streaming Transport), Processing (Stream Processing), and Control Plane (the control plane was only a script in this phase, later evolved into a full-fledged system). The separation of component responsibilities enabled the team to align on interfaces early on while unlocking the team’s productivity by focusing on different parts concurrently.</li></ul><p id="86b0"><strong>Bet 4: Invest in building a system that expects failures and monitors all operations </strong>versus delaying the investment. Due to the elastic, constantly-changing, higher-failure-probability characteristics of the cloud, we need to design the system to monitor, detect, and tolerate all sorts of failures. The failures can range from network blips, instance failure, zone failure, cluster failure, inter-service congestion &amp; backpressure, regional disaster failures, etc. We invested in building the systems with the assumption that failures are constant. We embraced DevOps practices early on: such as design for failure scenarios, rigorous automation, continuous deployment, shadow testing, automated monitoring, alerting, etc. This DevOps foundation enabled us the ultimate engineering agility to ship multiple times a day.</p><figure><p><img alt="" src="https://miro.medium.com/max/1096/0*wa_OF7emzlgjmkDC" width="548" height="310" role="presentation"/></p><figcaption>(reference: <a href="https://www.quora.com/What-is-Chaos-Monkey-in-DevOps" rel="noopener ugc nofollow" target="_blank">What is Chaos Money in DevOps</a> on Quora)</figcaption></figure><p id="a0ad"><strong>Having a psychologically safe environment to fail is essential for any team to lead changes.</strong></p><p id="4344">We made many mistakes. We failed miserably on the product launch day and had a company-wide incident that caused massive data loss. After investigation, it turned out that despite correctly estimating the traffic growth, the monstrous Kafka cluster (with over 200 brokers) we built out was too big and eventually hit its scaling limit. When one broker died, the cluster couldn’t recover on its own due to the limitations in Kafka (at the time) regarding broker and partition numbers. It eventually degraded to the point of no recovery.</p><p id="93dd">It was a terrifying experience to fail at this scale, but thanks to the psychologically safe team environment, we communicated transparently with all the stakeholders and turned the quick learnings into permanent solutions. For this particular case, we developed more granular cluster isolation capabilities (smaller Kafka clusters + isolated Zookeeper) to contain the blast radius.</p><p id="6d1e">There were many other failures. As we dug through the root cause, we realized that we wouldn’t be able to fully anticipate all edge scenarios, especially when we were building on a managed cloud where changes often go outside of our control, such as instance terminations or tenant colocation, etc. At the same time, our product is used by many and is too important to fail. It became an operational principle always to prepare our operations for the worst-case scenario.</p><p id="8c93">After the incident, we adopted a weekly Kafka cluster failover drill. Every week the oncall person would simulate a Kafka cluster failure. The team would ensure the failover automation works to migrate all traffic to a healthy cluster with minimum user impact. If you are interested in learning more about this practice, this <a href="https://www.youtube.com/watch?v=TrPvC7P8XjA&amp;t=1586s" rel="noopener ugc nofollow" target="_blank">video</a> has more details.</p><p id="d71e">After shipping the initial Keystone MVP and migrating a few internal customers, we gradually gained some trust and words spread to other engineering teams. Streaming gained momentum in Netflix because it’s now easy to move logs for analytical processing and gain on-demand operational insights.</p><p id="adb3">It was time for us to scale for general customers.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*HTY4M3ToHB03j5lp" width="700" height="440" role="presentation"/></p></div><figcaption>(Figure: evolving Keystone architecture diagram, circa 2016. Keystone includes Kafka and Flink engines as its core components. For more technical design details, please refer to blog posts focused on <a href="https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb" rel="noopener ugc nofollow" target="_blank">Kafka</a> and <a href="https://netflixtechblog.com/a3ee651812a" rel="noopener ugc nofollow" target="_blank">Flink</a>)</figcaption></figure><p id="a6b1"><strong>Challenge 1: Increased operation burden. </strong>We initially offered white-glove assistance to onboard new customers. However, it quickly became unsustainable given the growing demand. We needed to start evolving the MVP to support more than just a dozen customers. As a result, we need to rebuild some components (e.g., it’s time to turn the spreadsheet into a proper database-backed control plane).</p><p id="eb80"><strong>Challenge 2: Diverse needs arise</strong>. As we got more customer requests, we started seeing very diverse needs. There were two major categories:</p><ul><li id="bc1f">One group preferred a fully managed service that’s simple to use.</li><li id="e237">The other group preferred flexibility and needed complex computation capabilities to solve more advanced business problems, and they are ok with taking pagers and even managing some infrastructures.</li></ul><p id="b6ef">We can’t do both well at the same time.</p><p id="4b65"><strong>Challenge 3: We broke everything we touched</strong>. No kidding, we broke pretty much all our dependent services at some point due to the scale. We <a href="https://www.youtube.com/watch?v=w2AF8Ld-Ofs&amp;t=211s" rel="noopener ugc nofollow" target="_blank">broke aws S3 once</a>. We found many bugs in Kafka and Flink. We broke <a href="https://netflix.github.io/titus/" rel="noopener ugc nofollow" target="_blank">Titus</a> (the managed container infrastructure) multiple times and discovered weird CPU isolation and network issues. We broke <a href="https://spinnaker.io/" rel="noopener ugc nofollow" target="_blank">Spinnaker</a> (the continuous deployment platform) when we started to manage hundreds of customer deployments programmatically.</p><p id="c69f">Luckily those teams were also the best. They worked with us to solve these problems one by one. These efforts are critical to maturing the entire streaming ecosystem.</p><pre><span id="80d7">+-----------------------------------------------------------------+</span></pre><p id="fc9a"><strong>Bet 1: Focus on simplicity first </strong>versus exposing infrastructure complexities to users. We decided to first focus on a highly abstracted, fully managed service for the general streaming use cases for two reasons.</p><ol><li id="d603">It’d allow us to address most data movement and simple streaming ETL (e.g., projection, filtering) use cases. Providing such simple, high-level abstraction for data routing would enable engineers across all Netflix organizations to use the data routing as a “lego” building block in conjunction with other Platform services.</li><li id="12d7">It’d enable our users to focus on business logic.</li></ol><p id="4d2d">We’d tackle the more advanced use cases for later.</p><p id="5f27"><strong>Bet 2: Invest in a fully-managed multi-tenancy self-service </strong>versus continuing with manual white-glove support. We had to focus on automating the control plane and workload deployment. <strong>Customer workloads need to be </strong><a href="https://www.youtube.com/watch?v=TrPvC7P8XjA&amp;t=1380s" rel="noopener ugc nofollow" target="_blank"><strong>fully isolated</strong></a><strong>.</strong> We decided that one customer’s workload should not interfere with another customer’s in any way.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*DMecV2nL77FQv8TM" width="700" height="398" role="presentation"/></p></div><figcaption>(Figure: Keystone UI. showing a self-serve drag-n-drop experience powered by a fully managed multi-tenant streaming infrastructure)</figcaption></figure><p id="3b3d"><strong>Bet 3: Continue to invest in DevOps </strong>versus delaying it. We wanted to ship platform changes multiple times a day as needed. <strong>We also believe it’s necessary to enable our customers to ship changes anytime they want.</strong> Deployment should be done automatically and rolled into production safely minutes after the customer initiated it.</p><p id="5b3e"><strong>Learning 1: Deciding what NOT to work on is hard, but necessary.</strong> While it’s important to address customer requests, some can be distracting. Prioritization is the first step, but consciously deciding and communicating what to cut is even more critical. Saying no is hard. But be aware <strong>saying no is temporary, and saying yes is permanent.</strong></p><p id="db34"><strong>Learning 2: Pay attention to scaling velocity</strong>. It’s an exciting time after the initial product-market fit is validated. However, scaling too fast would risk the team being distracted from multiple directions, leaving behind loads of tech debt, and breaking customer trust. Scaling too slowly would leave the team unmotivated, customer needs unfulfilled for too long, also breaking their trust. It’s a delicate balance, but here are some signals you can watch out for:</p><ul><li id="d02e"><strong>Software quality</strong>. Does the deployment rollback frequency change? How often do the partner teams block the team? Do tests fail more often now? How often do incidents happen due to any system bottleneck?</li><li id="547f"><strong>Customer sentiment</strong>. Is the increase in customer support requests sublinear to the number of use cases? SLO violations trends. Do customers show excitement in new feature announcements? When there is an “urgent” ask from the customer, what alternatives have they considered?</li><li id="4456"><strong>Operational overhead</strong>. Does the team’s operation-to-development time ratio change? Does the RCA-to-incident ratio change? Is the team burned out from operation toil? Does the team’s innovation frequency change (e.g., blog posts, conference talks)?</li></ul><p id="f168"><strong>Learning 3: Educate your users and be patient in correcting misconceptions. </strong>There were many stream processing misconceptions around data quality like event losses or duplications, or around processing semantics like correctness guarantees under failure scenarios. Many of these misconceptions came from an old-time when Stream Processing was not mature, but a lot has evolved. Be patient with your users and educate them with data and stories!</p><p id="4b90">A year after initially launching the Keystone product, the number of use cases quickly rose from less than a dozen in 2015 to a few hundred in 2017 across all organizations. We had built a solid operational foundation at this point: customers were rarely notified during their oncalls, and all infrastructure issues were closely monitored and handled by the platform team. We had built a robust delivery platform, helping our customers to introduce changes into production just minutes after the intent.</p><p id="84df">Keystone product was very good at what it was originally designed to do: a streaming data routing platform that’s easy to use and almost infinitely scalable. However, it was becoming apparent that the full potential of stream processing was far from being realized. To satisfy custom use cases, we constantly stumble upon new needs for more granular control on complex processing capabilities, e.g., streaming joins, windowing, etc.</p><p id="bdc7">Meanwhile, Netflix has a unique freedom and responsibility culture where each team is empowered to make its own technical decisions. This culture enables each team to even invest in bespoke solutions. As a central platform team, we noticed such growth. If we don’t have a way to provide the coverage, it would mean high long-term costs to the company.</p><p id="0e96">It was time for the team to expand its scope. Again, we faced some new challenges.</p><p id="824a"><strong>Challenge 1: Custom use cases require a different developer &amp; operation experience.</strong></p><p id="6655">Let me first give two examples of custom stream processing use cases.</p><ol><li id="57cf"><strong>Compute </strong><a href="https://www.youtube.com/watch?v=o4C7TDneH00" rel="noopener ugc nofollow" target="_blank"><strong>streaming ground truth for Recommendation</strong></a><strong>.</strong> For the Netflix Recommendation algorithm to provide the best possible experience, it’s necessary to train the model with the latest data. One of the inputs to training the model is the Label dataset. The labels are the direct ground-truth indicator of whether the previous personalized predictions were accurate or not. If the user decides to watch a movie for a recommendation, we have a positive label. As you can guess, the faster we can get this Label dataset, the quicker the entire ML feedback loop. To compute the labels, we’ll need to join together the impressions stream and the user-clicks stream. However, user click-activity can often arrive late. For example, users sometimes take a few minutes to decide or simply leave their devices on without watching for hours. The use case requires the streaming pipeline to emit the labels as soon as all relevant information arrives but is still tolerant of late-arriving information.</li><li id="7c5e"><strong>Compute </strong><a href="https://www.infoq.com/presentations/netflix-event-stream-flink/" rel="noopener ugc nofollow" target="_blank"><strong>take-fraction for Recommendation</strong></a><strong>.</strong> Netflix makes personalized recommendations to optimize the user experience. One of which is an algorithm to select the best-personalized artwork (and the best location to show them) to optimize the user take-fraction. Underneath the hood, a stream processing pipeline computes this take-fraction metric in near-real-time by joining the playback data stream with the impression stream over some custom window. Due to the scale of hundreds of millions of Netflix’s user base, the streaming job needs to constantly checkpoint the internal state ranging between 1–10 TB.</li></ol><figure><p><img alt="" src="https://miro.medium.com/max/1276/0*DYsClApQStqF5_eT" width="638" height="314" role="presentation"/></p><figcaption>(Figure: A/B test to select the best artwork to personalize to the user,</figcaption></figure><p id="308a">reference: <a href="https://netflixtechblog.com/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6" rel="noopener ugc nofollow" target="_blank">Selecting the best artwork for videos through A/B testing | by Netflix</a>)</p><p id="6a96">These use cases involve more advanced stream processing capabilities, such as complex event/processing time and window semantics, allowed lateness, large-state checkpoint management. They also require more operational support around observability, troubleshooting, and recovery. A brand new developer experience is necessary, including more flexible programming interfaces and operation capabilities such as custom observability stack, backfill capabilities, and a proper infrastructure to manage 10s of TBs local states. We didn’t have these in Keystone, and we need to build a new product entry point but minimize redundant investment.</p><p id="768e"><strong>Challenge 2: Balancing between Flexibility &amp; Simplicity. </strong>With all the new custom use cases, we had to figure out the proper level of control exposure. We can go all the way to expose the lowest-level API at the tradeoff of much more challenging operations (because we would never be able to fully anticipate how users would use the engine). Or we can choose to go mid-way (e.g., expose limited functionality) at the risk of unsatisfied customers.</p><p id="cd4b"><strong>Challenge 3: Increased operation complexity. </strong>Supporting the custom use cases required us to increase the degree of freedom of the platform. As a result, we needed to improve operation observability in many complex scenarios. At the same time, as the platform integrated with many other data products, the touchpoints of our system increased, demanding operational coordination with other teams to serve our collective customers better.</p><p id="b082"><strong>Challenge 4: Central Platform vs. Local Platform. </strong>Our team’s responsibility was to provide a centralized stream processing platform. But due to the previous strategy to focus on simplicity, some teams have already invested in their local stream processing platforms using unsupported technology, e.g. Spark Streaming. We’ll have to convince them to move back to a paved path because they risk losing leverage from the platform and wasting resources on redundant investment. Now is the right time as we expand into custom use cases.</p><pre><span id="9413">+-----------------------------------------------------------------+</span></pre><p id="7965"><strong>Bet 1: Build a new product entry point but refactor existing architecture </strong>versus building a new product in isolation. On the analytical processing side, we decided to spin out a new platform from the original architecture to expose the full power of Stream Processing with Apache Flink. We would create a new internal customer segment from scratch, but we also decided it was the right time to refactor the architecture to minimize redundant investment (between the Keystone and Flink platforms). The lower Flink platform supports both Keystone and custom use cases in this new architecture.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*FyEBDW_I5A9eyKh3" width="700" height="781" role="presentation"/></p></div><figcaption>(Figure: Architecture splitting Flink Platform as a separate product entry point)</figcaption></figure><p id="5acc"><strong>Bet 2: Start with streaming ETL and observability use cases </strong>versus tackling all custom use cases all at once. There were many opportunities, but we decided to focus on streaming ETL use cases on the analytical side and real-time observability use cases on the operational side. These use cases are the most challenging due to their complexity and scale. With the intent to expose the full power of stream processing, it makes sense for us to tackle and learn from the most difficult ones first.</p><p id="d742"><strong>Bet 3: Share operation responsibilities with customers initially </strong>and<strong> </strong>gradually co-innovate to lower the burden over time. We were so lucky our early adopters were self-sufficient, and we also offered a white-glove support model whenever customers struggled. We gradually expanded operational investments such as autoscaling, managed deployments, intelligent alerting, backfill solutions, etc.</p><p id="546f"><strong>Learning 1:</strong> A new product entry point to support new custom use cases is a necessary evolution step. It’s also an opportunity to re-architect/refactor and <strong>fit into the existing product ecosystem</strong>. Do not be lured into building a new system in isolation. Avoid the <a href="https://en.wikipedia.org/wiki/Second-system_effect" rel="noopener ugc nofollow" target="_blank">second-system effect</a>.</p><p id="b37b"><strong>Learning 2: Simplicity attracts 80% of the use cases. Flexibility helps the bigger use cases</strong>. Retrospectively, these are the observations on the actual customer segments from the past few years. The point I want to convey to the reader here is that prioritizing between supporting either the majority or the higher impact use cases, all depends on the situation. The argument can be made both ways, but you should articulate the reasoning that fits into your business scenarios.</p><p id="64cc">Simplicity and flexibility are NOT two polarizing ends of the spectrum. It’s <strong>a closed innovation feedback loop.</strong> The power of flexibility will drive new co-innovation with a small subset of customers. In the beginning, these innovations may be more expensive, but after it’s proven, the value could eventually become a commodity and go back to the simplified experience. As these new values help growing customers, a small subset of the new users will ask for the power of flexibility again.</p><p id="9584"><strong>Learning 3: Treat your early adopters well. </strong>They are the most loyal customers and will do the marketing for you for free. Thanks to the endorsement from our early adopters, our use cases exploded to thousands during this phase.</p><p id="58aa"><strong>Learning 4: When things break, don’t panic</strong>. Trust all the people around you. A bonus point if you already have a community supporting the product. I remember a time we experienced a slow degradation of the entire platform. Every day, we were getting loads of pages, and we could not figure out the root cause for two weeks. It was terrifying, the team was miserable, and customers were suffering. But the team was able to work together across the team boundaries involving the folks with the right expertise, using data to logically reason through symptoms. Eventually, we found a bug in the Linux Kernel which caused a slow memory leak specific to the streaming workload. We had to trust all the people involved, sometimes even when we didn’t have all the expertise!</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*3mXqGTnwQlnhkVoR" width="700" height="599" role="presentation"/></p></div><figcaption>(Figure: how stream processing fits in Netflix — 2021)</figcaption></figure><p id="e467">As stream processing use cases expanded to all organizations in Netflix, we discovered new patterns, and we enjoyed the early success. But it’s not the time for complacency.</p><p id="aedc">Netflix, as a business, continued to explore new frontiers and made heavy investments in Content Production Studio, and more recently in Gaming. A series of new challenges emerged, and we jumped into solving these interesting problem spaces.</p><p id="bd52"><strong>Challenge 1: Diverse data technologies made coordination difficult</strong>. Since teams are empowered, many teams in Netflix are using various data technologies. For instance, on the transactional side: there are Cassandra, MySQL, Postgres, CockroachDB, Distributed Cache, etc. On the analytical side: there are Hive, Iceberg, Presto, Spark, Pig, Druid, Elasticsearch, etc. Many copies of the same data are often stored across different data stores within the Netflix data ecosystem.</p><p id="25a6">With many choices available, it’s human nature to put technologies in dividing buckets. Batch vs. streaming. Transactional store vs. analytical store. Online processing vs. offline processing. These are all commonly debated topics in the data world. The overlapping dividing boundaries often add more confusion to the end-users.</p><p id="7c31">Today, coordinating and working with data across technology boundaries are incredibly challenging. <strong>Frontiers are hard to push with dividing boundaries</strong>.</p><p id="e565"><strong>Challenge 2: Steeper learning curve. </strong>With an ever-increasing amount of available data tools and continued deepening specialization, it is challenging for users to learn and decide what technology fits into a specific use case.</p><p id="0607"><strong>Challenge 3: ML practices are not leveraging the full power of the Data Platform</strong>. All previously mentioned challenges add a toll on ML practices. Data Scientists’ feedback loops are long, Data Engineers’ productivity suffers, Product engineers have challenges sharing valuable data. Ultimately, many businesses lose opportunities to adapt to the fast-changing market.</p><p id="3192"><strong>Challenge 4: Scale limits on the Central Platform model. </strong>As the central data platform scales use cases at a superlinear rate, it’s unsustainable to have a single point of contact for support. It’s the right time to evaluate a model where the central platform supports local-central platforms for added leverage (this means we’ll prioritize supporting the local platforms that are built on top of our platform).</p><p id="6784">I’ll be relatively brief on this section and expand the details in future blog posts.</p><p id="8e6f"><strong>Use streams to connect worlds</strong>. For Stream Processing, besides the strength of low latency processing, it’s increasingly showing an even more critical strength in modern data platforms: connecting various technologies and enabling fluid data exchanges. Technologies such as Change Data Capture (CDC), streaming materialized views, and Data Mesh concepts are gaining popularity. Finally, Martin Kleppmann’s 2015 vision of “<a href="https://martin.kleppmann.com/2015/11/05/database-inside-out-at-oredev.html" rel="noopener ugc nofollow" target="_blank">Turning Database Inside Out”</a> starts to realize its value.</p><p id="2b56"><strong>Raise abstraction by combining the best of both simplicity and flexibility</strong>. There is much value in understanding the deep internals of various data technologies, but not everyone needs to do it. This line of thinking is especially true as cloud-first data infrastructures are becoming commodities. Properly raising data infrastructure abstraction becomes the immediate opportunity to make all the advanced capabilities easily accessible to a broader audience. Technologies such as streaming SQL will lower the barrier of entry, but it’s just the beginning. Data Platform should also raise abstraction to the dividing boundaries (e.g., streaming vs. batch) are invisible to the end-users.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*UzeDcyUG3mn78AZ0" width="700" height="642" role="presentation"/></p></div><figcaption>(Figure: the sweet spot between simplicity and flexibility)</figcaption></figure><p id="1ffa"><strong>Machine Learning needs more love from modern Data platforms</strong>. ML folks are arguably the most impactful to business and the most under-served group among all the developer personas. All ML platforms depend on data storage and processing. So Data Platform has many opportunities to extend helping hands to the ML world: such as data quality, reliability &amp; scalability, dev-to-prod feedback loop, real-time observability, overall efficiency, etc.</p><pre><span id="05a1">+-----------------------------------------------------------------+</span></pre><p id="fb85">Thank you for reaching this far. This blog post describes the high-level iterative journey of building out the stream processing infrastructure at Netflix. I would love to hear your feedback on what’s interesting, so I can follow up with future blog posts.</p><p id="be85">By design, I omitted many technical details in this post. But if you are interested in finding out more, please refer to the Appendix section for a complete timeline view of all the stream processing innovations in Netflix.</p><p id="2bdf">I am very excited about the future of Data Infrastructure, especially to support better Machine Learning experiences. I believe this is the next frontier we shall go boldly towards! If you are intrigued, I highly recommend an excellent read “<a href="https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html" rel="noopener ugc nofollow" target="_blank">Real-time machine learning: challenges and solutions</a>” from my amazing friend &amp; colleague Chip.</p><p id="0465">I am also excited to announce that I am starting a new journey with <a href="https://huyenchip.com/" rel="noopener ugc nofollow" target="_blank">Chip Huyen</a> to work on a streaming-first Machine Learning platform. We are still very early, and we are looking for a <a href="https://claypot.ai/careers.html" rel="noopener ugc nofollow" target="_blank">founding infrastructure engineer</a> to shape the future together! If you are remotely intrigued, we would love to <a href="https://www.linkedin.com/in/zhenzhong-xu-0243003/" rel="noopener ugc nofollow" target="_blank">hear from you</a>!</p><p id="4764">If this blog post resonates with you, please reach out. I’d like to connect!</p><figure><p><img alt="" src="https://miro.medium.com/max/472/0*p6tyFUAKjr9sDOB9" width="236" height="374" role="presentation"/></p></figure><pre><span id="c982">+-----------------------------------------------------------------+</span></pre><p id="297b">Indexed in chronological order …</p><ul><li id="6df1">[<strong>2015</strong>|Use Case] <a href="https://netflixtechblog.com/whats-trending-on-netflix-f00b4b037f61" rel="noopener ugc nofollow" target="_blank">What’s trending on Netflix? improving our recommender systems</a></li><li id="8b66">[<strong>2015</strong>|Use Case] <a href="https://netflixtechblog.com/sps-the-pulse-of-netflix-streaming-ae4db0e05f8a" rel="noopener ugc nofollow" target="_blank">SPS: the Pulse of Netflix Streaming | by Netflix</a></li><li id="e717">[<strong>2016</strong>|Platform] <a href="https://netflixtechblog.com/evolution-of-the-netflix-data-pipeline-da246ca36905" rel="noopener ugc nofollow" target="_blank">Evolution of the Netflix Data Pipeline | by Netflix</a></li><li id="3007">[<strong>2016</strong>|Platform] <a href="https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb" rel="noopener ugc nofollow" target="_blank">Kafka Inside Keystone Pipeline</a></li><li id="ecdb">[<strong>2016</strong>|Platform] <a href="https://netflixtechblog.com/stream-processing-with-mantis-78af913f51a6" rel="noopener ugc nofollow" target="_blank">Stream-processing with Mantis… | by Netflix</a></li><li id="7a37">[<strong>2017</strong>|Platform] <a href="https://www.youtube.com/watch?v=TrPvC7P8XjA" rel="noopener ugc nofollow" target="_blank">Running a Massively Parallel Self serve Distributed Data System At Scale — Zhenzhong Xu</a></li><li id="59c5">[<strong>2017</strong>|Platform] <a href="https://www.youtube.com/watch?v=sPB8w-YXX1s" rel="noopener ugc nofollow" target="_blank">FlinkForward: Stream Processing with Flink at Netflix — Monal Daxini</a></li><li id="6a22">[<strong>2017</strong>| Platform] <a href="https://www.youtube.com/watch?v=XUvqnsWm8yo" rel="noopener ugc nofollow" target="_blank">Custom, Complex Windows at Scale using Apache Flink — Matt Zimmer (Netflix)</a></li><li id="1a5c">[<strong>2017</strong>|Use Case] <a href="https://www.infoq.com/presentations/netflix-personalization-datasets-streaming/" rel="noopener ugc nofollow" target="_blank">Streaming for Personalization Datasets at Netflix</a></li><li id="0542">[<strong>2017</strong>|Use Case] <a href="https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f" rel="noopener ugc nofollow" target="_blank">ChAP: Chaos Automation Platform | by Netflix</a></li><li id="efa6">[<strong>2018</strong>|Platform] <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a" rel="noopener ugc nofollow" target="_blank">Keystone Real-time Stream Processing Platform | by Netflix</a></li><li id="3cdf">[<strong>2018</strong>|Use Case] <a href="https://www.youtube.com/watch?v=6UwcqiNsZ8U" rel="noopener ugc nofollow" target="_blank">“Scalable Anomaly Detection (with Zero Machine Learning)” by Arthur Gonigberg</a></li><li id="743e">[<strong>2018</strong>|Use Case] <a href="https://www.infoq.com/articles/netflix-migrating-stream-processing/" rel="noopener ugc nofollow" target="_blank">Migrating Batch ETL to Stream Processing: A Netflix Case Study with Kafka and Flink</a></li><li id="3714">[<strong>2018</strong>|Platform] <a href="https://conferences.oreilly.com/software-architecture/sa-ny-2018/cdn.oreillystatic.com/en/assets/1/event/281/Building%20stream%20processing%20as%20a%20service%20at%20Netflix%20Presentation.pdf" rel="noopener ugc nofollow" target="_blank">Building Stream Processing as a Service (SPaaS) — Steven Wu</a></li><li id="d1de">[<strong>2018</strong>|Platform] <a href="https://www.infoq.com/presentations/cloud-native-kafka-netflix/" rel="noopener ugc nofollow" target="_blank">Cloud-Native and Scalable Kafka Architecture</a></li><li id="5740">[<strong>2019</strong>|Platform] <a href="https://www.youtube.com/watch?v=w2AF8Ld-Ofs" rel="noopener ugc nofollow" target="_blank">Scaling Flink in Cloud — Steven Wu</a></li><li id="680e">[<strong>2019</strong>|Use Case] <a href="https://www.youtube.com/watch?v=lC0d3gAPXaI" rel="noopener ugc nofollow" target="_blank">Massive Scale Data Processing at Netflix using Flink — Snehal Nagmote &amp; Pallavi Phadnis</a></li><li id="d3bf">[<strong>2019</strong>|Use Case] <a href="https://vimeo.com/369638986" rel="noopener ugc nofollow" target="_blank">Cost-Effective, Realtime Operational Insights Into Production Systems</a></li><li id="f7d2">[<strong>2019</strong>|Use Case] <a href="https://www.youtube.com/watch?v=o4C7TDneH00" rel="noopener ugc nofollow" target="_blank">Real-time Processing with Flink for Machine Learning at Netflix — Elliot Chow</a></li><li id="203c">[<strong>2019</strong>|Platform] <a href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a" rel="noopener ugc nofollow" target="_blank">Open Sourcing Mantis: A Platform For Building Cost-Effective, Realtime, Operations-Focused Applications | by Netflix</a></li><li id="d3d0">[<strong>2019</strong>|Platform] <a href="https://www.youtube.com/watch?v=TO_IiN06jJ4" rel="noopener ugc nofollow" target="_blank">Netflix Data Mesh: Composable Data Processing — Justin Cunningham</a></li><li id="1312">[<strong>2019</strong>|Platform] <a href="https://www.alibabacloud.com/blog/netflix-evolving-keystone-to-an-open-collaborative-real-time-etl-platform_596679" rel="noopener ugc nofollow" target="_blank">Netflix: Evolving Keystone to an Open Collaborative Real-time ETL Platform — Alibaba Cloud Community</a></li><li id="a8b1">[<strong>2019</strong>|Use Case] <a href="https://netflixtechblog.com/delta-a-data-synchronization-and-enrichment-platform-e82c36a79aee" rel="noopener ugc nofollow" target="_blank">Delta: A Data Synchronization and Enrichment Platform | by Netflix</a></li><li id="ef64">[<strong>2019</strong>|Use Case] <a href="https://netflixtechblog.com/dblog-a-generic-change-data-capture-framework-69351fb9099b" rel="noopener ugc nofollow" target="_blank">DBLog: A Generic Change-Data-Capture Framework | by Netflix</a></li><li id="99cc">[<strong>2020</strong>|Platform] <a href="https://www.youtube.com/watch?v=NV0jvA5ZDNc" rel="noopener ugc nofollow" target="_blank">Autoscaling Flink at Netflix — Timothy Farkas</a></li><li id="b77e">[<strong>2020</strong>|Use Case] <a href="https://www.confluent.io/blog/how-kafka-is-used-by-netflix/" rel="noopener ugc nofollow" target="_blank">How Netflix Uses Kafka for Distributed Streaming — by Confluent</a></li><li id="d7c5">[<strong>2020</strong>|Use Case] <a href="https://www.youtube.com/watch?v=AUreuwzU4nA" rel="noopener ugc nofollow" target="_blank">Building metric platform using Flink for massive scale at Netflix — Abhay Amin</a></li><li id="a3d5">[<strong>2020</strong>|Use Case] <a href="https://www.infoq.com/presentations/netflix-event-stream-flink/" rel="noopener ugc nofollow" target="_blank">Taming Large State: Lessons from Building Stream Processing Joins for Datasets for Personalization</a></li><li id="2c4c">[<strong>2020</strong>|Use Case] <a href="https://netflixtechblog.com/building-netflixs-distributed-tracing-infrastructure-bb856c319304" rel="noopener ugc nofollow" target="_blank">Building Netflix’s Distributed Tracing Infrastructure | by Netflix</a></li><li id="9a8c">[<strong>2020</strong>|Use Case] <a href="https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba" rel="noopener ugc nofollow" target="_blank">Telltale: Netflix Application Monitoring Simplified | by Netflix</a></li><li id="9e1a">[<strong>2020</strong>|Use Case] <a href="https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06" rel="noopener ugc nofollow" target="_blank">How Netflix uses Druid for Real-time Insights to Ensure a High-Quality Experience</a></li><li id="9f49">[<strong>2021</strong>|Platform] <a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059" rel="noopener ugc nofollow" target="_blank">Data Movement in Netflix Studio via Data Mesh | by Netflix</a></li><li id="ac9a">[<strong>2021</strong>|Platform] <a href="https://www.youtube.com/watch?v=tB4rx_W9Xqw" rel="noopener ugc nofollow" target="_blank">Backfill Flink Data Pipelines with Iceberg Connector</a></li><li id="69fc">[<strong>2022</strong>|Platform] <a href="https://netflixtechblog.com/auto-diagnosis-and-remediation-in-netflix-data-platform-5bcc52d853d1" rel="noopener ugc nofollow" target="_blank">Auto-Diagnosis and Remediation in Netflix Data Platform | by Netflix</a></li></ul></div></div></section></div>
  </body>
</html>
