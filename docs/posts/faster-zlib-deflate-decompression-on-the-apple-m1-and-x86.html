<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dougallj.wordpress.com/2022/08/20/faster-zlib-deflate-decompression-on-the-apple-m1-and-x86/">Original</a>
    <h1>Faster zlib/DEFLATE decompression on the Apple M1 (and x86)</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<p>DEFLATE is a relatively slow compression algorithm from 1991, which (along with its wrapper format, zlib) is incredibly widely used, for example in the PNG, Zip and Gzip file formats and the HTTP, SSL, and SSH protocols. As such, I had assumed the most widely used implementation, <a href="https://github.com/madler/zlib">zlib-madler</a>, was extremely optimised. I’d also imagined <a href="https://github.com/apple-oss-distributions/zlib">Apple’s system zlib</a> might outperform open-source alternatives on their own hardware. I was wrong. Fortunately there are optimised forks of zlib such as <a href="https://github.com/zlib-ng/zlib-ng">zlib-ng</a>, <a href="https://chromium.googlesource.com/chromium/src/+/main/third_party/zlib/">Chromium’s zlib</a> and <a href="https://github.com/cloudflare/zlib.git">zlib-cloudflare</a>, which significantly outperform zlib-madler and Apple’s system zlib.</p>



<p>I tried optimising zlib’s decompression for the M1, using zlib-cloudflare as a starting point, and was able to show 1.51x speed compared to zlib-cloudflare, and 2.1x speed compared to Apple’s system zlib:</p>



<figure><a href="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png"><img data-attachment-id="1392" data-permalink="https://dougallj.wordpress.com/2022/08/20/faster-zlib-deflate-decompression-on-the-apple-m1-and-x86/screen-shot-2022-08-20-at-1-24-31-am/" data-orig-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png" data-orig-size="2548,1518" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="screen-shot-2022-08-20-at-1.24.31-am" data-image-description="" data-image-caption="" data-medium-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=300" data-large-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=628" src="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=1024" alt="Graph of decompression performance, showing zlib-dougallj outperforming 3 alternatives by about 50% on each of the files in the Silesia Corpus. zlib-cloudflare is consistently second best, closely followed by zlib-ng, with zlib-apple further behind, outperformed by about 2x. The files have been compressed with zlib level 6, and throughput is measured in compressed mb/s on M1 using Apple clang 13.1.6. zlib-dougallj performs between 282 and 380mb/s, cloudflare performs between 186 and 276mb/s.
xml: apple 122, ng 181, cf 194, dj 290.
x-ray: apple 181, ng 190, cf 209, dj 330.
webster: apple 132, ng 195, cf 205, dj 303.
sao: apple 234, ng 260, cf 276, dj 380.
samba: apple 151, ng 211, cf 221, dj 321.
reymont: apple 131, ng 210, cf 213, dj 338.
osdb: apple 173, ng 206, cf 224, dj 352.
ooffice: apple 142, ng 170, cf 186, dj 291.
nci: apple 119, ng 171, cf 191, dj 282.
mr: apple 164, ng 187, cf 208, dj 329.
mozilla: apple 153, ng 187, cf 199, dj 307.
dickens: apple 148, ng 219, cf 238, dj 344." srcset="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=1024 1024w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=2048 2048w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=150 150w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=300 300w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.31-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Decompression speed comparison (higher is better)</figcaption></figure>



<p>My version is also faster on x86, at ~1.3x the speed of zlib-cloudflare using clang (~1.7x the speed of Apple’s system zlib), and ~1.46x using gcc (~2.3x the speed of Ubuntu 20.04’s system zlib).</p>



<p>If you have a choice, <em>please use a modern alternative like <a href="https://github.com/facebook/zstd">zstd</a> instead of zlib</em>. If you need zlib, consider optimised zlib forks, like <a href="https://github.com/zlib-ng/zlib-ng">zlib-ng</a> or <a href="https://github.com/cloudflare/zlib">zlib-cloudflare</a> (or <a href="https://github.com/ebiggers/libdeflate">libdeflate</a> if you don’t need zlib API compatibility or streaming). If you maintain a zlib fork or library, please consider incorporating some of <a href="https://github.com/dougallj/zlib-dougallj/commits/main">my changes</a>. (If you want to use my code, you are free to do so under the zlib license, but it is under-tested, and the bugs tend to be vulnerabilities.)</p>



<p>This speedup is the product of a number of changes, which are discussed in the remainder of the post. The changes interact with each other in very complex ways, so individual speedup measurements shouldn’t be taken seriously – making the same changes in a different order would completely change many numbers.</p>



<h2>Bigger root tables</h2>



<p>The decoder uses a table to look up bit-sequences and map them to their decoded values. The longest Huffman code allowed by the DEFLATE specification is 15-bits, which would require a 128kb table, so to fit tables comfortably in cache, zlib splits this into a root table and a number of (possibly nested) subtables.</p>



<p>Every time the decoder reads a symbol that isn’t in the root table, we’ll have a branch mispredict, followed by extra latency for the subtable lookup, making root-table-misses very expensive. One of the simplest speedups was <a href="https://github.com/dougallj/zlib-dougallj/commit/82b2c9ea382596617118ca38385edda41a72c853">increasing root table size</a> for a ~7% decompression speed improvement.</p>



<p>(The table size is still limited to the maximum code length used, meaning this typically has zero performance impact on tiny files, where table-building time dominates, since they’re unlikely to use longer codes.)</p>



<h2>Adler32 with UDOT</h2>



<p>The zlib wrapper adds an adler32 checksum to the DEFLATE data, which is verified on decoding.</p>



<p>There’s already a NEON SIMD adler32 implementation in zlib-cloudflare, but, not unlike <a href="https://dougallj.wordpress.com/2022/05/22/faster-crc32-on-the-apple-m1/">my CRC32 post</a>, the existing implementation doesn’t take full advantage of the unusually wide M1. I added <a href="https://github.com/dougallj/zlib-dougallj/commit/078f36bb8d04ffe23559e0500e9701df7c24ef88">a new implementation</a> which is 2x wider, takes advantage of the amazing <a href="https://developer.arm.com/documentation/100069/0609/A64-SIMD-Vector-Instructions/UDOT--vector-">UDOT instruction</a> (part of the optional dotprod extension in ARMv8.2, mandatory in v8.4), and uses 64-bit scalar values to allow more iterations of the inner loop before reducing. I think this computes checksums at ~1.5x the speed of the old version (~60GB/s), but the decompression speedup is only about 1.01x, since it was already very fast.</p>



<p>I was very happy to see UDOT-based adler32 <a href="https://github.com/ebiggers/libdeflate/commit/d8b53901fb752fbfd9e496f5c21977be6dea0cc2">added to libdeflate</a>, with impressive performance numbers on non-Apple CPUs too (e.g. ~1.7x speed on Cortex-A55).</p>



<h2>Reading Bits</h2>



<p>Most of the speedup just comes from reading and applying Fabian Giesen’s posts on Huffman decoding:</p>



<ul><li><a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">A whirlwind introduction to dataflow graphs</a></li><li><a href="https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1/">Reading bits in far too many ways</a>, and <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a> (and the comment about variant 4, partially lost to the less-than/greater-than handling bugs), and <a href="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/">part 3</a></li><li><a href="https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/">Entropy coding in Oodle Data: the big picture</a></li><li><a href="https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/">Entropy coding in Oodle Data: Huffman coding</a></li><li><a href="https://fgiesen.wordpress.com/2022/04/04/entropy-decoding-in-oodle-data-huffman-decoding-on-the-jaguar/">Entropy decoding in Oodle Data: Huffman decoding on the Jaguar</a></li></ul>



<p>The goal was to use roughly the following <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/#:~:text=Variant%204%3A%20a%20different%20kind%20of%20lookahead">variant-4-style</a> loop to decode and unconditionally refill with eight-cycle latency on Apple M1:</p>



<pre><code>; do {
;  bitbuf |= read64LE(bitptr) &lt;&lt; bitcount 
ldr   x11, [x0]
lsl   x11, x11, x9              ; 1c
orr   x10, x11, x10             ; 1c

;  bitptr += 7
add   x0, x0, #0x7             

;  bitptr -= ((bitcount &gt;&gt; 3) &amp; 7)
ubfx  x11, x9, #3, #3
sub   x0, x0, x11

;  bitcount |= 56
orr   x9, x9, #0x38

;  value = table[bitbuf &amp; 0x1FF]
and   x11, x10, #0x1ff          ; 1c
ldr   w11, [x4, x11, lsl #2]    ; 4c

;  bitcount -= value
sub   x9, x9, x11               ; 1c

;  bitbuf &gt;&gt;= (value &amp; 63)
lsr   x10, x10, x11

;  *output++ = value &gt;&gt; 8
lsr   x11, x11, #8
strb  w11, [x1], #1

; } while (--n);
subs  x8, x8, #0x1
b.ne  x8, start</code></pre>



<p>When we don’t need to refill, we can just do a decode with six-cycle latency:</p>



<pre><code>;  value = table[bitbuf &amp; 0x1FF]
and   x11, x10, #0x1ff          ; 1c
ldr   w11, [x4, x11, lsl #2]    ; 4c

;  bitcount -= value
sub   x9, x9, x11  

;  bitbuf &gt;&gt;= (value &amp; 63)
lsr   x10, x10, x11             ; 1c

;  *output++ = value &gt;&gt; 8
lsr   x11, x11, #8
strb  w11, [x1], #1</code></pre>



<p>Most of <a href="https://github.com/dougallj/zlib-dougallj/commits/main">the commits</a> are trying to encourage the compiler to generate this code without breaking anything. Simply <a href="https://github.com/dougallj/zlib-dougallj/commit/1ff73d33092896385db2527631c6a0a6a68fe331">forcing the compiler to use a 32-bit, shifted load</a> was a ~9% speedup, and <a href="https://github.com/dougallj/zlib-dougallj/commit/eb3000f94edba63a61656f99d874c914463f7ad7">moving the shift amount to the least-significant bits</a> was another ~3.5%. (Ignoring the high bits of bitcount was accidentally merged into <a href="https://github.com/dougallj/zlib-dougallj/commit/fdbf6021366ffea88a4a6b4090d207a573f42fd0">this commit</a>, but had another measurable speedup. Git is counterintuitive. Unconditional refill was combined with the upcoming fastpath optimisation <a href="https://github.com/dougallj/zlib-dougallj/commit/80590ad0bda2073a071cac0f438abffc40df0308">here</a>.)</p>



<p>As covered in Fabian’s linked blog posts, we’re mostly concerned with latency. The M1 can run 8 instructions per cycle, meaning that we have time to run 64 instructions during that 8-cycle latency chain – a limit we’re unlikely to hit. So the focus is on reducing bit-reading latency and reducing branch mispredicts.</p>



<h2>Reading Extra Bits</h2>



<p>When reading either the length or distance of an LZ77 pair, the decoder would first read a Huffman code, and then optionally read a number of <em>extra</em> bits:</p>



<pre><code>int entry = table[bits &amp; 0x1FF]; // 5c
bits &gt;&gt;= (entry &amp; 63); // 1c
int extra_bits = entry &gt;&gt; 8;
int extra = bits &amp; ((1 &lt;&lt; extra_bits) - 1);
bits &gt;&gt;= extra_bits; // 1c</code></pre>



<p>This second shift adds a cycle of latency. I changed this to <a href="https://github.com/dougallj/zlib-dougallj/commit/34b9fc457b5247d7d2d732e6f28c9a80ff16abd7">combine the two shifts</a>, then look back at the old value to extract the extra bits, saving a cycle of latency:</p>



<pre><code>int old_bits = bits;
int entry = table[bits &amp; 0x1FF]; // 5c
bits &gt;&gt;= (entry &amp; 63); // 1c, includes extra
int non_extra_bits = (entry &gt;&gt; 8);
old_bits &amp;= ~(-1 &lt;&lt; (entry &amp; 63)); // clear the high bits
int extra = old_bits &gt;&gt; non_extra_bits;</code></pre>



<p>This required significant changes to the table format, making it one of the more complicated and error prone changes. Speedup ~5.5%.</p>



<h2>Fast-Path for Literals</h2>



<p>Since we can have 6-cycle latency (rather than 8-cycle latency) if we decode without refilling, I <a href="https://github.com/dougallj/zlib-dougallj/commit/80590ad0bda2073a071cac0f438abffc40df0308#diff-c5ce5086fb4393c662f39c44859f0880d910c984c89bb7a4faa9d55a515573f3R197">added a fast-path</a> to the top of the loop that can decode two literals if they’re in the root table, before proceeding as usual. This is cheap because we already have to branch on whether or not a value is a literal.</p>



<p>This can take the first 20-bits of our 56-bit buffer. Unfortunately, the worst-case for a length+distance pair with extra bits is 48-bits, which we may no longer have, so this requires a conditional refill if we don’t have enough bits while reading a distance code. However, this worst-case is exceedingly rare – the whole point of compression is to use short codes in the common cases – and so the “needs refill” branch predicts essentially perfectly, and this has a very low cost. Speedup ~4%.</p>



<h2>Overlapping Mispredict Latency</h2>



<p>There’s a fancy “chunkcopy” optimisation, which is responsible for some of zlib-cloudflare’s performance over zlib-madler (by copying LZ77 references 16-bytes at a time), but this can have expensive branch mispredicts. There’s a benefit to <a href="https://github.com/dougallj/zlib-dougallj/commit/fdbf6021366ffea88a4a6b4090d207a573f42fd0">performing the refill and Huffman table lookup for the next iteration</a> before the chunkcopy, allowing the load latency to overlap with any mispredict latency. Speedup ~6%.</p>



<h2>Table-building tweaks</h2>



<p>Table-building isn’t terribly expensive nor terribly frequent, but does involve counting the number of symbols of each length (1-15), which can be problematic due to the latency of reading-back recently written memory. This might be <a href="https://twitter.com/lamchester/status/1530297321333739521">a bit worse on the M1 than on x86</a>, with no zero-latency forwarding (but no extreme penalties). I instead used NEON intrinsics to keep the sum in two 128-bit registers with lower latency for a ~0.5% improvement in decompression time. I also use clang’s bitreverse intrinsic, and split the table-filling loop to have simpler code for filling the (now larger) root table, and only run the more complex code for the (now smaller and less common) subtables, for another ~0.5%. <a href="https://github.com/dougallj/zlib-dougallj/commit/f23fa25aa168ef782bab5e7cd6f9df50d7bb5eb2">These changes</a> give a total of a ~1% speedup.</p>



<p>(I was trying anything I could think of to try to get to 1.5x, and this was what finally did it… and then I realised my geometric mean calculation was wrong, and I was already past 1.5x without this change. But it is nice to have.)</p>



<h2>Final notes</h2>



<p>There’s been a lot of excellent (and inspirational) prior work on zlib performance: <a href="https://www.youtube.com/watch?v=xzL_xDcqhnw">Adenilson Cavalcanti has been optimizing zlib decompression on ARM in Chromium</a>, <a href="https://blog.cloudflare.com/cloudflare-fights-cancer/">Vlad Krasnov significantly improved compression performance</a>, <a href="https://aws.amazon.com/blogs/opensource/improving-zlib-cloudflare-and-comparing-performance-with-other-zlib-forks/">Janakarajan Natarajan and Volker Simonis ported the Chromium optimisations to cloudflare-zlib</a>, and <a href="https://nigeltao.github.io/blog/2021/fastest-safest-png-decoder.html">Nigel Tao wrote The Fastest, Safest PNG Decoder in the World</a> (a great read if you missed it).</p>



<p><a href="https://github.com/ebiggers/libdeflate">Eric Bigger’s libdeflate</a> also deserves more mention in this post, and should have been included in the graphs – I was looking mainly at zlib-compatible projects, but it is significantly faster than zlib-cloudflare. zlib-dougallj currently outperforms libdeflate version 1.13 on M1 at ~1.17x speed, but I didn’t test other platforms.</p>



<p>My changes seem to be a speedup regardless of file, which I hadn’t imagined was possible. There’s still a fair bit of room for further work here – I didn’t even look at the x86 disassembly, but hopefully this can save some time and energy. The code is <a href="https://github.com/dougallj/zlib-dougallj">on Github</a>, under the zlib license, but has not been thoroughly tested. I’m <a href="https://twitter.com/dougallj">@dougallj</a> on Twitter.</p>



<figure><a href="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png"><img data-attachment-id="1406" data-permalink="https://dougallj.wordpress.com/2022/08/20/faster-zlib-deflate-decompression-on-the-apple-m1-and-x86/screen-shot-2022-08-20-at-1-24-53-am/" data-orig-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png" data-orig-size="2546,1498" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="screen-shot-2022-08-20-at-1.24.53-am" data-image-description="" data-image-caption="" data-medium-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=300" data-large-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=628" src="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=1024" alt="Graph of performance on Canterbury corpus, showing slightly smaller, but still nearly 1.5x speedups." srcset="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=1024 1024w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=2046 2046w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=150 150w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=300 300w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.24.53-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"/></a></figure>



<figure><a href="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png"><img data-attachment-id="1407" data-permalink="https://dougallj.wordpress.com/2022/08/20/faster-zlib-deflate-decompression-on-the-apple-m1-and-x86/screen-shot-2022-08-20-at-1-25-07-am/" data-orig-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png" data-orig-size="2554,1500" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="screen-shot-2022-08-20-at-1.25.07-am" data-image-description="" data-image-caption="" data-medium-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=300" data-large-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=628" src="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=1024" alt="Graph of performance on Calgary corpus, showing slightly larger, but still around 1.5x speedups." srcset="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=1024 1024w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=2048 2048w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=150 150w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=300 300w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-20-at-1.25.07-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"/></a></figure>



<p><strong>Update:</strong> New graph including more alternatives. Note that libcompression won’t be checking adler32. I should have used zlib-madler as a baseline: 4.85x</p>



<figure><a href="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png"><img data-attachment-id="1434" data-permalink="https://dougallj.wordpress.com/2022/08/20/faster-zlib-deflate-decompression-on-the-apple-m1-and-x86/screen-shot-2022-08-21-at-4-31-45-pm/" data-orig-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png" data-orig-size="3304,1362" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="screen-shot-2022-08-21-at-4.31.45-pm" data-image-description="" data-image-caption="" data-medium-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=300" data-large-file="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=628" src="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=1024" alt="" srcset="https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=1024 1024w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=2048 2048w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=150 150w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=300 300w, https://dougallj.files.wordpress.com/2022/08/screen-shot-2022-08-21-at-4.31.45-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"/></a></figure>
			</div></div>
  </body>
</html>
