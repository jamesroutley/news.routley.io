<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/how-i-tripped-over-the-debian-weak-keys-vuln.html">Original</a>
    <h1>How I Tripped Over the Debian Weak Keys Vulnerability</h1>
    
    <div id="readability-page-1" class="page"><div id="body">
			
			<div id="content">

				
				<p>
					Posted: Tue,  9 April 2024
					| <a href="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/how-i-tripped-over-the-debian-weak-keys-vuln.html">permalink</a>
					| <a href="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/how-i-tripped-over-the-debian-weak-keys-vuln.html#comments">
						
							No comments
						
					</a>
				</p>
<p><em>Those of you who haven’t been in IT for far, far too long might not know that next month will be the 16th(!) anniversary of the <a href="https://security-tracker.debian.org/tracker/DSA-1571-1">disclosure</a> of what was, at the time, a fairly earth-shattering revelation: that for about 18 months, the Debian OpenSSL package was <a href="https://security-tracker.debian.org/tracker/CVE-2008-0166">generating entirely predictable private keys</a>.</em></p>

<p>The recent <a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">xz-stential threat</a> (thanks to <a href="https://infosec.exchange/@nixCraft@mastodon.social">@nixCraft</a> for <a href="https://mastodon.social/@nixCraft/112219225728684695">making me aware of that one</a>), has got me thinking about my own serendipitous interaction with a major vulnerability.
Given that the statute of limitations has (probably) run out, I thought I’d share it as a tale of how “huh, that’s weird” can be a powerful threat-hunting tool – but only if you’ve got the time to keep pulling at the thread.</p>



<p>Our story begins back in March 2008.
I was working at Engine Yard (EY), a now largely-forgotten Rails-focused hosting company, which pioneered several advances in Rails application deployment.
Probably EY’s greatest claim to lasting fame is that they helped launch a little code hosting platform you might have heard of, by providing them free infrastructure when they were little more than a glimmer in the Internet’s eye.</p>

<p>I am, of course, talking about everyone’s favourite Microsoft product: GitHub.</p>

<p>Since GitHub was in the right place, at the right time, with a compelling product offering, they quickly started to gain traction, and grow their userbase.
With growth comes challenges, amongst them the one we’re focusing on today: SSH login times.
Then, as now, GitHub provided SSH access to the git repos they hosted, by SSHing to <code>git@github.com</code> with publickey authentication.
They were using the standard way that everyone manages SSH keys: the <code>~/.ssh/authorized_keys</code> file, and that became a problem as the number of keys started to grow.</p>

<p>The way that SSH uses this file is that, when a user connects and asks for publickey authentication, SSH opens the <code>~/.ssh/authorized_keys</code> file and scans all of the keys listed in it, looking for a key which matches the key that the user presented.
This linear search is normally not a huge problem, because nobody in their right mind puts more than a few keys in their <code>~/.ssh/authorized_keys</code>, right?</p>

<figure>
<img src="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/images/2008_github_auth_keys_sideeye.jpg" alt="2008-era GitHub giving monkey puppet side-eye to the idea that nobody stores many keys in an authorized_keys file"/>
</figure>

<p>Of course, as a popular, rapidly-growing service, GitHub was gaining users at a fair clip, to the point that the one big file that stored all the SSH keys was starting to visibly impact SSH login times.
This problem was also not going to get any better by itself.
Something Had To Be Done.</p>

<p>EY management was keen on making sure GitHub ran well, and so despite it not <em>really</em> being a hosting problem, they were willing to help fix this problem.
For some reason, the late, great, Ezra Zygmuntowitz pointed GitHub in my direction, and let me take the time to <em>really</em> get into the problem with the GitHub team.
After examining a variety of different possible solutions, we came to the conclusion that the least-worst option was to patch OpenSSH to lookup keys in a MySQL database, indexed on the key fingerprint.</p>

<p>We didn’t take this decision on a whim – it wasn’t a case of “yeah, sure, let’s just hack around with OpenSSH, what could possibly go wrong?”.
We knew it was potentially catastrophic if things went sideways, so you can imagine how much worse the other options available were.
Ensuring that this wouldn’t compromise security was a lot of the effort that went into the change.
In the end, though, we rolled it out in early April, and lo! SSH logins were fast, and we were pretty sure we wouldn’t have to worry about this problem for a long time to come.</p>

<p>Normally, you’d think “patching OpenSSH to make mass SSH logins super fast” would be a good story on its own.
But no, this is just the opening scene.</p>



<p>Fast forward a little under a month, to the first few days of May 2008.
I get a message from one of the GitHub team, saying that <em>somehow</em> users were able to access other users’ repos over SSH.
Naturally, as we’d recently rolled out the OpenSSH patch, which touched <em>this very thing</em>, the code I’d written was suspect number one, so I was called in to help.</p>

<figure>
<img src="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/images/the_usual_suspects.png" alt="The lineup scene from the movie The Usual Suspects"/>
<figcaption>
  They&#39;re called The Usual Suspects for a reason, but sometimes, it really <b>is</b> Keyser Söze
</figcaption>
</figure>

<p>Eventually, after more than a little debugging, we discovered that, somehow, there were two users with keys that had the same key fingerprint.
This <em>absolutely</em> shouldn’t happen – it’s a bit like winning the lottery twice in a row<sup id="fnref:1"><a href="#fn:1">1</a></sup> – unless the users had somehow shared their keys with each other, of course.
Still, it was worth investigating, just in case it was a web application bug, so the GitHub team reached out to the users impacted, to try and figure out what was going on.</p>

<p>The users professed no knowledge of each other, neither admitted to publicising their key, and couldn’t offer any explanation as to how the other person could possibly have gotten their key.</p>

<p>Then things went from “weird” to “what the…?”.
Because <em>another</em> pair of users showed up, sharing a key fingerprint – but it was a <em>different</em> shared key fingerprint.
The odds now have gone from “winning the lottery multiple times in a row” to as close to “this literally cannot happen” as makes no difference.</p>

<figure>
<img src="https://www.hezmatt.org/~mpalmer/blog/2024/04/09/images/were_through_the_looking_glass.jpg" alt="Milhouse from The Simpsons says that We&#39;re Through The Looking Glass Here, People"/>
</figure>

<p>Once we were really, <em>really</em> confident that the OpenSSH patch wasn’t the cause of the problem, my involvement in the problem basically ended.
I wasn’t a GitHub employee, and EY had plenty of other customers who needed my help, so I wasn’t able to stay deeply involved in the on-going investigation of The Mystery of the Duplicate Keys.</p>

<p>However, the GitHub team did keep talking to the users involved, and managed to determine the only apparent common factor was that all the users claimed to be using Debian or Ubuntu systems, which was where their SSH keys would have been generated.</p>

<p>That was as far as the investigation had really goten, when along came May 13, 2008.</p>



<p>With the publication of <a href="https://security-tracker.debian.org/tracker/DSA-1571-1">DSA-1571-1</a>, everything suddenly became clear.
Through a well-meaning but ultimately disasterous cleanup of OpenSSL’s randomness generation code, the Debian maintainer had inadvertently reduced the number of possible keys that could be generated by a given user from “bazillions” to a little over 32,000.
With so many people signing up to GitHub – some of them no doubt following best practice and freshly generating a separate key – it’s unsurprising that some collisions occurred.</p>

<p>You can imagine the sense of “oooooooh, so <em>that’s</em> what’s going on!” that rippled out once the issue was understood.
I was mostly glad that we had conclusive evidence that my OpenSSH patch wasn’t at fault, little knowing how much more contact I was to have with Debian weak keys in the future, running <a href="https://pwnedkeys.com">a huge store of known-compromised keys</a> and using them to find <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1620772">misbehaving Certificate Authorities</a>, amongst other things.</p>



<p>While I’ve not found a description of exactly when and how Luciano Bello discovered the vulnerability that became CVE-2008-0166, I presume he first came across it some time before it was disclosed – likely before GitHub tripped over it.
The stable Debian release that included the vulnerable code had been released a year earlier, so there was plenty of time for Luciano to have discovered key collisions and go “hmm, I wonder what’s going on here?”, then keep digging until the solution presented itself.</p>

<p>The thought “hmm, that’s odd”, followed by intense investigation, leading to the discovery of a major flaw is also what ultimately brought down the recent XZ backdoor.
The critical part of that sequence is the ability to <em>do</em> that intense investigation, though.</p>

<p>When I reflect on my brush with the Debian weak keys vulnerability, what sticks out to me is the fact that I <em>didn’t</em> do the deep investigation.
I wonder if Luciano hadn’t found it, how long it might have been before it was found.
The GitHub team would have continued investigating, presumably, and perhaps they (or I) would have eventually dug deep enough to find it.
But we were all super busy – myself, working support tickets at EY, and GitHub feverishly building features and fighting the fires in their rapidly-growing service.</p>

<p>As it was, Luciano <em>was</em> able to take the time to dig in and find out what was happening, but just like the XZ backdoor, I feel like we, as an industry, got a bit lucky that someone with the skills, time, and energy was on hand at the right time to make a huge difference.</p>

<p>It’s a luxury to be able to take the time to really dig into a problem, and it’s a luxury that most of us rarely have.
Perhaps an understated takeaway is that somehow we all need to wrestle back some time to follow our hunches and really dig into the things that make us go “hmm…”.</p>



<p>If you’d like to help me be able to do intense investigations of mysterious software phenomena, you can <a href="https://ko-fi.com/tobermorytech">shout me a refreshing beverage on ko-fi</a>.</p>



<hr/>
			
			

			</div>
		</div></div>
  </body>
</html>
