<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.sebin-nyshkim.net/posts/nvidia-is-full-of-shit/">Original</a>
    <h1>Nvidia won, we all lost</h1>
    
    <div id="readability-page-1" class="page"><section><p>Since the disastrous launch of the RTX 50 series, NVIDIA has been unable to escape negative headlines: scalper bots are snatching GPUs away from consumers before official sales even begin, power connectors continue to melt, with no fix in sight, marketing is becoming increasingly deceptive, GPUs are missing processing units when they leave the factory, and the drivers, for which NVIDIA has always been praised, are currently falling apart. And to top it all off, NVIDIA is becoming increasingly insistent that media push a certain narrative when reporting on their hardware.</p><h2 id="what%E2%80%99s-an-msrp-anyway%3F" tabindex="-1">What‚Äôs an MSRP anyway?</h2><p>Just like with every other GPU launch in recent memory, this one has also been ripe with scalper bots snatching up stock before any real person could get any for themselves. Retailers have reported that they‚Äôve received <a href="https://forums.overclockers.co.uk/threads/patience-if-planning-to-buy-a-50-series.18998084/#post-37602115">very little stock to begin with</a>. This in turn sparked rumors about NVIDIA purposefully keeping stock low to make it look like the cards are in high demand to drive prices. And sure enough, on secondary markets, the cards go <em>way above</em> MSRP and some retailers have started to <a href="https://www.youtube.com/watch?v=8s4hxa2TjWY&amp;t=464s">bundle the cards with other inventory</a> (PSUs, monitors, keyboards and mice, etc.) to inflate the price even further and get rid of stuff in their warehouse people wouldn‚Äôt buy otherwise‚Äîand you don‚Äôt even get a working computer out of spending over twice as much as a GPU alone would cost you.</p><figure><picture><source sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=640 640w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=800 800w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=1440 1440w" type="image/webp"/><img alt="Newegg selling the ASUS ROG Astral GeForce RTX 5090 for $3,359" decoding="async" height="1270" loading="lazy" src="https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=640" width="1440" sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=640 640w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=800 800w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=1440 1440w"/></picture><figcaption>Newegg selling the ASUS ROG Astral GeForce RTX 5090 for $3,359 (MSRP: $1,999)</figcaption></figure><figure><picture><source sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=640 640w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=800 800w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=1440 1440w" type="image/webp"/><img alt="eBay Germany offering the same ASUS ROG Astral RTX 5090 for ‚Ç¨3,349,95" decoding="async" height="997" loading="lazy" src="https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=640" width="1440" sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=640 640w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=800 800w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=1440 1440w"/></picture><figcaption>eBay Germany offering the same ASUS ROG Astral RTX 5090 for ‚Ç¨3,349,95 (MSRP: ‚Ç¨2,229)</figcaption></figure><p>I had a look at GPU prices for previous generation models for both AMD and NVIDIA as recently as May 2025 and I wasn‚Äôt surprised to find even RTX 40 series are still very much overpriced, with the GeForce RTX 4070 (lower mid-tier) starting at $800 (MSRP: $599), whereas the same money can get you a Radeon RX 7900 XT (the second best GPU in AMD‚Äôs last generation lineup). The discrepancy in bang for buck couldn‚Äôt be more jarring. And that‚Äôs before considering that NVIDIA gave out defective chips to board partners that were <a href="https://www.techpowerup.com/332884/nvidia-geforce-rtx-50-cards-spotted-with-missing-rops-nvidia-confirms-the-issue-multiple-vendors-affected">missing ROPs</a> (Raster Operations Pipelines) from the factory, thus reducing their performance. Or, how NVIDIA put it in a statement to <a href="https://www.theverge.com/news/617901/nvidia-confirms-rare-rtx-5090-and-5070-ti-manufacturing-issue">The Verge</a>:</p><blockquote><p>We have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D and 5070 Ti GPUs which have one fewer ROP than specified. The average graphical performance impact is 4%, with no impact on AI and Compute workloads. Affected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.</p></blockquote><p>Those 4% can make an RTX 5070 Ti perform at the levels of an RTX 4070 Ti Super, completely eradicating the reason you‚Äôd get an RTX 5070 Ti in the first place. Not to mention that the generational performance uplift over the RTX 40 series was already received quite poorly in general. NVIDIA also had to later amend their statement to The Verge and admit the RTX 5080 was also missing ROPs.</p><p>It‚Äôs adding insult to injury with the cards‚Äô general <em>unobtainium</em> and it becomes even more ridiculous when you compare NVIDIA to another trillion dollar company that is also in the business of selling hardware to consumers: Apple.</p><p>How is it that one can supply customers with enough stock on launch consistently for decades, and the other can‚Äôt? The only reason I can think of is, that NVIDIA just doesn‚Äôt care. They‚Äôre making the big bucks with data center GPUs now, selling the shovels that drive the ‚ÄúAI‚Äù bullshit gold rush, to the point that selling to consumers is <a href="https://www.visualcapitalist.com/nvidia-revenue-by-product-line/">increasingly becoming</a> a rounding error on their balance sheets.</p><h2 id="these-cards-are-%F0%9F%94%A5%F0%9F%94%A5%F0%9F%94%A5-(and-not-the-good-kind)" tabindex="-1">These cards are üî•üî•üî• (and not the good kind)</h2><p>The RTX 50 series are the second generation of NVIDIA cards to use the 12VHPWR connector. The RTX 40 series became infamous as the GPU series with melting power connectors. So did they fix that?</p><p><a href="https://www.youtube.com/watch?v=Ndmoi1s0ZaY">No</a>. The cables can still melt, both on the GPU and PSU. It‚Äôs a design flaw in the board of the GPU itself which cannot be fixed unless the circuitry of the cards is replaced with a new design.</p><p>With the RTX 30 cards, each power input (i.e. the cables from the power supply) had its own shunt resistor. If one pin in a power input had not been connected properly, another pin would have had to take over in its stead. If both pins were not carrying any current, there would have been no phase on the shunt resistor and the card would not have started up. You‚Äôd get a black screen, but the hardware would still be fine.</p><p id="kb5YzMoVQyw"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/kb5YzMoVQyw" title="Embedded YouTube video" width="100%"></iframe></p><p>NVIDIA, in its infinite wisdom, changed this design starting with the RTX 40 series.</p><p>Instead of individual shunt resistors for each power input, the shunt resistors are now connected in parallel to all pins of the power input from a single 12VHPWR connector. Additionally, the lines are recombined behind the resistors. This mind-boggling design flaw makes it impossible for the card to detect if pins are unevenly loaded, since as much as the card is concerned, everything comes in through the same single line.</p><p>Connecting the shunt resistors in parallel also makes them pretty much useless since if one fails, the other will still have a phase and the card will happily keep drawing power and not be any the wiser. If the card is supplied with 100W on each pin and 5 of the 6 pins don‚Äôt supply a current, then a single pin has to supply the entire 600W the card demands. No wire is designed for this amount of power draw. As a result, excessive friction occurs from too many electrons traveling through the cable all at once and it melts (see: <a href="https://en.wikipedia.org/wiki/Joule_heating">Joule heating</a>).</p><p>NVIDIA realized that the design around the shunt resistors in the RTX 40 series was kinda stupid, so they revised it: by eliminating the redundant shunt resistor, but changing nothing else about the flawed design.</p><p id="oB75fEt7tH0"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/oB75fEt7tH0" title="Embedded YouTube video" width="100%"></iframe></p><p>There‚Äôs something to be said about the fact NVIDIA introduced the 12VHPWR connector to the ATX standard to allow for only a single connector to supply their cards with up to 600W of power but making it way less safe to operate at these loads. Worse yet, NVIDIA says the four ‚Äúsensing pins‚Äù on top of the load bearing 12 pins are <em>supposed</em> to prevent the GPU from pulling too much power. The fact of the matter is, however, that the ‚Äúsensing pins‚Äù only tell the GPU how much it‚Äôs allowed to pull when the system <em>turns on</em>, but they <strong>do not</strong> continuously monitor the power draw‚Äîthat would be for the shunt resistors on the GPU board, which we established, NVIDIA kept taking out.</p><p>If I had to guess, NVIDIA must‚Äôve been <em>very confident</em> that the ‚Äúsensing pins‚Äù are a suitable substitution for those shunt resistors in theory, but practice showed that they were not at all accounting for user error. That was their main excuse after after it blew up in their face and they investigated. And indeed, if the 12VHPWR connector isn‚Äôt properly inserted, pins could not make proper contact, causing the remaining wires to carry more load. This is something that the ‚Äúsensing pins‚Äù <strong>cannot</strong> detect, despite their name and NVIDIA selling it as some sort of safety measure.</p><figure><picture><source sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=640 640w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=800 800w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=1920 1920w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=2560 2560w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=3840 3840w" type="image/webp"/><img alt="Size comparison between the RTX 5090 FE (right) and its predecessor, the RTX 4090 FE (left)" decoding="async" height="2160" loading="lazy" src="https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=640" width="3840" sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=640 640w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=800 800w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=1920 1920w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=2560 2560w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=3840 3840w"/></picture><figcaption>Size comparison between the RTX 5090 FE (right) and its predecessor, the RTX 4090 FE (left) ¬© <a href="https://www.youtube.com/watch?v=5nj1qLazPlk">ZMASLO</a> (<a href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>) via <a href="https://commons.wikimedia.org/wiki/File:RTX_5090_-_du%C5%BCa_wydajno%C5%9B%C4%87_du%C5%BCym_kosztem_(2160p_30fps_VP9_LQ-96kbit_AAC)-00.05.04.568.png">Wikimedia</a></figcaption></figure><p>NVIDIA also clearly did not factor in the computer cases on the market that people would pair these cards with. The RTX 4090 was <em><strong>massive,</strong></em> a real heccin chonker. It was so huge in fact, that it kicked off the trend of needing <a href="https://www.antec.com/product/accessory#gpu_bracket">support brackets</a> to keep the GPU from sagging and straining the PCIe slot. It also had its power connector sticking out to the side of the card and computer cases were not providing enough clearance to not bend the plug. As was <a href="https://cablemod.com/12vhpwr/">clarified</a> after the first reports of molten cables came up, bending a 12VHPWR cable without at least 35mm (1.38in) clearance could loosen the connection of the pins and create the problem of the melting connectors‚Äîsomething that wasn‚Äôt a problem with the battle tested 6- and 8-pin PCIe connectors we‚Äôve been using up to this point.</p><p>Board partners like ASUS try to work around that design flaw by introducing intermediate shunt resistors for each individual load bearing pin before the ones according to NVIDIA‚Äôs designs, but these don‚Äôt solve the underlying issue, that the card won‚Äôt shut itself down if any of the lines aren‚Äôt drawing enough or any power. What you get at most is an indicator LED lighting up and some software telling you ‚ÄúHey, uh, something seems off, maybe take a look?‚Äù</p><p>The fact NVIDIA insists on keeping the 12VHPWR connector around and not do jack shit about the design flaws in their cards to prevent it from destroying itself from the slightest misuse should deter you from considering any card from them that uses it.</p><h2 id="a-carefully-constructed-moat" tabindex="-1">A carefully constructed moat</h2><p>Over the years NVIDIA has released a number of proprietary technologies to market that only work on their hardware‚ÄîDLSS, CUDA, NVENC and G-Sync to just name a few. The tight coupling with with NVIDIA‚Äôs hardware guarantees compatibility and performance.</p><p>However, this comes at a considerable price these days, as mentioned earlier. If you‚Äôre thinking about an upgrade you‚Äôre either looking at a down-payment on a house or an uprooting of your entire hardware and software stack if you switch vendors.</p><p>If you‚Äôre a creator, CUDA and NVENC are pretty much indispensable, or editing and exporting videos in Adobe Premiere or DaVinci Resolve will take you a lot longer. Same for live streaming, as using NVENC in OBS offloads video rendering to the GPU for smooth frame rates while streaming high-quality video.</p><p>Speaking of games: G-Sync in gaming monitors also requires a lock-in with NVIDIA hardware, both on the GPU side and the monitor itself. G-Sync monitors have a special chip inside that NVIDIA GPUs can talk to in order to align frame timings. This chip is expensive and monitor manufacturers have to get certified by NVIDIA. Therefore monitor manufacturers charge a premium for such monitors.</p><p>The competing open standard is FreeSync, spearheaded by AMD. Since 2019, NVIDIA also supports FreeSync, but under their ‚ÄúG-Sync Compatible‚Äù branding. Personally, I wouldn‚Äôt bother with G-Sync when a competing, open standard exists and differences are negligible.</p><h3 id="nvidia-giveth%2C-nvidia-taketh-away" tabindex="-1">NVIDIA giveth, NVIDIA taketh away</h3><p id="_dUjUNrbHis"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/_dUjUNrbHis" title="Embedded YouTube video" width="100%"></iframe></p><p>The PC, as gaming platform, has long been held in high regards for its backwards compatibility. With the RTX 50 series, NVIDIA broke that going forward.</p><p>PhysX, which NVIDIA introduced into their GPU lineup with the acquisition of Ageia in 2008, is a technology that allows a game to calculate game world physics on an NVIDIA GPU. After the launch of the RTX 50 series cards it was revealed that they lack support for the 32-bit variant of the tech. This causes games like Mirror‚Äôs Edge (2009) and Borderlands 2 (2012) that still run on today‚Äôs computers to take ungodly dips into single digit frame rates, because the physics calculations are forcibly performed on the CPU instead of the GPU.</p><p>Even though the first 64-bit consumer CPUs hit the market as early as 2003 (AMD Opteron, Athlon 64), 32-bit games were still very common around these times, as Microsoft would not release 64-bit versions of Windows to consumers until Vista in 2006. NVIDIA later released the source code for the GPU simulation kernel on <a href="https://github.com/NVIDIA-Omniverse/PhysX/discussions/384">GitHub</a>. The pessimist in me thinks they did this because they can‚Äôt be bothered to maintain this themselves and offload that maintenance burden to the public.</p><h3 id="dlss-is%2C-and-always-was%2C-snake-oil" tabindex="-1">DLSS is, and always was, snake oil</h3><p>Back in 2018 when the RTX 20 series launched as the first GPUs with hardware accelerated ray tracing, it sure was impressive and novel to have this tech in consumer graphics cards. However, NVIDIA also introduced upscaling tech alongside it to counterbalance the insane computational expense it introduced. From the beginning, the two were closely interlinked. If you wanted ray tracing in Cyberpunk 2077 (the only game at the time that really made use of the tech), you also had to enable upscaling if you didn‚Äôt want your gameplay experience to become a (ridiculously pretty) PowerPoint slide show.</p><p>That upscaling tech is the now ubiquitous DLSS, or <em>Deep Learning Super Sampling</em>. It renders a game at a lower resolution internally and then upscales it to the target resolution with specialized accelerator chips on the GPU die. The only issue back then was that because the tech was so new, barely any game made use of it.</p><p>What always rubbed me the wrong way about how DLSS was marketed is that it wasn‚Äôt only for the less powerful GPUs in NVIDIA‚Äôs line-up. No, it was marketed for the top of the line $1,000+ RTX 20 series flagship models to achieve the graphical fidelity with all the bells and whistles. That, to me, was a warning sign that maybe, just maybe, ray tracing was introduced prematurely and half-baked. Back then I theorized, that by tightly coupling this sort of upscaling tech to high-end cards and ray traced graphics, it sets a bad precedent. The kind of graphics NVIDIA was selling us on were beyond the cards‚Äô actual capabilities.</p><p>Needing to upscale to keep frame rates smooth already seemed ‚Äúfake‚Äù to me. If that amount of money for a single PC component still can‚Äôt produce those graphics without using software trickery to achieve acceptable frame rates, then what am I spending that money for to begin with exactly?</p><p>Fast-forward to today and nothing has really changed, besides NVIDIA now charging double the amount for the flagship RTX 5090. And guess what? It still doesn‚Äôt do Cyberpunk 2077‚Äî<em>the</em> flagship ray tracing game‚Äîwith full ray tracing at a playable framerate in native 4K, only with DLSS enabled.</p><p>From the RTX 4090 website:</p><p id="QGI8EIgf8Y8"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/QGI8EIgf8Y8" title="Embedded YouTube video" width="100%"></iframe></p><p>From the RTX 5090 website:</p><p id="_YXbkGuw3O8"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/_YXbkGuw3O8" title="Embedded YouTube video" width="100%"></iframe></p><table><thead><tr><th>GPU</th><th>MSRP</th><th>CP2077 4K native RT Overdrive FPS</th></tr></thead><tbody><tr><td>RTX 4090</td><td>$1,599</td><td>~20 FPS</td></tr><tr><td>RTX 5090</td><td>$1,999</td><td>~27 FPS</td></tr></tbody></table><p>So 7 years into ray traced real-time computer graphics and we‚Äôre still nowhere near 4K gaming at 60 FPS, even at $1,999. Sure, you could argue to simply turn RT off and performance improves. But then, that‚Äôs not why you spent all that money for, right? Pure generational uplift in performance of the hardware itself is miniscule. They‚Äôre selling us a solution to a problem they themselves introduced and co-opted every developer to include the tech into their games. Now they‚Äôre doing an even more computationally expensive version of ray tracing: path tracing. So all the generational improvements we could‚Äôve had are nullified again.</p><p>And even if you didn‚Äôt spend a lot of money on a GPU, what you get isn‚Äôt going to be powerful enough to make those ray traced graphics pop and still run well. So most peoples‚Äô experience with ray tracing is: turn it on to see how it looks, realize it eats almost all your FPS and never turn it on ever again, thinking ray tracing is a waste. So whatever benefits in realistic lighting was to be achieved is also nullified, because developers will still need to do lighting the old-fashioned way for the people who don‚Äôt or can‚Äôt use ray tracing.</p><p>Making the use of upscaling tech a requirement, at every GPU price point, for every AAA game, to achieve acceptable levels of performance gives the impression that the games we‚Äôre sold are targeting hardware that either doesn‚Äôt even exist yet or nobody can afford, and we need constant band-aids to make it work. Pretty much all upscalers force TAA for anti-aliasing and it makes the entire image on the screen look blurry as fuck the lower the resolution is.</p><p>Take for example this <em>Red Dead Redemption 2</em> footage showing TAA ‚Äúin action‚Äù, your $1,000+ at work:</p><p id="GJ0eFYJYkkw"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/GJ0eFYJYkkw" title="Embedded YouTube video" width="100%"></iframe></p><p>Frame generation exacerbates this problem further by adding to the ghosting of TAA because it guesstimates where pixels will <em>probably</em> go in an ‚ÄúAI‚Äù generated frame in between actually rendered frames. And when it‚Äôs off it really <strong>looks off.</strong> Both in tandem look like someone smeared your screen with vaseline. And this is what they expect us to pay a premium for? For the hardware <strong>and</strong> the games?!</p><p>Combine that with GPU prices being absolutely ridiculous in recent years and it all takes on the form of a scam.</p><p>As useful or impressive a technology as DLSS might be, game studios relying as heavily on it as they do, is turning out to be detrimental to the visual quality of their games and incentivizes aiming for a level of graphical fidelity and complexity with diminishing returns. Games from 2025 don‚Äôt look that dramatically different or better than games 10 years prior, yet they run way worse despite more modern and powerful hardware. Games these days demand such a high amount of compute that the use of upscaling tech like DLSS is becoming <em><strong>mandatory.</strong></em> The most egregious example of this being <em>Monster Hunter Wilds</em>, which states in its system requirements, that it <strong>needs</strong> frame generation to run at acceptable levels.</p><figure><picture><source sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=640 640w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=800 800w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=1920 1920w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=2560 2560w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=3840 3840w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=4401 4401w" type="image/webp"/><img alt="Monster Hunter Wilds recommended system requirements" decoding="async" height="4197" loading="lazy" src="https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=640" width="4401" sizes="(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px" srcset="https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=640 640w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=800 800w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=1920 1920w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=2560 2560w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=3840 3840w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=4401 4401w"/></picture><figcaption>Recommended system requirements for Monster Hunter Wilds noting 1080p on medium settings reaches 60 fps only with frame generation enabled</figcaption></figure><p>Meanwhile, Jensen Huang came up on stage during the keynote for the RTX 50 series cards and <a href="https://www.youtube.com/live/k82RwXqZHY8?t=1125">proudly proclaimed</a>:</p><blockquote><p>RTX 5070, 4090 performance at $549, impossible without artificial intelligence.</p></blockquote><p>What he meant by that, as it turns out, is the RTX 5070 only getting there with every trick DLSS has to offer, including new DLSS 4 Multi-Frame Generation only available on RTX 50 cards at the lowest quality setting and all DLSS trickery turned up to the max.</p><p>You cannot tell me this is anywhere near acceptable levels of image quality for thousands of bucks (video time-stamped):</p><p id="3nfEkuqNX4k"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/3nfEkuqNX4k?start=1176" title="Embedded YouTube video" width="100%"></iframe></p><p>Not only does that entail rendering games at a lower internal resolution, you also have to tell your GPU to pull 3 additional made up frames out of its ass so NVIDIA can waltz around claiming ‚ÄúRuns [insanely demanding game here] as 5,000 FPS!!!‚Äù for the <em>higher number = better</em> masturbator crowd. All the while the image gets smeared to shit, because NVIDIA just reinvented the motion smoothing option from your TV‚Äôs settings menu, but badly and also it‚Äôs ‚ÄúAI‚Äù now. Else what would all those Tensor-cores be doing than waste space on the GPU die that could‚Äôve gone to actual render units? NVIDIA likes you to believe DLSS can create FPS out of thin air and they‚Äôre trying to prove it with <a href="https://www.pcgamer.com/hardware/graphics-cards/92-percent-of-nvidia-users-turn-on-dlss-if-theyve-been-lucky-enough-to-bag-an-rtx-50-series-card-at-launch-and-have-the-nvidia-app-installed/">dubious statistics</a>‚Äîonly disclosing in barely readable fine print, that it‚Äôs a deliberately chosen very small sample size, so the numbers look more impressive.</p><p>The resolution is fake, the frames are fake, too, and so is the marketed performance. Never mind that frame generation introduces input lag that NVIDIA needs to counter-balance with their ‚ÄúReflex‚Äù technology, lest what you see on your screen isn‚Äôt actually where you think it is because, again, the frames faked in by Frame Generation didn‚Äôt originate from the game logic. They create problems for themselves, that they then create ‚Äúsolutions‚Äù for in an endless cycle of trying to keep up the smoke screen that these cards do more than they‚Äôre actually equipped to do, so a 20% premium for a 10% uplift in performance has the faintest resemblance of justification.</p><p>I was afraid DLSS would get used to fake improvements where there are barely any back then and I feel nothing if not vindicated for how NVIDIA is playing it up, while jacking up prices further and further with each generation. None of that is raw performance of their cards. This is downright deceitful bullshit.</p><h2 id="the-intimidations-will-continue-until-morale-improves" tabindex="-1">The intimidations will continue until morale improves</h2><p>NVIDIA lying on their own presentations about the real performance of their cards is one thing. It‚Äôs another thing entirely, when they start bribing and threatening reviewers, to steer the editorial direction in NVIDIA‚Äôs favor.</p><p>In December 2020, hardware review channel <em>Hardware Unboxed</em> <a href="https://www.youtube.com/watch?v=wdAMcQgR92k">received an email</a> from NVIDIA Senior PR Manager Bryan Del Rizzo, after they reviewed NVIDIA cards on pure rasterization performance without DLSS or ray tracing, saying that performance did not live up to their expectations:</p><blockquote><p>Hi Steve,</p><p>We have reached a critical juncture in the adoption of ray tracing, and it has gained industry wide support from top titles, developers, game engines, APIs, consoles and GPUs.</p><p>As you know, NVIDIA is all in for ray tracing. RT is important and core to the future of gaming. But it‚Äôs also only one part of our focused R&amp;D efforts on revolutionizing video games and creating a better experience for gamers. This philosophy is also reflected in developing technologies such as DLSS, Reflex and Broadcast that offer immense value to consumers who are purchasing a GPU. They don‚Äôt get free GPUs‚Äîthey work hard for their money and they keep their GPUs for multiple years.</p><p>Despite all of this progress, your GPU reviews and recommendations continue to focus singularly on rasterization performance and you have largely discounted all of the other technologies we offer to gamers. It is very clear from your community commentary that you do not see things the same way that we, gamers, and the rest of the industry do.</p><p>Our Founders Edition boards and other NVIDIA products are being allocated to media outlets that recognize the changing landscape of gaming and the features that are important to gamers and anyone buying a GPU today‚Äîbe it for gaming, content creation or studio and streaming.</p><p>Hardware Unboxed should continue to work with out add-in card partners to secure GPUs to review. Of course, you will still have access to obtain pre-release drivers and press materials. That won‚Äôt change.</p><p>We are open to revisiting this in the future should your editorial direction change.</p></blockquote><p>Hardware Unboxed was thus banned from receiving review samples of NVIDIA‚Äôs Founder Edition cards. It didn‚Äôt take long for NVIDIA to back-paddle after the heavily publicized outcry blew up in their face.</p><p>Which makes it all the more surprising, that a couple years later, they‚Äôre trying to pull this again. With <em>Gamers Nexus</em> of all outlets.</p><p id="AiekGcwaIho"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="100%" src="https://www.youtube-nocookie.com/embed/AiekGcwaIho" title="Embedded YouTube video" width="100%"></iframe></p><p>As Steve Burke explains in the video, NVIDIA approached him from the angle, that in order to still be given access to NVIDIA engineers for interviews and specials for their channel, Gamers Nexus needs to include Multi-Frame Generation metrics into their benchmark charts during reviews. Steve rightfully claims that this tactic of intimidating media by taking away access until they review NVIDIA cards in a way that agrees with the narrative NVIDIA wants to uphold, tarnishes the legitimacy of <em><strong>every</strong></em> review of every NVIDIA card ever made, past and present. It creates an environment of distrust that is not at all conductive when you‚Äôre trying to be a tech reviewer right now.</p><p>This also coincided with the launch of the RTX 5060, a supposedly more budget friendly offering. Interestingly, NVIDIA did not provide reviewers with the necessary drivers to test the GPU prior to launch. Instead, the card and the drivers launched at the same time all of these reviewers were off at Computex, a computer expo in Taipei, Taiwan. The only outlets that did get to talk about the card prior to release were cherry-picked by NVIDIA, and even then it was merely <em>previews</em> of details NVIDIA allowed them to talk about, <strong>not</strong> independent <em>reviews.</em> Because if they would‚Äôve been properly reviewed, they‚Äôd all come to the same conclusions: that the 8 GB of VRAM would make this $299 ‚Äúbudget card‚Äù age very poorly because that is not enough VRAM to last long in today‚Äôs gaming landscape.</p><p>But it probably doesn‚Äôt matter anyways, because NVIDIA is also busy tarnishing the reputation of their drivers, <a href="https://www.theverge.com/news/653115/nvidia-gpu-drivers-black-screen-crashes-issues">releasing hotfix after hotfix</a> in an attempt to stop their cards, old and new, from crashing seemingly randomly, when encountering certain combinations of games, DLSS and Multi-Frame Generation settings. Users of older generation NVIDIA cards can simply roll back to a previous version of the driver to alleviate these issues, but RTX 50 series owners don‚Äôt get this luxury, because older drivers won‚Äôt make their shiny new cards go.</p><h2 id="nvidia-won%2C-we-all-lost" tabindex="-1">NVIDIA won, we all lost</h2><p>With over 90% of the PC market running on NVIDIA tech, they‚Äôre the clear winner of the GPU race. The losers are every single one of us.</p><p>Ever since NVIDIA realized there is tons of more money to be made on everything that is <em>not</em> part of putting moving pixels on a screen, they‚Äôve taken that opportunity head on. When the gold rush for crypto-mining started, they were among the first to sell heavily price-inflated, GPU-shaped shovels to anybody with more money than brains. Same now with the ‚ÄúAI‚Äù gold rush. PC gamers were hung out to dry.</p><p>NVIDIA knows we‚Äôre stuck with them and it‚Äôs infuriating. They keep pulling their shenanigans and they will keep doing it until someone cuts them down a couple notches. But the only ones who could step up to the task won‚Äôt do it.</p><p>AMD didn‚Äôt even attempt at facing NVIDIA at the high-end segment this generation, instead trying to compete on merely the value propositions for the mid-range. Intel is seemingly still on the fence if they really wanna sell dedicated GPUs while shuffling their C-suite and generally being in disarray. Both of them could be compelling options when you‚Äôre on a budget, if it just wasn‚Äôt for the fact that NVIDIA has a longstanding habit of producing proprietary tech that only runs well on their hardware. Now they‚Äôve poisoned the well with convincing everybody that ray tracing is something every game needs now and games that incorporate it do so on an NVIDIA tech-stack which runs like shit on anything that is not NVIDIA. That is not a level playing field.</p><p>When ‚ÄúThe way it‚Äôs meant to be played‚Äù slowly turns into ‚ÄúThe only way it doesn‚Äôt run like ass‚Äù it creates a moat around NVIDIA that‚Äôs obviously hard to compete with. And gamers aren‚Äôt concerned about this because at the end of the day, all they care about is that the game runs well and looks pretty.</p><p>But I want you to consider this: Games imbued with such tech creates a vendor lock-in effect. It gives NVIDIA considerable leverage in terms of how games are made, which GPUs you consider buying to run these games and how well they will eventually, actually run on your system. If all games that include NVIDIA‚Äôs tech are made in a way that make it so you <em>have</em> to reach for the more expensive models, you can be sure that‚Äôs a soft power move NVIDIA is gonna pull.</p><p>And as we established, it looks like they‚Äôre already doing that. Tests show that the lower-end NVIDIA graphics cards cannot (and probably were never intended to) perform well enough, even with DLSS, because in order to get anything out of DLSS you need more VRAM, which these lower-end cards don‚Äôt have enough of. So they‚Äôre already upselling you on more expensive models by cutting corners in ways that make it a ‚Äúno-brainer‚Äù to spend more money on more expensive cards, when you otherwise wouldn‚Äôt have.</p><p>And they‚Äôre using their market dominance to control the narrative in the media, to make sure you keep giving them money and keep you un- or at the very least misinformed. When you don‚Äôt have to compete, but don‚Äôt have any improvements to sell either (or have no incentive for actual, real R&amp;D) you do what every monopolist does and wring out your consumer base until you‚Äôve bled them dry.</p><p>A few years back I would‚Äôve argued that that‚Äôs their prerogative if they provide the better technical solutions to problems in graphics development. Today, I believe that they are marauding monopolists, who are too high on their own supply and they‚Äôre ruining it for everybody. If NVIDIA had real generational improvements to sell, they wouldn‚Äôt do it by selling us <a href="https://www.youtube.com/watch?v=caU0RG0mNHg">outright lies</a>.</p><p>And I hate that they‚Äôre getting away with it, time and time again, for over seven years.</p></section></div>
  </body>
</html>
