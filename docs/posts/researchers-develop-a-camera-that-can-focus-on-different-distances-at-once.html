<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://engineering.cmu.edu/news-events/news/2025/12/19-perfect-shot.html">Original</a>
    <h1>Researchers develop a camera that can focus on different distances at once</h1>
    
    <div id="readability-page-1" class="page"><div>
	    
	    	    
	    	        	            <p>Imagine snapping a photo where every detail, near and far, is perfectly sharp—from the flower petal right in front of you to the distant trees on the horizon. For over a century, camera designers have dreamed of achieving that level of clarity. In a breakthrough that could transform photography, microscopy, and even smartphone cameras, researchers at Carnegie Mellon University have developed a new kind of lens that can bring an entire scene into sharp focus at once—no matter how far away or close different parts of the scene are.</p>
<p>The team, consisting of <a href="https://yingsiqin.github.io/" rel="noopener" target="_blank">Yingsi Qin</a>, an electrical and computer engineering Ph.D. student, <a href="https://www.ece.cmu.edu/directory/bios/sankaranarayanan-aswin.html" rel="noopener" target="_blank">Aswin Sankaranarayanan,</a> professor of electrical and computer engineering, and <a href="https://www.cs.cmu.edu/~motoole2/" rel="noopener" target="_blank">Matthew O’Toole</a>, associate professor of computer science and robotics, <a href="https://imaging.cs.cmu.edu/svaf/static/pdfs/Spatially_Varying_Autofocus.pdf" rel="noopener" target="_blank">presented their findings</a> at the <a href="https://iccv.thecvf.com/" rel="noopener" target="_blank">2025 International Conference on Computer Vision</a> and received a Best Paper Honorable Mention recognition.</p>
	                	        	           	           
                		           <div>
                	                	<figure>	
                	    <img alt="Illustration comparing a conventional photo with a defined focal plane to an All-In-Focus photo showcasing its spatially-varying autofocused focal surface." src="https://engineering.cmu.edu/_files/images/news/2025/1219-em-perfect-shot.png"/>
                	                  		<figcaption>
                		                    			                			    <p>Left: A conventional photo with a regular lens, where objects at a single focal plane appear sharp. Right: An all-in-focus photo captured through spatially-varying autofocusing. To achieve this, we combine (i) a programmable lens with spatially-varying control over focus, and (ii) a spatially-varying autofocus algorithm to drive the focus of this lens. Note that this is an optically-captured image of a real scene with no post-capture processing used.</p>
                			                		</figcaption>
                	                  	</figure>
                </div>
	                	        	            <p>Traditional camera lenses can only bring one flat layer of a scene into perfect focus at a time. Anything in front of or behind that layer turns soft and blurry. Narrowing the aperture can help, but it also dims the image and introduces new kinds of optical fuzziness caused by diffraction.</p>
<p>“We’re asking the question, ‘What if a lens didn’t have to focus on just one plane at all?’” says Qin. “What if it could bend its focus to match the shape of the world in front of it?”</p>
<p>The researchers developed a “computational lens”—a hybrid of optics and algorithm—that can adjust its focus differently for every part of a scene. The system builds on a design known as a Lohmann lens, which uses two curved, cubic lenses that shift against each other to tune focus. By combining this setup with a phase-only spatial light modulator—a device that controls how light bends at each pixel—the researchers were able to make different parts of the image focus at different depths simultaneously.</p>
<p>The system uses two autofocus methods. The first is Contrast-Detection Autofocus (CDAF), which divides the image into regions called superpixels. Each region independently finds the focus setting that maximizes its sharpness. The second is Phase-Detection Autofocus (PDAF), which uses a dual-pixel sensor to detect not just whether something is in focus, but which direction to adjust. This makes it faster and better suited for moving scenes—the team achieved 21 frames per second with their modified sensor.</p>
<p>“Together, they let the camera decide which parts of the image should be sharp—essentially giving each pixel its own tiny, adjustable lens,” explains O’Toole.</p>
	                	                        <blockquote>
                	<p>Our system represents a novel category of optical design. One that could fundamentally change how cameras see the world.</p>
                	<cite><strong>Aswin Sankaranarayanan</strong>, <em>Professor</em>, Electrical and Computer Engineering</cite>	
                </blockquote>
	                	        	            <p>Beyond its obvious appeal to photographers, the technology could have sweeping applications. Microscopes could capture every layer of a biological sample in focus at once. Autonomous vehicles might see their surroundings with unprecedented clarity. Even augmented and virtual reality systems could benefit, using similar optics to create more lifelike depth perception.</p>
<p>“Our system represents a novel category of optical design,” says Sankaranarayanan. “One that could fundamentally change how cameras see the world.”</p>
	                    </div></div>
  </body>
</html>
