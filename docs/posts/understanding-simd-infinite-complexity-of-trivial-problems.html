<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.modular.com/blog/understanding-simd-infinite-complexity-of-trivial-problems">Original</a>
    <h1>Understanding SIMD: Infinite complexity of trivial problems</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><figure><p>‍<em>This guest blog post is by </em><a href="https://github.com/ashvardanian"><em>Ash Vardanian</em></a><em>, founder of </em><a href="https://www.unum.cloud"><em>Unum</em></a><em>. He&#39;s focused on building high-performance computing and storage systems as part of the open-source </em><a href="https://github.com/unum-cloud"><em>Unum Cloud project</em></a><em>.</em></p><p>Modern CPUs have an incredible superpower: <strong>super-scalar operations, </strong>made available through single instruction, multiple data (SIMD) parallel processing<strong>.</strong> Instead of doing one operation at a time, a <em>single core</em> can do up to 4, 8, 16, or even 32 operations in parallel. In a way, a modern CPU is like a mini GPU, able to perform a lot of simultaneous calculations. Yet, because it’s so tricky to write parallel operations, almost all that potential remains untapped, resulting in code that only does one operation at a time</p><p>Recently, the four of us, Ash Vardanian (<a href="https://x.com/ashvardanian">@ashvardanian</a>), Evan Ovadia (<a href="https://x.com/verdagon">@verdagon</a>), Daniel Lemiere (<a href="https://x.com/lemire">@lemire</a>), and Chris Lattner (<a href="https://x.com/clattner_llvm">@clattner_llvm</a>) talked about what&#39;s holding developers back from effectively using super-scalar operations more, and how we can create better abstractions for writing optimal software for CPUs.</p><p>Here, we share what we learned from years of implementing SIMD kernels in the <a target="_blank" href="https://github.com/ashvardanian/simsimd">SimSIMD</a> library, which currently powers vector math in dozens of Database Management Systems (DBMS) products and AI companies–with software deployed on well over 100 million devices. While the first SIMD instruction sets were defined as early as 1966, the pain points of working with SIMD have been persistent:</p><ul role="list"><li><strong>High-performance computing is practically reverse-engineering of architecture details.</strong></li><li><strong>Auto-vectorization is unreliable:</strong> the compiler can&#39;t reliably figure this out for us.</li><li><strong>SIMD instructions are complex,</strong> and even Arm is starting to look more “CISCy” than x86!</li><li><strong>Performance is unpredictable,</strong> even for the same instruction sets shared across different CPUs.</li><li><strong>Debugging is complex:</strong> there&#39;s no easy way to view the registers.</li><li><strong>CPU-specific code is tricky,</strong> whether it be compile-time or dynamic dispatch.</li><li><strong>Computation precision is inconsistent</strong> for floating point operations across different CPUs and instruction sets.</li></ul><p>Let&#39;s explore these challenges and how Mojo helps address them by implementing one of the most trivial yet widespread operations in machine learning: <strong>cosine similarity.</strong> Cosine similarity computes how close two vectors are to one another by measuring the angle between the vectors.</p><h2>Introduction to cosine similarity</h2><p>You might be surprised by the widespread use of cosine similarity. Farming robots use it to <a target="_blank" href="https://www.youtube.com/watch?v=isl4G_-aTyk">zap 200,000 weeds per hour with</a> vision-guided lasers, new ML-based methods use it for improving the speed and accuracy of climate models, and nearly every  <a target="_blank" href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">Retrieval Augmented Generation</a> (RAG) pipeline measures the similarity of user prompts to existing knowledge bases with cosine similarity. </p><p>The following simplified diagram illustrates how RAG uses cosine similarity. A language processing ML pipeline converts text into vectors that numerically capture the meaning of words and sentences, with similar vectors sharing similar meanings. These semantically meaningful vectors are called embeddings. Cosine similarity is a way to compute the difference between embedding vectors, determining if they are semantically similar.</p><figure><p><img src="https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/673ebcb6a120863760b141e7_673eb94236e5912a20b1bc81_Cosine.png" loading="lazy" alt=""/></p><figcaption>Simplified example of measuring embedding distance. Adapted from <a href="https://mlops.community/vector-similarity-search-from-basics-to-production/">Samuel Partee</a></figcaption></figure><p>Cosine similarity can be expressed with this trivial Python code snippet, which takes the dot product of two vectors normalized by the length of the vectors:</p><div><div>
<p><span>Python</span></p><p>
def cosine_similarity(a: list[float], b: list[float]) -&gt; float:
    dot_product = sum(ai * bi for ai, bi in zip(a, b))
    norm_a = sum(ai * ai for ai in a) ** 0.5
    norm_b = sum(bi * bi for bi in b) ** 0.5
    if norm_a == 0 or norm_b == 0: return 1
    if dot_product == 0: return 0
    return dot_product / (norm_a * norm_b)
</p>
</div></div><p>Despite this algorithm&#39;s simplicity, we can squeeze <strong>several</strong> orders of magnitude of performance out of this code. The NumPy implementation illustrates a marked improvement over the naive algorithm, but we can do even better!</p><figure><p><img src="https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/673fc25b92ed56e5e8cca7b3_673fbcf232993c772c298178_perf-02.png" loading="lazy" alt=""/></p><figcaption>Performance of naive serial Python and NumPy implementations of cosine similarity.</figcaption></figure><p>In the remainder of this article, we’ll implement cosine similarity using C, taking advantage of common SIMD instructions available for various CPUs. We will also write a dispatch function that chooses different algorithms depending on our CPU. Along the way, we&#39;ll discuss the intricacies and complexities of SIMD programming and how to solve them.</p><p>In Part 2 of this series, we&#39;ll show how Mojo makes it easy to write similarly performant algorithms using built-in language features, so subscribe to the <a href="http://www.modular.com/blog/rss.xml">RSS feed</a> to get notified when that comes!</p><h2><strong>Mixed precision</strong></h2><p>For the remainder of this article, we&#39;ll be using the most common numeric type in Machine Learning these days, the <code>bfloat16</code> type.</p><p><code>bfloat16</code> is  a 16-bit floating point number type with a much smaller range and precision than the IEEE-standard <code>float32</code> type, making it much faster to compute with. It&#39;s also different from the IEEE-standard <code>float16</code> type.</p><figure><p><img src="https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/673ebcb6a120863760b141e4_673ebadeacaac1335d3ed9e0_bfloat.png" loading="lazy" alt=""/></p><figcaption>bfloat16 compared to bfloat32 </figcaption></figure><p>Surprising to many, the <code>bfloat16</code> is much easier to convert to and from <code>float32</code> than <code>float16</code> is -- it&#39;s just a single bit shift, because its exponent takes the same number of bits. This makes it much easier to use in practice, even on older hardware without native support: just shift to convert it to a <code>float32</code>, use a <code>float32</code> instruction, and shift to convert it back.</p><p>But as much as we like <code>bfloat16</code>, we shouldn&#39;t use it exclusively. The imprecision introduced by the type can compound quickly leading to wildly inaccurate results. To address this, we can <strong>mix different numeric types in the same computation.</strong></p><p>For example, we could use <code>bfloat16</code> for the vectors, multiply into <code>float32</code> accumulators, and upcast further to <code>float64</code> for the final <a href="https://mathworld.wolfram.com/NormalizedVector.html">normalization</a>. In C++23 it would look like this:</p><div><div>
<p><span>C++</span></p><div><p>
#include &lt;cmath&gt; // std::sqrt
#include &lt;stdfloat&gt; // std::bfloat16_t, std::float32_t, std::float64_t

std::float64_t cosine_similarity(std::bfloat16_t const * a, std::bfloat16_t const * b, std::size_t n) noexcept {
    std::float32_t dot_product = 0, norm_a = 0, norm_b = 0;
    for (std::size_t i = 0; i &lt; n; i++) {
        std::float32_t ai = a[i], bi = b[i];
        dot_product += ai * bi, norm_a += ai * ai, norm_b += bi * bi;
    }
    if (dot_product == 0) return 0;
    if (norm_a == 0 || norm_b == 0) return 1;
    return dot_product / (std::sqrt</p><std::float64_t>(norm_a) * std::sqrt<std::float64_t>(norm_b));
}
</std::float64_t></std::float64_t></div>
</div></div><p>Unfortunately, there are several issues with this code.</p><p>First, the C++ standard <a href="https://en.cppreference.com/w/cpp/header/stdfloat">doesn&#39;t require</a> those types to be defined, so not every compiler will have them. <strong>Portable code with these faster data types generally requires more effort.</strong></p><p>Second, it&#39;s well known that even in data-parallel code like this, compilers have a hard time <a href="https://stackoverflow.com/a/1422181">vectorizing</a> low-precision code, sometimes <a href="https://ashvardanian.com/posts/gcc-12-vs-avx512fp16/">leaving as much as 99% of single-core performance on the table</a>, and with mixed precision it&#39;s even worse! <strong>One needs to be explicit if they want reliable data parallelism.</strong></p><p>Lastly, double-precision division, combined with a bit-accurate <code>std::sqrt</code> <a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/ieee754/dbl-64/e_sqrt.c;h=017d30416ca9b92cdc87a6cc840ca30791bba558;hb=c064f6a613844181f411aabb2662384a6aefb69e">implementation</a>, introduces several code branches and a lot of <a href="https://cs.stackexchange.com/a/80871">latency</a> for small inputs. <strong>We can avoid these by using faster instructions.</strong></p><p>To solve these problems, let&#39;s use <code>rsqrt</code> reciprocal square root (1/√x) approximations, and explicitly use SIMD intrinsics to accelerate this code for every common target platform.</p><p>Let&#39;s start by looking at some C 99 cross-platform kernels from my <a href="https://github.com/ashvardanian/SimSIMD">SimSIMD</a> library.</p><h3><strong>Baseline implementation for Intel Haswell</strong></h3><p>Intel Haswell and newer generations of x86 CPUs all support <a href="https://dev.to/karapto/what-is-intel-avx2-2cad">AVX2</a>, which is a 256-bit SIMD instruction set for the 32 new YMM register files on the CPU. This allows us to do 8 simultaneous <code>float32</code> operations at a time.</p><p>Here&#39;s roughly what our above kernel would look like, specialized for AVX2 hardware.</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_haswell(simsimd_bf16_t const * a, simsimd_bf16_t const * b, simsimd_size_t n, simsimd_distance_t * result) {
    __m256 a_vec, b_vec;
    // Initialize our accumulators, each will have 8 words/scalars.
    __m256 ab_vec = _mm256_setzero_ps(), a2_vec = _mm256_setzero_ps(), b2_vec = _mm256_setzero_ps();

    while (n) {
        // Load the next 128 bits from the inputs, then cast.
        a_vec = _simsimd_bf16x8_to_f32x8_haswell(_mm_loadu_si128((__m128i const*)a));
        b_vec = _simsimd_bf16x8_to_f32x8_haswell(_mm_loadu_si128((__m128i const*)b));
        n -= 8, a += 8, b += 8;
        // TODO: Handle input lengths that aren&#39;t a multiple of 8

        // Multiply and add them to the accumulator variables.
        ab_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_vec);
        a2_vec = _mm256_fmadd_ps(a_vec, a_vec, a2_vec);
        b2_vec = _mm256_fmadd_ps(b_vec, b_vec, b2_vec);
    }

    // TODO: Reduce; combine the 8 words/scalars into one scalar number.
    // TODO: Normalize and divide, like: *result = ab / (sqrt(a2) * sqrt(b2))
}
</p>
</div></div><p>We&#39;ll implement the <code>TODO</code> blocks later below, but you can also see the completed <code>simsimd_cos_bf16_haswell</code> implementation in <a href="https://github.com/ashvardanian/SimSIMD/blob/81799b6dd8cf28ac71873db7fec34ed85f381ce8/include/simsimd/spatial.h#L942">simsimd/spatial.h</a>.</p><p>This loop does three main things.</p><p>First, it uses <code>_mm_loadu_si128</code> to load 128 bits from a potentially unaligned memory address. One should be careful to use this instead of <code>_mm_load_si128</code> which crashes if the address of the vector is not divisible by 16, which is the XMM register size in bytes.</p><p>Second, it uses <code>_simsimd_bf16x8_to_f32x8_haswell</code> to convert from <code>bfloat16</code> to <code>float32</code>. It interprets the input as 8 16-bit integers and converts them to 32-bit integers, and then shift them left by 16 bits:</p><div><div>
<p><span>C</span></p><p>
__m256 _simsimd_bf16x8_to_f32x8_haswell(__m128i a) {
    return _mm256_castsi256_ps(_mm256_slli_epi32(_mm256_cvtepu16_epi32(a), 16));
}
</p>
</div></div><p>Third, we use <code>_mm256_fmadd_ps</code>  to <strong>accumulate our dot product</strong> into <code>ab_vec</code>&#39;s 8 scalars by multiplying <code>a_vec</code>&#39;s 8 words with <code>b_vec</code>&#39;s 8 words. This is a fused multiply-add (FMA) instruction, which is faster than multiplying and adding numbers individually:</p><div><div>
<p><span>Text</span></p><p>
| Architecture       | `mul` Latency | `add` Latency | `fmadd` Latency |
| :----------------- | ------------: | ------------: | --------------: |
| Intel Haswell      |      5 cycles |      3 cycles |        5 cycles |
| Intel Skylake-X    |      4 cycles |      4 cycles |        4 cycles |
| Intel Ice Lake     |      4 cycles |      4 cycles |        4 cycles |
| AMD Zen4           |      3 cycles |      3 cycles |        4 cycles |
</p>
</div></div><p>For most of those instructions and CPU core designs, two such operations can be performed simultaneously. We call FMA three times in each cycle. Once for the actual dot product and twice to accumulate &#34;squared norms&#34; into <code>a2_vec</code> and <code>b2_vec</code>.</p><p>After the loop, we&#39;ll need to <strong>horizontally accumulate,</strong> which means add the 8 words of <code>ab_vec</code> together to get one single number. Let&#39;s talk about how!</p><h4><strong>Reducing horizontal accumulation</strong></h4><p>A tricky part of SIMD in practice is to <strong>avoid horizontal accumulation of the entire register,</strong> because it’s a slow operation.</p><p>Whenever possible, accumulation should be placed outside of the loop, which generally requires some intra-register shuffling for a tree-like reduction.</p><p>For example, <code>_simsimd_reduce_f32x8_haswell</code> may look like this under the hood:</p><div><div>
<p><span>C</span></p><p>
simsimd_f64_t _simsimd_reduce_f32x8_haswell(__m256 vec) {
    __m128 low_f32 = _mm256_castps256_ps128(vec);
    __m128 high_f32 = _mm256_extractf128_ps(vec, 1);
    __m256d low_f64 = _mm256_cvtps_pd(low_f32);
    __m256d high_f64 = _mm256_cvtps_pd(high_f32);
    __m256d sum = _mm256_add_pd(low_f64, high_f64);
    __m128d sum_low = _mm256_castpd256_pd128(sum);
    __m128d sum_high = _mm256_extractf128_pd(sum, 1);
    __m128d sum128 = _mm_add_pd(sum_low, sum_high);
    sum128 = _mm_hadd_pd(sum128, sum128);
    return _mm_cvtsd_f64(sum128);
}
</p>
</div></div><p>In our function, we&#39;ll use it like this:</p><div><div>
<p><span>C</span></p><p>
simsimd_f64_t ab = _simsimd_reduce_f32x8_haswell(ab_vec);
simsimd_f64_t a2 = _simsimd_reduce_f32x8_haswell(a2_vec);
simsimd_f64_t b2 = _simsimd_reduce_f32x8_haswell(b2_vec);
</p>
</div></div><p>Now, we have:</p><ul role="list"><li>A single <code>ab</code> number, the dot product of <code>a</code> and <code>b</code>.</li><li>A single <code>a2</code> number, the dot product of <code>a</code> with itself, the &#34;squared <a href="https://mathworld.wolfram.com/NormalizedVector.html">norm</a>&#34; of `a`.</li><li>A similar squared norm of `b`.</li></ul><h4><strong>Reciprocal square root approximation in AVX2</strong></h4><p>Our function&#39;s last line will be this:</p><div><div>
<p><span>C</span></p><p>
*result = _simsimd_cos_normalize_f64_haswell(ab, a2, b2);
</p>
</div></div><p>It will need to do the dot product of <code>a</code> and <code>b</code>, divided by the normalized <code>a</code> and the normalized <code>b</code>, like this pseudocode:</p><div><div>
<p><span>C</span></p><p>
*result = ab / (sqrt(a2) * sqrt(b2))
</p>
</div></div><p>That&#39;s right: to normalize the result, not one, but <em>two</em> square roots are required.</p><p>We&#39;ll use the <code>_mm_rsqrt_ps</code> instruction. It&#39;s not available for double-precision floats, but it&#39;s still a great choice for single-precision floats. Our function would look something like this:</p><div><div>
<p><span>C</span></p><p>
simsimd_distance_t _simsimd_cos_normalize_f64_haswell(simsimd_f64_t ab, simsimd_f64_t a2, simsimd_f64_t b2) {
    if (ab == 0) return 0;
    if (a2 == 0 || b2 == 0) return 1;

    __m128d squares = _mm_set_pd(a2, b2);
    __m128d rsqrts = _mm_cvtps_pd(_mm_rsqrt_ps(_mm_cvtpd_ps(squares)));
    
    // TODO: More precision!

    simsimd_f64_t a2_reciprocal = _mm_cvtsd_f64(_mm_unpackhi_pd(rsqrts, rsqrts));
    simsimd_f64_t b2_reciprocal = _mm_cvtsd_f64(rsqrts);
    return ab * a2_reciprocal * b2_reciprocal;
}
</p>
</div></div><p>However, the <code>_mm_rsqrt_ps</code> instruction can introduce errors as high as <code>1.5*2^-12</code>, as documented by Intel.</p><p>A great 2009 article <a href="https://web.archive.org/web/20210208132927/http://assemblyrequired.crashworks.org/timing-square-root/">Timing Square Root</a> comparing the accuracy and speed of the native <code>rsqrt</code> instruction (present in SSE and newer x86 SIMD instruction sets) with John Carmack&#39;s <a href="https://en.wikipedia.org/wiki/Fast_inverse_square_root">fast inverse square root</a> mentions using a <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton-Raphson iteration</a> as an alternative to improve numerical accuracy, and we&#39;ll do something similar here to improve our algorithm:</p><div><div>
<p><span>C</span></p><p>
simsimd_distance_t _simsimd_cos_normalize_f64_haswell(simsimd_f64_t ab, simsimd_f64_t a2, simsimd_f64_t b2) {
    if (ab == 0) return 0;
    if (a2 == 0 || b2 == 0) return 1;

    __m128d squares = _mm_set_pd(a2, b2);
    __m128d rsqrts = _mm_cvtps_pd(_mm_rsqrt_ps(_mm_cvtpd_ps(squares)));
    
    // Newton-Raphson iteration:
    rsqrts = _mm_add_pd(
        _mm_mul_pd(_mm_set1_pd(1.5), rsqrts),
        _mm_mul_pd(_mm_mul_pd(_mm_mul_pd(squares, _mm_set1_pd(-0.5)), rsqrts), _mm_mul_pd(rsqrts, rsqrts)));

    simsimd_f64_t a2_reciprocal = _mm_cvtsd_f64(_mm_unpackhi_pd(rsqrts, rsqrts));
    simsimd_f64_t b2_reciprocal = _mm_cvtsd_f64(rsqrts);
    return ab * a2_reciprocal * b2_reciprocal;
}
</p>
</div></div><p>One must keep in mind, various CPUs&#39; <code>rsqrt</code> instructions will have different performance and accuracy. After we finish our <code>simsimd_cos_bf16_haswell</code> function, we&#39;ll see how this compares to <a href="https://docs.google.com/document/d/1lUiTY6um-qRI0I1Rdo2ZAWkIBWZGM7E034QjWlu625s/edit#bookmark=id.ji429glcnc5d">AVX-512</a> further below.</p><h4><strong>Partial Loads</strong></h4><p>There&#39;s still an assumption we have to fix, inside our loop:</p><div><div>
<p><span>C</span></p><p>
// TODO: Handle input lengths that aren&#39;t a multiple of 8
</p>
</div></div><p>Potentially the ugliest part of this and many other production SIMD codebases is the &#34;partial load&#34; logic.</p><p>It&#39;s a natural consequence of slicing variable-length data into fixed-register-length chunks.</p><p>Because of this, almost every kernel has the &#34;main loop&#34; and a &#34;tail loop&#34; for the last few elements.</p><p>Prior to AVX-512 on x86 and SVE on Arm, there was no way to load a partial register, so the code has to be a bit more complex.</p><p>In Part 2, we&#39;ll talk about how Mojo solves this problem. But for now, we&#39;ll handle it manually by adding this inside the loop:</p><div><div>
<p><span>C</span></p><p>
    if (n &lt; 8) {
        a_vec = _simsimd_bf16x8_to_f32x8_haswell(_simsimd_partial_load_bf16x8_haswell(a, n));
        b_vec = _simsimd_bf16x8_to_f32x8_haswell(_simsimd_partial_load_bf16x8_haswell(b, n));
        n = 0;
    } else {
        ...
</p>
</div></div><p>That&#39;s it! Here&#39;s the complete function below. And as a bonus, we changed the <code>while</code> loop to a <code>goto</code> because that&#39;s just how I roll. Oftentimes, <code>goto</code> usage is discouraged in modern codebases, but in small functions they don’t convolute the control flow and can help reason about code in a more machine-accurate way. CPU’s don’t have <code>for</code> loops after all. Moreover, this way we avoid code duplication for the primary logic instead of repeating it twice in the main loop and the tail.</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_haswell(simsimd_bf16_t const * a, simsimd_bf16_t const * b, simsimd_size_t n, simsimd_distance_t * result) {
    __m256 a_vec, b_vec;
    __m256 ab_vec = _mm256_setzero_ps(), a2_vec = _mm256_setzero_ps(), b2_vec = _mm256_setzero_ps();

simsimd_cos_bf16_haswell_cycle:
    if (n &lt; 8) {
        a_vec = _simsimd_bf16x8_to_f32x8_haswell(_simsimd_partial_load_bf16x8_haswell(a, n));
        b_vec = _simsimd_bf16x8_to_f32x8_haswell(_simsimd_partial_load_bf16x8_haswell(b, n));
        n = 0;
    } else {
        a_vec = _simsimd_bf16x8_to_f32x8_haswell(_mm_loadu_si128((__m128i const*)a));
        b_vec = _simsimd_bf16x8_to_f32x8_haswell(_mm_loadu_si128((__m128i const*)b));
        n -= 8, a += 8, b += 8;
    }
    ab_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_vec);
    a2_vec = _mm256_fmadd_ps(a_vec, a_vec, a2_vec);
    b2_vec = _mm256_fmadd_ps(b_vec, b_vec, b2_vec);
    if (n)
        goto simsimd_cos_bf16_haswell_cycle;

    simsimd_f64_t ab = _simsimd_reduce_f32x8_haswell(ab_vec);
    simsimd_f64_t a2 = _simsimd_reduce_f32x8_haswell(a2_vec);
    simsimd_f64_t b2 = _simsimd_reduce_f32x8_haswell(b2_vec);
    *result = _simsimd_cos_normalize_f64_haswell(ab, a2, b2);
}
</p>
</div></div><p>You can also see <code>simsimd_cos_bf16_haswell</code> live, in <a href="https://github.com/ashvardanian/SimSIMD/blob/81799b6dd8cf28ac71873db7fec34ed85f381ce8/include/simsimd/spatial.h#L942">simsimd/spatial.h</a>.</p><p>Now we are done with one of the most &#34;vanilla&#34; kernels in SimSIMD. It targets broadly available CPUs starting with Intel Haswell, shipped from 2013 onwards. It is forward-compatible with 11 years of x86 hardware, performing mixed-precision operations in bf16-f32-f64, including numeric types not natively supported by hardware. This is a good warmup, but the most exciting parts of SimSIMD, and the reason it runs on hundreds of millions of devices, are still ahead!</p><h3><strong>Baseline implementation for AMD Genoa</strong></h3><p>Let&#39;s kick it up a notch, and bring in AVX-512, which is a 512-bit SIMD instruction set for the 32 new ZMM register files on the CPU. AMD Genoa is the first AMD CPU to support it, and it&#39;s also the earliest publicly available CPU with <code>AVX512_BF16</code> &#34;extension of extension&#34; for the <code>bfloat16</code> type, including the <code>vdpbf16ps</code> instruction for dot products.</p><p>Before we show the masking and the <code>vdpbf16ps</code> parts, here&#39;s roughly what our <code>simsimd_cos_bf16_genoa</code> will look like:</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_genoa(
    simsimd_bf16_t const * a, simsimd_bf16_t const * b,
    simsimd_size_t n,
    simsimd_distance_t * result) {

    __m512 ab_vec = _mm512_setzero_ps();
    __m512 a2_vec = _mm512_setzero_ps();
    __m512 b2_vec = _mm512_setzero_ps();
    __m512i a_i16_vec, b_i16_vec;

simsimd_cos_bf16_genoa_cycle:
    if (n &lt; 32) {
        // TODO: Partial (&#34;masked&#34;) load from a and b.
        n = 0;
    } else {
        a_i16_vec = _mm512_loadu_epi16(a);
        b_i16_vec = _mm512_loadu_epi16(b);
        a += 32, b += 32, n -= 32;
    }
    // TODO: Add to ab_vec/a2_vec/b2_vec with special dot-product instruction.

    if (n)
        goto simsimd_cos_bf16_genoa_cycle;

    simsimd_f64_t ab = _simsimd_reduce_f32x16_skylake(ab_vec);
    simsimd_f64_t a2 = _simsimd_reduce_f32x16_skylake(a2_vec);
    simsimd_f64_t b2 = _simsimd_reduce_f32x16_skylake(b2_vec);
    *result = _simsimd_cos_normalize_f64_skylake(ab, a2, b2);
}
</p>
</div></div><p>Structurally, the kernel is similar, but the loop step size grew from 8 to 32, and even bigger changes are hiding behind those TODOs.</p><p>You may have noticed that <code>a_i16_vec</code> and <code>b_i16_vec</code> are represented with <code>__m512i</code>, as opposed to the <code>bfloat16</code> vector type <code>__m512bh</code>. This is caused by incomplete support for <code>loadu</code> intrinsics, missing a <code>bfloat16</code> overload in some compilers.</p><p>Now, let&#39;s resolve those <code>TODO</code>s, starting with the masked loads.</p><h4><strong>Masking in x86 AVX-512</strong></h4><p>AVX-512 was the first SIMD instruction set to introduce masking, which is a way to <strong>partially execute instructions</strong> based on a mask. For example, if you have a register containing 8 words/scalars, you can use a mask to increment only 5 of them.</p><p>Each core contains not only 32 <code>ZMM</code> registers, but also 8 <code>KMASK</code> registers, which can be used to control the execution of instructions. Think of them as 8 additional normal scalar registers, containing 8, 16, 32, or 64 bits. Moving between those and general-purpose registers isn&#39;t free, but using them for masking is still better than branching.</p><p>Interleaving, testing, and merging masks with each other is also natively supported with these special mask manipulation instructions:</p><div><div>
<p><span>C</span></p><p>
int _mm512_kortestc(__mmask16 k1, __mmask16 k2);        // `kortestw`
int _mm512_kortestz(__mmask16 k1, __mmask16 k2);        // `kortestw`
__mmask16 _mm512_kand(__mmask16 a, __mmask16 b);        // `kandw`
__mmask16 _mm512_kandn(__mmask16 a, __mmask16 b);       // `kandnw`
__mmask16 _mm512_kmov(__mmask16 a);                     // `kmovw`
__mmask16 _mm512_knot(__mmask16 a);                     // `knotw`
__mmask16 _mm512_kor(__mmask16 a, __mmask16 b);         // `korw`
__mmask16 _mm512_kunpackb(__mmask16 a, __mmask16 b);    // `kunpckbw`
__mmask16 _mm512_kxnor(__mmask16 a, __mmask16 b);       // `kxnorw`
__mmask16 _mm512_kxor(__mmask16 a, __mmask16 b);        // `kxorw`
__mmask32 _mm512_kunpackw(__mmask32 a, __mmask32 b);    // `kunpckwd`
__mmask64 _mm512_kunpackd(__mmask64 a, __mmask64 b);    // `kunpckdq`
</p>
</div></div><p>When using SIMD, it can be easy to accidentally load past the end of the array, since we&#39;re loading in chunks, and the array might not be a multiple of our vector size. To avoid that, we&#39;ll do some masking to <strong>partially load</strong> from <code>a</code> and <code>b</code> in the last iteration. Our if-else looks like this now:</p><div><div>
<p><span>C</span></p><p>
if (n &lt; 32) {
    __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, n);
    a_i16_vec = _mm512_maskz_loadu_epi16(mask, a);
    b_i16_vec = _mm512_maskz_loadu_epi16(mask, b);
    n = 0;
} else {
    a_i16_vec = _mm512_loadu_epi16(a);
    b_i16_vec = _mm512_loadu_epi16(b);
    a += 32, b += 32, n -= 32;
}
</p>
</div></div><h4><strong>Dot products with <code>vdpbf16ps</code></strong></h4><p>Let&#39;s resolve this TODO now:</p><div><div>
<p><span>C</span></p><p>
// TODO: Add to ab_vec/a2_vec/b2_vec with special dot-product instruction.
</p>
</div></div><p>Previously, in our <code>simsimd_cos_bf16_haswell</code> kernel, we added 8 input <code>bfloat16</code>s to <code>ab_vec</code> etc. with this code:</p><div><div>
<p><span>C</span></p><p>
ab_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_vec);
a2_vec = _mm256_fmadd_ps(a_vec, a_vec, a2_vec);
b2_vec = _mm256_fmadd_ps(b_vec, b_vec, b2_vec);
</p>
</div></div><p><code>ab_vec</code> is 512 bits wide, and holds 16 <code>bfloat32</code>s. You might think that we would now add 16 input <code>bfloat16</code>s to it, but <strong>it&#39;s even better than that:</strong> the <code>vdpbf16ps</code> processes <strong>32</strong> input numbers into our 16-word <code>ab_vec</code>. It takes the dot product of the first <strong>two</strong> words of each input, and add their dot-products to the first word of the result (like <code>ab_vec[0] += a[0] * b[0] + a[1] * b[1]</code>), and then it does that for the rest of the words as well.</p><p>We&#39;ll use the <code>_mm512_dpbf16_ps</code> function to do this instruction, and add this to our loop:</p><div><div>
<p><span>C</span></p><p>
ab_vec = _mm512_dpbf16_ps(ab_vec, (__m512bh)(a_i16_vec), (__m512bh)(b_i16_vec));
a2_vec = _mm512_dpbf16_ps(a2_vec, (__m512bh)(a_i16_vec), (__m512bh)(a_i16_vec));
b2_vec = _mm512_dpbf16_ps(b2_vec, (__m512bh)(b_i16_vec), (__m512bh)(b_i16_vec));
</p>
</div></div><p>Here&#39;s the full function:</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_genoa(
    simsimd_bf16_t const * a, simsimd_bf16_t const * b,
    simsimd_size_t n,
    simsimd_distance_t * result) {
    __m512 ab_vec = _mm512_setzero_ps();
    __m512 a2_vec = _mm512_setzero_ps();
    __m512 b2_vec = _mm512_setzero_ps();
    __m512i a_i16_vec, b_i16_vec;
simsimd_cos_bf16_genoa_cycle:
    if (n &lt; 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, n);
        a_i16_vec = _mm512_maskz_loadu_epi16(mask, a);
        b_i16_vec = _mm512_maskz_loadu_epi16(mask, b);
        n = 0;
    } else {
        a_i16_vec = _mm512_loadu_epi16(a);
        b_i16_vec = _mm512_loadu_epi16(b);
        a += 32, b += 32, n -= 32;
    }
    ab_vec = _mm512_dpbf16_ps(ab_vec, (__m512bh)(a_i16_vec), (__m512bh)(b_i16_vec));
    a2_vec = _mm512_dpbf16_ps(a2_vec, (__m512bh)(a_i16_vec), (__m512bh)(a_i16_vec));
    b2_vec = _mm512_dpbf16_ps(b2_vec, (__m512bh)(b_i16_vec), (__m512bh)(b_i16_vec));
    if (n)
        goto simsimd_cos_bf16_genoa_cycle;
    simsimd_f64_t ab = _simsimd_reduce_f32x16_skylake(ab_vec);
    simsimd_f64_t a2 = _simsimd_reduce_f32x16_skylake(a2_vec);
    simsimd_f64_t b2 = _simsimd_reduce_f32x16_skylake(b2_vec);
    *result = _simsimd_cos_normalize_f64_skylake(ab, a2, b2);
}
</p>
</div></div><p>Now let&#39;s implement that <code>_simsimd_cos_normalize_f64_skylake</code> call at the end!</p><h4><strong>Reciprocal square root approximation for AVX-512</strong></h4><p>We need to conceptually calculate <code>ab / (sqrt(a2) * sqrt(b2))</code>.</p><p>In our <code>_simsimd_cos_normalize_f64_haswell</code> function from before, we used the <code>rsqrtps</code> instruction (via the <code>_mm_rsqrt_ps</code> intrinsic) to calculate our reciprocal square root. In AVX-512, those are superseded by the <code>_mm512_rsqrt14_ps</code> intrinsic and <code>vrsqrt14ps</code> instruction.</p><p>And fortunately, the maximum error bound improved from <code>1.5*2^-12</code> to <code>2^-14</code>.</p><p>There is also a double-precision instruction <code>vrsqrt14pd</code> (via the <code>_mm512_rsqrt14_pd</code> intrinsic).</p><p>It has the same precision, but we can avoid casting.</p><div><div>
<p><span>C</span></p><p>
simsimd_distance_t _simsimd_cos_normalize_f64_skylake(simsimd_f64_t ab, simsimd_f64_t a2, simsimd_f64_t b2) {
    if (ab == 0) return 0;
    if (a2 == 0 || b2 == 0) return 1;

    __m128d squares = _mm_set_pd(a2, b2);
    __m128d rsqrts = _mm_maskz_rsqrt14_pd(0xFF, squares);
    rsqrts = _mm_add_pd( // Newton-Raphson iteration:
        _mm_mul_pd(_mm_set1_pd(1.5), rsqrts),
        _mm_mul_pd(_mm_mul_pd(_mm_mul_pd(squares, _mm_set1_pd(-0.5)), rsqrts), _mm_mul_pd(rsqrts, rsqrts)));

    simsimd_f64_t a2_reciprocal = _mm_cvtsd_f64(_mm_unpackhi_pd(rsqrts, rsqrts));
    simsimd_f64_t b2_reciprocal = _mm_cvtsd_f64(rsqrts);
    return ab * a2_reciprocal * b2_reciprocal;
}
</p>
</div></div><p>A natural question may be: how important is this Newton-Raphson iteration?</p><div><div>
<p><span>Text</span></p><p>
| Datatype   |         NumPy Error | SimSIMD w/out Iteration |             SimSIMD |
| :--------- | ------------------: | ----------------------: | ------------------: |
| `bfloat16` | 1.89e-08 ± 1.59e-08 |     3.07e-07 ± 3.09e-07 | 3.53e-09 ± 2.70e-09 |
| `float16`  | 1.67e-02 ± 1.44e-02 |     2.68e-05 ± 1.95e-05 | 2.02e-05 ± 1.39e-05 |
| `float32`  | 2.21e-08 ± 1.65e-08 |     3.47e-07 ± 3.49e-07 | 3.77e-09 ± 2.84e-09 |
| `float64`  | 0.00e+00 ± 0.00e+00 |     3.80e-07 ± 4.50e-07 | 1.35e-11 ± 1.85e-11 |
</p>
</div></div><p>As you can see, it reduces our relative error by a lot. On 1536-dimensional inputs, the impact is as big as <strong>2-3 orders of magnitude!</strong></p><p>Keep this impact in mind. In a little bit, we&#39;ll see the error rate for Arm NEON&#39;s rsqrt instruction, and just how necessary it is to stay on top of our floating point precision for specific CPUs.</p><p>This is also why abstraction can be difficult and/or counterproductive when dealing with SIMD. The entire shape of the algorithm (such as whether we do a Newton-Raphson iteration) can change depending on the CPU or the precision of its instructions.</p><h3><strong>Baseline implementation for Arm NEON</strong></h3><p>Most CPUs aren&#39;t running x86, but Arm. Apple alone today accounts for 2.2 billion devices, and almost all of them run on Arm. Arm devices also have a SIMD instruction set, called NEON, which is similar to SSE, also with 128-bit registers.</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_neon(
    simsimd_bf16_t const * a, simsimd_bf16_t const * b,
    simsimd_size_t n, simsimd_distance_t * result) {

    float32x4_t ab_high_vec = vdupq_n_f32(0), ab_low_vec = vdupq_n_f32(0);
    float32x4_t a2_high_vec = vdupq_n_f32(0), a2_low_vec = vdupq_n_f32(0);
    float32x4_t b2_high_vec = vdupq_n_f32(0), b2_low_vec = vdupq_n_f32(0);
    bfloat16x8_t a_vec, b_vec;

simsimd_cos_bf16_neon_cycle:
    if (n &lt; 8) {
        a_vec = _simsimd_partial_load_bf16x8_neon(a, n);
        b_vec = _simsimd_partial_load_bf16x8_neon(b, n);
        n = 0;
    } else {
        a_vec = vld1q_bf16((bfloat16_t const*)a);
        b_vec = vld1q_bf16((bfloat16_t const*)b);
        n -= 8, a += 8, b += 8;
    }
    ab_high_vec = vbfmlaltq_f32(ab_high_vec, a_vec, b_vec);
    ab_low_vec = vbfmlalbq_f32(ab_low_vec, a_vec, b_vec);
    a2_high_vec = vbfmlaltq_f32(a2_high_vec, a_vec, a_vec);
    a2_low_vec = vbfmlalbq_f32(a2_low_vec, a_vec, a_vec);
    b2_high_vec = vbfmlaltq_f32(b2_high_vec, b_vec, b_vec);
    b2_low_vec = vbfmlalbq_f32(b2_low_vec, b_vec, b_vec);
    if (n)
        goto simsimd_cos_bf16_neon_cycle;

    simsimd_f32_t ab = vaddvq_f32(vaddq_f32(ab_high_vec, ab_low_vec)),
                  a2 = vaddvq_f32(vaddq_f32(a2_high_vec, a2_low_vec)),
                  b2 = vaddvq_f32(vaddq_f32(b2_high_vec, b2_low_vec));
    *result = _simsimd_cos_normalize_f64_neon(ab, a2, b2);
}
</p>
</div></div><p>You may be wondering, what happened to our <code>ab_vec</code>, and what are these <code>ab_high_vec</code> and <code>ab_low_vec</code> things?</p><p>These, my friends, are a perfect example of my next point.</p><h4><strong>CISC vs RISC, it&#39;s not what people say</strong></h4><p>The Arm instruction set is often called RISC (Reduced Instruction Set Computer), and the x86 instruction set is often called CISC (Complex Instruction Set Computer).</p><p>But in practice, the Arm instruction set is potentially more complex, especially given that all of those instructions practically apply only to 64-bit and 128-bit registers, as opposed to the x86 instructions which also include 256-bit and 512-bit registers!</p><p>Here is an example.</p><p>Before discovering the <code>vbfdotq_f32</code> intrinsic (suggested by Mark Reed), SimSIMD used the <code>vbfmmlaq_f32</code> and <code>vbfmmlaq_f32</code> intrinsics to compute the dot product of two vectors.</p><p>It&#39;s also a Fused Multiply-Add instruction, but designed to take only odd or even components of the vectors, so we aggregate them differently:</p><div><div>
<p><span>C</span></p><p>
ab_high_vec = vbfmlaltq_f32(ab_high_vec, a_vec, b_vec);
ab_low_vec = vbfmlalbq_f32(ab_low_vec, a_vec, b_vec);
a2_high_vec = vbfmlaltq_f32(a2_high_vec, a_vec, a_vec);
a2_low_vec = vbfmlalbq_f32(a2_low_vec, a_vec, a_vec);
b2_high_vec = vbfmlaltq_f32(b2_high_vec, b_vec, b_vec);
b2_low_vec = vbfmlalbq_f32(b2_low_vec, b_vec, b_vec);
</p>
</div></div><p>In other words, <code>ab_high_vec</code> contains only the even components, and <code>ab_low_vec</code> contains only the odd components.</p><p>This instruction probably makes more sense for complex dot-products, where we might want to load only the real words or only the imaginary words from a vector of complex numbers.</p><p>As for the cosine distance, there is one more non-obvious approach previously tested in SimSIMD.</p><p>NEON contains a <code>BFMMLA</code> instruction:</p><div><div>
<p><span>Text</span></p><p>
&gt; BFloat16 floating-point matrix multiply-accumulates into 2x2 matrix.
&gt; This instruction multiplies the 2x4 matrix of BF16 values held in the first 128-bit source vector by the 4x2 BF16 matrix in the second 128-bit source vector.
&gt; The resulting 2x2 single-precision matrix product is then added destructively to the 2x2 single-precision matrix in the 128-bit destination vector.
&gt; This is equivalent to performing a 4-way dot product per destination element.
&gt; The instruction ignores the FPCR and does not update the FPSR exception status.
</p>
</div></div><p>Doesn&#39;t look very RISCy to me! But at some point it looked promising. Concatenating vectors \(a\) and \(b\) into a matrix \(X\) and computing the dot product \(X^T X\) in a single instruction is a great idea. It will contain 4 dot products, and we can drop 1 of them, keeping 3 products: \(a \cdot b\), \(a \cdot a\), and \(b \cdot b\), all needed for cosine similarity. That kernel may look similar to this:</p><div><div>
<p><span>C</span></p><p>
float32x4_t products_low_vec = vdupq_n_f32(0.0f);
float32x4_t products_high_vec = vdupq_n_f32(0.0f);
for (; i + 8 &lt;= n; i += 8) {
    bfloat16x8_t a_vec = vld1q_bf16((bfloat16_t const*)a + i);
    bfloat16x8_t b_vec = vld1q_bf16((bfloat16_t const*)b + i);
    int16x8_t a_vec_s16 = vreinterpretq_s16_bf16(a_vec);
    int16x8_t b_vec_s16 = vreinterpretq_s16_bf16(b_vec);
    int16x8x2_t y_w_vecs_s16 = vzipq_s16(a_vec_s16, b_vec_s16);
    bfloat16x8_t y_vec = vreinterpretq_bf16_s16(y_w_vecs_s16.val[0]);
    bfloat16x8_t w_vec = vreinterpretq_bf16_s16(y_w_vecs_s16.val[1]);
    bfloat16x4_t a_low = vget_low_bf16(a_vec);
    bfloat16x4_t b_low = vget_low_bf16(b_vec);
    bfloat16x4_t a_high = vget_high_bf16(a_vec);
    bfloat16x4_t b_high = vget_high_bf16(b_vec);
    bfloat16x8_t x_vec = vcombine_bf16(a_low, b_low);
    bfloat16x8_t v_vec = vcombine_bf16(a_high, b_high);
    products_low_vec = vbfmmlaq_f32(products_low_vec, x_vec, y_vec);
    products_high_vec = vbfmmlaq_f32(products_high_vec, v_vec, w_vec);
}
float32x4_t products_vec = vaddq_f32(products_high_vec, products_low_vec);
simsimd_f32_t a2 = products_vec[0], ab = products_vec[1], b2 = products_vec[3];
</p>
</div></div><p>Unfortunately, this approach ended up being 25% slower than the naive approach.</p><p>When dealing with x86 ISA, if you find a weird rare instruction that can be used in your program, it&#39;s often a good idea to apply it.</p><p>In Arm it doesn&#39;t help most of the time, so in my opinion, <strong>Arm is more CISCy than x86</strong>.</p><p>This is also why it&#39;s important to have CPU-specific code, rather than relying on coarse abstractions that just use whatever instruction is present.</p><h4><strong>Reverse engineering Arm accuracy</strong></h4><p>Similar to x86, the <code>rsqrt</code> instruction is available in NEON, but only for single-precision floats.</p><p>Unlike x86, the documentation is limited:</p><p>1. We can&#39;t know the latency of the instruction.</p><p>2. We are not even informed of the maximum relative error.</p><p>The first can be vaguely justified by the fact that Arm doesn&#39;t produce its chips, it licenses the designs to other companies. Still, a repository of timings for different chips would be very helpful. Second is just a mystery. Until you reverse engineer it!</p><p>I wrote <a href="https://gist.github.com/ashvardanian/5e5cf585d63f8ab6d240932313c75411">this benchmark program</a>, which estimates the accuracy of <code>rsqrt</code> approximations in Arm NEON, SSE, AVX2, and AVX-512.</p><p>Luckily, it&#39;s only 4 billion unique inputs to test, just a few seconds on a modern CPU.</p><h3><strong>Baseline implementation for Arm SVE</strong></h3><p>Arm SVE is unusual.</p><p>It&#39;s not the first to support masking, like AVX-512.</p><p>But it <em>is</em> the first to define variable-width implementation-defined registers, which can be 128, 256, 512, 1024, or 2048 bits wide.</p><p>That means that on one CPU, a svbfloat16_t might be 128 bits wide (8 <code>bfloat16</code>s) and on another CPU it might be 1024 bits wide (fitting 64 <code>bfloat16</code>s), yet the compiled program is the same. One can only know their width at run-time by calling the <code>svcnth</code> function, which tells us how many 16-bit words fit in a SVE vector on this CPU.</p><p>At the end of this spectrum (2048 bits) the CPUs are practically converging with GPUs, but despite that, all real implementations are 128-bit wide.</p><p>This is what a kernel may look like:</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16_sve(
    simsimd_bf16_t const* a_enum, simsimd_bf16_t const* b_enum,
    simsimd_size_t n, simsimd_distance_t* result) {

    simsimd_size_t i = 0;
    svfloat32_t ab_vec = svdupq_n_f32(0.f, 0.f, 0.f, 0.f);
    svfloat32_t a2_vec = svdupq_n_f32(0.f, 0.f, 0.f, 0.f);
    svfloat32_t b2_vec = svdupq_n_f32(0.f, 0.f, 0.f, 0.f);
    bfloat16_t const* a = (bfloat16_t const*)(a_enum);
    bfloat16_t const* b = (bfloat16_t const*)(b_enum);
    do {
        svbool_t progress_vec = svwhilelt_b16((unsigned int)i, (unsigned int)n);
        svbfloat16_t a_vec = svld1_bf16(progress_vec, a + i);
        svbfloat16_t b_vec = svld1_bf16(progress_vec, b + i);
        ab_vec = svbfdot_f32(ab_vec, a_vec, b_vec);
        a2_vec = svbfdot_f32(a2_vec, a_vec, a_vec);
        b2_vec = svbfdot_f32(b2_vec, b_vec, b_vec);
        i += svcnth();
    } while (i &lt; n);

    simsimd_f32_t ab = svaddv_f32(svptrue_b32(), ab_vec);
    simsimd_f32_t a2 = svaddv_f32(svptrue_b32(), a2_vec);
    simsimd_f32_t b2 = svaddv_f32(svptrue_b32(), b2_vec);
    *result = _simsimd_cos_normalize_f32_neon(ab, a2, b2);
}
</p>
</div></div><p>This is another reason it&#39;s important to have CPU-specific code, and to not rely too heavily on abstractions: some architectures are very unusual.</p><h4><strong>Masking in Arm SVE</strong></h4><p>Seeing “variable-width implementation-defined registers” you may ask yourself, how does the final iteration of the loop handle the remainder of the elements, if they don&#39;t fill up an entire vector? In SVE, the recommended approach is to use “progress masks” (like that <code>progress_vec</code>) and build them using “while less than” intrinsics (like that <code>svwhilelt_b16</code>). It’s a more powerful alternative to _bzhi_u32 that we’ve used for AVX-512 tails.</p><h2>Adding up the performance gains</h2><p>Taking a look back at all of the hardware-specific optimizations, we can see an orders-of-magnitude improvement over our initial naive implementation and stock NumPy implementation. Utilizing and exploiting hardware features has delivered performance boosts from 10 MB/s to 60.3 GB/s on Intel hardware, and 4 MB/s to 29.7 GB/s on Arm hardware. It underscores the absolute importance that specialized hardware acceleration libraries have on even the simplest of computational algorithms.</p><figure><p><img src="https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/673fc25b92ed56e5e8cca7b6_673fc1e045a6660b4eeaa221_perf-final.png" loading="lazy" alt=""/></p><figcaption>Performance improvements from architecture-specific SIMD optimizations for cosine similarity.</figcaption></figure><h2><strong>Packaging and distributing libraries</strong></h2><p>Throughout this post, I&#39;ve mentioned a few times that CPU-specific code is important. But shipping CPU-specific code correctly can be tricky.</p><p>The easiest and fastest approach is just to compile for a specific hardware platform. With <code>-march=native</code>, for example, your compiler will compile the code assuming the target machine that will run this software supports all the same Assembly instructions as this one. We can implement several kernels for different platforms and select the best backend at compile time.</p><div><div>
<p><span>C</span></p><p>
void simsimd_cos_bf16(
simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n,
             simsimd_distance_t* d) {
#if SIMSIMD_TARGET_GENOA
    simsimd_cos_bf16_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_cos_bf16_haswell(a, b, n, d);
#elif SIMSIMD_TARGET_SVE_BF16
    simsimd_cos_bf16_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_BF16
    simsimd_cos_bf16_neon(a, b, n, d);
#else
    simsimd_cos_bf16_serial(a, b, n, d);
#endif
}
</p>
</div></div><p>But if we don&#39;t know the target machine ahead of time, we&#39;ll want to ship multiple optimized kernels for every function in one binary, and differentiate at runtime. When the program starts, it checks the model or the capabilities of the current CPU, and selects the right implementation. It’s generally called “dynamic dispatch”. To implement it, on x86, we call the CPUID instruction to query several feature registers:</p><div><div>
<p><span>C</span></p><p>
simsimd_capability_t simsimd_capabilities_x86(void) {
   /// The states of 4 registers populated for a specific &#34;cpuid&#34; assembly call
   union four_registers_t {
       int array[4];
       struct separate_t {
           unsigned eax, ebx, ecx, edx;
       } named;
   } info1, info7, info7sub1;


#if defined(_MSC_VER)
   __cpuidex(info1.array, 1, 0);
   __cpuidex(info7.array, 7, 0);
   __cpuidex(info7sub1.array, 7, 1);
#else
   __asm__ __volatile__(&#34;cpuid&#34;
                        : &#34;=a&#34;(info1.named.eax), &#34;=b&#34;(info1.named.ebx), &#34;=c&#34;(info1.named.ecx), &#34;=d&#34;(info1.named.edx)
                        : &#34;a&#34;(1), &#34;c&#34;(0));
   __asm__ __volatile__(&#34;cpuid&#34;
                        : &#34;=a&#34;(info7.named.eax), &#34;=b&#34;(info7.named.ebx), &#34;=c&#34;(info7.named.ecx), &#34;=d&#34;(info7.named.edx)
                        : &#34;a&#34;(7), &#34;c&#34;(0));
   __asm__ __volatile__(&#34;cpuid&#34;
                        : &#34;=a&#34;(info7sub1.named.eax), &#34;=b&#34;(info7sub1.named.ebx), &#34;=c&#34;(info7sub1.named.ecx),
                          &#34;=d&#34;(info7sub1.named.edx)
                        : &#34;a&#34;(7), &#34;c&#34;(1));
#endif
   unsigned supports_avx2 = (info7.named.ebx &amp; 0x00000020) != 0; // Function ID 7, EBX register
   unsigned supports_f16c = (info1.named.ecx &amp; 0x20000000) != 0; // Function ID 1, ECX register
   unsigned supports_fma = (info1.named.ecx &amp; 0x00001000) != 0; // Function ID 1, ECX register
   unsigned supports_avx512f = (info7.named.ebx &amp; 0x00010000) != 0; // Function ID 7, EBX register
   unsigned supports_avx512fp16 = (info7.named.edx &amp; 0x00800000) != 0; // Function ID 7, EDX register
   unsigned supports_avx512vnni = (info7.named.ecx &amp; 0x00000800) != 0; // Function ID 7, ECX register
   unsigned supports_avx512ifma = (info7.named.ebx &amp; 0x00200000) != 0; // Function ID 7, EBX register
   unsigned supports_avx512bitalg = (info7.named.ecx &amp; 0x00001000) != 0; // Function ID 7, ECX register
   unsigned supports_avx512vbmi2 = (info7.named.ecx &amp; 0x00000040) != 0; // Function ID 7, ECX register
   unsigned supports_avx512vpopcntdq = (info7.named.ecx &amp; 0x00004000) != 0; // Function ID 7, ECX register
   unsigned supports_avx512bf16 = (info7sub1.named.eax &amp; 0x00000020) != 0; // Function ID 7, Sub-leaf 1, EAX register
}
</p>
</div></div><p>This code uses inline Assembly, when compiled with Clang and GCC. Microsoft Visual Studio Compiler doesn’t support inline assembly, but provides an intrinsic to compensate for that.</p><p>Once the feature flags are queried, they must be unpacked and used to select the right kernel implementation for every combination of input argument types and CPU capabilities. In my other library <a href="https://github.com/ashvardanian/StringZilla">StringZilla</a> (which has a much smaller collection of defined string operations), that’s done at a function-level individually. But in SimSIMD, which already has over 350 kernels and is rapidly growing, all the kernels are grouped into several categories for different generations of CPUs, like “Haswell” and “Ice Lake” for Intel or “Genoa” for AMD.  It was a simpler and more future-proof design to dispatch to dozens of similar kernels given the fragmentation of the x86_64 ISA:</p><figure><p><img src="https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/673ebdac74846653465a7aac_673ebd6bda44347e26609f81_24Levels.png" loading="lazy" alt=""/></p></figure><p>In other words, instead of having a separate implementation for every one of <a href="https://x.com/InstLatX64/status/1520707682356842496">dozens of x86 core designs across Intel, AMD, and Centurion</a>, SimSIMD selects several “capability levels” named after the first server-side CPU line-up that supports a specific family of instructions: Sapphire Rapids for AVX512-FP16, Genoa for AVX-512BF16, Ice Lake for VNNI/VBMI and other AVX-512 integer-processing instructions, Skylake-X for basic AVX-512 support, Haswell for AVX2, F16C, and FMA, etc.</p><p>The next step is to map supported CPU features into our “capability levels”:</p><div><div>
<p><span>C</span></p><p>
   unsigned supports_haswell = supports_avx2 &amp;&amp; supports_f16c &amp;&amp; supports_fma;
   unsigned supports_skylake = supports_avx512f;
   unsigned supports_ice = supports_avx512vnni &amp;&amp; supports_avx512ifma &amp;&amp; supports_avx512bitalg &amp;&amp; supports_avx512vbmi2 &amp;&amp; supports_avx512vpopcntdq;
   unsigned supports_genoa = supports_avx512bf16;
   unsigned supports_sapphire = supports_avx512fp16;
</p>
</div></div><p>This approach won&#39;t work on Arm. On Arm and Linux, again we can query CPU registers, but on Apple devices we must query the OS. The process is no less convoluted than with x86.</p><div><div>
<p><span>C</span></p><p>
simsimd_capability_t simsimd_capabilities_arm(void) {
#if defined(SIMSIMD_DEFINED_LINUX)
   unsigned long id_aa64isar0_el1 = 0, id_aa64isar1_el1 = 0, id_aa64pfr0_el1 = 0, id_aa64zfr0_el1 = 0;
   __asm__ __volatile__(&#34;mrs %0, ID_AA64ISAR0_EL1&#34; : &#34;=r&#34;(id_aa64isar0_el1));
   unsigned supports_integer_dot_products = ((id_aa64isar0_el1 &gt;&gt; 44) &amp; 0xF) &gt;= 1;
   __asm__ __volatile__(&#34;mrs %0, ID_AA64ISAR1_EL1&#34; : &#34;=r&#34;(id_aa64isar1_el1));
   unsigned supports_i8mm = ((id_aa64isar1_el1 &gt;&gt; 52) &amp; 0xF) &gt;= 1;
   unsigned supports_bf16 = ((id_aa64isar1_el1 &gt;&gt; 44) &amp; 0xF) &gt;= 1;
   __asm__ __volatile__(&#34;mrs %0, ID_AA64PFR0_EL1&#34; : &#34;=r&#34;(id_aa64pfr0_el1));
   unsigned supports_sve = ((id_aa64pfr0_el1 &gt;&gt; 32) &amp; 0xF) &gt;= 1;
   unsigned supports_fp16 = ((id_aa64pfr0_el1 &gt;&gt; 20) &amp; 0xF) == 1;
   if (supports_sve) // Querying SVE2 and SVE2.1 requires SVE to be supported
       __asm__ __volatile__(&#34;mrs %0, ID_AA64ZFR0_EL1&#34; : &#34;=r&#34;(id_aa64zfr0_el1));
  
   unsigned supports_sve_i8mm = ((id_aa64zfr0_el1 &gt;&gt; 44) &amp; 0xF) &gt;= 1;
   unsigned supports_sve_bf16 = ((id_aa64zfr0_el1 &gt;&gt; 20) &amp; 0xF) &gt;= 1;
   unsigned supports_sve2 = ((id_aa64zfr0_el1) &amp; 0xF) &gt;= 1;
   unsigned supports_sve2p1 = ((id_aa64zfr0_el1) &amp; 0xF) &gt;= 2;
   unsigned supports_neon = 1; // NEON is always supported on our target platforms


   ....
#elif defined(SIMSIMD_DEFINED_APPLE)
   ...
#else // SIMSIMD_DEFINED_LINUX
   return simsimd_cap_serial_k;
#endif
}
</p>
</div></div><p>Feature resolution (like the CPUID above) is great, but it can be slow, sometimes even up to 300 CPU cycles. So you should only do it once, and cache the list of available CPU capabilities.</p><h2><strong>Conclusion</strong></h2><p>As you can see, SIMD is full of challenges! And keep in mind, this is just one simple case of us calculating a cosine distance.</p><p>But it&#39;s worth it. When one navigates the hurdles and harnesses SIMD correctly, it unlocks incredible performance, often an <em>order of magnitude</em> higher than naive serial code.</p><p>I hope you enjoyed this whirlwind tour of SIMD’s complexities! It was quite long, but we didn’t even try analyzing port utilization for different families of instructions on x86. We haven’t looked into SME and streaming SVE modes for recent arm cores. We didn’t study the CPU frequency scaling license, even though many of those aspects are already covered by <a href="https://github.com/ashvardanian/simsimd">SimSIMD</a> kernels running under the hood of your favorite data products. So much is still ahead!</p><p>In Part 2, we&#39;ll talk about how Mojo helps with a lot of these problems! Ping me (<a href="https://x.com/ashvardanian">@ashvardanian</a>) or anyone related (<a href="https://x.com/verdagon">@verdagon</a>, <a href="https://x.com/lemire">@lemire</a>, <a href="https://x.com/clattner_llvm">@clattner_llvm</a>) if you have any questions, and join the <a href="https://discord.gg/modular">Modular Discord</a> server if you have special requests for Part 2!</p></figure></div></div></div></div>
  </body>
</html>
