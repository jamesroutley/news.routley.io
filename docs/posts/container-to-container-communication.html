<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.miketheman.net/2021/12/28/container-to-container-communication/">Original</a>
    <h1>Container-to-Container Communication</h1>
    
    <div id="readability-page-1" class="page"><article id="post-747">

<div>
<h2>Question ‚ùì</h2>
<p>In a containerized world, is there a <strong>material difference</strong> between communicating over local network TCP vs local <a href="https://en.wikipedia.org/wiki/Unix_domain_socket">Unix domain sockets</a>?</p>
<p>Given an application with more than a single container that need to talk to each other, is there an <strong>observable</strong> difference in latency/throughput when using one inter-component communication method over another from an end-users‚Äô perspective?</p>
<hr/>
<div><h2>Background üåÜ</h2>
<p>There‚Äôs <a href="https://lists.freebsd.org/pipermail/freebsd-performance/2005-February/001143.html">this excellent write-up on the comparison back in 2005</a>, and many things have changed since then, especially around the optimizations in the kernel and networking stack, along with the container runtime that is usually abstracted away from the end user‚Äôs concerns.
<a href="https://redis.io/topics/benchmarks">Redis benchmarks from a few years ago</a> also point out significant improvements using Unix sockets when the server and benchmark are co-located.</p>
<p>There‚Äôs other studies out there that have their own performance comparisons, and produce <a href="https://www.researchgate.net/figure/Performance-Comparison-of-TCP-vs-Unix-Domain-Sockets-as-a-Function-of-Message-Size_fig3_221461399">images like these</a> ‚Äì and every example is going to have its own set of controls and caveats.</p>
<p>I wanted to use a common-ish scenario: a web service running on <strong>cloud infrastructure I don‚Äôt own</strong>.</p>
</div>
<div><h2>Components üß©</h2>
<p>For the experiment, I chose this set of components:</p>
<ul>
<li><a href="https://nginx.org/">nginx</a> (web server) ‚Äì terminate SSL, proxy requests to upstream web server</li>
<li><a href="https://gunicorn.org/">gunicorn</a> (http server) ‚Äì speaks HTTP and WSGI protocol, runs application</li>
<li><a href="https://www.starlette.io/">starlette</a> (python application framework) ‚Äì handle request/response</li>
</ul>
</div>
<figure><a href="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/components-1.png?ssl=1"><img loading="lazy" width="660" height="262" src="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/components-1.png?resize=660%2C262&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/components-1.png?w=799&amp;ssl=1 799w, https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/components-1.png?resize=300%2C119&amp;ssl=1 300w, https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/components-1.png?resize=768%2C305&amp;ssl=1 768w" sizes="(max-width: 660px) 100vw, 660px" data-recalc-dims="1"/></a><figcaption><em>components</em></figcaption></figure>
<div><p>I considered using <a href="https://fastapi.tiangolo.com/">FastAPI</a> for the application layer ‚Äì but since I didn‚Äôt need any of those features, I didn‚Äôt add it, but it‚Äôs a great framework ‚Äì check it out!</p>
<p>As <code>gunicorn</code> server runs the <code>starlette</code> framework and the custom application code, I will be referring to them as a single component later as &#34;app&#34;, as the tests I‚Äôm comparing is the behavior between <code>nginx</code> and the &#34;app&#34; layer, using overall user-facing latency and throughput as the main result.</p>
</div>
<div><h3>nginx üåê</h3>
<p>nginx is awesome. Really powerful, and has many built-in features, highly configurable. Been using it for years, and it‚Äôs my go-to choice for a reliable web server.</p>
<p>For our purposes, we need an external port to listen for inbound requests, and a stanza to proxy the requests to the upstream application server.</p>
<p>You might ask: Why use nginx at all, if Gunicorn can terminate connections directly?
Well, there‚Äôs often a class of problems that nginx is better suited at handling rather than a fully-fledged Python runtime ‚Äì examples include static file serving (<code>robots.txt</code>, <code>favicon.ico</code> et. al.) as well as caching, header or path rewriting, and more.</p>
<p>nginx is a commonly used in front of all manner of applications.</p>
</div>
<h3>Python Application üêç</h3>
<p>To support the testing of a real-world scenario, I‚Äôm creating a JSON response, as that‚Äôs how most web applications communicate today. This often incurs some serialization overhead in the application.</p>
<p>I took the <a href="https://www.starlette.io/#example">example from starlette</a> and added a couple of tweaks to emit the current timestamp and a random number. This prevents any potential caching occurring in any of the layers and polluting the experiment.</p>
<p>Here‚Äôs what the main request/response now looks like:</p>
<div><pre><code>async def homepage(request):
    return JSONResponse(
        {
            &#34;hello&#34;: &#34;world&#34;,
            &#34;utcnow&#34;: datetime.datetime.utcnow().isoformat(),
            &#34;random&#34;: random.random(),
        }
    )
</code></pre>
<p>A response looks like this:</p>
<pre><code>{
  &#34;hello&#34;: &#34;world&#34;,
  &#34;utcnow&#34;: &#34;2021-12-27T00:31:42.383861&#34;,
  &#34;random&#34;: 0.5352573557347882
}
</code></pre>
<p>And while there are ways to improve JSON serialization speed, or tweak the Python runtime, I wanted to keep the experiment with defaults, since the point isn‚Äôt about maximizing total throughput, rather seeing the difference between the architectures.</p>
<h3>Cloud Environment ‚òÅÔ∏è</h3>
<p>For this experiment, I chose <a href="https://aws.amazon.com/ecs/">Amazon Elastic Container Service</a> (ECS) with <a href="https://aws.amazon.com/fargate/">AWS Fargate</a> compute. These choices provide a way to construct all the pieces needed in a repeatable fashion in the shortest amount of time, and abstract a lot of the infra concerns.
To set everything up, I used <a href="https://aws.github.io/copilot-cli/">AWS Copilot CLI</a>, an open-source tool that does even more of the heavy lifting for me.</p>
<p>The Copilot Application type of <a href="https://aws.github.io/copilot-cli/docs/manifest/lb-web-service/">Load Balanced Web Service</a> will create an <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">Application Load Balancer</a> (ALB), which is the main external component outside my application stack, but an important one for actual scaling, SSL termination at the edge, and more.
For the sake of this experiment, we assume (possibly incorrectly!) that ALBs will perform consistently for each test.</p>
</div>
<div><h2>Architectures üèõ</h2>
<p>Using containers, I wanted to test multiple architecture combinations to see which one proved the &#34;best&#34; when it came to user-facing performance.</p>
<h3>Example 1: &#34;tcp&#34;</h3>
</div>
<figure><a href="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/tcp-1.png?ssl=1"><img loading="lazy" width="660" height="262" src="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/tcp-1.png?resize=660%2C262&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/tcp-1.png?w=968&amp;ssl=1 968w, https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/tcp-1.png?resize=300%2C119&amp;ssl=1 300w, https://i0.wp.com/www.miketheman.net/wp-content/uploads/2021/12/tcp-1.png?resize=768%2C305&amp;ssl=1 768w" sizes="(max-width: 660px) 100vw, 660px" data-recalc-dims="1"/></a></figure>
<div><p>The communication between nginx container and the app container takes places over the dedicated network created by the Docker runtime (or <a href="https://github.com/containernetworking/cni">Container Network Interface</a> in Fargate).
This means there‚Äôs TCP overhead between nginx and the app ‚Äì but is it significant? Let‚Äôs find out!</p>
<h3>Example 2: &#34;sharedvolume&#34;</h3>
</div>
<figure><a href="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?ssl=1"><img loading="lazy" width="660" height="213" src="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?resize=660%2C213&amp;ssl=1" alt="" srcset="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?resize=1024%2C331&amp;ssl=1 1024w, https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?resize=300%2C97&amp;ssl=1 300w, https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?resize=768%2C248&amp;ssl=1 768w, https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/sharedvol-1.png?w=1187&amp;ssl=1 1187w" sizes="(max-width: 660px) 100vw, 660px" data-recalc-dims="1"/></a></figure>
<div><p>Here we create a shared volume between the nginx container and the app container. Then we use a Unix domain socket to communicate between the containers using the shared volume.</p>
<p>This architecture maintains a separation of concerns between the two components, which is generally a good practice, so as to have a single essential process per container.</p>
<h3>Example 3: &#34;combined&#34;</h3>
</div>
<figure><a href="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/combined-1.png?ssl=1"><img loading="lazy" width="660" height="305" src="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/combined-1.png?resize=660%2C305&amp;ssl=1" alt="" srcset="https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/combined-1.png?w=975&amp;ssl=1 975w, https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/combined-1.png?resize=300%2C139&amp;ssl=1 300w, https://i1.wp.com/www.miketheman.net/wp-content/uploads/2021/12/combined-1.png?resize=768%2C355&amp;ssl=1 768w" sizes="(max-width: 660px) 100vw, 660px" data-recalc-dims="1"/></a></figure>
<div><p>In this example, we combine both nginx and app in a single container, and use local Unix sockets within the container to communicate.</p>
<p>The main difference here is that we add a process supervisor to run both nginx and app runtimes ‚Äì which some may consider an anti-pattern. I‚Äôm including it for the purpose of the experiment, mainly to uncover if there‚Äôs performance variation between a <strong>local</strong> volume and a <strong>shared</strong> volume.</p>
<p>This approach simulates what we‚Äôd expect in a single &#34;server&#34; scenario ‚Äì where a traditional instance (hardware or virtual) runs multiple processes and all have some access to a local shared volume for inter-process communication (IPC).</p>
<p>To make this a fair comparison, I‚Äôve also doubled the CPU and memory allocation.</p>
<h2>Copilot ‚úàÔ∏è</h2>
<p>Time to get off the ground.</p>
<p>Copilot CLI assumes you already have an app prepared in a <code>Dockerfile</code>. The <a href="https://aws.github.io/copilot-cli/docs/getting-started/first-app-tutorial/#step-2-download-some-code-to-deploy">Quickstart has you clone a repo</a> with a sample app ‚Äì so instead I‚Äôve created a <code>Dockerfile</code> for each of the architectures, along with a <code>docker-compose.yml</code> file for local orchestration of the components.</p>
<p>Then I‚Äôll be able to launch and test each one in AWS with its own isolated set of resources ‚Äì VPC, networking stack, and more.</p>
<p>I‚Äôm not going into all the details of how to install Copilot and launch the services, for that, read the Copilot CLI documentation (linked above), and read <a href="https://github.com/miketheman/ecs-network-perf">the experiment code</a>.</p>
<p>This test is using AWS Copilot CLI v1.13.0.</p>
<h2>Test Protocol üî¨</h2>
<p>There‚Äôs an ever-growing list of tools and approaches to benchmark web request/response performance.</p>
<p>For the sake of time, I‚Äôll use a single one here, to focus on the comparison of the server-side architecture performance.</p>
<p>All client-side requests will be performed from an <a href="https://aws.amazon.com/cloudshell/">AWS CloudShell</a> instance running in the same AWS Region as the running services (<code>us-east-1</code>) to isolate a lot of potential network chatter. It‚Äôs not a perfect isolation of potential variables, but it‚Äôll have to do.</p>
<p>To baseline, I ran each test locally (see later).</p>
<h3>Apache Bench</h3>
<p><a href="https://httpd.apache.org/docs/current/programs/ab.html">Apache Bench</a>, or <code>ab</code>, is a common tool for testing web endpoints, and is not specific to Apache <code>httpd</code> servers. I‚Äôm using: <code>Version 2.3 &lt;$Revision: 1879490 $&gt;</code></p>
<p>I chose single concurrency, and ran 1,000 requests. I also ignore variable length,
as the app can respond with a variable-length random number choice, and <code>ab</code>
considers different length responses a failure unless specified.</p>
<pre><code>ab -n 1000 -c 1 -l http://service-target....
</code></pre>
<p>Each test should take less than 5 seconds.</p>
<p>The important stats I‚Äôm comparing are:</p>
<ul>
<li>Requests per second (mean) ‚Äì higher is better</li>
<li>Time per request (mean) ‚Äì lower is better</li>
<li>Duration at 99th percentile. 99% of all requests completed within (milliseconds) ‚Äì lower is better</li>
</ul>
<p>To reduce variance, I also &#34;warmed up&#34; the container by running the test for a larger amount of requests</p>
<h4>Local Test</h4>
<p>To establish a baseline, I ran the same benchmark test against the local services.
Using <a href="https://www.docker.com/products/docker-desktop">Docker Desktop</a> 4.3.2 (72729) on macOS.
These aren‚Äôt demonstrative of a real user experience, but provides a sense of performance before launching the architectures in the cloud.</p>
<table>
<thead>
<tr>
<th>arch</th>
<th>reqs per sec</th>
<th>ms per req</th>
<th>99th pctile</th>
</tr>
</thead>
<tbody>
<tr>
<td>tcp (local)</td>
<td>679.77</td>
<td>1.471</td>
<td>2</td>
</tr>
<tr>
<td>sharedvolume (local)</td>
<td><strong>715.62</strong></td>
<td><strong>1.397</strong></td>
<td>2</td>
</tr>
<tr>
<td>combined (local)</td>
<td>705.55</td>
<td>1.871</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>In the local benchmark, the clear loser is the <code>tcp</code> architecture, and the <code>sharedvolume</code> has a slight edge on <code>combined</code> ‚Äì but not a huge win.
No real difference in the 99th percentiles ‚Äì requests are being served in under 2ms.</p>
<p>This shows that the shared resources for the <code>combined</code> architecture are near the performance of the <code>sharedvolume</code> ‚Äì possibly due to Docker Desktop‚Äôs bridging and network abstraction. A better comparison might be tested on a native Linux machine.</p>
<h4>Remote Test</h4>
<p>Once I ran through the setup steps using Copilot CLI to create the environment and services, I performed the same <code>ab</code> test, and collected the results in this table:</p>
<table>
<thead>
<tr>
<th>arch</th>
<th>reqs per sec</th>
<th>ms per req</th>
<th>99th pctile</th>
</tr>
</thead>
<tbody>
<tr>
<td>tcp (aws)</td>
<td><strong>447.57</strong></td>
<td><strong>2.234</strong></td>
<td>5</td>
</tr>
<tr>
<td>sharedvolume (aws)</td>
<td>394.55</td>
<td>2.535</td>
<td>6</td>
</tr>
<tr>
<td>combined (aws)</td>
<td>428.60</td>
<td>2.333</td>
<td><strong>4</strong></td>
</tr>
</tbody>
</table>
<p>With the remote tests, minor surprise that the <code>combined</code> service performed better than the <code>sharedvolume</code> service, as in the local test it performed worse.</p>
<p>The bigger surprise was to find that the <code>tcp</code> architecture wins slightly over the socket-based architectures.</p>
<p>This could be due to the way ECS Fargate uses the <a href="https://firecracker-microvm.github.io">Firecracker microvm</a>, and has tuned the network stack to perform faster than using a shared socket on a volume when communicating between two containers on the same host machine. The best part is ‚Äì as a consumer of a utility, I don‚Äôt care, as long as it‚Äôs performing well!</p>
<h4>ARM/Graviton Remote Test</h4>
<p>With the Copilot manifest defaults for the <a href="https://aws.github.io/copilot-cli/docs/manifest/lb-web-service/#platform">Intel x86 platform</a>, let‚Äôs also test the performance on the <code>linux/arm64</code> platform (Graviton2, probably).</p>
<p>For this to work, I had to rebuild the <code>nginx</code> sidecars manually, as Copilot doesn‚Äôt yet build&amp;push sidecar images. I also had to update the <code>manifest.yml</code> to set the desired platform, and deploy the service with <code>copilot svc deploy ...</code>. (The <code>combined</code> version needed some <code>Dockerfile</code> surgery too‚Ä¶)</p>
<p>Results:</p>
<table>
<thead>
<tr>
<th>arch</th>
<th>reqs per sec</th>
<th>ms per req</th>
<th>99th pctile</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>tcp (aws/arm)</strong></td>
<td><strong>475.03</strong></td>
<td><strong>2.105</strong></td>
<td>3</td>
</tr>
<tr>
<td>sharedvolume (aws/arm)</td>
<td>451.71</td>
<td>2.214</td>
<td>4</td>
</tr>
<tr>
<td>combined (aws/arm)</td>
<td>433.94</td>
<td>2.304</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>We can see that <strong>all</strong> the stats are better on the Graviton architecture, lending some more credibility to studies done by other <a href="https://www.infoq.com/articles/arm-vs-x86-cloud-performance/">benchmark posts</a> and <a href="https://d1.awsstatic.com/whitepapers/aws-graviton-performance-testing.pdf">papers</a>.</p>
<p><em>Aside:</em> The <code>linux/arm64</code>-based container images were tens of megabytes smaller, so if space and network <code>pull</code> time is a concern, these will be a few microseconds faster.</p>
<h4>Other Testing Tools</h4>
<p>If you‚Äôre interested in performing longer tests, or emulating different user types, check out some of these other benchmark tools I considered and didn‚Äôt use for this experiment:</p>
<ul>
<li>Python ‚Äì https://locust.io/ https://molotov.readthedocs.io/</li>
<li>JavaScript ‚Äì https://k6.io/</li>
<li>Golang ‚Äì https://github.com/rakyll/hey</li>
<li>C ‚Äì https://github.com/wg/wrk</li>
</ul>
<p>There‚Äôs also plenty of vendors that build out extensive load testing platforms ‚Äì I‚Äôm not covering any of them here. If you run a test with these, would definitely like to see your results!</p>
<h2>Conclusions üí°</h2>
<p>Using the Copilot CLI wasn‚Äôt without some missteps ‚Äì the team is hard at work improving the documentation, and are pretty responsive in both their GitHub Issues and Discussions, as well as their Gitter chat room ‚Äì always helpful when learning a new framework.
Once I got the basics, being able to establish a reproducible stack is valuable to the experimentation process, as I was able to provision and tear down the stack easily, as well as update with changes relatively easily.</p>
<p><strong>Remember:</strong> these are micro-benchmarks, on not highly-tuned environments or real-world workloads. This test was designed to test a very specific type of workload, which may change as more concurrency is introduced, CPU or memory saturation is achieved, auto-scaling of application instances comes into play, and more.</p>
<p><em>Your mileage may vary.</em></p>
<p>When I started this experiment, I assumed the winner would be a socket-based communication architecture (<code>sharedvol</code> or <code>combined</code>), from existing literature, and it also made sense to me. The overhead of creating TCP packets between the processes would be eliminated, and thus performance would be better.</p>
<p>However, in these benchmarks, I found that using the TCP communication architecture performs best, possibly due to optimizations beyond our view in the underlying stack. This is precisely what I want from an infrastructure vendor ‚Äì for them to figure out how to optimize performance without having to re-architect an application to perform better in a given deployment scenario.</p>
<p><strong>The main conclusion I‚Äôve drawn is</strong>: Using TCP to communicate between containers is best, as it affords the most flexibility, follows established patterns, and performs slightly better than the alternatives in a real(ish) world scenario.
And if you can, use Graviton2 (ARM) CPU architecture.</p>
<p>Go forth, test your own scenarios, and let me know what you come up with.
(Don‚Äôt forget to delete your resource when done!! üí∏ )</p>
</div>
 </div>

</article></div>
  </body>
</html>
