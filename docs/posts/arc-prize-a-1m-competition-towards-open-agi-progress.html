<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arcprize.org/blog/launch">Original</a>
    <h1>ARC Prize – a $1M&#43; competition towards open AGI progress</h1>
    
    <div id="readability-page-1" class="page"><div>


<p><strong>A $1,000,000+ competition towards open AGI progress.</strong></p>

<p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/2avWAHXUXXs?si=CTewe9L_Mp8XsDSo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media;" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p>AGI progress has stalled. New ideas are needed.</p>

<h4 id="intelligence-vs-memorization">Intelligence vs Memorization</h4>

<p>Modern AI (LLMs) have shown to be great memorization engines. They are able to memorize high-dimensional patterns in their training data and apply those patterns into adjacent contexts. This is also how their apparent reasoning capability works. LLMs are not actually reasoning. Instead they memorize reasoning patterns and apply those reasoning patterns into adjacent contexts. But they cannot generate new reasoning based on novel situations.</p>

<p>More training data lets you &#34;buy&#34; performance on memorization based benchmarks (MMLU, GSM8K, ImageNet, GLUE, etc.) But memorization alone is not general intelligence. General intelligence is the ability to efficiently acquire new skills.</p>

<p>More scale will not enable LLMs to learn new skills. We need new architectures or algorithms that enable AI systems to learn at test time. This is how humans are able to adapt to novel situations.</p>

<p>Beyond LLMs, for many years, we&#39;ve had AI systems that can beat humans at poker, chess, go, and other games. However, no AI system trained to succeed at one game can simply be retrained toward another. Instead researchers have had to re-architect and rebuild entirely new systems per game.</p>

<p>This is a failure to generalize.</p>

<p>Without this capability, AI will forever be rate-limited by the human general intelligence in the loop. We want AGI that can discover and invent alongside humans to push humanity forward.</p>

<p>Given the success and proven economic utility of LLMs over the past 4 years, the above may seem like extraodinary claims. Strong claims require strong evidence.</p>

<hr/>

<h3 id="arc-agi">ARC-AGI</h3>

<p>Introduced by <a href="https://x.com/fchollet" target="_blank">François Chollet</a> in his influencial paper &#34;<a href="https://arxiv.org/abs/1911.01547" target="_blank">On the Measure of Intelligence</a>&#34;, <a href="https://arcprize.org/arc">ARC-AGI</a> is the only AI eval which measures general intelligence: a system that can efficiently acquire new skills and solve novel, open-ended problems.</p>

<p>ARC-AGI was created in 2019 and the state-of-the-art (SOTA) high score was 20%. Today, only 34%.</p>

<p>Yet humans - even children - score 85% to 100%.</p>

<p>ARC-AGI is easy for humans and impossible for modern AI.</p>

<p>Most AI benchmarks rapidly saturate to human performance-level because they test only for memorization, which is something AI is superhuman at.</p>

<p>ARC-AGI is not saturating, in fact current pace is slowing down. It was designed to resist memorization and has proven extremely challenging for both the largest foundational transformer models as well as bespoke AI systems designed to defeat ARC-AGI.</p>

<p><img src="https://arcprize.org/media/images/ai-benchmarks.png" alt="ARC-AGI Benchmark Comparison"/></p>

<p>A solution to ARC-AGI, at a minimum, opens up a completely new programming paradigm where programs can perfectly and reliably generalize from an arbitrary set of priors. We also believe a solution is on the critical path towards AGI</p>

<p><img src="https://arcprize.org/media/images/ThreeARCTasks.png" alt="3 ARC-AGI Tasks"/></p>

<hr/>

<h3 id="open-source-agi-progress">Open Source AGI Progress</h3>

<p>If you accept new ideas are needed, let&#39;s consider how to increase the rate of new ideas. Unfortunately, trends in AI are going the wrong way.</p>

<h4 id="closed-vs-open">Closed vs Open</h4>

<p>Starting with the GPT-4 release, frontier AGI progress has gone closed source. The <a href="https://arxiv.org/abs/2303.08774" target="_blank">GPT-4 technical report</a> surprisingly contains no technical details. OpenAI said &#34;competitive&#34; reasons were the first reason why. Google&#39;s <a href="https://arxiv.org/abs/2312.11805" target="_blank">Gemini technical report</a> also contains no technical details on the long context window frontier innovation.</p>

<p>LLMs have also shifted the majority of research attention away from new architectures and new algorithms. Over <a href="https://www.sequoiacap.com/article/ai-ascent-2024/" target="_blank">$20B was deployed</a> to non-general AI companies in 2023 and many frontier DeepMind researchers were restaffed to Gemini (in order to compete with OpenAI.)</p>

<p>Leading labs have strong incentives to loudly claim, &#34;scale is all you need,&#34; and, &#34;don&#39;t try to compete with us on frontier research,&#34; even though they all quietly believe new ideas are needed to reach AGI. Their bet is they can discover all the necessary new ideas within their labs.</p>

<h4 id="llm-history">LLM History</h4>

<p>But let&#39;s look at the history of LLMs. Specifically the transformer architecture. Transformers emerged many years downstream of machine translation research (e.g., English to Spanish.)</p>

<ul>
  <li><strong>2014</strong>: Sutskever et. al. (Google) published <a href="https://arxiv.org/abs/1409.3215" target="_blank">Seq2Seq Learning</a> using RNNs and CNNs for variable length input vs output (English and Spanish words are not the same length.)</li>
  <li><strong>2016</strong>: Bahdanau et. al. (Jacobs University) popularized <a href="https://arxiv.org/pdf/1409.0473" target="_blank">the concept of &#34;attention&#34;</a> so a system could consider different parts of the input to predict output (English adjectives come before nouns, Spanish after.)</li>
  <li><strong>2017</strong>: Vaswani et. al. (Google) realized <a href="https://arxiv.org/pdf/1706.03762" target="_blank">&#34;attention is all you need&#34;</a>, dropping RNNs and CNNs, optimizing the architecture, enabling new scale</li>
  <li><strong>2018</strong>: Radford et. al. (OpenAI) <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">created GPT-2</a> built on top of the transformer architecture at frontier scale, showing emergent capabilities</li>
</ul>

<p>The story of the transformer is the story of science. Researchers in different labs and teams publish and build on top of each other&#39;s work.</p>

<p>While it is possible one lab could discover AGI alone, it is highly unlikely. The global chance of AGI discovery has decreased and will keep decreasing if we accepting this as status quo.</p>

<h4 id="progress">Progress</h4>

<p>I have spoken with many young students and would-be researchers over the past year. Many are depressed. There is a sense of dread that everything has been figured out already. But this is not true! The AI ecosystem is intentioanlly telling a partial-truth to boost their relative competitive positions to the detriment of actual progress towards AGI.</p>

<p>Worse, the inaccurate &#34;scale is all you need&#34; belief is now influencing the AI regulatory environment. Regulators are considering roadblocks to frontier AI research under the wrong assumption that AGI is imminent. The truth is no one knows how to build AGI.</p>

<p>We should be trying to incentivize new ideas, not slow them down. The internet and open source are the strongest innovation engines the world has ever seen.</p>

<p>By incentivizing open source we increase the rate of new ideas, increasing the chance we discover AGI, and ensure those new ideas are widely distributed to establish a more even playing field between small and large AI companies.</p>

<p>We hope ARC Prize can help counterbalance these trends.</p>

<hr/>

<p><img src="https://arcprize.org/media/images/m-and-f.jpg" alt="François Chollet and Mike Knoop"/></p>

<h3 id="arc-prize">ARC Prize</h3>

<p>Announcing ARC Prize, a $1,000,000+ prize pool competition to beat and open-source a solution to the ARC-AGI eval.</p>

<p>Hosted by <a href="https://mikeknoop.com" target="_blank">Mike Knoop</a> and <a href="https://fchollet.com/" target="_blank">François Chollet</a>. Presented by Infinite Monkey and Lab42.</p>

<h4 id="arc-prize-goals">ARC Prize Goals</h4>

<ol>
  <li>Increase the number of people working on frontier AGI research.</li>
  <li>Popularize an objective measure of AGI progress.</li>
  <li>Solve ARC-AGI and learn something new about the nature of intelligence.</li>
</ol>

<h4 id="get-started">Get Started</h4>

<p>Ready to make the first significant leap towards AGI in years? No matter who you are, where you come from, what you do for a living, you are welcome to join this competition. New ideas might come from anywhere. Possibly you?</p>



<p>Find competition format and prize details on <a href="https://arcprize.org/competition">ARC Prize 2024 here</a>.</p>

<p>For more information on how to get started solving ARC-AGI <a href="https://arcprize.org/guide">visit the guide.</a></p>

<p>To learn more how ARC-AGI measures general intelligence <a href="https://arcprize.org/arc">visit ARC-AGI.</a></p>

<p>Stay updated on ARC Prize progress and SOTA solutions on <a href="https://x.com/arcprize" target="_blank">X/Twitter</a>, <a href="https://www.youtube.com/@arcprize" target="_blank">YouTube</a>, <a href="https://arcprize.ck.page/bc80575d89" target="_blank">Email</a>, and <a href="https://discord.gg/9b77dPAmcA" target="_blank">Discord</a>. You can also contact us at team@arcprize.org.</p>

		</div></div>
  </body>
</html>
