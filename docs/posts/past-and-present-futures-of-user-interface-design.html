<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.datagubbe.se/futui/">Original</a>
    <h1>Past and Present Futures of User Interface Design</h1>
    
    <div id="readability-page-1" class="page"><div>



<p><b>Revolutionizing the desktop since 1975</b></p>
<p><i>Spring 2025</i></p>

<p>
In 1968, Douglas Engelbart demonstrated a computer system called the oN-Line System, or NLS. The NLS is the source of a lot of computer firsts. Among other things, Engelbart showed video conferencing, collaborative text editing, embedded graphics, copying and pasting, and hypertext - all of it accessible through a mouse and keyboard. This event has since become known as <a href="https://www.youtube.com/watch?v=yJDv-zdhzMY">&#34;The Mother of All Demos&#34;</a>.
</p>

<p>
Recently, Amelia Wattenberger published <a href="https://wattenberger.com/thoughts/our-interfaces-have-lost-their-senses">an article with ideas about a possible future for user interfaces</a>.
In short, she asks for interfaces with more tactile &#34;friction&#34;, praises multi-modality and suggest variations in both input devices and feedback options. We&#39;ll return to her text, but first, let&#39;s look at some other visions of future user interfaces.
</p>

<h3>Xerox Alto</h3>

<p>
<a href="https://www.datagubbe.se/futui/smalltalk-76.png"><img src="https://www.datagubbe.se/futui/smalltalk-76_crop.png" alt="A screenshot of Smalltalk-76, showing a paint program that looks a lot like MSPaint."/></a>
</p>

<p>
The Xerox Alto wasn&#39;t as much a vision as an actual product. Inspired by Engelbart and others, a team of researchers at Xerox PARC set about creating computers for the office of the future. This resulted in the Alto, a machine that would probably be recognizable to a lot of present day computer users.
The Alto was the origin of Smalltalk, a programming language with its own GUI environment - the one that inspired Steve Jobs to commission the Lisa. Looking at that GUI now, it&#39;s baffling how little desktop interfaces have changed since then, almost 50 years ago.
</p>

<p>
<img src="https://www.datagubbe.se/futui/alto.jpg" alt="A man sitting in front of a Xerox Alto computer."/>
</p>

<p>
<a href="https://www.youtube.com/watch?v=M0zgj2p7Ww4">In a commercial for the Alto</a>, we meet a man - some kind of upper middle management, presumably - going about his daily business. He works in a spacious private office and, using the Alto, he can read and send email and produce laser printouts. Eventually, the Alto conjures up a high resolution image of flowers. The man wonders why, and the computer replies - with text on screen - that it&#39;s the man&#39;s wedding anniversary. &#34;I forgot,&#34; says the man, to which the Alto replies, &#34;It&#39;s okay, we&#39;re only human.&#34;
</p>

<p>
Despite being a very advanced system for its time, the Alto was of course incapable of such banter - and yet, the commercial&#39;s producers saw fit to include it, in order to spice things up.
</p>


<h3>Sun Starfire</h3>

<p>
Commercials like that for the Alto always feel a bit stuffy and contrived. Upping the ante in several ways, <a href="https://www.youtube.com/watch?v=w9OKcKisUrY">Sun Microsystem&#39;s 1994 commercial for Starfire</a> (an imaginary future computer), is a cringe-filled orgy in stilty acting and terrible writing. The protagonist once more seems to be upper middle management, and she&#39;s working on a presentation of an electric car. <b><i>Future!</i></b> She also engages in a bunch of strange and/or morally questionable activities. We&#39;ll probably never know why the producers decided to give her a cold, or why she spends so much of her time spying on co-workers using the live CCTV function on her expensive computer. But I digress.
</p>

<p>
The commercial presents several concepts that - like in Engelbart&#39;s demo - are now commonplace. Tablet computers, video conferencing, touch screens, AI-augmented image and video editing, and instant scanning (today we&#39;d probably just photograph the document with our smartphone). Granted, imminent mainstream adoption of such functions was fairly obvious in 1994.
</p>

<p>
<img src="https://www.datagubbe.se/futui/starfire.jpg" alt="A woman standing in front of a large, curved touchscreen. The screen wraps around the desk. A computer mouse is visible, but no keyboard."/>
</p>

<p>
Our hero is working on her presentation in a grotesquely spacious private office, which is probably necessary considering the sheer size of the Starfire. Actually operating it includes a lot of swiping and tapping on its humongous screen - mixed with talking to it, of course.
</p>

<h3>The Office of the Future</h3>

<p>
There are many other, similar, visions of our digital future. Plenty of them include touch interfaces and speaking to our computers. In fact, many of these visions are curiously reminiscent of how computers work in Star Trek. There&#39;s some vague touch-type interface (LCARS), but you can also speak to the computer - which will reply in conversational English. And of course there&#39;s video conferencing.
</p>

<p>
<img src="https://www.datagubbe.se/futui/startrek3.jpg" alt="A still from an episode of Star Trek. Captain Janeway and some of her crew are using the LCARS computer interface. In the background, a video conference is going on."/>
</p>

<p>
Some of the more outlandish suggestions in the Starfire commercial (such as a roll of film slowly pouring a texture onto a 3D model) are clearly intended to raise eyebrows, rather than reflect actually working interfaces. Other concepts are recurring in visions of future UI:s, like giant touch displays and the constant desire to talk to our machines.
</p>

<p>
These suggestions also seem genuine, which brings us to Wattenberger&#39;s recent article.
</p>

<h3>Tactile computing</h3>

<p>
Wattenberger touches on some relevant issues, especially regarding the prevalence of touchscreens and their inherent lack of tactile feedback. Alas, I think the reasoning presented conflates a desire for &#34;genuine&#34; experiences with something that can be competitive in a global, mass market economy. The former doesn&#39;t necessarily <i>disappear</i> because of the latter, but &#34;friction&#34; isn&#39;t something that&#39;s going to sell a lot of software.
</p>

<p>
To riff of Wattenberger&#39;s own example: Most doughs aren&#39;t kneaded by humans. We have machines for that - even in our homes - and the majority of our bread is produced at an industrial scale. That doesn&#39;t mean we can&#39;t knead our own dough if we want to, or that an artisanal bakery can&#39;t do well in an affluent neighborhood.
</p>

<p>
Computers are successful not because they introduce friction, but because they <i>reduce</i> it. If friction was a selling point, we&#39;d give up spreadsheet software today and go back to the tactile way - using paper worksheets.
</p>

<p>
We&#39;re still allowed to dream, of course. I <a href="https://www.datagubbe.se/cosy/">absolutely agree</a> that more aspects of everyday life should be more tactile than an unyielding flatscreen. Most of that already exists, in the form of actual push buttons, keyboards, joysticks, levers, sliders and turning knobs. It&#39;s a question of hardware rather than software. Which is where the harsh reality of capitalism comes in and ruins everything: if it&#39;s cheaper with a touchscreen, and at all feasible, it&#39;s going to be a touchscreen. Not just in regular smart devices, but in cars, elevators, coffee machines and stovetops as well. Given this, I&#39;d rather we spent some energy on making those interfaces as good as humanly possible, instead of trying to reinvent things that already work well.
</p>

<p>
The interface suggestions made toward the end of the text is what usually crops up when talking about revolutionizing the way we work with computers: an infinite canvas and - harking back to both the Alto, Starfire and Star Trek - some voice control.
</p>

<h3>Design and Reality</h3>

<p>
The infinite canvas isn&#39;t bad, necessarily, but the unstructured sprawl of mixed information they often lead to seems to offer little value to the vast majority of computer users. When adding structure and bounds to the canvas, the idea does seem to appeal to a wider audience: spreadsheets are very popular. But there&#39;s a limited amount of spreadsheet-type tasks, and at some point we need a different kind of interface and another way to structure our data.
Wouter van Oortmerssen (of CryEngine and WebAssembly fame) released <a href="https://strlen.com/treesheets/">TreeSheets</a> some 16 years ago. While it&#39;s an interesting concept, it doesn&#39;t seem to have caught on.
</p>

<p>
Traits like structure and relationships also come into play when we work with information. We seem to need a mental model - a spatial psychological abstraction - of our data, to know from which angle to attack it. Such a model can be hard to build when everything exists in a vast 2D space without clear boundaries. That&#39;s where things like tree views, lists and grids come into play. Most users aren&#39;t performing the kind of work that lends itself to an infinite canvas, anyway: File an order, take the next call, file another one.
In short: if infinite canvases really were that great, I think we&#39;d see a lot more of them.
</p>

<p>
Enormous computer screens, like that of the Starfire, are now cheaper than ever to buy. Touch technology is everywhere. Thankfully, manufacturers have realized that the combination is untenable: Imagine having to raise your arm to swipe, pinch and tap across an ultra-wide screen several times per minute. Touch works best on small surfaces, even if it <i>looks impressive</i> on a bigger screen. For a while, manufacturers wanted us to buy touchscreen laptops. I haven&#39;t seen one of those in the wild for several years now - but pure tablets are commonplace. It seems as if there is a keyboard and mouse around, we pick them over touch for most desktop-type tasks.
</p>

<p>
Voice control always adds a certain futuristic vibe to a demo. In reality, it seems we&#39;re stuck with several circumstances making it effectively pointless. Our best LLM:s are a far cry from the AI:s in Star Trek, but even for simpler tasks use cases appear limited. I work with developers and other IT professionals all day, hang out with similar people on my free time, ride a commute jam packed with project managing office dwellers, and regularly find myself in big cities - but I haven&#39;t heard a single &#34;Hey, Siri&#34; for years. Like with the scarcity of touchscreen laptops, I&#39;m sure that means <i>something</i>.
</p>

<p>
Infinite canvases, laissez-faire touchscreens and voice control seem very <i>designer-y</i> to me. By that I mean they&#39;re reflecting how a graphical designer would like to work with a computer at certain points in their creative process.
</p>

<p>
But we shouldn&#39;t build entire paradigms, or even just individual interfaces, based on the assumption that everyone else is using computers the same way we ourselves do. Most people don&#39;t conceptualize graphic design ideas or freestyle pretend corporate presentations. Some are controlling an industrial process, editing a feature film, designing an airplane or writing code. Most, however, are probably filing a customer complaint, ordering food, booking a flight, or doing some accounting.
</p>
<p>
Some people have the luxury of working in their own, private offices - or at home. Most computer users don&#39;t. They&#39;re confined to open floor plan offices, hotel lobbies, bustling trading desks, loud workshops or busy stores. Most of them probably perform tasks where speaking - even if it had been in a quiet and solitary environment - is less efficient than typing, tapping or clicking.
</p>

<p>
The same goes for multi-sensory interfaces in general. Voice control have many apparent drawbacks, but so do UI:s with audio feedback. Ever been in an open floorplan office where people have audio notifications enabled when receiving a chat message? Welcome to hell.
</p>

<p>
Even in a spacious private office, audio signals can be annoying. They interfere with music, for example, which is something a lot of people use their computers for both at work and at home. Generally, relying on a single type of feedback is a bad UI idea: Apart from poor accessibility, it reduces the user&#39;s freedom to customize their working environment. Some, like me, are annoyed by sound. Others seem to love it, and others still <i>require it</i> if they&#39;re to use a computer at all.
</p>

<h3>The Actual Future</h3>

<p>
The desktop user interface is a mainstay of computing. Bread and butter, if you will. A pointer, icons, windows, menus and buttons, controlled using a keyboard and a mouse. Ingenious simplicity.
</p>

<p>
For almost half a century now, we haven&#39;t really managed to come up with something better, and that&#39;s not for lack of trying. This fact seems to annoy a lot of people looking for a problem to solve - which every so often leads to something rather silly. Removing scrollbars from desktop programs for example, only to realize that people then don&#39;t understand there&#39;s scrollable content, and then responding by <i>cooking up some other way of doing scrolling</i> instead of just reinstating scrollbars.
</p>

<p>
Blundering about like that isn&#39;t inventing a new paradigm, no matter what individual designers claim - it&#39;s just making an existing one worse. But is there a real new paradigm around the corner? Is the desktop going to disappear? I honestly don&#39;t know, but I sincerely doubt it. It&#39;s probably just going to get slightly worse in some aspects, and hopefully slightly better in others.
</p>

<p>
Truly new paradigms do appear from time to time, mostly out of necessity. The smartphone is a great example. It needs to pack a lot of different input methods into one tiny little surface, and touch is great for that. The results aren&#39;t always optimal, but they&#39;re good enough compromises, vastly improving the flexibility of such a small device.
</p>

<p>
To me, trying to reinvent the desktop experience feels a bit like complaining about steering wheels in cars. To others, it&#39;s a great opportunity to confuse <i>change</i> with <i>improvement</i> and <i>different</i> with <i>better</i>.
With that said, I&#39;m on board with Wattenberger&#39;s wish for slightly more tactile computers. In fact, if I&#39;m being honest, I&#39;d probably want computers and computing to be more like it <i>actually</i> was in 1994, and much less like our current Starfirey world.
</p>

<p>
Since that&#39;s probably not going to happen, I instead urge UI designers to consider the powerful concepts of <i>consistency</i> and <i>familiarity</i> just a bit more often than they presently seem to do.
</p>


</div></div>
  </body>
</html>
