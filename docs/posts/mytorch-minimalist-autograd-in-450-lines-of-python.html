<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/obround/mytorch">Original</a>
    <h1>MyTorch – Minimalist autograd in 450 lines of Python</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Easily extensible autograd implemented python with pytorch API. Uses numpy to do the heavy-lifting. Implementation is very similar to pytorch (graph-based reverse-mode autodiff). It wouldn&#39;t be too tough to extend the autograd, implement <code>torch.nn</code>, and possibly run on GPU (presumably with CuPy or Numba). It would be an interesting (but useless) endeavor to rewrite <code>mytorch</code> in a low level language using BLAS library calls instead on numpy, just like pytorch.</p>

<p dir="auto"><code>mytorch</code> supports the computation of arbitrarily high derivatives for both scalars and non-scalars. Both <code>torch.autograd.backward</code> and <code>torch.autograd.grad</code> are supported.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import mytorch as torch

a = torch.tensor(3., dtype=torch.float32, requires_grad=True)
b = torch.tensor(10., dtype=torch.float32, requires_grad=True)
c = 2 + (a + b ** 2) / (a + b + a * b)

print(&#34;a =&#34;, a)
print(&#34;b =&#34;, b)
print(&#34;c = 2 + (a + b ** 2) / (a + b + a * b) =&#34;, c)

# NOTE: You could also use c.backward() to accumulate the gradients in a.grad and b.grad
dc_da, dc_db = torch.autograd.grad(c, [a, b])
# NOTE: To get higher order derivatives like below, pytorch would require ∂c/∂a and
# ∂c/∂b to be calculated with create_graph=True; mytorch does not require it
d2c_da2 = torch.autograd.grad(dc_da, [a])[0]
d2c_db2 = torch.autograd.grad(dc_db, [b])[0]
print(f&#34;∂c/∂a = {dc_da}&#34;)
print(f&#34;∂c/∂b = {dc_db}&#34;)
print(f&#34;∂²c/∂a² = {d2c_da2}&#34;)
print(f&#34;∂²c/∂b² = {d2c_db2}&#34;)"><pre><span>import</span> <span>mytorch</span> <span>as</span> <span>torch</span>

<span>a</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>3.</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float32</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>b</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>10.</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float32</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>c</span> <span>=</span> <span>2</span> <span>+</span> (<span>a</span> <span>+</span> <span>b</span> <span>**</span> <span>2</span>) <span>/</span> (<span>a</span> <span>+</span> <span>b</span> <span>+</span> <span>a</span> <span>*</span> <span>b</span>)

<span>print</span>(<span>&#34;a =&#34;</span>, <span>a</span>)
<span>print</span>(<span>&#34;b =&#34;</span>, <span>b</span>)
<span>print</span>(<span>&#34;c = 2 + (a + b ** 2) / (a + b + a * b) =&#34;</span>, <span>c</span>)

<span># NOTE: You could also use c.backward() to accumulate the gradients in a.grad and b.grad</span>
<span>dc_da</span>, <span>dc_db</span> <span>=</span> <span>torch</span>.<span>autograd</span>.<span>grad</span>(<span>c</span>, [<span>a</span>, <span>b</span>])
<span># NOTE: To get higher order derivatives like below, pytorch would require ∂c/∂a and</span>
<span># ∂c/∂b to be calculated with create_graph=True; mytorch does not require it</span>
<span>d2c_da2</span> <span>=</span> <span>torch</span>.<span>autograd</span>.<span>grad</span>(<span>dc_da</span>, [<span>a</span>])[<span>0</span>]
<span>d2c_db2</span> <span>=</span> <span>torch</span>.<span>autograd</span>.<span>grad</span>(<span>dc_db</span>, [<span>b</span>])[<span>0</span>]
<span>print</span>(<span>f&#34;∂c/∂a = <span><span>{</span><span>dc_da</span><span>}</span></span>&#34;</span>)
<span>print</span>(<span>f&#34;∂c/∂b = <span><span>{</span><span>dc_db</span><span>}</span></span>&#34;</span>)
<span>print</span>(<span>f&#34;∂²c/∂a² = <span><span>{</span><span>d2c_da2</span><span>}</span></span>&#34;</span>)
<span>print</span>(<span>f&#34;∂²c/∂b² = <span><span>{</span><span>d2c_db2</span><span>}</span></span>&#34;</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="a = tensor(3.0, requires_grad=True)
b = tensor(10.0, requires_grad=True)
c = 2 + (a + b ** 2) / (a + b + a * b)
  = tensor(4.395348787307739, requires_grad=True)
∂c/∂a = tensor(-0.5895078420767982, requires_grad=True)
∂c/∂b = tensor(0.24229313142239048, requires_grad=True)
∂²c/∂a² = tensor(0.3016086633881293, requires_grad=True)
∂²c/∂b² = tensor(0.0014338360144389717, requires_grad=True)"><pre><span>a</span> <span>=</span> <span>tensor</span>(<span>3.0</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>b</span> <span>=</span> <span>tensor</span>(<span>10.0</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>c</span> <span>=</span> <span>2</span> <span>+</span> (<span>a</span> <span>+</span> <span>b</span> <span>**</span> <span>2</span>) <span>/</span> (<span>a</span> <span>+</span> <span>b</span> <span>+</span> <span>a</span> <span>*</span> <span>b</span>)<span></span>
  <span>=</span> <span>tensor</span>(<span>4.395348787307739</span>, <span>requires_grad</span><span>=</span><span>True</span>)
∂<span>c</span><span>/</span>∂<span>a</span> <span>=</span> <span>tensor</span>(<span>-</span><span>0.5895078420767982</span>, <span>requires_grad</span><span>=</span><span>True</span>)
∂<span>c</span><span>/</span>∂<span>b</span> <span>=</span> <span>tensor</span>(<span>0.24229313142239048</span>, <span>requires_grad</span><span>=</span><span>True</span>)
∂²<span>c</span><span>/</span>∂<span>a</span>² <span>=</span> <span>tensor</span>(<span>0.3016086633881293</span>, <span>requires_grad</span><span>=</span><span>True</span>)
∂²<span>c</span><span>/</span>∂<span>b</span>² <span>=</span> <span>tensor</span>(<span>0.0014338360144389717</span>, <span>requires_grad</span><span>=</span><span>True</span>)</pre></div>
<p dir="auto">Here is a non-scalar example (with broadcasting):</p>
<div dir="auto" data-snippet-clipboard-copy-content="import mytorch as torch

a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, requires_grad=True)
b = torch.tensor([7, 8, 9], dtype=torch.float32, requires_grad=True)
# b is broadcasted
c = a + b

print(&#34;a =&#34;, a)
print(&#34;b =&#34;, b)
print(&#34;c =&#34;, c)
c.backward(torch.ones(2, 3))
print(&#34;∂c/∂a =&#34;, a.grad)
print(&#34;∂c/∂b =&#34;, b.grad)"><pre><span>import</span> <span>mytorch</span> <span>as</span> <span>torch</span>

<span>a</span> <span>=</span> <span>torch</span>.<span>tensor</span>([[<span>1</span>, <span>2</span>, <span>3</span>], [<span>4</span>, <span>5</span>, <span>6</span>]], <span>dtype</span><span>=</span><span>torch</span>.<span>float32</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>b</span> <span>=</span> <span>torch</span>.<span>tensor</span>([<span>7</span>, <span>8</span>, <span>9</span>], <span>dtype</span><span>=</span><span>torch</span>.<span>float32</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span># b is broadcasted</span>
<span>c</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span>

<span>print</span>(<span>&#34;a =&#34;</span>, <span>a</span>)
<span>print</span>(<span>&#34;b =&#34;</span>, <span>b</span>)
<span>print</span>(<span>&#34;c =&#34;</span>, <span>c</span>)
<span>c</span>.<span>backward</span>(<span>torch</span>.<span>ones</span>(<span>2</span>, <span>3</span>))
<span>print</span>(<span>&#34;∂c/∂a =&#34;</span>, <span>a</span>.<span>grad</span>)
<span>print</span>(<span>&#34;∂c/∂b =&#34;</span>, <span>b</span>.<span>grad</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="a = tensor([[1. 2. 3.]
            [4. 5. 6.]], requires_grad=True)
b = tensor([7. 8. 9.], requires_grad=True)
c = tensor([[ 8. 10. 12.]
            [11. 13. 15.]], requires_grad=True)
∂c/∂a = tensor([[1. 1. 1.]
                [1. 1. 1.]], requires_grad=False)
∂c/∂b = tensor([2. 2. 2.], requires_grad=False)"><pre><span>a</span> <span>=</span> <span>tensor</span>([[<span>1.</span> <span>2.</span> <span>3.</span>]
            [<span>4.</span> <span>5.</span> <span>6.</span>]], <span>requires_grad</span><span>=</span><span>True</span>)
<span>b</span> <span>=</span> <span>tensor</span>([<span>7.</span> <span>8.</span> <span>9.</span>], <span>requires_grad</span><span>=</span><span>True</span>)
<span>c</span> <span>=</span> <span>tensor</span>([[ <span>8.</span> <span>10.</span> <span>12.</span>]
            [<span>11.</span> <span>13.</span> <span>15.</span>]], <span>requires_grad</span><span>=</span><span>True</span>)
∂<span>c</span><span>/</span>∂<span>a</span> <span>=</span> <span>tensor</span>([[<span>1.</span> <span>1.</span> <span>1.</span>]
                [<span>1.</span> <span>1.</span> <span>1.</span>]], <span>requires_grad</span><span>=</span><span>False</span>)
∂<span>c</span><span>/</span>∂<span>b</span> <span>=</span> <span>tensor</span>([<span>2.</span> <span>2.</span> <span>2.</span>], <span>requires_grad</span><span>=</span><span>False</span>)</pre></div>
</article></div></div>
  </body>
</html>
