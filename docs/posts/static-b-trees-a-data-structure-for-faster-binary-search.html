<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.algorithmica.org/hpc/data-structures/s-tree/">Original</a>
    <h1>Static B-Trees: A data structure for faster binary search</h1>
    
    <div id="readability-page-1" class="page"><article>
<p>This article is a follow-up to the <a href="https://en.algorithmica.org/hpc/data-structures/binary-search">previous one</a>, where we optimized binary search by the means of removing branching and improving the memory layout. Here, we will also be searching over sorted arrays, but this time we are not limited to fetching and comparing only one element at a time.</p>
<p>In this article, we generalize the techniques we developed for binary search to <em>static B-trees</em> and accelerate them further using <a href="https://en.algorithmica.org/hpc/simd">SIMD instructions</a>. In particular, we develop two new implicit data structures:</p>
<ul>
<li>The <a href="#b-tree-layout">first</a> is based on the memory layout of a B-tree, and, depending on the array size, it is up to 8x faster than <code>std::lower_bound</code> while using the same space as the array and only requiring a permutation of its elements.</li>
<li>The <a href="#b-tree-layout-1">second</a> is based on the memory layout of a B+ tree, and it is up to 15x faster than <code>std::lower_bound</code> while using just 6-7% more memory — or 6-7% <strong>of</strong> the memory if we can keep the original sorted array.</li>
</ul>
<p>To distinguish them from B-trees — the structures with pointers, hundreds to thousands of keys per node, and empty spaces in them — we will use the names <em>S-tree</em> and <em>S+ tree</em> respectively to refer to these particular memory layouts<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>To the best of my knowledge, this is a significant improvement over the existing <a href="http://kaldewey.com/pubs/FAST__SIGMOD10.pdf">approaches</a>. As before, we are using Clang 10 targeting a Zen 2 CPU, but the performance improvements should approximately transfer to most other platforms, including Arm-based chips. Use <a href="https://github.com/sslotin/amh-code/blob/main/binsearch/standalone.cc">this single-source benchmark</a> of the final implementation if you want to test it on your machine.</p>
<p>This is a long article, and since it also serves as a <a href="https://en.algorithmica.org/hpc/">textbook</a> case study, we will improve the algorithm incrementally for pedagogical goals. If you are already an expert and feel comfortable reading <a href="https://en.algorithmica.org/hpc/simd/intrinsics">intrinsic</a>-heavy code with little to no context, you can jump straight to the <a href="#implicit-b-tree-1">final implementation</a>.</p>
<span id="b-tree-layout"></span>
<h2><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#b-tree-layout">#</a>B-Tree Layout</h2><p>B-trees generalize the concept of binary search trees by allowing nodes to have more than two children. Instead of a single key, a node of a B-tree of order $k$ can contain up to $B = (k - 1)$ keys that are stored in sorted order and up to $k$ pointers to child nodes, each satisfying the property that all keys in the subtrees of the first $i$ children are not greater than the $i$-th key in the parent node.</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/b-tree.jpg"/>
<figcaption>A B-tree of order 4</figcaption>
</figure>

<p>The main advantage of this approach is that it reduces the tree height by $\frac{\log_2 n}{\log_k n} = \frac{\log k}{\log 2} = \log_2 k$ times while fetching each node still takes roughly the same time — as long it fits into a single <a href="https://en.algorithmica.org/hpc/external-memory/hierarchy/">memory block</a>.</p>
<p>B-trees were primarily developed for the purpose of managing on-disk databases, where the latency of randomly fetching a single byte is comparable with the time it takes to read the next 1MB of data sequentially. For our use case, we will be using the block size of $B = 16$ elements — or $64$ bytes, the size of the cache line — which makes the tree height and the total number of cache line fetches per query $\log_2 17 \approx 4$ times smaller compared to the binary search.</p>
<span id="implicit-b-tree"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#implicit-b-tree">#</a>Implicit B-Tree</h3><p>Storing and fetching pointers in a B-tree node wastes precious cache space and decreases performance, but they are essential for changing the tree structure on inserts and deletions. But when there are no updates and the structure of a tree is <em>static</em>, we can get rid of the pointers, which makes the structure <em>implicit</em>.</p>
<p>One of the ways to achieve this is by generalizing the <a href="https://en.algorithmica.org/hpc/data-structures/binary-search#eytzinger-layout">Eytzinger numeration</a> to $(B + 1)$-ary trees:</p>
<ul>
<li>The root node is numbered $0$.</li>
<li>Node $k$ has $(B + 1)$ child nodes numbered $\{k \cdot (B+1) + i\}$ for $i \in [1, B]$.</li>
</ul>
<p>This way, we can only use $O(1)$ additional memory by allocating one large two-dimensional array of keys and relying on index arithmetic to locate children nodes in the tree:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>const</span> <span>int</span> <span>B</span> <span>=</span> <span>16</span><span>;</span>

<span>int</span> <span>nblocks</span> <span>=</span> <span>(</span><span>n</span> <span>+</span> <span>B</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>B</span><span>;</span>
<span>int</span> <span>btree</span><span>[</span><span>nblocks</span><span>][</span><span>B</span><span>];</span>

<span>int</span> <span>go</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>int</span> <span>i</span><span>)</span> <span>{</span> <span>return</span> <span>k</span> <span>*</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>)</span> <span>+</span> <span>i</span> <span>+</span> <span>1</span><span>;</span> <span>}</span>
</code></pre></div>
<p>This numeration automatically makes the B-tree complete or almost complete with the height of $\Theta(\log_{B + 1} n)$. If the length of the initial array is not a multiple of $B$, the last block is padded with the largest value of its data type.</p>
<span id="construction"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#construction">#</a>Construction</h3><p>We can construct the B-tree similar to how we constructed the Eytzinger array — by traversing the search tree:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>void</span> <span>build</span><span>(</span><span>int</span> <span>k</span> <span>=</span> <span>0</span><span>)</span> <span>{</span>
    <span>static</span> <span>int</span> <span>t</span> <span>=</span> <span>0</span><span>;</span>
    <span>if</span> <span>(</span><span>k</span> <span>&lt;</span> <span>nblocks</span><span>)</span> <span>{</span>
        <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>B</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
            <span>build</span><span>(</span><span>go</span><span>(</span><span>k</span><span>,</span> <span>i</span><span>));</span>
            <span>btree</span><span>[</span><span>k</span><span>][</span><span>i</span><span>]</span> <span>=</span> <span>(</span><span>t</span> <span>&lt;</span> <span>n</span> <span>?</span> <span>a</span><span>[</span><span>t</span><span>++</span><span>]</span> <span>:</span> <span>INT_MAX</span><span>);</span>
        <span>}</span>
        <span>build</span><span>(</span><span>go</span><span>(</span><span>k</span><span>,</span> <span>B</span><span>));</span>
    <span>}</span>
<span>}</span>
</code></pre></div><p>It is correct because each value of the initial array will be copied to a unique position in the resulting array, and the tree height is $\Theta(\log_{B+1} n)$ because $k$ is multiplied by $(B + 1)$ each time we descend into a child node.</p>
<p>Note that this numeration causes a slight imbalance: left-er children may have larger subtrees, although this is only true for $O(\log_{B+1} n)$ parent nodes.</p>
<span id="searches"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#searches">#</a>Searches</h3><p>To find the lower bound, we need to fetch the $B$ keys in a node, find the first key $a_i$ not less than $x$, descend to the $i$-th child — and continue until we reach a leaf node. There is some variability in how to find that first key. For example, we could do a tiny internal binary search that makes $O(\log B)$ iterations, or maybe just compare each key sequentially in $O(B)$ time until we find the local lower bound, hopefully exiting from the loop a bit early.</p>
<p>But we are not going to do that — because we can use <a href="https://en.algorithmica.org/hpc/simd">SIMD</a>. It doesn’t work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the <code>ffs</code> instruction to find the bit corresponding to the first non-lesser element:</p>
<div><pre tabindex="0"><code data-lang="cpp"><span>int</span> <span>mask</span> <span>=</span> <span>(</span><span>1</span> <span>&lt;&lt;</span> <span>B</span><span>);</span>

<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>B</span><span>;</span> <span>i</span><span>++</span><span>)</span>
    <span>mask</span> <span>|=</span> <span>(</span><span>btree</span><span>[</span><span>k</span><span>][</span><span>i</span><span>]</span> <span>&gt;=</span> <span>x</span><span>)</span> <span>&lt;&lt;</span> <span>i</span><span>;</span>

<span>int</span> <span>i</span> <span>=</span> <span>__builtin_ffs</span><span>(</span><span>mask</span><span>)</span> <span>-</span> <span>1</span><span>;</span>
<span>// now i is the number of the correct child node
</span></code></pre></div><p>Unfortunately, the compilers are not smart enough yet to auto-vectorize this code, so we need to manually vectorize it with intrinsics:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>typedef</span> <span>__m256i</span> <span>reg</span><span>;</span>

<span>int</span> <span>cmp</span><span>(</span><span>reg</span> <span>x_vec</span><span>,</span> <span>int</span><span>*</span> <span>y_ptr</span><span>)</span> <span>{</span>
    <span>reg</span> <span>y_vec</span> <span>=</span> <span>_mm256_load_si256</span><span>((</span><span>reg</span><span>*</span><span>)</span> <span>y_ptr</span><span>);</span> <span>// load 8 sorted elements
</span><span></span>    <span>reg</span> <span>mask</span> <span>=</span> <span>_mm256_cmpgt_epi32</span><span>(</span><span>x_vec</span><span>,</span> <span>y_vec</span><span>);</span> <span>// compare against the key
</span><span></span>    <span>return</span> <span>_mm256_movemask_ps</span><span>((</span><span>__m256</span><span>)</span> <span>mask</span><span>);</span>    <span>// extract the 8-bit mask
</span><span></span><span>}</span>
</code></pre></div><p>This function works for 8-element vectors, which is half our block / cache line size. To process the entire block, we need to call it twice and then combine the masks:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>mask</span> <span>=</span> <span>~</span><span>(</span>
    <span>cmp</span><span>(</span><span>x</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>][</span><span>0</span><span>])</span> <span>+</span>
    <span>(</span><span>cmp</span><span>(</span><span>x</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>][</span><span>8</span><span>])</span> <span>&lt;&lt;</span> <span>8</span><span>)</span>
<span>);</span>
</code></pre></div><p>Now, to descend down the tree, we use <code>ffs</code> on that mask to get the correct child number and just call the <code>go</code> function we defined earlier:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>i</span> <span>=</span> <span>__builtin_ffs</span><span>(</span><span>mask</span><span>)</span> <span>-</span> <span>1</span><span>;</span>
<span>k</span> <span>=</span> <span>go</span><span>(</span><span>k</span><span>,</span> <span>i</span><span>);</span>
</code></pre></div><p>To actually return the result in the end, we’d want to just fetch <code>btree[k][i]</code> in the last node we visited, but the problem is that sometimes the local lower bound doesn’t exist ($i \ge B$) because $x$ happens to be greater than all the keys in the node. We could, in theory, do the same thing we did for the <a href="https://en.algorithmica.org/hpc/data-structures/binary-search/#search-implementation">Eytzinger binary search</a> and restore the correct element <em>after</em> we calculate the last index, but we don’t have a nice bit trick this time and have to do a lot of <a href="https://en.algorithmica.org/hpc/arithmetic/division">divisions by 17</a> to compute it, which will be slow and almost certainly not worth it.</p>
<p>Instead, we can just remember and return the last local lower bound we encountered when we descended the tree:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>lower_bound</span><span>(</span><span>int</span> <span>_x</span><span>)</span> <span>{</span>
    <span>int</span> <span>k</span> <span>=</span> <span>0</span><span>,</span> <span>res</span> <span>=</span> <span>INT_MAX</span><span>;</span>
    <span>reg</span> <span>x</span> <span>=</span> <span>_mm256_set1_epi32</span><span>(</span><span>_x</span><span>);</span>
    <span>while</span> <span>(</span><span>k</span> <span>&lt;</span> <span>nblocks</span><span>)</span> <span>{</span>
        <span>int</span> <span>mask</span> <span>=</span> <span>~</span><span>(</span>
            <span>cmp</span><span>(</span><span>x</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>][</span><span>0</span><span>])</span> <span>+</span>
            <span>(</span><span>cmp</span><span>(</span><span>x</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>][</span><span>8</span><span>])</span> <span>&lt;&lt;</span> <span>8</span><span>)</span>
        <span>);</span>
        <span>int</span> <span>i</span> <span>=</span> <span>__builtin_ffs</span><span>(</span><span>mask</span><span>)</span> <span>-</span> <span>1</span><span>;</span>
        <span>if</span> <span>(</span><span>i</span> <span>&lt;</span> <span>B</span><span>)</span>
            <span>res</span> <span>=</span> <span>btree</span><span>[</span><span>k</span><span>][</span><span>i</span><span>];</span>
        <span>k</span> <span>=</span> <span>go</span><span>(</span><span>k</span><span>,</span> <span>i</span><span>);</span>
    <span>}</span>
    <span>return</span> <span>res</span><span>;</span>
<span>}</span>
</code></pre></div><p>This implementation outperforms all previous binary search implementations, and by a huge margin:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-btree.svg"/>
<figcaption></figcaption>
</figure>

<p>This is very good — but we can optimize it even further.</p>
<span id="optimization"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#optimization">#</a>Optimization</h3><p>Before everything else, let’s allocate the memory for the array on a <a href="https://en.algorithmica.org/hpc/cpu-cache/paging">hugepage</a>:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>const</span> <span>int</span> <span>P</span> <span>=</span> <span>1</span> <span>&lt;&lt;</span> <span>21</span><span>;</span>                        <span>// page size in bytes (2MB)
</span><span></span><span>const</span> <span>int</span> <span>T</span> <span>=</span> <span>(</span><span>64</span> <span>*</span> <span>nblocks</span> <span>+</span> <span>P</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>P</span> <span>*</span> <span>P</span><span>;</span> <span>// can only allocate whole number of pages
</span><span></span><span>btree</span> <span>=</span> <span>(</span><span>int</span><span>(</span><span>*</span><span>)[</span><span>16</span><span>])</span> <span>std</span><span>::</span><span>aligned_alloc</span><span>(</span><span>P</span><span>,</span> <span>T</span><span>);</span>
<span>madvise</span><span>(</span><span>btree</span><span>,</span> <span>T</span><span>,</span> <span>MADV_HUGEPAGE</span><span>);</span>
</code></pre></div><p>This slightly improves the performance on larger array sizes:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-btree-hugepages.svg"/>
<figcaption></figcaption>
</figure>

<p>Ideally, we’d also need to enable hugepages for all <a href="https://en.algorithmica.org/hpc/data-structures/binary-search">previous implementations</a> to make the comparison fair, but it doesn’t matter that much because they all have some form of prefetching that alleviates this problem.</p>
<p>With that settled, let’s begin real optimization. First of all, we’d want to use compile-time constants instead of variables as much as possible because it lets the compiler embed them in the machine code, unroll loops, optimize arithmetic, and do all sorts of other nice stuff for us for free. Specifically, we want to know the tree height in advance:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>constexpr</span> <span>int</span> <span>height</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
    <span>// grow the tree until its size exceeds n elements
</span><span></span>    <span>int</span> <span>s</span> <span>=</span> <span>0</span><span>,</span> <span>// total size so far
</span><span></span>        <span>l</span> <span>=</span> <span>B</span><span>,</span> <span>// size of the next layer
</span><span></span>        <span>h</span> <span>=</span> <span>0</span><span>;</span> <span>// height so far
</span><span></span>    <span>while</span> <span>(</span><span>s</span> <span>+</span> <span>l</span> <span>-</span> <span>B</span> <span>&lt;</span> <span>n</span><span>)</span> <span>{</span>
        <span>s</span> <span>+=</span> <span>l</span><span>;</span>
        <span>l</span> <span>*=</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>);</span>
        <span>h</span><span>++</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>h</span><span>;</span>
<span>}</span>

<span>const</span> <span>int</span> <span>H</span> <span>=</span> <span>height</span><span>(</span><span>N</span><span>);</span>
</code></pre></div>
<p>Next, we can find the local lower bound in nodes faster. Instead of calculating it separately for two 8-element blocks and merging two 8-bit masks, we combine the vector masks using the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=3037,4870,6715,4845,3853,90,7307,5993,2692,6946,6949,5456,6938,5456,1021,3007,514,518,7253,7183,3892,5135,5260,3915,4027,3873,7401,4376,4229,151,2324,2310,2324,4075,6130,4875,6385,5259,6385,6250,1395,7253,6452,7492,4669,4669,7253,1039,1029,4669,4707,7253,7242,848,879,848,7251,4275,879,874,849,833,6046,7250,4870,4872,4875,849,849,5144,4875,4787,4787,4787,5227,7359,7335,7392,4787,5259,5230,5223,6438,488,483,6165,6570,6554,289,6792,6554,5230,6385,5260,5259,289,288,3037,3009,590,604,5230,5259,6554,6554,5259,6547,6554,3841,5214,5229,5260,5259,7335,5259,519,1029,515,3009,3009,3011,515,6527,652,6527,6554,288,3841,5230,5259,5230,5259,305,5259,591,633,633,5259,5230,5259,5259,3017,3018,3037,3018,3017,3016,3013,5144&amp;text=_mm256_packs_epi32&amp;techs=AVX,AVX2">packs</a> instruction and readily extract it using <code>movemask</code> just once:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>unsigned</span> <span>rank</span><span>(</span><span>reg</span> <span>x</span><span>,</span> <span>int</span><span>*</span> <span>y</span><span>)</span> <span>{</span>
    <span>reg</span> <span>a</span> <span>=</span> <span>_mm256_load_si256</span><span>((</span><span>reg</span><span>*</span><span>)</span> <span>y</span><span>);</span>
    <span>reg</span> <span>b</span> <span>=</span> <span>_mm256_load_si256</span><span>((</span><span>reg</span><span>*</span><span>)</span> <span>(</span><span>y</span> <span>+</span> <span>8</span><span>));</span>

    <span>reg</span> <span>ca</span> <span>=</span> <span>_mm256_cmpgt_epi32</span><span>(</span><span>a</span><span>,</span> <span>x</span><span>);</span>
    <span>reg</span> <span>cb</span> <span>=</span> <span>_mm256_cmpgt_epi32</span><span>(</span><span>b</span><span>,</span> <span>x</span><span>);</span>

    <span>reg</span> <span>c</span> <span>=</span> <span>_mm256_packs_epi32</span><span>(</span><span>ca</span><span>,</span> <span>cb</span><span>);</span>
    <span>int</span> <span>mask</span> <span>=</span> <span>_mm256_movemask_epi8</span><span>(</span><span>c</span><span>);</span>

    <span>// we need to divide the result by two because we call movemask_epi8 on 16-bit masks:
</span><span></span>    <span>return</span> <span>__tzcnt_u32</span><span>(</span><span>mask</span><span>)</span> <span>&gt;&gt;</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div><p>This instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register — in our case, effectively joining the vector masks into one. Note that we’ve swapped the order of comparison — this lets us not invert the mask in the end, but we have to subtract one from the search key once in the beginning to make it correct (otherwise, it works as <code>upper_bound</code>).</p>
<p>The problem is, it does this weird interleaving where the result is written in the <code>a1 b1 a2 b2</code> order instead of <code>a1 a2 b1 b2</code> that we want — many AVX2 instructions tend to do that. To correct this, we need to <a href="https://en.algorithmica.org/hpc/simd/shuffling">permute</a> the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>void</span> <span>permute</span><span>(</span><span>int</span> <span>*</span><span>node</span><span>)</span> <span>{</span>
    <span>const</span> <span>reg</span> <span>perm</span> <span>=</span> <span>_mm256_setr_epi32</span><span>(</span><span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>,</span> <span>7</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>);</span>
    <span>reg</span><span>*</span> <span>middle</span> <span>=</span> <span>(</span><span>reg</span><span>*</span><span>)</span> <span>(</span><span>node</span> <span>+</span> <span>4</span><span>);</span>
    <span>reg</span> <span>x</span> <span>=</span> <span>_mm256_loadu_si256</span><span>(</span><span>middle</span><span>);</span>
    <span>x</span> <span>=</span> <span>_mm256_permutevar8x32_epi32</span><span>(</span><span>x</span><span>,</span> <span>perm</span><span>);</span>
    <span>_mm256_storeu_si256</span><span>(</span><span>middle</span><span>,</span> <span>x</span><span>);</span>
<span>}</span>
</code></pre></div><p>Now we just call <code>permute(&amp;btree[k])</code> right after we are done building the node. There are probably faster ways to swap the middle elements, but we will leave it here as the preprocessing time is not that important for now.</p>
<p>This new SIMD routine is significantly faster because the extra <code>movemask</code> is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now can’t just do the <code>res = btree[k][i]</code> update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of <code>i</code>, but indexing a small lookup table turns out to be faster and also doesn’t require a new branch:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>const</span> <span>int</span> <span>translate</span><span>[</span><span>17</span><span>]</span> <span>=</span> <span>{</span>
    <span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span>
    <span>8</span><span>,</span> <span>9</span><span>,</span> <span>10</span><span>,</span> <span>11</span><span>,</span>
    <span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>,</span> <span>7</span><span>,</span>
    <span>12</span><span>,</span> <span>13</span><span>,</span> <span>14</span><span>,</span> <span>15</span><span>,</span>
    <span>0</span>
<span>};</span>

<span>void</span> <span>update</span><span>(</span><span>int</span> <span>&amp;</span><span>res</span><span>,</span> <span>int</span><span>*</span> <span>node</span><span>,</span> <span>unsigned</span> <span>i</span><span>)</span> <span>{</span>
    <span>int</span> <span>val</span> <span>=</span> <span>node</span><span>[</span><span>translate</span><span>[</span><span>i</span><span>]];</span>
    <span>res</span> <span>=</span> <span>(</span><span>i</span> <span>&lt;</span> <span>B</span> <span>?</span> <span>val</span> <span>:</span> <span>res</span><span>);</span>
<span>}</span>
</code></pre></div><p>This <code>update</code> procedure takes some time, but it’s not on the critical path between the iterations, so it doesn’t affect the actual performance that much.</p>
<p>Stitching it all together (and leaving out some other minor optimizations):</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>lower_bound</span><span>(</span><span>int</span> <span>_x</span><span>)</span> <span>{</span>
    <span>int</span> <span>k</span> <span>=</span> <span>0</span><span>,</span> <span>res</span> <span>=</span> <span>INT_MAX</span><span>;</span>
    <span>reg</span> <span>x</span> <span>=</span> <span>_mm256_set1_epi32</span><span>(</span><span>_x</span> <span>-</span> <span>1</span><span>);</span>
    <span>for</span> <span>(</span><span>int</span> <span>h</span> <span>=</span> <span>0</span><span>;</span> <span>h</span> <span>&lt;</span> <span>H</span> <span>-</span> <span>1</span><span>;</span> <span>h</span><span>++</span><span>)</span> <span>{</span>
        <span>unsigned</span> <span>i</span> <span>=</span> <span>rank</span><span>(</span><span>x</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>]);</span>
        <span>update</span><span>(</span><span>res</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>],</span> <span>i</span><span>);</span>
        <span>k</span> <span>=</span> <span>go</span><span>(</span><span>k</span><span>,</span> <span>i</span><span>);</span>
    <span>}</span>
    <span>// the last branch:
</span><span></span>    <span>if</span> <span>(</span><span>k</span> <span>&lt;</span> <span>nblocks</span><span>)</span> <span>{</span>
        <span>unsigned</span> <span>i</span> <span>=</span> <span>rank</span><span>(</span><span>x</span><span>,</span> <span>btree</span><span>[</span><span>k</span><span>]);</span>
        <span>update</span><span>(</span><span>res</span><span>,</span> <span>&amp;</span><span>btree</span><span>[</span><span>k</span><span>],</span> <span>i</span><span>);</span>
    <span>}</span>
    <span>return</span> <span>res</span><span>;</span>
<span>}</span>
</code></pre></div><p>All this work saved us 15-20% or so:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-btree-optimized.svg"/>
<figcaption></figcaption>
</figure>

<p>It doesn’t feel very satisfying so far, but we will reuse these optimization ideas later.</p>
<p>There are two main problems with the current implementation:</p>
<ul>
<li>The <code>update</code> procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.</li>
<li>We do a non-constant number of iterations, causing branch prediction problems similar to how it did for the <a href="https://en.algorithmica.org/binary-search/#removing-the-last-branch">Eytzinger binary search</a>; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.</li>
</ul>
<p>To address these problems, we need to change the layout a little bit.</p>
<span id="b-tree-layout-1"></span>
<h2><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#b-tree-layout-1">#</a>B+ Tree Layout</h2><p>Most of the time, when people talk about B-trees, they really mean <em>B+ trees</em>, which is a modification that distinguishes between the two types of nodes:</p>
<ul>
<li><em>Internal nodes</em> store up to $B$ keys and $(B + 1)$ pointers to child nodes. The key number $i$ is always equal to the smallest key in the subtree of the $(i + 1)$-th child node.</li>
<li><em>Data nodes</em> or <em>leaves</em> store up to $B$ keys, the pointer to the next leaf node, and, optionally, an associated value for each key — if the structure is used as a key-value map.</li>
</ul>
<p>The advantages of this approach include faster search time (as the internal nodes only store keys) and the ability to quickly iterate over a range of entries (by following next leaf node pointers), but this comes at the cost of some memory overhead: we have to store copies of keys in the internal nodes.</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/bplus.png"/>
<figcaption>A B+ tree of order 4</figcaption>
</figure>

<p>Back to our use case, this layout can help us solve our two problems:</p>
<ul>
<li>Either the last node we descend into has the local lower bound, or it is the first key of the next leaf node, so we don’t need to call <code>update</code> on each iteration.</li>
<li>The depth of all leaves is constant because B+ trees grow at the root and not at the leaves, which removes the need for branching. </li>
</ul>
<p>The disadvantage is that this layout is not succinct: we need some additional memory to store the internal nodes — about $\frac{1}{16}$-th of the original array size, to be exact — but the performance improvement will be more than worth it.</p>
<span id="implicit-b-tree-1"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#implicit-b-tree-1">#</a>Implicit B+ Tree</h3><p>To be more explicit with pointer arithmetic, we will store the entire tree in a single one-dimensional array. To minimize index computations during run-time, we will store each layer sequentially in this array and use compile-time computed offsets to address them: the keys of the node number <code>k</code> on layer <code>h</code> start with <code>btree[offset(h) + k * B]</code>, and its <code>i</code>-th child will at <code>btree[offset(h - 1) + (k * (B + 1) + i) * B]</code>.</p>
<p>To implement all that, we need slightly more <code>constexpr</code> functions:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>// number of B-element blocks in a layer with n keys
</span><span></span><span>constexpr</span> <span>int</span> <span>blocks</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
    <span>return</span> <span>(</span><span>n</span> <span>+</span> <span>B</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>B</span><span>;</span>
<span>}</span>

<span>// number of keys on the layer pervious to one with n element
</span><span></span><span>constexpr</span> <span>int</span> <span>prev_keys</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
    <span>return</span> <span>(</span><span>blocks</span><span>(</span><span>n</span><span>)</span> <span>+</span> <span>B</span><span>)</span> <span>/</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>)</span> <span>*</span> <span>B</span><span>;</span>
<span>}</span>

<span>// height of a balanced n-key B+ tree
</span><span></span><span>constexpr</span> <span>int</span> <span>height</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
    <span>return</span> <span>(</span><span>n</span> <span>&lt;=</span> <span>B</span> <span>?</span> <span>1</span> <span>:</span> <span>height</span><span>(</span><span>prev_keys</span><span>(</span><span>n</span><span>))</span> <span>+</span> <span>1</span><span>);</span>
<span>}</span>

<span>// where the layer h starts (0 is the largest)
</span><span></span><span>constexpr</span> <span>int</span> <span>offset</span><span>(</span><span>int</span> <span>h</span><span>)</span> <span>{</span>
    <span>int</span> <span>k</span> <span>=</span> <span>0</span><span>,</span> <span>n</span> <span>=</span> <span>N</span><span>;</span>
    <span>while</span> <span>(</span><span>h</span><span>--</span><span>)</span> <span>{</span>
        <span>k</span> <span>+=</span> <span>blocks</span><span>(</span><span>n</span><span>)</span> <span>*</span> <span>B</span><span>;</span>
        <span>n</span> <span>=</span> <span>prev_keys</span><span>(</span><span>n</span><span>);</span>
    <span>}</span>
    <span>return</span> <span>k</span><span>;</span>
<span>}</span>

<span>const</span> <span>int</span> <span>H</span> <span>=</span> <span>height</span><span>(</span><span>N</span><span>);</span>
<span>const</span> <span>int</span> <span>S</span> <span>=</span> <span>offset</span><span>(</span><span>H</span><span>);</span> <span>// the tree size is the offset of the (non-existent) layer H
</span><span></span>
<span>int</span> <span>*</span><span>btree</span><span>;</span> <span>// the tree itself is stored in a single hugepage-aligned array of size S
</span></code></pre></div><p>Note that we store the layers in reverse order, but the nodes within a layer and data in them are still left-to-right, and also the layers are numbered bottom-up: the leaves form the zeroth layer, and the root is the layer <code>H - 1</code>. These are just arbitrary decisions — it is just slightly easier to implement in code.</p>
<span id="construction-1"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#construction-1">#</a>Construction</h3><p>To construct the tree from a sorted array <code>a</code>, we first need to copy it into the zeroth layer and pad it with infinities:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>memcpy</span><span>(</span><span>btree</span><span>,</span> <span>a</span><span>,</span> <span>4</span> <span>*</span> <span>N</span><span>);</span>

<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>N</span><span>;</span> <span>i</span> <span>&lt;</span> <span>S</span><span>;</span> <span>i</span><span>++</span><span>)</span>
    <span>btree</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>INT_MAX</span><span>;</span>
</code></pre></div><p>Now we build the internal nodes, layer by layer. For each key, we need to descend to the right of it in, always go left until we reach a leaf node, and then take its first key — it will be the smallest on the subtree:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>for</span> <span>(</span><span>int</span> <span>h</span> <span>=</span> <span>1</span><span>;</span> <span>h</span> <span>&lt;</span> <span>H</span><span>;</span> <span>h</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>offset</span><span>(</span><span>h</span> <span>+</span> <span>1</span><span>)</span> <span>-</span> <span>offset</span><span>(</span><span>h</span><span>);</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>// i = k * B + j
</span><span></span>        <span>int</span> <span>k</span> <span>=</span> <span>i</span> <span>/</span> <span>B</span><span>,</span>
            <span>j</span> <span>=</span> <span>i</span> <span>-</span> <span>k</span> <span>*</span> <span>B</span><span>;</span>
        <span>k</span> <span>=</span> <span>k</span> <span>*</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>)</span> <span>+</span> <span>j</span> <span>+</span> <span>1</span><span>;</span> <span>// compare to the right of the key
</span><span></span>        <span>// and then always to the left
</span><span></span>        <span>for</span> <span>(</span><span>int</span> <span>l</span> <span>=</span> <span>0</span><span>;</span> <span>l</span> <span>&lt;</span> <span>h</span> <span>-</span> <span>1</span><span>;</span> <span>l</span><span>++</span><span>)</span>
            <span>k</span> <span>*=</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>);</span>
        <span>// pad the rest with infinities if the key doesn&#39;t exist 
</span><span></span>        <span>btree</span><span>[</span><span>offset</span><span>(</span><span>h</span><span>)</span> <span>+</span> <span>i</span><span>]</span> <span>=</span> <span>(</span><span>k</span> <span>*</span> <span>B</span> <span>&lt;</span> <span>N</span> <span>?</span> <span>btree</span><span>[</span><span>k</span> <span>*</span> <span>B</span><span>]</span> <span>:</span> <span>INT_MAX</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div><p>And just the finishing touch — we need to permute keys in internal nodes to search them faster:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>offset</span><span>(</span><span>1</span><span>);</span> <span>i</span> <span>&lt;</span> <span>S</span><span>;</span> <span>i</span> <span>+=</span> <span>B</span><span>)</span>
    <span>permute</span><span>(</span><span>btree</span> <span>+</span> <span>i</span><span>);</span>
</code></pre></div><p>We start from <code>offset(1)</code>, and we specifically don’t permute leaf nodes and leave the array in the original sorted order. The motivation is that we’d need to do this complex index translation we do in <code>update</code> if the keys were permuted, and it is on the critical path when this is the last operation. So, just for this layer, we switch to the original mask-blending local lower bound procedure.</p>
<span id="searching"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#searching">#</a>Searching</h3><p>The search procedure becomes simpler than for the B-tree layout: we don’t need to do <code>update</code> and only execute a fixed number of iterations — although the last one with some special treatment:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>lower_bound</span><span>(</span><span>int</span> <span>_x</span><span>)</span> <span>{</span>
    <span>unsigned</span> <span>k</span> <span>=</span> <span>0</span><span>;</span> <span>// we assume k already multiplied by B to optimize pointer arithmetic
</span><span></span>    <span>reg</span> <span>x</span> <span>=</span> <span>_mm256_set1_epi32</span><span>(</span><span>_x</span> <span>-</span> <span>1</span><span>);</span>
    <span>for</span> <span>(</span><span>int</span> <span>h</span> <span>=</span> <span>H</span> <span>-</span> <span>1</span><span>;</span> <span>h</span> <span>&gt;</span> <span>0</span><span>;</span> <span>h</span><span>--</span><span>)</span> <span>{</span>
        <span>unsigned</span> <span>i</span> <span>=</span> <span>permuted_rank</span><span>(</span><span>x</span><span>,</span> <span>btree</span> <span>+</span> <span>offset</span><span>(</span><span>h</span><span>)</span> <span>+</span> <span>k</span><span>);</span>
        <span>k</span> <span>=</span> <span>k</span> <span>*</span> <span>(</span><span>B</span> <span>+</span> <span>1</span><span>)</span> <span>+</span> <span>i</span> <span>*</span> <span>B</span><span>;</span>
    <span>}</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>direct_rank</span><span>(</span><span>x</span><span>,</span> <span>btree</span> <span>+</span> <span>k</span><span>);</span>
    <span>return</span> <span>btree</span><span>[</span><span>k</span> <span>+</span> <span>i</span><span>];</span>
<span>}</span>
</code></pre></div><p>Switching to the B+ layout more than paid off: this S+ tree is 1.5-3x faster compared to the optimized S-tree:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-bplus.svg"/>
<figcaption></figcaption>
</figure>

<p>The spikes at the high end of the graph are caused by the L1 TLB not being large enough: it has 64 entries, so it can handle at most 64 × 2 = 128MB of data, which is exactly what is required for storing <code>2^25</code> integers. The S+ tree hits this limit slightly sooner because of the ~7% memory overhead.</p>
<span id="comparison-with-stdlower_bound"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#comparison-with-stdlower_bound">#</a>Comparison with <code>std::lower_bound</code></h3><p>We’ve come a long way from binary search:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-all.svg"/>
<figcaption></figcaption>
</figure>

<p>On these scales, it makes more sense to look at the relative speedup:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-relative.svg"/>
<figcaption></figcaption>
</figure>

<p>The cliffs at the beginning of the graph are because the running time of <code>std::lower_bound</code> grows smoothly with the array size, while for an S+ tree, it is locally flat and increases in discrete steps when a new layer needs to be added.</p>
<p>One important asterisk we haven’t discussed is that what we are measuring is not real latency, but the <em>reciprocal throughput</em> — the total time it takes to execute a lot of queries divided by the number of queries:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>clock_t</span> <span>start</span> <span>=</span> <span>clock</span><span>();</span>

<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>m</span><span>;</span> <span>i</span><span>++</span><span>)</span>
    <span>checksum</span> <span>^=</span> <span>lower_bound</span><span>(</span><span>q</span><span>[</span><span>i</span><span>]);</span>

<span>float</span> <span>seconds</span> <span>=</span> <span>float</span><span>(</span><span>clock</span><span>()</span> <span>-</span> <span>start</span><span>)</span> <span>/</span> <span>CLOCKS_PER_SEC</span><span>;</span>
<span>printf</span><span>(</span><span>&#34;%.2f ns per query</span><span>\n</span><span>&#34;</span><span>,</span> <span>1e9</span> <span>*</span> <span>seconds</span> <span>/</span> <span>m</span><span>);</span>
</code></pre></div><p>To measure <em>actual</em> latency, we need to introduce a dependency between the loop iterations so that the next query can’t start before the previous one finishes:</p>
<div><pre tabindex="0"><code data-lang="c++"><span>int</span> <span>last</span> <span>=</span> <span>0</span><span>;</span>

<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>m</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>last</span> <span>=</span> <span>lower_bound</span><span>(</span><span>q</span><span>[</span><span>i</span><span>]</span> <span>^</span> <span>last</span><span>);</span>
    <span>checksum</span> <span>^=</span> <span>last</span><span>;</span>
<span>}</span>
</code></pre></div><p>In terms of real latency, the speedup is not that impressive:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-relative-latency.svg"/>
<figcaption></figcaption>
</figure>

<p>A lot of the performance boost of the S+ tree comes from removing branching and minimizing memory requests, which allows overlapping the execution of more adjacent queries — apparently, around three on average.</p>
<p>Although nobody except maybe the HFT people cares about real latency, and everybody actually measures throughput even when using the word “latency”, this nuance is still something to take into account when predicting the possible speedup in user applications.</p>
<span id="modifications-and-further-optimizations"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#modifications-and-further-optimizations">#</a>Modifications and Further Optimizations</h3>
<p>To minimize the number of memory accesses during a query, we can increase the block size. To find the local lower bound in a 32-element node (spanning two cache lines and four AVX2 registers), we can use a <a href="https://github.com/sslotin/amh-code/blob/a74495a2c19dddc697f94221629c38fee09fa5ee/binsearch/bplus32.cc#L94">similar trick</a> that uses two <code>packs_epi32</code> and one <code>packs_epi16</code> to combine masks.</p>
<p>We can also try to use the cache more efficiently by controlling where each tree layer is stored in the cache hierarchy. We can do that by prefetching nodes to a <a href="https://en.algorithmica.org/hpc/cpu-cache/prefetching/#software-prefetching">specific level</a> and using <a href="https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#bypassing-the-cache">non-temporal reads</a> during queries.</p>
<p>I implemented these two optimizations: the one with a block size of 32 and the one where the last read is non-temporal. They don’t improve the throughput:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-bplus-other.svg"/>
<figcaption></figcaption>
</figure>

<p>…but they do make the latency lower:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-latency-bplus.svg"/>
<figcaption></figcaption>
</figure>

<p>Ideas that I have not yet managed to implement but consider highly perspective are:</p>
<ul>
<li>
<p>Make the block size non-uniform. The motivation is that the slowdown from having one 32-element layer is less than from having two separate layers. Also, the root is often not full, so perhaps it should have only 8 keys or even just one key. Picking the optimal layer configuration for a given array size should remove the spikes from the relative speedup graph and make it look more like its upper envelope.</p>
<p>I know how to do it with code generation, but I went for a generic solution and tried to <a href="https://github.com/sslotin/amh-code/blob/main/binsearch/bplus-adaptive.cc">implement</a> it with the facilities of modern C++, but the compiler can’t produce optimal code this way.</p>
</li>
<li>
<p>Group nodes with one or two generations of its descendants (~300 nodes / ~5k keys) so that they are close in memory, similar to what <a href="http://kaldewey.com/pubs/FAST__SIGMOD10.pdf">FAST</a> calls hierarchical blocking. This reduces the severity of TLB misses and also may improve latency as the memory controller chooses to keep the <a href="https://en.algorithmica.org/hpc/cpu-cache/aos-soa/#ram-specific-timings">RAM row buffer</a> open, anticipating local reads.</p>
</li>
<li>
<p>Optionally use prefetching on some specific layers. Aside from to the $\frac{1}{17}$-th chance of it fetching the node we need, the hardware prefetcher may also get some of its neighbors for us if the data bus is not busy. It also has the same TLB and row buffer effects as with blocking.</p>
</li>
</ul>
<p>Other possible minor optimizations include:</p>
<ul>
<li>Permuting the nodes of the last layer also — if we only need the index and not the value.</li>
<li>Reversing the order in which the layers are stored to left-to-right so that the first few layers are on the same page.</li>
<li>Rewriting the whole thing in assembly, as the compiler seems to struggle with pointer arithmetic.</li>
</ul>
<p>Note that our implementation is specific to the AVX2 and may require some non-trivial changes to adapt to other platforms. It would be interesting to port it for Intel CPUs with AVX-512 and Arm CPUs with 128-bit NEON, which may require some <a href="https://github.com/WebAssembly/simd/issues/131">trickery</a> to work.</p>
<p>With these optimizations implemented, I wouldn’t be surprised to see another 10-30% improvement and over 10x speedup over <code>std::lower_bound</code> on large arrays for some platforms.</p>
<span id="as-a-dynamic-tree"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#as-a-dynamic-tree">#</a>As a Dynamic Tree</h3><p>The comparison is even more favorable against <code>std::set</code> and other pointer-based trees. In our benchmark, we add the same elements (without measuring the time it takes to add them) and use the same lower bound queries, and the S+ tree is up to 30x better:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-set-relative.svg"/>
<figcaption></figcaption>
</figure>

<p>This suggests that we can probably use this approach to also improve on <em>dynamic</em> search trees by a huge margin.</p>
<p>To validate this hypothesis, I added an array of 17 indices for each node that point to where their children should be and used this array instead of implicit numbering. This array is separate from the tree, not aligned, isn’t even on a hugepage, and the only optimization is that the first and the last pointer of a node is prefetched.</p>
<p>I also added <a href="https://abseil.io/blog/20190812-btree">B-tree from Abseil</a> to the comparison, which is the only widely-used B-tree implementation I know of. It performs just slightly better than <code>std::lower_bound</code>, while the S+ tree with pointers is ~15x faster for large arrays:</p>
<figure>
<img src="https://en.algorithmica.org/hpc/data-structures/img/search-set-relative-all.svg"/>
<figcaption></figcaption>
</figure>

<p>Of course, this comparison is not fair, as the dynamic search tree is a more high-dimensional problem. We’d also need to implement the update operation, which will not be that efficient, and for which we’d need to sacrifice our fanout factor. But it still seems possible to implement a 10-20x faster <code>std::set</code> and a 3-5x faster <code>absl::btree_set</code>, depending on how you define “faster” — and this is one of the next things we’ll try to do.</p>
<span id="acknowledgements"></span>
<h3><a href="https://en.algorithmica.org/hpc/data-structures/s-tree/#acknowledgements">#</a>Acknowledgements</h3><p>This <a href="https://stackoverflow.com/questions/20616605/using-simd-avx-sse-for-tree-traversal">StackOverflow answer</a> by Cory Nelson is where I took the permuted 16-element search trick from.</p>

</article></div>
  </body>
</html>
