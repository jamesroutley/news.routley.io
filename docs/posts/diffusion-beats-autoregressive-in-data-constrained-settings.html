<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/">Original</a>
    <h1>Diffusion Beats Autoregressive in Data-Constrained Settings</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<p><strong>TLDR</strong>:</p>



<p><em>If you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models.</em></p>



<p>Motivation</p>



<p>Progress in AI over the past decade has largely been driven by scaling compute and data. The recipe from GPT-1 to GPT-5 has appeared straightforward: train a larger model on more data, and the result is a more capable system. </p>



<figure><img loading="lazy" width="1024" height="436" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-1024x436.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-1024x436.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-300x128.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-1536x654.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-2048x872.png 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-970x413.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-320x136.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-80x34.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-300x128@2x.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Scaling plot from <a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a> </figcaption></figure>



<p>Yet a central question remains: will this recipe continue to hold from GPT-6 to GPT-N?</p>



<p>Many analysts and researchers believe the answer is no. For instance, Ilya Sutskever, in his NeurIPS 2024 Test-of-Time Award <a href="https://www.youtube.com/watch?v=YD-9NG1Ke5Y">talk</a>, remarked: “Compute is growing—better algorithms, better hardware, bigger clusters—but data is not growing. We have just one internet, the fossil fuel of AI.” </p>



<p>This concern is echoed by AI forecasters, who have analyzed compute and data growth more systematically and concluded that compute is outpacing data at an accelerating rate.</p>



<figure><img loading="lazy" width="1005" height="585" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1.png 1005w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1-300x175.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1-970x565.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1-320x186.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1-80x47.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/projections-1-300x175@2x.png 600w" sizes="(max-width: 1005px) 100vw, 1005px"/><figcaption><a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data" data-type="URL" data-id="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data">Epoch AI</a>‘s study that extrapolates the growth rates of internet data (stock of data), dataset usage (dataset size projection), and compute (measured in Chinchilla-optimal tokens). Around 2028, compute outpaces the total available training data on the internet, marking the onset of a data-constrained regime. I updated the figure by overlaying Figure 4 and Figure 5 in their paper. </figcaption></figure>



<p>The above Figure, illustrates this tension by overlaying projections from EpochAI’s <a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data">analysis</a>. Their study extrapolates historical trends in compute, dataset usage, and internet-scale data availability. The forecast suggests that by around 2028, we will enter a data-constrained regime: far more compute will be available than there are training tokens to consume.</p>



<p>This paper addresses the challenge by asking: <strong>how can we trade off more compute for less data?</strong> Our central idea is to revisit the foundations of modern generative modeling and compare the two dominant paradigms for scaling AI.</p>



<p>Broadly, there have been two families of algorithms that shaped recent progress in AI:</p>



<ul><li>Autoregressive models, popularized in 2019 in the text domain with the <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> paper.</li></ul>



<ul><li>Diffusion models, popularized in 2020 in the vision domain with the <a href="https://arxiv.org/abs/2006.11239">DDPM</a> paper.</li></ul>



<p>Both aim to maximize the joint likelihood, but they differ fundamentally in how they factorize this joint distribution. </p>



<p>The success of diffusion in vision and autoregression in language has sparked both excitement and confusion—especially as each community has begun experimenting with the other’s paradigm.</p>



<p>For example, the language community has explored diffusion on text: </p>



<p><a href="https://arxiv.org/abs/2107.03006">D3PM</a> introduced discrete diffusion via random masking, while <a href="https://arxiv.org/abs/2305.18619">Diffusion-LM</a> applied continuous diffusion by projecting tokens to embeddings before adding Gaussian noise. Since then, numerous works have extended this line of research.</p>



<p>Conversely, the vision community has experimented with doing autoregressive modeling on images. Models such as <a href="https://arxiv.org/abs/2206.10789">PARTI</a> and <a href="https://arxiv.org/abs/2102.12092">DALLE</a> exemplify this approach with strong results.</p>



<p>This cross-pollination has led to even greater uncertainty in robotics, where both diffusion-based and autoregressive approaches are widely adopted. To illustrate this, OpenAI Deep Research has compiled a list of robotics works across both paradigms, highlighting the lack of consensus in the field.</p>



<figure><img loading="lazy" width="1024" height="541" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-1024x541.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-1024x541.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-300x158.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-1536x811.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-970x512.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-320x169.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-80x42.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM.png 1916w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.23.31 PM-300x158@2x.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>This ambiguity raises a fundamental question: should we be training diffusion models or autoregressive models?</p>



<p>Quick Background:</p>



<p><strong>Autoregressive language models:</strong></p>



<figure><img loading="lazy" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-1024x238.png" alt="" width="483" height="111" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-1024x238.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-300x70.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-1536x357.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-2048x477.png 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-320x74.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-80x19.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-3-300x70@2x.png 600w" sizes="(max-width: 483px) 100vw, 483px"/></figure>



<p>They model data distribution in a left-to-right manner</p>



<p><strong>Diffusion language models:</strong></p>



<figure><img loading="lazy" width="1024" height="572" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-1024x572.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-1024x572.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-300x168.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-1536x858.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-970x542.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-320x179.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-80x45.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM.png 1740w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-4.44.55 PM-300x168@2x.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>For a more detailed understanding, with cool animations, please refer to this video from Jia-Bin Huang – <a href="https://www.youtube.com/watch?v=8BTOoc0yDVA">https://www.youtube.com/watch?v=8BTOoc0yDVA</a></p>



<p>Prior results with Diffusion Language models</p>



<p>Since 2021, diffusion language models have sparked significant interest, with many works focusing on improving their design and performance.</p>



<figure><img loading="lazy" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-1024x230.png" alt="" width="580" height="130" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-1024x230.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-300x67.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-1536x345.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-2048x460.png 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-970x218.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-320x72.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-80x18.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-4-300x67@2x.png 600w" sizes="(max-width: 580px) 100vw, 580px"/><figcaption>Numbers taken from: Sahoo etal “Simple and Effective Masked Diffusion Language Models”</figcaption></figure>



<p>In the table above, we highlight representative results from a popular work.</p>



<ul><li>Discrete diffusion performs better than continuous diffusion on text.</li><li>Autoregressive models still achieve the strongest results overall.</li></ul>



<p>Several works have also explored the scaling behavior of diffusion-based language models.</p>



<div><figure><img loading="lazy" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-1024x864.png" alt="" width="422" height="356" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-1024x864.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-300x253.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-1536x1297.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-2048x1729.png 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-740x625.png 740w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-272x230.png 272w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-261x220.png 261w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-80x68.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/image-5-300x253@2x.png 600w" sizes="(max-width: 422px) 100vw, 422px"/></figure></div>



<p><a href="https://arxiv.org/abs/2410.18514">Nie et al</a>  report that discrete diffusion LLMs require roughly 16× more compute than autoregressive LLMs to match the same negative log-likelihood. Similar results have been observed in multimodal domains—for instance, <a href="https://unidisc.github.io/">UniDisc</a> finds that discrete diffusion needs about 12× more compute than autoregression for comparable likelihoods.</p>



<p>However, these results conflate <em>data</em> and <em>compute</em> because they are measured in a single-epoch training regime. This raises an important ambiguity: do diffusion models truly require 16× more compute, or do they in fact require 16× more <em>data</em>?</p>



<p>In this work, we explicitly disentangle data and compute. Our goal is to study diffusion and autoregressive models specifically in data-constrained settings.</p>



<p>Our Motivation</p>



<p>To understand why diffusion may behave differently, let’s revisit its training objective.</p>



<figure><img loading="lazy" width="625" height="254" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/ezgif.com-animated-gif-maker.gif" alt=""/></figure>



<p>In diffusion training, tokens are randomly masked and the model learns to recover them. Importantly, left-to-right masking is a <em>special case</em> within this framework.</p>



<figure><img loading="lazy" width="590" height="200" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/ezgif.com-animated-gif-maker-1.gif" alt=""/></figure>



<p>Viewed this way, diffusion can be interpreted as a form of <strong>implicit data augmentation</strong> for autoregressive training. Instead of only learning from left-to-right sequences, the model also benefits from many alternative masking strategies.</p>



<p>And if diffusion is essentially data augmentation, then its benefits should be most pronounced when training is data-bottlenecked.</p>



<p>This perspective explains why prior works have reported weaker results for diffusion: they primarily evaluated in single-epoch settings, where data is abundant. In contrast, our study focuses on scenarios where data is limited and compute can be traded off more effectively.</p>



<p>Our Experiments</p>



<p>In this work,  we train hundreds of models spanning multiple orders of magnitude in model size, data quantity, and number of training epochs to fit scaling laws for diffusion models in the data-constrained setting. We summarize some of our key findings below.</p>



<p>Finding #1:</p>



<div><figure><img loading="lazy" width="1024" height="356" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-1024x356.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-1024x356.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-300x104.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-1536x534.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-2048x712.jpeg 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-970x337.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-320x111.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-80x28.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfAttraQAA2CK0-300x104@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure></div>



<p>Diffusion models outperform autoregressive models when trained with sufficient compute (i.e., more epochs &amp; parameters). Across different unique data scales, we observe:</p>



<ul><li>At low compute, Autoregressive models win.</li></ul>



<ul><li>After a certain amount of compute, performance matches—we call this the critical compute point.</li></ul>



<ul><li>Beyond this, diffusion keeps improving, while Autoregressive plateaus or overfits. </li></ul>



<p>Each point in the figure shows a model trained to convergence. The x-axis shows the total training FLOPs of that point, and the y-axis shows the best validation loss achieved by that model family under that training compute budget.</p>



<p>Finding #2:</p>



<figure><img loading="lazy" width="1024" height="537" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-1024x537.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-1024x537.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-300x157.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-1536x806.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-2048x1074.jpeg 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-970x509.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-320x168.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-80x42.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfA5x0aUAAoUvK-300x157@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>Autoregressive models begin to overfit much quickly, while diffusion shows no signs of overfitting even after 10x the number of epochs. In the above figure, we showed that increasing compute eventually favors diffusion. But compute can be scaled in two ways: (i) Increasing model size (ii) Increasing the number of epochs In the following plot, we separate these axes.</p>



<p>The colored star marks the 1-epoch point, where Autoregressive outperforms diffusion. The star (★) denotes the best loss achieved by each model.</p>



<ul><li>Autoregressive hits its best around the middle, then overfits.</li></ul>



<ul><li>Diffusion keeps improving and reaches its best loss at the far right. </li></ul>



<p>Not only does diffusion benefit from more training—it also achieves a better final loss than Autoregressive (3.51 vs. 3.71).</p>







<p>Finding #3:</p>



<figure><img loading="lazy" width="1024" height="347" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-1024x347.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-1024x347.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-300x102.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-1536x520.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-2048x693.jpeg 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-970x328.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-320x108.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-80x27.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBGmBbEAMhruY-300x102@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>Diffusion models are significantly more robust to data repetition than autoregressive (AR) models. </p>



<p>We show training curves of models trained with the same total compute, but different trade-offs between unique data and number of epochs. </p>



<p>An “epoch” here means reusing a smaller subset of data more times(e.g.,  4 Ep is  4 epochs while using 25% unique data, 2 Ep is 2 epochs with 50% and so on).</p>



<ul><li>AR models begin to overfit as repetition increases—their validation loss worsens and significantly diverges at higher epoch counts.</li></ul>



<ul><li>Diffusion models remain stable across all repetition levels, showing no signs of overfitting or diverging—even at 100 epochs.</li></ul>



<p>Finding #4:</p>



<figure><img loading="lazy" width="1024" height="254" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-1024x254.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-1024x254.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-300x74.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-1536x381.png 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-2048x509.png 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-970x241.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-320x79.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-80x20.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBWjybEAI-fXI-300x74@2x.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>Diffusion models exhibit a much higher half-life of data reuse (R_D*) —i.e., the number of epochs after which returns from repeating data begins to significantly diminish. </p>



<p>We adopt the data-constrained scaling framework introduced by Muennighoff et al. in their excellent <a href="https://arxiv.org/abs/2305.16264">NeurIPS paper</a> to fit scaling laws for diffusion models. While Muennighoff et al. found R_D* ~ 15 for autoregressive models, we find a significantly higher value of R_D* ~ 500 for diffusion models—highlighting their ability to benefit from far more data repetition.</p>



<figure><img loading="lazy" width="1024" height="281" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-1024x281.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-1024x281.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-300x82.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-1536x421.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-2048x562.jpeg 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-970x266.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-320x88.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-80x22.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBYoQbEAIXJpH-2-300x82@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>The above Figure studies the Decay rate of data value under repetition: left shows diffusion, middle AR, and right the average decay rate for both. </p>



<p>Points are empirical results (darker color = higher FLOPs, lighter color =</p>



<p>Finding #5:</p>



<figure><img loading="lazy" width="1024" height="342" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-1024x342.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-1024x342.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-300x100.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-1536x512.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-970x324.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-320x107.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-80x27.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW.jpeg 1793w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBkBnbEAAyBvW-300x100@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>Muennighoff et al. showed that repeating the dataset up to 4 epochs is nearly as effective as using fresh data for autoregressive models.</p>



<p> In contrast, we find that diffusion models can be trained on repeated data for up to 100 epochs, while having repeated data almost as effective as fresh data.</p>







<p>Finding #6:</p>



<figure><img loading="lazy" width="1024" height="508" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-1024x508.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-1024x508.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-300x149.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-1536x762.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-970x481.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-320x159.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-80x40.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8.jpeg 2023w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfBukKbEAEuab8-300x149@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>The compute required for diffusion to outperform AR follows a predictable power law. Above we defined the critical compute threshold as the amount of FLOPs where diffusion matches AR performance for a given unique dataset size. </p>



<figure><img loading="lazy" width="1024" height="85" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-1024x85.png" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-1024x85.png 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-300x25.png 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-970x80.png 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-320x26.png 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-80x7.png 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM.png 1380w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/Screenshot-2025-09-10-at-10.15.36 PM-300x25@2x.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>We find that we can derive a simple closed-form analytical expression for this threshold, this allows us to predict when diffusion will surpass AR given any unique data size. In the figure we show both the fitted curve and empirical critical threshold points, which align closely.</p>



<p>Finding #7:</p>



<figure><img loading="lazy" width="1024" height="516" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-1024x516.jpeg" alt="" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-1024x516.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-300x151.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-1536x774.jpeg 1536w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-2048x1032.jpeg 2048w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-970x489.jpeg 970w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-320x161.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-80x40.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GwfB5wbbEAQU11C-300x151@2x.jpeg 600w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>The data efficiency of diffusion models translates to better downstream performance.</p>



<p> Lastly we evaluate the best-performing diffusion and AR models (trained under the same data budget) on a range of language understanding tasks. </p>



<p>Across most benchmarks, diffusion models outperform AR models, confirming that diffusion’s lower validation loss translates to better downstream performance.</p>



<p>Finding #8:</p>



<div><figure><img loading="lazy" src="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-1024x673.jpeg" alt="" width="462" height="303" srcset="https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-1024x673.jpeg 1024w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-300x197.jpeg 300w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-951x625.jpeg 951w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-320x210.jpeg 320w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-80x53.jpeg 80w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X.jpeg 1460w, https://blog.ml.cmu.edu/wp-content/uploads/2025/09/GxslAC1awAIOf7X-300x197@2x.jpeg 600w" sizes="(max-width: 462px) 100vw, 462px"/></figure></div>



<p>Exposure to different token orderings helps explain diffusion’s data efficiency. By adding explicit data augmentations to AR training, we find that diffusion model’s advantage arises from their exposure to a diverse set of token orderings. </p>



<p>As seen in the above Figure, increasing N consistently lowered validation loss and delayed overfitting. At N = 16, the 100-epoch validation loss of AR models approached that of diffusion, suggesting that diverse orderings are indeed a key driver of diffusion’s data efficiency. These results support our interpretation that diffusion models outperform AR models in low-data regimes because they are implicitly trained on a richer distribution of conditional prediction tasks. </p>



<p>Finally, this analysis suggests a natural continuum between the two paradigms: by controlling task diversity through masking or reordering—we could design hybrid models that interpolate between compute efficiency (AR-like) and data efficiency (diffusion-like).</p>



<p>For more experiments and details please refer to original paper  –<a href="https://arxiv.org/abs/2507.15857">https://arxiv.org/abs/2507.15857</a></p>



<p>Conclusion</p>



<p>As the availability of high-quality data plateaus, improving data efficiency becomes essential for scaling deep learning. In this work, we show that masked diffusion models consistently outperform autoregressive (AR) models in data-constrained regimes — when training involves repeated passes over a limited dataset. We establish new scaling laws for diffusion models, revealing their ability to extract value from repeated data far beyond what AR models can achieve.</p>



<p> These results challenge the conventional belief that AR models are universally superior and highlight diffusion models as a compelling alternative when data—not compute—is the primary bottleneck. Looking ahead, efficient use of finite data may define the next frontier in scaling deep learning models. Although the studies have been performed in the context of language models, we believe these findings should apply across any kind of sequence modeling data, such as in robotics or healthcare. For practitioners, our takeaway is simple: <strong><em>if you are compute-constrained, use autoregressive models; if you are data-constrained, use diffusion models</em></strong>.</p>



<p>Bibtex:</p>



<p><code>@article{prabhudesai2025diffusion,</code></p>

                    </div></div>
  </body>
</html>
