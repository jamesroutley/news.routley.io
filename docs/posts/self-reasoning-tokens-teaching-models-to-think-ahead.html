<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reasoning-tokens.ghost.io/reasoning-tokens/">Original</a>
    <h1>Self-reasoning tokens: teaching models to think ahead</h1>
    
    <div id="readability-page-1" class="page"><div>

    <article>

        <header>

            

            <div>
                <p><a href="https://reasoning-tokens.ghost.io/author/felipe/">
                                <img src="https://www.gravatar.com/avatar/c7a32029edda420c1bff08b48f99b2cb?s=250&amp;r=x&amp;d=mp" alt="Felipe Sens Bonetto"/>
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2024-04-20">Apr 20, 2024</time>
                            <span><span>â€”</span> 4 min read</span>
                    </p>
                </div>
            </div>

            
        </header>

        <section>
            <p>What is the mathematical formulation of reasoning? How can we make LLMs like chatGPT think before they speak? And how can we make that baked into the model so it can learn to think in a self-supervised way without having to &#34;explain it step by step&#34; (or another famous prompt we use when we want to improve chatGPT performance drastically)? How can we teach models to think ahead? I will share with you the results of some experiments that may cast light on the path of &#34;Reasoning Tokens.&#34;</p><p><strong>Introduction</strong></p><p>As the authors of <a href="https://arxiv.org/abs/2211.00593?ref=reasoning-tokens.ghost.io" rel="noreferrer">&#34;Interpretability in the wild&#34;</a> have taught us, from looking inside transformers, we know that the computation of the next token includes some information computed in previous steps.  This may seem obvious at first glance, but there is more to this affirmation than what meets the eye. This means the language model expends some internal &#34;cognitive power&#34; processing and storing information that will be used, not for predicting the very next token but 2, 3, or even 10 tokens ahead.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png" alt="" loading="lazy" width="1520" height="654" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1520w" sizes="(min-width: 720px) 720px"/><figcaption><span>Internal computation of GPT-2, extracted from the &#34;Interpretability in the wild&#34; paper </span></figcaption></figure><p>As we can see from the image above, the attention heads produce computations that will be helpful only in the far future, and even some calculations that &#34;headge&#34; against the wrong answers, exposed in the paper as &#34;Negative Name Mover Heads&#34; or attention heads that suppress specific tokens.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png" alt="" loading="lazy" width="920" height="860" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 920w" sizes="(min-width: 720px) 720px"/><figcaption><span>Visual explanation extracted from the &#34;Do Language Models Plan for Future Tokens?&#34; paper</span></figcaption></figure><p>Further work has shown that LLMs indeed plan for future tokens. In the paper <a href="https://arxiv.org/abs/2404.00859?ref=reasoning-tokens.ghost.io" rel="noreferrer">&#34;Do Language Models Plan for Future Tokens?&#34;</a> the authors carefully crafted a mathematical formulation to impede what they call &#34;Pre-Caching,&#34; or the ability of the model to make intermediary computations that would be useful beyond the very next token. Their experiments found a small performance gap when the model was &#34;myopic&#34; or incapable of planning for future tokens. This is promising but could be better. This indicates that while GPTs plan ahead, most of their power is used to predict only the next word in the sequence. As a sanity check, this gap should increase as the length of the predicted text grows because the model would have more tokens to produce said computations, and indeed, that was what they found in the paper.</p><p><strong>How do we leverage that?</strong></p><p>What if we incentivized those intermediary calculations, which are useful only in future tokens, teaching the model to think ahead in a self-supervised way? It turns out that the formulation for such a task doesn&#39;t need to be that complicated.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png" alt="" loading="lazy" width="1312" height="504" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-1.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-1.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png 1312w" sizes="(min-width: 720px) 720px"/><figcaption><span>Gradient flow of Reasoning tokens!</span></figcaption></figure><p>In this first experiment, we introduce <strong>reasoning tokens</strong>! The model will produce two tokens for each token in the original sequence. As usual, the first token will be used to predict the next token. The second token, however, duplicates the input of the first one and does not receive a gradient &#34;answer&#34; from the very next token, only from future tokens; in fact, this token doesn&#39;t even participate in the calculation of the very next token. This incentivizes the model to &#34;pre-cache&#34; or <em>only</em> put information that is useful for the future in this spot. <em>But talk is cheap. Show me the results.</em></p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png" alt="" loading="lazy" width="2000" height="918" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-3.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-3.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-3.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png 2000w" sizes="(min-width: 720px) 720px"/><figcaption><span>Mini GPT-2 (10M params) trained on 82M tokens.</span></figcaption></figure><p>And the results are very promising, showing a reduction of <strong>35% in the loss</strong>! From 0.621 to 0.401. The experiment also shows that the model benefits from having multiple tokens to do its &#34;reasoning,&#34; forecasting the capability to form long-range dependencies. This validates the hypothesis that we can teach the models to plan for the future, an important first step to get to reasoning. </p><p>A GPT-2 Small (124M params) model was also trained on 300B tokens of the &#34;Open Web Text Corpus,&#34; and its results were also very promising, resulting in a 0.04 validation loss reduction from 2.85 to 2.81. In context, going from GPT-2 Large (~700M) to GPT-2 XL (1.5B) drops the validation loss by 0.13 in the same dataset. All training code was derived from Andrej Karpathy amazing <a href="https://github.com/karpathy/nanoGPT/tree/master?ref=reasoning-tokens.ghost.io" rel="noreferrer">GPT-2 implementation</a>.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png" alt="" loading="lazy" width="1088" height="628" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-4.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-4.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png 1088w" sizes="(min-width: 720px) 720px"/><figcaption><span>GPT-2 Small trained on 300B params - 1 Reasoning token</span></figcaption></figure><p><strong>What is next for Reasoning Tokens?</strong></p><p>Currently, I&#39;m experimenting with reasoning tokens in fine-tuned instruction following models, where planning can be much more useful. The formulation is very close to the first experiment. Still, this time, the model can choose when this internal reasoning will start, allowing it to choose when to reason before producing the next word in the sequence.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png" alt="" loading="lazy" width="1714" height="910" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-5.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-5.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-5.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png 1714w" sizes="(min-width: 720px) 720px"/><figcaption><span>Reasoning tokens in instruction tasks</span></figcaption></figure><p>The hypothesis being tested is that the addition of Reasoning Tokens can substitute and outperform models where a &#34;step by step&#34; explanation is included in the training phase. This would be useful because those explanations are expensive to produce/obtain. Although such explanations can be useful to the model, gradient descent could find other ways to do that reasoning using all the internal mathematical dimensions of the model in a way that does not necessarily make sense to us. It would be a great fit for &#34;Mixture of Experts&#34; (MoE) models, where we can have an expert just for the reasoning phase.</p><p>The future is bright. Stay tuned for the next advancements.</p>
        </section>

    </article>


</div></div>
  </body>
</html>
