<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/">Original</a>
    <h1>Qwen2.5-VL-32B: Smarter and Lighter</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><p><a href="https://chat.qwenlm.ai" target="_blank">QWEN CHAT</a>
<a href="https://github.com/QwenLM/Qwen2.5-VL" target="_blank">GITHUB</a>
<a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/collections/Qwen25-VL-58fbb5d31f1d47" target="_blank">MODELSCOPE</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><h2 id="introduction">Introduction</h2><p>At the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — <strong>Qwen2.5-VL-32B-Instruct</strong>. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:</p><ul><li><strong>Responses More Aligned with Human Preferences</strong>: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.</li><li><strong>Mathematical Reasoning</strong>: Significant improvement in the accuracy of solving complex mathematical problems.</li><li><strong>Fine-grained Image Understanding and Reasoning</strong>: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.</li></ul><h2 id="performance">Performance</h2><p>Extensive benchmarking against state-of-the-art (SoTA) models of comparable scale, <strong>Qwen2.5-VL-32B-Instruct</strong> has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger <strong>Qwen2-VL-72B-Instruct</strong>. Notably, it achieves significant advantages in multimodal tasks such as <strong>MMMU</strong>, <strong>MMMU-Pro</strong>, and <strong>MathVista</strong>, which focus on complex, multi-step reasoning. On <strong>MM-MT-Bench</strong>, a benchmark emphasizing subjective user experience evaluation, <strong>Qwen2.5-VL-32B-Instruct</strong> outperforms its predecessor <strong>Qwen2-VL-72B-Instruct</strong> by a substantial margin.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/qwen2.5vl-32b-vision.jpg" width="100%"/></figure><p>In addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/qwen2.5vl-32b-text.jpg" width="100%"/></figure><h2 id="demo-cases">Demo Cases</h2><div><div><p><span>Fine-grained Image Understanding and Reasoning</span>
<a>Next</a></p><div><p>User</p><p>I am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?
<img loading="lazy" src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/reasoning.png" alt="image"/></p><p>Qwen2.5-VL-32B</p><div><p>To determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:</p><ol><li><strong>Current Time</strong>: It is 12:00.</li><li><strong>Distance to Destination</strong>: 110 kilometers.</li><li><strong>Speed Limit for Trucks</strong>: The speed limit sign indicates that the maximum speed for trucks is <strong>100 km/h</strong>.</li></ol><h3 id="step-by-step-analysis">Step-by-Step Analysis:</h3><h4 id="1-calculate-the-required-travel-time">1. <strong>Calculate the Required Travel Time</strong></h4><p>The formula to calculate travel time is:
\[
\text{Time} = \frac{\text{Distance}}{\text{Speed}}
\]
Given:</p><ul><li>Distance = 110 km</li><li>Maximum Speed = 100 km/h</li></ul><p>Substitute the values:
\[
\text{Time} = \frac{110 \text{ km}}{100 \text{ km/h}} = 1.1 \text{ hours}
\]</p><h4 id="2-convert-time-to-hours-and-minutes">2. <strong>Convert Time to Hours and Minutes</strong></h4><p>1.1 hours is equivalent to:
\[
1 \text{ hour} + 0.1 \times 60 \text{ minutes} = 1 \text{ hour} + 6 \text{ minutes} = 1 \text{ hour and } 6 \text{ minutes}
\]</p><h4 id="3-determine-arrival-time">3. <strong>Determine Arrival Time</strong></h4><p>If you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be:
\[
12:00 + 1 \text{ hour and } 6 \text{ minutes} = 13:06
\]</p><h4 id="4-compare-with-target-time">4. <strong>Compare with Target Time</strong></h4><p>You want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is <strong>after</strong> 13:00.</p><h3 id="conclusion">Conclusion:</h3><p>Given the speed limit of 100 km/h for trucks, it is <strong>not possible</strong> to reach a place 110 kilometers away before 13:00.</p><p>Thus, the final answer is:
\[
\boxed{\text{No}}
\]</p></div></div></div></div><h2 id="next-step">Next Step</h2><p>While <strong>Qwen2.5-VL-32B</strong> has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of <strong>“fast thinking”</strong>. Our next research direction will prioritize <strong>long and effective reasoning processes</strong> to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.</p><h2 id="citation">Citation</h2><p>If you find our model helpful, feel free to cite it:</p><pre tabindex="0"><code>@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}
</code></pre></div></article></div></div>
  </body>
</html>
