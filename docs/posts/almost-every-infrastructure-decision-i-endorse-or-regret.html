<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/">Original</a>
    <h1>Almost every infrastructure decision I endorse or regret</h1>
    
    <div id="readability-page-1" class="page"><div><figure><img src="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/images/network-image.jpg" alt="Picture of networking"/><figcaption><p>Image from UnSplash</p></figcaption></figure><p>I’ve led infrastructure at a <a href="https://cresta.com/careers/" target="_blank" rel="noopener">startup</a> for the past 4 years that has had
to scale quickly. From the beginning I made some core decisions that the
company has had to stick to, for better or worse, these past four years. This post
will list some of the major decisions made and if I endorse them for your
startup, or if I regret them and advise you to pick something else.</p><h2 id="aws">AWS
<a href="#aws"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><h3 id="picking-aws-over-google-cloud">Picking AWS over Google Cloud
<a href="#picking-aws-over-google-cloud"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Early on, we were using both GCP and AWS. During that time, I had
no idea who my “account manager” was for Google Cloud, while at the same
time I had regular cadence meetings with our AWS account manager. There is a feel
that Google lives on robots and automation, while Amazon lives with a customer focus.
This support has helped us when evaluating new AWS services. Besides support, AWS has done a great job around stability
and minimizing backwards incompatible API changes.</p><p>There was a time when Google Cloud was the choice for Kubernetes clusters, especially
when there was ambiguity around if AWS would invest in EKS over <a href="https://aws.amazon.com/ecs/" target="_blank" rel="noopener">ECS</a>. Now though, with
all the extra Kubernetes integrations around AWS services (external-dns, external-secrets, etc),
this is not much of any issue anymore.</p><h3 id="ekshttpsawsamazoncomeks"><a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">EKS</a>
<a href="#ekshttpsawsamazoncomeks"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Unless you’re penny-pinching (and your time is free), there’s no reason to run your own
control plane rather than use EKS. The main advantage of using an alternative in AWS, like ECS,
is the deep integration into AWS services. Luckily, Kubernetes has caught up in many ways: for example,
using external-dns to integrate with Route53.</p><h3 id="eks-managed-addonshttpsdocsawsamazoncomekslatestuserguideeks-add-onshtml">EKS <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html" target="_blank" rel="noopener">managed addons</a>
<a href="#eks-managed-addonshttpsdocsawsamazoncomekslatestuserguideeks-add-onshtml"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟧 <em>Regret</em></p><p>We started with EKS managed addons because I thought it was the “right” way to use EKS. Unfortunately, we always
ran into a situation where we needed to customize the installation itself. Maybe the CPU requests, the image tag,
or some configmap. We’ve since switched to using helm charts for what were add-ons and things are running much better
with promotions that fit similar to our existing GitOps pipelines.</p><h3 id="rdshttpsawsamazoncomrds"><a href="https://aws.amazon.com/rds/" target="_blank" rel="noopener">RDS</a>
<a href="#rdshttpsawsamazoncomrds"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Data is the most critical part of your infrastructure. You lose your network: that’s downtime. You
lose your data: that’s a company ending event. The markup cost of using RDS (or any managed database)
is worth it.</p><h3 id="redis-elasticachehttpsawsamazoncomelasticacheredis"><a href="https://aws.amazon.com/elasticache/redis/" target="_blank" rel="noopener">Redis ElastiCache</a>
<a href="#redis-elasticachehttpsawsamazoncomelasticacheredis"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Redis has worked very well as a cache and general use product. It’s fast, the API is simple and
well documented, and the implementation is battle tested. Unlike other cache options, like
Memcached, Redis has a lot of features that make it useful for more than just caching. It’s a
great swiss army knife of “do fast data thing”.</p><p>Part of me is unsure what the state of Redis is for Cloud Providers, but I feel it’s so widely used by AWS customers
that AWS will continue to support it well.</p><h3 id="ecrhttpsawsamazoncomecr"><a href="https://aws.amazon.com/ecr/" target="_blank" rel="noopener">ECR</a>
<a href="#ecrhttpsawsamazoncomecr"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>We originally hosted on <a href="https://quay.io" target="_blank" rel="noopener">quay.io</a>. It was a hot mess of stability problems. Since moving to ECR,
things have been much more stable. The deeper permission integrations with EKS nodes or dev servers has also been a
big plus.</p><h3 id="aws-vpnhttpsawsamazoncomvpn"><a href="https://aws.amazon.com/vpn/" target="_blank" rel="noopener">AWS VPN</a>
<a href="#aws-vpnhttpsawsamazoncomvpn"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>There are Zero Trust VPN alternatives from companies like CloudFlare. I’m sure these products work
well, but a VPN is just so dead simple to setup and understand (“simplicity is preferable” is my mantra). We use
Okta to manage our VPN access and it’s been a great experience.</p><h3 id="aws-premium-supporthttpsawsamazoncompremiumsupport">AWS <a href="https://aws.amazon.com/premiumsupport/" target="_blank" rel="noopener">premium support</a>
<a href="#aws-premium-supporthttpsawsamazoncompremiumsupport"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟧 <em>Regret</em></p><p>It’s super expensive: almost (if not more) than the cost of another engineer. I think if we had very little
in house knowledge of AWS, it would be worth it.</p><h3 id="control-tower-account-factory-for-terraformhttpsdocsawsamazoncomcontroltowerlatestuserguideaft-overviewhtml"><a href="https://docs.aws.amazon.com/controltower/latest/userguide/aft-overview.html" target="_blank" rel="noopener">Control Tower Account Factory for Terraform</a>
<a href="#control-tower-account-factory-for-terraformhttpsdocsawsamazoncomcontroltowerlatestuserguideaft-overviewhtml"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Before integrating AFT, using control tower was a pain mostly because it was very difficult to automate. We’ve since
integrated AFT into our stack and spinning up accounts has worked well since. Another thing AFT makes easier is
standardizing tags for our accounts. For example, our production accounts have a tag that we can then use to make
peering decisions. Tags work better than organizations for us because the decision of “what properties describe
this account” isn’t always a tree structure.</p><h2 id="process">Process
<a href="#process"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><h3 id="automating-post-mortem-process-with-a-slack-bot">Automating post-mortem process with a slack bot
<a href="#automating-post-mortem-process-with-a-slack-bot"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Everyone is busy. It can feel like you’re the “bad guy” reminding people to fill out the post-mortem. Making a robot
be the bad guy had been great. It streamlines the process by nudging people to follow the SEV and post-mortem procedure.</p><p>It doesn’t have to be too complex to start. Just the basics of “It’s been an hour of no messages. Someone post an update” or
“It’s been a day with no calendar invite. Someone schedule the post-mortem meeting” can go a long ways.</p><p>🟩 <em>Endorse</em></p><p>Why reinvent the wheel? PagerDuty publishes a template of what to do during an incident. We’ve customized it a bit,
which is where the flexibility of Notion comes in handy, but it’s been a great starting point.</p><p>🟩 <em>Endorse</em></p><p>Alerting for a company goes like this:</p><ol><li>There are no alerts at all. We need alerts.</li><li>We have alerts. There are too many alerts, so we ignore them.</li><li>We’ve prioritized the alerts. Now only the critical ones wake me up.</li><li>We ignore the non-critical alerts.</li></ol><p>We have a two tiered alerting setup: critical and non-critical. Critical alerts wake people up. Non-critical alerts
are expected to ping the on-call async (email). The problem is that non-critical alerts are often ignored. To resolve
this, we have regular (usually every 2 weeks) PagerDuty review meetings where we go over all our alerts. For the critical
alerts, we discuss if it should stay critical. Then, we iterate the non-critical alerts (usually picking a few each meeting)
and discuss what we can do to clear those out as well (usually tweaking the threshold or creating some automation).</p><h3 id="monthly-cost-tracking-meetings">Monthly cost tracking meetings
<a href="#monthly-cost-tracking-meetings"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Early on, I set up a monthly meeting to go over all of our SaaS cost (AWS, DataDog, etc). Previously, this was just
something reviewed from a finance perspective, but it’s hard for them to answer general questions around “does this cost
number seem right”. During these meetings, usually attended by both finance and engineering, we go over every software
related bill we get and do a gut check of “does this cost sound right”. We dive into the numbers of each of the high bills
and try to break them down.</p><p>For example, with AWS we group items by tag and separate them by account. These two dimensions, combined with the general
service name (EC2, RDS, etc) gives us a good idea of where the major cost drivers are. Some things we do with this data
are go deeper into spot instance usage or which accounts contribute to networking costs the most. But don’t stop at
just AWS: go into all the major spend sinks your company has.</p><p>🟥 <em>Regret</em></p><p>Everyone should do post-mortems. Both <a href="https://www.datadoghq.com/product/incident-management/" target="_blank" rel="noopener">DataDog</a> and PagerDuty have integrations to manage writing post-mortems and we tried
each.
Unfortunately, they both make it hard to customize the post-mortem process. Given how powerful wiki-ish tools
like Notion are, I think it’s better to use a tool like that to manage post-mortems.</p><h3 id="not-using-function-as-a-servicefaas-more">Not using Function as a Service(FaaS) more
<a href="#not-using-function-as-a-servicefaas-more"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>There are no great FaaS options for running GPU workloads, which is why we could never go fully FaaS. However,
many CPU workloads could be FaaS (<a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener">lambda</a>, etc). The biggest counter-point people bring up is the cost. They’ll
say something like “This EC2 instance type running 24/7 at full load is way less expensive than a Lambda running”.
This is true, but it’s also a false comparison. Nobody runs a service at 100% CPU utilization and
moves on with their life. It’s always on some scaler that says “Never reach 100%. At 70% scale up another”. And it’s
always unclear when to scale back down, instead it’s a heuristic of “If we’ve been at 10% for 10 minutes, scale down”.
Then, people assume spot instances when they aren’t always on market.</p><p>Another hidden benefit of Lambda is that it’s very easy to track costs with high accuracy. When deploying services
in Kubernetes, cost can get hidden behind other per node objects or other services running on the same node.</p><h3 id="gitopshttpswwwgitopstech"><a href="https://www.gitops.tech/" target="_blank" rel="noopener">GitOps</a>
<a href="#gitopshttpswwwgitopstech"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>GitOps has so far scaled pretty well and we use it for many parts of our infrastructure: services,
terraform, and config to name a few. The main downside is that pipeline oriented workflows give
a clear picture of “here is the box that means you did a commit and here are arrows that go from
that box to the end of the pipeline”. With GitOps we’ve had to invest in tooling to help people answer
questions like “I did a commit: why isn’t it deployed yet”.</p><p>Even still, the flexibility of GitOps has been a huge win and I strongly recommend it for your company.</p><h3 id="prioritizing-team-efficiency-over-external-demands">Prioritizing team efficiency over external demands
<a href="#prioritizing-team-efficiency-over-external-demands"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Most likely, your company is not selling the infrastructure itself, but another product. This puts pressure on the
team to deliver features and not scale your own workload. But just like airplanes tell you to put your own mask on
first, you need to make sure your team is efficient. With rare exception, I have never regretted prioritizing
taking time to write some automation or documentation.</p><h3 id="multiple-applications-sharing-a-database">Multiple applications sharing a database
<a href="#multiple-applications-sharing-a-database"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>Like most tech debt, we didn’t <em>make</em> this decision, we just did not <em>not</em> make this decision. Eventually, someone
wants the product to do a new thing and makes a new table. This feels good because there are now foreign keys between
the two tables. But since <em>everything</em> is owned by <em>someone</em> and that <em>someone</em> is a row in a table, you’ve got
foreign keys between all objects in the entire stack.</p><p>Since the database is used by <em>everyone</em>, it becomes cared for by <em>no one</em>. Startups don’t have the luxury of a DBA,
and everything owned by <em>no one</em> is owned by <em>infrastructure</em> eventually.</p><p>The biggest problem with a shared database are:</p><ul><li>Crud accumulates in the database, and it’s unclear if it can be deleted.</li><li>When there are performance issues, infrastructure (without deep product knowledge) has to debug the database and figure out who to redirect to</li><li>Database users can push bad code that does bad things to the database. These bad things may PagerDuty alert the
infrastructure team (since they own the database). It feels bad to wake up one team for another team’s issue. With application owned databases,
the application team is the first responder.</li></ul><p>All that said, I’m not against stacks that want to share a single database either. Just be aware of the tradeoffs above
and have a good story for how you’ll manage them.</p><h2 id="saas">SaaS
<a href="#saas"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><h3 id="not-adopting-an-identity-platform-early-on">Not adopting an identity platform early on
<a href="#not-adopting-an-identity-platform-early-on"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>I stuck with Google Workspace at the start, using it to create groups for employees as a way to assign permissions. It just isn’t flexible enough.
In retrospect, I wish we had picked up <a href="https://www.okta.com/" target="_blank" rel="noopener">Okta</a> much sooner. It’s worked very well, has integrations for almost everything,
and solves a lot of compliance/security aspects. Just lean into an identity solution early on and only accept SaaS
vendors that integrate with it.</p><h3 id="notionhttpswwwnotionso"><a href="https://www.notion.so/" target="_blank" rel="noopener">Notion</a>
<a href="#notionhttpswwwnotionso"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Every company needs a place to put documentation. Notion has been a great choice and worked much easier than things
I’ve used in the past (Wikis, Google Docs, Confluence, etc). Their Database concept for page organization has also
allowed me to create pretty sophisticated organizations of pages.</p><h3 id="slackhttpsslackcom"><a href="https://slack.com/" target="_blank" rel="noopener">Slack</a>
<a href="#slackhttpsslackcom"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Thank god I don’t have to use HipChat anymore. Slack is great as a default communication tool, but to reduce stress
and noise I recommend:</p><ul><li>Using threads to condense communication</li><li>Communicating expectations that people may not respond quickly to messages</li><li>Discourage private messages and encourage public channels.</li></ul><h3 id="moving-off-jirahttpswwwatlassiancomsoftwarejira-onto-linearhttpslinearapp">Moving off <a href="https://www.atlassian.com/software/jira" target="_blank" rel="noopener">JIRA</a> onto <a href="https://linear.app/" target="_blank" rel="noopener">linear</a>
<a href="#moving-off-jirahttpswwwatlassiancomsoftwarejira-onto-linearhttpslinearapp"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Not even close. JIRA is so bloated I’m worried running it in an AI company it would just turn fully sentient. When
I’m using Linear, I will often think “I wonder if I can do X” and then I’ll try and I can!</p><h3 id="not-using-terraform-cloudhttpswwwterraformiocloud">Not using <a href="https://www.terraform.io/cloud" target="_blank" rel="noopener">Terraform Cloud</a>
<a href="#not-using-terraform-cloudhttpswwwterraformiocloud"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>No Regrets</em></p><p>Early on, I tried to migrate our terraform to Terraform Cloud. The biggest downside was that I couldn’t justify the
cost. I’ve since moved us to <a href="https://www.runatlantis.io/" target="_blank" rel="noopener">Atlantis</a>, and it has worked well enough. Where atlantis
falls short, we’ve written a bit of automation in our CI/CD pipelines to make up for it.</p><h3 id="github-actionshttpsdocsgithubcomenactions-for-cicd"><a href="https://docs.github.com/en/actions" target="_blank" rel="noopener">GitHub actions</a> for CI/CD
<a href="#github-actionshttpsdocsgithubcomenactions-for-cicd"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟧 <em>Endorse-ish</em></p><p>We, like most companies, host our code on GitHub. While originally using CircleCI, we’ve switched
to Github actions for CI/CD. The marketplace of actions available to use for your workflows is
large and the syntax is easy to read. The main downside of Github actions is their support
for self-hosted workflows is very limited. We’re using EKS and <a href="https://github.com/actions/actions-runner-controller" target="_blank" rel="noopener">actions-runner-controller</a>
for our self-hosted runners
hosted in EKS, but the integration is often buggy (but nothing we cannot work around).
I hope GitHub takes Kuberentes self-hosting more seriously in the future.</p><h3 id="datadoghttpswwwdatadoghqcom"><a href="https://www.datadoghq.com/" target="_blank" rel="noopener">Datadog</a>
<a href="#datadoghttpswwwdatadoghqcom"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>Datadog is a great product, but it’s expensive. More than just expensive, I’m worried
their cost model is especially bad for Kubernetes clusters and for AI companies. Kubernetes
clusters are most cost-effective when you can rapidly spin up and down many nodes, as well
as use spot instances. Datadog’s pricing model is based on the number of instances you
have and that means even if we have no more than 10 instances up at once, if we spin up
and down 20 instances in that hour, we pay for 20 instances. Similarly, AI companies
tend to use GPUs heavily. While a CPU node could have dozens of services running at once,
spreading the per node Datadog cost between many use cases, a GPU node is likely to have
only one service using it, making the per <strong>service</strong> Datadog cost much higher.</p><p>🟩 <em>Endorse</em></p><p>Pagerduty is a great product and well priced. We’ve never regretted picking it.</p><h2 id="software">Software
<a href="#software"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><h3 id="schema-migration-by-diff">Schema migration by Diff
<a href="#schema-migration-by-diff"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟧 <em>Endorse-ish</em></p><p>Schema management is hard no matter how you do it, mostly because of how scary it is. Data is important and a bad
schema migration can delete data. Of all the scary ways to solve this hard problem, I’ve been very happy with the idea
of checking in the entire schema into git and then using a <a href="https://michaelsogos.github.io/pg-diff/" target="_blank" rel="noopener">tool</a> to generate
the SQL to sync the database to the schema.</p><h2 id="ubuntu-for-dev-servers">Ubuntu for dev servers
<a href="#ubuntu-for-dev-servers"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>🟩 <em>Endorse</em></p><p>Originally I tried making the dev servers the same base OS that our Kubernetes nodes ran on, thinking this would make
the development environment closer to prod. In retrospect, the effort isn’t worth it. I’m happy we are sticking
with Ubuntu for development servers. It’s a well-supported OS and has most of the packages we need.</p><h3 id="appsmithhttpswwwappsmithcom"><a href="https://www.appsmith.com/" target="_blank" rel="noopener">AppSmith</a>
<a href="#appsmithhttpswwwappsmithcom"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>We frequently need to automate some process for an internal engineer: restart/promote/diagnose/etc. It’s easy enough
for us to make APIs to solve these problems, but it’s a bit annoying debugging someone’s specific installation of a
CLI/os/dependencies/etc. Being able to make a simple UI for engineers to interact with our scripts is very useful.</p><p>We self-host our AppSmith. It works … well enough. Of course there are things we would change, but it is enough
for the “free” price point. I originally explored deeper integration with <a href="https://retool.com/" target="_blank" rel="noopener">retool</a>, but I couldn’t justify the price
point for what, at the time, was just a few integrations.</p><h3 id="helmhttpshelmsh"><a href="https://helm.sh/" target="_blank" rel="noopener">helm</a>
<a href="#helmhttpshelmsh"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Helm v2 got a bad reputation (for good reason), but helm v3 has worked … well enough. There are still issues
with deploying CRDs and educating developers on why their helm chart did not deploy correctly. Overall, however,
helm works well enough as a way to package and deploy versioned Kubernetes objetcs and the Go templating language
is difficult to debug, but powerful.</p><h3 id="helm-charts-in-ecroci">helm charts in ECR(oci)
<a href="#helm-charts-in-ecroci"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Originally our helm charts were hosted inside S3 and downloaded with a plugin. The main downsides were needing to install
a custom helm plugin and manually managing lifecycles. We’ve since switched to OCI stored helm charts and haven’t
had any issues with this setup.</p><h3 id="bazelhttpsbazelbuild"><a href="https://bazel.build/" target="_blank" rel="noopener">bazel</a>
<a href="#bazelhttpsbazelbuild"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟧 <em>Unsure</em></p><p>To be fair, a lot of smart people like bazel, so I’m sure it’s not a <em>bad</em> choice to make.</p><p>When deploying Go services, bazel personally feels like overkill. I think Bazel is a great choice if your last company
used bazel, and you feel home sick. Otherwise, we have a build system that only a few engineers can dive deeply into,
compared to GitHub Actions, where it seems everyone knows how to get their hands dirty.</p><h3 id="not-using-open-telemetryhttpsopentelemetryio-early">Not using open <a href="https://opentelemetry.io/" target="_blank" rel="noopener">telemetry</a> early
<a href="#not-using-open-telemetryhttpsopentelemetryio-early"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>We started off sending metrics directly to DataDog using DataDog’s API. This has made it very hard to rip them out.</p><p>Open telemetry wasn’t as mature 4 years ago, but it’s gotten much better. I think the metrics telemetry is still
a bit immature, but the tracing is great. I recommend using it from the start for any company.</p><h3 id="picking-renovatebothttpsdocsrenovatebotcom-over-dependabothttpsgithubcomdependabot">Picking <a href="https://docs.renovatebot.com/" target="_blank" rel="noopener">renovatebot</a> over <a href="https://github.com/dependabot" target="_blank" rel="noopener">dependabot</a>
<a href="#picking-renovatebothttpsdocsrenovatebotcom-over-dependabothttpsgithubcomdependabot"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>I honestly wish we had thought about “keep your dependencies up to date” sooner. When you wait on this too long,
you end up with versions so old the upgrade process is long and inevitably buggy. Renovatebot has worked well with
the flexibility to customize it to your needs. The biggest, and it’s pretty big, downside is that it’s VERY complicated
to setup and debug. I guess it’s the best of all the bad options.</p><h3 id="kuberneteshttpskubernetesio"><a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>
<a href="#kuberneteshttpskubernetesio"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>You need <strong>something</strong> to host your long running services. Kuberentes is a popular choice and it’s worked well for us.
The Kubernetes community has done a great job integrating AWS services (like load balancers, DNS, etc) into the
Kubernetes ecosystem. The biggest downside with any flexible system is that there are a lot of ways to use it, and
any system with a lot of ways to use has a lot of ways to use wrong.</p><blockquote><p>any system with a lot of ways to use has a lot of ways to use wrong</p><ul><li>Jack Lindamood</li></ul></blockquote><h3 id="buying-our-own-ips">Buying our own IPs
<a href="#buying-our-own-ips"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>If you work with external partners, you’ll frequently need to publish a whitelist of your IPs for them. Unfortunately,
you may later develop more systems that need their own IPs. Buying your own IP block is a great way to avoid this by
giving the external partner a larger CIDR block to whitelist.</p><h3 id="picking-fluxhttpsfluxcdio-for-k8s-gitops">Picking <a href="https://fluxcd.io/" target="_blank" rel="noopener">Flux</a> for k8s GitOps
<a href="#picking-fluxhttpsfluxcdio-for-k8s-gitops"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>No Regrets</em></p><p>An early GitOps choice for Kubernetes was to decide between ArgoCD and Flux: I went with Flux (v1
at the time). It’s worked very well. We’re currently using Flux 2. The only downside is we’ve had to make our
own tooling to help people understand the state of their deployments.</p><p>I hear great things about ArgoCD, so I’m sure if you picked that you’re also safe.</p><h3 id="karpenterhttpskarpentersh-for-node-management"><a href="https://karpenter.sh/" target="_blank" rel="noopener">Karpenter</a> for node management
<a href="#karpenterhttpskarpentersh-for-node-management"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>If you’re using EKS (and not fully on Fargate), you should be using Karpenter. 100% full stop. We’ve used other autoscalers,
including the default Kubernetes autoscaler and <a href="https://spot.io/" target="_blank" rel="noopener">SpotInst</a>. Between them all, Karpenter has
been the most reliable and the most cost-effective.</p><h3 id="using-sealedsecretshttpsgithubcombitnami-labssealed-secrets-to-manage-k8s-secrets">Using <a href="https://github.com/bitnami-labs/sealed-secrets" target="_blank" rel="noopener">SealedSecrets</a> to manage k8s secrets
<a href="#using-sealedsecretshttpsgithubcombitnami-labssealed-secrets-to-manage-k8s-secrets"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>My original thought was to push secret management to something GitOps styled. The two main drawbacks of using
sealed-secrets were:</p><ul><li>It was more complicated for less infra knowledgeable developers to create/update secrets</li><li>We lost all the existing automations that AWS has around rotating secrets (<a href="https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/" target="_blank" rel="noopener">for example</a>)</li></ul><h3 id="using-externalsecretshttpsgithubcomexternal-secretsexternal-secrets-to-manage-k8s-secrets">Using <a href="https://github.com/external-secrets/external-secrets" target="_blank" rel="noopener">ExternalSecrets</a> to manage k8s secrets
<a href="#using-externalsecretshttpsgithubcomexternal-secretsexternal-secrets-to-manage-k8s-secrets"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>ExternalSecrets has worked very well to sync AWS -&gt; Kubernetes secrets. The process is simple for developers to
understand and lets us take advantage of terraform as a way to easily create/update the secrets inside AWS, as well
as give users a UI to use to create/update the secrets.</p><h3 id="using-externaldnshttpsgithubcomkubernetes-sigsexternal-dns-to-manage-dns">Using <a href="https://github.com/kubernetes-sigs/external-dns" target="_blank" rel="noopener">ExternalDNS</a> to manage DNS
<a href="#using-externaldnshttpsgithubcomkubernetes-sigsexternal-dns-to-manage-dns"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>ExternalDNS is a great product. It syncs our Kubernetes -&gt; Route53 DNS entries and has given us very little
problems in the past 4 years.</p><h3 id="using-cert-managerhttpsgithubcomcert-managercert-manager-to-manage-ssl-certificates">Using <a href="https://github.com/cert-manager/cert-manager" target="_blank" rel="noopener">cert-manager</a> to manage SSL certificates
<a href="#using-cert-managerhttpsgithubcomcert-managercert-manager-to-manage-ssl-certificates"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Very intuitive to configure and has worked well with no issues. Highly recommend using it to create your Let’s Encrypt
certificates for Kubernetes. The only downside is we sometimes have ANCIENT (SaaS problems am I right?) tech stack
customers that don’t trust Let’s Encrypt, and you need to go get a paid cert for those.</p><h3 id="bottlerockethttpsawsamazoncombottlerocket-for-eks"><a href="https://aws.amazon.com/bottlerocket/" target="_blank" rel="noopener">Bottlerocket</a> for EKS
<a href="#bottlerockethttpsawsamazoncombottlerocket-for-eks"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟥 <em>Regret</em></p><p>Our EKS cluster used to run on Bottlerocket. The main downside was we frequently ran into networking CSI issues
and debugging the bottlerocket images were much harder than debugging the standard EKS AMIs. Using the
<a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html" target="_blank" rel="noopener">EKS optimized</a>
AMIs for our nodes has given us no problems, and we still have a backdoor to debug the node itself when there are
strange networking issues.</p><h3 id="picking-terraformhttpswwwterraformio-over-cloudformationhttpsawsamazoncompmcloudformation">Picking <a href="https://www.terraform.io/" target="_blank" rel="noopener">Terraform</a> over <a href="https://aws.amazon.com/pm/cloudformation/" target="_blank" rel="noopener">Cloudformation</a>
<a href="#picking-terraformhttpswwwterraformio-over-cloudformationhttpsawsamazoncompmcloudformation"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>Endorse</em></p><p>Using Infrastructure as Code is a must for any company. Being in AWS, the two
main choices are Cloudformation and Terraform. I’ve used both and don’t
regret sticking with Terraform. It’s been easy to extend for other SaaS providers
(like Pagerduty), the syntax is easier to read than CloudFormation, and hasn’t been
a blocker or slowdown for us.</p><h3 id="not-using-more-code-ish-iac-solutions-pulumihttpswwwpulumicom-cdkhttpsawsamazoncomcdk-etc">Not using more code-ish IaC solutions (<a href="https://www.pulumi.com/" target="_blank" rel="noopener">Pulumi</a>, <a href="https://aws.amazon.com/cdk/" target="_blank" rel="noopener">CDK</a>, etc)
<a href="#not-using-more-code-ish-iac-solutions-pulumihttpswwwpulumicom-cdkhttpsawsamazoncomcdk-etc"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>No Regrets</em></p><p>While Terraform and CloudFormation are data files (HCL and YAML/JSON) that describe
your infrastructure, solutions like Pulumi or CDK allow you to write code that
does the same. Code is of course powerful, but I’ve found the restrictive nature of
Terraform’s HCL to be a benefit with reduced complexity. It’s not that it’s
impossible to write complex Terraform: it’s just that it’s more obvious when it’s
happening.</p><p>Some of these solutions, like Pulumi, were invented many years ago while
Terraform lacked a lot of the features it has today. Newer versions of Terraform
have integrated a lot of the features that we can use to reduce complexity. We instead
use a middleground that generates basic skeletons of our Terraform code for parts we want to abstract away.</p><h3 id="not-using-a-network-mesh-istiohttpsistioiolinkerdhttpslinkerdioetc">Not using a network mesh (<a href="https://istio.io/" target="_blank" rel="noopener">istio</a>/<a href="https://linkerd.io/" target="_blank" rel="noopener">linkerd</a>/etc)
<a href="#not-using-a-network-mesh-istiohttpsistioiolinkerdhttpslinkerdioetc"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>No regrets</em></p><p>Network meshes are really cool and a lot of smart people tend to endorse them, so I’m
convinced they are fine ideas. Unfortunately, I think companies underestimate the
complexity of things. My general infrastructure advice is “less is better”.</p><h3 id="nginxhttpsgithubcomkubernetesingress-nginx-load-balancer-for-eks-ingress"><a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">Nginx</a> load balancer for EKS ingress
<a href="#nginxhttpsgithubcomkubernetesingress-nginx-load-balancer-for-eks-ingress"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>🟩 <em>No Regrets</em></p><p>Nginx is old, it’s stable, and it’s battle tested.</p><h2 id="homebrewhttpsbrewsh-for-company-scripts"><a href="https://brew.sh/" target="_blank" rel="noopener">homebrew</a> for company scripts
<a href="#homebrewhttpsbrewsh-for-company-scripts"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>🟩 <em>Endorse</em></p><p>Your company will likely need a way to distribute scripts and binaries for your engineers to use. Homebrew has worked
<em>well enough</em> for both linux and Mac users as a way to distribute scripts and binaries.</p><h2 id="gohttpsgodev-for-services"><a href="https://go.dev/" target="_blank" rel="noopener">Go</a> for services
<a href="#gohttpsgodev-for-services"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>🟩 <em>Endorse</em></p><p>Go has been easy for new engineers to pick up and is a great choice overall. For non-GPU services that are mostly network
IO bound, Go should be your default language choice.</p></div></div>
  </body>
</html>
