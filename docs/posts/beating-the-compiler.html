<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mattkeeter.com/blog/2024-07-12-interpreter/">Original</a>
    <h1>Beating the Compiler</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>In modern times, everyone knows that writing assembly is a fool&#39;s errand:
compilers are the result of literal engineer-centuries of work, and they know
the processor much better than you do.</p>
<p>And yet – one hears <em>rumors</em>.</p>
<p>Written in <a href="https://jilp.org/vol5/v5paper12.pdf">ancient tomes</a>,
muttered in <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">quiet watering holes</a>,
scrawled on the walls of
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">bygone temples</a>,
hinted at by
<a href="https://llvm.org/docs/LangRef.html#calling-conventions">mysterious texts</a>;
the rumors paint a specific picture:</p>
<blockquote>
<p>Compilers are bad at generating code for interpreters, and it&#39;s possible to
outperform them by writing your interpreter in assembly.</p>
</blockquote>
<p>I recently <a href="https://www.mattkeeter.com/projects/raven">wrote a fast interpreter</a> for the
<a href="https://wiki.xxiivv.com/site/uxn.html">Uxn CPU</a>,
a stack-based architecture with 256 opcodes.  The interpreter is a simple loop
which reads a byte from RAM then selects the appropriate instruction:</p>
<pre><code>impl Uxn {
    /// Runs the VM starting at the given address until it terminates
    #[inline]
    pub fn run&lt;D: Device&gt;(&amp;mut self, dev: &amp;mut D, mut pc: u16) {
        loop {
            let op = self.ram[usize::from(pc)];
            pc = pc.wrapping_add(1);
            let Some(next) = self.op(op, dev, pc) else {
                break;
            };
            pc = next;
        }
    }

    /// Executes a single operation
    #[inline]
    fn op&lt;D: Device&gt;(&amp;mut self, op: u8, dev: &amp;mut D, pc: u16) -&gt; Option&lt;u16&gt; {
        match op {
            0x00 =&gt; op::brk(self, dev, pc),
            0x01 =&gt; op::inc::&lt;0b000&gt;(self, dev, pc),
            0x02 =&gt; op::pop::&lt;0b000&gt;(self, dev, pc),
            0x03 =&gt; op::nip::&lt;0b000&gt;(self, dev, pc),
            0x04 =&gt; op::swp::&lt;0b000&gt;(self, dev, pc),
            0x05 =&gt; op::rot::&lt;0b000&gt;(self, dev, pc),
            0x06 =&gt; op::dup::&lt;0b000&gt;(self, dev, pc),
            0x07 =&gt; op::ovr::&lt;0b000&gt;(self, dev, pc),
            0x08 =&gt; op::equ::&lt;0b000&gt;(self, dev, pc),
            0x09 =&gt; op::neq::&lt;0b000&gt;(self, dev, pc),
            0x0a =&gt; op::gth::&lt;0b000&gt;(self, dev, pc),
            0x0b =&gt; op::lth::&lt;0b000&gt;(self, dev, pc),
            0x0c =&gt; op::jmp::&lt;0b000&gt;(self, dev, pc),
            0x0d =&gt; op::jcn::&lt;0b000&gt;(self, dev, pc),
            0x0e =&gt; op::jsr::&lt;0b000&gt;(self, dev, pc),
            // ... etc
        }
    }
}
</code></pre>
<p>All of the opcode implementations end up monomorphized and inlined into the body
of <code>Uxn::run(..)</code>, and the compiler is smart enough to keep key values in
registers.  This makes it relatively fast; I see 10-20% speedup over the
<a href="https://git.sr.ht/%7Erabbits/uxn/">reference implementation</a>.</p>
<p>Let&#39;s look at the assembly and see what the compiler is doing – and whether we
can do any better.  For context, the Uxn CPU has four different memories:</p>
<ul>
<li>The data stack, which is a <code>[u8; 256]</code> along with a <code>u8</code> index</li>
<li>The return stack, which has the same format</li>
<li>RAM, which is a <code>[u8; 65535]</code></li>
<li>Device memory, which we&#39;ll ignore for the moment (along with the <code>D: Device</code>
argument)</li>
</ul>
<p>During evaluation, we also track the program counter <code>pc</code>, which is a <code>u16</code> used
to index into the RAM.  In each cycle, we load a byte from RAM, then call the
appropriate opcode.  Some opcodes can also read and write to RAM, so
self-modifying code is possible!</p>
<p>By examining the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">assembly</a>, we can
reverse-engineer which values are stored where.  Consider the <code>INC</code> operation,
which loads a value from the top of the data stack and increments it:</p>
<pre><code>; INC
0x100002d4c: ldrb w8, [x25]     ; read the current data stack index
0x100002d50: ldrb w9, [x24, x8] ; read a byte from the data stack
0x100002d54: add w9, w9, #1     ; increment that byte
0x100002d58: strb w9, [x24, x8] ; write that byte back to the stack
0x100002d5c: b 0x100002d1c      ; jump back to the dispatch loop
</code></pre>
<p>From this assembly, we learn the following:</p>
<ul>
<li><code>x25</code> is the <em>address</em> of the data stack index (not its value!)</li>
<li><code>x24</code> is the address of the data stack array</li>
<li><code>w9</code> is used as a temporary register</li>
</ul>
<p>Similarly, <code>INCr</code> – increment the top value in the <strong>return</strong> stack – teaches us
that <code>x22</code> and <code>x23</code> are the return stack&#39;s data and index addresses.</p>
<p><code>JMP</code> shows that our program counter is stored in <code>w27</code>:</p>
<pre><code>; JMP
0x100002eac: ldrb w8, [x25]      ; read the current data stack index
0x100002eb0: ldrsb w9, [x24, x8] ; read a signed jump offset from the data stack
0x100002eb4: sub w8, w8, #1      ; decrement the data stack index
0x100002eb8: strb w8, [x25]      ; write back the data stack index
0x100002ebc: add w27, w27, w9    ; apply the jump to our program counter
0x100002ec0: b 0x100002d1c       ; jump back to the dispatch loop
</code></pre>
<p>Finally, the dispatch loop itself is worth examining:</p>
<pre><code>0x100002d1c: and x10, x27, #0xffff          ; mask pc to a u16
0x100002d20: ldr x8, [x20, #256]            ; load RAM base from *mut Uxn
0x100002d24: ldrb w10, [x8, x10]            ; load opcode byte from RAM
0x100002d28: add w27, w27, #1               ; increment pc
0x100002d2c: adr x11, #-96                  ; load base for jump
0x100002d30: ldrh w12, [x27, x10, lsl  #1]  ; load per-opcode jump amount
0x100002d34: add x11, x11, x12, lsl #2      ; compute jump location
0x100002d38: br x11                         ; jump into opcode implementation
</code></pre>
<p>The compiler has generated a jump table of 256 offsets (each a 2-byte value,
indicated by <code>lsl #1</code>).  It reads an opcode-specific value from this table to
compute a jump target, then performs an indirect branch to jump into the
opcode&#39;s implementation.</p>
<p>We can run this in a debugger and dump the actual jump table:</p>
<pre><code>(lldb) disas -p -c3
raven-cli`raven_uxn::Uxn::run::had9dba0d7d1b5105:
-&gt;  0x100002d30 &lt;+236&gt;: ldrh   w12, [x27, x10, lsl  #1]
    0x100002d34 &lt;+240&gt;: add    x11, x11, x12, lsl #2
    0x100002d38 &lt;+244&gt;: br     x11
(lldb) reg read x27
     x27 = 0x0000000100170b10
(lldb) memory read -s2 -fu -c256 0x0000000100170b10
0x100170b10: 2923
0x100170b12: 31
0x100170b14: 36
0x100170b16: 40
0x100170b18: 44
0x100170b1a: 52
0x100170b1c: 28
0x100170b1e: 64
0x100170b20: 70
0x100170b22: 78
0x100170b24: 86
0x100170b26: 94
0x100170b28: 119
0x100170b2a: 102
0x100170b2c: 110
0x100170b2e: 125
; etc...
</code></pre>
<p>(indeed, this is how I generated the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">per-opcode instruction listing</a>)</p>
<hr/>
<p>Having looked at the assembly, there are two things that stick out as possible
inefficiencies:</p>
<ul>
<li>Some critical values (stack indices, the base address of RAM) are kept in
memory instead of registers; for example, <code>INC</code> has an extra load operation to
get the current data stack index.</li>
<li>The dispatch loop takes a single indirect branch to the opcode-specific
implementation.  This means that the branch will be nigh unpredictable!</li>
</ul>
<p>Profiling the code, the hottest instructions are all in the dispatch loop; the
<code>ldrh</code> takes over 1/3 of the total runtime!</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/profile.png" alt="screenshot of profiling info"/></p>
<p>(I&#39;m not confident that the profiler is attributing the time to the correct
specific instruction here, but the vibes definitely indicate that dispatch is
expensive)</p>
<hr/>
<p><a href="http://luajit.org/">LuaJIT</a> is the fast interpreter <em>par excellence</em>, and it&#39;s
written in assembly.  Mike Pall
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">specifically calls out</a>
keeping state in registers and indirect threading as two contributors to its
speed, which can only be
<a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">accomplished reliably</a>
in assembly.</p>
<p>Since persuading our compiler to generate extremely specific patterns is hard,
let&#39;s get started writing some assembly of our own.  My home machine is an M1
Macbook, so all of the assembly will be AArch64-flavored. The implementation
uses general-purpose registers; be aware that <code>w*</code> and <code>x*</code> refer to 32-bit and
64-bit views of the same register.</p>
<hr/>
<h3>Register assignment</h3>
<p>Our first optimization is to store <strong>all</strong> important data in registers, to avoid
superfluous loads and stores.  My implementation ends up using 9 registers
(<code>x0-x8</code>), along with a handful of scratch registers:</p>
<pre><code>; x0 - stack pointer (&amp;mut [u8; 256])
; x1 - stack index (u8)
; x2 - return stack pointer (&amp;mut [u8; 256])
; x3 - return stack index (u8)
; x4 - RAM pointer (&amp;mut [u8; 65536])
; x5 - program counter (u16), offset of the next value in RAM
; x6 - VM pointer (&amp;mut Uxn)
; x7 - Device handle pointer (&amp;DeviceHandle)
; x8 - Jump table pointer
; x9-15 - scratch registers
</code></pre>
<p>The <a href="https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)">AArch64 calling convention</a>
only gives you 8 input arguments, so we can&#39;t call a function directly with all
of these values in registers;
we&#39;ll need a C ABI-flavored entry point (<a href="#shims">discussed below</a>).</p>
<h3>Indirect threading</h3>
<p>Our second optimization is using
<a href="https://en.wikipedia.org/wiki/Threaded_code">threaded code</a>
to eliminate the dispatch loop.
Each opcode&#39;s implementation will end with a jump to the next opcode&#39;s
implementation.</p>
<p>Opcodes are stored as single bytes in VM RAM, with a base address of <code>x4</code>.  I&#39;ll
build a separate jump table of function pointers, then pass its address in
register <code>x8</code>.  On the Rust side, here&#39;s what that table looks like:</p>
<pre><code>extern &#34;C&#34; {
    fn BRK();
    fn INC();
    fn POP();
    fn NIP();
    fn SWP();
    fn ROT();
    fn DUP();
    fn OVR();
    fn EQU();
    // ...etc
}

const JUMP_TABLE: [unsafe extern &#34;C&#34; fn(); 256] = [
    (BRK as unsafe extern &#34;C&#34; fn()),
    (INC as unsafe extern &#34;C&#34; fn()),
    (POP as unsafe extern &#34;C&#34; fn()),
    (NIP as unsafe extern &#34;C&#34; fn()),
    (SWP as unsafe extern &#34;C&#34; fn()),
    (ROT as unsafe extern &#34;C&#34; fn()),
    (DUP as unsafe extern &#34;C&#34; fn()),
    (OVR as unsafe extern &#34;C&#34; fn()),
    (EQU as unsafe extern &#34;C&#34; fn()),
    (NEQ as unsafe extern &#34;C&#34; fn()),
    // ... etc
];
</code></pre>
<p>In assembly, we want to read the current byte from VM RAM (<code>x4</code>), use it to pick
an address in the jump table (<code>x8</code>), then jump to that address.  I defined a
macro to do this dispatch:</p>
<pre><code>.macro next
    ldrb w9, [x4, x5]          ; load the byte from RAM
    add x5, x5, #1             ; increment the program counter
    and x5, x5, #0xffff        ; wrap the program counter
    ldr x10, [x8, x9, lsl #3]  ; load the opcode implementation address
    br x10                     ; jump to the opcode&#39;s implementation
.endm
</code></pre>
<p>Notice that this is a <strong>macro</strong>, not a function; we&#39;ll add <code>next</code> to the end of
each opcode, which will expand into this text.</p>
<p>For example, here&#39;s <code>INC</code>:</p>
<pre><code>.global _INC
_INC:
    ldrb w9, [x0, x1]   ; read the byte from the top of the stack
    add w9, w9, #1      ; increment it
    strb w9, [x0, x1]   ; write it back
    next                ; jump to the next opcode
</code></pre>
<p>Unlike LuaJIT, there&#39;s no <strong>decoding</strong> step for instructions; there are no
register arguments, and the single-byte opcode uniquely defines program
behavior.</p>
<h3>Implementation</h3>
<p>Implementing the other 255 opcodes is mostly just turning the crank;
there&#39;s nothing particularly exotic here, just good honest assembly.</p>
<p>In many cases, I&#39;ll use helper macros to generate code for a group of
instructions:</p>
<pre><code>.macro binary_op op
    ldrb w10, [x0, x1]  ; read the top value from the data stack
    pop                 ; decrement the data stack index (this is a macro!)
    ldrb w11, [x0, x1]  ; read the next value from the data stack
    \op w10, w11, w10   ; do the actual math operation
    strb w10, [x0, x1]  ; write the result into the data stack
    next
.endm

.global _ADD
_ADD:
    binary_op add

.global _SUB
_SUB:
    binary_op sub

.global _MUL
_MUL:
    binary_op mul

.global _DIV
_DIV:
    binary_op udiv
</code></pre>
<p>The whole implementation ends up being about
<a href="https://github.com/mkeeter/raven/blob/main/raven-uxn/src/native/aarch64.s">2400 lines</a>.
It sounds like a lot, but only about half of that is unique:
most opcodes come in two flavors (with and without the <code>RET</code> flag),
which only differ in which stack is used.</p>
<h3>C Shims</h3>
<p>Of course, my whole program isn&#39;t hand-written in assembly.  We need a way to
call our assembly function from the rest of our (Rust) implementation.  This
looks like a (Rust) <code>entry</code> function, which calls into an (assembly)
<code>aarch64_entry</code> point (which is compatible with the C ABI):</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/entry.png" alt="diagram showing entry points"/></p>
<p>What do we actually pass into <code>aarch64_entry</code>?  We have too much state to pass
in function argument registers (<code>x0-x7</code>), so I defined a helper object which
contains everything we need:</p>
<pre><code>#[repr(C)]
pub(crate) struct EntryHandle {
    stack_data: *mut u8,
    stack_index: *mut u8,
    ret_data: *mut u8,
    ret_index: *mut u8,
    ram: *mut u8,
    vm: *mut core::ffi::c_void,  // *Uxn
    dev: *mut core::ffi::c_void, // *DeviceHandle
}

struct DeviceHandle&lt;&#39;a&gt;(&amp;&#39;a mut dyn Device);
</code></pre>
<p>The <code>DeviceHandle</code> is needed because <code>&amp;mut dyn Device</code> is a fat pointer, and is
therefore not safe to pass into a C function.  Like all computer problems, we
solve this with an extra level of indirection: put the <code>&amp;mut dyn Device</code> into a
<code>DeviceHandle</code>, then pass <em>its</em> address instead.</p>
<p>Calling into assembly is a simple matter of populating an <code>EntryHandle</code> object,
then branching into the danger zone:</p>
<pre><code>// Declaration of our entry point, written in assembly
extern &#34;C&#34; {
    pub fn aarch64_entry(
        h: *const EntryHandle,
        pc: u16,
        table: *const unsafe extern &#34;C&#34; fn(),
    ) -&gt; u16;
}

pub fn entry(vm: &amp;mut Uxn, dev: &amp;mut dyn Device, pc: u16) -&gt; u16 {
    let mut h = DeviceHandle(dev);
    let mut e = EntryHandle {
        stack_data: vm.stack.data.as_mut_ptr(),
        stack_index: &amp;mut vm.stack.index as *mut _,
        ret_data: vm.ret.data.as_mut_ptr(),
        ret_index: &amp;mut vm.ret.index as *mut _,
        ram: (*vm.ram).as_mut_ptr(),
        vm: vm as *mut _ as *mut _,
        dev: &amp;mut h as *mut _ as *mut _,
    };

    // SAFETY: do you trust me?
    unsafe {
        aarch64::aarch64_entry(&amp;mut e as *mut _, pc, JUMP_TABLE.as_ptr())
    }
}
</code></pre>
<p><code>aarch64_entry</code> is a hand-written entry point in the assembly code.  It shuffles
around registers to put everything in the right place for our opcodes, then
begins execution with the usual <code>next</code> macro:</p>
<pre><code>.global _aarch64_entry
_aarch64_entry:
    sub sp, sp, #0x200  ; make room in the stack
    stp   x29, x30, [sp, 0x0]   ; store stack and frame pointer
    mov   x29, sp

    // Unpack from EntryHandle into registers
    mov x5, x1 ; move PC (before overwriting x1)
    mov x8, x2 ; jump table (before overwriting x2)
    ldr x1, [x0, 0x8]  ; stack index pointer
    ldr x2, [x0, 0x10] ; ret data pointer
    ldr x3, [x0, 0x18] ; ret index pointer
    ldr x4, [x0, 0x20] ; RAM pointer
    ldr x6, [x0, 0x28] ; *mut Uxn
    ldr x7, [x0, 0x30] ; *mut DeviceHandle
    ldr x0, [x0, 0x00] ; stack data pointer (overwriting *EntryHandle)

    ; Convert from index pointers to index values in w1 / w3
    stp x1, x3, [sp, 0x10]      ; save stack index pointers
    ldrb w1, [x1]               ; load stack index
    ldrb w3, [x3]               ; load ret index

    ; Jump into the instruction list
    next
</code></pre>
<p>Finally, when exiting (via the <code>BRK</code> opcode), we need to update the data and
return stack indices, moving values from registers into the appropriate memory
addresses:</p>
<pre><code>.global _BRK
_BRK:
    ; Write index values back through index pointers
    ldp x9, x10, [sp, 0x10]     ; restore stack index pointers
    strb w1, [x9]               ; save data stack index
    strb w3, [x10]              ; save return stack index

    ldp   x29, x30, [sp, 0x0]   ; restore stack and frame pointer
    add sp, sp, #0x200          ; undo our stack offset

    mov x0, x5 ; return PC from function
    ret
</code></pre>
<h3>Device IO</h3>
<p>The <code>DEI</code> and <code>DEO</code> opcodes perform &#34;device I/O&#34;, which lets you attach
arbitrary peripherals to the system.  The most common set of peripherals is the
<a href="https://wiki.xxiivv.com/site/varvara.html">Varvara system</a>,
which adds everything you need to make the CPU into an actual computer: a
screen, keyboard and mouse input, audio, etc.</p>
<p>To keep the Uxn implementation generic, I defined a trait for a device:</p>
<pre><code>/// Trait for a Uxn-compatible device
pub trait Device {
    /// Performs the `DEI` operation for the given target
    ///
    /// This function must write its output byte to `vm.dev[target]`; the CPU
    /// evaluation loop will then copy this value to the stack.
    fn dei(&amp;mut self, vm: &amp;mut Uxn, target: u8);

    /// Performs the `DEO` operation on the given target
    ///
    /// The input byte will be written to `vm.dev[target]` before this function
    /// is called, and can be read by the function.
    ///
    /// Returns `true` if the CPU should keep running, `false` if it should
    /// exit.
    #[must_use]
    fn deo(&amp;mut self, vm: &amp;mut Uxn, target: u8) -&gt; bool;
}
</code></pre>
<p>The opcode implementation takes a <code>&amp;mut dyn Device</code>, i.e. something implementing
this trait, and calls trait methods on it:</p>
<pre><code>pub fn deo&lt;const FLAGS: u8&gt;(
    vm: &amp;mut Uxn,
    dev: &amp;mut dyn Device,
    pc: u16,
) -&gt; Option&lt;u16&gt; {
    let mut s = vm.stack_view::&lt;FLAGS&gt;();
    let i = s.pop_byte();
    let mut run = true;
    match s.pop() {
        Value::Short(v) =&gt; {
            let [lo, hi] = v.to_le_bytes();
            let j = i.wrapping_add(1);
            vm.dev[usize::from(i)] = hi;
            run &amp;= dev.deo(vm, i);
            vm.dev[usize::from(j)] = lo;
            run &amp;= dev.deo(vm, j);
        }
        Value::Byte(v) =&gt; {
            vm.dev[usize::from(i)] = v;
            run &amp;= dev.deo(vm, i);
        }
    }
    if run {
        Some(pc)
    } else {
        None
    }
}
</code></pre>
<p>However, this function is not compatible with the C ABI – it&#39;s both generic
<em>and</em> takes a trait object – so it can&#39;t be called directly from the <code>DEO</code>
opcode in assembly.</p>
<p>To let my opcodes call <code>DEO</code> and <code>DEI</code> functions, I again wrote a bunch of
shims:</p>
<pre><code>#[no_mangle]
extern &#34;C&#34; fn deo_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b000&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern &#34;C&#34; fn deo_2_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b001&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern &#34;C&#34; fn deo_r_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b010&gt;(dev.0, 0).is_some()
}

// etc, 16 functions in total for all DEI / DEO variants
</code></pre>
<p>The full path of the function looks something like this:</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/deo.png" alt="diagram showing deo calls"/></p>
<p>On the assembly side, there&#39;s one subtlety: during our normal opcode processing,
we keep data and return stack index values in <code>x1</code> and <code>x3</code> (leaving the
original values in the <code>&amp;mut Uxn</code> unchanged).  We have to write those registers
back into the appropriate memory locations in the <code>&amp;mut Uxn</code> <em>before</em> calling a
function that expects those values to be correct.</p>
<p>Here&#39;s the assembly code to call into our shim functions:</p>
<pre><code>.global _DEI
_DEI:
    ; We have to write our stack index pointers back into the &amp;mut Uxn
    ldp x11, x12, [sp, 0x10] ; restore stack index pointers
    strb w1, [x11]   ; modify stack index pointer
    strb w3, [x12]   ; modify return stack index pointer

    ; We&#39;re using caller-saved registers, so we have to back them up
    stp x0, x1, [sp, #0x20] ; store register state
    stp x2, x3, [sp, #0x30]
    stp x5, x4, [sp, #0x40]
    stp x6, x7, [sp, #0x50]
    str x8,     [sp, #0x60]

    ; set up our arguments, then call the shim function:
    mov x0, x6 ; x0 = Uxn pointer
    mov x1, x7 ; x1 = DeviceHandle pointer
    bl _dei_entry

    ldp x0, x1, [sp, #0x20] ; restore register state
    ldp x2, x3, [sp, #0x30]
    ldp x5, x4, [sp, #0x40]
    ldp x6, x7, [sp, #0x50]
    ldr x8,     [sp, #0x60]

    ; The DEO operation may have changed stack pointers, so reload them here 
    ldp x11, x12, [sp, 0x10]
    ldrb w1, [x11]  ; update stack index pointer
    ldrb w3, [x12]  ; update return stack index pointer
    next
</code></pre>
<h3>Performance</h3>
<p>I used two CPU-heavy workloads to test interpreter performance:</p>
<ul>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/exercises/fib.tal"><code>fib.tal</code></a>,
modified to print the first <strong>35</strong> numbers of the Fibonacci sequence</li>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/demos/mandelbrot.tal"><code>mandelbrot.tal</code></a>,
with <code>%SCALE</code> set to <code>#0020</code> (rendering a 672 × 512 image)</li>
</ul>
<p>Both of these programs do all of their computation at startup, so I added
instrumentation to print time spent in the entry vector (at <code>0x100</code>).</p>
<p>There are four different implementations being tested here:</p>
<ul>
<li>The <a href="https://git.sr.ht/%7Erabbits/uxn/"><code>uxnemu</code></a> reference implementation,
running natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running natively on my laptop</li>
<li>The optimized <code>raven-uxn</code> interpreter (hand-written in assembly), running
natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running in my browser (compiled to
WebAsembly)</li>
</ul>
<p>Here are the performance numbers that you&#39;ve been waiting for:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>uxnemu</code> (reference)</td><td><code>AArch64</code></td><td>1.57 s</td><td>2.03 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>wasm32</code></td><td>2.54 s</td><td>2.82 s
</td></tr></tbody></table>
<p>There are three clear trends:</p>
<ul>
<li><code>raven-uxn</code>&#39;s baseline interpreter (written in safe Rust) is faster than the
reference implementation; we already knew that from previous work</li>
<li>The assembly implementation is about <strong>30% faster</strong> than the baseline!</li>
<li>WebAssembly encurs a roughly 1.8× slowdown compared to the baseline</li>
</ul>
<h3>Ablation testing</h3>
<p>It&#39;s not obvious whether the speedup is due to keeping values in registers, or
adding dispatch to the end of each opcode (instead of a central branch).</p>
<p>We can easily test for the latter by changing our <code>next</code> macro:</p>
<pre><code>.macro next
    b next_dispatch
.endm
next_dispatch:
    ldrb w9, [x4, x5]
    add x5, x5, #1
    and x5, x5, #0xffff
    ldr x10, [x8, x9, lsl #3]
    br x10
</code></pre>
<p>Adding these new results to the chart( as &#34;assembly*&#34;), here&#39;s what I see:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly*)</td><td><code>AArch64</code></td><td>1.34 s</td><td>1.41 s
</td></tr></tbody></table>
<p>Centralized dispatch is a significant slowdown, and is nearly as slow as the
baseline interpreter!
It just goes to show:
<a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">do not taunt happy fun branch predictor</a>.</p>
<h3>Things that didn&#39;t work</h3>
<p>I did a bunch of other experiments, which didn&#39;t make things faster:</p>
<ul>
<li>Expanding RAM to store both user bytes and the jump targets (i.e. making RAM a
<code>[u64; 65536]</code>).  The user byte is stored in bits 48-54 of the pointer, since
those are unused, and I added masking + shifting depending on whether we were
using the data or pointer component.  This was noticeably slower, probably
because it&#39;s less cache-friendly (512 KiB, rather than 64 KiB + 1 KiB of jump
table)</li>
<li>Making all of the opcode implementations the same size (padding to the size of
the largest opcode implementation with <code>.balign 256</code>), then removing the jump
table entirely.  This was also slower, also probably because of cache
friendliness: the opcode implementations go from 16.6 KiB total to 64 KiB.</li>
</ul>
<h3>Conclusion</h3>
<p>I&#39;ve proven to my satisfaction that writing an interpreter in assembly is both
fun and performant!</p>
<p>There are strategies to get similar performance in high-level languages:
<a href="https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables">using computed goto</a>
and the <a href="https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md#m3-massey-meta-machine">Massey Meta Machine</a>
are both relevant prior art.</p>
<p>However, neither of these are feasible in Rust; to quote
<a href="https://pliniker.github.io/post/dispatchers/">this excellent writeup</a>.</p>
<blockquote>
<p>At this time there is no portable way to produce computed gotos or tail call
optimization in compiled machine code from Rust.</p>
</blockquote>
<p>On a brighter note, it should be relatively easy to port all of the assembly
code to x86-64, but I&#39;ll leave that as a challenge for someone else!</p>
<p>All of the relevant code is <a href="https://github.com/mkeeter/raven">on Github</a>, gated
by the <code>native</code> feature.  The <code>uxn-cli</code> and <code>uxn-gui</code> executables both accept a
<code>--native</code> flag to select the assembly interpreter backend.</p>
<p>Have fun!</p>

<!-- Begin footer -->
</div></div>
  </body>
</html>
