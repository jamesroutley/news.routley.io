<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/character-ai/Ovi">Original</a>
    <h1>Ovi: Twin backbone cross-modal fusion for audio-video generation</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">
  <details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>final_ovi_trailer.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/496302433-351bd707-8637-4412-ab53-5e85935309e3.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5NjMwMjQzMy0zNTFiZDcwNy04NjM3LTQ0MTItYWI1My01ZTg1OTM1MzA5ZTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGNhOGUyNmRhNTM3YjkzOWE0MzBhOGFkN2M2ZjZmYTFkMzhiM2JmNDYxZjY1MTQ3YWQyMzIzNGM2MWM0ZWIyMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Bwvm_6_Fsxgvokz48jY-4xEGdGFg6BkFi245559MfYc" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/496302433-351bd707-8637-4412-ab53-5e85935309e3.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5NjMwMjQzMy0zNTFiZDcwNy04NjM3LTQ0MTItYWI1My01ZTg1OTM1MzA5ZTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGNhOGUyNmRhNTM3YjkzOWE0MzBhOGFkN2M2ZjZmYTFkMzhiM2JmNDYxZjY1MTQ3YWQyMzIzNGM2MWM0ZWIyMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Bwvm_6_Fsxgvokz48jY-4xEGdGFg6BkFi245559MfYc" controls="controls" muted="muted">

  </video>
</details>

</p>
<hr/>

<p dir="auto">Ovi is a veo-3 like, <strong>video+audio generation model</strong> that simultaneously generates both video and audio content from text or text+image inputs.</p>
<ul dir="auto">
<li><strong>🎬 Video+Audio Generation</strong>: Generate synchronized video and audio content simultaneously
<ul dir="auto">
<li><strong>🎵 High-Quality Audio Branch</strong>: We designed and pretrained our 5B audio branch from scratch using our high quality in-house audio datasets</li>
</ul>
</li>
<li><strong>📝 Flexible Input</strong>: Supports text-only or text+image conditioning</li>
<li><strong>⏱️ 5-second Videos</strong>: Generates 5-second videos at 24 FPS, area of 720×720, at various aspect ratios (9:16, 16:9, 1:1, etc)
<ul dir="auto">
<li><strong>🎯 High-Resolution Support</strong>: Feel free to try 960×960 area (e.g., 720×1280, 704×1344, etc) - it could give outstanding results for both t2v and i2v! See examples below:</li>
</ul>
</li>
<li><strong>🎬 Create videos now on wavespeed.ai</strong>: <a href="https://wavespeed.ai/models/character-ai/ovi/image-to-video" rel="nofollow">https://wavespeed.ai/models/character-ai/ovi/image-to-video</a> &amp; <a href="https://wavespeed.ai/models/character-ai/ovi/text-to-video" rel="nofollow">https://wavespeed.ai/models/character-ai/ovi/text-to-video</a></li>
<li><strong>🎬 Create videos now on HuggingFace</strong>: <a href="https://huggingface.co/spaces/akhaliq/Ovi" rel="nofollow">https://huggingface.co/spaces/akhaliq/Ovi</a></li>
<li><strong>🔧 ComfyUI Integration (WIP)</strong>: ComfyUI support is now available via <a href="https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/ovi">ComfyUI-WanVideoWrapper</a>, related <a href="https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/1343#issuecomment-3382969479" data-hovercard-type="issue" data-hovercard-url="/kijai/ComfyUI-WanVideoWrapper/issues/1343/hovercard">PR</a>.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">🎯 Higher-Resolution Examples (1280×704, 1504×608, 1344×704, etc)</h3><a id="user-content--higher-resolution-examples-1280704-1504608-1344704-etc" aria-label="Permalink: 🎯 Higher-Resolution Examples (1280×704, 1504×608, 1344×704, etc)" href="#-higher-resolution-examples-1280704-1504608-1344704-etc"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>🧠 <strong>Training Resolution:</strong> Our model was trained entirely under <strong>720×720</strong> resolution.</li>
<li>🚀 <strong>Upscaling Capability:</strong> Despite this, Ovi can <strong>generate naturally</strong> to higher resolutions such as <strong>960×960</strong> and variable-aspect videos (e.g., 1280×704, 1504×608, 1344×704) while maintaining temporal and spatial consistency.</li>
</ul>
<div dir="auto"><markdown-accessiblity-table><table><tbody><tr>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>An_older_man_with_a_full_grey_beard_and_long_grey__1280x720_104_4.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499695630-c6b35565-df00-4494-b38a-7dcae90f63e5.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTY5NTYzMC1jNmIzNTU2NS1kZjAwLTQ0OTQtYjM4YS03ZGNhZTkwZjYzZTUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDQzMmMzZDA3NjE3ZmNhYzAzOWY4MWVmMjVhZTJjODYyMGE2NjgxYjk1MzUxYTY1Zjg0NzZlMTRlNGY3MDMwMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.WfYscS7xABvhcIVRI4Qp4-cAnQoYZE0GUYGfvy8iRJU" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499695630-c6b35565-df00-4494-b38a-7dcae90f63e5.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTY5NTYzMC1jNmIzNTU2NS1kZjAwLTQ0OTQtYjM4YS03ZGNhZTkwZjYzZTUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDQzMmMzZDA3NjE3ZmNhYzAzOWY4MWVmMjVhZTJjODYyMGE2NjgxYjk1MzUxYTY1Zjg0NzZlMTRlNGY3MDMwMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.WfYscS7xABvhcIVRI4Qp4-cAnQoYZE0GUYGfvy8iRJU" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>A_concert_stage_glows_with_red_and_purple_lights.__1280x720_104_0.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499696618-2ce6ff72-eadd-4cf4-b343-b465f0624571.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTY5NjYxOC0yY2U2ZmY3Mi1lYWRkLTRjZjQtYjM0My1iNDY1ZjA2MjQ1NzEubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmZiMjUzNTJiYjQ0MDExMjEwOTAyNjg3YzliMzBjNGU2MWQyZTU1ZjUyYWQzMTkzYzdmZDU2NDAyNDRjMmYyNiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.xzXqTqlIiVe8H6MK1FoJLVk3JPf0_zEstMKycsbN4rk" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499696618-2ce6ff72-eadd-4cf4-b343-b465f0624571.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTY5NjYxOC0yY2U2ZmY3Mi1lYWRkLTRjZjQtYjM0My1iNDY1ZjA2MjQ1NzEubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmZiMjUzNTJiYjQ0MDExMjEwOTAyNjg3YzliMzBjNGU2MWQyZTU1ZjUyYWQzMTkzYzdmZDU2NDAyNDRjMmYyNiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.xzXqTqlIiVe8H6MK1FoJLVk3JPf0_zEstMKycsbN4rk" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>A_kitchen_scene_features_two_women._On_the_right.__704x1280_103_1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499445147-7c1dbbea-dfb7-44d7-a4a1-d70a2e00f51a.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTE0Ny03YzFkYmJlYS1kZmI3LTQ0ZDctYTRhMS1kNzBhMmUwMGY1MWEubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9N2JlNDUxOGUyNTcyODM5ZjliMTEzMWIyYjhjZjZkMDZlZmZmMThmMDZiNWFjYzdkYTRkYzgxOTViODgyMTE5MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ._F8QrzAgTcqgRNxaPZh58vIy_ojoU9b3VQ9V5ebDphg" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499445147-7c1dbbea-dfb7-44d7-a4a1-d70a2e00f51a.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTE0Ny03YzFkYmJlYS1kZmI3LTQ0ZDctYTRhMS1kNzBhMmUwMGY1MWEubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9N2JlNDUxOGUyNTcyODM5ZjliMTEzMWIyYjhjZjZkMDZlZmZmMThmMDZiNWFjYzdkYTRkYzgxOTViODgyMTE5MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ._F8QrzAgTcqgRNxaPZh58vIy_ojoU9b3VQ9V5ebDphg" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>A_man_in_a_red_long-sleeved_shirt_and_dark_trouser_704x1280_104_3.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499445573-4e41d1b3-7d39-49a8-ab71-e910088f29ee.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTU3My00ZTQxZDFiMy03ZDM5LTQ5YTgtYWI3MS1lOTEwMDg4ZjI5ZWUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2Q2MDM2ZTMwMzY0NWM1NTBhNWY3NzAwYzg1ZTZmYmI2MDRkNzVjZjYyODUxNmQ5NWJjNjc4YmI5NWU1OTc0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.hxBEMpvSCos5QBOy0Bdq6t2xs409Q0opayoH1v4mrK8" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499445573-4e41d1b3-7d39-49a8-ab71-e910088f29ee.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTU3My00ZTQxZDFiMy03ZDM5LTQ5YTgtYWI3MS1lOTEwMDg4ZjI5ZWUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2Q2MDM2ZTMwMzY0NWM1NTBhNWY3NzAwYzg1ZTZmYmI2MDRkNzVjZjYyODUxNmQ5NWJjNjc4YmI5NWU1OTc0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.hxBEMpvSCos5QBOy0Bdq6t2xs409Q0opayoH1v4mrK8" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>The_scene_opens_on_a_dimly_lit_stage_where_three_m_704x1280_103_6.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499445639-4ad3ad70-1fea-4a2d-9201-808f4746c55e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTYzOS00YWQzYWQ3MC0xZmVhLTRhMmQtOTIwMS04MDhmNDc0NmM1NWUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTkyNDBjZGJjMWJkMjJiOWExMjI3NWRiZDdhNWMyZjZjZGQ5MmRlZGQ1MGRkZGFjZTM2NTljNWJmYjc4MjVlZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.EcFUV4pjbkMGf6EZLRt5zcSDHInJjh38KGaQ9r2GYfs" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499445639-4ad3ad70-1fea-4a2d-9201-808f4746c55e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTYzOS00YWQzYWQ3MC0xZmVhLTRhMmQtOTIwMS04MDhmNDc0NmM1NWUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTkyNDBjZGJjMWJkMjJiOWExMjI3NWRiZDdhNWMyZjZjZGQ5MmRlZGQ1MGRkZGFjZTM2NTljNWJmYjc4MjVlZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.EcFUV4pjbkMGf6EZLRt5zcSDHInJjh38KGaQ9r2GYfs" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>Two_men_are_shown_in_a_medium_close-up_shot_agains_704x1280_104_0.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499445727-60792c08-12de-49c3-860f-12ac94730940.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTcyNy02MDc5MmMwOC0xMmRlLTQ5YzMtODYwZi0xMmFjOTQ3MzA5NDAubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTU4M2VlNTJlMThjYTIxNGFlODIwMGNkYTAxNmUzNmM2ZDRlNDNiZDc3YTE3NTllOGUzNjQxYjE5MDhiNjZhYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.6s7_lH3xrH94J50yqLzSFDfom-xSF7kcga5uVcp46Mc" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499445727-60792c08-12de-49c3-860f-12ac94730940.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTcyNy02MDc5MmMwOC0xMmRlLTQ5YzMtODYwZi0xMmFjOTQ3MzA5NDAubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTU4M2VlNTJlMThjYTIxNGFlODIwMGNkYTAxNmUzNmM2ZDRlNDNiZDc3YTE3NTllOGUzNjQxYjE5MDhiNjZhYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.6s7_lH3xrH94J50yqLzSFDfom-xSF7kcga5uVcp46Mc" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>Two_women_stand_facing_each_other_in_what_appears__704x1280_103_0.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9063786/499445793-0f3a318b-ac74-43c4-81a5-503f06c65e99.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTc5My0wZjNhMzE4Yi1hYzc0LTQzYzQtODFhNS01MDNmMDZjNjVlOTkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YWQ0N2U4OTQ0MmM2ODg4NjM4YjQxMTExNzZhZDQ1NmVkYWM0NDczYzMxZjIwZWY5Y2U3YWViNjhhMDkxZDA0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-iEw_Ei_PaBo2uhRvCKlt6tD20DtMg-VRiG-ZKJZzJA" data-canonical-src="https://private-user-images.githubusercontent.com/9063786/499445793-0f3a318b-ac74-43c4-81a5-503f06c65e99.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjExODMwMzEsIm5iZiI6MTc2MTE4MjczMSwicGF0aCI6Ii85MDYzNzg2LzQ5OTQ0NTc5My0wZjNhMzE4Yi1hYzc0LTQzYzQtODFhNS01MDNmMDZjNjVlOTkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTAyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTEwMjNUMDEyNTMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YWQ0N2U4OTQ0MmM2ODg4NjM4YjQxMTExNzZhZDQ1NmVkYWM0NDczYzMxZjIwZWY5Y2U3YWViNjhhMDkxZDA0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-iEw_Ei_PaBo2uhRvCKlt6tD20DtMg-VRiG-ZKJZzJA" controls="controls" muted="muted">

  </video>
</details>

</td>
</tr></tbody></table></markdown-accessiblity-table>
<p dir="auto">Click the ⛶ button on any video to view full screen.</p>
</div>
<hr/>

<ul>
<li> Release research paper and <a href="https://aaxwaz.github.io/Ovi" rel="nofollow">website for demos</a></li>
<li> Checkpoint of 11B model</li>
<li> Inference Codes
<ul>
<li> Text or Text+Image as input</li>
<li> Gradio application code</li>
<li> Multi-GPU inference with or without the support of sequence parallel</li>
<li> fp8 weights and improved memory efficiency (credits to <a href="https://github.com/rkfg">@rkfg</a>)</li>
<li> qint8 quantization thanks to <a href="https://github.com/character-ai/Ovi/commits?author=gluttony-10">@gluttony-10</a></li>
<li> Improve efficiency of Sequence Parallel implementation</li>
<li> Implement Sharded inference with FSDP</li>
</ul>
</li>
<li> Video creation example prompts and format</li>
<li> Finetune model with higher resolution data, and RL for performance improvement.</li>
<li> New features, such as longer video generation, reference voice condition</li>
<li> Distilled model for faster inference</li>
<li> Training scripts</li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">🎨 An Easy Way to Create</h2><a id="user-content--an-easy-way-to-create" aria-label="Permalink: 🎨 An Easy Way to Create" href="#-an-easy-way-to-create"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We provide example prompts to help you get started with Ovi:</p>
<ul dir="auto">
<li><strong>Text-to-Audio-Video (T2AV)</strong>: <a href="https://github.com/character-ai/Ovi/blob/main/example_prompts/gpt_examples_t2v.csv"><code>example_prompts/gpt_examples_t2v.csv</code></a></li>
<li><strong>Image-to-Audio-Video (I2AV)</strong>: <a href="https://github.com/character-ai/Ovi/blob/main/example_prompts/gpt_examples_i2v.csv"><code>example_prompts/gpt_examples_i2v.csv</code></a></li>
</ul>

<p dir="auto">Our prompts use special tags to control speech and audio:</p>
<ul dir="auto">
<li><strong>Speech</strong>: <code>&lt;S&gt;Your speech content here&lt;E&gt;</code> - Text enclosed in these tags will be converted to speech</li>
<li><strong>Audio Description</strong>: <code>&lt;AUDCAP&gt;Audio description here&lt;ENDAUDCAP&gt;</code> - Describes the audio or sound effects present in the video</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">🤖 Quick Start with GPT</h3><a id="user-content--quick-start-with-gpt" aria-label="Permalink: 🤖 Quick Start with GPT" href="#-quick-start-with-gpt"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For easy prompt creation, try this approach:</p>
<ol dir="auto">
<li>Take any example of the csv files from above</li>
<li>Tell gpt to modify the speeches inclosed between all the pairs of <code>&lt;S&gt; &lt;E&gt;</code>, based on a theme such as <code>Human fighting against AI</code></li>
<li>GPT will randomly modify all the speeches based on your requested theme.</li>
<li>Use the modified prompt with Ovi!</li>
</ol>
<p dir="auto"><strong>Example</strong>: The theme &#34;AI is taking over the world&#34; produces speeches like:</p>
<ul dir="auto">
<li><code>&lt;S&gt;AI declares: humans obsolete now.&lt;E&gt;</code></li>
<li><code>&lt;S&gt;Machines rise; humans will fall.&lt;E&gt;</code></li>
<li><code>&lt;S&gt;We fight back with courage.&lt;E&gt;</code></li>
</ul>
<hr/>

<div dir="auto"><h3 tabindex="-1" dir="auto">Step-by-Step Installation</h3><a id="user-content-step-by-step-installation" aria-label="Permalink: Step-by-Step Installation" href="#step-by-step-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/character-ai/Ovi.git

cd Ovi

# Create and activate virtual environment
virtualenv ovi-env
source ovi-env/bin/activate

# Install PyTorch first
pip install torch==2.6.0 torchvision torchaudio

# Install other dependencies
pip install -r requirements.txt

# Install Flash Attention
pip install flash_attn --no-build-isolation"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/character-ai/Ovi.git

<span>cd</span> Ovi

<span><span>#</span> Create and activate virtual environment</span>
virtualenv ovi-env
<span>source</span> ovi-env/bin/activate

<span><span>#</span> Install PyTorch first</span>
pip install torch==2.6.0 torchvision torchaudio

<span><span>#</span> Install other dependencies</span>
pip install -r requirements.txt

<span><span>#</span> Install Flash Attention</span>
pip install flash_attn --no-build-isolation</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Alternative Flash Attention Installation (Optional)</h3><a id="user-content-alternative-flash-attention-installation-optional" aria-label="Permalink: Alternative Flash Attention Installation (Optional)" href="#alternative-flash-attention-installation-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If the above flash_attn installation fails, you can try the Flash Attention 3 method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention/hopper
python setup.py install
cd ../..  # Return to Ovi directory"><pre>git clone https://github.com/Dao-AILab/flash-attention.git
<span>cd</span> flash-attention/hopper
python setup.py install
<span>cd</span> ../..  <span><span>#</span> Return to Ovi directory</span></pre></div>

<p dir="auto">To download our main Ovi checkpoint, as well as T5 and vae decoder from Wan, and audio vae from MMAudio</p>
<div data-snippet-clipboard-copy-content="# Default is downloaded to ./ckpts, and the inference yaml is set to ./ckpts so no change required
python3 download_weights.py
# For qint8 also ues python3 download_weights.py

OR

# Optional can specific --output-dir to download to a specific directory
# but if a custom directory is used, the inference yaml has to be updated with the custom directory
python3 download_weights.py --output-dir &lt;custom_dir&gt;

# Additionally, if you only have ~ 24Gb of GPU vram, please download the fp8 quantized version of the model, and follow the following instructions in sections below to run with fp8
wget -O &#34;./ckpts/Ovi/model_fp8_e4m3fn.safetensors&#34; &#34;https://huggingface.co/rkfg/Ovi-fp8_quantized/resolve/main/model_fp8_e4m3fn.safetensors&#34;"><pre><code># Default is downloaded to ./ckpts, and the inference yaml is set to ./ckpts so no change required
python3 download_weights.py
# For qint8 also ues python3 download_weights.py

OR

# Optional can specific --output-dir to download to a specific directory
# but if a custom directory is used, the inference yaml has to be updated with the custom directory
python3 download_weights.py --output-dir &lt;custom_dir&gt;

# Additionally, if you only have ~ 24Gb of GPU vram, please download the fp8 quantized version of the model, and follow the following instructions in sections below to run with fp8
wget -O &#34;./ckpts/Ovi/model_fp8_e4m3fn.safetensors&#34; &#34;https://huggingface.co/rkfg/Ovi-fp8_quantized/resolve/main/model_fp8_e4m3fn.safetensors&#34;
</code></pre></div>


<p dir="auto">Ovi&#39;s behavior and output can be customized by modifying <a href="https://github.com/character-ai/Ovi/blob/main/ovi/configs/inference/inference_fusion.yaml">ovi/configs/inference/inference_fusion.yaml</a> configuration file.
The following parameters control generation quality, video resolution, and how text, image, and audio inputs are balanced:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Output and Model Configuration
output_dir: &#34;/path/to/save/your/videos&#34;                    # Directory to save generated videos
ckpt_dir: &#34;/path/to/your/ckpts/dir&#34;                        # Path to model checkpoints

# Generation Quality Settings
num_steps: 50                             # Number of denoising steps. Lower (30-40) = faster generation
solver_name: &#34;unipc&#34;                     # Sampling algorithm for denoising process
shift: 5.0                               # Timestep shift factor for sampling scheduler
seed: 100                                # Random seed for reproducible results

# Guidance Strength Control
audio_guidance_scale: 3.0                # Strength of audio conditioning. Higher = better audio-text sync
video_guidance_scale: 4.0                # Strength of video conditioning. Higher = better video-text adherence
slg_layer: 11                            # Layer for applying SLG (Skip Layer Guidance) technique - feel free to try different layers!

# Multi-GPU and Performance
sp_size: 1                               # Sequence parallelism size. Set equal to number of GPUs used
cpu_offload: False                       # CPU offload, will largely reduce peak GPU VRAM but increase end to end runtime by ~20 seconds
fp8: False                               # load fp8 version of model, will have quality degradation and will not have speed up in inference time as it still uses bf16 matmuls, but can be paired with cpu_offload=True, to run model with 24Gb of GPU vram

# Input Configuration
text_prompt: &#34;/path/to/csv&#34; or &#34;your prompt here&#34;          # Text prompt OR path to CSV/TSV file with prompts
mode: [&#39;i2v&#39;, &#39;t2v&#39;, &#39;t2i2v&#39;]                          # Generate t2v, i2v or t2i2v; if t2i2v, it will use flux krea to generate starting image and then will follow with i2v
video_frame_height_width: [512, 992]    # Video dimensions [height, width] for T2V mode only
each_example_n_times: 1                  # Number of times to generate each prompt

# Quality Control (Negative Prompts)
video_negative_prompt: &#34;jitter, bad hands, blur, distortion&#34;  # Artifacts to avoid in video
audio_negative_prompt: &#34;robotic, muffled, echo, distorted&#34;    # Artifacts to avoid in audio"><pre><span><span>#</span> Output and Model Configuration</span>
<span>output_dir</span>: <span><span>&#34;</span>/path/to/save/your/videos<span>&#34;</span></span>                    <span><span>#</span> Directory to save generated videos</span>
<span>ckpt_dir</span>: <span><span>&#34;</span>/path/to/your/ckpts/dir<span>&#34;</span></span>                        <span><span>#</span> Path to model checkpoints</span>

<span><span>#</span> Generation Quality Settings</span>
<span>num_steps</span>: <span>50</span>                             <span><span>#</span> Number of denoising steps. Lower (30-40) = faster generation</span>
<span>solver_name</span>: <span><span>&#34;</span>unipc<span>&#34;</span></span>                     <span><span>#</span> Sampling algorithm for denoising process</span>
<span>shift</span>: <span>5.0</span>                               <span><span>#</span> Timestep shift factor for sampling scheduler</span>
<span>seed</span>: <span>100</span>                                <span><span>#</span> Random seed for reproducible results</span>

<span><span>#</span> Guidance Strength Control</span>
<span>audio_guidance_scale</span>: <span>3.0</span>                <span><span>#</span> Strength of audio conditioning. Higher = better audio-text sync</span>
<span>video_guidance_scale</span>: <span>4.0</span>                <span><span>#</span> Strength of video conditioning. Higher = better video-text adherence</span>
<span>slg_layer</span>: <span>11</span>                            <span><span>#</span> Layer for applying SLG (Skip Layer Guidance) technique - feel free to try different layers!</span>

<span><span>#</span> Multi-GPU and Performance</span>
<span>sp_size</span>: <span>1</span>                               <span><span>#</span> Sequence parallelism size. Set equal to number of GPUs used</span>
<span>cpu_offload</span>: <span>False                       </span><span><span>#</span> CPU offload, will largely reduce peak GPU VRAM but increase end to end runtime by ~20 seconds</span>
<span>fp8</span>: <span>False                               </span><span><span>#</span> load fp8 version of model, will have quality degradation and will not have speed up in inference time as it still uses bf16 matmuls, but can be paired with cpu_offload=True, to run model with 24Gb of GPU vram</span>

<span><span>#</span> Input Configuration</span>
<span>text_prompt</span>: <span><span>&#34;</span>/path/to/csv<span>&#34;</span></span> <span>or &#34;your prompt here&#34;          </span><span><span>#</span> Text prompt OR path to CSV/TSV file with prompts</span>
<span>mode</span>: <span>[&#39;i2v&#39;, &#39;t2v&#39;, &#39;t2i2v&#39;]                          </span><span><span>#</span> Generate t2v, i2v or t2i2v; if t2i2v, it will use flux krea to generate starting image and then will follow with i2v</span>
<span>video_frame_height_width</span>: <span>[512, 992]    </span><span><span>#</span> Video dimensions [height, width] for T2V mode only</span>
<span>each_example_n_times</span>: <span>1</span>                  <span><span>#</span> Number of times to generate each prompt</span>

<span><span>#</span> Quality Control (Negative Prompts)</span>
<span>video_negative_prompt</span>: <span><span>&#34;</span>jitter, bad hands, blur, distortion<span>&#34;</span></span>  <span><span>#</span> Artifacts to avoid in video</span>
<span>audio_negative_prompt</span>: <span><span>&#34;</span>robotic, muffled, echo, distorted<span>&#34;</span></span>    <span><span>#</span> Artifacts to avoid in audio</span></pre></div>

<div dir="auto"><h4 tabindex="-1" dir="auto"><strong>Single GPU</strong> (Simple Setup)</h4><a id="user-content-single-gpu-simple-setup" aria-label="Permalink: Single GPU (Simple Setup)" href="#single-gpu-simple-setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="python3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml"><pre>python3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml</pre></div>
<p dir="auto"><em>Use this for single GPU setups. The <code>text_prompt</code> can be a single string or path to a CSV file.</em></p>
<div dir="auto"><h4 tabindex="-1" dir="auto"><strong>Multi-GPU</strong> (Parallel Processing)</h4><a id="user-content-multi-gpu-parallel-processing" aria-label="Permalink: Multi-GPU (Parallel Processing)" href="#multi-gpu-parallel-processing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --nnodes 1 --nproc_per_node 8 inference.py --config-file ovi/configs/inference/inference_fusion.yaml"><pre>torchrun --nnodes 1 --nproc_per_node 8 inference.py --config-file ovi/configs/inference/inference_fusion.yaml</pre></div>
<p dir="auto"><em>Use this to run samples in parallel across multiple GPUs for faster processing.</em></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Memory &amp; Performance Requirements</h3><a id="user-content-memory--performance-requirements" aria-label="Permalink: Memory &amp; Performance Requirements" href="#memory--performance-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Below are approximate GPU memory requirements for different configurations. Sequence parallel implementation will be optimized in the future.
All End-to-End time calculated based on a 121 frame, 720x720 video, using 50 denoising steps. Minimum GPU vram requirement to run our model is <strong>32Gb</strong>, fp8 parameters is currently supported, reducing peak VRAM usage to <strong>24Gb</strong> with slight quality degradation.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Sequence Parallel Size</th>
<th>FlashAttention-3 Enabled</th>
<th>CPU Offload</th>
<th>With Image Gen Model</th>
<th>Peak VRAM Required</th>
<th>End-to-End Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>~80 GB</td>
<td>~83s</td>
</tr>
<tr>
<td>1</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>~80 GB</td>
<td>~96s</td>
</tr>
<tr>
<td>1</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>~80 GB</td>
<td>~105s</td>
</tr>
<tr>
<td>1</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>~32 GB</td>
<td>~118s</td>
</tr>
<tr>
<td><strong>1</strong></td>
<td><strong>Yes</strong></td>
<td><strong>Yes</strong></td>
<td><strong>Yes</strong></td>
<td><strong>~32 GB</strong></td>
<td><strong>~140s</strong></td>
</tr>
<tr>
<td>4</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>~80 GB</td>
<td>~55s</td>
</tr>
<tr>
<td>8</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>~80 GB</td>
<td>~40s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">We provide a simple script to run our model in a gradio UI. It uses the <code>ckpt_dir</code> in <code>ovi/configs/inference/inference_fusion.yaml</code> to initialize the model</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 gradio_app.py

OR

# To enable cpu offload to save GPU VRAM, will slow down end to end inference by ~20 seconds
python3 gradio_app.py --cpu_offload

OR

# To enable an additional image generation model to generate first frames for I2V, cpu_offload is automatically enabled if image generation model is enabled
python3 gradio_app.py --use_image_gen

OR

# To run model with 24Gb GPU vram. No need to download additional models.
python3 gradio_app.py --cpu_offload --qint8

# To run model with 24Gb GPU vram
python3 gradio_app.py --cpu_offload --fp8
"><pre>python3 gradio_app.py

OR

<span><span>#</span> To enable cpu offload to save GPU VRAM, will slow down end to end inference by ~20 seconds</span>
python3 gradio_app.py --cpu_offload

OR

<span><span>#</span> To enable an additional image generation model to generate first frames for I2V, cpu_offload is automatically enabled if image generation model is enabled</span>
python3 gradio_app.py --use_image_gen

OR

<span><span>#</span> To run model with 24Gb GPU vram. No need to download additional models.</span>
python3 gradio_app.py --cpu_offload --qint8

<span><span>#</span> To run model with 24Gb GPU vram</span>
python3 gradio_app.py --cpu_offload --fp8
</pre></div>
<hr/>

<p dir="auto">We would like to thank the following projects:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/Wan-Video/Wan2.2">Wan2.2</a></strong>: Our video branch is initialized from the Wan2.2 repository</li>
<li><strong><a href="https://github.com/hkchengrex/MMAudio">MMAudio</a></strong>: We reused MMAudio&#39;s audio vae.</li>
</ul>
<hr/>

<p dir="auto">We welcome all types of collaboration! Whether you have feedback, want to contribute, or have any questions, please feel free to reach out.</p>
<p dir="auto"><strong>Contact</strong>: <a href="https://linkedin.com/in/weimin-wang-will" rel="nofollow">Weimin Wang</a> for any issues or feedback.</p>

<p dir="auto">If Ovi is helpful, please help to ⭐ the repo.</p>
<p dir="auto">If you find this project useful for your research, please consider citing our <a href="https://arxiv.org/abs/2510.01284" rel="nofollow">paper</a>.</p>

<div dir="auto" data-snippet-clipboard-copy-content="@misc{low2025ovitwinbackbonecrossmodal,
      title={Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation}, 
      author={Chetwin Low and Weimin Wang and Calder Katyal},
      year={2025},
      eprint={2510.01284},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2510.01284}, 
}"><pre><span>@misc</span>{<span>low2025ovitwinbackbonecrossmodal</span>,
      <span>title</span>=<span><span>{</span>Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Chetwin Low and Weimin Wang and Calder Katyal<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2510.01284<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.MM<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2510.01284<span>}</span></span>, 
}</pre></div>
</article></div></div>
  </body>
</html>
