<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2101.00027">Original</a>
    <h1>The Pile: An 800GB dataset of diverse text for language modeling (2020)</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao%2C+L">Leo Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Biderman%2C+S">Stella Biderman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Black%2C+S">Sid Black</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Golding%2C+L">Laurence Golding</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hoppe%2C+T">Travis Hoppe</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Foster%2C+C">Charles Foster</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Phang%2C+J">Jason Phang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+H">Horace He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Thite%2C+A">Anish Thite</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nabeshima%2C+N">Noa Nabeshima</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Presser%2C+S">Shawn Presser</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leahy%2C+C">Connor Leahy</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2101.00027">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Recent work has demonstrated that increased training dataset diversity
improves general cross-domain knowledge and downstream generalization
capability for large-scale language models. With this in mind, we present
\textit{the Pile}: an 825 GiB English text corpus targeted at training
large-scale language models. The Pile is constructed from 22 diverse
high-quality subsets -- both existing and newly constructed -- many of which
derive from academic or professional sources. Our evaluation of the untuned
performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on
many of its components, such as academic writing. Conversely, models trained on
the Pile improve significantly over both Raw CC and CC-100 on all components of
the Pile, while improving performance on downstream evaluations. Through an
in-depth exploratory analysis, we document potentially concerning aspects of
the data for prospective users. We make publicly available the code used in its
construction.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Leo Gao [<a href="https://arxiv.org/show-email/41bfbe00/2101.00027">view email</a>]
      </p></div></div>
  </body>
</html>
