<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/FMInference/FlexGen">Original</a>
    <h1>FlexGen: Running large language models on a single GPU</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows <strong>high-throughput</strong> generation by IO-efficient offloading, compression, and <strong>large effective batch sizes</strong>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-throughput-oriented-inference-for-large-language-models" aria-hidden="true" href="#throughput-oriented-inference-for-large-language-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Throughput-Oriented Inference for Large Language Models</h2>
<p dir="auto">In recent years, large language models (LLMs) have shown great performance across a
wide range of tasks. Increasingly, LLMs have been applied not only to interactive
applications (such as chat), but also to many &#34;back-of-house&#34; tasks.
These tasks include benchmarking, information extraction, data wrangling, and form processing.</p>
<p dir="auto">One key characteristic of these applications is that they are <strong>throughput-oriented</strong>: they require
running LLM inferences over millions of tokens in batches, e.g., all the private documents in a company&#39;s
corpus, or all the tasks in the <a href="https://crfm.stanford.edu/helm/latest/" rel="nofollow">HELM</a> benchmark.
These workloads are less sensitive to latency - the user starts up a job and lets it run overnight -
but increasing throughput is critical for reducing costs.
Throughput is a measure of tokens processed per second over the job&#39;s entire runtime (which can be hours).
Throughput-oriented workloads provide opportunities to trade off latency for higher throughput, which
makes it easier to take advantage of low-cost commodity GPUs.</p>
<p dir="auto">The goal of FlexGen is to create a high-throughput system to enable new and exciting applications of
foundation models to throughput-oriented tasks on low-cost hardware, such as a single commodity GPU
instead of expensive systems.</p>
<p dir="auto">Check out the <a href="#examples">examples</a> of what you can run on a single commodity GPU with FlexGen, including benchmarking and data wrangling.</p>
<p dir="auto"><g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji> <strong>Limitation</strong>. As an offloading-based system running on weak GPUs, FlexGen also has its limitations.
FlexGen can be significantly slower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases.
FlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.</p>
<hr/>
<p dir="auto">This project was made possible thanks to a collaboration with</p>
<p dir="auto"><a href="https://cs.stanford.edu/" rel="nofollow"><img src="https://camo.githubusercontent.com/6d0010e05cac9163690580056bb45dff85148e3c4c309d57cb76eb030278f0d8/68747470733a2f2f6964656e746974792e7374616e666f72642e6564752f77702d636f6e74656e742f75706c6f6164732f73697465732f332f323032302f30362f776f72646d61726b2d6e6f73706163652d7265642e706e67" height="20" data-canonical-src="https://identity.stanford.edu/wp-content/uploads/sites/3/2020/06/wordmark-nospace-red.png"/></a>    
<a href="https://sky.cs.berkeley.edu/" rel="nofollow"><img src="https://camo.githubusercontent.com/5af869d1c14bfd1f4dca2b7e6e949e4dd9ecb7cfc970f17e446ce2bb4ea71879/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f382f38322f556e69766572736974795f6f665f43616c69666f726e69612532435f4265726b656c65795f6c6f676f2e7376672f3132383070782d556e69766572736974795f6f665f43616c69666f726e69612532435f4265726b656c65795f6c6f676f2e7376672e706e67" height="22" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/University_of_California%2C_Berkeley_logo.svg/1280px-University_of_California%2C_Berkeley_logo.svg.png"/></a>    
<a href="https://www.andrew.cmu.edu/user/beidic/" rel="nofollow"><img src="https://camo.githubusercontent.com/0706d6989fd080140013c7468ba5b2f089f56e2b4f97b040eaf5fb918e19af74/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f392f39622f4361726e656769655f4d656c6c6f6e5f776f72646d61726b2e737667" height="20" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/9/9b/Carnegie_Mellon_wordmark.svg"/></a>    
<a href="https://www.together.xyz/" rel="nofollow"><img src="https://camo.githubusercontent.com/7bc4a3f35c479ee9ac7f8987ea77ac28266341e2430e4fd661a91bdb896d4a7b/68747470733a2f2f696d616765732e73717561726573706163652d63646e2e636f6d2f636f6e74656e742f76312f3633353862656132383231383961306164663537666531362f65656630393139312d363331662d343064392d396266642d6638373562323562636630622f746f6765746865722d6c6f676f2d626c61636b2d7472616e73706172656e74322e706e67" height="20" data-canonical-src="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/eef09191-631f-40d9-9bfd-f875b25bcf0b/together-logo-black-transparent2.png"/></a>    
<a href="https://research.yandex.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/5af3825838e30554b9a13de5fc3edf2d57c8d2b145417ad9b44202c7a9feacd8/68747470733a2f2f73746f726167652e79616e646578636c6f75642e6e65742f79616e6465782d72657365617263682f6173736574732f79616e6465785f72657365617263682e706e67" height="20" data-canonical-src="https://storage.yandexcloud.net/yandex-research/assets/yandex_research.png"/></a>    
<a href="https://ds3lab.inf.ethz.ch/" rel="nofollow"><img src="https://user-images.githubusercontent.com/1608867/220273382-c09669b3-42fd-47c2-b88c-7ed55cb43820.png" height="20"/></a></p>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-install" aria-hidden="true" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h2>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li>PyTorch &gt;= 1.12 <a href="https://pytorch.org/get-started/locally/" rel="nofollow">(Help)</a></li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-method-1-with-pip" aria-hidden="true" href="#method-1-with-pip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Method 1: With pip</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-method-2-from-source" aria-hidden="true" href="#method-2-from-source"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Method 2: From source</h3>
<div data-snippet-clipboard-copy-content="git clone https://github.com/FMInference/FlexGen.git
cd FlexGen
pip install -e ."><pre><code>git clone https://github.com/FMInference/FlexGen.git
cd FlexGen
pip install -e .
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-examples" aria-hidden="true" href="#examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-helm-benchmark" aria-hidden="true" href="#helm-benchmark"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>HELM Benchmark</h3>
<p dir="auto">FlexGen can be integrated into <a href="https://crfm.stanford.edu/helm" rel="nofollow">HELM</a>, a language model benchmark framework, as its execution backend.
You can use the commands below to run a Massive Multitask Language Understanding (MMLU) <a href="https://crfm.stanford.edu/helm/latest/?group=mmlu" rel="nofollow">scenario</a> with a single T4 (16GB) GPU and 200GB of DRAM.</p>
<div data-snippet-clipboard-copy-content="python3 -m flexgen.apps.helm_run --description mmlu:model=text,subject=abstract_algebra,data_augmentation=canonical --pad-to-seq-len 512 --model facebook/opt-30b --percent 20 80 0 100 0 100 --gpu-batch-size 48 --num-gpu-batches 3 --max-eval-instance 100"><pre><code>python3 -m flexgen.apps.helm_run --description mmlu:model=text,subject=abstract_algebra,data_augmentation=canonical --pad-to-seq-len 512 --model facebook/opt-30b --percent 20 80 0 100 0 100 --gpu-batch-size 48 --num-gpu-batches 3 --max-eval-instance 100
</code></pre></div>
<p dir="auto">Note that only a subset of HELM scenarios is tested. See more tested scenarios <a href="https://ntietz.com/FMInference/FlexGen/blob/main/flexgen/apps/helm_passed_30b.sh">here</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-data-wrangling" aria-hidden="true" href="#data-wrangling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Data Wrangling</h3>
<p dir="auto">You can run the examples in this paper, <a href="https://arxiv.org/abs/2205.09911" rel="nofollow">&#39;Can Foundation Models Wrangle Your Data?&#39;</a>, by following the instructions <a href="https://ntietz.com/FMInference/FlexGen/blob/main/flexgen/apps/data_wrangle">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-performance-benchmark" aria-hidden="true" href="#performance-benchmark"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance Benchmark</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-generation-throughput-tokens" aria-hidden="true" href="#generation-throughput-tokens"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Generation Throughput (token/s)</h3>
<p dir="auto">The corresponding effective batch sizes and lowest offloading devices are in parentheses. Please see <a href="https://ntietz.com/FMInference/FlexGen/blob/main/benchmark/batch_size_table.md">here</a> for more details.</p>
<table>
<thead>
<tr>
<th>System</th>
<th>OPT-6.7B</th>
<th>OPT-30B</th>
<th>OPT-175B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hugging Face Accelerate</td>
<td>25.12 (2 on GPU)</td>
<td>0.62 (8 on CPU)</td>
<td>0.01 (2 on disk)</td>
</tr>
<tr>
<td>DeepSpeed ZeRO-Inference</td>
<td>9.28 (16 on CPU)</td>
<td>0.60 (4 on CPU)</td>
<td>0.01 (1 on disk)</td>
</tr>
<tr>
<td>Petals</td>
<td>8.25 (2 on GPU)</td>
<td>2.84 (2 on GPU)</td>
<td>0.08 (2 on GPU)</td>
</tr>
<tr>
<td>FlexGen</td>
<td>25.26 (2 on GPU)</td>
<td>7.32 (144 on CPU)</td>
<td>0.69 (256 on disk)</td>
</tr>
<tr>
<td>FlexGen with Compression</td>
<td><strong>29.12</strong> (72 on GPU)</td>
<td><strong>8.38</strong> (512 on CPU)</td>
<td><strong>1.12</strong> (144 on CPU)</td>
</tr>
</tbody>
</table>
<ul dir="auto">
<li>Hardware: an NVIDIA T4 (16GB) instance on GCP with 208GB of DRAM and 1.5TB of SSD.</li>
<li>Workload: input sequence length = 512, output sequence length = 32. The batch size is tuned to <strong>a large value</strong> that maximizes the generation throughput for each system.</li>
<li>Metric: generation throughput (token/s) = number of the generated tokens / (time for processing prompts + time for generation).</li>
</ul>
<p dir="auto">How to <a href="https://ntietz.com/FMInference/FlexGen/blob/main/benchmark/flexgen">reproduce</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-roadmap" aria-hidden="true" href="#roadmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Roadmap</h2>
<p dir="auto">We plan to work on the following features.</p>
<ul>
<li> Optimize the performance for multiple GPUs on the same machine</li>
<li> Support more models (BLOOM, CodeGen, GLM)</li>
<li> Release the cost model and policy optimizer</li>
<li> Macbook Support (M1 and M2)</li>
<li> AMD Support</li>
</ul>
</article>
          </div></div>
  </body>
</html>
