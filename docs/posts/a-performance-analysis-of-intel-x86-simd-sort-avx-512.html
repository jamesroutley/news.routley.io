<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/text.md">Original</a>
    <h1>A performance analysis of Intel x86-SIMD-sort (AVX-512)</h1>
    
    <div id="readability-page-1" class="page"><div>

    <div data-target="readme-toc.content">
      
  


        <div id="readme">
    <article itemprop="text">
<p dir="auto">Author: Lukas Bergdoll @Voultapher</p>
<p dir="auto">This is a performance analysis of the recently popularized [<a href="https://www.phoronix.com/news/Intel-AVX-512-Quicksort-Numpy" rel="nofollow">1</a>] Intel AVX-512 sort implementation.</p>
<blockquote>
<p dir="auto">Intel Publishes Blazing Fast AVX-512 Sorting Library, Numpy Switching To It For 10~17x Faster Sorts</p>
</blockquote>
<p dir="auto">This analysis will look at the performance of Intel&#39; x86-simd-sort and how it compares to various other generic sort implementations such as the C++ standard library <code>std::sort</code> and vqsort another high-performance manually vectorized sort implementation. Breaking down complex performance characteristics into a single number is tricky and might have little predictive power. This analysis aims to put that &#39;10~17x&#39; number into perspective and how it relates to other high-performance implementations.</p>
<p dir="auto">TL;DR: Benchmarking is tricky. If you are using x86-simd-sort, you can get better overall performance and avoid catastrophic scaling for certain input patterns by using vqsort + Clang. In addition it is shown that hardware specific manual vectorization with wide AVX-512 SIMD is not the only way to write efficient software. ipnsort demonstrates comparable performance to x86-simd-sort while being generic, optimized for more than only peak performance and only using up to SSE2 instructions.</p>
<hr/>
<p dir="auto">Bias disclaimer. The author of this analysis is the author of ipnsort.</p>
<p dir="auto">The words sort implementation and sort algorithm, are expressly <em>not</em> used interchangeably. Practically all high-performance implementations are hybrids, using multiple sort algorithms. As such, the words &#39;sort algorithm&#39; will only be used to talk about the algorithmic nature of specific building blocks.</p>
<p dir="auto">Graphs with logarithmic axis are marked as such, these are primarily useful to examine the change of a property, <em>not</em> its absolute values.</p>
<h2 dir="auto"><a id="user-content-benchmarks" aria-hidden="true" href="#benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmarks</h2>
<h3 dir="auto"><a id="user-content-benchmark-setup" aria-hidden="true" href="#benchmark-setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmark setup</h3>
<p dir="auto">Benchmarking is notoriously tricky, and especially synthetic benchmarks may not be representative. An incomplete list of relevant factors:</p>
<ul dir="auto">
<li>Input size</li>
<li>Input type (price to move and price to compare)</li>
<li>Input pattern (already sorted, random, cardinality, streaks, mixed etc.)</li>
<li>Hardware prediction and cache effects</li>
</ul>
<p dir="auto">Hardware prediction and cache effects in hot benchmarks that do nothing but run a fixed input size, type and pattern combination in a loop may be misleading in their predictive power for real world performance. Especially for small input sizes. The later section &#34;Hot benchmarks&#34; looks at hot results and explains in more detail why they might not be as useful in this context.</p>
<h4 dir="auto"><a id="user-content-windows-test-machine" aria-hidden="true" href="#windows-test-machine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Windows test machine</h4>
<div data-snippet-clipboard-copy-content="Windows 10.0.19044
rustc 1.69.0-nightly (0416b1a6f 2023-02-14)
clang version 15.0.1
Microsoft (R) C/C++ Optimizing Compiler Version 19.31.31104 for x86
Intel Core i9-10980XE 18-Core Processor (Skylake micro-architecture)
CPU boost enabled."><pre><code>Windows 10.0.19044
rustc 1.69.0-nightly (0416b1a6f 2023-02-14)
clang version 15.0.1
Microsoft (R) C/C++ Optimizing Compiler Version 19.31.31104 for x86
Intel Core i9-10980XE 18-Core Processor (Skylake micro-architecture)
CPU boost enabled.
</code></pre></div>
<h4 dir="auto"><a id="user-content-linux-test-machine" aria-hidden="true" href="#linux-test-machine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Linux test machine</h4>
<div data-snippet-clipboard-copy-content="Linux 5.19
rustc 1.69.0-nightly (7aa413d59 2023-02-19)
clang version 14.0.6
Intel Xeon Gold 6154 18-Core Processor (Skylake micro-architecture)
CPU boost disabled."><pre><code>Linux 5.19
rustc 1.69.0-nightly (7aa413d59 2023-02-19)
clang version 14.0.6
Intel Xeon Gold 6154 18-Core Processor (Skylake micro-architecture)
CPU boost disabled.
</code></pre></div>
<p dir="auto">Some sort implementations are adaptive, they will try to exploit existing patterns in the data to do less work. A breakdown of the benchmark patterns:</p>
<ul dir="auto">
<li><code>ascending</code>, numbers <code>0..size</code></li>
<li><code>random</code>, random numbers generated by rand <code>StdRng::gen</code> [<a href="https://github.com/rust-random/rand">2</a>]</li>
<li><code>random_d20</code>, uniform random numbers in the range <code>0..=20</code></li>
<li><code>random_p5</code>, 95% 0 and 5% random data, not uniform</li>
<li><code>random_z1</code>, Zipfian distribution with characterizing exponent s == 1.0 [<a href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="nofollow">3</a>]</li>
<li><code>saws_long</code>, <code>(size as f64).log2().round()</code> number of randomly selected ascending and descending streaks</li>
<li><code>saws_short</code>, randomly selected ascending and descending streaks in the range of <code>20..70</code></li>
</ul>
<p dir="auto">Whether these patterns are representative will depend on your workload. These are fundamentally synthetic benchmarks exploring sort performance in isolation.</p>
<h3 dir="auto"><a id="user-content-contestants" aria-hidden="true" href="#contestants"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contestants</h3>
<p dir="auto">A selection of high-performance in-place sort implementations.</p>
<h4 dir="auto"><a id="user-content-generic-comparison-based" aria-hidden="true" href="#generic-comparison-based"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Generic comparison based</h4>
<div data-snippet-clipboard-copy-content="- rust_std_unstable          | `slice::sort_unstable` https://github.com/rust-lang/rust (1)
- rust_ipnsort_unstable      | https://github.com/Voultapher/sort-research-rs/blob/main/src/unstable/rust_ipnsort.rs (2) (3)
- cpp_std_msvc_unstable      | MSVC `std::sort` (4)
- cpp_std_gnu_unstable       | libstdc++ `std::sort` (8)
- cpp_std_libcxx_unstable    | libc++ `std::sort` (9)
- cpp_pdqsort_unstable       | https://github.com/orlp/pdqsort (4)
- c_crumsort_unstable        | https://github.com/scandum/crumsort (5) (6)"><pre><code>- rust_std_unstable          | `slice::sort_unstable` https://github.com/rust-lang/rust (1)
- rust_ipnsort_unstable      | https://github.com/Voultapher/sort-research-rs/blob/main/src/unstable/rust_ipnsort.rs (2) (3)
- cpp_std_msvc_unstable      | MSVC `std::sort` (4)
- cpp_std_gnu_unstable       | libstdc++ `std::sort` (8)
- cpp_std_libcxx_unstable    | libc++ `std::sort` (9)
- cpp_pdqsort_unstable       | https://github.com/orlp/pdqsort (4)
- c_crumsort_unstable        | https://github.com/scandum/crumsort (5) (6)
</code></pre></div>
<h4 dir="auto"><a id="user-content-manually-vectorized" aria-hidden="true" href="#manually-vectorized"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Manually vectorized</h4>
<div data-snippet-clipboard-copy-content="- cpp_vqsort                 | https://github.com/google/highway/tree/master/hwy/contrib/sort (7) (10)
- cpp_intel_avx512           | https://github.com/intel/x86-simd-sort (7)"><pre><code>- cpp_vqsort                 | https://github.com/google/highway/tree/master/hwy/contrib/sort (7) (10)
- cpp_intel_avx512           | https://github.com/intel/x86-simd-sort (7)
</code></pre></div>
<p dir="auto">Footnotes:</p>
<ol dir="auto">
<li>Vendored ca. mid 2022.</li>
<li>Still WIP and these are only preliminary results.</li>
<li>This sort was previously known as ipn_unstable, the previously presented ipn_stable is defunct now, and work on a stable sort implementation continuous under another name.</li>
<li>Built with msvc.</li>
<li>Compiled with <code>#define cmp(a, b) (*(a) &gt; *(b))</code>. This is required to be competitive, the regular way of providing a comparison function is problematic because of C language limitations. In effect this means that the results are only applicable to primitive types, and anything using a custom comparison function for user defined types will have a steep perf penalty.</li>
<li>crumsort does an initial analysis and switches to quadsort a merge sort that uses up to N memory making it not in-place <g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji>, it can fall back to in-place merging if the allocation fails. These tests were performed with allocations possible.</li>
<li>Built with clang and <code>-march=native</code>. Compiled with static dispatch, this would not be portable. Any CPU without AVX-512 support would fail to run the binary. It&#39;s unknown what the overhead of dynamic dispatch would be.</li>
<li>Built with gcc.</li>
<li>Built with clang.</li>
<li>Allocates fixed size memory buffer, not dependent on size of input. This blurs the line of in-place.</li>
</ol>
<h3 dir="auto"><a id="user-content-results-u64" aria-hidden="true" href="#results-u64"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Results <code>u64</code></h3>
<p dir="auto">A good benchmark to shine light into the ability of the sort to exploit instruction-level parallelism (ILP) is hot-u64-10000. The input are 10k <code>u64</code> values, which fits into the core private L2 data cache for the used test machines. The upper limit should be in the order of 4-5 instructions per cycle for such a dataset. 10k elements is enough to reliably exploit existing patterns in the input data. This can be reproduced by running <code>cargo bench hot-u64-&lt;pattern&gt;-10000</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-u64-10k-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-u64-10k-windows.png" width="600"/></a></p>
<p dir="auto">Using this data to write a headline like:</p>
<blockquote>
<p dir="auto">Rust std sort is 15x faster than C++ std sort</p>
</blockquote>
<p dir="auto">Is not an honest representation. And it seems questionable to compress so much information into a single number, while still remaining representative. Looking at this one input size, on one specific micro-architecture, using this specific set of compilers, and testing these synthetic patterns, yields:</p>
<ul dir="auto">
<li>intel_avx512 is generally faster than vqsort at this input size.</li>
<li>The two manually vectorized sort implementations intel_avx512 and vqsort, are not good at exploiting existing patterns in the input.</li>
<li>intel_avx512 struggles if there is one very common value in the input (random_p5).</li>
<li>cpp_std_msvc is generally the slowest of the comparison based sort implementations.</li>
<li>rust_std which is based on pdqsort performs similar to pdqsort, with the exception of fully ascending inputs.</li>
<li>ascending shows the largest variance with the fastest sort being ~31x faster han the slowest.</li>
<li>More natural random distributions like random_z1 close the gap between manually vectorized code and pdqsort derived designs, rust_std, ipnsort and crumsort.</li>
</ul>
<p dir="auto">Measuring random pattern performance across different sizes:
<a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>Classically bound by the Big-O complexity of O(N * log(N)) for random inputs, one could expect peak throughput to occur for the smallest inputs. However in reality for tiny input sizes the function call overhead, cold code, cache- and branch-prediction misses will limit throughput. And at very large sizes the main limit tends to be main-memory bandwidth, paired with the increased amount of work required per element. The peak throughput across most high-performance implementations, balancing these aforementioned factors, sits at input size ~1k.</li>
<li>vqsort as tested is exceedingly slow for small inputs. More on that below.</li>
<li>Starting at input size ~50k vqsort is the fastest.</li>
<li>ipnsort catches up to intel_avx512 at ~1m, despite only using SSE2 instructions and using no hardware specific code, and while caring about binary-size and compile-times, sometimes at the cost of performance.</li>
</ul>
<h4 dir="auto"><a id="user-content-linux-vs-windows" aria-hidden="true" href="#linux-vs-windows"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Linux vs Windows</h4>
<p dir="auto">Normally I would not expect any meaningful change between Linux and Windows, for code that with the exception of allocations and stack layout doesn&#39;t have any OS specific logic in it. Further, the two test machines use the same Skylake micro-architecture. Looking at the same cold-u64-random benchmark yields:</p>
<p dir="auto">Windows:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows.png" width="810"/></a></p>
<p dir="auto">Linux:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>intel_avx512, pdqsort, rust_std and ipnsort all lose performance</li>
<li>libstdc++ <code>std::sort</code> seems to perform better than the <code>msvc</code> version in this test. Showing significantly less regression than would be expected due to the CPU frequency difference.</li>
<li>The relative performance loss compared to Windows machine of ipnsort is larger than the one of intel_avx512. This is likely caused by the disabled CPU boost and overall conservative frequency cap of 3 GHz. The Linux machine is more indicative of a server environment. Where in contrast on the Windows machine ipnsort can boost higher than intel_avx512.</li>
<li>For larger inputs intel_avx512 shows better throughput on Windows with a measured sustained frequency of ~3.8GHz. E.g. at input size 1m on Windows ~67m elem/s vs ~54m elem/s on Linux. Which neatly matches the frequency difference.</li>
<li>vqsort however is a lot faster on Linux. Both in terms of peak performance and min input size to outperform the rest. This is an unexpected result, and considerable effort was spent to analyze the root cause of this. In the test vqsort used the same compiler and flags as intel_avx512. A different benchmark suite yields the same results, a different pre-built Clang version yields the same results. On a dual-booted Zen3 machine vqsort in AVX2 mode shows the same ~2x regression when going from Linux to Windows at input size 10k. In addition the higher frequency as shown by intel_avx512 on Windows should give it yet another boost above Linux which doesn&#39;t manifest. All this while the other sort implementations remain largely the same. The root cause for this was identified in collaboration with the vqsort authors [<a href="https://github.com/google/highway/tree/master/hwy/contrib/sort#optimizations-for-small-arrays">4</a>]. The tested vqsort version acquired randomness from the OS, for each call to vqsort in an attempt to mitigate a potential performance denial-of-service (DOS). This idea is similar to the use of SipHash as the default hasher in the Rust standard library. This explains why we see significantly different results on Windows and Linux. See the linked explanation for more details.</li>
<li>Another unexpected result is the peak throughput for intel_avx512 is lower than on Linux, despite higher frequencies. And the peak is reached earlier on Linux.</li>
</ul>
<p dir="auto">Measuring a more natural zipfian distribution random_z1 across different sizes:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random_z1-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random_z1-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random_z1-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random_z1-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>Generally similar to random pattern performance scaling.</li>
<li>intel_avx512 shows less uniform scaling, with overall worse performance compared to random.</li>
<li>The pdqsort derived implementations and vqsort see a performance uplift compared to random, by being able to efficiently filter out common values.</li>
</ul>
<p dir="auto">Measuring a random distribution where most (95%) values are the same value:
<a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random_p5-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random_p5-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random_p5-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random_p5-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>The pdqsort derived implementations, and rust_std_msvc on Windows and vqsort on Linux show excellent performance, several times faster than random pattern performance.</li>
<li>intel_avx512 shows worse than random pattern performance, yielding the overall worst performance in this scenario.</li>
</ul>
<h3 dir="auto"><a id="user-content-results-i32" aria-hidden="true" href="#results-i32"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Results <code>i32</code></h3>
<h4 dir="auto"><a id="user-content-i32-10k" aria-hidden="true" href="#i32-10k"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>i32-10k</h4>
<p dir="auto">The main difference between <code>i32</code> and <code>u64</code> is that <code>i32</code> is only 4 bytes compared to the 8 bytes of <code>u64</code>. Practically all high performance sort implementations are sensitive to the bandwidth of the CPUs memory subsystems. In theory this should allow higher cache throughput, better cache utilization and higher vector ALU throughput.</p>
<p dir="auto">Windows:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-i32-10k-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-i32-10k-windows.png" width="600"/></a></p>
<p dir="auto">Linux:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-i32-10k-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-i32-10k-linux.png" width="600"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>Both manually vectorized implementations can nearly double their throughput. vqsort only demonstrates this on Linux for this size. Showing a smaller relative gain on Windows.</li>
<li>All comparison based sort implementations only see a small change compared to <code>u64</code>.</li>
</ul>
<h4 dir="auto"><a id="user-content-i32-scaling" aria-hidden="true" href="#i32-scaling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>i32-scaling</h4>
<p dir="auto">Windows:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-i32-scaling-random-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-i32-scaling-random-windows.png" width="810"/></a></p>
<p dir="auto">Linux:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-i32-scaling-random-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-i32-scaling-random-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>The manually vectorized implementations are a lot better at leveraging the increased potential throughput, than the generic comparison based implementations.</li>
<li>Very similar scaling to <code>u64</code> for the non manually vectorized implementations.</li>
</ul>
<h3 dir="auto"><a id="user-content-direct-comparison" aria-hidden="true" href="#direct-comparison"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Direct comparison</h3>
<p dir="auto">A comprehensive look at performance for a single type can be achieved by comparing two implementations and plotting their symmetric relative speedup and slowdown on the Y-axis and the test size on the X-axis. Each line representing a pattern. Eg. a-vs-b, 1.7x means a is 1.7x faster than b, and -1.7x means b is 1.7x faster than a. The first name in the title is sort a, and the second name is b. The graphs are fixed to the Y-range -3x  to 3x to allow comparison between graphs.</p>
<p dir="auto">Comparing the two manually vectorized implementations <code>u64</code>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-u64-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-u64-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-u64-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-u64-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>There is stark difference between Linux and Windows. All lines move to the left, meaning vqsort overtakes intel_avx512 at smaller input sizes. And the graph moves down, in the size range of 100k to 10m vqsort is ~2x faster on Linux while only ~1.4x faster on Windows for random inputs.</li>
<li>On Windows below 10k and 200 on Linux, intel_avx512 is faster in every pattern than vqsort.</li>
<li>The random_5p pattern triggers bad partitions in intel_avx512 which does not safe-guard against it, making vqsort ~14x faster than intel_avx512 for <code>len == 10m</code> on Windows, and ~26x on Linux. This seems to indicate that intel_avx512 has worse than <code>O(N * log(N))</code> worst-case scaling, likely caused by poor pivot selection and no mitigation strategies.</li>
</ul>
<p dir="auto">Comparing the two manually vectorized implementations <code>i32</code>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-i32-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-i32-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-i32-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-vqsort-cold-i32-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li><code>i32</code> performs very similar to <code>u64</code> with the exception of random_z1 on Linux showing a larger advantage for vqsort.</li>
</ul>
<p dir="auto">Compared to the fastest generic comparison based implementation ipnsort:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-ipnsort-cold-u64-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-ipnsort-cold-u64-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/intel_avx512-vs-ipnsort-cold-u64-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/intel_avx512-vs-ipnsort-cold-u64-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>Between input size 36 and 1k, intel_avx512 is faster in nearly every pattern on Linux. Between 1k and 100k intel_avx512 is faster for fully random and saw like patterns.</li>
<li>ipnsort is faster for low- cardinality and zipfian patterns, where it can leverage the pdqsort derived ability to filter out common values.</li>
<li>On Linux and with a max frequency of 3 GHz intel_avx512 is faster for fully random and saw like patterns.</li>
<li>As noted earlier, intel_avx512 shows worse than <code>O(N * log(N))</code> scaling for random_p5, making ipnsort ~15x times faster on Windows and ~19x Linux for input size 10m.</li>
<li>Both intel_avx512 and vqsort are unable to leverage already sorted inputs. While the pdqsort derived implementations can handle this case in <code>O(N)</code>. At input size 100k ipnsort is ~20x faster on Windows and ~15x on Linux. And at input size 10m ~10x faster on Windows and ~16x on Linux. Where the 10m case will be mostly limited by main memory bandwidth, of which the Server Skylake implementation has more.</li>
</ul>
<h3 dir="auto"><a id="user-content-non-avx-512-results" aria-hidden="true" href="#non-avx-512-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Non AVX-512 results</h3>
<p dir="auto">The overwhelming majority of consumer devices does not support AVX-512. intel_avx512 only supports AVX-512 devices, however vqsort is written on top of highway [<a href="https://github.com/google/highway">5</a>], a SIMD abstraction that supports various platforms and hardware capabilities. For example it can be used on machines that only support AVX2, as well as Arm chips with Neon support.</p>
<h4 dir="auto"><a id="user-content-avx2-test-machine" aria-hidden="true" href="#avx2-test-machine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>AVX2 test machine</h4>
<div data-snippet-clipboard-copy-content="Linux 6.2
rustc 1.70.0-nightly (17c116721 2023-03-29)
clang version 15.0.7
AMD Ryzen 9 5900X 12-Core Processor (Zen3 micro-architecture)
CPU boost enabled."><pre><code>Linux 6.2
rustc 1.70.0-nightly (17c116721 2023-03-29)
clang version 15.0.7
AMD Ryzen 9 5900X 12-Core Processor (Zen3 micro-architecture)
CPU boost enabled.
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-zen3.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-zen3.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>vqsort shows the same exceptionally poor small size performance.</li>
<li>All generic comparison based implementations see a large performance uplift compared to Skylake.</li>
<li>ipnsort is much closer to vqsort for large inputs compared to the Skylake Linux machine.</li>
<li>crumsort shows smaller gains compared to pdqsort and rust_std than on the Skylake machines.</li>
<li>vqsort hits its peak throughput earlier than on the Skylake Linux machine.</li>
</ul>
<h4 dir="auto"><a id="user-content-neon-test-machine" aria-hidden="true" href="#neon-test-machine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Neon test machine</h4>
<div data-snippet-clipboard-copy-content="Darwin Kernel Version 22.1.0
rustc rustc 1.70.0-nightly (f15f0ea73 2023-03-04)
Apple clang version 14.0.0
M1 Pro 8-Core Processor (Firestorm P-core micro-architecture)
CPU boost enabled."><pre><code>Darwin Kernel Version 22.1.0
rustc rustc 1.70.0-nightly (f15f0ea73 2023-03-04)
Apple clang version 14.0.0
M1 Pro 8-Core Processor (Firestorm P-core micro-architecture)
CPU boost enabled.
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-neon.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-neon.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>vqsort shows the same exceptionally poor small size performance.</li>
<li>For most sort implementations, throughput is nearly doubled compared to the Skylake chip at similar clock speeds. Showcasing Firestorm&#39;s micro-architecture capabilities, leveraging a wide out-of-order (OoO) design.</li>
<li>Compared to the Skylake and Zen3 results, Firestorm can hit peak throughput earlier. Indicative of an excellent branch prediction implementation with smaller miss penalities, fast learning and overall an excellent cache setup.</li>
<li>cpp_std_libcxx performs the worst in general.</li>
<li>vqsort shows performance similar to pdqsort, limited by the Neon instruction set and 128-bit vector width.</li>
<li>The two implementations heavily exploiting ILP, crumsort and ipnsort, see larger uplifts compared to pdqsort as their baseline, with ipnsort showing ~1.9x the performance compared to ~1.6x on Skylake.</li>
<li>At input size 50k there is a performance valley that is reproducible and seems to affect both crumsort and ipnsort.</li>
<li>Power measurements indicate that ipnsort uses ~1.4x less power than vqsort while being nearly twice as fast. Which would imply ~2.7x better power efficiency. This indicates that in some scenarios code that focuses on exploiting ILP can be more energy efficient than vectorized code. With the additional benefit of not being hardware specific.</li>
</ul>
<h3 dir="auto"><a id="user-content-c-sort-interface" aria-hidden="true" href="#c-sort-interface"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>C sort interface</h3>
<p dir="auto">crumsort mirrors the C standard library <code>qsort</code> interface. Which asks for a comparison function <code>int (*comp)(const void *, const void *)</code>. And while a lot of code implement this with <code>return a - b;</code>, such code leads to UB for some integers and is not generic. A simple implementation as presented on the cppreference website is this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int compare_ints(const void* a, const void* b)
{
    int arg1 = *(const int*)a;
    int arg2 = *(const int*)b;
 
    if (arg1 &lt; arg2) return -1;
    if (arg1 &gt; arg2) return 1;
    return 0;
 
    // return (arg1 &gt; arg2) - (arg1 &lt; arg2); // possible shortcut
    // return arg1 - arg2; // erroneous shortcut (fails if INT_MIN is present)
}"><pre><span>int</span> <span>compare_ints</span>(<span>const</span> <span>void</span>* a, <span>const</span> <span>void</span>* b)
{
    <span>int</span> arg1 = *(<span>const</span> <span>int</span>*)a;
    <span>int</span> arg2 = *(<span>const</span> <span>int</span>*)b;
 
    <span>if</span> (arg1 &lt; arg2) <span>return</span> -<span>1</span>;
    <span>if</span> (arg1 &gt; arg2) <span>return</span> <span>1</span>;
    <span>return</span> <span>0</span>;
 
    <span><span>//</span> return (arg1 &gt; arg2) - (arg1 &lt; arg2); // possible shortcut</span>
    <span><span>//</span> return arg1 - arg2; // erroneous shortcut (fails if INT_MIN is present)</span>
}</pre></div>
<p dir="auto">However depending on your compiler, this will generate branches. Which in turn cause branch miss-prediction which is undesirable, especially for code that wants to exploit ILP. Another alternative to write this in a way that generates good branchless code-gen, especially with Clang is this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int compare_ints(const void* a_ptr, const void* b_ptr)
{
    const int a = *(const int*)a_ptr;
    const int b = *(const int*)b_ptr;

    const bool is_less = a &lt; b;
    const bool is_more = a &gt; b;
    return (is_less * -1) + (is_more * 1);
}"><pre><span>int</span> <span>compare_ints</span>(<span>const</span> <span>void</span>* a_ptr, <span>const</span> <span>void</span>* b_ptr)
{
    <span>const</span> <span>int</span> a = *(<span>const</span> <span>int</span>*)a_ptr;
    <span>const</span> <span>int</span> b = *(<span>const</span> <span>int</span>*)b_ptr;

    <span>const</span> <span>bool</span> is_less = a &lt; b;
    <span>const</span> <span>bool</span> is_more = a &gt; b;
    <span>return</span> (is_less * -<span>1</span>) + (is_more * <span>1</span>);
}</pre></div>
<p dir="auto">This can avoid the issue with branch miss-prediction, however another issue remains. C abstracts over user-defined logic with function pointers. In contrast to C++ and Rust closures these can&#39;t be inlined without profile-guided optimizations (PGO). crumsort until recently required a custom define which disables a custom comparison globally via a define <code>#define cmp(a, b) (*(a) &gt; *(b))</code> before the header only implementation is parsed. This is what is required for best performance, and what all benchmarks here used. However this makes crumsort more akin to the manually vectorized implementations in terms of supported types. User defined types that don&#39;t fit in a <code>long long</code> will require source level modifications. And for best performance with such interface a unique version of crumsort would have to be compiled for every type, via the build system. Fundamentally this is a C issue and only a crumsort issue, because it is implemented in C and mirrors the <code>qsort</code> interface.</p>
<p dir="auto">Comparing the results of crumsort to crumsort using the branchless comparison function (c_crumsort_generic) on the Skylake Linux machine gives:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-skylake-linux-crumsort.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-skylake-linux-crumsort.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/crumsort-vs-crumsort_generic-skylake-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/crumsort-vs-crumsort_generic-skylake-linux.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>When used in a generic fashion crumsort shows significantly worse performance, ~2.4x slower for larger sizes and most random patterns.</li>
<li>c_crumsort_generic shows performance close to cpp_std_gnu.</li>
<li>Patterns such as ascending and random_d20 which perform fewer total comparisons, are affected more.</li>
</ul>
<h3 dir="auto"><a id="user-content-hot-benchmarks" aria-hidden="true" href="#hot-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hot benchmarks</h3>
<p dir="auto">Running a sort implementation several million times in a loop, as is the standard for tools like google benchmark and criterion, with new unique inputs of the same size and pattern is not representative of real world application performance. To simulate a program that calls sort occasionally with varying sizes, and a cold CPU prediction state, the benchmarks are also run in a mode where between each sort call, the CPU prediction state is trashed [<a href="https://github.com/Voultapher/sort-research-rs/blob/b7bcd199e861d6f8b265164242f3c34d5c36c75f/benches/trash_prediction.rs#L7">6</a>]. These benchmarks are marked <code>cold-*</code>. And they for the basis for the above scaling graphs. Benchmarks using the default hot loop logic are marked <code>hot-*</code>. <strong>hot benchmark numbers should be interpreted as best case performance under laboratory conditions.</strong></p>
<h4 dir="auto"><a id="user-content-u64-scaling" aria-hidden="true" href="#u64-scaling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>u64-scaling</h4>
<p dir="auto">Measuring random pattern performance across different sizes:
<a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-u64-scaling-random-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-u64-scaling-random-windows.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>Even in a hot loop vqsort as tested is exceedingly slow for small inputs.</li>
<li>intel_avx512 is fast across all sizes, when benchmarked in a hot loop.</li>
<li>intel_avx512 and crumsort, differentiate themselves from the other implementations by not using insertion sort for such inputs.</li>
<li>Starting at ~50k vqsort is the fastest.</li>
<li>ipnsort catches up to intel_avx512 at ~1m, despite only using SSE2 instructions and using no hardware specific code.</li>
<li>Starting at inputs size ~10k hot and cold results are largely the same.</li>
</ul>
<p dir="auto">Instrumenting the Rust standard library and building a custom version of rustc that logs the input size and time spent in <code>slice::sort</code> [<a href="https://github.com/rust-lang/rust/pull/108005#issuecomment-1440360350" data-hovercard-type="pull_request" data-hovercard-url="/rust-lang/rust/pull/108005/hovercard">7</a>], which is the stable sort of the Rust standard library, yielded these insights clean compiling 60 crates:</p>
<p dir="auto">Out of the ~500k calls to <code>slice::sort</code> 71.6% were <code>len == 0</code>, 22.8% were <code>len == 1</code> and in total 99+% were <code>len &lt;= 20</code> and together they account for ~50% of the time spent in <code>slice::sort</code>.</p>
<p dir="auto">Looking at that important <code>len &lt;= 20</code> range:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows-zoomed.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-windows-zoomed.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>ipnsort leads the chart, in contrast to the hot results.</li>
<li>Peak throughput for <code>len &lt;= 20</code> drops by 5x compared to hot results.</li>
</ul>
<p dir="auto">Another aspect that should be mentioned, is frequency scaling. With the exception of the latest generations of Intel and AMD  micro-architectures, Golden Cove and Zen4, previous Intel AVX-512 implementations scaled down the CPU boost frequency when certain AVX-512 instructions were executed [<a href="https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html" rel="nofollow">8</a>]. This may affect real world usage and performance. Another aspect potentially affected by cold code is the AVX-512 startup, a phenomenon limited to the arguably poor Skylake AVX-512 implementation.</p>
<p dir="auto">The shown cold and hot benchmarks, model the two extremes of the likely range of possible results.</p>
<h4 dir="auto"><a id="user-content-further-hot-results" aria-hidden="true" href="#further-hot-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Further hot results</h4>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-i32-scaling-random-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-i32-scaling-random-windows.png" width="810"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/hot-i32-scaling-random-linux.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/hot-i32-scaling-random-linux.png" width="810"/></a></p>
<h2 dir="auto"><a id="user-content-authors-conclusion-and-opinion" aria-hidden="true" href="#authors-conclusion-and-opinion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Author&#39;s conclusion and opinion</h2>
<p dir="auto">intel_avx512 is a very fast sort implementation, especially for random inputs, and it shows nice scaling across different input sizes. However it shows worse than <code>O(N * log(N))</code> scaling in simple patterns, and suffers from poor pivot selection without a fallback. For types that are smaller than 8 bytes, such as <code>u8</code>, or <code>i32</code> both manually vectorized implementations showcase far higher performance than generic comparison based implementations. This analysis did not look into the overhead of runtime dispatch, which would be required to use it in code that is not uniquely compiled for a specific platform. Looking at some scenarios:</p>
<h4 dir="auto"><a id="user-content-specialized-sort-in-a-library" aria-hidden="true" href="#specialized-sort-in-a-library"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Specialized sort in a library</h4>
<p dir="auto">intel_avx512 can be a great choice, if the input consists of plain integers or floats, of varying sizes, assumed mostly random, not very small, not of low-cardinality and the appropriate hardware supporting AVX-512 is available. At the same time it can be problematic to use it as the sole replacement for calls to sort because of cold performance, worst case performance and potential frequency scaling issues.</p>
<h4 dir="auto"><a id="user-content-specialized-hpc-code" aria-hidden="true" href="#specialized-hpc-code"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Specialized HPC code</h4>
<p dir="auto">If you need the best possible throughput, and know you only have large inputs of supported types, vqsort is the faster option. Best vqsort performance requires Linux and clang. Other compilers should not be used and yield code several times slower.</p>
<h4 dir="auto"><a id="user-content-language-standard-library" aria-hidden="true" href="#language-standard-library"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Language standard library</h4>
<p dir="auto">A standard library implementation should be good in most scenarios, with bad performance in as few scenarios as possible. There are many glaring issues with intel_avx512 for such a use case. It would only support a small fraction of devices, would increase binary size for all x86 targets. It is not good at exploiting existing patterns in the input. It can&#39;t support user-defined comparisons, which would potentially mean that <code>v.sort_unstable()</code> and <code>v.sort_unstable_by(|a, b| a.cmp(b))</code> could have a surprising difference in performance. It can&#39;t support user-defined types without additional code by the user, eg. <code>#[derive(Copy, Clone)]struct X(i32)</code> could have a surprising difference in performance to just <code>i32</code>. On top of that, there are issues with cold performance and frequency scaling.</p>
<p dir="auto">All the results are a snapshot of the respective implementations. After performing these measurements, there have been changes to crumsort and ipnsort, and the people behind vqsort are looking into small input performance. I specifically chose not to re-run the benchmarks with newer versions to avoid a bias of fixing certain things such as poor saws_long performance in ipnsort, without giving all the implementations the chance to do such improvements.</p>
<h2 dir="auto"><a id="user-content-thanks" aria-hidden="true" href="#thanks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Thanks</h2>
<p dir="auto">Thank you Jan Wassenberg for your detailed feedback, extensive help in running benchmarks and helping me understand some of the observations. Thank you Roland Bock for your detailed feedback and ideas how to make this writeup more readable.</p>
<h2 dir="auto"><a id="user-content-addendum" aria-hidden="true" href="#addendum"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Addendum</h2>
<h3 dir="auto"><a id="user-content-updated-vqsort-results" aria-hidden="true" href="#updated-vqsort-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Updated vqsort results</h3>
<p dir="auto">New vqsort version fe85fdf. Same graphs as above, with cpp_vqsort_new added. Only vqsort was re-tested here.</p>
<p dir="auto">Skylake (AVX-512):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-vqsort-new-skylake-windows.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-vqsort-new-skylake-windows.png" width="810"/></a></p>
<p dir="auto">Zen3 (AVX2):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://www.thapaliya.com/Voultapher/sort-research-rs/blob/main/writeup/intel_avx512/assets/cold-u64-scaling-random-vqsort-new-zen3.png"><img src="https://www.thapaliya.com/Voultapher/sort-research-rs/raw/main/writeup/intel_avx512/assets/cold-u64-scaling-random-vqsort-new-zen3.png" width="810"/></a></p>
<p dir="auto">Observations:</p>
<ul dir="auto">
<li>The new version of vqsort addresses the main issue of very poor performance for smaller inputs, and even manages to improve overall performance for larger inputs. With this it is now undoubtedly the fastest sort implementation in my testing, when AVX2 or AVX-512 are available for random-like patterns.</li>
</ul>
</article>
  </div>

    </div>

  </div></div>
  </body>
</html>
