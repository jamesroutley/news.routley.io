<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/papers/2312.11514">Original</a>
    <h1>LLM in a Flash: Efficient LLM Inference with Limited Memory</h1>
    
    <div id="readability-page-1" class="page"><section><div data-props="{&#34;comments&#34;:[{&#34;id&#34;:&#34;65825e287cec0a2080cbaf0b&#34;,&#34;author&#34;:{&#34;avatarUrl&#34;:&#34;/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg&#34;,&#34;fullname&#34;:&#34;Michael Barry&#34;,&#34;name&#34;:&#34;MichaelBarryUK&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false},&#34;createdAt&#34;:&#34;2023-12-20T03:23:20.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;ðŸ’©&#34;,&#34;html&#34;:&#34;&lt;p&gt;ðŸ’©&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2023-12-20T03:23:20.638Z&#34;,&#34;author&#34;:{&#34;avatarUrl&#34;:&#34;/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg&#34;,&#34;fullname&#34;:&#34;Michael Barry&#34;,&#34;name&#34;:&#34;MichaelBarryUK&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false}},&#34;numEdits&#34;:0,&#34;editors&#34;:[&#34;MichaelBarryUK&#34;],&#34;reactions&#34;:[{&#34;reaction&#34;:&#34;ðŸ˜”&#34;,&#34;users&#34;:[&#34;nlpguy&#34;],&#34;count&#34;:1}],&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.7121967673301697},&#34;isReport&#34;:false}}],&#34;primaryEmailConfirmed&#34;:false,&#34;paper&#34;:{&#34;id&#34;:&#34;2312.11514&#34;,&#34;authors&#34;:[{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af2&#34;,&#34;name&#34;:&#34;Keivan Alizadeh&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af3&#34;,&#34;name&#34;:&#34;Iman Mirzadeh&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af4&#34;,&#34;name&#34;:&#34;Dmitry Belenko&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af5&#34;,&#34;name&#34;:&#34;Karen Khatamifard&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af6&#34;,&#34;name&#34;:&#34;Minsik Cho&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af7&#34;,&#34;name&#34;:&#34;Carlo C Del Mundo&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af8&#34;,&#34;name&#34;:&#34;Mohammad Rastegari&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af9&#34;,&#34;name&#34;:&#34;Mehrdad Farajtabar&#34;,&#34;hidden&#34;:false}],&#34;publishedAt&#34;:&#34;2023-12-12T18:57:08.000Z&#34;,&#34;title&#34;:&#34;LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory&#34;,&#34;summary&#34;:&#34;Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nintensive computational and memory requirements present challenges, especially\nfor devices with limited DRAM capacity. This paper tackles the challenge of\nefficiently running LLMs that exceed the available DRAM capacity by storing the\nmodel parameters on flash memory but bringing them on demand to DRAM. Our\nmethod involves constructing an inference cost model that harmonizes with the\nflash memory behavior, guiding us to optimize in two critical areas: reducing\nthe volume of data transferred from flash and reading data in larger, more\ncontiguous chunks. Within this flash memory-informed framework, we introduce\ntwo principal techniques. First, \&#34;windowing&#39;\&#34; strategically reduces data\ntransfer by reusing previously activated neurons, and second, \&#34;row-column\nbundling\&#34;, tailored to the sequential data access strengths of flash memory,\nincreases the size of data chunks read from flash memory. These methods\ncollectively enable running models up to twice the size of the available DRAM,\nwith a 4-5x and 20-25x increase in inference speed compared to naive loading\napproaches in CPU and GPU, respectively. Our integration of sparsity awareness,\ncontext-adaptive loading, and a hardware-oriented design paves the way for\neffective inference of LLMs on devices with limited memory.&#34;,&#34;upvotes&#34;:22},&#34;canReadDatabase&#34;:false,&#34;canManageCommunity&#34;:false,&#34;hasHfLevelAccess&#34;:false,&#34;publishedOnDailyAt&#34;:&#34;2023-12-20T02:32:43.361Z&#34;,&#34;upvoted&#34;:false,&#34;upvoters&#34;:[{&#34;avatarUrl&#34;:&#34;/avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Kaio Ken&#34;,&#34;user&#34;:&#34;kaiokendev&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/1b4c9afab0c4b10ac50fca0c738bb61a.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;lbn&#34;,&#34;user&#34;:&#34;llbn&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637711517996-6039478ab3ecf716b1a5fd4d.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;user&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/358503a958bacff790c5830f24946378.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Xin&#34;,&#34;user&#34;:&#34;Nuclear6&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649899774659-6254f8e5d21e4cc386b881ad.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Somshubra Majumdar&#34;,&#34;user&#34;:&#34;smajumdar94&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce50353529c21a228ab2d8/153N-GZ0Vj5YXWMW3noQe.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Se June Joo&#34;,&#34;user&#34;:&#34;Joocjun&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/d44dee29d7aae9e545cb7847d835bae7.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Aiden Shihadeh&#34;,&#34;user&#34;:&#34;sdkv2&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/1e9617311b04bbf296f061a9a85b12cc.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Ai Studio Lab&#34;,&#34;user&#34;:&#34;aistudiolab&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Arthur Zucker&#34;,&#34;user&#34;:&#34;ArthurZ&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/d6f733991b3011ce53c9055f3083332f.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Erland Hilman Fuadi&#34;,&#34;user&#34;:&#34;Masa-Erland&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637b53a7a2460cde612b127b/urzriWZ00OvHgESYnBlKX.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Krinal Joshi&#34;,&#34;user&#34;:&#34;krinal&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/8860b175ae0d292bb5ad8502a97b9b9f.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Mous&#34;,&#34;user&#34;:&#34;Anony&#34;,&#34;type&#34;:&#34;user&#34;}],&#34;acceptLanguages&#34;:[&#34;*&#34;]}" data-target="PaperContent">

<div><h2>Abstract</h2>
	<p>Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
intensive computational and memory requirements present challenges, especially
for devices with limited DRAM capacity. This paper tackles the challenge of
efficiently running LLMs that exceed the available DRAM capacity by storing the
model parameters on flash memory but bringing them on demand to DRAM. Our
method involves constructing an inference cost model that harmonizes with the
flash memory behavior, guiding us to optimize in two critical areas: reducing
the volume of data transferred from flash and reading data in larger, more
contiguous chunks. Within this flash memory-informed framework, we introduce
two principal techniques. First, &#34;windowing&#39;&#34; strategically reduces data
transfer by reusing previously activated neurons, and second, &#34;row-column
bundling&#34;, tailored to the sequential data access strengths of flash memory,
increases the size of data chunks read from flash memory. These methods
collectively enable running models up to twice the size of the available DRAM,
with a 4-5x and 20-25x increase in inference speed compared to naive loading
approaches in CPU and GPU, respectively. Our integration of sparsity awareness,
context-adaptive loading, and a hardware-oriented design paves the way for
effective inference of LLMs on devices with limited memory.</p></div>







</div></section></div>
  </body>
</html>
