<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/papers/2312.11514">Original</a>
    <h1>LLM in a Flash: Efficient LLM Inference with Limited Memory</h1>
    
    <div id="readability-page-1" class="page"><div>
<div><section><div data-props="{&#34;comments&#34;:[{&#34;id&#34;:&#34;65825e287cec0a2080cbaf0b&#34;,&#34;author&#34;:{&#34;avatarUrl&#34;:&#34;/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg&#34;,&#34;fullname&#34;:&#34;Michael Barry&#34;,&#34;name&#34;:&#34;MichaelBarryUK&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false},&#34;createdAt&#34;:&#34;2023-12-20T03:23:20.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;ðŸ’©&#34;,&#34;html&#34;:&#34;&lt;p&gt;ðŸ’©&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2023-12-20T03:23:20.638Z&#34;,&#34;author&#34;:{&#34;avatarUrl&#34;:&#34;/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg&#34;,&#34;fullname&#34;:&#34;Michael Barry&#34;,&#34;name&#34;:&#34;MichaelBarryUK&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false}},&#34;numEdits&#34;:0,&#34;editors&#34;:[&#34;MichaelBarryUK&#34;],&#34;reactions&#34;:[{&#34;reaction&#34;:&#34;ðŸ˜”&#34;,&#34;users&#34;:[&#34;nlpguy&#34;,&#34;10100101j&#34;,&#34;IsoSpandy&#34;,&#34;hyper88&#34;],&#34;count&#34;:4}],&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.7121967673301697},&#34;isReport&#34;:false}}],&#34;primaryEmailConfirmed&#34;:false,&#34;paper&#34;:{&#34;id&#34;:&#34;2312.11514&#34;,&#34;authors&#34;:[{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af2&#34;,&#34;name&#34;:&#34;Keivan Alizadeh&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af3&#34;,&#34;name&#34;:&#34;Iman Mirzadeh&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af4&#34;,&#34;name&#34;:&#34;Dmitry Belenko&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af5&#34;,&#34;name&#34;:&#34;Karen Khatamifard&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af6&#34;,&#34;name&#34;:&#34;Minsik Cho&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af7&#34;,&#34;name&#34;:&#34;Carlo C Del Mundo&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af8&#34;,&#34;name&#34;:&#34;Mohammad Rastegari&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;6582524a5f6d8343825e4af9&#34;,&#34;name&#34;:&#34;Mehrdad Farajtabar&#34;,&#34;hidden&#34;:false}],&#34;publishedAt&#34;:&#34;2023-12-12T18:57:08.000Z&#34;,&#34;title&#34;:&#34;LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory&#34;,&#34;summary&#34;:&#34;Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nintensive computational and memory requirements present challenges, especially\nfor devices with limited DRAM capacity. This paper tackles the challenge of\nefficiently running LLMs that exceed the available DRAM capacity by storing the\nmodel parameters on flash memory but bringing them on demand to DRAM. Our\nmethod involves constructing an inference cost model that harmonizes with the\nflash memory behavior, guiding us to optimize in two critical areas: reducing\nthe volume of data transferred from flash and reading data in larger, more\ncontiguous chunks. Within this flash memory-informed framework, we introduce\ntwo principal techniques. First, \&#34;windowing&#39;\&#34; strategically reduces data\ntransfer by reusing previously activated neurons, and second, \&#34;row-column\nbundling\&#34;, tailored to the sequential data access strengths of flash memory,\nincreases the size of data chunks read from flash memory. These methods\ncollectively enable running models up to twice the size of the available DRAM,\nwith a 4-5x and 20-25x increase in inference speed compared to naive loading\napproaches in CPU and GPU, respectively. Our integration of sparsity awareness,\ncontext-adaptive loading, and a hardware-oriented design paves the way for\neffective inference of LLMs on devices with limited memory.&#34;,&#34;upvotes&#34;:28},&#34;canReadDatabase&#34;:false,&#34;canManageCommunity&#34;:false,&#34;hasHfLevelAccess&#34;:false,&#34;publishedOnDailyAt&#34;:&#34;2023-12-20T02:32:43.361Z&#34;,&#34;upvoted&#34;:false,&#34;upvoters&#34;:[{&#34;avatarUrl&#34;:&#34;/avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Kaio Ken&#34;,&#34;user&#34;:&#34;kaiokendev&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/1b4c9afab0c4b10ac50fca0c738bb61a.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;lbn&#34;,&#34;user&#34;:&#34;llbn&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637711517996-6039478ab3ecf716b1a5fd4d.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;user&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/358503a958bacff790c5830f24946378.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Xin&#34;,&#34;user&#34;:&#34;Nuclear6&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649899774659-6254f8e5d21e4cc386b881ad.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Somshubra Majumdar&#34;,&#34;user&#34;:&#34;smajumdar94&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce50353529c21a228ab2d8/153N-GZ0Vj5YXWMW3noQe.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Se June Joo&#34;,&#34;user&#34;:&#34;Joocjun&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/d44dee29d7aae9e545cb7847d835bae7.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Aiden Shihadeh&#34;,&#34;user&#34;:&#34;sdkv2&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/1e9617311b04bbf296f061a9a85b12cc.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Ai Studio Lab&#34;,&#34;user&#34;:&#34;aistudiolab&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Arthur Zucker&#34;,&#34;user&#34;:&#34;ArthurZ&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/d6f733991b3011ce53c9055f3083332f.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Erland Hilman Fuadi&#34;,&#34;user&#34;:&#34;Masa-Erland&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637b53a7a2460cde612b127b/urzriWZ00OvHgESYnBlKX.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Krinal Joshi&#34;,&#34;user&#34;:&#34;krinal&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/8860b175ae0d292bb5ad8502a97b9b9f.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Mous&#34;,&#34;user&#34;:&#34;Anony&#34;,&#34;type&#34;:&#34;user&#34;}],&#34;acceptLanguages&#34;:[&#34;*&#34;]}" data-target="PaperContent">

<div><h2>Abstract</h2>
	<p>Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
intensive computational and memory requirements present challenges, especially
for devices with limited DRAM capacity. This paper tackles the challenge of
efficiently running LLMs that exceed the available DRAM capacity by storing the
model parameters on flash memory but bringing them on demand to DRAM. Our
method involves constructing an inference cost model that harmonizes with the
flash memory behavior, guiding us to optimize in two critical areas: reducing
the volume of data transferred from flash and reading data in larger, more
contiguous chunks. Within this flash memory-informed framework, we introduce
two principal techniques. First, &#34;windowing&#39;&#34; strategically reduces data
transfer by reusing previously activated neurons, and second, &#34;row-column
bundling&#34;, tailored to the sequential data access strengths of flash memory,
increases the size of data chunks read from flash memory. These methods
collectively enable running models up to twice the size of the available DRAM,
with a 4-5x and 20-25x increase in inference speed compared to naive loading
approaches in CPU and GPU, respectively. Our integration of sparsity awareness,
context-adaptive loading, and a hardware-oriented design paves the way for
effective inference of LLMs on devices with limited memory.</p></div>







</div></section>
		<section>

			<h2><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
				Models citing this paper
				<span>0</span></h2>
			<p>No model linking this paper</p>
				<p>Cite arxiv.org/abs/2312.11514 in a model README.md to link it from this page.
				</p>

			<h2><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
				Datasets citing this paper
				<span>0</span></h2>
			<p>No dataset linking this paper</p>
				<p>Cite arxiv.org/abs/2312.11514 in a dataset README.md to link it from this page.
				</p>

			<h3><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M7.80914 18.7462V24.1907H13.2536V18.7462H7.80914Z" fill="#FF3270"></path><path d="M18.7458 18.7462V24.1907H24.1903V18.7462H18.7458Z" fill="#861FFF"></path><path d="M7.80914 7.80982V13.2543H13.2536V7.80982H7.80914Z" fill="#097EFF"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M4 6.41775C4 5.08246 5.08246 4 6.41775 4H14.6457C15.7626 4 16.7026 4.75724 16.9802 5.78629C18.1505 4.67902 19.7302 4 21.4685 4C25.0758 4 28.0003 6.92436 28.0003 10.5317C28.0003 12.27 27.3212 13.8497 26.2139 15.02C27.243 15.2977 28.0003 16.2376 28.0003 17.3545V25.5824C28.0003 26.9177 26.9177 28.0003 25.5824 28.0003H17.0635H14.9367H6.41775C5.08246 28.0003 4 26.9177 4 25.5824V15.1587V14.9367V6.41775ZM7.80952 7.80952V13.254H13.254V7.80952H7.80952ZM7.80952 24.1907V18.7462H13.254V24.1907H7.80952ZM18.7462 24.1907V18.7462H24.1907V24.1907H18.7462ZM18.7462 10.5317C18.7462 9.0283 19.9651 7.80952 21.4685 7.80952C22.9719 7.80952 24.1907 9.0283 24.1907 10.5317C24.1907 12.0352 22.9719 13.254 21.4685 13.254C19.9651 13.254 18.7462 12.0352 18.7462 10.5317Z" fill="black"></path><path d="M21.4681 7.80982C19.9647 7.80982 18.7458 9.02861 18.7458 10.5321C18.7458 12.0355 19.9647 13.2543 21.4681 13.2543C22.9715 13.2543 24.1903 12.0355 24.1903 10.5321C24.1903 9.02861 22.9715 7.80982 21.4681 7.80982Z" fill="#FFD702"></path></svg>
				Spaces citing this paper
				<span>0</span></h3>

			<p>No Space linking this paper</p>
				<p>Cite arxiv.org/abs/2312.11514 in a Space README.md to link it from this page.
				</p>

			<h2><svg width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 
	12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
				Collections including this paper
				<span>4</span></h2>
			<nav>








					</nav></section></div></div></div>
  </body>
</html>
