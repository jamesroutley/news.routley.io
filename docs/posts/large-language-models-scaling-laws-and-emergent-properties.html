<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cthiriet.com/articles/scaling-laws">Original</a>
    <h1>Large language models: Scaling laws and emergent properties</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>This blog post gives more details on the science of training large autoregressive language models that have recently been successful worldwide (ChatGPT, GPT-4). Some thoughts on the similarities between the biological brain and artificial neural networks are also presented.</em></p>
<h2>What are the Scaling Laws?</h2>
<p>Today, the trend is to train increasingly larger models, some of which now exceed 540 billion parameters (<a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM</a>). This trend was motivated by a paper published by OpenAI in 2020 titled <em><a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a></em>.</p>
<p>Increasing computational resources to train ever more performant models raises a question: <strong>In what proportion should we increase the number of parameters <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span> and the size of the training dataset <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span>?</strong></p>
<p>This paper suggests that to train larger models, increasing the number of parameters is three times more important than increasing the size of the training data (consisting of individual units of text called <em>tokens</em>).</p>
<h3>Updated version</h3>
<p>However, another paper, <a href="https://arxiv.org/pdf/2203.15556.pdf">published by DeepMind in 2022</a>, shows empirically that increasing the size of the training data is just as important as increasing the number of parameters itself and that these <strong>must be increased in equal proportions</strong>. Moreover, training an optimal model requires about <strong>20 times more tokens than parameters</strong> (excluding embedding).</p>
<p>It seems that data is more important than the number of parameters. Moreover, recent research (<a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a>, <a href="https://arxiv.org/pdf/2203.15556.pdf">Chinchilla</a>) has shown that an efficiently trained GPT model (with Reinforcement Learning from Human Feedback and lots of data) containing 1.3B parameters, equals in performance a GPT-3 model containing 175B parameters.</p>
<p>Thus, recent models that have not applied DeepMind&#39;s scaling laws are under-trained. See Gopher and PaLM below.</p>
<table><thead><tr><th>Model</th><th>Parameters (in billions)</th><th>Tokens (in billions)</th><th>Loss (lower is better)</th></tr></thead><tbody><tr><td>Gopher (DeepMind)</td><td>280</td><td>300</td><td>1.993</td></tr><tr><td>Chinchilla (DeepMind)</td><td>70</td><td>1400</td><td>1.936</td></tr><tr><td>PaLM (Google Brain)</td><td>540</td><td>780</td><td>1.924</td></tr></tbody></table>
<p>According to Approach 2 of the Chinchilla paper, Google would have had to train PaLM with about 14 trillion tokens to obtain the optimal loss for a 540B parameters model.</p>
<p>According to the paper, the LM loss takes the following form:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><munder><munder><mfrac><mn>406.4</mn><msup><mi>N</mi><mn>0.34</mn></msup></mfrac><mo stretchy="true">⏟</mo></munder><mtext>finite model</mtext></munder><mo>+</mo><munder><munder><mfrac><mn>410.7</mn><msup><mi>D</mi><mn>0.28</mn></msup></mfrac><mo stretchy="true">⏟</mo></munder><mtext>finite data</mtext></munder><mo>+</mo><munder><munder><mn>1.69</mn><mo stretchy="true">⏟</mo></munder><mtext>irreductible</mtext></munder></mrow><annotation encoding="application/x-tex">L(N, D) = \underbrace{\frac{406.4}{N^{0.34}}}_{\text{finite model}} + \underbrace{\frac{410.7}{D^{0.28}}}_{\text{finite data}} + \underbrace{1.69}_{\text{irreductible}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span><span>(</span><span>N</span><span>,</span><span></span><span>D</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>finite model</span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span><span><span><span><span><span></span><span><span><span>0.34</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>406.4</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>finite data</span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span>D</span><span><span><span><span><span><span></span><span><span><span>0.28</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>410.7</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>irreductible</span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span>1.69</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></p>
<p>with <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span> the number of parameters (in billions) and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span> the number of tokens in the training dataset (in billions). The irreducible error captures the loss for an ideal generative process and corresponds to the entropy of natural language.</p>
<h3>Model training</h3>
<p>Following the publication of OpenAI&#39;s Scaling Laws, AI research labs have tended to train models with a larger number of parameters, which has led to major advances in the field of <a href="https://openai.com/research/techniques-for-training-large-neural-networks">training large neural networks</a> (pipeline parallelism, tensor parallelism, ...).</p>
<p>However, the arrival of Chinchilla poses a major problem because researchers are now faced with other training challenges. Indeed, the increase of training data requires more optimization steps (which cannot be easily parallelized) as well as the increase of the batch size (which however degrades the performance of the model from a certain point).</p>
<p>The question is therefore: <strong>How to increase the data size while keeping a good training efficiency?</strong></p>
<h3>Training data</h3>
<p>The collection of data on which these models are trained is unfortunately not the object of much interest in the papers.</p>
<p>Many of these models have been trained on datasets such as the Common Crawl or MassiveText. However, these raw data are not of great interest for training the models. <strong>A filtering process is necessary to ensure a high quality in the training data and to remove as much bias as possible.</strong></p>
<p>However, very little information is provided on these datasets. One can read that a large part of the data comes from the web, but with what method has it been scraped? filtered? can more data be obtained by extending the scope of the scrape?</p>
<h2>Emergent properties</h2>
<p>An amazing property of LLMs is the emergence of new capabilities as the size of the network increases. In other words, LLMs quickly learn to perform new tasks, without having been specifically trained to do so, and do so quite unpredictably.</p>
<blockquote>
<p>To illustrate this, consider a physical system such as water. Above 0°C, water obeys the laws of liquid physics. However, below 0°C, the physics of solids governs the system. There is an abrupt &#34;regime shift&#34; from the temperature of 0°C. However, it is not clear at this stage of the research what this &#34;regime shift&#34; means in practice in the case of LLMs.</p>
</blockquote>
<p>The open question is therefore: <strong>Can we expect the solution of more complex problems to be accessible only at very large model scales? Would improvements in the learning algorithms allow for more efficient model training?</strong></p>
<p>However,</p>
<ul>
<li>We do not know at what &#34;scale&#34; these emergent properties appear</li>
<li>We lack knowledge about the ability of these models to deal with emergent properties</li>
</ul>
<p>However, one problem persists: the amount of data available. Suppose we want to train a large model with 100 trillion parameters to study its emergent properties. According to the Chinchilla paper, this would mean training this model on a database of... 180 petabytes of text. We are simply running out of data since the entire Common Crawl dataset is &#34;only&#34; 12 petabytes at the time of writing...</p>
<h3>Model alignment</h3>
<p>Model alignment is a very important field of research today. An ideal model must have two essential qualities:</p>
<ul>
<li>Reliability, or being able to trust the answers produced by the model (which is not the case today due to the hallucinations of LLMs)</li>
<li>Controllability, or being able to control the model</li>
</ul>
<blockquote>
<p>The Californian start-up Anthropic has discovered empirically that the capability for moral self-correction emerges at around 22B parameters. For more scientific details, the paper is <a href="https://arxiv.org/pdf/2302.07459.pdf">here</a>.</p>
</blockquote>
<h2>A silicon brain?</h2>
<p>Questions remain about the similarity between the human brain and current neural networks.</p>
<h3>&#34;Forward-forward&#34; algorithm</h3>
<p>Many neuroscientists are convinced that the brain cannot implement backpropagation (the algorithm used to train neural networks), for the simple reason that the electrical signal produced by neurons propagates in only one direction, whereas backpropagation requires a &#34;forward&#34; pass to calculate the loss and a &#34;backward&#34; pass to modify the parameters in order to reduce the loss. The idea would be to create a &#34;forward-forward&#34; algorithm that approximates the correct properties of backpropagation, and that would relate to our biological model.</p>
<h3>ARHGAP11A gene</h3>
<p>In our brain, the number of neurons seems to play a very important role. It is the mutation of the ARHGAP11A gene 5 million years ago, which allowed a drastic increase in the number of neural stem cells in the neocortex (between 3 and 6 times more) and thus the development of new cognitive faculties such as reasoning or language.</p>
<p>Does the number of neurons play a more important role than data in a biological brain?</p>
<h3>Artificial sleep</h3>
<p>Sleep and its different phases (slow, deep, REM) plays a fundamental role in the functioning of any biological brain. However, the role that sleep plays on our intelligence is still to be discovered. Some hypotheses suggest a remodelling of the synapses leading to untimely tests of connections, to simulate problems and better anticipate them. No current artificial neural network offers a &#34;sleep state&#34; that would allow the knowledge acquired during training to be consolidated. Could an &#34;artificial&#34; sleep help the emergence of creativity in these deep neural networks?</p>
<h3>Reasoning ability</h3>
<p>To date, no neural network shows a capacity for reasoning and creativity worthy of an animal level.
Current LLMs are autoregressive models that predict the probability distribution of the next <em>token</em> given the set of previous <em>tokens</em> called <em>context</em>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">arg max</mi><mo>⁡</mo><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\argmax{P(w_{t+1} | w_{1}, ..., w_{t})}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>arg</span><span></span><span>max</span></span><span></span><span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span></p>
<p>with <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{t + 1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> the <em>token</em> to predict and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_{1}, ..., w_{t}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> the <em>context</em>.</p>
<p><img src="https://files.redbird.dev/assets/blog/llm-2.jpg" alt=""/></p>
<p>According to Ilya Sutskever, co-founder and Chief Scientist of OpenAI, these autoregressive models have the potential to achieve human-level intelligence, as the statistics, beyond the numbers, reveal an understanding of the underlying concepts.</p>
<p>Could finding a link between next word prediction accuracy and reasoning abilities be the way to bring the current autoregressive models to truly intelligent models?</p>
<h2>References</h2>
<ul>
<li><a href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">chinchilla&#39;s wild implications</a></li>
<li><a href="https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models">Emergent Abilities of Large Language Models</a></li>
<li><a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a></li>
<li><a href="https://arxiv.org/pdf/2203.15556.pdf">Training Compute-Optimal Large Language Models</a></li>
<li><a href="https://youtu.be/Yf1o0TQzry8">Interview with Ilya Sutskever</a></li>
</ul></div></div>
  </body>
</html>
