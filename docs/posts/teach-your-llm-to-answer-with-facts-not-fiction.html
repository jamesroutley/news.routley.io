<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.myscale.com/2023/07/17/teach-your-llm-vector-sql/">Original</a>
    <h1>Teach your LLM to answer with facts, not fiction</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody"><p><a href="https://huggingface.co/spaces/myscale/ChatData"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange"/></a><a href="https://github.com/myscale/chatdata"><img src="https://img.shields.io/badge/Open-Github-blue.svg?logo=github&amp;style=plastic)](https://github.com/myscale/chatdata)\"/></a></p> <p>Large Language Models are advanced AI systems that can answer a wide range of questions. Although they provide informative responses on topics they know, they are not always accurate on unfamiliar topics. This phenomenon is known as <strong>hallucination</strong>.</p> <h2 id="what-is-hallucination"><a href="#what-is-hallucination">#</a> What is Hallucination?</h2> <p>Before we look at an example of an LLM hallucination, let&#39;s consider a definition of the term &#34;hallucination&#34; as described by <a href="https://en.wikipedia.org/wiki/Hallucination" target="_blank" rel="noopener noreferrer">Wikipedia.com<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>:</p> <blockquote><p>&#34;A hallucination is a perception in the absence of an external stimulus that has the qualities of a real perception.&#34;</p></blockquote> <p>Moreover:</p> <blockquote><p>&#34;Hallucinations are vivid, substantial, and are perceived to be located in external objective space.&#34;</p></blockquote> <p>In other words, a hallucination is an error in (or a false) perception of something real or concrete. For example, ChatGPT (a famous Large Language Model by OpenAI) was asked what LLM hallucinations are, with the answer being:</p> <p><img src="https://blog.myscale.com/assets/img/hallucination.0d021d06.png" alt="LLM Hallucinations. source: aruna-x"/></p> <p>Therefore, the question begs, how do we improve on (or fix) this result? The concise answer is to add facts to your question, such as providing the LLM definition before or after you ask the question.</p> <p>For instance:</p> <blockquote><p>An LLM is a Large Language Model, an artificial neural network that models how humans talk and write. Please tell me, what is LLM hallucination?</p></blockquote> <p>The public domain answer to this question, provided by ChatGPT, is:</p> <p><img src="https://blog.myscale.com/assets/img/chatgpt-hallucination-response.435cec3e.png" alt="ChatGPT LLM Hallucinations Response"/></p> <p><strong>Note:</strong> The reason for the first sentence, &#34;Apologies for the confusion in my earlier response,&#34; is that we asked ChatGPT our first question, what LLM hallucinations are, before giving it our second prompt: &#34;An LLM...&#34;</p> <p>These additions have improved the quality of the answer. At least it no longer thinks an LLM hallucination is a &#34;Late-Life Migraine Accompaniment!&#34; ðŸ˜†</p> <h2 id="external-knowledge-reduces-hallucinations"><a href="#external-knowledge-reduces-hallucinations">#</a> External Knowledge Reduces Hallucinations</h2> <p>At this juncture, it is absolutely crucial to note that an LLM is not infallible nor the ultimate authority on all knowledge. LLMs are trained on large amounts of data and learn patterns in language, but they may not always have access to the most up-to-date information or have a comprehensive understanding of complex topics.</p> <p>What now? How do you increase the chance of reducing LLM hallucinations?</p> <p>The solution to this problem is to include supporting documents to the query (or prompt) to guide the LLM toward a more accurate and informed response. Like humans, it needs to learn from these documents to answer your question accurately and correctly.</p> <p>Helpful documents can come from many sources, including a search engine like Google or Bing and a digital library like Arxiv, among others, providing an interface to search for relevant passages. Using a database is also a good choice, providing a more flexible and private query interface.</p> <p>Knowledge retrieved from sources must be relevant to the question/prompt. There are several ways to retrieve relevant documents, including:</p> <ul><li><strong>Keyword-based:</strong> Searching for keywords in plain text, suitable for an exact match on terms.</li> <li><strong>Vector search-based:</strong> Searching for records closer to embeddings, helpful in searching for appropriate paraphrases or general documents.</li></ul> <p>Nowadays, vector searches are popular since they can solve paraphrase problems and calculate paragraph meanings. Vector search is not a one-size-fits-all solution; it should be paired with specific filters to maintain its performance, especially when searching massive volumes of records. For example, should you only want to retrieve knowledge about physics (as a subject), you must filter out all information about any other subjects. Thus, the LLM will not be confused by knowledge from other disciplines.</p> <h2 id="automate-the-whole-process-with-sql-and-vector-search"><a href="#automate-the-whole-process-with-sql-and-vector-search">#</a> Automate the Whole Process with SQL... and Vector Search</h2> <p>The LLM should also learn to query data from its data sources before answering the questions, automating the whole process. Actually, LLMs are already capable of writing SQL queries and following instructions.</p> <p><img src="https://blog.myscale.com/assets/img/pipeline.27770dfe.png" alt="Vector Pipeline"/></p> <p>SQL is powerful and can be used to construct complex search queries. It supports many different data types and functions. And it allows us to write a vector search in SQL with <code>ORDER BY</code> and <code>LIMIT</code>, treating the similarity score between embeddings as a column <code>distance</code>. Pretty straightforward, isn&#39;t it?</p> <blockquote><p>See the next section, <a href="#what-vector-sql-looks-like">What Vector SQL Looks Like</a>, for more information on structuring a vector SQL query.</p></blockquote> <p>There are significant benefits to using vector SQL to build complex search queries, including:</p> <ul><li>Increased flexibility for data type and function support</li> <li>Improved efficiency because SQL is highly optimized and executed inside the database</li> <li>Is human-readable and easy to learn as it is an extension of standard SQL</li> <li>Is LLM-friendly</li></ul> <p><strong>Note:</strong> Many SQL examples and tutorials are available on the Internet. LLMs are familiar with standard SQL as well as some of its dialects.</p> <p>Apart from MyScale, many SQL database solutions like Clickhouse and PostgreSQL are adding vector search to their existing functionality, allowing users to use vector SQL and LLMs to answer questions on complex topics. Similarly, an increasing number of application developers are starting to integrate vector searches with SQL into their applications.</p> <h2 id="what-vector-sql-looks-like"><a href="#what-vector-sql-looks-like">#</a> What Vector SQL Looks Like</h2> <p>Vector Structured Query Language (Vector SQL) is designed to teach LLMs how to query vector SQL databases and contains the following extra functions:</p> <ul><li><code>DISTANCE(column, query_vector)</code>: This function compares the distance between the column of vectors and the query vector either exactly or approximately.</li> <li><code>NeuralArray(entity)</code>: This function converts an entity (for example, an image or a piece of text) into an embedding.</li></ul> <p>With these two functions, we can extend the standard SQL for vector search. For example, if you want to search for 10 relevant records to word <code>flower</code>, you can use the following SQL statement:</p> <p>The <code>DISTANCE</code> function comprises the following:</p> <ul><li>The inner function, <code>NeuralArray(flower)</code>, converts the word <code>flower</code> into an embedding.</li> <li>This embedding is then serialized and injected into the <code>DISTANCE</code> function.</li></ul> <p>Vector SQL is an extended version of SQL that needs further translation based on the vector database used. For instance, many implementations have different names for the <code>DISTANCE</code> function. It is called <code>distance</code> in MyScale, and <code>L2Distance</code> or <code>CosineDistance</code> in Clickhouse. Additionally, based on the database, this function name will be translated differently.</p> <h2 id="how-to-teach-an-llm-to-write-vector-sql"><a href="#how-to-teach-an-llm-to-write-vector-sql">#</a> How to teach an LLM to write Vector SQL</h2> <p>Now that we understand the basic principles of vector SQL and its unique functions, let&#39;s use an LLM to help us to write a vector SQL query.</p> <h3 id="_1-teach-an-llm-what-standard-vector-sql-is"><a href="#_1-teach-an-llm-what-standard-vector-sql-is">#</a> 1. Teach an LLM What Standard Vector SQL is</h3> <p>First, we need to teach our LLM what standard vector SQL is. We aim to ensure that the LLM will do the following three things spontaneously when writing a vector SQL query:</p> <ul><li>Extract the keywords from our question/prompt. It could be an object, a concept, or a topic.</li> <li>Decide which column to use to perform the similarity search. It should always choose a vector column for similarity.</li> <li>Translate the rest of our question&#39;s constraints into valid SQL.</li></ul> <h3 id="_2-design-the-llm-prompt"><a href="#_2-design-the-llm-prompt">#</a> 2. Design the LLM Prompt</h3> <p>Having determined exactly what information the LLM requires to construct a vector SQL query, we can design the prompt as follows:</p> <p>This prompt should do its job. But the more examples you add, the better it will be, like using the following vector SQL-to-text pair as a prompt:</p> <p><strong>The SQL table create statement:</strong></p> <p><strong>The question and answer:</strong></p> <p>The more relevant examples you add to your prompt, the more the LLM&#39;s process of building the correct vector SQL query will improve.</p> <p>Lastly, here are several extra tips to help you when designing your prompt:</p> <ul><li>Cover all possible functions that might appear in any questions asked.</li> <li>Avoid monotonic questions.</li> <li>Alter the table schema, like adding/removing /modifying names and data types.</li> <li>Align the prompt&#39;s format.</li></ul> <h2 id="a-real-world-example-using-myscale"><a href="#a-real-world-example-using-myscale">#</a> A Real-World Example: Using MyScale</h2> <p>Let&#39;s now build <a href="https://huggingface.co/spaces/myscale/ChatData" target="_blank" rel="noopener noreferrer"><strong>a real-world example</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>, set out in the following steps:</p> <p><img src="https://blog.myscale.com/assets/img/myscale-example.a661ebc6.png" alt="A Real-World Example: Using MyScale"/></p> <h3 id="prepare-the-database"><a href="#prepare-the-database">#</a> Prepare the Database</h3> <p>We have prepared a playground for you with more than 2 million papers ready to query. You can access this data by adding the following Python code to your app.</p> <p>If you like, you can skip the following steps, where we create the table and insert its data using the MyScale console, and jump to where we play with vector SQL and <a href="#create-sqldatabasechain">create the <code>SQLDatabaseChain</code></a> to query the database.</p> <p><strong>Create the database table:</strong></p> <p><strong>Insert the data:</strong></p> <h3 id="create-the-sqldatabasechain"><a href="#create-the-sqldatabasechain">#</a> Create the <code>SQLDatabaseChain</code></h3> <p>This LangChain feature is currently under <a href="https://github.com/myscale/langchain/tree/preview" target="_blank" rel="noopener noreferrer">MyScale tech preview<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>. You can install it by executing the following installation script:</p> <p>Once you have installed this feature, the next step is to use it to query the database, as the following Python code demonstrates:</p> <h3 id="ask-with-retrievalqawithsourceschain"><a href="#ask-with-retrievalqawithsourceschain">#</a> Ask with <code>RetrievalQAwithSourcesChain</code></h3> <p>You can also use this SQLDatabaseChain as a Retriever. You can plugin it in to some retrieval QA chains just like other retievers in LangChain.</p> <p>We also provide a live demo on <a href="https://huggingface.co/spaces/myscale/ChatData" target="_blank" rel="noopener noreferrer"><strong>huggingface</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> and the code is available on <a href="https://github.com/myscale/ChatData" target="_blank" rel="noopener noreferrer"><strong>GitHub</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>! We used <a href="https://github.com/myscale/ChatData/blob/main/chains/arxiv_chains.py" target="_blank" rel="noopener noreferrer"><strong>a customized Retrieval QA chain</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> to maximize the performance our search and ask pipeline with LangChain!</p> <h2 id="in-conclusion"><a href="#in-conclusion">#</a> In Conclusion</h2> <p>In reality, most LLMs hallucinate. The most practical way to reduce its appearance is to add extra facts (external knowledge) to your question. External knowledge is crucial to improving the performance of LLM systems, allowing for the efficient and accurate retrieval of answers. Every word counts, and you don&#39;t want to waste your money on unused information that is retrieved by inaccurate queries.</p> <p>How?</p> <p>Enter Vector SQL, allowing you to execute finely-grained vector searches to target and retrieve the required information.</p> <p>Vector SQL is powerful and easy to learn for humans and machines. You can use many data types and functions to create complex queries. LLMs also like vector SQL, as its training dataset includes many references.</p> <p>Lastly, it is possible to translate Vector SQL into many vector databases using different embedding models. We believe that is the future of vector databases.</p> <p>Are interested in what we are doing? Join us on <a href="https://discord.gg/D2qpkqc4Jq" target="_blank" rel="noopener noreferrer">discord<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> today!</p></div></div>
  </body>
</html>
