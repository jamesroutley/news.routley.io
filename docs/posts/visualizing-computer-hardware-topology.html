<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hypodyne.net/visualizing-computer-hardware-topology/">Original</a>
    <h1>Visualizing Computer Hardware Topology</h1>
    
    <div id="readability-page-1" class="page"><div><p>There are times when you need a clear understanding of how a computer&#39;s hardware goes together. In my case, I&#39;m working on a project using <a href="https://en.wikipedia.org/wiki/Express_Data_Path" rel="noopener" target="_blank">XDP</a> to rapidly process packets in the kernel, bypassing the Linux networking stack.</p><p>For my purposes in testing and development, it&#39;s important to know which network cards are connected to which CPUs. For best performance, packet data should not cross the interconnect from one <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" rel="noopener" target="_blank">NUMA node</a> to another, but should instead be handled all within the same NUMA node.</p><p>In many cases <code>lshw</code> does the needful, but its output is rather verbose and one can lose sight of the forest for the trees:</p><pre><code><span>char@underbed:~$ sudo lshw | head -n 40
</span><span>underbed
</span><span>    description: Rack Mount Chassis
</span><span>    product: PowerEdge R630 (SKU=NotProvided;ModelName=PowerEdge R630)
</span><span>    vendor: Dell Inc.
</span><span>    serial: 5QP0B42
</span><span>    width: 64 bits
</span><span>    capabilities: smbios-2.8 dmi-2.8 smp vsyscall32
</span><span>    configuration: boot=normal chassis=rackmount sku=SKU=NotProvided;ModelName=PowerEdge R630 uuid=4c4c4544-0051-5010-8030-b5c04f423432
</span><span>  *-core
</span><span>       description: Motherboard
</span><span>       product: 0CNCJW
</span><span>       vendor: Dell Inc.
</span><span>       physical id: 0
</span><span>       version: A05
</span><span>       serial: .5QP0B42.CN7475151G0196.
</span><span>     *-firmware
</span><span>          description: BIOS
</span><span>          vendor: Dell Inc.
</span><span>          physical id: 0
</span><span>          version: 2.19.0
</span><span>          date: 12/12/2023
</span><span>          size: 64KiB
</span><span>          capacity: 16MiB
</span><span>          capabilities: isa pci pnp upgrade shadowing cdboot bootselect edd int13floppytoshiba int13floppy360 int13floppy1200 int13floppy720 int9keyboard int14serial int10video acpi usb biosbootspecification netboot uefi
</span><span>     *-cpu:0
</span><span>          description: CPU
</span><span>          product: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
</span><span>          vendor: Intel Corp.
</span><span>          physical id: 400
</span><span>          bus info: cpu@0
</span><span>          version: 6.63.2
</span><span>          slot: CPU1
</span><span>          size: 1974MHz
</span><span>          capacity: 4GHz
</span><span>          width: 64 bits
</span><span>          clock: 3705MHz
</span><span>          capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp x86-64 constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts vnmi md_clear flush_l1d ibpb_exit_to_user cpufreq
</span><span>          configuration: cores=8 enabledcores=8 microcode=73 threads=16
</span><span>        *-cache:0
</span><span>             description: L1 cache
</span><span>...
</span></code></pre><p>There&#39;s another useful tool called <code>lstopo</code> that is part of the <code>hwloc</code> package in many Linux distributions. <code>lstopo</code> creates a visualization of the system&#39;s layout that filters out less important information and even elides repeating objects to reveal that forest which <code>lshw</code> loses sight of. Here&#39;s an example output from the same system as seen above:</p><p><img alt="lstopo output visualization" src="https://hypodyne.net/visualizing-computer-hardware-topology/./topo.svg"/></p><p>Now one can clearly see what PCIe hardware is homed to which NUMA node. Node 0 on the left has <code>sda</code>, my disk drive along with 2 NICs. PCI 03:00 is a 2-port Intel 82599 10GbE NIC that I got for $15 from eBay and PCI 01:00 is a 4-port NIC that came with the server. On Node 1, there&#39;s another 2-port 82599 on PCI 81:00.</p><p>Along with the peripherals, you can also clearly see each NUMA node has shared RAM, an L3 cache common to all cores, and individual L2 and L1 caches per core.</p><p>Pretty neat!</p></div></div>
  </body>
</html>
