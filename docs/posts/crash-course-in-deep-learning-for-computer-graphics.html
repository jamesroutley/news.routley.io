<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gpuopen.com/learn/deep_learning_crash_course/">Original</a>
    <h1>Crash course in deep learning for computer graphics</h1>
    
    <div id="readability-page-1" class="page"><div data-id="3cfd864" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			<div role="main">
<section id="crash-course-in-deep-learning-for-computer-graphics">
<section id="introduction">
<h2 id="1.-introduction">1. Introduction<a href="#introduction" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>As I recently went through a journey of learning how to make use of deep learning (in the context of computer graphics), I thought it would be good to write down some notes to help others get up to speed quickly. The goal of this article is to make the reader familiar with terms and concepts used in deep learning and to implement a basic deep learning algorithm. This should make it easier to study and understand other deep learning resources. This article comes with the source code and a sample application written in HLSL/DirectX 12 available at <a href="https://github.com/boksajak/Dx12NN" target="_blank" rel="noopener">https://github.com/boksajak/Dx12NN</a>.</p>
</section>
<section id="neural-network">
<h2 id="2.-neural-network">2. Neural Network<a href="#neural-network" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>Deep learning algorithms are powered by artificial neural networks. These are collections of interconnected neurons (also called <strong>units</strong>), usually organized into <strong>layers</strong>. When a neural network has large number of layers, we say that the network is deep, giving us a name <strong>deep learning</strong>. Each neuron has its inputs connected to other neurons via weighted connections, performs a simple computation over them, and passes its output to other neurons.</p>
<p>The type of neural network that we’re going to use in this article is called <strong>multilayer perceptron (MLP)</strong>. It is relatively simple, but also powerful, and is widely used in practice (e.g., for neural radiance cache (NRC) and neural radiance fields (NeRFs)).</p>
<p>The MLP is constructed as follows:</p>
<ul>
<li>
<p>Neurons are organized into layers, where first layer is called <strong>input layer</strong>, last layer is <strong>output layer</strong> and between them are <strong>hidden layers.</strong></p>
</li>
<li>
<p>Each neuron has its inputs connected to outputs of all neurons in the previous layer. Therefore, we say that MLPs are <strong>fully connected networks</strong>, see the Figure 1 for an example.</p>
</li>
<li>
<p>This network architecture forms a directed acyclic graph, meaning that information only flows in one way, starting in the input layer and propagating through hidden layers to the output layer. We say that such network is a <strong>feedforward network</strong>.</p>
<ul>
<li>
<p>Note: Other network types, where information can also flow backwards (via <strong>feedback connections</strong>) are called <strong>recurrent networks</strong>. Some networks also have memory cells to store values from previous evaluations of the network.</p>
</li>
</ul>
</li>
</ul>
<p>Number of neurons and layers in the network determines how powerful the network is. Larger networks have the ability to learn more complex functions (we say that they have a larger <strong>capacity</strong>) but are typically more difficult to train and slower to evaluate. When designing a deep learning algorithm, we need to find a network size which is just right for the task. E.g., the NeRF paper uses MLP with 9 hidden layers consisting of 256 neurons each, and NRC uses 5 hidden layers with 64 neurons.</p>
<p>Note that these networks are small enough that they can be implemented in compute shaders and executed per pixel. In practice, some clever optimizations are necessary but it’s not impossible to have a deep learning algorithm running in real-time on current GPUs. It is also common to run neural networks using types with reduced precision, like FP16 or even smaller.</p>
<p><img width="1166" height="695" decoding="async" alt="invert" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20width=&#39;1166&#39;%20height=&#39;695&#39;%20viewBox=&#39;0%200%201166%20695&#39;%3E%3C/svg%3E" data-src="/docs_images/deep_learning_crash_course/deep_learning_crash_course-html-_images-Multi-layer_perceptron_neural_network.jpg"/></p>
<p><em>Figure 1 Architecture of a multi-layer perceptron neural network with 2 neurons in the input layer, 3 neurons per hidden layer and 1 neuron in the output layer.</em></p>
<section id="the-neuron">
<h3 id="2.1-the-neuron">2.1 The Neuron<a href="#the-neuron" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>All the computation that neural network does happens in the neurons. Output value of the neuron (also called its <strong>activation</strong>) is calculated as follows:</p>
<ol>
<li>
<p>First, we sum up activations of all neurons from the previous layer connected to this neuron, weighted by the strength of the connection (this strength is also called the <strong>weight</strong>)</p>
</li>
<li>
<p>We add <strong>bias</strong> of the evaluated neuron to the sum. Bias is a per-neuron value which helps to represent non-linear relationships between network inputs and outputs. Without the bias, output of the network for zero input could only be a zero.</p>
</li>
<li>
<p>We apply the <strong>activation function</strong> to this sum to produce final neuron activation. This must be a non-linear function, which introduces another non-linearity between inputs and outputs. Usually, we use a simple function like (max(0,x)). Without the activation function, outputs of the network could only be a linear combination of the inputs.</p>
</li>
</ol>
<p>More formally, we can write neuron evaluation as:</p>
<p><span data-katex-display="true">O_{i} = \sigma\left( \left( \sum_{j}^{M}{O_{j}*w_{ij}} \right) + b_{i} \right)</span></p>
<p>where <span data-katex-display="false">O_{i}</span> is the activation of the evaluated neuron, <span data-katex-display="false">\sigma</span> is the activation function, <span data-katex-display="false">M</span> is the set of input neurons connected to the evaluated neuron, <span data-katex-display="false">O_{j}</span> is the activation of the input neuron <span data-katex-display="false">j</span> from previous layer, <span data-katex-display="false">w_{ij}</span> is the weight of the connection between neurons <span data-katex-display="false">i</span> and <span data-katex-display="false">j</span> and <span data-katex-display="false">b_{i}</span> is the bias value of the evaluated neuron.</p>
<p>This means, that the output of the network for specific input is given by <strong>weights and biases</strong> and we need to set them accordingly to produce desired results. The process of adjusting weights and biases to make the network do what we want is called <strong>training</strong>.</p>
</section>
<section id="how-to-use-an-mlp">
<h3 id="2.2-how-to-use-an-mlp">2.2 How To Use an MLP<a href="#how-to-use-an-mlp" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>Neural networks based on MLPs are usually used for one of two main tasks: classification and regression:</p>
<ul>
<li>
<p><strong>Classification:</strong> Categorizes the input into one (or more) predefined categories. These neural networks have one output neuron for each category and assign a probability of input belonging to each category as its output. E.g., we can have an input image of a hand-written digit and train the MLP to assign probability of the digit belonging into categories representing digits from 0 to 9. This exercise is a common “hello world” example in deep learning and there is a freely available set of images of hand-written digits called <a href="https://git-disl.github.io/GTDLBench/datasets/mnist_datasets/" target="_blank" rel="noopener">MNIST</a> that can be used for it.</p>
</li>
<li>
<p><strong>Regression:</strong> For given input, the regression calculates a continuous numerical value on the output (also called a <strong>prediction</strong>). The goal is to train the network to perform a desired mapping between inputs and output values. E.g., the NRC algorithm trains the network to map inputs like surface position and normal to radiance values.</p>
</li>
</ul>
<p>In our sample application we will train the network to do regression, specifically it will learn to represent a 2D texture. We will provide UV coordinates of the texture as inputs, and we’ll expect RGB value of the corresponding texel on the output. We want it to learn the mapping:</p>
<p><span data-katex-display="true">\left( u,\ v \right) \rightarrow (R,\ G,\ B)</span></p>
<p>For this example, our network will have 2 neurons in the input layer, corresponding to the <span data-katex-display="false">u</span> and <span data-katex-display="false">v</span> coordinates in the texture. On the output, we will have 3 neurons corresponding to the RGB values of the texel. In practice, it is common to <strong>encode the input</strong> into different representation. A naïve encoding which simply assigns inputs to input neurons as they are is called <strong>identity,</strong> and while it works, some clever encoding schemes usually perform much better. We’ll talk more about input encodings in section 4. Note that the input layer doesn’t perform any computation – instead of calculating activation of the neurons in the input layer, we simply assign input values as their activations.</p>
<p>Before we can use the MLP, it must be trained to perform our desired mapping well. Training is relatively complex and will be described in section 3, so let’s now assume that we have trained the network, obtained the correct weights and biases and we want to calculate the network output (prediction) for given input – this process is also called <strong>inference</strong> and because the information flows forward through the network, it’s sometimes also called the <strong>forward pass</strong>.</p>
</section>
<section id="inference-implementation">
<h3 id="2.3-inference-implementation">2.3 Inference Implementation<a href="#inference-implementation" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>With the architecture of the MLP in mind, let’s now implement the inference. Before we start, it is useful to make it clear how we index data of the neural network in the arrays of weights, biases, activations etc. We need to store activations and biases <em>per neuron</em>, and the weights <em>per connection</em> between neurons. It is easy to make a mistake in indexing when working with graphs (our neural network is a graph), so let’s define a few rules for our indexing scheme:</p>
<ul>
<li>
<p>We will index layers starting from 0: input layer has index 0, and output layer has index <code><span>LAYER_COUNT</span> <span>-</span> <span>1</span></code>. Because the input layer doesn’t do any computation, it might be possible to skip it and start indexing from the first hidden layer, but we want to keep things clear and simple.</p>
</li>
<li>
<p>Connections between neurons will “belong” to the layer they are leading to. This means that layer 0 (input layer) doesn’t have any connections, and layer index <code><span>LAYER_COUNT</span> <span>-</span> <span>1</span></code> has connections from last hidden layer to the output layer.</p>
</li>
<li>
<p>Neurons in each layer are indexed from 0 to <code><span>NUM_NEURONS_PER_LAYER</span></code></p>
</li>
</ul>
<p>With this in mind, let’s define some helper functions to access data in global arrays:</p>
<div>
<div>

<pre><code><span>uint</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>uint</span><span> </span><span>layer</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuron</span><span>)</span>
<span>{</span>
<span>  </span><span>return</span><span> </span><span>neuronDataBaseOffsets</span><span>[</span><span>layer</span><span>]</span><span> </span><span>+</span><span> </span><span>neuron</span><span>;</span>
<span>}</span>

<span>uint</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>uint</span><span> </span><span>layer</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuronFrom</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuronTo</span><span>)</span>
<span>{</span>
<span>  </span><span>return</span><span> </span><span>connectionDataBaseOffsets</span><span>[</span><span>layer</span><span>]</span><span> </span><span>+</span><span> </span><span>(</span><span>neuronTo</span><span> </span><span>*</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>])</span><span> </span><span>+</span><span> </span><span>neuronFrom</span><span>;</span>
<span>}</span>
</code></pre>
</div>
</div>
<p>Base offsets used in these functions are pre-calculated for each layer based on number of layers and number of neurons per layer in the network. Details can be found in the source code accompanying this article in the function createComputePasses.</p>
<p>With these in place, we can now implement a forward pass which:</p>
<ol>
<li>
<p>Encodes input into <em>activations</em> array.</p>
</li>
<li>
<p>Iterates through all the layers where computation happens (all except the input layer).</p>
<p>a.  Evaluates activation for each neuron in the layer.</p>
</li>
<li>
<p>Reads output from the activations array and returns it.</p>
</li>
</ol>
<div>
<div>

<pre><code><span>float</span><span> </span><span>activations</span><span>[</span><span>LAYER_COUNT</span><span> </span><span>*</span><span> </span><span>MAX_NEURONS_PER_LAYER</span><span>];</span>

<span>// Identity encoding passes input as it is</span>
<span>activations</span><span>[</span><span>0</span><span>]</span><span> </span><span>=</span><span> </span><span>input</span><span>.</span><span>x</span><span>;</span>
<span>activations</span><span>[</span><span>1</span><span>]</span><span> </span><span>=</span><span> </span><span>input</span><span>.</span><span>y</span><span>;</span>

<span>// Calculate activations for every layer, going forward through the MLP network</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>layer</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span> </span><span>layer</span><span> </span><span>&lt;</span><span> </span><span>LAYER_COUNT</span><span>;</span><span> </span><span>layer</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronCountCurrentLayer</span><span> </span><span>=</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span>];</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronCountPreviousLayer</span><span> </span><span>=</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>];</span>

<span>  </span><span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>neuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>neuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountCurrentLayer</span><span>;</span><span> </span><span>neuron</span><span>++</span><span>)</span>
<span>  </span><span>{</span>
<span>    </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span>,</span><span> </span><span>neuron</span><span>);</span>

<span>    </span><span>// Evaluate neuron activation</span>
<span>    </span><span>float</span><span> </span><span>neuronValue</span><span> </span><span>=</span><span> </span><span>nnBiases</span><span>[</span><span>neuronDataIndex</span><span>];</span>

<span>    </span><span>// Accumulate weighted contribution from all neurons connected to this neuron in previous layer</span>
<span>    </span><span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>previousNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>previousNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountPreviousLayer</span><span>;</span><span> </span><span>previousNeuron</span><span>++</span><span>)</span>
<span>    </span><span>{</span>
<span>      </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightDataIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>layer</span><span>,</span><span> </span><span>previousNeuron</span><span>,</span><span> </span><span>neuron</span><span>);</span>
<span>      </span><span>const</span><span> </span><span>uint</span><span> </span><span>previousNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>previousNeuron</span><span>);</span>

<span>      </span><span>neuronValue</span><span> </span><span>+=</span><span> </span><span>nnWeights</span><span>[</span><span>weightDataIndex</span><span>]</span><span> </span><span>*</span><span> </span><span>activations</span><span>[</span><span>previousNeuronDataIndex</span><span>];</span>
<span>    </span><span>}</span>

<span>    </span><span>activations</span><span>[</span><span>neuronDataIndex</span><span>]</span><span> </span><span>=</span><span> </span><span>ACTIVATION_FUNCTION</span><span>(</span><span>neuronValue</span><span>);</span>
<span>  </span><span>}</span>
<span>}</span>

<span>const</span><span> </span><span>uint</span><span> </span><span>outputLayerActivationIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>0</span><span>);</span>
<span>const</span><span> </span><span>float3</span><span> </span><span>result</span><span> </span><span>=</span><span> </span><span>float3</span><span>(</span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>0</span><span>],</span>
<span>                </span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>1</span><span>],</span>
<span>                </span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>2</span><span>]);</span>
</code></pre>
</div>
</div>
<p>Note that our code has allocated an array called <strong>activations</strong> where we store activations of all neurons during the forward pass. But for inference we only need to store activations of 2 layers at any time: the one that we are evaluating and the previous layer. As an optimization, we can allocate two smaller arrays with the size <code><span>NUM_MAX_NEURONS_PER_LAYER</span></code> and ping-pong between them. However, during the training we will need to store activations for all neurons from the forward pass to perform a backpropagation pass.</p>
</section>
<section id="activation-functions">
<h3 id="2.4-activation-functions">2.4 Activation Functions<a href="#activation-functions" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>In the previous code listing, we have used the macro called <code><span>ACTIVATION_FUNCTION</span></code> in the place where we want to evaluate the activation function. Let’s now define it – remember that this must be a non-linear function, but we can pick any function assuming that it is differentiable (it has a derivative). The derivative will be needed for training.</p>
<p>Some of the functions commonly used are ReLU (rectified linear unit), leaky ReLU and sigmoid:</p>
<p><span data-katex-display="true">\sigma_{ReLU}(x) = max(0,x)</span></p>
<p><span data-katex-display="true">\sigma_{LeakyReLU}(x) = \left( \begin{matrix} 0.01x, x &lt; 0 \\ x, x \geq 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{Sigmoid} = \frac{1}{1 + e^{-x}}</span></p>
<p>Even though ReLU and leaky ReLU are very simple, they work well and are widely used, e.g., by both NeRF and NRC papers. The 0.01 value (slope) in leaky ReLU implementation is a variable and you can experiment with different values. Usually, the whole network uses the same activation function, but it is not uncommon for output layer to have a different function than hidden layers.</p>
<p>Let’s now look at derivatives of these functions:</p>
<p><span data-katex-display="true">\sigma_{ReLU}^{&#39;} = \left( \begin{matrix} 0,\ &amp; x &lt; 0\  \\ 1,\ &amp; x &gt; 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{LeakyReLU}^{&#39;}\left( x \right) = \left( \begin{matrix} 0.01, x &lt; 0 \\ 1, x &gt; 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{Sigmoid}^{&#39;}\left( x \right) = \sigma_{Sigmoid}\left( x \right)(1 - \ \sigma_{Sigmoid}\left( x \right))</span></p>
<p>Note: the derivative of ReLU and leaky ReLU is undefined at point zero, because the function is discontinuous there, but in practice we must use a value of 0 in that point.</p>
<p>For our sample application, we use leaky ReLU by default. Sigmoid function is also available, but the training takes a much longer time compared to ReLU and leaky ReLU.</p>
</section>
</section>
<section id="training">
<h2 id="3.-training">3. Training<a href="#training" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<section id="how-the-neural-network-learns">
<h3 id="3.1-how-the-neural-network-learns">3.1 How the Neural Network Learns<a href="#how-the-neural-network-learns" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>In this article, we implement neural network training based on a common <strong>supervised learning</strong> method. Supervised learning means that we evaluate the neural network on inputs for which we know correct outputs that we’d like to get, and we adjust it to give us results closer to desired outputs. Each input is called a <strong>sample</strong> or an <strong>example</strong>, and all inputs available for training are called a <strong>training set</strong>.</p>
<p>For each sample from the training set, we start by comparing network’s prediction to the correct output using a <strong>cost function</strong>. This function tells us how good the prediction was (lower is better). Then, we will update weights and biases in a way that minimizes this cost function. This is done using an algorithm called <strong>gradient descent</strong>. The gist of it is that we first calculate how the cost function changes if we change individual weights and biases, and then we adjust them in a direction that lowers the cost. In the following text, we will often refer to “weights and biases”, but I’m only going to write “weights” to keep the text more readable. Keep in mind that what applies to weights also applies to biases here.</p>
<p>To know how to adjust the weights to lower the cost we need to calculate a value for each weight that tells us how the cost function changes when that specific weight changes. These are called <strong>partial derivatives</strong> of a cost function wrt. the weight. The vector of all partial derivatives of our weights is called a <strong>gradient</strong>.</p>
<p>Now we can imagine the cost function as a multi-dimensional surface. The gradient points in a direction where the cost function rises most steeply. We can think about stochastic descent as a ball rolling down the hill on this surface (position of the ball is given by the current network state and the cost function value). We want to get the ball as low as possible to minimize the cost. By calculating gradients (which point up the hill on this surface), and moving in opposite direction, we basically roll the ball downhill to some local minimum. Gradient is calculated using the algorithm called <strong>backpropagation</strong> which we’ll discuss in section 3.5. The process of adjusting the weights is also called an <strong>optimization</strong> of the network.</p>
<p>Once we have calculated the gradient, we can simply nudge the weights in an opposite direction by some small amount. This method of minimizing the cost function will eventually get us to a local minimum of the cost function. Note that this method doesn’t find a <em>global</em> minimum (unless it gets very lucky), but in practice this is not a huge issue for highly dimensional problems which typically have many local minima, with values similar to the global minimum.</p>
<p>We repeat this process many times to iteratively lower the cost. In each step, weights are not adjusted by the whole magnitude of the gradient, but we first multiply it by a small number (e.g., 0.0001) to take smaller steps. This multiplication constant is called a <strong>learning rate,</strong> and it tells us how fast the gradient descent should progress along the path to the local minimum. Without the learning rate, gradient descent would be taking very large steps, while being unstable and oscillating around the local minimum. Picking a right learning rate is critical in order to ensure that training will be stable, but also sufficiently fast. In practice, we will often have an <strong>adaptive learning rate.</strong> We will start with highest learning rate, and we lower it after each training step according to some schedule (e.g., linearly or exponentially). In practice, more advanced <strong>optimizers</strong> are used to adjust the weights, like the Adam optimizer that we’ll describe in section 5.</p>
<p>Learning rate is what we call a <strong>hyperparameter</strong> – it’s a parameter of a neural network implementation which is not learned by training, but rather set by a user. More advanced training algorithms have many hyperparameters. Important thing to realize is that we don’t always have to set hyperparameters manually, but we can have other optimizing algorithm (even one based on machine learning) finding the optimal values of hyperparameters for us.</p>
<p>Let’s now look at the overview of how the whole training algorithm with gradient descent works. The single <strong>training step</strong> does the following:</p>
<ol>
<li>
<p>Evaluate the network for each sample from a training set.</p>
<p>a.  Calculate a cost function for each sample, using the calculated and expected outputs.</p>
<p>b.  Calculate a gradient for each sample.</p>
</li>
<li>
<p>Average the gradients calculated in step 1b (so that we obtain one partial derivative value for each weight and bias in the network).</p>
</li>
<li>
<p>Optimize the network: scale the averaged gradient by learning rate and subtract it from the current weights to obtain new weights.</p>
</li>
<li>
<p>Repeat from step 1.</p>
</li>
</ol>
<p>Going through all the inputs in the training set is called an <strong>epoch</strong> and we’ll need many epochs before the network <strong>converges</strong> to a local minimum of a cost function. Going through all the inputs in every step can be cumbersome, as the algorithm goes through the whole data set every time. As an optimization, we can use a modified method called <strong>stochastic</strong> <strong>gradient descent (SGD)</strong>.</p>
</section>
<section id="stochastic-gradient-descent">
<h3 id="3.2-stochastic-gradient-descent">3.2 Stochastic Gradient Descent<a href="#stochastic-gradient-descent" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>In practice, we don’t need to go through the whole training set before updating the weights. We can split the training set into a number of random subsets (<strong>batches</strong>) and update weights after processing every batch. This way, we will perform weight updates more often, achieving faster learning and consuming less memory. The downside is that our gradient is not so precise anymore and it doesn’t guide us to the local minimum along the shortest path. But if our subsets are good representatives of a whole training set, this doesn’t pose a big problem in practice. The process of using batches is also called <strong>mini-batching</strong>. SGD introduces at least two new hyperparameters – batch size and number of epochs to perform per training step.</p>
<p>Remember that our sample application wants to train the network to represent a 2D image, mapping UV coordinates to RGB texel values. We will use SGD for that, taking a batch of 2048 samples per each training step, and we’ll take one training step per frame. If we had to go through all the texels in every training step, the memory requirements and runtime performance would be unusable. For our example, we don’t even need to store the training set explicitly, we can simply generate desired number of random samples in run-time by randomly sampling the texture.</p>
</section>
<section id="when-to-stop-training">
<h3 id="3.3-when-to-stop-training">3.3 When To Stop Training<a href="#when-to-stop-training" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>The training as described above can in theory run indefinitely – improving the predictions and getting the cost function lower and lower. It is unlikely that it will ever reach zero, due to constraints of the neural network architecture and learning algorithm. In practice, we have to decide at some point that training has reached the best state possible and stop. At first it seems that we can set a threshold for the cost function that we want to reach and stop after achieving it, but there is a problem: If we only measure the cost on training data, we won’t know how it will perform on real world data it has not seen before. This is the problem of <strong>generalization</strong>. We want the network to perform well also on general data that it has not seen during the training. As the training progresses, the neural network will start <strong>remembering</strong> the training data, instead of learning some general relationships between inputs and outputs, and it will fail to generalize well to new data. We also say that our model is <strong>overfitting</strong> in this case.</p>
<p>To solve this, we should also track a <strong>validation error</strong> measured on a data set which is separate from the training set. This error will be high at the beginning (just like the training error), and will get lower with training, but after some time it will start to rise again. This rise happens at the point when neural network stops generalizing well to data not seen by training, and it is the point when we should stop training. It is common to create a <strong>validation set</strong> from the training data that we have available by splitting it into 80% training set and 20% validation set. It is, however, necessary to make sure that training and validation sets don’t mix. This method of stopping the training when validation error start rising is called <strong>early stopping</strong>.</p>
<p>Note that some algorithms like NRC don’t ever stop training – they run in real time to make sure that neural network adapts to changes in the scene. Our sample application also runs the training continuously because for our use case, the generalization is not a problem, we simply want it to learn one input image as good as it can.</p>
</section>
<section id="cost-function">
<h3 id="3.4-cost-function">3.4 Cost Function<a href="#cost-function" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>For our sample application, we will use a <strong>mean squared error (MSE)</strong> as a cost function:</p>
<p><span data-katex-display="true">\frac{1}{n}\sum_{i = 1}^{n}\left( target - output \right)^{2}</span></p>
<p>There are also other cost functions used in practice, like the <strong>L1 loss</strong> or <strong>L2 loss</strong>. We can use any function which gives lower score to better outcomes, but the function must have a derivative, which we’ll need for calculating the gradient.</p>
<p>Cost function is often also called a <strong>loss function</strong> and its value simply a <strong>loss</strong>. Cost functions can also have additional <strong>regularization</strong> terms which impose additional cost, e.g., for case when weights and biases get very large (this is called <strong>weight decay</strong>). This will force the weights to be as small as possible, which can help to prevent overfitting.</p>
</section>
<section id="backpropagation">
<h3 id="3.5-backpropagation">3.5 Backpropagation<a href="#backpropagation" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>Let’s now discuss how we calculate the gradient using the <strong>backpropagation</strong> algorithm. As the name suggests, the algorithm starts at the output layer and continues <strong>backwards</strong> through the network. It is therefore also called the <strong>backward pass</strong> through the network.</p>
<p>The goal of this algorithm is to calculate partial derivative for each weight and bias wrt. cost function, which we designate as <span data-katex-display="false">\frac{\partial cost}{\partial weight_{ij}}</span> and <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span>. Before we start let’s formally define some things we’ll need to calculate those partial derivatives:</p>
<p><span data-katex-display="true">Z_{i} = \sum_{}^{}{w_{ij}O_{j} + b_{i}}</span></p>
<p><span data-katex-display="true">O_{i} = \sigma(Z_{i})</span></p>
<p>where <span data-katex-display="false">Z_{i}</span> is the output of the neuron <span data-katex-display="false">i</span> without applying activation function, <span data-katex-display="false">w_{ij}</span> is a weight of the connection to the neuron <span data-katex-display="false">j</span>, <span data-katex-display="false">O_{j}</span> is the activation of connected neuron <span data-katex-display="false">j</span> , <span data-katex-display="false">b_{i}</span> is the bias value, <span data-katex-display="false">O_{i}</span> is the output with activation function applied, and <span data-katex-display="false">\sigma</span> is the activation function.</p>
<p>So how can we calculate these partial derivatives knowing the cost function value? The trick is to split calculations of <span data-katex-display="false">\frac{\partial cost}{\partial weight_{ij}}</span> and <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span> into several derivatives which are easier to calculate and combine them using a chain rule. Let’s start with the simplest, which is <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span>. This is the same as <span data-katex-display="false">\frac{\partial cost}{\partial Z_{i}}</span>, and is also called an <strong>error</strong> of the neuron:</p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial bias_{i}} = \frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}} = \frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} \bullet \frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span></p>
<p>We have now split <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span> into two simpler derivatives <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> and <span data-katex-display="false">\frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span>. For the neuron in the output layer, <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> is just telling us how the cost changes when its activation changes. This is equal to the partial derivative of the cost function with respect to evaluated neuron. When we use the MSE cost function, this derivative is:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} = c(target_{i} - O_{i})</span></p>
<p>where <span data-katex-display="false">c</span> is the constant coming from the derivation of MSE (in our sample where target is a 3-component vector, the <span data-katex-display="false">c</span> is equal to <span data-katex-display="false">2*\frac{1}{3} = 0.6\overline{66}</span>). In code, we can ignore this constant as it would only scale the whole gradient by the same number, and the gradient scale is already controlled by learning rate.</p>
<p>Next is the <span data-katex-display="false">\frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span> value. This tells us how the output of the neuron changes when we apply the activation function. This is simply a derivative of the activation function which we described in section 2.4.</p>
<p>We have now calculated the partial derivative of the bias term of the neuron in the output layer, and we can use it to adjust this neuron’s bias. Next, let’s look at the derivatives of weights connecting to this neuron, splitting it again like:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{YellowGreen} \partial weight_{ij}} = \frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}} \bullet \frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}}</span></p>
<p>The term <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}}</span> is the same error that we calculated before, and the new term <span data-katex-display="false">\frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}}</span> tells us how the neuron value without activation function changes when we change that weight. This is a partial derivative of <span data-katex-display="false">(w_{ij}O_{j} + b_{i})</span> wrt. <span data-katex-display="false">w_{ij}</span>, which just boils down to the activation of the connected neuron:</p>
<p><span data-katex-display="true">\frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}} = \ O_{j}</span></p>
<p>While these derivatives look intimidating at first, they boil down to a very simple calculation:</p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial bias_{i}} = \left( target_{i} - O_{i} \right) \bullet \sigma^{&#39;}(O_{i})</span></p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial weight_{ij}} = \frac{\partial cost}{\partial bias_{i}} \bullet O_{j}</span></p>
<p>With this knowledge, we can now implement gradient calculation for the output layer:</p>
<div>
<div>

<pre><code><span>// Gradient of bias</span>
<span>const</span><span> </span><span>uint</span><span> </span><span>neuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>neuron</span><span>);</span>

<span>const</span><span> </span><span>float</span><span> </span><span>neuronActivation</span><span> </span><span>=</span><span> </span><span>activations</span><span>[</span><span>neuronDataIndex</span><span>];</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dCost_O</span><span> </span><span>=</span><span> </span><span>(</span><span>neuronActivation</span><span> </span><span>-</span><span> </span><span>target</span><span>[</span><span>neuron</span><span>]);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dO_Z</span><span> </span><span>=</span><span> </span><span>ACTIVATION_FUNCTION_DERIV</span><span>(</span><span>neuronActivation</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dCost_Z</span><span> </span><span>=</span><span> </span><span>dCost_O</span><span> </span><span>*</span><span> </span><span>dO_Z</span><span>;</span>
<span>errors</span><span>[</span><span>neuronDataIndex</span><span>]</span><span> </span><span>=</span><span> </span><span>dCost_Z</span><span>;</span>
<span>InterlockedAdd</span><span>(</span><span>gradientBiases</span><span>[</span><span>neuronDataIndex</span><span>],</span><span> </span><span>packFloat</span><span>(</span><span>dCost_Z</span><span>));</span>

<span>// Gradient of weights</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>previousNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>previousNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountPreviousLayer</span><span>;</span><span> </span><span>previousNeuron</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>previousNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>2</span><span>,</span><span> </span><span>previousNeuron</span><span>);</span>
<span>  </span><span>const</span><span> </span><span>float</span><span> </span><span>dCost_weight</span><span> </span><span>=</span><span> </span><span>dCost_Z</span><span> </span><span>*</span><span> </span><span>activations</span><span>[</span><span>previousNeuronDataIndex</span><span>];</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>previousNeuron</span><span>,</span><span> </span><span>neuron</span><span>);</span>
<span>  </span><span>InterlockedAdd</span><span>(</span><span>gradientWeights</span><span>[</span><span>weightIndex</span><span>],</span><span> </span><span>packFloat</span><span>(</span><span>dCost_weight</span><span>));</span>
<span>}</span>
</code></pre>
</div>
</div>
<p>What remains is to calculate the same partial derivatives for the hidden layer, which is just slightly more complicated. Because the neurons in the hidden layer are connected to other neurons, their <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> values are not dependent on the cost function directly, but on the connected neurons. Specifically:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} = \sum_{j}^{}\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{j}} \bullet \frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}}</span></p>
<p>Where <span data-katex-display="false">j</span> is the j-th neuron connected to the output of evaluated neuron <span data-katex-display="false">i</span> , <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{j}}</span> is the error of the j-th neuron evaluated previously, and <span data-katex-display="false">\frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}}</span> tells us how the output of neuron <span data-katex-display="false">j</span> without activation function changes when the output of the evaluated neuron changes. This is a derivative of <span data-katex-display="false">(w_{ij}O_{j} + b_{i})</span> wrt. <span data-katex-display="false">O_{j}</span>, so simply a strength of the connection between the neurons:</p>
<p><span data-katex-display="true">\frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}} = weight_{ij}</span></p>
<p>The code for hidden layer gradient is almost the same, except for the <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> calculation, which is:</p>
<div>
<div>

<pre><code><span>float</span><span> </span><span>dCost_O</span><span> </span><span>=</span><span> </span><span>0.0f</span><span>;</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>nextNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>nextNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountNextLayer</span><span>;</span><span> </span><span>nextNeuron</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>layer</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>neuron</span><span>,</span><span> </span><span>nextNeuron</span><span>);</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>nextNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>nextNeuron</span><span>);</span>
<span>  </span><span>dCost_O</span><span> </span><span>+=</span><span> </span><span>(</span><span>errors</span><span>[</span><span>nextNeuronDataIndex</span><span>]</span><span> </span><span>*</span><span> </span><span>nnWeights</span><span>[</span><span>weightIndex</span><span>]);</span>
<span>}</span>
</code></pre>
</div>
</div>
<p>Notice that in order to implement this, we had to store errors of the previously evaluated neurons. We can say that the error is propagating backwards through the neural network, giving this algorithm its name – backpropagation. We also need to store activations for all neurons, which was not necessary for the forward pass.</p>
</section>
<section id="training-implementation-details">
<h3 id="3.6-training-implementation-details">3.6 Training Implementation Details<a href="#training-implementation-details" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>The code sample implements training as described above running on the GPU using Dx12 compute shaders. The whole training runs in 2 main kernel dispatches: <strong>gradient calculation</strong> and <strong>optimization</strong>. For gradient calculation, we run as many threads in parallel as we have training examples in the batch. Each thread generates a training example by randomly sampling a reference texture. Then it runs the forward pass to evaluate activations for the whole network, followed by the backpropagation pass to calculate the gradient. You may have noticed in the previous listings that we use InterlockedAdd operation to store the gradient. This is because we are interested in average gradient for all training examples in the batch, so we can add them all up and then divide the sum by the batch size before using it. The code for a gradient calculation step is:</p>
<div>
<div>

<pre><code><span>// Initialize random numbers generator</span>
<span>uint</span><span> </span><span>rng</span><span> </span><span>=</span><span> </span><span>initRNG</span><span>(</span><span>LaunchIndex</span><span>,</span><span> </span><span>uint2</span><span>(</span><span>1</span><span>,</span><span> </span><span>1</span><span>),</span><span> </span><span>gData</span><span>.</span><span>frameNumber</span><span>);</span>

<span>// Generate a random input (UV coordinates in the image)</span>
<span>const</span><span> </span><span>float2</span><span> </span><span>uvs</span><span> </span><span>=</span><span> </span><span>float2</span><span>(</span><span>rand</span><span>(</span><span>rng</span><span>),</span><span> </span><span>rand</span><span>(</span><span>rng</span><span>));</span>

<span>// Load target value to learn for this input from reference image</span>
<span>const</span><span> </span><span>float3</span><span> </span><span>target</span><span> </span><span>=</span><span> </span><span>targetTexture</span><span>[</span><span>uvs</span><span> </span><span>*</span><span> </span><span>float2</span><span>(</span><span>gData</span><span>.</span><span>outputWidth</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>gData</span><span>.</span><span>outputHeight</span><span> </span><span>-</span><span> </span><span>1</span><span>)].</span><span>rgb</span><span>;</span>

<span>// First run forward pass to evaluate network activations for given input</span>
<span>float</span><span> </span><span>activations</span><span>[</span><span>LAYER_COUNT</span><span> </span><span>*</span><span> </span><span>MAX_NEURONS_PER_LAYER</span><span>];</span>
<span>forwardPass</span><span>(</span><span>uvs</span><span>,</span><span> </span><span>activations</span><span>);</span>

<span>// Run backpropagation on current network state</span>
<span>backpropagation</span><span>(</span><span>target</span><span>,</span><span> </span><span>activations</span><span>);</span>
</code></pre>
</div>
</div>
<p>The next step – optimization – runs once after calculating gradient for every batch, reads the gradient and adjusts the weights and biases accordingly.</p>
</section>
<section id="neural-network-initialization">
<h3 id="3.7-neural-network-initialization">3.7 Neural Network Initialization<a href="#neural-network-initialization" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>There is one important aspect of training we haven’t covered yet and that’s the initial state of the network before we start the training. Initial weights and biases influence where our training starts and how fast can it approach the optimal solution, so a good initialization strategy is desirable. If we used some constant for all initial weights (e.g., zero or one), all activations and their gradients would be the same – the network wouldn’t be learning anything as every neuron would follow the same learning path (assuming we have a deterministic learning algorithm).</p>
<p>Because of this, we want to start with different initial weight and bias setting for each neuron to “break the symmetry”. Therefore, we initialize the network with some small random numbers centered around zero. We can take these numbers, e.g., from the uniform or normal distribution. To do that we must pick a range in which to generate random numbers, and in case of normal distribution also the standard deviation. These can either be hyperparameters tuned manually, or better, we can use one of the common strategies for setting them automatically. Such strategies are, e.g., <strong>LeCun uniform</strong>, <strong>Xavier uniform</strong> and their variations using normal distribution. These have been introduced in paper <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a>.</p>
<p>In our code sample we use the Xavier uniform initialization. It generates weights using a uniform random distribution within the range <span data-katex-display="false">\lbrack -x,x\rbrack</span>, where <span data-katex-display="false">x</span> is dependent on number of neurons in layers that the weight is connecting:</p>
<p><span data-katex-display="true">x = \sqrt{\frac{6}{n_{previousLayer} + n_{currentLayer}}}</span></p>
<p>Random initial values for weights are enough to break the symmetry and ensure proper learning, and therefore it is common to initialize biases to zeroes, or some small constant.</p>
</section>
</section>
<section id="improving-the-network-input-encodings">
<h2 id="4.-improving-the-network---input-encodings">4. Improving the Network – Input Encodings<a href="#improving-the-network-input-encodings" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>So far, we have only used the identity input encoding. Meaning, we have simply passed our UV coordinates into two input neurons. In this section we are going to explore a more advanced input encoding, the <strong>frequency encoding</strong>, which greatly improves performance for our sample application.</p>
<section id="frequency-input-encoding">
<h3 id="4.1-frequency-input-encoding">4.1 Frequency Input Encoding<a href="#frequency-input-encoding" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>This encoding was described in the <a href="https://arxiv.org/pdf/2003.08934" target="_blank" rel="noopener">NeRF paper</a> under the name “positional encoding”, but I see it’s often referred to as the “frequency encoding”. Idea is to transform the input into higher-dimensional space using the following formula:</p>
<p><span data-katex-display="true">\gamma\left( p \right) = \left( \sin\left( 2^{0}\pi p \right),\cos\left( 2^{0}\pi p \right), \ldots \sin\left( 2^{L - 1}\pi p \right),\cos\left( 2^{L - 1}\pi p \right) \right)</span></p>
<p>where <span data-katex-display="false">p</span> is the input value we want to encode, <span data-katex-display="false">L</span> is the number of frequencies we want to use (e.g., 8) and <span data-katex-display="false">\gamma</span> is a vector of encoded values. Note that with this encoding, we have greatly increased number of input neurons. For our sample application with 2 input values, and 8 frequencies, we get 32 input neurons.</p>
<p>Having inputs in higher-dimensional space will make it easier for neural network to discover more complex relationships between inputs and outputs. Implementation of this encoding can be found in the sample code in function called frequencyEncoding.</p>
</section>
<section id="other-input-encodings">
<h3 id="4.2-other-input-encodings">4.2 Other Input Encodings<a href="#other-input-encodings" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>In the deep learning literature, you will often encounter a <strong>one-hot encoding</strong>. This is a very simple encoding where input is encoded into a vector of values, where only a single entry is 1, and others are 0 (we say that one value is hot). E.g., we can encode a fact that object belongs to certain category by having a vector of values representing each category, and setting value 1 for the selected category, while zeroing out others.</p>
<p>For more advanced encodings, I recommend looking at the <strong>one-blob encoding</strong>, introduced in <a href="https://arxiv.org/pdf/1808.03856" target="_blank" rel="noopener">Neural Importance Sampling</a> paper, which extends one-hot encoding to have multiple entries activated, instead of just one. This essentially activates or shuts down parts of the network, depending on the input value.</p>
<p>Another useful encoding is a <strong>hash-grid encoding</strong> introduced in <a href="https://tom94.net/data/publications/mueller22instant/mueller22instant.pdf" target="_blank" rel="noopener">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a> paper.</p>
</section>
</section>
<section id="improving-the-network-adam-optimizer">
<h2 id="5.-improving-the-network---adam-optimizer">5. Improving the Network – Adam Optimizer<a href="#improving-the-network-adam-optimizer" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>So far, we have applied gradients during the training using a very simple way – we just multiplied it by the learning rate and subtracted it from current weights and biases. While this works, it is not optimal for several reasons: fixed learning rate is usually suboptimal at the beginning, when we want to take larger steps to proceed faster, but it is also suboptimal at the end, when it can oscillate around the optimal solution because the fixed step is too large to get into the valley of local minimum. In practice, we want to use an adaptive learning rate instead.</p>
<p>In this section we implement an improved optimizer called <em>Adam</em> (adaptive moment estimation), described in the paper <a href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a>. Adam adds two ingredients to the optimization process: adaptive learning rate and momentum.</p>
<p>We have described adaptive learning rate before, so let’s now look at the momentum. Remember our “ball rolling” example where we stated that minimizing the cost function is like rolling the ball on its surface to find a lowest point. Just like in real world, where the ball has certain momentum, we can add momentum to our algorithm by taking into consideration the gradients in previous steps and applying a weighted average of them, instead of just latest gradients. This way, we progress to the optimum faster and we have a higher chance of escaping shallow valleys with local minima to find even lower local minimum.</p>
<section id="adam-implementation">
<h3 id="5.1-adam-implementation">5.1 Adam Implementation<a href="#adam-implementation" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h3>
<p>Adam optimizer works by tracking the first and second moments (mean and variance) of the gradient. This means that for each weight and bias, we have to track mean and variance of their partial derivatives. They are averaged over time and their decay is controlled by new hyperparameters <span data-katex-display="false">\beta_{1}</span> and <span data-katex-display="false">\beta_{2}</span>. The adjustment done to weight or bias is calculated using its gradient, mean, and variance in the following way:</p>
<div>
<div>

<pre><code><span>// Update mean and variance for this training step</span>

<span>mean</span><span> </span><span>=</span><span> </span><span>lerp</span><span>(</span><span>gradient</span><span>,</span><span> </span><span>mean</span><span>,</span><span> </span><span>gData</span><span>.</span><span>adamBeta1</span><span>);</span>
<span>variance</span><span> </span><span>=</span><span> </span><span>lerp</span><span>((</span><span>gradient</span><span> </span><span>*</span><span> </span><span>gradient</span><span>),</span><span> </span><span>variance</span><span>,</span><span> </span><span>gData</span><span>.</span><span>adamBeta2</span><span>);</span>

<span>// Calculate weight (or bias) adjustment</span>
<span>const</span><span> </span><span>float</span><span> </span><span>correctedMean</span><span> </span><span>=</span><span> </span><span>mean</span><span> </span><span>/</span><span> </span><span>(</span><span>1.0f</span><span> </span><span>-</span><span> </span><span>gData</span><span>.</span><span>adamBeta1T</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>correctedVariance</span><span> </span><span>=</span><span> </span><span>variance</span><span> </span><span>/</span><span> </span><span>(</span><span>1.0f</span><span> </span><span>-</span><span> </span><span>gData</span><span>.</span><span>adamBeta2T</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>weightAdjustment</span><span> </span><span>=</span><span> </span><span>-</span><span>gData</span><span>.</span><span>learningRate</span><span> </span><span>*</span><span> </span><span>(</span><span>correctedMean</span><span> </span><span>/</span><span> </span><span>(</span><span>sqrt</span><span>(</span><span>correctedVariance</span><span>)</span><span> </span><span>+</span><span> </span><span>gData</span><span>.</span><span>adamEpsilon</span><span>));</span>
</code></pre>
</div>
</div>
<p>The updated mean and variance are stored next to weights and biases. Note that we still have to set a base learning rate when using this algorithm, the paper suggests a value of 0.001. In practice, the default values suggested in the paper for <span data-katex-display="false">\beta_{1} = 0.9</span> and <span data-katex-display="false">\beta_{2} = 0.999</span> are used as well. The default value for <span data-katex-display="false">\varepsilon</span> which prevents division by zero is <span data-katex-display="false">10^{- 8}</span>. Values <span data-katex-display="false">\beta_{1}^{T}</span> and <span data-katex-display="false">\beta_{2}^{T}</span> are derived from <span data-katex-display="false">\beta_{1}</span> and <span data-katex-display="false">\beta_{2}</span> and adjusted after each training step as follows:</p>
<div>
<div>

<pre><code><span>adamBeta1T</span><span> </span><span>=</span><span> </span><span>pow</span><span>(</span><span>adamBeta1</span><span>,</span><span> </span><span>training_steps</span><span> </span><span>+</span><span> </span><span>1</span><span>);</span>
<span>adamBeta2T</span><span> </span><span>=</span><span> </span><span>pow</span><span>(</span><span>adamBeta2</span><span>,</span><span> </span><span>training_steps</span><span> </span><span>+</span><span> </span><span>1</span><span>);</span>
</code></pre>
</div>
</div>
</section>
</section>
<section id="problems-with-training">
<h2 id="6.-problems-with-training">6. Problems with Training<a href="#problems-with-training" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>While the method for training described so far is fairly robust and efficient, there are several problems which we can encounter in practice, let’s briefly discuss some of them:</p>
<ul>
<li>
<p><strong>Insufficient training data:</strong> when we have low amount of training data, or when the batch size of stochastic gradient descent is too low, we won’t be able to find a stable solution that generalizes well. Make sure to have sufficient data to get into local minimum of the cost function.</p>
</li>
<li>
<p><strong>Non-deterministic data:</strong> So far, we assumed that there is a deterministic relationship between the training inputs and target outputs. Breaking this assumption by having some random values can make it hard or impossible for training to converge.</p>
</li>
<li>
<p><strong>Wrong learning rate:</strong> Having a learning rate too small will cause the training to proceed very slowly, while having it too large can make the training unstable.</p>
</li>
<li>
<p><strong>Vanishing gradients:</strong> For deep networks, gradients in certain layers can become very small, slowing down or even stopping the training. This happens because for typical activation functions, large changes on some inputs can only cause small changes (or no changes) of the output. E.g., the negative part of the leaky ReLU outputs very small values. In this case, partial derivatives become smaller and smaller, and training doesn’t change the weights much. One of the possible solutions is to use different activation functions.</p>
</li>
<li>
<p><strong>Exploding gradients:</strong> The opposite problem happens when gradients get very large and cause instability. As a solution, we can clamp gradients to some threshold value, limiting the rate at which they can change weights and biases – this is called the <strong>gradient clipping</strong>.</p>
</li>
</ul>
</section>
<section id="next-topics">
<h2 id="7.-next-topics">7. Next Topics<a href="#next-topics" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>There is a lot more to deep learning than what I described so far in this article. Before we wrap up, I want to mention at least two topics worth studying next, that you will encounter often when reading deep learning papers focused on computer graphics: auto-encoders and convolutional networks.</p>
<p>Auto-encoder is a type of neural network consisting of encoder and a decoder. Encoder translates input data into more compact representation (in latent space) and decoder is used to decompress it to original representation. It can be used for applications like data compression, denoising and image reconstruction.</p>
<p>Convolutional networks (CNNs) are specialized networks used mostly in image processing. They contain 2 types of hidden layers: convolutional layers intended to detect certain features in images, and pooling layers which downsample intermediate results into more compact representation.</p>
<p>While I was first learning about deep learning, I found great insight in the paper titled “<a href="https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf" target="_blank" rel="noopener">Multilayer Feedforward Networks are Universal Approximators</a>” from 1989. It says that MLPs with at least one hidden layer are <strong>universal approximators</strong>, meaning they can approximate any function to a degree allowed by the network capacity. It also says that failure to approximate given function can be attributed to inadequate learning, insufficient network capacity, or non-deterministic relation between network inputs and outputs. In practice, we can therefore use MLPs to replace parts of our algorithms which contain difficult functions mapping data from one space to another.</p>
</section>
<section id="conclusion">
<h2 id="8.-conclusion">8. Conclusion<a href="#conclusion" title="Permalink to this heading"><SPAN size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></SPAN></a></h2>
<p>In this article, I have described a few concepts and algorithms used in deep learning that I found most interesting for my use case (representing an image using an MLP), but there is much more to explore. With the knowledge provided here, I hope that you’ll have more fun reading deep learning papers and resources and implement your own deep learning ideas.</p>
<p>I recommend looking at the book <em><a href="https://www.glassner.com/portfolio/deep-learning-a-visual-approach/" target="_blank" rel="noopener">Deep Learning: A Visual Approach</a></em> by Glassner, the book <em>Deep Learning</em> by Goodfellow et al. which is <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">freely available online</a>, deep learning videos by <a href="https://www.youtube.com/@3blue1brown" target="_blank" rel="noopener">3Blue1Brown</a>, neural network <a href="https://blog.demofox.org/2017/03/09/how-to-train-neural-networks-with-backpropagation/" target="_blank" rel="noopener">blogs by demofox</a>, paper titled “<a href="https://www.researchgate.net/publication/277411157_Deep_Learning" target="_blank" rel="noopener">Deep learning</a>” by LeCun et al. from 2015, and graphics papers such as <a href="https://arxiv.org/pdf/2003.08934" target="_blank" rel="noopener">NeRF</a> and <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/mueller21realtime.pdf" target="_blank" rel="noopener">NRC</a>. When implementing neural networks, I recommend reading an article <a href="https://gpuopen.com/learn/wmma_on_rdna3/" target="_blank" rel="noopener">How to accelerate AI applications on RDNA 3 using WMMA</a> for insights how to use AI accelerating instructions of current GPUs. Finally, I recommend experimenting with libraries like <a href="https://pytorch.org/" target="_blank" rel="noopener">pyTorch</a> which enable much faster prototyping, than implementing everything from scratch.</p>
<p>I want to thank Daniel Meister for helpful comments and suggestions.</p>
<p><em>This blog was originally published by Jakub at <a href="https://boksajak.github.io/blog/DeepLearning" target="_blank" rel="noopener">https://boksajak.github.io/blog/DeepLearning</a></em>.</p>
</section>
</section>
</div>
		</div>
				</div></div>
  </body>
</html>
