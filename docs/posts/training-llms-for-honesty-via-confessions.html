<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2512.08093">Original</a>
    <h1>Training LLMs for Honesty via Confessions</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2512.08093">View PDF</a>
    <a href="https://arxiv.org/html/2512.08093v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
</blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Boaz Barak [<a href="https://arxiv.org/show-email/6e0924a5/2512.08093" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
