<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.tbray.org/ongoing/When/202x/2022/06/10/Quamina-Optimizing">Original</a>
    <h1>Making Code Faster</h1>
    
    <div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">I’ve enjoyed writing software for 40+ years now. Lots of activities fall into that “writing software”
    basket, and here’s my favorite:  When you have a body of code with a decent unit-test suite and you need to make
    it go faster. This part of the
    <a href="https://www.tbray.org/ongoing/What/Technology/Quamina%20Diary/">Quamina diary</a> is a case study of making a piece of the
    library faster.</p>

<p id="p-1"><span>How to think about it</span> · 
When you’re sitting down to create software, you probably start with preconceptions about whether and where there are likely
    to be performance issues.  If you’re writing Ruby code to turn your blog drafts into HTML, it doesn’t much matter if
    republishing takes a few extra seconds. So, write the first cut of that software in
    the simplest and clearest possible way, without sweating the milliseconds.</p>

<p>But for code that’s on the critical path of a service back-end running on a big sharded 
    fleet with lots of zeroes in the “per second” numbers, it makes sense to think hard about performance at design time.</p>

<p>At this point, I want to call out to Nelson Elhage’s
    <a href="https://blog.nelhage.com/post/reflections-on-performance/">Reflections on software performance</a>. Nelson approvingly
    quotes the following pieces of conventional wisdom:</p>

<ul>
<li><p>premature optimization is the root of all evil.</p>
</li>
<li><p>Make it work, then make it right, then make it fast.</p>
</li>
<li><p>CPU time is always cheaper than an engineer’s time.</p>
</li>
</ul>
<p>I approve too!  But… Sometimes it just has to be fast, and sometimes that means the performance has to be designed in.
    Nelson agrees and has smart things to say on the subject.</p>

<p>And then there’s the background against which I wrote the Quamina
    code: It’s a successor to similar library I wrote v1.0 of at AWS, and which is in production in multiple high-volume services,
    running hot. So I didn’t have to speculate about the kinds of stress that would be applied, nor about what kinds of
    design goofs would produce bottlenecks.</p>

<p id="p-2"><span>On profiling</span> · 
Nobody is smart enough figure out what’s making a piece of code slow by looking at its inputs, outputs, and structure. That’s
    why we have profilers. Fortunately for me, Quamina is written in Go, which means the profiler is built-in, helpful, 
    and doesn’t even slow your benchmarks down much. The other two languages I’ve used mostly in recent decades are Java and Ruby and
    the profiler situation is for both those languages is kind of shitty. I had to pay real money to get the Java profiler I used at
    AWS  and while it worked, it was klunky, not fun to use.</p>

<p>Whatever language you’re in, have a look around and find a good profiler you can live with and keep it handy.  And if you’re
    in Go-land, go
    <a href="https://hackernoon.com/go-the-complete-guide-to-profiling-your-code-h51r3waz">read up on the profiler</a> (avoid the
    official docs Google turns up, they’re outdated and useless).</p>

<p>The Go profiler has a very decent command line that will show you readouts in a local HTTP server or textual tables or
    exported as PDF. I find the latter easiest to use but YMMV.  But what I actually use is the built-in profiler module in
    <a href="https://www.jetbrains.com/go/promo/">GoLand</a>, JetBrains’ Go IDE.  There’s a “run with profiler” button and when
    you hit that the first thing you get is an IDE pane full of flame graph. Which takes a little while to figure out but once you
    do it’s brilliant. It might not always show you what your problem is but it sure as hell shows you where to look.  Here’s one of
    those.</p>

<p><a href="https://www.tbray.org/ongoing/When/202x/2022/06/10/-big/flame-graph.jpg.html"><img alt="GoLand flame-graph output from the Go profiler" title="GoLand flame-graph output from the Go profiler" src="https://www.tbray.org/ongoing/When/202x/2022/06/10/flame-graph.png"/></a></p>
<p>The middle bump is loading the test data, the right third is profiling overhead (I think), the app logic is in the left third.</p>
<p>The GoLand profiler module lets you walk around the call graph and seek out hot methods and so on; I never wanted it to do
    anything it couldn’t.</p>

<p>I’m not going do a how-to on the profiler; it’s not hard to learn, and you should learn it. Anyhow, most of
    you kids are in VS Code these days, which I assume has good profiler support. And having said all that, the graphics below are the
    PDF output from the Go profiler, no IDE involved.</p>

<p id="p-3"><span>Gotta have those tests</span> · 
So, you understand what the performance problem is, and you have a profiler setup that you know how to use. But just like it
    says in the first paragraph, good unit tests are totally essential. Because the profiler’s going to show you a problem, and you’re going
    to refactor and rewrite to fix the problem. You can do that fearlessly <em>if</em> you have a high-coverage unit-test
    suite that you can run with a single key combo and comes back in ten seconds or less. Otherwise, sucks to be you.</p>

<p>Because when you refactor and rewrite, you will break things! Guaranteed. But if you’ve got that unit-test backstop, you
    don’t need to worry, because you’ll find out <em>right away</em>, and you can iterate till it passes and find out if your work
    helped.</p>

<p>If you need to speed up code and it doesn’t have decent unit-test coverage… well, then, writing the unit tests is the price
    of admission. Which, at the end of the day, is just another reason to never let anyone check code in that doesn’t have those
    tests in good shape.</p>

<p id="p-6"><span>Benchmark choices</span> · 
With a profiler and unit tests, you have two legs of the tripod. The third is a nice data-driven repeatable benchmark.
    I use the
    <a href="https://catalog.data.gov/en/dataset/parcels-active-and-retired">San Francisco Parcels</a> JSON file, a couple of
    hundred thousand JSON objects comprising 190MB or so.  Big enough to get meaningful measurement; small enough that Quamina can
    run pattern sets on it in a second and change.</p>

<p>The San Francisco JSON objects have several large arrays full of 19-digit floating-point numbers, which is pretty well a worst-case
    scenario for the flattener, so most people will see better performance than we do while we’re testing. Finding a good benchmark
    data set is a good investment of your time.</p>

<p id="p-4"><span>Back Story</span> · 
V1.0 of Quamina’s ancestor at AWS (I’ve been calling it “The Matcher”) was about fast enough for EventBridge, so it didn’t
    really get any optimization love. Then when other services started picking it up, we started hearing the occasional gripe.  In
    particular some team down in the bowels of CloudWatch wanted to use it for something and benchmarked it against a few other
    options that looked slow to me.  The Matcher won, but not by very much, at which point it occurred to me to profile it. Did I
    ever get a surprise.</p>

<p>As discussed elsewhere in this Diary, matching Patterns to Events is a two-step process: First you have to “flatten” the
    event into a list of pathname/value fields, then you run the matching automaton. When I fired up the profiler, I found that he
    Matcher was spending like 90% of its time flattening events.  Ouch!</p>

<p>I’d been using a popular JSON library to turn the event into a tree structure, then running around the tree structure to peel
    out the fields. Which requires a whole lot of memory allocation. <b>BZZZZT! Wrong!</b> Anyone with significant optimization
    experience will tell you to keep memory management off your critical path; some of my biggest optimization wins over the years have
    come from pre-allocating buffers. At AWS, I juiced up the flattening code but can’t remember most details. Except for, I used a
    streaming JSON parser.</p>

<p id="p-5"><span>Cheap JSON</span> · 
Well, I wasn’t going to do the same dumb thing twice, so my first cut at Quamina’s JSON flattener used Go’s
    <a href="https://pkg.go.dev/github.com/segmentio/encoding/json">json</a> package’s
    <a href="https://pkg.go.dev/github.com/segmentio/encoding/json#Tokenizer">Tokenizer</a> streaming API. Thus I was pretty
    disappointed the first time I profiled a big Quamina run and saw the flattener burning 90% of the time, and and then again when
    the method burning most of the time was some internal thing with <code>Malloc</code> in its name.</p>

<p>A couple of minutes’ research revealed that yeah, that API is known to be slow and yeah, it’s because it gobbles memory like a
    hungry teenager.  There are a couple of alternate community-built tokenizers; I looked at them but for some reason nothing
    really appealed. Also I’d like for Quamina to have the minimum in dependencies, ideally zero, just because that makes it easier to
    adopt.</p>

<p>Also, I realized, I had a couple of advantages over someone writing a general-purpose tokenizer. As I’ve mentioned before,
    one of Quamina’s designed-in optimizations is that once you’ve added a bunch of Patterns to an instance, you know which fields
    you need to look at while matching and, crucially, which you can ignore.  So while you’re flattening, you can skip over lots of
    fields and for that, the memory cost should be zero.</p>

<p>Second, to match field values, Quamina uses a byte-driven automaton to support things like <code>*</code> and numerics. So
    there’s no need to turn the field values into strings or numeric data types, everything can be a <code>[]byte</code>. And since
    a message coming from outside is also a bit <code>[]byte</code> slice, there’s really no need to allocate anything except for
    sub-slices, and that only for the fields you care about.</p>

<p>Thus was born
    <a href="https://github.com/timbray/quamina/blob/main/flatten_json.go">flattenJSON</a>, a complete (but <em>not</em>
    general-purpose) JSON tokenizer in 750 lines of Go.  It’s a collection of state-machine mini-parsers; one at the top level, then
    sub-parsers for objects, arrays, numbers, string values, and so on.  As of now, it still takes more time to flatten than match
    an event, but it’s close. Which means the whole thing sped up by a factor of about five.</p>

<p id="p-7"><span>What next?</span> · 
Time to declare victory?  Well, let’s break out that profiler again. Here are reports on CPU and memory utilization.</p>

<p><a href="https://www.tbray.org/ongoing/When/202x/2022/06/10/-big/cpu-profile.jpg.html"><img alt="CPU profile of the Quamina flattener" title="CPU profile of the Quamina flattener" src="https://www.tbray.org/ongoing/When/202x/2022/06/10/cpu-profile.png"/></a></p>
<p>· · ·</p>
<p><a href="https://www.tbray.org/ongoing/When/202x/2022/06/10/-big/mem-profile.jpg.html"><img alt="Memory allocation profile of the Quamina flattener." title="Memory allocation profile of the Quamina flattener." src="https://www.tbray.org/ongoing/When/202x/2022/06/10/mem-profile.png"/></a></p>
<p>Above: CPU profile. Below: Memory profile.</p>
<p>For people who are learning the Go profiler, this command produced files named <code>cpu.prof</code> and <code>mem.prof</code>
    with the profile data:</p>

<pre><code><nobr>go test -cpuprofile cpu.prof -memprofile mem.prof  -v -run=TestFlatteningProfile</nobr></code></pre>
    <p>And here are the command lines that generated the PDFs:</p>

    <pre><code>go tool pprof -pdf -show_from flattenJSON cpu.prof
go tool pprof -pdf -show_from flattenJSON mem.prof</code></pre>      
    <p>So, what is the profiler trying to tell us?  The CPU profile makes it pretty clear that the flattener is spending the bulk of
    its time managing memory, names like <code>memmove</code> and <code>refillAllocCache</code> are a giveaway.  In an
    earlier revision, the heavy-hitting methods had <code>GC</code> in their names, so at least I’m now avoiding the garbage
    collector.</p>

    <p>If we look what called the heavy hitters, or alternatively just glance at the memory profile, it’s pretty obvious that
    <a href="https://github.com/timbray/quamina/blob/main/flatten_json.go#L700">storeArrayElementField</a> is our prime suspect.
    Let’s have a look.</p>

    <div><pre><span>// storeArrayElementField adds a field to be returned to the Flatten caller, straightforward except for the field needs its</span>
<span>//  own snapshot of the array-trail data, because it&#39;ll be different for each array element</span>
<span>//  NOTE: The profiler says this is the most expensive function in the whole matchesForJSONEvent universe, presumably</span>
<span>//   because of the necessity to construct a new arrayTrail for each element.</span>
<span>func</span> (<span>fj</span> <span>*</span><span>flattenJSON</span>) <span>storeArrayElementField</span>(<span>path</span> []<span>byte</span>, <span>val</span> []<span>byte</span>) {
	<span>f</span> <span>:=</span> <span>Field</span>{<span>Path</span>: <span>path</span>, <span>ArrayTrail</span>: <span>make</span>([]<span>ArrayPos</span>, <span>len</span>(<span>fj</span>.<span>arrayTrail</span>)), <span>Val</span>: <span>val</span>}
	<span>copy</span>(<span>f</span>.<span>ArrayTrail</span>, <span>fj</span>.<span>arrayTrail</span>)
	<span>fj</span>.<span>fields</span> <span>=</span> <span>append</span>(<span>fj</span>.<span>fields</span>, <span>f</span>)
}</pre></div>
    <p>Yep, it makes a new <code>Field</code> struct and then gives it a copy of that array of <code>ArrayPos</code> structs, and
    then appends the new <code>Field</code> to a list called <code>fields</code>.</p>

    <p>It’s that array-copy that’s getting us. Can you think of anything that might reduce the pain here?</p>

    <p>I can, but only because I know what this thing is all about.  To match correctly, you need to know about the arrays in the
    element you’re matching so that so that you don’t match fields from different array elements. For example, if you have an array
    of objects representing the Beatles, you don’t want a match of</p>

    <p>Anyhow, as you run through a long array, it turns out that each <code>Field</code> only needs an <code>ArrayTrail</code>
    copy in case it has child nodes that will append more stuff to the trail.
    So if you made the <code>ArrayTrail</code> segmented and <code>Fields</code> could share segments, you could probably 
    re-use the path-so-far segment and avoid a lot of the copying. Or you could think about of having
    an array of <code>*ArrayPos</code> rather than <code>ArrayPos</code>, so you could share steps and only copy pointers.</p>

    <p>Both options would introduce complexity and the second might not even be a big performance win, because later in the matching
    process you have to run through array and look at each element and you’ll lose a lot of memory locality as you fetch them.</p>

    <p>Will I go after <code>storeArrayElementField</code> at some future point?  Maybe, if I can think of something simpler. Got
    any good ideas? Quamina’s already pretty fast.</p>

    <p id="p-1"><span>Take-aways</span> · 
    Test. Benchmark. Refactor. Iterate. It’s not fancy. It’s fun. I have been known to whoop out loud with glee when some little
    move knocks the runtime down significantly.  How often do you do that at work?</p>

  <hr/>


<hr/>

</div></div>
  </body>
</html>
