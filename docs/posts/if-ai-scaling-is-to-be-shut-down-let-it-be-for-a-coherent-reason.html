<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scottaaronson.blog/?p=7174">Original</a>
    <h1>If AI scaling is to be shut down, let it be for a coherent reason</h1>
    
    <div id="readability-page-1" class="page"><div>
				
<p>There’s now an <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter</a> arguing that the world should impose a six-month moratorium on the further scaling of AI models such as GPT, by government fiat if necessary, to give AI safety and interpretability research a bit more time to catch up.  The letter is signed by many of my friends and colleagues, many who probably agree with each other about little else, over a thousand people including Elon Musk, Steve Wozniak, Andrew Yang, Jaan Tallinn, Stuart Russell, Max Tegmark, Yuval Noah Harari, Ernie Davis, Gary Marcus, and Yoshua Bengio. </p>



<p>Meanwhile, Eliezer Yudkowsky <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">published a piece in <em>TIME</em></a> arguing that the open letter doesn’t go nearly far enough, and that AI scaling needs to be shut down entirely until the AI alignment problem is solved—with the shutdown enforced by military strikes on GPU farms if needed, and treated as more important than preventing nuclear war.</p>



<p>Readers, as they do, asked me to respond.  Alright, alright.  While the open letter is presumably targeted at OpenAI more than any other entity, and while I’ve been <a href="https://scottaaronson.blog/?p=6484">spending the year at OpenAI</a> to work on theoretical foundations of AI safety, I’m going to answer strictly for myself.</p>



<p>Given the jaw-droppingly spectacular abilities of GPT-4—e.g., <a href="https://www.semafor.com/article/03/15/2023/how-gpt-4-performed-in-academic-exams">acing</a> the Advanced Placement biology and macroeconomics exams, correctly <a href="https://arxiv.org/abs/2303.12712">manipulating images</a> (via their source code) without having been programmed for anything of the kind, etc. etc.—the idea that AI now needs to be treated with extreme caution strikes me as far from absurd.  I don’t even dismiss the possibility that advanced AI could eventually require the same sorts of safeguards as nuclear weapons.</p>



<p>Furthermore, people might be surprised about the diversity of opinion about these issues <em>within</em> OpenAI, by how many there have discussed or even forcefully advocated slowing down.  And there’s a world not so far from this one where I, too, get behind a pause.  For example, one <em>actual</em> major human tragedy caused by a generative AI model might suffice to push me over the edge.  (What would push <em>you</em> over the edge, if you’re not already over?)</p>



<p>Before I join the slowdown brigade, though, I have (this being the week before Passover) four questions for the signatories:</p>



<ol>
<li>Would your rationale for this pause have applied to basically <em>any</em> nascent technology — the printing press, radio, airplanes, the Internet?  “We don’t yet know the implications, but there’s an excellent chance terrible people will misuse this, ergo the only responsible choice is to pause until we’re confident that they won’t”?</li>



<li>Why six months?  Why not six weeks or six years?</li>



<li>When, by your lights, would we ever know that it was safe to <em>resume</em> scaling AI—or at least that the risks of pausing exceeded the risks of scaling?  Why won’t the <a href="https://en.wikipedia.org/wiki/Precautionary_principle">precautionary principle</a> continue for apply forever?</li>



<li>Were you, until approximately last week, ridiculing GPT as unimpressive, a stochastic parrot, lacking common sense, piffle, a scam, etc. — before turning around and declaring that it could be existentially dangerous? How can you have it both ways?  If the problem, in your view, is that GPT-4 is too stupid, then shouldn’t GPT-5 be smarter <em>and therefore</em> <em>safer</em>?  Thus, shouldn’t we keep scaling AI as quickly as we can … <em>for safety reasons</em>?  If, on the other hand, the problem is that GPT-4 is too smart, then why can’t you bring yourself to say so?</li>
</ol>



<p>With the “why six months?” question, I confess that I was deeply confused, until I heard a dear friend and colleague in academic AI, one who’s long been skeptical of AI-doom scenarios, explain why <em>he</em> signed the open letter.  He said: look, we all started writing research papers about the safety issues with ChatGPT; then our work became obsolete when OpenAI released GPT-4 just a few months later.  So now we’re writing papers about GPT-4.  Will we <em>again</em> have to throw our work away when OpenAI releases GPT-5?  I realized that, while six months might not suffice to save human civilization, it’s just enough for the more immediate concern of getting papers into academic AI conferences.</p>



<p>Look: while I’ve spent <a href="https://scottaaronson.blog/?p=6821">multiple</a> <a href="https://scottaaronson.blog/?p=6823">posts</a> explaining how I part ways from the Orthodox Yudkowskyan position, I do find that position intellectually consistent, with conclusions that follow neatly from premises.  The Orthodox, in particular, can straightforwardly answer all four of my questions above:</p>



<ol>
<li>AI is manifestly different from any other technology humans have ever created, because it could become to us as we are to orangutans;</li>



<li>a six-month pause is very far from sufficient but is better than no pause;</li>



<li>we’ll know that it’s safe to scale when (and only when) we understand our AIs so deeply that we can <em>mathematically explain</em> why they won’t do anything bad; and</li>



<li>GPT-4 is <em>extremely</em> impressive—that’s why it’s so terrifying!</li>
</ol>



<p>On the other hand, I’m deeply confused by the people who signed the open letter, <em>even though they continue to downplay or even ridicule GPT’s abilities, as well as the “sensationalist” predictions of an AI apocalypse.</em>  I’d feel less confused if such people came out and argued explicitly: “yes, we should also have paused the rapid improvement of printing presses to avert Europe’s religious wars.  Yes, we should’ve paused the scaling of radio transmitters to prevent the rise of Hitler.  Yes, we should’ve paused the race for ever-faster home Internet to prevent the election of Donald Trump.  And yes, we should’ve trusted our governments to manage these pauses, to foresee brand-new technologies’ likely harms and take appropriate actions to mitigate them.”</p>



<p>Absent such an argument, I come back to the question of whether generative AI <em>actually</em> poses a near-term risk that’s totally unparalleled in human history, or perhaps approximated only by the risk of nuclear weapons.  After sharing an email from his partner, Eliezer rather movingly writes:</p>



<blockquote>
<p>When the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she’s not going to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.</p>
</blockquote>



<p>Look, I too have a 10-year-old daughter and a 6-year-old son, and I wish to see them grow up.  But the causal story that starts with a GPT-5 or GPT-4.5 training run, and ends with the sudden death of my children and of all carbon-based life, still has a few too many gaps for my aging, inadequate brain to fill in.  I can complete the story in my imagination, of course, but I could equally complete a story that starts with GPT-5 and ends with the world saved from various natural stupidities.  For better or worse, I lack the “Bayescraft” to see why the first story is <em>obviously</em> 1000x or 1,000,000x likelier than the second one.</p>



<p>But, I dunno, maybe I’m making the greatest mistake of my life?  Feel free to try convincing me that I should sign the letter.  But let’s see how polite and charitable everyone can be: hopefully a six-month moratorium won’t be needed to solve the alignment problem of the <em>Shtetl-Optimized</em> comment section.</p>

		
				
				<p>
					<small>
						This entry was posted
												on Thursday, March 30th, 2023 at 1:23 am						and is filed under <a href="https://scottaaronson.blog/?cat=8" rel="category">The Fate of Humanity</a>.
						You can follow any responses to this entry through the <a href="https://scottaaronson.blog/?feed=rss2&amp;p=7174">RSS 2.0</a> feed.

													You can <a href="#respond">leave a response</a>, or <a href="https://scottaaronson.blog/wp-trackback.php?p=7174" rel="trackback">trackback</a> from your own site.

						
					</small>
				</p>

			</div></div>
  </body>
</html>
