<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://runtipi.io">Original</a>
    <h1>Runtipi: Docker-based home server management</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
	

<p> Originally posted 2024-03-31</p>
<p> Tagged: <a href="https://www.moderndescartes.com/essays/tags/software_engineering">software engineering</a>, <a href="https://www.moderndescartes.com/essays/tags/machine_learning">machine learning</a>, <a href="https://www.moderndescartes.com/essays/tags/strategy">strategy</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr/>

<p>LLMs are really expensive to run, computationally speaking. I think
you may be surprised by the order of magnitude difference.</p>
<p>While working at Lilac, I coined a phrase “small data, big compute”
to describe this pattern, and used it to drive engineering
decisions.</p>
<h2 id="arithmetic-intensity">Arithmetic intensity</h2>
<p>Arithmetic intensity is a concept <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf">popularized
by NVidia</a> that measures a very simple ratio: how many arithmetic
operations are executed per byte transferred?</p>
<p>Consider a basic business analyst query:
<code>SELECT SUM(sales_amount) FROM table WHERE time &lt; end_range AND time &gt;= start_range</code>.
This query executes 1 addition for each 4-byte floating point number it
processes, for an arithmetic intensity of 0.25. However, the bytes
corresponding to <code>sales_amount</code> are usually interleaved with
the bytes for <code>time</code> and <code>row_id</code> and everything
else in the table, so only 1-10% of the bits read from disk are actually
relevant to the calculation, for a net arithmetic intensity of 0.01.</p>
<p>Is 0.01 good or bad? Well, computers can read data from disk at
roughly 1 GiB per second, or 250M floats per second; they can compute at
roughly 8-16 FLOPs per cycle, with 3GHz clock speed = 25-50B float ops
per second. Computers therefore have a 100:1 available ratio of compute
to disk. Any code with an arithmetic intensity of less than 100 is
underutilizing the CPU.</p>
<p>In other words, your typical business analyst query is horrendously
underutilizing the computer, by a factor of about 10,000x. This mismatch
is why there exists a $100B market for database companies and
technologies that can optimize these business queries (Spark, Parquet,
Hadoop, MapReduce, Flume, etc.). They do so by using columnar databases
and on-the-fly compression techniques like <a href="https://duckdb.org/2022/10/28/lightweight-compression.html">run-length
encoding, bit-packing, and delta compression</a>, which trade increased
compute for more effective use of bandwidth. The result is blazing fast
analytics queries that actually fully utilize the 100:1 available ratio
of compute to disk.</p>
<p>By the way, GPU cores have a compute-memory bandwidth ratio of around
100 - they are not fundamentally different from computers in this
regard. They are merely simpler, smaller in die area, and thus easier to
pack thousands of copies to a chip.</p>
<p>How many FLOPs do we spend per byte of user data in an LLM? Well…
consider the popular 7B model size. As a rough approximation, let’s say
each parameter-byte interaction results in 1 FLOP. You could quibble
about bytes vs. tokens or multiply vs. add and the cost of
exponentiation. But really, it doesn’t matter because the arithmetic
intensity is <span>\(10^{10}\)</span> operations per
byte processed. Other larger LLMs can go to <span>\(10^{14}\)</span>. This is at least 8 orders of
magnitude more expensive per byte than the business analyst query!
Ironically, LLMs end up bandwidth-limited despite the insane arithmetic
intensity quoted above - because if you also count the parameters of the
model in the “bytes transferred” denominator, then LLM arithmetic
intensity is only ~1, well under the ~100 that GPUs are capable of.</p>
<p>Convnets for image processing, by the way, have an arithmetic
intensity of <span>\(10^4\)</span> - <span>\(10^5\)</span>. It’s large but not unreasonable,
which is why they’ve found many applications in factory QC, agriculture,
satellite imagery processing, etc..</p>
<p>Needless to say, this insane arithmetic intensity breaks just about
every assumption and expectation that’s been baked into the way we think
about software for the past twenty years.</p>
<h2 id="technical-implications">Technical implications</h2>
<h3 id="massive-budget-for-bloat">Massive budget for bloat</h3>
<p>Ordinarily, inefficiencies in per-item handling can add up to a
significant cost. This includes things like network bandwidth/latency,
preprocessing of data in a slow language like Python, HTTP request
overhead, unnecessary dependencies, and so on.</p>
<p>LLMs are so expensive that everything else is peanuts. There is a lot
more budget for slop and I fully expect businesses to use this budget. I
am sorry to the people who are frustrated with the increasing bloat of
the modern software stack - LLMs will bring on yet another expansionary
era of bloat.</p>
<p>At Lilac, we ended up building a per-item progress saver into our
<code>dataset.map</code> call, because it was honestly a small cost,
relative to the fees that our users were incurring while making API
calls to OpenAI. In comparison, HuggingFace’s <code>dataset.map</code>
doesn’t implement checkpointing, because it would be an enormous waste
of time and compute and disk space to checkpoint the result of a trivial
arithmetic operation.</p>
<h3 id="latency-batching-tradeoffs">Latency-batching tradeoffs</h3>
<p>The arithmetic intensity of LLMs is roughly <span>\(\frac{nm}{n + m}\)</span>, where n = input bytes
and m = model bytes. Since <span>\(m \gg n\)</span>,
this implies that arithmetic intensity is proportional to <span>\(n\)</span>. Increasing batch size is thus a free
win, up to the point where the GPU is compute-bound rather than
bandwidth-bound.</p>
<p>For real-time use cases like chatbots, scale is king! When you have
thousands of queries per second, it becomes easy to wait 50 milliseconds
for a batch of user queries to accumulate, and then execute them in a
single batch. If you only have one query per second, you are in a
situation where you will either get poor GPU utilization (expensive
hardware goes to waste), or users will have to wait multiple seconds for
enough accumulated queries to make a batch.</p>
<p>For offline use cases like document corpus embedding/transformation,
we can automatically get full utilization through internal batching of
the corpus. Because GPUs are the expensive part, I expect organizations
to implement a queueing system to maximize usage of GPUs around the
clock, possibly even intermingling offline jobs with real-time
computation.</p>
<h3 id="no-need-for-distributed-systems">No need for distributed
systems</h3>
<p>Unless you work at a handful of companies that train LLMs from
scratch, you will not have the budget to operate LLMs on “big data”. A
single 1TB harddrive can store enough text data to burn 10 million
dollars in <a href="https://help.openai.com/en/articles/7127956-how-much-does-gpt-4-cost">GPT4
API calls</a>!</p>
<p>As a result, most business use cases for LLMs will inevitably operate
on small data - say, &lt;1 million rows.</p>
<p>The software industry has spent well over a decade learning how to
build systems that scale across trillions of rows and thousands of
machines, with the tradeoff that you would wait at least 30s per
invocation. We got used to this inconvenience because it let us turn a
10 day single-threaded job into a 20 minute distributed job.</p>
<p>Now, faced with the daunting prospect of a mere 1 million rows, all
of that is unnecessary complexity. Users deserve sub-second overheads
when doing non-LLM computations on such small data. Lilac utilizes
DuckDB to blast all cores on a single machine to compute basic summary
statistics for every column in the user’s dataset, in less than a second
- a luxury that we can afford because of our increased budget for
slop!</p>
<h3 id="minimal-viable-fine-tune">Minimal viable fine-tune</h3>
<p>As a corollary of “compute cost dominates all”, any and all ways to
optimize compute cost will be utilized. We will almost certainly see a
relentless drive towards specialization of cheaper fine-tuned models for
every conceivable use case. Stuff like <a href="https://arxiv.org/abs/2302.01318">speculative decoding</a> shows
just how expensive the largest LLMs are - you can productively run a
smaller LLM to try and predict the larger LLM’s output, in real
time!</p>
<p>In between engineering optimizations, fine-tuning/research
breakthroughs, and increased availability of massively parallel hardware
optimized for LLMs, the cost for any particular performance point will
decrease significantly - some people claim 4x every year, which sounds
aggressive but not even that unreasonable - 1.5x each from hardware,
research, and engineering optimizations gets you close to ~4x.</p>
<p>I expect there to be a good business in drastically reducing compute
costs by making it very easy to fine-tune a minimal viable model for a
specific purpose.</p>
<h2 id="business-implications">Business implications</h2>
<h3 id="data-egress-is-not-a-moat">Data egress is not a moat</h3>
<p>Cloud providers invest a lot of money into onboarding customers, with
the knowledge that once they’re inside, it becomes very expensive to
unwind all of the business integrations they’ve built. Furthermore, it
becomes very expensive to even try to diversify into multiple clouds,
because data egress outside of the cloud is <a href="https://www.hostdime.com/blog/data-egress-fees-cloud/">stupidly
expensive</a>. This is all part of an intentional strategy to make
switching harder.</p>
<p>Yet, the insane cost of LLMs means that data egress costs are a
relatively small deal. As a result, I expect that…</p>
<h3 id="a-new-gpu-cloud-will-emerge">A new GPU cloud will emerge</h3>
<p>Because of the ease with which data can flow between clouds, I expect
a new cloud competitor, focused on cheap GPU compute. Scale will be king
here, because increased scale results in negotiating power for GPU
purchase contracts, investments into GPU reliability, investments into
engineering tricks to maximize GPU utilization, and improved latency for
realtime applications. Modal, Lambda, and NVidia seem like potential
cloud winners here, but the truth is that we’re all winners, because
relentless competition will drive down GPU costs for everyone.</p>
<h3 id="attack-defense">Attack &gt; defense</h3>
<p>A certain class of user-generated content will become a Turing Arena
of sorts, where LLMs will generate fake text (think Amazon product
reviews or Google search result spam or Reddit commenter product/service
endorsements), and LLMs will try to detect LLM-generated text. I think
it’s a reasonable guess that LLMs will only be able to detect other LLMs
of lesser quality.</p>
<p>Unfortunately for the internet, I think attack will win over the
defense. The reason is safety in numbers.</p>
<p>A small number of attackers will have the resources to use the most
expensive LLMs to generate the most realistic looking fake reviews,
specifically in categories where the profit margins are highest (think
“best hotel in manhattan” or “best machu picchu tour”). However, a much
larger number of attackers will have moderate resources to use
medium-sized LLMs to generate a much larger volume of semi-realistic
fake reviews. The defense, on the other hand, has to scale up LLMs to
run on all user-generated content, and realistically they will only be
able to afford running medium or small LLMs to do so. Dan Luu’s <a href="https://danluu.com/diseconomies-scale/">logorrhea on the
diseconomies of scale</a> is exactly the right way to think here.</p>
<p>Eventually, I think it will actually push some sort of in-person
notarization or other reputation-based system to finally become a
reality - the physical logistics will eventually become cheaper than
running that many LLMs at scale. I won’t endorse anything cryptocurrency
related, but it’s clear that Sam Altman’s Worldcoin saw this eventuality
coming many years ago.</p>
<h2 id="conclusion">Conclusion</h2>
<p>“Small data, big compute” allowed us to optimize for a certain class
of dataset and take certain shortcuts. The Lilac team will be <a href="https://www.databricks.com/blog/lilac-joins-databricks-simplify-unstructured-data-evaluation-generative-ai">joining
Databricks</a> and I look forward to continuing to build systems
tailored to the unusual needs of LLMs!</p>


    </div>
</div></div>
  </body>
</html>
