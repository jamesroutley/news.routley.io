<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.sscardapane.it/alice-book">Original</a>
    <h1>Alice&#39;s adventures in a differentiable wonderland</h1>
    
    <div id="readability-page-1" class="page"><div>
      

<div id="main" role="main">
  <article itemscope="" itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Book: Aliceâ€™s Adventures in a differentiable wonderland"/>
    
    
    

    <section itemprop="text">
      

<p><img src="https://www.sscardapane.it/assets/alice/Alice.png"/></p>



<p><strong>Neural networks</strong> surround us, in the form of large language models, speech transcription systems, molecular discovery algorithms, robotics, and much more. Stripped of anything else, neural networks are compositions of <strong>differentiable primitives</strong>, and studying them means learning how to program and how to interact with these models, a particular example of what is called <a href="https://www.facebook.com/yann.lecun/posts/10155722686332143">differentiable programming</a>.</p>

<p>This primer is an introduction to this fascinating field imagined for someone, like Alice, who has just ventured into this strange <em>differentiable</em> wonderland. I overview the basics of optimizing a function via automatic differentiation, and a selection of the most common designs for handling sequences, graphs, texts, and audios. The focus is on a intuitive, <strong>self-contained introduction</strong> to the most important design techniques, including convolutional, attentional, and recurrent blocks, hoping to bridge the gap between theory and code (PyTorch and JAX) and leaving the reader capable of understanding some of the most advanced models out there, such as large language models (LLMs) and multimodal architectures.</p>

<h3 id="table-of-contents">Table of contents</h3>

<p> 
The book is currently in a draft form and available for feedback and beta reading from arXiv: <a href="https://arxiv.org/abs/2404.17625">arXiv preprint 2404.17625</a>.
</p>

<div>
<ol>
<li> Foreword and introduction</li>
<li> Mathematical preliminaries</li>
<li> Datasets and losses</li>
<li> Linear models</li>
<li> Fully-connected layers</li>
<li> Automatic differentiation</li>
<li> Convolutive layers</li>
<li> Convolutions beyond images</li>
<li> Scaling up models</li>
<li> Transformer models</li>
<li> Transformers in practice</li>
<li> Graph layers</li>
<li> Recurrent layers</li>
<li> Appendix A: Probability theory</li>
<li> Appendix B: Universal approximation in 1D</li>
</ol>
</div>

<h3 id="additional-chapters">Additional chapters</h3>

<p>I will publish here additional chapters on more advanced material that I could not fit into the first volume. Eventually, I hope these will be part of a second volume. More probably, they will languish here forever.</p>

<ol>
  <li><strong>Model re-use</strong> (including parameter-efficient fine-tuning and model merging).</li>
  <li><strong>Density estimation and generative modelling</strong>.</li>
  <li><strong>Conditional computation</strong> (mixture-of-experts, early exits).</li>
  <li><strong>Metric and self-supervised learning</strong>.</li>
  <li><strong>Debugging and understanding the models</strong>.</li>
</ol>

    </section>
  </article>
</div>

    </div></div>
  </body>
</html>
