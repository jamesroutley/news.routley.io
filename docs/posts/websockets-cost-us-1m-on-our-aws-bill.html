<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.recall.ai/post/how-websockets-cost-us-1m-on-our-aws-bill">Original</a>
    <h1>WebSockets cost us $1M on our AWS bill</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p>IPC is something that is rarely top-of-mind when it comes to optimising cloud costs. But it turns out that if you IPC 1TB of video per second on AWS it can result in enormous bills when done inefficiently. </p>
<p>Join us in this deep dive where we unexpectedly discover how using WebSockets over loopback was ultimately costing us $1M/year in AWS spend and the quest for an efficient high-bandwidth, low-latency IPC.</p>
<p>Recall.ai powers meeting bots for hundreds of companies. We capture millions of meetings per month, and operate enormous infrastructure to do so. </p>
<p>We run all this infrastructure on AWS. Cloud computing is enormously convenient, but also notoriously expensive, which means performance and efficiency is very important to us. </p>
<p><img alt="" src="https://cdn.prod.website-files.com/633275e23914a500db413038/671c2e403c908f07649aa600_60114ba1.jpeg"/></p>
<p>In order to deliver a cost-efficient service to our customers, we&#39;re determined to squeeze every ounce of performance we can from our hardware.</p>
<p>We do our video processing on the CPU instead of on GPU, as GPU availability on the cloud providers has been patchy in the last few years. Before we started our optimization efforts, our bots generally required 4 CPU cores to run smoothly in all circumstances. These 4 CPU cores powered all parts of the bot, from the headless Chromium used to join meetings to the real-time video processing piplines to ingest the media. </p>
<p>We set a goal for ourselves to cut this CPU requirement in half, and thereby cut our cloud compute bill in half. </p>
<p>A lofty target, and the first step to accomplish it would be to profile our bots.</p>
<h2>Our CPU is being spent doing what??</h2>
<p>Everyone knows that video processing is very computationally expensive. Given that we process a ton of video, we initially expected the majority of our CPU usage to be video encoding and decoding.</p>
<p>We profiled a sample of running bots, and came to a shocking realization. The majority of our CPU time was <em>actually</em> being spent in two functions: <code>__memmove_avx_unaligned_erms</code> and <code>__memcpy_avx_unaligned_erms</code>.</p>
<p>Let&#39;s take a brief detour to explain what these functions do. </p>
<p><code>memmove</code> and <code>memcpy</code> are both functions in the C standard library (glibc) that copy blocks of memory. <code>memmove</code> handles a few edge-cases around copying memory into overlapping ranges, but we can broadly categorize both these functions as &#34;copying memory&#34;.</p>
<p>The <code>avx_unaligned_erms</code> suffix means this function is specifically optimized for systems with Advanced Vector Extensions (AVX) support and is also optimized for unaligned memory access. The <code>erms</code> part stands for Enhanced REP MOVSB/STOSB, which are optimizations in recent Intel processors for fast memory movement. We can broadly categorize the suffix to mean &#34;a faster implementation, for this specific processor&#34;</p>
<p>In our profiling, we discovered that by far, the biggest callers of these functions were in our Python WebSocket client that was receiving the data, followed by Chromium&#39;s WebSocket implementation that was sending the data.</p>
<h2>An expensive set of sockets...</h2>
<p>After pondering this, the result started making more sense. For bots that join calls using a headless Chromium, we needed a way to transport the raw decoded video out of Chromium&#39;s Javascript environment and into our encoder.</p>
<p>We originally settled on running a local WebSocket server, connecting to it in the Javascript environment, and sending data over that channel.</p>
<p>WebSocket seemed like a decent fit for our needs. It was &#34;fast&#34; as far as web APIs go, convenient to access from within the JS runtime, supported binary data, and most importantly was already built-in to Chromium.</p>
<p>One complicating factor here is that raw video is surprisingly high bandwidth. A single 1080p 30fps video stream, in uncompressed I420 format, is <code>1080 * 1920 * 1.5 (bytes per pixel) * 30 (frames per second) = 93.312 MB/s</code></p>
<p>Our monitoring showed us that at scale, the p99 bot receives 150MB/s of video data.  </p>
<p>That&#39;s a lot of data to move around! </p>
<p>The next step was to figure out what <em>specifically</em> was causing the WebSocket transport to be so computationally expensive. We had to find the root cause, in order to make sure that our solution would sidestep WebSocket&#39;s pitfalls, and not introduce new issues of it&#39;s own.</p>
<p>We read through the <a href="https://datatracker.ietf.org/doc/html/rfc6455">WebSocket RFC</a>, and <a href="https://chromium.googlesource.com/chromium/src/net/+/master/websockets/">Chromium&#39;s WebSocket implementation</a>, dug through our profile data, and discovered two primary causes of slowness: fragmentation, and masking.</p>
<h3>Fragmentation</h3>
<p>The WebSocket specification supports fragmenting messages. This is the process of splitting a large message across several WebSocket frames. </p>
<p>According to <a href="https://datatracker.ietf.org/doc/html/rfc6455#section-5.4">Section 5.4 of the WebSocket RFC</a>):</p>
<blockquote>
<p>The primary purpose of fragmentation is to allow sending a message that is of unknown size when the message is started without having to buffer that message.  If messages couldn&#39;t be fragmented, then an endpoint would have to buffer the entire message so its length could be counted before the first byte is sent.  With fragmentation, a server or intermediary may choose a reasonable size buffer and, when the buffer is full, write a fragment to the network.</p>
<p>A secondary use-case for fragmentation is for multiplexing, where it is not desirable for a large message on one logical channel to monopolize the output channel, so the multiplexing needs to be free to split the message into smaller fragments to better share the output channel. (Note that the multiplexing extension is not described in this document.)</p>
</blockquote>
<p>Different WebSocket implementations have different standards </p>
<p>Looking into the Chromium <a href="https://chromium.googlesource.com/chromium/src.git/+/e77e5b7d482c12f3b74e9848f256af0e0b8c910d/services/network/websocket.cc#57">WebSocket source code</a>, messages larger than 131KB will be fragmented into multiple WebSocket frames. </p>
<p>A single 1080p raw video frame would be <code>1080 * 1920 * 1.5 = 3110.4 KB</code> in size, and therefore Chromium&#39;s WebSocket implementation would fragment it into 24 separate WebSocket frames. </p>
<p>That&#39;s a lot of copying and duplicate work!</p>
<h3>Masking</h3>
<p>The WebSocket specification <a href="https://datatracker.ietf.org/doc/html/rfc6455#section-5.1">also mandates</a> that data from client to server is masked.</p>
<blockquote>
<p>To avoid confusing network intermediaries (such as intercepting proxies) and for security reasons that are further discussed in Section 10.3, a client MUST mask all frames that it sends to the server</p>
</blockquote>
<p>Masking the data involves obtaining a random 32-bit masking key, and <code>XOR</code>-ing the bytes of the original data with the masking key in 32-bit chunks. </p>
<p>This has security benefits, because it prevents a client from controlling the bytes that appear on the wire. If you&#39;re interested in the precise reason why this is important, <a href="https://www.rfc-editor.org/rfc/rfc6455#section-10.3">read more here!</a></p>
<p>While this is great for security, the downside is masking the data means making an additional once-over pass over every byte sent over WebSocket -- insignificant for most web usages, but a meaningful amount of work when you&#39;re dealing with 100+ MB/s</p>
<h2>Quest for a cheaper transport!</h2>
<p>We knew we need to move away from WebSockets, so we began our quest to find a new mechanism to get data out of Chromium. </p>
<p>We realized pretty quickly that browser APIs are severely limited if we wanted something significantly more performant that WebSocket.</p>
<p>This meant we&#39;d need to fork Chromium and implement something custom. But this also meant that the sky was the limit for how efficient we could get.</p>
<p>We considered 3 options: raw TCP/IP, Unix Domain Sockets, and Shared Memory:</p>
<h3>TCP/IP</h3>
<p>Chromium&#39;s WebSocket implementation, and the WebSocket spec in general, create some especially bad performance pitfalls.</p>
<p>How about we go one level deeper and add an extension to Chromium to allow us to send raw TCP/IP packets over the loopback device? </p>
<p>This would bypass the issues around WebSocket fragmentation and masking, and this would be pretty straightforward to implement. The loopback device would also introduce minimal latency.</p>
<p>There were a few drawbacks however. Firstly, the maximum size for TCP/IP packets is much smaller than the size of our raw video frames, which means we still run into fragmentation.</p>
<p>In a typical TCP/IP network connected via ethernet, the standard MTU (<a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">Maximum Transmission Unit</a>) is 1500 bytes, resulting in a TCP MSS (<a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">Maximum Segment Size</a>) of 1448 bytes. This is much smaller than our 3MB+ raw video frames. </p>
<p>Even the theoretical maximum size of a TCP/IP packet, 64k, is much smaller than the data we need to send, so there&#39;s no way for us to use TCP/IP without suffering from fragmentation.</p>
<p>There was another issue as well. Because the <a href="https://docs.recall.ai/docs/bot-fundamentals">Linux networking stack</a> runs in kernel-space, any packets we send over TCP/IP need to be copied from user-space into kernel-space. This adds significant overhead as we&#39;re transporting a high volume of data.</p>
<h3>Unix Domain Sockets</h3>
<p>We also explored exiting the networking stack entirely, and using good old Unix domain sockets. </p>
<p>A classic choice for IPC, and it turns out Unix domain sockets can <a href="https://unix.stackexchange.com/questions/326510/local-unix-socket-rough-idea-of-throughput">actually be pretty fast</a>.</p>
<p>Most importantly however, Unix domain sockets are a native part of the Linux operating system we run our bots in, and there are pre-existing functions and libraries to push data through Unix sockets.</p>
<p>There is one con however. To send data through a Unix domain socket, it needs to be copied from user-space to kernel-space, and back again. With the volume of data we&#39;re working with, this is a decent amount of overhead.</p>
<h3>Shared Memory</h3>
<p>We realized we could go one step further. Both TCP/IP and Unix Domain Sockets would at minimum require copying the data between user-space and kernel-space. </p>
<p>With a bit of DIY, we could get even more efficient using Shared Memory.</p>
<p>Shared memory is memory that can be simultaneously accessed by multiple processes at a time. This means that our Chromium could write to a block of memory, which would then be read directly by our video encoder with no copying at all required in between.</p>
<p>However there&#39;s no standard interface for transporting data over shared memory. It&#39;s not a standard like TCP/IP or Unix Domain sockets. If we went the shared memory route, we&#39;d need to build the transport ourselves from the ground up, and there&#39;s a lot that could go wrong.</p>
<p>Glancing at our AWS bill gave us the resolve we needed to push forward. Shared memory, for maximum efficiency, was the way to go.</p>
<h2>Sharing is caring (about performance)</h2>
<p>As we need to continuously read and write data serially into our shared memory, we settled on a ring buffer as our high level transport design.</p>
<p><a href="https://crates.io/crates/circular-queue">There</a> <a href="https://crates.io/crates/ringbuf">are</a> <a href="https://crates.io/crates/ringbuffer">quite</a> <a href="https://crates.io/crates/ring_queue">a few</a> <a href="https://docs.rs/dasp_ring_buffer/0.11.0/dasp_ring_buffer/">ringbuffer</a> implementations in the Rust community, but we had a few specific requirements for our implementation:</p>
<ul>
<li><strong>Lock-free</strong>: We need consistent latency and no jitter, otherwise our real-time video processing would be disrupted.</li>
<li><strong>Multiple producer, single consumer</strong>: We have multiple chromium threads writing audio and video data into the buffer, and a single thread in the media pipline consuming this data.</li>
<li><strong>Dynamic Frame Sizes</strong>: Our ringbuffer needed to support audio packets, as well as video frames of different resolutions, meaning the size of each datum could vary drastically.</li>
<li><strong>Zero-Copy Reads</strong>: We want to avoid copies as much as possible, and therefore want our media pipeline to be able to read data out of the buffer without copying it. </li>
<li><strong>Sandbox Friendlyness</strong>: Chromium threads are sandboxed, and we need them to be able to access the ringbuffer easily.</li>
<li><strong>Low Latency Signalling</strong>: We need our Chromium threads to be able to signal to the media pipeline when new data is available, or when buffer space is available. </li>
</ul>
<p>We evaluated the off-the-shelf ringbuffer implementations, but didn&#39;t find one that fit our needs... so we decided to write our own!</p>
<p><img alt="" src="https://cdn.prod.website-files.com/633275e23914a500db413038/671c2e403c908f07649aa5fd_0631868f.jpeg"/></p>
<p>The most non-standard part of our ring-buffer implementation is our support for zero-copy reads. Instead of the typical two-pointers, we have three pointers in our ring buffer:</p>
<ul>
<li>write pointer: the next address to write to</li>
<li>peek pointer: the address of the next frame to read</li>
<li>read pointer: the address where data can be overwritten</li>
</ul>
<p>To support zero-copy reads we feed frames from the peek pointer into our media pipeline, and only advance the read pointer when the frame has been fully processed.</p>
<p>This means that it&#39;s safe for the media pipeline to hold a reference to the data inside the ringbuffer, since that reference is guaranteed to be valid until the data is fully processed and the read pointer is advanced.</p>
<p>We use atomic operations to update the pointers in a thread-safe manner, and to signal that new data is available or buffer space is free we use a <a href="https://man7.org/linux/man-pages/man7/sem_overview.7.html">named semaphore</a>.</p>
<p>After implementing this ringbuffer, and deploying this into production with a few other optimizations, we were able to reduce the CPU usage of our bots by up to 50%. </p>
<p>This exercise in optimizing IPC for CPU efficiency reduced our AWS bill by over a million dollars per year, a huge impact and a really great use of time!</p></div></div></div></div>
  </body>
</html>
