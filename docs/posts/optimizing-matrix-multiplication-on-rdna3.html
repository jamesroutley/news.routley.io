<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://seb-v.github.io/optimization/update/2025/01/20/Fast-GPU-Matrix-multiplication.html">Original</a>
    <h1>Optimizing Matrix Multiplication on RDNA3</h1>
    
    <div id="readability-page-1" class="page"><div>
        <h2 id="introduction">Introduction</h2>

<p>Hi everyone !</p>

<p>In this post, I will share with you all the steps to write an optimized FP32 matrix multiplication on AMD RDNA3 GPU outperforming rocBLAS by 60%. I will cover some basics and explain all the optimizations I have implemented. This will be done in a iterative way in 8 differents Kernels.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph0.jpg" alt="Alt text"/></p><p>Figure 1: sneak peek of the performance results</p>
</div>

<p>I primary intended to work on this to deepen my understanding of RDNA3 and try out HIP and I felt like I needed to share what I learned doing this :).</p>

<p>Few things I like to say before we start :</p>
<ul>
  <li>All the information I used comes from the publicly available ISA guide<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup></li>
  <li>I don’t intend to re-implement or replace rocBLAS</li>
  <li>I only focused on 4096x4096 matrices single precision (FP32) matrix multiplication for the sake of simplicity.</li>
  <li>All my tests were done on Windows 11 with a AMD Radeon 7900 XTX.</li>
</ul>

<p>That being said, let’s start !</p>



<p>There is a lot of research happening on the way to improve the performance of matrix multiplication nowadays. Being a core algorithm in ML applications, any FLOPS we can exploit is golden.</p>

<p>Before proceeding, let’s recall the basics of matrix multiplication. Given two matrices:</p>
<ul>
  <li>\(A\) of size \(M,K\)</li>
  <li>\(B\) of size \(K,N\)</li>
</ul>

<p>Their product, \(C\), is computed as follows:</p>

<p>
$$\large C_{ij} = \sum_{k=0}^{K-1} A_{ik} \cdot B_{kj}$$

$$ i \in [0, M-1] $$
$$ j \in [0, N-1] $$
</p>
<p>where \(C\) is the resulting matrix of size \(M,N\).</p>

<p>For each output value of matrix C, we compute the dot product between the rows of matrix A and the columns of matrix B.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph1.jpg" alt="Alt text"/></p><p>Figure 2: example for the first element of C</p>
</div>

<p>In terms of complexity, we have \(\large O(n^3)\) computational complexity and \(\large O(n^2)\) memory accesses.
If we don’t think about architectural details, this is clearly a compute bound problem and our goal will be to be compute bound on the GPU.</p>

<p>Let’s say we manage to write the best implementation possible for the 7900 XTX. How fast could it run ? To answer this questions we need to look a bit at RDNA3 architecture.</p>

<p>RDNA3 GPUs are made of arrays of WorkGroup Processors (WGP). Every WGP are split into 2 Compute Units (CUs), themself split into 2 SIMDs. A SIMD handles the work of multiple threads organized in waves (or warps for CUDA folks) and has a set of components to do some work (like arithmetic operations). For Floating point operations, there are two 32 way VALU units.</p>


<div>
  <p><img src="https://seb-v.github.io/assets/images/graph2.jpg" alt="Alt text"/></p><p>Figure 3: simplified representation of WGPs</p>
</div>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph3.jpg" alt="Alt text" width="280"/></p><p>Figure 4: simplified representation of a single SIMD</p>
</div>

<p>We can compute our theoritical floating point operation per second with this formula:</p><p>

\[\large FLOPS = {freq}*{nbSIMD}*{flopsPerSIMD}\]

</p><p>Every SIMD can issue 2 Floating points intructions per cycle (one on each vALU unit). If we use FMA instructions (Fused Multiply Add), each SIMD can issue \(32*2*2=128\) floating point operations per cycle.
The 7900 XTX has 48 WGPs, that’s \(48*2*2=192\) SIMDs.</p><p>

\[\large FLOPS = {2500}*{10}^6*{192}*{128} \; \text{FLOP/s}\]

\[\large FLOPS = {61.44} \; \text{TFLOP/s}\]

</p><p>Our theoritical VRAM bandwidth is given by :</p><p>

\[\large BW = {rate}*{busWidth}/8\]

</p><p>The 7900 XTX uses GDDR6 with a 384-bit bus running at 20 Gbps.</p><p>

\[\large BW = {20}*{384}/8 = 960 \text{GB/s}\]

</p><p>If we go back to our 4096x4096 matrix multiplication, we essentially need to do \(\large 2*4096*4096*4096\) operations.
With a 61 TFLops implementation, it would take roughly <strong>2.23 ms</strong> to do the work and the bandwidth required to sustain this rate would be \(\large {4096*4096*4*3}/{2.23}*10^{-3} = 90.2 \text {GB/s}\).</p>

<p>Of course, these are oversimplified calculations as they totally ignore memory hierarchy but we see that the available bandwidth is sufficiently high so that we can increase the amount of data we read to be closer to compute bound.</p>



<p>Let’s start with a naive implementation like this :</p>

<figure><pre><code data-lang="cuda"><span>__global__</span> <span>void</span> <span>kernel1_naive</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>A</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>B</span><span>,</span> <span>float</span> <span>*</span><span>C</span><span>,</span> <span>int</span> <span>M</span><span>,</span> <span>int</span> <span>K</span><span>,</span> <span>int</span> <span>N</span><span>,</span> <span>float</span> <span>alpha</span><span>,</span> <span>float</span> <span>beta</span><span>)</span>
<span>{</span>
    <span>int</span> <span>row</span> <span>=</span> <span>blockIdx</span><span>.</span><span>y</span> <span>*</span> <span>blockDim</span><span>.</span><span>y</span> <span>+</span> <span>threadIdx</span><span>.</span><span>y</span><span>;</span>
    <span>int</span> <span>col</span> <span>=</span> <span>blockIdx</span><span>.</span><span>x</span> <span>*</span> <span>blockDim</span><span>.</span><span>x</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>;</span>
    <span>if</span> <span>(</span><span>row</span> <span>&lt;</span> <span>M</span> <span>&amp;&amp;</span> <span>col</span> <span>&lt;</span> <span>N</span><span>)</span>
    <span>{</span>
        <span>float</span> <span>acc_c</span> <span>=</span> <span>0.0</span><span>f</span><span>;</span> 
        <span>for</span> <span>(</span><span>int</span> <span>k</span> <span>=</span> <span>0</span><span>;</span> <span>k</span> <span>&lt;</span> <span>K</span><span>;</span> <span>++</span><span>k</span><span>)</span>
        <span>{</span>
            <span>acc_c</span> <span>+=</span> <span>A</span><span>[</span><span>row</span> <span>*</span> <span>K</span> <span>+</span> <span>k</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>k</span> <span>*</span> <span>N</span> <span>+</span> <span>col</span><span>];</span>
        <span>}</span>
        <span>C</span><span>[</span><span>row</span> <span>*</span> <span>N</span> <span>+</span> <span>col</span><span>]</span> <span>=</span> <span>alpha</span> <span>*</span> <span>acc_c</span> <span>+</span> <span>beta</span> <span>*</span> <span>C</span><span>[</span><span>row</span> <span>*</span> <span>N</span> <span>+</span> <span>col</span><span>];</span>
    <span>}</span>
<span>}</span></code></pre></figure>

<p>You will notice I am doing  \(\large C={alpha}*A*B+beta*C\) instead of \(\large C=A*B\) here. This is because it makes easier to compare with libraries like rocBLAS where matrix multiplications is provided by SGEMM functions (Single-Precision General Matrix Multiply).</p>

<p>We launch 4096x4096 threads with a blocksize of 16x16 and each thread compute the inner dot product described before.</p>

<p>The performance for this kernel is <strong>136 ms (1010.60 GFlops/s)</strong>. I know, that’s pretty bad and far off our 61 TFLops target.</p>



<p>Now that we have seen possibly the worst implementation in terms of performance, let’s look at the official rocBLAS implementation.</p>

<figure><pre><code data-lang="cpp">    <span>const</span> <span>int</span> <span>M</span> <span>=</span> <span>N</span><span>;</span>
    <span>const</span> <span>int</span> <span>K</span> <span>=</span> <span>N</span><span>;</span>
    <span>CHECK_ROCBLAS_STATUS</span><span>(</span><span>rocblas_sgemm</span><span>(</span>
        <span>handle</span><span>,</span>
        <span>rocblas_operation_none</span><span>,</span> <span>// Transpose option for A</span>
        <span>rocblas_operation_none</span><span>,</span> <span>// Transpose option for B</span>
        <span>M</span><span>,</span>                      <span>// Number of rows in A and C</span>
        <span>N</span><span>,</span>                      <span>// Number of columns in B and C</span>
        <span>K</span><span>,</span>                      <span>// Number of columns in A and rows in B</span>
        <span>&amp;</span><span>alpha</span><span>,</span>                 <span>// alpha</span>
        <span>d_a</span><span>,</span>                    <span>// Matrix A on the device</span>
        <span>M</span><span>,</span>                      <span>// Leading dimension of A</span>
        <span>d_b</span><span>,</span>                    <span>// Matrix B on the device</span>
        <span>K</span><span>,</span>                      <span>// Leading dimension of B</span>
        <span>&amp;</span><span>beta</span><span>,</span>                  <span>// beta</span>
        <span>d_c</span><span>,</span>                    <span>// Matrix C on the device</span>
        <span>M</span>                       <span>// Leading dimension of C</span>
        <span>));</span></code></pre></figure>

<p>As discussed before, I used <code>rocblas_sgemm</code> function with <code>alpha</code> and <code>beta</code> set to <code>1.0</code><sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup></p>

<p>The performance for this kernel is <strong>4.49 ms (30547 GFLOPs/s)</strong>. This is clearly much better than our kernel 1 but still far from our theoritical 61.4 TFlops/s.</p>

<p>By inspecting the ISA in RGP<sup id="fnref:3"><a href="#fn:3" rel="footnote" role="doc-noteref">3</a></sup>, I couldn’t find any dual issue instructions in the kernel (only <code>v_fmac_f32_e32</code>)<sup id="fnref:4"><a href="#fn:4" rel="footnote" role="doc-noteref">4</a></sup></p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph4.jpg" alt="Alt text"/></p><p>Figure 5: extract of rocBLAS ISA code</p>
</div>

<p>This is very surprising as this essentially means one of the VALU unit is sitting there doing nothing.</p>

<p>Considering this, the VALU utilization of this kernel is pretty impressive and almost 100 %. However, it’s really surprising we can’t exploit these dual issue instructions properly. I’ll come to that later.</p>



<p>The main issue with our naive kernel is that our inner loop directly accesses global memory. This is inefficient because fetching data from global memory has a high latency, typically on the order of hundreds of cycles. Since each memory read is followed by minimal computation (just one multiplication and one addition), the GPU struggles to hide this latency, even with a large number of concurrent threads. Moreover, the algorithm repeatedly reads the same rows and columns from global memory across different threads, leading to redundant memory accesses and further exacerbating the performance bottleneck.</p>

<p>A solution to this problem is to load the data once into faster local memory and then iterate efficiently over it with all the threads. On RDNA3, we have the Local Data Store (LDS), a high-speed, low-latency memory accessible by all threads within a workgroup.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph6b.jpg" alt="Alt text" width="600"/></p><p>Figure 6: simplified representation of the memory hierarchy</p>
</div>

<p>Since the LDS has a much smaller capacity than global memory, we need to use tiling to divide our problem into smaller sub-matrix multiplications. One way to facilitate this is to restructure the computation by moving the inner loop’s dot product to the outer loop. The key idea is to cache a column of matrix A and a row of matrix B, then perform the computation across the entire tile. This approach is more cache-efficient and significantly reduces memory access latency.</p>

<p>The pseudo code for our kernel 1 is :</p>

<figure><pre><code data-lang="text">for i from 0 to M - 1:                  # Loop over rows of A
    for j from 0 to N - 1:              # Loop over columns of B
        sum = 0                         
        for k from 0 to K - 1:          # Loop over columns of A / rows of B
            sum += A[i][k] * B[k][j]  
        end for
        C[i][j] = sum                   
    end for
end for</code></pre></figure>

<p>If we move the dot product to the outer loop, we have this :</p>

<figure><pre><code data-lang="text">for k from 0 to K - 1:                  # Outer loop over the shared dimension
    for i from 0 to M - 1:              # Loop over rows of A
        for j from 0 to N - 1:          # Loop over columns of B
            C[i][j] += A[i][k] * B[k][j] 
        end for
    end for
end for</code></pre></figure>

<p>Tiling in this form is straightforward: each workgroup operates on a tile and follows these steps: (\(BK\) is the batch size, ie number of rows/columns we load to the LDS)</p>

<figure><pre><code data-lang="text">Init c to 0 
While kId is less than N:
  # Load A and B to Tile As and Bs
  Load BK columns A to As
  Load BK rows to Bs
  Syncthreads
  # Accumulate results using LDS
  for k from 0 to BK
    c += As[threadIdx.y][k] * Bs[k][threadIdx.x]
  Syncthreads
  Increment kId by BK
end for
c[row][col]=c</code></pre></figure>

<p>If we choose a tile size of 32x32 and \(BK=32\), our new kernel looks like this:</p>

<figure><pre><code data-lang="cuda"><span>#define TILE_SIZE 32
</span><span>__global__</span> <span>void</span> <span>kernel2_lds</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>A</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>B</span><span>,</span> <span>float</span> <span>*</span><span>C</span><span>,</span> <span>int</span> <span>N</span><span>)</span>
<span>{</span>
    <span>__shared__</span> <span>float</span> <span>As</span><span>[</span><span>TILE_SIZE</span><span>][</span><span>TILE_SIZE</span><span>];</span>
    <span>__shared__</span> <span>float</span> <span>Bs</span><span>[</span><span>TILE_SIZE</span><span>][</span><span>TILE_SIZE</span><span>];</span>

    <span>int</span> <span>row</span> <span>=</span> <span>blockIdx</span><span>.</span><span>y</span> <span>*</span> <span>TILE_SIZE</span> <span>+</span> <span>threadIdx</span><span>.</span><span>y</span><span>;</span>
    <span>int</span> <span>col</span> <span>=</span> <span>blockIdx</span><span>.</span><span>x</span> <span>*</span> <span>TILE_SIZE</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>;</span>

    <span>float</span> <span>sum</span> <span>=</span> <span>0.0</span><span>f</span><span>;</span>

    <span>for</span> <span>(</span><span>int</span> <span>t</span> <span>=</span> <span>0</span><span>;</span> <span>t</span> <span>&lt;</span> <span>N</span><span>;</span> <span>t</span> <span>+=</span> <span>TILE_SIZE</span><span>)</span>
    <span>{</span>
        <span>Bs</span><span>[</span><span>threadIdx</span><span>.</span><span>y</span><span>][</span><span>threadIdx</span><span>.</span><span>x</span><span>]</span> <span>=</span> <span>B</span><span>[</span><span>N</span> <span>*</span> <span>(</span><span>threadIdx</span><span>.</span><span>y</span> <span>+</span> <span>t</span><span>)</span> <span>+</span> <span>col</span><span>];</span>
        <span>As</span><span>[</span><span>threadIdx</span><span>.</span><span>y</span><span>][</span><span>threadIdx</span><span>.</span><span>x</span><span>]</span> <span>=</span> <span>A</span><span>[</span><span>N</span> <span>*</span> <span>row</span> <span>+</span> <span>t</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>];</span>

        <span>__syncthreads</span><span>();</span>

        <span>for</span> <span>(</span><span>int</span> <span>k</span> <span>=</span> <span>0</span><span>;</span> <span>k</span> <span>&lt;</span> <span>TILE_SIZE</span><span>;</span> <span>k</span><span>++</span><span>)</span>
        <span>{</span>
            <span>sum</span> <span>+=</span> <span>As</span><span>[</span><span>threadIdx</span><span>.</span><span>y</span><span>][</span><span>k</span><span>]</span> <span>*</span> <span>Bs</span><span>[</span><span>k</span><span>][</span><span>threadIdx</span><span>.</span><span>x</span><span>];</span>
        <span>}</span>

        <span>__syncthreads</span><span>();</span>
    <span>}</span>

    <span>if</span> <span>(</span><span>row</span> <span>&lt;</span> <span>N</span> <span>&amp;&amp;</span> <span>col</span> <span>&lt;</span> <span>N</span><span>)</span>
    <span>{</span>
        <span>C</span><span>[</span><span>row</span> <span>*</span> <span>N</span> <span>+</span> <span>col</span><span>]</span> <span>=</span> <span>sum</span><span>;</span>
    <span>}</span>
<span>}</span></code></pre></figure>

<p><code>__syncthreads();</code> is required here to ensure that all threads in the workgroup can see the data loaded into the LDS and to synchronize before any updates are made to the data.</p>

<p>We also ensure that the contents of both matrices A and B are loaded into the LDS by rows rather than columns to avoid uncoalesced memory accesses.
Indeed, if we were to read by columns, each thread in a wave would access a non-contiguous memory region, resulting in multiple separate transactions and reduced efficiency as shown in the 2 diagrams below.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph5.jpg" alt="Alt text" width="400"/></p><p>Figure 7: coalesced loads for matrix A. A single 128 bytes memory transaction for all threads</p>
</div>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph6.jpg" alt="Alt text" width="600"/></p><p>Figure 8: non coalesced loads for matrix A. Multiple 32 bytes memory transactions for a single wave</p>
</div>

<p>From the ISA guide, <em>device memory is accessed through 32-, 64-, or 128-byte transactions, which must be naturally aligned. Maximizing memory throughput requires coalescing memory accesses across threads within a wave to minimize the number of transactions <sup id="fnref:5"><a href="#fn:5" rel="footnote" role="doc-noteref">5</a></sup>.</em></p>

<p>The performance for this kernel is <strong>34.2 ms (4017 GFlops/s)</strong>. 4 times faster than our naive kernel !</p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 2</strong></td>
      <td><strong>LDS tiling</strong></td>
      <td><strong>34.2059</strong></td>
      <td><strong>4017.99</strong></td>
      <td><strong>13.1 %</strong></td>
    </tr>
  </tbody>
</table>


<div>
  <p><img src="https://seb-v.github.io/assets/images/graph7.jpg" alt="Alt text" width="280"/></p><p>Figure 9 : stats taken from the instruction tab in RGP</p>
</div>

<p>If we look at the ISA in the instruction timing tab, we see a couple of interesting things :</p>
<ul>
  <li>the inner loop has been unrolled</li>
  <li>we are not using v_dual_fmac_f32, only v_fmac_f32 just like rocBLAS</li>
  <li>we get a consistent 90 cycles stall (not hidden) on these LDS loads (check the s_waitcnt lgkmcnt(X) instructions)</li>
</ul>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph8.jpg" alt="Alt text"/></p><p>Figure 10 : instruction timing</p>
</div>

<p>To understand what is happening, we need to quickly explain how SIMD scheduling works. During each clock cycle, the SIMD selects an instruction from a pool of wave to issue. A SIMD can manage up to 16 wavefronts in parallel. When we refer to occupancy, we are actually talking about the ratio of active wave to the theoretical maximum number of wave that a SIMD can support.
The more active wavefronts there are, the greater the likelihood that the SIMD can switch between wave, increasing the chances of hiding latency within individual wavefronts.<sup id="fnref:6"><a href="#fn:6" rel="footnote" role="doc-noteref">6</a></sup></p>

<p>If we go back to our case, we are likely having something like this :</p>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph9.jpg" alt="Alt text"/></p><p>Figure 11 : wavefront scheduling within a SIMD</p>
</div>

<p>Here, we have a high-occupancy kernel launching many waves in parallel, all contending for access to the LDS. Since the time taken by our VALU operations is shorter than the LDS latency, the latency cannot be hidden, even with additional threads. This results in both LDS bandwidth congestion and resource waste due to latency.</p>

<p>One way to address this issue is by increasing the arithmetic intensity of our kernel, ensuring that the VALU operations per wave take longer than the LDS memory reads.</p>



<p>Now, we want to increase the arithmetic complexity of our kernel. This means having each thread perform more computations. Essentially, we aim to increase the ratio of computation to data read.
One way to achieve this is to compute a small output tile per thread—for example, an 8x8 tile. To do this, we introduce an additional level of tiling.</p>

<p>Each thread will be responsible for producing a small tile of the output matrix. We can cache the contents of matrices \(A\) and \(B\) into registers to enable very low-latency access. However, registers are limited on the GPU, with 1536 VGPRs (Vector General-Purpose Registers) available per SIMD and a maximum of 256 registers per kernel. Increasing register usage means we won’t be able to launch as many waves per SIMD, effectively reducing occupancy. However, this shouldn’t be an issue if we can maximize utilization of the SIMD’s VALUs (Vector Arithmetic Logic Units) with just a few waves.</p>

<p>Now, let’s look at the different levels of tiling:</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph10.jpg" alt="Alt text" width="500"/></p><p>Figure 12 : tiling levels</p>
</div>

<ul>
  <li>Each thread now outputs a 4x4 block (Thread Tile).</li>
  <li>Since a wave consists of 32 threads, we organize them into a 8x4 block, making a single wave responsible for outputting a 32×16 tile.</li>
  <li>Given that we have 256 threads per workgroup (i.e., 8 waves), we arrange them into a 2×4 grid of Wave Tiles.</li>
  <li>Each wave iterates over a 2x2 grid to cover the entire Wave Tile.</li>
</ul>

<p>Essentially, it means each thread will now be responsible to compute a 8x8 output tile.</p>

<p>Our kernel parameters looks like this:</p>

<figure><pre><code data-lang="cuda">    <span>#define BLOCK_SIZE 256
</span>    <span>// Block Tile size</span>
    <span>constexpr</span> <span>int</span> <span>BN</span> <span>=</span> <span>128</span><span>;</span>
    <span>constexpr</span> <span>int</span> <span>BM</span> <span>=</span> <span>128</span><span>;</span>

    <span>// Number of Row or column we read per batch</span>
    <span>constexpr</span> <span>int</span> <span>BK</span> <span>=</span> <span>8</span><span>;</span> 

    <span>// Thread Tile size . 4x4</span>
    <span>constexpr</span> <span>int</span> <span>TN</span> <span>=</span> <span>4</span><span>;</span>
    <span>constexpr</span> <span>int</span> <span>TM</span> <span>=</span> <span>4</span><span>;</span>

    <span>// A wave is a block of 8x4 of the output matrix</span>
    <span>constexpr</span> <span>int</span> <span>nbThreadXPerWave</span> <span>=</span> <span>8</span><span>;</span>
    <span>constexpr</span> <span>int</span> <span>nbThreadYPerWave</span> <span>=</span> <span>4</span><span>;</span>

    <span>// Number of waves in a block</span>
    <span>constexpr</span> <span>int</span> <span>nbWavesPerBlock</span> <span>=</span> <span>BLOCK_SIZE</span> <span>/</span> <span>32</span><span>;</span>

    <span>constexpr</span> <span>int</span> <span>WN</span> <span>=</span> <span>64</span><span>;</span>
    <span>constexpr</span> <span>int</span> <span>WM</span> <span>=</span> <span>BN</span> <span>*</span> <span>BM</span> <span>/</span> <span>nbWavesPerBlock</span> <span>/</span> <span>WN</span><span>;</span>

    <span>constexpr</span> <span>int</span> <span>nbIterWaveN</span> <span>=</span> <span>WN</span> <span>/</span> <span>(</span><span>nbThreadXPerWave</span> <span>*</span> <span>TN</span><span>);</span>
    <span>constexpr</span> <span>int</span> <span>nbIterWaveM</span> <span>=</span> <span>WM</span> <span>/</span> <span>(</span><span>nbThreadYPerWave</span> <span>*</span> <span>TM</span><span>);</span>

    <span>// LDS Tile</span>
    <span>__shared__</span> <span>float</span> <span>As</span><span>[</span><span>BK</span><span>][</span><span>BM</span><span>];</span>
    <span>__shared__</span> <span>float</span> <span>Bs</span><span>[</span><span>BK</span><span>][</span><span>BN</span><span>];</span>
    
    <span>// Column and row from A and B, stored into registers</span>
    <span>float</span> <span>A_col</span><span>[</span><span>nbIterWaveM</span> <span>*</span> <span>TM</span><span>];</span>
    <span>float</span> <span>B_row</span><span>[</span><span>nbIterWaveN</span> <span>*</span> <span>TN</span><span>];</span>

    <span>//Wave Tile (registers)</span>
    <span>float</span> <span>C_regs</span><span>[</span><span>TM</span> <span>*</span> <span>nbIterWaveM</span> <span>*</span> <span>TN</span> <span>*</span> <span>nbIterWaveN</span><span>]</span> <span>=</span> <span>{</span><span>0.0</span><span>f</span><span>};</span></code></pre></figure>

<p>The pseudo code for our new kernel:</p>

<figure><pre><code data-lang="text">    Initialize kId to 0
    While kId is less than N:
        # Loading Tile to LDS
        Load BK columns from A to As
        Load BK rows from B to Bs
        Syncthreads

        For k from 0 to BK - 1 do:
            Load col k of As to A_col
            Load row k of Bs to B_row

            # Wave Tile
            For idY from 0 to nbIterWaveM:
              For idX from 0 to nbIterWaveN:

                # Thread Tile
                For i from 0 to TM:
                  For j from 0 to TN:
                     x = idX * TN + j;
                     y = idY * TM + i;
                     C_regs[y][x] = A_col[y] * B_row[x]

        Syncthreads
        Increment kId by BK
    Write C_regs to C</code></pre></figure>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel3_registers.cpp">here</a></p>

<p>The performance for this kernel is <strong>6.03 ms (22777 GFlops/s)</strong>, 5 times faster than our previous kernel !</p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 3</strong></td>
      <td><strong>Register tiling</strong></td>
      <td><strong>6.0341</strong></td>
      <td><strong>22777.0</strong></td>
      <td><strong>74.6 %</strong></td>
    </tr>
  </tbody>
</table>



<p>We have a lower occupancy but VALU utilization has significantly increased.</p>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph11.jpg" alt="Alt text" width="300"/></p><p>Figure 13 : kernel 3 stats</p>
</div>

<p>The ISA looks good. We now have a lot of v_dual_fmac instructions—exactly what we wanted, even though some are still single fma.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph12.jpg" alt="Alt text"/></p><p>Figure 14 : kernel 3 instruction timing</p>
</div>

<p>Even though this is a significant improvement over Kernel 2, we can still see that we are waiting for the LDS. This is especially true for the first batch of ds_load instructions, where we observe more than 100 clock cycles of cumulative non-hidden latency as seen below :</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph13.jpg" alt="Alt text"/></p><p>Figure 15 : ds_load instructions latencies </p>
</div>

<p>Before diving into this, we need to first improve the way we read from global memory. According to RGP, this is now the biggest bottleneck in terms of performance.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph14.jpg" alt="Alt text"/></p><p>Figure 16 : gmem wait latency </p>
</div>

<p>Our cumulative latency for global memory waits exceeds 12 million clock cycles, which is four times more than the LDS load wait in the inner loop.</p>

<p>To further optimize performance, we will focus on better hiding the Global memory read latency.</p>



<p>With our current implementation, every wave must wait for global memory and then LDS write latency before doing any work. In a high-occupancy scenario, this shouldn’t be an issue if the GPU can find other waves to hide this latency. However, in practice, we often have multiple waves in the same state running simultaneously because we use a sync thread before and after reading from global memory.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph15.jpg" alt="Alt text"/></p><p>Figure 17 : several waves waiting for GMEM loads</p>
</div>

<p>One way to mitigate this is by using double buffering. We could allocate twice the memory and perform reads and writes to the LDS in parallel.</p>

<p>Alternatively, we could use intermediate registers to load data from global memory while working on the LDS, only writing to LDS just before it is needed. This ensures no waiting on global memory.</p>

<p>I prefer this approach for now, as I don’t want to introduce additional LDS pressure in the inner loop just yet.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph15b.jpg" alt="Alt text"/></p><p>Figure 18 : double buffering on GMEM loads</p>
</div>

<p>If we update our pseudo code, we now have :</p>

<figure><pre><code data-lang="text">    Initialize kId to 0
    # Load first batch before loop
    Load BK columns from A to As
    Load BK rows from B to Bs
    Syncthreads

    While kId is less than N:
        # Loading Tile to LDS
        Load BK columns from A to A_TMP (no wait)
        Load BK rows from B to B_TMP (no wait)

        For k from 0 to BK - 1 do:
            Load col k of As to A_col
            Load row k of Bs to B_row

            # Wave Tile
            For idY from 0 to nbIterWaveM:
              For idX from 0 to nbIterWaveN:

                # Thread Tile
                For i from 0 to TM:
                  For j from 0 to TN:
                     x = idX * TN + j;
                     y = idY * TM + i;
                     C_regs[y][x] = A_col[y] * B_row[x]

        Syncthreads
        Save A_TMP and B_TMP to As and Bs
        Syncthreads
        Increment kId by BK
    Write C_regs to C</code></pre></figure>

<p>To my surprise, the performance for this kernel decreased to <strong>14.3032 ms (9612.48 GFLOPS)</strong>, more than 2 times slower than kernel 3 !</p>

<p>Our double buffering algorithm utilizes more registers and reduces occupancy.
After inspecting the ISA in RGP, we see that the HIP compiler attempts to keep register usage low by using scratch memory instead—which is detrimental to performance<sup id="fnref:7"><a href="#fn:7" rel="footnote" role="doc-noteref">7</a></sup>.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph16.jpg" alt="Alt text"/></p><p>Figure 19 : scratch_load instructions introduced to reduce register usage</p>
</div>

<p>Unfortunately, we cannot directly set the maximum number of registers per kernel in HIP (which is theoretically 256). However, we can use the <a href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_cpp_language_extensions.html#launch-bounds"><strong>launch_bounds</strong></a> extension to provide hints to the compiler.</p>

<p>With this change, the performance is back to normal :  <strong>5.37 ms (25559.6 GFLOP/s)</strong>.</p>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel4_gmem_df.cpp">here</a></p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td>Kernel 3</td>
      <td>Register tiling</td>
      <td>6.0341</td>
      <td>22777.0</td>
      <td>74.6 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 4</strong></td>
      <td><strong>GMEM Double buffer</strong></td>
      <td><strong>5.3772</strong></td>
      <td><strong>25559.6</strong></td>
      <td><strong>83.7%</strong></td>
    </tr>
  </tbody>
</table>



<p>VALU utilization has increased from 43 % to 52 %.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph17.jpg" alt="Alt text" width="300"/></p><p>Figure 20 : kernel 4 stats</p>
</div>

<p>We can now go back to our LDS loads in the inner loop which have become the new bottleneck, as shown below.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph18.jpg" alt="Alt text"/></p><p>Figure 21 : latency on LDS loads</p>
</div>



<p>One thing I didn’t look at in previous kernels is whether or not we had bank conflicts on the LDS. This information is actually not easily accessible in RGP. If we look at ISA section where we write to the LDS, we see that the latency are unexpectly high.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph19.jpg" alt="Alt text"/></p><p>Figure 22 : latencies on LDS writes</p>
</div>

<p>According the RDNA3 programming guide, <em>the LDS memory is split into 64 banks of DWORD-wide RAMS. These 64 banks are further sub-divided into two sets of 32-banks each where 32 of the banks are affiliated with a pair of SIMD32’s, and the other 32 banks are affiliated with the other pair of SIMD32’s within the WGP. Each bank is a 512x32 two-port RAM (1R/1W per clock cycle). DWORDs are placed in the banks serially, but all banks can execute a store or load simultaneously.</em><sup id="fnref:1:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup></p>

<p>So, if threads within a wave access the same bank, the memory transactions will be serialized, which is exactly what happens when we write a column of matrix A to As.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph20.jpg" alt="Alt text"/></p><p>Figure 23 : Matrix A bank conflicts and how to remove them</p>
</div>

<p>Our current kernel reads the content of matrix A rows by rows to avoid uncoalesced memory loads. Given we then operates on columns of matrix A, we transpose matrix A into matrix As so that each line of As correspond to a tile column of A.</p>

<p>Now, if we look at how this work is mapped to waves, we see that we essentially write 8 times to 4 consecutive banks within each wave. One way to fix this is to add a padding of 4 elements to our LDS matrix, As.</p>

<figure><pre><code data-lang="cuda"><span>__shared__</span> <span>float</span> <span>As</span><span>[</span><span>BK</span><span>][</span><span>BM</span><span>+</span><span>4</span><span>];</span> <span>// 4 padding to avoid bank conflicts</span></code></pre></figure>

<p>Doing another RGP capture with this change:</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph21.jpg" alt="Alt text"/></p><p>Figure 24 : updated latency with padding</p>
</div>

<p>LDS latency has decreased a lot and our VALU utilization is now 62.3%.</p>

<p>However are kernel is still bound by these LDS loads. Let’s do some napkin math and check whether or not we are not reaching the limit of the LDS bandwidth.</p>

<p>As said before, each pair of SIMD has a 32-banks memory capable of reading DWORD. Our theoritical bandwidth should something like this:</p><p>

\[\large BW = {nbSIMD}/2*32*4*freq\]

\[\large BW = 96*32*4*2.371*10^9\]

\[\large BW = {29.1} \; \text{TBytes/s}\]

</p><p>Now, let’s analyze what our current algorithm does:</p>
<ul>
  <li>Each thread reads 8 DWORDS per matrix per iteration (equivalent Thread tile of 8x8)</li>
  <li>A wave read 32x8x2 DWORDS in total</li>
  <li>Our workgroup has 8 waves, so it’s 4096 reads per iteration.</li>
  <li>Given we have 4096 iterations, we read 4096x4096x4 bytes per workgroup.</li>
  <li>With 32x32 workgroup, that’s 68719476736 bytes in total.</li>
</ul>

<p>That’s for reading. We also write to the LDS : 4096x128x32x32x4x2 = 4294967296 bytes.</p>

<p>With our current execution time of 5.37 ms, the required LDS bandwidth is roughly <strong>13.56 TBytes/s</strong>.
This is less than 46% of the maximum capacity, but it is highly likely that our kernel experiences congestion in the LDS when multiple waves attempt to read or write simultaneously.</p>

<p>To mitigate this, we can try these 2 things :</p>
<ul>
  <li>enable CU mode</li>
  <li>increase our arithmetic intensity again to trade LDS reads vs GMEM reads</li>
</ul>

<p>According the RDNA3 programming guide, the LDS can operate on 2 distincts mode : WGP Mode and CU mode. HIP usse by default WGP mode.
In WGP mode, the LDS is one large contiguous memory that all waves on the WGP can access meaning we are more likely to get congestion on the LDS.
In CU mode,  the LDS is effectively split into a separate upper and lower LDS, each serving two SIMD32’s. Waves are allocated LDS space within the half of LDS which is associated with the SIMD the wave is running on.
By enabling the CU mode, we should reduce the probability of wave contending for the LDS<sup id="fnref:8"><a href="#fn:8" rel="footnote" role="doc-noteref">8</a></sup></p>

<p>Second thing we can try is to increase our Thread tile to 16x8 instead of 8x8. This will improve the computation-to-data-read ratio. It should still fit within the 256 VGPR budget we have for the kernel and reduce our bandwidth requirements to <strong>10.3 TBytes/s</strong></p>

<p>With all these changes, the performance for this kernel is now <strong>4.09 ms (33526 GFLOP/s)</strong>. That’s better than rocBLAS !</p>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel5_lds_optim.cpp">here</a></p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td>Kernel 3</td>
      <td>Register tiling</td>
      <td>6.0341</td>
      <td>22777.0</td>
      <td>74.6 %</td>
    </tr>
    <tr>
      <td>Kernel 4</td>
      <td>GMEM Double buffer</td>
      <td>5.3772</td>
      <td>25559.6</td>
      <td>83.7%</td>
    </tr>
    <tr>
      <td><strong>Kernel 5</strong></td>
      <td><strong>LDS Utilization Optimization</strong></td>
      <td><strong>4.0994</strong></td>
      <td><strong>33526.6</strong></td>
      <td><strong>109.8 %</strong></td>
    </tr>
  </tbody>
</table>



<div>
  <p><img src="https://seb-v.github.io/assets/images/graph22.jpg" alt="Alt text"/></p><p>Figure 25 : kernel 5 stats</p>
</div>

<p>If we look at the ISA, we now have a small LDS latency of less than 30 cycles and most of it is hidden.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph23.jpg" alt="Alt text"/></p><p>Figure 26 : kernel 5 instruction timing</p>
</div>

<p>OK, so our kernel outperforms rocBLAS, but since we are using dual_fmac instructions, the performance is still not as high as we would expect.</p>

<p>At this point, I tried several optimizations, but I struggled to get the HIP compiler to generate the code I wanted. Even small changes to the C++ code drastically altered the generated ISA, making optimization work very difficult. This was especially problematic with inline assembly, where the compiler would move instructions to incorrect locations due to a lack of explicit dependencies. Additionally, there is no way to manually assign specific VGPRs to particular instructions.</p>

<p>Because of these challenges, I decided to optimize directly at the ISA level, which is what we will focus on in the next steps.</p>

<p>Looking at RGP, one thing still puzzles me in the inner loop: the HIP compiler does not use dual_fmac instructions exclusively—we always see a few single FMA instructions mixed in. Another issue is that all the v_dual_fmac instructions have a minimum latency of 2–3 cycles. While this may seem insignificant, it adds up across all instructions and impacts overall performance at our current execution speed.</p>



<p>Before we go into the next optimizations, I need to be able to directly modify the ISA. To do so, I will now use the <a href="https://rocm.docs.amd.com/projects/HIP/en/latest/doxygen/html/group___module.html">Module Management API</a> so that we can load pre-compiled kernel code. Of course the idea is that we generate the ISA of our kernel from C++ once and then iterate on the ISA for any further version.</p>

<p>To do so, I need to extract the ISA source file from my current C++ kernel and ask hip to  build hsaco binary format:</p>

<p><code>hipcc --genco --offload-arch=gfx1100 kernel5_lds_optim.cpp -mcumode --save-temps -o tmp.hsaco</code></p>

<p>The <code>--save-temps</code>parameter will allow us to have access to the intermediate .s file containing the ISA.</p>

<p>HIP should produce these files:</p>

<div><div><pre><code>kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.bc
kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.hipi
kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.o
kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.out
kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.out.resolution.txt
kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.s
</code></pre></div></div>

<p><code>kernel5_lds_optim-hip-amdgcn-amd-amdhsa-gfx1100.s</code> is our guy.</p>

<p>Now we can take this file as a basis for our modifications and assemble it with the commands :</p>
<div><div><pre><code>hipcc <span>-target</span> amdgcn-amd-amdhsa <span>-mcpu</span><span>=</span>gfx1100 <span>-mcumode</span> <span>-c</span> kernel_modified.s <span>-o</span> kernel.o
ld.lld <span>-shared</span> kernel.o <span>-o</span> kernel.hsaco
</code></pre></div></div>

<p>The <code>kernel.hsaco</code>file can then be loaded at runtime using the Module Management API of HIP.</p>

<p>Direct control over the ISA is great for micro-benchmarking and makes it easier to instrument the code for performance assessment without worrying about unexpected compiler optimizations.</p>

<p>For example, I tried duplicating our dual_fmac instructions 32 times in the inner loop to see if we could artificially become VALU-bound. However, it turns out that our VALU utilization cannot exceed 75%!</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph24.jpg" alt="Alt text" width="250"/></p><p>Figure 27 : Fake kernel with 32x more VALU operations</p>
</div>

<p>Next thing I tried is to launch a single workgroup and have a single wave running. It turns out these 2-3 clocks latency are still there meaning it must comes from the VGPR distribution of these dual_fmac instructions.</p>

<p>Ok, so let’s take a closer look at these dual instructions and see if we can do something about it.
Dual instructions are of that form :</p>

<p><code>OpCodeX DSTX, SRCX0, SRCX1 :: OpCodeY DSTY, SRCY0, SRCY1</code></p>

<p>In our case :</p>

<p><code>v_dual_fmac_f32 DSTX, SRCX0, SRCX1 :: v_dual_fmac_f32 DSTY, SRCY0, SRCY1</code></p>

<p>The two instructions are executed at the same time, so there are no races between them if one reads a VGPR and the other writes the same VGPR. The ‘read’ gets the old value.</p>

<p>There are a number of constrains in order to use these instructions. Namely:</p>

<ul>
  <li>The instructions must be independent of each other</li>
  <li>SRCX0 and SRCY0 must use different VGPR banks</li>
  <li>Dest VGPRs: one must be even and the other odd</li>
  <li>VSRCX1 and VSRCY1 must use different banks</li>
</ul>

<p>On top of that, the RDNA 3 programming guide states :</p>
<ul>
  <li><em>There are 4 VGPR banks (indexed by SRC[1:0]), and each bank has a cache</em>.</li>
  <li><em>Each cache has 3 read ports: one dedicated to SRC0, one dedicated to SRC1 and one for SRC2.</em></li>
  <li><em>A cache can read all 3 of them at once, but it can’t read two SRC0’s at once (or SRC1/2).</em></li>
  <li><em>FMAC_F32 uses SRC2 as destination operand</em></li>
</ul>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph25.jpg" alt="Alt text" width="800"/></p><p>Figure 28 : my visualization of register banks and dual instructions. Example of an instruction using bank 0,1,2,3</p>
</div>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph26.jpg" alt="Alt text" width="350"/></p><p>Figure 29 : SRCX0 and SRCY0 must use different banks</p>
</div>

<p>Bank number of register \(\large X\) is given by \(\large X\%4\)</p>

<p>Taking a example:</p>

<div><div><pre><code><span>v_dual_fmac_f32</span> <span>v10</span><span>,</span> <span>v189</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v9</span><span>,</span> <span>v190</span><span>,</span> <span>v20</span>
</code></pre></div></div>

<p><code>FMAC Bank2, Bank1, Bank3 :: FMAC Bank1, Bank2, Bank0</code></p>

<p>With this instruction, we are reading the 4 different banks in parallel and writing to bank 1 and 2 the next cycle.
In practice we could read from the same bank in both OPX and OPY if it’s not using the same operand. For example this is valid given that SRCX0 and SRCY0 use different banks :</p>

<div><div><pre><code><span>v_dual_fmac_f32</span> <span>v123</span><span>,</span> <span>v139</span><span>,</span> <span>v144</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v114</span><span>,</span> <span>v140</span><span>,</span> <span>v143</span>
</code></pre></div></div>

<p><code>FMAC Bank3, Bank3, Bank0 :: FMAC Bank2, Bank0, Bank3</code></p>

<p>Both instructions are reading the same banks (0 &amp; 3). The way I see it (that’s not covered in the ISA guide AFAIK), 2 things could happen here  :</p>
<ul>
  <li>at least one of the VGPRs was already present in the cache meaning the instruction would have to fetch at most one value from the register file</li>
  <li>the VALU has to access 2 VGPRs on the same bank leading to a bank conflict and some stall latency.</li>
</ul>

<p>On top of that, we also need to take into account the VGPRs we are writing to even though we write on the next cycle.</p>

<p>So, even though we may have a valid VGPR distribution that successfully compiles, we could still encounter register bank conflicts, impacting performance.</p>

<p>Let’s take a look at what the HIP compiler has generated for us.</p>

<div><div><pre><code><span>v_dual_fmac_f32</span> <span>v127</span><span>,</span> <span>v138</span><span>,</span> <span>v144</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v122</span><span>,</span> <span>v139</span><span>,</span> <span>v143</span>
<span>v_dual_fmac_f32</span> <span>v128</span><span>,</span> <span>v138</span><span>,</span> <span>v145</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v121</span><span>,</span> <span>v139</span><span>,</span> <span>v142</span>
<span>v_dual_fmac_f32</span> <span>v123</span><span>,</span> <span>v139</span><span>,</span> <span>v144</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v114</span><span>,</span> <span>v140</span><span>,</span> <span>v143</span>
<span>v_dual_fmac_f32</span> <span>v124</span><span>,</span> <span>v139</span><span>,</span> <span>v145</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v113</span><span>,</span> <span>v140</span><span>,</span> <span>v142</span>
<span>v_dual_fmac_f32</span> <span>v115</span><span>,</span> <span>v140</span><span>,</span> <span>v144</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v110</span><span>,</span> <span>v141</span><span>,</span> <span>v143</span>
<span>v_dual_fmac_f32</span> <span>v116</span><span>,</span> <span>v140</span><span>,</span> <span>v145</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v109</span><span>,</span> <span>v141</span><span>,</span> <span>v142</span>
<span>v_dual_fmac_f32</span> <span>v111</span><span>,</span> <span>v141</span><span>,</span> <span>v144</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v90</span><span>,</span> <span>v138</span><span>,</span> <span>v147</span>
<span>v_dual_fmac_f32</span> <span>v112</span><span>,</span> <span>v141</span><span>,</span> <span>v145</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v89</span><span>,</span> <span>v138</span><span>,</span> <span>v146</span>
<span>v_dual_fmac_f32</span> <span>v91</span><span>,</span> <span>v138</span><span>,</span> <span>v148</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v94</span><span>,</span> <span>v139</span><span>,</span> <span>v147</span>
<span>v_dual_fmac_f32</span> <span>v92</span><span>,</span> <span>v138</span><span>,</span> <span>v149</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v93</span><span>,</span> <span>v139</span><span>,</span> <span>v146</span>
<span>v_dual_fmac_f32</span> <span>v95</span><span>,</span> <span>v139</span><span>,</span> <span>v148</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v98</span><span>,</span> <span>v140</span><span>,</span> <span>v147</span>
<span>v_dual_fmac_f32</span> <span>v96</span><span>,</span> <span>v139</span><span>,</span> <span>v149</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v97</span><span>,</span> <span>v140</span><span>,</span> <span>v146</span>
<span>v_dual_fmac_f32</span> <span>v99</span><span>,</span> <span>v140</span><span>,</span> <span>v148</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v118</span><span>,</span> <span>v141</span><span>,</span> <span>v147</span>
<span>v_dual_fmac_f32</span> <span>v100</span><span>,</span> <span>v140</span><span>,</span> <span>v149</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v117</span><span>,</span> <span>v141</span><span>,</span> <span>v146</span>
<span>v_dual_fmac_f32</span> <span>v119</span><span>,</span> <span>v141</span><span>,</span> <span>v148</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v70</span><span>,</span> <span>v138</span><span>,</span> <span>v151</span>
<span>v_dual_fmac_f32</span> <span>v120</span><span>,</span> <span>v141</span><span>,</span> <span>v149</span> <span>::</span> <span>v_dual_fmac_f32</span>	<span>v69</span><span>,</span> <span>v138</span><span>,</span> <span>v150</span>
<span>;...</span>
</code></pre></div></div>

<p>If we analyse both the banks and the cache state for the first instructions, we get something like this:</p>

<table>
  <tbody><tr>
    <th>DSTX</th> <th>SRC0X</th> <th>SRC1X</th> <th>DSTY</th> <th>SRC0Y</th> <th>SRC1Y</th>
  </tr>
  <tr>
    <td></td> <td>R2</td> <td>R0</td> <td></td> <td>R3</td> <td>R3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache2</td> <td>R1</td> <td>W2</td> <td>Cache3</td> <td>R2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache3</td> <td>Cache0</td> <td>W1</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache3</td> <td>Cache1</td> <td>W2</td> <td>Cache0</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache0</td> <td>Cache0</td> <td>W1</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache1</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache1</td> <td>Cache0</td> <td>W1</td> <td>Cache2</td> <td>R3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache1</td> <td>Cache1</td> <td>W2</td> <td>Cache2</td> <td>R2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache2</td> <td>R0</td> <td>W1</td> <td>Cache3</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache2</td> <td>R1</td> <td>W2</td> <td>Cache3</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache3</td> <td>Cache0</td> <td>W1</td> <td>Cache0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache3</td> <td>Cache1</td> <td>W2</td> <td>Cache0</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache0</td> <td>Cache0</td> <td>W1</td> <td>Cache1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache1</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W0</td> <td>Cache1</td> <td>Cache0</td> <td>W1</td> <td>Cache2</td> <td>R3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache1</td> <td>Cache1</td> <td>W2</td> <td>Cache2</td> <td>R2</td>
  </tr>
</tbody></table>



<ul>
  <li>R{k} means we read from bank k</li>
  <li>W{k} means we write to bank k</li>
  <li>Cache{k} means we read from one of the cache associated with bank k</li>
</ul>

<p>I assumed that writes are performed with a 1-clock delay, which is why the first row of DSTX and DSTY is empty.</p>

<p>We can see that the compiler does a great job of reusing the cache, as we only read a small amount of data. However, the access pattern is not consistent over time, and we often use the same bank more than twice.</p>

<p>I started creating microbenchmarks to understand the latencies displayed in RGP based on different access patterns in terms of VGPR banks. However, this turned out to be quite complex, likely due to the underlying architecture’s complexity.</p>

<p>Instead of spending too much time on this, I tried to design an implementation following these principles:</p>

<ul>
  <li>Write to all VGPR banks in a continuous pattern</li>
  <li>Maximize the number of different VGPR banks we read from per instruction.</li>
  <li>Maximize the use of the VGPR caches</li>
  <li>Maintain a single, consistent access pattern and aim for as much symmetry as possible.</li>
</ul>

<p>The good news is that we can ignore alignment constraints on the output matrix C, given that the number of iterations in the inner loop is quite high. In other words, we can freely shuffle register allocations during the accumulation phase and reorder them only once before writing to memory. This effectively removes one constraint, as we no longer need to maintain a direct mapping between contiguous memory locations and contiguous registers. This might be the reason the HIP compiler was struggling to only use dual_fmac instructions (write of matrix C_reg to C by global_store_b128 requires 4 consecutive VGPRs)</p>

<p>Since kernel 4, our inner loop consist of doing the multiplication between the 8 elements of a column of A and the 16 elements of a row of B. We can assume both A and B is contiguously distributed on the 4 different VGPR banks. Something like this :</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph27.jpg" alt="Alt text" width="650"/></p><p>Figure 30 : inner loop - product between A_col and B_row </p>
</div>

<p>For the sake of simplicity, I will only represent the algorithm on a 8x4 tile from now on. A naive approach is to create a dual instructions by shifting a small diagonal like this making sure both SRC0s &amp; SRC1 use different banks.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph28.jpg" alt="Alt text" width="400"/></p><p>Figure 31 : naive distribution</p>
</div>

<p><em>The cell number represents the instruction index.</em></p>

<p>We see it is perfectly doable to only use dual issue instructions however some of them are using multiple times the same banks. This is something we wanted to avoid. One way to get rid of this is to store A and B on a set of non-overlapping banks. For example B only on bank 0-1 and A on bank 2-3. The issue with this is that we won’t be able to use <code>ds_load_b128</code> instruction anymore as they target a 4 consecutive VGPRs. So instead of having 6 <code>ds_load_b128</code> instructions like we have now, we will have 12 <code>ds_load_b64</code> instead. If the performance uplift from our change is good enough, it shouldn’t matter.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph29.jpg" alt="Alt text" width="400"/></p><p>Figure 32 : separate banks between A and B</p>
</div>

<p>All green ! However if we look at the cache use and the read pattern, we have this :</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph30.jpg" alt="Alt text" width="400"/></p><p>Figure 33 : Cache usage per instruction</p>
</div>

<p>We have good reuse of the values from A, although instruction 8 performs two reads. However, if we look at the read pattern in the detailed table below, we can see that we mostly read from bank 0 and bank 1, and from instruction Y (which is not as symmetrical as we would like it to be)</p>

<table>
  <tbody><tr>
    <th>DSTX</th> <th>SRC0X</th> <th>SRC1X</th> <th>DSTY</th> <th>SRC0Y</th> <th>SRC1Y</th>
  </tr>
  <tr>
    <td></td> <td>R0</td> <td>R2</td> <td></td> <td>R1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>R2</td> <td>W0</td> <td>R1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>R0</td> <td>Cache3</td>
  </tr>
</tbody></table>



<p>Instead of iterating over the values of A alone, we could iterate over both A and B, swapping registers between instructions X and Y to maximize cache usage</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph31.jpg" alt="Alt text" width="400"/></p><p>Figure 34 : Optimal solution</p>
</div>

<p>Looking at the detailed view, we now have a nice and symetrical access pattern. Both instruction X and Y read the same amount of data from the register file and we iterate over the 4 banks in sequential way (not just bank 0 and 1)</p>

<table>
  <tbody><tr>
    <th>DSTX</th> <th>SRC0X</th> <th>SRC1X</th> <th>DSTY</th> <th>SRC0Y</th> <th>SRC1Y</th>
  </tr>
  <tr>
    <td></td> <td>R0</td> <td>R2</td> <td></td> <td>R1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache0</td> <td>Cache3</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>R2</td> <td>W0</td> <td>Cache1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>Cache0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>R0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache0</td> <td>Cache3</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>R2</td> <td>W0</td> <td>Cache1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>Cache0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>R0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache0</td> <td>Cache3</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>R2</td> <td>W0</td> <td>Cache1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>Cache0</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W3</td> <td>R0</td> <td>Cache2</td> <td>W0</td> <td>R1</td> <td>Cache3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache0</td> <td>Cache3</td> <td>W2</td> <td>Cache1</td> <td>Cache2</td>
  </tr>
  <tr>
    <td>W3</td> <td>Cache0</td> <td>R2</td> <td>W0</td> <td>Cache1</td> <td>R3</td>
  </tr>
  <tr>
    <td>W1</td> <td>Cache1</td> <td>Cache2</td> <td>W2</td> <td>Cache0</td> <td>Cache3</td>
  </tr>
</tbody></table>



<p>Right, now we are happy with our new access pattern how can we apply this change to the code ?
Here are the steps:</p>
<ol>
  <li>List the VGPRs used by the current code.</li>
  <li>Re-distribute these VGPRs to:
    <ul>
      <li>Ensure C_regs occupy a continuous segment of banks.</li>
      <li>Assign A_col and B_row to non-overlapping bank sets (e.g., banks 0-1 and banks 2-3).</li>
    </ul>
  </li>
  <li>Re-implement LDS loads for A_col and B_row.</li>
  <li>Re-write the inner loop (128 v_dual_fmac instructions).</li>
  <li>Restore the VGPR mapping after the loop to maintain compatibility with the existing code for writing to global memory.</li>
</ol>

<h3 id="list-the-vgprs-used">List the VGPRs used</h3>
<p>Let’s start with the <code>ds_load_b128</code>instructions:</p>

<div><div><pre><code><span>ds_load_b128</span> v[184:187], v183
<span>ds_load_b128</span> v[188:191], v183 offset:64
<span>ds_load_b128</span> v[192:195], v204
<span>ds_load_b128</span> v[196:199], v204 offset:128
<span>ds_load_b128</span> v[200:203], v204 offset:256
<span>ds_load_b128</span> v[204:207], v204 offset:384
</code></pre></div></div>

<p>The first 2 instructions are responsible for loading A.</p>
<ul>
  <li>v183 is the LDS address of As</li>
  <li>VGPR \(\large [184,191]\) are used to save As</li>
  <li>v204 is the LDS address of Bs</li>
  <li>VGPR \(\large [192,207]\) are used to save Bs.</li>
</ul>

<p>If we look at the fma instructions now :</p>

<div><div><pre><code><span>v_fmac_f32_e32</span> <span>v124</span><span>,</span> <span>v184</span><span>,</span> <span>v192</span>
<span>v_fmac_f32_e32</span> <span>v133</span><span>,</span> <span>v184</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v132</span><span>,</span> <span>v184</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v129</span><span>,</span> <span>v185</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v131</span><span>,</span> <span>v184</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v128</span><span>,</span> <span>v185</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v127</span><span>,</span> <span>v185</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v122</span><span>,</span> <span>v186</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v126</span><span>,</span> <span>v185</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v123</span><span>,</span> <span>v186</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v121</span><span>,</span> <span>v186</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v116</span><span>,</span> <span>v187</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v120</span><span>,</span> <span>v186</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v117</span><span>,</span> <span>v187</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v115</span><span>,</span> <span>v187</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v112</span><span>,</span> <span>v184</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v114</span><span>,</span> <span>v187</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v113</span><span>,</span> <span>v184</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v111</span><span>,</span> <span>v184</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v108</span><span>,</span> <span>v185</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v110</span><span>,</span> <span>v184</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v109</span><span>,</span> <span>v185</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v107</span><span>,</span> <span>v185</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v104</span><span>,</span> <span>v186</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v106</span><span>,</span> <span>v185</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v105</span><span>,</span> <span>v186</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v103</span><span>,</span> <span>v186</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v100</span><span>,</span> <span>v187</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v102</span><span>,</span> <span>v186</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v101</span><span>,</span> <span>v187</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v99</span><span>,</span> <span>v187</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v96</span><span>,</span> <span>v184</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v98</span><span>,</span> <span>v187</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v97</span><span>,</span> <span>v184</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v95</span><span>,</span> <span>v184</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v92</span><span>,</span> <span>v185</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v94</span><span>,</span> <span>v184</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v93</span><span>,</span> <span>v185</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v91</span><span>,</span> <span>v185</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v88</span><span>,</span> <span>v186</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v90</span><span>,</span> <span>v185</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v89</span><span>,</span> <span>v186</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v87</span><span>,</span> <span>v186</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v84</span><span>,</span> <span>v187</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v86</span><span>,</span> <span>v186</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v85</span><span>,</span> <span>v187</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v83</span><span>,</span> <span>v187</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v80</span><span>,</span> <span>v184</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v82</span><span>,</span> <span>v187</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v81</span><span>,</span> <span>v184</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v79</span><span>,</span> <span>v184</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v76</span><span>,</span> <span>v185</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v78</span><span>,</span> <span>v184</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v77</span><span>,</span> <span>v185</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v75</span><span>,</span> <span>v185</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v72</span><span>,</span> <span>v186</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v74</span><span>,</span> <span>v185</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v73</span><span>,</span> <span>v186</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v71</span><span>,</span> <span>v186</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v68</span><span>,</span> <span>v187</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v70</span><span>,</span> <span>v186</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v69</span><span>,</span> <span>v187</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v67</span><span>,</span> <span>v187</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v64</span><span>,</span> <span>v188</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v66</span><span>,</span> <span>v187</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v65</span><span>,</span> <span>v188</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v63</span><span>,</span> <span>v188</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v60</span><span>,</span> <span>v189</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v62</span><span>,</span> <span>v188</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v61</span><span>,</span> <span>v189</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v59</span><span>,</span> <span>v189</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v56</span><span>,</span> <span>v190</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v58</span><span>,</span> <span>v189</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v57</span><span>,</span> <span>v190</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v55</span><span>,</span> <span>v190</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v52</span><span>,</span> <span>v191</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v54</span><span>,</span> <span>v190</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v53</span><span>,</span> <span>v191</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v51</span><span>,</span> <span>v191</span><span>,</span> <span>v194</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v48</span><span>,</span> <span>v188</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v50</span><span>,</span> <span>v191</span><span>,</span> <span>v195</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v49</span><span>,</span> <span>v188</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v47</span><span>,</span> <span>v188</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v44</span><span>,</span> <span>v189</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v46</span><span>,</span> <span>v188</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v45</span><span>,</span> <span>v189</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v43</span><span>,</span> <span>v189</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v40</span><span>,</span> <span>v190</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v42</span><span>,</span> <span>v189</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v41</span><span>,</span> <span>v190</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v39</span><span>,</span> <span>v190</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v36</span><span>,</span> <span>v191</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v38</span><span>,</span> <span>v190</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v37</span><span>,</span> <span>v191</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v35</span><span>,</span> <span>v191</span><span>,</span> <span>v198</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v32</span><span>,</span> <span>v188</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v34</span><span>,</span> <span>v191</span><span>,</span> <span>v199</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v33</span><span>,</span> <span>v188</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v31</span><span>,</span> <span>v188</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v28</span><span>,</span> <span>v189</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v30</span><span>,</span> <span>v188</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v29</span><span>,</span> <span>v189</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v27</span><span>,</span> <span>v189</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v24</span><span>,</span> <span>v190</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v26</span><span>,</span> <span>v189</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v25</span><span>,</span> <span>v190</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v23</span><span>,</span> <span>v190</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v20</span><span>,</span> <span>v191</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v22</span><span>,</span> <span>v190</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v21</span><span>,</span> <span>v191</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v19</span><span>,</span> <span>v191</span><span>,</span> <span>v202</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v16</span><span>,</span> <span>v188</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v18</span><span>,</span> <span>v191</span><span>,</span> <span>v203</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v17</span><span>,</span> <span>v188</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v15</span><span>,</span> <span>v188</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v12</span><span>,</span> <span>v189</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v14</span><span>,</span> <span>v188</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v13</span><span>,</span> <span>v189</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v11</span><span>,</span> <span>v189</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v8</span><span>,</span> <span>v190</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v10</span><span>,</span> <span>v189</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v9</span><span>,</span> <span>v190</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v7</span><span>,</span> <span>v190</span><span>,</span> <span>v206</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v4</span><span>,</span> <span>v191</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v6</span><span>,</span> <span>v190</span><span>,</span> <span>v207</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v5</span><span>,</span> <span>v191</span><span>,</span> <span>v204</span>
<span>v_fmac_f32_e32</span> <span>v3</span><span>,</span> <span>v191</span><span>,</span> <span>v206</span>
<span>v_fmac_f32_e32</span> <span>v2</span><span>,</span> <span>v191</span><span>,</span> <span>v207</span>
</code></pre></div></div>

<p>Matrix C_reg is spread accross these ranges : 
\(\large[2, 117], [120, 124], [126, 129], [131,133]\)</p>

<h3 id="vgpr-redistribution">VGPR redistribution</h3>

<p>It turns out that the VGPR allocation for C_reg is already close to what we need. We just need to add an extra bank 2 VGPR to ensure that all 128 VGPRs are allocated sequentially across banks 0-3.</p>

<p>This is good news, as it allows us to maintain compatibility with the initialization code for C_reg (setting all values to 0.0).</p>

<p>New allocation for C_reg : \([2, 117], [120, 124], [126, 129], [131,133], [214]\)</p>

<p>For A_col and B_row, we also need to allocate extra registers given that B_row will only use bank 0-1.</p>

<p>New allocation for A_col and B_row :</p>

<ul>
  <li>
    <p>A_col : \([186,187],[190,191],[194,195],[198,199]\) (banks 2-3)</p>
  </li>
  <li>
    <p>B_row : \([184,185] , [188,189] , [192,193] , [196,197] , [200,201], [204,205] , [208,209] , [212,213]\) (banks 0-1)</p>
  </li>
</ul>

<h3 id="re-write-lds-loads">Re-write LDS loads</h3>

<p>Our new code for loading A_col from As:</p>

<div><div><pre><code><span>;A on bank 2-3</span>
<span>ds_load_b64</span> v[186:187], v183
<span>ds_load_b64</span> v[190:191], v183 offset: 8
<span>ds_load_b64</span> v[194:195], v183 offset: 64
<span>ds_load_b64</span> v[198:199], v183 offset: 72
</code></pre></div></div>

<p>Loading B_row from Bs:</p>

<div><div><pre><code><span>;B on bank 0-1</span>
<span>ds_load_b64</span> v[184:185], v202
<span>ds_load_b64</span> v[188:189], v202 offset: 8 
<span>ds_load_b64</span> v[192:193], v202 offset: 128
<span>ds_load_b64</span> v[196:197], v202 offset: 136 
<span>ds_load_b64</span> v[200:201], v202 offset: 256
<span>ds_load_b64</span> v[204:205], v202 offset: 264 
<span>ds_load_b64</span> v[208:209], v202 offset: 384
<span>ds_load_b64</span> v[212:213], v202 offset: 392 
</code></pre></div></div>
<p>v183 and v202 are the new VGPRs holding the addresses of A and B in the LDS memory.</p>

<h3 id="re-write-dual_fmas">Re-write dual_fmas</h3>

<p>We can then write our inner loop only as v_dual_fmac :</p>

<div><div><pre><code><span>v_dual_fmac_f32</span> <span>v5</span><span>,</span> <span>v186</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v2</span><span>,</span> <span>v187</span><span>,</span> <span>v185</span>
<span>v_dual_fmac_f32</span> <span>v3</span><span>,</span> <span>v186</span><span>,</span> <span>v185</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v4</span><span>,</span> <span>v187</span><span>,</span> <span>v184</span>
<span>v_dual_fmac_f32</span> <span>v9</span><span>,</span> <span>v186</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v6</span><span>,</span> <span>v187</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v7</span><span>,</span> <span>v187</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v8</span><span>,</span> <span>v186</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v13</span><span>,</span> <span>v190</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v10</span><span>,</span> <span>v191</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v11</span><span>,</span> <span>v190</span><span>,</span> <span>v189</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v12</span><span>,</span> <span>v191</span><span>,</span> <span>v188</span>
<span>v_dual_fmac_f32</span> <span>v17</span><span>,</span> <span>v190</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v14</span><span>,</span> <span>v191</span><span>,</span> <span>v185</span>
<span>v_dual_fmac_f32</span> <span>v15</span><span>,</span> <span>v191</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v16</span><span>,</span> <span>v190</span><span>,</span> <span>v185</span>
<span>v_dual_fmac_f32</span> <span>v21</span><span>,</span> <span>v194</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v18</span><span>,</span> <span>v195</span><span>,</span> <span>v185</span>
<span>v_dual_fmac_f32</span> <span>v19</span><span>,</span> <span>v194</span><span>,</span> <span>v185</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v20</span><span>,</span> <span>v195</span><span>,</span> <span>v184</span>
<span>v_dual_fmac_f32</span> <span>v25</span><span>,</span> <span>v194</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v22</span><span>,</span> <span>v195</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v23</span><span>,</span> <span>v195</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v24</span><span>,</span> <span>v194</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v29</span><span>,</span> <span>v198</span><span>,</span> <span>v188</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v26</span><span>,</span> <span>v199</span><span>,</span> <span>v189</span>
<span>v_dual_fmac_f32</span> <span>v27</span><span>,</span> <span>v198</span><span>,</span> <span>v189</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v28</span><span>,</span> <span>v199</span><span>,</span> <span>v188</span>
<span>v_dual_fmac_f32</span> <span>v33</span><span>,</span> <span>v198</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v30</span><span>,</span> <span>v199</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v31</span><span>,</span> <span>v199</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v32</span><span>,</span> <span>v198</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v37</span><span>,</span> <span>v186</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v34</span><span>,</span> <span>v187</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v35</span><span>,</span> <span>v186</span><span>,</span> <span>v193</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v36</span><span>,</span> <span>v187</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v41</span><span>,</span> <span>v186</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v38</span><span>,</span> <span>v187</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v39</span><span>,</span> <span>v187</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v40</span><span>,</span> <span>v186</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v45</span><span>,</span> <span>v190</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v42</span><span>,</span> <span>v191</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v43</span><span>,</span> <span>v190</span><span>,</span> <span>v197</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v44</span><span>,</span> <span>v191</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v49</span><span>,</span> <span>v190</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v46</span><span>,</span> <span>v191</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v47</span><span>,</span> <span>v191</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v48</span><span>,</span> <span>v190</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v53</span><span>,</span> <span>v194</span><span>,</span> <span>v192</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v50</span><span>,</span> <span>v195</span><span>,</span> <span>v193</span>
<span>v_dual_fmac_f32</span> <span>v51</span><span>,</span> <span>v194</span><span>,</span> <span>v193</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v52</span><span>,</span> <span>v195</span><span>,</span> <span>v192</span>
<span>v_dual_fmac_f32</span> <span>v57</span><span>,</span> <span>v194</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v54</span><span>,</span> <span>v195</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v55</span><span>,</span> <span>v195</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v56</span><span>,</span> <span>v194</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v61</span><span>,</span> <span>v198</span><span>,</span> <span>v196</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v58</span><span>,</span> <span>v199</span><span>,</span> <span>v197</span>
<span>v_dual_fmac_f32</span> <span>v59</span><span>,</span> <span>v198</span><span>,</span> <span>v197</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v60</span><span>,</span> <span>v199</span><span>,</span> <span>v196</span>
<span>v_dual_fmac_f32</span> <span>v65</span><span>,</span> <span>v198</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v62</span><span>,</span> <span>v199</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v63</span><span>,</span> <span>v199</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v64</span><span>,</span> <span>v198</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v69</span><span>,</span> <span>v186</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v66</span><span>,</span> <span>v187</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v67</span><span>,</span> <span>v186</span><span>,</span> <span>v201</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v68</span><span>,</span> <span>v187</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v73</span><span>,</span> <span>v186</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v70</span><span>,</span> <span>v187</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v71</span><span>,</span> <span>v187</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v72</span><span>,</span> <span>v186</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v77</span><span>,</span> <span>v190</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v74</span><span>,</span> <span>v191</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v75</span><span>,</span> <span>v190</span><span>,</span> <span>v205</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v76</span><span>,</span> <span>v191</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v81</span><span>,</span> <span>v190</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v78</span><span>,</span> <span>v191</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v79</span><span>,</span> <span>v191</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v80</span><span>,</span> <span>v190</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v85</span><span>,</span> <span>v194</span><span>,</span> <span>v200</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v82</span><span>,</span> <span>v195</span><span>,</span> <span>v201</span>
<span>v_dual_fmac_f32</span> <span>v83</span><span>,</span> <span>v194</span><span>,</span> <span>v201</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v84</span><span>,</span> <span>v195</span><span>,</span> <span>v200</span>
<span>v_dual_fmac_f32</span> <span>v89</span><span>,</span> <span>v194</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v86</span><span>,</span> <span>v195</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v87</span><span>,</span> <span>v195</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v88</span><span>,</span> <span>v194</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v93</span><span>,</span> <span>v198</span><span>,</span> <span>v204</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v90</span><span>,</span> <span>v199</span><span>,</span> <span>v205</span>
<span>v_dual_fmac_f32</span> <span>v91</span><span>,</span> <span>v198</span><span>,</span> <span>v205</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v92</span><span>,</span> <span>v199</span><span>,</span> <span>v204</span>
<span>v_dual_fmac_f32</span> <span>v97</span><span>,</span> <span>v198</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v94</span><span>,</span> <span>v199</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v95</span><span>,</span> <span>v199</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v96</span><span>,</span> <span>v198</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v101</span><span>,</span> <span>v186</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v98</span><span>,</span> <span>v187</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v99</span><span>,</span> <span>v186</span><span>,</span> <span>v209</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v100</span><span>,</span> <span>v187</span><span>,</span> <span>v208</span>
<span>v_dual_fmac_f32</span> <span>v105</span><span>,</span> <span>v186</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v102</span><span>,</span> <span>v187</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v103</span><span>,</span> <span>v187</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v104</span><span>,</span> <span>v186</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v109</span><span>,</span> <span>v190</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v106</span><span>,</span> <span>v191</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v107</span><span>,</span> <span>v190</span><span>,</span> <span>v213</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v108</span><span>,</span> <span>v191</span><span>,</span> <span>v212</span>
<span>v_dual_fmac_f32</span> <span>v113</span><span>,</span> <span>v190</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v110</span><span>,</span> <span>v191</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v111</span><span>,</span> <span>v191</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v112</span><span>,</span> <span>v190</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v117</span><span>,</span> <span>v194</span><span>,</span> <span>v208</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v114</span><span>,</span> <span>v195</span><span>,</span> <span>v209</span>
<span>v_dual_fmac_f32</span> <span>v115</span><span>,</span> <span>v194</span><span>,</span> <span>v209</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v116</span><span>,</span> <span>v195</span><span>,</span> <span>v208</span>
<span>v_dual_fmac_f32</span> <span>v121</span><span>,</span> <span>v194</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v122</span><span>,</span> <span>v195</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v123</span><span>,</span> <span>v195</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v120</span><span>,</span> <span>v194</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v129</span><span>,</span> <span>v198</span><span>,</span> <span>v212</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v126</span><span>,</span> <span>v199</span><span>,</span> <span>v213</span>
<span>v_dual_fmac_f32</span> <span>v127</span><span>,</span> <span>v198</span><span>,</span> <span>v213</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v124</span><span>,</span> <span>v199</span><span>,</span> <span>v212</span>
<span>v_dual_fmac_f32</span> <span>v133</span><span>,</span> <span>v198</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v214</span><span>,</span> <span>v199</span><span>,</span> <span>v185</span>
<span>v_dual_fmac_f32</span> <span>v131</span><span>,</span> <span>v199</span><span>,</span> <span>v184</span> <span>::</span> <span>v_dual_fmac_f32</span> <span>v128</span><span>,</span> <span>v198</span><span>,</span> <span>v185</span>
</code></pre></div></div>

<h3 id="restore-vgpr-mapping">Restore VGPR mapping</h3>

<p>We use a temporary VGPR to restore the full mapping like this:</p>

<div><div><pre><code><span>; v2 -&gt; v128 &amp; v128 -&gt;  v2</span>
<span>v_mov_b32</span> <span>v200</span><span>,</span> <span>v128</span>
<span>v_mov_b32</span> <span>v128</span><span>,</span> <span>v2</span>
<span>v_mov_b32</span> <span>v2</span><span>,</span> <span>v200</span>
<span>; v128 -&gt; v56 &amp; v56 -&gt;  v128</span>
<span>v_mov_b32</span> <span>v200</span><span>,</span> <span>v56</span>
<span>v_mov_b32</span> <span>v56</span><span>,</span> <span>v2</span>
<span>v_mov_b32</span> <span>v2</span><span>,</span> <span>v200</span>
<span>; v56 -&gt; v46 &amp; v46 -&gt;  v56</span>
<span>v_mov_b32</span> <span>v200</span><span>,</span> <span>v46</span>
<span>v_mov_b32</span> <span>v46</span><span>,</span> <span>v2</span>
<span>v_mov_b32</span> <span>v2</span><span>,</span> <span>v200</span>
<span>; v46 -&gt; v100 &amp; v100 -&gt;  v46</span>
<span>v_mov_b32</span> <span>v200</span><span>,</span> <span>v100</span>
<span>v_mov_b32</span> <span>v100</span><span>,</span> <span>v2</span>
<span>v_mov_b32</span> <span>v2</span><span>,</span> <span>v200</span>
 ...
</code></pre></div></div>

<p>To facilitate these changes, I wrote a small C++ program to parse the ISA, extract the mapping between the old and new VGPR distribution, and automatically generate all the necessary instructions.</p>

<p>Our kernel now uses 214 VGPRs instead of 208. We need to modify this in the .s file in the amdhsa.kernels section:</p>

<pre><code>.vgpr_count:     214
</code></pre>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel6_valu_optim.s">here</a></p>

<p>Performance for this kernel is <strong>3.63 ms (37791.2 GFLOP/s)</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td>Kernel 3</td>
      <td>Register tiling</td>
      <td>6.0341</td>
      <td>22777.0</td>
      <td>74.6 %</td>
    </tr>
    <tr>
      <td>Kernel 4</td>
      <td>GMEM Double buffer</td>
      <td>5.3772</td>
      <td>25559.6</td>
      <td>83.7%</td>
    </tr>
    <tr>
      <td>Kernel 5</td>
      <td>LDS Utilization Optimization</td>
      <td>4.0994</td>
      <td>33526.6</td>
      <td>109.8 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 6</strong></td>
      <td><strong>VALU Utilization Optimization</strong></td>
      <td><strong>3.6368</strong></td>
      <td><strong>37791.2</strong></td>
      <td><strong>123.7 %</strong></td>
    </tr>
  </tbody>
</table>



<p>Our VALU utilization has gone up again to 76.2 % (more than our 75 % with 32x the inner loop).</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph32.jpg" alt="Alt text" width="350"/></p><p>Figure 35 : kernel 6 stats</p>
</div>

<p>If we look at this ISA,  our inner loop consists solely of v_dual_fmac instructions, each with a 1-cycle latency. Beautiful !</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph33.jpg" alt="Alt text"/></p><p>Figure 36 : kernel 7 stats</p>
</div>

<p>We can also see that many cycles are wasted at the end of the loop on branching. Let’s try to optimize that in the next kernel.</p>



<p>I previously tried unrolling the inner loop in the C++ HIP implementation, but it didn’t work out. The kernel became too large as the compiler pre-fetched more values from the LDS, and performance remained unchanged.</p>

<p>Now that we have a highly efficient loop and full control over the ISA, we might have better luck. For this step, I will simply duplicate the added code from Kernel 6 eight times and remove the loop mechanism.</p>

<div><div><pre><code>s_cmpk_lg_i32 s14, 0x1000 ; Remove this line at the beginning of the loop
<span>s_waitcnt</span> lgkmcnt(0)
<span>v_dual_fmac_f32</span>  ...
<span>v_dual_fmac_f32</span>  ...
s_cbranch_scc1 .LBB0_9 ; Remove this line at the end of the loop
</code></pre></div></div>

<p>Duplicate 8 times our load and multiplication and make sure we increment the addresses :</p>

<div><div><pre><code><span>v_add_nc_u32_e32</span> <span>v183</span><span>,</span> 0x210, v183 ; B : 0x210 = (128+4)*4
<span>v_add_nc_u32_e32</span> <span>v202</span><span>,</span> 0x200, v202 ; A : 0x200 = (128)*4
</code></pre></div></div>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel7_unroll.s">here</a></p>

<p>The performance for this kernel is <strong>3.33 ms (41255.6 GFLOPS/s)</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td>Kernel 3</td>
      <td>Register tiling</td>
      <td>6.0341</td>
      <td>22777.0</td>
      <td>74.6 %</td>
    </tr>
    <tr>
      <td>Kernel 4</td>
      <td>GMEM Double buffer</td>
      <td>5.3772</td>
      <td>25559.6</td>
      <td>83.7%</td>
    </tr>
    <tr>
      <td>Kernel 5</td>
      <td>LDS Utilization Optimization</td>
      <td>4.0994</td>
      <td>33526.6</td>
      <td>109.8 %</td>
    </tr>
    <tr>
      <td>Kernel 6</td>
      <td>VALU Utilization Optimization</td>
      <td>3.6368</td>
      <td>37791.2</td>
      <td>123.7 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 7</strong></td>
      <td><strong>Unroll inner loop</strong></td>
      <td><strong>3.3314</strong></td>
      <td><strong>41255.6</strong></td>
      <td><strong>135.1 %</strong></td>
    </tr>
  </tbody>
</table>



<p>VALU utilization is now above 80 %.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph35.jpg" alt="Alt text" width="320"/></p><p>Figure 37 : kernel 7 stats</p>
</div>

<p>The instruction timing starts to look very good as well :</p>
<ul>
  <li>v_dual_fmac have on average a 1 clk latency</li>
  <li>ds_loads have an average 1 clk latency</li>
  <li>wait for the lds is only (34 cycles ) and most of it is hidden by VALU operations</li>
</ul>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph36.jpg" alt="Alt text"/></p><p>Figure 38 : instruction timing</p>
</div>

<p>So why aren’t we faster ?</p>

<p>If we look at the total latency clk in RGP, our biggest offender is the wait on the barrier. The s_waitcnt just before is the wait on the global memory loads.</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph37.jpg" alt="Alt text"/></p><p>Figure 39 : Remaining non hidden latencies</p>
</div>

<p>We can’t eliminate the barrier since we need to synchronize the threads before writing to LDS. However, if we examine the generated code for global memory loads, we notice a large code segment dedicated to it (128 lines)</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph38.jpg" alt="Alt text"/></p><p>Figure 40 : GMEM loads</p>
</div>
<p>I didn’t notice it before but even though latency is partly hidden, cumulated latency for a single load is around 1.3 Millions clks. Given that we do 16 different loads (8 for each matrix), that’s 20 millions clks latency here !</p>

<p>Let’s see how we can improve this in the next kernel.</p>



<p>Ok, let’s start looking at what HIP has generated for us (I have removed <code>s_delay_alu</code> instructions for better readability)</p>

<div><div><pre><code><span>v_add_nc_u32_e32</span> <span>v169</span><span>,</span> <span>s4</span><span>,</span> <span>v168</span>
<span>v_ashrrev_i32_e32</span> <span>v170</span><span>,</span> <span>31</span><span>,</span> <span>v169</span>
<span>v_lshlrev_b64</span> v[170:171], 2, v[169:170]
<span>v_add_co_u32</span> <span>v170</span><span>,</span> vcc_lo, s10, v170
<span>v_add_co_ci_u32_e32</span> <span>v171</span><span>,</span> vcc_lo, s11, v171, vcc_lo
<span>global_load_b32</span> <span>v168</span><span>,</span> v[170:171], off

<span>v_add_nc_u32_e32</span> <span>v170</span><span>,</span> <span>s4</span><span>,</span> <span>v169</span>
<span>v_ashrrev_i32_e32</span> <span>v171</span><span>,</span> <span>31</span><span>,</span> <span>v170</span>
<span>v_lshlrev_b64</span> v[171:172], 2, v[170:171]
<span>v_add_co_u32</span> <span>v171</span><span>,</span> vcc_lo, s10, v171
<span>v_add_co_ci_u32_e32</span> <span>v172</span><span>,</span> vcc_lo, s11, v172, vcc_lo
<span>global_load_b32</span> <span>v169</span><span>,</span> v[171:172], off
</code></pre></div></div>

<p>Here s[10:11] hold the address of matrix B. For each each global_load_b32, the compiler computes the read offset using VGPRs from the previous iteration (v170 and v171 here). This is not ideal for a couple of reasons :</p>
<ul>
  <li>Every global_load requires a VALU operation to complete first. VALU that won’t be used by other waves doing FMA operations.</li>
  <li>Dependencies between global_load operations introduce unnecessary latency</li>
  <li>Spending too many cycles in the GMEM state increases the likelihood of multiple waves on the same SIMD being in that state simultaneously, effectively reducing VALU work.</li>
</ul>

<p>So ideally, we would like this 128 line section of code to be just 16 lines:</p>

<div><div><pre><code><span>global_load_b32</span> <span>v169</span><span>,</span> v[171:172], off
<span>global_load_b32</span> <span>v170</span><span>,</span> v[173:174], off
<span>global_load_b32</span> <span>v171</span><span>,</span> v[175:176], off
.... 
</code></pre></div></div>

<p>However, this would require us to maintain additional VGPRs and potentially use VALU instructions to update the memory addresses as well. Given that we are already using 214 VGPRs, this is clearly not feasible.</p>

<p>That said, we still have a fairly good SGPR budget, and according to the RDNA3 programming guide, global_load instructions can use SGPR for base addressing.</p>

<div><div><pre><code><span>global_load_b32</span> <span>v171</span><span>,</span> <span>v214</span><span>,</span> s[10:11]
</code></pre></div></div>
<p>v214 is now a offset in bytes. s[10:11] a 64bit address in memory.</p>

<p>So we could pre-compute once all the needed addresses for the 16 loads and just increment once the offset in the loop. That would require an additional 16*2 SGPRs and 2 VGPRs to handle the offset.</p>

<p>After inspecting the ISA, it looks like :</p>
<ul>
  <li>s[0:1] contains the address of the kernel parameters.</li>
  <li>s14 &amp; s15 contains the blockIdx</li>
  <li>v0 is threadIdx.x</li>
</ul>

<p>That’s all we need to compute the needed base addresses.</p>

<p>First, we load the first 128 bytes to get matrix A and B addresses into s[20:21] and s[22:23]:</p>

<div><div><pre><code>s_load_b128 s[20:23], s[0:1], 0x0 ; Matrix A and B 
<span>s_waitcnt</span> lgkmcnt(0)
</code></pre></div></div>

<p>For matrix B, we will save the  base addresses with the pre-computed offsets in s[24:39]. If we go back to our C++ code, each offset is separated by <code>strideReadB*N = BLOCK_SIZE / BN * N</code>, that’s <code>4096x4 = 0x4000 bytes</code></p>

<div><div><pre><code><span>s_add_u32</span> <span>s24</span><span>,</span> <span>s22</span><span>,</span> 0x0000 
<span>s_addc_u32</span> <span>s25</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s26</span><span>,</span> <span>s22</span><span>,</span> 0x4000
<span>s_addc_u32</span> <span>s27</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s28</span><span>,</span> <span>s22</span><span>,</span> 0x8000
<span>s_addc_u32</span> <span>s29</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s30</span><span>,</span> <span>s22</span><span>,</span> 0xc000
<span>s_addc_u32</span> <span>s31</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s32</span><span>,</span> <span>s22</span><span>,</span> 0x10000
<span>s_addc_u32</span> <span>s33</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s34</span><span>,</span> <span>s22</span><span>,</span> 0x14000
<span>s_addc_u32</span> <span>s35</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s36</span><span>,</span> <span>s22</span><span>,</span> 0x18000
<span>s_addc_u32</span> <span>s37</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s38</span><span>,</span> <span>s22</span><span>,</span> 0x1c000
<span>s_addc_u32</span> <span>s39</span><span>,</span> <span>s23</span><span>,</span> <span>0</span>  
</code></pre></div></div>

<p>And to compute the index in bytes, we can do :</p>
<div><div><pre><code><span>; compute Matrix B offset</span>
<span>s_lshl_b32</span> <span>s19</span><span>,</span> <span>s14</span><span>,</span> <span>7</span>         		<span>; BN * blockIdx.x</span>
<span>v_add_nc_u32_e32</span> <span>v203</span><span>,</span> <span>s19</span><span>,</span> <span>v0</span> 		<span>; index = BN * blockIdx.x + threadIdx.x </span>
<span>v_lshlrev_b32_e32</span>  <span>v203</span><span>,</span><span>2</span><span>,</span> <span>v203</span>   <span>; offset = 4*index (to bytes offset)</span>
</code></pre></div></div>

<p>We apply the same logic for matrix A using s[40:55] for the base addresses and v215 for the offset.</p>

<div><div><pre><code><span>s_add_u32</span> <span>s40</span><span>,</span> <span>s20</span><span>,</span> 0x0000 
<span>s_addc_u32</span> <span>s41</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s42</span><span>,</span> <span>s20</span><span>,</span> 0x40000
<span>s_addc_u32</span> <span>s43</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s44</span><span>,</span> <span>s20</span><span>,</span> 0x80000
<span>s_addc_u32</span> <span>s45</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s46</span><span>,</span> <span>s20</span><span>,</span> 0xc0000
<span>s_addc_u32</span> <span>s47</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s48</span><span>,</span> <span>s20</span><span>,</span> 0x100000
<span>s_addc_u32</span> <span>s49</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s50</span><span>,</span> <span>s20</span><span>,</span> 0x140000
<span>s_addc_u32</span> <span>s51</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s52</span><span>,</span> <span>s20</span><span>,</span> 0x180000
<span>s_addc_u32</span> <span>s53</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  
<span>s_add_u32</span> <span>s54</span><span>,</span> <span>s20</span><span>,</span> 0x1c0000
<span>s_addc_u32</span> <span>s55</span><span>,</span> <span>s21</span><span>,</span> <span>0</span>  

<span>; compute Matrix A offset</span>
<span>s_lshl_b32</span> <span>s19</span><span>,</span> <span>s15</span><span>,</span> <span>19</span>          <span>; 4096 * 128 * blockIdx.y</span>
<span>v_lshrrev_b32_e32</span> <span>v1</span><span>,</span> <span>3</span><span>,</span> <span>v0</span>      <span>; threadIdx.x / 8 </span>
<span>v_lshlrev_b32_e32</span> <span>v1</span><span>,</span> <span>12</span><span>,</span> <span>v1</span>     <span>; 4096 * (threadIdx.x/8) </span>
<span>v_and_b32_e32</span> <span>v215</span><span>,</span> <span>7</span><span>,</span> <span>v0</span>        <span>; threadIdx.x % 8 </span>
<span>v_add_u32_e32</span> <span>v215</span><span>,</span> <span>v1</span><span>,</span> <span>v215</span>     <span>; index = 4096*(threadIdx.x/8) + threadIdx.x % 8</span>
<span>v_add_nc_u32_e32</span> <span>v215</span><span>,</span> <span>s19</span><span>,</span> <span>v215</span> <span>; index += 4096*128*blockIdx.y</span>
<span>v_lshlrev_b32_e32</span>  <span>v215</span><span>,</span><span>2</span><span>,</span> <span>v215</span>  <span>; offset = 4*index</span>
</code></pre></div></div>

<p>Now, in our main loop we can replace the 128 lines of code with this :</p>

<div><div><pre><code><span>v_add_nc_u32_e32</span> <span>v203</span><span>,</span> 0x20000, v203
<span>v_add_nc_u32_e32</span> <span>v215</span><span>,</span> 0x20, v215

<span>global_load_b32</span>	 <span>v167</span><span>,</span> <span>v203</span><span>,</span> s[24:25]
<span>global_load_b32</span>	 <span>v168</span><span>,</span> <span>v203</span><span>,</span> s[26:27]
<span>global_load_b32</span>	 <span>v169</span><span>,</span> <span>v203</span><span>,</span> s[28:29]
<span>global_load_b32</span>	 <span>v170</span><span>,</span> <span>v203</span><span>,</span> s[30:31]
<span>global_load_b32</span>	 <span>v171</span><span>,</span> <span>v203</span><span>,</span> s[32:33]
<span>global_load_b32</span>	 <span>v172</span><span>,</span> <span>v203</span><span>,</span> s[34:35]
<span>global_load_b32</span>	 <span>v173</span><span>,</span> <span>v203</span><span>,</span> s[36:37]
<span>global_load_b32</span>	 <span>v174</span><span>,</span> <span>v203</span><span>,</span> s[38:39]
<span>global_load_b32</span>	 <span>v175</span><span>,</span> <span>v215</span><span>,</span> s[40:41]
<span>global_load_b32</span>	 <span>v176</span><span>,</span> <span>v215</span><span>,</span> s[42:43]
<span>global_load_b32</span>	 <span>v177</span><span>,</span> <span>v215</span><span>,</span> s[44:45]
<span>global_load_b32</span>	 <span>v178</span><span>,</span> <span>v215</span><span>,</span> s[46:47]
<span>global_load_b32</span>	 <span>v179</span><span>,</span> <span>v215</span><span>,</span> s[48:49]
<span>global_load_b32</span>	 <span>v180</span><span>,</span> <span>v215</span><span>,</span> s[50:51]
<span>global_load_b32</span>	 <span>v181</span><span>,</span> <span>v215</span><span>,</span> s[52:53]
<span>global_load_b32</span>	 <span>v182</span><span>,</span> <span>v215</span><span>,</span> s[54:55]
</code></pre></div></div>

<p>Our modified kernel now uses 55 SGPRs instead of 18 and 216 VGPRs instead of 214.
If we take another RGP capture, we can see that this is much better, with less than 2 million clock cycles of latency for the entire process now</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph39.jpg" alt="Alt text"/></p><p>Figure 41 : Simplified GMEMs</p>
</div>

<p>After some experimentation, I found that spreading these 16 loads across the inner loop was more efficient. 
Our kernel currently executes six wavefronts per SIMD. Since our workgroup consists of 128 threads (4 waves), every time we execute a syncthreads, at least 2 of the 6 waves on the SIMD will compete for GMEM access. Additionally, if any of the remaining 4 waves happen to be in the same state, even more waves could be contending for memory access</p>
<div>
  <p><img src="https://seb-v.github.io/assets/images/graph41.jpg" alt="Alt text"/></p><p>Figure 42 : at least 2 waves requesting GMEM at the same time</p>
</div>

<p>By splitting these loads into chunks of 2, we reduce the likelihood of overlap between waves, as shown in the following diagram:</p>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph42.jpg" alt="Alt text"/></p><p>Figure 43 : splitting GMEM instructions in chunks of 2</p>
</div>

<p>Performance for this kernel is <strong>2.80 ms (49047 GFLOPS/s)</strong>. That’s now 60% faster than our reference rocBLAS version and almost 50 times faster than our naive approach !</p>

<p>Full kernel source code can be found <a href="https://github.com/seb-v/fp32_sgemm_amd/blob/main/src/kernel8_batched_gmem.s">here</a></p>

<table>
  <thead>
    <tr>
      <th>Kernel #</th>
      <th>Description</th>
      <th>Time (ms)</th>
      <th>Performance (GFLOPS)</th>
      <th>Relative Performance to rocBLAS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kernel 0</td>
      <td>rocBLAS</td>
      <td>4.4992</td>
      <td>30547.4</td>
      <td>100.0 %</td>
    </tr>
    <tr>
      <td>Kernel 1</td>
      <td>Naive version</td>
      <td>136.006</td>
      <td>1010.54</td>
      <td>3.3 %</td>
    </tr>
    <tr>
      <td>Kernel 2</td>
      <td>LDS tiling</td>
      <td>34.2059</td>
      <td>4017.99</td>
      <td>13.1 %</td>
    </tr>
    <tr>
      <td>Kernel 3</td>
      <td>Register tiling</td>
      <td>6.0341</td>
      <td>22777.0</td>
      <td>74.6 %</td>
    </tr>
    <tr>
      <td>Kernel 4</td>
      <td>GMEM Double buffer</td>
      <td>5.3772</td>
      <td>25559.6</td>
      <td>83.7 %</td>
    </tr>
    <tr>
      <td>Kernel 5</td>
      <td>LDS Utilization Optimization</td>
      <td>4.0994</td>
      <td>33526.6</td>
      <td>109.8 %</td>
    </tr>
    <tr>
      <td>Kernel 6</td>
      <td>VALU Utilization Optimization</td>
      <td>3.6368</td>
      <td>37791.2</td>
      <td>123.7 %</td>
    </tr>
    <tr>
      <td>Kernel 7</td>
      <td>Unroll inner loop</td>
      <td>3.3314</td>
      <td>41255.6</td>
      <td>135.1 %</td>
    </tr>
    <tr>
      <td><strong>Kernel 8</strong></td>
      <td><strong>GMEM loads</strong></td>
      <td><strong>2.8032</strong></td>
      <td><strong>49047.3</strong></td>
      <td><strong>160.6%</strong></td>
    </tr>
  </tbody>
</table>

<div>
  <p><img src="https://seb-v.github.io/assets/images/graph0.jpg" alt="Alt text"/></p><p>Figure 44 : performance results in GFLOPS/s</p>
</div>



<p>This has been an exciting journey. What started as a simple experiment to try out HIP on Windows turned into a deep dive into the hardware details of RDNA3. My biggest inspiration for this blog was Simon Boehm’s technical post<sup id="fnref:9"><a href="#fn:9" rel="footnote" role="doc-noteref">9</a></sup> on matrix multiplication in CUDA—an incredibly well-written piece that clearly influenced Kernel 3.</p>

<p>HIP tooling on Windows is quite limited. For instance, RGP does not display bank conflicts by default. However, with enough practice, it becomes possible to analyze most performance bottlenecks using the instruction timing view.</p>

<p>Even though the performance results are impressive—outperforming rocBLAS by 60%—this code is clearly not scalable in its current state. Furthermore, performing custom ISA optimizations makes these changes RDNA3-specific, limiting portability. As the codebase grows, modifications become increasingly difficult to implement.</p>

<p>That being said, the goal of this personal project was to push performance to the limit without worrying about maintainability or flexibility. While matrix multiplication can be implemented in just a few lines of code, writing an optimized implementation is incredibly challenging. We achieved a 50x speedup between the naive kernel and our best kernel, which, in my experience, would not have been possible using only HIP C++. This highlights the value of projects like OpenAI’s Triton, which I find particularly interesting and worth exploring in the future.</p>

<p>Although reaching almost 50 TFLOP/s is a solid achievement, we are still not fully VALU-bound, meaning there’s likely more performance left on the table. One technique I haven’t tested yet is LDS double buffering, which could eliminate one of the barriers and potentially improve the distribution of LDS instructions across the SIMD.</p>

<p>Finally, I want to thank Francois Guthmann for our brainstorming session on LDS optimization, which inspired the approach used in Kernel 4.</p>

<p>This project has been both fun and insightful, and I look forward to investigating further optimizations in the future</p>

<p>All the code for the 8 kernels can be found on this github <a href="https://github.com/seb-v/fp32_sgemm_amd">here</a></p>



<p><strong>18/02/2024</strong> : Updated Figures 28 and 29. Added missing SRC2 used by destination operand for FMAC_F32. Thanks to Aditya Atluri for pointing this out.</p>
<h2 id="references">References</h2>


      </div></div>
  </body>
</html>
