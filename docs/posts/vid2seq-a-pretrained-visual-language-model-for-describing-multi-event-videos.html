<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html">Original</a>
    <h1>Vid2Seq: A pretrained visual language model for describing multi-event videos</h1>
    
    <div id="readability-page-1" class="page"><div>
<div id="post-body-7154952942085542159">
<p><span>Posted by Antoine Yang, Student Researcher, and Arsha Nagrani, Research Scientist, Google Research, Perception team</span>


</p><p>
Videos have become an increasingly important part of our daily lives, spanning fields such as entertainment, education, and communication. Understanding the content of videos, however, is a challenging task as videos often contain multiple events occurring at different time scales. For example, a video of a musher hitching up dogs to a dog sled before they all race away involves a long event (the dogs pulling the sled) and a short event (the dogs being hitched to the sled). One way to spur research in video understanding is via the task of <a href="https://arxiv.org/abs/1705.00754">dense video captioning</a>, which consists of temporally localizing and describing all events in a minutes-long video. This differs from <a href="https://ai.googleblog.com/2018/09/conceptual-captions-new-dataset-and.html">single image captioning</a> and <a href="https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html">standard video captioning</a>, which consists of describing short videos with a <em>single</em> sentence. 
</p> <p>
Dense video captioning systems have wide applications, such as making videos accessible to people with visual or auditory impairments, automatically generating <a href="https://support.google.com/youtube/answer/9884579?hl=en">chapters for videos</a>, or improving the search of video moments in large databases. Current dense video captioning approaches, however, have several limitations — for example, they often contain highly specialized task-specific components, which make it challenging to integrate them into <a href="https://ai.googleblog.com/2022/05/image-text-pre-training-with.html">powerful foundation models</a>. Furthermore, they are often trained exclusively on manually annotated datasets, which are very difficult to obtain and hence are not a scalable solution.
</p>
<p>
In this post, we introduce “<a href="https://arxiv.org/abs/2302.14115">Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning</a>”, to appear at <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>. The Vid2Seq architecture augments a <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">language model</a> with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. In order to pre-train this unified model, we leverage <a href="https://arxiv.org/abs/2201.02639">unlabeled narrated videos</a> by reformulating sentence boundaries of transcribed speech as pseudo-event boundaries, and using the transcribed speech sentences as pseudo-event captions. The resulting Vid2Seq model pre-trained on millions of narrated videos improves the state of the art on a variety of dense video captioning benchmarks including <a href="https://arxiv.org/abs/1703.09788">YouCook2</a>, <a href="https://arxiv.org/abs/2011.11760">ViTT</a> and <a href="https://arxiv.org/abs/1705.00754">ActivityNet Captions</a>. Vid2Seq also generalizes well to the few-shot dense video captioning setting, the video paragraph captioning task, and the standard video captioning task. Finally, we have also released the <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq">code for Vid2Seq here</a>.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVVcI55hsEvo07M0sxzfbelN6zuwytp2d0EhfPQ3Ukq_VhZ3b-Teq7gkkAC-niiqs4LkCicFAHzbk5omE-MDHiudUKp-qUnFaoGjYa52A9EC4w8g6c7H5SUjQPmXgZn5QCucoeiGWmUeojqfbdL5EKpYUvQQ8S9PmZT5AeBP4McNPEcFRlsviVP_4I_w/s778/image1.gif" imageanchor="1"><img data-original-height="344" data-original-width="778" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVVcI55hsEvo07M0sxzfbelN6zuwytp2d0EhfPQ3Ukq_VhZ3b-Teq7gkkAC-niiqs4LkCicFAHzbk5omE-MDHiudUKp-qUnFaoGjYa52A9EC4w8g6c7H5SUjQPmXgZn5QCucoeiGWmUeojqfbdL5EKpYUvQQ8S9PmZT5AeBP4McNPEcFRlsviVP_4I_w/s16000/image1.gif"/></a></td></tr><tr><td>Vid2Seq is a visual language model that predicts dense event captions together with their temporal grounding in a video by generating a single sequence of tokens.</td></tr></tbody></table>




<h2>A visual language model for dense video captioning</h2>


<p>
Multimodal <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">transformer</a> architectures have improved the state of the art on a wide range of video tasks, such as <a href="https://ai.googleblog.com/2022/03/multimodal-bottleneck-transformer-mbt.html">action recognition</a>. However it is not straightforward to adapt such an architecture to the complex task of jointly localizing and captioning events in minutes-long videos.
</p>
<p>
For a general overview of how we achieve this, we augment a visual language model with special time tokens (like <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">text tokens</a>) that represent discretized timestamps in the video, similar to <a href="https://ai.googleblog.com/2022/04/pix2seq-new-language-interface-for.html">Pix2Seq</a> in the spatial domain. Given visual inputs, the resulting Vid2Seq model can both take as input and generate sequences of text and time tokens. First, this enables the Vid2Seq model to understand the temporal information of the transcribed speech input, which is cast as a single sequence of tokens. Second, this allows Vid2Seq to jointly predict dense event captions and temporally ground them in the video while generating a <em>single</em> sequence of tokens.
</p>
<p>
The Vid2Seq architecture includes a visual encoder and a text encoder, which encode the video frames and the transcribed speech input, respectively. The resulting encodings are then forwarded to a text decoder, which autoregressively predicts the output sequence of dense event captions together with their temporal localization in the video. The architecture is initialized with a <a href="https://arxiv.org/abs/2103.00020">powerful visual backbone</a> and a <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">strong language model</a>.
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN6d3wldxuXdGsb5wlGBf9zhxi4QgXrpZ2U1Tj3Hw41ZkoyIqePuHsmhFp3bTk05PpBb1-eG5dVWMmq_At1g_MOVKrSwouwN78LlkzgFIMffOUgLR2aqaZvDnv4vKahQCAf4S0W0nj5evWT29zDXFkzP3WMA5VYikAAG7TKzwUZyi9_P_KJf1vXSh6VQ/s518/image2.gif" imageanchor="1"><img data-original-height="518" data-original-width="518" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN6d3wldxuXdGsb5wlGBf9zhxi4QgXrpZ2U1Tj3Hw41ZkoyIqePuHsmhFp3bTk05PpBb1-eG5dVWMmq_At1g_MOVKrSwouwN78LlkzgFIMffOUgLR2aqaZvDnv4vKahQCAf4S0W0nj5evWT29zDXFkzP3WMA5VYikAAG7TKzwUZyi9_P_KJf1vXSh6VQ/s16000/image2.gif"/></a></td></tr><tr><td>Vid2Seq model overview: We formulate dense event captioning as a sequence-to-sequence problem, using special time tokens to allow the model to seamlessly understand and generate sequences of tokens containing both textual semantic information and temporal localization information grounding each text sentence in the video.</td></tr></tbody></table>




<h2>Large-scale pre-training on untrimmed narrated videos</h2>


<p>
Due to the dense nature of the task, the manual collection of annotations for dense video captioning is particularly expensive. Hence we pre-train the Vid2Seq model using unlabeled<em> </em><a href="https://arxiv.org/abs/1906.03327">narrated videos</a>, which are easily available at scale. In particular, we use the <a href="https://arxiv.org/abs/2201.02639">YT-Temporal-1B </a>dataset, which includes 18 million narrated videos covering a wide range of domains.
</p>
<p>
We use transcribed speech sentences and their corresponding timestamps as supervision, which are cast as a single sequence of tokens. We pre-train Vid2Seq with a generative objective that teaches the decoder to predict the transcribed speech sequence given visual inputs only, and a denoising objective that encourages multimodal learning by requiring the model to predict masked tokens given a noisy transcribed speech sequence and visual inputs. In particular, noise is added to the speech sequence by randomly masking out spans of tokens.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhSr5SYnii7e_vW6NBw06BuFEY5R0r9whWPNDWc8eNXlQq5KBJ4_vYkXwHVrAq5LIevI3DBCqbkwgR9GbS6rlzpGsvEPqMiG1O-PmzRY-8zl9bOiVR2Udvr9TDLSntKRE37cVsYc_JL7NeRAOrXaEuh1nhRnhCaonRvAJo81mVc_cgeMnrHDVoJYPgZTw/s864/image4.gif" imageanchor="1"><img data-original-height="310" data-original-width="864" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhSr5SYnii7e_vW6NBw06BuFEY5R0r9whWPNDWc8eNXlQq5KBJ4_vYkXwHVrAq5LIevI3DBCqbkwgR9GbS6rlzpGsvEPqMiG1O-PmzRY-8zl9bOiVR2Udvr9TDLSntKRE37cVsYc_JL7NeRAOrXaEuh1nhRnhCaonRvAJo81mVc_cgeMnrHDVoJYPgZTw/s16000/image4.gif"/></a></td></tr><tr><td>Vid2Seq is pre-trained on unlabeled narrated videos with a generative objective (<b>top</b>) and a denoising objective (<b>bottom</b>).</td></tr></tbody></table>



<h2>Results on downstream dense video captioning benchmarks</h2>


<p>
The resulting pre-trained Vid2Seq model can be fine-tuned on downstream tasks with a simple maximum likelihood objective using <a href="https://ieeexplore.ieee.org/document/6795228">teacher forcing</a> (i.e., predicting the next token given previous ground-truth tokens). After fine-tuning, Vid2Seq notably improves the state of the art on three standard downstream dense video captioning benchmarks (<a href="https://arxiv.org/abs/1705.00754">ActivityNet Captions</a>, <a href="https://arxiv.org/abs/1703.09788">YouCook2</a> and <a href="https://arxiv.org/abs/2011.11760">ViTT</a>) and two video clip captioning benchmarks (<a href="https://ieeexplore.ieee.org/document/7780940">MSR-VTT</a>, <a href="https://aclanthology.org/P11-1020/">MSVD</a>). In <a href="https://arxiv.org/abs/2302.14115">our paper</a> we provide additional ablation studies, qualitative results, as well as results in the few-shot settings and in the video paragraph captioning task.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfe2gp6IrtueFEdgj15U1KzQU8tNK2UUZ9qOKQK6wodAsMkVaJBqBoMCAdRKzdZSCQXWrrsapMfNI23rNx3U2CLLdApebyiUxIP6KW79yMS-dM-hAgO6S1SXnp9nOLk9LspvosmriEieo4GGzE-hHfFAg81ttF4HlWr0LiqH7M-jjP5cM8px0NKhu6pA/s1999/image3.png" imageanchor="1"><img data-original-height="1333" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfe2gp6IrtueFEdgj15U1KzQU8tNK2UUZ9qOKQK6wodAsMkVaJBqBoMCAdRKzdZSCQXWrrsapMfNI23rNx3U2CLLdApebyiUxIP6KW79yMS-dM-hAgO6S1SXnp9nOLk9LspvosmriEieo4GGzE-hHfFAg81ttF4HlWr0LiqH7M-jjP5cM8px0NKhu6pA/s16000/image3.png"/></a></td></tr><tr><td>Comparison to state-of-the-art methods for dense video captioning (<b>left</b>) and for video clip captioning (<b>right</b>), on the <a href="https://arxiv.org/abs/1411.5726">CIDEr</a> metric (higher is better).</td></tr></tbody></table>



<h2>Conclusion</h2>


<p>
We introduce Vid2Seq, a novel visual language model for dense video captioning that simply predicts all event boundaries and captions as a single sequence of tokens. Vid2Seq can be effectively pretrained on unlabeled narrated videos at scale, and achieves state-of-the-art results on various downstream dense video captioning benchmarks. Learn more from <a href="https://arxiv.org/abs/2302.14115">the paper</a> and grab the <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq">code here</a>.
</p>


<h2>Acknowledgements</h2>


<p>
<em>This research was conducted by Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic and Cordelia Schmid.</em>
</p>


</div>
</div></div>
  </body>
</html>
