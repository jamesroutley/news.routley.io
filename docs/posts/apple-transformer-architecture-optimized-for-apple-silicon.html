<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/ml-ane-transformers">Original</a>
    <h1>Apple: Transformer architecture optimized for Apple Silicon</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Use <code>ane_transformers</code> as a reference PyTorch implementation if you are considering deploying your Transformer models on Apple devices with an A14 or newer and M1 or newer chip to achieve up to <strong>10 times faster</strong> and <strong>14 times</strong> lower peak memory consumption compared to baseline implementations.</p>
<p dir="auto"><code>ane_transformers.reference</code> comprises a standalone reference implementation and <code>ane_transformers.huggingface</code> comprises optimized versions of <a href="https://huggingface.co/models" rel="nofollow">Hugging Face</a> model classes such as <code>distilbert</code> to demonstrate the application of the optimization principles laid out in our research article on existing third-party implementations.</p>
<p dir="auto">Please check out our <a href="https://machinelearning.apple.com/research/apple-neural-engine" rel="nofollow">research article</a> for a detailed explanation of the optimizations as well as interactive figures to explore latency and peak memory consumption data from our case study: <a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english" rel="nofollow">Hugging Face distilbert</a> model deployment on various devices and operating system versions. Below figures are non-interactive snapshots from the research article for iPhone 13 with iOS16.0 installed:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ane-transformers/blob/main/assets/iPhone13_iOS16_latency_comparison.png"><img src="https://github.com/apple/ml-ane-transformers/raw/main/assets/iPhone13_iOS16_latency_comparison.png"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ane-transformers/blob/main/assets/iPhone13_iOS16_memory_comparison.png"><img src="https://github.com/apple/ml-ane-transformers/raw/main/assets/iPhone13_iOS16_memory_comparison.png"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-tutorial-optimized-deployment-of-hugging-face-distilbert" aria-hidden="true" href="#tutorial-optimized-deployment-of-hugging-face-distilbert"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tutorial: Optimized Deployment of Hugging Face distilbert</h2>
<p dir="auto">This tutorial is a step-by-step guide to the model deployment process from the case study in our research article. The same code is used to generate the Hugging Face distilbert performance data in the figures above.</p>
<p dir="auto">In order to begin the optimizations, we initialize the baseline model as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import transformers
model_name = &#34;distilbert-base-uncased-finetuned-sst-2-english&#34;
baseline_model = transformers.AutoModelForSequenceClassification.from_pretrained(
    model_name,
    return_dict=False,
    torchscript=True,
).eval()"><pre><span>import</span> <span>transformers</span>
<span>model_name</span> <span>=</span> <span>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
<span>baseline_model</span> <span>=</span> <span>transformers</span>.<span>AutoModelForSequenceClassification</span>.<span>from_pretrained</span>(
    <span>model_name</span>,
    <span>return_dict</span><span>=</span><span>False</span>,
    <span>torchscript</span><span>=</span><span>True</span>,
).<span>eval</span>()</pre></div>
<p dir="auto">Then we initialize the mathematically equivalent but optimized model, and we restore its parameters using that of the baseline model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from ane_transformers.huggingface import distilbert as ane_distilbert
optimized_model = ane_distilbert.DistilBertForSequenceClassification(
    baseline_model.config).eval()
optimized_model.load_state_dict(baseline_model.state_dict())"><pre><span>from</span> <span>ane_transformers</span>.<span>huggingface</span> <span>import</span> <span>distilbert</span> <span>as</span> <span>ane_distilbert</span>
<span>optimized_model</span> <span>=</span> <span>ane_distilbert</span>.<span>DistilBertForSequenceClassification</span>(
    <span>baseline_model</span>.<span>config</span>).<span>eval</span>()
<span>optimized_model</span>.<span>load_state_dict</span>(<span>baseline_model</span>.<span>state_dict</span>())</pre></div>
<p dir="auto">Next we create sample inputs for the model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
tokenized = tokenizer(
    [&#34;Sample input text to trace the model&#34;],
    return_tensors=&#34;pt&#34;,
    max_length=128,  # token sequence length
    padding=&#34;max_length&#34;,
)"><pre><span>tokenizer</span> <span>=</span> <span>transformers</span>.<span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>)
<span>tokenized</span> <span>=</span> <span>tokenizer</span>(
    [<span>&#34;Sample input text to trace the model&#34;</span>],
    <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>,
    <span>max_length</span><span>=</span><span>128</span>,  <span># token sequence length</span>
    <span>padding</span><span>=</span><span>&#34;max_length&#34;</span>,
)</pre></div>
<p dir="auto">We then trace the optimized model to obtain the expected input format (Torchscript) for the coremltools conversion tool.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
traced_optimized_model = torch.jit.trace(
    optimized_model,
    (tokenized[&#34;input_ids&#34;], tokenized[&#34;attention_mask&#34;])
)"><pre><span>import</span> <span>torch</span>
<span>traced_optimized_model</span> <span>=</span> <span>torch</span>.<span>jit</span>.<span>trace</span>(
    <span>optimized_model</span>,
    (<span>tokenized</span>[<span>&#34;input_ids&#34;</span>], <span>tokenized</span>[<span>&#34;attention_mask&#34;</span>])
)</pre></div>
<p dir="auto">Finally, we use coremltools to generate the Core ML model package file and save it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import coremltools as ct
import numpy as np
ane_mlpackage_obj = ct.convert(
    traced_optimized_model,
    convert_to=&#34;mlprogram&#34;,
    inputs=[
        ct.TensorType(
                f&#34;input_{name}&#34;,
                    shape=tensor.shape,
                    dtype=np.int32,
                ) for name, tensor in tokenized.items()
            ],
            compute_units=ct.ComputeUnit.ALL,
)
out_path = &#34;HuggingFace_ane_transformers_distilbert_seqLen128_batchSize1.mlpackage&#34;
ane_mlpackage_obj.save(out_path)"><pre><span>import</span> <span>coremltools</span> <span>as</span> <span>ct</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>ane_mlpackage_obj</span> <span>=</span> <span>ct</span>.<span>convert</span>(
    <span>traced_optimized_model</span>,
    <span>convert_to</span><span>=</span><span>&#34;mlprogram&#34;</span>,
    <span>inputs</span><span>=</span>[
        <span>ct</span>.<span>TensorType</span>(
                <span>f&#34;input_<span><span>{</span><span>name</span><span>}</span></span>&#34;</span>,
                    <span>shape</span><span>=</span><span>tensor</span>.<span>shape</span>,
                    <span>dtype</span><span>=</span><span>np</span>.<span>int32</span>,
                ) <span>for</span> <span>name</span>, <span>tensor</span> <span>in</span> <span>tokenized</span>.<span>items</span>()
            ],
            <span>compute_units</span><span>=</span><span>ct</span>.<span>ComputeUnit</span>.<span>ALL</span>,
)
<span>out_path</span> <span>=</span> <span>&#34;HuggingFace_ane_transformers_distilbert_seqLen128_batchSize1.mlpackage&#34;</span>
<span>ane_mlpackage_obj</span>.<span>save</span>(<span>out_path</span>)</pre></div>
<p dir="auto">To verify performance, developers can now launch Xcode and simply add this model package file as a resource in their projects. After clicking on the Performance tab, the developer can generate a performance report on locally available devices, for example, on the Mac that is running Xcode or another Apple device that is connected to that Mac. The figure below shows a performance report generated for this model on an iPhone 13 Pro Max with iOS 16.0 installed.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ane-transformers/blob/main/assets/xcode_performance_report_comparison.png"><img src="https://github.com/apple/ml-ane-transformers/raw/main/assets/xcode_performance_report_comparison.png"/></a></p>
<p dir="auto">Based on the figure above, the latency is improved by a factor of 2.84 times for the sequence length of 128 and batch size of 1 that were chosen for the tutorial. Higher sequence lengths, such as 512, and batch sizes, such as 8, will yield up to <strong>10 times lower latency</strong> and <strong>14 times lower peak memory consumption</strong>. Please refer to Figure 2 from our <a href="https://machinelearning.apple.com/research/apple-neural-engine" rel="nofollow">research article</a> for detailed and interactive performance data.</p>
<p dir="auto">Note that the load and compilation times increase due to the number of operations increasing in the optimized model but these are one-time costs and user experience will not be affected if the model is loaded asynchronously.</p>
<p dir="auto">Note that 4 of the 606 operations in the optimized model are executed on the CPU. These are the embedding lookup related operations and they are more efficient to do on the CPU for this particular model configuration.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-a-note-on-unit-tests" aria-hidden="true" href="#a-note-on-unit-tests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>A Note on Unit Tests</h2>
<p dir="auto">The unit tests measure, among other things, the ANE speed-up factor. Since the device spec for this reference implementation is M1 or newer chips for the Mac and A14 and newer chips for the iPhone and iPad, the speed-up unit tests will print a warning message if executed on devices outside of this spec. Even if the model is generated using an out of spec Mac, the model should work as expected on in-spec devices.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-installation--troubleshooting" aria-hidden="true" href="#installation--troubleshooting"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation &amp; Troubleshooting</h2>
<ul dir="auto">
<li>Fastest: <code>pip install ane_transformers</code></li>
<li>Locally editable: <code>pip install -e .</code></li>
<li>If installation fails with <code>ERROR: Failed building wheel for tokenizers</code> or <code>error: can&#39;t find Rust compiler</code>, please follow this <a href="https://github.com/huggingface/transformers/issues/2831#issuecomment-592724471" data-hovercard-type="issue" data-hovercard-url="/huggingface/transformers/issues/2831/hovercard">solution</a></li>
</ul>
</article>
          </div></div>
  </body>
</html>
