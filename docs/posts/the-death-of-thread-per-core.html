<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/">Original</a>
    <h1>The death of thread per core</h1>
    
    <div id="readability-page-1" class="page"><div>
            
            <date>
                
                
                October 20, 2025
                
                
            </date>
            

            

            

            
            
            <p><img alt="NULL BITMAP.png" src="https://assets.buttondown.email/images/b496ee4d-509d-4108-91c9-289bd6057f33.png"/></p>
            
            

            
            
            <p>Programming language async runtimes are very focused on handling asynchronous, possibly long running tasks, that might yield for a variety of reasons, that themselves might spawn future work.</p>
<p>In an async runtime like async Rust, the model is that a task can <em>yield</em>, which, conceptually, creates a new piece of work that gets shoved onto the work queues (which is &#34;resume that task&#34;). You might not think of it as &#34;this task is suspended and will be resumed later&#34; as much as &#34;this piece of work is done and has spawned a new piece of work.&#34; This new piece of work gets pushed onto a local queue for later processing by the same thread. The primary distinction between thread-per-core approaches and work-stealing approaches is that in work-stealing models, if one thread doesn&#39;t have enough work to do, it can &#34;steal&#34; that task and move it over to its own queue.</p>
<p>This has several immediate consequences:</p>
<ul>
<li>It has to be okay to move those pieces of work across thread boundaries. This is the cause of people&#39;s frustration with their futures having to be<em> <code>Send</code></em>, in Rust.  </li>
<li>Work can be more evenly balanced. If stealing isn&#39;t allowed, then there might be a thread, or handful of threads, with a long work queue, while all the others (and their associated CPU cores) sit idle. Stealing is an elegant solution to that problem.  </li>
<li>If any task can be stolen from any other thread, you lose certain locality guarantees: if you know stealing isn&#39;t allowed, and two tasks both operate on similar data, you might hope that they can benefit from sharing cache lines.</li>
</ul>
<p>In the data processing world, for a couple years there it seemed like the needle had firmly swung in the direction of thread-per-core. Yes, of course you should partition your data across threads—cross-core data movement is the only enemy! Of course skewed data is a problem to be solved at a higher level, the data processing layer is optimized to scream through all the data you give it, so needing to be friendly in how you dish that work out is a small price to pay.</p>
<p>If your keys are basically random, this is great: the benefits are real, data tends to stay in cache, you don’t need slow MESI messages creating contention, and implementation is often dramatically simplified by restricting parallelism to very specific points in the code.</p>
<p>Except it seems like there’s been an increase in <a href="https://arxiv.org/abs/2505.04153?utm_source=jaffray&amp;utm_medium=email&amp;utm_campaign=the-death-of-thread-per-core" target="_blank">dissenters</a> over the last several years: actually, maybe the data processing layer can be the cleanest place to put dynamic reshuffling work. The paper on <a href="https://db.in.tum.de/~leis/papers/morsels.pdf?utm_source=jaffray&amp;utm_medium=email&amp;utm_campaign=the-death-of-thread-per-core" target="_blank">Morsel-Driven Parallelism</a> proposes some reasons why this kind of <a href="https://buttondown.com/jaffray/archive/reification-and-exchange/" target="_blank">exchange</a> focused parallelism might no longer be the best model:</p>
<ul>
<li>Increasing core counts on high end machines means that improperly handling skewed data distributions are more painful.  </li>
<li>Many traditional bottlenecks, like IO latency, have improved massively since the days where Exchange was state-of-the-art. At that time, ensuring maximum CPU utilization was not so important, since you’d typically be bound by other things, but things like disk speed has improved dramatically in the last 10 years  while CPU speeds have not.</li>
</ul>
<p>The problem of debating concurrency models for data processing is a bit different than that for programming language async runtimes, I think. We have a lot less heterogenous types of work, and we, as the scheduler, can do a lot more predictive introspection about what data a piece of work is likely to need, and we can manipulate tasks algebraically to even merge or split them up.</p>
<p>This sort of freedom is, I think, another big reason why shared-state concurrency has once again become popular. If you&#39;re implementing a query engine, you simply have more insight into the type of work you&#39;re going to do, which lets your scheduler make smarter decisions.</p>
<p>On top of all those things, I think another big reason is cultural: the more your data systems scale and need to handle things like multitenancy effectively, the more prone you are to skew you have very little control over. &#34;Solve the skew problem a layer up&#34; is not a particularly effective strategy for certain levels of scale, and you need to just bite the bullet and have systems that have that kind of elasticity built into them directly.</p>
            
            

            

            



        </div></div>
  </body>
</html>
