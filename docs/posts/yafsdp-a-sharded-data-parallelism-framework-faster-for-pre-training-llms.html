<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/yandex/YaFSDP">Original</a>
    <h1>YaFSDP: a sharded data parallelism framework, faster for pre-training LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p><a target="_blank" rel="noopener noreferrer" href="https://pjg1.site/yandex/YaFSDP/blob/main/assets/yafsdp_logo.png#gh-light-mode-only"><img src="https://pjg1.site/yandex/YaFSDP/raw/main/assets/yafsdp_logo.png#gh-light-mode-only" width="400px"/></a>
 <a target="_blank" rel="noopener noreferrer" href="https://pjg1.site/yandex/YaFSDP/blob/main/assets/yafsdp_logo_white.png#gh-dark-mode-only"><img src="https://pjg1.site/yandex/YaFSDP/raw/main/assets/yafsdp_logo_white.png#gh-dark-mode-only" width="400px"/></a>
</p>
Â 
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#advantages-over-fsdp">Advantages over FSDP</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#issues-and-questions">Issues and questions</a></li>
<li><a href="#citation">Citation</a></li>
</ul>

<p dir="auto">YaFSDP is a Sharded Data Parallelism framework, designed to work well with transformer-like
neural network architectures.</p>
<p dir="auto">You can find more info on YaFSDP internals in our blog posts on
<a href="https://medium.com/yandex/yafsdp-a-tool-for-faster-llm-training-and-optimized-gpu-utilization-is-no-632b7539f5b3" rel="nofollow">Medium</a>
and <a href="https://habr.com/ru/companies/yandex/articles/817509/" rel="nofollow">Habr</a>.</p>

<p dir="auto">YaFSDP is up to 20% faster for pre-training LLMs and performs better in high
memory pressure conditions. It is designed to reduce communications and memory
operations overhead.</p>
<p dir="auto">YaFSDP:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://pjg1.site/yandex/YaFSDP/blob/main/assets/ya_fsdp.png"><img src="https://pjg1.site/yandex/YaFSDP/raw/main/assets/ya_fsdp.png" alt="ya_fsdp"/></a></p>
<p dir="auto">FSDP:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://pjg1.site/yandex/YaFSDP/blob/main/assets/fsdp.png"><img src="https://pjg1.site/yandex/YaFSDP/raw/main/assets/fsdp.png" alt="fsdp"/></a></p>

<p dir="auto">We&#39;ve compared YaFSDP with FSDP on a variety of pre-training setups ranging from:</p>
<ul dir="auto">
<li>7B to 70B parameters</li>
<li>64 to 256 devices</li>
<li>2048 to 8192 tokens per sequence</li>
</ul>
<table>
<thead>
<tr>
<th>model</th>
<th>gpu-count</th>
<th>seq-len</th>
<th>num-ckpt-layers</th>
<th>speedup</th>
<th>YaFSDP iteration time (s)</th>
<th>FSDP iteration time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>2048</td>
<td>0</td>
<td>9.92%</td>
<td>0.81</td>
<td>0.90</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>4096</td>
<td>0</td>
<td>3.43%</td>
<td>1.16</td>
<td>1.21</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>8192</td>
<td>0</td>
<td>2.68%</td>
<td>2.23</td>
<td>2.29</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>9.57%</td>
<td>0.87</td>
<td>0.97</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>4096</td>
<td>0</td>
<td>2.42%</td>
<td>1.19</td>
<td>1.22</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>8192</td>
<td>0</td>
<td>2.32%</td>
<td>2.25</td>
<td>2.31</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>12.10%</td>
<td>1.55</td>
<td>1.76</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>128</td>
<td>4096</td>
<td>0</td>
<td>3.49%</td>
<td>2.06</td>
<td>2.14</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>20.70%</td>
<td>3.39</td>
<td>4.27</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>256</td>
<td>2048</td>
<td>0</td>
<td>21.99%</td>
<td>3.51</td>
<td>4.50</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>256</td>
<td>4096</td>
<td>5</td>
<td>8.35%</td>
<td>5.33</td>
<td>5.81</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>256</td>
<td>2048</td>
<td>10</td>
<td>21.48%</td>
<td>6.97</td>
<td>8.87</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>256</td>
<td>4096</td>
<td>50</td>
<td>7.17%</td>
<td>11.07</td>
<td>11.93</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>64</td>
<td>2048</td>
<td>0</td>
<td>11.91%</td>
<td>0.97</td>
<td>1.10</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>64</td>
<td>4096</td>
<td>0</td>
<td>7.86%</td>
<td>1.36</td>
<td>1.48</td>
</tr>
<tr>
<td>Llama 3 70B</td>
<td>256</td>
<td>2048</td>
<td>20</td>
<td>26.60%</td>
<td>7.17</td>
<td>9.76</td>
</tr>
</tbody>
</table>
<p dir="auto">Details:</p>
<ul dir="auto">
<li>In each run per-device batch size is set to 1.</li>
<li><code>speedup</code> represents relative iteration time decrease between YaFSDP and FSDP runs.</li>
<li><code>num-ckpt-layers</code> refers to the number of transformer layers to which
activation checkpointing was applied.</li>
<li>Performance was measured using a cluster of hosts with A100 80 GB GPUs.</li>
</ul>

<p dir="auto">You can find examples of LLM training using ðŸ¤— stack in the <code>examples</code> folder:</p>
<ol dir="auto">
<li><code>clm.md</code> for causal pre-training</li>
<li><code>sft.md</code> for supervised fine-tuning</li>
</ol>
<p dir="auto">Notice that both examples require a Docker image, which can be built using
<code>docker/build.sh</code> script. The image is based on the <a href="https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-02.html" rel="nofollow">NVIDIA PyTorch
image</a>
with some patched ðŸ¤— libraries. Patches for the libraries can be found in the
<code>patches</code> folder.</p>

<p dir="auto">If you encounter any bugs of have any questions <a href="https://github.com/yandex/YaFSDP/issues/new">feel free to open a GitHub issue</a>.</p>

<p dir="auto">If you use this codebase, please cite it by using the following BibTeX entry:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{YaFSDP2024,
  author =       {Mikhail Khrushchev and Anton Frolov and Ruslan Vasilev},
  title =        {YaFSDP: Yet another Fully Sharded Data Parallel},
  howpublished = {\url{https://github.com/yandex/YaFSDP}},
  year =         {2024}
}"><pre><span>@misc</span>{<span>YaFSDP2024</span>,
  <span>author</span> =       <span><span>{</span>Mikhail Khrushchev and Anton Frolov and Ruslan Vasilev<span>}</span></span>,
  <span>title</span> =        <span><span>{</span>YaFSDP: Yet another Fully Sharded Data Parallel<span>}</span></span>,
  <span>howpublished</span> = <span><span>{</span>\url{https://github.com/yandex/YaFSDP}<span>}</span></span>,
  <span>year</span> =         <span><span>{</span>2024<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
