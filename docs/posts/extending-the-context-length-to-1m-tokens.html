<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qwenlm.github.io/blog/qwen2.5-turbo/">Original</a>
    <h1>Extending the context length to 1M tokens</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><p><a href="https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen" target="_blank">API Documentation (Chinese)</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo" target="_blank">HuggingFace Demo</a>
<a href="https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo" target="_blank">ModelScope Demo</a></p><p>After the release of Qwen2.5, we heard the community’s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:</p><ol><li><strong>Longer Context Support</strong>: We have extended the model’s context length from 128k to <strong>1M</strong>, which is approximately <strong>1 million English words or 1.5 million Chinese characters</strong>, equivalent to 10 full-length novels, 150 hours of speech transcripts, or 30,000 lines of code. The model achieves <strong>100%</strong> accuracy in the 1M length Passkey Retrieval task and scores <strong>93.1</strong> on the long text evaluation benchmark RULER, surpassing GPT-4’s 91.6 and GLM4-9B-1M’s 89.9. Additionally, the model maintains very strong competitiveness in short sequence capabilities, on par with GPT-4o-mini.</li><li><strong>Faster Inference Speed</strong>: Using sparse attention mechanisms, we successfully reduced the time to first token for processing a context of 1M tokens from 4.9 minutes to 68 seconds, <strong>achieving a 4.3x speedup</strong>.</li><li><strong>Lower Cost</strong>: The price remains ¥0.3 / 1M tokens. At the same cost, Qwen2.5-Turbo can process <strong>3.6 times the number of tokens</strong> as GPT-4o-mini.</li></ol><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/cover_en.png#center" width="100%"/></figure><p>Now, you can use it through the API service of <a href="https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio">Alibaba Cloud Model Studio [Chinese]</a>, or through <a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo">HuggingFace Demo</a> or <a href="https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo">ModelScope Demo</a>.</p><p>We have prepared some demos to showcase the new uses that Qwen2.5-Turbo can bring, covering deep understanding of long novels, repository-level code assistant, and reading multiple papers.</p><div><div><p><span>Example: Understanding Long Novels</span>
<a>Next</a></p><div><p>User</p><div><p>[Upload the three Chinese novels of <code>The Three-Body Problem</code>, which contain 690k tokens.]</p><p>Provide a summary of the plots of these three novels in English</p></div><p>Qwen2.5-Turbo</p><div><p>The three novels you mentioned from the “Earth’s Past” series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.</p><p><strong>First Novel: “Earth’s Past”</strong>
In the first novel, the narrative focuses on the development of a top-secret project called “Red Coast” (红岸) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.</p><p><strong>Second Novel: “Dark Forest”</strong>
The second novel, titled “Dark Forest,” delves into the concept of the “Dark Forest Theory.” According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as “water drops” (水滴). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the “Wall-Bearing Plan” (面壁计划), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.</p><p><strong>Third Novel: “Death’s End”</strong>
In “Death’s End,” Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (执剑人) brings him immense personal struggle and psychological turmoil. As Luo Ji’s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (黑域) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth.
The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.</p></div></div></div></div><p>The latest Qwen2.5-Turbo supporting 1M tokens is used in the same way as the standard Qwen API and is compatible with the OpenAI API. Below is a simple Python call example (Note: You need to set the environment variable <code>YOUR_API_KEY</code> to your API Key. For more details, please visit <a href="https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen">Quick Start of Alibaba Cloud Model Studio [Chinese]</a>):</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>os</span>
</span></span><span><span>
</span></span><span><span><span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>
</span></span><span><span>
</span></span><span><span><span># Input a long text file</span>
</span></span><span><span><span>with</span> <span>open</span><span>(</span><span>&#34;example.txt&#34;</span><span>,</span> <span>&#34;r&#34;</span><span>,</span> <span>encoding</span><span>=</span><span>&#34;utf-8&#34;</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
</span></span><span><span>    <span>text</span> <span>=</span> <span>f</span><span>.</span><span>read</span><span>()</span>
</span></span><span><span><span>user_input</span> <span>=</span> <span>text</span> <span>+</span> <span>&#34;</span><span>\n\n</span><span>Summarize the above text.&#34;</span>
</span></span><span><span>
</span></span><span><span><span>client</span> <span>=</span> <span>OpenAI</span><span>(</span>
</span></span><span><span>    <span>api_key</span><span>=</span><span>os</span><span>.</span><span>getenv</span><span>(</span><span>&#34;YOUR_API_KEY&#34;</span><span>),</span>
</span></span><span><span>    <span>base_url</span><span>=</span><span>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span>,</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>completion</span> <span>=</span> <span>client</span><span>.</span><span>chat</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>(</span>
</span></span><span><span>    <span>model</span><span>=</span><span>&#34;qwen-turbo-latest&#34;</span><span>,</span>
</span></span><span><span>    <span>messages</span><span>=</span><span>[</span>
</span></span><span><span>      <span>{</span><span>&#39;role&#39;</span><span>:</span> <span>&#39;system&#39;</span><span>,</span> <span>&#39;content&#39;</span><span>:</span> <span>&#39;You are a helpful assistant.&#39;</span><span>},</span>
</span></span><span><span>      <span>{</span><span>&#39;role&#39;</span><span>:</span> <span>&#39;user&#39;</span><span>,</span> <span>&#39;content&#39;</span><span>:</span> <span>user_input</span><span>},</span>
</span></span><span><span>    <span>],</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>completion</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>]</span><span>.</span><span>message</span><span>)</span>
</span></span></code></pre></div><p>In this section, we evaluate the performance of Qwen2.5-Turbo through various benchmark tests and improvements in inference speed.</p><h2 id="passkey-retrieval">Passkey Retrieval</h2><p>We first conducted experiments on the 1M-token Passkey Retrieval task. The results show that Qwen2.5-Turbo can perfectly capture all hidden numbers in the 1M tokens of irrelevant text, demonstrating the model’s ability to capture detailed information in ultra-long contexts.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/passkey_retrieval.png#center" width="100%"/></figure><h2 id="more-complex-long-text-tasks">More Complex Long Text Tasks</h2><p>We select several datasets of long text understanding to test the model, including:</p><ul><li><a href="https://github.com/hsiehjackson/RULER">RULER</a>: An extended benchmark based on Needle in a Haystack, tasks include finding multiple “needles” in irrelevant contexts, answering multiple questions, or finding the most or least frequent words in the context. The maximum context length is 128K.</li><li><a href="https://github.com/infinigence/LVEval">LV-Eval</a>: A benchmark test requiring simultaneous understanding of numerous evidence fragments. We adjust the evaluation metrics in the original version of LV-Eval to avoid false negatives caused by overly strict matching rules. The maximum context length is 256K.</li><li><a href="https://github.com/THUDM/LongAlign">LongbenchChat</a>: A dataset evaluating human preference alignment in the tasks of long context. The maximum context length is 100K.</li></ul><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/long_result.png#center" width="100%"/></figure><p>The results show that Qwen2.5-Turbo has advantages in various tasks of long context:</p><ul><li>In the RULER benchmark test, Qwen2.5-Turbo scores 93.1, surpassing GPT-4o-mini and even GPT-4, proving its excellent ability to handle long text tasks.</li><li>In more tasks of long context understanding like LV-Eval and LongBench-Chat, Qwen2.5-Turbo surpasses GPT-4o-mini in most dimensions and can process tasks with a context of over 128K tokens.</li></ul><h2 id="short-text-tasks">Short Text Tasks</h2><p>In addition to performance improvements in tasks of long context, we are also concerned about the model’s performance in tasks of short context. The existing context length extension methods often lead to significant performance degradation when processing short texts. Therefore, we have paid special attention to this issue when building Qwen2.5-Turbo, ensuring that the extension of context length almost does not affect the ability of short text understanding.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/short_result.png#center" width="100%"/></figure><p>Results on short text benchmarks show that Qwen2.5-Turbo significantly surpasses previous open-source models with a context length of 1M tokens in most tasks; compared to GPT-4o-mini and Qwen2.5-14B-Instruct models, Qwen2.5-Turbo achieves similar performance in short text tasks while supporting 8 times the context length.</p><h2 id="inference-speed">Inference Speed</h2><p>We tested the TTFT (time to first token) for inputs of different lengths. On the sequences of 1M tokens, we used sparse attention to compress the computation of the attention by about 12.5 times, achieving a speedup of 3.2 to 4.3 times under different hardware configurations.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/inference_speed.png#center" width="70%"/></figure><p>While we are pleased to finally extend the context of Qwen2.5-Turbo to 1M tokens, we also recognize that the current model does not always perform satisfactorily when solving long sequence tasks in real applications. There are many unresolved challenges, such as the model’s performance being more unstable in long sequence tasks, and the cost of inference making it difficult to use larger models. However, we will actively explore further alignment of human preferences in long sequences, optimize inference efficiency to reduce computation time, and attempt to launch larger and stronger long-context models. We look forward to sharing new progress in developing long-context models with you soon, so stay tuned!</p></div></article></div></div>
  </body>
</html>
