<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://robotics-transformer-x.github.io/">Original</a>
    <h1>RT-X: the largest open-source robot dataset</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
        <div>
            <h2>
                <strong>
                    <SPAN size="+6">Open X-Embodiment: Robotic Learning Datasets and RT-X Models </SPAN><br/>
            </strong></h2></div><p><strong>
        


        

        <div>
            
            

            <div>
                <h3>
                    <b>Abstract</b>
                </h3>
                <p>
                    Large, high-capacity models trained on diverse datasets have shown remarkable successes on
                    efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led
                    to a consolidation of pretrained models, with general pretrained backbones serving as a starting
                    point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic
                    learning methods train a separate model for every application, every robot, and even every
                    environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new
                    robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and
                    models to make it possible to explore this possibility in the context of robotic manipulation,
                    alongside experimental results that provide an example of effective X-robot policies. We assemble a
                    dataset from 22 different robots collected through a collaboration between 21 institutions,
                    demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data,
                    which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by
                    leveraging experience from other platforms.
                </p>
            </div>
            <div>
                <div>
                    
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/pick_ice_cream_cropped_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>pick ice cream</p>
                    </div>
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/move_red_pepper_to_A_trimmed_cropped_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>move red pepper to A</p>
                    </div>
                </div>
            </div>
            <div>
                <p><span>RT-2-X (55B)</span>:
                    one of the
                    <span>biggest models</span> to date performing
                    <span>unseen tasks</span> in <span>academic labs</span>
                </p>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    <b>Dataset Overview</b>
                </h3>
                <p><img src="https://robotics-transformer-x.github.io/img/overview.png"/></p><p>We introduce the Open X-Embodiment Dataset, the largest open-source real robot dataset to date.
                    It contains 1M+ real robot trajectories spanning 22 robot embodiments,
                    from single robot arms to bi-manual robots and quadrupeds. </p>

                <p><img src="https://robotics-transformer-x.github.io/img/data_analysis.png"/></p><p>The dataset was constructed by pooling 60 existing robot datasets from 34 robotic research labs
                    around the world. Our analysis shows that the number of visually distinct scenes is
                    well-distributed across different robot embodiments and that the dataset includes a wide range
                    of common behaviors and household objects.
                    For a detailed listing of all included datasets,
                    see <a target="_blank" href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">this Google Sheet</a>.
                </p>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    <b>Model Overview</b>
                </h3>
                <p><img src="https://robotics-transformer-x.github.io/img/algorithm.png"/></p><p>We train two models on the robotics data mixture: (1) RT-1, an efficient Transformer-based
                    architecture designed for robotic control, and (2) RT-2, a large vision-language model co-fine-tuned
                    to output robot actions as natural language tokens.
                </p>

                <p>
                    Both models output robot actions represented with respect to the robot gripper frame. The robot
                    action is a 7-dimensional vector consisting of x, y, z, roll, pitch, yaw, and gripper opening or the
                    rates of these quantities. For data sets where some of these dimensions are not exercised by the
                    robot, during training, we set the value of the corresponding dimensions to zero.
                </p>

                <p>
                    We refer to the RT-1 model trained using the robotic data mixture as <span>RT-1-X</span>,
                    and the RT-2 model
                    trained using the robotic data mixture as <span>RT-2-X</span>.
                </p>

            </div>
        </div>

        <div>
            <p>
                <h3>
                    <b>Results</b>
                </h3>

                <h4>
                    RT-1-X evaluation on in-distribution skills
                </h4>
            </p>

            <div>
                <div>
                    
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/task_agnostic_open_drawer_rt1x_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>At University of Freiburg (AiS)</p>
                    </div>
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/nyuenv1_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>At NYU (CILVR)</p>
                    </div>
                </div>

                <div>
                    
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/bridge_cropped_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>At Stanford (IRIS)</p>
                    </div>
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/jaco_play_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>At USC (CLVR)</p>
                    </div>
                </div>
            </div>
            <div>

                <p><span>RT-1-X</span>
                    performing diverse tasks in
                    <span>6 academic labs</span>
                </p>

                <p><img src="https://robotics-transformer-x.github.io/img/rt1x.png"/></p><p><span>RT-1-X models</span>
                    outperform RT-1 or Original Methods trained on individual datasets by
                    <span> 50% in the small-data domain</span>
                </p>

                <p>
                    Original Method refers to the model developed by the creators of the dataset
                    trained only on that respective dataset. The Original Method constitutes a reasonable baseline
                    insofar as it can be expected that the model has been optimized to work well with the associated
                    data. The lab logos indicate the physical location of real robot evaluation, and the robot pictures
                    indicate the embodiment used for the evaluation.
                </p>

                <h4>
                    RT-2-X evaluation on emergent skills
                </h4>
            </div>
            <div>
                <div>
                    
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/move_apple_on_cloth_cropped_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>move apple on cloth</p>
                    </div>
                    <div>
                        <video onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);" autoplay="" loop="" muted="">
                            <source src="video/move_apple_between_can_and_orange_cropped_removed.mp4" type="video/mp4"/>
                            Your browser does not support the video tag.
                        </video>
                        <p>move apple between can &amp; orange</p>
                    </div>
                </div>
            </div>
            <div>

                <p><span>RT-2-X</span>
                        modulates low-level behaviors based on
                        <span> small changes in prepositions</span> (see
                        “on” vs “near” above) and demonstrates understanding of
                        <span> spatial relationships between
                            objects</span>
                </p>

                <div>
                    <p><img src="https://robotics-transformer-x.github.io/img/rt2x.png" alt="Image 1"/>
                    </p>
                    <p><img src="https://robotics-transformer-x.github.io/img/chart.png" alt="Image 2"/>
                    </p>
                </div>

                <p><span>RT-2-X</span>
                    outperforms RT-2 by <span>3x</span> in
                    emergent skill evaluations
                </p>


                <p>
                    RT-2-X demonstrates skills that
                    the RT-2 model was not capable of previously, including better spatial understanding in both the
                    absolute and relative sense. Small changes in preposition in the task string can also modulate
                    low-level robot behavior. The skills used for evaluation are illustrated in the figure above.
                </p>

            </div>

        </div>

        <div>
            <div>
                <h3>
                    <b>Citation</b>
                </h3>

                <p>
                    If you&#39;re using the Open X-Embodiment dataset and RT-X in your research, <a target="_blank" href="https://robotics-transformer-x.github.io/citation.txt">please cite</a>. If you&#39;re specifically using datasets that have been contributed to the joint effort, please cite those as well.
                    
                    We provide a <a target="_blank" href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">dataset spreadsheet</a> with citation for each dataset for your convenience.
                </p>

            </div>
        </div>



        <div>
            <div>
                <h3>
                    <b>Acknowledgements</b>
                </h3>

                <p>
                    We would like to thank John Guilyard for the amazing animations used for this website. The authors
                    would like to acknowledge Yuheng Kuang, Ning Hou, Utsav Malla, Sarah Nguyen, Rochelle Dela Cruz,
                    Justice Carbajal, Brianna Zitkovich, Emily Perez, Elio Prado, Jodilyn Peralta, Tran Pham, Deeksha
                    Manjunath, Samuel Wan, Jaspiar singh and the greater Google DeepMind team for their feedback and
                    contributions. The authors would like to thank Sanah Choudhry, Michael Griessel and Jon Small for their legal advice.
                </p>

                
            </div>
        </div>
    </strong></p></div></div>
  </body>
</html>
