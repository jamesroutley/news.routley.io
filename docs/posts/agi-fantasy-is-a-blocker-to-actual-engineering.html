<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.tomwphillips.co.uk/2025/11/agi-fantasy-is-a-blocker-to-actual-engineering/">Original</a>
    <h1>AGI fantasy is a blocker to actual engineering</h1>
    
    <div id="readability-page-1" class="page"><div>


<p>
  <i>
    <time datetime="2025-11-14">
      14 Nov 2025
    </time>
  </i>
</p>
<p>Reading <a href="https://www.penguin.co.uk/books/460331/empire-of-ai-by-hao-karen/9780241678923"><em>Empire of AI</em> by Karen Hao</a>, I was struck by how people associated with OpenAI <em>believe</em> in AGI. They really do think someone, perhaps them, will build AGI, and that it will lead to either the flourishing or destruction of humanity.</p>
<p>Elon Musk founded OpenAI because he thought Demis Hassabis was an evil genius who would build AGI first:</p>
<blockquote>
<p>…Musk would regularly characterise Hassabis as a supervillain who needed to be stopped. Musk would make unequivocally clear that OpenAI was the good to DeepMind’s evil. … “He literally made a video game where an evil genius tries to create AI to take over the world,” Musk shouted [at an OpenAI off-site], referring to Hassabis’s 2004 title <em>Evil Genius</em>, “and fucking people don’t see it. Fucking people don’t see it! And Larry [Page]? Larry thinks he controls Demis but he’s too busy fucking windsurfing to realize that Demis is gathering the power.”</p>
</blockquote>
<p>OpenAI’s co-founder and chief scientist Ilya Sutskever regularly told audiences and employees to “feel the AGI”. At a company off-site in Yosemite in September 2022, employees gathered around a firepit:</p>
<blockquote>
<p>In the pit, [Sutskever] had placed a wooden effigy that he’d commissioned from a local artist, and began a dramatic performance. This effigy, he explained represented a good, aligned AGI that OpenAI had built, only to discover it was actually lying and deceitful. OpenAI’s duty, he said, was to destroy it. … Sutskever doused the effigy in lighter fluid and lit on fire.</p>
</blockquote>
<p>I think it’s remarkable that what was until recently sci-fi fantasy has become a mainstream view in Silicon Valley.</p>
<p>Hao writes that GPT-2 was a bet on the “pure language” hypothesis, that asserts that since we communicate through language, then AGI should emerge from training a model solely on language. This is contrast to the “grounding” hypothesis, that asserts an AGI needs to perceive the world. Successfully scaling GPT to GPT-2 convinced enough people at OpenAI that the pure language hypothesis was valid. They just needed more data, more model parameters, and more compute.</p>
<p>So the belief in AGI, plus the recent results from LLMs, necessitates scaling, and justifies building data centres that <a href="https://restofworld.org/2024/data-centers-environmental-issues/">consume hundreds of litres of water a second</a>, <a href="https://www.theregister.com/2025/05/08/xai_turbines_colossus/">run on polluting gas generators because the grid can’t supply the power</a> (<a href="https://www.theregister.com/2024/04/01/microsoft_openai_5gw_dc/">and might use as much power as entire cities</a>), driving up CO2 emissions from manufacture and operation of new hardware, and <a href="https://www.wsj.com/tech/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483">exploits and traumatises data workers</a> to make sure ChatGPT doesn’t generate outputs like child sexual abuse material and hate speech or encourage users to self-harm. (The thirst for data is so great that they stopped curating training data and instead consume the internet, warts and all, and manage the model output using <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a>.)</p>
<p>And this is all fine, because they’re going to make AGI and the <a href="https://en.wikipedia.org/wiki/Expected_value">expected value</a> (EV) of it will be huge! (Briefly, the argument goes that if there is a 0.001% chance of AGI delivering an extremely large amount of value, and 99.999% chance of much less or zero value, then the EV is still extremely large because <code>(0.001% * very_large_value) + (99.999% * small_value) = very_large_value</code>).</p>
<p>But AGI arguments based on EV are nonsensical because the values and probabilities are made up and unfalsifiable. They also ignore externalities like environmental damage, which in contrast to AGI, have known negative value and certain probability: costs borne by everyone else right now.</p>
<p>As a technologist I want to solve problems effectively (by bringing about the desired, correct result), efficiently (with minimal waste) and without harm (to people or the environment).</p>
<p>LLMs-as-AGI fail on all three fronts. The computational profligacy of LLMs-as-AGI is dissatisfying, and the exploitation of data workers and the environment unacceptable. Instead, if we drop the AGI fantasy, we can evaluate LLMs and other generative models as solutions for specific problems, rather than <em>all</em> problems, with proper cost benefit analysis. For example, by using smaller purpose-built generative models, or even discriminative (non-generative) models. In other words, make trade-offs and actually do engineering.</p>



  </div></div>
  </body>
</html>
