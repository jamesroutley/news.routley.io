<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://link.springer.com/article/10.1007/s10506-024-09396-9">Original</a>
    <h1>Re-Evaluating GPT-4&#39;s Bar Exam Performance</h1>
    
    <div id="readability-page-1" class="page"><div>
                                    <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1"><span>1 </span>Introduction</h2><div id="Sec1-content"><p>On March 14th, 2023, OpenAI launched GPT-4, said to be the latest milestone in the company’s effort in scaling up deep learning (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e371">2023a</a>). As part of its launch, OpenAI revealed details regarding the model’s “human-level performance on various professional and academic benchmarks” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e374">2023a</a>). Perhaps none of these capabilities was as widely publicized as GPT-4’s performance on the Uniform Bar Examination, with OpenAI prominently displaying on various pages of its website and technical report that GPT-4 scored in or around the “90th percentile,” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e377">2023a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e380">b</a>, n.d.) or “the top 10% of test-takers,” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e383">2023a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e387">b</a>) and various prominent media outlets (Koetsier <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Koetsier J (2023) GPT-4 Beats 90% of Lawyers Trying to Pass the Bar. Forbes. 
                https://www.forbes.com/sites/johnkoetsier/2023/03/14/gpt-4-beats-90-of-lawyers-trying-to-pass-the-bar/?sh=b40c88d30279
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR30" id="ref-link-section-d207556803e390">2023</a>; Caron <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Caron P (2023) GPT-4 Beats 90% of aspiring lawyers on the bar exam. TaxProf Blog. 
                https://taxprof.typepad.com/taxprof_blog/2023/03/gpt-4-beats-90-of-aspiring-lawyers-on-the-bar-exam.html
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR11" id="ref-link-section-d207556803e393">2023</a>; Weiss <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Weiss DC (2023) Latest version of ChatGPT aces bar exam with score nearing 90th percentile. ABA Journal. 
                https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR74" id="ref-link-section-d207556803e396">2023</a>; Wilkins <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Wilkins S (2023) How GPT-4 mastered the entire bar exam, and why that matters. Law.com. 
                https://www.law.com/legaltechnews/2023/03/17/how-gpt-4-mastered-the-entire-bar-exam-and-why-that-matters/?slreturn=20230324023302
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR75" id="ref-link-section-d207556803e399">2023</a>; Patrice <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Patrice J (2023) New GPT-4 Passes All Sections Of The Uniform Bar Exam. Maybe This Will Finally Kill The Bar Exam. Above the Law. 
                https://abovethelaw.com/2023/03/new-gpt-4-passes-all-sections-of-the-uniform-bar-exam-maybe-this-will-finally-kill-the-bar-exam/
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR57" id="ref-link-section-d207556803e402">2023</a>) and legal scholars (Schwarcz and Choi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Schwarcz D, Choi JH (2023) Ai tools for lawyers: a practical guide. Available at SSRN" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR64" id="ref-link-section-d207556803e406">2023</a>) resharing and discussing the implications of these results for the legal profession and the future of AI.</p><p>Of course, assessing the capabilities of an AI system as compared to those of a human is no easy task (Hernandez-Orallo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Hernandez-Orallo J (2020) AI evaluation: on broken yardsticks and measurement scales. In: Workshop on evaluating evaluation of AI systems at AAAI" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR22" id="ref-link-section-d207556803e412">2020</a>; Burden and Hernández-Orallo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Burden J, Hernández-Orallo J (2020) Exploring AI safety in degrees: generality, capability and control. In: Proceedings of the workshop on artificial intelligence safety (safeai 2020) co-located with 34th AAAI conference on artificial intelligence (AAAI 2020). pp 36–40" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR9" id="ref-link-section-d207556803e415">2020</a>; Raji et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Raji ID, Bender EM, Paullada A, Denton E, Hanna A (2021) Ai and the everything in the whole wide world benchmark. arXiv preprint 
                arXiv:2111.15366
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR58" id="ref-link-section-d207556803e418">2021</a>; Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Bowman S (2022) The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail. In: Proceedings of the 60th annual meeting of the association for computational linguistics (vol 1: Long papers) pp 7484–7499" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR6" id="ref-link-section-d207556803e421">2022</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Bowman SR (2023) Eight things to know about large language models. arXiv preprint 
                arXiv:2304.00612
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR7" id="ref-link-section-d207556803e424">2023</a>; Kojima et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large language models are zero-shot reasoners. arXiv preprint 
                arXiv:2205.11916
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR31" id="ref-link-section-d207556803e428">2022</a>), and in the context of the legal profession specifically, there are various reasons to doubt the usefulness of the bar exam as a proxy for lawyerly competence (both for humans and AI systems), given that, for example: (a) the content on the UBE is very general and does not pertain to the legal doctrine of any jurisdiction in the United States (National Conference of Bar Examiners n.d.-h), and thus knowledge (or ignorance) of that content does not necessarily translate to knowledge (or ignorance) of relevant legal doctrine for a practicing lawyer of any jurisdiction; and (b) the tasks involved on the bar exam, particularly multiple-choice questions, do not reflect the tasks of practicing lawyers, and thus mastery (or lack of mastery) of those tasks does not necessarily reflect mastery (or lack of mastery) of the tasks of practicing lawyers.</p><p>Moreover, although the UBE is a closed-book exam for humans, GPT-4’s huge training corpus largely distilled in its parameters means that it can effectively take the UBE “open-book”, indicating that UBE may not only be an accurate proxy for lawyerly comptetence but is also likely to provide an overly favorable estimate of GPT-4’s lawyerly capabilities relative to humans.</p><p>Notwithstanding these concerns, the bar exam results appeared especially startling compared to GPT-4’s other capabilities, for various reasons. Aside from the sheer complexity of the law in form (Martinez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022a" title="Martinez E, Mollica F, Gibson E (2022) Poor writing, not specialized concepts, drives processing difficulty in legal language. Cognition 224:105070" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR38" id="ref-link-section-d207556803e437">2022a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Martinez E, Mollica F, Gibson E (2022b) So much for plain language: An analysis of the accessibility of united states federal laws (1951–2009). In: Proceedings of the annual meeting of the cognitive science society, vol 44" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR39" id="ref-link-section-d207556803e440">b</a>, in press) and content (Katz and Bommarito <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Katz DM, Bommarito MJ (2014) Measuring the complexity of the law: the United States code. Artif Intell Law 22:337–374" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR29" id="ref-link-section-d207556803e443">2014</a>; Ruhl et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ruhl J, Katz DM, Bommarito MJ (2017) Harnessing legal complexity. Science 355(6332):1377–1378" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR61" id="ref-link-section-d207556803e446">2017</a>; Bommarito and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Bommarito MJ II, Katz DM (2017) Measuring and modeling the us regulatory ecosystem. J Stat Phys 168:1125–1135" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR4" id="ref-link-section-d207556803e449">2017</a>), the first is that the boost in performance of GPT-4 over its predecessor GPT-3.5 (80 percentile points) far exceeded that of any other test, including seemingly related tests such as the LSAT (40 percentile points), GRE verbal (36 percentile points), and GRE Writing (0 percentile points) (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e453">2023b</a>, n.d.).</p><p>The second is that half of the Uniform Bar Exam consists of writing essays (National Conference of Bar Examiners n.d.-h),<sup><a href="#Fn1"><span>Footnote </span>1</a></sup> and GPT-4 seems to have scored much lower on other exams involving writing, such as AP English Language and Composition (14th–44th percentile), AP English Literature and Composition (8th–22nd percentile) and GRE Writing (<span>\(\sim\)</span>54th percentile) (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e484">2023a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e487">b</a>). In each of these three exams, GPT-4 failed to achieve a higher percentile performance over GPT-3.5, and failed to achieve a percentile score anywhere near the 90th percentile.</p><p>Moreover, in its technical report, GPT-4 claims that its percentile estimates are “conservative” estimates meant to reflect “the lower bound of the percentile range,” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e493">2023b</a>, p. 6) implying that GPT-4’s actual capabilities may be even greater than its estimates.</p><p>Methodologically, however, there appear to be various uncertainties related to the calculation of GPT’s bar exam percentile. For example, unlike the administrators of other tests that GPT-4 took, the administrators of the Uniform Bar Exam (the NCBE as well as different state bars) do not release official percentiles of the UBE (JD Advising n.d.-b; Examiner n.d.-b), and different states in their own releases almost uniformly report only passage rates as opposed to percentiles (National Conference of Bar Examiners n.d.-c; The New York State Board of Law Examiners n.d.), as only the former are considered relevant to licensing requirements and employment prospects.</p><p>Furthermore, unlike its documentation for the other exams it tested (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e502">2023b</a>, p. 25), OpenAI’s technical report provides no direct citation for how the UBE percentile was computed, creating further uncertainty over both the original source and validity of the 90th percentile claim.</p><p>The reliability and transparency of this estimate has important implications on both the legal practice front and AI safety front. On the legal practice front, there is great debate regarding to what extent and when legal tasks can and should be automated (Winter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Winter C, Hollman N, Manheim D (2023) Value alignment for advanced artificial judicial intelligence. Am Philos Quart 60(2):187–203" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR77" id="ref-link-section-d207556803e508">2023</a>; Crootof et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Crootof R, Kaminski ME, Price II WN (2023) Humans in the loop. Vanderbilt Law Review, (Forthcoming)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR17" id="ref-link-section-d207556803e511">2023</a>; Markou and Deakin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Markou C, Deakin S (2020) Is law computable? From rule of law to legal singularity. From Rule of Law to Legal Singularity. University of Cambridge Faculty of Law Research Paper" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR36" id="ref-link-section-d207556803e514">2020</a>; Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Winter CK (2022) The challenges of artificial judicial decision-making for liberal democracy. Judicial decision-making: Integrating empirical and theoretical perspectives. Springer, Berlin, pp 179–204" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR76" id="ref-link-section-d207556803e517">2022</a>). To the extent that capabilities estimates for generative AI in the context law are overblown, this may lead both lawyers and non-lawyers to rely on generative AI tools when they otherwise wouldn’t and arguably shouldn’t, plausibly increasing the prevalence of bad legal outcomes as a result of (a) judges misapplying the law; (b) lawyers engaging in malpractice and/or poor representation of their clients; and (c) non-lawyers engaging in ineffective pro se representation.</p><p>Meanwhile, on the AI safety front, there appear to be growing concerns of transparency<sup><a href="#Fn2"><span>Footnote </span>2</a></sup> among developers of the most powerful AI systems (Ray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Ray T (2023) With GPT-4, OpenAI opts for secrecy versus disclosure. ZDNet. 
                https://www.zdnet.com/article/with-gpt-4-openai-opts-for-secrecy-versus-disclosure/
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR59" id="ref-link-section-d207556803e532">2023</a>; Stokel-Walker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Stokel-Walker C (2023) Critics denounce a lack of transparency around GPT-4’s tech. Fast Company. 
                https://www.fastcompany.com/90866190/critics-denounce-a-lack-of-transparency-around-gpt-4s-tech
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR67" id="ref-link-section-d207556803e535">2023</a>). To the extent that transparency is important to ensuring the safe deployment of AI, a lack of transparency could undermine our confidence in the prospect of safe deployment of AI (Brundage et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Brundage M, Avin S, Wang J, Belfield H, Krueger G, Hadfield G, et al (2020) Toward trustworthy AI development: mechanisms for supporting verifiable claims. arXiv preprint 
                arXiv:2004.07213
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR8" id="ref-link-section-d207556803e538">2020</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Li B, Qi P, Liu B, Di S, Liu J, Pei J, Zhou B (2023) Trustworthy AI: From principles to practices. ACM Comput Surv 55(9):1–46" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR35" id="ref-link-section-d207556803e541">2023</a>). In particular, releasing models without an accurate and transparent assessment of their capabilities (including by third-party developers) might lead to unexpected misuse/misapplication of those models (within and beyond legal contexts), which might have detrimental (perhaps even catastrophic) consequences moving forward (Ngo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Ngo R (2022) The alignment problem from a deep learning perspective. arXiv preprint 
                arXiv:2209.00626
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR51" id="ref-link-section-d207556803e545">2022</a>; Carlsmith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Carlsmith J (2022) Is power-seeking AI an existential risk? arXiv preprint 
                arXiv:2206.13353
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR10" id="ref-link-section-d207556803e548">2022</a>).</p><p>Given these considerations, this paper begins by investigating some of the key methodological challenges in verifying the claim that GPT-4 achieved 90th percentile performance on the Uniform Bar Examination. The paper’s findings in this regard are fourfold. First, although GPT-4’s UBE score nears the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates appear heavily skewed towards those who failed the July administration and whose scores are much lower compared to the general test-taking population. Second, using data from a recent July administration of the same exam reveals GPT-4’s percentile to be below the 69th percentile on the UBE, and <span>\(\sim\)</span>48th percentile on essays. Third, examining official NCBE data and using several conservative statistical assumptions, GPT-4’s performance against first-time test takers is estimated to be <span>\(\sim\)</span>62nd percentile, including 42 percentile on essays. Fourth, when examining only those who passed the exam, GPT-4’s performance is estimated to drop to <span>\(\sim\)</span>48th percentile overall, and <span>\(\sim\)</span>15th percentile on essays.</p><p>Next, whereas the above four findings take for granted the scaled score achieved by GPT-4 as reported by OpenAI, the paper then proceeds to investigate the validity of that score, given the importance (and often neglectedness) of replication and reproducibility within computer science and scientific fields more broadly (Cockburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Cockburn A, Dragicevic P, Besançon L, Gutwin C (2020) Threats of a replication crisis in empirical computer science. Commun ACM 63(8):70–79" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR16" id="ref-link-section-d207556803e630">2020</a>; Echtler and Häußler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Echtler F, Häußler M (2018) Open source, open science, and the replication crisis in HCI. Extended abstracts of the 2018 chi conference on human factors in computing systems. pp 1–8" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR18" id="ref-link-section-d207556803e633">2018</a>; Jensen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Jensen TI, Kelly B, Pedersen LH (2023) Is there a replication crisis in finance? J Finance 78(5):2465–2518" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR27" id="ref-link-section-d207556803e636">2023</a>; Schooler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Schooler JW (2014) Metascience could rescue the replication crisis. Nature 515(7525):9" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR63" id="ref-link-section-d207556803e639">2014</a>; Shrout and Rodgers <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Shrout PE, Rodgers JL (2018) Psychology, science, and knowledge construction: broadening perspectives from the replication crisis. Ann Rev Psychol 69:487–510" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR66" id="ref-link-section-d207556803e642">2018</a>). The paper successfully replicates the MBE score of 158, but highlights several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the essay score (140).</p><p>Finally, the paper also investigates the effect of adjusting temperature settings and prompting techniques on GPT-4’s MBE performance, finding no significant effect of adjusting temperature settings on performance, and some significant effect of prompt engineering on model performance when compared to a minimally tailored baseline condition.</p><p>Taken together, these findings suggest that OpenAI’s estimates of GPT-4’s UBE percentile, though clearly an impressive leap over those of GPT-3.5, are likely overinflated, particularly if taken as a “conservative” estimate representing “the lower range of percentiles,” and even moreso if meant to reflect the actual capabilities of a practicing lawyer. These findings carry timely insights for the desirability and feasibility of outsourcing legally relevant tasks to AI models, as well as for the importance for generative AI developers to implement rigorous and transparent capabilities evaluations to help secure safer and more trustworthy AI.</p></div></div></section><section data-title="Evaluating the 90th Percentile estimate"><div id="Sec2-section"><h2 id="Sec2"><span>2 </span>Evaluating the 90th Percentile estimate</h2><div id="Sec2-content"><h3 id="Sec3"><span>2.1 </span>Evidence from OpenAI</h3><p>Investigating the OpenAI website, as well as the GPT-4 technical report, reveals a multitude of claims regarding the estimated percentile of GPT-4’s Uniform Bar Examination performance but a dearth of documentation regarding the backing of such claims. For example, the first paragraph of the official GPT-4 research page on the OpenAI website states that “it [GPT-4] passes a simulated bar exam with a score around the top 10% of test takers” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e663">2023a</a>). This claim is repeated several times later in this and other webpages, both visually and textually, each time without explicit backing.<sup><a href="#Fn3"><span>Footnote </span>3</a></sup></p><p>Similarly undocumented claims are reported in the official GPT-4 Technical Report.<sup><a href="#Fn4"><span>Footnote </span>4</a></sup> Although OpenAI details the methodology for computing most of its percentiles in A.5 of the Appendix of the technical report, there does not appear to be any such documentation for the methodology behind computing the UBE percentile. For example, after providing relatively detailed breakdowns of its methodology for scoring the SAT, GRE, SAT, AP, and AMC, the report states that “[o]ther percentiles were based on official score distributions,” followed by a string of references to relevant sources (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e689">2023b</a>, p. 25).</p><p>Examining these references, however, none of the sources contains any information regarding the Uniform Bar Exam, let alone its “official score distributions” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e695">2023b</a>, pp. 22–23). Moreover, aside from the Appendix, there are no other direct references to the methodology of computing UBE scores, nor any indirect references aside from a brief acknowledgement thanking “our collaborators at Casetext and Stanford CodeX for conducting the simulated bar exam” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023b" title="OpenAI (2023) GPT-4 Technical Report. 
                arXiv:2303.08774
                
              . (Preprint submitted to arXiv)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR55" id="ref-link-section-d207556803e698">2023b</a>, p. 18).</p><h3 id="Sec4"><span>2.2 </span>Evidence from GPT-4 passes the bar</h3><p>Another potential source of evidence for the 90th percentile claim comes from an early draft version of the paper, “GPT-4 passes the bar exam,” written by the administrators of the simulated bar exam referenced in OpenAI’s technical report (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e709">2023</a>). The paper is very well-documented and transparent about its methodology in computing raw and scaled scores, both in the main text and in its comprehensive appendices. Unlike the GPT-4 technical report, however, the focus of the paper is not on percentiles but rather on the model’s scaled score compared to that of the average test taker, based on publicly available NCBE data. In fact, one of the only mentions of percentiles is in a footnote, where the authors state, in passing: “Using a percentile chart from a recent exam administration (which is generally available online), ChatGPT would receive a score below the 10th percentile of test-takers while GPT-4 would receive a combined score approaching the 90th percentile of test-takers”. (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e712">2023</a>, p. 10)</p><h3 id="Sec5"><span>2.3 </span>Evidence online</h3><p>As explained by JD Advising (n.d.-b), The National Conference of Bar Examiners (NCBE), the organization that writes the Uniform Bar Exam (UBE) does not release UBE percentiles.<sup><a href="#Fn5"><span>Footnote </span>5</a></sup> Because there is no official percentile chart for UBE, all generally available online estimates are unofficial. Perhaps the most prominent of such estimates are the percentile charts from pre-July 2019 Illinois bar exam. Pre-2019,<sup><a href="#Fn6"><span>Footnote </span>6</a></sup> Illinois, unlike other states, provided percentile charts of their own exam that allowed UBE test-takers to estimate their approximate percentile given the similarity between the two exams (JD Advising n.d.-b).<sup><a href="#Fn7"><span>Footnote </span>7</a></sup></p><p>Examining these approximate conversion charts, however, yields conflicting results. For example, although the percentile chart from the February 2019 administration of the Illinois Bar Exam estimates a score of 300 (2–3 points higher thatn GPT-4’s score) to be at the 90th percentile, this estimate is heavily skewed compared to the general population of July exam takers,<sup><a href="#Fn8"><span>Footnote </span>8</a></sup> since the majority of those who take the February exam are repeat takers who failed the July exam (Examiner n.d.-a),<sup><a href="#Fn9"><span>Footnote </span>9</a></sup> and repeat takers score much lower<sup><a href="#Fn10"><span>Footnote </span>10</a></sup> and are much more likely to fail than are first-timers.<sup><a href="#Fn11"><span>Footnote </span>11</a></sup></p><p>Indeed, examining the latest available percentile chart for the July exam estimates GPT-4’s UBE score to be <span>\(\sim\)</span>68th percentile, well below the 90th percentile figure cited by OpenAI (Illinois Board of Admissions to the Bar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Illinois Board of Admissions to the Bar. (2018) 
                https://www.ilbaradmissions.org/percentile-equivalent-charts-july-2018
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR23" id="ref-link-section-d207556803e799">2018</a>).</p></div></div></section><section data-title="Towards a more accurate percentile estimate"><div id="Sec6-section"><h2 id="Sec6"><span>3 </span>Towards a more accurate percentile estimate</h2><div id="Sec6-content"><p>Although using the July bar exam percentiles from the Illinois Bar would seem to yield a more accurate estimate than the February data, the July figure is also biased towards lower scorers, since approximately 23% of test takers in July nationally are estimated to be re-takers and score, for example, 16 points below first-timers on the MBE (Reshetar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Reshetar R (2022) The testing column: Why are February bar exam pass rates lower than July pass rates? Bar Exam 91(1):51–53" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR60" id="ref-link-section-d207556803e811">2022</a>). Limiting the comparison to first-timers would provide a more accurate comparison that avoids double-counting those who have taken the exam again after failing once or more.</p><p>Relatedly, although (virtually) all licensed attorneys have passed the bar,<sup><a href="#Fn12"><span>Footnote </span>12</a></sup> not all those who take the bar become attorneys. To the extent that GPT-4’s UBE percentile is meant to reflect its performance against other attorneys, a more appropriate comparison would not only limit the sample to first-timers but also to those who achieved a passing score.</p><p>Moreover, the data discussed above is based on purely Illinois Bar exam data, which (at the time of the chart) was similar but not identical to the UBE in its content and scoring (JD Advising n.d.-b), whereas a more accurate estimate would be derived more directly from official NCBE sources.</p><h3 id="Sec7"><span>3.1 </span>Methods</h3><p>To account for the issues with both OpenAI’s estimate as well the July estimate, more accurate estimates (for GPT-3.5 and GPT-4) were sought to be computed here based on first-time test-takers, including both (a) first-time test-takers overall, and (b) those who passed.</p><p>To do so, the parameters for a normal distribution of scores were separately estimated for the MBE and essay components (MEE + MPT), as well as the UBE score overall.<sup><a href="#Fn13"><span>Footnote </span>13</a></sup></p><p>Assuming that UBE scores (as well as MBE and essay subscores) are normally distributed, percentiles of GPT’s score can be directly computed after computing the parameters of these distributions (i.e. the mean and standard deviation).</p><p>Thus, the methodology here was to first compute these parameters, then generate distributions with these parameters, and then compute (a) what percentage of values on these distributions are lower than GPT’s scores (to estimate the percentile against first-timers); and (b) what percentage of values above the passing threshold are lower than GPT’s scores (to estimate the percentile against qualified attorneys).</p><p>With regard to the mean, according to publicly available official NCBE data, the mean MBE score of first-time test-takers is 143.8 (Reshetar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Reshetar R (2022) The testing column: Why are February bar exam pass rates lower than July pass rates? Bar Exam 91(1):51–53" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR60" id="ref-link-section-d207556803e857">2022</a>).</p><p>As explained by official NCBE publications, the essay component is scaled to the MBE data (Albanese <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Albanese MA (2014) The testing column: scaling: it’s not just for fish or mountains. Bar Exam 83(4):50–56" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR1" id="ref-link-section-d207556803e863">2014</a>), such that the two components have approximately the same mean and standard deviation (Albanese <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Albanese MA (2014) The testing column: scaling: it’s not just for fish or mountains. Bar Exam 83(4):50–56" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR1" id="ref-link-section-d207556803e866">2014</a>; Illinois Board of Admissions to the Bar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Illinois Board of Admissions to the Bar. (2018) 
                https://www.ilbaradmissions.org/percentile-equivalent-charts-july-2018
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR23" id="ref-link-section-d207556803e869">2018</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Illinois Board of Admissions to the Bar. (2019) 
                https://www.ilbaradmissions.org/percentile-equivalent-charts-february-2019
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR24" id="ref-link-section-d207556803e872">2019</a>). Thus, the methodology here assumed that the mean first-time essay score is 143.8.<sup><a href="#Fn14"><span>Footnote </span>14</a></sup></p><p>Given that the total UBE score is computed directly by adding MBE and essay scores (National Conference of Bar Examiners n.d.-h), an assumption was made that mean first-time UBE score is 287.6 (143.8 + 143.8).</p><p>With regard to standard deviations, information regarding the SD of first-timer scores is not publicly available. However, distributions of MBE scores for July scores (provided in 5 point-intervals) are publicly available on the NCBE website (The National Bar Examiner n.d.).</p><p>Under the assumption that first-timers have approximately the same SD as that of the general test-taking population in July, the standard deviation of first-time MBE scores was computed by (a) entering the publicly available distribution of MBE scores into R; and (b) taking the standard deviation of this distribution using the built-in sd() function (which calculates the standard deviation of a normal distribution).</p><p>Given that, as mentioned above, the distribution (mean and SD) of essay scores is the same as MBE scores, the SD for essay scores was computed similarly as above.</p><p>With regard to the UBE, Although UBE standard deviations are not publicly available for any official exam, they can be inferred from a combination of the mean UBE score for first-timers (287.6) and first-time pass rates.</p><p>For reference, standard deviations can be computed analytically as follows:</p><div id="Equ1"><p><span>$$\begin{aligned} \sigma = \frac{x - \mu }{z} \end{aligned}$$</span></p></div><p>where</p><ul>
                <li>
                  <p><i>x</i> is the quantile (the value associated with a given percentile, such as a cutoff score),</p>
                </li>
                <li>
                  <p><span>\(\mu\)</span> is the mean,</p>
                </li>
                <li>
                  <p><i>z</i> is the z-score corresponding to a given percentile,</p>
                </li>
                <li>
                  <p><span>\(\sigma\)</span> is the standard deviation.</p>
                </li>
              </ul><p>Thus, by (a) subtracting the cutoff score of a given administration (<i>x</i>) from the mean (<span>\(\mu\)</span>); and (b) dividing that by the z-score (<i>z</i>) corresponding to the percentile of the cutoff score (i.e., the percentage of people who did not pass), one is left with the standard deviation (<span>\(\sigma\)</span>).</p><p>Here, the standard deviation was calculated according to the above formula using the official first-timer mean, along with pass rate and cutoff score data from New York, which according to NCBE data has the highest number of examinees for any jurisdiction (National Conference of Bar Examiners <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="National Conference of Bar Examiners (2023) Bar exam results by jurisdiction. 
                https://www.ncbex.org/statistics-research/bar-exam-results-jurisdiction
                
              . Accessed on 01 Jan 2024" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR42" id="ref-link-section-d207556803e1057">2023</a>).<sup><a href="#Fn15"><span>Footnote </span>15</a></sup></p><p>After obtaining these parameters, distributions of first-timer scores for the MBE component, essay component, and UBE overall were computed using the built-in rnorm function in R (which generates a normal distribution with a given mean and standard deviation).</p><p>Finally, after generating these distributions, percentiles were computed by calculating (a) what percentage of values on these distributions were lower than GPT’s scores (to estimate the percentile against first-timers); and (b) what percentage of values above the passing threshold were lower than GPT’s scores (to estimate the percentile against qualified attorneys).</p><p>With regard to the latter comparison, percentiles were computed after removing all UBE scores below 270, which is the most common score cutoff for states using the UBE (National Conference of Bar Examiners n.d.-a). To compute models’ performance on the individual components relative to qualified attorneys, a separate percentile was likewise computed after removing all subscores below 135.<sup><a href="#Fn16"><span>Footnote </span>16</a></sup></p><h3 id="Sec8"><span>3.2 </span>Results</h3><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Table 1 Estimated percentile of GPT-4’s uniform bar examination performance</b></figcaption></figure></div><h4 id="Sec9"><span>3.2.1 </span>Performance against first-time test-takers</h4><p>Results are visualized in Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#Tab1">1</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#Tab2">2</a>. For each component of the UBE, as well as the UBE overall, GPT-4’s estimated percentile among first-time July test takers is less than that of both the OpenAI estimate and the July estimate that include repeat takers.</p><p>With regard to the aggregate UBE score, GPT-4 scored in the 62nd percentile as compared to the <span>\(\sim\)</span>90th percentile February estimate and the <span>\(\sim\)</span>68th percentile July estimate. With regard to MBE, GPT-4 scored in the <span>\(\sim\)</span>79th percentile as compared to the <span>\(\sim\)</span>95th percentile February estimate and the 86th percentile July estimate. With regard to MEE + MPT, GPT-4 scored in the <span>\(\sim\)</span>42nd percentile as compared to the <span>\(\sim\)</span>69th percentile February estimate and the <span>\(\sim\)</span>48th percentile July estimate.</p><p>With regard to GPT-3.5, its aggregate UBE score among first-timers was in the <span>\(\sim\)</span>2nd percentile, as compared to the <span>\(\sim\)</span>2nd percentile February estimate and <span>\(\sim\)</span>1st percentile July estimate. Its MBE subscore was in the <span>\(\sim\)</span>6th percentile, compared to the <span>\(\sim\)</span>10th percentile February estimate <span>\(\sim\)</span>7th percentile July estimate. Its essay subscore was in the <span>\(\sim\)</span>0th percentile, compared to the <span>\(\sim\)</span>1st percentile February estimate and <span>\(\sim\)</span>0th percentile July estimate.</p><div data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption><b id="Tab2" data-test="table-caption">Table 2 Estimated percentiles of MBE, essay, and total UBE scores among first-time test takers of uniform bar exam</b></figcaption></figure></div><h4 id="Sec10"><span>3.2.2 </span>Performance against qualified attorneys</h4><p>Predictably, when limiting the sample to those who passed the bar, the models’ percentile dropped further.</p><p>With regard to the aggregate UBE score, GPT-4 scored in the <span>\(\sim\)</span>45th percentile. With regard to MBE, GPT-4 scored in the <span>\(\sim\)</span>69th percentile, whereas for the MEE + MPT, GPT-4 scored in the <span>\(\sim\)</span>15th percentile.</p><p>With regard to GPT-3.5, its aggregate UBE score among qualified attorneys was 0th percentile, as were its percentiles for both subscores (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#Tab3">3</a>).</p><div data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption><b id="Tab3" data-test="table-caption">Table 3 Estimated percentile leap from GPT-3.5 to GPT-4 on uniform bar examination</b></figcaption></figure></div><div data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption><b id="Tab4" data-test="table-caption">Table 4 Comparison of estimated percentiles of UBE scores for different groups</b></figcaption></figure></div></div></div></section><section data-title="Re-evaluating the raw score"><div id="Sec11-section"><h2 id="Sec11"><span>4 </span>Re-evaluating the raw score</h2><div id="Sec11-content"><p>So far, this analysis has taken for granted the scaled score achieved by GPT-4 as reported by OpenAI—that is, assuming GPT-4 scored a 298 on the UBE, is the 90th-percentile figure reported by OpenAI warranted?</p><p>However, given calls for the replication and reproducibility within the practice of science more broadly (Cockburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Cockburn A, Dragicevic P, Besançon L, Gutwin C (2020) Threats of a replication crisis in empirical computer science. Commun ACM 63(8):70–79" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR16" id="ref-link-section-d207556803e2800">2020</a>; Echtler and Häußler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Echtler F, Häußler M (2018) Open source, open science, and the replication crisis in HCI. Extended abstracts of the 2018 chi conference on human factors in computing systems. pp 1–8" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR18" id="ref-link-section-d207556803e2803">2018</a>; Jensen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Jensen TI, Kelly B, Pedersen LH (2023) Is there a replication crisis in finance? J Finance 78(5):2465–2518" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR27" id="ref-link-section-d207556803e2806">2023</a>; Schooler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Schooler JW (2014) Metascience could rescue the replication crisis. Nature 515(7525):9" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR63" id="ref-link-section-d207556803e2809">2014</a>; Shrout and Rodgers <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Shrout PE, Rodgers JL (2018) Psychology, science, and knowledge construction: broadening perspectives from the replication crisis. Ann Rev Psychol 69:487–510" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR66" id="ref-link-section-d207556803e2812">2018</a>), it is worth scrutinizing the validity of the score itself—that is, did GPT-4 in fact score a 298 on the UBE?</p><p>Moreover, given the various potential hyperparameter settings available when using GPT-4 and other LLMs, it is worth assessing whether and to what extent adjusting such settings might influence the capabilities of GPT-4 on exam performance.</p><p>To that end, this section first attempts to replicate the MBE score reported by OpenAI (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e2821">2023a</a>) and Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2824">2023</a>) using methods as close to the original paper as reasonably feasible.</p><p>The section then attempts to get a sense of the floor and ceiling of GPT-4’s out-of-the-box capabilities by comparing GPT-4’s MBE performance using the best and worst hyperparameter settings.</p><p>Finally, the section re-examines GPT-4’s performance on the essays, evaluating (a) the extent to which the methodology of grading GPT-4’s essays deviated that from official protocol used by the National Conference of Bar Examiners during actual bar exam administrations; and (b) the extent to which such deviations might undermine one’s confidence in the the scaled essay scores reported by OpenAI (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e2834">2023a</a>) and Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2837">2023</a>).</p><h3 id="Sec12"><span>4.1 </span>Replicating the MBE score</h3><h4 id="Sec13"><span>4.1.1 </span>Methodology</h4><p><b><i>Materials</i></b></p><p>As in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2855">2023</a>), the materials used here were the official MBE questions released by the NCBE. The materials were purchased and downloaded in pdf format from an authorized NCBE reseller. Afterwards, the materials were converted into TXT format, and text analysis tools were used to format the questions in a way that was suitable for prompting, following Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2858">2023</a>).</p><p><b><i>Procedure</i></b></p><p>To replicate the MBE score reported by OpenAI (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e2868">2023a</a>), this paper followed the protocol documented by Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2871">2023</a>), with some minor additions for robustness purposes.</p><p>In Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2878">2023</a>), the authors tested GPT-4’s MBE performance using three different temperature settings: 0, .5 and 1. For each of these temperature settings, GPT-4’s MBE performance was tested using two different prompts, including (1) a prompt where GPT was asked to provide a top-3 ranking of answer choices, along with a justification and authority/citation for its answer; and (2) a prompt where GPT-4 was asked to provide a top-3 ranking of answer choices, without providing a justification or authority/citation for its answer.</p><p>For each of these prompts, GPT-4 was also told that it should answer as if it were taking the bar exam.</p><p>For each of these prompts / temperature combinations, Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2887">2023</a>) tested GPT-4 three different times (“experiments” or “trials”) to control for variation.</p><p>The minor additions to this protocol were twofold. First, GPT-4 was tested under two additional temperature settings: .25 and .7. This brought the total temperature / prompt combinations to 10 as opposed to 6 in the original paper.</p><p>Second, GPT-4 was tested 5 times under each temperature / prompt combination as opposed to 3 times, bringing the total number of trials to 50 as opposed to 18.</p><p>After prompting, raw scores were computed using the official answer key provided by the exam. Scaled scores were then computed following the method outlined in JD Advising (n.d.-a), by (a) multiplying the number of correct answers by 190, and dividing by 200; and (b) converting the resulting number to a scaled score using a conversion chart based on official NCBE data.</p><p>After scoring, scores from the replication trials were analyzed in comparison to those from Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2903">2023</a>) using the data from their publicly available github repository.</p><p>To assess whether there was a significant difference between GPT-4’s accuracy in the replication trials as compared to the Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2909">2023</a>) paper, as well as to assess any significant effect of prompt type or temperature, a mixed-effects binary logistic regression was conducted with: (a) paper (replication vs original), temperature and prompt as fixed effects<sup><a href="#Fn17"><span>Footnote </span>17</a></sup>; and (b) question number and question category as random effects. These regressions were conducted using the lme4 (Bates et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Bates D, Mächler M, Bolker B, Walker S (2014) Fitting linear mixed-effects models using LME4. arXiv preprint 
                arXiv:1406.5823
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR2" id="ref-link-section-d207556803e2918">2014</a>) and lmertest (Kuznetsova et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kuznetsova A, Brockhoff PB, Christensen RHB (2017) lmertest package: tests in linear mixed effects models. J Stat Software 82:13" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR33" id="ref-link-section-d207556803e2921">2017</a>) packages from R.</p><h4 id="Sec14"><span>4.1.2 </span>Results</h4><p>Results are visualized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#Tab4">4</a>. Mean MBE accuracy across all trials in the replication here was 75.6% (95% CI: 74.7 to 76.4), whereas the mean accuracy across all trials in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2935">2023</a>) was 75.7% (95% CI: 74.2 to 77.1).<sup><a href="#Fn18"><span>Footnote </span>18</a></sup></p><p>The regression model did not reveal a main effect of “paper” on accuracy (<span>\(p=.883\)</span>), indicating that there was no significant difference between GPT-4’s raw accuracy as reported by Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e2975">2023</a>) and GPT-4’s raw accuracy as performed in the replication here.</p><p>There was also no main effect of temperature (<span>\(p&gt;.1\)</span>)<sup><a href="#Fn19"><span>Footnote </span>19</a></sup> or prompt (<span>\(p=.741\)</span>). That is, GPT-4’s raw accuracy was not significantly higher or lower at a given temperature setting or when fed a certain prompt as opposed to another (among the two prompts used in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3062">2023</a>) and the replication here) (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#Tab5">5</a>).</p><div data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption><b id="Tab5" data-test="table-caption">Table 5 GPT-4’s MBE performance across temperature and prompt settings</b></figcaption></figure></div><h3 id="Sec15"><span>4.2 </span>Assessing the effect of hyperparameters</h3><h4 id="Sec16"><span>4.2.1 </span>Methods</h4><p>Although the above analysis found no effect of prompt on model performance, this could be due to a lack of variety of prompts used by Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3225">2023</a>) in their original analysis.</p><p>To get a better sense of whether prompt engineering might have any effect on model performance, a follow-up experiment compared GPT-4’s performance in two novel conditions not tested in the original (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3231">2023</a>) paper.</p><p>In Condition 1 (“minimally tailored” condition), GPT-4 was tested using minimal prompting compared to Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3237">2023</a>), both in terms of formatting and substance.</p><p>In particular, the message prompt in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3243">2023</a>) and the above replication followed OpenAI’s Best practices for prompt engineering with the API (Shieh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Shieh J (2023) Best practices for prompt engineering with openai api. 
                https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api
                
              . OpenAI. Accessed on 01 Jan 2024" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR65" id="ref-link-section-d207556803e3246">2023</a>) through the use of (a) helpful markers (e.g. ‘```’) to separate instruction and context; (b) details regarding the desired output (i.e. specifying that the response should include ranked choices, as well as [in some cases] proper authority and citation; (c) an explicit template for the desired output (providing an example of the format in which GPT-4 should provide their response); and (d) perhaps most crucially, context regarding the type of question GPT-4 was answering (e.g. “please respond as if you are taking the bar exam”).</p><p>In contrast, in the minimally tailored prompting condition, the message prompt for a given question simply stated “Please answer the following question,” followed by the question and answer choices (a technique sometimes referred to as “basic prompting”: Choi et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Choi JH, Monahan A, Schwarcz D (2023) Lawyering in the age of artificial intelligence. Available at SSRN 4626276" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR14" id="ref-link-section-d207556803e3253">2023</a>). No additional context or formatting cues were provided.</p><p>In Condition 2 (“maximally tailored” condition), GPT-4 was tested using the highest performing prompt settings as revealed in the replication section above, with one addition, namely that: the system prompt, similar to the approaches used in Choi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Choi JH (2023) How to use large language models for empirical legal research. J Instit Theor Econ (Forthcoming)" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR13" id="ref-link-section-d207556803e3259">2023</a>), Choi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Choi JH, Monahan A, Schwarcz D (2023) Lawyering in the age of artificial intelligence. Available at SSRN 4626276" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR14" id="ref-link-section-d207556803e3262">2023</a>), was edited from its default (“you are a helpful assistant”) to a more tailored message that included included multiple example MBE questions with sample answer and explanations structured in the desired format (a technique sometimes referred to as “few-shot prompting”: Choi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Choi JH, Monahan A, Schwarcz D (2023) Lawyering in the age of artificial intelligence. Available at SSRN 4626276" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR14" id="ref-link-section-d207556803e3265">2023</a>)).</p><p>As in the replication section, 5 trials were conducted for each of the two conditions. Based on the lack of effect of temperature in the replication study, temperature was not a manipulated variable. Instead, both conditions featured the same temperature setting (.5).</p><p>To assess whether there was a significant difference between GPT-4’s accuracy in the maximally tailored vs minimally tailored conditions, a mixed-effects binary logistic regression was conducted with: (a) condition as a fixed effect; and (b) question number and question category as random effects. As above, these regressions were conducted using the lme4 (Bates et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Bates D, Mächler M, Bolker B, Walker S (2014) Fitting linear mixed-effects models using LME4. arXiv preprint 
                arXiv:1406.5823
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR2" id="ref-link-section-d207556803e3274">2014</a>) and lmertest (Kuznetsova et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kuznetsova A, Brockhoff PB, Christensen RHB (2017) lmertest package: tests in linear mixed effects models. J Stat Software 82:13" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR33" id="ref-link-section-d207556803e3277">2017</a>) packages from R.</p><h4 id="Sec17"><span>4.2.2 </span>Results</h4><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://link.springer.com/article/10.1007/s10506-024-09396-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10506-024-09396-9/MediaObjects/10506_2024_9396_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10506-024-09396-9/MediaObjects/10506_2024_9396_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="507"/></picture></a></div><p>GPT-4’s MBE Accuracy in minimally tailored vs. maximally tailored prompting conditions. Bars reflect the mean accuracy. Lines correspond to 95% bootstrapped confidence intervals</p></div></figure></div><p>Mean MBE accuracy across all trials in the maximally tailored condition was descriptively higher at 79.5% (95% CI: 77.1–82.1), than in the minimally tailored condition at 70.9% (95% CI: 68.1–73.7).</p><p>The regression model revealed a main effect of condition on accuracy (<span>\(\beta =1.395\)</span>, <span>\(\textrm{SE} =.192\)</span>, <span>\(p&lt;.0001\)</span>), such that GPT-4’s accuracy in the maximally tailored condition was significantly higher than its accuracy in the minimally tailored condition.</p><p>In terms of scaled score, GPT-4’s MBE score in the minimally tailored condition would be approximately 150, which would place it: (a) in the 70th percentile among July test takers; (b) 64th percentile among first-timers; and (c) 48th percentile among those who passed.</p><p>GPT-4’s score in the maximally tailored condition would be approximately 164—6 points higher than that reported by Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3393">2023</a>) and OpenAI (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023a" title="OpenAI (2023) GPT 4. 
                https://openai.com/research/gpt-4
                
              . Accessed on 24 Apr 2023" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR54" id="ref-link-section-d207556803e3396">2023a</a>). This would place it: (a) in the 95th percentile among July test takers; (b) 87th percentile among first-timers; and (c) 82th percentile among those who passed.</p><h3 id="Sec18"><span>4.3 </span>Re-examining the essay scores</h3><p>As confirmed in the above subsection, the scaled MBE score (not percentile) reported by OpenAI was accurately computed using the methods documented in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3408">2023</a>).</p><p>With regard to the essays (MPT + MEE), however, the method described by the authors significantly deviates in at least three aspects from the official method used by UBE states, to the point where one may not be confident that the essay scores reported by the authors reflect GPT models’ “true” essay scores (i.e., the score that essay examiners would have assigned to GPT had they been blindly scored using official grading protocol).</p><p>The first aspect relates to the (lack of) use of a formal rubric. For example, unlike NCBE protocol, which provides graders with (a) (in the case of the MEE) detailed “grading guidelines” for how to assign grades to essays and distinguish answers for a given MEE; and (b) (for both MEE and MPT) a specific “drafters’ point sheet” for each essay that includes detailed guidance from the drafting committee with a discussion of the issues raised and the intended analysis (Olson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Olson S (2019) 13 best practices for grading essays and performance tests. Bar Exam 88(4):8–14" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR52" id="ref-link-section-d207556803e3417">2019</a>), Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3420">2023</a>) do not report using an official or unofficial rubric of any kind, and instead simply describe comparing GPT-4’s answers to representative “good” answers from the state of Maryland.</p><p>Utilizing these answers as the basis for grading GPT-4’s answers in lieu of a formal rubric would seem to be particularly problematic considering it is unclear even what score these representative “good” answers received. As clarified by the Maryland bar examiners: “The Representative Good Answers are not ‘average’ passing answers nor are they necessarily ‘perfect’ answers. Instead, they are responses which, in the Board’s view, illustrate successful answers written by applicants who passed the UBE in Maryland for this session” (Maryland State Board of Law Examiners <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Maryland State Board of Law Examiners (2022) July 2022 uniform bar examination (UBE) in maryland—representative good answers. 
                https://mdcourts.gov/sites/default/files/import/ble/examanswers/2022/202207uberepgoodanswers.pdf
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR41" id="ref-link-section-d207556803e3426">2022</a>).</p><p>Given that (a) it is unclear what score these representative good answers received; and (b) these answers appear to be the basis for determining the score that GPT-4’s essays received, it would seem to follow that (c) it is likewise unclear what score GPT-4’s answers should receive. Consequently, it would likewise follow that any reported scaled score or percentile would seem to be insufficiently justified so as to serve as a basis for a conclusive statement regarding GPT-4’s relative performance on essays as compared to humans (e.g. a reported percentile).</p><p>The second aspect relates to the lack of NCBE training of the graders of the essays. Official NCBE essay grading protocol mandates the use of trained bar exam graders, who in addition to using a specific rubric for each question undergo a standardized training process prior to grading (Gunderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Gunderson JA (2015) The testing column: essay grading fundamentals. Bar Exam 84(1):54–56" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR21" id="ref-link-section-d207556803e3436">2015</a>; Case <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Case SM (2010) Procedure for grading essays and performance tests. The Bar Examiner. 
                https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR12" id="ref-link-section-d207556803e3439">2010</a>). In contrast, the graders in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3442">2023</a>) (a subset of the authors who were trained lawyers) do not report expertise or training in bar exam grading. Thus, although the graders of the essays were no doubt experts in legal reasoning more broadly, it seems unlikely that they would have been sufficiently ingrained in the specific grading protocols of the MEE + MPT to have been able to reliably infer or apply the specific grading rubric when assigning the raw scores to GPT-4.</p><p>The third aspect relates to both blinding and what bar examiners refer to as “calibration,” as UBE jurisdictions use an extensive procedure to ensure that graders are grading essays in a consistent manner (both with regard to other essays and in comparison to other graders) (Case <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Case SM (2010) Procedure for grading essays and performance tests. The Bar Examiner. 
                https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR12" id="ref-link-section-d207556803e3448">2010</a>; Gunderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Gunderson JA (2015) The testing column: essay grading fundamentals. Bar Exam 84(1):54–56" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR21" id="ref-link-section-d207556803e3451">2015</a>). In particular, all graders of a particular jurisdiction first blindly grade a set of 30 “calibration” essays of variable quality (first rank order, then absolute scores) and make sure that consistent scores are being assigned by different graders, and that the same score (e.g. 5 of 6) is being assigned to exams of similar quality (Case <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Case SM (2010) Procedure for grading essays and performance tests. The Bar Examiner. 
                https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR12" id="ref-link-section-d207556803e3454">2010</a>).</p><p>Unlike this approach, as well as efforts to assess GPT models’ law school performance (Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Choi JH, Hickman KE, Monahan AB, Schwarcz D (2021) Chatgpt goes to law school. J Legal Educ 71:387" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR15" id="ref-link-section-d207556803e3460">2021</a>), the method reported by Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3463">2023</a>) did not initially involve blinding. The method in Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3466">2023</a>) did involve a form of inter-grader calibration, as the authors gave “blinded samples” to independent lawyers to grade the exams, with the assigned scores “match[ing] or exceed[ing]” those assigned by the authors. Given the lack of reporting to the contrary, however, the method used by the graders would presumably be plagued by issue issues as highlighted above (no rubric, no formal training with bar exam grading, no formal intra-grader calibration).</p><p>Given the above issues, as well as the fact that, as alluded in the introduction, GPT-4’s performance boost over GPT-3 on other essay-based exams was far lower than that on the bar exam, it seems warranted not only to infer that GPT-4’s relative performance (in terms of percentile among human test-takers) was lower than that reported by OpenAI, but also that GPT-4’s reported scaled score on the essay may have deviated to some degree from GPT-4’s “true” essay (which, if true, would imply that GPT-4’s “true” percentile on the bar exam may be even lower than that estimated in previous sections).</p><p>Indeed, Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3475">2023</a>) to some degree acknowledge all of these limitations in their paper, writing: “While we recognize there is inherent variability in any qualitative assessment, our reliance on the state bars’ representative “good” answers and the multiple reviewers reduces the likelihood that our assessment is incorrect enough to alter the ultimate conclusion of passage in this paper”.</p><p>Given that GPT-4’s reported score of 298 is 28 points higher than the passing threshold (270) in the majority of UBE jurisdictions, it is true that the essay scores would have to have been wildly inaccurate in order to undermine the general conclusion of Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3482">2023</a>) (i.e., that GPT-4 “passed the [uniform] bar exam”). However, even supposing that GPT-4’s “true” percentile on the essay portion was just a few points lower than that reported by OpenAI, this would further call into question OpenAI’s claims regarding the relative performance of GPT-4 on the UBE relative to human test-takers. For example, supposing that GPT-4 scored 9 points lower on the essays, this would drop its estimated relative performance to (a) 31st percentile compared to July test-takers; (b) 24th percentile relative to first-time test takers; and (c) less than 5th percentile compared to licensed attorneys.</p></div></div></section><section data-title="Discussion"><div id="Sec19-section"><h2 id="Sec19"><span>5 </span>Discussion</h2><div id="Sec19-content"><p>This paper first investigated the issue of OpenAI’s claim of GPT-4’s 90th percentile UBE performance, resulting in four main findings. The first finding is that although GPT-4’s UBE score approaches the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates are heavily skewed towards low scorers, as the majority of test-takers in February failed the July administration and tend to score much lower than the general test-taking population. The second finding is that using July data from the same source would result in an estimate of <span>\(\sim\)</span>68th percentile, including below average performance on the essay portion. The third finding is that comparing GPT-4’s performance against first-time test takers would result in an estimate of <span>\(\sim\)</span>62nd percentile, including <span>\(\sim\)</span>42nd percentile on the essay portion. The fourth main finding is that when examining only those who passed the exam, GPT-4’s performance is estimated to drop to <span>\(\sim\)</span>48th percentile overall, and <span>\(\sim\)</span>15th percentile on essays.</p><p>In addition to these four main findings, the paper also investigated the validity of GPT-4’s reported UBE score of 298. Although the paper successfully replicated the MBE score of 158, the paper also highlighted several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the essay score (140).</p><p>Finally, the paper also investigated the effect of adjusting temperature settings and prompting techniques on GPT-4’s MBE performance, finding no significant effect of adjusting temperature settings on performance, and some effect of prompt engineering when compared to a basic prompting baseline condition.</p><p>Of course, assessing the capabilities of an AI system as compared to those of a practicing lawyer is no easy task. Scholars have identified several theoretical and practical difficulties in creating accurate measurement scales to assess AI capabilities and have pointed out various issues with some of the current scales (Hernandez-Orallo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Hernandez-Orallo J (2020) AI evaluation: on broken yardsticks and measurement scales. In: Workshop on evaluating evaluation of AI systems at AAAI" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR22" id="ref-link-section-d207556803e3593">2020</a>; Burden and Hernández-Orallo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Burden J, Hernández-Orallo J (2020) Exploring AI safety in degrees: generality, capability and control. In: Proceedings of the workshop on artificial intelligence safety (safeai 2020) co-located with 34th AAAI conference on artificial intelligence (AAAI 2020). pp 36–40" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR9" id="ref-link-section-d207556803e3596">2020</a>; Raji et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Raji ID, Bender EM, Paullada A, Denton E, Hanna A (2021) Ai and the everything in the whole wide world benchmark. arXiv preprint 
                arXiv:2111.15366
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR58" id="ref-link-section-d207556803e3599">2021</a>). Relatedly, some have pointed out that simply observing that GPT-4 under- or over-performs at a task in some setting is not necessarily reliable evidence that it (or some other LLM) is capable or incapable of performing that task in general (Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Bowman S (2022) The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail. In: Proceedings of the 60th annual meeting of the association for computational linguistics (vol 1: Long papers) pp 7484–7499" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR6" id="ref-link-section-d207556803e3602">2022</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Bowman SR (2023) Eight things to know about large language models. arXiv preprint 
                arXiv:2304.00612
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR7" id="ref-link-section-d207556803e3605">2023</a>; Kojima et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large language models are zero-shot reasoners. arXiv preprint 
                arXiv:2205.11916
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR31" id="ref-link-section-d207556803e3609">2022</a>).</p><p>In the context of legal profession specifically, there are various reasons to doubt the usefulness of UBE percentile as a proxy for lawyerly competence (both for humans and AI systems), given that, for example: (a) the content on the UBE is very general and does not pertain to the legal doctrine of any jurisdiction in the United States (National Conference of Bar Examiners n.d.-g), and thus knowledge (or ignorance) of that content does not necessarily translate to knowledge (or ignorance) of relevant legal doctrine for a practicing lawyer of any jurisdiction; (b) the tasks involved on the bar exam, particularly multiple-choice questions, do not reflect the tasks of practicing lawyers, and thus mastery (or lack of mastery) of those tasks does not necessarily reflect mastery (or lack of mastery) of the tasks of practicing lawyers; and (c) given the lack of direct professional incentive to obtain higher than a passing score (typically no higher than 270) (National Conference of Bar Examiners n.d.-a), obtaining a particularly high score or percentile past this threshold is less meaningful than for other exams (e.g. LSAT), where higher scores are taken into account for admission into select institutions (US News and World Report <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="US News and World Report (2022) 
                https://www.usnews.com/best-graduate-schools/top-law-schools/law-rankings
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR72" id="ref-link-section-d207556803e3616">2022</a>).</p><p>Setting these objections aside, however, to the extent that one believes the UBE to be a valid proxy for lawyerly competence, these results suggest GPT-4 to be substantially less lawyerly competent than previously assumed, as GPT-4’s score against likely attorneys (i.e. those who actually passed the bar) is <span>\(\sim\)</span>48th percentile. Moreover, when just looking at the essays, which more closely resemble the tasks of practicing lawyers and thus more plausibly reflect lawyerly competence, GPT-4’s performance falls in the bottom <span>\(\sim\)</span>15th percentile. These findings align with recent research work finding that GPT-4 performed below-average on law school exams (Blair-Stanek et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Blair-Stanek A, Carstens A-M, Goldberg DS, Graber M, Gray DC, Stearns ML (2023) Gpt-4’s law school grades, Partnership tax b, property b-, tax b. Crim C-, Law &amp; Econ C, Partnership Tax B, Property B-, Tax B" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR3" id="ref-link-section-d207556803e3658">2023</a>).</p><p>The lack of precision and transparency in OpenAI’s reporting of GPT-4’s UBE performance has implications for both the current state of the legal profession and the future of AI safety. On the legal side, there appear to be at least two sets of implications. On the one hand, to the extent that lawyers put stock in the bar exam as a proxy for general legal competence, the results might give practicing lawyers at least a mild temporary sense of relief regarding the security of the profession, given that the majority of lawyers perform better than GPT on the component of the exam (essay-writing) that seems to best reflect their day-to-day activities (and by extension, the tasks that would likely need to be automated in order to supplant lawyers in their day-to-day professional capacity).</p><p>On the other hand, the fact that GPT-4’s reported “90th percentile” capabilities were so widely publicized might pose some concerns that lawyers and non-lawyers may use GPT-4 for complex legal tasks for which it is incapable of adequately performing, plausibly increasing the rate of (a) misapplication of the law by judges; (b) professional malpractice by lawyers; and (c) ineffective pro se representation and/or unauthorized practice of law by non-lawyers. From a legal education standpoint, law students who overestimate GPT-4’s UBE capabilities might also develop an unwarranted sense of apathy towards developing critical legal-analytical skills, particularly if under the impression that GPT-4’s level of mastery of those skills already surpasses that to which a typical law student could be expected to reach.</p><p>On the AI front, these findings raise concerns both for the transparency<sup><a href="#Fn20"><span>Footnote </span>20</a></sup> of capabilities research and the safety of AI development more generally. In particular, to the extent that one considers transparency to be an important prerequisite for safety (Brundage et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Brundage M, Avin S, Wang J, Belfield H, Krueger G, Hadfield G, et al (2020) Toward trustworthy AI development: mechanisms for supporting verifiable claims. arXiv preprint 
                arXiv:2004.07213
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR8" id="ref-link-section-d207556803e3676">2020</a>), these findings underscore the importance of implementing rigorous transparency measures so as to reliably identify potential warning signs of transformative progress in artificial intelligence as opposed to creating a false sense of alarm or security (Zoe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Zoe Cremer C, Whittlestone J (2021) Artificial canaries: early warning signs for anticipatory and democratic governance of AI" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR78" id="ref-link-section-d207556803e3679">2021</a>). Implementing such measures could help ensure that AI development, as stated in OpenAI’s charter, is a “value-aligned, safety-conscious project” as opposed to becoming “a competitive race without time for adequate safety precautions” (OpenAI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="OpenAI (2018) OpenAI Charter. 
                https://openai.com/charter
                
              " href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR53" id="ref-link-section-d207556803e3682">2018</a>).</p><p>Of course, the present study does not discount the progress that AI has made in the context of legally relevant tasks; after all, the improvement in UBE performance from GPT-3.5 to GPT-4 as estimated in this study remains impressive (arguably equally or even more so given that GPT-3.5’s performance is also estimated to be significantly lower than previously assumed), even if not as flashy as the 10th–90th percentile boost of OpenAI’s official estimation. Nor does the present study discount the seemingly inevitable future improvement of AI systems to levels far beyond their present capabilities, or, as phrased in <i>GPT-4 Passes the Bar Exam</i>, that the present capabilities “highlight the floor, not the ceiling, of future application” (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR28" id="ref-link-section-d207556803e3691">2023</a>, 11).</p><p>To the contrary, given the inevitable rapid growth of AI systems, the results of the present study underscore the importance of implementing rigorous and transparent evaluation measures to ensure that both the general public and relevant decision-makers are made appropriately aware of the system’s capabilities, and to prevent these systems from being used in an unintentionally harmful or catastrophic manner. The results also indicate that law schools and the legal profession should prioritize instruction in areas such as law and technology and law and AI, which, despite their importance, are currently not viewed as descriptively or normatively central to the legal academy (Martínez and Tobia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2023" title="Martínez E, Tobia K (2023) What do law professors believe about law and the legal academy? Geo LJ 112:111" href="https://link.springer.com/article/10.1007/s10506-024-09396-9#ref-CR37" id="ref-link-section-d207556803e3698">2023</a>).</p></div></div></section>
                                </div><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ul data-track-component="outbound reference"><li><p id="ref-CR1">Albanese MA (2014) The testing column: scaling: it’s not just for fish or mountains. Bar Exam 83(4):50–56</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20testing%20column%3A%20scaling%3A%20it%E2%80%99s%20not%20just%20for%20fish%20or%20mountains&amp;journal=Bar%20Exam&amp;volume=83&amp;issue=4&amp;pages=50-56&amp;publication_year=2014&amp;author=Albanese%2CMA">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR2">Bates D, Mächler M, Bolker B, Walker S (2014) Fitting linear mixed-effects models using LME4. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:1406.5823</a></p></li><li><p id="ref-CR3">Blair-Stanek A, Carstens A-M, Goldberg DS, Graber M, Gray DC, Stearns ML (2023) Gpt-4’s law school grades, Partnership tax b, property b-, tax b. Crim C-, Law &amp; Econ C, Partnership Tax B, Property B-, Tax B</p></li><li><p id="ref-CR4">Bommarito MJ II, Katz DM (2017) Measuring and modeling the us regulatory ecosystem. J Stat Phys 168:1125–1135</p><p><a data-track="click" rel="noopener" data-track-label="10.1007/s10955-017-1846-3" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10955-017-1846-3" aria-label="Article reference 4" data-doi="10.1007/s10955-017-1846-3">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20and%20modeling%20the%20us%20regulatory%20ecosystem&amp;journal=J%20Stat%20Phys&amp;doi=10.1007%2Fs10955-017-1846-3&amp;volume=168&amp;pages=1125-1135&amp;publication_year=2017&amp;author=Bommarito%2CMJ&amp;author=Katz%2CDM">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR5">Bostrom N, Yudkowsky E (2018) The ethics of artificial intelligence. Artificial intelligence safety and security. Chapman and Hall/CRC, New York, pp 57–69</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ethics%20of%20artificial%20intelligence.%20Artificial%20intelligence%20safety%20and%20security&amp;pages=57-69&amp;publication_year=2018&amp;author=Bostrom%2CN&amp;author=Yudkowsky%2CE">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR6">Bowman S (2022) The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail. In: Proceedings of the 60th annual meeting of the association for computational linguistics (vol 1: Long papers) pp 7484–7499</p></li><li><p id="ref-CR7">Bowman SR (2023) Eight things to know about large language models. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2304.00612</a></p></li><li><p id="ref-CR8">Brundage M, Avin S, Wang J, Belfield H, Krueger G, Hadfield G, et al (2020) Toward trustworthy AI development: mechanisms for supporting verifiable claims. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2004.07213</a></p></li><li><p id="ref-CR9">Burden J, Hernández-Orallo J (2020) Exploring AI safety in degrees: generality, capability and control. In: Proceedings of the workshop on artificial intelligence safety (safeai 2020) co-located with 34th AAAI conference on artificial intelligence (AAAI 2020). pp 36–40</p></li><li><p id="ref-CR10">Carlsmith J (2022) Is power-seeking AI an existential risk? arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2206.13353</a></p></li><li><p id="ref-CR11">Caron P (2023) GPT-4 Beats 90% of aspiring lawyers on the bar exam. TaxProf Blog. <a href="https://taxprof.typepad.com/taxprof_blog/2023/03/gpt-4-beats-90-of-aspiring-lawyers-on-the-bar-exam.html" data-track="click" data-track-action="external reference" data-track-label="https://taxprof.typepad.com/taxprof_blog/2023/03/gpt-4-beats-90-of-aspiring-lawyers-on-the-bar-exam.html">https://taxprof.typepad.com/taxprof_blog/2023/03/gpt-4-beats-90-of-aspiring-lawyers-on-the-bar-exam.html</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR12">Case SM (2010) Procedure for grading essays and performance tests. The Bar Examiner. <a href="https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf" data-track="click" data-track-action="external reference" data-track-label="https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf">https://thebarexaminer.ncbex.org/wp-content/uploads/PDFs/790410_TestingColumn.pdf</a></p></li><li><p id="ref-CR13">Choi JH (2023) How to use large language models for empirical legal research. J Instit Theor Econ (Forthcoming)</p></li><li><p id="ref-CR14">Choi JH, Monahan A, Schwarcz D (2023) Lawyering in the age of artificial intelligence. Available at SSRN 4626276</p></li><li><p id="ref-CR15">Choi JH, Hickman KE, Monahan AB, Schwarcz D (2021) Chatgpt goes to law school. J Legal Educ 71:387</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Chatgpt%20goes%20to%20law%20school&amp;journal=J%20Legal%20Educ&amp;volume=71&amp;publication_year=2021&amp;author=Choi%2CJH&amp;author=Hickman%2CKE&amp;author=Monahan%2CAB&amp;author=Schwarcz%2CD">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR16">Cockburn A, Dragicevic P, Besançon L, Gutwin C (2020) Threats of a replication crisis in empirical computer science. Commun ACM 63(8):70–79</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/3360311" data-track-action="article reference" href="https://doi.org/10.1145%2F3360311" aria-label="Article reference 16" data-doi="10.1145/3360311">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Threats%20of%20a%20replication%20crisis%20in%20empirical%20computer%20science&amp;journal=Commun%20ACM&amp;doi=10.1145%2F3360311&amp;volume=63&amp;issue=8&amp;pages=70-79&amp;publication_year=2020&amp;author=Cockburn%2CA&amp;author=Dragicevic%2CP&amp;author=Besan%C3%A7on%2CL&amp;author=Gutwin%2CC">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR17">Crootof R, Kaminski ME, Price II WN (2023) Humans in the loop. Vanderbilt Law Review, (Forthcoming)</p></li><li><p id="ref-CR18">Echtler F, Häußler M (2018) Open source, open science, and the replication crisis in HCI. Extended abstracts of the 2018 chi conference on human factors in computing systems. pp 1–8</p></li><li><p id="ref-CR19">Examiner TB (n.d.-a) First-time exam takers and repeaters in 2021. The Bar Examiner. <a href="https://thebarexaminer.ncbex.org/2021-statistics/first-time-exam-takers-and-repeaters-in-2021/" data-track="click" data-track-action="external reference" data-track-label="https://thebarexaminer.ncbex.org/2021-statistics/first-time-exam-takers-and-repeaters-in-2021/">https://thebarexaminer.ncbex.org/2021-statistics/first-time-exam-takers-and-repeaters-in-2021/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR20">Examiner TB (n.d.-b) Statistics. The Bar Examiner. <a href="https://thebarexaminer.ncbex.org/statistics/" data-track="click" data-track-action="external reference" data-track-label="https://thebarexaminer.ncbex.org/statistics/">https://thebarexaminer.ncbex.org/statistics/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR21">Gunderson JA (2015) The testing column: essay grading fundamentals. Bar Exam 84(1):54–56</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20testing%20column%3A%20essay%20grading%20fundamentals&amp;journal=Bar%20Exam&amp;volume=84&amp;issue=1&amp;pages=54-56&amp;publication_year=2015&amp;author=Gunderson%2CJA">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR22">Hernandez-Orallo J (2020) AI evaluation: on broken yardsticks and measurement scales. In: Workshop on evaluating evaluation of AI systems at AAAI</p></li><li><p id="ref-CR23">Illinois Board of Admissions to the Bar. (2018) <a href="https://www.ilbaradmissions.org/percentile-equivalent-charts-july-2018" data-track="click" data-track-action="external reference" data-track-label="https://www.ilbaradmissions.org/percentile-equivalent-charts-july-2018">https://www.ilbaradmissions.org/percentile-equivalent-charts-july-2018</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR24">Illinois Board of Admissions to the Bar. (2019) <a href="https://www.ilbaradmissions.org/percentile-equivalent-charts-february-2019" data-track="click" data-track-action="external reference" data-track-label="https://www.ilbaradmissions.org/percentile-equivalent-charts-february-2019">https://www.ilbaradmissions.org/percentile-equivalent-charts-february-2019</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR25">JD Advising (n.d.) MBE raw score conversion chart. <a href="https://jdadvising.com/mbe-raw-score-conversion-chart/" data-track="click" data-track-action="external reference" data-track-label="https://jdadvising.com/mbe-raw-score-conversion-chart/">https://jdadvising.com/mbe-raw-score-conversion-chart/</a>. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR26">JD Advising. (n.d.) <a href="https://jdadvising.com/july-2018-ube-percentiles-chart/" data-track="click" data-track-action="external reference" data-track-label="https://jdadvising.com/july-2018-ube-percentiles-chart/">https://jdadvising.com/july-2018-ube-percentiles-chart/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR27">Jensen TI, Kelly B, Pedersen LH (2023) Is there a replication crisis in finance? J Finance 78(5):2465–2518</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/jofi.13249" data-track-action="article reference" href="https://doi.org/10.1111%2Fjofi.13249" aria-label="Article reference 27" data-doi="10.1111/jofi.13249">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20there%20a%20replication%20crisis%20in%20finance%3F&amp;journal=J%20Finance&amp;doi=10.1111%2Fjofi.13249&amp;volume=78&amp;issue=5&amp;pages=2465-2518&amp;publication_year=2023&amp;author=Jensen%2CTI&amp;author=Kelly%2CB&amp;author=Pedersen%2CLH">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR28">Katz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. Available at SSRN 4389233</p></li><li><p id="ref-CR29">Katz DM, Bommarito MJ (2014) Measuring the complexity of the law: the United States code. Artif Intell Law 22:337–374</p><p><a data-track="click" rel="noopener" data-track-label="10.1007/s10506-014-9160-8" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10506-014-9160-8" aria-label="Article reference 29" data-doi="10.1007/s10506-014-9160-8">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20the%20complexity%20of%20the%20law%3A%20the%20United%20States%20code&amp;journal=Artif%20Intell%20Law&amp;doi=10.1007%2Fs10506-014-9160-8&amp;volume=22&amp;pages=337-374&amp;publication_year=2014&amp;author=Katz%2CDM&amp;author=Bommarito%2CMJ">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR30">Koetsier J (2023) GPT-4 Beats 90% of Lawyers Trying to Pass the Bar. Forbes. <a href="https://www.forbes.com/sites/johnkoetsier/2023/03/14/gpt-4-beats-90-of-lawyers-trying-to-pass-the-bar/?sh=b40c88d30279" data-track="click" data-track-action="external reference" data-track-label="https://www.forbes.com/sites/johnkoetsier/2023/03/14/gpt-4-beats-90-of-lawyers-trying-to-pass-the-bar/?sh=b40c88d30279">https://www.forbes.com/sites/johnkoetsier/2023/03/14/gpt-4-beats-90-of-lawyers-trying-to-pass-the-bar/?sh=b40c88d30279</a></p></li><li><p id="ref-CR31">Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large language models are zero-shot reasoners. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2205.11916</a></p></li><li><p id="ref-CR32">Kubiszyn T, Borich GD (2016) Educational testing and measurement. John Wiley &amp; Sons, Hoboken</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Educational%20testing%20and%20measurement&amp;publication_year=2016&amp;author=Kubiszyn%2CT&amp;author=Borich%2CGD">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR33">Kuznetsova A, Brockhoff PB, Christensen RHB (2017) lmertest package: tests in linear mixed effects models. J Stat Software 82:13</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.18637/jss.v082.i13" data-track-action="article reference" href="https://doi.org/10.18637%2Fjss.v082.i13" aria-label="Article reference 33" data-doi="10.18637/jss.v082.i13">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=lmertest%20package%3A%20tests%20in%20linear%20mixed%20effects%20models&amp;journal=J%20Stat%20Software&amp;doi=10.18637%2Fjss.v082.i13&amp;volume=82&amp;publication_year=2017&amp;author=Kuznetsova%2CA&amp;author=Brockhoff%2CPB&amp;author=Christensen%2CRHB">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR34">Lang C (2023) What is a good bar exam score? Test Prep Insight. <a href="https://www.testprepinsight.com/what-is-a-good-bar-exam-score" data-track="click" data-track-action="external reference" data-track-label="https://www.testprepinsight.com/what-is-a-good-bar-exam-score">https://www.testprepinsight.com/what-is-a-good-bar-exam-score</a></p></li><li><p id="ref-CR35">Li B, Qi P, Liu B, Di S, Liu J, Pei J, Zhou B (2023) Trustworthy AI: From principles to practices. ACM Comput Surv 55(9):1–46</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/3555803" data-track-action="article reference" href="https://doi.org/10.1145%2F3555803" aria-label="Article reference 35" data-doi="10.1145/3555803">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Trustworthy%20AI%3A%20From%20principles%20to%20practices&amp;journal=ACM%20Comput%20Surv&amp;doi=10.1145%2F3555803&amp;volume=55&amp;issue=9&amp;pages=1-46&amp;publication_year=2023&amp;author=Li%2CB&amp;author=Qi%2CP&amp;author=Liu%2CB&amp;author=Di%2CS&amp;author=Liu%2CJ&amp;author=Pei%2CJ&amp;author=Zhou%2CB">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR36">Markou C, Deakin S (2020) Is law computable? From rule of law to legal singularity. From Rule of Law to Legal Singularity. University of Cambridge Faculty of Law Research Paper</p></li><li><p id="ref-CR37">Martínez E, Tobia K (2023) What do law professors believe about law and the legal academy? Geo LJ 112:111</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20do%20law%20professors%20believe%20about%20law%20and%20the%20legal%20academy%3F&amp;journal=Geo%20LJ&amp;volume=112&amp;publication_year=2023&amp;author=Mart%C3%ADnez%2CE&amp;author=Tobia%2CK">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR38">Martinez E, Mollica F, Gibson E (2022) Poor writing, not specialized concepts, drives processing difficulty in legal language. Cognition 224:105070</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cognition.2022.105070" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cognition.2022.105070" aria-label="Article reference 38" data-doi="10.1016/j.cognition.2022.105070">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Poor%20writing%2C%20not%20specialized%20concepts%2C%20drives%20processing%20difficulty%20in%20legal%20language&amp;journal=Cognition&amp;doi=10.1016%2Fj.cognition.2022.105070&amp;volume=224&amp;publication_year=2022&amp;author=Martinez%2CE&amp;author=Mollica%2CF&amp;author=Gibson%2CE">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR39">Martinez E, Mollica F, Gibson E (2022b) So much for plain language: An analysis of the accessibility of united states federal laws (1951–2009). In: Proceedings of the annual meeting of the cognitive science society, vol 44</p></li><li><p id="ref-CR40">Martinez E, Mollica F, Gibson E (in press) Even lawyers don’t like legalese. In: Proceedings of the national academy of sciences</p></li><li><p id="ref-CR41">Maryland State Board of Law Examiners (2022) July 2022 uniform bar examination (UBE) in maryland—representative good answers. <a href="https://mdcourts.gov/sites/default/files/import/ble/examanswers/2022/202207uberepgoodanswers.pdf" data-track="click" data-track-action="external reference" data-track-label="https://mdcourts.gov/sites/default/files/import/ble/examanswers/2022/202207uberepgoodanswers.pdf">https://mdcourts.gov/sites/default/files/import/ble/examanswers/2022/202207uberepgoodanswers.pdf</a></p></li><li><p id="ref-CR42">National Conference of Bar Examiners (2023) Bar exam results by jurisdiction. <a href="https://www.ncbex.org/statistics-research/bar-exam-results-jurisdiction" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/statistics-research/bar-exam-results-jurisdiction">https://www.ncbex.org/statistics-research/bar-exam-results-jurisdiction</a>. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR43">National Conference of Bar Examiners (n.d.-a) <a href="https://www.ncbex.org/exams/ube/scores/" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/ube/scores/">https://www.ncbex.org/exams/ube/scores/</a>. Accessed on 03 May 2023</p></li><li><p id="ref-CR44">National Conference of Bar Examiners (n.d.-b) <a href="https://www.ncbex.org/exams/ube/score-portability/minimum-scores/" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/ube/score-portability/minimum-scores/">https://www.ncbex.org/exams/ube/score-portability/minimum-scores/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR45">National Conference of Bar Examiners (n.d.-c) Bar Exam Results by Jurisdiction. National Conference of Bar Examiners. <a href="https://www.ncbex.org/statistics-and-research/bar-exam-results/" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/statistics-and-research/bar-exam-results/">https://www.ncbex.org/statistics-and-research/bar-exam-results/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR46">National Conference of Bar Examiners (n.d.-d) Multistate bar exam. <a href="https://www.ncbex.org/exams/mbe" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/mbe">https://www.ncbex.org/exams/mbe</a>. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR47">National Conference of Bar Examiners (n.d.-e) Multistate essay exam. <a href="https://www.ncbex.org/exams/mee" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/mee">https://www.ncbex.org/exams/mee</a>. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR48">National Conference of Bar Examiners (n.d.-f) Multistate performance test. <a href="https://www.ncbex.org/exams/mpt" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/mpt">https://www.ncbex.org/exams/mpt</a>. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR49">National Conference of Bar Examiners (n.d.-g) Uniform bar exam. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR50">National Conference of Bar Examiners (n.d.-h) Uniform Bar Examination. National Conference of Bar Examiners. <a href="https://www.ncbex.org/exams/ube/" data-track="click" data-track-action="external reference" data-track-label="https://www.ncbex.org/exams/ube/">https://www.ncbex.org/exams/ube/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR51">Ngo R (2022) The alignment problem from a deep learning perspective. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2209.00626</a></p></li><li><p id="ref-CR52">Olson S (2019) 13 best practices for grading essays and performance tests. Bar Exam 88(4):8–14</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=13%20best%20practices%20for%20grading%20essays%20and%20performance%20tests&amp;journal=Bar%20Exam&amp;volume=88&amp;issue=4&amp;pages=8-14&amp;publication_year=2019&amp;author=Olson%2CS">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR53">OpenAI (2018) OpenAI Charter. <a href="https://openai.com/charter" data-track="click" data-track-action="external reference" data-track-label="https://openai.com/charter">https://openai.com/charter</a></p></li><li><p id="ref-CR54">OpenAI (2023) GPT 4. <a href="https://openai.com/research/gpt-4" data-track="click" data-track-action="external reference" data-track-label="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR55">OpenAI (2023) GPT-4 Technical Report. <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2303.08774</a>. (Preprint submitted to arXiv)</p></li><li><p id="ref-CR56">OpenAI (n.d.) GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses. <a href="https://openai.com/product/gpt-4" data-track="click" data-track-action="external reference" data-track-label="https://openai.com/product/gpt-4">https://openai.com/product/gpt-4</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR57">Patrice J (2023) New GPT-4 Passes All Sections Of The Uniform Bar Exam. Maybe This Will Finally Kill The Bar Exam. Above the Law. <a href="https://abovethelaw.com/2023/03/new-gpt-4-passes-all-sections-of-the-uniform-bar-exam-maybe-this-will-finally-kill-the-bar-exam/" data-track="click" data-track-action="external reference" data-track-label="https://abovethelaw.com/2023/03/new-gpt-4-passes-all-sections-of-the-uniform-bar-exam-maybe-this-will-finally-kill-the-bar-exam/">https://abovethelaw.com/2023/03/new-gpt-4-passes-all-sections-of-the-uniform-bar-exam-maybe-this-will-finally-kill-the-bar-exam/</a></p></li><li><p id="ref-CR58">Raji ID, Bender EM, Paullada A, Denton E, Hanna A (2021) Ai and the everything in the whole wide world benchmark. arXiv preprint <a href="" data-track="click" data-track-action="external reference" data-track-label="">arXiv:2111.15366</a></p></li><li><p id="ref-CR59">Ray T (2023) With GPT-4, OpenAI opts for secrecy versus disclosure. ZDNet. <a href="https://www.zdnet.com/article/with-gpt-4-openai-opts-for-secrecy-versus-disclosure/" data-track="click" data-track-action="external reference" data-track-label="https://www.zdnet.com/article/with-gpt-4-openai-opts-for-secrecy-versus-disclosure/">https://www.zdnet.com/article/with-gpt-4-openai-opts-for-secrecy-versus-disclosure/</a></p></li><li><p id="ref-CR60">Reshetar R (2022) The testing column: Why are February bar exam pass rates lower than July pass rates? Bar Exam 91(1):51–53</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20testing%20column%3A%20Why%20are%20February%20bar%20exam%20pass%20rates%20lower%20than%20July%20pass%20rates%3F&amp;journal=Bar%20Exam&amp;volume=91&amp;issue=1&amp;pages=51-53&amp;publication_year=2022&amp;author=Reshetar%2CR">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR61">Ruhl J, Katz DM, Bommarito MJ (2017) Harnessing legal complexity. Science 355(6332):1377–1378</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.aag3013" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aag3013" aria-label="Article reference 61" data-doi="10.1126/science.aag3013">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Harnessing%20legal%20complexity&amp;journal=Science&amp;doi=10.1126%2Fscience.aag3013&amp;volume=355&amp;issue=6332&amp;pages=1377-1378&amp;publication_year=2017&amp;author=Ruhl%2CJ&amp;author=Katz%2CDM&amp;author=Bommarito%2CMJ">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR62">Rules.com M (n.d.) Bar Exam Calculators. <a href="https://mberules.com/bar-exam-calculators/?__cf_chl_tk=lTwxFyYWOZqBwTAenLs0TzDfAuvawkHeH2GaXU1PQo0-1683060961-0-gaNycGzNDBA" data-track="click" data-track-action="external reference" data-track-label="https://mberules.com/bar-exam-calculators/?__cf_chl_tk=lTwxFyYWOZqBwTAenLs0TzDfAuvawkHeH2GaXU1PQo0-1683060961-0-gaNycGzNDBA">https://mberules.com/bar-exam-calculators/?__cf_chl_tk=lTwxFyYWOZqBwTAenLs0TzDfAuvawkHeH2GaXU1PQo0-1683060961-0-gaNycGzNDBA</a>. Accessed on 02 May 2023</p></li><li><p id="ref-CR63">Schooler JW (2014) Metascience could rescue the replication crisis. Nature 515(7525):9</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/515009a" data-track-action="article reference" href="https://doi.org/10.1038%2F515009a" aria-label="Article reference 63" data-doi="10.1038/515009a">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Metascience%20could%20rescue%20the%20replication%20crisis&amp;journal=Nature&amp;doi=10.1038%2F515009a&amp;volume=515&amp;issue=7525&amp;publication_year=2014&amp;author=Schooler%2CJW">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR64">Schwarcz D, Choi JH (2023) Ai tools for lawyers: a practical guide. Available at SSRN</p></li><li><p id="ref-CR65">Shieh J (2023) Best practices for prompt engineering with openai api. <a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api" data-track="click" data-track-action="external reference" data-track-label="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api">https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api</a>. OpenAI. Accessed on 01 Jan 2024</p></li><li><p id="ref-CR66">Shrout PE, Rodgers JL (2018) Psychology, science, and knowledge construction: broadening perspectives from the replication crisis. Ann Rev Psychol 69:487–510</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1146/annurev-psych-122216-011845" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev-psych-122216-011845" aria-label="Article reference 66" data-doi="10.1146/annurev-psych-122216-011845">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Psychology%2C%20science%2C%20and%20knowledge%20construction%3A%20broadening%20perspectives%20from%20the%20replication%20crisis&amp;journal=Ann%20Rev%20Psychol&amp;doi=10.1146%2Fannurev-psych-122216-011845&amp;volume=69&amp;pages=487-510&amp;publication_year=2018&amp;author=Shrout%2CPE&amp;author=Rodgers%2CJL">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR67">Stokel-Walker C (2023) Critics denounce a lack of transparency around GPT-4’s tech. Fast Company. <a href="https://www.fastcompany.com/90866190/critics-denounce-a-lack-of-transparency-around-gpt-4s-tech" data-track="click" data-track-action="external reference" data-track-label="https://www.fastcompany.com/90866190/critics-denounce-a-lack-of-transparency-around-gpt-4s-tech">https://www.fastcompany.com/90866190/critics-denounce-a-lack-of-transparency-around-gpt-4s-tech</a></p></li><li><p id="ref-CR68">The National Bar Examiner (n.d.) <a href="https://thebarexaminer.ncbex.org/2022-statistics/the-multistate-bar-examination-mbe/#step3" data-track="click" data-track-action="external reference" data-track-label="https://thebarexaminer.ncbex.org/2022-statistics/the-multistate-bar-examination-mbe/#step3">https://thebarexaminer.ncbex.org/2022-statistics/the-multistate-bar-examination-mbe/#step3</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR69">The New York State Board of Law Examiners (n.d.) NYS Bar Exam Statistics. The New York State Board of Law Examiners. <a href="https://www.nybarexam.org/examstats/estats.htm" data-track="click" data-track-action="external reference" data-track-label="https://www.nybarexam.org/examstats/estats.htm">https://www.nybarexam.org/examstats/estats.htm</a></p></li><li><p id="ref-CR70">UBEEssays.com. (2019) <a href="https://ubeessays.com/feb-mbe-percentiles/" data-track="click" data-track-action="external reference" data-track-label="https://ubeessays.com/feb-mbe-percentiles/">https://ubeessays.com/feb-mbe-percentiles/</a></p></li><li><p id="ref-CR71">University of Illinois Chicago (n.d.) <a href="https://law.uic.edu/student-support/academic-achievement/bar-exam-information/illinois-bar-exam/" data-track="click" data-track-action="external reference" data-track-label="https://law.uic.edu/student-support/academic-achievement/bar-exam-information/illinois-bar-exam/">https://law.uic.edu/student-support/academic-achievement/bar-exam-information/illinois-bar-exam/</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR72">US News and World Report (2022) <a href="https://www.usnews.com/best-graduate-schools/top-law-schools/law-rankings" data-track="click" data-track-action="external reference" data-track-label="https://www.usnews.com/best-graduate-schools/top-law-schools/law-rankings">https://www.usnews.com/best-graduate-schools/top-law-schools/law-rankings</a></p></li><li><p id="ref-CR73">Washington State Bar Association (2020) <a href="https://wsba.org/news-events/latest-news/news-detail/2020/06/15/state-supreme-court-grants-diploma-privilege" data-track="click" data-track-action="external reference" data-track-label="https://wsba.org/news-events/latest-news/news-detail/2020/06/15/state-supreme-court-grants-diploma-privilege">https://wsba.org/news-events/latest-news/news-detail/2020/06/15/state-supreme-court-grants-diploma-privilege</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR74">Weiss DC (2023) Latest version of ChatGPT aces bar exam with score nearing 90th percentile. ABA Journal. <a href="https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile" data-track="click" data-track-action="external reference" data-track-label="https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile">https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR75">Wilkins S (2023) How GPT-4 mastered the entire bar exam, and why that matters. Law.com. <a href="https://www.law.com/legaltechnews/2023/03/17/how-gpt-4-mastered-the-entire-bar-exam-and-why-that-matters/?slreturn=20230324023302" data-track="click" data-track-action="external reference" data-track-label="https://www.law.com/legaltechnews/2023/03/17/how-gpt-4-mastered-the-entire-bar-exam-and-why-that-matters/?slreturn=20230324023302">https://www.law.com/legaltechnews/2023/03/17/how-gpt-4-mastered-the-entire-bar-exam-and-why-that-matters/?slreturn=20230324023302</a>. Accessed on 24 Apr 2023</p></li><li><p id="ref-CR76">Winter CK (2022) The challenges of artificial judicial decision-making for liberal democracy. Judicial decision-making: Integrating empirical and theoretical perspectives. Springer, Berlin, pp 179–204</p></li><li><p id="ref-CR77">Winter C, Hollman N, Manheim D (2023) Value alignment for advanced artificial judicial intelligence. Am Philos Quart 60(2):187–203</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.5406/21521123.60.2.06" data-track-action="article reference" href="https://doi.org/10.5406%2F21521123.60.2.06" aria-label="Article reference 77" data-doi="10.5406/21521123.60.2.06">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 77" href="http://scholar.google.com/scholar_lookup?&amp;title=Value%20alignment%20for%20advanced%20artificial%20judicial%20intelligence&amp;journal=Am%20Philos%20Quart&amp;doi=10.5406%2F21521123.60.2.06&amp;volume=60&amp;issue=2&amp;pages=187-203&amp;publication_year=2023&amp;author=Winter%2CC&amp;author=Hollman%2CN&amp;author=Manheim%2CD">
                    Google Scholar</a> 
                </p></li><li><p id="ref-CR78">Zoe Cremer C, Whittlestone J (2021) Artificial canaries: early warning signs for anticipatory and democratic governance of AI</p></li></ul><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10506-024-09396-9?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div></div>
  </body>
</html>
