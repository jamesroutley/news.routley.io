<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/felafax/felafax">Original</a>
    <h1>Show HN: Tune LLaMa3.1 on Google Cloud TPUs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/felafax/felafax/blob/main/utils/assets/image.jpg"><img src="https://github.com/felafax/felafax/raw/main/utils/assets/image.jpg" alt="image"/></a></p>
<p dir="auto">Felafax is a framework for continued-training and fine-tuning open source LLMs using <strong>XLA runtime</strong>. We take care of neceessary runtime setup and provide a Jupyter notebook out-of-box to just get started.</p>
<ul dir="auto">
<li>Easy to use.</li>
<li>Easy to configure all aspects of training (designed for ML researchers and hackers).</li>
<li>Easy to scale training from a single TPU VM with 8 cores to entire TPU Pod containing 6000 TPU cores (<strong>1000X</strong>)!</li>
</ul>

<p dir="auto">Our goal at <a href="https://felafax.ai" rel="nofollow">felafax</a> is to build infra to make it easier to run AI workloads on non-NVIDIA hardware (TPU, AWS Trainium, AMD GPU, and Intel GPU).</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Currently supported models</h2><a id="user-content-currently-supported-models" aria-label="Permalink: Currently supported models" href="#currently-supported-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>LLaMa-3.1 JAX Implementation</strong> <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="ce6f78b4f9c0e9ab6960e51bd4f034c6">$${\color{red}New!}$$</math-renderer></p>
<ul dir="auto">
<li>Converted from PyTorch to JAX for improved performance</li>
<li>By default, runs 2-way data parallel and 2-way model parallel training (2 data parallel model copies and each model copy is sharded across two TPU chips).</li>
<li>Compatible with NVIDIA GPUs and TPUs</li>
<li>Full-precision training support</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLaMa-3/3.1 PyTorch XLA</strong></p>
<ul dir="auto">
<li>LoRA and full-precision training support</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Gemma2 Models (2B, 9B, 27B)</strong></p>
<ul dir="auto">
<li>Optimized for Cloud TPUs</li>
<li>Fast full-precision training</li>
</ul>
</li>
</ul>

<p dir="auto"><strong>For a hosted version with a seamless workflow, please visit <a href="https://app.felafax.ai" rel="nofollow">app.felafax.ai</a></strong> ðŸ¦Š.</p>
<p dir="auto">We are also <strong>onboarding people to try out Google&#39;s latest generation TPUs</strong>, if you are interested sign up to the waitlist <a href="https://tally.so/r/mRLeaQ" rel="nofollow">here</a>.</p>
<p dir="auto">If you prefer a self-hosted training version, follow the instructions below. These steps will guide you through launching a TPU VM on your Google Cloud account and starting a Jupyter notebook. With just 3 simple steps, you&#39;ll be up and running in under 10 minutes. ðŸš€</p>
<ol dir="auto">
<li>
<p dir="auto">Install gcloud command-line tool and authenticate your account (SKIP this STEP if you already have gcloud installed and have used TPUs before! ðŸ˜Ž)</p>
<div dir="auto" data-snippet-clipboard-copy-content=" # Download gcloud CLI
 curl https://sdk.cloud.google.com | bash
 source ~/.bashrc

 # Authenticate gcloud CLI
 gcloud auth login

 # Create a new project for now
 gcloud projects create LLaMa3-tunerX --set-as-default

 # Config SSH and add
 gcloud compute config-ssh --quiet

 # Set up default credentials
 gcloud auth application-default login

 # Enable Cloud TPU API access
 gcloud services enable compute.googleapis.com tpu.googleapis.com storage-component.googleapis.com aiplatform.googleapis.com"><pre> <span><span>#</span> Download gcloud CLI</span>
 curl https://sdk.cloud.google.com <span>|</span> bash
 <span>source</span> <span>~</span>/.bashrc

 <span><span>#</span> Authenticate gcloud CLI</span>
 gcloud auth login

 <span><span>#</span> Create a new project for now</span>
 gcloud projects create LLaMa3-tunerX --set-as-default

 <span><span>#</span> Config SSH and add</span>
 gcloud compute config-ssh --quiet

 <span><span>#</span> Set up default credentials</span>
 gcloud auth application-default login

 <span><span>#</span> Enable Cloud TPU API access</span>
 gcloud services <span>enable</span> compute.googleapis.com tpu.googleapis.com storage-component.googleapis.com aiplatform.googleapis.com</pre></div>
</li>
<li>
<p dir="auto">Spin up a TPU v5-8 VM ðŸ¤ .</p>

<p dir="auto">Keep an eye on the terminal -- you might be asked to input SSH key password and need to put in your HuggingFace token.</p>
</li>
<li>
<p dir="auto">Clone the repo and install dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/felafax/felafax.git
cd felafax
pip install -r requirements.txt"><pre>git clone https://github.com/felafax/felafax.git
<span>cd</span> felafax
pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto">Open the Jupyter notebook at <code>https://localhost:888</code> and start fine-tuning!</p>
</li>
</ol>

<ul dir="auto">
<li>Google Deepmind&#39;s <a href="https://github.com/google-deepmind/gemma">Gemma repo</a>.</li>
<li><a href="https://github.com/young-geng/EasyLM">EasyLM</a> for great work on llama models in JAX</li>
<li>PyTorch XLA FSDP and SPMD testing done by <a href="https://github.com/HeegyuKim/torch-xla-SPMD">HeegyuKim</a>.</li>
<li>Examples from <a href="https://github.com/pytorch/xla/">PyTorch-XLA</a> repo.</li>
</ul>

<p dir="auto">If you have any questions, please contact us at <a href="mailto:founders@felafax.ai">founders@felafax.ai</a>.</p>
</article></div></div>
  </body>
</html>
