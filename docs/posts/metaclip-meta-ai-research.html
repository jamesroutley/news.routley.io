<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/MetaCLIP">Original</a>
    <h1>MetaCLIP â€“ Meta AI Research</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repository contains the code for the MetaCLIP, described in the paper <a href="https://arxiv.org/abs/2309.16671" rel="nofollow">Demystifying CLIP Data</a> that formalizes CLIP data curation as a simple algorithm. The main contributions are:</p>
<ul dir="auto">
<li>Curating data from scratch without filtering via prior models (e.g., different from existing open source <a href="https://arxiv.org/abs/2111.02114" rel="nofollow">efforts</a> that uses the original CLIP model as a teacher for filtering <strong>student</strong> data.</li>
<li>Making training data more transparent, we released our <strong>training data distribution</strong> over <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metadata.json">metadata</a>;</li>
<li>A scalable algorithm running in the data pipeline, allowing to scale the <strong>data pool</strong> to the whole CommonCrawl (CC) w/ 300+B image-text pairs. We observe that data quality is <strong>much more</strong> important than quantity (different from existing <a href="https://arxiv.org/abs/2210.08402" rel="nofollow">open source efforts</a> or <a href="https://arxiv.org/abs/2102.05918" rel="nofollow">ALIGN</a> that mostly scale quantity);</li>
<li><a href="https://github.com/facebookresearch/MetaCLIP/blob/main/run_configs_400m.py">standard CLIP training setup</a> for controlled experiments and fair comparisons under fixed training and model configuration.</li>
</ul>
<p dir="auto">We conclude that:</p>
<ul dir="auto">
<li>Effective pretraining data should <strong>maximally preserve signal and mitigate noise</strong>, instead of hard removal of noise with blackbox filters that lead to unknown distribution</li>
<li>Our algorithm is simpler and scalable to curate the whole Internet</li>
<li>Open-sourcing does not just entail a trained model checkpoint but more importantly the <strong>pre-training data distribution</strong>.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{xu2023metaclip,
   title={Demystifying CLIP Data},
   author={Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu, Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer},
   journal={arXiv preprint arXiv:2309.16671},
   year={2023}
}"><pre><span>@inproceedings</span>{<span>xu2023metaclip</span>,
   <span>title</span>=<span><span>{</span>Demystifying CLIP Data<span>}</span></span>,
   <span>author</span>=<span><span>{</span>Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu, Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer<span>}</span></span>,
   <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2309.16671<span>}</span></span>,
   <span>year</span>=<span><span>{</span>2023<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" id="user-content-updates" dir="auto"><a href="#updates">Updates<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>09/28/2023: initial release.</li>
</ul>
<h2 tabindex="-1" id="user-content-quick-links" dir="auto"><a href="#quick-links">Quick Links<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#metadata">Metadata</a></li>
<li><a href="#pre-trained-models">Pre-trained Models</a></li>
<li><a href="#curation">Curation</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#bugs-or-questions">Bugs or Questions?</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">This code is developed with minimal changes on top of <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a>. The following command should install requirements for OpenCLIP and <code>submitit=1.2.1</code> used by this repo:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n python=3.10 pytorch torchvision pytorch-cuda=11.7 tqdm ftfy braceexpand regex pandas submitit=1.2.1 \
    -c pytorch-nightly \
    -c nvidia \
    -c conda-forge \
    -c anaconda"><pre>conda create -n python=3.10 pytorch torchvision pytorch-cuda=11.7 tqdm ftfy braceexpand regex pandas submitit=1.2.1 \
    -c pytorch-nightly \
    -c nvidia \
    -c conda-forge \
    -c anaconda</pre></div>
<h2 tabindex="-1" id="user-content-metadata" dir="auto"><a href="#metadata">Metadata<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">MetaCLIP uses 500,000 queries as <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metadata.json">metadata</a> to align the training data to distribution over quality writing of Wikipedia/WordNet terms. This metadata also allows us to release training data distribution of a released model as <strong>data card</strong>.</p>
<h3 tabindex="-1" id="user-content-pre-trained-models" dir="auto"><a href="#pre-trained-models">Pre-trained Models<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">We change OpenCLIP to match training in the default CLIP model setup (w/ <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/src/open_clip/model_configs/ViT-B-16-quickgelu.json">ViT-B-16-quickgelu</a>, <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/src/open_clip/model_configs/ViT-L-14-quickgelu.json">ViT-L-14-quickgelu</a> and <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/src/open_clip/model_configs/ViT-H-14-quickgelu.json">ViT-H-14-quickgelu</a>). Most OpenCLIP models use <code>nn.GELU</code> not <code>quickgelu</code> used by vanilla CLIP. We hope this helps research w/ controlled experiments in the &#34;CLIP era of ImageNet&#34;.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from PIL import Image
import open_clip

model, _, preprocess = open_clip.create_model_and_transforms(&#39;ViT-B-32-quickgelu&#39;, pretrained=&#39;metaclip/b32_400m.pt&#39;)

image = preprocess(Image.open(&#34;CLIP.png&#34;)).unsqueeze(0)
text = open_clip.tokenize([&#34;a diagram&#34;, &#34;a dog&#34;, &#34;a cat&#34;])

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)

print(&#34;Label probs:&#34;, text_probs)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>import</span> <span>open_clip</span>

<span>model</span>, <span>_</span>, <span>preprocess</span> <span>=</span> <span>open_clip</span>.<span>create_model_and_transforms</span>(<span>&#39;ViT-B-32-quickgelu&#39;</span>, <span>pretrained</span><span>=</span><span>&#39;metaclip/b32_400m.pt&#39;</span>)

<span>image</span> <span>=</span> <span>preprocess</span>(<span>Image</span>.<span>open</span>(<span>&#34;CLIP.png&#34;</span>)).<span>unsqueeze</span>(<span>0</span>)
<span>text</span> <span>=</span> <span>open_clip</span>.<span>tokenize</span>([<span>&#34;a diagram&#34;</span>, <span>&#34;a dog&#34;</span>, <span>&#34;a cat&#34;</span>])

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>image_features</span> <span>=</span> <span>model</span>.<span>encode_image</span>(<span>image</span>)
    <span>text_features</span> <span>=</span> <span>model</span>.<span>encode_text</span>(<span>text</span>)
    <span>image_features</span> <span>/=</span> <span>image_features</span>.<span>norm</span>(<span>dim</span><span>=</span><span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>)
    <span>text_features</span> <span>/=</span> <span>text_features</span>.<span>norm</span>(<span>dim</span><span>=</span><span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>)

    <span>text_probs</span> <span>=</span> (<span>100.0</span> <span>*</span> <span>image_features</span> @ <span>text_features</span>.<span>T</span>).<span>softmax</span>(<span>dim</span><span>=</span><span>-</span><span>1</span>)

<span>print</span>(<span>&#34;Label probs:&#34;</span>, <span>text_probs</span>)</pre></div>
<table>
<thead>
<tr>
<th>Model</th>
<th>Data Card</th>
<th>IN ZS Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/b32_400m.pt" rel="nofollow">MetaCLIP B32 400M</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_400m.json" rel="nofollow">data card</a></td>
<td>65.5</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/b16_400m.pt" rel="nofollow">MetaCLIP B16 400M</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_400m.json" rel="nofollow">data card</a></td>
<td>70.8</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_400m.pt" rel="nofollow">MetaCLIP L14 400M</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_400m.json" rel="nofollow">data card</a></td>
<td>76.2</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/b32_fullcc2.5b.pt" rel="nofollow">MetaCLIP B32 FullCC2.5B</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_fullcc2.5b.json" rel="nofollow">data card</a></td>
<td>67.6</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/b16_fullcc2.5b.pt" rel="nofollow">MetaCLIP B16 FullCC2.5B</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_fullcc2.5b.json" rel="nofollow">data card</a></td>
<td>72.1</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_fullcc2.5b.pt" rel="nofollow">MetaCLIP L14 FullCC2.5B</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_fullcc2.5b.json" rel="nofollow">data card</a></td>
<td>79.2</td>
</tr>
<tr>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/h14_fullcc2.5b.pt" rel="nofollow">MetaCLIP H14 FullCC2.5B</a></td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_fullcc2.5b.json" rel="nofollow">data card</a></td>
<td>80.5</td>
</tr>
<tr>
<td>MetaCLIP G14 FullCC2.5B</td>
<td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/datacard_fullcc2.5b.json" rel="nofollow">data card</a></td>
<td>ongoing</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-how-to-curate-" dir="auto"><a href="#how-to-curate-">How to Curate ?<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">We have a <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/demo.ipynb">demo notebook</a> to show how the proposed algorithm works.</p>
<h3 tabindex="-1" id="user-content-i-already-have-a-head-distributed-dataset" dir="auto"><a href="#i-already-have-a-head-distributed-dataset">I already have a (head distributed) dataset:<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">CLIP curation can still help as online balancing (Table 6 in the paper). We wrap CLIP curation in two key functions: <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metaclip/substr_matching.py">substring matching</a> (recommended to run offline) and <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metaclip/balancing.py">balancing</a> (either offline or online, please check <code>metaclip.balancing:main</code>).</p>
<div dir="auto" data-snippet-clipboard-copy-content="import json
import numpy as np
from metaclip.substr_matching import substr_matching
from metaclip.balancing import balance_sampling

with open(&#34;metadata.json&#34;) as f:
  metadata = json.load(f)
# entry counts for our 1.6B(pool) -&gt; 400M(curated); please check balance_sampling:main and substr match and count on your own data.
with open(&#34;metaclip/entry_counts_400m.json&#34;) as f:
  entry_count_json = json.load(f)
entry_count = np.array([entry_count_json[entry] for entry in metadata], dtype=np.uint64)  # uint64 to be safe for scaling.

t = 20000
entry_count[entry_count &lt; t] = t
entry_prob = t / entry_count

for text in [&#34;jacksons chameleon&#34;, &#34;battery plate&#34;]:
  matched_entry_ids = substr_matching(text, metadata)
  if balance_sampling(matched_entry_ids, entry_prob):
    print(f&#34;&#39;{text}&#39; curated&#34;)"><pre><span>import</span> <span>json</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>metaclip</span>.<span>substr_matching</span> <span>import</span> <span>substr_matching</span>
<span>from</span> <span>metaclip</span>.<span>balancing</span> <span>import</span> <span>balance_sampling</span>

<span>with</span> <span>open</span>(<span>&#34;metadata.json&#34;</span>) <span>as</span> <span>f</span>:
  <span>metadata</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span># entry counts for our 1.6B(pool) -&gt; 400M(curated); please check balance_sampling:main and substr match and count on your own data.</span>
<span>with</span> <span>open</span>(<span>&#34;metaclip/entry_counts_400m.json&#34;</span>) <span>as</span> <span>f</span>:
  <span>entry_count_json</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>entry_count</span> <span>=</span> <span>np</span>.<span>array</span>([<span>entry_count_json</span>[<span>entry</span>] <span>for</span> <span>entry</span> <span>in</span> <span>metadata</span>], <span>dtype</span><span>=</span><span>np</span>.<span>uint64</span>)  <span># uint64 to be safe for scaling.</span>

<span>t</span> <span>=</span> <span>20000</span>
<span>entry_count</span>[<span>entry_count</span> <span>&lt;</span> <span>t</span>] <span>=</span> <span>t</span>
<span>entry_prob</span> <span>=</span> <span>t</span> <span>/</span> <span>entry_count</span>

<span>for</span> <span>text</span> <span>in</span> [<span>&#34;jacksons chameleon&#34;</span>, <span>&#34;battery plate&#34;</span>]:
  <span>matched_entry_ids</span> <span>=</span> <span>substr_matching</span>(<span>text</span>, <span>metadata</span>)
  <span>if</span> <span>balance_sampling</span>(<span>matched_entry_ids</span>, <span>entry_prob</span>):
    <span>print</span>(<span>f&#34;&#39;<span><span>{</span><span>text</span><span>}</span></span>&#39; curated&#34;</span>)</pre></div>
<h3 tabindex="-1" id="user-content-i-want-to-curate-data-from-scratch" dir="auto"><a href="#i-want-to-curate-data-from-scratch">I want to curate data from scratch:<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">We release a skeleton code for <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metaclip/cc_matching.py">sub-string matching</a> from CommonCrawl WAT or WARC and <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metaclip/balancing.py">balancing</a>. Check <a href="https://github.com/facebookresearch/MetaCLIP/blob/main/metaclip/README.md">here</a> for details.</p>
<h2 tabindex="-1" id="user-content-training" dir="auto"><a href="#training">Training<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<div dir="auto" data-snippet-clipboard-copy-content="python submitit_openclip.py b32_400m"><pre><span>python</span> <span>submitit_openclip</span>.<span>py</span> <span>b32_400m</span></pre></div>
<p dir="auto">Please config the corresponding <code>training_data</code> in <code>run_configs_400m.py</code>.</p>
<h2 tabindex="-1" id="user-content-bugs-or-questions" dir="auto"><a href="#bugs-or-questions">Bugs or questions?<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">If you have any questions related to the code or the paper, feel free to email Hu Xu (<code>huxu@meta.com</code>).</p>
<h2 tabindex="-1" id="user-content-citation" dir="auto"><a href="#citation">Citation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">Please cite our paper if MetaCLIP helps your work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{xu2023metaclip,
   title={Demystifying CLIP Data},
   author={Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu, Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer},
   journal={arXiv preprint arXiv:2309.16671},
   year={2023}
}"><pre><span>@inproceedings</span>{<span>xu2023metaclip</span>,
   <span>title</span>=<span><span>{</span>Demystifying CLIP Data<span>}</span></span>,
   <span>author</span>=<span><span>{</span>Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu, Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer<span>}</span></span>,
   <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2309.16671<span>}</span></span>,
   <span>year</span>=<span><span>{</span>2023<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" id="user-content-reference" dir="auto"><a href="#reference">Reference<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The training code is developed based on <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a>, modified to the vanilla CLIP training setup.</p>
<h2 tabindex="-1" id="user-content-todo" dir="auto"><a href="#todo">TODO<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>cross-json URL dedup in skeleton code;</li>
<li>numpy implementation for matching and balancing;</li>
<li>support online downloading;</li>
<li>support vanilla CLIP API;</li>
<li>Huggingface integration;</li>
<li>(welcome your use cases or suggestions to update this codebase regularly)</li>
</ul>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The majority of MetaCLIP is licensed under CC-BY-NC, however portions of the project are available under separate license terms: open_clip is licensed under the <a href="https://github.com/mlfoundations/open_clip">https://github.com/mlfoundations/open_clip</a> license.</p>
</article>
          </div></div>
  </body>
</html>
