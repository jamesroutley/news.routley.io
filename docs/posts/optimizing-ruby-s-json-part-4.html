<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://byroot.github.io/ruby/json/2024/12/29/optimizing-ruby-json-part-4.html">Original</a>
    <h1>Optimizing Ruby&#39;s JSON, Part 4</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><a href="https://byroot.github.io/ruby/json/2024/12/27/optimizing-ruby-json-part-3.html">In the previous post</a>, we established that as long as <code>ruby/json</code> wasn’t competitive on
micro-benchmarks, public perception wouldn’t change. Since what made <code>ruby/json</code> appear so bad on micro-benchmarks was its setup cost, we had to
find ways to reduce it further.</p>

<h2 id="spot-the-seven-differences">Spot the Seven Differences</h2>

<p>So I decided to file <a href="https://github.com/ruby/json/issues/655">this performance discrepancy as a bug</a>, and investigate it as such and started
profiling Stephen’s micro-benchmark with both <code>ruby/json</code> and <code>oj</code>:</p>

<figure><pre><code data-lang="ruby"><span>benchmark_encoding</span> <span>&#34;small mixed&#34;</span><span>,</span> <span>[</span><span>1</span><span>,</span> <span>&#34;string&#34;</span><span>,</span> <span>{</span> <span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span> <span>},</span> <span>[</span><span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]]</span></code></pre></figure>

<p>As mentioned in previous parts, I expected the extra allocation would be the main issue, and that re-using the <code>JSON::State</code> object would
put us on par with <code>Oj</code>, but it’s always good to revalidate our assumptions:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
        json (reuse)   467.051k i/100ms
                json   252.570k i/100ms
                  oj   529.741k i/100ms
Calculating -------------------------------------
        json (reuse)      4.857M (± 1.9%) i/s  (205.88 ns/i) -     24.287M in   5.001995s
                json      2.689M (± 0.5%) i/s  (371.86 ns/i) -     13.639M in   5.071865s
                  oj      5.860M (± 0.6%) i/s  (170.65 ns/i) -     29.665M in   5.062753s

Comparison:
        json (reuse):  4857171.1 i/s
                  oj:  5859811.8 i/s - 1.21x  faster
                json:  2689181.9 i/s - 1.81x  slower
</code></pre></div></div>

<p>Even without that extra allocation, we were still 20% slower, that was unexpected, and should be fixed before exploring ways to eliminate the <code>State</code> allocation.</p>

<p>As always, this meant profiling, but this time I profiled both <code>ruby/json</code> and <code>Oj</code> to see where the difference might be:</p>

<figure><pre><code data-lang="ruby"><span>require</span> <span>&#34;json&#34;</span>

<span>i</span> <span>=</span> <span>20_000_000</span>
<span>data</span> <span>=</span> <span>[</span><span>1</span><span>,</span> <span>&#34;string&#34;</span><span>,</span> <span>{</span> <span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span> <span>},</span> <span>[</span><span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]]</span>
<span>state</span> <span>=</span> <span>JSON</span><span>::</span><span>State</span><span>.</span><span>new</span>
<span>while</span> <span>i</span> <span>&gt;</span> <span>0</span>
  <span>i</span> <span>-=</span> <span>1</span>
  <span>state</span><span>.</span><span>generate</span><span>(</span><span>data</span><span>)</span>
<span>end</span></code></pre></figure>

<p><a href="https://share.firefox.dev/3W29huf">Full profile</a></p>

<figure><pre><code data-lang="ruby"><span>require</span> <span>&#34;oj&#34;</span>

<span>Oj</span><span>.</span><span>default_options</span> <span>=</span> <span>Oj</span><span>.</span><span>default_options</span><span>.</span><span>merge</span><span>(</span><span>mode: :compat</span><span>)</span>

<span>i</span> <span>=</span> <span>20_000_000</span>
<span>data</span> <span>=</span> <span>[</span><span>1</span><span>,</span> <span>&#34;string&#34;</span><span>,</span> <span>{</span> <span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span> <span>},</span> <span>[</span><span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]]</span>
<span>while</span> <span>i</span> <span>&gt;</span> <span>0</span>
  <span>i</span> <span>-=</span> <span>1</span>
  <span>Oj</span><span>.</span><span>dump</span><span>(</span><span>data</span><span>)</span>
<span>end</span></code></pre></figure>

<p><a href="https://share.firefox.dev/40d4VTH">Full profile</a></p>

<p>Once I got the two profiles, it was a matter of playing “Spot the seven differences”.</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/oj-flamegraph.png" alt=""/></p>

<p><img src="https://byroot.github.io/assets/articles/json-4/json-flamegraph.png" alt=""/></p>

<p>Something that jumped to me quite quickly, is that on that micro-benchmark, even though we’re re-using our <code>JSON::State</code> object,
we still spend a significant amount of time allocating and freeing our internal buffer. Still, on the <code>Oj</code> profile, there wasn’t
any <code>malloc</code> or <code>free</code> call. This suggested that <code>Oj</code> re-used a persistent buffer across calls or allocated it on the stack.</p>

<p>A quick investigation of <code>Oj</code>’s source code confirmed it was the latter:</p>

<figure><pre><code data-lang="c"><span>typedef</span> <span>struct</span> <span>_out</span> <span>{</span>
    <span>char</span>      <span>stack_buffer</span><span>[</span><span>4096</span><span>];</span>
    <span>char</span>     <span>*</span><span>buf</span><span>;</span>
    <span>char</span>     <span>*</span><span>end</span><span>;</span>
    <span>char</span>     <span>*</span><span>cur</span><span>;</span>
    <span>// ...</span>
<span>}</span> <span>*</span><span>Out</span><span>;</span>

<span>// ...</span>

<span>/* Document-method: dump
 * call-seq: dump(obj, options={})
 *
 * Dumps an Object (obj) to a string.
 * - *obj* [_Object_] Object to serialize as an JSON document String
 * - *options* [_Hash_] same as default_options
 */</span>
<span>static</span> <span>VALUE</span> <span>dump</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>VALUE</span> <span>*</span><span>argv</span><span>,</span> <span>VALUE</span> <span>self</span><span>)</span> <span>{</span>
    <span>struct</span> <span>dump_arg</span> <span>arg</span><span>;</span>
    <span>struct</span> <span>_out</span>     <span>out</span><span>;</span> <span>// Stack allocation</span>
    <span>struct</span> <span>_options</span> <span>copts</span> <span>=</span> <span>oj_default_options</span><span>;</span>
    <span>// ...</span>
<span>}</span></code></pre></figure>

<h2 id="stack-and-heap">Stack and Heap</h2>

<p>Since this post is intended for people not necessarily familiar with C, I need to explain a bit what the stack and the heap are.
This is just meant as a quick introduction.</p>

<p>The heap is most of the RAM available on your system, if you need memory to store some data, you call <code>malloc(number_of_bytes)</code>
and get a pointer back that is at least as big as the number of bytes you asked for, and once you are done with it you call <code>free(pointer)</code>.</p>

<p>There are many different allocators (e.g. <code>jemalloc</code>, <code>tcmalloc</code>), using various algorithms and techniques to keep track of which memory is used and
how large each allocated chunk is, but even with the best allocators, <code>malloc</code> and <code>free</code> are somewhat costly. In addition, if you don’t know upfront
how much memory you actually need, you might need to call <code>new_pointer = realloc(pointer, new_size)</code> to allocate a larger chunk and copy the content
over and free the old chunk, this is fairly expensive.</p>

<p>And for Ruby C extensions specifically, you generally don’t use <code>malloc / free / realloc</code>, but <code>ruby_xmalloc / ruby_xfree / ruby_xrealloc</code>,
which are wrappers around the standard functions which additionally update Ruby GC statistics, so that the GC can trigger after some
threshold is reached which can further increase the cost of heap allocations.</p>

<p>On the other hand, the stack is a memory region that’s preallocated for each native thread, and that is used to store the current state of a function,
such as local variables while calling another function. For instance, if you have a function <code>f</code> with two <code>int64_t</code> local variables <code>a</code> and <code>b</code>,
<code>a</code> will be stored at <code>stack_pointer + 0</code> and <code>b</code> at <code>stack_pointer + 8</code>. And if <code>f</code> calls into another function <code>f2</code>, the stack pointer will be
incremented by <code>16</code> before entering <code>f2</code>, and restored back to its previous value when returning from <code>f2</code>.</p>

<p>This makes stack allocations essentially free, at least compared to heap allocations, and it’s almost guaranteed data stored there will be in the CPU
cache as it’s a very “hot” memory region.</p>

<p>But stack allocation isn’t a silver bullet, first because whenever you return from the function that memory should be considered freed, so in many
cases that’s not suitable. You can also not resize (<code>realloc</code>) it from a callee function.
Additionally, the stack is limited in size.
On most modern UNIX-like systems you got a fairly generous <code>8MiB</code> of stack space for the main thread, but only <code>1MiB</code> on Windows.
And most systems give less stack space to additional threads, for instance, Alpine Linux which uses the <code>musl libc</code> only gives
<code>128kiB</code> of stack space to additional threads, which really isn’t a lot. That’s why it’s not rare for Ruby C extension maintainers
to get Alpine-specific bug reports.</p>

<h2 id="stack-allocated-buffer-struct">Stack Allocated Buffer Struct</h2>

<p>So stack allocations should be used carefully and reasonably, the conventional wisdom being to not allocate more than <code>1kiB</code> on the stack, and to only
do it in leaf functions (that don’t call any other functions), or functions that only call into a few known functions.</p>

<p>In our case, the <code>JSON::State#generate</code> method isn’t a leaf function, and might call into arbitrary Ruby code if it needs to call <code>#to_json</code> or
<code>#to_s</code> on an object, so <code>4kiB</code> seemed a bit excessive to me, but still, we could use stack allocations reasonably to gain some performance.</p>

<p><code>ruby/json</code> wasn’t just doing one <code>malloc+free</code> call, but two. The first one in <code>cState_prepare_buffer</code> allocates the <code>FBuffer</code> struct, which
contains the buffer metadata, such as its capacity:</p>

<figure><pre><code data-lang="c"><span>typedef</span> <span>struct</span> <span>FBufferStruct</span> <span>{</span>
    <span>unsigned</span> <span>long</span> <span>initial_length</span><span>;</span>
    <span>char</span> <span>*</span><span>ptr</span><span>;</span>
    <span>unsigned</span> <span>long</span> <span>len</span><span>;</span>
    <span>unsigned</span> <span>long</span> <span>capa</span><span>;</span>
<span>}</span> <span>FBuffer</span><span>;</span></code></pre></figure>

<p>That struct being just <code>32B</code> large, it makes a lot of sense to allocate it on the stack, which would save a pair of <code>malloc+free</code> calls,
and only increase the stack size by a negligible amount.</p>

<p><a href="https://github.com/ruby/json/pull/657/">You can see the diff</a>, it’s not complicated but requires a lot of small changes across the
codebase. Additionally, since the parser also used <code>FBuffer</code>, it had to be modified too to embed the <code>FBuffer</code> struct inside the <code>JSON_ParserStruct</code>
instead of just keeping a pointer.</p>

<p>The gains were pretty good for such a small change:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   265.435k i/100ms
Calculating -------------------------------------
               after      2.831M (± 1.2%) i/s  (353.28 ns/i) -     14.333M in   5.064502s

Comparison:
              before:  2630445.8 i/s
               after:  2830628.7 i/s - 1.08x  faster
</code></pre></div></div>

<p>But still not enough.</p>

<h2 id="efficient-integer-printing">Efficient Integer Printing</h2>

<p>Before continuing on reducing the setup cost, another thing that surprised me on that profile was the <code>3.6%</code> spent in <code>fltoa</code>.
Not that <code>3.6%</code> is anywhere near a hotspot, but that’s a bit much for such a simple function.
If you are not familiar with C naming conventions, you may wonder what this function is doing. In the C standard library you have
several functions to parse strings as various integer types, such as <code>atoi</code>, <code>atol</code>, and <code>atoll</code>, for <code>int</code>, <code>long</code> and <code>long long</code>
respectively. Why <code>ato</code>? Because C doesn’t really have strings, but arrays of bytes, hence “array to int” -&gt; <code>atoi</code>. That’s also probably
where the Ruby <code>#to_i</code> method got its name from.</p>

<p>So here, <code>fltoa</code> is a <code>long to string</code> conversion function, and <code>f</code> is just the namespace for the <code>fbuffer.h</code> file.</p>

<p>Let’s have a look at how it is done:</p>

<figure><pre><code data-lang="c"><span>static</span> <span>void</span> <span>freverse</span><span>(</span><span>char</span> <span>*</span><span>start</span><span>,</span> <span>char</span> <span>*</span><span>end</span><span>)</span>
<span>{</span>
    <span>char</span> <span>c</span><span>;</span>

    <span>while</span> <span>(</span><span>end</span> <span>&gt;</span> <span>start</span><span>)</span> <span>{</span>
        <span>c</span> <span>=</span> <span>*</span><span>end</span><span>,</span> <span>*</span><span>end</span><span>--</span> <span>=</span> <span>*</span><span>start</span><span>,</span> <span>*</span><span>start</span><span>++</span> <span>=</span> <span>c</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>static</span> <span>long</span> <span>fltoa</span><span>(</span><span>long</span> <span>number</span><span>,</span> <span>char</span> <span>*</span><span>buf</span><span>)</span>
<span>{</span>
    <span>static</span> <span>char</span> <span>digits</span><span>[]</span> <span>=</span> <span>&#34;0123456789&#34;</span><span>;</span>
    <span>long</span> <span>sign</span> <span>=</span> <span>number</span><span>;</span>
    <span>char</span><span>*</span> <span>tmp</span> <span>=</span> <span>buf</span><span>;</span>

    <span>if</span> <span>(</span><span>sign</span> <span>&lt;</span> <span>0</span><span>)</span> <span>number</span> <span>=</span> <span>-</span><span>number</span><span>;</span>
    <span>do</span> <span>*</span><span>tmp</span><span>++</span> <span>=</span> <span>digits</span><span>[</span><span>number</span> <span>%</span> <span>10</span><span>];</span> <span>while</span> <span>(</span><span>number</span> <span>/=</span> <span>10</span><span>);</span>
    <span>if</span> <span>(</span><span>sign</span> <span>&lt;</span> <span>0</span><span>)</span> <span>*</span><span>tmp</span><span>++</span> <span>=</span> <span>&#39;-&#39;</span><span>;</span>
    <span>freverse</span><span>(</span><span>buf</span><span>,</span> <span>tmp</span> <span>-</span> <span>1</span><span>);</span>
    <span>return</span> <span>tmp</span> <span>-</span> <span>buf</span><span>;</span>
<span>}</span>

<span>static</span> <span>void</span> <span>fbuffer_append_long</span><span>(</span><span>FBuffer</span> <span>*</span><span>fb</span><span>,</span> <span>long</span> <span>number</span><span>)</span>
<span>{</span>
    <span>char</span> <span>buf</span><span>[</span><span>20</span><span>];</span>
    <span>unsigned</span> <span>long</span> <span>len</span> <span>=</span> <span>fltoa</span><span>(</span><span>number</span><span>,</span> <span>buf</span><span>);</span>
    <span>fbuffer_append</span><span>(</span><span>fb</span><span>,</span> <span>buf</span><span>,</span> <span>len</span><span>);</span>
<span>}</span></code></pre></figure>

<p>There’s something quite odd here. First, we allocate a <code>20B</code> buffer on the stack, write the number in reverse in the buffer, reverse the string
and finally copy the stack buffer onto the output buffer.</p>

<p>In Ruby, it would look like:</p>

<figure><pre><code data-lang="c"><span>DIGITS</span> <span>=</span> <span>(</span><span>&#39;0&#39;</span><span>..</span><span>&#39;9&#39;</span><span>).</span><span>to_a</span>

<span>def</span> <span>fltoa</span><span>(</span><span>number</span><span>)</span>
  <span>negative</span> <span>=</span> <span>number</span><span>.</span><span>negative</span><span>?</span>
  <span>number</span> <span>=</span> <span>number</span><span>.</span><span>abs</span>

  <span>buffer</span> <span>=</span> <span>&#34;&#34;</span><span>.</span><span>b</span>

  <span>loop</span> <span>do</span>
    <span>buffer</span> <span>&lt;&lt;</span> <span>DIGITS</span><span>[</span><span>number</span> <span>%</span> <span>10</span><span>]</span>
    <span>number</span> <span>/=</span> <span>10</span>
    <span>break</span> <span>if</span> <span>number</span><span>.</span><span>zero</span><span>?</span>
  <span>end</span>

  <span>buffer</span> <span>&lt;&lt;</span> <span>&#34;-&#34;</span> <span>if</span> <span>negative</span>

  <span>buffer</span><span>.</span><span>reverse</span><span>!</span>
  <span>buffer</span>
<span>end</span></code></pre></figure>

<p>Writing the number in reverse can be a useful trick if you are appending it to an existing buffer of dynamic length because you don’t know upfront
how long the number will be nor where the buffer ends.</p>

<p>But here we’re writing inside a stack buffer of known size and then copying the result, so it’s a bit wasteful.</p>

<p>Instead <a href="https://github.com/ruby/json/pull/656">we can write in the stack buffer backward, starting from the end of the buffer</a>,
and save on having to reverse the digits at the end.</p>

<figure><pre><code data-lang="c"><span>static</span> <span>long</span> <span>fltoa</span><span>(</span><span>long</span> <span>number</span><span>,</span> <span>char</span> <span>*</span><span>buf</span><span>)</span>
<span>{</span>
    <span>static</span> <span>const</span> <span>char</span> <span>digits</span><span>[]</span> <span>=</span> <span>&#34;0123456789&#34;</span><span>;</span>
    <span>long</span> <span>sign</span> <span>=</span> <span>number</span><span>;</span>
    <span>char</span><span>*</span> <span>tmp</span> <span>=</span> <span>buf</span><span>;</span>

    <span>if</span> <span>(</span><span>sign</span> <span>&lt;</span> <span>0</span><span>)</span> <span>number</span> <span>=</span> <span>-</span><span>number</span><span>;</span>
    <span>do</span> <span>*</span><span>tmp</span><span>--</span> <span>=</span> <span>digits</span><span>[</span><span>number</span> <span>%</span> <span>10</span><span>];</span> <span>while</span> <span>(</span><span>number</span> <span>/=</span> <span>10</span><span>);</span>
    <span>if</span> <span>(</span><span>sign</span> <span>&lt;</span> <span>0</span><span>)</span> <span>*</span><span>tmp</span><span>--</span> <span>=</span> <span>&#39;-&#39;</span><span>;</span>
    <span>return</span> <span>buf</span> <span>-</span> <span>tmp</span><span>;</span>
<span>}</span>

<span>#define LONG_BUFFER_SIZE 20
</span><span>static</span> <span>void</span> <span>fbuffer_append_long</span><span>(</span><span>FBuffer</span> <span>*</span><span>fb</span><span>,</span> <span>long</span> <span>number</span><span>)</span>
<span>{</span>
    <span>char</span> <span>buf</span><span>[</span><span>LONG_BUFFER_SIZE</span><span>];</span>
    <span>char</span> <span>*</span><span>buffer_end</span> <span>=</span> <span>buf</span> <span>+</span> <span>LONG_BUFFER_SIZE</span><span>;</span>
    <span>long</span> <span>len</span> <span>=</span> <span>fltoa</span><span>(</span><span>number</span><span>,</span> <span>buffer_end</span> <span>-</span> <span>1</span><span>);</span>
    <span>fbuffer_append</span><span>(</span><span>fb</span><span>,</span> <span>buffer_end</span> <span>-</span> <span>len</span><span>,</span> <span>len</span><span>);</span>
<span>}</span></code></pre></figure>

<p>Here again, it’s a small optimization on a very specific part of the generator, so I crafted a micro-benchmark to see if it had the expected benefits:</p>

<figure><pre><code data-lang="ruby"><span>benchmark_encoding</span> <span>&#34;integers&#34;</span><span>,</span> <span>(</span><span>1_000_000</span><span>..</span><span>1_001_000</span><span>).</span><span>to_a</span></code></pre></figure>

<div><div><pre><code>== Encoding integers (8009 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after     9.770k i/100ms
Calculating -------------------------------------
               after     97.929k (± 0.9%) i/s   (10.21 μs/i) -    498.270k in   5.088542s

Comparison:
              before:    88309.9 i/s
               after:    97928.6 i/s - 1.11x  faster
</code></pre></div></div>

<p>Not bad, probably will only be noticeable for documents containing lots of large integers, but also a very simple optimization.</p>

<p>This can probably be optimized further by writing directly in the output buffer so that we don’t need to copy, and maybe even use <code>log</code> to
compute upfront how many digits the number has, but that was good enough for now, so I went back to reduce setup cost.</p>

<h2 id="using-an-rstring-as-buffer">Using an RString as Buffer</h2>

<p>So I went to profile Stephen’s micro-benchmark again:</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/json-flamegraph-2.png" alt=""/></p>

<p><a href="https://share.firefox.dev/4a4oocr">Full profile</a></p>

<p>As you can see, we’re now calling <code>malloc</code> and <code>free</code> half as much, but still 100% more than <code>Oj</code>, and once we’re done filling our buffer,
we copy all its content in another memory region managed by Ruby when calling <code>str_enc_new</code> (actually <code>rb_utf8_str_new</code>, but the profiler doesn’t see it because of inlining).</p>

<p>On micro-benchmarks the copy is negligible, but on larger ones like <code>twitter.json</code>, it can amount to as much as 4% of the overall runtime:</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/strnew-flamegraph.png" alt=""/></p>

<p>The cost of allocating the String object is close to invisible compared to the copy.</p>

<p>So at that point, you are probably wondering why not simply directly use the Ruby String as our buffer.
We would let Ruby manage the memory right from the start, save the copy, and for micro-benchmarks, we’d probably
fit inside an embedded String (more on that later). We also wouldn’t have to be extra careful to free our internal buffer
in case an exception is raised, so it would eliminate many potential sources of memory leaks.</p>

<p>That’s not exactly a novel idea, there are a bunch of methods inside Ruby itself that do that exact thing, like <code>Time#strftime</code>,
and <a href="https://github.com/ruby/json/compare/master...etiennebarrie:json:use-ruby-strings">I had prototyped it a couple of month prior during a pairing session</a>
with <a href="https://github.com/etiennebarrie">Étienne Barrié</a>.</p>

<p>So I went on <a href="https://github.com/byroot/json/commit/8e61886e009f4df1e447f1808293f8e62a09c90a">to reimplement that again</a>, given so much had
changed since then that rebasing would have been harder.</p>

<p>Unfortunately, it wasn’t the win you could expect, quite the opposite:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   208.242k i/100ms
Calculating -------------------------------------
               after      2.201M (± 1.0%) i/s  (454.41 ns/i) -     11.037M in   5.015727s

Comparison:
              before:  2648506.5 i/s
               after:  2200665.8 i/s - 1.20x  slower


== Encoding twitter.json (466906 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   205.000 i/100ms
Calculating -------------------------------------
               after      2.065k (± 1.5%) i/s  (484.37 μs/i) -     10.455k in   5.065262s

Comparison:
              before:     2099.6 i/s
               after:     2064.5 i/s - same-ish: difference falls within error
</code></pre></div></div>

<p>It didn’t move the needle on real-world benchmarks, and noticeably degraded performance on micro-benchmarks.</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/rstring-buffer-flamegraph.png" alt=""/></p>

<p><a href="https://share.firefox.dev/40dqHXk">Full profile</a>.</p>

<p>Why did it end up slower? The answer is it depends. When resizing a Ruby String, Ruby doesn’t simply call <code>realloc</code> like <code>ruby/json</code>
does for its raw buffer, it also calls <a href="https://man7.org/linux/man-pages/man3/malloc_usable_size.3.html"><code>malloc_usable_size</code></a>,
or the platform equivalent, <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/malloc_size.3.html"><code>malloc_size</code> on <code>macOS</code></a>
or <a href="https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/msize?view=msvc-170"><code>_msize</code> on Windows</a>.</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/str-resize-flamegraph.png" alt=""/></p>

<figure><pre><code data-lang="c"><span>void</span> <span>*</span>
<span>rb_gc_impl_realloc</span><span>(</span><span>void</span> <span>*</span><span>objspace_ptr</span><span>,</span> <span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>size_t</span> <span>new_size</span><span>,</span> <span>size_t</span> <span>old_size</span><span>)</span>
<span>{</span>
    <span>// snip...</span>

    <span>old_size</span> <span>=</span> <span>objspace_malloc_size</span><span>(</span><span>objspace</span><span>,</span> <span>ptr</span><span>,</span> <span>old_size</span><span>);</span>
    <span>TRY_WITH_GC</span><span>(</span><span>new_size</span><span>,</span> <span>mem</span> <span>=</span> <span>RB_GNUC_EXTENSION_BLOCK</span><span>(</span><span>realloc</span><span>(</span><span>ptr</span><span>,</span> <span>new_size</span><span>)));</span>
    <span>if</span> <span>(</span><span>!</span><span>mem</span><span>)</span> <span>return</span> <span>mem</span><span>;</span>
    <span>new_size</span> <span>=</span> <span>objspace_malloc_size</span><span>(</span><span>objspace</span><span>,</span> <span>mem</span><span>,</span> <span>new_size</span><span>);</span>

    <span>// snip...</span>

    <span>objspace_malloc_increase</span><span>(</span><span>objspace</span><span>,</span> <span>mem</span><span>,</span> <span>new_size</span><span>,</span> <span>old_size</span><span>,</span> <span>MEMOP_TYPE_REALLOC</span><span>);</span>

    <span>return</span> <span>mem</span><span>;</span>
<span>}</span></code></pre></figure>

<p>This again is to keep the GC statistics up to date and give the opportunity to the GC to trigger if some threshold is hit.</p>

<p>Something I wonder though, and that I ought to investigate, is that <code>rb_gc_impl_realloc</code> is provided with the known <code>old_size</code>
and <code>new_size</code>. Sometimes it’s <code>0</code> when the called doesn’t know what the size was, but for strings, I believe it does, and yet
data information is simply ignored unless no <code>malloc_usable_size</code> is available:</p>

<figure><pre><code data-lang="c"><span>static</span> <span>inline</span> <span>size_t</span>
<span>objspace_malloc_size</span><span>(</span><span>rb_objspace_t</span> <span>*</span><span>objspace</span><span>,</span> <span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>size_t</span> <span>hint</span><span>)</span>
<span>{</span>
<span>#ifdef HAVE_MALLOC_USABLE_SIZE
</span>    <span>return</span> <span>malloc_usable_size</span><span>(</span><span>ptr</span><span>);</span>
<span>#else
</span>    <span>return</span> <span>hint</span><span>;</span>
<span>#endif
</span><span>}</span></code></pre></figure>

<p>Just like <code>malloc / free</code> etc, the performance of <code>malloc_usable_size</code> varies a lot depending on the allocator, I haven’t benchmarked
on Linux, nor with <code>jemalloc</code>, so it’s possible that this overhead would have been negligible there, and that may be why Ruby doesn’t try to skip
that call when possible?</p>

<p>But as mentioned at the end of the last post, we’re here to change perception, so we have to be faster on the machines users are more likely to use
for benchmarking, and that includes <code>macOS</code> with the default allocator.</p>

<p>In hindsight, there’s something else I could have done to make using a Ruby string as a buffer faster:</p>

<figure><pre><code data-lang="diff"><span>diff --git a/ext/json/ext/generator/generator.c b/ext/json/ext/generator/generator.c
index da78fe1..effc8cc 100644
</span><span>--- a/ext/json/ext/generator/generator.c
</span><span>+++ b/ext/json/ext/generator/generator.c
</span><span>@@ -1024,8 +1024,8 @@</span> static VALUE cState_partial_generate(VALUE self, VALUE obj)
 {
     GET_STATE(self);
 
<span>-    VALUE string = rb_utf8_str_new(NULL, 0);
-    rb_str_resize(string, state-&gt;buffer_initial_length - 1);
</span><span>+    VALUE string = rb_str_buf_new(state-&gt;buffer_initial_length - 1);
+    rb_enc_associate_index(string, utf8_encindex);
</span>     SBuffer buffer = {
         .capa = state-&gt;buffer_initial_length - 1,
         .str = string,</code></pre></figure>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   290.040k i/100ms
Calculating -------------------------------------
               after      3.127M (± 0.3%) i/s  (319.84 ns/i) -     15.662M in   5.009436s

Comparison:
              before:  2616126.5 i/s
               after:  3126563.7 i/s - 1.20x  faster


== Encoding twitter.json (466906 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   202.000 i/100ms
Calculating -------------------------------------
               after      2.049k (± 2.9%) i/s  (488.12 μs/i) -     10.302k in   5.032888s

Comparison:
              before:     2114.5 i/s
               after:     2048.7 i/s - same-ish: difference falls within error
</code></pre></div></div>

<p>Quite a nice gain for such a small change, and you may wonder what’s so different about these two lines of code.</p>

<h2 id="variable-width-allocation-and-embeded-objects">Variable Width Allocation and Embeded Objects</h2>

<p>When Ruby allocates an object, it doesn’t call <code>malloc</code> like a C program would.
Instead, it asks the GC for what’s called a “slot”, which means a fixed-size memory region inside a memory page
managed by the GC.</p>

<p>Up until the introduction of <a href="https://bugs.ruby-lang.org/issues/18045">Variable Width Allocation</a> by <a href="https://peterzhu.ca/">Peter Zhu</a> and
<a href="https://www.eightbitraptor.com/">Matt Valentine-House</a> in Ruby 3.2, all Ruby slots were of the same size: <code>40B</code>.</p>

<p>You might wonder, how can all objects be of the same size if you are able to create strings or arrays of arbitrary size?
That’s because many of the Ruby core types, like <code>String</code>, <code>Array</code> etc, have multiple internal representations.</p>

<p>To stick with the <code>String</code> example, here is a simplified version of what its layout looks like in <code>rstring.h</code>:</p>

<figure><pre><code data-lang="c"><span>struct</span> <span>RString</span> <span>{</span>
    <span>struct</span> <span>RBasic</span> <span>{</span>
      <span>VALUE</span> <span>flags</span><span>;</span>
      <span>VALUE</span> <span>klass</span><span>;</span>
    <span>}</span> <span>basic</span><span>;</span>

    <span>long</span> <span>len</span><span>;</span>
    <span>union</span> <span>{</span>
        <span>struct</span> <span>{</span>
            <span>char</span> <span>*</span><span>ptr</span><span>;</span>
            <span>long</span> <span>capa</span><span>;</span>
        <span>}</span> <span>heap</span><span>;</span>

        <span>struct</span> <span>{</span>
            <span>char</span> <span>ary</span><span>[</span><span>1</span><span>];</span>
        <span>}</span> <span>embed</span><span>;</span>
    <span>}</span> <span>as</span><span>;</span>
<span>};</span></code></pre></figure>

<p>If you are unfamiliar with C’s <code>union</code>, it means that the struct can contain either of its sub structs.</p>

<p>To better visualize, here’s how Ruby stores the <code>&#34;Hello World&#34;</code> string:</p>

<table>
  <thead>
    <tr>
      <th>flags</th>
      <th>klass</th>
      <th>length</th>
      <th>*ptr</th>
      <th>capa</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0x000234</td>
      <td>0xffeff</td>
      <td>11</td>
      <td>Hello Wo</td>
      <td>rld\0</td>
    </tr>
  </tbody>
</table>

<p>Each column is 8 bytes wide, or 64 bits, the first 8 bytes are used to store the object flags, we touched on those before,
the following 8 bytes are used to store a pointer to the object class, and then the last 24 bytes are used to store the string content inside
the object slot.</p>

<p>So you can deduce that strings can be as long as 16 ASCII characters, or rather 15 because you need one byte to store the terminating <code>NULL</code> byte.
Perhaps you’ve read in the past about this limit, and remember that it was 23 characters. That was true but <a href="https://github.com/ruby/ruby/pull/7908">was recently changed by Peter</a>
because it required packing the embedded length inside the <code>flags</code> bitmap instead, which made things slower. That’s your classic memory usage vs execution speed tradeoff.</p>

<p>Now if we try to append content to that string, and go past the embedded capacity, say, 200 characters long, it will call <code>malloc(201)</code>, store the string content inside
that malloced region and the object slot will look like this instead:</p>

<table>
  <thead>
    <tr>
      <th>flags</th>
      <th>klass</th>
      <th>length</th>
      <th>*ptr</th>
      <th>capa</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0x000234</td>
      <td>0xffeff</td>
      <td>200</td>
      <td>0xbbbef</td>
      <td>200</td>
    </tr>
  </tbody>
</table>

<p>But with the introduction of Variable Width Allocation, slots are still fixed-sized in a way, but there are now multiple sizes: <code>40</code>, <code>80</code>, <code>160</code>,
<code>320</code> and <code>640</code>. A slot can’t grow in size, so in the above scenario where we appended to a string, nothing changes, Ruby will still have to “spill”
the content on the string on the heap by calling <code>malloc</code>.</p>

<p>However, if we ask Ruby upfront for a larger string, it will make sure to allocate a larger slot for it if that allows it to be embedded.</p>

<p>In the diff above I call <code>rb_str_buf_new(state-&gt;buffer_initial_length - 1)</code> or <code>rb_str_buf_new(511)</code>, so on Ruby 3.2+, Ruby will allocate a <code>640B</code>
wide slot for us, allowing us to store up to <code>640 - 24 - 1 = 615</code> bytes before having to spill on the heap, and given our micro-benchmark only needs <code>34B</code>
it means no <code>malloc</code> nor <code>free</code> call for the buffer, only a Ruby object slot allocation, which is way cheaper in most cases.</p>

<p>Since we’ll need to ask Ruby to allocate us an object slot so we can return a String object, we might as well ask for a larger one in case we can 
fit in it. If the cost for a <code>40B</code> or <code>640B</code> slot is the same, might as well get the bigger one.</p>

<p>In addition to saving on the <code>malloc</code> call, we also save on the <code>free</code> call. When GC triggers and there’s no longer any reference to that slot, Ruby
will just mark the slot as available.</p>

<p>But I didn’t think of this at that time, so maybe that’s something I’ll need to revisit in the future.</p>

<h2 id="be-nice-to--your-mother">Be Nice To  Your Mother</h2>

<p>Instead <a href="https://github.com/ruby/json/commit/fe607f4806ac1d448c1ea5ae7324fdbab183d2ca">I resigned myself to using a stack allocation for the buffer content too</a>.
But I went with a much more conservative size than <code>Oj</code>, a mere <code>512B</code>.</p>

<p>The implementation is rather simple, I simply had to add one extra <code>type</code> field inside the <code>FBuffer</code> struct
to keep track of the buffer provenance, so that we behave a bit differently inside <code>fbuffer_inc_capa</code> if the buffer is on the stack.
Here’s the implementation with some extra comments:</p>

<figure><pre><code data-lang="c"><span>static</span> <span>inline</span> <span>void</span> <span>fbuffer_inc_capa</span><span>(</span><span>FBuffer</span> <span>*</span><span>fb</span><span>,</span> <span>unsigned</span> <span>long</span> <span>requested</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>RB_UNLIKELY</span><span>(</span><span>requested</span> <span>&gt;</span> <span>fb</span><span>-&gt;</span><span>capa</span> <span>-</span> <span>fb</span><span>-&gt;</span><span>len</span><span>))</span> <span>{</span>
        <span>unsigned</span> <span>long</span> <span>required</span><span>;</span>

        <span>if</span> <span>(</span><span>RB_UNLIKELY</span><span>(</span><span>!</span><span>fb</span><span>-&gt;</span><span>ptr</span><span>))</span> <span>{</span>
            <span>fb</span><span>-&gt;</span><span>ptr</span> <span>=</span> <span>ALLOC_N</span><span>(</span><span>char</span><span>,</span> <span>fb</span><span>-&gt;</span><span>initial_length</span><span>);</span>
            <span>fb</span><span>-&gt;</span><span>capa</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>initial_length</span><span>;</span>
        <span>}</span>

        <span>for</span> <span>(</span><span>required</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>capa</span><span>;</span> <span>requested</span> <span>&gt;</span> <span>required</span> <span>-</span> <span>fb</span><span>-&gt;</span><span>len</span><span>;</span> <span>required</span> <span>&lt;&lt;=</span> <span>1</span><span>);</span>

        <span>if</span> <span>(</span><span>required</span> <span>&gt;</span> <span>fb</span><span>-&gt;</span><span>capa</span><span>)</span> <span>{</span>
            <span>if</span> <span>(</span><span>fb</span><span>-&gt;</span><span>type</span> <span>==</span> <span>STACK</span><span>)</span> <span>{</span>
                <span>// If the buffer is on the stack</span>
                <span>const</span> <span>char</span> <span>*</span><span>old_buffer</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>ptr</span><span>;</span>
                <span>// We allocate a larger buffer on the heap</span>
                <span>fb</span><span>-&gt;</span><span>ptr</span> <span>=</span> <span>ALLOC_N</span><span>(</span><span>char</span><span>,</span> <span>required</span><span>);</span>
                <span>// Mark it as now being on the heap</span>
                <span>fb</span><span>-&gt;</span><span>type</span> <span>=</span> <span>HEAP</span><span>;</span>
                <span>// Copy the old content over</span>
                <span>MEMCPY</span><span>(</span><span>fb</span><span>-&gt;</span><span>ptr</span><span>,</span> <span>old_buffer</span><span>,</span> <span>char</span><span>,</span> <span>fb</span><span>-&gt;</span><span>len</span><span>);</span>
            <span>}</span> <span>else</span> <span>{</span>
                <span>REALLOC_N</span><span>(</span><span>fb</span><span>-&gt;</span><span>ptr</span><span>,</span> <span>char</span><span>,</span> <span>required</span><span>);</span>
            <span>}</span>
            <span>fb</span><span>-&gt;</span><span>capa</span> <span>=</span> <span>required</span><span>;</span>
        <span>}</span>
    <span>}</span>
<span>}</span></code></pre></figure>

<p>This had the expected effect on micro-benchmarks, a nice 7% improvement:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   286.112k i/100ms
Calculating -------------------------------------
               after      3.024M (± 0.7%) i/s  (330.67 ns/i) -     15.164M in   5.014435s

Comparison:
              before:  2836034.1 i/s
               after:  3024200.8 i/s - 1.07x  faster
</code></pre></div></div>

<p>However, I quickly noticed that it also became way slower on real-world benchmarks:</p>

<div><div><pre><code>== Encoding twitter.json (466906 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   156.000 i/100ms
Calculating -------------------------------------
               after      1.572k (± 1.7%) i/s  (636.13 μs/i) -      7.956k in   5.062686s

Comparison:
              before:     2134.0 i/s
               after:     1572.0 i/s - 1.36x  slower
</code></pre></div></div>

<p>While I was determined to spend a lot of effort in improving <code>ruby/json</code> performance on micro-benchmarks, degrading its performance
on real-world benchmarks was a huge red line for me, so I had to figure out what happened.</p>

<p>So I went back to my profiler, and started playing “Spot the seven differences” again:</p>

<p>Before:</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/before.png" alt=""/></p>

<p>After:</p>

<p><img src="https://byroot.github.io/assets/articles/json-4/after.png" alt=""/></p>

<p>It’s far from obvious if you don’t know what to look for, but you can see that before we were spending <code>50%</code> of the runtime in <code>generate_json_string</code>, and afterward, only <code>3.1%</code>
and instead, the top was trusted by a bunch of smaller functions called by <code>generate_json_string</code>.</p>

<p>These are the signs of what is sometimes referred to as “the mother of all optimizations”: inlining.</p>

<p>Even in C, calling a function isn’t that cheap. It’s cheap enough that you generally don’t think about it but costly enough that you try to minimize
function calls in hotspots.</p>

<p>You can do that by refactoring your code to use bigger functions, or even copying code around using macros, but that gets old quickly.
Instead, the compiler does that for us, it identifies the small leaf functions that aren’t worth calling and instead copies its content inside the parent,
even if it means copy-pasting it dozens and dozens of times. In addition to saving on the overhead of a function call, it also allows to optimize the caller
and callee together, sometimes allowing to eliminate redundant computations or simply dead code.</p>

<p>That’s what the <code>inline</code> keyword is for in the <code>static inline void fbuffer_inc_capa...</code> declaration, it’s a way to tell the compiler that it would be
a good idea to inline this function. But that’s all it is, just a compiler hint, the compiler can still decide that you are wrong and that it knows better.</p>

<p>I don’t know the intricacies of <code>LLVM/clang</code> enough to know for certain why it decided to no longer inline all these functions, but I guessed that it
was because I made <code>fbuffer_inc_capa</code> much larger.</p>

<p>The reason it’s important <code>fbuffer_inc_capa</code> is inlined, is because 99%+ of the time, we return from it after just a very simple comparison:
<code>RB_UNLIKELY(requested &gt; fb-&gt;capa - fb-&gt;len)</code>. That’s the part we want inlined, so we don’t pay for a function call just for that check.
The rest of the function we don’t care so much, we rarely ever go into it.</p>

<p>So to appease the compiler, and make that conditional appealing to inline again, <a href="https://github.com/ruby/json/commit/41c021580e48754aa4bfc71c8363b1fb233ed8c8">the solution would be to extract the large amount of code that is
rarely executed in another function that isn’t marked as <code>inline</code></a>:</p>

<figure><pre><code data-lang="c"><span>static</span> <span>void</span> <span>fbuffer_do_inc_capa</span><span>(</span><span>FBuffer</span> <span>*</span><span>fb</span><span>,</span> <span>unsigned</span> <span>long</span> <span>requested</span><span>)</span>
<span>{</span>
    <span>// snip...</span>
<span>}</span>

<span>static</span> <span>inline</span> <span>void</span> <span>fbuffer_inc_capa</span><span>(</span><span>FBuffer</span> <span>*</span><span>fb</span><span>,</span> <span>unsigned</span> <span>long</span> <span>requested</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>RB_UNLIKELY</span><span>(</span><span>requested</span> <span>&gt;</span> <span>fb</span><span>-&gt;</span><span>capa</span> <span>-</span> <span>fb</span><span>-&gt;</span><span>len</span><span>))</span> <span>{</span>
        <span>fbuffer_do_inc_capa</span><span>(</span><span>fb</span><span>,</span> <span>requested</span><span>);</span>
    <span>}</span>
<span>}</span></code></pre></figure>

<p>Running the benchmarks again with both changes, we finally had what we expected:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   290.616k i/100ms
Calculating -------------------------------------
               after      3.093M (± 0.3%) i/s  (323.30 ns/i) -     15.693M in   5.073761s

Comparison:
              before:  2829771.3 i/s
               after:  3093060.4 i/s - 1.09x  faster


== Encoding twitter.json (466906 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
               after   208.000 i/100ms
Calculating -------------------------------------
               after      2.088k (± 0.5%) i/s  (479.01 μs/i) -     10.608k in   5.081469s

Comparison:
              before:     2108.3 i/s
               after:     2087.6 i/s - same-ish: difference falls within error
</code></pre></div></div>

<p>We squeezed a tiny bit more performance on the micro-benchmark, and the real work benchmark wasn’t noticeably impacted.</p>

<h2 id="to-be-continued">To Be Continued</h2>

<p>At that point, with all the above optimizations, we were now faster than <code>Oj</code> when reusing the <code>JSON::State</code> object,
but still quite a bit slower when allocating it on every call:</p>

<div><div><pre><code>== Encoding small mixed (34 bytes)
ruby 3.4.1 (2024-12-25 revision 48d4efcb85) +YJIT +PRISM [arm64-darwin23]
Warming up --------------------------------------
        json (reuse)   619.700k i/100ms
                json   291.441k i/100ms
                  oj   532.966k i/100ms
Calculating -------------------------------------
        json (reuse)      6.628M (± 4.9%) i/s  (150.88 ns/i) -     33.464M in   5.064856s
                json      3.191M (± 0.5%) i/s  (313.35 ns/i) -     16.029M in   5.022818s
                  oj      5.873M (± 0.9%) i/s  (170.26 ns/i) -     29.846M in   5.082087s

Comparison:
        json (reuse):  6627811.3 i/s
                  oj:  5873337.3 i/s - 1.13x  slower
                json:  3191361.6 i/s - 2.08x  slower
</code></pre></div></div>

<p>So there was no way around it, I had to find how to automatically re-use that <code>JSON::State</code> object. Or how to not allocate it at all?</p>

<p>But that’s a story for the next part.</p>

  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
