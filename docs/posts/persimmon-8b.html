<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.adept.ai/blog/persimmon-8b">Original</a>
    <h1>Persimmon-8B</h1>
    
    <div id="readability-page-1" class="page"><div>
        
  <p>We’re excited to open-source Persimmon-8B, the best fully permissively-licensed model in the 8B class.
The code and weights are <a href="https://github.com/persimmon-ai-labs/adept-inference">here</a>.</p>
<p>At Adept, we’re working towards an AI agent that can help people do anything they need to do on a computer. We’re not in the business of shipping isolated language models (LMs)—this was an early output of the model scaling program that will support our products.</p>
<p>Over the last year, we’ve been amazed by how smart small models are becoming, and we wanted to give the community access to an even better 8B LM to build on for any use case, with an open Apache license and publicly accessible weights. The 8B size is a sweet spot for most users without access to large-scale compute—they can be finetuned on a single GPU, run at a decent speed on modern MacBooks, and may even fit on mobile devices.</p>
<p>Persimmon-8B has several nice properties:</p>
<ol>
<li>This is the most capable open-source, fully permissive model with fewer than 10 billion parameters. We are releasing it under an Apache license for maximum flexibility.</li>
<li>We trained it from scratch using a context size of 16K. Many LM use cases are context-bound; our model has 4 times the context size of LLaMA2 and 8 times that of GPT-3, MPT, etc.</li>
<li>Our base model exceeds other ~8B models and matches LLaMA2 performance despite having been trained on only 0.37x as much data as LLaMA2.</li>
<li>The model has 70k unused embeddings for multimodal extensions, and has sparse activations.</li>
<li>The inference code we’re releasing along with the model is unique—it combines the speed of C++ implementations (e.g. FasterTransformer) with the flexibility of naive Python inference.</li>
</ol>
<p>We’re excited to see how the community takes advantage of these capabilities not present in other open source language models, and we hope this model spurs even greater innovation!</p>
<p><strong>Because this is a raw model release, we have not added further finetuning, postprocessing or sampling strategies to control for toxic outputs.</strong></p>
<h2 id="a-more-realistic-way-of-doing-evals">A more realistic way of doing evals</h2>
<p>Determining the quality of a language model is still as much art as science. Model quality is not an absolute metric and depends on how the language model will be used. In most use cases, we expect language models to generate text. However, a common methodology for evaluating language models doesn’t actually ask them to generate any text at all. Consider the following multiple choice question from the common HellaSwag eval set. The goal is to pick which of the four answers best continues the “question.”</p>
<p><em>A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She…</em></p>
<p><em>a) rinses the bucket off with soap and blow dries the dog’s head.</em></p>
<p>One way to evaluate the model is to simply ask it to answer the question and then see which choice it makes – (a), (b), (c), or (d).  This mimics the experience of how people actually interact with language models – they ask questions and expect answers. This is analogous to e.g. <a href="https://crfm.stanford.edu/helm/latest/">HELM</a>.</p>
<p>A more common practice in ML is instead to use the implicit probabilities that the language model assigns to each choice. For option (a) above, we calculate the probability of “rinses” given the previous sentences, and then probability of “the” given the previous sentences plus “rinses,“ and so on. We then multiply all these probabilities together, giving the probability of the entire sequence for option (a). We do this for all four choices (optionally adding length normalization to account for different length sequences) and select the option with the highest sequence probability. This is a fine way to measure the intrinsic knowledge of a language model, but a poor way to understand what actually interacting with it is like.</p>
<p>Since we care about interacting with language models, we do all of our evals with the former technique–we directly generate answers from the model. We’re releasing the prompts we use so that others can reproduce these numbers.</p>
<h2 id="results">Results</h2>
<p>We compared Persimmon-8B to the current most powerful model in its size range—<a href="https://ai.meta.com/llama/">LLama 2</a>—and to <a href="https://www.mosaicml.com/blog/mpt-7b">MPT 7B Instruct</a>.
Our instruction-fine-tuned model—Persimmon-8B-FT—is the strongest performing model on all but one of the metrics.
Our base model—Persimmon-8B-Base—performs comparably to Llama 2, despite having seen only 37% as much training data.</p>






















































<table><thead><tr><th>Eval Task</th><th>MPT 7B Instruct 1-Shot</th><th>Llama 2 Base 7B 1-Shot</th><th>Persimmon-8B-Base 1-Shot</th><th>Persimmon-8B-FT 1-Shot</th></tr></thead><tbody><tr><td>MMLU</td><td>27.6</td><td>36.6</td><td>36.5</td><td><strong>41.2</strong></td></tr><tr><td>Winogrande</td><td>49.1</td><td>51.1</td><td>51.4</td><td><strong>54.6</strong></td></tr><tr><td>Arc Easy</td><td>32.5</td><td>53.7</td><td>48.1</td><td><strong>64.0</strong></td></tr><tr><td>Arc Challenge</td><td>28.8</td><td>43.8</td><td>34.5</td><td><strong>46.8</strong></td></tr><tr><td>TriviaQA</td><td>33.9</td><td><strong>36.6</strong></td><td>24.3</td><td>17.2</td></tr><tr><td>HumanEval</td><td>12.8</td><td>0 / 12.2<sup><a href="#user-content-fn-humaneval" id="user-content-fnref-humaneval" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></td><td>18.9</td><td><strong>20.7</strong></td></tr></tbody></table>
<h2 id="model-details">Model Details</h2>
<p>Persimmon-8B is a standard decoder-only transformer with several architecture modifications.</p>
<p>We use the squared ReLU activation function<sup><a href="#user-content-fn-activationnote" id="user-content-fnref-activationnote" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. We use rotary positional encodings – our internal experiments found it superior to Alibi. We add layernorm to the Q and K embeddings before they enter the attention calculation.</p>
<p>The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the model–it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.</p>
<p>Furthermore, in a space-constrained environment, the 70k unused embeddings (corresponding to reserved tokens) could be removed from the input/output embedding matrices.
This would reduce the model size by approximately 570M parameters.</p>
<p>We train the model from start to finish with a sequence length of 16K on 737B tokens uniformly sampled from a much larger dataset, which is a mix of text (~75%) and code (~25%).</p>
<p>Natively training on such long sequences throughout training is made possible by our development of an improved version of <a href="https://www.adept.ai/blog/flashier-attention">FlashAttention</a>, (<a href="https://github.com/Dao-AILab/flash-attention">Github</a>). We also modified the base for rotary calculations to allow for full position resolution at this longer length. This contrasts with all other open source models, which use a sequence length of at most 4096 for the majority of training. We use a vocabulary of 262k tokens, built using a unigram sentencepiece model.</p>
<p>We’ve included a table with important model information below:</p>





































<table><thead><tr><th>Attribute</th><th>Value</th></tr></thead><tbody><tr><td>Hidden Size</td><td>4096</td></tr><tr><td>Heads</td><td>64</td></tr><tr><td>Layers</td><td>36</td></tr><tr><td>Batch Size</td><td>120</td></tr><tr><td>Sequence Length</td><td>16384</td></tr><tr><td>Training Iterations</td><td>375000</td></tr><tr><td>Tokens Seen</td><td>737 Billion</td></tr></tbody></table>
<h2 id="flexible-and-fast-inference">Flexible and Fast Inference</h2>
<p>We’re also releasing fast inference code for this model–with a short prompt, we can sample ~56 tokens per second on one 80GB A100 GPU<sup><a href="#user-content-fn-embeddingnote" id="user-content-fnref-embeddingnote" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>. While most optimized inference code is complicated and brittle, we’ve managed to make ours flexible without sacrificing speed. We can define models in PyTorch, run inference with minimal changes, and still be faster than FasterTransformer.</p>
<p>There are two main things that slow down traditional inference implementations:</p>
<ol>
<li>First, both the Python runtime and CUDA kernel dispatch incur per-operation overheads.</li>
<li>Second, failing to fuse operations means we spend time writing to memory and then reading back again the same values; while this overhead might go unnoticed during training (which is compute bound), inference is usually bottlenecked by memory bandwidth.</li>
</ol>
<p>The standard practice for achieving fast inference is to rewrite the entire model inference loop in C++, as in FasterTransformer, and call out to special fused kernels in CUDA. But this means that any changes to the model require painfully reimplementing every feature twice: once in Python / PyTorch in the training code and again in C++ in the inference codebase.  We found this process too cumbersome and error prone to iterate quickly on the model.</p>
<p>We wanted a strategy that would fix both of these slowdowns without maintaining a separate C++ codebase.</p>
<ol>
<li>To handle operator fusion, we’ve extracted one of the attention kernels<sup><a href="#user-content-fn-kernelname" id="user-content-fnref-kernelname" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> from <a href="https://github.com/NVIDIA/FasterTransformer">NVIDIA’s FasterTransformer repo</a>. During Python inference, we simply replace the attention operation with a call to this kernel. Because our architecture modifications don’t touch the core attention operation, this highly complex kernel can remain unmodified.</li>
<li>To handle the per-operator overheads, we use <a href="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/">CUDA graphs</a> to capture and replay the forward pass. We’ve also implemented this in a way that works with tensor parallelism, which lets us easily use multiple GPUs for inference.</li>
</ol>
<p>This strategy gives us the best of worlds—we can write model code in only one place while still doing inference faster than FasterTransformer. We really hope this accelerates the exciting applications that folks in the community can build.</p>
<p>This is just the first small release in a series of things we’re excited to put out this fall and winter. Enjoy!</p>
<p>—</p>


      </div></div>
  </body>
</html>
