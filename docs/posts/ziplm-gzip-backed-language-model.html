<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Futrell/ziplm">Original</a>
    <h1>Ziplm: Gzip-Backed Language Model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Useless but mildly interesting language model using compressors built-in to Python.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">You can &#34;train&#34; it using some training data:</p>
<div data-snippet-clipboard-copy-content="data = open(my_favorite_text_file).read().lower()
alphabet = &#34;qwertyuiopasdfghjklzxcvbnm,.;1234567890 &#34;
model = ziplm.ZipModel(alphabet, training=data)
&#34;&#34;.join(model.sample_sequence(10)) # sample 10 characters from the alphabet"><pre lang="{python}"><code>data = open(my_favorite_text_file).read().lower()
alphabet = &#34;qwertyuiopasdfghjklzxcvbnm,.;1234567890 &#34;
model = ziplm.ZipModel(alphabet, training=data)
&#34;&#34;.join(model.sample_sequence(10)) # sample 10 characters from the alphabet
</code></pre></div>
<p dir="auto">You can also run it without any training data, and just forward sample to see what kinds of patterns gzip likes:</p>
<div data-snippet-clipboard-copy-content="alphabet = &#34;abc&#34;
model = ziplm.ZipModel(alphabet)
&#34;&#34;.join(model.sample_sequence(100)) # I get &#39;ccabcabcabcabcabcabcabcabcabcabcabcabcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbccabcbcbcbc&#39;"><pre lang="{python}"><code>alphabet = &#34;abc&#34;
model = ziplm.ZipModel(alphabet)
&#34;&#34;.join(model.sample_sequence(100)) # I get &#39;ccabcabcabcabcabcabcabcabcabcabcabcabcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbccabcbcbcbc&#39;
</code></pre></div>
<p dir="auto">You can also get the probability for a sequence:</p>
<div data-snippet-clipboard-copy-content="alphabet = &#34;qwertyuiopasdfghjklzxcvbnm &#34;
model = ziplm.ZipModel(alphabet)
model.sequence_logprob(&#34;this is my favorite string&#34;) # I get -83.8"><pre lang="{python}"><code>alphabet = &#34;qwertyuiopasdfghjklzxcvbnm &#34;
model = ziplm.ZipModel(alphabet)
model.sequence_logprob(&#34;this is my favorite string&#34;) # I get -83.8
</code></pre></div>
<p dir="auto">You can also try using <code>bz2</code> and <code>lzma</code> as language models by passing them as the <code>compressor</code> argument to the model</p>
<div data-snippet-clipboard-copy-content="import lzma
model = ziplm.ZipModel(alphabet, compressor=lzma)
&#34;&#34;.join(model.sample_sequence(100)) # I get &#39;cccbaaaaacccccabcacccbaaaaabaacaabaacaabaacaabaabacaaaaaaaaaaacccbabacaaaaaaaaaaaccccacaaccbaaaaaccc&#39;"><pre lang="{python}"><code>import lzma
model = ziplm.ZipModel(alphabet, compressor=lzma)
&#34;&#34;.join(model.sample_sequence(100)) # I get &#39;cccbaaaaacccccabcacccbaaaaabaacaabaacaabaacaabaabacaaaaaaaaaaacccbabacaaaaaaaaaaaccccacaaccbaaaaaccc&#39;
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-why-does-this-work" aria-hidden="true" href="#why-does-this-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why does this &#34;work&#34;?</h2>
<p dir="auto">This works because of two facts:</p>
<ol dir="auto">
<li>A language model is nothing but a distribution on the next token given previous tokens, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$p(x \mid c)$</math-renderer>.</li>
<li>There is a general equivalence between <em>probability distributions</em> and <em>codes</em>.</li>
</ol>
<p dir="auto">The second point is what makes this interesting. Information theory tells us that we can derive codes from probability distributions. That is, if I have some datapoints <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$x$</math-renderer>, and I know that they follow probability distribution <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$p(x)$</math-renderer>, I can come up with a lossless binary code to encode the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$x$</math-renderer> where the length of each code is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$-\log_2 p(x)$</math-renderer>. This code minimizes the average code length: the only way to get shorter average code length would be to go into the realm of lossy compression. This is called the Shannon Limit.</p>
<p dir="auto">Since I can convert probability distributions to codes in this way, I can also convert codes to probability distributions. If I have a code (like gzip) that describes my datapoint with length <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$l(x)$</math-renderer> in binary, then that corresponds to a probability distribution <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$p(x) = 2^{-l(x)}$</math-renderer>. If the code is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$K$</math-renderer>-ary, then the corresponding distribution is
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$$p(x) = K^{-l(x)}.$$</math-renderer></p>
<p dir="auto">The ZipLM model works by converting code lengths to probabilities in this way. If I have a vocabulary of size <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$K$</math-renderer>, and a string <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$c$</math-renderer>, then the probability distribution for continuations <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$x$</math-renderer> is:
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$$p(x \mid c) \propto K^{-l(cx)},$$</math-renderer>
where the proportionality reflects the fact that we have to sum over the compressed lengths of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$cx^\prime$</math-renderer> for all <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6882ab249219be5b83bd430f19d6d1a9">$x^\prime$</math-renderer> in the vocabulary. That&#39;s all there is to it.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-well-does-it-work" aria-hidden="true" href="#how-well-does-it-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How well does it work?</h2>
<p dir="auto">It&#39;s pretty bad, but it doesn&#39;t generate total junk. Here I trained the gzip model in Moby Dick---from the <a href="https://www.gutenberg.org/files/2701/2701-0.txt" rel="nofollow">Project Gutenberg text</a>---and the output at least has some recognizable parts:</p>
<div data-snippet-clipboard-copy-content="data = open(&#34;mobydick.txt&#34;).read().lower()
alphabet = &#34;qwertyuiopasdfghjkl;&#39;zxcvbnm,. &#34;
model = ziplm.Model(alphabet, data)
&#34;&#34;.join(model.sample_sequence(100)) "><pre lang="{python}"><code>data = open(&#34;mobydick.txt&#34;).read().lower()
alphabet = &#34;qwertyuiopasdfghjkl;&#39;zxcvbnm,. &#34;
model = ziplm.Model(alphabet, data)
&#34;&#34;.join(model.sample_sequence(100)) 
</code></pre></div>
<p dir="auto">This gives me</p>
<div data-snippet-clipboard-copy-content="&#34;&#39;theudcanvas. ;cm,zumhmcyoetter toauuo long a one aay,;wvbu.mvns. x the dtls and enso.;k.like bla.njv&#39;&#34;"><pre lang="{python}"><code>&#34;&#39;theudcanvas. ;cm,zumhmcyoetter toauuo long a one aay,;wvbu.mvns. x the dtls and enso.;k.like bla.njv&#39;&#34;
</code></pre></div>
<p dir="auto">which at least seems to have &#34;long a one&#34; in it.</p>
</article>
          </div></div>
  </body>
</html>
