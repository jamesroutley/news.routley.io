<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers">Original</a>
    <h1>Data accidentally exposed by Microsoft AI researchers</h1>
    
    <div id="readability-page-1" class="page"><div><h2><span></span><a id="executive-summary-0"></a><strong>Executive summary</strong> </h2><ul><li><p>Microsoft’s AI research team, while publishing a bucket of open-source training data on GitHub, accidentally exposed 38 terabytes of additional private data — including a disk backup of two employees’ workstations. </p></li><li><p>The backup includes secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. </p></li><li><p>The researchers shared their files using an Azure feature called SAS tokens, which allows you to share data from Azure Storage accounts. </p></li><li><p>The access level can be limited to specific files only; however, in this case, the link was configured to share the entire storage account — including another 38TB of private files. </p></li><li><p>This case is an example of the new risks organizations face when starting to leverage the power of AI more broadly, as more of their engineers now work with massive amounts of training data. As data scientists and engineers race to bring new AI solutions to production, the massive amounts of data they handle require additional security checks and safeguards. </p></li></ul><h2><span></span><a id="introduction-and-microsoft-findings-2"></a><strong>Introduction and Microsoft findings</strong> </h2><p>As part of the Wiz Research Team’s <a href="https://www.youtube.com/watch?v=rbHALyrxj0Y">ongoing work</a> on accidental exposure of cloud-hosted data, the team scanned the internet for misconfigured storage containers. In this process, we found a GitHub repository under the Microsoft organization named <code>robust-models-transfer</code>. The repository belongs to Microsoft’s AI research division, and its purpose is to provide open-source code and AI models for image recognition. Readers of the repository were instructed to download the models from an Azure Storage URL:  </p><p>The exposed storage URL, taken from Microsoft’s GitHub repository</p><p>However, this URL allowed access to more than just open-source models. It was configured to grant permissions on the entire storage account, exposing additional private data by mistake. </p><p>Our scan shows that this account contained 38TB of additional data — including Microsoft employees’ personal computer backups. The backups contained sensitive personal data, including passwords to Microsoft services, secret keys, and over 30,000 internal Microsoft Teams messages from 359 Microsoft employees. </p><p>Exposed containers under the &#39;robustnessws4285631339&#39; storage account </p><p>A small sample of sensitive files found on the computer backups</p><p>Redacted Teams conversation between two Microsoft employees</p><p>In addition to the overly permissive access scope, the token was also misconfigured to allow “full control” permissions instead of read-only. Meaning, not only could an attacker view all the files in the storage account, but they could delete and overwrite existing files as well. </p><p>This is particularly interesting considering the repository’s original purpose: providing AI models for use in training code. The repository instructs users to download a model data file from the SAS link and feed it into a script. The file’s format is <code>ckpt</code>, a format produced by the TensorFlow library. It’s formatted using Python’s <code>pickle</code> formatter, which is <a href="https://huggingface.co/docs/hub/security-pickle">prone to arbitrary code execution</a> by design. Meaning, an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft’s GitHub repository would’ve been infected by it.  </p><p>However, it’s important to note this storage account wasn’t directly exposed to the public; in fact, it was a private storage account. The Microsoft developers used an Azure mechanism called “SAS tokens”, which allows you to create a shareable link granting access to an Azure Storage account’s data — while upon inspection, the storage account would still seem completely private. </p><h2><span></span><a id="introduction-to-sas-tokens-16"></a><strong>Introduction to SAS tokens</strong> </h2><p>In Azure, a Shared Access Signature (SAS) token is a signed URL that grants access to Azure Storage data. The access level can be customized by the user; the permissions range between read-only and full control, while the scope can be either a single file, a container, or an entire storage account. The expiry time is also completely customizable, allowing the user to create never-expiring access tokens. This granularity provides great agility for users, but it also creates the risk of granting too much access; in the most permissive case (as we’ve seen in Microsoft’s token above), the token can allow full control permissions, on the entire account, forever – essentially providing the same access level as the account key itself.   </p><p>There are 3 types of SAS tokens: Account SAS, Service SAS, and User Delegation SAS. In this blog we will focus on the most popular type – Account SAS tokens, which were also used in Microsoft’s repository. </p><p>Generating an Account SAS is a simple process. As can be seen in the screen below, the user configures the token’s scope, permissions, and expiry date, and generates the token. Behind the scenes, the browser downloads the account key from Azure, and signs the generated token with the key. This entire process is done on the client side; it’s not an Azure event, and the resulting token is not an Azure object. </p><p>Creating a high privilege non-expiring SAS token</p><p>Because of this, when a user creates a highly-permissive non-expiring token, there is no way for an administrator to know this token exists and where it circulates. Revoking a token is no easy task either — it requires rotating the account key that signed the token, rendering all other tokens signed by same key ineffective as well. These unique pitfalls make this service an easy target for attackers looking for exposed data. </p><p>Besides the risk of accidental exposure, the service’s pitfalls make it an effective tool for attackers seeking to maintain persistency on compromised storage accounts. A recent <a href="https://www.microsoft.com/en-us/security/blog/2023/09/07/cloud-storage-security-whats-new-in-the-threat-matrix/#:~:text=Create%20SAS%20Token">Microsoft report</a> indicates that attackers are taking advantage of the service’s lack of monitoring capabilities in order to issue privileged SAS tokens as a backdoor. Since the issuance of the token is not documented anywhere, there is no way to know that it was issued and act against it.  </p><h2><span></span><a id="sas-security-risks-24"></a><strong>SAS security risks</strong> </h2><p>SAS tokens pose a security risk, as they allow sharing information with external unidentified identities. The risk can be examined from several angles: permissions, hygiene, management and monitoring. </p><h3><span></span><a id="permissions-27"></a><strong>Permissions</strong> </h3><p>A SAS token can grant a very high access level to a storage account, whether through excessive permissions (like read, list, write or delete), or through wide access scopes that allow users to access adjacent storage containers.  </p><h3><span></span><a id="hygiene-29"></a><strong>Hygiene</strong> </h3><p>SAS tokens have an expiry problem — our scans and monitoring show organizations often use tokens with a very long (sometimes infinite) lifetime, as there is no upper limit on a token&#39;s expiry. This was the case with Microsoft’s token, which was valid until 2051. </p><h3><span></span><a id="management-and-monitoring-31"></a><strong>Management and monitoring</strong> </h3><p>Account SAS tokens are extremely hard to manage and revoke. There isn&#39;t any official way to keep track of these tokens within Azure, nor to monitor their issuance, which makes it difficult to know how many tokens have been issued and are in active use. The reason even issuance cannot be tracked is that SAS tokens are created on the client side, therefore it is not an an Azure tracked activity, and the generated token is not an Azure object. Because of this, even what appears to be a private storage account may potentially be widely exposed. </p><p>As for revocation, there isn&#39;t a way to revoke a singular Account SAS; the only solution is revoking the entire account key, which invalidates all the other tokens issued with the same key as well. </p><p>Monitoring the usage of SAS tokens is another challenge, as it requires enabling logging on each storage account separately. It can also be costly, as the pricing depends on the request volume of each storage account.  </p><h2><span></span><a id="sas-security-recommendations-35"></a><strong>SAS security recommendations</strong> </h2><p>SAS security can be significantly improved with the following recommendations.</p><h3><span></span><a id="management-37"></a><strong>Management</strong> </h3><p>Due to the lack of security and governance over Account SAS tokens, they should be considered as sensitive as the account key itself. Therefore, it is highly recommended to avoid using Account SAS for external sharing. Token creation mistakes can easily go unnoticed and expose sensitive data.  </p><p>For external sharing, consider using a Service SAS with a <a href="https://learn.microsoft.com/en-us/rest/api/storageservices/define-stored-access-policy">Stored Access Policy</a>. This feature connects the SAS token to a server-side policy, providing the ability to manage policies and revoke them in a centralized manner. </p><p>If you need to share content in a time-limited manner, consider using a <a href="https://learn.microsoft.com/en-us/rest/api/storageservices/create-user-delegation-sas">User Delegation SAS</a>, since their expiry time is capped at 7 days. This feature connects the SAS token to Azure Active Directory’s identity management, providing control and visibility over the identity of the token’s creator and its users. </p><p>Additionally, we recommend creating dedicated storage accounts for external sharing, to ensure that the potential impact of an over-privileged token is limited to external data only. </p><p>To avoid SAS tokens completely, organizations will have to <a href="https://learn.microsoft.com/en-us/azure/storage/common/shared-key-authorization-prevent">disable SAS access</a> for each of their storage accounts separately. We recommend using a CSPM to track and enforce this as a policy. </p><p>Another solution to disable SAS token creation is by blocking access to the “<a href="https://learn.microsoft.com/en-us/rest/api/storagerp/storage-accounts/list-keys">list storage account keys</a>” operation in Azure (since new SAS tokens cannot be created without the key), then rotating the current account keys, to invalidate pre-existing SAS tokens. This approach would still allow creation of User Delegation SAS, since it relies on the user’s key instead of the account key. </p><h3><span></span><a id="monitorin-g-44"></a><strong>Monitorin</strong><strong><u>g</u></strong> </h3><p>To track active SAS token usage, you need to <a href="https://learn.microsoft.com/en-us/azure/storage/common/manage-storage-analytics-logs">enable Storage Analytics logs</a> for each of your storage accounts. The resulting logs will contain details of SAS token access, including the signing key and the permissions assigned. However, it should be noted that only actively used tokens will appear in the logs, and that enabling logging comes with extra charges — which might be costly for accounts with extensive activity. </p><p><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/blob-storage-monitoring-scenarios">Azure Metrics</a> can be used to monitor SAS tokens usage in storage accounts. By default, Azure records and aggregates storage account events up to <a href="https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics#retention-of-metrics">93 days</a>. Utilizing Azure Metrics, users can look up SAS-authenticated requests, highlighting storage accounts with SAS tokens usage. </p><h3><span></span><a id="secret-scanning-47"></a><strong>Secret scanning</strong> </h3><p>In addition, we recommend using secret scanning tools to detect leaked or over-privileged SAS tokens in artifacts and publicly exposed assets, such as mobile apps, websites, and GitHub repositories — as can be seen in the Microsoft case.  </p><p>For more information on cloud secret scanning, please check out our recent talk from the fwd:cloudsec 2023 conference, <a href="https://youtu.be/rbHALyrxj0Y">&#34;Scanning the internet for external cloud exposures&#34;</a>. </p><h3><span></span><a id="for-wiz-customers-50"></a><strong>For Wiz customers</strong> </h3><p>Wiz customers can leverage the Wiz secret scanning capabilities to identify SAS tokens in internal and external assets and explore their permissions. In addition, customers can use the Wiz CSPM to track storage accounts with SAS support. </p><ul><li><p><strong>Detect SAS tokens:</strong> use this <a href="https://app.wiz.io/graph#~(query~(type~(~&#39;SECRET_DATA)~select~true~where~(presignedURL_type~(EQUALS~(~&#39;PresignedURLTypeAzureSASToken)))~relationships~(~(type~(~(type~&#39;PERMITS))~optional~true~with~(type~(~&#39;STORAGE_ACCOUNT)~select~true))~(type~(~(type~&#39;INSTANCE_OF~reverse~true))~with~(type~(~&#39;SECRET_INSTANCE)~select~true~relationships~(~(type~(~(type~&#39;CONTAINS~reverse~true))~with~(type~(~&#39;CLOUD_RESOURCE)~select~true)))))))~view~&#39;table~columns~(~(~&#39;0~49)~(~&#39;1~17)~(~&#39;2~17)~(~&#39;3~17)))">query</a> to surface all SAS tokens in all your monitored cloud environments. </p></li><li><p><strong>Detect high-privilege SAS tokens:</strong> use the following <a href="https://app.wiz.io/graph#~(control~&#39;wc-id-927~view~&#39;table)">control</a> to detect highly-privileged SAS tokens located on publicly exposed workloads. </p></li><li><p><strong>CSPM rule for blocking SAS tokens:</strong> use the following <a href="https://app.wiz.io/graph#~(query~(type~(~&#39;STORAGE_ACCOUNT)~select~true~relationships~(~(type~(~(type~&#39;ALERTED_ON~reverse~true))~with~(type~(~&#39;CONFIGURATION_FINDING)~select~true~where~(configurationRuleShortName~(EQUALS~(~&#39;StorageAccount-026)))))~(type~(~(type~&#39;CONTAINS~reverse~true))~optional~true~with~(type~(~&#39;SUBSCRIPTION)~select~true)))))">Cloud Configuration Rule</a> to track storage accounts allowing SAS token usage. </p></li></ul><h2><span></span><a id="security-risks-in-the-ai-pipeline-53"></a><strong>Security risks in the AI pipeline</strong></h2><p>As companies embrace AI more widely, it is important for security teams to understand the inherent security risks at each stage of the AI development process. </p><p>The incident detailed in this blog is an example of two of these risks. </p><p>The first is <strong>oversharing of data</strong>.<strong> </strong>Researchers collect and share massive amounts of external and internal data to construct the required training information for their AI models. This poses inherent security risks tied to high-scale data sharing. It is crucial for security teams to define clear guidelines for external sharing of AI datasets. As we’ve seen in this case, separating the public AI data set to a dedicated storage account could’ve limited the exposure. </p><p>The second is the risk of <strong>supply chain attacks</strong>. Due to improper permissions, the public token granted write access to the storage account containing the AI models. As noted above, injecting malicious code into the model files could’ve led to a supply chain attack on other researchers who use the repository’s models. Security teams should review and sanitize AI models from external sources, since they can be used as a remote code execution vector.  </p><h2><span></span><a id="takeaways-59"></a><strong>Takeaways </strong> </h2><p>The simple step of sharing an AI dataset led to a major data leak, containing over 38TB of private data. The root cause was the usage of Account SAS tokens as the sharing mechanism. Due to a lack of monitoring and governance, SAS tokens pose a security risk, and their usage should be as limited as possible. These tokens are very hard to track, as Microsoft does not provide a centralized way to manage them within the Azure portal. In addition, these tokens can be configured to last effectively forever, with no upper limit on their expiry time. Therefore, using Account SAS tokens for external sharing is unsafe and should be avoided. </p><p>In the wider scope, similar incidents can be prevented by granting security teams more visibility into the processes of AI research and development teams. As we see wider adoption of AI models within companies, it’s important to raise awareness of relevant security risks at every step of the AI development process, and make sure the security team works closely with the data science and research teams to ensure proper guardrails are defined.   </p><p>Microsoft&#39;s account of this issue is available on the <a href="https://msrc.microsoft.com/blog/2023/09/microsoft-mitigated-exposure-of-internal-information-in-a-storage-account-due-to-overly-permissive-sas-token/">MSRC blog</a>.</p><h2><span></span><a id="timeline-63"></a><strong>Timeline</strong> </h2><ul><li><p><strong>Jul. 20, 2020</strong> – SAS token first <a href="https://github.com/microsoft/robust-models-transfer/blob/e61568d613c025adfd07f61f2639f3ae78852143/README.md?plain=1#L36">committed</a> to GitHub; expiry set to Oct. 5, 2021 </p></li><li><p><strong>Oct. 6, 2021</strong> – SAS token expiry <a href="https://github.com/microsoft/robust-models-transfer/commit/a9e0e80bcd49bd8651c0b3198c7dc89179b2c0ac">updated</a> to Oct. 6, 2051 </p></li><li><p><strong>Jun. 22, 2023</strong> – Wiz Research finds and reports issue to MSRC </p></li><li><p><strong>Jun. 24, 2023</strong> – SAS token invalidated by Microsoft </p></li><li><p><strong>Jul. 7, 2023</strong> – SAS token <a href="https://github.com/microsoft/robust-models-transfer/commit/c26ebfff3d01bd2a52a6c14febb8b7ea0234431d">replaced</a> on GitHub </p></li><li><p><strong>Aug. 16, 2023</strong> – Microsoft completes internal investigation of potential impact </p></li><li><p><strong>Sep. 18, 2023</strong> – Public disclosure </p></li></ul><h2><span></span><a id="stay-in-touch-65"></a><strong>Stay in touch!</strong> </h2><p>Hi there! We are Hillai Ben-Sasson (<a href="https://twitter.com/hillai">@hillai</a>), Shir Tamari (<a href="https://twitter.com/shirtamari">@shirtamari</a>), Nir Ohfeld (<a href="https://twitter.com/nirohfeld">@nirohfeld</a>), Sagi Tzadik (<a href="https://twitter.com/sagitz_">@sagitz_</a>) and Ronen Shustin (<a href="https://twitter.com/ronenshh">@ronenshh</a>) from the Wiz Research Team. We are a group of veteran white-hat hackers with a single goal: to make the cloud a safer place for everyone. We primarily focus on finding new attack vectors in the cloud and uncovering isolation issues in cloud vendors.</p><p>We would love to hear from you! Feel free to contact us on Twitter or via email: <a href="mailto:research@wiz.io">research@wiz.io</a>. </p></div></div>
  </body>
</html>
