<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Picsart-AI-Research/Text2Video-Zero">Original</a>
    <h1>Text2Video-Zero Code and Weights Released by Picsart AI Research (12G VRAM)</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repository is the official implementation of <a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero</a>.</p>
<p dir="auto"><strong><a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></strong>
</p>
<p dir="auto"><a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Paper</a> | <a href="https://www.dropbox.com/s/uv90mi2z598olsq/Text2Video-Zero.MP4?dl=0" rel="nofollow">Video</a> | <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://www.wildlondon.org.uk/Picsart-AI-Research/Text2Video-Zero/blob/main/__assets__/github/teaser/teaser_final.png"><img src="https://www.wildlondon.org.uk/Picsart-AI-Research/Text2Video-Zero/raw/main/__assets__/github/teaser/teaser_final.png" width="800px"/></a>  
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-news" aria-hidden="true" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>News</h2>
<ul dir="auto">
<li>[03/23/2023] Paper <a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero</a> released!</li>
<li>[03/25/2023] The <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow">first version</a> of our huggingface demo (containing <code>zero-shot text-to-video generation</code> and  <code>Video Instruct Pix2Pix</code>) released!</li>
<li>[03/27/2023] The <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow">full version</a> of our huggingface demo released! Now also included: <code>text and pose conditional video generation</code>, <code>text and canny-edge conditional video generation</code>, and
<code>text, canny-edge and dreambooth conditional video generation</code>.</li>
<li>[03/28/2023] Code for all our generation methods released! We added a new low-memory setup. Minimum required GPU VRAM is currently <strong>12 GB</strong>. It will be further reduced in the upcoming releases.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-requirements" aria-hidden="true" href="#requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Requirements</h3>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>

<h3 tabindex="-1" dir="auto"><a id="user-content-weights" aria-hidden="true" href="#weights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Weights</h3>
<h4 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-pose-guidance" aria-hidden="true" href="#text-to-video-with-pose-guidance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Pose Guidance</h4>
<p dir="auto">Download the pose model weights used in <a href="https://arxiv.org/abs/2302.05543" rel="nofollow">ControlNet</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget -P annotator/ckpts https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth
wget -P annotator/ckpts https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth"><pre>wget -P annotator/ckpts https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth
wget -P annotator/ckpts https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth</pre></div>

<h4 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth</h4>
<p dir="auto">Integrate a <code>SD1.4</code> Dreambooth model into ControlNet using <a href="https://github.com/lllyasviel/ControlNet/discussions/12" data-hovercard-type="discussion" data-hovercard-url="/lllyasviel/ControlNet/discussions/12/hovercard">this</a> procedure. Load the model into <code>models/control_db/</code>. Dreambooth models can be obtained, for instance, from <a href="https://civitai.com" rel="nofollow">CIVITAI</a>.</p>
<p dir="auto">We provide already prepared model files for <code>anime</code> (keyword <code>1girl</code>), <code>arcane style</code> (keyword <code>arcane style</code>) <code>avatar</code> (keyword <code>avatar style</code>) and <code>gta-5 style</code>  (keyword <code>gtav style</code>). To this end, download the model files from <a href="https://drive.google.com/drive/folders/1uwXNjJ-4Ws6pqyjrIWyVPWu_u4aJrqt8?usp=share_link" rel="nofollow">google drive</a> and extract them into <code>models/control_db/</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-inference-api" aria-hidden="true" href="#inference-api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference API</h2>
<p dir="auto">To run inferences create an instance of <code>Model</code> class</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from model import Model

model = Model(device = &#34;cuda&#34;, dtype = torch.float16)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>model</span> <span>import</span> <span>Model</span>

<span>model</span> <span>=</span> <span>Model</span>(<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span>, <span>dtype</span> <span>=</span> <span>torch</span>.<span>float16</span>)</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video" aria-hidden="true" href="#text-to-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video</h3>
<p dir="auto">To directly call our text-to-video generator, run this python command which stores the result in <code>tmp/text2video/A_horse_galloping_on_a_street.mp4</code> :</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#34;A horse galloping on a street&#34;
params = {&#34;t0&#34;: 44, &#34;t1&#34;: 47 , &#34;motion_field_strength_x&#34; : 12, &#34;motion_field_strength_y&#34; : 12, &#34;video_length&#34;: 8}

out_path, fps = Path(f&#34;tmp/text2video/{prompt.replace(&#39; &#39;,&#39;_&#39;)}.mp4&#34;), 4
if not out_path.parent.exists():
  out_path.parent.mkdir(parents=True)
model.process_text2video(prompt, fps = fps, path = out_path.as_posix(), **params)"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#34;A horse galloping on a street&#34;</span>
<span>params</span> <span>=</span> {<span>&#34;t0&#34;</span>: <span>44</span>, <span>&#34;t1&#34;</span>: <span>47</span> , <span>&#34;motion_field_strength_x&#34;</span> : <span>12</span>, <span>&#34;motion_field_strength_y&#34;</span> : <span>12</span>, <span>&#34;video_length&#34;</span>: <span>8</span>}

<span>out_path</span>, <span>fps</span> <span>=</span> <span>Path</span>(<span>f&#34;tmp/text2video/<span><span>{</span><span>prompt</span>.<span>replace</span>(<span>&#39; &#39;</span>,<span>&#39;_&#39;</span>)<span>}</span></span>.mp4&#34;</span>), <span>4</span>
<span>if</span> <span>not</span> <span>out_path</span>.<span>parent</span>.<span>exists</span>():
  <span>out_path</span>.<span>parent</span>.<span>mkdir</span>(<span>parents</span><span>=</span><span>True</span>)
<span>model</span>.<span>process_text2video</span>(<span>prompt</span>, <span>fps</span> <span>=</span> <span>fps</span>, <span>path</span> <span>=</span> <span>out_path</span>.<span>as_posix</span>(), <span>**</span><span>params</span>)</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-hyperparameters-optional" aria-hidden="true" href="#hyperparameters-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hyperparameters (Optional)</h4>
<p dir="auto">You can define the following hyperparameters:</p>
<ul dir="auto">
<li>
<strong>Motion field strength</strong>:   <code>motion_field_strength_x</code> = <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$\delta_x$</math-renderer>  and <code>motion_field_strength_y</code> = <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$\delta_x$</math-renderer> (see our paper, Sect. 3.3.1). Default: <code>motion_field_strength_x=motion_field_strength_y= 12</code>.</li>
<li>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$T$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$T&#39;$</math-renderer> (see our paper, Sect. 3.3.1). Define values <code>t0</code> and <code>t1</code> in the range <code>{0,...,50}</code>. Default: <code>t0=44</code>, <code>t1=47</code> (DDIM steps). Corresponds to timesteps <code>881</code> and <code>941</code>, respectively.</li>
<li>
<strong>Video length</strong>: Define the number of frames <code>video_length</code> to be generated. Default: <code>video_length=8</code>.</li>
</ul>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-pose-control" aria-hidden="true" href="#text-to-video-with-pose-control"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Pose Control</h3>
<p dir="auto">To directly call our text-to-video generator with pose control, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#39;an astronaut dancing in outer space&#39;
motion_path = Path(&#39;__assets__/poses_skeleton_gifs/dance1_corr.mp4&#39;)
out_path = Path(f&#39;./{prompt}.gif&#39;)
model.process_controlnet_pose(motion_path.as_posix(), prompt=prompt, save_path=out_path.as_posix())"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#39;an astronaut dancing in outer space&#39;</span>
<span>motion_path</span> <span>=</span> <span>Path</span>(<span>&#39;__assets__/poses_skeleton_gifs/dance1_corr.mp4&#39;</span>)
<span>out_path</span> <span>=</span> <span>Path</span>(<span>f&#39;./<span><span>{</span><span>prompt</span><span>}</span></span>.gif&#39;</span>)
<span>model</span>.<span>process_controlnet_pose</span>(<span>motion_path</span>.<span>as_posix</span>(), <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>.<span>as_posix</span>())</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-control" aria-hidden="true" href="#text-to-video-with-edge-control"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Control</h3>
<p dir="auto">To directly call our text-to-video generator with edge control, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#39;oil painting of a deer, a high-quality, detailed, and professional photo&#39;
video_path = Path(&#39;__assets__/canny_videos_mp4/deer.mp4&#39;)
out_path = Path(f&#39;./{prompt}.mp4&#39;)
model.process_controlnet_canny(video_path.as_posix(), prompt=prompt, save_path=out_path.as_posix())"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#39;oil painting of a deer, a high-quality, detailed, and professional photo&#39;</span>
<span>video_path</span> <span>=</span> <span>Path</span>(<span>&#39;__assets__/canny_videos_mp4/deer.mp4&#39;</span>)
<span>out_path</span> <span>=</span> <span>Path</span>(<span>f&#39;./<span><span>{</span><span>prompt</span><span>}</span></span>.mp4&#39;</span>)
<span>model</span>.<span>process_controlnet_canny</span>(<span>video_path</span>.<span>as_posix</span>(), <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>.<span>as_posix</span>())</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-hyperparameters" aria-hidden="true" href="#hyperparameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hyperparameters</h4>
<p dir="auto">You can define the following hyperparameters for Canny edge detection:</p>
<ul dir="auto">
<li>
<strong>low threshold</strong>. Define value <code>low_threshold</code> in the range <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$(0, 255)$</math-renderer>. Default: <code>low_threshold=100</code>.</li>
<li>
<strong>high threshold</strong>. Define value <code>high_threshold</code> in the range <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="e14a7564693efcdf1571017bb30d703b">$(0, 255)$</math-renderer>. Default: <code>high_threshold=200</code>. Make sure that <code>high_threshold</code> &gt; <code>low_threshold</code>.</li>
</ul>
<p dir="auto">You can give hyperparameters as arguments to <code>model.process_controlnet_canny</code></p>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth-specialization" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth-specialization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth specialization</h3>
<p dir="auto">Load a dreambooth model then proceed as described in <code>Text-To-Video with Edge Guidance</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#39;your prompt&#39;
video_path = Path(&#39;path/to/your/video&#39;)
dreambooth_model_path = Path(&#39;path/to/your/dreambooth/model&#39;)
out_path = Path(f&#39;./{prompt}.gif&#39;)
model.process_controlnet_canny_db(dreambooth_model_path.as_posix(), video_path.as_posix(), prompt=prompt, save_path=out_path.as_posix())"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#39;your prompt&#39;</span>
<span>video_path</span> <span>=</span> <span>Path</span>(<span>&#39;path/to/your/video&#39;</span>)
<span>dreambooth_model_path</span> <span>=</span> <span>Path</span>(<span>&#39;path/to/your/dreambooth/model&#39;</span>)
<span>out_path</span> <span>=</span> <span>Path</span>(<span>f&#39;./<span><span>{</span><span>prompt</span><span>}</span></span>.gif&#39;</span>)
<span>model</span>.<span>process_controlnet_canny_db</span>(<span>dreambooth_model_path</span>.<span>as_posix</span>(), <span>video_path</span>.<span>as_posix</span>(), <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>.<span>as_posix</span>())</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-video-instruct-pix2pix" aria-hidden="true" href="#video-instruct-pix2pix"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Video Instruct-Pix2Pix</h3>
<p dir="auto">To perform pix2pix video editing, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#39;make it Van Gogh Starry Night&#39;
video_path = Path(&#39;__assets__/pix2pix video/camel.mp4&#39;)
out_path = Path(f&#39;./{prompt}.mp4&#39;)
model.process_pix2pix(video_path.as_posix(), prompt=prompt, save_path=out_path.as_posix())"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#39;make it Van Gogh Starry Night&#39;</span>
<span>video_path</span> <span>=</span> <span>Path</span>(<span>&#39;__assets__/pix2pix video/camel.mp4&#39;</span>)
<span>out_path</span> <span>=</span> <span>Path</span>(<span>f&#39;./<span><span>{</span><span>prompt</span><span>}</span></span>.mp4&#39;</span>)
<span>model</span>.<span>process_pix2pix</span>(<span>video_path</span>.<span>as_posix</span>(), <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>.<span>as_posix</span>())</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-low-memory-inference" aria-hidden="true" href="#low-memory-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Low Memory Inference</h3>
<p dir="auto">Each of the above introduced interface can be run in a low memory setup. In the minimal setup, a GPU with <strong>12 GB VRAM</strong> is sufficient.</p>
<p dir="auto">To reduce the memory usage, add <code>chunk_size=k</code> as additional parameter when calling one of the above defined inference APIs. The integer value <code>k</code> must be in the range <code>{2,...,video_length}</code>. It defines the number of frames that are processed at once (without any loss in quality). The lower the value the less memory is needed.</p>
<p dir="auto">We plan to release soon a new version that further reduces the memory usage.</p>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-ablation-study" aria-hidden="true" href="#ablation-study"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Ablation Study</h3>
<p dir="auto">To replicate the ablation study, add additional parameters when calling the above defined inference APIs.</p>
<ul dir="auto">
<li>To deactivate <code>cross-frame attention</code>: Add <code>use_cf_attn=False</code> to the parameter list.</li>
<li>To deactivate enriching latent codes with <code>motion dynamics</code>: Add <code>use_motion_field=False</code> to the parameter list.</li>
</ul>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-inference-using-gradio" aria-hidden="true" href="#inference-using-gradio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference using Gradio</h2>
<p dir="auto">From the project root folder, run this shell command:</p>

<p dir="auto">Then access the app <a href="http://127.0.0.1:7860" rel="nofollow">locally</a> with a browser.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-results" aria-hidden="true" href="#results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Results</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-1" aria-hidden="true" href="#text-to-video-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-pose-guidance-1" aria-hidden="true" href="#text-to-video-with-pose-guidance-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Pose Guidance</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance" aria-hidden="true" href="#text-to-video-with-edge-guidance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth-specialization-1" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth-specialization-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth specialization</h3>

<h2 tabindex="-1" dir="auto"><a id="user-content-video-instruct-pix2pix-1" aria-hidden="true" href="#video-instruct-pix2pix-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Video Instruct Pix2Pix</h2>

<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">Our code is published under the CreativeML Open RAIL-M license. The license provided in this repository applies to all additions and contributions we make upon the original stable diffusion code. The original stable diffusion code is under the CreativeML Open RAIL-M license, which can found <a href="https://github.com/CompVis/stable-diffusion/blob/main/LICENSE">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-bibtex" aria-hidden="true" href="#bibtex"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BibTeX</h2>
<p dir="auto">If you use our work in your research, please cite our publication:</p>
<div data-snippet-clipboard-copy-content="@article{text2video-zero,
    title={Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:2303.13439},
    year={2023}
}"><pre><code>@article{text2video-zero,
    title={Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:2303.13439},
    year={2023}
}
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
