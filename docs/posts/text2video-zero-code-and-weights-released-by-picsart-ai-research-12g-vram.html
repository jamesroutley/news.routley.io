<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Picsart-AI-Research/Text2Video-Zero">Original</a>
    <h1>Text2Video-Zero Code and Weights Released by Picsart AI Research (12G VRAM)</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repository is the official implementation of <a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero</a>.</p>
<p dir="auto"><strong><a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></strong>
</p>
<p dir="auto"><a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Paper</a> | <a href="https://www.dropbox.com/s/uv90mi2z598olsq/Text2Video-Zero.MP4?dl=0" rel="nofollow">Video</a> | <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Picsart-AI-Research/Text2Video-Zero/blob/main/__assets__/github/teaser/teaser_final.png"><img src="https://github.com/Picsart-AI-Research/Text2Video-Zero/raw/main/__assets__/github/teaser/teaser_final.png" width="800px"/></a>  
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-news" aria-hidden="true" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>News</h2>
<ul dir="auto">
<li>[03/23/2023] Paper <a href="https://arxiv.org/abs/2303.13439" rel="nofollow">Text2Video-Zero</a> released!</li>
<li>[03/25/2023] The <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow">first version</a> of our huggingface demo (containing <code>zero-shot text-to-video generation</code> and  <code>Video Instruct Pix2Pix</code>) released!</li>
<li>[03/27/2023] The <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow">full version</a> of our huggingface demo released! Now also included: <code>text and pose conditional video generation</code>, <code>text and canny-edge conditional video generation</code>, and
<code>text, canny-edge and dreambooth conditional video generation</code>.</li>
<li>[03/28/2023] Code for all our generation methods released! We added a new low-memory setup. Minimum required GPU VRAM is currently <strong>12 GB</strong>. It will be further reduced in the upcoming releases.</li>
<li>[03/29/2023] Improved <a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" rel="nofollow">Huggingface demo</a>! (i) For text-to-video generation, any base model for stable diffusion hosted on huggingface can now be loaded (including dreambooth models!). (ii) The generated videos can have arbitrary length. (iii) We improved the quality of Video Instruct-Pix2Pix. (iv) We added two longer examples for Video Instruct-Pix2Pix.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-contribute" aria-hidden="true" href="#contribute"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contribute</h2>
<p dir="auto">We are on a journey to democratize AI and empower the creativity of everyone, and we believe Text2Video-Zero is a great research direction to unleash the zero-shot video generation and editing capacity of the amazing text-to-image models!</p>
<p dir="auto">To achieve this goal, all contributions are welcome. Please check out these external implementations and extensions of Text2Video-Zero. We thank the authors for their efforts and contributions:</p>
<ul dir="auto">
<li><a href="https://github.com/JiauZhang/Text2Video-Zero">https://github.com/JiauZhang/Text2Video-Zero</a></li>
<li><a href="https://github.com/camenduru/text2video-zero-colab">https://github.com/camenduru/text2video-zero-colab</a></li>
<li><a href="https://github.com/SHI-Labs/Text2Video-Zero-sd-webui">https://github.com/SHI-Labs/Text2Video-Zero-sd-webui</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h2>
<ol dir="auto">
<li>Clone this repository and enter:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Picsart-AI-Research/Text2Video-Zero.git
cd Text2Video-Zero/"><pre>git clone https://github.com/Picsart-AI-Research/Text2Video-Zero.git
<span>cd</span> Text2Video-Zero/</pre></div>
<ol start="2" dir="auto">
<li>Install requirements using Python 3.9</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="virtualenv --system-site-packages -p python3.9 venv
source venv/bin/activate
pip install -r requirements.txt"><pre>virtualenv --system-site-packages -p python3.9 venv
<span>source</span> venv/bin/activate
pip install -r requirements.txt</pre></div>

<h4 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth</h4>
<p dir="auto">Integrate a <code>SD1.4</code> Dreambooth model into ControlNet using <a href="https://github.com/lllyasviel/ControlNet/discussions/12" data-hovercard-type="discussion" data-hovercard-url="/lllyasviel/ControlNet/discussions/12/hovercard">this</a> procedure. Load the model into <code>models/control_db/</code>. Dreambooth models can be obtained, for instance, from <a href="https://civitai.com" rel="nofollow">CIVITAI</a>.</p>
<p dir="auto">We provide already prepared model files derived from CIVITAI for <code>anime</code> (keyword <code>1girl</code>), <code>arcane style</code> (keyword <code>arcane style</code>) <code>avatar</code> (keyword <code>avatar style</code>) and <code>gta-5 style</code>  (keyword <code>gtav style</code>).</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-inference-api" aria-hidden="true" href="#inference-api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference API</h2>
<p dir="auto">To run inferences create an instance of <code>Model</code> class</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from model import Model

model = Model(device = &#34;cuda&#34;, dtype = torch.float16)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>model</span> <span>import</span> <span>Model</span>

<span>model</span> <span>=</span> <span>Model</span>(<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span>, <span>dtype</span> <span>=</span> <span>torch</span>.<span>float16</span>)</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video" aria-hidden="true" href="#text-to-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video</h3>
<p dir="auto">To directly call our text-to-video generator, run this python command which stores the result in <code>tmp/text2video/A_horse_galloping_on_a_street.mp4</code> :</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &#34;A horse galloping on a street&#34;
params = {&#34;t0&#34;: 44, &#34;t1&#34;: 47 , &#34;motion_field_strength_x&#34; : 12, &#34;motion_field_strength_y&#34; : 12, &#34;video_length&#34;: 8}

out_path, fps = f&#34;./text2video_{prompt.replace(&#39; &#39;,&#39;_&#39;)}.mp4&#34;, 4
model.process_text2video(prompt, fps = fps, path = out_path, **params)"><pre><span>prompt</span> <span>=</span> <span>&#34;A horse galloping on a street&#34;</span>
<span>params</span> <span>=</span> {<span>&#34;t0&#34;</span>: <span>44</span>, <span>&#34;t1&#34;</span>: <span>47</span> , <span>&#34;motion_field_strength_x&#34;</span> : <span>12</span>, <span>&#34;motion_field_strength_y&#34;</span> : <span>12</span>, <span>&#34;video_length&#34;</span>: <span>8</span>}

<span>out_path</span>, <span>fps</span> <span>=</span> <span>f&#34;./text2video_<span><span>{</span><span>prompt</span>.<span>replace</span>(<span>&#39; &#39;</span>,<span>&#39;_&#39;</span>)<span>}</span></span>.mp4&#34;</span>, <span>4</span>
<span>model</span>.<span>process_text2video</span>(<span>prompt</span>, <span>fps</span> <span>=</span> <span>fps</span>, <span>path</span> <span>=</span> <span>out_path</span>, <span>**</span><span>params</span>)</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-hyperparameters-optional" aria-hidden="true" href="#hyperparameters-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hyperparameters (Optional)</h4>
<p dir="auto">You can define the following hyperparameters:</p>
<ul dir="auto">
<li>
<strong>Motion field strength</strong>:   <code>motion_field_strength_x</code> = <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$\delta_x$</math-renderer>  and <code>motion_field_strength_y</code> = <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$\delta_x$</math-renderer> (see our paper, Sect. 3.3.1). Default: <code>motion_field_strength_x=motion_field_strength_y= 12</code>.</li>
<li>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$T$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$T&#39;$</math-renderer> (see our paper, Sect. 3.3.1). Define values <code>t0</code> and <code>t1</code> in the range <code>{0,...,50}</code>. Default: <code>t0=44</code>, <code>t1=47</code> (DDIM steps). Corresponds to timesteps <code>881</code> and <code>941</code>, respectively.</li>
<li>
<strong>Video length</strong>: Define the number of frames <code>video_length</code> to be generated. Default: <code>video_length=8</code>.</li>
</ul>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-pose-control" aria-hidden="true" href="#text-to-video-with-pose-control"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Pose Control</h3>
<p dir="auto">To directly call our text-to-video generator with pose control, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path

prompt = &#39;an astronaut dancing in outer space&#39;
motion_path = &#39;__assets__/poses_skeleton_gifs/dance1_corr.mp4&#39;
out_path = f&#34;./text2video_pose_guidance_{prompt.replace(&#39; &#39;,&#39;_&#39;)}.gif&#34;
model.process_controlnet_pose(motion_path, prompt=prompt, save_path=out_path)"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>

<span>prompt</span> <span>=</span> <span>&#39;an astronaut dancing in outer space&#39;</span>
<span>motion_path</span> <span>=</span> <span>&#39;__assets__/poses_skeleton_gifs/dance1_corr.mp4&#39;</span>
<span>out_path</span> <span>=</span> <span>f&#34;./text2video_pose_guidance_<span><span>{</span><span>prompt</span>.<span>replace</span>(<span>&#39; &#39;</span>,<span>&#39;_&#39;</span>)<span>}</span></span>.gif&#34;</span>
<span>model</span>.<span>process_controlnet_pose</span>(<span>motion_path</span>, <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>)</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-control" aria-hidden="true" href="#text-to-video-with-edge-control"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Control</h3>
<p dir="auto">To directly call our text-to-video generator with edge control, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &#39;oil painting of a deer, a high-quality, detailed, and professional photo&#39;
video_path = &#39;__assets__/canny_videos_mp4/deer.mp4&#39;
out_path = f&#39;./text2video_edge_guidance_{prompt}.mp4&#39;
model.process_controlnet_canny(video_path, prompt=prompt, save_path=out_path)"><pre><span>prompt</span> <span>=</span> <span>&#39;oil painting of a deer, a high-quality, detailed, and professional photo&#39;</span>
<span>video_path</span> <span>=</span> <span>&#39;__assets__/canny_videos_mp4/deer.mp4&#39;</span>
<span>out_path</span> <span>=</span> <span>f&#39;./text2video_edge_guidance_<span><span>{</span><span>prompt</span><span>}</span></span>.mp4&#39;</span>
<span>model</span>.<span>process_controlnet_canny</span>(<span>video_path</span>, <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>)</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-hyperparameters" aria-hidden="true" href="#hyperparameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hyperparameters</h4>
<p dir="auto">You can define the following hyperparameters for Canny edge detection:</p>
<ul dir="auto">
<li>
<strong>low threshold</strong>. Define value <code>low_threshold</code> in the range <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$(0, 255)$</math-renderer>. Default: <code>low_threshold=100</code>.</li>
<li>
<strong>high threshold</strong>. Define value <code>high_threshold</code> in the range <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="09b9efe3e1fbfed104f8a3a5fd9b00b1">$(0, 255)$</math-renderer>. Default: <code>high_threshold=200</code>. Make sure that <code>high_threshold</code> &gt; <code>low_threshold</code>.</li>
</ul>
<p dir="auto">You can give hyperparameters as arguments to <code>model.process_controlnet_canny</code></p>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth-specialization" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth-specialization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth specialization</h3>
<p dir="auto">Load a dreambooth model then proceed as described in <code>Text-To-Video with Edge Guidance</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="
prompt = &#39;your prompt&#39;
video_path = &#39;path/to/your/video&#39;
dreambooth_model_path = &#39;path/to/your/dreambooth/model&#39;
out_path = f&#39;./text2video_edge_db_{prompt}.gif&#39;
model.process_controlnet_canny_db(dreambooth_model_path, video_path, prompt=prompt, save_path=out_path)"><pre><span>prompt</span> <span>=</span> <span>&#39;your prompt&#39;</span>
<span>video_path</span> <span>=</span> <span>&#39;path/to/your/video&#39;</span>
<span>dreambooth_model_path</span> <span>=</span> <span>&#39;path/to/your/dreambooth/model&#39;</span>
<span>out_path</span> <span>=</span> <span>f&#39;./text2video_edge_db_<span><span>{</span><span>prompt</span><span>}</span></span>.gif&#39;</span>
<span>model</span>.<span>process_controlnet_canny_db</span>(<span>dreambooth_model_path</span>, <span>video_path</span>, <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>)</pre></div>
<p dir="auto">The value <code>video_path</code> can be the path to a <code>mp4</code> file. To use one of the example videos provided, set <code>video_path=&#34;woman1&#34;</code>, <code>video_path=&#34;woman2&#34;</code>, <code>video_path=&#34;woman3&#34;</code>, or <code>video_path=&#34;man1&#34;</code>.</p>
<p dir="auto">The value <code>dreambooth_model_path</code> can either be a link to a diffuser model file, or the name of one of the dreambooth models provided. To this end, set <code>dreambooth_model_path = &#34;Anime DB&#34;</code>, <code>dreambooth_model_path = &#34;Avatar DB&#34;</code>, <code>dreambooth_model_path = &#34;GTA-5 DB&#34;</code>, or <code>dreambooth_model_path = &#34;Arcane DB&#34;</code>.  The corresponding keywords are: <code>1girl</code> (for <code>Anime DB</code>), <code>arcane style</code> (for <code>Arcane DB</code>) <code>avatar style</code> (for <code>Avatar DB</code>) and <code>gta-5 style</code>  (for <code>GTA-5 DB</code>).</p>
<p dir="auto">If the model file is not in diffuser format, it must be <a href="https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py">converted</a>.</p>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-video-instruct-pix2pix" aria-hidden="true" href="#video-instruct-pix2pix"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Video Instruct-Pix2Pix</h3>
<p dir="auto">To perform pix2pix video editing, run this python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &#39;make it Van Gogh Starry Night&#39;
video_path = &#39;__assets__/pix2pix video/camel.mp4&#39;
out_path = f&#39;./video_instruct_pix2pix_{prompt}.mp4&#39;
model.process_pix2pix(video_path, prompt=prompt, save_path=out_path)"><pre><span>prompt</span> <span>=</span> <span>&#39;make it Van Gogh Starry Night&#39;</span>
<span>video_path</span> <span>=</span> <span>&#39;__assets__/pix2pix video/camel.mp4&#39;</span>
<span>out_path</span> <span>=</span> <span>f&#39;./video_instruct_pix2pix_<span><span>{</span><span>prompt</span><span>}</span></span>.mp4&#39;</span>
<span>model</span>.<span>process_pix2pix</span>(<span>video_path</span>, <span>prompt</span><span>=</span><span>prompt</span>, <span>save_path</span><span>=</span><span>out_path</span>)</pre></div>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-low-memory-inference" aria-hidden="true" href="#low-memory-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Low Memory Inference</h3>
<p dir="auto">Each of the above introduced interface can be run in a low memory setup. In the minimal setup, a GPU with <strong>12 GB VRAM</strong> is sufficient.</p>
<p dir="auto">To reduce the memory usage, add <code>chunk_size=k</code> as additional parameter when calling one of the above defined inference APIs. The integer value <code>k</code> must be in the range <code>{2,...,video_length}</code>. It defines the number of frames that are processed at once (without any loss in quality). The lower the value the less memory is needed.</p>
<p dir="auto">When using the gradio app, set <code>chunk_size</code> in the <code>Advanced options</code>.</p>
<p dir="auto">We plan to release soon a new version that further reduces the memory usage.</p>
<hr/>
<h3 tabindex="-1" dir="auto"><a id="user-content-ablation-study" aria-hidden="true" href="#ablation-study"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Ablation Study</h3>
<p dir="auto">To replicate the ablation study, add additional parameters when calling the above defined inference APIs.</p>
<ul dir="auto">
<li>To deactivate <code>cross-frame attention</code>: Add <code>use_cf_attn=False</code> to the parameter list.</li>
<li>To deactivate enriching latent codes with <code>motion dynamics</code>: Add <code>use_motion_field=False</code> to the parameter list.</li>
</ul>
<p dir="auto">Note: Adding <code>smooth_bg=True</code> activates background smoothing. However, our  code does not include the salient object detector necessary to run that code.</p>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-inference-using-gradio" aria-hidden="true" href="#inference-using-gradio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference using Gradio</h2>
<p dir="auto">From the project root folder, run this shell command:</p>

<p dir="auto">Then access the app <a href="http://127.0.0.1:7860" rel="nofollow">locally</a> with a browser.</p>
<p dir="auto">To access the app remotely, run this shell command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python app.py --public_access"><pre>python app.py --public_access</pre></div>
<p dir="auto">For security information about public access we refer to the documentation of gradio [<a href="https://gradio.app/sharing-your-app/#security-and-file-access" rel="nofollow">https://gradio.app/sharing-your-app/#security-and-file-access</a>].</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-results" aria-hidden="true" href="#results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Results</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-1" aria-hidden="true" href="#text-to-video-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-pose-guidance" aria-hidden="true" href="#text-to-video-with-pose-guidance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Pose Guidance</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance" aria-hidden="true" href="#text-to-video-with-edge-guidance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance</h3>

<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-video-with-edge-guidance-and-dreambooth-specialization-1" aria-hidden="true" href="#text-to-video-with-edge-guidance-and-dreambooth-specialization-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-To-Video with Edge Guidance and Dreambooth specialization</h3>

<h2 tabindex="-1" dir="auto"><a id="user-content-video-instruct-pix2pix-1" aria-hidden="true" href="#video-instruct-pix2pix-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Video Instruct Pix2Pix</h2>

<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">Our code is published under the CreativeML Open RAIL-M license. The license provided in this repository applies to all additions and contributions we make upon the original stable diffusion code. The original stable diffusion code is under the CreativeML Open RAIL-M license, which can found <a href="https://github.com/CompVis/stable-diffusion/blob/main/LICENSE">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-bibtex" aria-hidden="true" href="#bibtex"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BibTeX</h2>
<p dir="auto">If you use our work in your research, please cite our publication:</p>
<div data-snippet-clipboard-copy-content="@article{text2video-zero,
    title={Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:2303.13439},
    year={2023}
}"><pre><code>@article{text2video-zero,
    title={Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:2303.13439},
    year={2023}
}
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
