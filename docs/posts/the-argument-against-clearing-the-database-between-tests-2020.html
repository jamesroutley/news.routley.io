<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://calpaterson.com/against-database-teardown.html">Original</a>
    <h1>The argument against clearing the database between tests (2020)</h1>
    
    <div id="readability-page-1" class="page"><div>
        <article>
            
            <p><time datetime="2020-04-23T00:00:00Z">April 2020</time></p>
            <p id="article-description">Some reasons why you might
            not want to remove data from the database between automated tests: speed,
            correctness, data growth issues and parallelism advantages</p>
            <p>I&#39;m of the school of thought that most useful &#34;unit&#34;<sup id="fnref:whatever"><a href="#fn:whatever">1</a></sup>
            tests <a href="https://dhh.dk/2014/slow-database-test-fallacy.html">should
            involve the database</a>. Consequently I don&#39;t <a href="https://martinfowler.com/articles/mocksArentStubs.html">mock out, fake or
            stub</a> the database in tests that I write.</p>
            <p>On every project, I had a small piece of test harness code that cleans
            the database between tests<sup id="fnref:before"><a href="#fn:before">2</a></sup>:</p>
            <div>
                <pre><span></span><span>@before_test</span>
<span>def</span> <span>clean_session</span><span>:</span>
    <span>for</span> <span>table</span> <span>in</span> <span>all_tables</span><span>():</span>
        <span>db</span><span>.</span><span>session</span><span>.</span><span>execute</span><span>(</span><span>&#34;truncate </span><span>%s</span><span>;&#34;</span> <span>%</span> <span>table</span><span>.</span><span>name</span><span>)</span>
    <span>db</span><span>.</span><span>session</span><span>.</span><span>commit</span><span>()</span>
    <span>return</span> <span>db</span><span>.</span><span>session</span>
</pre>
            </div>
            <p>The reason for this was that it seemed <em>obvious</em> that each test
            should start with a completely clean slate in order to make it a fair test.
            All other data should be deleted so that nothing from other tests that can
            get conflated and cause the test to go wrong somehow - either spuriously
            passing or failing.</p>
            <p>Recently I&#39;ve come to the conclusion that it can (but not always) make
            sense to run the tests with a dirty database - not only not cleaning
            between individual tests but also not cleaning between whole test runs -
            and keeping all the old test data around on a (near)<sup id="fnref:near"><a href="#fn:near">3</a></sup> permanent
            basis.</p>
            <h2>The required mentality change</h2>
            <p>Upturning the base assumption that &#34;the database is clean&#34; in a test
            requires a small adjustment to your mindset.</p>
            <p>You can&#39;t write tests that assume that data it has created is the only
            thing present, such as this one:</p>
            <div>
                <pre><span></span><span>def</span> <span>test_adding_a_user</span><span>():</span>
    <span>db_session</span><span>.</span><span>add</span><span>(</span><span>user</span><span>)</span>
    <span># won&#39;t work, assumes the only user is the one added above</span>
    <span>assert</span> <span>db_session</span><span>.</span><span>query</span><span>(</span><span>User</span><span>)</span><span>.</span><span>count</span><span>()</span> <span>==</span> <span>1</span>
</pre>
            </div>
            <p>Instead each test needs to be written to assert only on the basis of
            data it has created (or has not created). It should handle data created by
            others. A corrected example:</p>
            <div>
                <pre><span></span><span>def</span> <span>test_adding_a_user_2</span><span>(</span><span>session</span><span>):</span>
    <span>user</span> <span>=</span> <span>make_user</span><span>()</span>
    <span>db_session</span><span>.</span><span>add</span><span>(</span><span>user</span><span>)</span>
    <span># this is safe, doesn&#39;t assume no other users exist</span>
    <span>assert</span> <span>db_session</span><span>.</span><span>query</span><span>(</span><span>User</span><span>)</span><span>.</span><span>get</span><span>(</span><span>user</span><span>.</span><span>user_id</span><span>)</span> <span>is</span> <span>not</span> <span>None</span>
</pre>
            </div>
            <p>This isn&#39;t an easy change to make in existing tests. The assumption of
            clean data is tricky to refactor away. There are reasons to consider it
            though.</p>
            <h2>The advantages</h2>
            <h3>It&#39;s quicker</h3>
            <p>Tearing down data between tests or schemas between test runs is not free
            of (computational) charge.</p>
            <p>The time taken to clean the database is usually proportional to the
            number of tables and while this cost is small to begin with it can grow
            over time<sup id="fnref:delete-tip"><a href="#fn:delete-tip">4</a></sup>.</p>
            <p>When you have a large number of tests this per-test overhead becomes a
            problem. My personal experience is that the problem starts to get serious
            when you have around one hundred tests. For the typical test suite tearing
            down data can take anywhere from one to ten percent of total runtime,
            depending on how efficiently it&#39;s done.</p>
            <p>There are ways to be quicker, here are a few:</p>
            <ul>
                <li>avoid tearing down/recreating static data such as lookup
                tables</li>
                <li>track which tables have been touched and don&#39;t clean untouched
                ones</li>
                <li>turn off crash safety while testing</li>
            </ul>
            <p>That all aside, the fact remains that tearing down the database is never
            as fast as not tearing it down.</p>
            <h3>More realistic data shape</h3>
            <p>One perennial problem with code that uses data is that when the volume
            of data grows the performance can change considerably: when there are just
            three rows in a table a logarithmic time operation (fast) is
            indistinguishable from a polynomial time operation (slow) <sup id="fnref:n"><a href="#fn:n">5</a></sup>.</p>
            <p>It&#39;s a sad fact that the majority of tests and indeed most development
            time is spent with the database in an empty or nearly-empty state. As a
            result there is a loss of feedback. Without the daily experience of running
            with realistic data sets, detecting a slow data access pattern requires
            thoughtful analysis and/or experience.</p>
            <p>When your tests don&#39;t clean the database your test database will slowly
            fill with data which is, although not a complete match with the shape of
            production, presumably along similar lines. Sometimes you will notice
            problem queries before finding out in production and without doing
            analysis.</p>
            <p>Better yet, when your tests don&#39;t assume they&#39;re starting from a clean
            sheet, you can run your tests with a dump from production loaded into your
            database. This can help confirm that data access patterns you&#39;re using will
            work when the database, as a whole, is at production size.</p>
            <h3>More realistic data</h3>
            <p>Testing-by-example (as opposed to <a href="https://hypothesis.readthedocs.io/en/latest/quickstart.html">property-based
            testing</a>) is based on the programmer coming up with examples of inputs
            and asserting that when given those inputs the program produces the right
            outputs - or at least - the right sort of outputs. There is also the
            implicit assertion (present in all automated tests) that the program does
            not raise an exception or crash.</p>
            <p>Hopefully, at least some of the residual data your tests leave behind is
            highly contrived and contains lots of &#34;bad&#34; data and error cases. For
            example: users that have started the order process but who haven&#39;t gotten
            as far as entering their email address, users who have been marked as
            duplicates, users whose names contain semi-colons and so on.</p>
            <p>Running your tests in the presence of all this realistic wonky data can
            help tease out real bugs. If one of your tests inserts customer names based
            on the <a href="https://github.com/minimaxir/big-list-of-naughty-strings">Big List of
            Naughty Strings</a> then you might find more bugs in other areas when other
            tests exercise different parts of the same system.</p>
            <p>Of course - you shouldn&#39;t rely on such &#34;atmospheric&#34; bad data as an aid
            to correctness. Each time you find a new bug based on bad data left lying
            around from another test or loaded in from production a new, specific, test
            should be added for that condition. However having a load of crap data
            loaded can help uncover issues that might not otherwise have been uncovered
            at the development stage.</p>
            <h3>Test parallelism becomes a smaller step</h3>
            <p>Test parallelism is often discussed but my experience is that relatively
            few projects ever implement it, even though quite a few would benefit.</p>
            <p>The problem is as follows. At first your tests are fast because there
            aren&#39;t many of them and so most teams put off parallel tests until &#34;later&#34;.
            When &#34;later&#34; arrives, the build is now slow and test parallelism would help
            but it&#39;s usually not easy to adapt existing serial tests to run in
            parallel.</p>
            <p>This is because tests typically manipulate state in odd ways and if each
            test assumes they are the only process manipulating state they tend to need
            complete isolation to be able to run in parallel - separate database
            instances, separate <a href="https://min.io/">S3 test doubles</a> - even
            separate filesystems occasionally. Complete isolation is expense, hassle
            and more moving parts (8 SQL databases for an 8-process test suite is no
            fun).</p>
            <p>Tests written to assume the presence of irrelevant data are much easier
            to parallelise - usually it can all be done within one environment. This
            makes it easier to do and so much more likely to happen.</p>
            <h2>Disadvantages (and some mitigations)</h2>
            <h3>Debugging is harder</h3>
            <p>When a test is failing for a reason that isn&#39;t understood the debugging
            method is simple: run only that test, in isolation, and narrow it down
            until the issue is understood.</p>
            <p>This is more difficult when the database is full of background data -
            you aren&#39;t starting from a &#34;clean&#34; state. I don&#39;t think this is an
            insurmountable problem - in this case, just change to running with a clean
            database until you can diagnose the problem. Perhaps it&#39;s worth backing up
            the original dataset so you can refer to it later.</p>
            <p>Running tests designed for &#34;dirty&#34; datasets with clean datasets is not a
            problem - it&#39;s easy to switch back to using a clean dataset. Going in the
            other direction is much harder.</p>
            <h3>Precondition clashes</h3>
            <p>Some tests have very different preconditions - one test might require
            that a certain type of data is absent while other tests will add this data
            (or will have added it in previous runs).</p>
            <p>These types of tests are difficult to adapt but hopefully fairly rare.
            If they can&#39;t be reworked and really are essential they can be run against
            a second instance of the data store in question that <em>is</em> cleaned
            between test runs.</p>
            <h2>How far I&#39;ve gotten</h2>
            <p>I haven&#39;t used this technique in many places and have only use it myself
            for a short time. The idea for it came to me when I saw a traditional,
            PHP-style, hand-crafted test database that wasn&#39;t being reset between tests
            (but which is reset between test <em>runs</em>). While I&#39;m not a fan of
            that approach it got me thinking. I&#39;ve since tried not tearing down the
            database on a side project, to some success.</p>
            <p>I&#39;m ready now to try this strategy in more places, bolstered by the
            knowledge that if tests are written for a dirty database it is very easy to
            change them to running with a clean database later (usually you don&#39;t have
            to do anything).</p>
            <h2>Contact/etc</h2>
            
            
        </article>
    </div></div>
  </body>
</html>
