<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rkp.science/an-alternative-construction-of-shannon-entropy/">Original</a>
    <h1>An alternative construction of Shannon entropy</h1>
    
    <div id="readability-page-1" class="page"><article id="post-806">
	
	<!-- .entry-header -->

	<div>
		
<p><strong>TL;DR: Shannon’s entropy formula is usually justified by showing it satisfies key mathematical criteria, or by computing how much space is needed to encode a variable. But one can also construct Shannon’s formula starting purely from the simpler notion of entropy as a (logarithm of a) count—of how many different ways a distribution could have emerged from a sequence of samples.</strong></p>



<h3>Entropy of a probability distribution</h3>



<p>The underpinning of information theory is Shannon’s formula for the entropy <em>H</em> of a discrete probability distribution <em>P</em>. Derived in 1948 in the paper “A mathematical theory of communication”, this is given by</p>


<p><span>   </span><span>   </span><img decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-932f4df0d7fefa98158b85a4a6b9b7d1_l3.png" height="52" width="223" alt="\[  H[P] = - \sum_{i=1}^N p_i \log p_i \quad \quad (1) \]" title="Rendered by QuickLaTeX.com"/></p>



<p>where the <em>p<sub>i</sub></em> = <em>p</em>(<em>x<sub>i</sub></em>) is the probability of sampling <em>x<sub>i</sub></em> from <em>P</em>.</p>



<p>There are many ways to think about entropy, but one intuition is that <em>H</em> quantifies how spread out <em>P</em> is. It is a more general statistic than e.g. variance, because entropy does not require a notion of distance between different <em>x<sub>i</sub></em>. Thus one can compute the entropy of a distribution over colors, names, categories, etc., for which variance might not make sense.</p>



<p>For a uniform distribution over <em>N</em> outcomes, the Shannon entropy is:</p>


<p><span>   </span><span>   </span><img decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-8fe33bf846774d7b450a9952d821b9a9_l3.png" height="52" width="333" alt="\[  H[P] = - \sum_{i=1}^N \frac{1}{N} \log \frac{1}{N} = - \log \frac{1}{N} = \log N. \]" title="Rendered by QuickLaTeX.com"/></p>



<p>The entropy is just the number of possible outcomes, counted in log units. Log units are used because the number of outcomes often grows exponentially with system size. For instance, the number of unique binary words of length <em>T</em> is 2<sup><em>T</em></sup>. The entropy of a uniform distribution over such words is therefore log 2<sup><em>T</em></sup> = <em>T</em> bits.</p>



<p>But Eq. 1 applies to all discrete distributions, not just uniform ones. At first glance the general formula is not particularly intuitive. Why the minus sign? Why a probability times a logarithm of a probability? Where does Eq. 1 come from?</p>



<h3>Shannon’s derivation</h3>



<p>The usual explanation of why Eq. 1 is the right formula for entropy can be found in Shannon’s <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" data-type="link" data-id="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">paper itself</a> (p. 10-11).</p>



<p>Essentially Eq. 1 is the unique solution (up to a positive multiplicative factor) satisfying three criteria: (1) <em>H</em> is continuous in the probability levels, (2) <em>H</em> of a uniform distribution increases with the number of possible outcomes, and (3) if an observation from <em>P</em> can be broken into two or more sequential observations (e.g. first observing whether a sample is in category A or B, then observing its exact value), then <em>H</em> should be decomposable into a weighted sum of the individual entropies.</p>



<p>Appendix 2 of Shannon’s paper shows why Eq. 1 satisfies these, and the proof is quite accessible. However, the derivation essentially starts by assuming one already knows the answer, obscuring how one would have discovered Eq. 1 in the first place.</p>



<h3>Expected space required to encode a sample</h3>



<p>A second derivation of Shannon’s formula comes from computing the expected number of bits required to <em>encode</em> a sample from <em>P</em>. For instance, if <em>P</em> is a uniform distribution over 4 possible outcomes, it takes log<sub>2</sub>(4) = 2 bits to encode one of these outcomes (00, 01, 10, or 11), i.e. to communicate to someone else which outcome occurred. More generally, it takes log 1/<em>p<sub>i</sub></em> = -log <em>p<sub>i</sub> </em>bits to encode a sample that occurs with probability <em>p<sub>i</sub></em>, and the expectation of this quantity is E[-log p<sub>i</sub>] which is equivalent to Eq. 1.</p>



<p>This derivation is elegant and intuitive, but it requires introducing the notion of encoding. And even though it kind of makes sense that rarer outcomes would require more space to encode, it takes a bit of head-scratching (at least for me) to arrive at why log 1/p<sub>i</sub> is the right quantity for a non-uniform distribution. (Of course, the link between entropy and encoding is a core element of Information Theory, so it’s definitely worth working through this, but it takes a bit of thinking.)</p>



<h3>An alternative construction</h3>



<p>Is there a way to arrive at Shannon’s formula without knowing what it should look like ahead of time? And without the notion of encoding? In particular, <strong>can we go from the simpler notion of entropy as the log number of possible outcomes of something to Shannon’s formula for an arbitrary <em>P</em></strong>?</p>



<p>It turns out Shannon entropy emerges when we choose that <em>something</em> to be<strong> a sequence of samples consistent with a histogram approximating <em>P</em></strong> (i.e. that converges to <em>P</em> in the limit of infinite samples). If we count <em>these</em> in log units, Eq. 1 emerges.</p>



<p>I first saw this derivation in Leonard Susskind’s excellent Statistical Mechanics course (<a href="https://www.youtube.com/watch?v=EmM1jOV1uSY">Lecture 3</a>), albeit in a different context—connecting Boltzmann and Gibbs entropy. Below I’ll present it differently, but the math is the same. This derivation is longer than Shannon’s, but it shows how one might have come upon Eq. 1 without already knowing what it should look like and without introducing the notion of encoding.</p>



<h3>Entropy as a log count of sequences of samples</h3>



<p>Consider drawing <em>L</em> samples from a distribution <em>P</em>, one after another. As <em>L</em> increases, the histogram constructed from the samples will tend toward <em>P</em>. How many different sequences of <em>L</em> samples could have produced a given histogram?</p>



<p>Suppose <em>P</em> is defined over <em>N</em> possible outcomes, <em>x</em><sub>1</sub> , …, <em>x</em><sub>N</sub> , and we have collected <em>L</em><sub>1</sub> observations of <em>x</em><sub>1</sub>, <em>L</em><sub>2</sub> observations of <em>x</em><sub>2</sub>, etc, where <em>L</em> = <em>L</em><sub>1</sub> + … + <em>L</em><sub>N</sub>. The number of ways (call this <em>N<sub>seq</sub></em>) we could have gotten<em> L<sub>1</sub></em> samples of  <em>x<sub>1</sub></em>, <em>L</em><sub>2</sub> samples of <em>x</em><sub>2</sub>, etc. is</p>


<p><span>   </span><span>   </span><img decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-91e5dd8a7d66e5e86504cdde6fe5ac7a_l3.png" height="43" width="419" alt="\[ N_{seq}(L) = {L \choose L_1} {L - L_1 \choose L_2} \cdots {L - L_1 - \cdots - L_{N-1} \choose L_N} \]" title="Rendered by QuickLaTeX.com"/></p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-2a078b193fa7737b47984865dcf90df7_l3.png" height="43" width="439" alt="\[ = \frac{L!}{L_1! (L-L_1)!} \times \frac{(L-L_1)!}{L_2! (L-L_1-L_2)!} \cdots = \frac{L!}{L_1! L_2! \cdots L_N!} \]" title="Rendered by QuickLaTeX.com"/></p>



<p>where we have canceled the second term in each denominator with the next numerator.</p>



<p>Counting in log units we then find that</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-33b5f17fe4cdd061fdc5704ccaaa4719_l3.png" height="43" width="509" alt="\[ \log N_{seq}(L) =  \log \left( \frac{L!}{L_1! \cdots L_N!} \right) = \log L! - \log L_1! - \dots - \log L_N!, \]" title="Rendered by QuickLaTeX.com"/></p>



<p>the log number of different sequences of <em>L</em> samples that would have produced our histogram. Next, letting <em>L</em> be large enough to use Stirling’s approximation (log <em>L</em>! ≈ <em>L</em> log <em>L</em> – <em>L</em>), we find</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-991c0866714b257c513cd88405598b32_l3.png" height="19" width="549" alt="\[ \log N_{seq}(L) \approx L \log L - L - (L_1 \log L_1 - L_1) - \dots - (L_N \log L_N - L_N). \]" title="Rendered by QuickLaTeX.com"/></p>



<p>Regrouping terms,</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-33892d2c560a13d04fa4110528f43436_l3.png" height="19" width="582" alt="\[ \log N_{seq}(L) \approx L \log L - (L_1 \log L_1 + \dots + L_N \log L_N) - L + (L_1 + \dots + L_N) \]" title="Rendered by QuickLaTeX.com"/></p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-a88ced4d8eb297d48b780b3fc488ca91_l3.png" height="19" width="317" alt="\[ = L \log L - (L_1 \log L_1 + \dots + L_N \log L_N) \]" title="Rendered by QuickLaTeX.com"/></p>



<p>since <em>L</em> = <em>L</em><sub>1</sub> + … + <em>L</em><sub>N</sub>. Factoring out <em>L </em>we have</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-7ffe047d2e0bde5aff1a95f8dd1388ca_l3.png" height="43" width="455" alt="\[ \log N_{seq}(L) \approx L\left[ \log L - \left(\frac{L_1}{L} \log L_1 + \dots + \frac{L_N}{L} \log L_N \right) \right]. \]" title="Rendered by QuickLaTeX.com"/></p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-37d2f5ba33bb540acf1dcbee746cf9ed_l3.png" height="19" width="534" alt="\[ = L\left[ \log L - (\hat{p}_1 \log L + \dots + \hat{p}_N \log L) - (\hat{p}_1 \log \hat{p}_1 + \cdots + \hat{p}_N \log \hat{p}_N) \right] \]" title="Rendered by QuickLaTeX.com"/></p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-30f6fa71d0b6f887ca89574fe9437966_l3.png" height="19" width="374" alt="\[ = L\left[ \log L - \log L - (\hat{p}_1 \log \hat{p}_1 + \cdots + \hat{p}_N \log \hat{p}_N) \right] \]" title="Rendered by QuickLaTeX.com"/></p>



<p>where we have used</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-7f71cbcb863110c1e6e51877f335f6e7_l3.png" height="38" width="219" alt="\[ \hat{p}_i \equiv L_i/L \quad \text{and} \quad \sum_i \hat{p}_i = 1   \]" title="Rendered by QuickLaTeX.com"/></p>



<p>Thus</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-1cabd4a04738bb2f7b88c505b771d402_l3.png" height="38" width="503" alt="\[ \log N_{seq}(L) \approx  L\left[ -(\hat{p}_1 \log \hat{p}_1 + \cdots + \hat{p}_N \log \hat{p}_N) \right] = -L \sum_i \hat{p}_i \log \hat{p}_i. \]" title="Rendered by QuickLaTeX.com"/></p>



<p>Since our histogram approaches <em>P</em> as <em>L</em> → ∞ we arrive at</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-104073d8b692a3647ab867b1bf965700_l3.png" height="38" width="233" alt="\[ \log N_{seq}(L) \to -L\sum_i p_i \log p_i . \]" title="Rendered by QuickLaTeX.com"/></p>



<p>Finally, to obtain a quantity that does not depend on the number of samples we divide both sides by <em>L</em>:</p>


<p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://rkp.science/wp-content/ql-cache/quicklatex.com-366a3cff1ef32ef47646f78c4a9e3017_l3.png" height="47" width="286" alt="\[ \frac{\log N_{seq}(L)}{L} \to -\sum_i p_i \log p_i = H[P]. \]" title="Rendered by QuickLaTeX.com"/></p>



<p>Thus, when we count how many different sequences of samples could have produced <em>P</em>, in log units and normalizing by the number of samples, we arrive once again at Eq. 1. That is, <em>Shannon’s formula is a normalized log-count</em> of how many ways a distribution could have emerged.</p>



<h4>Further reading</h4>



<p><a href="https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x" data-type="link" data-id="https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x">C.E. Shannon.<em> </em>A Mathematical Theory of Communication. <em>The Bell System Technical Journal </em>1948.</a></p>



<p><a href="https://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981" data-type="link" data-id="https://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981">David MacKay. Information Theory, Inference and Learning Algorithms. <em>Cambridge University Press</em> 2003.</a>  (+ <a href="https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6">online lectures)</a></p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>
  </body>
</html>
