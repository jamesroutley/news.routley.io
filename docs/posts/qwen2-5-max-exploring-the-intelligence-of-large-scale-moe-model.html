<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qwenlm.github.io/blog/qwen2.5-max/">Original</a>
    <h1>Qwen2.5-Max: Exploring the intelligence of large-scale MoE model</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5-max-banner.png" width="100%"/></figure><p><a href="https://chat.qwenlm.ai" target="_blank">QWEN CHAT</a>
<a href="https://www.alibabacloud.com/help/en/model-studio/developer-reference/what-is-qwen-llm" target="_blank">API</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><p>It is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. Today, we are excited to share the performance results of Qwen2.5-Max and announce the availability of its <a href="https://www.alibabacloud.com/help/en/model-studio/developer-reference/what-is-qwen-llm">API</a> through Alibaba Cloud. We also invite you to explore Qwen2.5-Max on <a href="https://chat.qwenlm.ai">Qwen Chat</a>!</p><h2 id="performance">Performance</h2><p>We evaluate Qwen2.5-Max alongside leading models, whether proprietary or open-weight, across a range of benchmarks that are of significant interest to the community. These include MMLU-Pro, which tests knowledge through college-level problems, LiveCodeBench, which assesses coding capabilities, LiveBench, which comprehensively tests the general capabilities, and Arena-Hard, which approximates human preferences. Our findings include the performance scores for both base models and instruct models.</p><p>We begin by directly comparing the performance of the instruct models, which can serve for downstream applications such as chat and coding. We present the performance results of Qwen2.5-Max alongside leading state-of-the-art models, including DeepSeek V3, GPT-4o, and Claude-3.5-Sonnet.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5-max-instruct.jpg" width="100%"/></figure><p>Qwen2.5-Max outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also demonstrating competitive results in other assessments, including MMLU-Pro.</p><p>When comparing base models, we are unable to access the proprietary models such as GPT-4o and Claude-3.5-Sonnet. Therefore, we evaluate Qwen2.5-Max against DeepSeek V3, a leading open-weight MoE model, Llama-3.1-405B, the largest open-weight dense model, and Qwen2.5-72B, which is also among the top open-weight dense models. The results of this comparison are presented below.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5-Max.jpeg" width="100%"/></figure><p>Our base models have demonstrated significant advantages across most benchmarks, and we are optimistic that advancements in post-training techniques will elevate the next version of Qwen2.5-Max to new heights.</p><h2 id="use-qwen25-max">Use Qwen2.5-Max</h2><p>Now Qwen2.5-Max is available in Qwen Chat, and you can directly chat with the model, or play with artifacts, search, etc.</p><video width="100%" autoplay="" loop="" muted="" playsinline="">
<source src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen-max.mp4" type="video/mp4"/></video><p>The API of Qwen2.5-Max (whose model name is <code>qwen-max-2025-01-25</code>) is available. You can first <a href="https://account.alibabacloud.com/register/intl_register.htm">register an Alibaba Cloud account</a> and activate Alibaba Cloud Model Studio service, and then navigate to the console and create an API key.</p><p>Since the APIs of Qwen are OpenAI-API compatible, we can directly follow the common practice of using OpenAI APIs. Below is an example of using Qwen2.5-Max in Python:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>
</span></span><span><span><span>import</span> <span>os</span>
</span></span><span><span>
</span></span><span><span><span>client</span> <span>=</span> <span>OpenAI</span><span>(</span>
</span></span><span><span>    <span>api_key</span><span>=</span><span>os</span><span>.</span><span>getenv</span><span>(</span><span>&#34;API_KEY&#34;</span><span>),</span>
</span></span><span><span>    <span>base_url</span><span>=</span><span>&#34;https://dashscope-intl.aliyuncs.com/compatible-mode/v1&#34;</span><span>,</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>completion</span> <span>=</span> <span>client</span><span>.</span><span>chat</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>(</span>
</span></span><span><span>    <span>model</span><span>=</span><span>&#34;qwen-max-2025-01-25&#34;</span><span>,</span>
</span></span><span><span>    <span>messages</span><span>=</span><span>[</span>
</span></span><span><span>      <span>{</span><span>&#39;role&#39;</span><span>:</span> <span>&#39;system&#39;</span><span>,</span> <span>&#39;content&#39;</span><span>:</span> <span>&#39;You are a helpful assistant.&#39;</span><span>},</span>
</span></span><span><span>      <span>{</span><span>&#39;role&#39;</span><span>:</span> <span>&#39;user&#39;</span><span>,</span> <span>&#39;content&#39;</span><span>:</span> <span>&#39;Which number is larger, 9.11 or 9.8?&#39;</span><span>}</span>
</span></span><span><span>    <span>]</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>completion</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>]</span><span>.</span><span>message</span><span>)</span>
</span></span></code></pre></div><h2 id="future-work">Future Work</h2><p>The scaling of data and model size not only showcases advancements in model intelligence but also reflects our unwavering commitment to pioneering research. We are dedicated to enhancing the thinking and reasoning capabilities of large language models through the innovative application of scaled reinforcement learning. This endeavor holds the promise of enabling our models to transcend human intelligence, unlocking the potential to explore uncharted territories of knowledge and understanding.</p><p>Feel free to cite the following article if you find Qwen2.5 helpful.</p><pre tabindex="0"><code>@article{qwen25,
  title={Qwen2.5 technical report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
</code></pre></div></article></div></div>
  </body>
</html>
