<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.metaphysic.ai/the-emergence-of-full-body-gaussian-splat-deepfake-humans/">Original</a>
    <h1>The Emergence of Full-Body Gaussian Splat Deepfake Humans</h1>
    
    <div id="readability-page-1" class="page"><div data-id="230321ec" data-element_type="widget" data-widget_type="theme-post-content.default"><div><div data-elementor-type="wp-post" data-elementor-id="13694" data-elementor-post-type="post"><section data-id="30ddc03" data-element_type="section"><div><div data-id="11e5553" data-element_type="column"><div><div data-id="64acc66" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Since the advent of <a href="https://arxiv.org/pdf/2308.04079.pdf">Gaussian Splats</a>, in August of this year, the image synthesis research community has clearly embraced this innovative approach to neural recreation of people, things and scenery.</p><p>Currently, the daily submissions list at the Computer Vision section of Arxiv, and at other platforms, <a href="https://archive.is/vzvwp">features</a> a growing frequency of splat-related papers, as the interval lengthens between initial publication and follow-on projects.</p><p>One noteworthy development this month is a general increase in the number of research projects that attempt neural simulation of entire human bodies – a sub-strand of synthesis research that, to date, <a href="https://lemonatsu.github.io/anerf/">has</a> <a href="https://arxiv.org/abs/2305.06356">been</a> <a href="https://arxiv.org/abs/2203.12575">dominated</a> by <a href="https://blog.metaphysic.ai/nerf-successor-deepfakes/">Neural Radiance Fields</a> (NeRF), despite the <a href="https://blog.metaphysic.ai/entanglement-in-image-synthesis/#nerflocked">relative rigidity</a> of that older technology.</p><p>As with NeRF, Gaussian Splatting is, by default, only capable of creating explorable <em><i>static</i></em> neural scenes; and, as with NeRF, the sector is rapidly developing ways around this limitation. Last week we took a look at the first noteworthy <a href="https://blog.metaphysic.ai/controllable-deepfakes-with-gaussian-avatars/">splat-based facial deepfake system</a>; and this week, among the slew of potential new academic projects capable of supporting <a href="https://blog.metaphysic.ai/the-road-to-realistic-full-body-deepfakes/">full-body deepfakes</a>, a new offering led by ETH Zurich is proposing an economical and ingenious method of generating Gaussian humans that can be controlled in real time.</p></div></div></div></div></div></section><section data-id="4827121" data-element_type="section"></section><section data-id="10e18b8" data-element_type="section"><div><div data-id="b5a6dc8" data-element_type="column"><div><div data-id="435a645" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><i>From the project page of the new paper, examples of novel viewpoint rendering of humans captured from multi-view video, and interpreted via parametric meshes and Gaussian Splats into virtual humans that can not only recreate the motion depicted in the source videos, but can adapt to novel motion input by the end user.</i></em> Source: https://vcai.mpi-inf.mpg.de/projects/ash/</span></p></div></div></div></div></section><section data-id="23dde8c" data-element_type="section"><div><div data-id="ae3a108" data-element_type="column"><div><div data-id="6ebacca" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The authors of the new work have devised a way to animate the Gaussian figures by interpreting the Splats in 2D space, through the use of a parametric human template (a <a href="https://blog.metaphysic.ai/3d-morphable-models-3dmms/">common bridging method</a> between CGI and neural workflows). Since each representative Gaussian Splat scene (i.e., each potential ‘frame’) is a complete model in itself, the alternative would be equivalent to animating the Mona Lisa by painting 24 separate masterpieces per second.</p><p>The method, called <em><i>ASH</i></em>, is capable of real-time translation and rendering, and, the new paper reports, achieves notably superior results in tests against similar approaches, including the only other analogous real-time approach currently in existence.</p><p>The authors have, additionally, devised a user interface that allows one to impose skeletal poses and motion into captured human data and have the Splatted representations perform novel movement that was not in the original capture data, as well as allowing the user to thoroughly explore the recreation – in a GUI that begins to resemble the <a href="https://metaphysic.ai/3d-morphable-models-3dmms/#poser">CGI Poser/Daz applications</a> that have facilitated the creation of moving CGI humans for over twenty years.</p></div></div></div></div></div></section><section data-id="c7b20b0" data-element_type="section"></section><section data-id="a2d5e0e" data-element_type="section"><div><div data-id="bd34b6a" data-element_type="column"><div><div data-id="94dd5cf" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><i><strong>Click to play.</strong> The ASH player, currently in a rudimentary state, runs in a browser and allows the viewer to control Splatted humans even with movements that did not feature in the original footage from which their appearance was compiled – though, naturally, there are limitations to how effectively truly extraordinary novel movements could be (for instance, if the person did not remove their outer jacket in the source footage, such a movement could not convincingly be represented in this instance).</i></em></span></p></div></div></div></div></section><section data-id="1e9abf1" data-element_type="section"><div><div data-id="357e63c" data-element_type="column"><div><div data-id="f1ac0df" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Combined with the clearly growing capacity for Gaussian Splats to become editable, projects of this nature seem set to bring the flexibility of CGI to a neural representation framework in a way that NeRF has struggled to do over the last few years.</p><p>The paper states:</p></div></div></div></div></div></section><section data-id="f83d844" data-element_type="section"><div><div data-id="7492a82" data-element_type="column"><div><div data-id="9d87edb" data-element_type="widget" data-widget_type="text-editor.default"><div><p><em><i>‘</i>[O</em><em><i>ur] animatable human avatar is parameterized using Gaussian splats. However, naively learning a mapping from skeletal pose to Gaussian parameters in 3D is intractable given the limited compute budget when constraining ourselves to real-time performance. </i></em></p><p><em><i>‘Thus, we propose to attach the Gaussians onto a deformable mesh template of the human. Given the mesh’s [UV] parameterization, it allows learning the Gaussian parameters efficiently in 2D texture space. Here, each texel covered by a triangle represents a Gaussian. </i></em></p><p><em><i>‘Thus, the number of Gaussians remains constant, which is in stark contrast to the original formulation.’</i></em></p></div></div></div></div></div></section><section data-id="1ab2947" data-element_type="section"></section><section data-id="674f021" data-element_type="section"></section><section data-id="2c1128d" data-element_type="section"><div><div data-id="0c4a62d" data-element_type="column"><div><div data-id="c82c141" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><i><strong>Click to play.</strong> Free viewpoint rendering in ASH</i></em>.</span></p></div></div></div></div></section><section data-id="2970488" data-element_type="section"><div><div data-id="c7e147f" data-element_type="column"><div><div data-id="8378504" data-element_type="widget" data-widget_type="text-editor.default"><p>The <a href="https://arxiv.org/pdf/2312.05941.pdf">new paper</a> is titled <em><i>ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering</i></em>, and comes from five researchers across the Max Planck Institute for Informatics, ETH Zurich, Freiburg University, and the Saarbrucken Research Center for Visual Computing, Interaction and AI.</p></div></div></div></div></section><section data-id="0d2e7e8" data-element_type="section"></section><section data-id="6b0bfe3" data-element_type="section"></section><section data-id="329253f" data-element_type="section"><div><div data-id="6dfadf3" data-element_type="column"><div><div data-id="3c6a33c" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Before we take a deeper look at this interesting new project, let’s consider the difference between Gaussian Splatting and prior neural techniques, so that an examination of the researcher’s techniques will make more sense.</p><p>A Gaussian Splat is in some ways analogous to a pixel, which is a single minimal unit of color in an image, and which has no transparency (or alpha) value in ‘flat’ formats such as JPG, but may have transparency in formats which support it, such as PNG. Since pictures are two-dimensional, a pixel only exists in X/Y space (i.e., up and down).</p></div></div></div></div></div></section><section data-id="6bf0265" data-element_type="section"><div><div data-id="18b5156" data-element_type="column"><div><div data-id="cce66f8" data-element_type="widget" data-widget_type="image.default"><div><figure> <img decoding="async" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/elementor/thumbs/pixel-qgu9c5vzgxus5epzbiuzo25lwmivcf97p5jbqsdd62.jpg" title="pixel" alt="A pixel is the minimum unit in a bitmapped image, such as a JPG or PNG. Source: https://graphicdesign.stackexchange.com/questions/70786/vectorizing-pixels-in-bitmap-images-for-infinite-resizing-possible" loading="lazy"/><figcaption>A pixel is the minimum unit in a bitmapped image, such as a JPG or PNG. Source: https://graphicdesign.stackexchange.com/questions/70786/vectorizing-pixels-in-bitmap-images-for-infinite-resizing-possible</figcaption></figure></div></div></div></div></div></section><section data-id="cb03974" data-element_type="section"><div><div data-id="1b158bc" data-element_type="column"><div><div data-id="9e851a8" data-element_type="widget" data-widget_type="text-editor.default"><p>In 3D space, in traditional (though more recent) CGI methodologies, a <a href="https://archive.is/Tnl4i">voxel</a> has been, for some time, the 3D equivalent of a pixel. A voxel is a mathematical or parametric object or entity positioned in full <a href="https://help.autodesk.com/view/MOBPRO/2024/ENU/?guid=GUID-F90A9BF3-1A41-4FB7-AE58-53D73BDDEF6B">XYZ</a> (3D) space. It has color and (optional) alpha transparency, and, unlike a bitmap-textured CGI mesh, it is calculated as volume, rather than simply wrapping image textures around virtual wire meshes (where the inside of the resultant object is conceptually ‘hollow’).</p></div></div></div></div></section><section data-id="97c617e" data-element_type="section"><div><div data-id="2b2c740" data-element_type="column"><div><div data-id="0143974" data-element_type="widget" data-widget_type="image.default"><div><figure> <img decoding="async" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/elementor/thumbs/voxel-qgu9dfy2rtltyav2ukrpk8j70i7utlc27hi49ch2vk.jpg" title="voxel" alt="A voxel occupies a 3D space mapping that isn&#39;t dissimilar to LEGO bricks, conceptually, though most of the bricks are transparent (else you would not be able to see the objects at the center. Source: https://archive.is/Tnl4i" loading="lazy"/><figcaption>A voxel occupies a 3D space mapping that isn&#39;t dissimilar to LEGO bricks, conceptually, though most of the bricks are transparent (else you would not be able to see the objects at the center. Source: https://archive.is/Tnl4i</figcaption></figure></div></div></div></div></div></section><section data-id="7d421e2" data-element_type="section"><div><div data-id="37cb433" data-element_type="column"><div><div data-id="707c329" data-element_type="widget" data-widget_type="text-editor.default"><p>In Neural Radiance Fields, or NeRF, the unit in question is calculated by tracing the path from a camera’s point-of-view down to the last opaque point where the ‘ray’ can travel no further (i.e., there is no more empty space between the camera and the object, and the surface of the object is now 100% opaque).</p></div></div></div></div></section><section data-id="97c7485" data-element_type="section"></section><section data-id="a73c7e9" data-element_type="section"><div><div data-id="664131e" data-element_type="column"><div><div data-id="5b5e576" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><i><strong>Click to play.</strong> The NeRF capture process is similar to CGI ray-tracing, building up an interpretive neural network composed of pixel values with 3D (instead of just 2D) coordinates, and with transparency (alpha) channels, so that glass and empty or ‘cut-out’ sections of geometry can be correctly interpreted. </i></em>Source: https://www.youtube.com/watch?v=JuH79E8rdKc’</span></p></div></div></div></div></section><section data-id="72a9e1c" data-element_type="section"><div><div data-id="747aaaa" data-element_type="column"><div><div data-id="7292643" data-element_type="widget" data-widget_type="text-editor.default"><p>A NeRF calculates such values from <em><i>multiple pictures</i></em> of the same object, scene or person, so that all these interpretations can be collated into a neural representation:</p></div></div></div></div></section><section data-id="3c7219d" data-element_type="section"></section><section data-id="8d79d88" data-element_type="section"><div><div data-id="727e6ae" data-element_type="column"><div><div data-id="c9e2137" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><i><strong>Click to play.</strong> Multiple photos combine to provide an explorable neural environment in NeRF.</i></em> Source: https://www.youtube.com/watch?v=DJ2hcC1orc4<br/></span></p></div></div></div></div></section><section data-id="eb86872" data-element_type="section"><div><div data-id="34ddc6c" data-element_type="column"><div><div data-id="4478dff" data-element_type="widget" data-widget_type="text-editor.default"><div><p>However, these ‘virtual pixels’ in NeRF, though they can be calculated in XYZ 3D space, and though they can have whatever transparency the scene requires, besides requiring onerous compute resources and storage, are all bound to the ray-tracing action of the multiple viewpoints from the data capture.</p><p>A <a href="https://youtu.be/HVv_IQKlafQ?t=15">Gaussian Splat</a>, instead, is a neural* representation unit that is not limited in this way – not only can it be assigned anywhere in XYZ/3D space, but it can as necessary multiply and subdivide into additional splats, as coverage requires.</p></div></div></div></div></div></section><section data-id="e386197" data-element_type="section"><div><div data-id="5762fbe" data-element_type="column"><div><div data-id="b805173" data-element_type="widget" data-widget_type="image.default"><div><figure> <img fetchpriority="high" fetchpriority="high" decoding="async" width="768" height="415" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/2023/12/splat-compared-768x415.jpg" alt="The pixel is limited to two dimensions; the voxel can roam freely in three dimensions, but is constricted to the traditional limitations on authenticity associated with CGI; the Neural Radiance Field (NeRF) is constrained by the camera-based ray-tracing that fuels NeRF&#39;s synthesis process; but the divisible Gaussian Splat can be assigned at will in 3D space, and even assigned to CGI meshes, as kind of neural &#39;supertexture&#39;." data-srcset="https://blog.metaphysic.ai/wp-content/uploads/2023/12/splat-compared-768x415.jpg 768w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/splat-compared-300x162.jpg 300w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/splat-compared.jpg 1000w" sizes="(max-width: 768px) 100vw, 768px"/><figcaption>The pixel is limited to two dimensions; the voxel can roam freely in three dimensions, but is constricted to the traditional limitations on authenticity associated with CGI; the Neural Radiance Field (NeRF) is constrained by the camera-based ray-tracing that fuels NeRF&#39;s synthesis process; but the divisible Gaussian Splat can be assigned at will in 3D space, and even assigned to CGI meshes, as  kind of neural &#39;supertexture&#39;.</figcaption></figure></div></div></div></div></div></section><section data-id="b7d91aa" data-element_type="section"></section><section data-id="2b45ce3" data-element_type="section"><div><div data-id="109b9ac" data-element_type="column"><div><div data-id="8d8bcf7" data-element_type="widget" data-widget_type="text-editor.default"><p>Since the <a href="https://blog.metaphysic.ai/what-is-the-latent-space-of-an-image-synthesis-system/">latent space</a> of trained human AI representations can be hard to control, a growing number of human synthesis projects are using specially-designed CGI interfaces as a bridge between trained data and user control. These are essentially old-school CGI humans in a <a href="https://blog.metaphysic.ai/real-time-photorealistic-hands-for-neural-environments/#canon">canonical</a> (or basic, default) pose that can be rigged to equivalent perceived captured data points (such as faces or full bodies). As mentioned earlier, this method has already been used to create a <a href="https://blog.metaphysic.ai/controllable-deepfakes-with-gaussian-avatars/#gsplatimage">Splat-based deepfake process</a>.</p></div></div></div></div></section><section data-id="2b55b90" data-element_type="section"></section><section data-id="196ffdf" data-element_type="section"><div><div data-id="ebdf477" data-element_type="column"><div><div data-id="efd3fc3" data-element_type="widget" data-widget_type="text-editor.default"><p>In the case of ASH, the 2021 Real-time<a href="https://people.mpi-inf.mpg.de/~mhaberma/projects/2021-ddc/data/paper.pdf"> Deep Dynamic Characters</a> (DDC) mesh was used as a method of instrumentality for the neural avatars.</p></div></div></div></div></section><section data-id="a794530" data-element_type="section"><div><div data-id="9d0164c" data-element_type="column"><div><div data-id="4589876" data-element_type="widget" data-widget_type="image.default"><div><figure> <img decoding="async" width="768" height="432" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/2023/12/ddc-768x432.jpg" alt="The DDC model can take account of uncommon types of movement, such as the dynamics of a large dress in motion. Source: https://people.mpi-inf.mpg.de/~mhaberma/projects/2021-ddc/data/paper.pdf" data-srcset="https://blog.metaphysic.ai/wp-content/uploads/2023/12/ddc-768x432.jpg 768w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/ddc-300x169.jpg 300w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/ddc-1024x576.jpg 1024w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/ddc.jpg 1200w" sizes="(max-width: 768px) 100vw, 768px"/><figcaption>The DDC model can take account of uncommon types of movement, such as the dynamics of a large dress in motion. Source: https://people.mpi-inf.mpg.de/~mhaberma/projects/2021-ddc/data/paper.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="08c86c3" data-element_type="section"><div><div data-id="16bf296" data-element_type="column"><div><div data-id="d1f9afc" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Among other reasons, DDC was chosen over other popular parametric models because it is unusually capable of reproducing large and flowing movement of clothes, such as the motion of a billowing dress as a woman turns dynamically.</p><p>The initial data for ASH is gathered in the form of video of individual people in movement, taken from multiple cameras, from which bitmapped data is processed and skeletal pose is inferred. The authors state:</p></div></div></div></div></div></section><section data-id="c5204aa" data-element_type="section"><div><div data-id="2641afd" data-element_type="column"><div><div data-id="a4cc0fc" data-element_type="widget" data-widget_type="text-editor.default"><div><p><em><i>‘Our goal is to generate motion-controllable, photorealistic renderings of humans learned solely from multi-view RGB [videos]. Specifically, ASH takes the skeletal motions and a virtual camera view as input at inference, and produces high-fidelity renderings in real-time (</i></em><em><i>∼</i></em><em><i>30f ps). To this end, we propose to model the dynamic character with 3D Gaussian splats, parametrized as texels in the texture space of a deformable template mesh. </i></em></p><p><em><i>‘This texel-based parameterization of 3D Gaussian splats enables us to model the mapping from skeletal motions to the Gaussian splat parameters as a 2D image-2-image translation task.’</i></em></p></div></div></div></div></div></section><section data-id="e6275dc" data-element_type="section"><div><div data-id="ea09db4" data-element_type="column"><div><div data-id="068d766" data-element_type="widget" data-widget_type="text-editor.default"><p>So what is happening in ASH is that the high-compute dynamics of the interpreted Splats are being transliterated into a more flexible and lightweight environment, courtesy of the DDC human mesh, and it is this adroit implementation that allows for real-time operation.</p></div></div></div></div></section><section data-id="bf0b5e7" data-element_type="section"><div><div data-id="a4b6d42" data-element_type="column"><div><div data-id="532e147" data-element_type="widget" data-widget_type="image.default"><div><figure> <img loading="lazy" loading="lazy" decoding="async" width="1200" height="349" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/2023/12/approach.jpg" alt="Conceptual workflow for ASH." data-srcset="https://blog.metaphysic.ai/wp-content/uploads/2023/12/approach.jpg 1200w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/approach-300x87.jpg 300w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/approach-1024x298.jpg 1024w, https://blog.metaphysic.ai/wp-content/uploads/2023/12/approach-768x223.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px"/><figcaption>Conceptual workflow for ASH.</figcaption></figure></div></div></div></div></div></section><section data-id="1da8df8" data-element_type="section"></section><section data-id="8c32992" data-element_type="section"><div><div data-id="3890254" data-element_type="column"><div><div data-id="fc44ac9" data-element_type="widget" data-widget_type="text-editor.default"><div><p><em><i>‘</i>A</em><em><i>SH generates high-fidelity rendering given a skeletal motion and a virtual camera view. A motion-dependent, canonicalized template mesh is generated with a learned deformation network. From the canonical template mesh, we can render the motion-aware textures, which are further adopted for predicting the Gaussian splat parameters with two 2D convolutional networks, i.e., the Geometry and Appearance Decoder, as the texels in the 2D texture space. </i></em></p><p><em><i>‘Through UV mapping and DQ skinning, we warp the Gaussian splats from the canonical space to the posed space. Then, splatting is adopted to render the posed Gaussian splats.’</i></em></p></div></div></div></div></div></section><section data-id="ea12ae3" data-element_type="section"></section><section data-id="0e8fc26" data-element_type="section"><div><div data-id="6b68e7b" data-element_type="column"><div><div data-id="3d913dc" data-element_type="widget" data-widget_type="text-editor.default"><p><em><i><strong>Click to play.</strong> Skeletal motion can be imposed upon the learned Splat representation, and the scene interactively explored.</i></em></p></div></div></div></div></section><section data-id="8fce6df" data-element_type="section"></section><section data-id="23a3052" data-element_type="section"><div><div data-id="cba4045" data-element_type="column"><div><div data-id="3b91977" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Due to the challenging nature of the proposition, training was divided into two phases: a ‘warm-up’ phase that uses data from the DynaCap dataset (from the DDC project that is enabling the CGI human instrumentality), together with footage specially captured by the researchers.</p><p>At this stage, frames are sampled evenly across the source data, and initial 3D Gaussian Splat parameters are learned, which serve as <em><i>faux</i></em> <a href="https://c3.ai/glossary/machine-learning/ground-truth/">ground truth</a> for the subsequent stages.</p><p>In the second phase, the motion-aware decoder (see architectural flow image above) is trained further on the entire sequence of data, with <a href="https://blog.metaphysic.ai/loss-functions-in-machine-learning/">loss functions</a> <a href="https://cpatdowling.github.io/notebooks/regression_2">L1</a> and <a href="https://blog.metaphysic.ai/loss-functions-in-machine-learning/#ssim">Structural Similarity Index</a> (SSIM) used to minimize loss and make the model more accurate.</p><p>In accordance with the prior DDC methodology, the researchers <a href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data">held back</a> four camera views from the obtained data, in order to use these to assess the final model’s resilience to novel data.</p><p>Regarding the original material created by the authors, two sequences were recorded showing subjects performing various dynamic actions such as jogging, dancing, and jumping, to obtain a total of 27,000 frames of training data and 7,000 frames of post-training (split) test data.</p><p>Skeletal motion was inferred by <a href="https://archive.is/jl85l">Captury</a>, with <a href="https://blog.metaphysic.ai/semantic-segmentation/">segmentation</a> (i.e., isolation of subject from the irrelevant background) facilitated by <a href="https://arxiv.org/pdf/2304.02643.pdf">Segment Anything</a> and <em><i><a href="https://grail.cs.washington.edu/projects/background-matting/">The World is Your Green Screen</a></i></em>.</p><p>Testing for novel view synthesis (video below), where the system is required to make the neural actors perform motions that were not included in the source captures, the authors pitted ASH against four systems (not all of which are represented in video or image results, though all are accounted for elsewhere): DDC itself, the only tested method that is capable, as ASH is, of real-time operation; <a href="https://arxiv.org/pdf/2206.08929.pdf">Template-free Animatable Volumetric Actors</a> (TAVA), a hybrid method that manipulates implicit trained fields in canonical space (i.e., it warps a ‘default’ pose); <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/b4b758962f17808746e9bb832a6fa4b8-Paper.pdf">Neural Sparse Voxel Fields</a> (‘Neural actor’ – <em><i>NA</i></em>), which – unusually – explores the use of traditional <a href="https://motion.cs.illinois.edu/RoboticSystems/InverseKinematics.html">inverse kinematics</a> for a human representation instrumentalized by a parametric human mesh; and <a href="https://arxiv.org/pdf/2210.12003.pdf">HDHumans</a>, which likewise models neural humans based on the feature map obtained from a deformable mesh template.</p><p>Metrics used were <a href="https://www.mathworks.com/help/vision/ref/psnr.html">Peak Signal-to-Noise Ratio</a> (PSNR) and <a href="https://blog.metaphysic.ai/loss-functions-in-machine-learning/#lpips">Learned Perceptual Similarity Metrics</a> (LPIPS), assessed at 1k resolution, and averaged across every tenth frame. Results were differentiated between subjects wearing ‘clinging’ clothes (denoted as <em><i>Tight Outfits</i></em>, which are less challenging to reproduce), and those with more billowy apparel (denoted as <em><i>Loose Outfits</i></em>, where accurate reproduction is more problematic for systems of this nature).</p></div></div></div></div></div></section><section data-id="05f9919" data-element_type="section"></section><section data-id="db1c691" data-element_type="section"><div><div data-id="ebd438f" data-element_type="column"><div><div data-id="d3caf43" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><strong>Click to play.</strong> Tests against rival programs for novel view synthesis, where the performed motion comes from training data, but the viewpoint does not.</em></span></p></div></div></div></div></section><section data-id="767342e" data-element_type="section"><div><div data-id="17e860e" data-element_type="column"><div><div data-id="bce8ddd" data-element_type="widget" data-widget_type="text-editor.default"><p>For a quantitative comparison for novel view synthesis (pre-trained motion, original viewpoint), the authors note that ASH outperforms the real-time DDC and the prior non-real-time methods by a considerable margin:</p></div></div></div></div></section><section data-id="8a27445" data-element_type="section"><div><div data-id="b4e4c2d" data-element_type="column"><div><div data-id="52c4d5a" data-element_type="widget" data-widget_type="image.default"><div><figure> <img decoding="async" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/elementor/thumbs/quantitative-results-qgua8a1frb1lod95pkfe9ze7x2gwy9iitsv7wosu12.jpg" title="quantitative-results" alt="Results for the quantitative comparison against rival frameworks, with second-best scores indicated in brown." loading="lazy"/><figcaption>Results for the quantitative comparison against rival frameworks, with second-best scores indicated in brown.</figcaption></figure></div></div></div></div></div></section><section data-id="b803c35" data-element_type="section"><div><div data-id="5adafda" data-element_type="column"><div><div data-id="cb33de3" data-element_type="widget" data-widget_type="text-editor.default"><p>For novel pose, ASH was able to achieve the highest PSNR and the second-best LPIPS score (after Neural Actor, a <em><i>non</i></em>-real-time method) among the tested frameworks, for tight outfits – but outperformed all rivals for subjects with loose clothing:</p></div></div></div></div></section><section data-id="fe6fc03" data-element_type="section"><div><div data-id="6830a4f" data-element_type="column"><div><div data-id="b59392b" data-element_type="widget" data-widget_type="image.default"><div><figure> <img decoding="async" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://blog.metaphysic.ai/wp-content/uploads/elementor/thumbs/novel-pose-qgua9gc6auni6tjpukplw6pyneofkn6fzm82i52492.jpg" title="novel-pose" alt="Quantitative results for novel pose (i.e., the neural actors doing things that the real people did not do in the source data)." loading="lazy"/><figcaption>Quantitative results for novel pose (i.e., the neural actors doing things that the real people did not do in the source data).</figcaption></figure></div></div></div></div></div></section><section data-id="61c00e7" data-element_type="section"></section><section data-id="e982993" data-element_type="section"><div><div data-id="5819855" data-element_type="column"><div><div data-id="9e63341" data-element_type="widget" data-widget_type="text-editor.default"><p><em><i>‘[DDC] is the only competing method with real-time capability. Although it captures coarse motion-aware appearances, its output tends to be blurry and lacks detail. ASH matches the real-time capability as DDC, while generating renderings with much finer details.’</i></em></p></div></div></div></div></section><section data-id="e2b733b" data-element_type="section"></section><section data-id="4bdaa07" data-element_type="section"><div><div data-id="ed7dc3c" data-element_type="column"><div><div data-id="0982cc5" data-element_type="widget" data-widget_type="text-editor.default"><p><span><em><strong>Click to play.</strong> Novel pose synthesis tests.</em></span></p></div></div></div></div></section><section data-id="1056a78" data-element_type="section"><div><div data-id="67e24d5" data-element_type="column"><div><div data-id="62702f4" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Though the HDHumans method achieves comparable results to ASH, the authors observe, it requires extensive sampling for volume rendering, and takes seconds to render one single frame in contrast to the real-time performance that ASH is currently capable of.</p><p>In terms of limitations, the authors concede that ASH does not extract detailed geometry from the Splats, but that this could be addressed in future work through <a href="https://deepai.org/machine-learning-glossary-and-terms/backpropagation">backpropagation</a>, making the process less of a one-way street. However, presumably, this will present an optimization challenge, in terms of maintaining real-time performance.</p><p>Neither does the system model extreme topological changes, such as the opening of a jacket, which deforms the jacket and reveals previously hidden human detail. This, the researchers state, could also be addressed in later work, through the adaptive adding and removal of Gaussian Splats (native Splat functionality that is not used in ASH, which currently assigns a fixed number of Splats and does not divide or multiply them on demand).</p></div></div></div></div></div></section><section data-id="8a2740e" data-element_type="section"></section><section data-id="8a2d5db" data-element_type="section"><div><div data-id="30602db" data-element_type="column"><div><div data-id="0e4eace" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Does this kind of thing qualify as a ‘deepfake’? Unlike <a href="https://blog.metaphysic.ai/controllable-deepfakes-with-gaussian-avatars/">last week’s Gaussian Avatars</a>, ASH currently has no functionality for transposition of identity. However since it can manifestly make trained avatars do things that the source actors did not do in the training videos, and since movement and action can be swapped over and manually edited as necessary, simply by altering the skeletal poses, it certainly does seem to qualify.</p><p>In terms of risk, the need for multi-viewpoint video capture largely obviates the possibility of abusing such a system, which currently requires the explicit cooperation of the subject. Presumably, further down the road, the ability to infer similar functionality from static images (as NeRF does) or non-synchronous video (multiple diverse clips of the same subject) could change this situation.</p><p>The ultimate downstream objective of such research, at least for VFX professionals, is the facile capture of subjects with the minimal necessary information, and the transformation of this data (either via Gaussian Splats or later technologies or iterations) into neural-based equivalent methods of CGI’s current ability to completely model the human form – and to arrive, perhaps, at neural recreations of people that can not only perform any action on command, but which adhere adequately to motion physics – and which can survive a close-up!</p><p>In an ideal world it would be better to do this kind of thing at 24-Mona-Lisas-Per-Second – i.e., that hardware and storage resources might eventually become adequate to perform native real-time volume rendering. As it stands, we seem set for six or more months of Splat-based papers centering around sleight-of-hand proxy schemes such as ASH, and, perhaps later, for genuine optimizations of full-volume operations, such as <a href="https://alexyu.net/plenoctrees/">PlenOctrees</a>, <a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/">NVIDIA NGP</a>, and later similar projects achieved for NeRF.</p></div></div></div></div></div></section><section data-id="b9590d2" data-element_type="section"></section><section data-id="db635b9" data-element_type="section"><div><div data-id="eb7b12f" data-element_type="column"><div><div data-id="6f2207e" data-element_type="widget" data-widget_type="text-editor.default"><p><em>* Technically it’s a rasterization unit rather than a neural unit, but in all current Splat implementations that are of any power or interest to the synthesis community, it ends up as a neural unit, passed through standard training processes.</em></p></div></div></div></div></section><section data-id="1635297" data-element_type="section"><div><div data-id="a428d54" data-element_type="column"><div><div data-id="a05e096" data-element_type="widget" data-widget_type="text-editor.default"><p><em>Amended Friday, December 15, 2023 13:33:47 EET to clarify ‘neural unit’</em></p></div></div></div></div></section></div></div></div></div>
  </body>
</html>
