<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://utcc.utoronto.ca/~cks/space/blog/web/WebServersShouldServeMinimally">Original</a>
    <h1>Web servers should refuse requests for random, unnecessary URLs</h1>
    
    <div id="readability-page-1" class="page"><div><h2>Web servers should refuse requests for random, unnecessary URLs</h2>

	<p><small>July  4, 2023</small></p>
</div><div><p><a href="https://support.cs.toronto.edu/">We</a> periodically check our own
networks with an (open source) vulnerability scanner, whose rules
get updated from time to time. Recently a scan report lit up with
a lot of reports to the effect of &#39;a home directory is accessible
via this web server&#39; for our machines. The web servers in question
were all on port 9100, and the reason the security scanner triggered
this alert is that it could successfully request a couple of URLs
like &#39;<code>/.bash_history</code>&#39; from them.</p>

<p>As you might guess, this is a false positive. On our machines, TCP
port 9100 is where <a href="https://github.com/prometheus/node_exporter">the Prometheus host agent</a> listens so that it
can be scraped by <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/PrometheusGrafanaSetup-2019">our Prometheus server</a>, and it definitely wasn&#39;t
serving anyone&#39;s home directories (although the host agent is a
HTTP server, because HTTP is basically the universal protocol at
this point). What was happening instead is that the Prometheus
host agent&#39;s HTTP server code will give you a HTTP 200 answer (with
a generic front page) for any URL except the special URL for its
metrics endpoint. Since the security scanner asked for various
URLs like &#39;<code>/.bash_history</code>&#39; and got a HTTP 200 response, it
decided each of the machines it checked on port 9100 had that
vulnerability.</p>

<p>Neither party is exactly wrong here, but the result is not ideal.
Given that security scanners and other things like them aren&#39;t
uncommon, my view is that web servers should try to be more selective.
A web server like this can actually be selective without even
changing the HTML served; all it would need to do is only give a
HTTP 200 response for &#39;/&#39; and then a 404 (with the same HTML) for
everything else that it answers with the generic front page. This
would have the same functional result (visitors would get a page
with the URL of the metrics endpoint), but avoid false positives
from security scanners and anything else poking around.</p>

<p>(In practice, web browsers and people mostly don&#39;t care about or
notice the HTTP return code. The browser presentation of the HTML
of a HTTP error page is generally identical to the presentation of
the same HTML from an URL that had a HTTP 200 success reply.)</p>

<p>Ideally, the APIs of web service libraries would make it easy to
do this. Here, <a href="https://pkg.go.dev/net/http#ServeMux">the Go net/http ServeMux API</a> is less than ideal, since
there&#39;s no simple way to register something that handles only the
root URL and not everything under it. Instead, your request handler
has to specifically check for this case (as covered in the example
for <a href="https://pkg.go.dev/net/http#ServeMux.Handle">ServeMux&#39;s Handle() method</a>).</p>

<p>PS: Security scanners and other tools could adopt various heuristics
to detect this sort of situation and reduce false positives, but
ultimately they&#39;re only heuristics, which means they&#39;ll always be
incomplete and sometimes may be wrong. Dealing with this in the web
server is the better way.</p>
</div></div>
  </body>
</html>
