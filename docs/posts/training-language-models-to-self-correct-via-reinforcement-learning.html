<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2409.12917">Original</a>
    <h1>Training Language Models to Self-Correct via Reinforcement Learning</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A">Aviral Kumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhuang,+V">Vincent Zhuang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+R">Rishabh Agarwal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+Y">Yi Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Co-Reyes,+J+D">John D Co-Reyes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+A">Avi Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Baumli,+K">Kate Baumli</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Iqbal,+S">Shariq Iqbal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bishop,+C">Colton Bishop</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roelofs,+R">Rebecca Roelofs</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L+M">Lei M Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McKinney,+K">Kay McKinney</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shrivastava,+D">Disha Shrivastava</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Paduraru,+C">Cosmin Paduraru</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tucker,+G">George Tucker</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Precup,+D">Doina Precup</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Behbahani,+F">Feryal Behbahani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Faust,+A">Aleksandra Faust</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2409.12917">View PDF</a></p><blockquote>
            <span>Abstract:</span>Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM&#39;s self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model&#39;s own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model&#39;s own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models&#39; self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Vincent Zhuang [<a href="https://arxiv.org/show-email/eac614a3/2409.12917">view email</a>]      </p></div></div>
  </body>
</html>
