<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search">Original</a>
    <h1>Show HN: LLM Verified with Monte Carlo Tree Search</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This prototype synthesizes verified code with an LLM.</p>
<p dir="auto">Using Monte Carlo Tree Search (MCTS), it explores the space of possible generation of a verified program, and it checks at every step that it&#39;s on the right track by calling the verifier.</p>
<p dir="auto">This prototype uses Dafny.</p>
<p dir="auto">Logs for example runs can be found in the <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/log">log</a> directory.
Scroll to the very end of a log to see <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/log/opt0_alt.txt#L7661">a chosen solution</a>.
Note that the linked solution is optimal for the problem.</p>
<p dir="auto">By using this technique, weaker models that might not even know the generated language all that well can compete with stronger models.</p>
<p dir="auto">We can also reinforce the snippets that succeed positively and that fail negatively through <a href="https://huggingface.co/docs/trl/main/en/ppo_trainer" rel="nofollow">PPO training</a>.
The model after PPO can solve the prompts without backtracking!
For example, the <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/log/fact_run_after_ppo_opt0.txt">log</a> for solving the problem <code>fact</code> after PPO training on another problem, <code>opt0</code>.</p>
<h2 tabindex="-1" id="user-content-running" dir="auto"><a href="#running">Running<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">This project relies on GPU access. It has been tested on a multi-GPU machine with two NVIDIA A100s.</p>
<h3 tabindex="-1" id="user-content-setup" dir="auto"><a href="#setup">Setup<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">Using <code>mamba</code> or equivalently <code>conda</code>:</p>
<div data-snippet-clipboard-copy-content="mamba create --name llm-verified python=3.10
mamba activate llm-verified
pip install -r requirements.txt"><pre><code>mamba create --name llm-verified python=3.10
mamba activate llm-verified
pip install -r requirements.txt
</code></pre></div>
<h3 tabindex="-1" id="user-content-execution" dir="auto"><a href="#execution">Execution<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">Pick an LLM in <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/model.py">model.py</a> and a prompt in <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/prompts.py">prompts.py</a>, then:</p>

<p dir="auto">For the PPO trainer (slow!), do:</p>

<h2 tabindex="-1" id="user-content-todos" dir="auto"><a href="#todos">TODOs<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul>
<li> Support other verifiers in addition to Dafny:
<ul>
<li> Coq</li>
</ul>
</li>
<li> Support other LLM infrastructures in addition to Hugging Face:
<ul>
<li> <a href="https://ollama.ai" rel="nofollow">Ollama</a></li>
</ul>
</li>
<li> Support test cases.</li>
<li> Design a steerable interaction to give human or tool feedback to the LLM.</li>
<li> Design a reinforcement learning scheme, whereas the LLM learns from trial.
<ul>
<li> Evaluate whether the model after PPO suffers degradation for some tasks, even unrelated.</li>
<li> Force the PPO solution to converge to an optimal known one, using it entirely for training rather than discovery.</li>
</ul>
</li>
<li> Get wandb to work.</li>
</ul>
<h2 tabindex="-1" id="user-content-credits" dir="auto"><a href="#credits">Credits<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>The <a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search/blob/main/montecarlo">montecarlo</a> library is adapted from <a href="https://github.com/ImparaAI/monte-carlo-tree-search">ImparaAI/monte-carlo-tree-search</a>.</li>
<li>The inspiration comes from <a href="https://codeaimcts.github.io/" rel="nofollow"><em>Planning with Large Language Models for Code Generation</em> (ICLR 2023)</a>.</li>
</ul>
</article>
          </div></div>
  </body>
</html>
