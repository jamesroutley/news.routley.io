<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2025/May/21/gemini-diffusion/">Original</a>
    <h1>Gemini Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div>



<p><strong><a href="https://deepmind.google/models/gemini-diffusion/">Gemini Diffusion</a></strong>. Another of the announcements from Google I/O yesterday was Gemini Diffusion, Google&#39;s first LLM to use diffusion (similar to image models like Imagen and Stable Diffusion) in place of transformers.</p>
<p>Google describe it like this:</p>
<blockquote>
<p>Traditional autoregressive language models generate text one word – or token – at a time. This sequential process can be slow, and limit the quality and coherence of the output.</p>
<p>Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.</p>
</blockquote>
<p>The key feature then is <em>speed</em>. I made it through the waitlist and tried it out just now and <em>wow</em>, they are not kidding about it being fast.</p>
<p>In this video I prompt it with &#34;Build a simulated chat app&#34; and it responds at 857 tokens/second, resulting in an interactive HTML+JavaScript page (embedded in the chat tool, Claude Artifacts style) within single digit seconds.</p>


<p>The performance feels similar to <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">the Cerebras Coder tool</a>, which used Cerebras to run Llama3.1-70b at around 2,000 tokens/second.</p>
<p>How good is the model? I&#39;ve not seen any independent benchmarks yet, but Google&#39;s landing page for it promises &#34;the performance of Gemini 2.0 Flash-Lite at 5x the speed&#34; so presumably they think it&#39;s comparable to Gemini 2.0 Flash-Lite, one of their least expensive models.</p>
<p>Prior to this the only commercial grade diffusion model I&#39;ve encountered is <a href="https://www.inceptionlabs.ai/introducing-mercury">Inception Mercury</a> back in February this year.</p>
<p><strong>Update</strong>: a correction from <a href="https://news.ycombinator.com/item?id=44057820#44057939">synapsomorphy on Hacker News</a>:</p>
<blockquote>
<p>Diffusion isn&#39;t in place of transformers, it&#39;s in place of autoregression. Prior diffusion LLMs like <a href="https://www.inceptionlabs.ai/introducing-mercury">Mercury</a> still use a transformer, but there&#39;s no causal masking, so the entire input is processed all at once and the output generation is obviously different. I very strongly suspect this is also using a transformer.</p>
</blockquote>
<p>nvtop <a href="https://news.ycombinator.com/context?id=44059646">provided this explanation</a>:</p>
<blockquote>
<p>Despite the name, diffusion LMs have little to do with image diffusion and are much closer to BERT and old good masked language modeling. Recall how BERT is trained:</p>
<ol>
<li>Take a full sentence (&#34;the cat sat on the mat&#34;)</li>
<li>Replace 15% of tokens with a [MASK] token (&#34;the cat [MASK] on [MASK] mat&#34;)</li>
<li>Make the Transformer predict tokens at masked positions. It does it in parallel, via a single inference step.</li>
</ol>
<p>Now, diffusion LMs take this idea further. BERT can recover 15% of masked tokens (&#34;noise&#34;), but why stop here. Let&#39;s train a model to recover texts with 30%, 50%, 90%, 100% of masked tokens.</p>
<p>Once you&#39;ve trained that, in order to generate something from scratch, you start by feeding the model all [MASK]s. It will generate you mostly gibberish, but you can take some tokens (let&#39;s say, 10%) at random positions and assume that these tokens are generated (&#34;final&#34;). Next, you run another iteration of inference, this time input having 90% of masks and 10% of &#34;final&#34; tokens. Again, you mark 10% of new tokens as final. Continue, and in 10 steps you&#39;ll have generated a whole sequence. This is a core idea behind diffusion language models. [...]</p>
</blockquote>



</div></div>
  </body>
</html>
