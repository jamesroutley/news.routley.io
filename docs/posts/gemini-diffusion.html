<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2025/May/21/gemini-diffusion/">Original</a>
    <h1>Gemini Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div>



<p><strong><a href="https://deepmind.google/models/gemini-diffusion/">Gemini Diffusion</a></strong>. Another of the announcements from Google I/O yesterday was Gemini Diffusion, Google&#39;s first LLM to use diffusion (similar to image models like Imagen and Stable Diffusion) in place of transformers.</p>
<p>Google describe it like this:</p>
<blockquote>
<p>Traditional autoregressive language models generate text one word – or token – at a time. This sequential process can be slow, and limit the quality and coherence of the output.</p>
<p>Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.</p>
</blockquote>
<p>The key feature then is <em>speed</em>. I made it through the waitlist and tried it out just now and <em>wow</em>, they are not kidding about it being fast.</p>
<p>In this video I prompt it with &#34;Build a simulated chat app&#34; and it responds at 857 tokens/second, resulting in an interactive HTML+JavaScript page (embedded in the chat tool, Claude Artifacts style) within single digit seconds.</p>


<p>The performance feels similar to <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">the Cerebras Coder tool</a>, which used Cerebras to run Llama3.1-70b at around 2,000 tokens/second.</p>
<p>How good is the model? I&#39;ve not seen any independent benchmarks yet, but Google&#39;s landing page for it promises &#34;the performance of Gemini 2.0 Flash-Lite at 5x the speed&#34; so presumably they think it&#39;s comparable to Gemini 2.0 Flash-Lite, one of their least expensive models.</p>
<p>Prior to this the only commercial grade diffusion model I&#39;ve encountered is <a href="https://www.inceptionlabs.ai/introducing-mercury">Inception Mercury</a> back in February this year.</p>
<p><strong>Update</strong>: a correction from <a href="https://news.ycombinator.com/item?id=44057820#44057939">synapsomorphy on Hacker News</a>:</p>
<blockquote>
<p>Diffusion isn&#39;t in place of transformers, it&#39;s in place of autoregression. Prior diffusion LLMs like <a href="https://www.inceptionlabs.ai/introducing-mercury">Mercury</a> still use a transformer, but there&#39;s no causal masking, so the entire input is processed all at once and the output generation is obviously different. I very strongly suspect this is also using a transformer.</p>
</blockquote>



</div></div>
  </body>
</html>
