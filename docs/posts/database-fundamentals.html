<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tontinton.com/posts/database-fundementals/">Original</a>
    <h1>Database Fundamentals</h1>
    
    <div id="readability-page-1" class="page"><div><article><section><p>About a year ago, I tried thinking which database I should choose for my next project, and came to the realization that I don&#39;t really know the differences of databases enough. I went to different database websites and saw mostly marketing and words I don&#39;t understand.</p><p>This is when I decided to read the excellent books <code>Database Internals</code> by Alex Petrov and <code>Designing Data-Intensive Applications</code> by Martin Kleppmann.</p><p>The books piqued my curiosity enough to write my own little database I called <a href="https://github.com/tontinton/dbeel">dbeel</a>.</p><p>This post is basically a short summary of these books, with a focus on the fundamental problems a database engineer thinks about in the shower.</p><p>Let&#39;s start with the simplest database program ever written, just 2 bash functions (we&#39;ll call it <code>bashdb</code>):</p><pre data-lang="bash"><code data-lang="bash"><span>#!/bin/bash
</span><span>
</span><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>&#34;$</span><span>1</span><span>,$</span><span>2</span><span>&#34; </span><span>&gt;&gt;</span><span> database
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    </span><span>grep </span><span>&#34;^$</span><span>1</span><span>,&#34;</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>&#34;s/^$</span><span>1</span><span>,//&#34; </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>}
</span></code></pre><p>Try it out:</p><pre data-lang="sh"><code data-lang="sh"><span>$</span><span> db_set 500 </span><span>&#39;{&#34;movie&#34;: &#34;Airplane!&#34;, &#34;rating&#34;: 9}&#39;
</span><span>
</span><span>$</span><span> db_set 111 </span><span>&#39;{&#34;movie&#34;: &#34;Tokio Drift&#34;, &#34;rating&#34;: 6}&#39;
</span><span>
</span><span>$</span><span> db_get 500
</span><span>{</span><span>&#34;movie&#34;</span><span>: </span><span>&#34;Airplane!&#34;</span><span>, </span><span>&#34;rating&#34;</span><span>: 9}
</span></code></pre><p>Before you continue reading, I want you to pause and think about why you wouldn&#39;t use <code>bashdb</code> in production.</p><pre><code><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>Some space for you to think :)
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span></code></pre><p>You probably came up with at least a dozen issues in <code>bashdb</code>. Now I won&#39;t go over <em>all</em> of the possible issues, for this post I will focus on the following ones:</p><ul><li><strong>Durability</strong> - If the machine crashes after a successful <code>db_set</code>, the data might be lost, as it was not flushed to disk.</li><li><strong>Atomicity</strong> - If the machine crashes while you call <code>db_set</code>, data might be written partially, corrupting our data.</li><li><strong>Isolation</strong> - If one process calls <code>db_get</code>, while another calls <code>db_set</code> concurrently on the same item, the first process might read only part of the data, leading to a corrupt result.</li><li><strong>Performance</strong> - <code>db_get</code> uses <code>grep</code>, so search goes line by line and is <code>O(n)</code>, <code>n</code> = all items saved.</li></ul><p>Could you figure out these problems yourself? If you could, well done, you don&#39;t need me, you already understand databases 😀</p><p>In the next section, we&#39;ll try get rid of these problems, to make <code>bashdb</code> a <em>real</em> database we might use in production (not really, please don&#39;t, just use <code>PostgreSQL</code>).</p><h2 id="improving-bashdb-to-be-acid">Improving bashdb to be ACID</h2><p>Before we begin, know that I did not come up with most of these problems on my own, they are part of an acronym named <code>ACID</code>, which almost all databases strive to guarantee:</p><ul><li><strong>Atomicity</strong> - Not to be confused with multi-threading&#39;s definition of atomicity (which is more similar to isolation), a transaction is considered atomic when a fault happens in the middle of a write, and the database either undos or aborts it completely, as if the write never started, leaving no partially written data.</li><li><strong>Consistency</strong> - This one doesn&#39;t really belong on ACID as a property of database transactions, as it is a property of the application.</li><li><strong>Isolation</strong> - No race conditions in concurrent accesses to the same data. There are multiple isolation levels, and we will discuss some of them later.</li><li><strong>Durability</strong> - The first thing that comes to mind when talking about a database. It should store data you wrote to it, forever, even in the event of monkeys pulling the power plug out.</li></ul><blockquote><p>Not all database transactions need to guarantee ACID, for some use cases, it is fine to drop guarantees for performance reasons.</p></blockquote><p>But <em>how</em> can we make <code>bashdb</code> ACID?</p><p>We can start with durability, as it&#39;s pretty easy to make <code>bashdb</code> durable by running <code>sync</code> right after writing in <code>db_set</code>:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>&#34;$</span><span>1</span><span>,$</span><span>2</span><span>&#34; </span><span>&gt;&gt;</span><span> database </span><span>&amp;&amp; </span><span>sync</span><span> -d</span><span> database
</span><span>}
</span></code></pre><p>But wait a minute, what is going on, what is <code>sync</code> really doing? And what is that <code>-d</code>?</p><h3 id="durability">Durability</h3><p>The <code>write</code> syscall writes a buffer to a file, but who said it writes to disk?</p><p>The buffer you write could end up in any cache along the way to the non volatile memory. For example, the kernel stores the buffer in the page cache with each page marked as dirty, meaning it will flush it to disk sometime in the future.</p><p>To make matters worse, the disk device, or something managing your disks (for example a RAID system), might have a write cache as well.</p><p>So how do you tell all the systems in the middle to flush all dirty pages to the disk? For that we have <code>fsync</code> / <code>fdatasync</code>, let&#39;s see what <code>man</code> has to say:</p><pre><code><span>$ man 2 fsync
</span><span>
</span><span>...
</span><span>
</span><span>fsync() transfers (&#34;flushes&#34;) all modified in-core data of (i.e., modified buffer cache pages for)
</span><span>the file referred to by the file descriptor fd to the disk device (or other permanent storage
</span><span>device) so that all changed information can be retrieved even if the system crashes or is rebooted.
</span><span>This includes writing through or flushing a disk cache if present.
</span><span>The call blocks until the device reports that the transfer has completed.
</span><span>
</span><span>...
</span><span>
</span><span>fdatasync() is similar to fsync(), but does not flush modified metadata unless that metadata itself
</span><span>in order to allow a subsequent data  retrieval to be correctly handled.
</span><span>
</span><span>...
</span></code></pre><p>In short, <code>fdatasync</code> flushes the dirty raw buffers we gave <code>write</code>. <code>fsync</code> also flushes the file&#39;s metadata like <code>mtime</code>, which we don&#39;t really care about.</p><p>The <code>sync</code> program is basically like running <code>fsync</code> on all dirty pages, unless a specific file is specified as one of the arguments. It has the <code>-d</code> flag for us to call <code>fdatasync</code> instead of <code>fsync</code>.</p><p>The biggest drawback in adding <code>sync</code> is that we get worse performance. Usually sync is slower than even the write itself. But hey, at least we are now <em>durable</em>.</p><blockquote><p>A short but important note about fsync. When fsync() returns success it means &#34;all writes since the last fsync have hit disk&#34; when you might have assumed it means &#34;all writes since the last SUCCESSFUL fsync have hit disk&#34;. PostgreSQL learned about this only recently (2018), which led to them modifying the behavior of syncing from retrying fsync until a success is returned, to simply panic on fsync failure. This incident got famous and was named fsyncgate. You can learn a lot more about fsync failures <a href="https://www.usenix.org/system/files/atc20-rebello.pdf">here</a>.</p></blockquote><blockquote><p>Dear <code>MongoDB</code> users, know that by default writes are <a href="https://www.mongodb.com/docs/manual/core/journaling/writes">synced every 100ms</a>, meaning it is not 100% durable.</p></blockquote><h3 id="isolation">Isolation</h3><p>The simplest way to have multiprocess isolation in <code>bashdb</code> is to add a lock before we read / write to the storage file.</p><p>There&#39;s a program in linux called <code>flock</code>, which locks a file, and you can even provide it with the <code>-s</code> flag, to specify that you will not modify the file, meaning all callers who specify <code>-s</code> are allowed to read the file concurrently. <code>flock</code> blocks until it has taken the lock.</p><blockquote><p>flock simply calls the flock syscall</p></blockquote><p>With such an awesome program, <code>bashdb</code> can guarantee <em>isolation</em>, here&#39;s the code:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> 9 </span><span>&amp;&amp; </span><span>echo </span><span>&#34;$</span><span>1</span><span>,$</span><span>2</span><span>&#34; </span><span>&gt;&gt;</span><span> database </span><span>&amp;&amp; </span><span>sync</span><span> -d</span><span> database
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> -s</span><span> 9 </span><span>&amp;&amp; </span><span>grep </span><span>&#34;^$</span><span>1</span><span>,&#34;</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>&#34;s/^$</span><span>1</span><span>,//&#34; </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span></code></pre><p>The biggest drawback is that we are now locking the entire database whenever we write to it.</p><p>The only things left are atomicity and improving the algorithm to not be <code>O(n)</code>.</p><h2 id="bad-news">Bad News</h2><p>I&#39;m sorry, this is as far as I could get with <code>bashdb</code>, I could not find a simple way to ensure atomicity in bash ☹️</p><p>I mean you could somehow probably use <code>mv</code> for this, I&#39;ll leave it as an excercise for you.</p><p>And even if it was possible, we still need to fix the <code>O(n)</code> situation.</p><p>Before beginning the <code>bashdb</code> adventure, I knew that we won&#39;t be able to easily solve all these problems in less than 10 lines of bash, but by trying to, you&#39;ve hopefully started to get a feel for the problems database engineers face.</p><p>Let&#39;s start with the first big component of a database, the <code>Storage Engine</code>.</p><p>The purpose of the storage engine is to provide an abstraction over reading and writing data to persistent storage, with the main goal to be <strong>fast</strong>, i.e. have <strong>high throughput</strong> and <strong>low latency</strong> on requests.</p><p>But what makes software slow?</p><pre><code><span>Latency Comparison Numbers (~2012)
</span><span>----------------------------------
</span><span>L1 cache reference                           0.5 ns
</span><span>Branch mispredict                            5   ns
</span><span>L2 cache reference                           7   ns                      14x L1 cache
</span><span>Mutex lock/unlock                           25   ns
</span><span>Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
</span><span>Compress 1K bytes with Zippy             3,000   ns        3 us
</span><span>Send 1K bytes over 1 Gbps network       10,000   ns       10 us
</span><span>Read 4K randomly from SSD              150,000   ns      150 us          ~1GB/sec SSD
</span><span>Read 1 MB sequentially from memory     250,000   ns      250 us
</span><span>Round trip within same datacenter      500,000   ns      500 us
</span><span>Read 1 MB sequentially from SSD      1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
</span><span>Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
</span><span>Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD
</span><span>Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms
</span></code></pre><p>If L1 cache reference took as long as a heart beat (around half a second), reading 1 MB sequentially from SSD would take ~12 days and reading 1 MB sequentially from disk would take ~8 months.</p><p>This is why the main limitation of storage engines is the disk itself, and thus all designs try to minimize disk I/O and disk seeks as much as possible. Some designs even get rid of disks in favor of SSDs (although they are much more expensive).</p><p>A storage engine design usually consists of:</p><ul><li>The underlying data structure to store items on disk.</li><li>ACID transactions. <ul><li>Some may skip this to achieve better performance for specific use cases where ACID is not important.</li></ul></li><li>Some cache - to not read from disk <em>every</em> time. <ul><li>Most use buffered I/O to let the OS cache for us.</li></ul></li><li>API layer - SQL / document / graph / ...</li></ul><p>Storage engine data structures come in all shapes and sizes, I&#39;m going to focus on the 2 categories you will most likely find in the wild - mutable and immutable data structures.</p><p>Mutable means that after writing data to a file, the data can be overwritten later in the future, while immutable means that after writing data to a file, it can only be read again.</p><h2 id="mutable-b-trees">Mutable B-Trees</h2><p>To achieve the goal of maintaining good performance as the amount of data scales up, the data structure we use should be able to search an item in at most logarithmic time, and not linear time like in <code>bashdb</code>.</p><p>A simple data structure you are probably familiar with is the BST (binary search tree), where lookups are made in <code>O(log n)</code> time.</p><p>The problem with BSTs is nodes are placed randomly apart from each other, which means that after reading a node while traversing the tree, the next node is most likely going to be somewhere far away on disk. To minimize disk I/O &amp; seeks, each page read from disk should be read as much as possible from memory again, without reaching to disk.</p><p>The property we&#39;re looking for is called &#34;spatial locality&#34;, and one of the most famous &#34;spatially local&#34; variations of BSTs are B-trees.</p><p>B-tree generalizes BST, allowing for nodes with more than two children. Here&#39;s what they look like:</p><pre><code><span>                  ------------------------------------
</span><span>                  |     7     |     16     |    |    |
</span><span>                  ------------------------------------
</span><span>                 /            |             \
</span><span>-----------------     ----------------       -----------------
</span><span>| 1 | 2 | 5 | 6 |     | 9 | 12 |  |  |       | 18 | 21 |  |  |
</span><span>-----------------     ----------------       -----------------
</span></code></pre><p>With the search algorithm in pseudo python code:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get</span><span>(</span><span>node</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>for </span><span>i, child </span><span>in </span><span>enumerate</span><span>(node</span><span>.</span><span>children):
</span><span>        </span><span>if not </span><span>child:
</span><span>            </span><span>return </span><span>None
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>== </span><span>key:
</span><span>            </span><span># Found it!
</span><span>            </span><span>return </span><span>child</span><span>.</span><span>value
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>&gt; </span><span>key:
</span><span>            </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[i], key)
</span><span>
</span><span>    </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[</span><span>-</span><span>1</span><span>], key)
</span></code></pre><p>On each read of a page from disk (usually 4KB or 8KB), we iterate over multiple nodes sequentially from memory and the various CPU caches, trying to keep the least amount of bytes read go to waste.</p><p>Remember, reading from memory and the CPU caches is a few order of magnitudes faster than disk, so much faster in fact, that it can be considered to be basically free in comparison.</p><p>I know some of you reading this right now think to themselves <em>&#34;Why not binary search instead of doing it linearly?&#34;</em>, to you I say, please look at the L1 / L2 cache reference times in the latency comparison numbers table again. Also, modern CPUs execute multiple operations in parallel when it operates on sequential memory thanks to <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>, <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">instruction pipelining</a> and <a href="https://en.wikipedia.org/wiki/Cache_prefetching">prefetching</a>. You would be surprised just how far reading sequential memory can take you in terms of performance.</p><p>There&#39;s a variation of the B-tree that takes this model even further, called a B+ tree, where the final leaf nodes hold a value and all other nodes hold only keys, thus fetching a page from disk results in a lot more keys to compare.</p><p>B-trees, to be space optimized, need to sometimes reclaim space as a consequence of data fragmentation created by operations on the tree like:</p><ul><li>Big value updates - updating a value into a larger value might overwrite data of the next node, so the tree relocates the item to a different location, leaving a &#34;hole&#34; in the original page.</li><li>Small value updates - updating a value to a smaller value leaves a &#34;hole&#34; at the end.</li><li>Deletes - deletion causes a &#34;hole&#34; right where the deleted value used to reside.</li></ul><p>The process that takes care of space reclamation and page rewrites can sometimes be called vacuum, compaction, page defragmentation, and maintenance. It is usually done in the background to not interfere and cause latency spikes to user requests.</p><blockquote><p>See for example how in <code>PostgreSQL</code> you can configure an <a href="https://www.postgresql.org/docs/current/routine-vacuuming.html">auto vacuum daemon</a>.</p></blockquote><p>B-trees are most commonly used as the underlying data structure of an index (<code>PostgreSQL</code> creates B-tree indexes by default), or all data (I&#39;ve seen <code>DynamoDB</code> once jokingly called <em>&#34;a distributed B-tree&#34;</em>).</p><h2 id="immutable-lsm-tree">Immutable LSM Tree</h2><p>As we have already seen in the latency comparison numbers table, disk seeks are really expensive, which is why the idea of sequentially written immutable data structures got so popular.</p><p>The idea is that if you only append data to a file, the disk needle doesn&#39;t need to move as much to the next position where data will be written. On write heavy workloads it has been proven very beneficial.</p><p>One such append only data structure is called the <code>Log Structured Merge tree</code> or <code>LSM tree</code> in short, and is what powers <em>a lot</em> of modern database storage engines, such as <code>RocksDB</code>, <code>Cassandra</code> and my personal favorite <code>ScyllaDB</code>.</p><p>LSM trees&#39; general concept is to buffer writes to a data structure in memory, preferably one that is easy to iterate in a sorted fashion (for example <code>AVL tree</code> / <code>Red Black tree</code> / <code>Skip List</code>), and once it reaches some capacity, flush it sorted to a new file called a <code>Sorted String Table</code> or <code>SSTable</code>. An SSTable stores sorted data, letting us leverage binary search and sparse indexes to lower the amount of disk I/O.</p><img src="https://tontinton.com/lsm_tree_write.svg"/><p>To maintain durability, when data is written to memory, the action is stored in a <code>Write-Ahead Log</code> or <code>WAL</code>, which is read on program&#39;s startup to reset state to as it was before shutting down / crashing.</p><p>Deletions are also appended the same way a write would, it simply holds a tombstone instead of a value. The tombstones get deleted in the compaction process detailed later.</p><p>The read path is where it a bit wonky, reading from an LSM tree is done by first searching for the item of the provided key in the data structure in memory, if not found, it then searches for the item by iterating over all SSTables on disk, from the newest one to the oldest.</p><img src="https://tontinton.com/lsm_tree_read.svg"/><p>You can probably already tell that as more and more data is written, there will be more SSTables to go through to find an item of a specific key, and even though each file is sorted, going over a lot of small files is slower than going over one big file with all items (lookup time complexity: <code>log(num_files * table_size) &lt; num_files * log(table_size)</code>). This is another reason why LSM trees require compaction, in addition to removing tombstones.</p><p>In other words: compaction combines a few small SSTables into one big SSTable, removing all tombstones in the process, and is usually run as a background process.</p><img src="https://tontinton.com/lsm_tree_compact.svg"/><p>Compaction can be implemented using a binary heap / priority queue, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>compact</span><span>(</span><span>sstables</span><span>, </span><span>output_sstable</span><span>): 
</span><span>    </span><span># Ordered by ascending key. pop() results in the item of the smallest key.
</span><span>    heap </span><span>= </span><span>heapq</span><span>.</span><span>heapify</span><span>([(sstable</span><span>.</span><span>next</span><span>(), sstable) </span><span>for </span><span>sstable </span><span>in </span><span>sstables])
</span><span>
</span><span>    </span><span>while </span><span>(item, sstable) </span><span>:= </span><span>heap</span><span>.</span><span>pop</span><span>()
</span><span>        </span><span>if not </span><span>item</span><span>.</span><span>is_tombstone</span><span>():
</span><span>            output_sstable</span><span>.</span><span>write</span><span>(item)
</span><span>
</span><span>        </span><span>if </span><span>item </span><span>:= </span><span>sstable</span><span>.</span><span>next</span><span>():
</span><span>            </span><span># For code brevity, imagine pushing an item with a key that exists
</span><span>            </span><span># in the heap removes the item with the smaller timestamp,
</span><span>            </span><span># resulting in last write wins.
</span><span>            heap</span><span>.</span><span>push</span><span>((item, sstable))
</span></code></pre><blockquote><p>For a real working example in rust 🦀, <a href="https://github.com/tontinton/dbeel/blob/ee3de152a5/src/storage_engine/lsm_tree.rs#L1038">click here</a>.</p></blockquote><p>To optimize an LSM tree, you should decide <em>when</em> to compact and on <em>which</em> sstable files. <code>RocksDB</code> for example implements <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction">Leveled Compaction</a>, where the newly flushed sstables are said to reside in level 0, and once a configured N number of files are created in a level, they are compacted and the new file is promoted to the next level.</p><p>It&#39;s important to handle removal of tombstones with care to not cause data resurrection. An item might be removed and then resurrected on compaction with another file that holds that item, even if the write happened before the deletion, there is no way to know once deleted in a previous compaction. <code>RocksDB</code> keeps tombstones around until a compaction of files that result in a promotion to the last level.</p><h3 id="bloom-filters">Bloom Filters</h3><p>LSM trees can be further optimized by something called a bloom filter.</p><p>A bloom filter is a probabilistic set data structure that lets you to efficiently check whether an item doesn&#39;t exist in a set. Checking whether an item exists in the set results in either <code>false</code>, which means the item is definitely not in the set, or in <code>true</code>, which means the item is <strong>maybe</strong> in the set, and that&#39;s why it&#39;s called a <em>probabilistic</em> data structure.</p><p>The beauty is that the space complexity of a bloom filter set of <code>n</code> items is <code>O(log n)</code>, while a regular set with <code>n</code> items is <code>O(n)</code>.</p><p>How do they work? The answer is hash functions! On insertion, they run multiple different hash functions on the inserted key, then take the results and store 1 in the corresponding bit (<code>result % number_of_bits</code>).</p><pre data-lang="python"><code data-lang="python"><span># A bloom filter&#39;s bitmap of size 8 (bits).
</span><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>]
</span><span>
</span><span># Inserting key - first run 2 hash functions.
</span><span>Hash1</span><span>(key1) </span><span>= </span><span>100
</span><span>Hash2</span><span>(key1) </span><span>= </span><span>55
</span><span>
</span><span># Then calculate corresponding bits.
</span><span>bits </span><span>= </span><span>[</span><span>100 </span><span>% </span><span>8</span><span>, </span><span>55 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>4</span><span>, </span><span>7</span><span>]
</span><span>
</span><span># Set 1 to corresponding bits.
</span><span>bloom[</span><span>4</span><span>] </span><span>= </span><span>1
</span><span>bloom[</span><span>7</span><span>] </span><span>= </span><span>1
</span><span>
</span><span># After insertion it should look like:
</span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span></code></pre><p>Now comes the exciting part - checking!</p><pre data-lang="python"><code data-lang="python"><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span><span>
</span><span># To check a key, simply run the 2 hash functions and find the corresponding
</span><span># bits, exactly like you would on insertion:
</span><span>Hash1</span><span>(key2) </span><span>= </span><span>34
</span><span>Hash2</span><span>(key2) </span><span>= </span><span>35
</span><span>
</span><span>bits </span><span>= </span><span>[</span><span>34 </span><span>% </span><span>8</span><span>, </span><span>35 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>2</span><span>, </span><span>3</span><span>]
</span><span>
</span><span># And then check whether all the corresponding bits hold 1, if true, the item
</span><span># maybe exists in the set, otherwise it definitely isn&#39;t.
</span><span>result </span><span>= </span><span>[bloom[</span><span>2</span><span>], bloom[</span><span>3</span><span>]] </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>] </span><span>= </span><span>false
</span><span>
</span><span># false. key2 was never inserted in the set, otherwise those exact same bits
</span><span># would have all been set to 1.
</span></code></pre><blockquote><p>Think about why it is that even when all checked bits are 1, it doesn&#39;t guarantee that the same exact key was inserted before.</p></blockquote><p>A nice benefit of bloom filters is that you can control the chance of being certain that the item doesn&#39;t exist in the set, by allocating more memory for the bitmap and by adding more hash functions. There are even <a href="https://hur.st/bloomfilter/">calculators</a> for it.</p><p>LSM trees can store a bloom filter for each SSTable, to skip searching in SSTables if their bloom filter validates that an item doesn&#39;t exist in it. Otherwise, we search the SSTable normally, even if the item doesn&#39;t necessarily exist in it.</p><h2 id="write-ahead-log">Write Ahead Log</h2><p>Remember ACID? Let&#39;s talk briefly about how storage engines achieve ACID transactions.</p><p>Atomicity and durability are properties of whether data is correct at all times, even when power shuts down the machine.</p><p>The most popular method to survive sudden crashes is to log all transaction actions into a special file called a <code>Write-Ahead Log</code> / <code>WAL</code> (we touched on this briefly in the <code>LSM tree</code> section).</p><p>When the database process starts, it reads the <code>WAL</code> file, and reconstructs the state of the data, skipping all transactions that don&#39;t have a commit log, thus achieving atomicity.</p><p>Also, as long a write request&#39;s data is written + flushed to the <code>WAL</code> file before the user receives the response, the data is going to be 100% read at startup, meaning you also achieve durability.</p><p>WALs are basically a sort of <a href="https://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> of the transactional events.</p><h2 id="isolation-1">Isolation</h2><p>To achieve isolation, you can either:</p><ul><li>Use pessimistic locks - Block access to data that is currently being written to.</li><li>Use optimistic locks - Update a copy of the data and then commit it only whether the data was not modified during the transaction, if it did, retry on the new data. Also known as optimistic concurrency control.</li><li>Read a copy of the data - MVCC (Multiversion concurrency control) is a common method used to avoid blocking user requests. In MVCC when data is mutated, instead of locking + overwriting it, you create a new version of the data that new requests read from. Once no readers remain that are reading the old data it can be safely removed. With MVCC, each user sees a <em>snapshot</em> of the database at a specific instant in time.</li></ul><p>Some applications don&#39;t require perfect isolation (or <code>Serializable Isolation</code>), and can relax their read isolation levels.</p><p>The ANSI/ISO standard SQL 92 includes 3 different possible outcomes from reading data in a transaction, while another transaction might have updated that data:</p><ul><li><strong>Dirty reads</strong> - A dirty read occurs when a transaction retrieves a row that has been updated by another transaction that is not yet committed.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>-- no commit here
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves in 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Non-repeatable reads</strong> - A non-repeatable read occurs when a transaction retrieves a row twice and that row is updated by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Phantom reads</strong> - A phantom read occurs when a transaction retrieves a set of rows twice and new rows are inserted into or removed from that set by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice and Bob
</span><span>	
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>INSERT INTO</span><span> users </span><span>VALUES</span><span> (</span><span>3</span><span>, </span><span>&#39;Carol&#39;</span><span>, </span><span>26</span><span>);
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice, Bob and Carol
</span><span>COMMIT</span><span>;
</span></code></pre><p>Your application might not need a guarantee of no dirty reads for example in a specific transaction, so it can choose a different isolation level to allow greater performance, as to achieve higher isolation levels, you usually sacrifice performance.</p><p>Here are isolation levels defined by the ANSI/SQL 92 standard from highest to lowest (higher levels guarantee at least everything lower levels guarantee):</p><ul><li><strong>Serializable</strong> - The highest isolation level. Reads always return data that is committed, including range based writes on multiple rows (avoiding phantom reads).</li><li><strong>Repeatable reads</strong> - Phantom reads are acceptable.</li><li><strong>Read committed</strong> - Non-repeatable reads are acceptable.</li><li><strong>Read uncommitted</strong> - The lowest isolation level. Dirty reads are acceptable.</li></ul><blockquote><p>The ANSI/SQL 92 standard isolation levels are often criticized for not being complete. For example, many MVCC implementations offer <a href="https://en.wikipedia.org/wiki/Snapshot_isolation">snapshot isolation</a> and not serializable isolation (for the differences, read the provided wikipedia link). If you want to learn more about MVCC, I recommend reading about <a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">HyPer</a>, a fast serializable MVCC algorithm.</p></blockquote><p>So to conclude the storage engine part of this post, the fundamental problems you solve writing a storage engine are: how to store / retrieve data while trying to guarantee some ACID transactions in the most performant way.</p><blockquote><p>One topic I left out is the API to choose when writing a database / storage engine, but I&#39;ll leave a post called <a href="https://www.scattered-thoughts.net/writing/against-sql/">&#34;Against SQL&#34;</a> for you to start exploring the topic yourself.</p></blockquote><p>Going distributed should be a last mile resort, introducing it to a system adds a <strong>ton</strong> of complexity, as we will soon learn. Please avoid using distributed systems when non distributed solutions suffice.</p><blockquote><p>A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable. ~Leslie Lamport</p></blockquote><p>The common use cases of needing to distribute data across multiple machines are:</p><ul><li><strong>Availability</strong> - If for some reason the machine running the database crashes / disconnects from our users, we might still want to let users use the application. By distributing data, when one machine fails, you can simply point requests to another machine holding the &#34;redundant&#34; data.</li><li><strong>Horizontal Scaling</strong> - Conventionally, when an application needed to serve more user requests than it can handle, we would have upgraded the machine&#39;s resources (faster / more disk, RAM, CPUs). This is called <code>Vertical Scaling</code>. It can get very expensive and for some workloads there just doesn&#39;t exist hardware to match the amount of resources needed. Also, most of the time you don&#39;t need all those resources, except in peaks of traffic (imagine Shopify on Black Friday). Another strategy called <code>Horizontal Scaling</code>, is to operate on multiple separate machines connected over a network, seemingly working as a single machine.</li></ul><p>Sounds like a dream, right? What can go wrong with going distributed?</p><p>Well, you have now introduced operational complexity (deployments / etc...) and more importantly partitioning / network partitioning, infamous for being the P in something called the CAP theorem.</p><p>The CAP theorem states that a system can guarantee only 2 of the following 3:</p><ul><li><strong>Consistency</strong> - Reads receive the most recent write.</li><li><strong>Availability</strong> - All requests succeed, no matter the failures.</li><li><strong>Partition Tolerance</strong> - The system continues to operate despite dropped / delayed messages between nodes.</li></ul><p>To understand why this is, imagine a database operating on a single machine. It is definitely <em>partition tolerant</em>, as messages in the system are not sent through something like a network, but through function calls operating on the same hardware (CPU / memory). It is also <em>consistent</em>, as the state of the data is saved on the same hardware (memory / disk) that all other read / write requests operate on. Once the machine fails (be it software failures like SIGSEGV or hardware failures like the disk overheating) all new requests to it fail, violating <em>availability</em>.</p><p>Now imagine a database operating on 2 machines with separate CPUs, memory and disks, connected through some cable. When a request to one of the machines fails, for whatever reason, the system can choose to do one of the following:</p><ul><li>Cancel the request, thus sacrificing <em>availability</em> for <em>consistency</em>.</li><li>Allow the request to continue only on the working machine, meaning once the other machine will now have inconsistent data (reads from it will not return the most recent write), thus sacrificing <em>consistency</em> for <em>availability</em>. When a system does this, it is called eventually consistent.</li></ul><p>The original <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">dynamo paper</a> is famous for many things, one of them being Amazon stating that amazon.com&#39;s shopping cart should be highly available, and that it&#39;s more important to them than consistency. In the unlikely scenario a user sees 2 of the same item in the shopping cart, they will simply remove one of them, which is a better situation then them not being able to purchase and pay money!</p><blockquote><p>I really enjoy out of the box thinking of sacrificing something that adds software complexity (like consistency in Amazon&#39;s shopping cart) for a simpler human solution like the user getting a refund. Software complexity can get more expensive to operate than having a refund budget for example.</p></blockquote><p>To achieve <em>availability</em> it&#39;s not enough to have multiple nodes together combining all the data, there must also be data redundancy, or in other words, for each item a node stores there must be at least 1 other node to store a copy of that item. These nodes are usually called <strong>replicas</strong>, and the process of copying the data is called <strong>replication</strong>.</p><p>Assigning more replica nodes means that the system will be more <em>available</em>, with the obvious drawback of needing more resources to store all these copies.</p><blockquote><p>Copies of data don&#39;t need to be stored &#34;whole&#34;, they can be split and scattered across multiple nodes using a technique called erasure coding, which also has some interesting <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">latency characteristics</a> (by the way brooker&#39;s blog is simply amazing for learning distributed systems).</p></blockquote><h2 id="consistent-hashing">Consistent Hashing</h2><p>Now that you have multiple nodes, you need some kind of load balancing / data partitioning method. When a request to store some data comes in, how do you determine which node receives the request?</p><p>You could go for the simplest solution, which is to simply always take a primary key (some id) in addition to the data, hash the key and modulo the result by the number of available nodes, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>return </span><span>nodes[</span><span>hash</span><span>(key) </span><span>% </span><span>len</span><span>(nodes)] 
</span></code></pre><p>This modulo method works fine, until a node is either added or removed from the cluster. Once that happens, the calculation returns a different result because the number of available nodes changed, meaning a different node will be selected for the same key. To accommodate, each node can migrate keys that should now live on different nodes, but then almost all items are migrated, which is really expensive.</p><p>One method to lower the amount of items to be migrated on node addition / removal that is used by some databases (e.g. <code>Dynamo</code> and <code>Cassandra</code>) is <code>Consistent Hashing</code>.</p><p>Consistent hashing creates a ring of nodes instead of an array, placing each node&#39;s name hash on the ring. Then each request&#39;s key is hashed just like before, but instead of doing the modulo operation, we get the first node in the ring whose name&#39;s hash is greater or equal to the request key hash:</p><pre data-lang="python"><code data-lang="python"><span># Assume nodes are sorted, with the first node having the smallest hash value.
</span><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>if </span><span>len</span><span>(nodes) </span><span>== </span><span>0</span><span>:
</span><span>        </span><span>return </span><span>None
</span><span>
</span><span>    key_hash </span><span>= </span><span>hash</span><span>(key)
</span><span>
</span><span>    </span><span>for </span><span>node </span><span>in </span><span>nodes:
</span><span>        </span><span>if </span><span>node</span><span>.</span><span>hash </span><span>&gt;= </span><span>key_hash:
</span><span>            </span><span>return </span><span>node
</span><span>
</span><span>    </span><span>return </span><span>nodes[</span><span>0</span><span>]
</span></code></pre><p>For a visual explanation, imagine a ring that goes from 0 -&gt; 99, holding nodes with the names &#34;half&#34;, &#34;quarter&#34; and &#34;zero&#34; whose hashes are 50, 25 and 0 respectively:</p><pre><code><span>   zero
</span><span> /      \
</span><span>|     quarter 
</span><span> \      /
</span><span>   half
</span></code></pre><p>Let&#39;s say a user now wants to set an item with the key &#34;four-fifths&#34;, with a hash value of 80. The first node with a name hash greater or equal to 80 is &#34;half&#34; (with hash value of 50), so that&#39;s the node to receive the request!</p><p>Choosing replicas is very simple, when an item is set to be stored on a specific node, go around the ring counter-clockwise, the next node will store a copy of that item. In our example, &#34;zero&#34; is the replica node for all items &#34;half&#34; owns, so when &#34;half&#34; dies and requests will now be routed to &#34;zero&#34;, it can serve these requests, keeping our system <em>available</em>. This method is sometimes called <code>Leaderless Replication</code> and is used by &#34;Dynamo&#34; style databases like <code>Cassandra</code>.</p><blockquote><p>Another method is to choose a leader node and replica nodes is <code>Leader Election</code>, which is a huge topic on its own that I won&#39;t get into in this post.</p></blockquote><p>Now, what happens when a node is added to the cluster? Let&#39;s add a node named &#34;three-quarters&#34; with a hash value of 75, the item &#34;four-fifths&#34; should be migrated to the new &#34;three-quarters&#34; node, as new requests to it will now point to it.</p><p>This migration process is a lot less expensive than what we previously had in the modulo solution. The number of keys that need to be migrated is equal to <code>num_keys / num_nodes</code> on average.</p><p>A cool trick is to introduce the concept of virtual nodes, where you add multiple instances of a node to the ring, to lower the chances of some nodes owning more items than other nodes (in our example &#34;half&#34; will store twice as many items on average than the other nodes). You can generate virtual node names by for example adding an index as a suffix to the node name (&#34;half-0&#34;, &#34;half-1&#34;, etc...) and then the hash will result in a completely different location on the ring.</p><p>Here&#39;s a more detailed example of a migration in a cluster with a replication factor of 3:</p><img src="https://tontinton.com/migration.svg"/><blockquote><p>Same colored nodes are virtual nodes of the same node, green arrows show to which node an item is being migrated to, red arrows show item deletions from nodes and the brown diamonds are items.</p></blockquote><h2 id="leaderless-replication">Leaderless Replication</h2><p>In a leaderless setup, you get amazing <em>availability</em>, while sacrificing <em>consistency</em>. If the owning node is down on a write request, it will be written to the replica, and once the owning node is up and running again, a read request will read stale data.</p><p>When <em>consistency</em> is needed for a specific request, read requests can be sent in parallel to several replica nodes as well as to the owning node. The client will pick the most up to date data. Write requests are usually sent in parallel to all replica nodes but wait for an acknowledgement from only some of them. By choosing the number of read requests and number of write requests acknowledge, you can tune the <em>consistency</em> level on a request level.</p><p>To know whether a request is <em>consistent</em>, you just need to validate that <code>R + W &gt; N/2 + 1</code>, where:</p><ul><li><strong>N</strong> - Number of nodes holding a copy of the data.</li><li><strong>W</strong> - Number of nodes that will acknowledge a write for it to succeed.</li><li><strong>R</strong> - Number of nodes that have to respond to a read operation for it to succeed.</li></ul><blockquote><p>Sending a request to a majority of nodes (where <code>W</code> or <code>R</code> is equal to <code>N/2 + 1</code>) is called a quorum.</p></blockquote><p>Picking the correct read as the latest written one is called <code>Conflict Resolution</code> and it is not a simple task, you might think that simply comparing timestamps and choosing the biggest one is enough, but using times in a distributed system are unreliable.</p><blockquote><p>This didn&#39;t stop <a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html#data-versioning">Cassandra from using timestamps</a> though.</p></blockquote><p>Each machine has its own hardware clock, and the clocks <em>drift</em> apart as they are not perfectly accurate (usually a quartz crystal oscillator). Synchronizing clocks using NTP (Network Time Protocol), where a server returns the time from a more accurate time source such as a GPS receiver, is not enough to provide accurate results, as the NTP request is over the network (another distributed system) and we can&#39;t know exactly how much time will pass before receiving a response.</p><blockquote><p>Google&#39;s <code>Spanner</code> actually did achieve consistency with clocks, by uses special high precision time hardware and its API exposes the time range uncertainty of each timestamp. You can read more about it <a href="https://research.google/pubs/pub39966.pdf">here</a>.</p></blockquote><p>But if clocks are so unreliable, how else are we supposed to know which value is correct?</p><p>Some systems (for example <code>Dynamo</code>) try to solve this partially using <code>Version Vectors</code>, where you attach a (node, counter) pair for each version of an item, which gives you the ability to find causality between the different versions. By finding versions of values that are definitely newer (have a higher counter) you can remove some versions of a value, which makes the problem easier.</p><img src="https://tontinton.com/version_vector.svg"/><blockquote><p>An example showing how easily conflicts arise. At the end we are left with {v2, v3} as the conflicting values for the same key. The reason I removed v1 is to show that by using something like <code>Version Vectors</code>, versions of values can be safely removed to minimize the amount of conflicts. To learn more on <code>Version Vectors</code> and their implementations, I recommend reading <a href="https://github.com/ricardobcl/Dotted-Version-Vectors">Dotted Version Vectors</a>.</p></blockquote><p>We could also decide to simply let the application decide how to deal with conflicts, by returning all conflicting values for the requested item. The application might know a lot more on the data than the database, so why not let it resolve conflicts? This is what <code>Riak KV</code> does for example.</p><blockquote><p>An idea I think about often is that you could even allow users to compile conflict resolution logic as a WASM module, and upload it to the database, so that when conflicts occur, the database resolves them, never relying on the application.</p></blockquote><p>There are lots of different ideas to reduce conflicts in an eventually consistent system, they usually fall under the umbrella term <code>Anti Entropy</code>.</p><h2 id="anti-entropy">Anti Entropy</h2><p>Here are examples of some of the most popular <code>Anti Entropy</code> mechanisms:</p><p><strong>Read Repair</strong> - After a client chooses the &#34;latest&#34; value from a read request that went to multiple nodes (by conflict resolution), it sends that value back to all the nodes that don&#39;t currently store that value, thus <em>repairing</em> them.</p><p><strong>Hinted Handoff</strong> - When a write request can&#39;t reach one of the target nodes, send it instead as a &#34;hint&#34; to some other node. As soon as that target node is available again, send it the saved &#34;hint&#34;. On a quorum write, this mechanism is also called <code>Sloppy Quorum</code>, which provides even better <em>availability</em> for quorum requests.</p><p><strong>Merkle Trees</strong> - Because read repair only fixes queried data, a lot of data can still become inconsistent for a long time. Nodes can choose to start a synchronization process by talking to each other and see the differences in data. This is really expensive when there is a lot of data (<code>O(n)</code>). To make the sync algorithm faster (<code>O(log n)</code>) we can introduce <a href="https://en.wikipedia.org/wiki/Merkle_tree">merkle trees</a>. A merkle tree stores the hash of a range of the data in lowest leaf nodes, with the parent leaf nodes being a combined hash of the 2 of its children, thus creating a hierarchy of hashes up to the root of the tree. The sync process now starts by one node comparing the root of the merkle tree to another node&#39;s merkle tree, if the hashes are the same, it means they have exactly the same data. If the hashes differ, the leaf hashes are checked the same way, recursively until the inconsistent data is found.</p><p><strong>Gossip Dissemination</strong> - Send broadcast events to all nodes in the cluster in a simple and reliable way, by imitating how humans spread rumors or a disease. You send the event message to a configured number of randomly chosen nodes (called the &#34;fanout&#34;), then when they receive the message they repeat the process and send the message to another set of randomly chosen <code>N</code> nodes. To not repeat the message forever in the cluster, a node stops broadcasting a gossip message when it sees it a configured number of times. To get a feel for how data converges using gossip, head over to the <a href="https://www.serf.io/docs/internals/simulator.html">simulator</a>! As an optimization, gossip messages are usually sent using UDP, as the mechanism is just that reliable.</p><p>There is a lot more to talk about databases, be it the use of <a href="https://yarchive.net/comp/linux/o_direct.html">O_DIRECT</a> in linux and implementing your own page cache, failure detection in distributed systems, consensus algorithms like <a href="https://raft.github.io/">raft</a>, distributed transactions, leader election, and an almost infinite amount more.</p><p>I hope I have piqued your curiosity enough to explore the world of databases further, or provided the tools for you to better understand which database to pick in your next project 😀</p></section></article></div></div>
  </body>
</html>
