<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://source.opennews.org/articles/so-many-ocr-options/">Original</a>
    <h1>Our search for the best OCR tool (2019)</h1>
    
    <div id="readability-page-1" class="page"><div>
                <p>Do you need to pay a lot of money to get reliable <span>OCR</span> results? Is Google Cloud Vision actually better than Tesseract? Are any cutting edge neural network-based <span>OCR</span> engines worth the time investment of getting them set up? </p>

<p><span>OCR</span>, or optical character recognition, allows us to transform a scan or photograph of a letter or court filing into searchable, sortable text that we can analyze. One of our projects at Factful is to build tools that make state of the art machine learning and artificial intelligence accessible to investigative reporters. We have been testing the components that already exist so we can prioritize our own efforts. </p>

<p>We couldn’t find single side by side comparison of the most accessible <span>OCR</span> options, so we ran a handful of documents through seven different tools, and compared the results. </p>

<p>There are a lot of <span>OCR</span> options available. Some are easy to use, some require a bit of programming to make them work, some require a lot of programming. Some are quite expensive, some are free and open source. </p>

<p>We selected several documents—two easy to read reports, a receipt, an historical document, a legal filing with a lot of redaction, a filled in disclosure form, and a water damaged page—to run through the <span>OCR</span> engines we are most interested in. We tested three free and open source options (Calamari, OCRopus and Tesseract) as well as one desktop app (Adobe Acrobat Pro) and three cloud services (Abbyy Cloud, Google Cloud Vision, and Microsoft Azure Computer Vision). </p>

<p>All the scripts we used, as well as the complete output from each <span>OCR</span> engine, are available on <a href="https://github.com/factful/ocr_testing">GitHub</a>. You can use the scripts to check our work, or to run your own documents against any of the clients we tested. </p>

<p>The quality of results varied between applications, but there wasn’t a stand out winner. Most of the tools handled a clean document just fine. None got perfect results on trickier documents, but most were good enough to make text significantly more comprehensible. In most cases if you need a complete, accurate transcription you’ll have to do additional review and correction. </p>

<h3>What Does the Future Hold?</h3>

<p>The current slate of good document recognition <span>OCR</span> engines use a mix of techniques to read text from images, but they are all optimized for documents. They assume that material fits on a rectangular page. Most start with a line detection process that identifies lines of text in a document and then breaks them down into words or letter forms. Some use a dictionary to improve results—when a string is ambiguous, the engine will err on the side of the known word. A dictionary isn’t always enough, however, <a href="https://wraabe.wordpress.com/2009/03/07/an-ocr-cliche-into-hisher-anus/">as Wesley Raabe learned</a> as he was transcribing the 1879 edition of <em>Uncle Tom’s Cabin</em>.</p>

<p>The most promising advances in <span>OCR</span> technology are happening in the field of scene text recognition. As researchers and programmers look for ways to identify text in the wild (think street signs and package labels) and not just on linear documents, they’re developing tools that do a better job of identifying and interpreting text that isn’t neatly arranged in rows and paragraphs. Current <span>OCR</span> tools often choke on font changes, inline graphics, and skewed text—scene recognition has to accommodate all of those hurdles. Scene recognition engines have to be better about spotting letter glyphs.</p>

<p>There is also a good deal of promising research on techniques for pre-processing images—doing things like straightening out warped text, super resolution to boost missing details, spotting text in arbitrary locations or at odd angles, and techniques for accommodating lower resolution text.</p>

<p>If you’re interested in going deep on the future of <span>OCR</span>, <a href="https://arxiv.org/abs/1811.04256v2">Scene Text Detection and Recognition: The Deep Learning Era</a> is an excellent survey of current literature on scene recognition. <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DocUNet_Document_Image_CVPR_2018_paper.html">DocUNet: Document Image Unwarping via a Stacked U-Net</a> is a good introduction to scholarly theory on image pre-processing.</p>

<h3>How We Tested</h3>

<p>With all that in mind, we identified a few sample documents to run through <span>OCR</span> systems so we could compare the results:</p>

<ul>
<li><p>A <strong>receipt</strong>—This receipt from the Riker’s commissary was included in <a href="https://statesofincarceration.org/states/new-york-rikers-island-ny-11370-plain-sight">States of Incarceration</a>, a collaborative storytelling project and traveling exhibition about incarceration in America.</p></li>
<li><p>A <strong>heavily redacted document</strong>—<a href="http://www.kingpin.cc/wp-content/uploads/2018/11/Carter-Page-release-9-November-2018.pdf">Carter Page’s <span>FISA</span> warrant</a> is a legal filing with a lot of redacted portions, just the kind of exasperating thing reporters deal with all the time.</p></li>
<li><p>Something <strong>historical</strong>—<a href="https://www.archives.gov/historical-docs/todays-doc/?dod-date=219">Executive Order 9066</a> authorized the internment of Japanese Americans in 1942. The scanned image available in the national archives is fairly high quality but it is still an old, typewritten document.</p></li>
<li><p>A <strong>form</strong>—This <a href="http://204.65.203.5/public/100721233.pdf">Texas campaign finance report</a>, from <a href="https://www.texastribune.org/2018/11/01/harris-county-texas-juvenile-judges-private-attorneys/amp/">a Texas Tribune story</a> about abuses in the juvenile justice system has very clean text but the formatting is important to understanding the document.</p></li>
<li><p>Something <strong>damaged</strong>— in early 2014 a group of divers retrieved <a href="https://yanukovychleaks.org/en/">hundreds of pages of documents</a> from a lake at Ukrainian President Viktor Yanukovych’s vast country estate. The former president or his staff had dumped the records there in the hopes of destroying them, but many pages were still at least somewhat legible. Reporters laid them out to dry and began the process of transcribing the waterlogged papers. We selected a page that is more or less readable to the human eye but definitely warped with water damage. </p></li>
</ul>

<p>The tools we tested support text in multiple languages—and most did at least as well with the waterlogged cyrillic documents as they did with the other English language documents we tested. If you want to test these <span>OCR</span> engines against your own sample documents, the Ruby scripts we used are all included in our <a href="https://github.com/factful/ocr_testing">repository</a>. </p>

<h3>What About Layout?</h3>

<p>All the tools we tested will output a text file. Most will also output either <span>JSON</span> or hOCR files that include data about where each word and line sits on a particular page. <a href="http://kba.cloud/hocr-spec/1.2/">hOCR</a> is an open standard for representing <span>OCR</span> results—there are a few open source <span>CSS</span> and JavaScript libraries that can help you view and display hOCR formats. Check out <a href="http://kba.cloud/hocrjs/">hocrjs</a>, <a href="https://github.com/not-implemented/hocr-proofreader">hOCR Proofreader</a>, and <a href="https://github.com/ultrasaurus/hocr-javascript">hOCR JavaScript</a> for some good starting points for actually taking advantage of hOCR.</p>

<h3>Free and Open Source Options</h3>

<h4><a href="https://github.com/Calamari-OCR/calamari">Calamari</a></h4>

<p>Calamari is built on <a href="https://www.tensorflow.org/tutorials/">TensorFlow</a>, an open-source machine learning library, which allows Calamari to take advantage of TensorFlow’s neural network capacity. It’s relatively straightforward to use, but it comes with some tricky dependencies. Because Calamari only does text recognition, you have to use another engine (they recommend OCRopus) to increase contrast, deskew, and segment the images you want to read. OCRopus requires Python 2 and Calamari is written in Python 3—not an insurmountable obstacle but one to be alert to.</p>

<p><strong>Pricing:</strong> Calamari is free and open source software. </p>

<h4><a href="https://github.com/tmbdev/ocropy">OCRopus</a></h4>

<p>OCRopus is a collection of document analysis tools that add up to a functional <span>OCR</span> engine if you throw in a final script to stitch the recognize output into a text file. OCRopus will output hOCR.</p>

<p>OCRopus requires Python 2.7 so you probably want to use virtualenv to install it and manage dependencies. We had hiccups using the installation instructions in the <a href="https://github.com/tmbdev/ocropy#running">Readme file</a>, but found workable <a href="https://github.com/tmbdev/ocropy/issues/241">installation instructions</a> hiding in an issue. You’ll also need to<a href="https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/"> follow some specialized instructions</a> to get matplotlib running in a Python 2.7 virtualenv.</p>

<p>Dan Vanderkam’s <a href="https://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html">blog post</a> about his experiences with OCRopus is also helpful.</p>

<p>OCRopus needs higher resolution images than the other <span>OCR</span> engines we tested—you’ll <a href="https://github.com/tmbdev/ocropy/wiki/FAQ#what-exactly-is-meant-by-300-dpi-for-digital-images">see a lot of errors</a> if your resolution is below 300 dpi. Unlike most tools we tested, OCRopus won’t catch documents that are sideways or upside down, so you’ll need to make sure your pages are oriented correctly. </p>

<p>Note: We ran our test documents through the original OCRopus. Nvidia <a href="https://blogs.nvidia.com/blog/2019/01/24/deep-learning-deciphers-historical-documents/">hired OCRopus developer Thomas Breuel</a> to rebuild the tool to take advantage of advances in neural network learning, and he recently released that work as <a href="https://github.com/NVlabs/ocropus3">ocropus3</a>. Early reports suggest that ocropus3 is significantly more reliable than its predecessor, OCRopus. </p>

<h4><a href="http://kraken.re/">Kraken</a></h4>

<p>Kraken is a turnkey <span>OCR</span> system forked from OCRopus. Kraken does output geometry in hOCR or <span>ALTO</span> format. Analyzed Layout and Text Object is an <span>XML</span> schema for text and layout information. It’s a well developed standard but we didn’t encounter other tools that output <span>ALTO</span> in our testing. Kraken is just OCRopus bundled nicely, so the actual results will be on par with OCRopus results. </p>

<p><strong>Pricing:</strong> OCRopus and Kraken are free and open source software.</p>

<h4><a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a></h4>

<p>Tesseract is a free and open source command line <span>OCR</span> engine that was developed at Hewlett-Packard in the mid 80s, and has been maintained by Google since 2006. It is well documented. Tesseract is written in C/C++. Their <a href="https://github.com/tesseract-ocr/tesseract/wiki">installation instructions</a> are reasonably comprehensive. We were able to follow them and get Tesseract running without any additional troubleshooting.</p>

<p>Tesseract will return results as plain text, hOCR or in a <span>PDF</span>, with text overlaid on the original image.</p>

<p><strong>Pricing:</strong> Tesseract is free and open source software.</p>

<div> <p><img src="http://media.opennews.org/img/uploads/article_images/190206-ocr/190206-ocr-tesseract.jpg" alt="Image of document and results of OCR testing"/></p><p>Tesseract accurately transcribed the handwritten text (“Come again…”) at the bottom of the Rikers commissary receipt. None of the tools we tested accurately captured the handwriting at the top (“Chips A’hoy Keeps Me Happy”). Tesseract definitely garbled the prices, however. </p></div>

<h3>Desktop Apps</h3>

<h4><a href="https://acrobat.adobe.com/us/en/acrobat/how-to/ocr-software-convert-pdf-to-text.html">Adobe Acrobat Pro</a></h4>

<p>Adobe Acrobat Pro doesn’t provide <span>API</span> access to their <span>OCR</span> tools, but they will batch process documents. Acrobat Pro only takes PDFs (no images) and only returns PDFs with searchable text inline. If you need a separate text file, you can use <a href="https://github.com/documentcloud/docsplit">Docsplit</a> to extract a plain text file from a <span>PDF</span> after you’ve run it through Acrobat. </p>

<p><strong>Pricing:</strong> <a href="https://acrobat.adobe.com/us/en/acrobat.html">Adobe Acrobat Pro <span>DC</span></a> is a desktop app but you have to pay a recurring monthly subscription to use it—pricing ranges from $25/mo with no commitment to $15/mo with a full year commitment. </p>

<div> <p><img src="http://media.opennews.org/img/uploads/article_images/190206-ocr/190206-ocr-adobe.jpg" alt="Image of document and results of OCR testing"/></p><p>Adobe Acrobat Pro gave very garbled results on the historical document. </p></div>

<h3>Cloud Services</h3>

<p>Abbyy Cloud, Google Cloud Vision and Azure Computer Vision are commercial cloud services. The steps to setting each up can be a bit circular. Looking for a “quickstart” guide and following the steps in it turned out to be a less frustrating path than just charging ahead and assuming you can sort it out. There are a bunch of steps.</p>

<h4><a href="https://www.ocrsdk.com/">Abbyy Cloud</a></h4>

<p>Of all the cloud services we tested, Abbyy Cloud is the most straightforward to set up because you aren’t setting up access to a whole cloud platform—<span>OCR</span> is the only thing they do. Use their <a href="https://www.ocrsdk.com/documentation/quick-start-guide/">Quickstart Guide</a> to get started. Abbyy has been in the <span>OCR</span> business since 1993 and in addition to their Cloud <span>API</span> service they also sell a desktop app that starts at $200 and access to an <span>SDK</span> that developers can use to incorporate <span>OCR</span> functionality into software. </p>

<p>Abbyy did a better job of preserving spacing in their text only results than most of the tools we tested. In addition to plain text, Abbyy will return <span>JSON</span>, <span>XML</span>, or a <span>PDF</span> with the text searchable inline. </p>

<p><strong>Pricing:</strong> Abbyy will let you <span>OCR</span> 50 pages with a <a href="https://www.ocrsdk.com/plans-and-pricing/">free account</a>. After that you need to sign up for either a monthly subscription or a 90-day package. Packages start at 10¢ per page for 100 pages or 6¢ per page with a $29.99 monthly subscription. Pricing goes as low as 3¢ per page when you get into the tens of thousands of pages. Their desktop app is $200, and comes without any page count restrictions. </p>

<div> <p><img src="http://media.opennews.org/img/uploads/article_images/190206-ocr/190296-ocr-abbyy.jpg" alt="Image of document and results of OCR testing"/></p><p>Abbyy preserved much of the formatting on the receipt but introduced some wonky spacing. It isn’t clear why Abbyy couldn’t read the “Ramen Soup” price. </p></div>

<h4><a href="https://cloud.google.com/vision/">Google Cloud Vision</a></h4>

<p>Google’s cloud services include an <span>OCR</span> tool, Cloud Vision. Of all the tools we tested, Cloud Vision did the best job of extracting useful results from the low resolution images we fed it. There are a few steps to getting it up and running, but <a href="https://cloud.google.com/vision/docs/how-to">the documentation</a> covers them well. If you follow the instructions you should be able to get set up. If it feels like you’re going in circles, you might still be on the right track. When you create your account and first log in, you have to actually select Console from the landing page to get to the settings you need. From the console, start by creating a “project” (if it’s not your first project, the option is hiding under <em>Select a Project</em>. If you don’t see an option at the top left to create or select a project, try reloading the page—our <em>Select a Project</em> pulldown actually disappeared briefly.) Once you’ve selected (or created and then selected) a project, you will need to either search for “vision” to find the Cloud Vision <span>API</span> or select *APIs &gt; enable APIs and services *and then select <em>Cloud Vision <span>API</span></em>. However you get there, your next step is the enable button, and then create credentials—you’ll need to tell the system, again, which <span>API</span> we’re using. Once the project is set up, you also need to create a “Service Account”. We used “Project Owner” as the “role” for ours, but if you read the documentation you might be able to make a more precise selection choice. Once you hit Create you should be prompted to download your credentials. Save the file as <em>credentials.json</em> and you’re ready to run our script.</p>

<p><a href="https://source.opennews.org/people/dan-nguyen/">Dan Nguyen</a> has published a <a href="https://gist.github.com/dannguyen/a0b69c84ebc00c54c94d">few additional Python scripts</a> that he used to compare Cloud Vision and Tesseract.</p>

<p><strong>Pricing:</strong> Your first 1000 pages each month are free. After that you’ll pay $1.50 per thousand pages. In addition, Google Cloud Vision currently offers a free trial that will get you $300 in free credits, which is enough to process 200K pages in one month. When you get to 10 million pages the price drops to $0.60 per thousand pages.</p>

<div> <p><img src="http://media.opennews.org/img/uploads/article_images/190206-ocr/190206-ocr-google-cloud-vision.jpg" alt="Image of document and results of OCR testing"/></p><p>Google Cloud Vision did better than any other tool on this heavily redacted <span>FISA</span> warrant, but still choked on an otherwise readable sentence. “1. (U) Identity of Federal Officer Making Application This application is made by” was reduced to “dentit made”.  </p></div>

<h4><a href="https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/">Microsoft Azure Computer Vision</a></h4>

<p>Computer Vision is Microsoft Azure’s <span>OCR</span> tool. It’s available as an <span>API</span> or as an <span>SDK</span> if you want to bake it into another application. Azure provides sample jupyter notebooks, which is helpful. Their <span>API</span> doesn’t return plain text results, however. The only way to get those is to scrape the text out of the bounding boxes. Our script or their sample scripts will do that nicely though.</p>

<p>There are a handful of steps that you need to follow to use Computer Vision—their <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/quickstarts/python-disk">quickstart guide</a> spells them out, but you need to set up an Azure cloud account, create a “resource” (the “location” option is <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/vision-api-how-to-topics/howtosubscribe">oddly circular</a>, but if you stick to the default you should be okay), wait a moment for it to deploy, and then you will be able to actually go to resources to grab your credentials and the <span>API</span> endpoint. Add those to credentials.json in our Azure sample script and you’re ready to run it. We inexplicably got locked out of our account—reentry took more steps than it should have, but we did get back in.</p>

<p><strong>Pricing:</strong> Your first 5000 pages each month are free. After that you’ll pay $1.50 per thousand pages and the per-thousand page price drops again at 1,000,000 pages and at 5,000,000 pages.</p>

<div> <p><img src="http://media.opennews.org/img/uploads/article_images/190206-ocr/190206-ocr-azure.jpg" alt="Image of document and results of OCR testing"/></p><p>Azure seemed to do a nice job of breaking the receipt into columns, but was actually a bit erratic in its implementation. The segment shown here includes a lot of numbers from the “price” column and from the “total” column—this wouldn’t be easy to import into a spreadsheet. </p></div>

<h3>All The Results in One Place</h3>

<p>You can flip through our viewer to see how each tool did with each of the documents.</p>



<h3>Tools We Skipped</h3>

<h4>Amazon Web Services</h4>

<p><a href="https://aws.amazon.com/rekognition/">Amazon’s Rekognition <span>API</span></a> is primarily designed to identify text in images of signs and labels, rather than in documents. It can only pick out <a href="https://docs.aws.amazon.com/rekognition/latest/dg/text-detection.html">50 words per image</a> so it isn’t a great option for full pages of text. </p>

<p>Amazon Textract is a new service from Amazon. We applied for access to the beta but hadn’t received a response by the time we went to press.</p>

<h4><a href="https://github.com/garnele007/SwiftOCR">SwiftOCR</a></h4>

<p>SwiftOCR is a free and open source <span>OCR</span> library written on top of a machine learning library called <a href="https://github.com/Swift-AI/Swift-AI">Swift</a>. It has some smart pre-processing built in but it is optimized for short text strings (think codes on giftcards) rather than paragraphs. SwiftOCR takes a lot more set up than other tools we looked at and it really isn’t a stand alone <span>OCR</span> engine—it’s designed to be built into a larger application that needs to be able to read gift cards or license plates or other short blocks of text. </p>

<h4>Attention <span>OCR</span></h4>

<p>Attention-<span>OCR</span> is a free and open source TensorFlow project, based on an approach proposed in <a href="https://arxiv.org/pdf/1609.04938.pdf">a 2017 research paper</a>. <a href="https://www.tensorflow.org/tutorials/">TensorFlow</a> is an open-source machine learning library. The authors of the original Attention-<span>OCR</span> paper published their <a href="https://github.com/da03/Attention-OCR">proof of concept code on GitHub</a>, while a forked <a href="https://github.com/emedvedev/attention-ocr">version of Attention-<span>OCR</span></a> is stylistically closer to TensorFlow’s recommended usage. Both versions require extensive training to run. Attention-<span>OCR</span> is still very much a research project, rather than a full fledged <span>OCR</span> application. </p>

<h3><del>Don’t</del> Try This At Home</h3>

<p>Or perhaps you should? All the scripts we used to run each of these tests are available in a <a href="https://github.com/factful/ocr_testing">repository</a> on GitHub. </p>
                
                

                <!-- /end .links-article-social -->
            </div></div>
  </body>
</html>
