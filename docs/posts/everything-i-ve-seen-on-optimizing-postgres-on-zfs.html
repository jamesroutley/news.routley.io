<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vadosware.io/post/everything-ive-seen-on-optimizing-postgres-on-zfs-on-linux/">Original</a>
    <h1>Everything I&#39;ve seen on optimizing Postgres on ZFS</h1>
    
    <div id="readability-page-1" class="page"><div>
        <div>

          <div>
            
            
            

            
              
              
            


            
            
            
            

            
              
<div>
    <h3>UPDATE (2021/12/21)</h3>
    <p>
    After lots of feedback on Reddit (thanks /u/BucketOfSpinningRust!) and doing some more experimenting and digging, I&#39;ve updated this post with more information -- new/updated sections are marked &#34;Update&#34;.
    </p>
</div>
<p><strong>tl;dr - Tips for running Postgres on (Open)ZFS-on-Linux (ZoL), which I‚Äôm nicknaming ‚ÄúPoZoL‚Äù, check out the TOC (or skip to the <a href="#all-the-links">resource list at the bottom</a>) and browse to whatever looks interesting.</strong></p>
<p>This post is a shameless ripoff/reformulation of everything I‚Äôve seen so far across conference presentations, the internet, reddit, forum posts and other places about how to run Postgres on ZFS reliably and with good performance. I‚Äôve been doing a lot of tinkering with my storage setup (deciding on <code>mdraid</code> +/- <a href="https://sourceware.org/lvm2">LVM</a> vs <a href="https://openzfs.github.io/openzfs-docs">ZFS</a> for direct disk access, <a href="https://docs.ceph.com">Ceph</a> vs <a href="https://docs.ceph.com">Longhorn</a> for distributed storage, etc) since I‚Äôm gearing up to offer <a href="https://nimbusws.com?ref=blog-pg-zfs">Postgres as a Service on NimbusWS</a>, but I thought I should share some of the information I‚Äôve managed to gather.</p>
<p>As you might imagine, this post is not exhaustive ‚Äì there are great companies out there that can take your Postgres installation to the next level:</p>
<ul>
<li><a href="https://www.2ndquadrant.com">2nd Quadrant</a> (<a href="https://www.2ndquadrant.com/en/blog/pg-phriday-postgres-zfs/">They‚Äôve got a post on ZFS</a>)</li>
<li><a href="https://www.enterprisedb.com">EDB</a></li>
<li><a href="https://www.percona.com">Percona</a></li>
<li><a href="https://crunchydata.com">CrunchyData</a></li>
<li><a href="https://www.cybertec-postgresql.com">CYBERTEC</a></li>
</ul>
<p>They‚Äôve almost certainly got lots of hard-won secret sauce tunables over years of experience so obviously you‚Äôre going to want to pay them first if you need problems diagnose or sage advice ASAP.</p>
<p>If you‚Äôre still here, what we‚Äôre going to be talking about is running Postgres on (Open)ZFS-on-Linux I‚Äôm going to call the effort ‚ÄúPoZoL‚Äù.</p>

<h2 id="setting-recordsize-to-8k">Setting <code>recordsize</code> to <code>8k</code></h2>
<p>While the default <code>recordsize</code> of ZFS is <code>128k</code>, a <code>recordsize</code> of <code>8k</code> exactly matches the sizing that <a href="http://www.postgresql.org/docs/current/storage-page-layout.html">Postgres‚Äôs page size</a> ‚Äì this is great for postgres since it cuts down on unnecessary writes (especially to solid state disks), and it gets even better with compression made available by ZFS.</p>
<p>One thing I misunderstood about this (and <a href="https://www.reddit.com/r/zfs/comments/qgkk1c/numbers_from_zfs_vs_lvmmdraid_fio_tests_on_nvme/hiaf5qo/">thankfully was corrected by /u/mercenary_sysadmin on Reddit</a>), is that while the default <code>recordsize</code> of <code>128k</code> can be considered an ‚Äúupper bound‚Äù, random IO in large files is done in increments of <code>recordsize</code>. He‚Äôs laid it out pretty nicely so I‚Äôll copy his comment here:</p>
<blockquote>
<p>recordsize is only ‚Äúan upper bound‚Äù in the sense that small files get small records. For any workload which does random I/O inside larger files, recordsize is exactly the increment which the random I/O happens in.</p>
<p>If you set recordsize=1M as you originally did, but then test with eg blocksize=4K, you get an amplification factor of 256x‚Ä¶ and, even worse, you get a read-modify-write cycle when re-writing blocks.</p>
<p>Let‚Äôs say you ask fio to do a bs=4K run when recordsize=1M. When fio wants to write 4K of new data to the middle of a testfile, it first has to read in 1M of data, then modify 4K out of that data, then write all 1M back out again.</p>
<p>If you re-run the test on a dataset with recordsize=4K there is no RMW and no amplification‚Äîyou ask fio to write 4K of data to the middle of the testfile, it writes 4K of data, done.</p>
</blockquote>
<p>Obviously a <code>blocksize=1M</code> is not a reasonable (the tests I ran that he‚Äôs referring to are <code>fio</code> tests) but it was a pretty big insight to me to realize that <code>recordsize</code> isn‚Äôt automatically pared down. I‚Äôve seen some arguments against modifying <code>recordsize</code> but at this point it looks pretty clear that aligning the storage to the IO you‚Äôre going to do (and avoiding write amplification wher epossible) is a good idea.</p>
<h2 id="optional-setting-recordsize-to-16k">(optional) Setting <code>recordsize</code> to <code>16k</code></h2>
<p>A <a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">talk from Scale15x in 2017</a> recommends <code>blocksize=16k</code>. The primary reason here was being able to essentially pre-fault the next page, which is very useful for sequential scans.</p>
<p>It‚Äôs not related to Postgres but <a href="https://www.percona.com/blog/mysql-zfs-performance-update/">percona also has a decent post on the effect on MySQL (which does have 16k record sizes)</a>.</p>
<h2 id="enable-compression">Enable <code>compression</code></h2>
<p>Regardless of your <code>recordsize</code>, Postgres and your storage density in general will benefit from enabling <code>compression</code>. While compression may increase the CPU resources required by ZFS, it‚Äôs worth it, as with <code>compresssion=lz4</code> (or <a href="">the newer <code>zstd</code></a>, you can expect more data to fit in the same space on disk ‚Äì compression rates can vary hugely but members of the community (giving talks) have reported compression ratios anywhere from 1 to 100 (realistically you‚Äôll probably see 1-4x space savings).</p>
<h2 id="benchmark-pgbench-blocksize8k-vs-blocksize16k">Benchmark (<code>pgbench</code>): <code>blocksize=8k</code> vs <code>blocksize=16k</code></h2>
<p>As this is relatively easy to test, I set up some <code>pgbench</code>marks to figure out if there was a large difference between <code>8k</code> and <code>16k</code>. In general the settings were as follows:</p>
<ul>
<li><code>pgbench</code> jobs have 4 CPU 8GB</li>
<li><code>postgres</code> instances have 4 CPU 16GB, with storage of 32GB</li>
<li>(ZFS) <code>full_page_writes=off</code></li>
<li>(ZFS) <code>wal_init_zero=off</code></li>
<li>(ZFS) <code>wal_recycle=off</code></li>
</ul>
<p>The results for <code>recordsize=8k</code>:</p>
<details>
<summary>üìúÔ∏è PGBench output (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>pgbench (14.1)
</span><span>starting vacuum...end.
</span><span>transaction type: &lt;builtin: TPC-B (sort of)&gt;
</span><span>scaling factor: 1000
</span><span>query mode: simple
</span><span>number of clients: 40
</span><span>number of threads: 4
</span><span>duration: 300 s
</span><span>number of transactions actually processed: 409828
</span><span>latency average = 29.273 ms
</span><span>initial connection time = 102.610 ms
</span><span>tps = 1366.435557 (without initial connection time)
</span><span>statement latencies in milliseconds:
</span><span>         0.001  \set aid random(1, 100000 * :scale)
</span><span>         0.001  \set bid random(1, 1 * :scale)
</span><span>         0.001  \set tid random(1, 10 * :scale)
</span><span>         0.000  \set delta random(-5000, 5000)
</span><span>         0.606  BEGIN;
</span><span>         2.105  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
</span><span>         0.788  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
</span><span>         0.918  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
</span><span>         1.225  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
</span><span>         0.822  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
</span><span>        22.795  END;
</span></code></pre></div>
  
</details>




<p>The results for <code>recordsize=16k</code>:</p>
<details>
<summary>üìúÔ∏è PGBench output (recordsize=16k) (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>pgbench (14.1)
</span><span>starting vacuum...end.
</span><span>transaction type: &lt;builtin: TPC-B (sort of)&gt;
</span><span>scaling factor: 1000
</span><span>query mode: simple
</span><span>number of clients: 40
</span><span>number of threads: 4
</span><span>duration: 300 s
</span><span>number of transactions actually processed: 506702
</span><span>latency average = 23.679 ms
</span><span>initial connection time = 92.940 ms
</span><span>tps = 1689.255984 (without initial connection time)
</span><span>statement latencies in milliseconds:
</span><span>         0.001  \set aid random(1, 100000 * :scale)
</span><span>         0.001  \set bid random(1, 1 * :scale)
</span><span>         0.001  \set tid random(1, 10 * :scale)
</span><span>         0.000  \set delta random(-5000, 5000)
</span><span>         0.602  BEGIN;
</span><span>         1.094  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
</span><span>         0.757  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
</span><span>         0.786  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
</span><span>         1.100  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
</span><span>         0.799  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
</span><span>        18.530  END;
</span></code></pre></div>
  
</details>




<p><strong>Wow, a ~20% improvement there</strong> ‚Äì looks like <code>recordsize=16k</code> is the better setting in this <em>very</em> limited set of testing.</p>
<h2 id="reducing-read-ahead">Reducing read-ahead</h2>
<p>ZFS read ahead should be minimized as much as possible ‚Äì as a lot of random IO will be happening inside of given files which may not be read completely. This tip was contributed by /u/mercenary_sysadmin and seems to be good advice as well. I‚Äôll let the quote speak for itself:</p>
<blockquote>
<p>There are some other tweaks you want with postgres as a workload; readhead should generally be off for one thing (as it should be in any dataset where you‚Äôre primarily doing random I/O inside files which are not expected to need to be read in their entirety).</p>
</blockquote>
<h2 id="tuning-primarycache">Tuning <code>primarycache</code></h2>
<p><a href="https://www.patpro.net/blog/index.php/2014/03/19/2628-zfs-primarycache-all-versus-metadata/"><code>primarycache</code> is a ZFS tunable</a> that can be set to <code>all</code> or <code>metadata</code>, and generally the rule seems to be:</p>
<ul>
<li>If it fits in RAM = primarycache=metadata</li>
<li>If your working set exceeds RAM, primary_cache=all</li>
</ul>
<p>This goes with a few more modifications to prevent double-caching:</p>
<ul>
<li>max ARC size = 15%-&gt;25%</li>
<li><code>shared_buffers</code> =&gt; physical RAM + 50%</li>
</ul>

<h2 id="setting-full_page_writesoff">Setting <code>full_page_writes=off</code></h2>
<p>This is a pretty common recommendation, for good reason. It comes up in the <a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">Scale15x talk from 2017</a> and lots of other places. ZFS does not need full page writes to prevent torn pages, because ZFS never tears pages (yay for <a href="https://en.wikipedia.org/wiki/Copy_on_write">Copy on Write</a>.</p>
<h3 id="drawback-interferes-with-cascading-replication">Drawback: ‚Äúinterferes‚Äù with cascading replication</h3>
<p>This does suffer from a drawback that it can interfere with cascading replication ‚Äì I haven‚Äôt run into this issue in particular myself wiht this setup, but as cascading replication itself is pretty niche (and there are a bunch of options/things you can try to do), I didn‚Äôt think to look into it too much, or try to reproduce.</p>
<h2 id="disable-postgres-checksumming">Disable postgres checksumming</h2>
<p>ZFS is doing all the checksumming, scrubbing and safe writing you‚Äôll need. By default <a href="https://www.postgresql.org/docs/14/checksums.html">data checksums are off</a>:</p>
<blockquote>
<p>By default, data pages are not protected by checksums, but this can optionally be enabled for a cluster.</p>
</blockquote>
<h2 id="disable-postgres-compression">Disable Postgres compression</h2>
<p>Since ZFS will be compressing all the data before it hits disk (and has access to some better compression tech like <code>zstd</code> first), you can pretty safely let ZFS handle your compression as well. Postgres compression <em>may</em> still offer an advantage in certain situations with certain data storage patterns but in general it should be much less advantageous considering the lower layer compression at work.</p>
<h2 id="tune-wal_init_zero--wal_recycle">Tune <code>wal_init_zero</code> &amp; <code>wal_recycle</code></h2>
<p>In a <a href="https://blog.crunchydata.com/blog/waiting-for-postgis-3.1-performance">paper created by Tomas Munro over at Microsoft</a>, there‚Äôs lots of insight on how to run Postgres on FreeBSD (which of course <a href="https://docs.freebsd.org/handbook/zfs-zpool.html">utilizes ZFS</a>), but one that stuck out to me was tuning the <code>wal_init_zero</code> and <code>wal_recycle</code> options for Postgres ‚Äì they help you avoid zero-filling of new files used for <a href="https://www.postgresql.org/docs/current/runtime-config-wal.html">Postgres‚Äôs Write Ahead Log</a>. The reasoning here is that since ZFS overwrites these anyway, you might as well get yourself a new inode.</p>
<h2 id="tune-wal_sync_method-trying-fdatasync">Tune <code>wal_sync_method</code>, trying <code>fdatasync</code></h2>
<p>This tip also comes from <a href="https://blog.crunchydata.com/blog/waiting-for-postgis-3.1-performance">Thomas‚Äôs paper</a>, but the insight here is that we can skip writing metadata (the difference between <a href="https://linux.die.net/man/2/fdatasync"><code>fdatasync</code> and <code>fsync</code></a>), and save ourselves some time writing to disk.</p>
<h3 id="warning-dont-do-this-if-you-cant-rely-on-your-drives-writeback-caching">WARNING: Don‚Äôt do this if you can‚Äôt rely on your drive‚Äôs writeback caching</h3>
<p>I‚Äôve found out the painful way (by getting instant data corruption on large <code>pgbench</code>runs) that it may be possible that you can‚Äôt rely on your drives writeback cache ‚Äì <code>fsync</code> <em>must</em> be used in these cases. I found this in particular with <a href="https://www.samsung.com/semiconductor/ssd/client-ssd/MZVLB512HBJQ-00$00-07/">consumer-grade Samsung MZVLB512HBJQ disks</a>, but probably best to be wary of it in general, and run some punishing <code>pgbench</code> runs before you put anything in production (and of course, have backups handy.</p>
<h3 id="drawback-modified-time-may-be-lost-after-crash">Drawback: modified time may be lost after crash</h3>
<p>As Thomas goes into (read the slides!), there are some issues where modified time may be lost after a crash.</p>
<h3 id="alternative-wal_sync_methodopen_datasync">Alternative: <code>wal_sync_method=open_datasync</code></h3>
<p>An alternative to <code>fdatasync</code> is to use <code>open_datasync</code>, which is equivalent to trying to write WAL with <code>open()</code> and the option <code>O_DSYNC</code> set. As of the time of this post, ZoL does not support Direct IO (it‚Äôs in the work), so Im&#39; not sure how much benefit there is to this.</p>
<h3 id="future-alternative-wal_sync_methodaio_fsyncdatasync">Future Alternative: <code>wal_sync_method=aio_f[sync|datasync]</code>?</h3>
<p>Plugging into the Linux async IO pipeline for WAL syncing seems like a pretty attractive option that will be available in the future.</p>
<h2 id="increasing-postgres-blocksize">Increasing Postgres <code>blocksize</code></h2>
<p>Vladimir Mihailenco who maintains <a href="https://bun.uptrace.dev/">Bun (A DB client for Postres, MySQL and SQLite)</a> has a <a href="https://bun.uptrace.dev/">great guide and and explaration on PoZoL</a>. His insight flips the <code>recordsize</code> insight in the ZFS section and flips it on it‚Äôs head ‚Äì  we focused on decreasing <code>recordsize</code> to match Postgres‚Äôs default <code>blocksize</code>, but there‚Äôs also the option of actually <em>increasing</em> <code>blocksize</code> on the postres side. This requires rebuilding Postgres but can considerably improve data-hungry queries. A quote:</p>
<blockquote>
<p>You can improve this situation by increasing PostgreSQL block size to 32k and WAL block size to 64k. This requires re-compiling PostgreSQL and re-initializing [the] database.</p>
</blockquote>
<p>As he notes the principal tension here is that smaller blocksize will usually mean higher Transactions Per Second (TPS). This one I‚Äôm not too sure about doing but it‚Äôs definitely a fascinating experiment to try someday.</p>
<p>*** Drawback: requires recompile</p>
<h2 id="controversial-zfs_txg_timeout1--synchronous_commitoff--logbiasthroughput">(Controversial) <code>zfs_txg_timeout=1</code> &amp; <code>synchronous_commit=off</code> &amp; <code>logbias=throughput</code></h2>
<p>This intense bit of advice comes from the <a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">Scale15x talk</a> ‚Äì the idea is that setting the ZFS transaction flushing timeout (<code>zfs_txg_timeout</code>) which is 5 seconds by default <strong>to only 1 second</strong> gives you lots more performance at the risk of 1 second of data loss (with writes not being written to disk). Not waiting for writes to be served at all at the Postgres level, but allowing for approximately 1 second of possible data loss seems like a pretty reasonable tradeoff, but of course turning <code>synchronous_commit</code> to <code>off</code> is definitely something anyone should think about well before doing.</p>
<p>.ZFS plays into this beause it will never be inconsistent, so you don‚Äôt have to worry so much about data integrity. Considering the problems I had with <code>fdatasync</code> I‚Äôm a little worried about this but it‚Äôs definitely worth trying. Another thing that makes this hard to try is that this is a pool-wide setting ‚Äì it can‚Äôt be set <a href="https://www.reddit.com/r/zfs/comments/76cj5f/can_i_set_zfs_txg_timeout_on_a_dataset/">on just one dataset</a></p>
<h3 id="drawback-1s-of-data-loss">Drawback: 1s of data loss</h3>
<h3 id="drawback-horrific-fragmentation">Drawback: ‚Äúhorrific‚Äù fragmentation</h3>
<p><em>Yet another</em> quote from /u/mercenary_sysadmin (<a href="https://www.reddit.com/r/zfs/comments/azt8sz/logbiasthroughput_without_a_slog/)">https://www.reddit.com/r/zfs/comments/azt8sz/logbiasthroughput_without_a_slog/)</a>:</p>
<blockquote>
<p>logbias=throughput with no SLOG will likely improve performance if your workload is lots of big block writes, which is a workload that usually isn‚Äôt suffering from performance issues much in the first place.</p>
<p>Logbias=throughput with no SLOG and small block writes will result in the most horrific fragmentation imaginable, which will penalize you both in the initial writes AND when you reread that data from metal later.</p>
</blockquote>
<p>Unfortunately I don‚Äôt have any direct tests I can show for this (they‚Äôd be easy to run, only one ZFS setting needs to be changed), but hopefully I‚Äôll get a chance to test this out. The logic is pretty sound so it looks like <code>logbias=latency</code> results in much better metadta/data spacing.</p>
<h2 id="setting-logbiaslatency-instead-of-logbiasthroughput">Setting <code>logbias=latency</code> (instead of <code>logbias=throughput</code>)</h2>
<p>The <a href="https://bun.uptrace.dev/postgres/tuning-zfs-aws-ebs.html#logbias">Bun guide has a great setction on <code>logbias</code></a> ‚Äì <code>logbias=throughput</code> looks to cause lots of fragmentation and is generally <em>not</em> a good idea.</p>
<h2 id="update-20121221-results-from-zfs_txg_timeout1syncdisabledlogbiaslatency">Update (2012/12/21): Results from <code>zfs_txg_timeout=1</code>,<code>sync=disabled</code>,<code>logbias=latency</code></h2>
<p>In local testing I‚Äôve found that 16k has the slight edge over 8k, and zstd has the slight edge over lz4, so I wanted to test <code>zfs_txg_timeout</code> as well, and I did ‚Äì it turns out it also has an edge over the 5s wait (on NVMe, at least) ‚Äì with no visible downsides (so far). The timeout used to be much longer for ZFS and went from 30 seconds to 5 ‚Äì moving the rest of the distance from 5 to 1 (and why you should/shouldn‚Äôt) seems to be more rarely discussed.</p>
<p>If I had to guess (as I‚Äôm not a ZFS expert) the issues would be drive wear, inefficient transaction groupings (maybe when doing particularly large writes), and in general this is probably something that only make sense on NVMe.</p>
<p>Anyway, here are the results for <code>recordsize=16k</code>,<code>compression=zstd</code>,<code>logbias=latency</code>, with <code>zfs_txg_timeout=5</code> (the default):</p>
<div><pre tabindex="0"><code data-lang="console"><span>pgbench (14.1)
</span><span>starting vacuum...end.
</span><span>transaction type: &lt;builtin: TPC-B (sort of)&gt;
</span><span>scaling factor: 1000
</span><span>query mode: simple
</span><span>number of clients: 40
</span><span>number of threads: 4
</span><span>duration: 300 s
</span><span>number of transactions actually processed: 1088807
</span><span>latency average = 11.018 ms
</span><span>initial connection time = 102.951 ms
</span><span>tps = 3630.302820 (without initial connection time)
</span><span>statement latencies in milliseconds:
</span><span>         0.001  \set aid random(1, 100000 * :scale)
</span><span>         0.001  \set bid random(1, 1 * :scale)
</span><span>         0.001  \set tid random(1, 10 * :scale)
</span><span>         0.000  \set delta random(-5000, 5000)
</span><span>         0.585  BEGIN;
</span><span>         1.141  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
</span><span>         0.685  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
</span><span>         0.676  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
</span><span>         0.775  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
</span><span>         0.733  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
</span><span>         6.410  END;
</span></code></pre></div><p>And with <code>zfs_txg_timeout=1</code>, <code>sync=disabled</code> (postgres still thinks it‚Äôs doing synchronous writes):</p>
<div><pre tabindex="0"><code data-lang="console"><span>kubectl logs job/pgbench
</span><span>pgbench (14.1)
</span><span>starting vacuum...end.
</span><span>transaction type: &lt;builtin: TPC-B (sort of)&gt;
</span><span>scaling factor: 1000
</span><span>query mode: simple
</span><span>number of clients: 40
</span><span>number of threads: 4
</span><span>duration: 300 s
</span><span>number of transactions actually processed: 1219432
</span><span>latency average = 9.838 ms
</span><span>initial connection time = 98.798 ms
</span><span>tps = 4066.023297 (without initial connection time)
</span><span>statement latencies in milliseconds:
</span><span>         0.001  \set aid random(1, 100000 * :scale)
</span><span>         0.001  \set bid random(1, 1 * :scale)
</span><span>         0.001  \set tid random(1, 10 * :scale)
</span><span>         0.000  \set delta random(-5000, 5000)
</span><span>         0.640  BEGIN;
</span><span>         1.134  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
</span><span>         0.881  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
</span><span>         0.809  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
</span><span>         0.891  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
</span><span>         0.921  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
</span><span>         4.553  END;
</span></code></pre></div><p><strong>About a ~12% improvement!</strong> I‚Äôm not sure this improvement is quite worth <em>such</em> a drastic change ‚Äì <code>zfs_txg_timeout</code> is set at the node level so it will affect other workloads. Again to go over the logic:</p>
<h3 id="controversial-re-convincing-myself-zfs_txg_timeout1-and-syncdisabled-is-ok-in-the-case-of-postgres">(Controversial) Re-convincing myself <code>zfs_txg_timeout=1</code> and <code>sync=disabled</code> is OK in the case of Postgres</h3>
<p>The Idea is that ZFS will be lying to postgres about synchronous writes speed (we‚Äôll be essentially writing to RAM via the ARC) but since <code>txg_timeout</code> means writes are flushed every second we actually only have one potential second of data loss.</p>
<p>Normally, even one second of dataloss (that the upstream application thinks was written) <em>would be an issue</em>, but since Postgres has a WAL, if we suffer a crash we will lose 1 second of data, but <em>not</em> completely corrupt our database.Postgres is OK with this data loss because upon restart it will replay the WAL, and ZFS cannot ever be in an inconsistent state (torn pages, etc), so there is virtually no risk outside of 1 second of data loss.</p>
<p>When postgres replays it‚Äôs WAL it will have either:</p>
<ul>
<li>The exact right (there were no new writes in the second before interruption)</li>
<li>An updated WAL that doesn‚Äôt match the data files (writes came in but didn‚Äôt make it to disk), these writes will be replayed</li>
<li>(impossible) Updated data files that don‚Äôt match the WAL ‚Äì WAL writes go before data writes @ checkpoints</li>
</ul>
<p>BTW if you want to join me on this dangerous journey, you can set <code>zfs_txg_timeout</code> via the kernel module, or temporarily by <code>echo</code>-ing values to <code>/sys/module/zfs/parameters/zfs_txg_timeout</code> (see the docs for your distribution or <a href="https://www.reddit.com/r/zfs/comments/mgibzy/where_can_i_set_the_value_of_zfs_txg_timeout/">this reddit post</a>).</p>
<h2 id="separate-wal-writes-into-a-separate-zfs-dataset">Separate WAL writes into a separate ZFS dataset</h2>
<p>The <a href="https://bun.uptrace.dev/postgres/tuning-zfs-aws-ebs.html#postgresql-block-size-and-wal-size">Bun guide</a> covers this prfetty succinctly ‚Äì by splitting the WAL off into it‚Äôs own ZFS dataset, it‚Äôs easier to take snapshots of without the rest of the data. In the end <a href="https://pgbackrest.org">PGBackrest</a>, <a href="https://pgbarman.org/">Barman</a> or other backup tooling are probably better to use than simply <code>zfs send</code>/<code>zfs recv</code> but if you find yourself using <code>zfs send</code>/<code>zfs recv</code> then this is worth paying attention to.</p>

<p>There are lots more tips for just general postgres, but those are mostly not covered here. Here are a few that I came across that are worth a mention. They‚Äôre not really related to PoZoL per-say but are great to take note of.</p>

<p>Pretty obvious of course, but <code>shared_buffers</code> is a big part of tuning Postgres performance ‚Äì <a href="https://www.percona.com/sites/default/files/presentations/pg-Performance-Tuning.pdf">Percona has a fantastic set of slides on it</a>. <code>shared_buffers</code> is probably one of the quickest ways to more perf out there.</p>
<h2 id="tuning-work_mem">Tuning <code>work_mem</code></h2>
<p>Another large part of tuning postgres performance which is mentioned in <a href="https://www.percona.com/sites/default/files/presentations/pg-Performance-Tuning.pdf">Percona‚Äôs set of slides on Performance tuning</a>. <code>work_mem</code> is used for operations like in-memory sorting, which I think most people can spare more than the default (2MB) for!</p>
<h2 id="tuning-maintenance_work_mem">Tuning <code>maintenance_work_mem</code></h2>
<p>Similar to <code>work_mem</code>, <code>maintenance_work_mem</code> is used for maintenance tasks (ex. <code>VACUUM</code>, <code>CREATE INDEX</code>, <code>ALTER_TABLE</code>), and the default is only 64MB. I think most modern databases can afford to spare a bit more memory for such important operations. These days there are lots of things that can be done <code>CONCURRENTLY</code> but this is a relatively free win in my opinion.</p>
<h2 id="tuning-wal_buffers">Tuning <code>wal_buffers</code></h2>
<p>Increasing the amount of shared memory used for WAL data that has not been written to disk (which defaults to 16MB, w/ 8k blocks) can improve the performance of your database if you have a lot of concurrent connections. Bigger WAL can increase your concurrent performance but also makes backup/recovery take longer (bigger files to lug around) so this has to be considered.</p>
<h2 id="use-pg_repack">Use <code>pg_repack</code></h2>
<p>Also in the <a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">scale15x 2017 talk</a>, <a href="https://reorg.github.io/pg_repack/"><code>pg_repack</code></a> allows you to remove bloat (similar to <code>CLUSTER</code>/<code>VACUUM FULL</code>), but with the database online, without holding an exclusive lock on the processed table.</p>
<h2 id="encode-using-utf8-sort-using-c">Encode using UTF8, sort using C</h2>
<p>Another gem from the <a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">scale15x 2017 talk</a>, the insight here is to only enable local changes when you <em>need</em> them.</p>
<h2 id="tuning-effective_io_concurrency">Tuning <code>effective_io_concurrency</code></h2>
<p>This controls the maximum <code>posix_fadvise</code>/<code>pread</code> sequences generated by a query ‚Äì tuning this should help you avoid stalls and encourage concurrent IO.</p>
<h2 id="tuning-max_connections">Tuning <code>max_connections</code></h2>
<p>Obviously, you‚Äôre going to want to manage <code>max_connections</code> ‚Äì the default is 100 (with 3 reserved for super users). There‚Äôs some nuance here in <em>how</em> you should tune it, and <a href="https://www.cybertec-postgresql.com/en/tuning-max_connections-in-postgresql/">CYBERTEC has a great post on that</a></p>
<h2 id="tuning-effective_cache_size">Tuning <code>effective_cache_size</code></h2>
<p>Giving Postgres hints on the size of the kernel‚Äôs disk buffer cache (and ZFS ARC) can also improve the heuristics and statistics that Postgres will use internally.</p>

<p>Alternatively, how to discover that volatile cache writebacks can bite you on consumer-grade hardware.</p>
<p>When I tried out a lot of these tips, I quickly found out that <code>fdatasync</code> wasn‚Äôt going to work for me, by running into database corruption with <em>just</em> <code>pgbench</code>. Leaving <code>shared_buffers</code> set to the default value made the corruption pretty easy to trigger. Here‚Äôs what the logs look like:</p>
<details>
<summary>üìúÔ∏è Postgres block read error output (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>2021-11-28 16:31:45.700 UTC [299] ERROR:  could not read block 0 in file &#34;base/13754/16431_vm&#34;: Bad address
</span><span>2021-11-28 16:31:45.700 UTC [299] CONTEXT:  automatic vacuum of table &#34;postgres.public.pgbench_history&#34;
</span><span>2021-11-28 16:32:45.728 UTC [308] ERROR:  could not read block 0 in file &#34;base/13754/16431_vm&#34;: Bad address
</span><span>2021-11-28 16:32:45.728 UTC [308] CONTEXT:  automatic vacuum of table &#34;postgres.public.pgbench_history&#34;
</span><span>2021-11-28 16:33:45.787 UTC [317] ERROR:  could not read block 0 in file &#34;base/13754/16431_vm&#34;: Bad address
</span><span>2021-11-28 16:33:45.787 UTC [317] CONTEXT:  automatic vacuum of table &#34;postgres.public.pgbench_history&#34;
</span><span>2021-11-28 16:34:45.835 UTC [326] ERROR:  could not read block 0 in file &#34;base/13754/16431_vm&#34;: Bad address
</span></code></pre></div>
  
</details>




<p>When I saw these messages at first I was really confused because of course the only thing that these messages point to is database corruption, and all I was doing was running dinky old <code>pgbench</code>! A few resources pointed me in a few directions:</p>
<ul>
<li><a href="https://www.postgresql.org/message-id/CAA-aLv6Dp_ZsV-44QA-2zgkqWKQq%3DGedBX2dRSrWpxqovXK%3DPg%40mail.gmail.com">https://www.postgresql.org/message-id/CAA-aLv6Dp_ZsV-44QA-2zgkqWKQq%3DGedBX2dRSrWpxqovXK%3DPg%40mail.gmail.com</a></li>
<li><a href="https://github.com/LiskHQ/lisk-sdk/issues/268">https://github.com/LiskHQ/lisk-sdk/issues/268</a> (leads to an issue with their logic/transactions?)</li>
<li><a href="https://dba.stackexchange.com/questions/44508/error-could-not-read-block-x-of-relation-base-y-z">https://dba.stackexchange.com/questions/44508/error-could-not-read-block-x-of-relation-base-y-z</a></li>
</ul>
<p>And all roads lead to ~Rome~ database corruption. At first I suspected <code>mdraid</code> since that‚Äôs where I was running the tests at the time (to test against ZFS) so I ran it some more:</p>
<details>
<summary>üìúÔ∏è more Postgres block read error output (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>pgbench: error: client 3 script 0 aborted in command 5 query 0: ERROR:  could not read block 263392 in file &#34;base/13754/16404.2&#34;: Bad address
</span><span>pgbench: error: client 14 script 0 aborted in command 5 query 0: ERROR:  could not read block 193763 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 7 script 0 aborted in command 5 query 0: ERROR:  could not read block 209098 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 27 script 0 aborted in command 5 query 0: ERROR:  could not read block 62738 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 9 script 0 aborted in command 5 query 0: ERROR:  could not read block 24230 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 35 script 0 aborted in command 5 query 0: ERROR:  could not read block 21950 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench:pgbench: error: client 26 script 0 aborted in command 5 query 0: ERROR:  could not read block 53585 in file &#34;base/13754/16404&#34;: Bad address
</span><span> error: client 17 script 0 aborted in command 5 query 0: ERROR:  could not read block 112867 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 31 script 0 aborted in command 5 query 0: ERROR:  could not read block 8971 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 32 script 0 aborted in command 5 query 0: ERROR:  could not read block 127138 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 34 script 0 aborted in command 5 query 0: ERROR:  could not read block 566008 in file &#34;base/13754/16396.4&#34;: Bad address
</span><span>pgbench: error: client 38 script 0 aborted in command 5 query 0: ERROR:  could not read block 116455 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: pgbench: error: client 4 script 0 aborted in command 5 query 0: ERROR:  could not read block 31640 in file &#34;base/13754/16404&#34;: Bad address
</span><span>client 29 script 0 aborted in command 5 query 0: ERROR:  could not read block 137124 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 19 script 0 aborted in command 5 query 0: ERROR:  could not read block 78349 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: pgbench: error: client 20 script 0 aborted in command 5 query 0: ERROR:  could not read block 56435 in file &#34;base/13754/16404&#34;: Bad address
</span><span>client 0 script 0 aborted in command 5 query 0: ERROR:  could not read block 141619 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 12 script 0 aborted in command 5 query 0: ERROR:  could not read block 46705 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 1 script 0 aborted in command 5 query 0: ERROR:  could not read block 335297 in file &#34;base/13754/16396.2&#34;: Bad address
</span><span>pgbench: error: client 15 script 0 aborted in command 5 query 0: ERROR:  could not read block 131663 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 6 script 0 aborted in command 5 query 0: ERROR:  could not read block 2895 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 22 script 0 aborted in command 5 query 0: ERROR:  could not read block 251229 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 8 script 0 aborted in command 5 query 0: ERROR:  could not read block 129323 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 36 script 0 aborted in command 5 query 0: ERROR:  could not read block 25085 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 2 script 0 aborted in command 5 query 0: ERROR:  could not read block 259029 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 23 script 0 aborted in command 5 query 0: ERROR:  could not read block 168628 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 33 script 0 aborted in command 5 query 0: ERROR:  could not read block 233469 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>pgbench: error: client 24 script 0 aborted in command 5 query 0: ERROR:  could not read block 92632 in file &#34;base/13754/16404&#34;: Bad address
</span><span>pgbench: error: client 18 script 0 aborted in command 5 query 0: ERROR:  could not read block 40478 in file &#34;base/13754/16404&#34;: Bad address
</span><span>transaction type: &lt;builtin: TPC-B (sort of)&gt;
</span><span>scaling factor: 1000
</span><span>query mode: simple
</span><span>number of clients: 40
</span><span>number of threads: 4
</span><span>duration: 300 s
</span><span>number of transactions actually processed: 1251
</span><span>pgbench: fatal: Run was aborted; the above results are incomplete.
</span><span>latency average = 10.681 ms
</span><span>initial connection time = 100.183 ms
</span><span>tps = 3744.914729 (without initial connection time)
</span><span>statement latencies in milliseconds:
</span><span>         0.001  \set aid random(1, 100000 * :scale)
</span><span>         0.001  \set bid random(1, 1 * :scale)
</span><span>         0.001  \set tid random(1, 10 * :scale)
</span><span>         0.001  \set delta random(-5000, 5000)
</span><span>         0.447  BEGIN;
</span><span>         2.017  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
</span><span>         0.519  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
</span><span>         0.545  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
</span><span>         0.585  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
</span><span>         0.595  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
</span><span>         5.635  END;
</span></code></pre></div>
  
</details>




<p>Well that‚Äôs not good ‚Äì the errors actually <em>still</em> happens on ZFS, which is what <strong>tipped me off to this being a underlying hardware problem</strong>. ZFS was protecting me a bit more than I thought (due to the ARC absorbing some load) ‚Äì the problem definitely happens a lot more, and it looks increasingly clear that it is likely filesystem corruption.</p>
<p>Well luckily for me the Postgres Wiki has <a href="https://wiki.postgresql.org/wiki/Corruption">a page on Corruption</a>, so I took a look through there and thought if there were any things there that could have been issues‚Ä¶ Defective CPU or RAM didn‚Äôt seem to be the case (I didn‚Äôt have ECC RAM in the server but I ran some checks on the RAM and they seemed fine, the tests also consistently failed across machines).</p>
<p>At the very least the fact that it was happening on <em>both</em> <code>mdraid</code> and ZFS was a good sign that it wasn‚Äôt the ZFS subsystem and it wasn‚Äôt <code>mdraid</code> in particular. The problem seemed to be ‚Äúmasked‚Äù by <code>shared_buffers</code> so I made <a href="https://www.reddit.com/r/PostgreSQL/comments/r5kdpo/pg_on_different_storage_mediums_shared_buffers/">a reddit post</a> and got some great help from the community.</p>
<details>
<summary>üìúÔ∏è pgbench error output (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>2021-11-29 04:36:49.649 UTC [115] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -2070 WHERE aid = 30688590;
</span><span>2021-11-29 04:36:49.649 UTC [95] ERROR:  could not read block 66695 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.649 UTC [95] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 3387 WHERE aid = 24410232;
</span><span>2021-11-29 04:36:49.649 UTC [114] ERROR:  could not read block 27935 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.649 UTC [114] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 4906 WHERE aid = 10195188;
</span><span>2021-11-29 04:36:49.649 UTC [109] ERROR:  could not read block 33635 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.649 UTC [109] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 649 WHERE aid = 12338677;
</span><span>2021-11-29 04:36:49.649 UTC [126] ERROR:  could not read block 70731 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.649 UTC [126] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -283 WHERE aid = 25795973;
</span><span>2021-11-29 04:36:49.649 UTC [116] ERROR:  could not read block 64302 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.649 UTC [116] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -3404 WHERE aid = 23451372;
</span><span>2021-11-29 04:36:49.650 UTC [93] ERROR:  could not read block 213483 in file &#34;base/13754/16404.1&#34;: Bad address
</span><span>2021-11-29 04:36:49.650 UTC [93] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -1552 WHERE aid = 77858559;
</span><span>2021-11-29 04:36:49.651 UTC [111] ERROR:  could not read block 67287 in file &#34;base/13754/16404&#34;: Bad address
</span><span>2021-11-29 04:36:49.651 UTC [111] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 4587 WHERE aid = 24539716;
</span><span>2021-11-29 04:37:37.732 UTC [135] ERROR:  could not read block 0 in file &#34;base/13754/16431_vm&#34;: Bad address
</span><span>2021-11-29 04:37:37.732 UTC [135] CONTEXT:  while scanning block 0 of relation &#34;public.pgbench_history&#34;
</span></code></pre></div>
  
</details>




<p>With some of the error messages I could directly see the statements that were failing and they seemed like <em>very</em> average statements so I started to get a bit worried. The next thing I tried was downgrading to PG 13, thinking I might have found a Postgres bug as version 14.1 was very new (of course not). The logs were very similar:</p>
<details>
<summary>üìúÔ∏è Postgres container output (click to expand)</summary>
  
  
  
  

  
    <div><pre tabindex="0"><code data-lang="console"><span>...
</span><span>PostgreSQL init process complete; ready for start up.
</span><span></span><span>
</span><span></span><span>2021-12-02 11:16:23.317 UTC [1] LOG:  starting PostgreSQL 13.5 on x86_64-pc-linux-musl, compiled by gcc (Alpine 10.3.1_git20211027) 10.3.1 20211027, 64-bit
</span><span>2021-12-02 11:16:23.317 UTC [1] LOG:  listening on IPv4 address &#34;0.0.0.0&#34;, port 5432
</span><span>2021-12-02 11:16:23.317 UTC [1] LOG:  listening on IPv6 address &#34;::&#34;, port 5432
</span><span>2021-12-02 11:16:23.323 UTC [1] LOG:  listening on Unix socket &#34;/var/run/postgresql/.s.PGSQL.5432&#34;
</span><span>2021-12-02 11:16:23.329 UTC [51] LOG:  database system was shut down at 2021-12-02 11:16:23 UTC
</span><span>2021-12-02 11:16:23.334 UTC [1] LOG:  database system is ready to accept connections
</span><span>2021-12-02 11:18:17.933 UTC [52] LOG:  checkpoints are occurring too frequently (29 seconds apart)
</span><span>2021-12-02 11:18:17.933 UTC [52] HINT:  Consider increasing the configuration parameter &#34;max_wal_size&#34;.
</span><span>2021-12-02 11:19:47.736 UTC [116] ERROR:  could not read block 648535 in file &#34;base/13444/16396.4&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [116] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 2027 WHERE aid = 39560672;
</span><span>2021-12-02 11:19:47.736 UTC [123] ERROR:  could not read block 585913 in file &#34;base/13444/16396.4&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [123] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 2180 WHERE aid = 35740744;
</span><span>2021-12-02 11:19:47.736 UTC [115] ERROR:  could not read block 838163 in file &#34;base/13444/16396.6&#34;: read only 4352 of 8192 bytes
</span><span>2021-12-02 11:19:47.736 UTC [115] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -4292 WHERE aid = 51127988;
</span><span>2021-12-02 11:19:47.736 UTC [127] ERROR:  could not read block 52370 in file &#34;base/13444/16396&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [127] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -89 WHERE aid = 3194591;
</span><span>2021-12-02 11:19:47.736 UTC [122] ERROR:  could not read block 166523 in file &#34;base/13444/16396.1&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [122] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 1505 WHERE aid = 10157924;
</span><span>2021-12-02 11:19:47.736 UTC [119] ERROR:  could not read block 54214 in file &#34;base/13444/16404&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [119] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + -3274 WHERE aid = 19771731;
</span><span>2021-12-02 11:19:47.736 UTC [94] ERROR:  could not read block 25913 in file &#34;base/13444/16404&#34;: Bad address
</span><span>2021-12-02 11:19:47.736 UTC [94] STATEMENT:  UPDATE pgbench_accounts SET abalance = abalance + 3160 WHERE aid = 9450375;
</span><span>2021-12-02 11:19:47.736 UTC [129] ERROR:  could not read block 29298 in file &#34;base/13444/16404&#34;: Bad address
</span></code></pre></div>
  
</details>




<p>Very curiously ,the folder seemed to be stuck at around 16GB before the errors would show up.</p>

<p>Generally shared_buffers should be 25% of RAM it seems‚Ä¶ the nodes did have 16GB of RAM so 4GB would be what to set it to. The runs would complete with <code>shared_buffers</code> set to 4GB (but <em>thankfully</em> I didn‚Äôt stop there) ‚Äì obviously, a lack of memory space (<code>shared_buffers</code>) should not cause a database like Postgres to los integrity. I may have found a workaround but something is fundamentally wrong.</p>
<h2 id="dead-end-max_wal_size-">Dead end: <code>max_wal_size</code> ?</h2>
<p>Increasing the max_wal_size to 4096 (4GB) also delayed the problem but didn‚Äôt fix it ‚Äì <code>pgbench</code> runs were still failing. Looking at <code>max_wal_size</code> did give me an idea though ‚Äì maybe I was running out of some kind of resource?</p>
<h2 id="dead-end-ulimit">Dead end: ulimit?</h2>
<p>I started thinking that maybe I was hitting some file system limit, so I checked <code>ulimit -a</code>:</p>
<div><pre tabindex="0"><code data-lang="console"><span>root@node-2 ~ # ulimit -a
</span><span>core file size          (blocks, -c) 0
</span><span>data seg size           (kbytes, -d) unlimited
</span><span>scheduling priority             (-e) 0
</span><span>file size               (blocks, -f) unlimited
</span><span>pending signals                 (-i) 255813
</span><span>max locked memory       (kbytes, -l) 65536
</span><span>max memory size         (kbytes, -m) unlimited
</span><span>open files                      (-n) 1024
</span><span>pipe size            (512 bytes, -p) 8
</span><span>POSIX message queues     (bytes, -q) 819200
</span><span>real-time priority              (-r) 0
</span><span>stack size              (kbytes, -s) 8192
</span><span>cpu time               (seconds, -t) unlimited
</span><span>max user processes              (-u) 255813
</span><span>virtual memory          (kbytes, -v) unlimited
</span><span>file locks                      (-x) unlimited
</span></code></pre></div><p>And the ‚Äúreal‚Äù usage during <code>pgbench</code> <em>seemed</em> to be:</p>
<div><pre tabindex="0"><code data-lang="console"><span>root@node-2 ~ # cat /proc/sys/fs/file-nr
</span><span>50112   0       9223372036854775807
</span></code></pre></div><p>Based on this I thought maybe the open files limit was the issue, but then I checked the <code>max</code>:</p>
<pre tabindex="0"><code>root@node-2 ~ # sysctl fs.file-max
fs.file-max = 9223372036854775807
</code></pre><p>Well‚Ä¶ It looks like the limit is <em>not</em> the issue. In fact, you can actually check by a specific process and find out what the limits are and get the right values (which are set by container orchestration tooling):</p>
<pre tabindex="0"><code>root@node-2 ~ # ulimit -n 65536
root@node-2 ~ # ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 255813
max locked memory       (kbytes, -l) 65536
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65536
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 255813
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
</code></pre><p>Clearly no problem there.</p>
<h2 id="solution-force-proper-disk-syncing">Solution: Force proper disk syncing</h2>
<p>The insight came from <a href="https://www.reddit.com/r/PostgreSQL/comments/r5kdpo/pg_on_different_storage_mediums_shared_buffers/hmxxs88/">/u/nikowek on reddit</a> ‚Äì the lost writes (that couldn‚Äôt be read back) coupled with <em>absolutely no feedback</em> from the Kernel or Postgres itself on writes failing means that the drives below <em>must</em> have been reporting writes successful when they <em>weren‚Äôt</em>.</p>
<p>It looks like I‚Äôd run head first into a feature of the drives below that wasn‚Äôt quite working as I‚Äôd expected ‚Äì the ‚ÄúVolatile cache‚Äù write cache is to blame (there‚Äôs a great post on this in <a href="https://techcommunity.microsoft.com/t5/storage-at-microsoft/don-t-do-it-consumer-grade-solid-state-drives-ssd-in-storage/ba-p/425914">some microsoft forums</a>).</p>
<p>As this is the problem, there are only two easy ways around it:</p>
<ul>
<li><code>wal_sync_method=fsync</code> at the postgres level (instead of <code>fdatasync</code>)</li>
<li>Disable write back cache at the drive level with (<code>apt install nvme-cli</code>)</li>
</ul>
<p>Since I don‚Äôt <em>ever</em> want to run into this problem again (at least until I upgrade to enterprise HDDs across the board, I guess), I‚Äôve gone with the second one ‚Äì which is disabling the volatile write cache at the drive level:</p>
<div><pre tabindex="0"><code data-lang="console"><span>$ </span>nvme set-feature -f <span>6</span> -v <span>0</span> /dev/nvme0n1
</code></pre></div><h2 id="same-same-but-different-longhorn-has-the-same-issue">Same same, but different: Longhorn has the same issue</h2>
<p>While I was here, I ran the <code>pgbench</code> tests against a postgres instance with storage backed by my <a href="https://longhorn.io/docs">Longhorn</a> installation. I was instantly suspicious because the Longhorn runs actually ran <em>as fast as</em> <code>mdraid</code> and ZFS runs which just can‚Äôt be right ‚Äì Longhorn performs <em>synchronous</em> remote writes ‚Äì there‚Äôs no way the performace should be at par with local-only disk writes, even with increased <code>shared_buffers</code> size.</p>
<p>Once I reduced the size of <code>shared_buffers</code> the issue presented itself (I guess <code>longhorn</code> uses <code>fdatasync</code> or one of the other options underneath?), so <strong>the right way to fix the problem was indeed disabling the volatile write cache at the drive.</strong></p>

<p>Here‚Äôs all the sources so you can look for yourself. Thanks to all these people for sharing their wealth of knowledge. Special thanks to /u/mercenary_sysadmin on Reddit and all the comitters and contributors on the Postgres and ZFS mailing lists.</p>
<ul>
<li><a href="https://people.freebsd.org/%7Eseanc/postgresql/scale15x-2017-postgresql_zfs_best_practices.pdf">Slides from a PoZoL presentation at Scale15x Conference in 2017</a> (<a href="https://www.youtube.com/watch?v=dwMQXLOXUco&amp;t=5560s">Talk on Youtube</a>)
<ul>
<li><a href="https://lobste.rs/s/ux1gik/postgresql_zfs_best_practices_standard">Lobster.rs discussion</a></li>
</ul>
</li>
<li><a href="https://bun.uptrace.dev/postgres/tuning-zfs-aws-ebs.html#postgresql-block-size-and-wal-size">Bun‚Äôs post on Postgres on ZFS on AWS</a></li>
<li><a href="https://papers.freebsd.org/2020/BSDcan/munro-PostgreSQL_on_FreeBSD.files/munro-PostgreSQL_on_FreeBSD.pdf">Thomas Munro‚Äôs paper on Postgres for FreeBSD</a></li>
<li><a href="https://www.slideshare.net/GrantMcAlister/tuning-postgresql-for-high-write-throughput">Grant McAlister‚Äôs talk on tuning Postgres for High Write Throughput</a></li>
<li><a href="https://www.percona.com/sites/default/files/presentations/pg-Performance-Tuning.pdf">Percona‚Äôs presentation on Postgres Performance Tuning</a></li>
<li><a href="https://dan.langille.org/2013/02/15/zfs-tuning-2/">Dan Langille‚Äôs post on ZFS tuning</a></li>
<li><a href="https://www.percona.com/blog/mysql-zfs-performance-update/">Percona‚Äôs MySQL ZFS performance <em>update</em> post</a> (not Postgres but still useful!)</li>
<li><a href="https://www.cybertec-postgresql.com/en/tuning-max_connections-in-postgresql/">CYBERTEC‚Äôs post on tuning <code>max_connections</code></a></li>
</ul>

<p>So there are lots more links and information to read through. I got going down a rabbit hole, so I want to leave all these resources here for anyone else who wants to jump down:</p>
<ul>
<li><a href="https://www.percona.com/blog/2018/05/15/about-zfs-performance/">Percona blog: About ZFS Performance</a>
<ul>
<li>Benchmark seems to be flawed because ZFS w/ L2ARC was <em>not</em> compared to the equivalent for XFS (<code>bcache</code>) or EXT4 (<code>dm-cache</code> +/- LVM caching?)</li>
<li>Commenter called out <code>logbias=throughput</code> as terrible for causing fragmentation (just read all the quotes by ‚ÄúJanet Campbell‚Äù)</li>
</ul>
</li>
<li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/898/6/062054/pdf">Paper: Evaluation of ZFS as an efficient WLCG storage backend</a>
<ul>
<li>ZFS is compared with XFS and EXT4 directly</li>
<li><a href="https://indico.cern.ch/event/505613/contributions/2230928/">Talk schedule page which comes with more PDFs</a> (at the bottom)</li>
</ul>
</li>
<li><a href="https://www.snia.org/sites/default/files/SDC/2019/presentations/File_Systems/McKenzie_Ryan_Best_Practices_for_OpenZFS_L2ARC_in_the_Era_of_NVMe.pdf">Slides: Best Practices for OpenZFS L2ARC in the Era of NVMe by Ryan McKenzie of iXsystems</a></li>
<li><a href="https://napp-it.org/doc/downloads/optane_slog_pool_performane.pdf">Long Paper: napp-it ZFS storage performance tests</a>
<ul>
<li>This seems like <em>kinda</em> like a paid advertisement for Oracle Solaris / Intel Optane, but it has a very helfpul TL;DR.</li>
</ul>
</li>
<li><a href="https://martin.heiland.io/2018/02/23/zfs-tuning/">Article on ZFS tuning by Martin Heiland</a>
<ul>
<li>I think he reaches the wrong conclusion about compression here, but some interesting insights for VM workloads.</li>
</ul>
</li>
<li></li>
</ul>
<p>And here‚Äôs the rabbit hole I went down‚Ä¶.</p>
<ul>
<li><a href="https://www.joyent.com/blog/bruning-questions-zfs-record-size">Post: Bruning questions about ZFS record size (Max Bruning at Joyent)</a>
<ul>
<li>This post leads to the edge of the rabbit hole, which is a dead link to <a href="https://blogs.oracle.com/solaris/post/tuning-zfs-recordsize">An old Oracle post about tuning ZFS recordsize</a></li>
</ul>
</li>
<li><a href="https://blogs.oracle.com/solaris/post/tuning-zfs-recordsize">Post: Tuning ZFS Recordsize (Oracle)</a>
<ul>
<li>At this point, I went through every ZFS related post on oracle and picked out everything that looked interesting‚Ä¶</li>
</ul>
</li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-and-oltp">https://blogs.oracle.com/solaris/post/zfs-and-oltp</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-write-throttle-observations">https://blogs.oracle.com/solaris/post/zfs-write-throttle-observations</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools">https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools</a>
<ul>
<li><a href="https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-1-getting-started-zpool-create-status-import-export">https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-1-getting-started-zpool-create-status-import-export</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-2-mirrors-disk-failures-and-spare-disks">https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-2-mirrors-disk-failures-and-spare-disks</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-3-raidz2-scrubbing-and-resilvering">https://blogs.oracle.com/solaris/post/a-hands-on-introduction-to-zfs-pools-part-3-raidz2-scrubbing-and-resilvering</a></li>
</ul>
</li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-compression-perf-disk-space-and-watts-the-naked-truth-part-1-of-2">https://blogs.oracle.com/solaris/post/zfs-compression-perf-disk-space-and-watts-the-naked-truth-part-1-of-2</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-compression-perf-disk-space-and-watts-the-naked-truth-part-2-of-2">https://blogs.oracle.com/solaris/post/zfs-compression-perf-disk-space-and-watts-the-naked-truth-part-2-of-2</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/understanding-the-space-used-by-zfs">https://blogs.oracle.com/solaris/post/understanding-the-space-used-by-zfs</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-compression-a-win-win">https://blogs.oracle.com/solaris/post/zfs-compression-a-win-win</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/does-zfs-really-use-more-ram">https://blogs.oracle.com/solaris/post/does-zfs-really-use-more-ram</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/the-new-zfs-write-throttle">https://blogs.oracle.com/solaris/post/the-new-zfs-write-throttle</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/the-dynamics-of-zfs">https://blogs.oracle.com/solaris/post/the-dynamics-of-zfs</a></li>
<li><a href="https://blogs.oracle.com/oracle-systems/post/reducing-storage-costs-with-zfs-compression">https://blogs.oracle.com/oracle-systems/post/reducing-storage-costs-with-zfs-compression</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/zfs-space-reporting-snapshots-clones-and-how-nfs-deals-with-it">https://blogs.oracle.com/solaris/post/zfs-space-reporting-snapshots-clones-and-how-nfs-deals-with-it</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/nfs-and-zfs-a-fine-combination">https://blogs.oracle.com/solaris/post/nfs-and-zfs-a-fine-combination</a></li>
<li><a href="https://blogs.oracle.com/virtualization/post/availability-best-practices-using-a-mirrored-zfs-pool-with-virtual-disks">https://blogs.oracle.com/virtualization/post/availability-best-practices-using-a-mirrored-zfs-pool-with-virtual-disks</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/when-to-and-not-to-use-raid-z">https://blogs.oracle.com/solaris/post/when-to-and-not-to-use-raid-z</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/zero-copy-io-aggregation">https://blogs.oracle.com/solaris/post/zero-copy-io-aggregation</a></li>
<li><a href="https://blogs.oracle.com/solaris/post/need-inodes">https://blogs.oracle.com/solaris/post/need-inodes</a></li>
</ul>
<p>Another completely separate rabbit hole which looks like it holds even <em>richer</em> treasures is <a href="https://utcc.utoronto.ca/~cks/space/blog/__TopicZFS">Chris Seibermann of UToronto‚Äôs blog posts on ZFS</a> (thanks to /u/BucketOfSpinningRust on Reddit).</p>
<p>/u/BucketOfSpinningRust on reddit also was kind enough to share some explanations of behavior on the ZFS side in two separate comments:</p>
<ul>
<li><a href="https://www.reddit.com/r/zfs/comments/rjavyc/everything_ive_seen_on_optimizing_postgres_on_zfs/hp6vrra/">https://www.reddit.com/r/zfs/comments/rjavyc/everything_ive_seen_on_optimizing_postgres_on_zfs/hp6vrra/</a>
<ul>
<li>This was a correction of the notion that multiple pages ever go into the same ‚ÄúZFS Record‚Äù ‚Äì this is a mischaracterization that I parroted, what happens is that more pages will be able to <em>likely</em> fit in the same area of disk ‚Äì compression is done <em>after</em> the concept of a ZFS record is gone (i.e. when you know the physical size of one or more writes in a transaction group).</li>
</ul>
</li>
<li><a href="https://www.reddit.com/r/zfs/comments/rjavyc/everything_ive_seen_on_optimizing_postgres_on_zfs/hp8u2rk/">https://www.reddit.com/r/zfs/comments/rjavyc/everything_ive_seen_on_optimizing_postgres_on_zfs/hp8u2rk/</a>
<ul>
<li>Great explanation on how record size is treated, reproduced below, just in case in some weird future this site is up and Reddit isn‚Äôt (or it‚Äôs paywalled):</li>
</ul>
</li>
</ul>
<blockquote>
<p>Files smaller than the record size are compressed (if applicable), and then stored as unique records that physically occupy unique sectors on disk (IE round up to the nearest ashift value). With a record size of 8k (or larger), you can store a 5k file that compresses down to 3k in 4k of disk space with ashift=12. Files larger than a record size are broken into blocks of the record size, then compressed and stored, again rounding up. A 5M file with 1M records could be stored as 2 uncompressed 1M records, a 512k record, a 100k record, and another 1M record. Side note: There are some weird edge cases with very large record sizes. A ‚Äã1.001M file with 1M record sizes will be stored as 2 1M records, then compressed.</p>
<p>In response to your edit: You can set your record size to some multiple of the page size in whatever database you‚Äôre using. There are some perfectly legitimate reasons to do this. Jumping to 16 and 32k records often gives you moderate bumps in compression ratios. (Going past 32k rarely gives you worthwhile benefits in my experience.) This means you can store more in the ARC and/or use less memory. It also means that you warm the cache faster which is relevant for spinning up disposable VMs for things like load balancing. For databases where you frequently read multiple adjacent pages, it acts as a form of prefetching. For exceptionally large databases that aren‚Äôt under heavy I/O load the compression can save on disk space (that‚Äôs a rare usage case admittedly).</p>
<p>The catch is write amplification, or even worse, read modify writes. Write amplification itself isn‚Äôt always a major issue on rust because the limiting factor is seek time. The difference between doing 4, 8, 16 and 32k writes is fairly small in terms of IOPS. Yeah, you can do fewer operations with larger writes because the head needs to spend longer writing (duh), but the time spent writing on full random workloads is somewhat trivial compared to the time the head spends moving around. What usually fucks your performance when oversizing is read modify writes. If everything is cached in RAM, yeah you‚Äôre using a bit of extra memory throughput and burning some CPU time doing compression, checksums, etc, but that often doesn‚Äôt matter. What matters is disk throughput. If oversizing records gives you better compression to fit more stuff, or your workload does enough adjacent reads to make the pseudo prefetching give you better ARC hit rates, you can actually reduce disk usage.</p>
<p>It‚Äôs a balancing act. Time spent reading off of rust is time that cannot be spent writing to disk, and vice versa. Oversizing ZFS records can reduce the number of reads going to disk, which can result in increased performance, but if you go too far it‚Äôs very easy to completely tank your performance.</p>
</blockquote>

<p>There is lots of information and links out there on the internet about Postgres and ZFS ‚Äì hopefully you‚Äôve discovered some new sources to read up on! As always, if you have some nitpicks or good corrections I‚Äôd love to hear them so I can update and make this post even better (and help all the lost souls who wander in). I‚Äôd love to hear about areas that I‚Äôve overlooked or new optimization opportunities I‚Äôve never heard of.</p>
<p>Thanks for reading!</p>

            
          </div>

        </div>
      </div></div>
  </body>
</html>
