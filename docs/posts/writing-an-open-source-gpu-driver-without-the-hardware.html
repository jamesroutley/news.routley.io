<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.collabora.com/news-and-blog/blog/2022/01/27/writing-an-open-source-gpu-driver-without-the-hardware/">Original</a>
    <h1>Writing an open source GPU driver without the hardware</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>After <a href="http://harihareswara.net/posts/2022/low-availability-till-early-march/news-and-blog/blog/2021/07/19/reverse-engineering-the-mali-g78/">six months of reverse-engineering</a>, the new Arm “Valhall” GPUs (Mali-G57, Mali-G78) are getting free and open source Panfrost drivers. With a <a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/14542">new compiler, driver patches</a>, and some kernel hacking, these new GPUs are almost ready for upstream.</p>
<p>In 2021, there were no Valhall devices running mainline Linux. While a lack of devices poses an obvious obstacle to device driver development, there is no better time to write drivers than before hardware reaches end-users. Developing and distributing production-quality drivers takes time, and we don’t want users to be reliant on closed source blobs. If development doesn’t start until a device hits shelves, that device could reach “end-of-life” by the time there are mature open drivers. But with a head start, we can have drivers ready by the time devices reach end users.</p>
<p>Let’s see how.</p>
<h2 id="reverse-engineering-without-root">Reverse-engineering without root</h2>
<p>Over the summer, Collabora purchased an Android phone with a Mali-G78. The phone isn’t rooted, so we can’t replace its graphics drivers with our own. We <em>can</em> put the phone in developer mode, so we can run test applications with the proprietary graphics driver and inject our own code with <code>LD_PRELOAD</code>, allowing us to inspect the graphics memory prepared by the proprietary driver and “passively” reverse-engineer the hardware. This memory includes compiled shader binaries in the Valhall instruction set, as well as Valhall’s data structures controlling graphics state like textures, blending, and culling.</p>
<p>Reverse-engineering “actively” is possible, too. We can <em>modify</em> compiled shaders and GPU data structures, allowing us to experiment with individual bits. We can go further, constructing our own shaders and data structures and validating them against the hardware.</p>
<p>To motivate this technique, consider the reverse-engineering of Valhall’s “buffer descriptor”. This new data structure describes a buffer of memory, accessed by a new “load buffer” instruction (<code>LD_BUFFER</code>). After guessing the layout of the buffer descriptor and encoding of <code>LD_BUFFER</code>, we can build our own buffer descriptor and write a shader using <code>LD_BUFFER</code> to validate our guess and probe the low-level semantics.</p>
<p>When reverse-engineering Valhall’s new data structures, we have legacy to guide us. While Valhall reorganizes its data structures to reduce Vulkan driver overhead, the bit-level contents resemble older Mali GPUs. If we find the “contours” of new data structures, we can fill in the details by comparing with older hardware.</p>
<p>As we learn about the data structures, we document our findings in a formal <a href="https://gitlab.freedesktop.org/mesa/mesa/-/blob/main/src/panfrost/lib/genxml/v9.xml">XML hardware description</a>. This file has the same format as the XML for older Mali architectures already supported by Panfrost. Since the Valhall data structures descend from these older architectures, we can fork an older Mali’s XML to save us some typing and keep naming consistent.</p>
<p>After enough reverse-engineering, we can slot our XML into Panfrost, automatically generating code to pack and unpack the data structures. Thanks to tireless work by Collaboran <a href="https://gitlab.freedesktop.org/bbrezillon">Boris Brezillon</a>, Panfrost’s performance-critical code is specialized at compile-time to the target architecture, allowing us to add new architectures without adding overhead to existing hardware. So with our XML in hand, we can get started writing a Valhall driver.</p>
<h2 id="writing-drivers-without-hardware">Writing drivers without hardware</h2>
<p>It is November 2021. I’ve written a Valhall compiler. I’ve reverse-engineered enough to write a driver. I still have no Linux hardware to test my code.</p>
<p>That’s a major road block.</p>
<p>Good thing I know a detour.</p>
<p>We can develop the driver on <em>any</em> Linux machine, without testing against real hardware. To pull that off, unit testing is <strong>mandatory</strong>. With no hardware, we can’t run integration tests, but unit tests can run on any hardware. For the Valhall compiler, I wrote unit tests for everything from instruction packing to optimization. Although the coverage isn’t exhaustive, it caught numerous bugs early on.</p>
<p>There is a caveat: unit testing can’t tell us if our expectations of the hardware are correct. However, it <em>can</em> confirm that our code matches our expectations. If our reverse-engineering is thorough, these expectations should be correct.</p>
<p>Even so, unit testing alone isn’t enough.</p>
<p>Enter <code>drm-shim</code>.</p>
<h2 id="drm-shim">drm-shim</h2>
<p>Mesa drivers like Panfrost can mock test hardware with <code>drm-shim</code>, a small library which stubs out the system calls used by userspace graphics drivers to communicate with the kernel. With <code>drm-shim</code>, unmodified userspace drivers think they’re running against real hardware – including Valhall hardware.</p>
<p>Graphics guru <a href="http://anholt.net/">Emma Anholt</a> designed <code>drm-shim</code> to run Mesa’s compilers as cross-compilers for use in continuous integration (CI). Outside of CI, <code>drm-shim</code> allows testing compilers on our development machines, which may be significantly faster than the embedded devices we target. But it’s not limited to compilers; we can run entire test suites under <code>drm-shim</code>, “cross-testing” for any hardware we please. The tests won’t pass, since <code>drm-shim</code> does no rendering; it is a shim, not an emulator. But it allows us to exercise new driver code paths without the constraints of real hardware.</p>
<p>As <code>drm-shim</code> runs on <em>any</em> Linux machine, I wanted to use the fastest Linux machine I own: my Apple M1. Bizarrely, <code>drm-shim</code> didn’t work on my M1 Linux box, although it works on everyone else’s computers. That calls for a debugging session.</p>
<p>After some poking around, I stumbled on the offending code:</p>
<div id="cb1">
<pre><code><span id="cb1-1">bo-&gt;addr = util_vma_heap_alloc(&amp;heap, size, <span>4096</span>);</span>
<span id="cb1-2">mmap(NULL, ..., bo-&gt;addr);</span></code></pre>
</div>
<p>This code allocates a chunk of memory aligned to a page and uses its address as the offset in a call to <code>mmap</code>. On my system, the <code>mmap</code> call fails, so I consulted the man page for <code>mmap</code>:</p>
<blockquote>
<p><code>offset</code> must be a multiple of the page size as returned by <code>sysconf(_SC_PAGE_SIZE)</code>.</p>
</blockquote>
<p>The <code>mmap</code> in <code>drm-shim</code> works, because the page size on Linux is 4096 bytes (4K)…</p>
<p>Until it isn’t.</p>
<p>Apple’s input/output memory management unit uses larger pages, 16384 bytes (16K) large. As a consequence, when we run Linux bare metal on Apple platforms, we configure Linux to use 16K pages everywhere to keep life simple. That means that on Apple platforms running Linux, <code>sysconf(_SC_PAGE_SIZE)</code> returns 16384, so the <code>mmap</code> fails. The fix is easy:</p>
<div id="cb2">
<pre><code><span id="cb2-1">bo-&gt;addr = util_vma_heap_alloc(&amp;heap, size, sysconf(_SC_PAGE_SIZE));</span>
<span id="cb2-2">mmap(NULL, ..., bo-&gt;addr);</span></code></pre>
</div>
<p>With that, <code>drm-shim</code> works on systems with page sizes larger than 4K, including my M1. That means I can compile thousands of shaders per second with the Valhall compiler, far more than any system with a Mali GPU could. I can also run Khronos’s OpenGL ES Conformance Test Suite:</p>
<pre><code>PAN_MESA_DEBUG=valhall,trace LIBGL_DRIVERS_PATH=~/lib/dri/ LD_PRELOAD=~/mesa/build/src/panfrost/drm-shim/libpanfrost_noop_drm_shim.so PAN_GPU_ID=9091 EGL_PLATFORM=surfaceless ./deqp-gles31 --deqp-surface-type=pbuffer --deqp-gl-config-name=rgba8888d24s8ms0 --deqp-surface-width=256 --deqp-surface-height=256&#39;</code></pre>
<p>Long commands like this one run tests and produce pretty-printed dumps of GPU memory, ready for manual inspection. If the dumps look like the dumps from the proprietary driver, there’s a good chance the tests will pass on real hardware, too.</p>
<h2 id="code-sharing">Code sharing</h2>
<p>Since Valhall is similar to its predecessors, the years we’ve spent nurturing Panfrost mean we only need to modify the driver in areas where Valhall introduces breaking changes.</p>
<p>For example, Valhall’s instruction set resembles the older “Bifrost” instruction set, so we may embed the Valhall compiler as an additional backend in the existing Bifrost compiler. Shared compiler passes like instruction selection and register allocation “just work” on Valhall, even though they were developed and debugged for Bifrost.</p>
<p>Once we adapt Panfrost for Valhall, we’ll have a conformant, performant driver ready out-of-the-box.</p>
<p>…In theory.</p>
<h2 id="real-hardware-real-pain">Real hardware, real pain</h2>
<p>I couldn’t test on real Valhall hardware until early January, when I procured a Chromebook with a MediaTek MT8192 system-on-chip and a matching serial cable. MT8192 sports a Valhall “Mali-G57” GPU, compatible with the Mali-G78 I’m reverse-engineering. Mainline kernel support for MT8192 is sparse, but Linux does boot. With patches by other Collaborans, USB works too. That’s enough to get to work on the GPU. Sure, the <em>display</em> doesn’t work, but who needs <em>that</em>?!</p>
<p>We’ll start by teaching Linux how to find the GPU. On desktops, ACPI and UEFI let the operating system discover any connected hardware. While these standards exist for Arm, in practice Arm systems require a <em>device tree</em> describing the hardware: what parts there are, which registers and clocks they use, and how they’re connected. We don’t know much about MT8192, but ChromeOS supports it, so ChromeOS has a complete device tree. Adapting that device tree for mainline, we soon see signs of life:</p>
<pre><code>[  1.942843] panfrost 13000000.gpu: <strong>unknown</strong> id 0x9093 major 0x0 minor 0x0 status 0x0
</code></pre>
<p>The kernel cannot identify the connected Mali GPU, but that’s expected – after all, it has never seen a Mali-G57 before. We need to add a mapping from Mali-G57’s hardware ID to its name, feature list, and hardware bug list. Then the driver loads.</p>
<pre><code>[  1.942843] panfrost 13000000.gpu: <strong>mali-g57</strong> id 0x9093 major 0x0 minor 0x0 status 0x0
[  1.982322] [drm] Initialized panfrost 1.2.0 20180908 for 13000000.gpu on minor 0
</code></pre>
<p>Based on the downstream kernel module released by Arm, we know the parts of Valhall relevant to the kernel are backwards-compatible with Mali GPUs from a decade ago. Panfrost supports existing Mali hardware, so in theory, we can test drive the Mali-G57 right now.</p>
<p>When it comes to hardware, theory and practice never agree.</p>
<p>Let’s try submitting a “null job” to the hardware, a simple job that does nothing whatsoever:</p>
<div id="cb4">
<pre><code><span id="cb4-1"><span>struct</span> mali_job_descriptor_header job = {</span>
<span id="cb4-2">    .job_type = MALI_JOB_TYPE_NULL,</span>
<span id="cb4-3">    .job_index = <span>1</span></span>
<span id="cb4-4">};</span></code></pre>
</div>
<p>Only 2 bits set in the entire data structure. We can even hard-code this job into the kernel and submit it as soon as the hardware powers on. Since this job is correct, the hardware will run it fine.</p>
<pre><code>[   2.094748] panfrost 13000000.gpu: js fault, js=1, status=DATA_INVALID_FAULT, head=0x6087000, tail=0x6087000</code></pre>
<p>What? The hardware claims the job is invalid, even though the job <em>is</em> clearly valid. Apparently, the hardware is reading something different from memory than we wrote.</p>
<p>That symptom is eerily familiar. When Collaboran <a href="https://gitlab.freedesktop.org/tomeu">Tomeu Vizoso</a> and I added support for Mali-G52 two years ago, we observed the same symptoms on an Amlogic system-on-chip. The culprit was an Amlogic-specific <em>cache coherency</em> issue. That fix doesn’t apply here, so it’s time to hunt for MediaTek-specific bugs.</p>
<p>Crawling through ChromeOS code, I found that MediaTek submitted an unexplained change to the GPU driver, setting a single bit belonging to a <em>clock</em> on MT8192 in order to “disable ACP”, fixing bus faults. This change is the embodiment of a “fix everything” magic bit, the kind only rumoured to exist and the stuff of reverse-engineers’ nightmares.</p>
<p>…But <a href="https://patchwork.kernel.org/project/linux-arm-kernel/patch/20220110181330.3224-1-alyssa.rosenzweig@collabora.com/">setting that bit in our kernel</a> makes our null job complete successfully.</p>
<p>…Wait, what?</p>
<p>It turns out ACP is the “Accelerator Coherency Port”, responsible for managing cache coherency between the CPU and the GPU. Apparently, ACP was not supposed to be enabled on MT8192, but due a hardware bug was enabled accidentally. The kernel must set this bit to disable ACP as a workaround.</p>
<p>Again, what?</p>
<p>Pressing on, we can submit the same null job from userspace. To the hardware, kernelspace and userspace are the same, so this must work.</p>
<p>It does not.</p>
<p>The job times out before completing. Inspecting the kernel log, we notice an earlier timeout, waiting for the GPU to wake up after being reset.</p>
<p>Littering the kernel with <code>printk</code>s, eventually we find that <em>the GPU is powered off</em> once Linux boots, and <em>nothing we do will power it back on</em>. No wonder everything times out.</p>
<p>For some problems, we can only hope for a leprechaun to whisper the solution in our ear. Our leprechaun comes in the form of kernel wizard <a href="https://gitlab.freedesktop.org/mmind">Heiko Stuebner</a>. Heiko suggested that Linux might be powering off the GPU. To save power, Linux turns off <em>unused</em> clocks and power domains. If Linux doesn’t know a clock or power domain is used by the GPU, it’ll turn off the GPU inadvertently.</p>
<p>For debugging, we can disable this mechanism by setting the <code>clk_ignore_unused pd_ignore_unused</code> kernel arguments. Doing so makes our userspace tests work.</p>
<p>Sometimes the simplest solutions are in front of us.</p>
<p>What is the root cause? MediaTek has a complicated hierarchy of clocks and power domains, and we missed some in our device tree. We’ll need to update our code to teach Linux about the extra clocks and power domains to fix the issue properly.</p>
<p>Nevertheless, we can now test our driver on real hardware. It’s a rough start: the first job we submit returns a Data Invalid Fault. Experimenting, it seems Valhall requires greater pointer alignment of its data structures than Bifrost did. Increasing the alignment at which we allocate fixes the faults, and decreasing again lets us determine the minimum required alignment. This information is accessible once we can run code on the hardware, but inaccessible when studying hardware in vitro. Reverse-engineering and driver development are better together.</p>
<h2 id="success-at-last">Success at last</h2>
<p>With these fixes, we finally see our first passing test, running on real hardware, with data structures prepared by our open source Mesa driver and shaders compiled by our Valhall compiler. Woo!</p>
<p>It only took me a few days after getting the hardware and a serial cable to pass hundreds of tests on the new architecture. Months of speculatively developing the driver came with a huge pay off.</p>
<p>Sounds like we’ll have Valhall drivers in time for end-users after all.</p>


</div></div>
  </body>
</html>
