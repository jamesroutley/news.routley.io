<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zilliz.com/learn/approximate-nearest-neighbor-oh-yeah-ANNOY">Original</a>
    <h1>Approximate Nearest Neighbor Oh Yeah (Annoy)</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Welcome back to <a href="https://zilliz.com/blog?tag=39&amp;page=1">Vector Database 101</a>.</p>
<p>In the previous tutorial, we deep-dived into <a href="https://zilliz.com/blog/hierarchical-navigable-small-worlds-HNSW">Hierarchical Navigable Small Worlds (HNSW)</a>. HNSW is a graph-based indexing algorithm that today is one of the most popular indexing strategies used in <a href="https://zilliz.com/learn/what-is-vector-database">vector databases</a>.</p>
<p>In this <a href="https://codelabs.milvus.io/">tutorial</a>, we&#39;ll switch gears and talk about tree-based vector indexes. Specifically, we&#39;ll talk about <strong>Approximate Nearest Neighbor Oh Yeah (Annoy)</strong>, an algorithm that uses a forest of trees to conduct the nearest neighbor search. For those familiar with random forests or gradient-boosted decision trees, Annoy can seem like a natural extension of these algorithms, only for the nearest neighbor search rather than machine learning. As with our HNSW tutorial, we&#39;ll first walk through how Annoy works from a high level before developing our own simple Python implementation.</p>
<p><iframe src="https://player.vimeo.com/video/847199990?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="ANNOY Vector Index | Vector Database Fundamentals"></iframe></p>

<p>While HNSW is built upon the connected graph and skip list, Annoy uses binary search trees as the core data structure. The key idea behind Annoy (and other tree-based indexes) is to repeatedly partition our vector space and search only a subset of the partitions for nearest neighbors. If this sounds like IVF, you&#39;re right; the idea is the same, but the execution is slightly different.</p>
<p><img src="https://raw.github.com/spotify/annoy/master/ann.png"/>
</p>
<p><sub>Annoy, visualized (from https://github.com/spotify/annoy).</sub></p>
<p>The best way to understand Annoy is to visualize how a single tree is built. However, remember that high-dimensional hyperspaces are very different from 2D/3D Euclidean spaces from an intuitive perspective, so the images below are only for reference.</p>
<p>Let&#39;s start with indexing. For Annoy, this is a recursive process where the maximum size of the call stack is the depth of the tree. In the first iteration, two random dataset vectors, <strong>a</strong> and <strong>b</strong>, are selected, and the full hyperspace is split along a hyperplane equidistant from both <strong>a</strong> and <strong>b</strong>. Then, vectors in the &#34;left&#34; half of the hyperspace get assigned to the left half of the tree, while vectors in the &#34;right&#34; half of the subspace are given to the right half of the tree. Note that this can be done without actually computing the hyperplane itself - for every dataset vector, we need to determine whether <strong>a</strong> (left) or <strong>b</strong> (right) is closer.</p>
<p>
      <span>
        <a href="https://assets.zilliz.com/After_the_first_second_and_Nth_iteration_respectively_9386181d4e.png" title="" target="_blank"><img src="https://assets.zilliz.com/After_the_first_second_and_Nth_iteration_respectively_9386181d4e.png" alt="" id=""/></a>
        <span></span>
      </span>
    </p>
<center>
<p>After the first, second, and Nth iteration, respectively. <a href="https://sds-aau.github.io/M3Port19/portfolio/ann/">Source</a>.</p>
</center>
<p>The second iteration repeats this process for both the left and right subtrees generated by the first iteration, resulting in a tree with a depth of two and four leaf nodes. This process continues for the third, fourth, and subsequent iterations until a leaf node has fewer than a pre-defined number of elements K. In the <a href="https://github.com/spotify/annoy/blob/master/src/annoylib.h#L892">original Annoy implementation</a>, <code>K</code> is a variable value that the user can set.</p>
<p>With the index fully built, we can now move on to querying. Given some query vector <strong>q</strong>, we can search by traversing the tree. A hyperplane splits each intermediate node, and we can determine which side of the hyperplane the query vector falls on by computing its distance to the left and right vectors. We repeat this process until we reach a leaf node containing an array of, at most, <code>K</code> vectors. We can then rank and return these vectors to the user.</p>
<p>Now we know how Annoy works, and let&#39;s start with the implementation. As usual, we&#39;ll first create a dataset of (128 dimensional) vectors:</p>
<pre><code><span>&gt;&gt;&gt; </span><span>import</span> numpy <span>as</span> np
<span>&gt;&gt;&gt; </span>dataset = np.random.normal(size=(<span>1000</span>, <span>128</span>))
</code></pre>
<p>Let&#39;s first define a <code>Node</code> class containing left and right subtrees:</p>
<pre><code><span>class</span> <span>Node</span>(<span>object</span>):
    <span>&#34;&#34;&#34;Initialize with a set of vectors, then call `split()`.
    &#34;&#34;&#34;</span>

    <span>def</span> <span>__init__</span>(<span>self, ref: np.ndarray, vecs: <span>List</span>[np.ndarray]</span>):
        self._ref = ref
        self._vecs = vecs
        self._left = <span>None</span>
        self._right = <span>None</span>

<span>    @property</span>
    <span>def</span> <span>ref</span>(<span>self</span>) -&gt; <span>Optional</span>[np.ndarray]:
        <span>&#34;&#34;&#34;Reference point in n-d hyperspace. Evaluates to `False` if root node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._ref

<span>    @property</span>
    <span>def</span> <span>vecs</span>(<span>self</span>) -&gt; <span>List</span>[np.ndarray]:
        <span>&#34;&#34;&#34;Vectors for this leaf node. Evaluates to `False` if not a leaf.
        &#34;&#34;&#34;</span>
        <span>return</span> self._vecs

<span>    @property</span>
    <span>def</span> <span>left</span>(<span>self</span>) -&gt; <span>Optional</span>[<span>object</span>]:
        <span>&#34;&#34;&#34;Left node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._left

<span>    @property</span>
    <span>def</span> <span>right</span>(<span>self</span>) -&gt; <span>Optional</span>[<span>object</span>]:
        <span>&#34;&#34;&#34;Right node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._right
</code></pre>
<p>The <code>vecs</code> variable contains a list of all vectors within the node. If the length of this list is less than some value <code>K</code>, then they will remain as-is; otherwise, these vectors will then get propagated to <code>left</code> and <code>right</code>, with <code>vecs[0]</code> and <code>vecs[1]</code> remaining as the two randomly selected vectors used to split the hyperplane.</p>
<p>Let&#39;s now move to indexing. First, Recall that every node in the tree is split by a hyperplane orthogonal to the line connecting two randomly selected dataset vectors. Conveniently, we can determine which side of the hyperplane a query vector lies on by computing distance. As usual, we&#39;ll use NumPy&#39;s vectorized math for this:</p>
<pre><code><span>def</span> <span>_is_query_in_left_half</span>(<span>q, node</span>):
   
   dist_l = np.linalg.norm(q - node.vecs[<span>0</span>])
   dist_r = np.linalg.norm(q - node.vecs[<span>1</span>])
   <span>return</span> dist_l &lt; dist_r
</code></pre>
<p>Now let&#39;s move to building the actual tree.</p>
<pre><code><span>import</span> random


<span>def</span> <span>split_node</span>(<span>node, K: <span>int</span>, imb: <span>float</span></span>) -&gt; <span>bool</span>:

    
    <span>if</span> <span>len</span>(node._vecs) &lt;= K:
        <span>return</span> <span>False</span>

    
    <span>for</span> n <span>in</span> <span>range</span>(<span>5</span>):
        left_vecs = []
        right_vecs = []

        
        left_ref = node._vecs.pop(np.random.randint(<span>len</span>(node._vecs)))
        right_ref = node._vecs.pop(np.random.randint(<span>len</span>(node._vecs)))

        
        <span>for</span> vec <span>in</span> node._vecs:
            dist_l = np.linalg.norm(vec - left_ref)
            dist_r = np.linalg.norm(vec - right_ref)
            <span>if</span> dist_l &lt; dist_r:
                left_vecs.append(vec)
            <span>else</span>:
                right_vecs.append(vec)

        
        r = <span>len</span>(left_vecs) / <span>len</span>(node._vecs)
        <span>if</span> r &lt; imb <span>and</span> r &gt; (<span>1</span> - imb):
            node._left = Node(left_ref, left_vecs)
            node._right = Node(right_ref, right_vecs)
            <span>return</span> <span>True</span>

        
        node._vecs.append(left_ref)
        node._vecs.append(right_ref)

    <span>return</span> <span>False</span>


<span>def</span> <span>_build_tree</span>(<span>node, K: <span>int</span>, imb: <span>float</span></span>):
    <span>&#34;&#34;&#34;Recurses on left and right halves to build a tree.
    &#34;&#34;&#34;</span>
    node.split(K=K, imb=imb)
    <span>if</span> node.left:
        _build_tree(node.left, K=K, imb=imb)
    <span>if</span> node.right:
        _build_tree(node.right, K=K, imb=imb)


<span>def</span> <span>build_forest</span>(<span>vecs: <span>List</span>[np.ndarray], N: <span>int</span> = <span>32</span>, K: <span>int</span> = <span>64</span>, imb: <span>float</span> = <span>0.95</span></span>) -&gt; <span>List</span>[Node]:
    <span>&#34;&#34;&#34;Builds a forest of `N` trees.
    &#34;&#34;&#34;</span>
    forest = []
    <span>for</span> _ <span>in</span> <span>range</span>(N):
        root = Node(<span>None</span>, vecs)
        _build_tree(root, K, imb)
        forest.append(root)
    <span>return</span> forest
</code></pre>
<p>This is a denser code block, so let&#39;s walk through it step-by-step. First, given an already-initialized <code>Node</code>, we randomly select two vectors and split the dataset into left and right halves. We then use the function we defined earlier to determine which of the two halves the subvectors belong to. Note that we&#39;ve added an <code>imb</code> parameter to maintain tree balance - if one side of the tree contains more than 95% of all the subvectors, we redo the split process.</p>
<p>With node splitting in place, the <code>build_tree</code> function will recursively call itself on all nodes. Leaf nodes are defined as those which contain fewer than <code>K</code> subvectors.</p>
<p>Great, so we&#39;ve built a binary tree that lets us significantly reduce the scope of our search. Now let&#39;s implement querying as well. Querying is pretty straightforward; we traverse the tree, continuously moving along the left or right branches until we&#39;ve arrived at the one we&#39;re interested in:</p>
<pre><code><span>def</span> <span>_query_linear</span>(<span>vecs: <span>List</span>[np.ndarray], q: np.ndarray, k: <span>int</span></span>) -&gt; <span>List</span>[np.ndarray]:
    <span>return</span> <span>sorted</span>(vecs, key=<span>lambda</span> v: np.linalg.norm(q-v))[:k]


<span>def</span> <span>query_tree</span>(<span>root: Node, q: np.ndarray, k: <span>int</span></span>) -&gt; <span>List</span>[np.ndarray]:
    <span>&#34;&#34;&#34;Queries a single tree.
    &#34;&#34;&#34;</span>

    <span>while</span> root.left <span>and</span> root.right:
        dist_l = np.linalg.norm(q - node.left.ref)
        dist_r = np.linalg.norm(q - node.right.ref)
        root = root.left <span>if</span> dist_l &lt; dist_r <span>else</span> root.right

    
    <span>return</span> _query_linear(root.vecs, q, k)
</code></pre>
<p>This chunk of code will greedily traverse the tree, returning a single nearest neighbor (<code>nq = 1</code>). However, recall that we&#39;re often interested in finding multiple nearest neighbors. Additionally, multiple nearest neighbors can live in other leaf nodes as well. So how can we solve these issues?</p>
<p>(Yes, I do realize that the main character&#39;s name is spelled &#34;Forrest&#34; in the <a href="https://en.wikipedia.org/wiki/Forrest_Gump">American classic</a>.)</p>
<p>In a <a href="https://zilliz.com/blog/vector-index">previous tutorial on IVF</a>, recall that we often expanded our search beyond the Voronoi cell closest to the query vector. The reason is due to <em>cell edges</em> - if a query vector is close to a cell edge, it&#39;s very likely that some of its nearest neighbors may be in a neighboring cell. These &#34;edges&#34; are much more common in high-dimensional spaces, so a large-ish value of <code>nprobe</code> is often used when a high recall is needed.</p>
<p>We face the same problem for tree-based indexes - some of our nearest neighbors may be outside the nearest leaf node/polygon. Annoy solves this by 1) allowing searches on both sides of a split and 2) creating a <em>forest</em> of trees.</p>
<p>Let&#39;s first expand on our implementation in the previous section to search both sides of a split:</p>
<pre><code><span>def</span> <span>_select_nearby</span>(<span>node: Node, q: np.ndarray, thresh: <span>int</span> = <span>0</span></span>):
    <span>&#34;&#34;&#34;Functions identically to _is_query_in_left_half, but can return both.
    &#34;&#34;&#34;</span>
    <span>if</span> <span>not</span> node.left <span>or</span> <span>not</span> node.right:
        <span>return</span> ()
    dist_l = np.linalg.norm(q - node.left.ref)
    dist_r = np.linalg.norm(q - node.right.ref)
    <span>if</span> np.<span>abs</span>(dist_l - dist_r) &lt; thresh:
        <span>return</span> (node.left, node.right)
    <span>if</span> dist_l &lt; dist_r:
        <span>return</span> (node.left,)
    <span>return</span> (node.right,)


<span>def</span> <span>_query_tree</span>(<span>root: Node, q: np.ndarray, k: <span>int</span></span>) -&gt; <span>List</span>[np.ndarray]:
    <span>&#34;&#34;&#34;This replaces the `query_tree` function above.
    &#34;&#34;&#34;</span>

    pq = [root]
    nns = []
    <span>while</span> pq:
        node = pq.pop(<span>0</span>)
        nearby = _select_nearby(node, q, thresh=<span>0.05</span>)

        
        <span>if</span> nearby:
            pq.extend(nearby)
        <span>else</span>:
            nns.extend(node.vecs)

    
    <span>return</span> _query_linear(nns, q, k)


<span>def</span> <span>query_forest</span>(<span>forest: <span>List</span>[Node], q, k: <span>int</span> = <span>10</span></span>):
    nns = <span>set</span>()
    <span>for</span> root <span>in</span> forest:
        
        res = _query_tree(root, q, k)
        nns.update(res)
    <span>return</span> _query_linear(nns, q, k)
</code></pre>
<p>Next, we&#39;ll add a function to allow us to build the full index as a forest of trees:</p>
<pre><code><span>def</span> <span>build_forest</span>(<span>vecs: <span>List</span>[np.ndarray], N: <span>int</span> = <span>32</span>, K: <span>int</span> = <span>64</span>, imb: <span>float</span> = <span>0.95</span></span>) -&gt; <span>List</span>[Node]:
    <span>&#34;&#34;&#34;Builds a forest of `N` trees.
    &#34;&#34;&#34;</span>
    forest = []
    <span>for</span> _ <span>in</span> <span>range</span>(N):
        root = Node(<span>None</span>, vecs)
        _build_tree(root, K, imb)
        forest.append(root)
    <span>return</span> forest
</code></pre>
<p>With everything implemented, let&#39;s now put it all together, as we&#39;ve done for IVF, SQ, PQ, and HNSW:</p>
<pre><code><span>from</span> typing <span>import</span> <span>List</span>, <span>Optional</span>
<span>import</span> random

<span>import</span> numpy <span>as</span> np


<span>class</span> <span>Node</span>(<span>object</span>):
    <span>&#34;&#34;&#34;Initialize with a set of vectors, then call `split()`.
    &#34;&#34;&#34;</span>

    <span>def</span> <span>__init__</span>(<span>self, ref: np.ndarray, vecs: <span>List</span>[np.ndarray]</span>):
        self._ref = ref
        self._vecs = vecs
        self._left = <span>None</span>
        self._right = <span>None</span>

<span>    @property</span>
    <span>def</span> <span>ref</span>(<span>self</span>) -&gt; <span>Optional</span>[np.ndarray]:
        <span>&#34;&#34;&#34;Reference point in n-d hyperspace. Evaluates to `False` if root node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._ref

<span>    @property</span>
    <span>def</span> <span>vecs</span>(<span>self</span>) -&gt; <span>List</span>[np.ndarray]:
        <span>&#34;&#34;&#34;Vectors for this leaf node. Evaluates to `False` if not a leaf.
        &#34;&#34;&#34;</span>
        <span>return</span> self._vecs

<span>    @property</span>
    <span>def</span> <span>left</span>(<span>self</span>) -&gt; <span>Optional</span>[<span>object</span>]:
        <span>&#34;&#34;&#34;Left node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._left

<span>    @property</span>
    <span>def</span> <span>right</span>(<span>self</span>) -&gt; <span>Optional</span>[<span>object</span>]:
        <span>&#34;&#34;&#34;Right node.
        &#34;&#34;&#34;</span>
        <span>return</span> self._right

    <span>def</span> <span>split</span>(<span>self, K: <span>int</span>, imb: <span>float</span></span>) -&gt; <span>bool</span>:

        
        <span>if</span> <span>len</span>(self._vecs) &lt;= K:
            <span>return</span> <span>False</span>

        
        <span>for</span> n <span>in</span> <span>range</span>(<span>5</span>):
            left_vecs = []
            right_vecs = []

            
            left_ref = self._vecs.pop(np.random.randint(<span>len</span>(self._vecs)))
            right_ref = self._vecs.pop(np.random.randint(<span>len</span>(self._vecs)))

            
            <span>for</span> vec <span>in</span> self._vecs:
                dist_l = np.linalg.norm(vec - left_ref)
                dist_r = np.linalg.norm(vec - right_ref)
                <span>if</span> dist_l &lt; dist_r:
                    left_vecs.append(vec)
                <span>else</span>:
                    right_vecs.append(vec)

            
            r = <span>len</span>(left_vecs) / <span>len</span>(self._vecs)
            <span>if</span> r &lt; imb <span>and</span> r &gt; (<span>1</span> - imb):
                self._left = Node(left_ref, left_vecs)
                self._right = Node(right_ref, right_vecs)
                <span>return</span> <span>True</span>

            
            self._vecs.append(left_ref)
            self._vecs.append(right_ref)

        <span>return</span> <span>False</span>


<span>def</span> <span>_select_nearby</span>(<span>node: Node, q: np.ndarray, thresh: <span>int</span> = <span>0</span></span>):
    <span>&#34;&#34;&#34;Functions identically to _is_query_in_left_half, but can return both.
    &#34;&#34;&#34;</span>
    <span>if</span> <span>not</span> node.left <span>or</span> <span>not</span> node.right:
        <span>return</span> ()
    dist_l = np.linalg.norm(q - node.left.ref)
    dist_r = np.linalg.norm(q - node.right.ref)
    <span>if</span> np.<span>abs</span>(dist_l - dist_r) &lt; thresh:
        <span>return</span> (node.left, node.right)
    <span>if</span> dist_l &lt; dist_r:
        <span>return</span> (node.left,)
    <span>return</span> (node.right,)


<span>def</span> <span>_build_tree</span>(<span>node, K: <span>int</span>, imb: <span>float</span></span>):
    <span>&#34;&#34;&#34;Recurses on left and right halves to build a tree.
    &#34;&#34;&#34;</span>
    node.split(K=K, imb=imb)
    <span>if</span> node.left:
        _build_tree(node.left, K=K, imb=imb)
    <span>if</span> node.right:
        _build_tree(node.right, K=K, imb=imb)


<span>def</span> <span>build_forest</span>(<span>vecs: <span>List</span>[np.ndarray], N: <span>int</span> = <span>32</span>, K: <span>int</span> = <span>64</span>, imb: <span>float</span> = <span>0.95</span></span>) -&gt; <span>List</span>[Node]:
    <span>&#34;&#34;&#34;Builds a forest of `N` trees.
    &#34;&#34;&#34;</span>
    forest = []
    <span>for</span> _ <span>in</span> <span>range</span>(N):
        root = Node(<span>None</span>, vecs)
        _build_tree(root, K, imb)
        forest.append(root)
    <span>return</span> forest


<span>def</span> <span>_query_linear</span>(<span>vecs: <span>List</span>[np.ndarray], q: np.ndarray, k: <span>int</span></span>) -&gt; <span>List</span>[np.ndarray]:
    <span>return</span> <span>sorted</span>(vecs, key=<span>lambda</span> v: np.linalg.norm(q-v))[:k]


<span>def</span> <span>_query_tree</span>(<span>root: Node, q: np.ndarray, k: <span>int</span></span>) -&gt; <span>List</span>[np.ndarray]:
    <span>&#34;&#34;&#34;Queries a single tree.
    &#34;&#34;&#34;</span>

    pq = [root]
    nns = []
    <span>while</span> pq:
        node = pq.pop(<span>0</span>)
        nearby = _select_nearby(node, q, thresh=<span>0.05</span>)

        
        <span>if</span> nearby:
            pq.extend(nearby)
        <span>else</span>:
            nns.extend(node.vecs)

    
    <span>return</span> _query_linear(nns, q, k)


<span>def</span> <span>query_forest</span>(<span>forest: <span>List</span>[Node], q, k: <span>int</span> = <span>10</span></span>):
    nns = <span>set</span>()
    <span>for</span> root <span>in</span> forest:
        
        res = _query_tree(root, q, k)
        nns.update(res)
    <span>return</span> _query_linear(nns, q, k)
</code></pre>
<p>And that&#39;s it for Annoy!</p>
<p>In this tutorial, we did a deep dive into Annoy, a tree-based indexing strategy with a playful name. There are better languages than Python for implementing vector search data structures due to interpreter overhead. Still, we use as much numpy-based array math. We can do many optimizations to prevent copying memory back and forth, but I&#39;ll leave them as an exercise for the reader.</p>
<p>In the following tutorial, we&#39;ll continue our deep dive into indexing strategies with a rundown of the Vamana algorithm - also known more commonly as <em>DiskANN</em> - a unique graph-based indexing algorithm that is tailored specifically towards querying directly from solid state hard drives.</p>
<p>All code for this tutorial is freely available on <a href="https://github.com/fzliu/vector-search">my Github</a>.</p>
<ol>
<li><a href="https://zilliz.com/blog/introduction-to-unstructured-data">Introduction to Unstructured Data</a></li>
<li><a href="https://zilliz.com/learn/what-is-vector-database">What is a Vector Database?</a></li>
<li><a href="https://zilliz.com/learn/comparing-vector-database-vector-search-library-and-vector-search-plugin">Comparing Vector Databases, Vector Search Libraries, and Vector Search Plugins</a></li>
<li><a href="https://zilliz.com/blog/introduction-to-milvus-vector-database">Introduction to Milvus</a></li>
<li><a href="https://zilliz.com/blog/milvus-vector-database-quickstart">Milvus Quickstart</a></li>
<li><a href="https://zilliz.com/blog/vector-similarity-search">Introduction to Vector Similarity Search</a></li>
<li><a href="https://zilliz.com/blog/vector-index">Vector Index Basics and the Inverted File Index</a></li>
<li><a href="https://zilliz.com/blog/scalar-quantization-and-product-quantization">Scalar Quantization and Product Quantization</a></li>
<li><a href="https://zilliz.com/blog/hierarchical-navigable-small-worlds-HNSW">Hierarchical Navigable Small Worlds (HNSW)</a></li>
<li><a href="https://zilliz.com/learn/approximate-nearest-neighbor-oh-yeah-ANNOY">Approximate Nearest Neighbor Oh Yeah (ANNOY)</a></li>
<li><a href="https://thesequence.substack.com/p/guest-post-choosing-the-right-vector">Choosing the Right Vector Index for Your Project</a></li>
<li><a href="https://zilliz.com/learn/DiskANN-and-the-Vamana-Algorithm">DiskANN and the Vamana Algorithm</a></li>
</ol>
<hr/>
</div></div></div>
  </body>
</html>
