<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.evalapply.org/posts/shell-aint-a-bad-place-to-fp-part-1-doug-mcilroys-pipeline/">Original</a>
    <h1>Shell ain&#39;t a bad place to FP: part 1/N</h1>
    
    <div id="readability-page-1" class="page"><div><div>
<p>Or, <em><strong>the one in which we “take apart” Douglas McIlroy’s pipeline from 1986.</strong></em></p>
<p>Doing so teaches an object lesson about the essence of modular, composable,
functional architecture. And things start to really heat up when it dawns on
us, how a good set of standard parts can be used to express totally different
ideas <em>just by composing them in different ways</em>.</p>
<blockquote>
<p>“Designing is <em>fundamentally</em> about taking things apart. It’s about taking
things apart <em>in such a way</em> that they can be put back together. i.e.
Separating into things that can be composed.”</p>
<ul>
<li>Rich Hickey, “<a href="https://www.youtube.com/watch?v=QCwqnjxqfmY">Design, Composition, and Performance</a>”, 2013</li>
</ul>
<div><pre tabindex="0"><code data-lang="shell">tr -cs A-Za-z <span>&#39;\n&#39;</span> | tr A-Z a-z | sort | uniq -c | sort -rn | sed 10q
</code></pre></div><ul>
<li>Douglas McIlroy, <a href="https://dl.acm.org/doi/10.1145/5948.315654">Communications of the ACM</a>, 1986</li>
</ul>
</blockquote>
<ul>
<li><a href="#the-pipeline-that-douglas-built">The Pipeline that Douglas Built</a></li>
<li><a href="#take-apart-semantics-idioms-functions">Take Apart! Semantics/Idioms -&gt; Functions</a></li>
<li><a href="#play-semantics-functions-ooh-what-if-i-dot-dot-dot">Play! Semantics -&gt; Functions -&gt; “Ooh, what if I…&#34;</a></li>
<li><a href="#compose-again-semantics-functions-play-grand-new-pipeline">Compose Again! Semantics -&gt; Functions -&gt; Play -&gt; Grand New Pipeline</a></li>
<li><a href="#addendum-remarkable-aspects-of-doug-s-o-dot-g-dot-pipeline">Addendum: Remarkable aspects of Doug’s O.G. pipeline</a></li>
</ul>
<p>Previously: <a href="https://evalapply.org/posts/shell-aint-a-bad-place-to-fp-part-0-intro/">Shell ain’t a bad place to FP: part 0/N</a></p>
<hr/>
<h2 id="the-pipeline-that-douglas-built">The Pipeline that Douglas Built<a href="#the-pipeline-that-douglas-built" arialabel="Anchor">⌗</a> </h2>
<p>Douglas McIlroy famously (infamously?) wrote the following in reply to a
problem posed by Jon Bentley for his column <em>“Programming pearls: a literate
program”</em> (<em>Communications of the ACM</em> magazine, June 1986, Vol. 29, No. 6).</p>
<p>I first heard of it some years ago in <a href="https://leancrew.com/all-this/2011/12/more-shell-less-egg/">More Shell Less Egg</a>, and saw it again
in the book <a href="https://www.oreilly.com/library/view/classic-shell-scripting/0596005954/">Classic Shell Scripting</a> (which taught me much of my shell-fu).
The original was not online then. Now I see the ACM has kindly <a href="https://dl.acm.org/doi/10.1145/5948.315654">published it</a>
along with the rest of their archives!</p>
<p>Here it is, lightly paraphrased:</p>
<div><pre tabindex="0"><code data-lang="shell"><span># Problem statement (word frequency):</span>
<span>#</span>
<span># - Read a file of text</span>
<span># - Determine the n most frequently-used words</span>
<span># - Print out a sorted list of all the words, along with their frequencies</span>

<span># Douglas McIlroy&#39;s answer</span>

<span># 1. Transliterate complement (-c) of words into newlines,</span>
<span># squeezing out (-s) duplicates</span>
tr -cs A-Za-z <span>&#39;\n&#39;</span> |
    <span># 2. Transliterate uppercase to lowercase</span>
    tr A-Z a-z |
    <span># 3. Sort to bring identical words together</span>
    sort |
    <span># 4. Replace each run of duplicate words with</span>
    <span># a single representative, and include a count</span>
    uniq -c |
    <span># 5. Sort reverse (-r), numeric (-n)</span>
    sort -rn |
    <span># 6. Pass through stream editor; quit after printing the</span>
    <span># the first 10 lines received</span>
    sed 10q
</code></pre></div><p>Here I am, punching the Bash manual page through it…</p>
<div><pre tabindex="0"><code data-lang="shell">man bash |
    tr -cs A-Za-z <span>&#39;\n&#39;</span> | tr A-Z a-z |
    sort | uniq -c | sort -rn |
    sed 10q
</code></pre></div><p>… and here are the top 10 words by frequency.</p>
<div><pre tabindex="0"><code data-lang="text">4200 the
1822 is
1251 to
1221 a
1147 of
 869 if
 805 and
 570 shell
 570 in
 563 command
</code></pre></div><p>“<em>Coolcoolcoolcool nodoubt nodoubt… So, uh… that’s it?</em>”</p>
<h2 id="take-apart-semantics-idioms-functions">Take Apart! Semantics/Idioms -&gt; Functions<a href="#take-apart-semantics-idioms-functions" arialabel="Anchor">⌗</a> </h2>
<p>It’s worth observing that the <em>same</em> tools composed in <em>different</em> ways express
<em>totally different</em> concepts. <code>sort</code> just sorts. <code>uniq</code> just returns uniques.
<em>But</em> <code>sort | uniq</code> is an idiom for <em>set of things</em>. Whereas <code>sort | uniq -c | sort -rn</code> is an idiom for <em>frequency distribution</em>.</p>
<p>Now…</p>
<p>What if we use Bash functions to name the idioms we see in McIlroy’s pipeline?</p>
<div><pre tabindex="0"><code data-lang="shell">flatten_paragraphs<span>()</span> <span>{</span>
    <span># English-only for easy explanation, but can be more general</span>
    tr -cs A-Za-z <span>&#39;\n&#39;</span>
<span>}</span>

tokenise_lowercase<span>()</span> <span>{</span>
    <span># Transliterate uppercase to lowercase</span>
    tr A-Z a-z
<span>}</span>

frequencies<span>()</span> <span>{</span>
    <span># Produce frequency distribution of input</span>
    sort | uniq -c | sort -rn
<span>}</span>

take_n<span>()</span> <span>{</span>
    <span># Given a number n, return those many lines of input</span>
    <span># or 10 lines by default, if n is not specified.</span>
    sed <span>${</span>1<span>:-</span>10<span>}</span>q
<span>}</span>
</code></pre></div><p>And what if we update the pipeline with <em>function calls</em> like this?</p>
<div><pre tabindex="0"><code data-lang="shell">man bash |
    flatten_paragraphs |
    tokenise_lowercase |
    frequencies |
    take_n <span>10</span>
</code></pre></div><p>Yes, we get the same result!</p>
<div><pre tabindex="0"><code data-lang="text">4200 the
1822 is
1251 to
1221 a
1147 of
 869 if
 805 and
 570 shell
 570 in
 563 command
</code></pre></div><p>Yes, yes, <em><strong>YES</strong></em>! Functions + pipes = mind blown!</p>
<h2 id="play-semantics-functions-ooh-what-if-i-dot-dot-dot">Play! Semantics -&gt; Functions -&gt; “Ooh, what if I…”<a href="#play-semantics-functions-ooh-what-if-i-dot-dot-dot" arialabel="Anchor">⌗</a> </h2>
<p>Now that we lifted out a couple of text processing functions, we can try to
make <em>more</em> text processing functions. Here are some examples.</p>
<div><pre tabindex="0"><code data-lang="shell">sort_dictionary<span>()</span> <span>{</span>
    sort -b -d -k2
<span>}</span>

sort_rhyme<span>()</span> <span>{</span>
    rev | sort -b -d | rev
<span>}</span>

<span># eliminate stop-words</span>
drop_stopwords<span>()</span> <span>{</span>
    local stopwords<span>=</span><span>${</span>1<span>:-</span><span>&#34;the,is,to,a,of,if,and,in,or,be,by,not,with,for,when,it&#34;</span><span>}</span>
    local grep_pattern<span>=</span><span>$(</span>tr , <span>&#39;\|&#39;</span> <span>&lt;&lt;&lt;</span><span>&#34;</span><span>${</span>stopwords<span>}</span><span>&#34;</span><span>)</span>
    grep -v -E <span>${</span>grep_pattern<span>}</span>
<span>}</span>

<span># n-grams</span>

butlast_n<span>()</span> <span>{</span>
    <span># utility for picking appropriate collection of n-grams</span>
    head -n -<span>${</span>1<span>:-</span>0<span>}</span>
<span>}</span>

bigram<span>()</span> <span>{</span>
    <span># we need intermediate state, but we can make it stream,</span>
    <span># instead of accumulating in temp files</span>
    mkfifo bigram_buffer

    tee &gt;<span>(</span>tail +2 &gt; bigram_buffer<span>)</span> |
        paste - bigram_buffer |
        <span># take all but the last entry as it is not a bigram</span>
        butlast_n <span>1</span>

    rm bigram_buffer
<span>}</span>

trigram<span>()</span> <span>{</span>
    <span># we need intermediate state, but we can make it stream,</span>
    <span># instead of accumulating in temp files</span>
    mkfifo trigram_buffer_one trigram_buffer_two

    tee &gt;<span>(</span>tail +2 &gt; trigram_buffer_one<span>)</span> |
        tee &gt;<span>(</span>tail +3 &gt; trigram_buffer_two<span>)</span> |
        paste - trigram_buffer_one trigram_buffer_two |
        <span># take all but the last 2 entries as they are not trigrams</span>
        butlast_n <span>2</span>

    rm trigram_buffer_one trigram_buffer_two
<span>}</span>
</code></pre></div><p>Clearly there is a lot to explore about functions and pipelines and other
techniques in this code. We will do deep dives in upcoming posts. For now
just know that Bash functions…</p>
<ul>
<li>name a group of shell statements,</li>
<li>can be composed with pipes</li>
<li>thus intermix with regular shell tools, and</li>
<li>can help create domain-specific abstractions out of domain-agnostic ones.</li>
</ul>
<p>But before we go there, indulge me and my <em>Oh, and One More Thing (TM)</em> …</p>
<h2 id="compose-again-semantics-functions-play-grand-new-pipeline">Compose Again! Semantics -&gt; Functions -&gt; Play -&gt; Grand New Pipeline<a href="#compose-again-semantics-functions-play-grand-new-pipeline" arialabel="Anchor">⌗</a> </h2>
<p>What’s the point of making a text processing library of functions if we don’t
process any text?</p>
<p>Well…</p>
<ul>
<li>Start a new shell session.</li>
<li>Copy-paste all the Bash functions above into it.</li>
<li>Then copy-paste this pipeline and</li>
<li>Hit Enter!</li>
</ul>
<div><pre tabindex="0"><code data-lang="shell"><span># I assume you have Bash version 4+.</span>
man bash |
    <span># pre-process</span>
    flatten_paragraphs |
    tokenise_lowercase |
    drop_stopwords |
    <span># cache raw pre-processed data, if we need to re-analyse later</span>
    tee /tmp/bash_manpage_raw_tokens.txt |
    <span># cache various views or compressions of the raw data</span>
    tee &gt;<span>(</span>sort_dictionary | uniq &gt; /tmp/bash_manpage_sorted_as_dictionary.txt<span>)</span> |
    tee &gt;<span>(</span>sort_rhyme | uniq &gt; /tmp/bash_manpage_sorted_as_rhyme.txt<span>)</span> |
    <span># accumulate various analyses of the OG raw data</span>
    tee &gt;<span>(</span>frequencies &gt; /tmp/bash_manpage_token_freqs.txt<span>)</span> |
    tee &gt;<span>(</span>bigram | frequencies &gt; /tmp/bash_manpage_bigram_freqs.txt<span>)</span> |
    tee &gt;<span>(</span>trigram | frequencies &gt; /tmp/bash_manpage_trigram_freqs.txt<span>)</span> |
    take_n
</code></pre></div><p>And why not experiment?!</p>
<p>Reorder it! Remove parts of it! Change parts of it! Give it 10 GiB of input!</p>
<p>Play and learn!!!</p>
<p>(#protip: The shell can auto-complete functions. Type <em>flat</em> and hit <em>TAB</em>,
and you should get a completion for <em>flatten_paragraphs</em>.)</p>

<p>The UNIX tools philosophy is clearly at work. <code>sort</code> just sorts, <code>uniq</code> just
returns uniques, pipes connect parts. Ho hum.</p>
<p>The things I <em>do</em> find remarkable are:</p>
<ul>
<li>
<p>Now the year is 2022, i.e. McIlroy wrote the program about 4 <em>decades</em> ago.
It continues to edify, meaning the ideas it contains have a timeless quality.</p>
</li>
<li>
<p><em>It also works as-is</em>, on my cheap Thinkpad running a GNU Linux (Ubuntu),
even though the original code was written for a UNIX that might live only
in a museum today (or maybe in your bank). Odds look good that come 2036, it
will continue to still work as-is on mainstream boxen.</p>
</li>
<li>
<p>It is plain text, and so eminently portable. (I slapped it into the org-mode
file of this blog post, evaluated it via org-babel, and captured the results
inline. How? Because Emacs org-babel can simply “shell out”; i.e. make a
standard request to a standard shell to evaluate the program and have the
shell process return any result in a standard way.)</p>
</li>
<li>
<p>I bet it runs <em>way</em> faster now because my box is a supercomputer v/s the
UNIX boxen of that era.</p>
</li>
<li>
<p>Pipes remove the burden of explicit state handling. Oh, also, Douglas McIlroy
invented UNIX pipes.</p>
</li>
<li>
<p>The entire composition is itself a function.</p>
</li>
<li>
<p><code>map</code> (tokenise), <code>filter</code> (uniquify), <code>reduce</code> (frequency distribution),
and early termination (<code>take</code> first 10) are <em>automatic</em>, needing no special
machinery.</p>
</li>
<li>
<p>It is an abstract computation that is independent of data source/sink. We
can hook into any I/O combination of sockets, or fifo pipes, or files on
disk without modifying the pipeline code—much like Clojure transducers
or monadic I/O in Haskell land.</p>
</li>
<li>
<p><em>Most importantly</em>, a rank amateur like me could figure out each part <em>and</em>
the whole in one sitting. It is eminently doable because:</p>
<ul>
<li>each sub part is understandable in isolation <em>and</em></li>
<li>the whole is amenable to incremental as well as large-scale adaptation,</li>
<li>in playful, interactive, low-risk ways.</li>
</ul>
</li>
</ul>
<p>I was clueless then and had to dig through manpages and flail about at the
command line. It took me a while to grok the function of each tool and how
it is applied to the text processing problem.</p>
<p>If you haven’t already, I’d say bear that small cost, because it teaches a
priceless lesson in modular, composable, functional architecture.</p>
<p>Plus, why not step up one’s shell-fu?</p>
<hr/>
<p>Next up: Part 2/N: Deep-dive into bash functions and function design techniques</p>
<ul>
<li>Using functions to craft one’s own Bytes-sized UNIX tools</li>
<li>Using them interactively like regular UNIX tools</li>
<li>maybe more…</li>
</ul>
<p>The ol&#39; noodle is noodlin&#39; over it. Stay tooned!</p>
</div></div></div>
  </body>
</html>
