<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ashbyhq.com/blog/engineering/detecting-event-loop-blockers">Original</a>
    <h1>Detecting Node Event Loop Blockers</h1>
    
    <div id="readability-page-1" class="page"><div><div id="blog-content"><div><p>Node.js is a fantastic piece of technology, and we use it as the foundation for our application servers.</p><p>There is, however, one big caveat when working with Node: our application code runs (<a href="https://nodejs.org/en/docs/guides/dont-block-the-event-loop/" rel="noopener" target="_blank">for the most part</a>) in a single thread (the Event Loop thread). Running in a single thread is fine when the application spends most of its time doing asynchronous I/O since each request consumes only a small amount of CPU. However, when your requests perform CPU-intensive computation, all concurrent requests will be blocked waiting for it to complete!</p><p>As the Ashby backend grew in size, we ran into some CPU-intensive code paths, either due to non-optimized code that didn&#39;t scale very well or the lack of result set limits (which could cause an endpoint to return an arbitrarily large amount of data).</p><p>Join us as we take you through our journey to identify and eliminate requests that block with the help of community NPM packages and our favorite monitoring tool, Datadog.</p><h2>The Node Event Loop: a Primer</h2><p>This section is a brief introduction to the Node Event Loop, meant only to provide the necessary context for the rest of this post. If you want to dive deeper into this topic, we recommend <a href="https://nodejs.org/en/docs/guides/dont-block-the-event-loop/" target="_blank">Node&#39;s excellent documentation on the topic</a>.</p><p>At a very high level, the Event Loop consists of a single thread running in a continuous loop, going through multiple phases, as described below:</p><p><img src="https://www.ashbyhq.com/images/event_loop_blockers/event_loop_phases.png" alt="Node Event Loop Phases" title="Node Event Loop phases"/><span>Node Event Loop phases</span></p><p>Each phase contains a callback queue of all callbacks that need to be processed in that phase. These include various tasks, such as processing timers (scheduled by <code>setTimeout</code> and <code>setInterval</code>), processing I/O events, and executing I/O callbacks (including the code that handles incoming HTTP requests).</p><p>All callbacks in the Event Loop are processed sequentially. Suppose a specific callback performs a CPU-intensive operation without yielding the Event Loop. In that case, all other callbacks will remain in the queue and not be processed until the Event Loop frees. We call these &#34;Event Loop Blockers.&#34;</p><h2>Detecting Event Loop Blockers</h2><p>When we set out to look for solutions to detect Event Loop Blockers, we found two great libraries in npmjs.com: <a href="https://github.com/tj/node-blocked" rel="noopener" target="_blank">blocked</a> and <a href="https://github.com/naugtur/blocked-at" rel="noopener" target="_blank">blocked-at</a>. Both provide a similar interface: a threshold value and a callback that gets invoked when the Event Loop gets blocked for longer than the threshold.</p><p>One critical difference between the two, which stems from the fact that their implementations are radically different, is that <code>blocked-at</code> provides the callback with the stack trace for the Event Loop Blocker&#39;s entry point, whereas <code>blocked</code> only provides the Event Loop Blocker&#39;s duration. This extra information came at a cost: the performance hit of using <code>blocked-at</code> is significant, as indicated in their README:</p><blockquote><p>It&#39;s recommended to detect blocking exists with something without the performance overhead and use <code>blocked-at</code> in testing environment to pinpoint where the slowdown happens. Rule of thumb is you should not be running this in production unless desperate.</p></blockquote><p>Let&#39;s look at how each of these libraries work and why the cost of capturing the stack trace is so high.</p><h3><code>blocked</code></h3><p>The mechanism behind <code>blocked</code> is ingeniously simple (the whole library comes in at just <a href="https://github.com/tj/node-blocked/blob/master/index.js" rel="noopener" target="_blank">518 bytes</a>). When installed, it registers a timer (using
<code>setInterval</code>) to keep track of the time elapsed between invocations. The added delay between invocations can be a good indicator of an Event Loop Blocker since concurrent operations that block the Event Loop may delay timers from being fired. On every execution of the timer, if the drift between the desired and actual invocation times is higher than the configured threshold, the callback is invoked.</p><p>Since Event Loop Blocker detection happens in a different context from the running application (within the timer), it cannot gather any information about the source of the Event Loop Blocker. On the other hand, the work on each iteration is so fast that its overhead is minimal.</p><h3><code>blocked-at</code></h3><p>The <code>blocked-at</code> library uses <a href="https://nodejs.org/api/async_hooks.html" rel="noopener" target="_blank">Async Hooks</a> to capture the additional
information about the stack trace. Node recently introduced the Async Hooks API to track asynchronous resources (such as Promises) within a Node process. A developer can register callbacks on asynchronous resource creation (<code>init</code>), when its callback is invoked (<code>before</code>), and when its callback finishes (<code>after</code>).</p><p>The <code>init</code> hook is called within the context where the resource is created, so we can capture the <a href="https://nodejs.org/api/errors.html#errorcapturestacktracetargetobject-constructoropt" rel="noopener" target="_blank">current stack trace</a>. <code>blocked-at</code> does precisely that and stores the trace in a global map, indexed by the current <code>triggerAsyncId</code> (the mechanism Async Hooks use to identify asynchronous resources). Then, once the resource&#39;s callback is ready to run, the <code>before</code> hook is invoked, capturing the invocation&#39;s timestamp (and once again stored in the global
map where the trace is already stored). Once the callback finishes, the <code>after</code> hook is invoked, and the current timestamp is compared against the previously stored timestamp from the <code>before</code> invocation. If the difference is greater than the configured threshold, the callback is called, passing along the stack trace captured in <code>init</code>.</p><p>This approach, while providing valuable information, is too slow to be feasible, due to the following reasons:</p><ul><li>Async Hooks run on <em>all</em> asynchronous resources and may cause significant overhead. Recent versions of
Node have improved this overhead considerably.</li><li>We can only capture the stack trace in the <code>init</code> hook, so before we know whether we need to invoke the callback, we waste a considerable amount of resources. Capturing a stack trace is a relatively expensive operation, and our code will discard it most of the time (when the Event Loop is not blocked).</li></ul><h2>Our First Attempt</h2><p>In our first attempt at finding our Event Loop Blockers, we wanted to get as much information as possible, so we started by measuring the performance impact of <code>blocked-at</code>.</p><p>After deploying it to our staging environment, we noticed that the response times for our most common GraphQL queries were a few orders of magnitude slower. Our test traffic was enough to consume all the available CPU resources in our staging instances! We also realized that the provided stack trace information wasn&#39;t particularly useful. In most cases, it pointed to the Apollo Field Resolver, which is responsible for invoking our code, instead of pointing to the code itself.</p><p>With this in mind, we circled back and replaced <code>blocked-at</code> with <code>blocked</code>. Even without the stack trace, we would at least be able to identify when the Event Loop was blocked and, with some manual investigation, figure out where the Event Loop Blocker occurred in our code.</p><p>With <code>blocked</code> successfully deployed in production, with minimal impact to performance or resource usage (we noticed approximately 0.5% increased resource usage with the timer running every 100ms), we were, as expected, able to identify critical code paths that were contributing to the blocking issues. However, each event took nearly 5 minutes to investigate, and with hundreds of blocking events per hour, this quickly proved unsustainable.</p><h2>Datadog Integration</h2><p>We use multiple Datadog products in our monitoring stack, particularly <a href="https://www.datadoghq.com/product/apm/" rel="noopener" target="_blank">Datadog APM</a> and <a href="https://www.datadoghq.com/product/log-management/" rel="noopener" target="_blank">Datadog Log Management</a>. When using these two products, we can <a href="https://docs.datadoghq.com/tracing/connect_logs_and_traces/" rel="noopener" target="_blank">connect Logs and Traces</a> and view all log lines generated in the context of a specific HTTP Request Trace in a single interface. In our short-lived experiment with <code>blocked-at</code>, we noticed that the logs correctly connected with the trace for the request that generated them.</p><p>After some investigation, we noticed that the Async Hook used by <code>blocked-at</code> allowed us to access the reference to the
&#34;current trace&#34; (as Datadog was also using Async Hooks to propagate its context). What if
instead of capturing the stack trace, we used the Async Hook to surface the blocker in the Datadog Trace?</p><p>We started by writing our own Async Hook with heavy inspiration from <code>blocked-at</code>. Our first version did little more
than detect when a blocking event happened:</p><pre><code><span>const</span><span> cache = </span><span>new</span><span> </span><span>Map</span><span>&lt;</span><span>number</span><span>, [</span><span>number</span><span>, </span><span>number</span><span>]&gt;();
</span>
<span></span><span>function</span><span> </span><span>before</span><span>(</span><span>asyncId: </span><span>number</span><span>) </span><span>{
</span>  cache.set(asyncId, process.hrtime());
}

<span></span><span>function</span><span> </span><span>after</span><span>(</span><span>asyncId: </span><span>number</span><span>) </span><span>{
</span><span>  </span><span>const</span><span> cached = cache.get(asyncId);
</span><span>  </span><span>if</span><span> (cached == </span><span>null</span><span>) {
</span><span>    </span><span>return</span><span>;
</span>  }
  cache.delete(asyncId);

<span>  </span><span>const</span><span> diff = process.hrtime(cached);
</span><span>  </span><span>const</span><span> diffNs = diff[</span><span>0</span><span>] * </span><span>1e9</span><span> + diff[</span><span>1</span><span>];
</span><span>  </span><span>if</span><span> (diffNs &gt; thresholdNs) {
</span><span>    </span><span>const</span><span> time = diffNs / </span><span>1e6</span><span>;
</span>    logger.warn({
<span>      </span><span>label</span><span>: </span><span>&#34;EventLoopMonitor&#34;</span><span>,
</span><span>      </span><span>message</span><span>: </span><span>`Event loop was blocked for </span><span>${time}</span><span>ms`</span><span>,
</span><span>      </span><span>metadata</span><span>: {
</span>        time,
      },
    });
  }
}

<span></span><span>const</span><span> asyncHook = createHook({ before, after });
</span>
asyncHook.enable();
</code></pre><p>In the <code>before</code> hook, we are capturing the current timestamp (side note: we are using the deprecated <code>process.hrtime()</code> method, instead of <code>process.hrtime.bigint()</code>, as the latter has a considerable performance impact in Node 16), and storing it in the cache, indexed by the current Execution Async ID (like <code>blocked-at</code> does).</p><p>Then, in the <code>after</code> hook, we fetch the initial timestamp from the cache and compare it against the current timestamp. If the diff exceeds the configured threshold, we log a warning message (like in <code>blocked-at</code>, which is associated with the current Datadog Trace).</p><p>The final piece of the puzzle is surfacing the block in the trace itself, which we achieved by adding the following to the code path where the block is detected:</p><pre><code><span>const</span><span> span = requestTracer.startSpan(</span><span>&#34;EventLoopBlock&#34;</span><span>, {
</span><span>  </span><span>childOf</span><span>: requestTracer.scope().active() ?? </span><span>undefined</span><span>,
</span><span>  </span><span>startTime</span><span>: </span><span>new</span><span> </span><span>Date</span><span>().getTime() - time,
</span><span>  </span><span>tags</span><span>: {
</span><span>    [tags.SERVICE_NAME]: </span><span>&#34;ashby-backend-event-loop-monitor&#34;</span><span>,
</span>  },
});
span.finish();
</code></pre><p>Here, we manually create a trace span named <code>EventLoopBlock</code>, which we create as a child of the &#34;current trace.&#34; We also set the start time to be the current timestamp minus the total block time (the use of <code>new Date().getTime()</code> means we lose sub-millisecond precision, but this is acceptable for our use case). Doing this causes the span to be
rendered as part of our trace:</p><p><img src="https://www.ashbyhq.com/images/event_loop_blockers/trace.png" alt="Trace with Event Loop Blockers" title="Trace with Event Loop Blockers"/><span>Trace with Event Loop Blockers</span></p><p>This approach meant that the total amount of work done in the &#34;good&#34; case (where no blockers are detected) is much
smaller than in <code>blocked-at</code>: two API calls to fetch the current timestamp, two cache writes, and one cache read. The
performance impact of this approach was much more acceptable to us, ranging between 1% to 2% additional CPU usage (and a comparable increase in total response time) when deployed in production.</p><h2>Results</h2><p>Since we deployed our new Event Loop Monitor, we have been able to identify most of the sources of Event Loop Blocking,
and have both reduced our total number of Blocker events by and p99 latency of said events by up to 80%!</p><p><img src="https://www.ashbyhq.com/images/event_loop_blockers/events.png" alt="Event Loop Blocker events over time" title="Event Loop Blocker events over time"/><span>Event Loop Blocker events over time</span>
<img src="https://www.ashbyhq.com/images/event_loop_blockers/latency.png" alt="Event Loop Blocker p99 latency over time" title="Event Loop Blocker p99 latency over time."/><span>Event Loop Blocker p99 latency over time.</span></p><h2>References</h2><ul><li><a href="https://nodejs.org/en/docs/guides/dont-block-the-event-loop/" rel="noopener" target="_blank">https://nodejs.org/en/docs/guides/dont-block-the-event-loop/</a></li><li><a href="https://nodejs.org/en/docs/guides/timers-in-node/" rel="noopener" target="_blank">https://nodejs.org/en/docs/guides/timers-in-node/</a></li><li><a href="https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/" rel="noopener" target="_blank">https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/</a></li><li><a href="https://nodejs.org/docs/latest-v16.x/api/process.html#processhrtimetime" rel="noopener" target="_blank">https://nodejs.org/docs/latest-v16.x/api/process.html#processhrtimetime</a></li></ul></div></div></div></div>
  </body>
</html>
