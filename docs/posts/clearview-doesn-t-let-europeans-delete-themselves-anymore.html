<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/kashhill/status/1715048994223567281">Original</a>
    <h1>Clearview doesn&#39;t let Europeans delete themselves anymore</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/gelatinous-cube.png" title="gelatinous-cube" data-thumbnail="gelatinous-cube.png" data-sub-html="&lt;h2&gt;Gelatinous Cube&lt;/h2&gt;&lt;p&gt;gelatinous-cube&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="gelatinous-cube.png" data-srcset="gelatinous-cube.png, gelatinous-cube.png 1.5x, gelatinous-cube.png 2x" data-sizes="auto" alt="gelatinous-cube.png"/>
    </a><figcaption>Gelatinous Cube</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Get a feel for how unsupervised clustering algorithms work and their differences.</p>
<h2 id="unsupervised-clustering">Unsupervised Clustering</h2>
<p>A set of algorithms used to identify groups within unlabeled dataset. If we go back to the word embeddings examples, running a clustering algorithm would return groups of words with similar meanings, or sharing a common topic (e.g. [king, queen, prince, princess], [apple, lemon, banana, coconut]).</p>
<p>Lets run through a few popular clustering algorithms.</p>
<h2 id="k-means">K-means</h2>
<p>Chose <code>k</code> random points from the dataset as centroids of our k clusters. Greedily assign each point in the dataset to the closest centroid’s cluster. Assign the new centroids to the mean of each cluster and repeat until we reach an equilibrium.</p>
<p>There is no guarantee for this solution to be any good, we selected the initial centoids at random. So the idea is to repeat the process a bunch of time, and stick with the solution that minimize sum of the clusters variance.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>kmeans</span><span>(</span><span>data</span><span>,</span> <span>n_clusters</span><span>=</span><span>n_clusters</span><span>,</span> <span>n_iters</span><span>=</span><span>10</span><span>):</span>
</span></span><span><span>    <span>num_samples</span> <span>=</span> <span>data</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>f</span><span>():</span>
</span></span><span><span>        <span>old_cluster</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>num_samples</span><span>)</span>
</span></span><span><span>        <span>centroids</span> <span>=</span> <span>data</span><span>[</span><span>torch</span><span>.</span><span>randperm</span><span>(</span><span>num_samples</span><span>)[:</span><span>n_clusters</span><span>],</span> <span>:]</span>
</span></span><span><span>        <span>while</span> <span>True</span><span>:</span> <span># repeat until stable</span>
</span></span><span><span>            <span>distances</span> <span>=</span> <span>torch</span><span>.</span><span>cdist</span><span>(</span><span>data</span><span>,</span> <span>centroids</span><span>)</span>
</span></span><span><span>            <span>clusters</span> <span>=</span> <span>distances</span><span>.</span><span>argmin</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>old_cluster</span><span>.</span><span>equal</span><span>(</span><span>clusters</span><span>):</span> <span>return</span> <span>clusters</span>
</span></span><span><span>            <span>old_cluster</span> <span>=</span> <span>clusters</span>
</span></span><span><span>            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_clusters</span><span>):</span> <span># can we vectorize that?</span>
</span></span><span><span>                <span>centroids</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>data</span><span>[</span><span>clusters</span> <span>==</span> <span>i</span><span>,</span> <span>:]</span><span>.</span><span>mean</span><span>(</span><span>dim</span><span>=</span><span>0</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>variance</span><span>(</span><span>clusters</span><span>):</span>
</span></span><span><span>        <span>return</span> <span>sum</span><span>([</span><span>data</span><span>[</span><span>clusters</span> <span>==</span> <span>i</span><span>,</span> <span>:]</span><span>.</span><span>var</span><span>(</span><span>dim</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sum</span><span>()</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_clusters</span><span>)])</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>min</span><span>([</span><span>f</span><span>()</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>n_iters</span><span>)],</span> <span>key</span><span>=</span><span>variance</span><span>)</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/kmeans.png" title="kmeans" data-thumbnail="kmeans.png" data-sub-html="&lt;h2&gt;K-means clustering&lt;/h2&gt;&lt;p&gt;kmeans&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="kmeans.png" data-srcset="kmeans.png, kmeans.png 1.5x, kmeans.png 2x" data-sizes="auto" alt="kmeans.png"/>
    </a><figcaption>K-means clustering</figcaption>
    </figure>
<p>In this animation we can see the process happening under the hood over several generations or random starting points.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/kmeans.gif" title="kmeans" data-thumbnail="kmeans.gif" data-sub-html="&lt;h2&gt;K-means steps over 10 generations&lt;/h2&gt;&lt;p&gt;kmeans&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="kmeans.gif" data-srcset="kmeans.gif, kmeans.gif 1.5x, kmeans.gif 2x" data-sizes="auto" alt="kmeans.gif"/>
    </a><figcaption>K-means steps over 10 generations</figcaption>
    </figure>
<h2 id="dbscan">DBSCAN</h2>
<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that works better than k-means for nested clusters. It also does not require to chose how many clusters we expect in advance.</p>
<p>It works by classifying each points as <code>core point</code> if they have at least <code>min_points</code> within <code>epsilon</code> distance, and <code>non-core point</code> otherwise.</p>
<p>Then picking a random core point to become a cluster, and recursively glue neighboring core points to it, and (non-recursively) glue non-core points to it. Repeat until there’s no core point unasigned.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># make a crappy hashable version of tensor because float precision keep biting me</span>
</span></span><span><span><span>def</span> <span>h</span><span>(</span><span>t</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>f</span><span>&#39;</span><span>{</span><span>t</span><span>}</span><span>&#39;</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>dbscan</span><span>(</span><span>data</span><span>,</span> <span>eps</span><span>=</span><span>1</span><span>,</span> <span>min_points</span><span>=</span><span>4</span><span>):</span>
</span></span><span><span>    <span># classify points as core or non-core</span>
</span></span><span><span>    <span>core</span><span>,</span> <span>non_core</span> <span>=</span> <span>[],</span> <span>set</span><span>()</span>
</span></span><span><span>    <span>for</span> <span>p</span> <span>in</span> <span>data</span><span>:</span>
</span></span><span><span>        <span>if</span> <span>(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>data</span> <span>-</span> <span>p</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span> <span>&lt;</span> <span>eps</span><span>)</span><span>.</span><span>sum</span><span>()</span> <span>&gt;=</span> <span>min_points</span><span>:</span>
</span></span><span><span>            <span>core</span><span>.</span><span>append</span><span>(</span><span>p</span><span>)</span>
</span></span><span><span>        <span>else</span><span>:</span>
</span></span><span><span>            <span>non_core</span><span>.</span><span>add</span><span>(</span><span>p</span><span>)</span>
</span></span><span><span>    <span># build clusters</span>
</span></span><span><span>    <span>seen</span> <span>=</span> <span>set</span><span>()</span>
</span></span><span><span>    <span>clusters</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>while</span> <span>core</span><span>:</span>
</span></span><span><span>        <span>p</span> <span>=</span> <span>core</span><span>.</span><span>pop</span><span>()</span>
</span></span><span><span>        <span>if</span> <span>h</span><span>(</span><span>p</span><span>)</span> <span>in</span> <span>seen</span><span>:</span> <span>continue</span>
</span></span><span><span>        <span>clusters</span><span>.</span><span>append</span><span>([])</span>
</span></span><span><span>        <span>q</span> <span>=</span> <span>[</span><span>p</span><span>]</span>
</span></span><span><span>        <span>while</span> <span>q</span><span>:</span>
</span></span><span><span>            <span>p</span> <span>=</span> <span>q</span><span>.</span><span>pop</span><span>()</span>
</span></span><span><span>            <span>if</span> <span>h</span><span>(</span><span>p</span><span>)</span> <span>in</span> <span>seen</span><span>:</span> <span>continue</span>
</span></span><span><span>            <span>seen</span><span>.</span><span>add</span><span>(</span><span>h</span><span>(</span><span>p</span><span>))</span>
</span></span><span><span>            <span>clusters</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>append</span><span>(</span><span>p</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>p</span> <span>in</span> <span>non_core</span><span>:</span> <span>continue</span>
</span></span><span><span>            <span>neighbors</span> <span>=</span> <span>data</span><span>[(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>data</span> <span>-</span> <span>p</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span> <span>&lt;</span> <span>eps</span><span>)</span><span>.</span><span>nonzero</span><span>()</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)]</span>
</span></span><span><span>            <span>q</span><span>.</span><span>extend</span><span>(</span><span>neighbors</span><span>)</span>
</span></span><span><span>        <span>clusters</span><span>[</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>clusters</span><span>[</span><span>-</span><span>1</span><span>])</span>
</span></span><span><span>    <span>return</span> <span>clusters</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/dbscan.png" title="dbscan" data-thumbnail="dbscan.png" data-sub-html="&lt;h2&gt;DBSCAN&lt;/h2&gt;&lt;p&gt;dbscan&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="dbscan.png" data-srcset="dbscan.png, dbscan.png 1.5x, dbscan.png 2x" data-sizes="auto" alt="dbscan.png"/>
    </a><figcaption>DBSCAN</figcaption>
    </figure>
<h3 id="hyperparameters-heuristics">Hyperparameters heuristics</h3>
<p><code>epsilon</code>: One common approach is to use the k-distance graph to determine an appropriate value for epsilon. The idea is to plot the distance to the kth nearest neighbor for each point in the dataset and look for a knee or elbow point in the graph. This knee point can be a good estimate for epsilon.</p>
<p><code>min_points</code>: A common rule of thumb is to set min_points to the dimensionality of your dataset plus one (<code>min_points ≥ D + 1</code>), where D is the number of features. However, you may need to adjust this based on the density and distribution of your data.</p>
<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>
<p>This one sounds like a case study for union-find with path compression. Compute all distance pairs, merge the two closest points into a cluster. Rince and repeat.</p>
<p>There’s 3 main options to compare a point to a cluster:</p>
<ul>
<li>centroid: take the average of the cluster</li>
<li>single-linkage: the closest point in each cluster (this seems like the most computationaly efficient to me)</li>
<li>complete-linkage: the furthest point in each cluster (and this one the most inneficient)</li>
</ul>
<p>There’s also options to compute the distance e.g.:</p>
<ul>
<li>Euclidean</li>
<li>Manathan</li>
<li>Cosine</li>
</ul>
<p>I wanted to flex my tensor muscle and try to write a vectorized version of the code, but note that it is a bad idea, and union-find would have a much better complexity.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># WARNING: this is horribly slow, the complexity is terrible, and it should use union-find with path compression.</span>
</span></span><span><span><span>@torch.no_grad</span><span>()</span>
</span></span><span><span><span>def</span> <span>hierarchical_clustering</span><span>(</span><span>data</span><span>,</span> <span>n_clusters</span><span>=</span><span>6</span><span>):</span>
</span></span><span><span>    <span>num_samples</span> <span>=</span> <span>data</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
</span></span><span><span>    <span>distances</span> <span>=</span> <span>torch</span><span>.</span><span>cdist</span><span>(</span><span>data</span><span>,</span> <span>data</span><span>)</span> <span># pairwise distances</span>
</span></span><span><span>    <span>mask</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>distances</span><span>)</span><span>.</span><span>fill_diagonal_</span><span>(</span><span>math</span><span>.</span><span>inf</span><span>)</span> <span># mask the diagonal (we don&#39;t care about distance to self)</span>
</span></span><span><span>    <span>clusters</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>num_samples</span><span>)</span> <span># everything is its own cluster</span>
</span></span><span><span>
</span></span><span><span>    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>num_samples</span> <span>-</span> <span>n_clusters</span><span>):</span>
</span></span><span><span>        <span>am</span> <span>=</span> <span>(</span><span>distances</span> <span>+</span> <span>mask</span><span>)</span><span>.</span><span>argmin</span><span>()</span> <span># find the shortest pair of clusters</span>
</span></span><span><span>        <span>i</span><span>,</span> <span>j</span> <span>=</span> <span>am</span> <span>//</span> <span>num_samples</span><span>,</span> <span>am</span> <span>%</span> <span>num_samples</span> <span># emulate a divmod here</span>
</span></span><span><span>
</span></span><span><span>        <span># mask the merged cluster i</span>
</span></span><span><span>        <span>mask</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>math</span><span>.</span><span>inf</span>
</span></span><span><span>        <span>mask</span><span>[:,</span> <span>i</span><span>]</span> <span>=</span> <span>math</span><span>.</span><span>inf</span>
</span></span><span><span>
</span></span><span><span>        <span># update the distances, by merging cluster i into j</span>
</span></span><span><span>        <span># - torch.min() for single-linkage clustering</span>
</span></span><span><span>        <span># - torch.max() for complete-linkage clustering</span>
</span></span><span><span>        <span>distances</span><span>[</span><span>j</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>min</span><span>(</span><span>distances</span><span>[</span><span>i</span><span>],</span> <span>distances</span><span>[</span><span>j</span><span>])</span>
</span></span><span><span>
</span></span><span><span>        <span># everything that was in cluster i, is now in cluster j</span>
</span></span><span><span>        <span>clusters</span> <span>=</span> <span>torch</span><span>.</span><span>where</span><span>(</span><span>clusters</span> <span>==</span> <span>clusters</span><span>[</span><span>i</span><span>],</span> <span>clusters</span><span>[</span><span>j</span><span>],</span> <span>clusters</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>clusters</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/hierarchical-clustering.png" title="hierarchical-clustering" data-thumbnail="hierarchical-clustering.png" data-sub-html="&lt;h2&gt;Hierarchical clustering&lt;/h2&gt;&lt;p&gt;hierarchical-clustering&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="hierarchical-clustering.png" data-srcset="hierarchical-clustering.png, hierarchical-clustering.png 1.5x, hierarchical-clustering.png 2x" data-sizes="auto" alt="hierarchical-clustering.png"/>
    </a><figcaption>Hierarchical clustering</figcaption>
    </figure>
<p>Step by step it looks like:</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/hierarchical-clustering.gif" title="hierarchical-clustering" data-thumbnail="hierarchical-clustering.gif" data-sub-html="&lt;h2&gt;Hierarchical clustering in action&lt;/h2&gt;&lt;p&gt;hierarchical-clustering&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="hierarchical-clustering.gif" data-srcset="hierarchical-clustering.gif, hierarchical-clustering.gif 1.5x, hierarchical-clustering.gif 2x" data-sizes="auto" alt="hierarchical-clustering.gif"/>
    </a><figcaption>Hierarchical clustering in action</figcaption>
    </figure>
<h2 id="meanshift">Meanshift</h2>
<p>Meanshift like DBSCAN does not require to provide the number of clusters (<code>k</code>) in advance.</p>
<p>It has an after taste of physics gravity simulation. For each point compute a new position as the weighted average of the distances to every other point. With further points having less influence on the average.</p>
<p>The distance decay uses a Gaussian kernel $e^{-\frac{||x - x’||^2}{2\sigma^2}} \over {x’\sqrt{2\sigma^2\pi}}$</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>gaussian</span><span>(</span><span>d</span><span>,</span> <span>bw</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>torch</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>0.5</span><span>*</span><span>((</span><span>d</span><span>/</span><span>bw</span><span>))</span><span>**</span><span>2</span><span>)</span> <span>/</span> <span>(</span><span>bw</span><span>*</span><span>math</span><span>.</span><span>sqrt</span><span>(</span><span>2</span><span>*</span><span>math</span><span>.</span><span>pi</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>one_update</span><span>(</span><span>X</span><span>):</span>
</span></span><span><span>    <span>for</span> <span>i</span><span>,</span> <span>x</span> <span>in</span> <span>enumerate</span><span>(</span><span>X</span><span>):</span>
</span></span><span><span>        <span>dist</span> <span>=</span> <span>torch</span><span>.</span><span>norm</span><span>(</span><span>X</span> <span>-</span> <span>x</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>weights</span> <span>=</span> <span>gaussian</span><span>(</span><span>dist</span><span>,</span> <span>2.5</span><span>)</span>
</span></span><span><span>        <span>X</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>(</span><span>X</span> <span>*</span> <span>weights</span><span>.</span><span>unsqueeze</span><span>(</span><span>1</span><span>))</span><span>.</span><span>sum</span><span>(</span><span>dim</span><span>=</span><span>0</span><span>)</span> <span>/</span> <span>weights</span><span>.</span><span>sum</span><span>()</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>meanshift</span><span>(</span><span>data</span><span>,</span> <span>n_iter</span><span>=</span><span>5</span><span>):</span>
</span></span><span><span>    <span>X</span> <span>=</span> <span>data</span><span>.</span><span>clone</span><span>()</span>
</span></span><span><span>    <span>for</span> <span>it</span> <span>in</span> <span>range</span><span>(</span><span>n_iter</span><span>):</span>
</span></span><span><span>        <span>one_update</span><span>(</span><span>X</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>X</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/meanshift.png" title="meanshift" data-thumbnail="meanshift.png" data-sub-html="&lt;h2&gt;Meanshift clustering&lt;/h2&gt;&lt;p&gt;meanshift&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="meanshift.png" data-srcset="meanshift.png, meanshift.png 1.5x, meanshift.png 2x" data-sizes="auto" alt="meanshift.png"/>
    </a><figcaption>Meanshift clustering</figcaption>
    </figure>
<p>Animating the points of each clusters zero-ing in on the centroids.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/unsupervised-clustering/meanshift.gif" title="meanshift" data-thumbnail="meanshift.gif" data-sub-html="&lt;h2&gt;Meanshift in action&lt;/h2&gt;&lt;p&gt;meanshift&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="meanshift.gif" data-srcset="meanshift.gif, meanshift.gif 1.5x, meanshift.gif 2x" data-sizes="auto" alt="meanshift.gif"/>
    </a><figcaption>Meanshift in action</figcaption>
    </figure>
<h2 id="the-code">The code</h2>
<p>You can get the code at:</p>
<ul>
<li><a href="https://github.com/peluche/ml-misc/blob/master/kmeans.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ml-misc/blob/master/kmeans.ipynb</a></li>
<li><a href="https://github.com/peluche/ml-misc/blob/master/dbscan.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ml-misc/blob/master/dbscan.ipynb</a></li>
<li><a href="https://github.com/peluche/ml-misc/blob/master/hierarchical_clustering.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ml-misc/blob/master/hierarchical_clustering.ipynb</a></li>
<li><a href="https://github.com/peluche/ml-misc/blob/master/meanshift.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ml-misc/blob/master/meanshift.ipynb</a></li>
</ul>
</div></div>
  </body>
</html>
