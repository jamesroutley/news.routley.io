<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.vectorware.com/blog/async-await-on-gpu/">Original</a>
    <h1>Async/Await on the GPU</h1>
    
    <div id="readability-page-1" class="page"><article><header><a href="https://www.vectorware.com/"><img alt="VectorWare logo" loading="lazy" width="200" height="200" decoding="async" data-nimg="1" srcset="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=256&amp;q=75&amp;dpl=dpl_5ARrLfKhyaduwauTB3AsEu1HNyqi 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=640&amp;q=75&amp;dpl=dpl_5ARrLfKhyaduwauTB3AsEu1HNyqi 2x" src="https://www.vectorware.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=640&amp;q=75&amp;dpl=dpl_5ARrLfKhyaduwauTB3AsEu1HNyqi"/><span>VectorWare</span></a><div><p><a href="https://www.vectorware.com/blog/">Dispatches</a></p><div><p><time datetime="2026-02-17" aria-label="February 17, 2026">February 17, 2026</time><span aria-hidden="true">Â·</span><span>15<!-- --> min read</span></p><p><span>Pedantic mode:</span><span>Off</span></p></div></div><p>GPU code can now use Rust&#39;s async/await. We share the reasons why and what this unlocks for GPU programming.</p></header><section><div><p>At <a href="https://www.vectorware.com/">VectorWare</a>, we are building the first<!-- -->
<a href="https://www.vectorware.com/blog/announcing-vectorware/">GPU-native software company</a>. Today, we are excited to
announce that we can successfully use Rust&#39;s
<a href="https://doc.rust-lang.org/core/future/trait.Future.html"><code>Future</code></a> trait and
<code>async</code>/<code>await</code> on the GPU. This milestone marks a significant step towards our vision
of enabling developers to write complex, high-performance applications that leverage the
full power of GPU hardware using familiar Rust abstractions.</p>
<h2>Concurrent programming on the GPU</h2>
<p>GPU programming traditionally focuses on data parallelism. A developer writes a single
operation and the GPU runs that operation in parallel across different parts of the
data.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark github-light"><code data-language="rust" data-theme="github-dark github-light"><span data-line=""><span>fn</span><span> conceptual_gpu_kernel</span><span>(data) {</span></span>
<span data-line=""><span>    // All threads in all warps do the same thing to different parts of data</span></span>
<span data-line=""><span>    data[thread_id] </span><span>=</span><span> data[thread_id] </span><span>*</span><span> 2</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This model works well for standalone and uniform tasks such as graphics rendering,
matrix multiplication, and image processing.</p>
<p>As GPU programs grow more sophisticated, developers use <a href="https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf">warp
specialization</a> to introduce more complex
control flow and dynamic behavior. With warp specialization, different parts of the GPU
run different parts of the program concurrently.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark github-light"><code data-language="rust" data-theme="github-dark github-light"><span data-line=""><span>fn</span><span> conceptual_gpu_kernel</span><span>(data) {</span></span>
<span data-line=""><span>    let</span><span> communication </span><span>=</span><span> ...</span><span>;</span></span>
<span data-line=""><span>    if</span><span> warp </span><span>==</span><span> 0</span><span> {</span></span>
<span data-line=""><span>        // Have warp 0 load data from main memory</span></span>
<span data-line=""><span>        load</span><span>(data, communication);</span></span>
<span data-line=""><span>    } </span><span>else</span><span> if</span><span> warp </span><span>==</span><span> 1</span><span> {</span></span>
<span data-line=""><span>        // Have warp 1 compute A on loaded data and forward it to B</span></span>
<span data-line=""><span>        compute_A</span><span>(communication);</span></span>
<span data-line=""><span>    } </span><span>else</span><span> {</span></span>
<span data-line=""><span>        // Have warp 2 and 3 compute B on loaded data and store it</span></span>
<span data-line=""><span>        compute_B</span><span>(communication, data);</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Warp specialization shifts GPU logic from uniform data parallelism to explicit
task-based parallelism. This enables more sophisticated programs that make better use of
the hardware. For example, one warp can load data from memory while another performs
computations to improve utilization of both compute and memory.</p>
<p>This added expressiveness comes at a cost. Developers must manually manage concurrency
and synchronization because there is no language or runtime support for doing so.
Similar to threading and synchronization on the CPU, this is error-prone and difficult
to reason about.</p>
<h2>Better concurrent programming on the GPU</h2>
<p>There are many projects that aim to provide the benefits of warp specialization without
the pain of manual concurrency and synchronization.</p>
<p><a href="https://github.com/jax-ml/jax">JAX</a> models GPU programs as computation graphs that encode
dependencies between operations. The JAX compiler analyzes this graph to
determine ordering, parallelism, and placement before generating the program that
executes. This allows JAX to manage and optimize execution while presenting a high-level
programming model in a Python-based DSL. The same model supports multiple hardware
backends, including CPUs and TPUs, without changing user code.</p>
<p><a href="https://github.com/triton-lang/triton">Triton</a> expresses computation in terms of blocks
that execute independently on the GPU. Like JAX, Triton uses a Python-based DSL to
define how these blocks should execute. The Triton compiler lowers block definitions
through a <a href="https://pytorch.org/blog/triton-kernel-compilation-stages/">multi-level
pipeline</a> of <a href="https://triton-lang.org/main/dialects/dialects.html">MLIR
dialects</a>, where it applies
block-level data-flow analysis to manage and optimize the generated program.</p>
<p>More recently, NVIDIA introduced <a href="https://developer.nvidia.com/cuda/tile">CUDA Tile</a>.
Like Triton, CUDA Tile organizes computation around blocks. It additionally introduces
&#34;tiles&#34; as first-class units of data. Tiles make data dependencies explicit rather than
inferred, which improves both performance opportunities and reasoning about correctness.
CUDA Tile ingests code written in existing languages such as Python, lowers it to an
MLIR dialect called <a href="https://github.com/NVIDIA/cuda-tile">Tile IR</a>, and executes on the
GPU.</p>
<p>We are excited and inspired by these efforts, especially CUDA Tile. We think it is a
great idea to have GPU programs structured around explicit units of work and data,
separating the definition of concurrency from its execution. We believe that GPU
hardware aligns naturally with <a href="https://en.wikipedia.org/wiki/Structured_concurrency">structured
concurrency</a> and changing the
software to match will enable safer and more performant code.</p>
<h2>The downsides of current approaches</h2>
<p>These higher-level approaches to GPU programming require developers to structure code in
new and specific ways. This can make them a poor fit for some classes of applications.</p>
<p>Additionally, a new programming paradigm and ecosystem is a significant barrier to
adoption. Developers use JAX and Triton primarily for machine learning workloads where they
align well with the underlying computation. CUDA Tile is newer and more general but has
yet to see broader adoption. Virtually no one writes their entire application with these
technologies. Instead, they write parts of their application in these frameworks and
other parts in more traditional languages and models.</p>
<p>Code reuse is also limited. Existing CPU libraries assume a conventional language
runtime and execution model and cannot be reused directly. Existing GPU libraries rely
on manual concurrency management and similarly do not compose with these frameworks.</p>
<p>Ideally, we want an abstraction that captures the benefits of explicit and structured
concurrency without requiring a new language or ecosystem. It should compose with
existing CPU code and execution models. It should provide fine-grained control when
needed, similar to warp specialization. It should also provide ergonomic defaults for the
common case.</p>
<h2>Rust&#39;s <code>Future</code> trait and <code>async</code>/<code>await</code></h2>
<p>We believe Rust&#39;s <a href="https://doc.rust-lang.org/core/future/trait.Future.html"><code>Future</code></a>
trait and <code>async</code>/<code>await</code> provide such an abstraction. They encode structured
concurrency<!-- --> directly in an existing language without committing to a specific execution
model.</p>
<p>A future represents a computation that may not be complete yet. A future does not
specify whether it runs on a thread, a core, a block, a tile, or a warp. It does not
care about the hardware or operating system it runs on. The <a href="https://doc.rust-lang.org/core/future/trait.Future.html"><code>Future</code>
trait</a> itself is intentionally
minimal. Its core operation is
<a href="https://doc.rust-lang.org/core/future/trait.Future.html#tymethod.poll"><code>poll</code></a>, which
returns either
<a href="https://doc.rust-lang.org/core/task/enum.Poll.html#variant.Ready"><code>Ready</code></a> or
<a href="https://doc.rust-lang.org/core/task/enum.Poll.html#variant.Pending"><code>Pending</code></a>.
Everything else is layered on top. This separation is what allows the same async code to
be driven in different environments. For more detailed info, see the <a href="https://rust-lang.github.io/async-book/">Rust async
book</a>.</p>
<p>Like JAX&#39;s computation graphs, futures are deferred and composable.<!-- --> Developers construct programs as values before executing them.
This allows the compiler to analyze dependencies and composition ahead of execution
while preserving the shape of user code.</p>
<p>Like Triton&#39;s blocks, futures naturally express independent units of concurrency.
Depending on how futures are combined, they represent whether a block of work runs
serially or in parallel. Developers express concurrency using normal Rust control flow,
trait implementations, and future combinators rather than a separate DSL.</p>
<p>Like CUDA Tile&#39;s explicit tiles and data dependencies, Rust&#39;s ownership model makes data
constraints explicit in the program structure.<!-- --> Futures capture the data they operate on and that captured
state becomes part of the compiler-generated state machine. Ownership, borrowing,
<a href="https://doc.rust-lang.org/std/pin/struct.Pin.html"><code>Pin</code></a>, and bounds such as
<a href="https://doc.rust-lang.org/core/marker/trait.Send.html"><code>Send</code></a> and
<a href="https://doc.rust-lang.org/core/marker/trait.Sync.html"><code>Sync</code></a> encode how data can be
shared and transferred between concurrent units of work.</p>
<p>Warp specialization is not typically described this way, but in effect, it reduces to
manually written task state machines.<!-- -->
Futures compile down to state machines that the Rust compiler generates and manages
automatically.</p>
<p>Because Rust&#39;s futures are just compiler-generated state machines there is no reason
they cannot run on the GPU. That is exactly what we have done.</p>
<h2>A world first:<!-- --> <code>async</code>/<code>await</code> running on the GPU</h2>
<p>Running <code>async</code>/<code>await</code> on the GPU is difficult to demonstrate visually because the code
looks and runs like ordinary Rust. By design, the same syntax used on the CPU runs
unchanged on the GPU.</p>
<p>Here we define a small set of async functions and invoke them from a single GPU kernel
using <code>block_on</code>. Together, they exercise the core features of Rust&#39;s async model:
simple futures, chained futures, conditionals, multi-step workflows, async blocks, and
third-party combinators.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark github-light"><code data-language="rust" data-theme="github-dark github-light"><span data-line=""><span>// Simple async functions that we will call from the GPU kernel below.</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> fn</span><span> async_double</span><span>(x</span><span>:</span><span> i32</span><span>) </span><span>-&gt;</span><span> i32</span><span> {</span></span>
<span data-line=""><span>    x </span><span>*</span><span> 2</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> fn</span><span> async_add_then_double</span><span>(a</span><span>:</span><span> i32</span><span>, b</span><span>:</span><span> i32</span><span>) </span><span>-&gt;</span><span> i32</span><span> {</span></span>
<span data-line=""><span>    let</span><span> sum </span><span>=</span><span> a </span><span>+</span><span> b;</span></span>
<span data-line=""><span>    async_double</span><span>(sum)</span><span>.await</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> fn</span><span> async_conditional</span><span>(x</span><span>:</span><span> i32</span><span>, do_double</span><span>:</span><span> bool</span><span>) </span><span>-&gt;</span><span> i32</span><span> {</span></span>
<span data-line=""><span>    if</span><span> do_double {</span></span>
<span data-line=""><span>        async_double</span><span>(x)</span><span>.await</span></span>
<span data-line=""><span>    } </span><span>else</span><span> {</span></span>
<span data-line=""><span>        x</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> fn</span><span> async_multi_step</span><span>(x</span><span>:</span><span> i32</span><span>) </span><span>-&gt;</span><span> i32</span><span> {</span></span>
<span data-line=""><span>    let</span><span> step1 </span><span>=</span><span> async_double</span><span>(x)</span><span>.await</span><span>;</span></span>
<span data-line=""><span>    let</span><span> step2 </span><span>=</span><span> async_double</span><span>(step1)</span><span>.await</span><span>;</span></span>
<span data-line=""><span>    step2</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>#[</span><span>unsafe</span><span>(no_mangle)]</span></span>
<span data-line=""><span>pub</span><span> unsafe</span><span> extern</span><span> &#34;ptx-kernel&#34;</span><span> fn</span><span> demo_async</span><span>(</span></span>
<span data-line=""><span>    val</span><span>:</span><span> i32</span><span>,</span></span>
<span data-line=""><span>    flag</span><span>:</span><span> u8</span><span>,</span></span>
<span data-line=""><span>) {</span></span>
<span data-line=""><span>    // Basic async functions with a single await execute correctly on the device.</span></span>
<span data-line=""><span>    let</span><span> doubled </span><span>=</span><span> block_on</span><span>(</span><span>async_double</span><span>(val));</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // Chaining multiple async calls works as expected.</span></span>
<span data-line=""><span>    let</span><span> chained </span><span>=</span><span> block_on</span><span>(</span><span>async_add_then_double</span><span>(val, doubled));</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // Conditionals inside async code are supported.</span></span>
<span data-line=""><span>    let</span><span> conditional </span><span>=</span><span> block_on</span><span>(</span><span>async_conditional</span><span>(val, flag));</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // Async functions with multiple await points also work.</span></span>
<span data-line=""><span>    let</span><span> multi_step </span><span>=</span><span> block_on</span><span>(</span><span>async_multi_step</span><span>(val));</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // Async blocks work and compose naturally.</span></span>
<span data-line=""><span>    let</span><span> from_block </span><span>=</span><span> block_on</span><span>(</span><span>async</span><span> {</span></span>
<span data-line=""><span>        let</span><span> doubled_a </span><span>=</span><span> async_double</span><span>(val)</span><span>.await</span><span>;</span></span>
<span data-line=""><span>        let</span><span> doubled_b </span><span>=</span><span> async_double</span><span>(chained)</span><span>.await</span><span>;</span></span>
<span data-line=""><span>        doubled_a</span><span>.</span><span>wrapping_add</span><span>(doubled_b)</span></span>
<span data-line=""><span>    });</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // CPU-based async utilities also work. Here we use combinators from the</span></span>
<span data-line=""><span>    // `futures_util` crate to build and compose futures without writing new</span></span>
<span data-line=""><span>    // async functions.</span></span>
<span data-line=""><span>    use</span><span> futures_util</span><span>::</span><span>future</span><span>::</span><span>ready;</span></span>
<span data-line=""><span>    use</span><span> futures_util</span><span>::</span><span>FutureExt</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    let</span><span> from_combinator </span><span>=</span><span> block_on</span><span>(</span></span>
<span data-line=""><span>        ready</span><span>(val)</span><span>.</span><span>then</span><span>(</span><span>move</span><span> |</span><span>v</span><span>|</span><span> ready</span><span>(v</span><span>.</span><span>wrapping_mul</span><span>(</span><span>2</span><span>)</span><span>.</span><span>wrapping_add</span><span>(</span><span>100</span><span>)))</span></span>
<span data-line=""><span>    );</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Getting this all working required fixing bugs and closing gaps across multiple compiler
backends. We also encountered issues in NVIDIA&#39;s <code>ptxas</code> tool, which we reported and
worked around.</p>
<h2>Executors on the GPU</h2>
<p>Using <code>async</code>/<code>await</code> makes it ergonomic to express concurrency on the GPU. However, in
Rust futures do not execute themselves and must be driven to completion by an executor.
Rust deliberately does not include a built-in executor and instead third parties provide
executors with different features and tradeoffs.</p>
<p>Our initial goal was to prove that Rust&#39;s async model could run on the GPU at all. To do
that, we started with a simple
<a href="https://docs.rs/futures/latest/futures/executor/fn.block_on.html"><code>block_on</code></a> as our
executor. <code>block_on</code> takes a single future and drives it to completion by repeatedly
polling it on the current thread. While simple and blocking, it was sufficient to
demonstrate that futures and <code>async</code>/<code>await</code> could compile to correct GPU code. While
the <code>block_on</code> executor may seem limiting, because futures are lazy and composable we
were still able to express complex concurrent workloads via combinators and async
functions.</p>
<p>Once we had futures working end to end, we moved to a more capable executor. The Embassy
executor is <a href="https://embassy.dev/">designed for embedded systems</a> and operates in Rust&#39;s
<code>#![no_std]</code> environment. This makes it a natural fit for GPUs, which lack a traditional
operating system and thus do not support Rust&#39;s standard library. Adapting it to run on
the GPU required very few changes. This ability to reuse existing open source libraries
is much better than what exists in other (non-Rust) GPU ecosystems.</p>
<p>Here we construct three independent async tasks that loop indefinitely and increment
counters in shared state to demonstrate scheduling.<!-- --> The tasks themselves do not perform useful computation. Each task awaits a simple
future that performs work in small increments and yields periodically. This allows the
executor to interleave progress between tasks.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark github-light"><code data-language="rust" data-theme="github-dark github-light"><span data-line=""><span>#![no_std]</span></span>
<span data-line=""><span>#![feature(abi_ptx)]</span></span>
<span data-line=""><span>#![feature(stdarch_nvptx)]</span></span>
<span data-line=""> </span>
<span data-line=""><span>use</span><span> core</span><span>::</span><span>future</span><span>::</span><span>Future</span><span>;</span></span>
<span data-line=""><span>use</span><span> core</span><span>::</span><span>pin</span><span>::</span><span>Pin</span><span>;</span></span>
<span data-line=""><span>use</span><span> core</span><span>::</span><span>sync</span><span>::</span><span>atomic</span><span>::</span><span>{</span><span>AtomicU32</span><span>, </span><span>Ordering</span><span>};</span></span>
<span data-line=""><span>use</span><span> core</span><span>::</span><span>task</span><span>::</span><span>{</span><span>Context</span><span>, </span><span>Poll</span><span>};</span></span>
<span data-line=""> </span>
<span data-line=""><span>use</span><span> embassy_executor</span><span>::</span><span>Executor</span><span>;</span></span>
<span data-line=""><span>use</span><span> ptx_embassy_shared</span><span>::</span><span>SharedState</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>pub</span><span> struct</span><span> InfiniteWorkFuture</span><span> {</span></span>
<span data-line=""><span>    pub</span><span> shared</span><span>:</span><span> &amp;</span><span>&#39;</span><span>static</span><span> SharedState</span></span>
<span data-line=""><span>    pub</span><span> iteration_counter</span><span>:</span><span> &amp;</span><span>&#39;</span><span>static</span><span> AtomicU32</span><span>,</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>impl</span><span> Future</span><span> for</span><span> InfiniteWorkFuture</span><span> {</span></span>
<span data-line=""><span>    type</span><span> Output</span><span> =</span><span> ();</span></span>
<span data-line=""> </span>
<span data-line=""><span>    fn</span><span> poll</span><span>(</span><span>self</span><span>:</span><span> Pin</span><span>&lt;</span><span>&amp;mut</span><span> Self</span><span>&gt;, cx</span><span>:</span><span> &amp;mut</span><span> Context</span><span>&lt;&#39;</span><span>_</span><span>&gt;) </span><span>-&gt;</span><span> Poll</span><span>&lt;()&gt; {</span></span>
<span data-line=""><span>        // Check if host requested stop</span></span>
<span data-line=""><span>        if</span><span> self</span><span>.</span><span>shared</span><span>.</span><span>stop_flag</span><span>.</span><span>load</span><span>(</span><span>Ordering</span><span>::</span><span>Relaxed</span><span>) </span><span>!=</span><span> 0</span><span> {</span></span>
<span data-line=""><span>            unsafe</span><span> { </span><span>core</span><span>::</span><span>arch</span><span>::</span><span>nvptx</span><span>::</span><span>trap</span><span>() };</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""> </span>
<span data-line=""><span>        // Track iterations and activity for demonstration purposes</span></span>
<span data-line=""><span>        self</span><span>.</span><span>iteration_counter</span><span>.</span><span>fetch_add</span><span>(</span><span>1</span><span>, </span><span>Ordering</span><span>::</span><span>Relaxed</span><span>);</span></span>
<span data-line=""><span>        self</span><span>.</span><span>shared</span><span>.</span><span>last_activity</span><span>.</span><span>fetch_add</span><span>(</span><span>1</span><span>, </span><span>Ordering</span><span>::</span><span>Relaxed</span><span>);</span></span>
<span data-line=""> </span>
<span data-line=""><span>        // Simulate work</span></span>
<span data-line=""><span>        unsafe</span><span> {</span></span>
<span data-line=""><span>            core</span><span>::</span><span>arch</span><span>::</span><span>nvptx</span><span>::</span><span>_nanosleep</span><span>(</span><span>100</span><span>);</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""> </span>
<span data-line=""><span>        cx</span><span>.</span><span>waker</span><span>()</span><span>.</span><span>wake_by_ref</span><span>();</span></span>
<span data-line=""><span>        Poll</span><span>::</span><span>Pending</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>// Three very similar tasks, incrementing different variables</span></span>
<span data-line=""><span>#[embassy_executor</span><span>::</span><span>task]</span></span>
<span data-line=""><span>async</span><span> fn</span><span> task_a</span><span>(shared</span><span>:</span><span> &amp;</span><span>&#39;</span><span>static</span><span> SharedState</span><span>) {</span></span>
<span data-line=""><span>    InfiniteWorkFuture</span><span> {</span></span>
<span data-line=""><span>        iteration_counter</span><span>:</span><span> &amp;</span><span>shared</span><span>.</span><span>task_a_iterations,</span></span>
<span data-line=""><span>        shared,</span></span>
<span data-line=""><span>    }</span><span>.await</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>#[embassy_executor</span><span>::</span><span>task]</span></span>
<span data-line=""><span>async</span><span> fn</span><span> task_b</span><span>(shared</span><span>:</span><span> &amp;</span><span>&#39;</span><span>static</span><span> SharedState</span><span>) {</span></span>
<span data-line=""><span>    InfiniteWorkFuture</span><span> {</span></span>
<span data-line=""><span>        iteration_counter</span><span>:</span><span> &amp;</span><span>shared</span><span>.</span><span>task_b_iterations,</span></span>
<span data-line=""><span>        shared,</span></span>
<span data-line=""><span>    }</span><span>.await</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>#[embassy_executor</span><span>::</span><span>task]</span></span>
<span data-line=""><span>async</span><span> fn</span><span> task_c</span><span>(shared</span><span>:</span><span> &amp;</span><span>&#39;</span><span>static</span><span> SharedState</span><span>) {</span></span>
<span data-line=""><span>    InfiniteWorkFuture</span><span> {</span></span>
<span data-line=""><span>        iteration_counter</span><span>:</span><span> &amp;</span><span>shared</span><span>.</span><span>task_c_iterations,</span></span>
<span data-line=""><span>        shared,</span></span>
<span data-line=""><span>    }</span><span>.await</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>#[</span><span>unsafe</span><span>(no_mangle)]</span></span>
<span data-line=""><span>pub</span><span> unsafe</span><span> extern</span><span> &#34;ptx-kernel&#34;</span><span> fn</span><span> run_forever</span><span>(shared_state</span><span>:</span><span> *mut</span><span> SharedState</span><span>) {</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // ... executor setup and initialization ...</span></span>
<span data-line=""> </span>
<span data-line=""><span>    // Safety: the CPU needs to ensure the buffer says alive</span></span>
<span data-line=""><span>    // for as long as this is running</span></span>
<span data-line=""><span>    let</span><span> shared </span><span>=</span><span> unsafe</span><span> { </span><span>&amp;const</span><span> (</span><span>*</span><span>shared_state) };</span></span>
<span data-line=""><span>    executor</span><span>.</span><span>run</span><span>(</span><span>|</span><span>spawner</span><span>|</span><span> {</span></span>
<span data-line=""><span>        if</span><span> let</span><span> Ok</span><span>(token) </span><span>=</span><span> task_a</span><span>(shared) {</span></span>
<span data-line=""><span>            spawner</span><span>.</span><span>spawn</span><span>(token);</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>        if</span><span> let</span><span> Ok</span><span>(token) </span><span>=</span><span> task_b</span><span>(shared) {</span></span>
<span data-line=""><span>            spawner</span><span>.</span><span>spawn</span><span>(token);</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>        if</span><span> let</span><span> Ok</span><span>(token) </span><span>=</span><span> task_c</span><span>(shared) {</span></span>
<span data-line=""><span>            spawner</span><span>.</span><span>spawn</span><span>(token);</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>    });</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Below is an <a href="https://asciinema.org/">Asciinema</a> recording of the GPU running the async
tasks via Embassy&#39;s executor. Performance is not representative as the example runs
empty infinite loops and uses atomics to track activity. The important point is that
multiple tasks execute concurrently on the GPU, driven by an existing, production-grade
executor using Rust&#39;s regular <code>async</code>/<code>await</code>.</p>

<p>Taken together, we think Rust and its async model are a strong fit for the GPU. Notably,
similar ideas are emerging in other language ecosystems, such as NVIDIA&#39;s
<a href="https://github.com/nvidia/stdexec"><code>stdexec</code></a> work for C++. The difference is these
abstractions already exist in Rust, are widely used, and are supported by a mature
ecosystem of executors and libraries.</p>
<h2>Downsides of Rust&#39;s <code>async</code>/<code>await</code> on the GPU</h2>
<p>Futures are cooperative. If a future does not yield, it can starve other work and degrade
performance. This is not unique to GPUs, as cooperative multitasking on CPUs has the
same failure mode.</p>
<p>GPUs do not provide interrupts. As a result, an executor running on the device must
periodically poll futures to determine whether they can make progress. This involves
spin loops or similar waiting mechanisms. APIs such as
<a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#miscellaneous-instructions-nanosleep"><code>nanosleep</code></a>
can trade latency for efficiency, but this remains less efficient than interrupt-driven
execution and reflects a limitation of current GPU architectures. We have some ideas for
how to mitigate this and are experimenting with different approaches.</p>
<p>Driving futures and maintaining scheduling state increases register pressure. On GPUs,
this can reduce occupancy and impact performance.</p>
<p>Finally, Rust&#39;s async model on the GPU still carries the same <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">function coloring
problem</a>
that exists on the CPU.</p>
<h2>Future work</h2>
<p>On the CPU, executors such as <a href="https://tokio.rs/">Tokio</a>,
<a href="https://github.com/DataDog/glommio">Glommio</a>, and
<a href="https://github.com/smol-rs/smol">Smol</a> make different tradeoffs around scheduling,
latency, and throughput. We expect a similar diversity to emerge on the GPU. We are
experimenting with GPU-native executors designed specifically around GPU hardware
characteristics.</p>
<p>A GPU-native executor could leverage mechanisms such as <a href="https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/cuda-graphs.html">CUDA
Graphs</a>
or CUDA Tile for efficient task scheduling or shared memory for fast communication
between concurrent tasks. It could also integrate more deeply with GPU scheduling
primitives than a direct port of an embedded or CPU-focused executor.</p>
<p>At VectorWare, we have recently <a href="https://www.vectorware.com/blog/rust-std-on-gpu">enabled <code>std</code> on the GPU</a>.
Futures are <code>no_std</code> compatible, so this does not impact their core functionality.
However, having the Rust standard library available on the GPU opens the door to richer
runtimes and tighter integration with existing Rust async libraries.</p>
<p>Finally, while we believe futures and <code>async</code>/<code>await</code> map well to GPU hardware and align
naturally with efforts such as CUDA Tile, they are not the only way to express
concurrency. We are exploring alternative Rust-based approaches with different tradeoffs
and will share more about those experiments in future posts.</p>
<h2>Is VectorWare only focused on Rust?</h2>
<p>We completed this work months ago. The speed at which we are able to make progress on
the GPU is a testament to the power of Rust&#39;s abstractions and ecosystem.</p>
<p>As a company, we understand that not everyone uses Rust. Our future products will
support multiple programming languages and runtimes. However, we believe Rust is
uniquely well suited to building high-performance, reliable GPU-native applications and
that is what we are most excited about.</p>
<h2>Follow along</h2>
<p>Follow us on <a href="https://x.com/vectorware">X</a>,
<a href="https://bsky.app/profile/vectorware.com">Bluesky</a>,
<a href="https://www.linkedin.com/company/vectorware/">LinkedIn</a>, or subscribe to our
<a href="https://www.vectorware.com/blog">blog</a> to stay updated on our progress. We will be sharing more about our work in
the coming months. You can also reach us at <a href="mailto:hello@vectorware.com">hello@vectorware.com</a>.</p>
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
</div></section></article></div>
  </body>
</html>
