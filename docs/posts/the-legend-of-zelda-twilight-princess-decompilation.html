<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/zeldaret/tp">Original</a>
    <h1>The Legend of Zelda: Twilight Princess Decompilation</h1>
    
    <div id="readability-page-1" class="page"><div>
      
<h3>Week 3 at Recurse Center</h3>
<p>I&#39;ve just finished my third week at Recurse Center, which means I&#39;m already halfway through my residency here. Although I&#39;m still digesting the material in <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy&#39;s Neural Net series</a>, I&#39;ve finished working through the videos, and in the coming week, I&#39;ll be continuing to implement my own model system to improve my understanding and intuition about these systems.</p>
<p>The last video in the series is centered around the pivotal <a href="">Attention is All You Need</a> paper, which presents the <em>transformer</em> architecture central to OpenAI&#39;s GPT models. In working through Karpathy&#39;s lecture series, I&#39;m struck by the elegance of neural net architecture, their composition through the repetition of simple elements. And I&#39;m struck too by the degree to which much of the &#34;art&#34; of designing these systems relies on simple arithmetic operations that nudge the state of the system within a regime of being &#34;well-behaved&#34;. Scale is the main thing, which led Rich Sutton to conclude in his <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><em>Bitter Lesson</em></a>: &#34;We have to learn the bitter lesson that building in how we think we think does not work in the long run.&#34;</p>
<p>Meanwhile ... OpenAI continues to make dramatic headlines, with the sudden firing of Sam Altman. At the time of this writing, conflicts within the board around the speed and safety of the development of their products seem to be at the core of this rift.</p>
<p>The question of social disalignment around the topic of <em>AI Alignment</em> within the leadership team at OpenAI is poignant, provocative, and perhaps all too predictable ... disalignment around what it means to build in how we think. Institutions have emergent behaviors and a kind of artificial intelligence too. It seems worth pondering how theories around the problem of alignment in both technologic and social domains emerged at a similar time, as is explored by Orit Halpern in a <a href="https://www.journals.uchicago.edu/doi/10.1086/717313">recent paper on the conjoined histories of neural nets and Hayekian neoliberal economics</a>. Which is to say, <a href="https://www.reddit.com/r/singularity/comments/17yxl1x/the_head_of_applied_research_at_openai_seems_to/">e/acc</a> is nothing new, and alignment has perhaps been out of vogue for nearly a century.</p>
<p>In my own microcosm of space-time at Recurse, the question of alignment (on a personal scale) feels like a desire for the reassertion of control: in optimizing my time here, having finished projects, the setting and completion of goals. Alignment presumes there&#39;s a reward or loss function at the heart of the model/subject (as is the case for both neural nets and neoliberal economics), but the archetype of the <a href="https://miguelabreugallery.com/wp-content/uploads/2016/12/GroundingVision_MAG_2017_Sequence_06-1-1024x276.jpg">meandering line</a> still feels more appropriate to me. And I wonder, too, about the inverse of Sutton&#39;s <em>Bitter Lesson</em>, of how the things we build changes how we think, how the logic embedded in formal systems become heuristics for our own sense-making.</p>

    </div></div>
  </body>
</html>
