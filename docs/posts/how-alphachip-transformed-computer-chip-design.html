<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/">Original</a>
    <h1>How AlphaChip transformed computer chip design</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
    
  
  
  
    

    
    
      
        <div>
          
            
            
              
              
<div>
    <div>
      <p>Research</p>
      

      
    <dl>
      
        <dt>Published</dt>
        <dd><time datetime="2024-09-26">26 September 2024</time></dd>
      
      
        <dt>Authors</dt>
        
      
    </dl>
  

      
    </div>

    
      
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w2144-h1206-n-nu-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w1856-h1044-n-nu-rw 2x"/><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w1056-h594-n-nu-rw 2x"/>
      <img alt="Close-up photograph of Google&#39;s Tensor Processing Unit (TPU) Trillium." height="603" src="https://lh3.googleusercontent.com/d1OBmuqgWhx5m_cQZVa-F2I47x1AwUF2Kog-H-xMkTkC-U52j9hl_NxB9tfD-dP9JX9SRO87JzR7TBRpQekzxqeNE5s1wlOXZH_GnJ3xPcQBu6JP=w1072-h603-n-nu" width="1072"/>
    </picture>
    
  
    
  </div>
            
          
            
            
              
              <div>
  <h4 data-block-key="gnu37">Our AI method has accelerated and optimized chip design, and its superhuman chip layouts are used in hardware around the world</h4><p data-block-key="2eiij">In 2020, we released a <a href="https://arxiv.org/pdf/2004.10746" rel="noopener" target="_blank">preprint</a> introducing our novel reinforcement learning method for designing chip layouts, which we later <a href="https://www.nature.com/articles/s41586-021-03544-w" rel="noopener" target="_blank">published in Nature</a> and <a href="https://github.com/google-research/circuit_training" rel="noopener" target="_blank">open sourced</a>.</p><p data-block-key="1qnl3">Today, we’re <a href="https://www.nature.com/articles/s41586-024-08032-5" rel="noopener" target="_blank">publishing a Nature addendum</a> that describes more about our method and its impact on the field of chip design. We’re also releasing a <a href="https://github.com/google-research/circuit_training/?tab=readme-ov-file#PreTrainedModelCheckpoint" rel="noopener" target="_blank">pre-trained checkpoint</a>, sharing the model weights and announcing its name: AlphaChip.</p><p data-block-key="93pa">Computer chips have fueled remarkable progress in artificial intelligence (AI), and AlphaChip returns the favor by using AI to accelerate and optimize chip design. The method has been used to design superhuman chip layouts in the last three generations of Google’s custom AI accelerator, the <a href="https://cloud.google.com/tpu?hl=en" rel="noopener" target="_blank">Tensor Processing Unit</a> (TPU).</p><p data-block-key="dg0up">AlphaChip was one of the first reinforcement learning approaches used to solve a real-world engineering problem. It generates superhuman or comparable chip layouts in hours, rather than taking weeks or months of human effort, and its layouts are used in chips all over the world, from data centers to mobile phones.</p>
</div>
            
          
            
            
              
              <figure>
  <blockquote>
    <p>“</p>
    <p data-block-key="pesui">AlphaChip’s groundbreaking AI approach revolutionizes a key phase of chip design.</p>
  </blockquote>
  <figcaption><p data-block-key="9oba8">SR Tsai, Senior Vice President of MediaTek</p></figcaption>
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="isrhj">How AlphaChip works</h2><p data-block-key="6vcl">Designing a chip layout is not a simple task. Computer chips consist of many interconnected blocks, with layers of circuit components, all connected by incredibly thin wires. There are also lots of complex and intertwined design constraints that all have to be met at the same time. Because of its sheer complexity, chip designers have struggled to automate the chip floorplanning process for over sixty years.</p><p data-block-key="4179m">Similar to <a href="https://deepmind.google/technologies/alphago/" rel="noopener" target="_blank">AlphaGo</a> and <a href="https://deepmind.google/technologies/alphazero-and-muzero/" rel="noopener" target="_blank">AlphaZero</a>, which learned to master the games of Go, chess and shogi, we built AlphaChip to approach chip floorplanning as a kind of game.</p><p data-block-key="6j69d">Starting from a blank grid, AlphaChip places one circuit component at a time until it’s done placing all the components. Then it’s rewarded based on the quality of the final layout. A novel “edge-based” graph neural network allows AlphaChip to learn the relationships between interconnected chip components and to generalize across chips, letting AlphaChip improve with each layout it designs.</p>
</div>
            
          
            
            
              
              




<figure aria-labelledby="single-media-fbf5a211-7683-48e8-9249-1dc49e1b9e8d-figcaption">
  
  
    <figcaption id="single-media-fbf5a211-7683-48e8-9249-1dc49e1b9e8d-figcaption">
      <p data-block-key="wsykb">Left: Animation showing AlphaChip placing the open-source, Ariane RISC-V CPU, with no prior experience. Right: Animation showing AlphaChip placing the same block after having practiced on 20 TPU-related designs.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="gnu37">Using AI to design Google’s AI accelerator chips</h2><p data-block-key="e8433">AlphaChip has generated superhuman chip layouts used in every generation of Google’s TPU since its publication in 2020. These chips make it possible to massively scale-up AI models based on Google’s <a href="https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/" rel="noopener" target="_blank">Transformer</a> architecture.</p><p data-block-key="8k0o4">TPUs lie at the heart of our powerful generative AI systems, from large language models, like <a href="https://gemini.google.com/" rel="noopener" target="_blank">Gemini</a>, to image and video generators, <a href="https://deepmind.google/technologies/imagen-3/" rel="noopener" target="_blank">Imagen</a> and <a href="https://deepmind.google/technologies/veo/" rel="noopener" target="_blank">Veo</a>. These AI accelerators also lie at the heart of Google&#39;s AI services and are <a href="https://cloud.google.com/tpu" rel="noopener" target="_blank">available</a> to external users via Google Cloud.</p>
</div>
            
          
            
            
              
              




<figure aria-labelledby="single-media-fddc11f0-446b-44f4-b505-2bd1bb9fd1fa-figcaption">
  
  
    <figcaption id="single-media-fddc11f0-446b-44f4-b505-2bd1bb9fd1fa-figcaption">
      <p data-block-key="c46ob">A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="gnu37">To design TPU layouts, AlphaChip first practices on a diverse range of chip blocks from previous generations, such as <a href="https://en.wikipedia.org/wiki/Network_on_a_chip" rel="noopener" target="_blank">on-chip and inter-chip network blocks</a>, <a href="https://en.wikipedia.org/wiki/Memory_controller" rel="noopener" target="_blank">memory controllers</a>, and <a href="https://en.wikipedia.org/wiki/Data_buffer" rel="noopener" target="_blank">data transport buffers</a>. This process is called pre-training. Then we run AlphaChip on current TPU blocks to generate high-quality layouts. Unlike prior approaches, AlphaChip becomes better and faster as it solves more instances of the chip placement task, similar to how human experts do.</p><p data-block-key="blred">With each new generation of TPU, including our latest <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus" rel="noopener" target="_blank">Trillium</a> (6th generation), AlphaChip has designed better chip layouts and provided more of the overall floorplan, accelerating the design cycle and yielding higher-performance chips.</p>
</div>
            
          
            
            
              
              




<figure aria-labelledby="single-media-e5559907-7a8e-4691-b105-600819e44c8b-figcaption">
  
  
    <figcaption id="single-media-e5559907-7a8e-4691-b105-600819e44c8b-figcaption">
      <p data-block-key="c46ob">Bar graph showing the number of AlphaChip designed chip blocks across three generations of Google’s Tensor Processing Units (TPU), including v5e, v5p and Trillium.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              




<figure aria-labelledby="single-media-80a69e5d-5c52-42da-851f-eebd2ca1cbe0-figcaption">
  
  
    <figcaption id="single-media-80a69e5d-5c52-42da-851f-eebd2ca1cbe0-figcaption">
      <p data-block-key="c46ob">Bar graph showing AlphaChip’s average wirelength reduction across three generations of Google’s Tensor Processing Units (TPUs), compared to placements generated by the TPU physical design team.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="gnu37">AlphaChip’s broader impact</h2><p data-block-key="7tafo">AlphaChip’s impact can be seen through its applications across Alphabet, the research community and the chip design industry. Beyond designing specialized AI accelerators like TPUs, AlphaChip has generated layouts for other chips across Alphabet, such as <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu" rel="noopener" target="_blank">Google Axion Processors</a>, our first Arm-based general-purpose data center CPUs.</p><p data-block-key="6cgui">External organizations are also adopting and building on AlphaChip. For example, MediaTek, one of the top chip design companies in the world, extended AlphaChip to accelerate development of their most advanced chips — like the <a href="https://www.mediatek.com/products/smartphones/dimensity-5g" rel="noopener" target="_blank">Dimensity Flagship 5G</a> used in Samsung mobile phones — while improving power, performance and chip area.</p>
</div>
            
          
            
            
              
              <p data-block-key="gnu37">AlphaChip has triggered an explosion of work on AI for chip design, and has been extended to other critical stages of chip design, such as <a href="https://openreview.net/forum?id=0t1O8ziRZp" rel="noopener" target="_blank">logic synthesis</a> and <a href="https://ieeexplore.ieee.org/document/9980637" rel="noopener" target="_blank">macro selection</a>.</p>
            
          
            
            
              
              <figure>
  <blockquote>
    <p>“</p>
    <p data-block-key="pesui">AlphaChip has inspired an entirely new line of research on reinforcement learning for chip design, cutting across the design flow from logic synthesis to floorplanning, timing optimization and beyond.</p>
  </blockquote>
  <figcaption><p data-block-key="9oba8">Professor Siddharth Garg, NYU Tandon School of Engineering</p></figcaption>
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="gnu37">Creating the chips of the future</h2><p data-block-key="do3u6">We believe AlphaChip has the potential to optimize every stage of the chip design cycle, from computer architecture to manufacturing — and to transform chip design for custom hardware found in everyday devices such as smartphones, medical equipment, agricultural sensors and more.</p><p data-block-key="5crob">Future versions of AlphaChip are now in development and we look forward to working with the community to continue revolutionizing this area and bring about a future in which chips are even faster, cheaper and more power-efficient.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              



            
          
            
            
              
              
            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  
  

  

  </article>

    </div></div>
  </body>
</html>
