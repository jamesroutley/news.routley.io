<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qwenlm.github.io/blog/qwen1.5-110b/">Original</a>
    <h1>Qwen1.5-110B</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><p><a href="https://github.com/QwenLM/Qwen1.5" target="_blank">GITHUB</a>
<a href="https://huggingface.co/Qwen" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/organization/qwen" target="_blank">MODELSCOPE</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-Demo" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><p>Recently we have witnessed a burst of large-scale models with over 100 billion parameters in the opensource community. These models have demonstrated remarkable performance in both benchmark evaluation and chatbot arena. Today, we release the first 100B+ model of the Qwen1.5 series, Qwen1.5-110B, which achieves comparable performance with Meta-Llama3-70B in the base model evaluation, and outstanding performance in the chat evaluation, including MT-Bench and AlpacaEval 2.0.</p><p>Qwen1.5-110B is similar to other Qwen1.5 models and built with the same Transformer decoder architecture. It consists of grouped query attention (GQA) and it can be efficient in model serving. The model supports the context length 32K tokens, and the model is still multilingual, supporting a large number of languages including English, Chinese, French, Spanish, German, Russian, Korean, Japanese, Vietnamese, Arabic, etc.</p><p>We conduct a series of evaluations for the base language models, and we compare with Meta-Llama3-70B, the recent SOTA language model as well as Mixtral-8x22B.</p><table><thead><tr><th></th><th>Qwen1.5-110B</th><th>Qwen1.5-72B</th><th>Llama-3-70B</th><th>Mixtral-8x22B</th></tr></thead><tbody><tr><td>MMLU</td><td>80.4</td><td>77.5</td><td>79.5</td><td>77.8</td></tr><tr><td>TheoremQA</td><td>34.9</td><td>29.3</td><td>32.0</td><td>35.9</td></tr><tr><td>GPQA</td><td>35.9</td><td>36.3</td><td>36.4</td><td>34.3</td></tr><tr><td>Hellaswag</td><td>87.5</td><td>86.0</td><td>88.0</td><td>88.7</td></tr><tr><td>BBH</td><td>74.8</td><td>65.5</td><td>76.6</td><td>69.2</td></tr><tr><td>ARC-C</td><td>69.6</td><td>65.9</td><td>68.8</td><td>70.7</td></tr><tr><td>GSM8K</td><td>85.4</td><td>79.5</td><td>79.2</td><td>78.6</td></tr><tr><td>MATH</td><td>49.6</td><td>34.1</td><td>41.0</td><td>41.7</td></tr><tr><td>HumanEval</td><td>52.4</td><td>41.5</td><td>45.7</td><td>45.1</td></tr><tr><td>MBPP</td><td>58.1</td><td>53.4</td><td>55.1</td><td>71.2</td></tr></tbody></table><p>The above results show that the new 110B model is at least competitive with the Llama-3-70B model in terms of base capabilities. In terms of this model, we did not change the pretraining and posttraining recipes drastically, and thus we believe that the performance improvement compared with 72B comes from increasing model size.</p><p>We also test the chat models on MT-Bench and AlpacaEval 2.0.</p><table><tbody><tr><th rowspan="2">Models</th><th colspan="1">MT-Bench</th><th colspan="1">AlpacaEval 2.0</th></tr><tr><th>Avg. Score</th><th>LC Win Rate</th></tr><tr><td>Llama-3-70B-Instruct</td><td>8.85</td><td>34.40</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>8.61</td><td>36.60</td></tr><tr><td>Qwen1.5-110B-Chat</td><td>8.88</td><td>43.90</td></tr></tbody></table><p>Compared with the previously released 72B model, on the two benchmark evaluation for chat models the 110B performs significantly better. The consistent improvement in the evaluation indicates that stronger and larger base language models can lead to better chat models even without changing the post-training recipes much.</p><h2 id="develop-with-qwen15-110b">Develop with Qwen1.5-110B</h2><p>We advise you to read our blog for <a href="https://qwenlm.github.io/blog/qwen1.5/">Qwen1.5</a> to figure out the usages with Transformers, vLLM, llama.cpp, Ollama, etc.</p><h2 id="conclusion">Conclusion</h2><p>The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!</p></div></article></div></div>
  </body>
</html>
