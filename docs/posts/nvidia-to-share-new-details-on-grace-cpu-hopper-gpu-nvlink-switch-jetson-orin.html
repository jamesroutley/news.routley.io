<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/">Original</a>
    <h1>Nvidia to Share New Details on Grace CPU, Hopper GPU, NVLink Switch, Jetson Orin</h1>
    
    <div id="readability-page-1" class="page"><div id="bsf_rt_marker"><div data-url="https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/" data-title="NVIDIA to Share New Details on Grace CPU, Hopper GPU, NVLink Switch, Jetson Orin Module at Hot Chips" data-hashtags=""><p>In four talks over two days, senior NVIDIA engineers will describe innovations in <a href="https://blogs.nvidia.com/blog/2021/09/01/what-is-accelerated-computing/">accelerated computing</a> for modern data centers and systems <a href="https://blogs.nvidia.com/blog/2019/10/22/what-is-edge-computing/">at the edge</a> of the network.</p>
<p>Speaking at a virtual <a href="https://hotchips.org/">Hot Chips</a> event, an annual gathering of processor and system architects, they’ll disclose performance numbers and other technical details for NVIDIA’s first server CPU, the Hopper GPU, the latest version of the NVSwitch interconnect chip and the NVIDIA Jetson Orin system on module (SoM).</p>
<p>The presentations provide fresh insights on how the NVIDIA platform will hit new levels of performance, efficiency, scale and security.</p>
<p>Specifically, the talks demonstrate a design philosophy of innovating across the full stack of chips, systems and software where GPUs, CPUs and DPUs act as peer processors. Together they create a platform that’s already running AI, data analytics and high performance computing jobs inside cloud service providers, supercomputing centers, corporate data centers and autonomous systems.</p>
<h2><b>Inside NVIDIA’s First Server CPU</b></h2>
<p>Data centers require flexible clusters of CPUs, GPUs and other accelerators sharing massive pools of memory to deliver the energy-efficient performance today’s workloads demand.</p>
<p>To meet that need, Jonathon Evans, a distinguished engineer and 15-year veteran at NVIDIA, will describe the <a href="https://www.nvidia.com/en-us/data-center/nvlink-c2c/">NVIDIA NVLink-C2C</a>. It connects CPUs and GPUs at 900 gigabytes per second with 5x the energy efficiency of the existing PCIe Gen 5 standard, thanks to data transfers that consume just 1.3 picojoules per bit.</p>
<p>NVLink-C2C connects two CPU chips to create the <a href="https://www.nvidia.com/en-us/data-center/grace-cpu/">NVIDIA Grace CPU</a> with 144 Arm Neoverse cores. It’s a processor built to solve the world’s largest computing problems.</p>
<p>For maximum efficiency, the Grace CPU uses LPDDR5X memory. It enables a terabyte per second of memory bandwidth while keeping power consumption for the entire complex to 500 watts.</p>
<h2><b>One Link, Many Uses</b></h2>
<p>NVLink-C2C also links Grace CPU and Hopper GPU chips as memory-sharing peers in the <a href="https://nvidianews.nvidia.com/news/nvidia-introduces-grace-cpu-superchip">NVIDIA Grace Hopper Superchip</a>, delivering maximum acceleration for performance-hungry jobs such as AI training.</p>
<p>Anyone can build custom chiplets using NVLink-C2C to coherently connect to NVIDIA GPUs, CPUs, DPUs and SoCs, expanding this new class of integrated products. The interconnect will support AMBA CHI and CXL protocols used by Arm and x86 processors, respectively.</p>
<figure id="attachment_59078" aria-describedby="caption-attachment-59078"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-scaled.jpg"><picture>
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-672x221.jpg.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-400x131.jpg.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-768x252.jpg.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-1536x504.jpg.webp 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-scaled.jpg.webp 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-842x276.jpg.webp 842w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-406x133.jpg.webp 406w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-188x62.jpg.webp 188w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-1280x420.jpg.webp 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
<img src="https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-672x221.jpg" alt="Memory benchmarks for Grace and Grace Hopper" width="672" height="221" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-672x221.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-400x131.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-768x252.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-1536x504.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-842x276.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-406x133.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-188x62.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Grace-benchmarks-draft-1280x420.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
</picture>
</a><figcaption id="caption-attachment-59078">First memory benchmarks for Grace and Grace Hopper.</figcaption></figure>
<p>To scale at the system level, the new <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/nvswitch-technical-overview.pdf">NVIDIA NVSwitch</a> connects multiple servers into one AI supercomputer. It uses NVLink, interconnects running at 900 gigabytes per second, more than 7x the bandwidth of PCIe Gen 5.</p>
<p>NVSwitch lets users link 32 <a href="https://www.nvidia.com/en-us/data-center/dgx-h100/">NVIDIA DGX H100</a> systems into an AI supercomputer that delivers an <a href="https://blogs.nvidia.com/blog/2022/07/26/what-is-an-exaflop/">exaflop</a> of peak AI performance.</p>
<p>Alexander Ishii and Ryan Wells, both veteran NVIDIA engineers, will describe how the switch lets users build systems with up to 256 GPUs to tackle demanding workloads like training AI models that have more than 1 trillion parameters.</p>
<p>The switch includes engines that speed data transfers using the NVIDIA Scalable Hierarchical Aggregation Reduction Protocol. <a href="https://docs.nvidia.com/networking/display/sharpv214">SHARP</a> is an in-network computing capability that debuted on NVIDIA Quantum InfiniBand networks. It can double data throughput on communications-intensive AI applications.</p>
<figure id="attachment_59081" aria-describedby="caption-attachment-59081"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-scaled.jpg"><picture loading="lazy">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-672x339.jpg.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-400x202.jpg.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-768x388.jpg.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-1536x775.jpg.webp 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-scaled.jpg.webp 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-842x425.jpg.webp 842w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-406x205.jpg.webp 406w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-188x95.jpg.webp 188w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-1280x646.jpg.webp 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
<img loading="lazy" src="https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-672x339.jpg" alt="NVSwitch systems enable exaflop-class AI" width="672" height="339" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-672x339.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-400x202.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-768x388.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-1536x775.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-842x425.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-406x205.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-188x95.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2022/08/NVSwitch-system-1280x646.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
</picture>
</a><figcaption id="caption-attachment-59081">NVSwitch systems enable exaflop-class AI supercomputers.</figcaption></figure>
<p>Jack Choquette, a senior distinguished engineer with 14 years at the company, will provide a detailed tour of the <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPU</a>, aka Hopper.</p>
<p>In addition to using the new interconnects to scale to unprecedented heights, it packs many advanced features that boost the accelerator’s performance, efficiency and security.</p>
<p>Hopper’s new <a href="https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/">Transformer Engine</a> and upgraded Tensor Cores deliver a 30x speedup compared to the prior generation on AI inference with the world’s largest neural network models. And it employs the world’s first HBM3 memory system to deliver a whopping 3 terabytes of memory bandwidth, NVIDIA’s biggest generational increase ever.</p>
<p>Among other new features:</p>
<ul>
<li aria-level="1">Hopper adds <a href="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">virtualization support</a> for multi-tenant, multi-user configurations.</li>
<li aria-level="1">New <a href="https://blogs.nvidia.com/blog/2022/03/22/nvidia-hopper-accelerates-dynamic-programming-using-dpx-instructions/">DPX instructions</a> speed recurring loops for select mapping, DNA and protein-analysis applications.</li>
<li aria-level="1">Hopper packs support for enhanced security with <a href="https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/">confidential computing</a>.</li>
</ul>
<p>Choquette, one of the lead chip designers on the Nintendo64 console early in his career, will also describe parallel computing techniques underlying some of Hopper’s advances.</p>
<p>Michael Ditty, chief architect for Orin and a 17-year tenure at the company, will provide new performance specs for <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson AGX Orin</a>, an engine for edge AI, robotics and advanced autonomous machines.</p>
<p>It integrates 12 Arm Cortex-A78 cores and an NVIDIA Ampere architecture GPU to deliver up to 275 trillion operations per second on AI inference jobs. That’s up to 8x greater performance at <a href="https://blogs.nvidia.com/blog/2022/04/06/mlperf-edge-ai-inference-orin/">2.3x higher energy efficiency</a> than the prior generation.</p>
<p>The <a href="https://blogs.nvidia.com/blog/2022/08/03/nvidia-jetson-agx-orin-32gb-production-modules/">latest production module</a> packs up to 32 gigabytes of memory and is part of a compatible family that scales down to pocket-sized 5W Jetson Nano developer kits.</p>
<figure id="attachment_59164" aria-describedby="caption-attachment-59164"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf.jpg"><picture loading="lazy">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-666x500.jpg.webp 666w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-400x300.jpg.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-768x576.jpg.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-1536x1152.jpg.webp 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-600x450.jpg.webp 600w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-287x215.jpg.webp 287w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-133x100.jpg.webp 133w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-1280x960.jpg.webp 1280w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf.jpg.webp 1746w" sizes="(max-width: 666px) 100vw, 666px"/>
<img loading="lazy" src="https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-666x500.jpg" alt="Performance benchmarks for NVIDIA Orin" width="666" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-666x500.jpg 666w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-400x300.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-768x576.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-1536x1152.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-600x450.jpg 600w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-287x215.jpg 287w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-133x100.jpg 133w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf-1280x960.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2022/08/Orin-FINAL-perf.jpg 1746w" sizes="(max-width: 666px) 100vw, 666px"/>
</picture>
</a><figcaption id="caption-attachment-59164">Performance benchmarks for NVIDIA Orin</figcaption></figure>
<p>All the new chips support the NVIDIA software stack that accelerates more than 700 applications and is used by 2.5 million developers.</p>
<p>Based on the CUDA programming model, it includes dozens of NVIDIA SDKs for vertical markets like automotive (<a href="https://developer.nvidia.com/drive">DRIVE</a>) and healthcare (<a href="https://developer.nvidia.com/clara">Clara</a>), as well as technologies such as recommendation systems (<a href="https://developer.nvidia.com/nvidia-merlin">Merlin</a>) and conversational AI (<a href="https://developer.nvidia.com/riva">Riva</a>).</p>
<p>The NVIDIA AI platform is available from every major cloud service and system maker.</p>
</div></div></div>
  </body>
</html>
