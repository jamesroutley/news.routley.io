<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rachelbythebay.com/w/2021/09/22/membind/">Original</a>
    <h1>A different take on the NUMA OOM killer story</h1>
    
    
<p>
I was digging through some notes on old outages tonight and found 
something potentially useful for other people.  It&#39;s something I
<a href="https://rachelbythebay.com/w/2018/03/30/oom/">have mentioned before,</a>
but it seems like maybe that post didn&#39;t have enough specifics to
make it really &#34;land&#34; when someone else does a web search.
</p>
<p>
So, in the hopes of spreading some knowledge, here is a little story 
about a crashy service.
</p>
<p>
On a Wednesday night not so long ago, someone opened a SEV (that is, a 
notification of something going wrong) and said the individual tasks for 
their service were &#34;crash looping&#34;.  These things ran in an environment 
where they were supervised by the job runner, so if they died, they&#39;d be 
restarted, but you never wanted to keep it in this kind of state.
</p>
<p>
This got the attention of at least one engineer who happened to be 
hanging around on the company chat channel where production stuff was 
discussed because a bot announced the SEV&#39;s creation.  The SEV-creating 
engineer was also in that channel as best practices dictated, and a 
conversation started.  They started debugging things together.
</p>
<p>
It looked like this had started when some update had rolled out.  That 
is, prior to the update, their service was fine.  After they changed 
something, it started crashing.  Things were so unstable that some of 
the tasks never &#34;became healthy&#34; - they would go from &#34;starting up&#34; to 
&#34;dead&#34; and back again - they never managed to reach a steady 
&#34;ready/online/good/healthy&#34; state.  This meant they weren&#39;t serving 
requests, and the service was tanking.
</p>
<p>
The job supervisory service was noticing this, and it logged the fact 
that the binaries were exiting due to signal 9, that is, SIGKILL.  Given 
that the job supervisor it wasn&#39;t sending it, the process wasn&#39;t sending 
it to itself, and nobody else was on these boxes poking at things, what 
else could it be?
</p>
<p>
It was the kernel.  Specifically, it was the OOM (out of memory) killer.
</p>
<div class="terminal">
<p>
date time host kernel: [nnn.nnn] Killed process nnn (name) ...
</p>
</div>
<p>
This line in the syslog explained that it was getting smacked down for 
using too much memory.  However, the odd thing is that the amount of 
actual physical memory in use... the resident set size... (RSS) was 
merely 6 GB.  This was on a 16 GB machine.  It should not be dying 
there, particularly since the machine was supposed to be dedicated to 
this work.  Nothing else had &#34;reserved&#34; the rest of it.
</p>
<p>
The question then became: if it&#39;s a 16 GB box, why does it die at 6?
</p>
<p>
One of the engineers noticed the job was using &#34;numactl&#34; to start things 
up.  This is one of those tools which people sometimes use to do special 
high-performance gunk on their machines.  Non-uniform memory access is 
something you get on larger systems where not all of the CPUs have 
access to all of the memory equally well - hence, the &#34;non-uniform&#34; 
part of the name.
</p>
<p>
Because of an outage a few years before, there was a nugget of &#34;tribal 
knowledge&#34; floating around: if you are doing funny stuff with NUMA and 
request an allocation from just a particular node, you can totally &#34;OOM 
the node&#34;.  That is, the system will have plenty of memory, but that 
particular node (grouping of CPU + memory that&#39;s close by) might not!  
If you get into that situation, Linux will happily OOM-kill you.
</p>
<p>
They dug around in their config, and sure enough, they had numactl 
specified with &#34;membind&#34;.  The man page for that tool hints at what can 
happen but doesn&#39;t quite convey the potential danger in my opinion:
</p>
<blockquote>
<p>
--membind=nodes, -m nodes
</p>
<p>
Only allocate memory from nodes.  Allocation will fail when there is 
not enough memory available on these nodes.  nodes may be specified as 
noted above.
</p>
</blockquote>
<p>
It&#39;s not just that it&#39;ll fail - you&#39;ll probably *be killed* if you 
happen to be the biggest thing on there.  If not, you might live, but 
you&#39;ll cause something far bigger (and probably much more important) to 
die, all because of your piddling memory request!
</p>
<p>
The engineers pulled that out of the config, pushed again, and things 
went back to normal.  It&#39;s not clear if they later came back and did 
&#34;interleave&#34; or otherwise twiddled things to get a clean &#34;fit&#34; into each 
of the available nodes on their systems.
</p>
<p>
If you ever switched on membind with numactl and started a mass 
slaughter of processes on your Linux boxes, this might be why.  I hope 
this helps someone.
</p>

  </body>
</html>
