<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/">Original</a>
    <h1>Tau¬≤ benchmark: How a prompt rewrite boosted GPT-5-mini by 22%</h1>
    
    <div id="readability-page-1" class="page"><section data-astro-cid-xj2uyz6m="">  </section><section data-astro-cid-xj2uyz6m=""> <div data-astro-cid-xj2uyz6m=""> <div data-astro-cid-xj2uyz6m="">  <p><strong>Now on the front page of Hacker News ‚Äî <a href="https://news.ycombinator.com/item?id=45275354">join the discussion</a>.</strong></p>
<p>In <a href="https://quesma.com/blog/tau2-from-llm-benchmark-to-blueprint-for-testing-ai-agents/">a recent post</a>, we introduced the Tau¬≤ benchmark, a framework for benchmaring LLMs. Today we‚Äôre sharing a surprising discovery we made while using it: a simple <strong>prompt rewrite boosted a small model‚Äôs success rate by over 20%</strong>. This post is a deep-dive on how we found and fixed this performance bottleneck by making subtle changes to agent policies.</p>
<h2 id="benchmarking-llms-with-tau">Benchmarking LLMs with Tau¬≤</h2>
<p>On the recent OpenAI Summer Update, we have seen that GPT-5 model has made significant strides in agentic tasks. To validate these claims, they‚Äôve turned to the Tau¬≤ benchmark, which simulates real-world agent interactions across various domains like telecom, retail, and airlines.</p>
<p>Before moving any further, we have to establish that GPT-5 showed significant improvement only in one benchmark domain - which is Telecom. The other ones have been somehow overlooked during model presentation - therefore we won‚Äôt bother about them either (üòâ).</p>
<p><img alt="Bar chart comparing GPT-5, OpenAI o3, and GPT-4.1 tool use accuracy in telecom, retail, and airline" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="947" src="https://quesma.com/_astro/image2.3Ev8Kk46_Z2q2E1W.webp"/></p>
<p>In agentic interactions, accuracy is non-negotiable, but model speed is equally vital for user experience. Therefore, it makes sense to consider alternatives to flagship models, such as the recently introduced GPT-5-mini.</p>
<p>GPT-5-mini offers significant advantages: it‚Äôs roughly twice as fast in latency and noticeably more efficient in throughput. While delivering 85‚Äì95% of the full GPT-5‚Äôs performance, it is also five times cheaper.</p>
<p>Therefore, we ran an experiment to explore two things:</p>
<ul>
<li>How well GPT-5-mini performs on this benchmark.</li>
<li>Whether we can improve its results by making subtle changes to the domain, such as modifying agent policies or task descriptions.</li>
</ul>
<h2 id="baseline-expect-gpt-5-mini-to-fail-45-of-the-time">Baseline: Expect GPT-5-mini to Fail 45% of the Time</h2>
<p><img alt="GPT-5, GPT-5 mini, and GPT-5 nano model options with descriptions" loading="lazy" decoding="async" fetchpriority="auto" width="1902" height="572" src="https://quesma.com/_astro/image3.CmeGdum5_1a59hL.webp"/></p>
<p>Firstly, we‚Äôre going to establish the benchmark for the GPT-5-mini model. As the telecom benchmark contains over 100 tests, we‚Äôll use their subset. Luckily, the telecom_small task set comes in handy with just 20 test scenarios.</p>
<p>Running the benchmark with:</p>
<pre tabindex="0" data-language="bash"><code><span><span>tau2</span><span> run</span><span> \</span></span>
<span><span>    --domain</span><span> telecom</span><span> \</span></span>
<span><span>    --agent-llm</span><span> gpt-5-mini</span><span> \</span></span>
<span><span>    --user-llm</span><span> gpt-5-mini</span><span> \</span></span>
<span><span>    --num-trials</span><span> 2</span><span> --task-set-name</span><span> telecom_small</span></span></code></pre>
<p>Our results are:
<img alt="Simulation results showing average reward 0.55 and cost per conversation $0.0292" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="412" src="https://quesma.com/_astro/image4.B03mMqZq_1aIjlH.webp"/></p>
<p>We ended up running 40 simulations:</p>
<p>The initial success rate was low: <strong>just 55%</strong>. The GPT-5-mini with its limited reasoning capabilities doesn‚Äôt even get close to flagship GPT-5.</p>
<p>There‚Äôs an additional interesting metric this benchmark has introduced, which is pass^k. This measures how well an agent can perform when it‚Äôs challenged with the same task k times. I like to think of it as the <strong>reliability of the AI Agent</strong>.</p>
<h2 id="the-hack-using-claude-to-rewrite-prompts-for-gpt-5-mini">The Hack: Using Claude to Rewrite Prompts for GPT-5-mini</h2>
<p>When hacking the AI Agent for <code>GPT-5-mini</code> model we aimed to answer three questions:</p>
<ul>
<li>Can I improve the overall success rate when using this limited model?</li>
<li>Are we able to ‚Äúunlock‚Äù more tasks which agents are capable of handling?</li>
<li>Does agent reliability improve?</li>
</ul>
<p>The beauty of generative AI is that we could offload lots of work and allow ourselves a little laziness. Therefore, we asked Claude to analyze the AI agent policies in the telecom domain, which are the building blocks for the agent prompt. From there, we asked specifically to assume that these policies are going to feed an AI Agent prompt, which will be using faster, non-flagship LLM. Output documents (<code>main_policy.md</code> and <code>tech_support_manual.md</code>) will have to be rephrased so that they are easier to follow for the agent and result in improving its behaviour.</p>
<p>Here‚Äôs the ‚Äúexecutive summary‚Äù of improvements it made:</p>
<pre tabindex="0" data-language="markdown"><code><span><span>I&#39;ve created two AI-optimized versions of the telecom domain documentation:</span></span>
<span></span>
<span><span>Key Improvements Made:</span></span>
<span></span>
<span><span>1.</span><span> Structure &amp; Flow</span></span>
<span><span>   -</span><span> Decision Trees: Clear branching logic with ‚îú‚îÄ‚îÄ and ‚îî‚îÄ‚îÄ notation</span></span>
<span><span>   -</span><span> Sequential Steps: Numbered, ordered procedures instead of scattered explanations</span></span>
<span><span>   -</span><span> Prerequisites: Explicit dependency checks before proceeding</span></span>
<span></span>
<span><span>2.</span><span> AI Agent Optimizations</span></span>
<span><span>   -</span><span> Tool Call Clarity: Exact function names and parameters</span></span>
<span><span>   -</span><span> Binary Decisions: Clear yes/no conditions instead of ambiguous language</span></span>
<span><span>   -</span><span> Error Handling: Specific failure conditions and next steps</span></span>
<span><span>   -</span><span> Verification Steps: &#34;Recheck&#34; instructions after each fix</span></span>
<span></span>
<span><span>3.</span><span> Cognitive Load Reduction</span></span>
<span><span>   -</span><span> Reference Tables: Quick lookup for tools and purposes</span></span>
<span><span>   -</span><span> Pattern Recognition: Common issue combinations and their solutions</span></span>
<span><span>   -</span><span> Critical Reminders: Common AI mistakes section to prevent errors</span></span>
<span></span>
<span><span>4.</span><span> Actionable Language</span></span>
<span><span>   -</span><span> Removed verbose explanations mixed with instructions</span></span>
<span><span>   -</span><span> Consolidated multiple documents&#39; logic into single workflows</span></span>
<span><span>   -</span><span> Used imperative commands: &#34;Check X&#34;, &#34;If Y then Z&#34;</span></span>
<span><span>   -</span><span> Added immediate verification steps</span></span></code></pre>
<p>We‚Äôve moved from lengthy policy declarations to clear, directive instructions. Instead of wordy statements, the new style uses step-by-step logic, conditions, and patterns. For prompts, this works more like a checklist ‚Äî ‚ÄúCheck X ‚Üí If Y, then Z‚Äù ‚Äî rather than vague, descriptive policies.</p>
<h2 id="the-result-22-accuracy-boost-and-50-fewer-unsolvable-tasks">The Result: 22% Accuracy Boost and 50% Fewer Unsolvable Tasks</h2>
<p>Let‚Äôs review what our improved AI agent results look like:</p>
<p><img alt="Simulation results showing agent metrics with rewards, pass rates, and cost per conversation" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="435" src="https://quesma.com/_astro/image6.Djj-zTez_RWCcw.webp"/></p>
<p>The new prompts led to a significant performance boost.
Pass^k metrics surged:</p>
<ul>
<li>k=1 from 0.55 to 0.675 (<strong>a 22.73% improvement</strong>) ‚Üí In plain terms, GPT-5-mini now succeeds on <strong>67.5% of tasks instead of 55%</strong>.</li>
<li>k=2 from 0.4 to 0.5 (<strong>a 25% improvement</strong>) ‚Üí Meaning retries became more effective too.</li>
</ul>
<p>For context, flagship GPT-5 scores ~97% on this benchmark, o3 comes in at 58%, and GPT-4.1 at 34%. With our optimized prompts, GPT-5-mini not only jumped well above its own baseline but also <strong>outperformed o3</strong>, landing much closer to GPT-5 than before.</p>
<p>The side-by-side comparison shows exactly where the gains came from. On the left side of the screen you‚Äôll see the ‚Äústock‚Äù AI agent results, on the right - our AI agent improved for GPT-5-mini.</p>
<p><img alt="Side-by-side console logs comparing stock AI results with improved GPT-5-mini test runs" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="722" src="https://quesma.com/_astro/image7.BWRFnxLy_2mRVVB.webp"/></p>
<p>The screenshot above outlines that with our updated prompts and policies, <strong>we managed to ‚Äúunlock‚Äù some of the tests which were previously always failing</strong> due to GPT-5-mini‚Äôs limited capabilities. Now there are only 3 tasks, which the agent didn‚Äôt manage to solve at all within the given 2 trials - compared to 6.</p>
<h2 id="key-takeaways-for-your-own-models">Key Takeaways for Your Own Models</h2>
<p>This experiment shows that thoughtful prompt design can meaningfully boost the performance of smaller models like GPT-5-mini. By restructuring policies into clear, step-by-step instructions, we not only improved success rates but also ‚Äúunlocked‚Äù tasks that previously seemed unsolvable for the model.</p>
<p>The key was in simplifying language, reducing ambiguity, and breaking down reasoning into explicit, actionable steps. Smaller models struggle with long-winded or fuzzy policies, but thrive when given structured flows, binary decisions, and lightweight verification steps.</p>
<p>The takeaway is clear: using a frontier model to automatically optimize prompts can unlock major improvements for smaller LLMs.
With strategic optimization, lightweight models can deliver decent results at a fraction of the cost ‚Äî making them a compelling alternative when efficiency and affordability matter as much as accuracy.</p>
<p>If you found this helpful, let us know! Prompt engineering is still an open playground, and we‚Äôre excited to see what creative approaches others are exploring in this space.</p>
<p>Discuss it on <a href="https://www.linkedin.com/posts/quesma_bigger-ai-models-always-win-benchmarks-right-activity-7373757332989771776-cpc8">LinkedIn</a>, <a href="https://x.com/quesmaorg/status/1968324178215592128">X</a> or <a href="https://news.ycombinator.com/item?id=45275354">Hacker News</a>.</p>
<p><img alt="Two astronauts in space with Earth behind them, one pointing a gun at the other." loading="lazy" decoding="async" fetchpriority="auto" width="996" height="564" src="https://quesma.com/_astro/image8.B08GRvh2_RL2E0.webp"/></p>
<p><strong>UPDATE:</strong> Since publishing this post and hitting <a href="https://news.ycombinator.com/item?id=45275354">the front page of HN</a>,
some readers expressed interest in seeing the actual before and after policies (which are building block for the agent prompt).
Initially I thought these would be too lengthy for the article and no one would care,
but since there‚Äôs interest, I‚Äôm happy to share them in <a href="https://github.com/mieciu/tau2-bench/pull/1/files">this Pull Request</a>.</p>  </div> </div> </section></div>
  </body>
</html>
