<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rachelbythebay.com/w/2024/03/05/outage/">Original</a>
    <h1>Sometimes the dam breaks even after plenty of warnings</h1>
    
    
<p>
Oh dear, it&#39;s
<a href="https://rachelbythebay.com/w/2021/10/04/callthecops/">popcorn for breakfast </a>
yet again.  Another outage in a massive set of web sites.
</p>
<p>
It&#39;s been about 10 years, so let&#39;s talk about the outage that marks the 
point where I started feeling useful in that job: Friday, August 1, 
2014.  That&#39;s the one where FB went down and people started calling 911 
to complain about it, and someone from the LA County sheriff&#39;s office 
got on Twitter to say &#34;knock it off, we know and it&#39;s not an 
emergency&#34;.
</p>
<p>
Right, so, it&#39;s been well-documented what happened that day, even on the 
outside world - SRECon talks, a bunch of references in papers, you name 
it.  It was time for &#34;push&#34;, and as it was being seeded, that process 
pretty much consumed all of the available memory (and swap) on the 
smallest machines.
</p>
<p>
Then there was this program which ran on every box as root, and its job 
was to run a bunch of awful subprocesses, capture their outputs, parse 
them somewhat, and ship the results to a time series database or a 
logging system.  This program is the one that had the infamous bug in it 
where it would call fork() and saved the return value, but didn&#39;t check 
it for failure: the -1 retval.
</p>
<p>
So, later on, it went to kill this &#34;child process&#34; that never started, 
and did the equivalent of &#39;kill -9 -1&#39;, and on Linux, that whacks 
everything but yourself and pid 1 (init).  Unsurprisingly, this took 
down the web server and pretty much everything else.  This was 
pre-systemd on CentOS 5 machines running Upstart, so the only things 
that &#34;came back&#34; were the &#34;respawn&#34; entries in inittab, like [a]getty on 
the text consoles.
</p>
<p>
This is how we were able to fire up a remote console on one of the 
affected machines and log in and see that there was basically init, the 
shell that had just been started, and this fbagent process which was 
responsible for assassinating the entire system that morning.
</p>
<p>
The rest of the story has also been told, which is where it took me a 
couple of weeks to figure out why we kept losing machines this way, and 
when I did, I found the source had already been patched.  Another 
engineer unrelated to the fbagent project had been hitting the same 
problem, decided to go digging, found the &#34;-1&#34; pid situation leaking 
through, and fixed it.
</p>
<p>
Even though the fix was committed, it wasn&#39;t shipped, because this 
binary was big and scary and ran as root on (then) hundreds of thousands 
of machines, and the person who usually shipped it was on vacation 
getting married somewhere.  As a result, the old version stayed in prod 
for much longer than it otherwise would have, complete with the 
hair-trigger bug that would nuke every process on the machine.
</p>
<p>
All it needed was something that would screw up fork, and on that 
morning, it finally happened.
</p>
<p>
What hasn&#39;t really been told is that the memory situation had been 
steadily getting worse on those machines that whole summer.  We had been 
watching it creep up, and kept trying to make things happen, but by and 
large, few people really cared.  Also, people had been adding more and 
more crap to what the web servers would run.  Back in those days, you 
could just tell your endpoint to run arbitrary code, and it basically 
would, right there on the web server!
</p>
<p>
Case in point: people had started running ffmpeg on our web servers.  
They decided that was an AWESOME place to transcode videos.  By doing 
that, they didn&#39;t have to build out their own &#34;tier&#34; of machines to do 
that work, which would have meant requesting resources, and all of that 
other stuff.  Instead, they just slipped that into a release and slowly 
turned up the percentage knob until it was everywhere.
</p>
<p>
ffmpeg is no small thing.  One instance could pull nine CPU cores and 
use 800 MB of memory - that&#39;s actual memory, not just virtual mappings.  
Also, this made requests run really long, and when that happened, the 
&#34;treadmill&#34; in the web server couldn&#39;t happen sufficiently quickly.
</p>
<p>
What&#39;s the treadmill?  Well, when you have memory allocations for a 
bunch of requests that then finish, you have to garbage-collect them 
eventually.  My understanding is that the treadmill essentially worked 
by waiting until every request that had been active at the same time was 
also gone, and then it would free up the resources.
</p>
<p>
This is a little confusing so think about it this way.  These machines 
were true multitasking, so they&#39;d possibly have 100 or more web server 
threads running, each potentially servicing a request.  Let&#39;s say 
requests A-M were running and then request N started up and allocated 
some memory.  The memory allocated by N would only be freed once not 
only N was done, but A-M too, since they had overlapped it in time.
If any of them were sticking around for a while, then N&#39;s resources 
couldn&#39;t be freed until that first one exited.
</p>
<p>
Given this, it&#39;s not too hard to see that really long-running requests 
effectively limit how often the &#34;treadmill&#34; can run, and thus how often 
the server will release memory for use in other things.
</p>
<p>
Also, there were other things going on which were just really expensive 
endpoints which could chew a gig of memory all by themselves.  This was 
NOT scalable.  You simply couldn&#39;t sustain that on these systems.
</p>
<p>
Basically, if you were to make a time-traveling phone call to me a few 
weeks before &#34;Call the Cops&#34; happened, and ask me what I was worried 
about, &#34;web tier chewing memory and going into swap&#34; probably would have 
been pretty high on the list.
</p>
<p>
To give some idea of how long this had been going on, that year, July 
4th (a national holiday) fell on a Friday, so we had a &#34;three day 
weekend&#34;.  When this happened, the site didn&#39;t get pushed.  This 
mattered because push would usually get the machines to free up a bunch 
of memory at once and generally become less-burdened.
</p>
<p>
A regular two-day weekend would leave things looking pretty thin by the 
time Monday&#39;s push rolled around, but a three-day weekend made things a 
lot worse... and this was a full month before everything finally broke.
</p>
<p>
So, yeah, the site broke that morning, but it&#39;s not like it was too 
surprising.  The signs had been visible for quite a while in advance.  
Imagine standing on top of a massive dam and you start seeing one leak, 
then two, then four, and so on.  You try to get help but it&#39;s just not 
happening.
</p>
<p>
Of course, once the dam actually fails, then somehow you find the 
resources to get people caring about dam maintenance.  It&#39;s funny how 
that works.
</p>

  </body>
</html>
