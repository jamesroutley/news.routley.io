<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.oranlooney.com/post/history-of-computing-2/">Original</a>
    <h1>The Prehistory of Computing, Part II</h1>
    
    <div id="readability-page-1" class="page"><article>
      <img src="https://www.oranlooney.com/post/history-of-computing-2_files/lead.jpg"/>
      
      
      
      <hr/>
      <ul>
        
        <li><time>September 27, 2025</time></li>
        <li>
          <a href="https://www.oranlooney.com/tags/computer-science/">
            <i></i>
            Computer Science
          </a> 
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/history/">
            <i></i>
            History
          </a> 
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/math/">
            <i></i>
            Math
          </a> 
        </li>
      </ul>
      

      

<p>In <a href="https://www.oranlooney.com/post/history-of-computing/">part I</a> of this two-part series we covered lookup tables and simple
devices with at most a handful of moving parts. This time we’ll pick up in the
17th centuries, when computing devices started to became far more complex and
the groundwork for later theoretical work began to be laid.</p>

<h2 id="pascal">Pascal</h2>

<p>We enter the era of mechanical calculators in 1642 when <a href="https://en.wikipedia.org/wiki/Blaise_Pascal">Pascal</a>
invented a machine, charmingly called the <a href="https://en.wikipedia.org/wiki/Pascaline">pascaline</a>, which could perform
addition and subtraction:</p>

<p><img src="https://www.oranlooney.com/post/history-of-computing_files/pascaline.jpg" alt="The Pascaline"/></p>

<p>The primary problem that must be solved to build a working adding machine,
mechanical or electrical, is the <a href="https://en.wikipedia.org/wiki/Carry_(arithmetic)">carry</a>. Yes, I’m talking about that
operation you learned in elementary school, where you “carry the 1” when the
digits in a column add up to more than 10. This problem is far less trivial
than it sounds; even today, chip designers must decide to implement either a
simple <a href="https://en.wikipedia.org/wiki/Adder_(electronics)#Ripple-carry_adder">ripple-carry adder</a> or one of many competing designs for a
<a href="https://en.wikipedia.org/wiki/Carry-lookahead_adder">carry-lookahead adder</a> depending on their transistor budget and logic
depth. Carry, it turns out, is the very soul of the adding machine.</p>

<p>That’s why the pascaline represents such a crucial breakthrough: it featured
the world’s first successful mechanical <a href="https://en.wikipedia.org/wiki/Pascaline#Carry_mechanism">carry mechanism</a>.</p>

<p>Still, the machine wasn’t very useful - adding and subtracting aren’t
particularly hard, and the time spent fiddling with the dials to input the
numbers easily dwarfed the time saved. The real prize would be a machine that
could multiply and divide, and that was the problem that <a href="https://history-computer.com/people/gottfried-leibniz-complete-biography/">Gottfried Wilhelm
von Leibniz</a> set out to solve.</p>

<h2 id="leibniz">Leibniz</h2>

<p>Leibniz, if you’ve only heard of him as a philosopher, may seem like an
unlikely person to tackle this problem. In philosophy he’s chiefly remembered
for his incomprehensible theory of monads and for stating that this was the
best of all possible worlds, a point of view that was caricatured by Voltaire
as the thoroughly unworldly and deeply impractical Dr. Pangloss in his novel
<em>Candide</em>.</p>

<p>But of course he also helped invent Calculus, proved that kinetic energy was a
conserved quantity distinct from momentum, designed hydraulic fountains and
pumps, and generally produced an enormous body of work that was original,
practical, and rigorous. If luminaries such as Voltaire viewed him as
ridiculous, it was perhaps because he was <em>too</em> logical; or rather that he kept
trying to apply rigorous logic to theology and the humanities, which is rarely
a recipe for success.</p>

<p>In fact, Leibniz is something of an unacknowledged <a href="https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz#Computation">grandfather</a> of
computer science, with multiple parallel threads leading back to him. The first
of these is mechanical computation. Leibniz built the first 4-operation
calculator, which he called the <a href="https://en.wikipedia.org/wiki/Stepped_reckoner">stepped reckoner</a>:</p>

<p><img src="https://www.oranlooney.com/post/history-of-computing_files/stepped_reckoner.jpg" alt="Leibniz&#39;s Stepped Reckoner"/></p>

<p>The name comes from a key internal component, the stepped drum, also known as
the <a href="https://en.wikipedia.org/wiki/Leibniz_wheel">Leibniz wheel</a>. This animation from Wikipedia perfectly illustrates
its operation:</p>

<p><a href="https://en.wikipedia.org/wiki/Leibniz_wheel#/media/File:Cylindre_de_Leibniz_anim%C3%A9.gif" target="_blank">
    <img src="https://www.oranlooney.com/post/history-of-computing_files/leibniz_wheel.gif"/>
</a></p>

<p>Each time the drum makes one complete revolution, the smaller red gear rotates
by a number of ticks determined by its position. Slide the red gear to the top
of the drum, and it will catch all nine teeth and advance a full revolution.
Slide it part way down, and it will catch only some of the teeth and miss
others, resulting in less rotation. Thus, if you slide the red gear to a
position representing $n$ and perform $m$ complete rotations of the drum, the
rotation of the red axle will encode $n \times m$. Furthermore, the carry
increments by one each time the red axle completes one full rotation, so it’s
not hard to imagine how a mechanical carry could be implemented. The full
stepped reckoner is simply a series of Leibniz wheels coupled by the <a href="https://alenasolcova.cz/wp-content/uploads/2017/10/G_W_-L_Compt3-20.pdf#page=17">carry
mechanism</a>.</p>

<p>Leibniz worked on his machine for decades, producing a series of prototypes,
giving live demonstrations, and writing several papers about it, but in his
lifetime the machine never worked well enough to see any practical adoption.
Despite its flaws, other inventors could see its potential and for the next <a href="https://en.wikipedia.org/wiki/Mechanical_calculator#The_18th_century">two
centuries</a>, the Leibniz wheel and the <a href="https://en.wikipedia.org/wiki/Pinwheel_calculator">Pinwheel calculator</a> (a variation
which also traces back to Leibniz’s 1685 book) would dominate the field. Nearly
every mechanical calculator produced in this time derived from Leibniz’s ideas.</p>

<p>Two centuries later, the Leibniz wheel would form the basis of the first
commercially successful mechanical calculator, the <a href="https://en.wikipedia.org/wiki/Arithmometer">Colmar arithmometer</a>.
Even a cursory glance at its internals shows how little the basic design had
changed:</p>

<p><a href="https://www.crisvandevel.de/tdc.htm" target="_blank">
    <img src="https://www.oranlooney.com/post/history-of-computing_files/arithmometer_internal.jpg"/>
</a></p>

<p>So why did the Colmar succeed where Leibniz failed? The answer is simple: the
vast improvement in precision machining techniques in the intervening
centuries.</p>

<p>While we don’t have time to go into the full <a href="https://en.wikipedia.org/wiki/Machine_tool#History" title="History of Machine Tools">history</a> of <a href="https://en.wikipedia.org/wiki/Gauge_block#History" title="Gauge Block History">precision</a>
<a href="https://en.wikipedia.org/wiki/Interchangeable_parts#Origins_of_the_modern_concept" title="Origins of Interchangeable Parts">machining</a> during the <a href="https://en.wikipedia.org/wiki/Dividing_engine" title="Dividing Engine">industrial</a> <a href="https://en.wikipedia.org/wiki/American_system_of_manufacturing#History" title="American System of Manufacturing – History">revolution</a>, suffice it to
say that what was impossible for a lone craftsman in the 17th century was
commonplace by the 19th. Leibniz himself lamented the difficulties, saying:</p>

<blockquote>
<p>“If only a craftsman could execute the instrument as I had thought the model!”
</p>
</blockquote>

<p>This is the main reason I don’t think we would have ended up with a <a href="https://en.wikipedia.org/wiki/The_Difference_Engine">steampunk
computing era</a> even if Leibniz or Babbage had gotten their machines to
work—machines made of metal and gears are orders of magnitude slower, more
expensive, and harder to work with than those made with <a href="https://en.wikipedia.org/wiki/CMOS">CMOS</a> and
<a href="https://en.wikipedia.org/wiki/Photolithography">photolithography</a>. (Matthew Jones goes into considerable detail about
these technical and financial difficulties in his book, <em><a href="https://www.amazon.com/Reckoning-Matter-Calculating-Innovation-2016-11-29/dp/B01NGZURDX">Reckoning with
Matter</a></em>.) While limited by the technology of his time, Leibniz clearly
saw their vast potential:</p>

<blockquote>
<p>“It is unworthy of excellent men to lose hours like slaves in the labour of
calculation which could safely be relegated to anyone else if machines were
used.”</p>
</blockquote>

<h2 id="formal-languages">Formal Languages</h2>

<p>There’s another thread that links Leibniz to modern computer science, on the
theoretical rather than practical side. As a young man, he wrote a paper called
<a href="https://en.wikipedia.org/wiki/De_Arte_Combinatoria"><em>On the Combinatorial Art</em></a> where he envisioned encoding concepts as
numbers and operating on them algebraically. Throughout the rest of his life,
he tried to develop what he called “<a href="https://en.wikipedia.org/wiki/Characteristica_universalis">Characteristica universalis</a>”, a kind
of universal formal language. One of his attempts in this direction, the
<a href="https://iep.utm.edu/leib-log/#SH3c">plus-minus calculus</a>, was a formal, axiomatic language which presaged
Boolean algebra and set theory. He <a href="https://spectrum.ieee.org/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas">dreamed</a> of setting logic and
mathematics on the same solid foundation as Euclidean geometry so that
arguments could be <a href="https://en.wikipedia.org/wiki/Formal_verification">formally verified</a>:</p>

<blockquote>
<p>“When there are disputes among persons, we can simply say, ‘Let us
calculate,’ and without further ado, see who is right.”
</p>
</blockquote>

<p>Leibniz’s ideas on this subject never got as far as he liked, but nevertheless
proved fruitful—<a href="https://en.wikipedia.org/wiki/Gottlob_Frege">Frege</a> cites Leibniz as an inspiration, placing him
at the head of the chain that led to Russell, Gödel, and the entire field of
formal logic and the foundations of mathematics. Today we have tools like
<a href="https://en.wikipedia.org/wiki/Lean_(proof_assistant)">Lean</a> or <a href="https://us.metamath.org/">Metamath</a> which can automatically verify proofs.</p>

<p>When talking about Leibniz, it’s easy to forget he was working in the 17th
century, not the 19th. His work would have been credible and relevant (and
perhaps better appreciated) even two centuries later.</p>

<h2 id="approximation-theory">Approximation Theory</h2>

<p>For this next section, I’m going to do something slightly inadvisable: I’m
going to deviate from strict chronological order and jump ahead a few decades
to discuss some mathematics that wasn’t fully developed until decades after
Babbage’s career. Later, we’ll jump back in time to discuss the Difference
Engine. This detour is necessary to appreciate why the Difference Engine (the
sub-Turing precursor to the Analytical Engine) was such a good idea. Babbage
certainly understood in a rough-and-ready way that <a href="https://en.wikipedia.org/wiki/Finite_difference_method">finite difference
methods</a> were useful in approximating functions, but he couldn’t have
known that in some sense they were all we really need, because Chebyshev hadn’t
proved it yet.</p>

<p><a href="https://en.wikipedia.org/wiki/Pafnuty_Chebyshev">Pafnuty Chebyshev</a> was a 19th-century Russian mathematician who is best
known for his theoretical work in number theory and statistics:</p>

<p><img src="https://www.oranlooney.com/post/history-of-computing_files/chebyshev.png" alt="Chebyshev&#39;s Magnificent Beard"/></p>

<p>He was also very interested in the practical problem of designing linkages. A
<a href="https://en.wikipedia.org/wiki/Linkage_(mechanical)">linkage</a> is a series of rigid rods that sweep a particular motion. They
are used in the design of everything from <a href="https://en.wikipedia.org/wiki/Watt%27s_linkage">steam engines</a> to <a href="https://en.wikipedia.org/wiki/Pumpjack">oil
pumps</a>. Chebyshev even designed one himself that closely approximates
linear motion over a portion of its path without undue strain on any of the
rods:</p>

<p><img alt="Chebyshev Linkage" src="https://www.oranlooney.com/post/history-of-computing_files/chebyshev_linkage.gif"/></p>

<p>But it’s not linkages as such that we’re interested in, but rather how they
inspired Chebyshev. Trying to design a linkage that approximates a certain path
<a href="https://bhavana.org.in/math-and-motion-a-look-at-chebyshevs-works-on-linkages/">led him</a> to the abstract mathematical question of how well we can
approximate <em>any</em> function, which today we call <a href="https://en.wikipedia.org/wiki/Approximation_theory">approximation theory</a>.</p>

<p>Chebyshev’s most important result in this area was the <a href="https://en.wikipedia.org/wiki/Equioscillation_theorem">equioscillation
theorem</a>, which shows that any well-behaved continuous function can be
approximated by a polynomial over a finite region with an uniform bound on
error. Such approximations will “oscillate” around the true value of the
function, sometimes shooting high, sometimes low, but always keeping to within
some fixed $\varepsilon$ of the true value.</p>

<p><img src="https://www.oranlooney.com/post/history-of-computing_files/remez.png" alt="Equioscillation Example"/></p>

<p>Chebyshev’s proof was non-constructive—it didn’t tell you how to find
such a polynomial, only that it exists—but it certainly waggled its
eyebrows in that direction, and by 1934 Remez was able to work out the details
of just such an <a href="https://en.wikipedia.org/wiki/Remez_algorithm">algorithm</a>. His algorithm tells us how to find the
coefficients of a polynomial using only a hundred or so evaluations of the
original function. It always works in theory and in practice it usually works
<em>very</em> well. Note the scale of the y-axis on the above plot; it shows an
8th-degree polynomial approximating $\sin(x)$ to an absolute error of less than
$10^{-9}$ over the region $[0, \tfrac{\pi}{2}]$. A 12th-degree polynomial can
approximate it to less than the <a href="https://en.wikipedia.org/wiki/Machine_epsilon">machine epsilon</a> of a 64-bit float.</p>

<p>Its main practical limitation is that it requires the function to be bounded on
the target interval, but for many functions such as $\tan(x)$ that have poles
where it tends to infinity, the <a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant">Padé approximant</a> can be used instead.
The Padé approximant is the ratio of two polynomials:</p>

<p>\[
R(x) = \frac{
    \displaystyle
    \sum_{i=0}^m a_i x^i
}{
    \displaystyle
    1 + \sum_{i=1}^n b_i x^i
} = \frac{
    a_0 + a_1 x + a_2 x^2 + \dots + a_m x^m
}{
    1 + b_1 x + b_2 x^2 + \dots + b_n x^n
}
\]</p>

<p>This requires performing a single division operation at the end of the
calculation, but the bulk of the work is simply evaluating the two polynomials.</p>

<p>It’s also true that the Remez algorithm only works over finite, bounded
intervals, but that is easily remedied through the simple expedient of breaking
the function up into different regions and fitting a separate polynomial to
each.</p>

<p>The practical upshot of all this is that to this day, whenever you use a
<a href="https://github.com/lattera/glibc/blob/master/sysdeps/ieee754/dbl-64/e_asin.c">function from a standard math library</a> when programming, it almost always
uses piecewise polynomial approximation under the hood. The coefficients
used for those polynomials are often found with the Remez algorithm
since the uniform bound to error it offers is very attractive in numerical
analysis but as they say on the BBC, <a href="https://fiveable.me/lists/key-approximation-algorithms">other algorithms</a> are available.</p>

<p>All of which is just a very long-winded way of saying polynomials are pretty
much all you need for a huge subset of practical computing. So wouldn’t it be
nice if we had a machine that was really, really good at evaluating
polynomials?</p>

<h2 id="the-difference-engine">The Difference Engine</h2>

<p>Babbage’s first idea, the Difference Engine, was a machine for evaluating
series and polynomials. If it had been built, it could have rapidly generated
tables for 7th-degree polynomials to a high degree of precision and reliability.</p>

<p>“Just polynomials,” you might ask, “not trigonometric functions, logarithms, or
special functions? Seems less useful than the older stuff we’ve already talked
about.” Well, technically he proposed using the <a href="https://en.wikipedia.org/wiki/Finite_difference_method">finite difference
method</a>, which is an early form of polynomial interpolation. This would
have worked well, as we saw above.</p>

<p>OK, so how did Babbage propose to automate polynomial evaluation? The key idea
is that differences turn polynomial evaluation into simple addition. If you
have a polynomial $p(x)$ and you want its values at equally spaced points $x,
x+1, x+2, \dots$, you can avoid recomputing the full polynomial each time.
Instead, you build a difference table:</p>

<ul>
<li>The <em>first differences</em> are the differences between consecutive values of the polynomial.<br/></li>
<li>The <em>second differences</em> are the differences of those differences, and so on.<br/></li>
<li>For a polynomial of degree $n$, the $n$-th differences are constant.<br/></li>
</ul>

<p>Once you know the starting value $p(x)$ and all the differences at that point,
you can get every subsequent value just by repeated additions. Let’s take
the following polynomial as an example:</p>

<p>\[
p(x) = x^3 - 2x^2 + 3x - 1
\]</p>

<p>If we first evaluate this polynomial at a number of evenly spaced points and
then (working left to right) take the difference of consecutive rows, and
take the differences of those differences and so on, we can construct a table
like so:</p>



<table>
<thead>
<tr>
<th>$x$</th>
<th>$p(x)$</th>
<th>$\Delta p$</th>
<th>$\Delta^2 p$</th>
<th>$\Delta^3 p$</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>-1</td>
<td>2</td>
<td>2</td>
<td>6</td>
</tr>

<tr>
<td>1</td>
<td>1</td>
<td>4</td>
<td>8</td>
<td>6</td>
</tr>

<tr>
<td>2</td>
<td>5</td>
<td>12</td>
<td>14</td>
<td>6</td>
</tr>

<tr>
<td>3</td>
<td>17</td>
<td>26</td>
<td>20</td>
<td>6</td>
</tr>

<tr>
<td>4</td>
<td>43</td>
<td>46</td>
<td>26</td>
<td>6</td>
</tr>
</tbody>
</table>

<p>Crucially, after three differences (the same as the order of our polynomial)
the difference becomes constant. Why does that matter? Because we can reverse
the process. Working right to left, we can compute the differences and
ultimately $p(x)$ from the preceding row using nothing more complex than
addition! Here we’ve taken an integer step for the sake of clarity, but this
$\Delta x$ can be arbitrarily small, and the whole process can be used to
create large tables with high precision. The Difference Engine would have
automated this process, allowing it to generate large tables very efficiently.</p>

<p>The Difference Engine may not have had that magical Turing-completeness, but it
was still a very clever and important idea.</p>

<h2 id="conclusion">Conclusion</h2>

<p>You know the rest of the story. <a href="https://en.wikipedia.org/wiki/Charles_Babbage">Babbage</a> moved on from the <a href="https://en.wikipedia.org/wiki/Difference_engine">Difference
Engine</a> to the much more ambitious <a href="https://en.wikipedia.org/wiki/Analytical_engine">Analytical Engine</a>. Neither was
successfully completed: as Leibniz had found centuries earlier, designing
mechanical computers is a lot easier than actually getting them to work.
Despite this, <a href="https://en.wikipedia.org/wiki/Ada_Lovelace">Ada Lovelace</a> wrote the first <a href="https://en.wikipedia.org/wiki/Note_G">computer program</a> for
it, targeting its envisioned <a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">instruction set architecture</a>.</p>

<p>After Babbage, there was a brief era of electro-mechanical computers: <a href="https://en.wikipedia.org/wiki/Herman_Hollerith">Herman
Hollerith</a> and the <a href="https://www.columbia.edu/cu/computinghistory/hh/">punch card</a>, the <a href="https://en.wikipedia.org/wiki/IBM_602">IBM 602 Calculating
Punch</a>, the delightfully named <a href="https://en.wikipedia.org/wiki/Harvard_Mark_I">Bessy the Bessel engine</a>, the
<a href="https://en.wikipedia.org/wiki/Z3_(computer)">Z3</a>, and similar machines. Slide rules were still in common use until
WWII, and mechanical calculators such as the <a href="https://en.wikipedia.org/wiki/Curta">Curta</a> were still being
manufactured as late as the 1970s. Meanwhile, <a href="https://en.wikipedia.org/wiki/Turing_machine">Turing</a>, <a href="https://en.wikipedia.org/wiki/Information_theory">Shannon</a>,
<a href="https://en.wikipedia.org/wiki/Lambda_calculus">Church</a>, <a href="https://en.wikipedia.org/wiki/General_recursive_function">Gödel</a>, <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">von Neumann</a>, and <a href="https://en.wikipedia.org/wiki/History_of_computer_science">many others</a> were
putting computer science on a <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis">firm theoretical foundation</a>.</p>

<p>With the advent of the <a href="https://en.wikipedia.org/wiki/Transistor">transistor</a>, <a href="https://en.wikipedia.org/wiki/CMOS">CMOS</a> and
<a href="https://en.wikipedia.org/wiki/Photolithography">photolithography</a>, it was clear that digital was the way to go. Today,
electronic computers are a billion (and, in some special cases like GPUs, a
trillion) times faster than their mechanical counterparts, while also being
cheaper and more reliable.</p>

<p>Despite this, not much has changed: we still use lookup tables, polynomial
approximation, and logarithms as needed to speed up calculations; the only
difference is we’ve pushed these down into libraries where we hardly ever
have to think about them. This is undoubtedly a good thing:</p>

<blockquote>
<p>“Civilization advances by extending the number of important operations which
we can perform without thinking of them.”
</p>
</blockquote>

<p>But if we stand on the shoulders of giants today, it is only because
generations of humans sat by smoky campfires scratching lessons onto bones to
teach their children how to count beyond their number of fingers, or stayed up
late into the night to measure the exact positions of stars, or tried to teach
a box of rods and wheels how to multiply.</p>

<p>Archimedes once said, “Give me a place to stand and a lever long enough, and I
will move the Earth.” It occurs to me that the computer is an awfully long
lever, except what it multiplies is not mechanical force, but thought itself. I
wonder what we will move with it.</p>

<hr/>



<p>Here are a few of the books I consulted while working on this article and which
you may find interesting if you’d like to learn more about these topics:</p>

<p><a href="https://www.amazon.com/Reckoning-Matter-Calculating-Innovation-2016-11-29/dp/B01NGZURDX" target="_blank">
        <img src="https://www.oranlooney.com/post/history-of-computing_files/reckoning_with_matter.jpg" alt="Reckoning With Matter"/>
    </a>
    <a href="https://www.amazon.com/Daring-Invention-Logarithm-Tables-simplified/dp/0999140205" target="_blank">
        <img src="https://www.oranlooney.com/post/history-of-computing_files/daring_invention_logarithm_tables.jpg" alt="The Daring Invention of Logarithm Tables"/>
    </a>
    <a href="https://www.amazon.com/Notebook-History-Thinking-Paper-ebook/dp/B0D3R76WBZ" target="_blank">
        <img src="https://www.oranlooney.com/post/history-of-computing_files/notebook_history_thinking_matter.jpg" alt="The Notebook: A History of Thinking on Paper"/>
    </a>
    <a href="https://www.amazon.com/Little-History-Mathematics-Histories/dp/0300273738" target="_blank">
        <img src="https://www.oranlooney.com/post/history-of-computing_files/little_history_mathematics.jpg" alt="A Little History of Mathematics"/>
    </a>
</p>

<p>Note: I am not an Amazon affiliate, and I do not earn any commission from the links above.</p>

<h2 id="appendix-b-ia-and-the-extended-mind-hypothesis">Appendix B: IA and the Extended Mind Hypothesis</h2>

<p>Implicit in the above is the idea that external devices, even those as simple
as a clay tablet or a hinge, can be used to overcome our inherent limitations
and augment our intelligence. This is sometimes called <a href="https://en.wikipedia.org/wiki/Intelligence_amplification">Intelligence
Amplification</a> (IA) in deliberate contrast to AI.</p>

<p>Here are a few of the seminal works in the genre:</p>

<ul>
<li><a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/"><em>As We May Think</em></a> (Vannevar Bush, 1945)</li>
<li><a href="https://archive.org/details/introductiontocy00ashb"><em>Introduction to Cybernetics</em></a> (W. Ross Ashby, 1956)</li>
<li><a href="https://groups.csail.mit.edu/medg/people/psz/Licklider.html"><em>Man-Computer Symbiosis</em></a> (J. C. R. Licklider, 1960)</li>
<li><a href="https://www.dougengelbart.org/pubs/augment-3906.html"><em>Augmenting Human Intellect</em></a> (Douglas Engelbart, 1962)</li>
<li><a href="https://www.alice.id.tue.nl/references/clark-chalmers-1998.pdf"><em>The Extended Mind</em></a> (Clark &amp; Chalmers, 1998)</li>
</ul>

<p>Note that none of these require anything as invasive as a <a href="https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface">brain-computer
interface</a> but instead explore the implications of systems comprised of
both a human and a computer.</p>

<p>While all these works are basically futurist in character, the concept also
provides a very useful lens when looking back on the early history of
computing.</p>

    </article></div>
  </body>
</html>
