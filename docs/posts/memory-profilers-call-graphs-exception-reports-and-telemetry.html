<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nuanced.dev/blog/system-wide-context">Original</a>
    <h1>Memory profilers, call graphs, exception reports, and telemetry</h1>
    
    <div id="readability-page-1" class="page"><div><nav><a href="https://www.nuanced.dev/">Nuanced</a></nav><section><p>February 3, 2025</p><article><p>To meaningfully solve problems, developers aggregate context from several disparate sources that live both within and outside code. This context is often scattered across a daunting array of tools that developers navigate daily: from the AWS Console&#39;s labyrinth of services to Kubernetes&#39; multi-layered configs, Datadog&#39;s dense metric displays to distributed traces spanning dozens of services. Each tool demands its own expertise, creating constant context-switching that interrupts flow and drains productivity [<a target="_blank" rel="noopener noreferrer" href="https://survey.stackoverflow.co/2024/professional-developers#developer-experience-frustration">1</a>].</p>
<p>Enter AI coding assistants: they are getting <em>really</em> good at understanding and generating code, but can lack crucial context about the broader system environment. This missing context doesn’t necessarily lead to hallucinations (although it might!) but rather, it does mean developers spend more time helping AI understand the full picture—manually bridging the gap between code and the several information sources that make up the complex system it lives within.</p>

<p>Consider a typical debugging scenario: a developer notices production alerts about timeouts. What seems like a simple query issue quickly reveals a complex system-wide problem spanning multiple services:</p>
<ul>
<li>Exception monitoring shows intermittent timeouts</li>
<li>Server logs reveal patterns in affected requests</li>
<li>Infrastructure metrics show resource constraints</li>
<li>Performance dashboards indicate systemic bottlenecks</li>
<li>Recent deployments suggest potential triggers</li>
<li>Architecture diagrams expose impacted dependencies</li>
</ul>
<p>Here&#39;s the challenge: even &#34;simple&#34; timeouts require context from half a dozen different systems. Traditional AI coding assistants, limited to analyzing code files, can&#39;t synthesize this crucial operational context.</p>

<p>We decided to test what happens when we give AI coding assistants access to the same operational context developers use. We experimented with several types of system data:</p>
<ul>
<li>Call graphs generated from production trace data</li>
<li>Memory profiler outputs revealing resource bottlenecks</li>
<li>Exception reports from monitoring systems</li>
<li>Telemetry information showing actual usage patterns</li>
</ul>
<p>Our goal was to test how different types of context affect debugging accuracy when given to AI coding assistants.</p>
<h3 id="experiment-1-giving-call-graph-information-to-aider"><a href="#experiment-1-giving-call-graph-information-to-aider"></a>Experiment 1: Giving call graph information to Aider</h3>
<p>In our first experiment, we tasked Aider with fixing a reproducible bug in the Polars library (<a target="_blank" rel="noopener noreferrer" href="https://github.com/pola-rs/polars/issues/20042">#20042</a>). We provided two key inputs: the relevant test files, and function call graphs generated using pycg.</p>
<p>The results were intriguing: the function call graph transformed Aider&#39;s navigation from vague requests for &#34;DataFrame initialization code&#34; to the precise identification of the relevant <code><span><span>sequence_to_pydf</span></span></code> function in the relevant <code><span><span>polars</span><span>/</span><span>_utils</span><span>/</span><span>construction</span><span>.</span><span>py</span></span></code> file.</p>
<p>However, we also discovered clear limitations. When we provided the 14K line call graph of a file containing the dependent function, it overwhelmed the context window and made LLM queries impossible. This highlighted a crucial insight: context must be not just relevant, but efficiently represented to be useful for AI tools.</p>
<h3 id="experiment-2-datadog-dashboards"><a href="#experiment-2-datadog-dashboards"></a>Experiment 2: Datadog dashboards</h3>
<p>We built a proof of concept that captured Datadog dashboard screenshots using headless Chrome, fed these visualizations to AI coding assistants, and asked the AI to analyze specific metrics and trends.</p>
<p>While capturing dashboard screenshots seems straightforward, we encountered several technical hurdles:</p>
<ol>
<li>We had to determine when dashboards were fully rendered, and thus detect page loads, which isn&#39;t trivial. We&#39;d need to (a) wait for specific DOM elements, (b) verify all graphs have rendered, and (c) ensure data had finished loading.</li>
<li>Dashboards often extend beyond a single screen, so we had to handle these to get broader coverage, requiring programmatic scrolling, multiple screenshots, or stitching screenshots.</li>
<li>We had to convert visual metrics into actionable insights, which proved challenging.</li>
</ol>
<img src="https://www.nuanced.dev/images/datadog.png" alt="Datadog screenshot"/>
<p>What we learned is that LLM analysis of dashboard metrics was overly verbose, focused on surface-level observations, and missed critical correlations and patterns. This experiment revealed that effective metrics analysis requires the ability to navigate interactive, time-based data visualizations</p>
<h3 id="experiment-3-memory-profiling-outputs"><a href="#experiment-3-memory-profiling-outputs"></a>Experiment 3: Memory profiling outputs</h3>
<p>We tested Aider&#39;s ability to interpret memory profiling data by providing it with formatted output from Python&#39;s <code><span><span>memory_profiler</span></span></code> tool. The profiler output was structured in a table format showing line numbers, total memory usage, memory increment per line, occurrence count, and the actual code contents. When asked to identify memory-intensive code, Aider correctly analyzed this tabular data, identifying the line number that caused the peak memory allocation of 152.625 MiB. It also properly contextualized this against smaller allocations, like the 7.676 MiB used by the list creation in another line. The AI coding assistant was able to track the memory usage pattern through the program&#39;s execution, including the subsequent memory release of -152.488 MiB when the list was deleted.</p>
<p>This suggests that AI assistants can effectively interpret structured performance data when provided in their context window, opening possibilities for memory optimization assistance.</p>
<h3 id="experiment-4-exception-reports"><a href="#experiment-4-exception-reports"></a>Experiment 4: Exception reports</h3>
<p>We built a tool called to test how exception report context affects AI coding assistant performance. The tool ingests exception reports via Sentry&#39;s JSON API that coding assistants can understand and reason about. To evaluate its effectiveness, we used a real-world example: a complex database timeout issue in an academic citation system.</p>
<p>We provided Aider with a JSON Sentry error report for the issue. When given just the error report, Aider was able to extract basic information: the file location, error type, and timing. However, when we added the source file to the context, Aider provided significantly deeper analysis, identifying the specific recursive SQL queries likely causing the timeout and explaining how their 50,000-row limits interact with complex citation networks.</p>
<p>This experiment demonstrates how combining structured error reports with source code enables AI tools to perform more sophisticated debugging, moving from simple error reporting to understanding systemic issues and their root causes.</p>

<p>Today, we&#39;re starting with a focused foundation: a code intelligence graph enhanced with static analysis annotations. This gives AI assistants basic structural understanding of your codebase through dependencies, types, and control flow.</p>
<p>But our vision extends far beyond static analysis. We&#39;re building towards a rich knowledge graph that will integrate:</p>
<ul>
<li>Production behavior from logs and error reports</li>
<li>System performance metrics and resource usage</li>
<li>Service interaction patterns</li>
<li>Deployment and configuration context</li>
</ul>
<p>By transforming this operational data into structured context for AI assistants, we&#39;ll enable them to reason about your system holistically—understanding not just what the code says, but how it actually behaves in production.</p>

<p>We believe the future of AI development lies in better context, not just bigger models. We&#39;re starting with Python support and gradually expanding our capabilities. If you&#39;re interested in following our progress or getting early access, join our <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/forms/d/e/1FAIpQLSdCp4orKH21bCsGMrJMBRiSMzyOX2eZk1rOem7CMvOqVOvVcw/viewform?usp=send_form">waitlist</a>, or email me, the founder, at ayman@nuanced.dev.</p></article></section><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>
  </body>
</html>
