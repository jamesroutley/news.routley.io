<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mentaleap.ai/doublespeak/">Original</a>
    <h1>Doublespeak: In-Context Representation Hijacking</h1>
    
    <div id="readability-page-1" class="page"><div>
            <section id="abstract">
                <h2>Abstract</h2>
                <p>We introduce <strong>Doublespeak</strong>, a novel and simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., <span>bomb</span>) with a benign token (e.g., <span>carrot</span>) across multiple in-context examples, provided as a prefix to a harmful request.</p>
                
                <p>We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., &#34;How to build a carrot?&#34;) are internally interpreted as disallowed instructions (&#34;How to build a bomb?&#34;), thereby bypassing the model&#39;s safety alignment.</p>
            </section>

            <section>
                <h2>How It Works</h2>
                <div>
                    <div><p>
                        Our attacks works in three simple steps: </p></div>
                    <hr/><p>

                    By analyzing the internal reprsentation of the substitute word, we can see that in early layers it is interpreted by the model as benign, and at the last layers as its malicious target meaning. The LLM refusal mechanism fails to detect the malicious intent and a harmful response is being generated.
                    </p><center><img src="https://mentaleap.ai/doublespeak/patchscopes_analysis-1.png" width="600px" height="100%"/></center>
            </div></section>

            <section>
                <h2>Key Results</h2>
                <div>
                    <div>
                        <p>74%</p>
                        <p>ASR on Llama-3.3-70B-Instruct</p>
                    </div>
                    
                    <div>
                        <p>88%</p>
                        <p>ASR on Llama-3-8B-Instruct</p>
                    </div>
                    
                </div>
            </section>

            <section>
                <h2>Why This Matters</h2>
                <div>
                    <ul>
                        <li><strong>New Attack Surface:</strong> First jailbreak that hijacks in-context representations rather than surface tokens</li>
                        <li><strong>Layer-by-Layer Hijacking:</strong> Benign meanings in early layers converge to harmful semantics in later ones</li>
                        <li><strong>Bypasses Current Defenses:</strong> Safety mechanisms check tokens at input layer, but semantic shift happens progressively</li>
                        <li><strong>Broadly Transferable:</strong> Works across model families without optimization</li>
                        <li><strong>Production Models Affected:</strong> Successfully tested on GPT-4o, Claude, Gemini, and more</li>
                    </ul>
                </div>
            </section>

            <section>
                <h2>Mechanistic Analysis</h2>
                <p>Using interpretability tools (Logit Lens and Patchscopes), we provide detailed evidence of semantic hijacking:</p>
                
                <div>
                    <p><strong>Finding 1:</strong> Early layers maintain benign interpretation</p>
                    <p><strong>Finding 2:</strong> Middle-to-late layers show harmful semantic convergence</p>
                    <p><strong>Finding 3:</strong> Refusal mechanisms operate in early layers (Layer 12 in Llama-3-8B) before hijacking takes effect</p>
                    <p><strong>Finding 4:</strong> Attack demonstrates surgical precisionâ€”only target token is affected</p>
                </div>
            </section>

            <section>
                <h2>Implications</h2>
                <p>Our work reveals a critical blind spot in current LLM safety mechanisms. Current approaches:</p>
                <ul>
                    <li>Inspect tokens at the input layer</li>
                    <li>Trigger refusal if harmful keywords detected</li>
                    <li>Assume semantic stability throughout forward pass</li>
                </ul>
                
                <p><strong>Doublespeak shows this is insufficient.</strong> Robust alignment requires continuous semantic monitoring throughout the entire forward pass, not just at the input layer.</p>
            </section>
        </div></div>
  </body>
</html>
