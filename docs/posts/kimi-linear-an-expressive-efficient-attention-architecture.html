<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/MoonshotAI/Kimi-Linear">Original</a>
    <h1>Kimi Linear: An Expressive, Efficient Attention Architecture</h1>
    
    <div id="readability-page-1" class="page"><p dir="auto">Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including long,, short,  and reinforcement learning (RL) scaling regimes.
At it&#39;s core is Kimi Delta Attention (KDA)â€”a refined version of <a href="https://arxiv.org/abs/2412.06464" rel="nofollow">Gated DeltaNet</a> that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.</p><p dir="auto">Kimi Linear achieves performance, superior  and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up 75%, to  and boosts decoding throughput by up to <math-renderer data-run-id="86ae22bf997d4eb5e33ee3424653c6c3">$6\times$</math-renderer> for context as long as 1M tokens.</p><p dir="auto">We open-sourced the KDA kernel <a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda">FLA</a>,, in  and released two versions model checkpoints trained with 5.7T tokens.</p><div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &#34;moonshotai/Kimi-Linear-48B-A3B-Instruct&#34;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&#34;auto&#34;,
    device_map=&#34;auto&#34;,
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

messages = [
    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant provided by Moonshot-AI.&#34;},
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Is 123 a prime?&#34;}
]
input_ids = tokenizer.apply_chat_template(
    messages, 
    add_generation_prompt=True, 
    return_tensors=&#34;pt&#34;
).to(model.device)
generated_ids = model.generate(inputs=input_ids, max_new_tokens=500)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model_name</span> <span>=</span> <span>&#34;moonshotai/Kimi-Linear-48B-A3B-Instruct&#34;</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(
    <span>model_name</span>,
    <span>torch_dtype</span><span>=</span><span>&#34;auto&#34;</span>,
    <span>device_map</span><span>=</span><span>&#34;auto&#34;</span>,
    <span>trust_remote_code</span><span>=</span><span>True</span>
)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)

<span>messages</span> <span>=</span> [
    {<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;You are a helpful assistant provided by Moonshot-AI.&#34;</span>},
    {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Is 123 a prime?&#34;</span>}
]
<span>input_ids</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(
    <span>messages</span>, 
    <span>add_generation_prompt</span><span>=</span><span>True</span>, 
    <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>
).<span>to</span>(<span>model</span>.<span>device</span>)
<span>generated_ids</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span><span>=</span><span>input_ids</span>, <span>max_new_tokens</span><span>=</span><span>500</span>)
<span>response</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>generated_ids</span>)[<span>0</span>]
<span>print</span>(<span>response</span>)</pre></div><p dir="auto">For deployment, you can use the latest vllm to create an OpenAI-compatible API endpoint.</p><div dir="auto" data-snippet-clipboard-copy-content="@misc{team2025kimi,
    title         = {Kimi Linear: An Expressive, Efficient Attention Architecture},
    author        = {Zhang, Yu  and Lin, Zongyu  and Yao, Xingcheng  and Hu, Jiaxi  and Meng, Fanqing  and Liu, Chengyin  and Men, Xin  and Yang, Songlin  and Li, Zhiyuan  and Li, Wentao  and Lu, Enzhe  and Liu, Weizhou  and Chen, Yanru  and Xu, Weixin  and Yu, Longhui  and Wang, Yejie  and Fan, Yu  and Zhong, Longguang  and Yuan, Enming  and Zhang, Dehao  and Zhang, Yizhi  and T. Liu, Y.  and Wang, Haiming  and Fang, Shengjun  and He, Weiran  and Liu, Shaowei  and Li, Yiwei  and Su, Jianlin  and Qiu, Jiezhong  and Pang, Bo  and Yan, Junjie  and Jiang, Zhejun  and Huang, Weixiao  and Yin, Bohong  and You, Jiacheng  and Wei, Chu  and Wang, Zhengtao  and Hong, Chao  and Chen, Yutian  and Chen, Guanduo  and Wang, Yucheng  and Zheng, Huabin  and Wang, Feng  and Liu, Yibo  and Dong, Mengnan  and Zhang, Zheng  and Pan, Siyuan  and Wu, Wenhao  and Wu, Yuhao  and Guan, Longyu  and Tao, Jiawen  and Fu, Guohong  and Xu, Xinran  and Wang, Yuzhi  and Lai, Guokun  and Wu, Yuxin  and Zhou, Xinyu  and Yang, Zhilin  and Du, Yulun},
    year          = {2025},
    eprint        = {2510.26692},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CL}
}"><pre><span>@misc</span>{<span>team2025kimi</span>,
    <span>title</span>         = <span><span>{</span>Kimi Linear: An Expressive, Efficient Attention Architecture<span>}</span></span>,
    author        = {Zhang, Yu  and Lin, Zongyu  and Yao, Xingcheng  and Hu, Jiaxi  and Meng, Fanqing  and Liu, Chengyin  and Men, Xin  and Yang, Songlin  and Li, Zhiyuan  and Li, Wentao  and Lu, Enzhe  and Liu, Weizhou  and Chen, Yanru  and Xu, Weixin  and Yu, Longhui  and Wang, Yejie  and Fan, Yu  and Zhong, Longguang  and Yuan, Enming  and Zhang, Dehao  and Zhang, Yizhi  and T. Liu, Y.  and Wang, Haiming  and Fang, Shengjun  and He, Weiran  and Liu, Shaowei  and Li, Yiwei  and Su, Jianlin  and Qiu, Jiezhong  and Pang, Bo  and Yan, Junjie  and Jiang, Zhejun  and Huang, Weixiao  and Yin, Bohong  and You, Jiacheng  and Wei, Chu  and Wang, Zhengtao  and Hong, Chao  and Chen, Yutian  and Chen, Guanduo  and Wang, Yucheng  and Zheng, Huabin  and Wang, Feng  and Liu, Yibo  and Dong, Mengnan  and Zhang, Zheng  and Pan, Siyuan  and Wu, Wenhao  and Wu, Yuhao  and Guan, Longyu  and Tao, Jiawen  and Fu, Guohong  and Xu, Xinran  and Wang, Yuzhi  and Lai, Guokun  and Wu, Yuxin  and Zhou, Xinyu  and Yang, Zhilin  and Du, Yulun},
    <span>year</span>          = <span><span>{</span>2025<span>}</span></span>,
    <span>eprint</span>        = <span><span>{</span>2510.26692<span>}</span></span>,
    <span>archivePrefix</span> = <span><span>{</span>arXiv<span>}</span></span>,
    <span>primaryClass</span>  = <span><span>{</span>cs.CL<span>}</span></span>
}</pre></div></div>
  </body>
</html>
