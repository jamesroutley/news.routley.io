<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/MoonshotAI/Kimi-Linear">Original</a>
    <h1>Kimi Linear: An Expressive, Efficient Attention Architecture</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a href="https://github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf"><img width="80%" src="https://github.com/MoonshotAI/Kimi-Linear/raw/master/figures/banner.png"/></a>
</p>

<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/MoonshotAI/Kimi-Linear/blob/master/figures/perf_speed.png"><img width="90%" src="https://github.com/MoonshotAI/Kimi-Linear/raw/master/figures/perf_speed.png"/></a></p><p dir="auto"><em><b>(a)</b> On MMLU-Pro (4k context length), Kimi Linear achieves 51.0 performance with similar speed as full attention. On RULER (128k context length), it shows Pareto-optimal performance (84.3) and a 3.98x speedup. <b>(b)</b> Kimi Linear achieves 6.3x faster TPOT compared to MLA, offering significant speedups at long sequence lengths (1M tokens).</em></p>
</div>

<p dir="auto">Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regimes.
At it&#39;s core is Kimi Delta Attention (KDA)â€”a refined version of <a href="https://arxiv.org/abs/2412.06464" rel="nofollow">Gated DeltaNet</a> that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.</p>
<p dir="auto">Kimi Linear achieves superior performance and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up to 75% and boosts decoding throughput by up to <math-renderer data-run-id="f25158f9fd31eccbf36457b4050ca2f6">$6\times$</math-renderer> for context as long as 1M tokens.</p>
<p dir="auto">We open-sourced the KDA kernel in <a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda">FLA</a>, and released two versions model checkpoints trained with 5.7T tokens.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>#Total Params</strong></th>
<th><strong>#Activated Params</strong></th>
<th><strong>Context Length</strong></th>
<th><strong>Download Link</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Kimi-Linear-Base</td>
<td>48B</td>
<td>3B</td>
<td>1M</td>
<td><a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Base" rel="nofollow">ðŸ¤— Hugging Face</a></td>
</tr>
<tr>
<td>Kimi-Linear-Instruct</td>
<td>48B</td>
<td>3B</td>
<td>1M</td>
<td><a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct" rel="nofollow">ðŸ¤— Hugging Face</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li>
<strong>Kimi Delta Attention (KDA):</strong> A linear attention mechanism that refines the gated delta rule with finegrained gating.</li>
<li>
<strong>Hybrid Architecture:</strong> A 3:1 KDA-to-global MLA ratio reduces memory usage while maintaining or surpassing the quality of full attention.</li>
<li>
<strong>Superior Performance:</strong> Outperforms full attention in a variety of tasks, including long-context and RL-style benchmarks on 1.4T token training runs with fair comparisons.</li>
<li>
<strong>High Throughput:</strong> Achieves up to <math-renderer data-run-id="f25158f9fd31eccbf36457b4050ca2f6">$6\times$</math-renderer> faster decoding and significantly reduces time per output token (TPOT).</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/MoonshotAI/Kimi-Linear/blob/master/figures/arch.png"><img width="60%" src="https://github.com/MoonshotAI/Kimi-Linear/raw/master/figures/arch.png"/></a>
</p>

<div dir="auto"><h3 tabindex="-1" dir="auto">Inference with Hugging Face Transformers</h3><a id="user-content-inference-with-hugging-face-transformers" aria-label="Permalink: Inference with Hugging Face Transformers" href="#inference-with-hugging-face-transformers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To use the Kimi Linear model, we recommend the following:</p>
<ul dir="auto">
<li>Language: <code>python</code> &gt;= 3.10</li>
<li>Package: <code>torch</code> &gt;= 2.6</li>
<li>Package: <code>fla-core</code> &gt;= 0.4.0</li>
</ul>

<p dir="auto">Example Code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &#34;moonshotai/Kimi-Linear-48B-A3B-Instruct&#34;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&#34;auto&#34;,
    device_map=&#34;auto&#34;,
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

messages = [
    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant provided by Moonshot-AI.&#34;},
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Is 123 a prime?&#34;}
]
input_ids = tokenizer.apply_chat_template(
    messages, 
    add_generation_prompt=True, 
    return_tensors=&#34;pt&#34;
).to(model.device)
generated_ids = model.generate(inputs=input_ids, max_new_tokens=500)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model_name</span> <span>=</span> <span>&#34;moonshotai/Kimi-Linear-48B-A3B-Instruct&#34;</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(
    <span>model_name</span>,
    <span>torch_dtype</span><span>=</span><span>&#34;auto&#34;</span>,
    <span>device_map</span><span>=</span><span>&#34;auto&#34;</span>,
    <span>trust_remote_code</span><span>=</span><span>True</span>
)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)

<span>messages</span> <span>=</span> [
    {<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;You are a helpful assistant provided by Moonshot-AI.&#34;</span>},
    {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Is 123 a prime?&#34;</span>}
]
<span>input_ids</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(
    <span>messages</span>, 
    <span>add_generation_prompt</span><span>=</span><span>True</span>, 
    <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>
).<span>to</span>(<span>model</span>.<span>device</span>)
<span>generated_ids</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span><span>=</span><span>input_ids</span>, <span>max_new_tokens</span><span>=</span><span>500</span>)
<span>response</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>generated_ids</span>)[<span>0</span>]
<span>print</span>(<span>response</span>)</pre></div>

<p dir="auto">For deployment, you can use the latest vllm to create an OpenAI-compatible API endpoint.</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve moonshotai/Kimi-Linear-48B-A3B-Instruct \
  --port 8000 \
  --tensor-parallel-size 4 \
  --max-model-len 1048576 \
  --trust-remote-code"><pre>vllm serve moonshotai/Kimi-Linear-48B-A3B-Instruct \
  --port 8000 \
  --tensor-parallel-size 4 \
  --max-model-len 1048576 \
  --trust-remote-code</pre></div>

<p dir="auto">If you found our work useful, please cite</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{kimi2025kda,
  title  = {Kimi Linear: An Expressive, Efficient Attention Architecture},
  author = {kimi Team},
  year   = {2025},
  url    = {https://github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf}
}"><pre><span>@article</span>{<span>kimi2025kda</span>,
  <span>title</span>  = <span><span>{</span>Kimi Linear: An Expressive, Efficient Attention Architecture<span>}</span></span>,
  <span>author</span> = <span><span>{</span>kimi Team<span>}</span></span>,
  <span>year</span>   = <span><span>{</span>2025<span>}</span></span>,
  <span>url</span>    = <span><span>{</span>https://github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
