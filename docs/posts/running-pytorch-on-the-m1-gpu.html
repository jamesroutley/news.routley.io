<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html">Original</a>
    <h1>Running PyTorch on the M1 GPU</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        




<div>
  

  <article>
    <p>Today, <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/">PyTorch officially introduced</a> GPU support for Apple‚Äôs ARM M1 chips. This is an exciting day for Mac users out there, so I spent a few minutes tonight trying it out in practice. In this short blog post, I will summarize my experience and thoughts with the M1 chip for deep learning tasks.</p>

<h2 id="my-m1-experience-so-far">My M1 Experience So Far</h2>

<p>Back at the beginning of 2021, I happily sold my loud and chunky 15-inch Intel MacBook Pro to buy a much cheaper M1 MacBook Air. It‚Äôs been a fantastic machine so far: it is silent, lightweight, super-fast, and has terrific battery life.</p>

<p>When I was writing my new book, I noticed that it didn‚Äôt only feel fast in everyday use, but it also sped up several computations. For example, preprocessing the IMDb movie dataset <a href="https://github.com/rasbt/machine-learning-book/blob/main/ch08/ch08.ipynb">took only 21 seconds</a> instead of <a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb">1 minute and 51 seconds</a> on my 2019 Intel MacBook Pro.</p>

<p>Similarly, all scikit-learn-related workflows were much faster on the M1 MacBook Air! I could even run small neural networks in PyTorch in a reasonable time (various multilayer perceptrons and convolutional neural networks for teaching purposes). I recall making a LeNet-5 runtime comparison between the M1 and a GeForce 1080Ti and finding similar speeds.</p>

<p>Even though the M1 MacBook is an amazing machine, it is really not feasible to train modern deep neural networks on it. It really can‚Äôt handle anything beyond LeNets. However, I should note that I compiled PyTorch myself back then, as an early adopter, and I could only utilize the M1 CPU in PyTorch.</p>

<h2 id="pytorch-m1-gpu-support">PyTorch M1 GPU Support</h2>

<p>Today, the PyTorch Team <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/">has finally announced M1 GPU support</a>, and I was excited to try it. Along with the announcement, their benchmark showed that the M1 GPU was about 8x faster than a CPU for training a VGG16. And it was about 21x faster for inference (evaluation). According to the fine print, they tested this on a Mac Studio with an M1 Ultra. I am assuming CPU here refers to the M1 Ultra CPU.</p>

<p>How do we install the PyTorch version with M1 GPU support? I expect the M1-GPU support to be included in the 1.12 release and recommend watching the <a href="https://github.com/pytorch/pytorch/releases">release list</a> for updates. But for now, we can install it from the latest nightly release:</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/pytorch-m1-gpu/m1-support.png" alt="m1 installation"/></p>

<p>(Screenshot from <a href="https://pytorch.org">pytorch.org</a>)</p>

<p>Personally, I recommend installing it as follows from the terminal:</p>

<div><div><pre><code><span>$ </span>conda create <span>-n</span> torch-nightly <span>python</span><span>=</span>3.8 

<span>$ </span>conda activate torch-nightly

<span>$ </span>pip <span>install</span> <span>--pre</span> torch torchvision torchaudio <span>--extra-index-url</span> https://download.pytorch.org/whl/nightly/cpu
</code></pre></div></div>

<p>Then, if you want to run PyTorch code on the GPU, use <code>torch.device(&#34;mps&#34;)</code> analogous to <code>torch.device(&#34;cuda&#34;)</code> on an Nvidia GPU.</p>

<h2 id="my-benchmarks">My Benchmarks</h2>

<p>Just out of curiosity, I wanted to try this myself and trained deep neural networks for one epoch on various hardware, including the 12-core Intel CPU of a beefy deep learning workstation and a MacBook Pro with an M1 Pro chip. I started with a multilayer perceptron with three fully connected layers (sizes 512, 256, and 64 hidden units).</p>

<p>Below are the results:</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/pytorch-m1-gpu/mlp-benchmark.png" alt="MLP M1 GPU Benchmark"/></p>

<p>For these fully-connected layers, it seems that the M1 processors are really fast in general! An interesting takeaway is also that the M1 / M1 Pro CPUs are much faster than the M1 / M1 Pro GPUs. But okay, this is a very small neural network, and MNIST is a very small dataset. It‚Äôs also hard to tell whether the speed advantages over the other machines is simply due to faster data loading capabilities because the MacBooks have a faster SSD than the the other machines.</p>

<p>So let‚Äôs try again on VGG16 with CIFAR-10 images rescaled to 224x224 pixels (typical ImageNet sizes for VGG16):</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/pytorch-m1-gpu/vgg-benchmark-training.png" alt="VGG16 M1 GPU Benchmark Training"/></p>

<p>(The M1 Max results were <a href="https://github.com/rasbt/machine-learning-notes/issues/3">kindly provided by a reader</a>.)</p>

<p>Compared to the M1 Pro CPU (second row from the top) and M1 Pro GPU (second row from the bottom), the M1 Pro GPU trains the network twice as fast. That‚Äôs at least something!</p>

<p>Next, let‚Äôs take a look at the inference speeds. Here, inference means evaluationg the model on the test sets:</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/pytorch-m1-gpu/vgg-benchmark-inference.png" alt="VGG16 M1 GPU Benchmark Inference"/></p>

<p>During inference, the speed advantage between M1 / M1 Pro CPU and GPUs is much more noticeable. For example, the M1 Pro  GPU is four times as fast (2.1 min) as the M1 Pro CPU (8.63 min).</p>

<p>Unfortunately, when it comes to neural network training ‚Äì something I was most curious about ‚Äì my MacBooks don‚Äôt play well with the M1-GPU-supported PyTorch version from the nightly release yet. They made sure that they used the GPUs fully during training, and maybe it requires an M1 Ultra to see the benefits.</p>

<p>Some other potential explanations for the  lower-than-expected performance:</p>

<ul>
  <li>
    <p>I noticed that the convolutional networks need much more RAM when running them on a CPU or M1 GPU (compared to a CUDA GPU), and there may be issues regarding swapping (at least for the regular M1 MacBook Air, which only has 16 Gb RAM). However, the M1 Pro MacBook Pro has 32 Gb RAM, and training <a href="https://twitter.com/rasbt/status/1527115038720512000?s=20&amp;t=k1374Sdramu2rFp0EQGkxA">the neural networks never exceeded 80% memory utilization</a>.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/DrCMcMaster/status/1527117188603461633?s=20&amp;t=k1374Sdramu2rFp0EQGkxA">As suggested</a>, not maxing out the batch size on the M1 GPU runs could be another explanation. However, for fairness, I ran all training runs with a batch size of 32 ‚Äì the 2080Ti and 1080Ti couldn‚Äôt handle more due to their limited 11Gb VRAM. <strong>Update:</strong> I repeated the M1 Pro GPU run with a batch size of 64, and it took 48.71 min. Slightly faster than the 59.74 min shown in the plot above with batch size 32.</p>
  </li>
</ul>

<p>Also, we should keep in mind that  this is an early release of a brand-new feature, and it might improve over time. The fact that it works and is under active development is pretty exciting by itself, though! üôå</p>

<p>If you want to run the code yourself, <a href="https://github.com/rasbt/machine-learning-notes/tree/main/benchmark/pytorch-m1-gpu">here is a link to the scripts</a>.</p>

<p>(<a href="https://twitter.com/rasbt/status/1527102613749125120?s=20&amp;t=gerAjN40WmmDglsNDFj5Mg">Please let me know</a> if you have any ideas on how to improve the GPU performance!)</p>

<h2 id="conclusions">Conclusions</h2>

<p>Is the MacBook with M1 GPU going to be my go-to for deep learning? A hard no. I don‚Äôt think we should think of laptops as our primary deep neural network training machines. That‚Äôs because</p>

<ul>
  <li>they have a high cost/performance ratio (compared to workstations);</li>
  <li>they can get relatively hot which surely isn‚Äôt healthy;</li>
  <li>if we utilize them under full load, they become pretty limited for other tasks that we usually want them to do.</li>
</ul>

<p>However, laptops are my go-to productivity machines. I use them for pretty much everything, including simple scientific computations and debugging complicated neural network code before running it on my workstation or cloud cluster.</p>

<p>My personal takeaway from this benchmark is that the recent PyTorch M1 GPU support finally works, and this is exciting! There may still be some kinks to be ironed out to work smoothly and faster than the CPUs, and I expect it to make laptops (ehm, MacBooks) significantly more attractive as productivity, prototyping, and debugging machines for deep learning. üéâ</p>

<p>PS: If you have any ideas on how to speed up the M1 GPU performance, please <a href="https://twitter.com/rasbt/status/1527102613749125120?s=20&amp;t=gerAjN40WmmDglsNDFj5Mg">reach out!</a> üôè.</p>


  </article>


  <!--<strong>Have feedback on this post? I would love to hear it. Let me know and send me a <a href="https://twitter.com/intent/tweet?text=. @rasbt http://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html">tweet</a> or <a href="http://sebastianraschka.com/email.html">email</a>.</strong>-->

</div>
      </div>
    </div></div>
  </body>
</html>
