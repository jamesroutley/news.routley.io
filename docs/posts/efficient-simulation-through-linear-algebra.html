<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mht.wtf/post/sparse-solves/">Original</a>
    <h1>Efficient simulation through linear algebra</h1>
    
    <div id="readability-page-1" class="page"><header>
    <h2><a href="https://mht.wtf">mht.wtf</a></h2>
    <i>A blog about computer science, programming, and whatnot.</i>
</header>



  
  <p>August 12, 2022</p>
  
  <p>I spent a lot of time working on a project in which physically plausible simulation of soft materials with pressure chambers was a key part,
and in doing so, we managed to improve a part of our simulation by a significant amount.
I was very happy with how this small part of the whole system turned out, and I’ve been wanting to share it for a while.</p>
<p>A fair warning though, we need to spend a little time setting up the context in order to see <em>why</em> this is a thing that can happen very naturally,
as opposed to a magic algebraic trick that we can pull out of a hat.</p>
<p>If you haven’t seen physically based simulations before, don’t worry too much about the details.
It helps if we can get on the same page regarding <em>why</em> we are even here in the first place, but the details of the context really doesn’t matter for the point I’m trying to get across.</p>
<p>If you <em>have</em> seen physically based simulations before, also don’t worry too much about the details.
There isn’t anything fancy going on here; no second order elements, no fancy time stepping, no dynamics, basically nothing that hasn’t been around for 20 years<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
The trick is somewhat fancy though, to me anyways.</p>
<h2 id="finite-elements">Finite-Elements</h2>
<p>We wanted to simulate the behavior of a soft material with a certain geometry when we inject pressurized air into it.
A simple way of doing so is by representing the geometry of the material with a <a href="https://wias-berlin.de/software/index.jsp?id=TetGen&amp;lang=1">tetrahedral mesh</a>,
and defining an energy that is a function of the deformation of those <a href="https://en.wikipedia.org/wiki/Tetrahedron">tetrahedra</a>, or “tets”.
The nodal positions of the mesh are our “degrees of freedom”: they are what we can move around, and the energy of the system is a function of those positions.
You can imagine an energy function for each tet similar to<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>energy</span>(nodes: [Node;<span> </span><span>4</span>])<span> </span>-&gt; <span>f64</span> {<span> </span><span>..</span>.<span> </span>}<span>
</span></span></span></code></pre></div><p>If you are given <em>any</em> nodal positions, you can compute an energy from it. For instance, if a tet was supposed to have 1 volume, but it is stretched out to have 2 volume, it
would make sense that it has a lot of energy, which it can “use”<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> to return to it’s preferred (“rest”) position, of having 1 volume again.
The energy function<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> defines exactly how much the tet would want to return to some other configuration when deformed.
By summing the energies for all the tets in the system, we get the total energy of the whole system.</p>
<p>You can also have other energies that adds into the whole system.
Since we are dealing with a pneumatic system, we assign pressure forces to the faces of our mesh that are adjacent to the pressure chamber,
such that the forces are proportional to the face area and the pressure.
If we know how much gas is in the chamber (this is one our our degrees of freedom), and we know the volume of the chamber, we can compute the pressure using the <a href="https://en.wikipedia.org/wiki/Ideal_gas_law">ideal gas law</a><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p>
<h3 id="finding-equilibrium">Finding Equilibrium</h3>
<p>Having only this energy function, we can compute the <em>forces</em> that act on the nodes in our system as the direction in which they would have to move to <em>decrease</em> that energy.
In other words, we let $$f = -\frac{\partial E}{\partial x}.$$
Note the minus sign: the gradient of a function is the direction in which it <em>increases</em> the most, and we would like it to <em>decrease</em>.
This is also where notation gets a little messy: the $x$ above represents the positions of all the nodes, so it is really a vector in $\mathbb R^{3n}$ for a 3 dimensional system of $n$ nodes.</p>
<p>For a single tet, we have 12 numbers, namely the $x$, $y$, and $z$ coordinate of the four vertices.
We can pretend that the energy function above reads</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>energy</span>(nodes: [<span>f64</span>;<span> </span><span>12</span>])<span> </span>-&gt; <span>f64</span> {<span> </span><span>..</span>.<span> </span>}<span>
</span></span></span></code></pre></div><p>With this, we see that $f_t$, the forces on a single tet, is also a vector of 12 numbers, which corresponds to the forces on the respective nodes in their respective coordinate, whichever way we flattened<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> it in the first place<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.
$$f_t \in\mathbb R^{12}$$</p>
<p>We can use this information to move the nodes in our system in order to decrease the global energy of the whole system:
loop over all tets, compute the forces from that tet to its four nodes, sum up the forces on all the nodes into one big vector $f\in \mathbb R^{3n}$, and move the vertices some amount $\eta &gt; 0$ in this direction:
$$x^{(i+1)} = x^{(i)} + \eta f.$$
This is called <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, and it’s not so great, at least not for these kinds of systems, because it takes a long time before it finds equilibrium.
When $f=0$ we have reached equilibrium, and we’re at rest.</p>
<h3 id="newtons-method">Newton’s Method</h3>
<p>To improve <a href="https://en.wikipedia.org/wiki/Convergent_series">convergence</a><sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> we can compute yet another derivative, namely
$$
\frac{\partial^2 E}{\partial x \partial x} =
\frac{\partial f}{\partial x},\qquad
\frac{\partial f_t}{\partial x_t}\in\mathbb R^{12\times 12}$$
Now we’ve got $12 \times 12 = 144$ numbers, for each tet! Similarly to what we did above, we can combine all of these smaller matrices to one giant matrix<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> that we’ll call the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a> $H\in\mathbb R^{3n \times 3n}$,
and perform <a href="https://en.wikipedia.org/wiki/Newton&#39;s_method">Newtons’s method</a>.</p>
<p>What we want to do with $H$ is find a direction $d$ such that $Hd = -f$ and then set our new node positions to be<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>
$$x^{(i+1)}=x^{(i)}+\eta d$$
Don’t panic if this jumps out of nowhere, because it kind of does.
Roughly speaking, what this means is that we pretend that our energy function is <a href="https://en.wikipedia.org/wiki/Quadratic_function">quadratic</a>, because
then this update will make us go straight to the minimum point, which in our case is force equilibrium.
If the function is <em>not</em> quadratic (and it probably isn’t), then we hope that we’ll get closer, and indeed, as long as we start “sufficiently close” to the minima, we will.</p>
<h2 id="linear-systems">Linear Systems</h2>
<p>How do we “solve” $Hd = -f$ when we know $H$ and $f$?
This is what we call a “linear system of equations”, and is a workhorse of scientific computation, geometry processing, computer graphics, and many related fields.
It is often written as the equation</p>
<p>$$Ax = b$$</p>
<p>or, if we choose dimensions of the variables (I chose 6 here) and write everything out explicitly:</p>
<p>$$
\begin{pmatrix}
a_{1,1} &amp; a_{1,2} &amp;  a_{1,3} &amp; a_{1,4} &amp;  a_{1,5} &amp; a_{1,6}\\
a_{2,1} &amp; a_{2,2} &amp;  a_{2,3} &amp; a_{2,4} &amp;  a_{2,5} &amp; a_{2,6}\\
a_{3,1} &amp; a_{3,2} &amp;  a_{3,3} &amp; a_{3,4} &amp;  a_{3,5} &amp; a_{3,6}\\
a_{4,1} &amp; a_{4,2} &amp;  a_{4,3} &amp; a_{4,4} &amp;  a_{4,5} &amp; a_{4,6}\\
a_{5,1} &amp; a_{5,2} &amp;  a_{5,3} &amp; a_{5,4} &amp;  a_{5,5} &amp; a_{5,6}\\
a_{6,1} &amp; a_{6,2} &amp;  a_{6,3} &amp; a_{6,4} &amp;  a_{6,5} &amp; a_{6,6}
\end{pmatrix}
\begin{pmatrix} x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6 \end{pmatrix}
=\begin{pmatrix} b_1\\ b_2\\ b_3\\ b_4\\ b_5\\ b_6\end{pmatrix}
$$</p>
<p>The operation we want to do is find the $x$ given $A$ and $b$.
That is, which $x$ (if any!) should I multiply $A$ with to get $b$?
Algebraically, we can simply write
$$x = A^{-1}b,$$
but this is very rarely done in practice because computing the inverse of a matrix is rather expensive<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>.
People have figured out that there are ways of finding $x$ without computing $A^{-1}$ explicitly, and it is this we mean by a <em>linear solve</em>.</p>
<p>For instance, in Julia we can use the <code>\</code> operator for linear solves. Observe:</p>
<div><pre tabindex="0"><code data-lang="julia"><span><span>julia<span>&gt;</span> A <span>=</span> rand(<span>6</span>,<span>6</span>) <span># Get a random 6x6 matrix (and hope it is full rank)</span>
</span></span><span><span><span>6</span><span>×</span><span>6</span> <span>Matrix</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>0.610793</span>     <span>0.0588659</span>  <span>0.90725</span>   <span>0.723158</span>  <span>0.480303</span>   <span>0.00631715</span>
</span></span><span><span> <span>0.10528</span>      <span>0.229984</span>   <span>0.536642</span>  <span>0.91345</span>   <span>0.650178</span>   <span>0.237762</span>
</span></span><span><span> <span>0.600606</span>     <span>0.24921</span>    <span>0.349393</span>  <span>0.626754</span>  <span>0.0971094</span>  <span>0.771216</span>
</span></span><span><span> <span>0.536192</span>     <span>0.0458314</span>  <span>0.541457</span>  <span>0.556307</span>  <span>0.132692</span>   <span>0.55307</span>
</span></span><span><span> <span>0.936709</span>     <span>0.215612</span>   <span>0.284619</span>  <span>0.304965</span>  <span>0.926599</span>   <span>0.719019</span>
</span></span><span><span> <span>0.000957923</span>  <span>0.852531</span>   <span>0.290136</span>  <span>0.151528</span>  <span>0.129307</span>   <span>0.0528658</span>
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> b <span>=</span> rand(<span>6</span>) <span># Get a random b</span>
</span></span><span><span><span>6</span><span>-</span>element <span>Vector</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>0.7716876359155332</span>
</span></span><span><span> <span>0.4285009788970344</span>
</span></span><span><span> <span>0.8110655185850537</span>
</span></span><span><span> <span>0.19638254649350662</span>
</span></span><span><span> <span>0.6621420580446692</span>
</span></span><span><span> <span>0.06633609289427767</span>
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> x <span>=</span> A <span>\</span> b
</span></span><span><span><span>6</span><span>-</span>element <span>Vector</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span>  <span>2.77721146947569</span>
</span></span><span><span>  <span>0.7894422416781481</span>
</span></span><span><span> <span>-</span><span>2.7841498287174837</span>
</span></span><span><span>  <span>2.5819747913641087</span>
</span></span><span><span> <span>-</span><span>0.6223503841138821</span>
</span></span><span><span> <span>-</span><span>2.1248845687489477</span>
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> A <span>*</span> x <span>-</span> b <span># If  Ax = b  then  Ax-b = 0</span>
</span></span><span><span><span>6</span><span>-</span>element <span>Vector</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>-</span><span>1.1102230246251565e-16</span>
</span></span><span><span>  <span>2.220446049250313e-16</span>
</span></span><span><span>  <span>0.0</span>
</span></span><span><span>  <span>5.551115123125783e-16</span>
</span></span><span><span>  <span>1.1102230246251565e-16</span>
</span></span><span><span>  <span>5.551115123125783e-17</span>
</span></span></code></pre></div><p>There are many things to be said about solving linear systems, but there’s only one more thing we’ll need to know here: sparsity.</p>
<h3 id="solving-sparse-linear-systems">Solving Sparse Linear Systems</h3>
<p>The picture below is the Hessian matrix $H$ of a one of these finite elements systems.
The pixel at position <code>i,j</code> correspond to $H_{ij}$, and it is color coded so that blue means negative, red means positive, and gray is zero.</p>
<p><img src="https://mht.wtf/post/sparse-solves/0804-hessian-reorder.png" alt="A Hessian texture where each pixel is color coded with the numeric value for its coordinates."/></p>
<p>The noteworthy thing about this picture is the amount of gray: <em>most</em> pixels are gray.
Since the Hessian quantifies how sensitive the <em>forces</em> on our nodes are to the <em>position</em> of the nodes themselves, this makes sense.
Moving around a node on one side of the mesh does not change anything about the forces on the other side.
That is, unless those nodes both are on the pressure boundary: in this case the volume is changed ever so slightly, which in turn changes the pressure,
which in changes the forces on <em>all</em> of the nodes that are on the pressure boundary.
These nodes correspond to the <em>block</em> we are seeing in the upper left corner or the picture<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>.</p>
<p>Recall from <a href="#linear-systems">above</a> that there are a bunch of methods for solving these systems, but, perhaps obviously, any one of these methods will for sure need to look at each element in the matrix.
If there are many elements in the matrix, there will be a lot of work; you can think of this as $O(n^2)$<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> where $n$ is the number of degrees of freedom we have (the number of rows and columns in $H$).
On the other hand, if most of the elements in $H$ are zero, we can store the matrix in a <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse format</a>, so that any algorithm working
on $H$ does not have to iterate over a whole lot of zeroes.
It will still need to look at each non-zero number, but if we only have a constant number $c$ of entries in each row (or column), we only have a total of $O(cn)$ entries in total.</p>
<p>The problem, of course, is that the matrix in the picture above isn’t really sparse, since it has this giant block of roughly $\frac{1}{4}n^2$ numbers in it.</p>
<p>… or is it?</p>
<h2 id="property-vs-representation">Property vs. Representation</h2>
<p>This brings us to the key of post.
It certainly looks like the matrix is dense, and in general, there is no way of making a dense matrix sparse, since there is simply more information in a dense matrix.
But maybe there is a lot of duplicate information in our matrix?
To show what I mean, consider the matrix
$$A = uv^\top\qquad\text{or equivalently }\qquad A_{i,j} = u_iv_j$$
or for some concrete numbers, consider this:</p>
<div><pre tabindex="0"><code data-lang="julia"><span><span>julia<span>&gt;</span> u, v <span>=</span> rand(<span>6</span>), rand(<span>6</span>);
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> u
</span></span><span><span><span>6</span><span>-</span>element <span>Vector</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>0.17648645508411875</span>
</span></span><span><span> <span>0.9501460722894218</span>
</span></span><span><span> <span>0.7570256767954698</span>
</span></span><span><span> <span>0.9097476055645976</span>
</span></span><span><span> <span>0.7514042466862265</span>
</span></span><span><span> <span>0.2594892833200104</span>
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> v
</span></span><span><span><span>6</span><span>-</span>element <span>Vector</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>0.9880351017724492</span>
</span></span><span><span> <span>0.7271356154478763</span>
</span></span><span><span> <span>0.29724548913210114</span>
</span></span><span><span> <span>0.7470357266014565</span>
</span></span><span><span> <span>0.8131233770317735</span>
</span></span><span><span> <span>0.26312703421677464</span>
</span></span><span><span>
</span></span><span><span>julia<span>&gt;</span> u <span>*</span> v<span>&#39;</span> <span># v&#39; is Julia&#39;s way of transposing</span>
</span></span><span><span><span>6</span><span>×</span><span>6</span> <span>Matrix</span>{<span>Float64</span>}<span>:</span>
</span></span><span><span> <span>0.174375</span>  <span>0.12833</span>   <span>0.0524598</span>  <span>0.131842</span>  <span>0.143505</span>  <span>0.0464384</span>
</span></span><span><span> <span>0.938778</span>  <span>0.690885</span>  <span>0.282427</span>   <span>0.709793</span>  <span>0.772586</span>  <span>0.250009</span>
</span></span><span><span> <span>0.747968</span>  <span>0.55046</span>   <span>0.225022</span>   <span>0.565525</span>  <span>0.615555</span>  <span>0.199194</span>
</span></span><span><span> <span>0.898863</span>  <span>0.66151</span>   <span>0.270418</span>   <span>0.679614</span>  <span>0.739737</span>  <span>0.239379</span>
</span></span><span><span> <span>0.742414</span>  <span>0.546373</span>  <span>0.223352</span>   <span>0.561326</span>  <span>0.610984</span>  <span>0.197715</span>
</span></span><span><span> <span>0.256385</span>  <span>0.188684</span>  <span>0.077132</span>   <span>0.193848</span>  <span>0.210997</span>  <span>0.0682786</span>
</span></span></code></pre></div><p>The matrix is a “full” matrix of 36 numbers, but they all come from only 12 numbers<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup>.
In a sense, the matrix <em>should</em> be sparse, because it’s only 12 numbers, but its <em>representation</em> is not sparse.
If we can rewrite our $H$ above into a form that looks like this, maybe there’s hope for speeding up the solves.</p>
<p>The way we compute pressure forces on the faces of the tets is first to compute the volume of the air chamber,
compute the pressure using the ideal gas law, and apply the pressure on each face so that the force is proportional to both the pressure and the face area, and in the direction of the inward normal of the face.
Roughly, following the notation I’ve used already, it looks like this<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup>:
$$f_p = p(x) n(x)$$
where both the pressure $p$ and the area scaled normal vector $n$ is a function of the node positions $x$.
When we compute the Hessian entries $H_p$ for only the pressure forces, we use the product rule to get
$$H_p=\frac{\partial f_p}{\partial x} = \frac{\partial p}{\partial x}(x)n(x) + p(x)\frac{\partial n}{\partial x}(x).$$
Writing it all out like this is useful since we can pinpoint exactly where in the formulas the density problem comes from.
The term $\partial p /\partial x$ is dense, since it depends on the volume of the air chamber, and all nodes along the boundary of this chamber influences the volume if they move<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup> .
But do note here that, similarly to the toy example above, we really only have $3n$ numbers in ${\partial p}/{\partial x}$, since
for given $x$, $p(x)$ is only a single number — the pressure — so $\frac{\partial p}{\partial x} \in \mathbb R^{3n}$ (because $x$ is $3n$ numbers).
Somehow this is expanded to $O(n^2)$ numbers in the process of assembly.</p>
<p>In fact, if we write $u=\partial p/\partial x$ and $v=n(x)$ then the first summand is just $uv^\top$.
We use this to rewrite the computation of $H$ by first doing the pressure computation separately, and then the rest of $H$:
$$H = H_p + H_r$$
($r$ for rest) and then write the pressure terms as
$$H_p = uv^\top + p(x)\frac{\partial n}{\partial x}(x)$$
and at last, we write the whole Hessian in a slightly more readable form as
$$H = H_s + uv^\top,\qquad H_s = H_r + p(x)\frac{\partial n}{\partial x}(x)$$
This system is still as dense as before if we multiply out $uv^\top$ and add it all together, but we’re not going to do that.</p>
<h2 id="solving-the-new-system">Solving The New System</h2>
<p>Before we had the system $Hd = -f$ which we wanted to solve for $d$. Now our new system is the slightly less nice
$$(H_s + uv^\top) d = -f$$
and it doesn’t seem like we’ve made much progress.</p>
<p>What helps us is the <a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman-Morrison formula</a>,
which tells us how to invert a matrix of type $A + uv^\top$;
see <a href="https://kristianeschenburg.github.io/2018/05/rank-one-updates">this</a> and <a href="https://timvieira.github.io/blog/post/2021/03/25/fast-rank-one-updates-to-matrix-inverse/">this</a> post on solving these systems.
The closed form solution includes inverting $A$ itself ($H_s$ in our case), but we can avoid computing this explicitly because we are not looking for the inverse of the matrix we have, we just want to solve the linear system.</p>
<p>For matrices that are easy to invert, the formula <em>is</em> useful for us; in particular, we choose $A=I$, and write out the inverse explicitly:
$${\left(I + uv^T\right)}^{-1} = I - \frac{uv^T}{1 + u^Tv}.$$
Again, this does not help us directly yet, because in our case we have $H_s$ as the matrix inside the parenthesis, and not $I$.
We will need to somehow massage it out.</p>
<p>The first step is to take our system
$$(H_s + uv^\top)d = -f$$
and algebraically multiply in $H_s^{-1}$ from the left so that we get
$$(I + H_s^{-1}uv^\top)d = -H_s^{-1}f.$$
Let’s call $H_s^{-1}u=w$, or in other words, $H_sw = u$. Since $H_s$ is sparse we can easily solve for $w$, and insert this back into the equation:
$$(I + wv^\top)d = -H_s^{-1}f.$$
Now we introduce a new variable, just to make this step easier: let $c = (I + wv^\top)d$. We haven’t found $c$ yet, and we still don’t know $d$, this too is just algebra.
We are left with
$$c = -H_s^{-1}f$$
or
$$H_s c = -f$$
in which only $c$ is unknown. $H_s$ is still sparse, so we can solve for $c$.
At last, we look at the definition of $c$ that we came up with. We have all quantities<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup> except for $d$:
$$(I + wv^\top) d = c$$
and we already have a analytical inverse for this matrix, thanks to Sherman-Morrison.
By inserting the inverse on the right and multiplying out (notice that we don’t even have to construct the matrix that is the SM inverse!) we get:
$$\begin{align}
d &amp;= {\left(I + wv^\top \right)}^{-1} c \\
&amp;= (I - \frac{wv^\top }{1 + w^\top v}) c\\
&amp;= c - \frac{w(v^\top c)}{1 + w^\top v}
\end{align}$$
which is just two dot product, a scalar-vector multiply, and a vector-vector subtraction.</p>
<p>That’s quite a mouthful, but in the end we have only solved two sparse linear system with the <em>same</em> matrix $H_s$, and done a few dot products at the end.
We avoided the dense solve, and in fact, we avoided even <em>constructing</em> a new matrix.</p>
<p>The fact that we used the same matrix on both of the linear solves is also really important: linear solvers usually factorize the matrix in some way or another before they solve the system,
for instance into an <a href="https://en.wikipedia.org/wiki/LU_decomposition">LU</a>, <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#LDL_decomposition">LDLT</a>, or <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR</a> factorization.
When we have the factorization we can very easily solve the system, and so by solving multiple linear systems
with the same matrix (and different $b$s) we only need to factorize once, so the second solve is really fast.</p>
<h2 id="quick-micro-benchmark">Quick Micro benchmark</h2>
<p>What does this really give us?
Instead of making a proper comparison from the simulation code base, I decided to hack together a small Julia program to illustrate.
Here is the measured data of solving what basically amounts to the linear system above.</p>
<table>
<thead>
<tr>
<th>$n$</th>
<th>slow</th>
<th>fast</th>
<th>speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>500</td>
<td>0.01742</td>
<td>0.01053</td>
<td>1.65</td>
</tr>
<tr>
<td>1000</td>
<td>0.03732</td>
<td>0.01933</td>
<td>1.93</td>
</tr>
<tr>
<td>2000</td>
<td>0.12346</td>
<td>0.06351</td>
<td>1.94</td>
</tr>
<tr>
<td>5000</td>
<td>0.96995</td>
<td>0.45724</td>
<td>2.12</td>
</tr>
<tr>
<td>10000</td>
<td>6.12298</td>
<td>2.22139</td>
<td>2.75</td>
</tr>
<tr>
<td>30000</td>
<td>131.273</td>
<td>39.2934</td>
<td>3.34</td>
</tr>
</tbody>
</table>
<p>The data is generated from the following Julia code:</p>
<div><pre tabindex="0"><code data-lang="julia"><span><span><span>using</span> LinearAlgebra
</span></span><span><span><span>using</span> SparseArrays
</span></span><span><span>
</span></span><span><span>mod1p(n, m) <span>=</span>  ((n <span>-</span> <span>1</span>) <span>%</span> m) <span>+</span> <span>1</span>
</span></span><span><span>
</span></span><span><span><span># Compute a random sparse matrix in which each column has at most `k` entries</span>
</span></span><span><span><span>function</span> randomsparse(n, k)
</span></span><span><span>    A <span>=</span> zeros(n, n)
</span></span><span><span>    <span>for</span> i<span>=</span><span>1</span><span>:</span>n
</span></span><span><span>        ixs <span>=</span> rand(<span>UInt32</span>, k) <span>.|&gt;</span> a<span>-&gt;</span>mod1p(a, n)
</span></span><span><span>        nums <span>=</span> rand(k)
</span></span><span><span>        A[ixs,i] <span>=</span> nums
</span></span><span><span>    <span>end</span>
</span></span><span><span>    sparse(A <span>+</span> I) <span># ensure we get a full rank</span>
</span></span><span><span><span>end</span>
</span></span><span><span>    
</span></span><span><span><span>function</span> doit(n)
</span></span><span><span>    A <span>=</span> randomsparse(n, <span>5</span>)
</span></span><span><span>    u <span>=</span> rand(n)
</span></span><span><span>    v <span>=</span> rand(n)
</span></span><span><span>    b <span>=</span> rand(n)
</span></span><span><span>
</span></span><span><span>    @time(<span>begin</span> <span># slow path</span>
</span></span><span><span>        slow <span>=</span> A <span>+</span> u <span>*</span> v<span>&#39;</span>
</span></span><span><span>        factor <span>=</span> factorize(slow)
</span></span><span><span>        x <span>=</span> factor <span>\</span> b
</span></span><span><span>    <span>end</span>);
</span></span><span><span>
</span></span><span><span>    @time(<span>begin</span> <span># fast path</span>
</span></span><span><span>        factor <span>=</span> factorize(A)
</span></span><span><span>        w <span>=</span> factor <span>\</span> u
</span></span><span><span>        c <span>=</span> factor <span>\</span> b
</span></span><span><span>        x <span>=</span> c <span>-</span> w <span>*</span> dot(v, c) <span>/</span> (<span>1</span> <span>+</span> dot(w, v))
</span></span><span><span>    <span>end</span>);
</span></span><span><span><span>end</span>
</span></span></code></pre></div><p>The code for the fast path <em>is</em> a little more complicated than the straight-forward slow path, but overall, not by a lot.
And the speedup we’re getting is well worth it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>One of the reasons for why I really like this solution is that it’s such a good example of good things happening because we looked closely at our problem.
We already knew that linear solves would be the majority of the time spent in our pipeline.
We also knew that sparse solves are quicker than dense solves.
We <em>also</em> knew that our system felt dense due to the dependence of all the nodes along the air chamber boundary.
<em>Despite</em> all of this, we managed to massage the problem we had from one dense solve into two sparse solves, and we got a significant speedup out of it.</p>
<p>This wouldn’t have happened if we were content with the fact that “Linear solves takes up the majority of time in Newton’s algorithm” (which is true; the linear solve <em>is</em> the bottleneck).</p>
<p>This wouldn’t have happened it we looked at the Hessian and concluded that “The system is dense, therefore the solve will be slow” (which is true; dense systems <em>are</em> slower to solve).</p>
<p>Sometimes there <em>are</em> better solutions, but they require that we look closely at the problem at hand. Without looking closely in the first place, we wouldn’t even have known that better solutions could exist.</p>
<p>Even though this example was full of math I really think the general sentiment translates well into programming, or completely different
aspects of life.
It is really hard to tell the difference between how something appears and how it really is<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup>.
I can’t illustrate this with an example from <em>your</em> life, but I hope that having made the distinction here, you might come up with one.</p>
<p>Comments, questions, pointers, and prefactorized matrices, can be sent to my <a href="mailto:https://lists.sr.ht/~mht/public-inbox">public inbox</a> (plain text email only).</p>
<p>Thanks for reading.</p>



  <p id="cc-box"> <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"/></a></p>




</div>
  </body>
</html>
