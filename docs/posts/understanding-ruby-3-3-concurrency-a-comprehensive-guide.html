<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.bestwebventures.in/understanding-ruby-concurrency-a-comprehensive-guide">Original</a>
    <h1>Understanding Ruby 3.3 Concurrency: A Comprehensive Guide</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content-parent"><div id="post-content-wrapper"><p><strong>Ruby 3.3, released with significant improvements in concurrency capabilities,</strong> marks a pivotal shift in how Ruby applications handle parallel processing and concurrent operations.</p>
<p>This advancement is particularly crucial as <mark>modern applications increasingly demand efficient handling of multiple tasks simultaneously</mark>, especially in domains like AI and ML.</p>
<p>This article explores Ruby 3&#39;s concurrency ecosystem, its various concurrency models, and their practical applications in modern software development.</p>
<h2 id="heading-ruby-concurrency-and-its-ecosystem">Ruby Concurrency and its Ecosystem</h2>
<p>Ruby&#39;s concurrency ecosystem has evolved significantly, offering developers multiple tools and abstractions to handle concurrent operations effectively.</p>
<p>The ecosystem now includes improved implementations of <mark>Threads</mark>, <mark>Fibers</mark>, and the revolutionary <mark>Ractor system</mark>, along with various libraries and frameworks that leverage these features.</p>
<h3 id="heading-key-components-of-rubys-concurrency-ecosystem">Key Components of Ruby&#39;s Concurrency Ecosystem:</h3>
<ul>
<li><p>Thread API for traditional multi-threading</p>
</li>
<li><p>Fiber API for lightweight concurrency</p>
</li>
<li><p>Ractor for parallel execution</p>
</li>
<li><p>AsyncIO libraries (async, async-io)</p>
</li>
<li><p>Concurrent Ruby gem</p>
</li>
<li><p>Ruby Queue implementation</p>
</li>
</ul>
<h2 id="heading-code-examples-demonstrating-rubys-concurrency-capabilities">Code Examples Demonstrating Ruby’s Concurrency Capabilities</h2>
<p>Let&#39;s look at two practical examples that demonstrate Ruby 3&#39;s concurrency capabilities:</p>
<h3 id="heading-example-1-concurrent-http-requests-using-thread-pool"><strong>Example 1: Concurrent HTTP Requests using Thread Pool</strong></h3>
<pre><code>
<span>require</span> <span>&#39;net/http&#39;</span>
<span>require</span> <span>&#39;concurrent-ruby&#39;</span>

<span><span>class</span> <span>WebCrawler</span></span>
  <span><span>def</span> <span>fetch_urls</span><span>(urls, <span>max_threads:</span> <span>5</span>)</span></span>
    pool = Concurrent::FixedThreadPool.new(max_threads)
    promises = urls.map <span>do</span> <span>|url|</span>
      Concurrent::Promise.execute(<span>executor:</span> pool) <span>do</span>
        fetch_url(url)
      <span>end</span>
    <span>end</span>

    results = promises.map(&amp;<span>:value!</span>)
    pool.shutdown
    results
  <span>end</span>

  private

  <span><span>def</span> <span>fetch_url</span><span>(url)</span></span>
    uri = URI(url)
    response = Net::HTTP.get_response(uri)
    { <span>url:</span> url, <span>status:</span> response.code, <span>body:</span> response.body }
  <span>rescue</span> =&gt; e
    { <span>url:</span> url, <span>error:</span> e.message }
  <span>end</span>
<span>end</span>


crawler = WebCrawler.new
urls = [<span>&#39;https://api1.example.com&#39;</span>, <span>&#39;https://api2.example.com&#39;</span>]
results = crawler.fetch_urls(urls)
</code></pre>
<p><strong>Explanation</strong>: This example demonstrates <strong>a thread pool-based web crawler</strong> that efficiently fetches multiple URLs concurrently. It uses the concurrent-ruby gem&#39;s FixedThreadPool to manage a limited number of threads, preventing resource exhaustion. The <strong>Promise class handles asynchronous operations and error handling</strong>, making it ideal for I/O-bound tasks like HTTP requests.</p>
<h3 id="heading-example-2-concurrent-data-processing-with-queues"><strong>Example 2: Concurrent Data Processing with Queues</strong></h3>
<pre><code>
<span>require</span> <span>&#39;thread&#39;</span>

<span><span>class</span> <span>DataProcessor</span></span>
  <span><span>def</span> <span>initialize</span><span>(<span>worker_count:</span> <span>4</span>)</span></span>
    @queue = Queue.new
    @results = Queue.new
    @workers = worker_count
  <span>end</span>

  <span><span>def</span> <span>process_data</span><span>(items)</span></span>
    start_workers
    enqueue_items(items)
    collect_results(items.size)
  <span>end</span>

  private

  <span><span>def</span> <span>start_workers</span></span>
    @worker_threads = @workers.times.map <span>do</span>
      Thread.new <span>do</span>
        <span>while</span> item = @queue.pop
          result = process_item(item)
          @results.push(result)
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>

  <span><span>def</span> <span>enqueue_items</span><span>(items)</span></span>
    items.each { <span>|item|</span> @queue.push(item) }
    @workers.times { @queue.push(<span>nil</span>) } 
  <span>end</span>

  <span><span>def</span> <span>collect_results</span><span>(expected_count)</span></span>
    results = []
    expected_count.times { results &lt;&lt; @results.pop }
    @worker_threads.each(&amp;<span>:join</span>)
    results
  <span>end</span>

  <span><span>def</span> <span>process_item</span><span>(item)</span></span>
    
    sleep(rand * <span>0</span>.<span>1</span>)
    { <span>item:</span> item, <span>processed:</span> <span>true</span> }
  <span>end</span>
<span>end</span>
</code></pre>
<p><strong>Explanation</strong>: This implementation showcases <strong>a thread-safe <mark>producer-consumer pattern</mark> using Ruby&#39;s Queue class</strong>. It creates a pool of worker threads that process items from an input queue and push results to an output queue. The pattern is particularly useful for processing large datasets where tasks can be broken down into smaller, independent units of work.</p>
<h2 id="heading-rubys-concurrency-vs-parallelism">Ruby’s Concurrency vs Parallelism</h2>
<p>While concurrency and parallelism are often used interchangeably, they represent different concepts in Ruby 3:</p>
<ul>
<li><p><strong>Concurrency</strong>: <mark>Managing multiple tasks</mark> and making progress on them over time</p>
</li>
<li><p><strong>Parallelism</strong>: <mark>Executing multiple tasks simultaneously</mark> on different processors</p>
</li>
</ul>
<p>Ruby 3 introduces better <strong>support for true parallelism through Ractors</strong>, while maintaining its existing concurrency mechanisms. Here are two examples demonstrating both concepts:</p>
<h3 id="heading-example-1-parallel-processing-with-ractors"><strong>Example 1: Parallel Processing with Ractors</strong></h3>
<pre><code><span><span>class</span> <span>ParallelProcessor</span></span>
  <span><span>def</span> <span>self</span>.<span>parallel_map</span><span>(array)</span></span>
    slice_size = (array.size / <span>4.0</span>).ceil
    ractors = array.each_slice(slice_size).map <span>do</span> <span>|slice|</span>
      Ractor.new(slice) <span>do</span> <span>|data|</span>
        data.map <span>do</span> <span>|n|</span>
          
          Math.sqrt(n ** <span>3</span>).round(<span>5</span>)
        <span>end</span>
      <span>end</span>
    <span>end</span>

    ractors.map(&amp;<span>:take</span>).flatten
  <span>end</span>
<span>end</span>


numbers = (<span>1</span>..<span>1000</span>).to_a
result = ParallelProcessor.parallel_map(numbers)

puts result
</code></pre>
<p><strong>Explanation</strong>: This <mark>example demonstrates true parallelism using </mark> <strong><mark>Ruby’s experimental Ractor feature</mark></strong>. It splits an array into chunks, with each chunk processed independently in parallel by separate Ractors. Each Ractor performs a CPU-intensive calculation, allowing for effective utilization of multiple CPU cores. <em>By embedding the computation logic directly in each Ractor block, <mark>this implementation avoids issues related to Proc isolation and unshareable objects</mark>, which can arise when working with Ractors.</em> This approach is especially beneficial for mathematical computations or other CPU-bound tasks that can be parallelized efficiently.</p>
<h3 id="heading-example-2-concurrent-vs-parallel-file-processing"><strong>Example 2: Concurrent vs Parallel File Processing</strong></h3>
<pre><code>
<span>require</span> <span>&#39;fileutils&#39;</span>

<span><span>class</span> <span>FileProcessor</span></span>
  <span><span>def</span> <span>process_files_concurrent</span><span>(files)</span></span>
    threads = files.map <span>do</span> <span>|file|</span>
      Thread.new <span>do</span>
        process_file(file)
      <span>end</span>
    <span>end</span>
    threads.map(&amp;<span>:value</span>)
  <span>end</span>

  <span><span>def</span> <span>process_files_parallel</span><span>(files)</span></span>
    ractors = files.map <span>do</span> <span>|file|</span>
      Ractor.new(file) <span>do</span> <span>|f|</span>
        process_file(f)
      <span>end</span>
    <span>end</span>
    ractors.map(&amp;<span>:take</span>)
  <span>end</span>

  private

  <span><span>def</span> <span>process_file</span><span>(file)</span></span>
    
    content = File.read(file)
    processed = content.upcase
    FileUtils.mkdir_p(<span>&#39;processed&#39;</span>)
    File.write(<span>&#34;processed/<span>#{File.basename(file)}</span>&#34;</span>, processed)
    { <span>file:</span> file, <span>status:</span> <span>&#39;processed&#39;</span> }
  <span>end</span>
<span>end</span>
</code></pre>
<p><strong>Explanation</strong>: This <strong>example contrasts concurrent and parallel approaches to file processing</strong>. The <mark>concurrent version uses threads</mark>, suitable for I/O-bound operations like file reading/writing, while the <mark>parallel version uses Ractors</mark> for true parallelism. It demonstrates how the same task can be implemented differently based on whether the bottleneck is I/O (threads) or CPU (Ractors).</p>
<h2 id="heading-ruby-threads-vs-fibers-vs-ractors">Ruby - Threads vs Fibers vs Ractors</h2>
<p><strong>Understanding the differences between Threads, Fibers, and Ractors is crucial for choosing the right concurrency primitive for your needs.</strong></p>
<p>Let&#39;s explore how these three mechanisms differ in their approach to concurrent programming and their ideal use cases.</p>
<h3 id="heading-understanding-the-core-differences">Understanding the Core Differences</h3>
<p>Ruby&#39;s concurrency story has evolved significantly with these three distinct mechanisms.</p>
<p><strong><mark>Threads</mark>, the traditional approach, operate within the same process and share memory space.</strong> They provide a familiar concurrency model but are limited by the Global Interpreter Lock (GIL).</p>
<p>Each thread maintains its own execution context and stack, making them relatively heavyweight compared to other options.</p>
<p><strong><mark>Fibers</mark>, introduced as a lightweight alternative, representing a fundamentally different approach to concurrency.</strong> They operate on a cooperative scheduling model, where each fiber must explicitly yield control to others.</p>
<p>This makes them exceptionally efficient for managing many concurrent operations, particularly in scenarios involving I/O operations. <strong>Unlike threads, fibers share the same execution context and require minimal memory overhead.</strong></p>
<p><strong><mark>Ractors</mark>, Ruby&#39;s newest concurrency primitive, take a revolutionary approach by <mark>providing true parallel execution</mark> capabilities.</strong> They operate with isolated memory spaces and communicate through message passing, effectively bypassing the GIL&#39;s limitations.</p>
<p>This isolation prevents the common pitfalls of shared-state concurrency while enabling genuine parallel execution of Ruby code.</p>
<h3 id="heading-performance-and-resource-utilization">Performance and Resource Utilization</h3>
<p>When it comes to resource utilization, each mechanism has distinct characteristics.</p>
<p><strong><mark>Threads typically consume around 8MB of memory per instance</mark> and involve operating system overhead for context switching.</strong></p>
<p>While this makes them more resource-intensive, their preemptive scheduling makes them <strong>ideal for long-running tasks that need to share processor time fairly</strong>.</p>
<p><strong><mark>Fibers, in contrast, use only a few kilobytes of memory</mark> per instance and handle their own scheduling.</strong></p>
<p>This efficiency makes them <strong>perfect for applications that need to manage thousands of concurrent operations</strong>, such as web servers handling multiple simultaneous connections.</p>
<p>However, their cooperative nature means that <strong>poorly written fiber code can block other fibers from executing</strong>.</p>
<p><strong><mark>Ractors introduce additional overhead compared to threads</mark> but provide true parallelism in return.</strong></p>
<p><strong><mark>Each Ractor runs in its own thread</mark></strong> and maintains its own isolated heap, making them <strong>more memory-intensive than both threads and fibers</strong>.</p>
<p>However, this isolation enables them to <mark>fully utilize multiple CPU cores</mark>, making them invaluable for CPU-bound workloads.</p>
<h3 id="heading-practical-implementation-considerations">Practical Implementation Considerations</h3>
<p>When implementing concurrent systems, each mechanism requires different design approaches.</p>
<p><strong>Thread-based systems need <mark>careful consideration of synchronization mechanisms like mutexes and locks</mark> to prevent race conditions.</strong></p>
<p>This can make thread-based code more complex to write and debug, but threads remain valuable for their ability to handle blocking operations without stopping the entire program.</p>
<p><strong>Fiber-based systems <mark>require explicit yield points</mark> and careful attention to the execution flow.</strong></p>
<p>While this might seem restrictive, it actually makes fiber-based code more predictable and easier to reason about.</p>
<p>The <mark>async/await pattern</mark>, commonly implemented using fibers, provides a clean and intuitive way to handle concurrent operations.</p>
<p><strong>Ractor-based systems <mark>demand a message-passing approach to communication</mark>, similar to actor-based concurrency models.</strong></p>
<p>While this requires rethinking how components interact, it eliminates many traditional concurrency bugs by preventing shared state access.</p>
<p>This makes <mark>Ractors particularly suitable for parallel processing tasks</mark> where data can be cleanly partitioned.</p>
<h3 id="heading-threads-code-example">Threads - Code Example</h3>
<p>Threads in Ruby operate within the same process and share memory space. They&#39;re limited by the Global Interpreter Lock (GIL) but are excellent for I/O-bound operations.</p>
<pre><code><span><span>class</span> <span>ThreadBasedProcessor</span></span>
  <span><span>def</span> <span>process_batch</span><span>(items)</span></span>
    threads = []
    results = Queue.new

    items.each <span>do</span> <span>|item|</span>
      threads &lt;&lt; Thread.new <span>do</span>
        <span>begin</span>
          result = complex_calculation(item)
          results.push({ <span>status:</span> <span>:success</span>, <span>item:</span> item, <span>result:</span> result })
        <span>rescue</span> =&gt; e
          results.push({ <span>status:</span> <span>:error</span>, <span>item:</span> item, <span>error:</span> e.message })
        <span>end</span>
      <span>end</span>
    <span>end</span>

    threads.each(&amp;<span>:join</span>)
    collect_results(results, items.size)
  <span>end</span>

  private

  <span><span>def</span> <span>complex_calculation</span><span>(item)</span></span>
    sleep(<span>0</span>.<span>1</span>) 
    item * <span>2</span>
  <span>end</span>

  <span><span>def</span> <span>collect_results</span><span>(results, expected_count)</span></span>
    Array.new(expected_count) { results.pop }
  <span>end</span>
<span>end</span>
</code></pre>
<p><strong>Explanation</strong>: This example shows <mark>how to use threads for parallel processing with error handling</mark>. It creates a thread for each item, performs a calculation, and collects results in a thread-safe queue. The implementation includes proper thread joining and error handling, making it robust for production use.</p>
<h3 id="heading-fibers-code-example">Fibers - Code Example</h3>
<p>Fibers are lightweight concurrency primitives that enable cooperative scheduling and are excellent for handling many concurrent operations without the overhead of threads.</p>
<pre><code><span>require</span> <span>&#39;fiber&#39;</span>
<span>require</span> <span>&#39;async&#39;</span>

<span><span>class</span> <span>FiberBasedProcessor</span></span>
  <span><span>def</span> <span>process_async_batch</span><span>(items)</span></span>
    Async <span>do</span> <span>|task|</span>
      results = []
      mutex = Async::Mutex.new

      fibers = items.map <span>do</span> <span>|item|</span>
        task.async <span>do</span>
          result = process_item(item)
          mutex.synchronize { results &lt;&lt; result }
        <span>end</span>
      <span>end</span>

      fibers.each(&amp;<span>:wait</span>)
      results
    <span>end</span>
  <span>end</span>

  private

  <span><span>def</span> <span>process_item</span><span>(item)</span></span>
    Async <span>do</span>
      
      sleep(<span>0</span>.<span>1</span>)
      { <span>item:</span> item, <span>processed_at:</span> Time.now }
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p><strong>Explanation</strong>: This code demonstrates using <mark>Fibers with the async gem for concurrent processing</mark>. It creates lightweight fibers for each item, processes them asynchronously, and collects results using a mutex for thread-safety. Fibers are particularly efficient for I/O-bound operations as they consume less memory than threads.</p>
<h3 id="heading-ractors-code-example">Ractors - Code Example</h3>
<p><strong>Ractors enable true parallel execution with isolated memory spaces, making them ideal for CPU-bound operations.</strong></p>
<p>Unlike traditional threads in Ruby, which are limited by the Global Interpreter Lock (GIL), <mark>Ractors allow parallel execution without memory sharing issues</mark>.</p>
<p>This isolation of memory spaces enables efficient parallel processing, especially for tasks that require intensive CPU usage.</p>
<pre><code><span><span>class</span> <span>RactorBasedProcessor</span></span>
  <span><span>def</span> <span>process_parallel_batch</span><span>(items, worker_count = <span>4</span>)</span></span>
    
    chunks = items.each_slice((items.size.to_f / worker_count).ceil).to_a

    
    ractors = chunks.map <span>do</span> <span>|chunk|</span>
      Ractor.new(chunk) <span>do</span> <span>|data|</span>
        
        <span><span>def</span> <span>complex_math</span><span>(n)</span></span>
          (<span>1</span>..<span>1000</span>).reduce(n) { <span>|sum, i|</span> sum + Math.sqrt(i ** <span>2</span>) }
        <span>end</span>

        
        data.map <span>do</span> <span>|item|</span>
          result = complex_math(item)
          [item, result]
        <span>end</span>
      <span>end</span>
    <span>end</span>

    
    results = {}
    ractors.each <span>do</span> <span>|ractor|</span>
      ractor.take.each <span>do</span> <span>|item, result|</span>
        results[item] = result
      <span>end</span>
    <span>end</span>
    results
  <span>end</span>
<span>end</span>


processor = RactorBasedProcessor.new
result = processor.process_parallel_batch([<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>0</span>])
puts result
</code></pre>
<p><strong>Explanation</strong>: This implementation <mark>demonstrates Ractors performing CPU-intensive calculations in parallel by dividing an array into chunks and processing each chunk in separate Ractors</mark>. Each Ractor runs independently, performing complex calculations on its assigned data, and returns the results. <em>Unlike threads, <mark>Ractors can achieve true parallelism by bypassing the Global Interpreter Lock (GIL)</mark>, making them ideal for CPU-bound tasks.</em> <em><mark>By defining the computational method directly within each Ractor, this implementation also avoids scope isolation issues</mark></em>, ensuring that each Ractor remains isolated and self-contained.</p>
<h2 id="heading-why-concurrency-is-required-in-ai-and-ml">Why Concurrency is Required in AI and ML?</h2>
<p>Concurrency plays a crucial role in AI and ML for several compelling reasons:</p>
<ol>
<li><p><strong>Data Processing Efficiency</strong></p>
<ul>
<li><p>Large datasets require parallel processing</p>
</li>
<li><p>Multiple data streams need concurrent handling</p>
</li>
<li><p>Real-time data processing demands</p>
</li>
</ul>
</li>
<li><p><strong>Model Training Optimization</strong></p>
<ul>
<li><p>Parallel training of multiple models</p>
</li>
<li><p>Concurrent hyperparameter tuning</p>
</li>
<li><p>Distributed learning processes</p>
</li>
</ul>
</li>
<li><p><strong>Resource Utilization</strong></p>
<ul>
<li><p>Efficient use of available CPU cores</p>
</li>
<li><p>Better memory management</p>
</li>
<li><p>Improved throughput for computational tasks</p>
</li>
</ul>
</li>
<li><p><strong>Response Time Requirements</strong></p>
<ul>
<li><p>Real-time prediction serving</p>
</li>
<li><p>Concurrent user request handling</p>
</li>
<li><p>Batch processing optimization</p>
</li>
</ul>
</li>
<li><p><strong>Scalability Needs</strong></p>
<ul>
<li><p>Horizontal scaling capabilities</p>
</li>
<li><p>Load distribution</p>
</li>
<li><p>Resource allocation flexibility</p>
</li>
</ul>
</li>
</ol>
<h3 id="heading-example-1-concurrent-openai-api-processing"><strong>Example 1:</strong> Concurrent OpenAI API Processing</h3>
<pre><code><span>require</span> <span>&#39;openai&#39;</span>
<span>require</span> <span>&#39;concurrent-ruby&#39;</span>

<span><span>class</span> <span>ConcurrentAIProcessor</span></span>
  <span><span>def</span> <span>initialize</span><span>(api_key)</span></span>
    @client = OpenAI::Client.new(<span>access_token:</span> api_key)
    @pool = Concurrent::FixedThreadPool.new(<span>5</span>)  
  <span>end</span>

  <span><span>def</span> <span>process_batch_queries</span><span>(queries)</span></span>
    promises = queries.map <span>do</span> <span>|query|</span>
      Concurrent::Promise.execute(<span>executor:</span> @pool) <span>do</span>
        <span>begin</span>
          response = @client.chat(
            <span>parameters:</span> {
              <span>model:</span> <span>&#34;gpt-3.5-turbo&#34;</span>,
              <span>messages:</span> [{ <span>role:</span> <span>&#34;user&#34;</span>, <span>content:</span> query }],
              <span>temperature:</span> <span>0</span>.<span>7</span>,
              <span>max_tokens:</span> <span>150</span>
            }
          )

          { 
            <span>status:</span> <span>:success</span>,
            <span>query:</span> query,
            <span>response:</span> response.dig(<span>&#39;choices&#39;</span>, <span>0</span>, <span>&#39;message&#39;</span>, <span>&#39;content&#39;</span>),
            <span>tokens:</span> response.dig(<span>&#39;usage&#39;</span>, <span>&#39;total_tokens&#39;</span>)
          }
        <span>rescue</span> =&gt; e
          {
            <span>status:</span> <span>:error</span>,
            <span>query:</span> query,
            <span>error:</span> e.message
          }
        <span>end</span>
      <span>end</span>
    <span>end</span>

    
    results = promises.map(&amp;<span>:value!</span>)
    @pool.shutdown

    
    {
      <span>successful:</span> results.count { <span>|r|</span> r[<span>:status</span>] == <span>:success</span> },
      <span>failed:</span> results.count { <span>|r|</span> r[<span>:status</span>] == <span>:error</span> },
      <span>total_tokens:</span> results.sum { <span>|r|</span> r[<span>:tokens</span>].to_i },
      <span>responses:</span> results
    }
  <span>end</span>
<span>end</span>


processor = ConcurrentAIProcessor.new(<span>&#39;your-api-key&#39;</span>)
queries = [
  <span>&#34;Explain quantum computing in simple terms&#34;</span>,
  <span>&#34;What is machine learning?&#34;</span>,
  <span>&#34;How does natural language processing work?&#34;</span>
]
results = processor.process_batch_queries(queries)
</code></pre>
<p><strong>Explanation</strong>: This <mark>example showcases concurrent processing of OpenAI API requests using a thread pool</mark>. It manages rate limiting through pool size, handles errors gracefully, and provides detailed analytics for each batch of queries. The implementation <mark>uses Concurrent::Promise for non-blocking execution</mark> and proper resource management.</p>
<h3 id="heading-example-2-parallel-ml-model-training-with-ractors"><strong>Example 2: Parallel ML Model Training with Ractors</strong></h3>
<pre><code><span><span>class</span> <span>MLModelTrainer</span></span>
  <span><span>def</span> <span>train_models_in_parallel</span><span>(training_data, model_configs)</span></span>
    
    data_folds = create_cross_validation_folds(training_data, <span>folds:</span> <span>5</span>)

    
    ractors = model_configs.map <span>do</span> <span>|config|</span>
      Ractor.new(data_folds, config) <span>do</span> <span>|folds, model_params|</span>
        results = folds.map <span>do</span> <span>|fold|</span>
          {
            <span>params:</span> model_params,
            <span>metrics:</span> train_and_evaluate(fold[<span>:train</span>], fold[<span>:test</span>], model_params)
          }
        <span>end</span>

        
        avg_metrics = calculate_average_metrics(results)
        [model_params, avg_metrics]
      <span>end</span>
    <span>end</span>

    
    results = ractors.map(&amp;<span>:take</span>).to_h
    select_best_model(results)
  <span>end</span>

  private

  <span><span>def</span> <span>train_and_evaluate</span><span>(train_data, test_data, params)</span></span>
    
    model = initialize_model(params)
    history = train_model(model, train_data, params)

    
    {
      <span>accuracy:</span> evaluate_accuracy(model, test_data),
      <span>f1_score:</span> calculate_f1_score(model, test_data),
      <span>training_time:</span> history[<span>:training_time</span>],
      <span>convergence_epoch:</span> history[<span>:convergence_epoch</span>]
    }
  <span>end</span>

  <span><span>def</span> <span>initialize_model</span><span>(params)</span></span>
    
    {
      <span>learning_rate:</span> params[<span>:learning_rate</span>],
      <span>layers:</span> params[<span>:layers</span>],
      <span>activation:</span> params[<span>:activation</span>]
    }
  <span>end</span>

  <span><span>def</span> <span>train_model</span><span>(model, data, params)</span></span>
    epochs = params[<span>:epochs</span>] <span>||</span> <span>100</span>
    start_time = Time.now

    
    convergence_epoch = (epochs * <span>0</span>.<span>7</span>).to_i 
    epochs.times <span>do</span> <span>|epoch|</span>
      
      sleep(<span>0</span>.<span>01</span>) 
      <span>break</span> <span>if</span> epoch &gt;= convergence_epoch
    <span>end</span>

    {
      <span>training_time:</span> Time.now - start_time,
      <span>convergence_epoch:</span> convergence_epoch
    }
  <span>end</span>

  <span><span>def</span> <span>create_cross_validation_folds</span><span>(data, <span>folds:</span>)</span></span>
    
    folds.times.map <span>do</span> <span>|i|</span>
      {
        <span>train:</span> data.select.with_index { <span>|_, idx|</span> idx % folds != i },
        <span>test:</span> data.select.with_index { <span>|_, idx|</span> idx % folds == i }
      }
    <span>end</span>
  <span>end</span>

  <span><span>def</span> <span>calculate_average_metrics</span><span>(fold_results)</span></span>
    metrics = fold_results.map { <span>|r|</span> r[<span>:metrics</span>] }
    {
      <span>avg_accuracy:</span> metrics.sum { <span>|m|</span> m[<span>:accuracy</span>] } / metrics.size,
      <span>avg_f1_score:</span> metrics.sum { <span>|m|</span> m[<span>:f1_score</span>] } / metrics.size,
      <span>avg_training_time:</span> metrics.sum { <span>|m|</span> m[<span>:training_time</span>] } / metrics.size,
      <span>avg_convergence_epoch:</span> metrics.sum { <span>|m|</span> m[<span>:convergence_epoch</span>] } / metrics.size
    }
  <span>end</span>

  <span><span>def</span> <span>select_best_model</span><span>(results)</span></span>
    
    best_config = results.max_by { <span>|_, metrics|</span> metrics[<span>:avg_accuracy</span>] }
    {
      <span>best_config:</span> best_config[<span>0</span>],
      <span>metrics:</span> best_config[<span>1</span>]
    }
  <span>end</span>
<span>end</span>


trainer = MLModelTrainer.new
training_data = (<span>1</span>..<span>1000</span>).map { <span>|i|</span> [i, i * <span>2</span>] } 

model_configs = [
  { <span>learning_rate:</span> <span>0</span>.<span>01</span>, <span>layers:</span> [<span>64</span>, <span>32</span>], <span>activation:</span> <span>&#39;relu&#39;</span>, <span>epochs:</span> <span>100</span> },
  { <span>learning_rate:</span> <span>0</span>.<span>001</span>, <span>layers:</span> [<span>128</span>, <span>64</span>], <span>activation:</span> <span>&#39;tanh&#39;</span>, <span>epochs:</span> <span>100</span> },
  { <span>learning_rate:</span> <span>0</span>.<span>005</span>, <span>layers:</span> [<span>32</span>, <span>16</span>], <span>activation:</span> <span>&#39;relu&#39;</span>, <span>epochs:</span> <span>100</span> }
]

best_model = trainer.train_models_in_parallel(training_data, model_configs)
</code></pre>
<p><strong>Explanation</strong>: This <mark>examples uses Ractors for true parallel processing of ML model training</mark>. Each Ractor handles a complete cross-validation cycle for a specific model configuration, enabling parallel exploration of different hyperparameter combinations. The code includes k-fold cross-validation, metrics calculation, and best model selection. <mark>Using Ractors instead of Threads allows for true parallel execution of CPU-intensive training tasks</mark>, making it more efficient for ML workloads.</p>
<h2 id="heading-conclusion">Conclusion</h2>
<p>Ruby&#39;s concurrency features represent a significant evolution in the language&#39;s capabilities for handling parallel and concurrent operations.</p>
<p><strong>The introduction of Ractors, alongside the mature Thread and Fiber implementations, provides developers with a robust toolkit for building efficient, scalable applications.</strong></p>
<p>Key takeaways from this exploration:</p>
<ol>
<li><p>Choose the right concurrency primitive based on your specific use case:</p>
<ul>
<li><p>Threads for I/O-bound operations</p>
</li>
<li><p>Fibers for lightweight concurrency</p>
</li>
<li><p>Ractors for CPU-bound parallel processing</p>
</li>
</ul>
</li>
<li><p>Consider the trade-offs between complexity and performance when implementing concurrent solutions</p>
</li>
<li><p>Leverage the rich ecosystem of concurrent programming tools and libraries available in Ruby</p>
</li>
<li><p>Pay special attention to concurrency when building AI and ML applications, as it can significantly impact performance and scalability</p>
</li>
</ol>
<p>As Ruby continues to evolve, its concurrency capabilities will likely expand further, making it an increasingly powerful choice for building modern, concurrent applications, particularly in the domains of AI and ML.</p>
</div></div></div>
  </body>
</html>
