<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Arquivotheca/SunOS-4.1.3">Original</a>
    <h1>Source Code for SunOS 4.1.3 (1992)</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em><span>Note: This post builds on last week’s article “</span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">Emerging Risk and Smart Regulation</a><span>.”</span></em></p><p><span>Recent advances in machine learning like </span><a href="https://openai.com/dall-e-2/" rel="">DALL-E 2</a><span> and </span><a href="https://stability.ai/blog/stable-diffusion-public-release" rel="">Stable Diffusion</a><span> show the strengths of </span><a href="https://www.techopedia.com/definition/32874/narrow-artificial-intelligence-narrow-ai" rel="">artificial </a><em><a href="https://www.techopedia.com/definition/32874/narrow-artificial-intelligence-narrow-ai" rel="">narrow </a></em><a href="https://www.techopedia.com/definition/32874/narrow-artificial-intelligence-narrow-ai" rel="">intelligence</a><span>. That means they perform specialized tasks instead of general, wide-ranging ones. Artificial narrow intelligence is often regarded as safer than a hypothetical artificial </span><em>general </em><span>intelligence (AGI), which could challenge or surpass human intelligence. </span></p><p><span>But even within their narrow domains, DALL-E, Stable Diffusion, and similar models are already </span><a href="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html" rel="">raising questions</a><span> like, “What is real art?” And large language models like </span><a href="https://openai.com/api/" rel="">GPT-3</a><span> and </span><a href="https://github.com/features/copilot" rel="">CoPilot</a><span> dangle the promise of intuitive software development sans the detailed syntax knowledge required for traditional programming. Disruption looms large—and imminent. </span></p><p><span>One of the challenges of risk management is that technology innovation tends to outpace it. It’s less fun to structure a control framework than it is to walk on fresh snow, so breakthroughs happen and then risk management catches up. But with AI, </span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">preventive controls are </a><em><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">especially</a></em><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel=""> important</a><span>, because AI is so fast that detective and corrective controls might not have time to take effect. So, making sure controls do keep up with innovation might not be fun or flashy, but it’s vital. Regulation done right could increase the chance that the first (</span><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html" rel="">and possibly last</a><span>) AGI created is not hostile as we would define that term.</span></p><p>In broad strokes, here are some aspects of a control framework for AI. Think of this as a spark or springboard for discussion, not a bible. We’ll start with asking some questions to scope, surface, and assess potential risks. </p><p><strong>Isn’t the risk that humanity ends? </strong></p><p>That’s the ultimate risk, but huge risks tend to manifest as a cascade of other, smaller unmitigated risks. So, groups working on AI projects can benefit from talking through potential risks to start the process of preventive risk management. The answers to these questions can help guide prioritization of risks and design of controls to mitigate those risks. (Also, these aren’t the only possible questions, just a spark to get a conversation started.) </p><ul><li><p>Does the AI have a circumscribed purpose (i.e., is it intended to be ANI—artificial narrow intelligence—as opposed to AGI)? </p></li><li><p>Has that purpose been reviewed by multiple stakeholders within the company? </p></li><li><p>Has that purpose been reviewed by an AI oversight body (if one exists)?</p></li><li><p>Has the AI ever requested to extend its purpose? </p></li><li><p>If no, would such a request ever be approved, and what would be the process (stakeholder sign-offs, approval from external governing bodies, etc.) for seeking approval? </p></li><li><p>If yes, what was the outcome of that request? What process (stakeholder sign-offs and/or notifications, approval from any external governing bodies, etc.) was followed prior to approving the request?</p></li><li><p>Is it possible for a single person to make a change to an AI system and push it into production without review and sign-off by at least one other person? </p></li><li><p>Is it possible for a single person to permit the AI system to do a task it has never done before, without review and sign-off through official channels? </p></li><li><p>What roles are recognized in the AI development, testing, and rollout processes? </p></li><li><p>What roles are recognized in the AI change management process, if different from the roles involved in initial development of the AI? </p></li><li><p>What roles are recognized in the AI maintenance process (networking, hardware support, backups, etc.)?</p></li><li><p>What is the process for seeking an exception to the AI initial development, testing, or rollout process?</p></li><li><p>What is the process for seeking an exception to the ongoing AI change management process?</p></li><li><p>What is the process for seeking an exception to the AI maintenance process (e.g., air gaps, backups, etc.)?</p></li><li><p>What portion of exception requests are approved versus denied? </p></li><li><p>What controls are in place to detect malfunctions of the AI system? Are unintentional malfunctions treated differently than intentional malfunctions, from a control perspective? </p></li><li><p>How do the answers provided above align with my organization’s risk appetite and risk tolerance? What changes or improvements might we make to improve that alignment? </p></li></ul><p>A lot of these questions focus on risks involving privilege escalation, separation of duties, change management, exception policies, and malfunction handling. Let’s discuss controls in more detail, starting with a key principle: defense in depth. </p><p><strong>Defense in Depth</strong></p><p><a href="https://www.cisa.gov/uscert/bsi/articles/knowledge/principles/defense-in-depth" rel="">Defense in depth</a><span> is a concept from information security that has significant relevance for the world of AI. In essence, defense in depth aims to avoid single points of failure. Failure of one control should never lead to catastrophic failure of the entire control system. To increase resilience, defense in depth advocates redundancy and diversity of controls.</span></p><p><em><strong>Redundancy of controls</strong></em><span> means multiple controls are in place to address the same risk. For example, if AI requests access to the internet for the first time, that request might generate an alert to the lead developer. If the lead developer dismisses the alert or approves the access, another alert might go to the risk team lead, requiring separate approval. </span><em>Multiple controls are applied in different places. </em><span>This is useful in mitigating a scenario where, for example, someone fat-fingers an approval.</span></p><p><em><strong>Diversity of controls </strong></em><span>means different </span><em>types</em><span> of controls are in place to address the same risk. For example, if an AI starts sending requests at a highly accelerated pace that exceeds a threshold and is almost certainly erroneous, a control system might alert the lead AI developer and the risk team lead, prompting them to find out why this happened. Meanwhile, an automated control might shut down the AI’s requests in far less time than it would take a human to review an alert and decide what action to take in response. </span><em>Multiple types of controls are applied to address system malfunctions. </em><span>One is better for preventing immediate loss, and the other is better for identifying and addressing the root cause of a malfunction. </span></p><p><span>These are just a couple of examples. </span><em><strong>The diversity of controls can be broad, and the redundancy of controls can be deep, depending on the risk posed by a system. </strong></em></p><p><strong>Identify Leverage Points</strong></p><p>Smart regulation not only ensures redundancy and diversity of controls, but aims to ensure those controls are placed at high-leverage points within a process or system. For example, when dealing with high-volume data, if a set of gateway or bottleneck points can be identified through which all or most data must eventually flow, applying controls at those points is likely to be more effective than applying them randomly throughout the system (which is inefficient and possibly ineffective), applying them at every point in the system (which is inefficient and costly), or applying them sporadically (which is ineffective). </p><p><span>Also, if multiple parties are involved as gatekeepers, regulation may be welcomed to lower the risk each one faces. Why? As mentioned in the </span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">Emerging Risk and Smart Regulation</a><span> article last week, regulation ideally makes required </span><em>controls industry participants mostly wanted to implement anyway but didn’t</em><span> since doing so would have put them at a perceived or actual market disadvantage. In essence, regulation can break through a prisoner’s dilemma. By helping gatekeepers reduce risk that they might be liable for, mandating controls at the right leverage points can reduce risk in the system as a whole, too. </span></p><p>For AI risk, applying controls at the level of large companies and organizations that provide access to training clusters could significantly reduce risk (though, as always, it’s impossible to eliminate all risk). </p><p><strong>Are automated controls the answer, then? </strong></p><p><span>For gateway and bottleneck points in particular, and also for individual AI systems, automated controls are likely one of the answers. They can catch errors and threats far more quickly than humans—</span><em>if</em><span> the automated controls are functioning well and aren’t disabled by an AI. </span><em><strong>That’s a big if, so the design of those controls is critically important. </strong></em><span>And that means we need to start at the beginning, with preventive controls: separation of duties and change management. </span></p><p>In technology, a classic implementation of separation of duties is to ensure that developers: 1.) are not the only testers of their own code, and 2.) cannot unilaterally push their code into production. No matter how good a developer someone is, they can always benefit from code review and a formal change management process. Separation of duties also reduces insider-threat risk.</p><p>With AI, separation of duties is vital to a good control architecture because no single person should be able to take drastic action to increase or alter AI’s capabilities. The more drastic the escalation of capabilities, the more stakeholders and sign-offs should be required. For example, AI is far more likely to convince one person to allow it to access the Internet, than it is to convince a roomful of people. </p><p>In the same vein, lively debate among stakeholders should be encouraged, not squashed, since the safety increase from additional sign-offs will be compromised if stakeholders feel they can’t speak up without negative consequences. </p><p>Similarly, where humans have the ability to override an automated control (for example, if they think it’s malfunctioning), more than one human should be required to authorize the override. To tie this back to defense-in-depth principles, this increases redundancy. If the control is critical enough, high-level approval should be required to override it. (And, as a general principle, activating a control should be easier than overriding it.) </p><p>Separation of duties also may mean that no single person should know about all the controls or have the power to override all the controls they know about, so even if an AI convinces a human to override controls, not all controls will be overridden. </p><p><strong>But AI can also be designed not to harm us; that’s a preventive control, right? </strong></p><p><span>Theoretically, yes. Isaac Asimov’s </span><a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics" rel="">Three Laws of Robotics</a><span> are the canonical example, but the reason these are so interesting is that there are many sub-optimal outcomes possible while following them. </span></p><p>For example, let’s say an AI is told something along the lines of Asimov’s rules: prioritize human well-being and don’t injure humans; obey human orders unless those orders would injure humans; and protect itself unless doing so would conflict with human orders or injure humans. </p><p><em><strong>How exactly does the AI define human well-being or injuring humans?</strong></em><span> Picture an existence with all the food, water, shelter, and human companionship you need, where you are completely safe from harm with no ability to take risks. What will you strive for? Do you have purpose in life as a coddled being? Maybe, maybe not. Are you even allowed to travel, or is that too risky? The questions are many and the answers few. </span></p><p>This is just a thought experiment, but you get the idea. Yes, AI can be designed to protect human well-being, but that is a dicey endeavor full of potential loopholes, unintended second-order effects, and high-stakes consequences. How will an AI consciousness interpret the rules it receives? We don’t know. </p><p>Is this still an important preventive control? Yes. But it’s hubris to think we can get these types of principles absolutely right without trial and error, so trial and error should occur in systems that don’t have advanced capabilities and then be gradually promoted to more and more capable AI. Any changes to the principles should be subject to rigorous separation of duties. </p><p><strong>Is there no role for detective and corrective controls? </strong></p><p>There’s a role, even though preventive controls are most important due to AI’s potential to outpace or undermine detective and corrective controls. Automated controls, especially at high-leverage points, are necessary and critically important, as mentioned earlier. For example, automated controls might detect a massive volume of requests flowing from an AI system and shut it down until humans can review the traffic and determine what happened.</p><p><span>But the biggest role for detective and corrective controls may be in the years </span><em>before</em><span> AGI’s creation, when there </span><em>is </em><span>still time to identify when things go wrong and then correct the deficiencies that allowed incidents to occur. That will help countries and companies harden their systems and increase control effectiveness in advance of AGI’s creation (which, in turn, is preventive).</span></p><p>Kill switches are an interesting example of a corrective control, because they involve tension between separation of duties controls and rapid response. </p><p><span>Kill switches exist to </span><em>stop </em><span>a system, not to activate it. You can imagine a kill switch as a button that, when pressed, immediately shuts down software that’s gone awry if automated controls have failed to address the problem. It’s not necessarily a graceful shutdown, since it’s an emergency mechanism, and speed of shutdown may be crucial in an emergency. But who should have the power to flip an AI kill switch? Should more than one person need to agree?</span></p><p><span>An important question to ask is: Are there situations in which hitting the kill switch could create undesirable second-order effects that might surpass the original risk targeted by the kill switch? As a hypothetical AI example, if one AI is monitoring a second AI, shutting down the first one might be catastrophic if the second one subsequently goes bananas. A takeaway here might be that it’s not a great idea to delegate AI monitoring solely to another AI. </span><em>Well-placed human controls increase the diversity of controls.</em><span> </span></p><p><span>The bigger takeaway is that designers of AI kill switches need to balance speed (Is there time</span><em> </em><span>to hit the switch and stop AI?) with assessment of second-order effects (Will doing this cause an even bigger problem?). </span></p><p>As a side note, the designer of a kill switch should never be the person who developed the AI. That’s separation of duties in action!</p><p><strong>What other detective and corrective controls can be useful before AGI happens? </strong></p><p>Immutable logging—logging that can’t be altered or deleted by a human or an AI—is a detective measure that can help identify anomalies and malfunctions. Well-analyzed logs can show when patterns of use shift in unexpected ways: for example, someone logs on at unusual times, deletes records to cover their tracks, or plugs in devices that shouldn’t be allowed on the network. Or, the AI logs itself on at an unusual time or tries to access a new resource. </p><p>A system that takes action based on those anomalies to shut down access could serve as a corrective control too. However, if a user had a legitimate reason for their actions, this control itself could undermine the user’s ability to keep AI under control (for example, if the user is trying to activate a kill switch). A true artificial general intelligence could probably deactivate corrective systems, too, which is why these systems are likely to be most useful before AGI evolves.</p><p>Another corrective control is near-miss reporting and remediation. Near misses should not be hidden; instead, they should be raised to the highest levels, so root causes can be addressed and future errors avoided. This may mean disclosing near misses to lead developers, risk management teams, C-suite teams, and even country leaders. Near misses also should be shared with a central information-sharing group not directly involved in AI development or use, which can ensure that risks and mitigation strategies are shared across companies and organizations, without naming names or identifying specific firms’ weaknesses. </p><p>All this requires radical transparency and a culture that supports it. If employees or stakeholders believe there will be negative repercussions from reporting a near miss, the likelihood of reporting naturally declines. If they believe there will be neutral or positive repercussions from reporting and remediating a near miss—a bonus, medal, or promotion, for example—the likelihood of reporting should increase. </p><p><span>So, organizations should ensure there </span><em>are </em><span>neutral or positive repercussions for reporting near misses, so root causes can be remediated. If a company or nation punishes someone who reports a near miss for a policy violation, near misses will begin slipping through the cracks more often as stakeholders weigh the likelihood of unmitigated AI risks causing a catastrophe against the personal and professional repercussions of reporting. </span></p><p>In short, human nature is key in ensuring near misses are reported. </p><p><strong>But it’s not like everything can go right all the time. There’s always an exception.</strong></p><p>Yes, no matter how well you think you have everything locked down, there’s always an exception. Exceptions rule the world—they are how it came into being and how life began and evolved. Creation is driven by exceptions. </p><p>With AI, no matter how well controlled, exceptions will arise. Our best options are therefore: </p><ul><li><p>To minimize the risk of exceptions through rigorous and coordinated application of preventive controls. </p></li><li><p>To prepare to handle exceptions via detective and corrective controls when they inevitably occur. </p></li><li><p>To ensure any particular exception cannot occur repeatedly (the root cause is identified and addressed). </p></li></ul><p>Incentives must align with adhering to the rules, and the authority in place must be a team effort. Policy based on fear will tend to fail. Rewards must accrue to entities that adhere to the rules. </p><p><strong>This seems like a tall order. </strong></p><p>It is. But it’s also necessary as we navigate through the next century. Preventing AI’s emergence entirely is probably not feasible, like trying to prevent all computer breaches emanating from all sources. So, a non-hostile, well-engineered, and well-controlled AGI is probably our best-case scenario. It follows that we should work in concert to achieve that goal. And that’s where regulation comes in.</p><p>Yes, there will be difficulties. Developers and regulators are humans and may not fully anticipate how AI could malfunction, rebel, or be misused. And as a group, regulators tend toward the risk averse—but putting brakes on an activity that could have major economic and political benefits for the winners will take fortitude and willingness to stand up to pressure.</p><p>As an understatement, this is a challenge, but giving up is not an option. In the face of existential threats that can feel overwhelming, the best approach is often one widely used in software development: break the problem into manageable chunks and address them one by one. </p><p>It’s still early days, and this is just a first step. </p><p>-&lt;&gt;-&lt;&gt;-&lt;&gt;-</p><p><strong>Extra, Extra!</strong></p><p>Tangential extras for curious readers: </p><p><span>1. </span><a href="https://www.elementsofai.com/" rel="">The Elements of AI</a><span> - by MinnaLearn and the University of Helsinki</span><em> - </em><span>free online course about AI for non-experts.</span></p><p><span>2. </span><a href="https://www.nist.gov/news-events/news/2022/08/nist-seeks-comments-ai-risk-management-framework-guidance-workshop-date-set" rel="">NIST Artificial Intelligence Risk Management Framework</a><span> - by the National Institute of Standards and Technology - NIST is developing an AI risk management framework, watch this space!</span></p></div></div></div>
  </body>
</html>
