<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://knowingmachines.org/models-all-the-way">Original</a>
    <h1>Models all the way down</h1>
    
    <div id="readability-page-1" class="page"><section id="scrolly">


<article>
<p><span>If you want to make a really big AI model — the kind that can generate images or do your homework, or build this website, or fake a moon landing — you start by finding a really big training set.</span></p>
<p><span>Images and words, harvested by the billions from the internet, material to build the world that your AI model will reflect back to you.</span></p>
<div id="1.03" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>What this training set contains is extremely important. More than any other thing, it will influence what your model can do and how well it does it.</span></p><p><span>Yet few people in the world have spent the time to look at what these sets that feed their models contain. </span></p></div>
<p><span>When they do, very real problems emerge, often with serious legal ramifications.</span></p>
<p><span>In December, researchers from Stanford&#39;s Internet Observatory identified more than 3,000 images categorized as Child Sexual Abuse Material (CSAM) in one of the most influential AI training sets of the moment: LAION-5B.</span></p>
<div id="1.05" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>LAION-5B is a really big, open-source dataset of images and text captions scraped from the internet, designed for large AI models. It was released in 2022 by LAION, a German non-profit organization. </span></p><p><span>LAION-5B is what we call a &#34;foundation dataset&#34; for generative artificial intelligence.</span></p></div>
<p><span>Training a model on LAION-5B is meant to give it a comprehensive representation of the world, to build a kind of vocabulary of things and concepts.</span></p>
<div id="1.07" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>The stated goal of the project to create LAION-5B was to conduct basic research into dataset curation. Specifically, its authors wanted to create an image training set with purely automated methods - with no humans in the mix.</span></p><p><span>The resulting &#34;hands-off&#34; dataset has been used in hundreds of academic projects. The paper announcing LAION-5B has been cited 1,331 times.</span></p></div>
<div id="1.08" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>On their homepage, its creators explicitly warn against its use in real-world contexts:</span></p><p><span>“Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products ...”</span></p></div>
<div id="1.09" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>Largely, this warning has been ignored. Midjourney and Stable Diffusion, two large models for which some of the data sources are known, are both trained in part on LAION-5B. </span></p><p><span>It’s likely that many other commercial models - perhaps hundreds - have been trained on the set. Models that power chat bots and image generators and have hundreds of thousands of users. </span></p></div>
<div id="1.1" +="" data-cuecode="stopCrawlers();
crawlers = [];" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe="TRUE"><p><span>The fact that there are CSAM images in LAION-5B is alarming, but it is certainly not surprising.</span></p><p><span>The scale of LAION-5B means that human curation of the dataset borders on the impossible.</span></p></div>
<p><span>If your full-time, eight-hours-a-day, five-days-a-week job were to look at each image in the dataset for just one second, it would take you 781 years.</span></p>
<div id="1.12" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>You&#39;d be doing important work.</span></p><p><span>Investigating training sets is an essential avenue to understanding how generative AI models work; the ways they see and re-create the world.</span></p></div>
<p><span>Scrutinizing these sets is perhaps the only way to get a clear look at the models that are trained on them. </span></p>
<div id="1.14" +="" data-cuecode="stopCrawlers();
crawlers = [];" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>These investigations can help us to identify biases, and to better predict the risks and harms that might arise as the models go into wider use.</span></p><p><span>So how do you investigate something that takes lifetimes to look at?</span></p></div>
<p><span>You start by trying to understand exactly how it was made.</span></p>
<p><span>Part 2: Seeing Like an Algorithm</span></p>
<p><span>LAION-5B was itself built from an even larger dataset, which comes from another non profit organization: Common Crawl.</span></p>
<p><span>Common Crawl is a corpus of web data that comes from a monthly crawl of the web. It contains data for more than 3 billion websites. </span></p>
<div id="2.03" +="" data-cuecode="clearNetwork();
currentGrid = addGrid(pinImages, 9, 5, true);" +="" data-upcode="removeGrid(currentGrid);" +="" data-mobilecuecode="clearNetwork();
currentGrid = addGrid(pinImages, 4, 5, true);" +="" data-keyframe="TRUE"><p><span>Some of these websites in particular are very well-represented in LAION-5B. </span></p><p><span>There are nearly 155 million images pairs (images + captions) from Pinterest - about one in every forty pairs.</span></p></div>
<p><span>2,4% of LAION-5B - 140 million image pairs - comes from Shopify.</span></p>
<p><span>72 million pairs are from SlidePlayer, a platform for storing and sharing PowerPoint presentations.</span></p>
<p><span>These particular domains are well represented in LAION-5B in part because they host a lot of images. </span></p>
<p><span>On top of that, content from these three sites is served up in a way that makes it particularly appealing to LAION’s methods of statistical curation.</span></p>
<div id="2.08" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="TRUE"><p><span>To make LAION-5B, developers processed Common Crawl looking for HTML IMG tags which have an ALT attribute.</span></p><p><span>The intended purpose of the ALT attribute is to improve accessibility, in particular for vision-impaired users who use screen readers.</span></p></div>
<div id="2.11" +="" data-cuecode="clearNetwork();
currentGrid = addGrid(slideImages, 2, 12, true);
gridToAlt(currentGrid);" +="" data-upcode="undefined" +="" data-mobilecuecode="
clearNetwork();
currentGrid = addGrid(slideImages, 2, 8, true);
gridToAlt(currentGrid);" +="" data-keyframe="TRUE"><p><span>Under 40% of the images across the web have ALT tags. But for some sites this is much higher. </span></p><p><span>SlidePlayer, for example, seems to add ALT tags automatically, populating them with text from the PowerPoint slides it ingests.</span></p></div>
<p><span>Pinterest generates the captions on its pages from the ALT tags, so users learned to write them before they ‘pinned’ their images.</span></p>
<p><span>Shopify users often have their eyes on high Google PageRank scores, and write ALT tag descriptions with SEO (Search Engine Optimization) in mind.</span></p>
<p><span>All of this means that ALT tags are not so much descriptions of image contents as they are artifacts of the web’s workings and of creators’ retail ambitions.</span></p>
<div id="2.1" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>The content of an ALT tag <em>should</em> describe what is in the image. </span></p><p><span>
<h2>IMAGE</h2>
<img src="https://cdn.xxl.thumbs.canstockphoto.com/canstock20816168.jpg" alt="Garden center worker selling potted flower"/>
</span>
<span>
<h2>ALT TEXT</h2>
<p>
Garden center worker selling potted flower
</p>
</span>
</p></div>
<div id="2.13" +="" data-cuecode="" +="" data-upcode="removeAllGrids();" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>However, ALT tags most often describe what the site&#39;s owners want algorithms
to see, not what they want humans to see.
</span></p><p><span>
<h2>IMAGE</h2>
<img src="https://machinist.smokingheaps.net/api/datasets/8/files/78274500" alt="Heart Shaped Sunnies - Chynna Dolls Swimwear"/>
</span>
<span>
<h2>ALT TEXT</h2>
<p>
Heart Shaped Sunnies - Chynna Dolls Swimwear
</p>
</span>
</p>
</div>
<div id="2.14" +="" data-cuecode="currentGrid = addGrid(cfiles, 9, 5, true);" +="" data-upcode="removeAllGrids();" +="" data-mobilecuecode="currentGrid = addGrid(cfiles, 4, 5, true);" +="" data-keyframe="TRUE"><p><span>Here we find an important truth about LAION-5B:</span></p><p><span><b>It contains less about how humans see the world than it does about how search engines see the world. It is a dataset that is powerfully shaped by commercial logics.</b></span></p></div>
<p><span>A key part of LAION-5B&#39;s construction was to try to select images and text captions from Common Crawl where the text of the ALT attribute most closely matched the contents of the image.</span></p>
<p><span>To do this, LAION developers used a model called CLIP (Contrastive Language–Image Pre-training), a neural network developed by researchers at Open AI.</span></p>
<p><span>The developers used CLIP to get a score for how well a string of text matches to an image: a metric for similarity between the image and its ALT tag.</span></p>
<p><span>LAION-5B&#39;s authors used this score to determine which images and text captions from Common Crawl would end up in their dataset.</span></p>
<p><span>To arrive at a minimun acceptable score, LAION-5B trained a model with samples from Common Crawl against popular benchmark datasets, such as ImageNet-1K.</span></p>
<p><span>They determined that image/text pairs with a similarity scores above a threshold of 0.26 - 0.28 (depending on the language of the caption) would be included in LAION.</span></p>
<p><span>This one thin sliver of a threshold is the thing that, more than anything else, defines which images LAION-5B contains. </span></p>
<p><span>CLIP, like most neural networks, is hard to reverse engineer. OpenAI is not forthcoming about the data that CLIP was trained on.
The image pairs in LAION - and their scores - give us a glimpse into some of its workings.</span></p>
<p><span>By looking at images at the extreme edges of LAION-5B’s similarity thresholds, we can get a sense of how CLIP perceives images, and how the use of this score might influence what LAION includes and excludes.</span></p>
<div id="2.25" +="" data-cuecode="clearNetwork();
currentGrid = addGrid(highImages, 10, 6, true);
gridToScore(currentGrid);" +="" data-upcode="" +="" data-mobilecuecode="clearNetwork();
currentGrid = addGrid(highImages, 2, 5, true);
gridToScore(currentGrid);" +="" data-keyframe="TRUE"><p><span>High similarity scores tend to be given to pairs where there is text in the image that matches the ALT tag exactly. </span></p><p><span>The CLIP scores that the LAION team generated seem to have upward bounds in the 0.5 range; there are only 22,645 images in the entire 5B set that score higher than 0.5.</span></p></div>
<div id="2.26" +="" data-cuecode="gridToAlt(currentGrid);" +="" data-upcode="removeAllGrids();
currentGrid = addGrid(highImages, 10, 6, true); gridToScore(currentGrid);" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>Here is a set of image/text pairs with similarity scores &gt; 0.46</span></p><p><span>These are mostly pairs where the images contain text, and where the ALT tags match that text.</span></p></div>
<div id="2.28" +="" data-cuecode="removeAllGrids();
currentGrid = addGrid(multiImages, 20, 20, true);
gridToScore(currentGrid);" +="" data-upcode="" +="" data-mobilecuecode="removeAllGrids();
currentGrid = addGrid(lowImages, 16, 20, true);
gridToScore(currentGrid);" +="" data-keyframe="TRUE"><p><span>Looking across LAION-5B, it&#39;s clear that these CLIP scores are very unequally distributed.</span></p><p><span>Very often the scores are near LAION&#39;s chosen threshold.</span></p></div>
<p><span>A full 16% of the total images across all subsets have scores within 0.1 of the lower bounds.</span></p>
<p><span>This means if the LAION developers had chosen to nudge the CLIP similarity threshold up by just 0.01, they would have removed 937,489,831 pairs from the set.
</span></p>
<div id="2.31" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>The tiniest of shifts in LAION&#39;s thresholds could have excluded or included <em>hundreds of millions of images.</em> </span></p><p><span>What the images contain plays no role at all in deciding what stays and what goes.</span></p></div>
<p><span>This is what curation by statistics looks like: tiny tweaks to code can have profound effects on the content of training sets, and on the models that use them to shape their computational worldview.</span></p>
<p><span>There are two important things we can learn here.</span></p>
<p><span><b>First, that algorithmic curation commonly depends on numeric thresholds which are very often poorly understood.</b></span></p>
<div id="2.34" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>For decades datasets were constructed by human intervention. This generally yielded datasets that are of high quality but too small to make today&#39;s LLM’s yield meaningful results.</span></p><p><span>LAION set out to build a dataset for these newer, hungrier models. They built a dataset that is purely constructed by machine processes, by running models and tweaking thresholds: LAION-5B is made by measure.</span></p></div>
<p><span>But what is getting measured? The quality of data? The capacities of CLIP? The success of a model against a benchmark? The benchmark itself?</span></p>
<p><span><b>Second, that there is a circularity inherent to the authoring of AI training sets.</b></span></p>
<p><span>Because they need to be so large, their construction necessarily involves the use of other models, which themselves were trained on algorithmically curated training sets. </span></p>
<div id="2.375" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>Consider LAION-5B’s similarity score. It is the result of a model (CLIP) trained on a dataset (which OpenAI does not disclose). </span></p><p><span>To arrive at a threshold, LAION made another model, trained on a fraction of their own dataset, and compared it to benchmarks (e.g. ImageNet-1K). </span></p></div>
<p><span>The gold standard for this benchmark was set in 2020 by a third model, a widely-used neural network called ResNet50.</span></p>
<p><span><b>There are models on top of models, and trainings sets on top of training sets.</b></span></p>
<p><span>Omissions and biases and blind spots from these stacked-up models and training sets shape all of the resulting new models and new training sets.</span></p>
<p><span>It&#39;s only by looking at datasets that we can get a better sense of how AI models work, and the gaps, errors, and biases that can emerge. </span></p>
<p><span>Part 3: LAION-5B&#39;s Great Divide</span></p>
<div id="3.01" +="" data-cuecode="multiGrid = addGrid(lang20, 15, 19, true, windowWidth * 0.3, windowHeight * 0.75, windowWidth * 0.33, 0);" +="" data-upcode="removeGrid(noGrid);" +="" data-mobilecuecode="removeGrid(engGrid);
noGrid = addGrid(multiImages, 10, 15, true, windowWidth * 0.3, windowHeight * 0.35, windowWidth * 0.66, 0);
" +="" data-keyframe=""><p><span>It&#39;s convenient to refer to LAION-5B in the singular: as one gigantic training set.</span></p><p><span>In reality, most researchers who train models with LAION-5B use a subset of the data, constructed with a specific purpose or task in mind.</span></p></div>
<p><span>Indeed, there is no way to directly download all of LAION-5B; instead you need to choose one of three language subsets.</span></p>
<div id="3.03" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>
<h2>LAION-2B EN</h2>
<p>
2.3 billion image-text pairs where the text was algorithmically identified as English.
</p>
</span><span>
<h2>LAION-2B MULTI</h2>
<p>
2.26 billion image-text pairs where the text was algorithmically identified as a non-English language.
</p>
</span><span>
<h2>LAION-1B NOLANG</h2>
<p>
1.27 billion image-text pairs where the language couldn&#39;t be detected by the algorithm, or the confidence level was too low.
</p>
</span></p>
</div>
<p><span>To make these subsets, LAION developers again relied on a machine learning model developed by Google called CLD3 (Compact Language Detector 3) to classify the language of the ALT attribute into one of 107 language classes.</span></p>
<p><span>Beside English, Russian is the most common language that can be found in the set. Russian is followed by French and then German.</span></p>
<p><span>For each of the 255 million Russian speakers on the planet, there is one image in the dataset labeled as Russian.</span></p>
<p><span>For every 2 French speakers there is one image caption labeled as French.</span></p>
<p><span>For every 35 of the 71.6 million Swahili speakers on Earth, there is one image caption in LAION-5B labeled as Swahili.</span></p>
<p><span>On the other hand, for every English speaker on the planet there are 1.6 captions labeled as English.</span></p>
<p><span>For every Dutch speaker there are 3 captions in LAION-5B labeled as Dutch.</span></p>
<p><span>For every Icelandic speaker there are 7 image captions labeled as Icelandic.</span></p>
<p><span>These statistics tell us less about the composition of the originating dataset - Common Crawl - than they do about the shortcomings of the language detection model the LAION-5B developers chose to use.</span></p>
<div id="3.13" +="" data-cuecode="" +="" data-upcode="removeAllGrids();" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>The language distribution differs significantly between Common Crawl and LAION-5B.</span></p><p><span>That is, the quantity of pages in a language in Common Crawl may not match the number of images in LAION-5B for the same language.</span></p></div>
<div id="3.14" +="" data-cuecode="luxGrid = addGrid(luxImagesFull, 12, 10, true);
" +="" data-upcode="" +="" data-mobilecuecode="luxGrid = addGrid(luxImagesFull, 6, 8, true);
" +="" data-keyframe="TRUE"><p><span>To give a stark example, there are 34,270,773 text captions in LAION-5B which CLD3 has classified as Luxembourgian: a language that only 300,000 people speak. This is 1 caption for every 114 images.</span></p><p><span>Meanwhile there are only around 53,500 pages classified as Luxembourgian in the Common Crawl: 1 for every 58,000 pages.</span></p></div>
<p><span>Looking at a set of supposedly Luxembourgian image pairs, we can quickly see that the ALT text is mostly English or in a different language altogether.</span></p>
<div id="3.16" +="" data-cuecode="removeGrid(luxGrid);
addCrawler(luxSmall, [9, 5], 50, [3, 0], 33, [],[&#34;r&#34;,&#34;d&#34;,&#34;d&#34;,&#34;r&#34;]);
startCrawlers();" +="" data-upcode="" +="" data-mobilecuecode="removeGrid(luxGrid);
addCrawler(luxSmall, [4, 5], 50, [2, 0], 33, [],[&#34;r&#34;,&#34;d&#34;,&#34;d&#34;,&#34;r&#34;]);
startCrawlers();" +="" data-keyframe="TRUE"><p><span>Exactly why CLD3 fails toward Luxembourgish is a question for another investigation (and another training set). </span></p><p><span>It does, however, offer a particularly clear example of how LAION’s ‘hands-off’ processes fail.</span></p></div>
<p><span>Despite the fact that many English captions seem to have been mis-classified as other languages, it&#39;s obvious that LAION-5B as a whole prioritizes English content.</span></p>
<p><span>This prioritization can also be observed in the fundament that LAION-5B is built upon – Common Crawl, where about 45% of websites have primarily English language content.
</span></p>
<div id="3.19" +="" data-cuecode="stopCrawlers();
engGrid = addGrid(cfiles, 12, 17, true, windowWidth * 0.48, windowHeight, 0, 0);
multiGrid = addGrid(multiImages, 12, 17, true, windowWidth * 0.47, windowHeight, windowWidth * 0.53, 0);
gridToLang(engGrid);
gridToLang(multiGrid);" +="" data-upcode="" +="" data-mobilecuecode="stopCrawlers();
engGrid = addGrid(cfiles, 7, 10, true, windowWidth * 0.48, windowHeight, 0, 0);
multiGrid = addGrid(multiImages, 7, 10, true, windowWidth * 0.47, windowHeight, windowWidth * 0.53, 0);
gridToLang(engGrid);
gridToLang(multiGrid);" +="" data-keyframe="TRUE"><p><span>This split tells us something specific about the worldview that LAION-5B contains; a perspective that is carried into AI models that are trained on it.</span></p><p><span>For models trained on LAION-5B, English (and English-speaking culture) is valued more than the other 107 languages combined.</span></p></div>
<div id="3.2" +="" data-cuecode="" +="" data-upcode="engGrid = addGrid(cfiles, 12, 17, true, windowWidth * 0.48, windowHeight, 0, 0);
multiGrid = addGrid(multiImages, 12, 17, true, windowWidth * 0.47, windowHeight, windowWidth * 0.53, 0);
gridToLang(engGrid);
gridToLang(multiGrid);" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>The creators of LAION-5B are aware of the representative shortcomings of Common Crawl, when it comes to language and cultural representation.</span></p><p><span>But they deem the situation workable. Their whole endeavour, after all, is a research project, not to be used to create &#39;ready-to-go industrial projects&#39;.</span></p></div>
<p><span>Yet the LAION team has created other subsets of their really big dataset, built to satisfy the very particular needs of consumer-facing generative AIs. </span></p>
<p><span>Part 4: Made to Fit</span></p>
<p><span>Besides the language subsets, LAION has released several other datasets based on LAION-5B targeted toward specific purposes.</span></p>
<div id="4.02" +="" data-cuecode="stopCrawlers();
crawlers = [];
aGrid2 = addGrid(aesthetic, 20, 15, true, windowWidth, windowHeight, 0, 0);" +="" data-upcode="" +="" data-mobilecuecode="stopCrawlers();
crawlers = [];
aGrid2 = addGrid(aesthetic, 10, 15, true, windowWidth, windowHeight, 0, 0);" +="" data-keyframe=""><p><span>Perhaps the most interesting of these is LAION-Aesthetics, a subset intended to include images that have &#34;high visual quality&#34;.</span></p><p><span>Ask Midjourney or Stable Diffusion to generate an image for you, and you’ll get a result that has been fine-tuned on this subset of LAION.</span></p></div>
<p><span>The model used to build this subset was trained on three sources: 15,000 images of logos, as well as two different batches of images that were rated by humans to be visually appealing.</span></p>
<p><span>One batch of images came from a training set called Simulacra Aesthetic Captions (SAC). This set contains synthetic images, produced by Generative AIs including CompVis latent GLIDE and Stable Diffusion.</span></p>
<p><span>These synthetic images – 238,000 of them – were rated by users of the Discord communities for GLIDE and Stable Diffusion.</span></p>
<div id="4.07" +="" data-cuecode="stopCrawlers();
sacGrid = addGrid(sacHigh, 6, 5, true, windowWidth, windowHeight, 0, 0);" +="" data-upcode="removeAllGrids();" +="" data-mobilecuecode="stopCrawlers();
sacGrid = addGrid(sacHigh, 3, 5, true, windowWidth, windowHeight, 0, 0);" +="" data-keyframe="TRUE"><p><span>Users were asked to rate images on a scale of 1 to 10.</span></p><p><span>These images are a sample of the 176,000 top scoring ones in the set.</span></p></div>
<p><span>The creators of SAC are transparent about the shortcomings of the set, specifically the fact that the scores were submitted by users who were both WEIRD (Western, Educated, Industrialized, Rich, and Democratic) and developers of AI art, a demographic they describe as leaning toward &#34;nerdy&#34; and &#34;esoteric.&#34;</span></p>
<p><span>Furthermore, they admit that most of the ratings in the dataset were submitted by a &#34;handful of users,&#34; whose &#34;aesthetic preferences dominate the dataset.&#34;</span></p>
<div id="4.1" +="" data-cuecode="removeAllGrids();" +="" data-upcode="removeAllGrids();" +="" data-mobilecuecode="" +="" data-keyframe="TRUE"><p><span>Another of the aesthetic training sets uses images scored on the website dpchallenge.com, which is described as a &#34;digital photography challenge&#34;. </span></p><p><span>250,000 images were scraped from the website, along with user-contributed ratings, for the Aesthetic Visual Analysis (AVA) dataset.</span></p></div>
<p><span>Here is a sample of the highest rating images from the set.</span></p>
<div id="4.11" +="" data-cuecode="removeGrid(dpGrid1);
dpGrid2 = addGrid(dpusers, 7, 4, true, windowWidth, windowHeight, 0, 0);" +="" data-upcode="" +="" data-mobilecuecode="removeGrid(dpGrid1);
dpGrid2 = addGrid(dpusers, 3, 4, true, windowWidth, windowHeight, 0, 0);" +="" data-keyframe="TRUE"><p><span>dpchallenge.com posts a leaderboard of image reviewers.</span></p><p><span>The top 50 reviewers, responsible for more than 7.5 million total reviews, appear to fall neatly in the WEIRD demographic. </span></p></div>
<div id="4.12" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>Of the 41 users who share location info, 95% are in the US, Canada, or Europe.</span></p><p><span>They are, mostly, middle-aged photography enthusiasts from small American cities.</span></p></div>
<div id="4.13" +="" data-cuecode="removeGrid(dpGrid2);" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>Using the SAC set, LAION-Logos and AVA images, LAION developers trained a model called LAION-Aesthetics_Predictor V2. </span></p><p><span>This model produces an aesthetic score based on the output from the CLIP model&#39;s analysis of an image.</span></p></div>
<p><span>They used this model to score the English language set (2.3 billion images) and released a series of subsets with different score thresholds.</span></p>
<p><span>The smallest of these subsets, with 600 million images scoring 5 or higher, was used by Midjourney to fine-tune the results of their model, with the goal to produce output that would be more appealing to the users of their tool.
</span></p>
<div id="4.16" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>Here we find another truth about generative AI: </span></p><p><span><b>The concepts of what is and isn&#39;t visually appealing can be influenced in outsized ways by the tastes of a very small group of individuals, and the processes that are chosen by dataset creators to curate the datasets.</b></span></p></div>
<p><span>In the case of Midjourney, by a handful of esoteric nerds, and by a 65-year old mechanical engineer living in Southeastern Wisconsin.</span></p>
<p><span>Part 5: Big is the new small.</span></p>
<p><span>Over the last two years, generative AI models have forced people to ask hard questions about ownership and about safety.</span></p>
<p><span>At the root of these conversations are training sets like LAION-5B, whose contents cross all manner of boundaries (both ethical and legal).</span></p>
<p><span>The LAION-5B training set does address both ownership and safety, albeit in a typically statistical fashion.</span></p>
<div id="" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>For image ownership, the LAION-5B set offers a score indicating the probability that the image is watermarked, and a flag for NSFW content.</span></p><p><span>Both of these metrics were produced by models created by LAION.</span></p></div>
<div id="5.05" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>The researchers admit that these models are &#34;not perfect.&#34; </span></p><p><span>Again, the caveat: that the metrics should not be used to create “production-ready” subsets.</span></p></div>
<p><span>This is a convenient way to avoid responsibility, and leans heavily on a core philosophy of software-based research: that if you make the problems visible, someone down the line will step up and fix them.</span></p>
<div id="5.07" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>In their paper, the LAION devs &#34;advocate using these tags responsibly,&#34; to not rely on them for making &#34;truly safe&#34; versions of their dataset.</span></p><p><span>Beyond that, no advice is given about what responsible use might look like.</span></p></div>
<p><span>Responsible use might start with a more careful look at how these scores connect to the content of the pages from which the images and text captions were collected.</span></p>
<div id="5.09" +="" data-cuecode="" +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>There is one thing in particular that LAION get right: they publish their datasets as open-source. </span></p><p><span>In the field of AI they are alone in doing so. It is what allowed us to dissect LAION-5B in the first place.</span></p></div>
<p><span>Openness in the AI field matters, not just for model biases, but for the structural biases in the ecosystem. An ongoing problem is that curation by statistics amplifies many of those structural biases.</span></p>
<p><span>Driven by investments going into the trillions, datasets and AI models, which are too complex or large to be truly understood, are being deployed with a break-neck speed.</span></p>
<div id="5.11" +="" data-cuecode="addCrawler(aesthetic, [9, 7], 50, [4, 4], 17); " +="" data-upcode="" +="" data-mobilecuecode="" +="" data-keyframe=""><p><span>As artists, academics, practitioners, or as journalists, dataset investigation is one of the few tools we have available to gain insight and understanding into the most complex systems ever conceived by humans.</span></p><p><span>This is why advocating for dataset transparency is so important if AI systems are ever going to be accountable for their impacts in the world. </span></p></div>
<div id="5.12" +="" data-cuecode="undefined" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="undefined"><p><span>LAION-5B has, since the CSAM findings in December, been unavailable for download.</span></p><p><span>The developers say they are working on remediating it.</span></p></div>
<p><span>In the meantime, they&#39;ve released a new dataset of images and textcalled CommonPool, which contains 12.8 billion image pairs.
</span></p>
<p><span>12.8 billion images pairs, culled from Common Crawl.</span></p>
<p><span>12.8 billion images pairs, scored by CLIP.</span></p>
<p><span>12.8 billion images pairs, curated by statistics.</span></p>
<div id="5.17" +="" data-cuecode="stopCrawlers();" +="" data-upcode="undefined" +="" data-mobilecuecode="undefined" +="" data-keyframe="TRUE"><p><span>This piece was created by Knowing Machines, a research project tracing the histories, practices, and politics of how machine learning systems are trained to interpret the world. </span></p><p><span><a href="https://knowingmachines.org" target="blank">knowingmachines.org</a></span></p><p><span>Special thanks to Kate Crawford and Michael Weinberg.</span></p></div>
<p><span>.</span></p>
</article>
</section></div>
  </body>
</html>
