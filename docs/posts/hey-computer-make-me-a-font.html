<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font">Original</a>
    <h1>Hey, computer, make me a font</h1>
    
    <div id="readability-page-1" class="page"><div><p>This is a story of my journey learning to build generative ML models from scratch and teaching a computer to create fonts in the process. Yes, genuine <em>true type fonts</em>, with a capital-only set of glyphs. The model takes a font description as an input, and produces a font file as an output. I named the project &#39;FontoGen&#39;.</p>
<p>Here are a few examples of fonts generated by the FontoGen model:</p>
<p><span><p>bold, sans</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG?</p></span>
<span><p>italic, serif</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG!</p></span>
<span><p>techno, sci-fi, extrabold</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.</p></span></p><p>If you want to generate your very own font, head to the <a href="https://github.com/SerCeMan/fontogen">GitHub project</a>, clone it, and don’t forget to leave a star. Then download the weights from <a href="https://huggingface.co/SerCe/fontogen">Huggingface</a>, and follow the instructions here. And if you want to learn the full story, keep reading.</p>
<p><img src="https://blog.plover.com/images/make-me-a-font/bender.jpeg" alt=""/></p>
<div><blockquote><p>I&#39;m gonna go build my own theme park</p></blockquote><div><p>— </p><!-- --><p>Bender Bending Rodríguez</p></div></div>
<h2 id="intro"><a href="#intro"><span></span></a>Intro</h2>
<p>At the beginning of 2023, when AI started creating ripples across the internet, like many others I became very interested in the topic. I was sucked into the world of making memes with Stable Diffusion, training LoRAs on my friends’ faces, and fine-tuning text-to-speech models to mimic famous voices.</p>
<p>At some point, I started looking at text-to-SVG generation which, as it turned out, is a much harder task compared to raster-based text-to-image generation. Not only is the format itself quite complex, it also allows for representing the exact same shape in many different ways. As I was interested in learning how to build a generative ML model from scratch, this became my weekend project.</p>
<h2 id="the-idea"><a href="#the-idea"><span></span></a>The Idea</h2>
<p>As I began exploring different ways to generate SVGs, I came across the IconShop<a href="#references"><sup>2</sup></a> paper which achieved pretty impressive results. It took me some time to reproduce them by building a model based on the description in the paper. After finally achieving close-enough results, I realised that the process of generating fonts could be similar to the process of generating SVGs, and started working on the project.</p>
<figure><img src="https://blog.plover.com/images/make-me-a-font/fontogen.png"/><figcaption>The final result</figcaption></figure>
<p>Compared to SVG images, fonts are both easier and harder to generate. The easier part is that fonts don’t have the colour component present in colourful SVG images. However, the harder part is that a single font consists of many glyphs, and all glyphs in a font must maintain stylistic consistency. Maintaining consistency turned out to be a significant challenge which I&#39;ll describe in more detail below.</p>
<h2 id="the-model-architecture"><a href="#the-model-architecture"><span></span></a>The Model Architecture</h2>
<p>Inspired by the SVG generation approach described in the IconShop paper, the model is a sequence-to-sequence model trained on sequences that consist of text embeddings followed by font embeddings.</p>
<figure><img src="https://blog.plover.com/images/make-me-a-font/embeddings.png"/><figcaption>Input Sequence</figcaption></figure>
<h3 id="text-embeddings"><a href="#text-embeddings"><span></span></a>Text Embeddings</h3>
<p>To produce text embeddings, I used a pre-trained BERT encoder model, which helps to capture the &#34;meaning&#34; of the prompt. The text sequence is limited to 16 tokens, which in BERT’s case roughly corresponds to the same number of words. While the text prompt could potentially be longer, memory constraints were a significant concern for my single-GPU setup. So, all textual font descriptions present in the dataset were summarised to a set of a few keywords with the help of OpenAI’s GPT-3.</p>
<h3 id="font-embeddings"><a href="#font-embeddings"><span></span></a>Font Embeddings</h3>
<p>In order to produce font embeddings, the fonts first need to be converted to a sequence of tokens similar to how text is tokenised with the BERT tokeniser. In this project, I’ve only considered the glyph shapes and ignored the width, height, offset, and other useful metadata present in the font files. Each glyph was downsampled to 150x150 and normalised. I found that the 150x150 dimension preserves font features with minimal glyph deformation, which was more pronounced at lower resolutions.</p>
<p><img src="https://blog.plover.com/images/make-me-a-font/glyphs.png" alt=""/></p>
<p>I used Python’s <a href="https://github.com/fonttools/fonttools">fonttools</a> to parse font files which can conveniently process each glyph as a <a href="https://fonttools.readthedocs.io/en/latest/pens/recordingPen.html#fontTools.pens.recordingPen.DecomposingRecordingPen">sequence</a> of curves, lines, and move commands, where each command can be followed by zero or more points. I decided to limit the glyph set to the following glyphs to get a minimal usable font.</p>
<pre><code>ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?
</code></pre>
<p>The final model vocabulary needed to represent 22547 different tokens:</p>
<ul>
<li>40 glyphs,</li>
<li>5 line path operations: <em>moveTo</em>, <em>lineTo</em>, <em>qCurveTo</em>, <em>curveTo</em>, <em>closePath</em>,</li>
<li>2 tokens to represent EOS (end of sequence) and PAD (padding),</li>
<li>150^2 = 22500 different points.</li>
</ul>
<figure><img src="https://blog.plover.com/images/make-me-a-font/font-encoding.png"/><figcaption>An example font token sequence.</figcaption></figure>
<p>The token sequence is then converted into an embedding vector using learnable embedding matrices. Additionally, as proposed in the SkexGen<a href="#references"><sup>1</sup></a> paper, separate matrices were used specifically for <em>x</em> and <em>y</em> coordinates. And the final step was to apply positional embeddings.</p>
<h3 id="transformer"><a href="#transformer"><span></span></a>Transformer</h3>
<p>The model is an autoregressive encoder-only transformer consisting of 16 layers and 8 blocks. The model’s dimension is 512, resulting in a total of 73.7 million parameters.</p>
<pre><code>  | Name  | Type     | Params
-----------------------------------
0 | model | Fontogen | 73.7 M
-----------------------------------
73.7 M    Trainable params
0         Non-trainable params
73.7 M    Total params
294.728   Total estimated model params size (MB)
</code></pre>
<p>I computed the loss using simple cross-entropy and disregarded the padding token.</p>
<h3 id="attention"><a href="#attention"><span></span></a>Attention</h3>
<p>Every time a part of the glyph is generated, several factors influence the decision on which token comes next. First, the model prompt affects the glyph’s shape. Next, the model needs to consider all previously generated tokens for that glyph. Finally, it needs to take into account all other glyphs generated so far to ensure consistency in style.</p>
<p>When doing initial experiments with only a handful of glyphs, I started with full attention. However, as the sequence length increased, this approach became impractical, prompting a shift to sparse attention. After exploring various options, I settled on BigBird<a href="#references"><sup>3</sup></a> attention. This approach supports both global attention, to focus on the initial prompt, and window attention, which observes N previous tokens, capturing the style of several preceding glyphs.</p>
<figure><img src="https://blog.plover.com/images/make-me-a-font/bigbird.png"/><figcaption>BigBird attention</figcaption></figure>
<p>Given that a single glyph can have a variable number of tokens, I set the attention mechanism to consider at least the 3 preceding glyphs. While most of the time, the approach has been successful at preserving the overall font style, in some complex cases, the style would slowly drift into unrecoverable mess.</p>
<figure><img src="https://blog.plover.com/images/make-me-a-font/calfailure.png"/><figcaption>Calligraphy is hard</figcaption></figure>
<h2 id="training"><a href="#training"><span></span></a>Training</h2>
<p>To train the model, I assembled a dataset of 71k distinct fonts. 60% of all fonts only had a vague category assigned to them, while 20% fonts were accompanied by longer descriptions, so the descriptions were condensed to a few keywords using GPT-3.5. Additionally, I included 15% fonts where the prompt only contained the font&#39;s name, and the remaining 5% of the dataset had an empty textual description assigned to them to ensure that the model is capable of generating fonts with no prompt at all.</p>
<p>Due to large memory requirements, my Nvidia 4090 with 24G of VRAM could only fit two font sequences in a single batch, and I’d often observe gradient explosions. Using gradient accumulation and gradient clipping helped to resolve the issue. The model was trained for 50 epochs which took 127 hours. I restarted training once after 36 epochs, and kept training for another 14 epochs with reduced gradient accumulation. The training was stopped when the validation loss showed very little improvements.</p>
<div><div><figure><img src="https://blog.plover.com/images/make-me-a-font/loss.png"/><figcaption>Validation loss of the first 36 epochs</figcaption></figure></div><div><figure><img src="https://blog.plover.com/images/make-me-a-font/loss2.png"/><figcaption>Validation loss of the remaining 14 epochs</figcaption></figure></div></div>
<h2 id="chasing-performance"><a href="#chasing-performance"><span></span></a>Chasing Performance</h2>
<p>Achieving good training performance was critical since I was training on a single GPU, and training took a significant amount of time.</p>
<ul>
<li>In the initial iteration, I processed font files and textual descriptions directly within the model on each step. While this codebase structure streamlined prototyping, it meant that the same tasks had to be repeated over and over again, making the training process slower. Additionally, having BERT loaded in memory meant that it would take up precious VRAM. By shifting as much as possible to the dataset preprocessing stage, I achieved a threefold performance boost.</li>
<li>Originally, the model relied on huggingface&#39;s transformers. Migrating the code to xformers<a href="#references"><sup>4</sup></a> gave a very visible boost in speed and memory usage.</li>
</ul>
<h2 id="instead-of-conclusion"><a href="#instead-of-conclusion"><span></span></a>Instead Of Conclusion</h2>
<p>I achieved what I set out to do – I learned how to build a generative transformer model, and built a project that&#39;s capable of generating fonts as a side effect. But there are so many things that I still haven&#39;t tried. For example, what if the model could be integrated into the existing font editors so that the font designer only creates a single glyph A, and all other glyphs are generated by the model. Or maybe the font editor could suggest the control points for bézier curves as they&#39;re being drawn! The horizon is vast, and there&#39;s much left to explore.</p>
<p>If you&#39;ve read this article, and you think that I&#39;ve overlooked something obvious, there&#39;s a good chance I did! I&#39;m always keen to learn more, so please reach out and let me know what you think.</p>
<h2 id="thank-you-to"><a href="#thank-you-to"><span></span></a>Thank you to</h2>
<ul>
<li><a href="https://twitter.com/ptuls">Paul Tune</a> for answering many questions I had about building transformer models.</li>
</ul>
<h2 id="references"><a href="#references"><span></span></a>References</h2>
<ol>
<li><a href="https://arxiv.org/abs/2207.04632">SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks</a></li>
<li><a href="https://arxiv.org/abs/2304.14400">IconShop: Text-Guided Vector Icon Synthesis with Autoregressive
Transformers</a></li>
<li><a href="https://arxiv.org/abs/2007.14062">Big Bird: Transformers for Longer Sequences</a></li>
<li><a href="https://github.com/facebookresearch/xformers">xFormers: A modular and hackable Transformer modelling library</a></li>
</ol>
<h2 id="discuss-on"><a href="#discuss-on"><span></span></a>Discuss on</h2>
<ul>
<li><a href="https://twitter.com/SerCeMan/status/1708799954700181912">Twitter</a></li>
<li><a href="https://news.ycombinator.com/item?id=37750859">Hacker News</a></li>
</ul><div><h2>Subscribe</h2><div><p>I&#39;ll be sending an email every time I publish a new post.</p><div><form></form></div><p>Or, subscribe with <a href="https://blog.plover.com/feed.xml">RSS</a>.</p></div></div></div></div>
  </body>
</html>
