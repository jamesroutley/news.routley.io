<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2306.07899">Original</a>
    <h1>33-46% of workers on MTurk used LLMs in a text production task</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2306.07899">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Large language models (LLMs) are remarkable data annotators. They can be used
to generate high-fidelity supervised training data, as well as survey and
experimental data. With the widespread adoption of LLMs, human gold--standard
annotations are key to understanding the capabilities of LLMs and the validity
of their results. However, crowdsourcing, an important, inexpensive way to
obtain human annotations, may itself be impacted by LLMs, as crowd workers have
financial incentives to use LLMs to increase their productivity and income. To
investigate this concern, we conducted a case study on the prevalence of LLM
usage by crowd workers. We reran an abstract summarization task from the
literature on Amazon Mechanical Turk and, through a combination of keystroke
detection and synthetic text classification, estimate that 33-46% of crowd
workers used LLMs when completing the task. Although generalization to other,
less LLM-friendly tasks is unclear, our results call for platforms,
researchers, and crowd workers to find new ways to ensure that human data
remain human, perhaps using the methodology proposed here as a stepping stone.
Code/data: <a href="https://github.com/epfl-dlab/GPTurk" rel="external noopener nofollow">this https URL</a>

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Veniamin Veselovsky [<a href="https://arxiv.org/show-email/497284fe/2306.07899">view email</a>]
      </p></div></div>
  </body>
</html>
