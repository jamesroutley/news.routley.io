<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ounapuu.ee/posts/2025/10/06/datablocks-white-label-drives/">Original</a>
    <h1>Testing two 18 TB white label SATA hard drives from datablocks.dev</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      <p>This post is <em><strong>NOT</strong></em> sponsored, the products were bought with my hard-earned money.</p>
<p>I’ve been running a full SSD storage setup for a few years in my home server and I’ve been happy with it, except for the
storage anxiety that I get with running small pools of fast storage, which is why I started looking at how the hard
drive market is doing.</p>
<p>Half of tech YouTube has been sponsored by companies like ServerPartDeals, so they were one of the first
places I looked at, but they seem to only operate within the US and the shipping+taxes destroy any price advantages from
ordering there to Estonia (which is in Europe).</p>
<p>At some point I stumbled upon <a href="https://datablocks.dev/">datablocks.dev</a>, which seems to operate within a similar niche,
but in Europe and on a much smaller scale. What caught my eye were their white label hard drive offerings. Their website
has a good explanation on
the <a href="https://datablocks.dev/blogs/news/white-label-vs-recertified-drives">differences between recertified and white label hard drives.</a>
In short: white label drives have no branding, have no or very low number of power-on hours, may have small scratches or
dents, but are in all other aspects completely functional and usable.</p>
<p>White label drives also have a price advantage compared to branded recertified drives. Here’s one example with 18 TB
drives, the recertified one is 16.7% more expensive compared to the white label one, and the only obvious difference
seems to be the sticker on the drive. I highly suspect that the white label one is also manufactured by Seagate based on
the physical similarities.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/pricing.png">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/pricing_hu_901f087494d71f4c.png" width="711" height="673" alt="The price difference between a recertified and a white label drive."/>
    </a>
    <figcaption>
      The price difference between a recertified and a white label drive.
    </figcaption>
    
</figure>

<p>I took some time to think things over and compared the pricing of various drives. The drives were all competitively
priced between each other, with the price per terabyte hovering around 13 EUR/TB, so it didn’t matter much which drive
size you picked, you’d still get a pretty solid deal. It was also a better deal compared to using an WD Elements/My Book
drive of the same size.</p>
<p>I decided to go with two 18 TB hard drives. I considered buying the 20 TB or 22 TB capacities, but decided to go with 18
TB because it’s the largest single hard drive that I can easily and quickly buy a replacement for in the form of a WD
Elements/My Book drive.</p>
<p>The stock on <code>datablocks.dev</code> is quite volatile, the drives are in stock when new batches arrive, but they can also
quickly go out of stock. I saw this live with the 22 TB hard drives, one day there are 35 left, the next day there can
be 7 left, and then only one lone drive.</p>
<p>At the time of writing, the 18 TB model that I bought is out of stock, so my choice to go with a slightly smaller but
more easily replaceable one is validated.</p>
<p>For those that have followed my blog for a while will know that I’m a huge fan of all-SSD server
builds, <a href="https://www.jeffgeerling.com/blog/2024/radxas-sata-hat-makes-compact-pi-5-nas">especially this one by Jeff Geerling that I still consider building from time to time.</a>
If I dislike noise, higher power usage and slower performance, then why did I get the hard drives? It’s simple, really:
I now have an actual closet that I can stash my home server in, meaning that noise isn’t that big of a worry, and as
long as my home server takes about the same amount of power as my refrigerator or dishwasher, then that’s fine.
SSD prices still haven’t gone down as much as I’ve hoped over the years, so the all-SSD build ideas that I have are way
outside my budget.</p>
<p>The drives arrived in a reasonable time window. The packaging was adequate, although I was slightly concerned with the
cardboard box showing signs of something hitting it hard. The drives were packaged within sealed antistatic bags, and
with ample bubble wrap surrounding them.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-1.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-1_hu_f0725a4d78717af7.jpg" width="751" height="1000" alt="The cardboard box with a slight dent."/>
    </a>
    <figcaption>
      The cardboard box with a slight dent.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-2.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-2_hu_c84204beb3309c10.jpg" width="1000" height="751" alt="Plenty of paper inside to prevent the drives from flying around."/>
    </a>
    <figcaption>
      Plenty of paper inside to prevent the drives from flying around.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-3.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-3_hu_bf97e9f84eb7ed8d.jpg" width="1000" height="751" alt="Drives were wrapped in bubble wrap, with the drives themselves also separated with a few layers of it for maximum
protection."/>
    </a>
    <figcaption>
      Drives were wrapped in bubble wrap, with the drives themselves also separated with a few layers of it for maximum
protection.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-4.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/packaging-4_hu_778fe7f902c613c.jpg" width="1000" height="751" alt="Drives in anti-static bags."/>
    </a>
    <figcaption>
      Drives in anti-static bags.
    </figcaption>
    
</figure>

<p>Just as described, the drives did have slight scratches and very minor dents in them, but in all other aspects they
looked like new.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-0.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-0_hu_3a4c602cd17b2eb5.jpg" width="751" height="1000" alt="One of the hard drives. It does have slight dents and scratches, matching the description."/>
    </a>
    <figcaption>
      One of the hard drives. It does have slight dents and scratches, matching the description.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-1.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-1_hu_1fe40988ba084b3b.jpg" width="751" height="1000" alt="The second drive had a more noticeable bump in it."/>
    </a>
    <figcaption>
      The second drive had a more noticeable bump in it.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-backside.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-backside_hu_de8d9d5cfe95fcbc.jpg" width="1000" height="751" alt="The backside of the drives."/>
    </a>
    <figcaption>
      The backside of the drives.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-wd-adapter.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drive-wd-adapter_hu_93ea28c0f2aa53a1.jpg" width="1000" height="751" alt="Those USB-SATA adapters from shucking are really darn handy now. Adapter courtesy of my brother-in-law."/>
    </a>
    <figcaption>
      Those USB-SATA adapters from shucking are really darn handy now. Adapter courtesy of my brother-in-law.
    </figcaption>
    
</figure>

<p>Before putting them to use, I formatted the drives using <code>badblocks</code>. It took a full 24 hours to do a full drive write.
The write performance peaked at 275 MB/s and slowed down to 123 MB/s at the end, which is expected.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/performance.png">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/performance_hu_d7fe6d01eb254fb8.png" width="1000" height="300" alt="The performance of the drive during the full drive format."/>
    </a>
    <figcaption>
      The performance of the drive during the full drive format.
    </figcaption>
    
</figure>

<p>I also had to choose a larger block size for <code>badblocks</code> because otherwise it could not handle the drive, resulting in
the command
being <code>badblocks -wsv -b 8192 /dev/sdX</code>.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/peak-performance.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/peak-performance_hu_f327bec7166114c9.jpg" width="1000" height="751" alt="This is what peak jank looks like."/>
    </a>
    <figcaption>
      This is what peak jank looks like.
    </figcaption>
    
</figure>

<p>I unfortunately did not save the SMART data from the time I received the drives, but the contents were as expected,
there were no more than a few power on hours and other metrics were OK. Keep in mind that it’s also possible to reset
SMART data on a drive so this information cannot be taken at face value.</p>
<p>The drives are noisy, as expected. They run at 7200 RPM and do the usual clicks and clacks that a normal hard drive
does. If this bothers you, <a href="https://nickdrozd.github.io/posts/2021/04/02/tech-tip-1/">use foam to fix it.</a> The soft side of a sponge can work
just as well.</p>
<p>With these drives <a href="https://nickdrozd.github.io/posts/2023/03/26/tiered-storage/">I’ve now followed my own advice</a> and tiered my storage: two 1 TB
SSD-s for the things that benefit from good speed and latency (databases, containers), and 18 TB hard drives for
bulk storage, backups and less frequently used data. Coming from an all-SSD build, I expected the performance to drop in
day-to-day operations, but in most cases I cannot tell a difference. My family photos load just fine, media plays back
well, and backups take slightly longer, which isn’t noticeable due to them running during the night. Only when I look at
the Prometheus node exporter graphs do I notice that sometimes the server is waiting behind the disks a bit more due to
higher <code>iowait</code>.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/iowait.png">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/iowait_hu_5fbd1ca7071b9432.png" width="1000" height="452" alt="During full backups or disk scrubs, the iowait is more prevalent on graphs (the red part), but that doesn&#39;t seem to
impact my other workloads in a significant way."/>
    </a>
    <figcaption>
      During full backups or disk scrubs, the iowait is more prevalent on graphs (the red part), but that doesn&#39;t seem to
impact my other workloads in a significant way.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drives-in-use-0.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drives-in-use-0_hu_53aac14bee14b07b.jpg" width="1000" height="751" alt="The drives are connected via two WD Elements/My Book USB-SATA adapters, over USB 3.0, and stored right below my ThinkPad
T430, which is proudly running as my home server."/>
    </a>
    <figcaption>
      The drives are connected via two WD Elements/My Book USB-SATA adapters, over USB 3.0, and stored right below my ThinkPad
T430, which is proudly running as my home server.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drives-in-use-1.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/drives-in-use-1_hu_5cef07c017dab5af.jpg" width="751" height="1000" alt="I added glue-on rubber feet on the stand to make sure the drives do not accidentally slip off anywhere. It does nothing
to reduce the noise, though, and I&#39;m convinced that it&#39;s actually making the noise worse."/>
    </a>
    <figcaption>
      I added glue-on rubber feet on the stand to make sure the drives do not accidentally slip off anywhere. It does nothing
to reduce the noise, though, and I&#39;m convinced that it&#39;s actually making the noise worse.
    </figcaption>
    
</figure>








  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/home-server.jpg">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/home-server_hu_7516862d99af8cca.jpg" width="1000" height="751" alt="I&#39;m not proud of the lack of cable management, but this setup works well. Given how often I get new ideas, it doesn&#39;t
make sense to organize this too much anyway."/>
    </a>
    <figcaption>
      I&#39;m not proud of the lack of cable management, but this setup works well. Given how often I get new ideas, it doesn&#39;t
make sense to organize this too much anyway.
    </figcaption>
    
</figure>

<p>The power usage did shoot up as a result, roughly 10-20 W. Not ideal, but my whole networking and home server setup is
idling at below 45 W, and I’ve
had <a href="https://nickdrozd.github.io/posts/2021/03/17/server-setups-throughout-the-years/">less efficient home servers in the past,</a> so it’s not that
big of a deal.</p>







  




<figure>
    
    <a href="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/setup-power-usage.png">
        <img src="https://nickdrozd.github.io/posts/2025/10/06/datablocks-white-label-drives/media/setup-power-usage_hu_fcad3c54990505d5.png" width="1000" height="601" alt="The power usage was elevated while I was formatting and copying files over to the new drives, but after that it&#39;s
stabilized to around 1.2 kWh per day."/>
    </a>
    <figcaption>
      The power usage was elevated while I was formatting and copying files over to the new drives, but after that it&#39;s
stabilized to around 1.2 kWh per day.
    </figcaption>
    
</figure>

<p>In this configuration, the drives run quite cool. During formatting on a hot day, I saw them go up to a maximum of 51°C,
but in general use they sit at around 38-42°C.</p>
<p>Overall, I’m reasonably happy with the drives. I expect these to last me at least 5 years, and I’m probably going to
switch one of the drives out a bit sooner to reduce the risk of a full drive pool failure. They’ve made it the first 50
days, so that’s good!</p>


    </div></div>
  </body>
</html>
