<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/o40/seesay">Original</a>
    <h1>Show HN: I created a PoC for live descriptions of the surroundings for the blind</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">I wanted to see if I could create a low-cost tool for the blind to get live description of the scene in front of a camera.</p>
<p dir="auto">The idea is to have images taken at a set interval, which are then described using an AI model, and read back to the user using voice synthesis.</p>
<p dir="auto">Since I was going for low cost (&lt;30$), and wanted to learn more about software development on arduino, I bought a ESP32-CAM with built-in WiFi to capture the images.</p>
<p dir="auto">To describe the image I selected the gpt-4o-mini model. I didn&#39;t think much about which model to use, but this seemed like a good start.</p>
<p dir="auto">The proof-of-concept solution works, but has some short-comings.</p>
<ul dir="auto">
<li>The cell-phone needs to have a web page open where the descriptions are updated, to have it read back.</li>
<li>The camera/powerbank currently needs to be held in hands, and is not easy to mount. Having the camera in glasses would be optimal.</li>
<li>Security is not considered for this PoC, so anyone can access the web page if they know the URL.</li>
</ul>

<p dir="auto">One driving force for creating this tool is how expensive the alternatives are. However alternatives seem to be emerging at the moment so there is hope.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Product</th>
<th>Price</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ray-Ban Meta Glasses</td>
<td>$300</td>
<td>No app available for visual descriptions (yet). Closed API. Can maybe be hacked.</td>
</tr>
<tr>
<td>EchoVision Smart Glasses</td>
<td>$599</td>
<td>Interesting product that seem to do what I want. Unclear how it actually works. Only available for pre-order.</td>
</tr>
<tr>
<td>Envision Glasses</td>
<td>$3200</td>
<td>Google Glass Enterprise Edition 2 hardware</td>
</tr>
<tr>
<td>OrCam MyEye</td>
<td>$5900</td>
<td>Clunky solution that you attach to glasses</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Use a cell phone with internet sharing enabled that the ESP32-CAM connects to. Then software on the ESP32-CAM uploads images to a HTTP server, which then provides the image the OpenAI API which returns the description of the image. The description is updated on a page in the HTTP server which the cellphone has open.</p>
<p dir="auto">When new text arrives, the voice-over (for iPhone) reads the updated text to the user.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/overview.png"><img src="https://github.com/o40/seesay/raw/main/overview.png" alt="alt text"/></a></p>
<p dir="auto">Setup using a powerbank as battery source for the ESP32-CAM.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/hw.png"><img src="https://github.com/o40/seesay/raw/main/hw.png" alt="alt text"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Installing arduino software to ESP32-CAM</h2><a id="user-content-installing-arduino-software-to-esp32-cam" aria-label="Permalink: Installing arduino software to ESP32-CAM" href="#installing-arduino-software-to-esp32-cam"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The source code for the ESP32-CAM is located in the &#34;esp32&#34; folder. Open it using the Arduino IDE. Update the WiFi credentials and target HTTP server before flashing. Some links that may help with installing the ESP32-CAM drivers.</p>
<p dir="auto"><a href="https://randomnerdtutorials.com/upload-code-esp32-cam-mb-usb/" rel="nofollow">https://randomnerdtutorials.com/upload-code-esp32-cam-mb-usb/</a>
<a href="https://randomnerdtutorials.com/installing-esp32-arduino-ide-2-0/" rel="nofollow">https://randomnerdtutorials.com/installing-esp32-arduino-ide-2-0/</a></p>
<p dir="auto">Set the baud-rate to 115200 in the serial monitor to get the output from the ESP32-CAM.</p>

<p dir="auto">Set the OpenAI API KEY as an environment variable before starting the web server. You need to update the server IP:PORT in both templates/index.html and server.py.</p>
<p dir="auto">Powershell</p>
<div dir="auto" data-snippet-clipboard-copy-content="$Env:API_KEY = &#34;sk-proj-BLABLABLA...&#34;"><pre><span>$<span>Env:</span>API_KEY</span> <span>=</span> <span><span>&#34;</span>sk-proj-BLABLABLA...<span>&#34;</span></span></pre></div>
<p dir="auto">Bash</p>
<div dir="auto" data-snippet-clipboard-copy-content="export API_KEY = &#34;sk-proj-BLABLABLA...&#34;"><pre><span>export</span> API_KEY = <span><span>&#34;</span>sk-proj-BLABLABLA...<span>&#34;</span></span></pre></div>
<p dir="auto">Then the server can be started:</p>

<p dir="auto">If you only want to test prompts for the description of the image, test.py can be used to just send a saved image without involving the ESP32 hardware.</p>

<p dir="auto">Initially I tested the solution by feeding an image to the OpenAI API manually to see that I get a decent response, and that the latency is low. The latency was less than 2 seconds, and the description seemed ok, so I went out for a walk with the powerbank/camera in one hand and the cellphone in the other.</p>

<p dir="auto">The first test was somewhat successful. The camera updated every 10 seconds and descriptions was pushed to the page as expected. However the descriptions contained a lot that is not interesting in this context. For example weather and guessing where in the world the photo was taken.</p>
<p dir="auto">Prompt: &#34;What’s in this scene and where? Less text is better.&#34;</p>

<p dir="auto"><em>&#34;The scene shows a park area with a sign reading &#34;Aseparken.&#34; There are trees and a pathway, likely in a semi-urban or rural location. The weather appears overcast.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-28_142000.276.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-28_142000.276.jpg" alt="Image"/></a></p>

<p dir="auto"><a href="https://github.com/o40/seesay/blob/main/test_1.txt">Full log</a></p>

<p dir="auto">Updated to a longer prompt to reduce &#34;fluff&#34; in the descriptions.</p>
<p dir="auto">Prompt: &#34;Give a short description of the image and where objects are located in the image. Do not mention that this is an image. Do not mention weather or geographical location. Less text is better.&#34;</p>
<p dir="auto">The descriptions became much better and is sometimes useful, but the prompt can be improved for sure. Due to what I think is the low quality of the camera the descriptions are more &#34;gloomy&#34; than they are.</p>
<p dir="auto">The API cost for the ~25 minute walk was $0.23 where one image was describes every 7 seconds.</p>

<p dir="auto"><em>&#34;A pathway extends ahead, flanked by trees on both sides. To the left, a bench is positioned alongside the path. There is a slight curve in the path leading into the distance.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-31_114137.446.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-31_114137.446.jpg" alt="Image"/></a></p>
<p dir="auto"><em>&#34;A grassy slope is in the background, with pathways leading to a fork on the left and right. On the left side, there’s a signpost. In the foreground, there is a small object lying on the ground. A building is visible on the right side.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-31_114646.763.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-31_114646.763.jpg" alt="Image"/></a></p>
<p dir="auto"><em>&#34;A pathway runs through the center, with grass on both sides. Toward the far left, a person is near the edge of the grass. In the background, there are vehicles and a few trees on the left. A sign structure is visible on the right side.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-31_115126.815.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-31_115126.815.jpg" alt="Image"/></a></p>
<p dir="auto">&#34;<em>A person walks on a path holding the handle of a stroller. Another individual walks alongside them. To the left, there are trees, and a light pole stands beside the path. In the background, there are grassy areas and some structures.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-31_115306.805.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-31_115306.805.jpg" alt="Image"/></a></p>
<p dir="auto"><em>&#34;A dirt path leads into a grassy area, flanked by trees. To the left, there are two magpies on the ground. In the background, a brick building is visible, with additional structures further down the path on the right side. The scene has a sparse, open feel.&#34;</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/o40/seesay/blob/main/images/2024-12-31_115508.385.jpg"><img src="https://github.com/o40/seesay/raw/main/images/2024-12-31_115508.385.jpg" alt="Image"/></a></p>

<p dir="auto"><a href="https://github.com/o40/seesay/blob/main/test_2.txt">Full log</a></p>

<p dir="auto">I reached my goals of learning more about arduino development, and what can be done with low quality images in combination with an AI image description service.</p>
<p dir="auto">A productified version of this would be beneficial for the vision impaired for sure, where the camera is attached to glasses, and has a higher quality camera.</p>
<p dir="auto">If I find glasses with an open API that is capable of continuously providing images like in this project (but with higher quality), then I would be interested in continuing development of a solution like this. Even if it is only for own use and not for productifying/selling.</p>
<p dir="auto">Feel free to message me to discuss the project or suggest hardware that can do what I want.</p>
</article></div></div>
  </body>
</html>
