<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.zdnet.com/article/ibm-open-sources-its-granite-ai-models-and-they-mean-business/">Original</a>
    <h1>IBM open-sources its Granite AI models – and they mean business</h1>
    
    <div id="readability-page-1" class="page"><div><div><figure><div><div><picture><source media="(max-width: 767px)" srcset="https://www.zdnet.com/a/img/resize/7aa9a547099fa270971e6c174880bed49f07ad64/2024/05/13/1c956d0d-78f9-4c6f-a7e8-80a8e579bcc8/models56gettyimages-2148123501.jpg?auto=webp&amp;width=768" alt="models56gettyimages-2148123501"/><source media="(max-width: 1023px)" srcset="https://www.zdnet.com/a/img/resize/fa55478ee07726514e4698188568428a11f9a37c/2024/05/13/1c956d0d-78f9-4c6f-a7e8-80a8e579bcc8/models56gettyimages-2148123501.jpg?auto=webp&amp;width=1024" alt="models56gettyimages-2148123501"/><source media="(max-width: 1440px)" srcset="https://www.zdnet.com/a/img/resize/74cd3a1f04ae214d6fd81ff75d3dda36e9be041e/2024/05/13/1c956d0d-78f9-4c6f-a7e8-80a8e579bcc8/models56gettyimages-2148123501.jpg?auto=webp&amp;width=1280" alt="models56gettyimages-2148123501"/> <img src="https://www.zdnet.com/a/img/resize/74cd3a1f04ae214d6fd81ff75d3dda36e9be041e/2024/05/13/1c956d0d-78f9-4c6f-a7e8-80a8e579bcc8/models56gettyimages-2148123501.jpg?auto=webp&amp;width=1280" alt="models56gettyimages-2148123501" width="1280" height="656.4102564102564" fetchpriority="low"/></picture></div> <!----></div> <figcaption><div></div> <span>BlackJack3D/Getty Images</span></figcaption></figure><p>Open-sourcing large language models (LLMs) isn&#39;t easy. Just ask the <a href="https://opensource.org/" target="_blank" rel="noopener nofollow">Open Source Initiative</a> (OSI), which has been working on an AI-compatible <a href="https://www.zdnet.com/article/open-source-initiative-expands-its-role-to-ai-and-machine-learning/" rel="follow">open-source definition</a> for nearly two years. Some companies -- Meta, for example -- <a href="https://www.zdnet.com/article/meta-releases-big-new-open-source-ai-large-language-model/" rel="follow">claim to have open-sourced</a> their LLMs. (They haven&#39;t.) But, now <a href="https://research.ibm.com/blog/granite-code-models-open-source" target="_blank" rel="noopener nofollow">IBM has gone ahead and done it</a>. </p><p>IBM managed the open sourcing of <a href="https://github.com/ibm-granite/granite-code-models" target="_blank" rel="noopener nofollow">Granite code</a> by using pretraining data from publicly available datasets, such as GitHub Code Clean, Starcoder data, public code repositories, and GitHub issues. In short, IBM has gone to great lengths to avoid copyright or legal issues. The Granite Code Base models are trained on 3- to 4-terabyte tokens of code data and natural language code-related datasets. </p><p><strong>Also: <a href="https://www.zdnet.com/article/this-is-why-open-source-generative-ai-models-are-a-step-behind-gpt-4/" rel="follow">Why open-source generative AI models are still a step behind GPT-4</a></strong></p><p>All these models are licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="noopener nofollow">Apache 2.0 license</a> for research and commercial use. It&#39;s that last word -- commercial -- that stopped the other major LLMs from being open-sourced. No one else wanted to share their LLM goodies. </p><p>But, as IBM Research chief scientist Ruchir Puri said, &#34;We are transforming the generative AI landscape for software by releasing the highest performing, cost-efficient code LLMs, empowering the open community to innovate without restrictions.&#34;</p><p>Without restrictions, perhaps, but not without specific applications in mind. </p><!----><p>The Granite models, as IBM ecosystem general manager Kate Woolley said last year, are not &#34;about trying to be everything to everybody. <a href="https://www.crn.com/news/cloud/ibm-channel-chief-woolley-our-ai-is-not-for-writing-poems-about-your-dog" target="_blank" rel="noopener nofollow">This is not about writing poems about your dog.</a> This is about curated models that can be tuned and are very targeted for the business use cases we want the enterprise to use. Specifically, they&#39;re for programming.&#34;</p><p>These decoder-only models, trained on code from 116 programming languages, range from 3 to 34 billion parameters. They support many developer uses, from complex application modernization to on-device memory-constrained tasks.</p><p>IBM has already used these LLMs internally in <a href="https://www.ibm.com/products/watsonx-code-assistant" target="_blank" rel="noopener nofollow">IBM Watsonx Code Assistant (WCA)</a> products, such as<a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant/2.x_latest/html-single/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant_user_guide/index" target="_blank" rel="noopener nofollow"> WCA for Ansible Lightspeed</a> for IT Automation and <a href="https://www.ibm.com/architectures/hybrid/genai-code-generation-z" target="_blank" rel="noopener nofollow">WCA for IBM Z</a> for modernizing COBOL applications. Not everyone can afford Watsonx, but now, anyone can work with the Granite LLMs using <a href="https://developers.redhat.com/articles/2024/05/07/instructlab-open-source-generative-ai" target="_blank" rel="noopener nofollow">IBM and Red Hat&#39;s InstructLab</a>. </p><p><strong>Also: <a href="https://www.zdnet.com/article/best-ai-chatbot/" rel="follow">The best AI chatbots: ChatGPT and alternatives</a></strong></p><p>As Red Hat SVP and chief product officer Ashesh Badani said, InstructLab will &#34;lower many of the barriers facing GenAI across the hybrid cloud, from limited data science skills to the sheer resources required.&#34; The point is to lower the entry level for developers who want to use LLMs. </p><p>How low? As Matt Hicks said at the <a href="https://www.redhat.com/en/summit" target="_blank" rel="noopener nofollow">Red Hat Summit</a>, &#34;Capabilities that, just a year ago, were coupled to high-end, fairly exotic hardware can now run on a laptop. Training techniques that once ran in the hundreds of millions of dollars are now being replicated for a few thousand.&#34; </p><p>For example, besides InstructLab, you can use Ollma to run LLMs locally. As Bala Priya C explains in <a href="https://www.kdnuggets.com/" target="_blank" rel="noopener nofollow">KDnuggets</a>, &#34;With Ollama, everything you need to run an LLM -- model weights and all of the config -- is packaged into a single Modelfile. <a href="https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple" target="_blank" rel="noopener nofollow">Think Docker for LLMs</a>.&#34; The models are available on platforms like <a href="https://huggingface.co/" target="_blank" rel="noopener nofollow">Hugging Face</a>, <a href="https://github.com/" target="_blank" rel="noopener nofollow">GitHub</a>, <a href="http://watsonx.ai" target="_blank" rel="noopener nofollow">Watsonx.ai</a>, and <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai" target="_blank" rel="noopener nofollow">Red Hat Enterprise Linux (RHEL) AI</a>.</p><p>IBM anticipates that programmers, in addition to writing code with the Granite LLMs, will save time and energy by using these LLMs to create tests and find and fix bugs. &#34;Many of the quotidian but essential tasks that are part of a developer&#39;s day -- from generating unit tests to writing documentation or running vulnerability tests -- could be automated with these models.  </p><p><strong>Also: <a href="https://www.zdnet.com/article/ai21-and-databricks-show-open-source-can-radically-slim-down-ai/" rel="follow">AI21 and Databricks show open source can radically slim down AI</a></strong></p><p>Besides helping developers, IBM sees business benefits in Granite models because, unlike many others, their licensing is clear, as is how the models have been trained. In addition, the data has been cleaned and filtered for hate, abuse, and profane language.</p><p>So, if your company has hesitated to explore using AI to build programs for legal reasons, IBM has just provided you with the open-source tools you&#39;ll need to improve your software development work. Give them a try. Some of you will build great things from these Granite blocks. </p></div></div></div>
  </body>
</html>
