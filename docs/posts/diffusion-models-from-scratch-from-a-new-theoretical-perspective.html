<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.chenyang.co/diffusion.html">Original</a>
    <h1>Diffusion models from scratch, from a new theoretical perspective</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
<article>
<h2>Diffusion models from scratch, from a new theoretical perspective</h2>
<section>
\[\newcommand{\Kset}{\mathcal{K}}
\newcommand{\distK}{ {\rm dist}_{\Kset} }
\newcommand{\projK}{ {\rm proj}_{\Kset} }
\newcommand{\eps}{\epsilon}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\norm}[1]{\left\lVert #1 \right\lVert}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\softmin}{softmin}
\DeclareMathOperator{\distop}{dist}\]
<p>Diffusion models have recently produced impressive results in generative
modeling, in particular sampling from multimodal distributions. Not only has
diffusion models seen widespread adoption in text-to-image generation tools such
as <a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion</a>, they
also excel in other application domains such as
<a href="https://text-to-audio.github.io/">audio</a>/<a href="https://openai.com/research/video-generation-models-as-world-simulators">video</a>/<a href="https://zero123.cs.columbia.edu/">3D</a>
generation, <a href="https://www.nature.com/articles/s41586-023-06415-8">protein
design</a>, <a href="https://diffusion-policy.cs.columbia.edu/">robotics path
planning</a>, all of which require
sampling from multimodal distributions.</p>
<p>This tutorial aims to introduce diffusion models from an optimization
perspective as introduced in <a href="https://arxiv.org/abs/2306.04848">our paper</a> (joint work with <a href="https://www.mit.edu/~fperment/">Frank
Permenter</a>). It will go over both theory and
code, using the theory to explain how to implement diffusion models from
scratch. By the end of the tutorial, you will learn how to implement training
and sampling code for a toy dataset, which will also work for larger datasets
and models.</p>
<p>In this tutorial we will mainly reference code from
<a href="https://github.com/yuanchenyang/smalldiffusion"><code>smalldiffusion</code></a>. For
pedagogical purposes, the code presented here will be simplified from the
<a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/src/smalldiffusion/">original library code</a>, which is on its own well-commented
and easy to read.</p>
<h4 id="training-diffusion-models">Training diffusion models</h4>
<p>Diffusion models aim to generate samples from a set that is learned from
training examples, which we will denote by \(\mathcal{K}\). For example, if we
want to generate images, \(\mathcal{K} \subset \mathbb{R}^{c\times h \times w}\)
is the set of pixel values that correspond to realistic images. Diffusion models
also work for \(\mathcal{K}\) corresponding to modalities other than images,
such as audio, video, robot trajectories, and even in discrete domains such as
text generation.</p>
<p>In a nutshell, diffusion models are trained by:</p>
<ol>
<li>Sampling \(x_0 \sim \mathcal{K}\), noise level \(\sigma \sim [\sigma_\min,
\sigma_\max]\), noise \(\epsilon \sim N(0, I)\)</li>
<li>Generating noisy data \(x_\sigma = x_0 + \sigma \epsilon\)</li>
<li>Predicting \(\epsilon\) (direction of noise) from \(x_\sigma\) by minimizing squared loss</li>
</ol>
<p>This amounts to training a \(\theta\)-parameterized neural network
\(\epsilon_\theta(x, \sigma)\), by minimizing the loss function</p>
\[\Loss(\theta) = \mathop{\mathbb{E}}
\lVert\epsilon_\theta(x_0 + \sigma_t \epsilon, \sigma_t) - \epsilon \lVert^2\]
<p>In practice, this is done by the following simple <code>training_loop</code>:</p>
<div><div><pre><code><span>def</span> <span>training_loop</span><span>(</span><span>loader</span>  <span>:</span> <span>DataLoader</span><span>,</span>
                  <span>model</span>   <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
                  <span>schedule</span><span>:</span> <span>Schedule</span><span>,</span>
                  <span>epochs</span>  <span>:</span> <span>int</span> <span>=</span> <span>10000</span><span>):</span>
    <span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>for</span> <span>x0</span> <span>in</span> <span>loader</span><span>:</span>
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            <span>sigma</span><span>,</span> <span>eps</span> <span>=</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>,</span> <span>schedule</span><span>)</span>
            <span>eps_hat</span> <span>=</span> <span>model</span><span>(</span><span>x0</span> <span>+</span> <span>sigma</span> <span>*</span> <span>eps</span><span>,</span> <span>sigma</span><span>)</span>
            <span>loss</span> <span>=</span> <span>nn</span><span>.</span><span>MSELoss</span><span>()(</span><span>eps_hat</span><span>,</span> <span>eps</span><span>)</span>
            <span>optimizer</span><span>.</span><span>backward</span><span>(</span><span>loss</span><span>)</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
</code></pre></div></div>
<p>The training loop iterates over batches of <code>x0</code>, then samples noise level
<code>sigma</code> and noise vector <code>eps</code> using <code>generate_train_sample</code>:</p>
<div><div><pre><code><span>def</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span>schedule</span><span>:</span> <span>Schedule</span><span>):</span>
    <span>sigma</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_batch</span><span>(</span><span>x0</span><span>)</span>
    <span>eps</span> <span>=</span> <span>torch</span><span>.</span><span>randn_like</span><span>(</span><span>x0</span><span>)</span>
    <span>return</span> <span>sigma</span><span>,</span> <span>eps</span>
</code></pre></div></div>
<h5 id="noise-schedules">Noise schedules</h5>
<p>In practice, \(\sigma\) is not sampled uniformly from the interval \([\sigma_\min,
\sigma_\max]\), instead this interval is discretized into \(N\) distinct values
called a <em>\(\sigma\) schedule</em>: \(\{ \sigma_t \}_{t=1}^N\), and \(\sigma\) is instead
sampled uniformly from the \(N\) possible values of \(\sigma_t\). We define the
<code>Schedule</code> class that encapsulates the list of possible <code>sigmas</code>, and sample
from this list during training.</p>
<div><div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>sigmas</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>):</span>
        <span>self</span><span>.</span><span>sigmas</span> <span>=</span> <span>sigmas</span>
    <span>def</span> <span>__getitem__</span><span>(</span><span>self</span><span>,</span> <span>i</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>.</span><span>sigmas</span><span>[</span><span>i</span><span>]</span>
    <span>def</span> <span>__len__</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
        <span>return</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>sigmas</span><span>)</span>
    <span>def</span> <span>sample_batch</span><span>(</span><span>self</span><span>,</span> <span>x0</span><span>:</span><span>torch</span><span>.</span><span>FloatTensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>[</span><span>torch</span><span>.</span><span>randint</span><span>(</span><span>len</span><span>(</span><span>self</span><span>),</span> <span>(</span><span>x0</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],))].</span><span>to</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div></div>
<p>In this tutorial, we will use a log-linear schedule defined below:</p>
<div><div><pre><code><span>class</span> <span>ScheduleLogLinear</span><span>(</span><span>Schedule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>N</span><span>:</span> <span>int</span><span>,</span> <span>sigma_min</span><span>:</span> <span>float</span><span>=</span><span>0.02</span><span>,</span> <span>sigma_max</span><span>:</span> <span>float</span><span>=</span><span>10</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>torch</span><span>.</span><span>logspace</span><span>(</span><span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_min</span><span>),</span> <span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_max</span><span>),</span> <span>N</span><span>))</span>
</code></pre></div></div>
<p>Other commonly used schedules include <code>ScheduleDDPM</code> for pixel-space diffusion
models and <code>ScheduleLDM</code> for latent diffusion models such as
Stable Diffusion. The following plot compares these three schedules with default
parameters.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/schedule.png" alt=""/>
</p>
<figcaption>A comparison plot of different diffusion schedules</figcaption>
</figure>
<h5 id="toy-example">Toy example</h5>
<p>In this tutorial we will start with a toy dataset used in one of the first
diffusion papers <a href="https://arxiv.org/abs/1503.03585">[Sohl-Dickstein
et.al. 2015]</a>, where \(\Kset \subset \R^2\)
are points sampled from a spiral. We first construct and visualize this dataset:</p>
<div><div><pre><code><span>dataset</span> <span>=</span> <span>Swissroll</span><span>(</span><span>np</span><span>.</span><span>pi</span><span>/</span><span>2</span><span>,</span> <span>5</span><span>*</span><span>np</span><span>.</span><span>pi</span><span>,</span> <span>100</span><span>)</span>
<span>loader</span>  <span>=</span> <span>DataLoader</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>2048</span><span>)</span>
</code></pre></div></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/swissroll.png" alt=""/>
</p>
<figcaption>Swissroll toy dataset</figcaption>
</figure>
<p>For this simple dataset, we can implement the denoiser using a multi-layer
perceptron (MLP):</p>
<div><div><pre><code><span>def</span> <span>get_sigma_embeds</span><span>(</span><span>sigma</span><span>):</span>
    <span>sigma</span> <span>=</span> <span>sigma</span><span>.</span><span>unsqueeze</span><span>(</span><span>1</span><span>)</span>
    <span>return</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>torch</span><span>.</span><span>sin</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>),</span>
                      <span>torch</span><span>.</span><span>cos</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>)],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>

<span>class</span> <span>TimeInputMLP</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dims</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>layers</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>in_dim</span><span>,</span> <span>out_dim</span> <span>in</span> <span>pairwise</span><span>((</span><span>dim</span> <span>+</span> <span>2</span><span>,)</span> <span>+</span> <span>hidden_dims</span><span>):</span>
            <span>layers</span><span>.</span><span>extend</span><span>([</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>in_dim</span><span>,</span> <span>out_dim</span><span>),</span> <span>nn</span><span>.</span><span>GELU</span><span>()])</span>
        <span>layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_dims</span><span>[</span><span>-</span><span>1</span><span>],</span> <span>dim</span><span>))</span>
        <span>self</span><span>.</span><span>net</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span><span>*</span><span>layers</span><span>)</span>
        <span>self</span><span>.</span><span>input_dims</span> <span>=</span> <span>(</span><span>dim</span><span>,)</span>

    <span>def</span> <span>rand_input</span><span>(</span><span>self</span><span>,</span> <span>batchsize</span><span>):</span>
        <span>return</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>batchsize</span><span>,)</span> <span>+</span> <span>self</span><span>.</span><span>input_dims</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>sigma_embeds</span> <span>=</span> <span>get_sigma_embeds</span><span>(</span><span>sigma</span><span>)</span>         <span># shape: b x 2
</span>        <span>nn_input</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>x</span><span>,</span> <span>sigma_embeds</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span> <span># shape: b x (dim + 2)
</span>        <span>return</span> <span>self</span><span>.</span><span>net</span><span>(</span><span>nn_input</span><span>)</span>

<span>model</span> <span>=</span> <span>TimeInputMLP</span><span>(</span><span>dim</span><span>=</span><span>2</span><span>,</span> <span>hidden_dims</span><span>=</span><span>(</span><span>16</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>16</span><span>))</span>
</code></pre></div></div>
<p>The MLP takes the concatenation of \(x \in \R^2\) and an embedding of the noise
level \(\sigma\), then predicts the noise \(\epsilon \in \R^2\). Although many
diffusion models use a sinusoidal positional embedding for \(\sigma\), the
simple two-dimensional embedding works just as well:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sigma_embedding.png" alt=""/>
</p>
<figcaption>Two-dimensional \(\sigma_t\) embedding</figcaption>
</figure>
<p>Now we have all the ingredients to train a diffusion model.</p>
<div><div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLogLinear</span><span>(</span><span>N</span><span>=</span><span>200</span><span>,</span> <span>sigma_min</span><span>=</span><span>0.005</span><span>,</span> <span>sigma_max</span><span>=</span><span>10</span><span>)</span>
<span>trainer</span>  <span>=</span> <span>training_loop</span><span>(</span><span>loader</span><span>,</span> <span>model</span><span>,</span> <span>schedule</span><span>,</span> <span>epochs</span><span>=</span><span>15000</span><span>)</span>
<span>losses</span>   <span>=</span> <span>[</span><span>ns</span><span>.</span><span>loss</span><span>.</span><span>item</span><span>()</span> <span>for</span> <span>ns</span> <span>in</span> <span>trainer</span><span>]</span>
</code></pre></div></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/training_loss.png" alt=""/>
</p>
<figcaption>Training loss over 15000 epochs, smoothed with moving average</figcaption>
</figure>
<p>The learned denoiser \(\eps_\theta(x, \sigma)\) can be visualized as a vector
field parameterized by the noise level \(\sigma\), by plotting \(x - \sigma
\eps_\theta(x, \sigma)\) for different \(x\) and levels of \(\sigma\).</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/predicted_eps_field.png" alt=""/>
</p>
<figcaption>Plot of predicted \(\hat{x}_0 = x - \sigma \eps_\theta(x, \sigma)\) for
different \(x\) and \(\sigma\)</figcaption>
</figure>
<p>In the plots above, the arrows point from each noisy datapoint \(x\) to the
“clean” datapoint predicted by the denoiser with noise level \(\sigma\). At high
levels of \(\sigma\), the denoiser tends to predict the mean of the data, but at
low noise levels the denoiser predicts actual data points, provided that its
input \(x\) is also close to the data.</p>
<p>How do we interpret what the denoiser is learning, and how do we create a
procedure to sample from diffusion models? We will next build a theory of
diffusion models, then draw on this theory to derive sampling algorithms.</p>
<h4 id="denoising-as-approximate-projection">Denoising as approximate projection</h4>
<p>The diffusion training procedure learns a denoiser \(\eps_\theta(x,
\sigma)\). In <a href="https://arxiv.org/abs/2306.04848">our paper</a>, we interpret the learned denoiser as an
approximate projection to the data manifold \(\Kset\), and the goal of the
diffusion process as minimizing the distance to \(\Kset\). This motivates us to
introduce a relative-error approximation model to analyze the convergence of
diffusion sampling algorithms. First we introduce some basic properties of
distance and projection functions.</p>
<h5 id="distance-and-projection-functions">Distance and projection functions</h5>
<p>The <em>distance function</em> to a set \(\Kset \subseteq \R^n\) is defined as</p>
\[\distK(x) := \min \{ \norm{x-x_0} : x_0 \in \Kset \}.\]
<p>The <em>projection</em> of \(x \in \R^n\), denoted \(\projK(x)\), is the set of points
that attain this distance:</p>
\[\projK(x) := \{ x_0 \in \Kset : \distK(x) = \norm{x-x_0} \}\]
<p>If \(\projK(x)\) is unique, the gradient of \(\distK(x)\), the direction of
steepest descent of the distance function, points towards this unique
projection:</p>
<p><strong>Proposition</strong> Suppose \(\Kset\) is closed and \(x \not \in \Kset\). If \(\projK(x)\)
is unique, then</p>
\[\nabla \frac{1}{2} \distK(x)^2 = \distK(x) \nabla \distK(x) = x-\projK(x).\]
<p>This tells us that if we can learn \(\nabla \distK(x)\) for every \(x\), we can
simply move in this direction to find the projection of \(x\) onto \(\Kset\).
One issue with learning this gradient is that \(\distK\) is not differentiable
everywhere, thus \(\nabla \distK\) is not a continuous function. To solve this
problem, we introduce a squared-distance function smoothed by a parameter
\(\sigma\) using the \(\softmin\) operator instead of \(\min\).</p>
\[\distop^2_\Kset(x, \sigma)
:= \substack{\softmin_{\sigma^2} \\ x_0 \in \Kset} \norm{x_0 - x}^2
= {\textstyle -\sigma^2 \log\left(\sum_{x_0 \in \Kset}
\exp\left(-\frac{\norm{x_0 - x}^2}{2\sigma^2}\right)\right)}\]
<p>The following picture from <a href="https://arxiv.org/pdf/2108.10480.pdf">[Madan and Levin
2022]</a> shows the contours of both the
distance function and its smoothed version.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/smoothed_dist_contour.png" alt=""/>
</p>
<figcaption>Smoothed distance function has continuous gradients</figcaption>
</figure>
<p>From this picture we can see that \(\nabla \distK(x)\) points toward the closest
point to \(x\) in \(\Kset\), and \(\nabla \distop^2(x, \sigma)\) points toward a
weighted average of points in \(\Kset\) determined by \(x\).</p>
<h5 id="ideal-denoiser">Ideal denoiser</h5>
<p>The ideal or optimal denoiser \(\epsilon^*\) for a particular noise level
\(\sigma\) is an exact minimizer of the training loss function. When the data is
a discrete uniform distribution over a finite set \(\Kset\), the ideal
denoiser has an exact closed-form expression given by:</p>
\[\eps^*(x_\sigma, \sigma) = \frac{\sum_{x_0 \in \Kset} (x_\sigma-x_0) \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}
{\sigma \sum_{x_0 \in \Kset} \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}\]
<p>From the above expression, we see that the ideal denoiser points towards a
weighted mean of all the datapoints in \(\Kset\), where the weight for each
\(x_0 \in \Kset\) determines the distance to \(x_0\). Using this expression, we
can also implement the ideal denoiser, which is computationally tractable for
small datasets:</p>
<div><div><pre><code><span>def</span> <span>sq_norm</span><span>(</span><span>M</span><span>,</span> <span>k</span><span>):</span>
    <span># M: b x n --(norm)--&gt; b --(repeat)--&gt; b x k
</span>    <span>return </span><span>(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>M</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span><span>**</span><span>2</span><span>).</span><span>unsqueeze</span><span>(</span><span>1</span><span>).</span><span>repeat</span><span>(</span><span>1</span><span>,</span><span>k</span><span>)</span>

<span>class</span> <span>IdealDenoiser</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dataset</span><span>:</span> <span>torch</span><span>.</span><span>utils</span><span>.</span><span>data</span><span>.</span><span>Dataset</span><span>):</span>
        <span>self</span><span>.</span><span>data</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>list</span><span>(</span><span>dataset</span><span>))</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>x</span> <span>=</span> <span>x</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>d</span> <span>=</span> <span>self</span><span>.</span><span>data</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>xb</span><span>,</span> <span>db</span> <span>=</span> <span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>d</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>sq_diffs</span> <span>=</span> <span>sq_norm</span><span>(</span><span>x</span><span>,</span> <span>db</span><span>)</span> <span>+</span> <span>sq_norm</span><span>(</span><span>d</span><span>,</span> <span>xb</span><span>).</span><span>T</span> <span>-</span> <span>2</span> <span>*</span> <span>x</span> <span>@</span> <span>d</span><span>.</span><span>T</span>
        <span>weights</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>functional</span><span>.</span><span>softmax</span><span>(</span><span>-</span><span>sq_diffs</span><span>/</span><span>2</span><span>/</span><span>sigma</span><span>**</span><span>2</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
        <span>return </span><span>(</span><span>x</span> <span>-</span> <span>torch</span><span>.</span><span>einsum</span><span>(</span><span>&#39;</span><span>ij,j...-&gt;i...</span><span>&#39;</span><span>,</span> <span>weights</span><span>,</span> <span>self</span><span>.</span><span>data</span><span>))</span><span>/</span><span>sigma</span>
</code></pre></div></div>
<p>For our toy dataset, we can plot the direction of \(\epsilon^*\) as predicted by
the ideal denoiser for different noise levels \(\sigma\):</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_eps_field.png" alt=""/>
</p>
<figcaption>Plot of direction of \(\eps^*(x, \sigma)\) for different \(x\) and
\(\sigma\)</figcaption>
</figure>
<p>From our plots we see that for large values of \(\sigma\), \(\epsilon^*\) points
towards the mean of the data, but for smaller values of \(\sigma\),
\(\epsilon^*\) points towards the nearest data-point.</p>
<p>One insight from our paper is that the ideal denoiser for a fixed \(\sigma\) is
equivalent to the gradient of a \(\sigma\)-smoothed squared-distance function:</p>
<p><strong>Theorem</strong> For all \(\sigma &gt; 0\) and \(x \in \R^n\), we have</p>
\[\frac{1}{2} \nabla_x \distop^2_\Kset(x, \sigma) = \sigma \eps^*(x, \sigma).\]
<p>This tells us that the ideal denoiser found by minimizing the diffusion training
objective \(\Loss(\theta)\) is in fact the gradient of a smoothed
squared-distance function to the underlying data manifold \(\Kset\). This
connection is key to motivating our interpretation that the denoiser is an
approximate projection.</p>
<h5 id="relative-error-model">Relative error model</h5>
<p>In order to analyze the convergence of diffusion sampling algorithms, we
introduced a relative error model which states that the projection predicted by
the denoiser \(x-\sigma \epsilon_{\theta}( x, \sigma)\) well approximates
\(\projK(x)\) when the input to the denoiser \(\sigma\) well estimates
\(\distK(x)/\sqrt{n}\). For constants \(1 &gt; \eta \ge 0\) and \(\nu \ge 1\), we
assume that</p>
\[\norm{ x-\sigma \epsilon_{\theta}( x, \sigma) - \projK(x)} \le \eta \distK(x)\]
<p>when \((x, \sigma)\) satisfies \(\frac{1}{\nu}\distK(x) \le \sqrt{n}\sigma \le
\nu \distK(x)\). In addition to the discussion about ideal denoisers above, this
error model is motivated by the following observations.</p>
<p><em>Low noise</em> When \(\sigma\) is small and the manifold hypothesis holds,
denoising approximates projection because most of the added noise is orthogonal
to the data manifold.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/low_noise.png" alt=""/>
</p>
<figcaption>When added noise is small, most of noise is orthogonal to tangent space
of manifold. Under the manifold hypothesis, denoising is approximately projection.</figcaption>
</figure>
<p><em>High noise</em> When \(\sigma\) is large relative to the diameter of \(\Kset\),
then any denoiser predicting any weighted mean of the data \(\Kset\) has small
relative error.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/high_noise.png" alt=""/>
</p>
<figcaption>When added noise is large compared to diameter of data, denoising and
projection point in the same direction</figcaption>
</figure>
<p>We also perform empirical tests of our error model for pre-trained diffusion
models on image datasets. The CIFAR-10 dataset is small enough for tractable
computation of the ideal denoiser. Our experiments show that for this dataset,
the relative error between the exact projection and ideal denoiser output is
small over sampling trajectories.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_denoiser_error.png" alt=""/>
</p>
<figcaption>Ideal denoiser well-approximates projection onto the CIFAR-10 dataset
under relative-error model</figcaption>
</figure>

<h4 id="sampling-from-diffusion-models">Sampling from diffusion models</h4>
<p>Given a learned denoiser \(\epsilon_\theta(x, \sigma)\), how do we sample from
it to obtain a point \(x_0 \in \Kset\)? Given noisy \(x_t\) and noise level
\(\sigma_t\), the denoiser \(\eps_\theta(x_t, \sigma_t)\) predicts \(x_0\) via</p>
\[\hat x_0^t := x_t - \sigma_t \eps_\theta(x_t, \sigma_t)\]
<p>Intuition from the relative error assumption tells us that we want to start with
\((x_T, \sigma_T)\) where \(\distK(x_T)/\sqrt{n} \approx \sigma_T\). This is
achieved by choosing \(\sigma_T\) to be large relative to the diameter of
\(\Kset\), and \(x_T\) sampled i.i.d. from \(N(0, \sigma_T)\), a Gaussian with
variance \(\sigma_T\). This ensures that \(x_T\) is far away from
\(\Kset\).</p>
<p>Although \(\hat x_0^T = x_T - \sigma_T \eps_\theta(x_T, \sigma_T)\) has small
relative error, the absolute error \(\distK(\hat x_0^T)\) can still be large as
\(\distK(x_T)\) is large. In fact, at high noise levels, the expression of the
ideal denoiser tells us that \(\hat x_0^T\) should be close to the mean of the
data \(\Kset\). We cannot obtain a sample close to \(\Kset\) with a single call
to the denoiser.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/denoise.png" alt=""/>
</p>
<figcaption>Sampling process iteratively calls the denoiser based on \(\sigma_t\)
schedule.</figcaption>
</figure>
<p>Thus we want to <em>iteratively call the denoiser</em> to obtain a sequence \(x_T,
\ldots, x_t, \ldots x_0\) using a pre-specified schedule of \(\sigma_t\), hoping
that \(\distK(x_t)\) decreases in concert with \(\sigma_t\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t-1}) \eps_\theta(x_t, \sigma_t)\]
<p>This is exactly the deterministic <a href="https://arxiv.org/abs/2010.02502">DDIM sampling algorithm</a>, though
presented in different coordinates through a change of variable. See <a href="https://arxiv.org/abs/2306.04848">Appendix A
of our paper</a> for more details and a proof of equivalence.</p>
<h4 id="diffusion-sampling-as-distance-minimization">Diffusion sampling as distance minimization</h4>
<p>We can interpret the diffusion sampling iterations as gradient descent on the
squared-distance function \(f(x) = \frac{1}{2} \distK(x)^2\). In a nutshell,</p>
<p><strong>DDIM is approximate gradient descent on \(f(x)\) with stepsize \(1-
\sigma_{t-1}/\sigma_t\), with \(\nabla f(x_t)\) estimated by \(\eps_\theta(x_t,
\sigma_t)\).</strong></p>
<p>How should we choose the \(\sigma_t\) schedule? This determines the number and
size of gradient steps we take during sampling. If there are too few steps,
\(\distK(x_t)\) might not decrease and the algorithm may not converge. On the
other hand, if we take many small steps, we need to evaluate the denoiser for as
many times, a computationally expensive operation. This motivates our definition
of admissible schedules.</p>
<p><strong>Definition</strong> An <em>admissible schedule</em> \(\{ \sigma_t \}_{t=0}^T\) ensures
\(\frac{1}{\nu} \distK(x_t) \le \sqrt{n} \sigma_t \le \nu \distK(x_t)\) holds at
each iteration. In particular, a geometrically decreasing (i.e. log-linear)
sequence of \(\sigma_t\) is an admissible schedule.</p>
<p>Our main theorem states that if \(\{\sigma_t\}_{t=0}^T\) is an admissible
schedule and \(\epsilon_\theta(x_t, \sigma_t)\) satisfies our relative error
model, the relative error can be controlled, and the sampling procedure aiming
to minimize distance converges.</p>
<p><strong>Theorem</strong> Let \(x_t\) denote the sequence generated by DDIM and suppose that
\(\nabla \distK(x)\) exists for all \(x_t\) and \(\distK(x_T) = \sqrt n
\sigma_T\). Then</p>
<ol>
<li>\(x_t\) is generated by gradient descent on the squared-distance function
with stepsize \(1 - \sigma_{t-1}/\sigma_{t}\)</li>
<li>\(\distK(x_{t})/\sqrt{n} \approx \sigma_{t}\) for all \(t\)</li>
</ol>
<p>Coming back to our toy example, we can find an admissible schedule by
subsampling from the original log-linear schedule, and implement the DDIM
sampler as follows:</p>
<div><div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>...</span>
    <span>def</span> <span>sample_sigmas</span><span>(</span><span>self</span><span>,</span> <span>steps</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>indices</span> <span>=</span> <span>list</span><span>((</span><span>len</span><span>(</span><span>self</span><span>)</span> <span>*</span> <span>(</span><span>1</span> <span>-</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>steps</span><span>)</span><span>/</span><span>steps</span><span>))</span>
                       <span>.</span><span>round</span><span>().</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>[</span><span>indices</span> <span>+</span> <span>[</span><span>0</span><span>]]</span>

<span>batchsize</span> <span>=</span> <span>2000</span>
<span>sigmas</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>20</span><span>)</span>
<span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>sig</span><span>,</span> <span>sig_prev</span> <span>in</span> <span>pairwise</span><span>(</span><span>sigmas</span><span>):</span>
    <span>eps</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>))</span>
    <span>xt</span> <span>-=</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_prev</span><span>)</span> <span>*</span> <span>eps</span>
</code></pre></div></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ddim.png" alt=""/>
</p>
<figcaption>Samples from 20-step DDIM</figcaption>
</figure>
<p>We see that most samples lie close to the original data, but there is room for
improvement. One way is to increase the number of DDIM steps, but this incurs an
additional computational cost. Next, we use our interpretation of diffusion
models to derive a more efficient sampler.</p>
<h4 id="improved-sampler-with-gradient-estimation">Improved sampler with gradient estimation</h4>
<p>Since \(\nabla \distK(x)\) is invariant between \(x\) and \(\projK(x)\), we aim
to minimize estimation error \(\sqrt{n} \nabla \distK(x) -
\epsilon_{\theta}(x_t, \sigma_t)\) with the update:</p>
\[\bar\eps_t = \gamma \eps_{\theta}(x_t, \sigma_t) + (1-\gamma) \eps_{\theta}(x_{t+1}, \sigma_{t+1})\]
<p>Intuitively, this update corrects any error made in the previous step using the
current estimate:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ge_step.png" alt=""/>
</p>
<figcaption>Our gradient estimation update step</figcaption>
</figure>
<p>This leads to faster convergence compared to the DDIM sampler, as seen from the
samples on our toy model lying closer to the original data.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ge.png" alt=""/>
</p>
<figcaption>Samples from 20-step gradient estimation sampler</figcaption>
</figure>
<p>Compared to the default DDIM sampler, our sampler can be interpreted as adding
momentum, causing the trajectory to potentially overshoot but converge faster.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_gamma.png" alt=""/>
</p>
<figcaption>Sampling trajectories varying momentum term \(\gamma\)</figcaption>
</figure>
<p>Empirically, adding noise during the generation process also improves the
sampling quality. In order to do so while sticking to our original \(\sigma_t\)
schedule, we need to denoise to a smaller \(\sigma_{t&#39;}\) then add back noise
\(w_t \sim N(0, I)\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t&#39;}) \epsilon_\theta(x_t, \sigma_t) + \eta w_t\]
<p>If we assume that \(\mathop{\mathbb{E}}\norm{w_t}^2 = \norm{\epsilon_\theta(x_t,
\sigma_t)}^2\), we should choose \(\eta\) so that the norm of the update is
constant in expectation. This leads to the choice of \(\sigma_{t-1} =
\sigma_t^\mu \sigma_{t-1}^{1-\mu}\) and \(\eta = \sqrt{\sigma_{t-1}^2 -
\sigma_{t&#39;}^2}\) where \(0 \le \mu &lt; 1\). When \(\mu = \frac{1}{2}\), we exactly
recover the <a href="https://arxiv.org/abs/2006.11239">DDPM sampler</a> (see <a href="https://arxiv.org/abs/2306.04848">Appendix A of
our paper</a> for a derivation).</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_mu.png" alt=""/>
</p>
<figcaption>Sampling trajectories varying amount of noise added during sampling</figcaption>
</figure>
<p>Our gradient estimation update can be combined with adding noise during
sampling. In summary, our full update step is</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t&#39;}) \bar\eps_t + \eta w_t\]
<p>The full sampler that generalizes DDIM (<code>gam=1, mu=0</code>), DDPM (<code>gam=1, mu=0.5</code>)
and our gradient estimation sampler (<code>gam=2, mu=0</code>) is implemented below.</p>
<div><div><pre><code><span>@torch.no_grad</span><span>()</span>
<span>def</span> <span>samples</span><span>(</span><span>model</span>      <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
            <span>sigmas</span>     <span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span># Iterable with N+1 values for N sampling steps
</span>            <span>gam</span>        <span>:</span> <span>float</span> <span>=</span> <span>1.</span><span>,</span>        <span># Suggested to use gam &gt;= 1
</span>            <span>mu</span>         <span>:</span> <span>float</span> <span>=</span> <span>0.</span><span>,</span>        <span># Requires mu in [0, 1)
</span>            <span>xt</span>         <span>:</span> <span>Optional</span><span>[</span><span>torch</span><span>.</span><span>FloatTensor</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
            <span>batchsize</span>  <span>:</span> <span>int</span> <span>=</span> <span>1</span><span>):</span>
    <span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
    <span>eps</span> <span>=</span> <span>None</span>
    <span>for</span> <span>i</span><span>,</span> <span>(</span><span>sig</span><span>,</span> <span>sig_prev</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>pairwise</span><span>(</span><span>sigmas</span><span>)):</span>
        <span>eps</span><span>,</span> <span>eps_prev</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>)),</span> <span>eps</span>
        <span>eps_av</span> <span>=</span> <span>eps</span> <span>*</span> <span>gam</span> <span>+</span> <span>eps_prev</span> <span>*</span> <span>(</span><span>1</span><span>-</span><span>gam</span><span>)</span>  <span>if</span> <span>i</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>eps</span>
        <span>sig_p</span> <span>=</span> <span>(</span><span>sig_prev</span><span>/</span><span>sig</span><span>**</span><span>mu</span><span>)</span><span>**</span><span>(</span><span>1</span><span>/</span><span>(</span><span>1</span><span>-</span><span>mu</span><span>))</span> <span># sig_prev == sig**mu sig_p**(1-mu)
</span>        <span>eta</span> <span>=</span> <span>(</span><span>sig_prev</span><span>**</span><span>2</span> <span>-</span> <span>sig_p</span><span>**</span><span>2</span><span>).</span><span>sqrt</span><span>()</span>
        <span>xt</span> <span>=</span> <span>xt</span> <span>-</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_p</span><span>)</span> <span>*</span> <span>eps_av</span> <span>+</span> <span>eta</span> <span>*</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>).</span><span>to</span><span>(</span><span>xt</span><span>)</span>
        <span>yield</span> <span>xt</span>
</code></pre></div></div>
<h4 id="large-scale-examples">Large-scale examples</h4>
<p>The training code above not only works for our toy dataset, they can also be
used to train image diffusion models from scratch. See <a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/examples/fashion_mnist.py">this
example</a> for an example of training on the FashionMNIST
dataset to get a second-place FID score on <a href="https://paperswithcode.com/sota/image-generation-on-fashion-mnist">this
leaderboard</a>:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/fashion-mnist-samples-large.png" alt=""/>
</p>
<figcaption>Samples from a diffusion model trained on the FashionMNIST dataset</figcaption>
</figure>
<p>The sampling code works without modifications to sample from state-of-the-art
pretrained latent diffusion models:</p>
<div><div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLDM</span><span>(</span><span>1000</span><span>)</span>
<span>model</span>    <span>=</span> <span>ModelLatentDiffusion</span><span>(</span><span>&#39;</span><span>stabilityai/stable-diffusion-2-1-base</span><span>&#39;</span><span>)</span>
<span>model</span><span>.</span><span>set_text_condition</span><span>(</span><span>&#39;</span><span>An astronaut riding a horse</span><span>&#39;</span><span>)</span>
<span>*</span><span>xts</span><span>,</span> <span>x0</span> <span>=</span> <span>samples</span><span>(</span><span>model</span><span>,</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>50</span><span>))</span>
<span>decoded</span>  <span>=</span> <span>model</span><span>.</span><span>decode_latents</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div></div>
<p>We can visualize the different effects of our momentum term \(\gamma\) on high
resolution text-to-image generation.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sd_examples.jpg" alt=""/>
</p>
<figcaption>Text-to-image samples using Stable Diffusion</figcaption>
</figure>
<h4 id="other-resources">Other resources</h4>
<p>Also recommended are the following blog posts on diffusion models:</p>
<ol>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">What are diffusion models</a> introduces
diffusion models from the discrete-time perspective of reversing a Markov
process</li>
<li><a href="https://yang-song.net/blog/2021/score/" target="_blank">Generative modeling by estimating gradients of the data
distribution</a> introduces diffusion models from
the continuous time perspective of reversing a stochastic differential
equation</li>
<li><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank">The annotated diffusion model</a> goes over
a pytorch implementation of a diffusion model in detail</li>
</ol>
<h4 id="citation">Citation</h4>
<p>If you found this blog useful, please consider citing our paper:</p>
<div><div><pre><code>@article{permenter2023interpreting,
  title={Interpreting and improving diffusion models using the euclidean distance function},
  author={Permenter, Frank and Yuan, Chenyang},
  journal={arXiv preprint arXiv:2306.04848},
  year={2023}
}
</code></pre></div></div>

</section>
</article>
</div></div>
  </body>
</html>
