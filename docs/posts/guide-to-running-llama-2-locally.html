<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://replicate.com/blog/run-llama-locally">Original</a>
    <h1>Guide to running Llama 2 locally</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    

<article>
  <hgroup>
    

    <p>
      Posted
      <time datetime="2023-07-22">
        July 22, 2023
      </time>
      by
      

      
      <a href="https://replicate.com/zeke">@zeke</a>

      
    </p>
  </hgroup>

  <div>
    <p>Weâ€™ve been talking a lot about <a href="https://replicate.com/blog/llama-2-roundup">how to run</a> and <a href="https://replicate.com/blog/fine-tune-llama-2">fine-tune Llama 2</a> on Replicate. But you can also run Llama locally on your M1/M2 Mac, on Windows, on Linux, or even your phone. The cool thing about running Llama 2 locally is that you donâ€™t even need an internet connection.</p>
<p>Hereâ€™s an example using a locally-running Llama 2 to whip up a website about why llamas are cool:</p>
<video controls="" src="/static/blog/run-llama-locally/why-llamas-are-cool.mp4"></video>

<p>Itâ€™s only been a couple days since Llama 2 was released, but there are already a handful of techniques for running it locally. In this blog post weâ€™ll cover three open-source tools you can use to run Llama 2 on your own devices:</p>
<ul>
<li>Llama.cpp (Mac/Windows/Linux)</li>
<li>Ollama (Mac)</li>
<li>MLC LLM (iOS/Android)</li>
</ul>
<h2 id="llamacpp-macwindowslinux">Llama.cpp (Mac/Windows/Linux)</h2>
<p><a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a> is a port of Llama in C/C++, which makes it possible to run Llama 2 locally using 4-bit integer quantization on Macs. However, Llama.cpp also has support for Linux/Windows.</p>
<p>Hereâ€™s a one-liner you can use to install it on your M1/M2 Mac:</p>
<pre><code>curl -L &#34;https://replicate.fyi/install-llama-cpp&#34; | bash
</code></pre>
<p>Hereâ€™s what that one-liner does:</p>
<pre><code>#!/bin/bash

# Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Build it. `LLAMA_METAL=1` allows the computation to be executed on the GPU
LLAMA_METAL=1 make

# Download model
export MODEL=llama-2-13b-chat.ggmlv3.q4_0.bin
if [ ! -f models/${MODEL} ]; then
    curl -L &#34;https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/${MODEL}&#34; -o models/${MODEL}
fi

# Set prompt
PROMPT=&#34;Hello! How are you?&#34;

# Run in interactive mode
./main -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin \
  --color \
  --ctx_size 2048 \
  -n -1 \
  -ins -b 256 \
  --top_k 10000 \
  --temp 0.2 \
  --repeat_penalty 1.1 \
  -t 8
</code></pre>
<p>Here&#39;s a one-liner for your intel Mac, or Linux machine. It&#39;s the same as above, but we&#39;re not including the <code>LLAMA_METAL=1</code> flag:</p>
<pre><code>curl -L &#34;https://replicate.fyi/install-llama-cpp-cpu&#34; | bash
</code></pre>
<p>Here&#39;s a one-liner to run on Windows on <a href="https://learn.microsoft.com/en-us/windows/wsl/about">WSL</a>:</p>
<pre><code>curl -L &#34;https://replicate.fyi/windows-install-llama-cpp&#34; | bash
</code></pre>
<h2 id="ollama-mac">Ollama (Mac)</h2>
<p><a href="https://ollama.ai/">Ollama</a> is an open-source macOS app (for Apple Silicon) that lets you run, create, and share large language models with a command-line interface. Ollama already has support for Llama 2.</p>
<p>To use the Ollama CLI, download the macOS app at <a href="https://ollama.ai/download">ollama.ai/download</a>. Once you&#39;ve got it installed, you can download Lllama 2 without having to register for an account or join any waiting lists. Run this in your terminal:</p>
<pre><code># download the 7B model (3.8 GB)
ollama pull llama2

# or the 13B model (7.3 GB)
ollama pull llama2:13b
</code></pre>
<p>Then you can run the model and chat with it:</p>
<pre><code>ollama run llama2
&gt;&gt;&gt; hi
Hello! How can I help you today?
</code></pre>
<p>Note: Ollama recommends that have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.</p>
<h2 id="mlc-llm-llama-on-your-phone">MLC LLM (Llama on your phone)</h2>
<p><a href="https://github.com/mlc-ai/mlc-llm">MLC LLM</a> is an open-source project that makes it possible to run language models locally on a variety of devices and platforms, including iOS and Android.</p>
<p>For iPhone users, thereâ€™s an <a href="https://apps.apple.com/us/app/mlc-chat/id6448482937">MLC chat app</a> on the App Store. MLC now has support for the 7B, 13B, and 70B versions of Llama 2, but itâ€™s still in beta and not yet on the Apple Store version, so youâ€™ll need to install TestFlight to try it out. Check out out the <a href="https://mlc.ai/mlc-llm/docs/get_started/try_out.html">instructions for installing the beta version here</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>We&#39;d love to see what you build. <a href="https://discord.gg/replicate">Hop in our Discord and share it with our community.</a></li>
<li>Replicate lets you run machine learning models in the cloud. Run Llama 2 on Replicate:<ul>
<li><a href="https://replicate.com/a16z-infra/llama7b-v2-chat">a16z-infra/llama7b-v2-chat</a></li>
<li><a href="https://replicate.com/a16z-infra/llama13b-v2-chat">a16z-infra/llama13b-v2-chat</a></li>
<li><a href="https://replicate.com/replicate/llama70b-v2-chat">replicate/llama70b-v2-chat</a></li>
</ul>
</li>
<li><a href="https://replicate.com/blog/fine-tune-llama-2">Fine-tune Llama 2 on Replicate</a></li>
</ul>
<p>Happy hacking! ðŸ¦™</p>
  </div>

  
</article>


    
  </div></div>
  </body>
</html>
