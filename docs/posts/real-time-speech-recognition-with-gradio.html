<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gradio.app/real_time_speech_recognition/">Original</a>
    <h1>Real Time Speech Recognition with Gradio</h1>
    
    <div id="readability-page-1" class="page"><div><h2 id="introduction">Introduction</h2><p>Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).</p><p>Using <code>gradio</code>, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.</p><p>This tutorial will show how to take a pretrained speech to text model and deploy it with a Gradio interface. We will start with a <strong><em>full-context</em></strong> model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it <strong><em>streaming</em></strong>, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or <a rel="noopener" target="_blank" href="https://huggingface.co/spaces/abidlabs/streaming-asr-paused">in a new tab</a>!):</p><p>Real-time ASR is inherently <em>stateful</em>, meaning that the model&#39;s predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos.</p><h3 id="prerequisites">Prerequisites</h3><p>Make sure you have the <code>gradio</code> Python package already <a rel="noopener" target="_blank" href="https://gradio.app/getting_started">installed</a>. You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:</p><ul><li>Transformers (for this, <code>pip install transformers</code> and <code>pip install torch</code>)</li><li>DeepSpeech (<code>pip install deepspeech==0.8.2</code>)</li></ul><p>Make sure you have at least one of these installed so that you can follow along the tutorial. You will also need <code>ffmpeg</code> <a rel="noopener" target="_blank" href="https://www.ffmpeg.org/download.html">installed on your system</a>, if you do not already have it, to process files from the microphone.</p><h2 id="step-1-setting-up-the-transformers-asr-model">Step 1 — Setting up the Transformers ASR Model</h2><p>First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, <code>Wav2Vec2</code>.</p><p>Here is the code to load <code>Wav2Vec2</code> from Hugging Face <code>transformers</code>.</p><div><pre><code>from transformers import pipeline

p = pipeline(&#34;automatic-speech-recognition&#34;)</code></pre></div><p>That&#39;s it! By default, the automatic speech recognition model pipeline loads Facebook&#39;s <code>facebook/wav2vec2-base-960h</code> model.</p><h2 id="step-2-creating-a-full-context-asr-demo-with-transformers">Step 2 — Creating a Full-Context ASR Demo with Transformers</h2><p>We will start by creating a <em>full-context</em> ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the <code>pipeline</code> object above.</p><p>We will use <code>gradio</code>&#39;s built in <code>Audio</code> component, configured to take input from the user&#39;s microphone and return a filepath for the recorded audio. The output component will be a plain <code>Textbox</code>.</p><div><pre><code>import gradio as gr

def transcribe(audio):
    text = p(audio)[&#34;text&#34;]
    return text

gr.Interface(
    fn=transcribe, 
    inputs=gr.inputs.Audio(source=&#34;microphone&#34;, type=&#34;filepath&#34;), 
    outputs=&#34;text&#34;).launch()</code></pre></div><p>So what&#39;s happening here? The <code>transcribe</code> function takes a single parameter, <code>audio</code>, which is a filepath to the audio file that the user has recorded. The <code>pipeline</code> object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox.</p><p>Let&#39;s see it in action! (Record a short audio clip and then click submit, or <a rel="noopener" target="_blank" href="https://huggingface.co/spaces/abidlabs/full-context-asr">open in a new tab</a>):</p><h2 id="step-3-creating-a-streaming-asr-demo-with-transformers">Step 3 — Creating a Streaming ASR Demo with Transformers</h2><p>Ok great! We&#39;ve built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a <em>streaming</em> interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.</p><p>The good news is that it&#39;s not too difficult to adapt the demo we just made to make it streaming, using the same <code>Wav2Vec2</code> model.</p><p>The biggest change is that we must now introduce a <code>state</code> parameter, which holds the audio that has been <em>transcribed so far</em>. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed.</p><p>When adding state to a Gradio demo, you need to do a total of 3 things:</p><ul><li>Add a <code>state</code> parameter to the function</li><li>Return the updated <code>state</code> at the end of the function</li><li>Add the <code>&#34;state&#34;</code> components to the <code>inputs</code> and <code>outputs</code> in <code>Interface</code></li></ul><p>Here&#39;s what the code looks like:</p><div><pre><code>import gradio as gr

def transcribe(audio, state=&#34;&#34;):
    text = p(audio)[&#34;text&#34;]
    state += text + &#34; &#34;
    return state, state

gr.Interface(
    fn=transcribe, 
    inputs=[
        gr.inputs.Audio(source=&#34;microphone&#34;, type=&#34;filepath&#34;), 
        &#34;state&#34;
    ],
    outputs=[
        &#34;textbox&#34;,
        &#34;state&#34;
    ],
    live=True).launch()</code></pre></div><p>Notice that we&#39;ve also made one other change, which is that we&#39;ve set <code>live=True</code>. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.</p><p>Let&#39;s see how it does (try below or <a rel="noopener" target="_blank" href="https://huggingface.co/spaces/abidlabs/streaming-asr">in a new tab</a>)!</p><p>One thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A &#34;hacky&#34; fix to this is to simply increase the runtime of the <code>transcribe()</code> function so that longer audio chunks are processed. We can do this by adding a <code>time.sleep()</code> inside the function, as shown below (we&#39;ll see a proper fix next)</p><div><pre><code>from transformers import pipeline
import gradio as gr
import time

p = pipeline(&#34;automatic-speech-recognition&#34;)

def transcribe(audio, state=&#34;&#34;):
    time.sleep(2)
    text = p(audio)[&#34;text&#34;]
    state += text + &#34; &#34;
    return state, state

gr.Interface(
    fn=transcribe, 
    inputs=[
        gr.inputs.Audio(source=&#34;microphone&#34;, type=&#34;filepath&#34;), 
        &#34;state&#34;
    ],
    outputs=[
        &#34;textbox&#34;,
        &#34;state&#34;
    ],
    live=True).launch()</code></pre></div><p>Try the demo below to see the difference (or <a rel="noopener" target="_blank" href="https://huggingface.co/spaces/abidlabs/streaming-asr-paused">open in a new tab</a>)!</p><h2 id="step-4-creating-a-streaming-asr-demo-with-deepspeech">Step 4 — Creating a Streaming ASR Demo with DeepSpeech</h2><p>You&#39;re not restricted to ASR models from the <code>transformers</code> library -- you can use your own models or models from other libraries. The <code>DeepSpeech</code> library contains models that are specifically designed to handle streaming audio data. These models perform really well with streaming data as they are able to account for previous chunks of audio data when making predictions.</p><p>Going through the DeepSpeech library is beyond the scope of this Guide (check out their <a rel="noopener" target="_blank" href="https://deepspeech.readthedocs.io/en/r0.9/">excellent documentation here</a>), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model.</p><p>Here&#39;s a complete example (on Linux):</p><p>First install the DeepSpeech library and download the pretrained models from the terminal:</p><div><pre><code>wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm
wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer
apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg
pip install deepspeech==0.8.2
</code></pre></div><p>Then, create a similar <code>transcribe()</code> function as before:</p><div><pre><code>from deepspeech import Model
import numpy as np

model_file_path = &#34;deepspeech-0.8.2-models.pbmm&#34;
lm_file_path = &#34;deepspeech-0.8.2-models.scorer&#34;
beam_width = 100
lm_alpha = 0.93
lm_beta = 1.18

model = Model(model_file_path)
model.enableExternalScorer(lm_file_path)
model.setScorerAlphaBeta(lm_alpha, lm_beta)
model.setBeamWidth(beam_width)


def reformat_freq(sr, y):
    if sr not in (
        48000,
        16000,
    ):  # Deepspeech only supports 16k, (we convert 48k -&gt; 16k)
        raise ValueError(&#34;Unsupported rate&#34;, sr)
    if sr == 48000:
        y = (
            ((y / max(np.max(y), 1)) * 32767)
            .reshape((-1, 3))
            .mean(axis=1)
            .astype(&#34;int16&#34;)
        )
        sr = 16000
    return sr, y


def transcribe(speech, stream):
    _, y = reformat_freq(*speech)
    if stream is None:
        stream = model.createStream()
    stream.feedAudioContent(y)
    text = stream.intermediateDecode()
    return text, stream
</code></pre></div><p>Then, create a Gradio Interface as before (the only difference being that the return type should be <code>numpy</code> instead of a <code>filepath</code> to be compatible with the DeepSpeech models)</p><div><pre><code>import gradio as gr

gr.Interface(
    fn=transcribe, 
    inputs=[
        gr.inputs.Audio(source=&#34;microphone&#34;, type=&#34;numpy&#34;), 
        &#34;state&#34;
    ], 
    outputs= [
        &#34;text&#34;, 
        &#34;state&#34;
    ], 
    live=True).launch()</code></pre></div><p>Running all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.</p><hr/><p>And you&#39;re done! That&#39;s all the code you need to build a web-based GUI for your ASR model.</p><p>Fun tip: you can share your ASR model instantly with others simply by setting <code>share=True</code> in <code>launch()</code>.</p></div></div>
  </body>
</html>
