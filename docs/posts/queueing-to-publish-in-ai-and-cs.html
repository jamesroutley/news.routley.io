<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://damaru2.github.io/general/queueing_to_publish_in_AI_or_CS/">Original</a>
    <h1>Queueing to publish in AI and CS</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>Written by <a href="https://damaru2.github.io" title="About David Martínez-Rubio">David Martínez-Rubio</a> in collaboration with <a href="https://www.pokutta.com/">Sebastian Pokutta</a>.</p>

<hr/>

<p>Does the common CS conference publication model with a fixed low acceptance rate over submissions make sense? What are some consequences of it? Here, I analyze some interesting properties that model the reviewing and acceptance system of machine learning conferences, but applies to CS more generally.</p>

<!--- MathJax definitions for this post only (with links) --->


<p><strong>Disclaimer:</strong> This is a toy model and more knowledgeable people have devoted greater effort to other models and ideas [<a href="https://arxiv.org/pdf/2303.09020">1</a>, <a href="https://link.springer.com/article/10.1007/s11192-025-05271-9">2</a>, <a href="https://dl.acm.org/doi/pdf/10.1145/3528086">3</a>, <a href="https://arxiv.org/abs/2505.04966">4</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28860">5</a>], among many others. Below there is simple and to-the-point food for thought. I don’t know yet if I consider these conclusions based on simplified models to be valid for the real case.</p>

<h2 id="the-ideal-case-no-giving-up">The ideal case: no giving up</h2>

<p>First, let’s assume that authors keep resubmitting their unaccepted papers indefinitely, without restrictions on how papers are accepted.</p>

<figure>
  <img src="https://damaru2.github.io/images/conferences_evolution.png" alt=""/>
  <figcaption></figcaption>
</figure>

<p>Assume a sequence of non-overlapping conference calls (e.g. 3 per year), and each time <span id="def_number_new_papers">$\N$</span> new papers are added to the pool of papers to be published, and we let <span id="def_rate_of_acceptance">$\p \in (0, 1]$</span> be a fixed rate of acceptance.<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> The pool of unaccepted papers evolves like the following dynamical system for $x_1 \gets \N$:</p><p>

\[x_{t+1} \gets x_t (1-\p) + \N.\]

</p><p>This converges fast to a fixed point <span id="def_number_new_papers">$\xast$</span>, which is the solution to $\xast = \xast(1-\p) + \N$, yielding</p><p>

\[\xast = \frac{\N}{\p},\]

\[\textit{#accepted_papers}= \xast \cdot \p = \frac{\N}{\p} \cdot \p = \N.\]

</p><p><strong>Amazing!!</strong> <em>#accepted_papers</em> does not change with $\p$. If we reduce $\p$, the only effect is that the pool size grows until it’s so big that a fraction $\p$ of it ends up being the same number of papers $\N$, and <strong>we review more for nothing $(\propto \N/\p)$</strong>. We’d accept the same amount of papers in each conference! More easily: at the fixed point the number of papers that come in must be the number of papers that come out, that is $\N$. In fact <a href="https://en.wikipedia.org/wiki/Little%27s_law">Little’s Law</a> from queueing theory implies this fact and further generalizations. So:</p>

<div>
    <center>
    <em>In the ideal case, by reducing the rate of acceptance, we end up accepting the same amount of papers, and we just review more.</em></center>
</div>

<p>Now, what happens if authors do give up?</p>

<hr/>

<h2 id="if-authors-give-up">If authors give up</h2>

<p>We model papers of three different qualities. The results are similar to before for a relevant interval for $\p$. The simulations yield that a decrease in the rate of acceptance from 35% to 20% increases the number of abandoned bad papers comparably to changing their wait in the pool for two or three rounds. At the same time, it increases the pool size and reviewing load by about 46%, for $T=6$. With the slider, you’ll be able to see other cases. How faithful the model is would require further investigation. But one possible conclusion from it could be:</p>

<center><em>
We are significantly increasing reviewers&#39; and authors&#39; time trying to prevent a few bad papers to get in, randomly leaving out many acceptable papers in return. Reviewing load is still close to $\propto \N / \p$ so it can increase sharply with low $\p$.
    </em></center>


<p>Here is the model, inspired by the previous <a href="https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/">2014 and 2021 Neurips experiments</a>. Assume there are $\N=5000$ new papers for a sequence of non-overlapping conferences, and we have: great papers, average papers, and bad papers in proportion of 15% / 70% / 15%. We model this by assigning them a probability of acceptance proportional to $15, 5$ and $1$, respectively.</p>

<p>At each iteration, we approximate the number of papers in each category that should be accepted, given the quality weights of each category, until we accept a fraction $\p$ of them. Then, we remove them from the pool, and consider them accepted. If a paper stays in the pool for <span id="def_max_retries">$\T$</span>  iterations, it is removed.</p>

<p>The following plot shows the percentage of papers in each category that end up abandoning the pool out of all the papers produced in that category, depending on the rate $\p$, at the system’s equilibrium. Note that the acceptance plot would be the reverse one: papers that are not abandoned are accepted. The second plot is the same but zooming on $\p \in [0.2, 0.35]$. If we compare $p=0.2$ with respect to $p=0.35$ for $\T=6$, the number of bad papers abandoned increases from ~60% of the total bad papers to ~77% but for average papers it increases 478% from ~4% to ~24% (and recall there are around 5 times as much average papers as bad papers so the absolute effect is bigger). At the same time reviewing load increases 46%, because the pool still increases to something close to $\N / \p$. The numbers change a bit with the value of the uncontrolled variable $\T$. Test it yourself with the slider.</p>



<p>Below, you can see how the size of the pool changes significantly with $\p$. Recall in the ideal case it’s $\N/\p$ and here it is close to it. Do you observe 20K+ submissions on a conference? You are actually seeing ~$\N / \p$, not $\N$, there is a big difference!</p>



<p>In other words, there is of course, some trade-off. Lower rate of acceptance in this less-idealized case does imply more papers abandon the system and prevents some of the bad papers to get in, but at the expense of significant extra reviewing costs and affecting more to average papers that are left out just by bad luck. See for instance <a href="https://x.com/MarkSchmidtUBC/status/1959302293054275663">this case of NeurIPS 2025</a> where some area chairs were asked to reject papers with good reviews, just to meet a low $\p$. I’ve heard arguments saying that in Machine Learning conferences a higher $\p$ would make the conferences too big for any venue, but this does not take into account that higher rate of acceptance has a reduction effect on the pool of unaccepted papers and does not change absolute acceptances dramatically. And in any case, <strong>there are other solutions worth considering</strong> such as federated conferences and other models (with an attempt from the <a href="https://blog.neurips.cc/2025/07/16/neurips-announces-second-physical-location-in-mexico-city/">second NeurIPS 2025 location</a> and the <a href="https://eurips.cc/">European NeurIPS 2025 Hub</a>, see also <a href="https://cspaper.org/topic/128/the-ai-conference-bubble-is-about-to-burst-publish-or-perish-at-the-edge-of-collapse">The AI Conference Bubble is About to Burst</a>). Of course, one should be aware that a lower bar could encourage people to submit weaker papers. Also, as you can see in the appendix, the right metric to have in mind is that of <a href="#real_acceptance_rate">effective acceptance rates for these systems</a>. And note that these effective rates can be played by authors by increasing their $\T,$ which only increases the reviewing load.</p>

<p>Any process designed for people will be inherently flawed. But what feels clear to me is that we currently need some changes and this should most likely require:</p>

<ul>
  <li>Getting people to ask what the community needs and wants.</li>
  <li>Making quick reviewing experiments outside of conferences to iterate faster. Trusty quick-iteration methods can be borrowed from game development and usability research.</li>
  <li>Being mindful of resources (papers do not need to have 4-5 reviewers).</li>
  <li>Don’t be afraid of (tested) change.</li>
</ul>

<p>What are your thoughts on this? What do you think about this post’s model? Would you change anything about where we stand in the trade-off?</p>

<p>[BibTeX]</p>
<div id="bib_johnson2025optimal">
<pre id="pre_johnson2025optimal">@misc{martinezrubio-2025-queuing-to-publish
  title = {Queueing to publish in AI (and CS)},
  author    = {Mart{\&#39;\i}nez-Rubio, David and Pokutta, Sebastian},
  year = {2025},
  month = {09},
  howpublished = {Blog post},
  url = {https://damaru2.github.io/general/queueing_to_publish_in_AI_or_CS/},
}
</pre>
</div>

<hr/>

<h2 id="appendix">Bonus - Interactive Appendix</h2>

<ul>
  <li>Check <a href="https://www.pokutta.com/blog/littles-law-for-conference-review-backlogs/" target="_blank">the post in Sebastian’s blog</a> for the relevant math on Queueing Theory and a cool funnel simulation.</li>
</ul>

<!--- + If we had one single quality category, one can compute that if people give up after $\T$ calls, then the size of the pool converges to $ \N \frac{1-(1-\p)^\T}{\p}$, the rate of accepted papers is $1 - (1-\p)^\T$ and thus the rate of abandonment is $(1-\p)^\T$. Acceptances rise only moderately with $\p$ but the pool size and review volume drop sharply, as also observed in the simulation. It is a simple consequence of the model and can also be obtained using queueing theory. We could even apply Little's Law for each individual class but the computations are more messy.
--->

<ul>
  <li><strong>Simulation.</strong> The system essentially stabilizes after $\T+1$ iterations. One can observe other natural phenomena, such as a great reduction in the absolute number of average papers that were rejected merely due to bad luck wrt the others, when $\p$ increases to $0.35$. One can observe how much the black line of waiting papers decreases when acceptance rate is increased.</li>
</ul>




<p><b>Axes:</b> Left y: Waiting papers. Right y: Accepted (up) &amp; Abandoned (down).
</p>



<ul>
  <li>
    <p><strong>Growth over time.</strong> You can also play with what happens if you assume a $2\%$ growth by checking this box
  , but the conclusions are essentially the same since the new equilibria are tracked fast. Note that the observed growth in submitted papers is approximately that $2\%$ as well but this number is approximately $1/\p$ times the new papers that are produced. <strong>Huge submission counts are inflated by acceptance rates.</strong></p>
  </li>
  <li>
    <p><span id="real_acceptance_rate"></span><strong>Real acceptance rate: much higher than one conference’s rate, since authors resubmit.</strong> Clearly not 20% of newly produced papers are accepted if $\p=0.2$. Every conference, we’d actually accept 20% of a pool that has been artificially inflated by rejecting many previous papers. Below you can see the final acceptance rate if authors only give up after $\T$ iterations: the area under the curve shows contributions by quality. Compare how it changes with $\T$ using the slider.</p>
  </li>
</ul>



<ul>
  <li><strong>Quality of accepted / abandoned papers:</strong> On the first plot, we show the categories for the overall accepted paper count given $\p$, in percentage. On the second plot, we show the same for abandoned papers. Lower acceptance rate (that increases the pool size and  reviewer load) generally increases the quality of <strong>both</strong> accepted and abandoned papers, in proportion. <small>Also, fact: For large $\T$, when $\p \to 1$ in the first case or $\p\to 0$ in the second, it’s essentially the 15%–70%–15% split of the original distribution.</small></li>
</ul>



<p><strong>Acknowledgements:</strong> We want to thank José Céspedes Martínez for proofreading and writing suggestions.</p>

<!-- Floating controls -->












      <hr/>
      
    </div></div>
  </body>
</html>
