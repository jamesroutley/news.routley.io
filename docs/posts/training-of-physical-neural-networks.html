<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2406.03372">Original</a>
    <h1>Training of Physical Neural Networks</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Momeni,+A">Ali Momeni</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Rahmani,+B">Babak Rahmani</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Scellier,+B">Benjamin Scellier</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Wright,+L+G">Logan G. Wright</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=McMahon,+P+L">Peter L. McMahon</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Wanjura,+C+C">Clara C. Wanjura</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Li,+Y">Yuhang Li</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Skalli,+A">Anas Skalli</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Berloff,+N+G">Natalia G. Berloff</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Onodera,+T">Tatsuhiro Onodera</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Oguz,+I">Ilker Oguz</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Morichetti,+F">Francesco Morichetti</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=del+Hougne,+P">Philipp del Hougne</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Gallo,+M+L">Manuel Le Gallo</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Sebastian,+A">Abu Sebastian</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Mirhoseini,+A">Azalia Mirhoseini</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Zhang,+C">Cheng Zhang</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Markovi%C4%87,+D">Danijela Marković</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Brunner,+D">Daniel Brunner</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Moser,+C">Christophe Moser</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Gigan,+S">Sylvain Gigan</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Marquardt,+F">Florian Marquardt</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Ozcan,+A">Aydogan Ozcan</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Grollier,+J">Julie Grollier</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Liu,+A+J">Andrea J. Liu</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Psaltis,+D">Demetri Psaltis</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Al%C3%B9,+A">Andrea Alù</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Fleury,+R">Romain Fleury</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2406.03372">View PDF</a>
    <a href="https://arxiv.org/html/2406.03372v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely &#34;yes, with enough research&#34;: PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Ali Momeni [<a href="https://arxiv.org/show-email/72d69ebe/2406.03372">view email</a>]      </p></div></div>
  </body>
</html>
