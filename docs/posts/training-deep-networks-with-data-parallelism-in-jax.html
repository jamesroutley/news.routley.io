<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mishalaskin.com/posts/data_parallel">Original</a>
    <h1>Training Deep Networks with Data Parallelism in Jax</h1>
    
    <div id="readability-page-1" class="page"><div id="__next"><article dir="ltr"><img alt="Photo" srcset="/_next/image?url=%2Fimages%2Fdata_parallel_diagram.png&amp;w=1200&amp;q=75 1x, /_next/image?url=%2Fimages%2Fdata_parallel_diagram.png&amp;w=3840&amp;q=75 2x" src="https://www.mishalaskin.com/_next/image?url=%2Fimages%2Fdata_parallel_diagram.png&amp;w=3840&amp;q=75" width="1125" height="750" decoding="async" data-nimg="1"/>
<p>One of the main challenges in training large neural networks, whether they are LLMs or VLMs, is that they are too large to fit on a single GPU. To address this issue, their training can be parallelized across multiple GPUs. This means either parallelizing the data or model to distribute computation across several devices. In this post, we&#39;ll cover batch splitting, also known as data parallelism, and show how to use JAX&#39;s pmap function to parallelize computations across multiple devices.</p>
<h2>Parallelizing a 300M GPT model<span id="parallelizing-a-300m-gpt-model"></span><a href="#parallelizing-a-300m-gpt-model" aria-label="Permalink for this section"></a></h2>
<p>Let&#39;s start with an illustrative example. Suppose we&#39;re training GPT model with has 300M parameters on a machine with 8 Tesla V100 GPUs. How can we parallelize the model across GPUs to get a maximally efficient runtime? Since the model has 300M parameters, storing its parameters takes up 1.2GB of RAM (<a target="_blank" rel="noreferrer" href="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters">see here<span> (opens in a new tab)</span></a>). However, the memory footprint for training a transformer model will be dominated by the activations. Let <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo separator="true">,</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">T, D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span>,</span><span></span><span>D</span></span></span></span></span> denote the sequence length and hidden dimensions of the model. Assuming <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>≫</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">T \gg D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>≫</span><span></span></span><span><span></span><span>D</span></span></span></span></span>, the memory footprint of a transformer is <a target="_blank" rel="noreferrer" href="https://stats.stackexchange.com/questions/563919/formula-to-compute-approximate-memory-requirements-of-transformer-models">roughly<span> (opens in a new tab)</span></a>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>≈</mo><msub><mi>M</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mfrac><mrow><mi>B</mi><msup><mi>T</mi><mn>2</mn></msup></mrow><mrow><mn>4</mn><mi>N</mi><msup><mi>D</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">M \approx M_{model} \frac{BT^2}{4ND^2}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>M</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>M</span><span><span><span><span><span><span></span><span><span><span>m</span><span>o</span><span>d</span><span>e</span><span>l</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>4</span><span>N</span><span><span>D</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>B</span><span><span>T</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p>where <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>B</span></span></span></span></span> is that batch size and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span> is the number of attention heads. Now let&#39;s assume <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">T=1024</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>1024</span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">N=8</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>8</span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">D=128</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span><span></span><span>=</span><span></span></span><span><span></span><span>128</span></span></span></span></span>, then the formula reduces to:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>≈</mo><mn>1.2</mn><mo>⋅</mo><mn>8</mn><mi>B</mi><mo>=</mo><mn>9.6</mn><mi>B</mi></mrow><annotation encoding="application/x-tex">M \approx 1.2 \cdot 8 B = 9.6 B</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>M</span><span></span><span>≈</span><span></span></span><span><span></span><span>1.2</span><span></span><span>⋅</span><span></span></span><span><span></span><span>8</span><span>B</span><span></span><span>=</span><span></span></span><span><span></span><span>9.6</span><span>B</span></span></span></span></span></p>
<p>In other words, storing the activations will require around <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>10</span></span></span></span></span>GB for each batch element when you train your model. Current Tesla V100 GPUs have 32GB of RAM so this means you will be able to fit this model with a batch size of 3 on each device. The optimal training set up is then to have a batch size of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>24</mn></mrow><annotation encoding="application/x-tex">B=24</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>B</span><span></span><span>=</span><span></span></span><span><span></span><span>24</span></span></span></span></span> and split the the minibatch across 8 devices. Note how you can store the entire model in RAM but the limiting factor is the batch size. This is why this parallelization technique is called data parallelism - you copy the model across each device but parallelize the data.</p>
<h2>Data Parallelism in Jax<span id="data-parallelism-in-jax"></span><a href="#data-parallelism-in-jax" aria-label="Permalink for this section"></a></h2>
<p>Let&#39;s walk through a simple example of data parallelism in JAX by looking at how to parallelize the forward pass of a linear layer. We&#39;ll first show how to pass a single data point through the layer, then multiple points on one device, and finally multiple points across multiple devices.</p>
<h4>A linear layer<span id="a-linear-layer"></span><a href="#a-linear-layer" aria-label="Permalink for this section"></a></h4>
<p>First, let&#39;s define a simple linear layer.</p>

<h4>One data point<span id="one-data-point"></span><a href="#one-data-point" aria-label="Permalink for this section"></a></h4>
<p>Now let&#39;s pass one data point through the linear layer.</p>

<p>The output is just a scalar.</p>
<h4>Multiple data points on one device<span id="multiple-data-points-on-one-device"></span><a href="#multiple-data-points-on-one-device" aria-label="Permalink for this section"></a></h4>
<p>To efficiently pass multiple data points on one device we can use the <code dir="ltr">vmap</code> function to apply a function in parallel without worrying about tensor shapes.</p>

<p>The output is a vector of length 16 that is stored on a single device.</p>
<h4>Multiple data points on multiple devices<span id="multiple-data-points-on-multiple-devices"></span><a href="#multiple-data-points-on-multiple-devices" aria-label="Permalink for this section"></a></h4>
<p>Parallelizing this operation across multiple devices works. By using <code dir="ltr">pmap</code> we get the same vectorized functionality as <code dir="ltr">vmap</code> but on multiple devices.</p>

<p>The output is a matrix of shape <code dir="ltr">(8, 2)</code> that is stored on 8 devices. When flattened this matrix is equivalent to the vector of length 16 that we got from <code dir="ltr">vmap</code>. The only awkward part about this example is that we replicated the weights across all devices. We can avoid this step if we use <code dir="ltr">in_axes</code> in the <code dir="ltr">pmap</code> function:</p>

<h4>Data parallelized linear regression<span id="data-parallelized-linear-regression"></span><a href="#data-parallelized-linear-regression" aria-label="Permalink for this section"></a></h4>
<p>We can now write out an example of how training with data parallelism works. For each minibatch, each device computes the gradients with respect to the parameters and sends them to a central server. The server averages the gradients and sends the result back to each device. Each device then updates its copy of the parameters using the average gradients. This ensures that each device is training the model using the most up-to-date parameters.</p>
<p>Here&#39;s a simple example of data parallel linear regression, which I&#39;ve modified from this <a target="_blank" rel="noreferrer" href="https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/06-parallelism.ipynb">colab<span> (opens in a new tab)</span></a> where you can find a full working example.</p>

<p>Here, the <code dir="ltr">functools.partial</code> decorator wraps the <code dir="ltr">update</code> function with a <code dir="ltr">pmap</code> with <code dir="ltr">axis_name=&#39;num_devices&#39;</code> as an input argument to <code dir="ltr">pmap</code>. This means that the <code dir="ltr">update</code> function will be applied in parallel across all devices. The <code dir="ltr">pmean</code> function is used to average the gradients across all devices. The <code dir="ltr">pmean</code> function is similar to <code dir="ltr">np.mean</code> but it also takes an <code dir="ltr">axis_name</code> argument. This argument is used to specify which axis to average across. In this case, we average across the <code dir="ltr">num_devices</code> axis but this name is just a placeholder, we can change it to any string (e.g. <code dir="ltr">&#39;i&#39;</code> or <code dir="ltr">&#39;data&#39;</code> and it will work the same).</p>
<h2>What <code dir="ltr">pmap</code> does under the hood<span id="what-pmap-does-under-the-hood"></span><a href="#what-pmap-does-under-the-hood" aria-label="Permalink for this section"></a></h2>
<p>The <code dir="ltr">pmap</code> operation is pretty magical, it automatically parallelizes your data for you. Under the hood, <code dir="ltr">jax.pmap</code> uses <a target="_blank" rel="noreferrer" href="https://www.tensorflow.org/xla">XLA<span> (opens in a new tab)</span></a> (Accelerated Linear Algebra), a domain-specific compiler for linear algebra operations that JAX is built on. XLA compiles our computation into a series of low-level machine instructions that can be executed efficiently on the underlying hardware.</p>
<p>To implement data parallelism using XLA, <code dir="ltr">jax.pmap</code> generates a series of XLA computations that run on each device simultaneously. These computations are then coordinated using a communication protocol to ensure that each device has the correct data to perform its computation and that the results are combined correctly. The protocol that jax.pmap uses to combine the outputs across all devices into a single variable at the end of the computation is called All-Reduce.</p>
<p>All-Reduce is a common communication protocol in distributed computing that allows multiple devices to exchange information and compute an aggregate value. In the case of jax.pmap, the All-Reduce protocol is used to combine the outputs from each device into a single output.</p>
<p>The All-Reduce protocol works by first computing a local sum on each device, then exchanging the partial sums among all devices, and finally computing the global sum of all partial sums. This approach ensures that all devices have access to the most up-to-date information when computing the aggregate value.</p>
<p>Specifically, in the case of jax.pmap, after each device has computed its local result, it sends its result to a central server, which then applies the All-Reduce protocol to compute the aggregate value. The result is then broadcast back to each device, which can then update its local copy of the result.</p></article></div></div>
  </body>
</html>
