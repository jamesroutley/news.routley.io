<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://estuary.dev/streaming-joins-are-hard/">Original</a>
    <h1>Streaming joins are hard</h1>
    
    <div id="readability-page-1" class="page"><div><figure><div data-gatsby-image-wrapper=""><p><img alt="" role="presentation" aria-hidden="true" src="data:image/svg+xml;charset=utf-8,%3Csvg%20height=&#39;916&#39;%20width=&#39;1344&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20version=&#39;1.1&#39;%3E%3C/svg%3E"/></p><picture><source type="image/webp" srcset="/static/ae5b237045e07b905a42b15e964fd55e/f6570/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.webp 336w,/static/ae5b237045e07b905a42b15e964fd55e/d2f03/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.webp 672w,/static/ae5b237045e07b905a42b15e964fd55e/92786/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.webp 1344w" sizes="(min-width: 1344px) 1344px, 100vw"/><img data-gatsby-image-ssr="" placeholder="none" data-main-image="" sizes="(min-width: 1344px) 1344px, 100vw" decoding="async" loading="eager" src="https://estuary.dev/static/ae5b237045e07b905a42b15e964fd55e/4434a/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.png" srcset="/static/ae5b237045e07b905a42b15e964fd55e/b6323/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.png 336w,/static/ae5b237045e07b905a42b15e964fd55e/c1c78/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.png 672w,/static/ae5b237045e07b905a42b15e964fd55e/4434a/Screenshot_2024_10_21_at_18_12_53_8a5588ce5e.png 1344w" alt="Screenshot 2024-10-21 at 18.12.53.png"/></picture></div><figcaption>They are</figcaption></figure><p><span>When working with batch processing in databases, joining tables is straightforward and familiar. However, when you try to apply the same principles to real-time streaming data, the complexity increases significantly.</span></p><p><span>Streaming data isn&#39;t static like tables in databases—it&#39;s unbounded, constantly updating, and poses significant challenges in managing state.</span></p><p><span>In this article, we’ll explore why streaming joins are difficult, and provide real-world examples to illustrate how they work in practice.</span></p><h2 id="traditional-joins-a-refresher"><span>Traditional Joins: A Refresher</span></h2><p><span>To understand why streaming joins are complex, let’s briefly review how joins work in traditional database systems. Suppose we have two tables—</span><span>authors</span><span> and </span><span>books</span><span>—and we want to join them on the </span><span>author_id</span><span> field:</span></p><figure><table><colgroup><col/><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>author_id</strong></span></p></td><td><p><span><strong>name</strong></span></p></td><td><p><span><strong>genre</strong></span></p></td><td><p><span><strong>country</strong></span></p></td><td><p><span><strong>birth_year</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>J.K. Rowling</span></td><td><span>Fantasy</span></td><td><span>United Kingdom</span></td><td><span>1965</span></td></tr><tr><td><span>2</span></td><td><span>George Orwell</span></td><td><span>Dystopian</span></td><td><span>United Kingdom</span></td><td><span>1903</span></td></tr><tr><td><span>3</span></td><td><span>Agatha Christie</span></td><td><span>Mystery</span></td><td><span>United Kingdom</span></td><td><span>1890</span></td></tr></tbody></table><figcaption>Authors table</figcaption></figure><p>And the other one:</p><figure><table><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>book_id</strong></span></p></td><td><p><span><strong>author_id</strong></span></p></td><td><p><span><strong>title</strong></span></p></td><td><p><span><strong>publication_date</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>1</span></td><td><span>Harry Potter</span></td><td><span>1997-06-26</span></td></tr><tr><td><span>2</span></td><td><span>2</span></td><td><span>1984</span></td><td><span>1949-06-08</span></td></tr><tr><td><span>3</span></td><td><span>3</span></td><td><span>Murder on the Orient Express</span></td><td><span>1934-01-01</span></td></tr></tbody></table><figcaption><span>Books table</span></figcaption></figure><p><span>To join these tables on the </span><span>author_id</span><span>, we would write an SQL query:</span></p><pre><code><p>plaintext</p>SELECT *
FROM authors
JOIN books
ON authors.author_id = books.author_id;</code></pre><p><span>The output might look like this:</span></p><figure><table><colgroup><col/><col/><col/><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>author_id</strong></span></p></td><td><p><span><strong>name</strong></span></p></td><td><p><span><strong>genre</strong></span></p></td><td><p><span><strong>country</strong></span></p></td><td><p><span><strong>book_id</strong></span></p></td><td><p><span><strong>title</strong></span></p></td><td><p><span><strong>publication_date</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>J.K. Rowling</span></td><td><span>Fantasy</span></td><td><span>United Kingdom</span></td><td><span>1</span></td><td><span>Harry Potter</span></td><td><span>1997-06-26</span></td></tr><tr><td><span>2</span></td><td><span>George Orwell</span></td><td><span>Dystopian</span></td><td><span>United Kingdom</span></td><td><span>2</span></td><td><span>1984</span></td><td><span>1949-06-08</span></td></tr><tr><td><span>3</span></td><td><span>Agatha Christie</span></td><td><span>Mystery</span></td><td><span>United Kingdom</span></td><td><span>3</span></td><td><span>Murder on the Orient Express</span></td><td><span>1934-01-01</span></td></tr></tbody></table><figcaption>Join results</figcaption></figure><p><span>While this is conceptually very simple, the query planner actually has a number of different execution strategies that it&#39;ll use depending on scenario. We can ignore most of the details of that, and just talk illustratively about the </span><a href="https://en.wikipedia.org/wiki/Hash_join"><span><u>hash join algorithm</u></span></a><span>.</span></p><p><span>The basic hash join is really quite simple. Iterate through all the records of either </span><span>authors</span><span> or </span><span>books</span><span> (usually whichever has fewer rows) and build a hash map of the id to the row value. Here&#39;s a trivial example in Python:</span></p><pre><code><p>python</p>author_map = {}
<span>for</span> author_row <span>in</span> authors_table:
  author_map[author_row[<span>&#39;id&#39;</span>]] = author_row

<span>for</span> books_row <span>in</span> books_table:
  
  <span>if</span> author_map[books_row[<span>&#39;author_id&#39;</span>]]:
    output_joined_row(author_map[books_row[<span>&#39;author_id&#39;</span>]], books_row)</code></pre><p><span>This is simple and effective in batch processing because the data is finite, and all rows are accessible at the time of the query.</span></p><h2 id="why-doesnt-this-translate-into-streaming-contexts"><span>Why doesn&#39;t this translate into streaming contexts?</span></h2><p><span>The simplest explanation is that, with streaming data, all data sets are implicitly <strong>unbounded</strong>.</span></p><p><span>Let&#39;s unpack that.</span></p><p><span>Unlike a database, which is framed in terms of &#34;tables&#34;, &#34;rows&#34;, and &#34;queries&#34;, streaming systems have &#34;streams&#34;, &#34;events&#34;, and &#34;consumers&#34; (in </span><a href="https://github.com/estuary/flow"><span><u>Flow</u></span></a><span> we call them Collections, Documents, and Tasks). A &#34;table&#34; is a store containing a finite number of &#34;rows&#34;, which is why you can easily iterate them in a query. But a &#34;stream&#34; is an infinite sequence of &#34;events&#34;, where each event represents a modification of one or more entities. And instead of short-lived ad-hoc &#34;queries&#34;, you process streaming data with &#34;consumers&#34; which are long-lived processes that observe each event.</span></p><p><span>In other words, you can&#39;t &#34;just&#34; iterate all the rows in </span><span><code>authors_table</code></span><span> because you <strong>don&#39;t know what they are</strong>.</span></p><p><span>Now you may be thinking, well, wait a minute, can&#39;t we just build our </span><span><code>author_map</code></span><span> inside a consumer process? And couldn&#39;t we just do a lookup in that </span><span><code>author_map</code></span><span> whenever we observe a new </span><span><code>books</code></span><span> event?</span></p><p><span>And you&#39;d of course be right. You <i>can</i> do that, as long as you have enough memory to store every single author you&#39;ve ever seen. In many cases, this can be a perfectly reasonable approach to joining data in streaming systems, Flow included.</span></p><p><span>But recall that steams are conceptually infinite. You don&#39;t know how many authors you might observe in the lifetime of a given stream and consumer, and it may not be possible to store them all. Consider if we instead tried to join </span><span><code>user_sessions</code></span><span> to </span><span><code>audit_logs</code></span><span>. There is no upper bound on the number of user sessions or audit logs. If our app has many users over the course of a long period, it will quickly become impractical (or impossible) to store data on every single user session in a hash map.</span></p><p><span>So this is the essence of what makes &#34;streaming joins&#34; difficult, and it&#39;s really just the same thing that makes <i>everything</i> difficult in computers: <strong>state</strong>. In the context of streaming joins, the most important thing is to ensure that your state (e.g. </span><span><code>author_map</code></span><span>) doesn&#39;t exhibit <i>unbounded</i> growth.</span></p><p><span>But fear not, because even in cases where there&#39;s an infinite number of distinct entities, there are ways to prevent the state from growing too large. You can also use an external system for storing state, which allows it to grow well beyond what fits in the memory of your consumer.</span></p><p><span>We&#39;ll discuss a number of different approaches, and help you figure out which ones are most appropriate for different use cases.</span></p><p><span>So, to summarize the challenges:</span></p><ol><li><span><strong>Unbounded Data</strong>: Unlike batch tables, streams are infinite. You can&#39;t &#34;just wait&#34; for all the rows to arrive before performing a join.</span></li><li><span><strong>State Management</strong>: You need to store the intermediate state (i.e., the partial join results) while waiting for more data to arrive.</span></li><li><span><strong>Memory Constraints</strong>: Streams can generate an unbounded number of events, and you can&#39;t store everything in memory.</span></li><li><span><strong>Latency</strong>: Joins in a streaming context need to be performed in real-time, and you must consider how long to wait for matching events from the other stream.</span></li><li><span><strong>Out-of-order Data</strong>: Events in streams might arrive late or out of sequence, complicating the joining logic.</span></li></ol><p><span>Let’s take a look at how we can tackle these in a streaming environment.</span></p><h2 id="joining-two-datasets-in-streaming-context"><span>Joining Two Datasets in Streaming Context</span></h2><p><span>Let’s take a practical example using the </span><span><code>artists</code></span><span> and </span><span><code>albums</code></span><span> tables from your original scenario:</span></p><p><span><strong>Artists Table Stream:</strong></span></p><figure><table><colgroup><col/><col/><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>artist_id</strong></span></p></td><td><p><span><strong>name</strong></span></p></td><td><p><span><strong>genre</strong></span></p></td><td><p><span><strong>country</strong></span></p></td><td><p><span><strong>formed_year</strong></span></p></td><td><p><span><strong>monthly_listeners</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>The Melodics</span></td><td><span>Pop</span></td><td><span>USA</span></td><td><span>2010</span></td><td><span>500000</span></td></tr><tr><td><span>2</span></td><td><span>Rhythm Riders</span></td><td><span>Rock</span></td><td><span>UK</span></td><td><span>2005</span></td><td><span>750000</span></td></tr></tbody></table><figcaption>Artists</figcaption></figure><p><span><strong>Albums Table Stream:</strong></span></p><figure><table><colgroup><col/><col/><col/><col/><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>album_id</strong></span></p></td><td><p><span><strong>artist_id</strong></span></p></td><td><p><span><strong>title</strong></span></p></td><td><p><span><strong>release_date</strong></span></p></td><td><p><span><strong>total_tracks</strong></span></p></td><td><p><span><strong>album_type</strong></span></p></td><td><p><span><strong>label</strong></span></p></td><td><p><span><strong>total_plays</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>1</span></td><td><span>Harmonic Waves</span></td><td><span>2022-05-10</span></td><td><span>12</span></td><td><span>Studio</span></td><td><span>Melody Records</span></td><td><span>1000000</span></td></tr><tr><td><span>2</span></td><td><span>1</span></td><td><span>Acoustic Dreams</span></td><td><span>2020-03-15</span></td><td><span>10</span></td><td><span>EP</span></td><td><span>Melody Records</span></td><td><span>750000</span></td></tr></tbody></table><figcaption>Albums</figcaption></figure><p><span>Now, let’s say you want to join the artist data with album data in real-time. Here&#39;s the SQL query you would use in theory:</span></p><pre><code><p>python</p>SELECT artists.artist_id, artists.name, albums.title, albums.total_plays
FROM artists
JOIN albums
ON artists.artist_id = albums.artist_id;</code></pre><p><span>In a traditional database, you would expect an immediate result, like this:</span></p><figure><table><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td><p><span><strong>artist_id</strong></span></p></td><td><p><span><strong>name</strong></span></p></td><td><p><span><strong>title</strong></span></p></td><td><p><span><strong>total_plays</strong></span></p></td></tr><tr><td><span>1</span></td><td><span>The Melodics</span></td><td><span>Harmonic Waves</span></td><td><span>1000000</span></td></tr><tr><td><span>1</span></td><td><span>The Melodics</span></td><td><span>Acoustic Dreams</span></td><td><span>750000</span></td></tr></tbody></table><figcaption>Join results</figcaption></figure><p><span>However, in a streaming scenario, the results don&#39;t materialize all at once. Instead, you would see events trickle in over time as albums are released or new artist data arrives.</span></p><h3 id="handling-out-of-order-data"><span>Handling Out-of-Order Data</span></h3><p><span>Sometimes, an album might arrive before the corresponding artist information, or vice versa. In streaming, this is called out-of-order data, and it&#39;s essential to manage late arrivals effectively. One common technique is to use <strong>windowing</strong> strategies, where you define a time window (e.g., 1 hour) to wait for matching data before considering the data incomplete.</span></p><p><span>For example, you might wait up to 1 hour for an artist event after receiving an album event. If no matching artist event is received within that time, the album data might be considered &#34;incomplete&#34; or handled separately.</span></p><p><span>In this scenario, you would need to store intermediate data. When an album event arrives, but the corresponding artist event hasn’t, you need to temporarily store the album data until the matching artist data arrives. This introduces state management complexities, as streams can grow indefinitely.</span></p><p><span>For example:</span></p><ul><li><span><strong>Albums stream receives</strong>: </span><span>&#34;Harmonic Waves&#34;</span><span> with </span><span>artist_id=1</span></li><li><span><strong>Artists stream hasn’t received</strong> the artist information for </span><span>artist_id=1</span><span> yet</span></li></ul><p><span>At this point, we store the album event in memory and wait for the corresponding artist event. Once both events are present, we can output the join result:</span></p><pre><code><p>python</p>{<span>&#34;artist_id&#34;</span>:<span>1</span>, <span>&#34;name&#34;</span>:<span>&#34;The Melodics&#34;</span>, <span>&#34;title&#34;</span>:<span>&#34;Harmonic Waves&#34;</span>, <span>&#34;total_plays&#34;</span>:<span>1000000</span>}</code></pre><p><span>A few more techniques for efficient streaming joins:</span></p><ol><li><span><strong>State Limiting</strong>: Avoid unbounded memory usage by limiting how much intermediate state (e.g., unmatched albums) you store. Once a threshold is reached, decide how to handle excess data.</span></li><li><span><strong>Materialized Joins</strong>: Offload the results of streaming joins into materialized views or tables in a downstream system, allowing you to handle larger datasets than can fit in memory.</span></li><li><span><strong>Bloom Filters</strong>: Use probabilistic data structures like bloom filters to efficiently test whether a matching key exists in a dataset, reducing the need for full in-memory storage.</span></li></ol><h3 id="visualizing-streaming-joins"><span>Visualizing Streaming Joins</span></h3><p><span>Let’s look at an intermediate result after processing several streaming events. For instance:</span></p><p><span><strong>Before Join (Separate Streams):</strong></span></p><pre><code><p>python</p>{<span>&#34;artist_id&#34;</span>:<span>1</span>, <span>&#34;name&#34;</span>:<span>&#34;The Melodics&#34;</span>, <span>&#34;monthly_listeners&#34;</span>:<span>500000</span>}
{<span>&#34;album_id&#34;</span>:<span>1</span>, <span>&#34;artist_id&#34;</span>:<span>1</span>, <span>&#34;title&#34;</span>:<span>&#34;Harmonic Waves&#34;</span>, <span>&#34;total_plays&#34;</span>:<span>1000000</span>}</code></pre><p><span><strong>After Join (Combined):</strong></span></p><pre><code><p>python</p>{<span>&#34;artist_id&#34;</span>:<span>1</span>, <span>&#34;name&#34;</span>:<span>&#34;The Melodics&#34;</span>, <span>&#34;title&#34;</span>:<span>&#34;Harmonic Waves&#34;</span>, <span>&#34;total_plays&#34;</span>:<span>1000000</span>, <span>&#34;monthly_listeners&#34;</span>:<span>500000</span>}</code></pre><p><span>In real-time, this process happens continuously as new artist and album data flows in, with the system managing state to ensure that when an album event is received, it either immediately joins with existing artist data or waits for that data to arrive.</span></p><h2 id="estuary-flows-approaches-to-streaming-joins"><span>Estuary Flow’s Approaches To Streaming Joins</span></h2><p><span>With all this theory out of the way, let’s take a look at how Flow thinks about streaming joins.</span></p><p><span>Let’s say you want to implement the following join in Estuary Flow:</span></p><pre><code><p>python</p>select * <span>from</span> authors left outer join books on authors.<span>id</span> = books.author_id</code></pre><p><span>Because of how Flow’s continuous map-reduce architecture for streaming transformations, a join transformation can be broken down into a map and a reduce phase as well.</span></p><p><span>In the first, map phase, Flow selects the fields you wish to be present in the final collection.</span></p><p><span>In the second phase, during <i>reduction</i>, Flow will utilize the target system’s merge capabilities to condense all documents that belong together, using the target as <strong>state</strong>.</span></p><figure><div data-gatsby-image-wrapper=""><p><img alt="" role="presentation" aria-hidden="true" src="data:image/svg+xml;charset=utf-8,%3Csvg%20height=&#39;844&#39;%20width=&#39;774&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20version=&#39;1.1&#39;%3E%3C/svg%3E"/></p><picture><source type="image/webp" srcset="/static/e7b84900d72d35f3dcf9e1634242f420/c4add/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.webp 194w,/static/e7b84900d72d35f3dcf9e1634242f420/9e135/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.webp 387w,/static/e7b84900d72d35f3dcf9e1634242f420/890e5/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.webp 774w" sizes="(min-width: 774px) 774px, 100vw"/><img data-gatsby-image-ssr="" placeholder="none" data-main-image="" sizes="(min-width: 774px) 774px, 100vw" decoding="async" loading="eager" src="https://estuary.dev/static/e7b84900d72d35f3dcf9e1634242f420/6f853/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.png" srcset="/static/e7b84900d72d35f3dcf9e1634242f420/0063d/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.png 194w,/static/e7b84900d72d35f3dcf9e1634242f420/ae40f/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.png 387w,/static/e7b84900d72d35f3dcf9e1634242f420/6f853/Screenshot_2024_10_21_at_18_17_51_fa50bb25ab.png 774w" alt="Screenshot 2024-10-21 at 18.17.51.png"/></picture></div><figcaption>Streaming MapReduce</figcaption></figure><p><span>Note: must use equality conditions (e.g. </span><span><code>where a.thing = b.other_thing</code></span><span>). Range predicates like </span><span><code>where a.thing &gt; b.other_thing</code></span><span> won&#39;t work.</span></p><p><span>If you’re interested in getting started with Derivations in Flow: check out </span><a href="https://estuary.dev/derivations-join-collections-sql/"><span><u>this article</u></span></a><span> for a step-by-step guide.</span></p><p><span><strong>What about inner joins?</strong></span></p><p><span>Fair question. Keep in mind that it&#39;s typically pretty trivial to turn an outer join into an inner join, just by filtering out rows where either side of the join is null. The state is all offloaded into the destination table, which allows it to be much bigger than would fit in the memory of the consumer.</span></p><h3 id="storing-state-internally"><span>Storing state internally</span></h3><p><a href="https://docs.estuary.dev/concepts/derivations/#sqlite"><span><u>SQL derivations</u></span></a><span> can store state in their own SQLite tables, which gives a lot more flexibility in the types of joins that this approach can accommodate. But in order to do this, it must be practical to store all the required state in the derivation&#39;s SQLite database.</span></p><blockquote><p><span>A <strong>derivation</strong> is a </span><a href="https://docs.estuary.dev/concepts/#collections">collection</a><span> that is constructed by applying transformations to one or more sourced collections. Derivations operate continuously, keeping up with updates to the source collections as they happen.</span></p></blockquote><p><span>We don&#39;t define, much less enforce, any particular limit on how much data you can put into a SQLite database. But of course, there&#39;s such a thing as too much. This is the primary limitation on what you can and can&#39;t do in a streaming join. We can define an arbitrary limit on how much data a SQLite database can store, but then we&#39;d still need a way to estimate the amount of space required for various joins.</span></p><p><span>A simplifying assumption for the purposes of this estimation is to assume that we must store a complete materialization of all collection data used in the join. For example, in order to approximate the required amount of space to accomplish something like </span><span><code>select * from a join b on a.id = b.a_id join c on a.id = c.a_id</code></span><span>, we could assume that the SQLite database would need to store a row per distinct key per table.</span></p><p><span>You &#34;materialize&#34; your data into the derivation SQLite database, and then you can run whatever join queries you want to, albeit with huge caveats about handling deletions and reductions.</span></p><p><span>But of course, a derivation is not the same as a materialization. There&#39;s no way to use reduce annotations when adding data to a derivation SQLite database. So if you wanted to do any sort of rollup of one of the source collections, you&#39;d need to implement that manually using SQLite lambdas. That includes handling deletion events by both deleting from the derivation SQLite table and emitting some sort of deletion or update event for the joined row. I don&#39;t think there are any generic instructions on how to do that. It depends on the desired semantics of the join query and the data model of the source and derived collections.</span></p><p><span>Windowing is one of the main strategies that&#39;s used to bound the amount of state in a streaming join. This is a whole topic of its own, and there are lots of important details and different ways to do it.</span></p><p><span>So to summarize, keeping state in a SQL derivation allows for more types of joins, but we&#39;d probably need to break this approach down further into some guides that make some simplifying assumptions about the data and the type of join queries you can do.</span></p><h2 id="wrapping-up"><span>Wrapping up</span></h2><p><span>In summary, streaming joins are difficult because they deal with unbounded data, require efficient state management, and must operate under strict memory and latency constraints. Fortunately, with the right strategies—like windowing, state limiting, and leveraging external storage systems—many of these challenges can be mitigated.</span></p><p><span>Whether you’re dealing with high-volume streams or complex multi-way joins, the right approach and platform make all the difference.</span></p><p><span>If you are interested in Estuary Flow, sign up for a free trial </span><a href="https://dashboard.estuary.dev/register"><span><u>here</u></span></a><span>.</span></p><p><span>Join our community Slack channel: </span><a href="https://go.estuary.dev/slack"><span><u>https://go.estuary.dev/slack</u></span></a></p></div></div>
  </body>
</html>
