<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list">Original</a>
    <h1>DeepSeek&#39;s multi-head latent attention and other KV cache tricks</h1>
    
    <div id="readability-page-1" class="page"><div><section id="blog"><div><!--$--><p><img alt="DeepSeek&#39;s Multi-Head Latent Attention and Other KV Cache Tricks" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=1920&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT 1x, /_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT 2x" src="https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT"/></p><!--/$--><div><!--$--><p><time datetime="2025-01-21">January 21, 2025 (1w ago)</time><span>•</span></p><!--/$--></div><div><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/jeankaddour"><img alt="Jean Kaddour" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fauthor.jpg&amp;w=48&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT 1x, /_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT 2x" src="https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_GxSyszb87HeGTYs8Xjk4WiifwgPT"/></a></div><article><p><strong>Overview</strong>:</p>
<ol>
<li><strong>Introduction</strong>: We&#39;ll explore how Key-Value (KV) caches make language models like ChatGPT and DeepSeek faster at generating text, by making a clever trade-off between memory usage and computation time.</li>
<li><strong>MLA and other Tricks</strong>: We&#39;ll then look at 11 recent research papers, including <strong>DeepSeek&#39;s Multi-head Latent Attention (MLA)</strong>, that build upon this basic idea to make LLM inference even more time-efficient.</li>
</ol>
<hr/>
<h2>Understanding the Problem: Why Text Generation is Slow</h2>
<p>Let&#39;s start with a simple analogy. Imagine you&#39;re writing a story, and for each new word you write, you need to re-read the entire story so far to maintain consistency. The longer your story gets, the more time you spend re-reading. This is exactly what large language models face during text generation.</p>
<h3>The Basic Building Block: Self-Attention</h3>
<p>At the heart of modern language models is a mechanism called <strong>self-attention</strong>. For a sequence of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> tokens (think of tokens as roughly corresponding to words), each token needs to &#34;look at&#34; or &#34;attend to&#34; all other tokens to understand the context.</p>
<p>This looking-at-everything process has a computational cost that grows with the sequence length:</p>
<ul>
<li>For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> tokens, each token needs to look at all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> tokens</li>
<li>This means the cost is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi><mo>=</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n \times n = n^2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>n</span><span></span><span>=</span><span></span></span><span><span></span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></li>
<li>In mathematical notation, we write this as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span> complexity</li>
</ul>
<h3>The Real Problem: Generating Text One Token at a Time</h3>
<p>When a language model generates text, it does so one token at a time, and this is where things get computationally expensive:</p>
<ol>
<li><strong>First token</strong>: Look at 1 token (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>1</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>)</li>
<li><strong>Second token</strong>: Look at 2 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>2</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>)</li>
<li><strong>Third token</strong>: Look at 3 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>3</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>3</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>)</li>
<li>And so on until the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span>-th token: Look at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>)</li>
</ol>
<p>If we add up all these costs for generating a sequence of length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span>, we get:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>3</mn><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>≈</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2 + 2^2 + 3^2 + \dots + n^2) \approx O(n^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>1</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>3</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>⋯</span><span></span><span>+</span><span></span></span><span><span></span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span>
<p>This <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span> cost means that as your text gets longer, the generation time grows <strong>extremely quickly</strong>. For example, generating a sequence twice as long takes roughly <strong>eight times</strong> as long! Clearly, we need a better approach.</p>
<hr/>
<h2>The Solution: Key-Value (KV) Cache</h2>
<p>The key insight behind KV caching is that we&#39;re doing a lot of redundant work. When generating each new token, we&#39;re recomputing things for all previous tokens that we&#39;ve already processed before. Let&#39;s see how we can fix this.</p>
<h3>What is a Key-Value Cache?</h3>
<p>Think of a KV cache like a smart notepad where we write down important information about each token the first time we see it. For each token, we compute and store two things:</p>
<ol>
<li>A <strong>key</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>): Think of this as an addressing mechanism - it helps determine how relevant this token is to future tokens</li>
<li>A <strong>value</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span></span></span></span>): Think of this as the actual information that gets used when this token is found to be relevant</li>
</ol>
<p>Mathematically, we compute these as:</p>
<ul>
<li>Key: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">k = x W_K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span><span><span>W</span><span><span><span><span><span><span></span><span><span>K</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span></span></span></span> is the token and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span>K</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a learned transformation)</li>
<li>Value: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">v = x W_V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span><span><span>W</span><span><span><span><span><span><span></span><span><span>V</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span>V</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is another learned transformation)</li>
</ul>
<p>When generating a new token, we use its query (computed similarly to keys) to find relevant information in our cache by comparing it with all stored keys. The matching values are then used to help generate the token.</p>
<h3>How the KV Cache Makes Things Faster</h3>
<p>With a KV cache, the process becomes much more efficient:</p>
<ol>
<li>When we see a new token, we only need to compute its key and value <strong>once</strong></li>
<li>For all future tokens, we can just look up these pre-computed values from our cache</li>
<li>This means each new token only needs to do a small amount of new work, instead of redoing all previous computations</li>
</ol>
<p>The trade-off is clear:</p>
<ul>
<li>We use more memory to store all the keys and values. For a model with:
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span></span></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> attention heads</li>
<li>Sequence length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span></li>
<li>Key/value dimension <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>
The total memory cost is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">L \times H \times n \times d_k \times 2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span><span></span><span>×</span><span></span></span><span><span></span><span>H</span><span></span><span>×</span><span></span></span><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>×</span><span></span></span><span><span></span><span>2</span></span></span></span> values (the factor of 2 accounts for both keys and values).
This grows linearly with sequence length (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span>), but the constant factors can be substantial for large models.</li>
</ul>
</li>
<li>But in return, we reduce the computation cost from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span></li>
</ul>
<p>To understand why it&#39;s <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>, let&#39;s look at the cost at each step:</p>
<ol>
<li><strong>Step 1</strong>: Process 1 token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>1</span><span>)</span></span></span></span></li>
<li><strong>Step 2</strong>: Process 1 new token + look at 1 cached token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>2</span><span>)</span></span></span></span></li>
<li><strong>Step 3</strong>: Process 1 new token + look at 2 cached tokens → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>3</span><span>)</span></span></span></span></li>
<li>And so on...</li>
</ol>
<p>Adding these up:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>3</mn><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1 + 2 + 3 + \dots + n) = O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>1</span><span></span><span>+</span><span></span></span><span><span></span><span>2</span><span></span><span>+</span><span></span></span><span><span></span><span>3</span><span></span><span>+</span><span></span></span><span><span></span><span>⋯</span><span></span><span>+</span><span></span></span><span><span></span><span>n</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span>
<p>This is a dramatic improvement over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span>! While we still have to do the fundamental work of looking at all previous tokens (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>), we avoid the costly recomputation at each step.</p>
<hr/>
<h2>The Memory Challenge: Why We Need Better Solutions</h2>
<p>While KV cache is a powerful optimization, it comes with a significant memory cost. Let&#39;s look at a concrete example using a modern large language model like Llama3 70B with:</p>
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">L = 80</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span><span></span><span>=</span><span></span></span><span><span></span><span>80</span></span></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">H = 64</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span><span></span><span>=</span><span></span></span><span><span></span><span>64</span></span></span></span> attention heads</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">B = 8</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>B</span><span></span><span>=</span><span></span></span><span><span></span><span>8</span></span></span></span> batch size of 8 sequences</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">d_k = 128</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>128</span></span></span></span> key/value dimension</li>
<li>16-bit precision</li>
</ul>
<p>The memory required for a batch of 8 sequences of 1000 tokens each would be:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>B</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext> bytes</mtext><mo>=</mo><mn>80</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>1000</mn><mo>×</mo><mn>128</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext> bytes</mtext><mo>=</mo><mn>20.97</mn><mtext>GB</mtext></mrow><annotation encoding="application/x-tex">L \times H \times B \times n \times d_k \times 2 \times 2 \text{ bytes} = 80 \times 64 \times 8 \times 1000 \times 128 \times 2 \times 2 \text{ bytes} = 20.97\text{GB}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span><span></span><span>×</span><span></span></span><span><span></span><span>H</span><span></span><span>×</span><span></span></span><span><span></span><span>B</span><span></span><span>×</span><span></span></span><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>×</span><span></span></span><span><span></span><span>2</span><span></span><span>×</span><span></span></span><span><span></span><span>2</span><span><span> bytes</span></span><span></span><span>=</span><span></span></span><span><span></span><span>80</span><span></span><span>×</span><span></span></span><span><span></span><span>64</span><span></span><span>×</span><span></span></span><span><span></span><span>8</span><span></span><span>×</span><span></span></span><span><span></span><span>1000</span><span></span><span>×</span><span></span></span><span><span></span><span>128</span><span></span><span>×</span><span></span></span><span><span></span><span>2</span><span></span><span>×</span><span></span></span><span><span></span><span>2</span><span><span> bytes</span></span><span></span><span>=</span><span></span></span><span><span></span><span>20.97</span><span><span>GB</span></span></span></span></span></span>
<p>This substantial memory usage creates several challenges:</p>
<ol>
<li><strong>Scales linearly</strong> with sequence length</li>
<li><strong>Multiplies</strong> with batch size for parallel processing</li>
<li><strong>Limits</strong> the maximum context length we can handle</li>
<li><strong>Constrains</strong> deployment on memory-limited devices</li>
</ol>
<p>These challenges have sparked a wave of innovation in the research community, leading to various techniques for optimizing KV cache usage. Let&#39;s explore these cutting-edge solutions.</p>

<p>The following papers represent key innovations in KV cache optimization. We&#39;ll explore them through three main approaches: token selection, post-hoc compression techniques, and architectural redesigns.</p>
<h2>Token Selection and Pruning Approaches</h2>
<h3>1) <a href="https://arxiv.org/abs/2306.14048">Heavy-Hitter Oracle (H2O)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/h2o_alg.png" alt=""/></p>
<p>H2O introduces the concept of identifying and preserving important tokens in the KV cache:</p>
<ul>
<li><strong>Heavy-Hitter Tokens</strong>: H2O identifies tokens with the highest accumulated attention scores during generation, following a power-law distribution. These tokens are critical for model functionality and are prioritized in the cache.</li>
<li><strong>Dynamic Submodular Eviction</strong>: The method frames cache management as an optimization problem with a submodular objective function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(S)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>F</span><span>(</span><span>S</span><span>)</span></span></span></span> that quantifies the importance of a token set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>S</span></span></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">F(S) = \sum_{i \in S} A_{i}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>F</span><span>(</span><span>S</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>∈</span><span>S</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the accumulated attention score for token <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span>. The cache <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">S_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is updated by:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><msub><mtext>argmax</mtext><mrow><mi>S</mi><mo>⊆</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mo separator="true">,</mo><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>k</mi></mrow></msub><mtext> </mtext><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S_t = \text{argmax}_{S \subseteq S_{t-1} \cup \{i\}, |S| \leq k} \, F(S)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>argmax</span></span><span><span><span><span><span><span></span><span><span><span>S</span><span>⊆</span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∪</span><span>{</span><span>i</span><span>}</span><span>,</span><span>∣</span><span>S</span><span>∣</span><span>≤</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>F</span><span>(</span><span>S</span><span>)</span></span></span></span></span>
ensuring that at most one token is evicted per step. This greedy algorithm is computationally efficient and guarantees near-optimal performance under submodular constraints.</li>
<li><strong>Results</strong>: Achieves <strong>5× reduction</strong> in KV cache size with negligible accuracy loss and up to <strong>29×</strong> throughput improvement.</li>
</ul>
<h3>2) <a href="https://arxiv.org/abs/2309.17453">StreamLLM</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/streamingLLM.png" alt=""/></p>
<ul>
<li>The authors observe the phenomenon of <strong>Attention Sinks</strong>: Initial tokens that act as natural attention anchors during decoding
<ul>
<li>Without these attention sink tokens, the performance of naive window attention drops</li>
</ul>
</li>
<li>Based on that observation, they introduce a <strong>Rolling Cache</strong> for recent context with retained initial tokens, enabling infinite-length sequence processing.</li>
<li>They show that these sink tokens can also be <strong>trained</strong>; serving as dedicated attention anchors, reducing reliance on multiple initial tokens.</li>
</ul>
<h3>3) <a href="https://arxiv.org/abs/2406.12335">Value-Aware Token Pruning (VATP)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/vatp.png" alt=""/></p>
<p>VATP extends H2O&#39;s token importance concept by considering both attention patterns and value vector properties:</p>
<ul>
<li><strong>Importance Scoring</strong>: Combines attention scores with value vector information:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>⋅</mo><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>t</mi></mrow></munder><msub><mi>a</mi><mrow><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">I_k^t = S_k^t \cdot \|v_k\|_1, \quad S_k^t = \sum_{k \leq j \leq t} a_{j,k}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>I</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span>∥</span><span><span>v</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>k</span><span>≤</span><span>j</span><span>≤</span><span>t</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>a</span><span><span><span><span><span><span></span><span><span><span>j</span><span>,</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">S_k^t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the accumulated attention score and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\|v_k\|_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∥</span><span><span>v</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the value vector&#39;s L1 norm.</li>
<li><strong>Token Pruning</strong>: Tokens are ranked by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">I_k^t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>I</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and those with the lowest scores are pruned, while <strong>attention sink tokens</strong> (e.g., start or newline tokens) are preserved to prevent performance degradation.</li>
<li><strong>Performance and Efficiency</strong>:
<ul>
<li>Outperforms baselines like H2O and Scissorhands in 12–14 out of 16 LongBench tasks.</li>
<li>Achieves effective <strong>50% compression</strong> with minimal performance loss.</li>
<li>Introduces negligible computational overhead and is compatible with FlashAttention when integrated with Scissorhands.</li>
</ul>
</li>
</ul>
<h2>Post-hoc Compression Techniques</h2>
<p>These methods compress or optimize the KV cache while preserving the standard transformer architecture.</p>
<h3>4) <a href="https://arxiv.org/pdf/2310.01801">Adaptive KV Compression (FastGen)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/fastgen_1.png" alt=""/></p>
<p>FastGen introduces adaptive compression based on attention patterns observed at run-time:</p>
<ul>
<li><strong>Attention Profiling</strong>: during prompt encoding, FastGen identifies attention patterns and selects compression policies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">C^*</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span></span></span></span> that minimize memory cost while preserving attention recovery:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>C</mi><mo>∈</mo><mi mathvariant="script">C</mi></mrow></munder><mtext>CacheMemoryCost</mtext><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mspace width="1em"></mspace><mtext>s.t.</mtext><mspace width="1em"></mspace><mi mathvariant="normal">∥</mi><mi>A</mi><mo>−</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>K</mi><mi>C</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi><mo>≤</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">C^* = \arg\min_{C \in \mathcal{C}} \text{CacheMemoryCost}(C) \quad \text{s.t.} \quad \|A - \text{softmax}(QK_C^T)\| \leq 1 - T.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>ar<span>g</span></span><span></span><span><span><span><span><span><span></span><span><span><span>C</span><span>∈</span><span>C</span></span></span></span><span><span></span><span><span>min</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>CacheMemoryCost</span></span><span>(</span><span>C</span><span>)</span><span></span><span><span>s.t.</span></span><span></span><span>∥</span><span>A</span><span></span><span>−</span><span></span></span><span><span></span><span><span>softmax</span></span><span>(</span><span>Q</span><span><span>K</span><span><span><span><span><span><span></span><span><span>C</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>∥</span><span></span><span>≤</span><span></span></span><span><span></span><span>1</span><span></span><span>−</span><span></span></span><span><span></span><span>T</span><span>.</span></span></span></span></span>
</li>
<li><strong>Adaptive Compression Policies</strong>:
<ul>
<li>Compression strategies include:
<ul>
<li><strong>Special Tokens</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>special</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>): Retain only special tokens.</li>
<li><strong>Locality</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>local</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{local}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>local</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>): Evict tokens beyond a relative distance <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">r_l</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>r</span><span><span><span><span><span><span></span><span><span>l</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</li>
<li><strong>Frequency</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>frequent</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{frequent}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>frequent</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>): Keep tokens with high cumulative attention scores (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>r</span><span><span><span><span><span><span></span><span><span>f</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>).</li>
<li><strong>Hybrid Policies</strong> combine strategies, starting with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>special</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and applies them adaptively to each head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">C</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo>+</mo><msub><mi>C</mi><mtext>punct</mtext></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>C</mi><mtext>full</mtext></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\mathcal{C} = \{C_{\text{special}}, C_{\text{special}} + C_{\text{punct}}, \ldots, C_{\text{full}}\}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>C</span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>special</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>special</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>punct</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span><span><span>full</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span></span></span></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li><strong>Token Generation</strong>:
<ul>
<li>During decoding, pre-selected compression policies manage the KV cache efficiently:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo separator="true">,</mo><msub><mi>V</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{C_i}, V_{C_i} = f(K, V, C_i).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>C</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>C</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span>K</span><span>,</span><span></span><span>V</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span></span></span></span>
</li>
</ul>
</li>
</ol>
<h3>5) <a href="https://arxiv.org/pdf/2403.09636">Dynamic Memory Compression (DMC)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/dmc.png" alt=""/></p>
<p>DMC introduces adaptive token merging:</p>
<ul>
<li><strong>Decision Mechanism</strong>: At time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>, predicts merge decisions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\omega_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">⌊</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo stretchy="false">⌉</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>ω</mi><mi>t</mi></msub><mo>=</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\alpha_t = \lfloor \text{sigmoid}(k_t[0]) \rceil, \quad \omega_t = \text{sigmoid}(q_t[0]).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>⌊</span><span><span>sigmoid</span></span><span>(</span><span><span>k</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>0</span><span>])⌉</span><span>,</span><span></span><span></span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>sigmoid</span></span><span>(</span><span><span>q</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>0</span><span>])</span><span>.</span></span></span></span></span>
</li>
<li><strong>Weighted Merging</strong>: When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_t = 1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span>, merges current and previous entries:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>k</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo><mspace width="1em"></mspace><msup><mi>v</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">k&#39; = \frac{\omega_t k_t + z_{t-1} k_{t-1}}{\omega_t + z_{t-1}}, \quad v&#39; = \frac{\omega_t v_t + z_{t-1} v_{t-1}}{\omega_t + z_{t-1}},</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>,</span><span></span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>,</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t = z_{t-1} + \omega_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>ω</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> accumulates importance weights.</li>
<li><strong>Training</strong>:
<ul>
<li>Uses a Gumbel-Sigmoid relaxation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to allow end-to-end training with gradient descent:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>∼</mo><mtext>Gumbel-Sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>τ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\alpha_t \sim \text{Gumbel-Sigmoid}(k_t[0], \tau),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∼</span><span></span></span><span><span></span><span><span>Gumbel-Sigmoid</span></span><span>(</span><span><span>k</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>0</span><span>]</span><span>,</span><span></span><span>τ</span><span>)</span><span>,</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>τ</span></span></span></span> is a temperature parameter.</li>
<li>Optimizes a combined objective:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub><mo>+</mo><mi>λ</mi><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mi>n</mi><mtext>CR</mtext></mfrac><mo>−</mo><munder><mo>∑</mo><mi>t</mi></munder><msub><mi>α</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \max\left(0, \frac{n}{\text{CR}} - \sum_{t} \alpha_t \right),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span><span></span><span>=</span><span></span></span><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>LM</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>λ</span><span></span><span>max</span><span></span><span><span><span>(</span></span><span>0</span><span>,</span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>CR</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>−</span><span></span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>,</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{LM}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>LM</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the language modeling loss, and the second term encourages the model to match a target compression ratio (CR).</li>
</ul>
</li>
<li><strong>Results</strong>: Up to <strong>8× compression</strong> with maintained performance.</li>
</ul>
<h3>6) <a href="https://arxiv.org/pdf/2406.11430"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> Norm-Based Compression</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/l2.png" alt=""/></p>
<p>This paper presents a surprising observation: A clear correlation between the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> norm and the attention scores over cached KV pairs, where a low <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> norm of a key embedding usually leads to a high attention score during decoding. Consequently, they introduce a simple but effective compression objective:</p>
<ul>
<li><strong>Norm-Based Selection</strong>: For a set of cached keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>k</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K = \{k_1, k_2, \dots, k_n\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>k</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span>, computes and sorts key norms:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msubsup><mi>k</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|k_i\|_2 = \sqrt{\sum_{j=1}^d k_{i,j}^2}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∥</span><span><span>k</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>d</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>i</span><span>,</span><span>j</span></span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="3.5176em" viewBox="0 0 400000 3517" preserveAspectRatio="xMinYMin slice"><path d="M702 80H40000040
H742v3383l-4 4-4 4c-.667.7 -2 1.5-4 2.5s-4.167 1.833-6.5 2.5-5.5 1-9.5 1
h-12l-28-84c-16.667-52-96.667 -294.333-240-727l-212 -643 -85 170
c-4-3.333-8.333-7.667-13 -13l-13-13l77-155 77-156c66 199.333 139 419.667
219 661 l218 661zM702 80H400000v40H742z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>
</li>
<li><strong>Sorting and Selection</strong>: To compress the KV cache, sort all keys by their L2 norm values:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>sorted</mtext></msub><mo>=</mo><mtext>Sort</mtext><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mo stretchy="false">{</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>1</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>2</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>n</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo stretchy="false">}</mo><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo></mrow><annotation encoding="application/x-tex">K_{\text{sorted}} = \text{Sort}\big(\{\|k_1\|_2, \|k_2\|_2, \dots, \|k_n\|_2\}\big)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>sorted</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>Sort</span></span><span><span>(</span></span><span>{</span><span>∥</span><span><span>k</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>∥</span><span><span>k</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>∥</span><span><span>k</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span><span>)</span></span></span></span></span></span>
Retain the top-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span></span></span></span> keys with lowest norms, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mo stretchy="false">⌊</mo><mi>c</mi><mo>⋅</mo><mi>n</mi><mo stretchy="false">⌋</mo></mrow><annotation encoding="application/x-tex">m = \lfloor c \cdot n \rfloor</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span><span></span><span>=</span><span></span></span><span><span></span><span>⌊</span><span>c</span><span></span><span>⋅</span><span></span></span><span><span></span><span>n</span><span>⌋</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>c</span></span></span></span> is the compression ratio.</li>
<li><strong>Compressed Cache</strong>: The compressed key-value cache consists of:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∣</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>∈</mo><msub><mi>K</mi><mtext>sorted</mtext></msub><mo stretchy="false">[</mo><mn>1</mn><mo>:</mo><mi>m</mi><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>v</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>K</mi><mtext>compressed</mtext></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K_{\text{compressed}} = \{k_i \mid \|k_i\|_2 \in K_{\text{sorted}}[1:m]\}, \quad V_{\text{compressed}} = \{v_i \mid k_i \in K_{\text{compressed}}\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>compressed</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>k</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span>∥</span><span><span>k</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>sorted</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>1</span><span></span><span>:</span><span></span></span><span><span></span><span>m</span><span>]}</span><span>,</span><span></span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>compressed</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>compressed</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span></span>
</li>
<li>Due to its simplicity, this approach maintains compatibility with <strong>FlashAttention</strong>.</li>
</ul>
<h2>Architectural Redesigns</h2>
<p>These approaches change the Transformers architecture to handle KV caches more efficiently, often incorporating compression directly into the architecture.</p>
<h3>7) <a href="https://arxiv.org/pdf/2305.13245">Multi-Query Attention (MQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mqa.png" alt=""/></p>
<ul>
<li><strong>Key Idea</strong>: MQA reduces the KV cache size by sharing a <strong>single key-value head</strong> across all query heads, replacing the traditional Multi-Head Attention (MHA):
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K = XW_K, \quad V = XW_V,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span></span><span>=</span><span></span></span><span><span></span><span>X</span><span><span>W</span><span><span><span><span><span><span></span><span><span>K</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span>V</span><span></span><span>=</span><span></span></span><span><span></span><span>X</span><span><span>W</span><span><span><span><span><span><span></span><span><span>V</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span> are the shared key and value projections.</li>
<li><strong>Benefits</strong>: Reduces the KV cache size by a factor of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> (the number of attention heads), significantly lowering memory bandwidth overhead.</li>
<li><strong>Trade-Off</strong>: While MQA is faster, it often suffers from <strong>quality degradation</strong>, especially in tasks requiring diverse attention patterns.</li>
</ul>
<h3>8) <a href="https://arxiv.org/abs/2305.13245">Group-Query Attention (GQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/gqa.png" alt=""/></p>
<ul>
<li><strong>Key Idea</strong>: GQA interpolates between full multi-head attention and MQA to offering a scalable trade-off between inference speed and model quality. It divides query heads into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>G</span></span></span></span> groups, where each group shares a single key-value head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>K</mi><mi>h</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>V</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">K_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} K_h, \quad V_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} V_h</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>group</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>∣</span><span>G</span><span>∣</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>h</span><span>∈</span><span>G</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>h</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>group</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>∣</span><span>G</span><span>∣</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>h</span><span>∈</span><span>G</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>h</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<ul>
<li><strong>GQA-1</strong>: Equivalent to MQA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">G = 1 </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>G</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span>).</li>
<li><strong>GQA-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span></strong>: Equivalent to MHA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">G = H </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>G</span><span></span><span>=</span><span></span></span><span><span></span><span>H</span></span></span></span>).</li>
</ul>
</li>
<li><strong>Uptraining</strong>: GQA can be introduced to existing pre-trained models through fine-tuning:
<ul>
<li>First, convert MHA checkpoints to GQA by <strong>mean pooling</strong> key and value heads into groups</li>
<li>Then fine-tune (&#34;uptrain&#34;) the model briefly to adapt to the new attention pattern</li>
<li>This adaptation process requires only <strong>5% of the original pre-training compute</strong>, making it very efficient</li>
<li>The resulting model maintains quality while gaining GQA&#39;s memory benefits</li>
</ul>
</li>
</ul>
<h3>9) <a href="https://arxiv.org/abs/2405.04434">Multi-head Latent Attention (MLA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mla.png" alt=""/></p>
<p>DeepSeek&#39;s <strong>Multi-Head Latent Attention (MLA)</strong> takes a novel approach to reducing KV cache overhead. While MQA and GQA achieve this through head-sharing, MLA instead employs a <strong>low-rank latent compression</strong> technique that maintains the benefits of multiple attention heads.</p>
<ul>
<li>MLA reduces KV cache size by compressing keys and values into low-dimensional latent vectors before reconstruction.</li>
<li>It down-project key-value embeddings into a compressed latent space:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mtext>DKV</mtext></msub><msub><mi>h</mi><mi>t</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UK</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>v</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UV</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}, t} = W_{\text{DKV}} h_t, \quad k_C = W_{\text{UK}} c_{\text{KV}, t}, \quad v_C = W_{\text{UV}} c_{\text{KV}, t}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span><span><span>KV</span></span><span>,</span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>DKV</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>h</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>C</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>UK</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>c</span><span><span><span><span><span><span></span><span><span><span><span>KV</span></span><span>,</span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>C</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>UV</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>c</span><span><span><span><span><span><span></span><span><span><span><span>KV</span></span><span>,</span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>DKV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{DKV}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>DKV</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the down-projection matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UK</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UK}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>UK</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UV}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>UV</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are up-projection matrices for keys and values.</li>
<li>It retains per-head flexibility through compressed representations, unlike MQA&#39;s complete head sharing.</li>
<li>It introduces <strong>Rotary Positional Embeddings (RoPE)</strong> for decoupling position-aware keys:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub><mo>=</mo><mtext>RoPE</mtext><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>K</mi><mi>R</mi></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi>k</mi><mi>C</mi></msub><mo separator="true">;</mo><msub><mi>k</mi><mi>R</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">k_R = \text{RoPE}(W_{KR} h_t), \quad k_t = [k_C; k_R]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>R</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>RoPE</span></span><span>(</span><span><span>W</span><span><span><span><span><span><span></span><span><span><span>K</span><span>R</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>h</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span></span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>[</span><span><span>k</span><span><span><span><span><span><span></span><span><span>C</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>;</span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>R</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>]</span></span></span></span></span>
This reduces KV cache storage further by caching only compressed latent vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext>KV</mtext></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span><span><span>KV</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and positional keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">k_R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>R</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</li>
</ul>
<h3>10) <a href="https://arxiv.org/pdf/2404.14469">SnapKV</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/snapKV.png" alt=""/></p>
<ul>
<li>SnapKV introduces an <strong>Observation Window</strong>: Uses end-of-prompt tokens to identify attention patterns:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>L</mi><mtext>obs</mtext></msub></munderover><msub><mi>W</mi><mtext>obs</mtext></msub><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">]</mo><mo separator="true">,</mo><mspace width="1em"></mspace><mi>I</mi><mo>=</mo><msub><mtext>Top</mtext><mi>k</mi></msub><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C = \sum_{i=0}^{L_{\text{obs}}} W_{\text{obs}}[:, i, :], \quad I = \text{Top}_k(C, k)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>C</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>obs</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>obs</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>:</span></span><span><span></span><span>,</span><span></span><span>i</span><span>,</span><span></span><span>:</span></span><span><span></span><span>]</span><span>,</span><span></span><span></span><span>I</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>Top</span></span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>C</span><span>,</span><span></span><span>k</span><span>)</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>obs</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{obs}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span><span>obs</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> represents the attention weights, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> is determined by the compression rate.</li>
<li><strong>Compression</strong>: Clusters features around the selected positions using a pooling layer to preserve context completeness.</li>
</ul>
<h3>11) <a href="https://arxiv.org/pdf/2405.05254">You Only Cache Once (YOCO)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/yoco_2.png" alt=""/></p>
<p>YOCO modifies the transformer architecture for caching:</p>
<ul>
<li><strong>Global Cache</strong>: Uses a decoder-decoder design with a single shared KV cache.</li>
<li><strong>Complexity Reduction</strong>: Reduces memory from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>×</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \times L)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>×</span><span></span></span><span><span></span><span>L</span><span>)</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N + L)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>+</span><span></span></span><span><span></span><span>L</span><span>)</span></span></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> is sequence length and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>L</span></span></span></span> is the number of layers.</li>
<li><strong>Efficient Attention</strong>: The self-decoder employs <strong>sliding-window attention</strong> or <strong>gated retention</strong>, enabling constant memory usage (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(C)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>C</span><span>)</span></span></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>C</span></span></span></span> is a small window size).</li>
</ul>
<hr/>

<p>Key-Value caching techniques are central to scaling and optimizing Transformer-based models for real-world use. Innovations like dynamic eviction, compression, and structured approximations continue to push the boundaries on what is possible in long-context or resource-constrained scenarios. KV caching remains a lively research area, offering both theoretical insights and practical improvements.</p>
<p>PS: This blog post is mostly AI-generated using a PySpur workflow with minor human edits.</p></article></div><section id="cta"><div><div><p><h2>Ready to get started?</h2><h3>Support us by starring our repository.</h3></p></div></div></section></section></div></div>
  </body>
</html>
