<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/agents">Original</a>
    <h1>The Anatomy of Autonomy: Why Agents Are the Next AI Killer App</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><em><span>Welcome to the 12k new listeners who checked out </span><a href="https://www.latent.space/p/segment-anything-roboflow#details" rel="">the Segment Anything pod</a><span>!</span></em></p><p><em><span>We’ve launched a </span><a href="https://www.latent.space/p/community" rel="">new community page</a><span> with our upcoming events in SF, NY and </span><a href="https://partiful.com/e/c4ox9BmcglwzLB3QkW3d" rel="">Miami</a><span> (this Friday)! See you if you’re in town!</span></em></p><p><span>“GPTs are General Purpose Technologies”</span></p><p><span>, but every GPT needs a killer app. Personal Computing needed VisiCalc, the smartphone brought us Uber, Instagram, Pokemon Go and iMessage/WhatsApp, and mRNA research enabled rapid production of the Covid vaccine</span></p><p><span>.</span></p><p>One of the strongest indicators that the post GPT-3 AI wave is more than “just hype” is that the killer apps are already evident, each &gt;$100m opportunities:</p><ul><li><p><strong>Generative Text for writing</strong><span> - Jasper AI going 0 to $75m ARR in 2 years</span></p></li><li><p><strong>Generative Art for non-artists</strong><span> - Midjourney/Stable Diffusion</span><a href="https://www.latent.space/p/multiverse-not-metaverse?utm_source=%2Fsearch%2Fmultiverse&amp;utm_medium=reader2" rel=""> Multiverses</a></p></li><li><p><strong>Copilot for knowledge workers</strong><span> - both GitHub’s </span><a href="https://twitter.com/swyx/status/1638550858073006089" rel="">Copilot X</a><span> and “</span><a href="https://www.latent.space/p/what-building-copilot-for-x-really" rel="">Copilot for X</a><span>”</span></p></li><li><p><strong>Conversational AI UX</strong><span> - </span><a href="https://lspace.swyx.io/p/everything-we-know-about-chatgpt" rel="">ChatGPT</a><span> / Bing Chat, with a long tail of Doc QA startups</span></p></li></ul><p>I write all this as necessary context to imply: </p><p><span>The fifth killer app is here, and it is </span><strong>Autonomous Agents</strong><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png" width="1456" height="858" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/c9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:858,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:783355,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9dfcfd3-d90e-4981-9e6e-eb625290a6ee_2118x1248.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>But first, as usual, let’s start with an executive summary to catch up those out of the loop.</p><p><a href="https://github.com/Significant-Gravitas/Auto-GPT/tree/master/autogpt" rel="">Auto-GPT</a><span> (and its younger sibling </span><a href="https://github.com/yoheinakajima/babyagi" rel="">BabyAGI</a><span>) are independently developed Python projects, open sourced March </span><a href="https://fortune.com/2023/04/15/babyagi-autogpt-openai-gpt-4-autonomous-assistant-agi/" rel="">30th</a><span> and </span><a href="https://github.com/yoheinakajima/babyagi/graphs/contributors" rel="">April 2nd</a><span> respectively, which have caught enormous popularity, with Auto-GPT </span><a href="https://twitter.com/SigGravitas/status/1646117442031349761?s=20" rel="">trending #1 on Twitter and GitHub</a><span> in the past 2 weeks (</span><a href="https://star-history.com/#Significant-Gravitas/Auto-GPT&amp;CompVis/stable-diffusion&amp;facebookresearch/segment-anything&amp;hwchase17/langchain&amp;Date" rel="">far outpacing every other open source AI project</a><span> including </span><a href="https://www.latent.space/p/segment-anything-roboflow#details" rel="">Segment-Anything</a><span>, Stable Diffusion, and the now Sequoia-crowned </span><a href="https://web.archive.org/web/20230414032253/https://www.businessinsider.com/sequoia-leads-funding-round-generative-artificial-intelligence-startup-langchain-2023-4?r=US&amp;IR=T" rel="">$200m-valuation-LangChain</a><span>).</span></p><p><span>Both projects do </span><em><strong>not</strong></em><span> involve foundation model training or indeed any deep ML innovation; rather they demonstrate viability of applying </span><em>existing</em><span> LLM APIs (GPT3, 4, or any of the alternatives) and reasoning/tool selection prompt patterns </span><strong>in an infinite loop,</strong><span> to do potentially </span><strong>indefinitely long-running</strong><span>, iterative work to </span><strong>accomplish a high level goal</strong><span> set by a human user.</span></p><p><span>We do mean “high level” — </span><a href="https://www.linkedin.com/in/toran-bruce-richards-54139814b/?originalSubdomain=uk" rel="">Toran Richards</a><span>’ </span><a href="https://github.com/Significant-Gravitas/Auto-GPT/commit/c74ad984cfa755031a85296c45a8a8d77e0b8906?short_path=b335630#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5" rel="">original demo</a><span> for Auto-GPT was “</span><em>an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth</em><span>”, while Yohei Nakajima coded up </span><a href="https://twitter.com/jacksonfall/status/1636107218859745286" rel="">Jackson Fall’s viral HustleGPT prompt on ChatGPT</a><span> and told it to </span><em><span>“</span><a href="https://twitter.com/yoheinakajima/status/1640068466974633987?s=20" rel="">start and grow a mobile AI startup</a><span>”</span></em><span>. In the 2 weeks since, community members have </span><a href="https://twitter.com/SullyOmarr/status/1646632867412459521" rel="">built extensions</a><span> and </span><a href="https://twitter.com/_Lonis_/status/1646641412182536196" rel="">clones</a><span> and </span><a href="https://twitter.com/thegarrettscott/status/1645918390413066240" rel="">agent managers</a><span> and </span><a href="https://twitter.com/dysmemic/status/1645535982996054016" rel="">frameworks</a><span> and </span><a href="https://twitter.com/skirano/status/1646582731629887503" rel="">ChatGPT plugins</a><span> and </span><a href="https://twitter.com/wm_eddie/status/1646752950541492225" rel="">visual toolkits</a><span> and so on, with usecases in </span><a href="https://twitter.com/SullyOmarr/status/1645205292756418562" rel="">market research</a><span>, </span><a href="https://twitter.com/adamcohenhillel/status/1644836492294905856" rel="">test driven development</a><span>, and </span><a href="https://twitter.com/eimenhmdt/status/1644199933321306112" rel="">scientific literature review</a><span>.</span></p><p>Beyond those similarities, the projects are very different in their approaches.</p><ul><li><p><span>BabyAGI is intentionally small, </span><a href="https://twitter.com/yoheinakajima/status/1644326587389853697?s=20" rel="">adding and stripping out LangChain</a><span>, with its initial code being </span><a href="https://replit.com/@YoheiNakajima/babyagi?v=1#main.py" rel="">less than 150 lines</a><span> and </span><a href="https://github.com/yoheinakajima/babyagi/blob/17f1e830e44d83f8b6d52e3b932ba26a98e702a0/.env.example#L8" rel="">10 env vars</a><span> (it is now ~800LOC).</span></p></li><li><p><span>While Auto-GPT is </span><strong>extremely expansive</strong><span> (7300 LOC), with the ability to clone GitHub repos, start other agents, </span><a href="https://twitter.com/SigGravitas/status/1641649430230351872?s=20" rel="">speak</a><span>, send tweets, and generate images, with </span><a href="https://github.com/Significant-Gravitas/Auto-GPT/blob/ecf2ba12db11ff19bce359b842f810f0e2d09d6a/.env.template" rel="">50 env vars</a><span> to support every vector database and LLM provider/Text to Image model/Browser.</span></p></li></ul><p><span>The projects have caught the imagination of leading AI figures as well, with Andrej Karpathy calling AutoGPTs the “</span><a href="https://twitter.com/karpathy/status/1642598890573819905?s=20" rel="">next frontier of prompt engineering</a><span>” and Eliezer Yudkowsky </span><a href="https://twitter.com/ESYudkowsky/status/1640511156254289926" rel="">approvingly observing</a><span> BabyAGI’s refusal to turn the world into paperclips even when prompted.</span></p><p><span>In my understanding of neurobiology, </span><a href="https://blogs.scientificamerican.com/talking-back/einsteins-brain-more-special-than-we-ever-knew/" rel="">every convolution</a><span> that wrinkles the brain a bit more makes us a little smarter. In a similar way, AI progresses by “convolutions”, and in retrospect our path to the present day has been obvious. I’d like to map it out:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png" width="1456" height="858" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:858,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:783355,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb414d5f9-c9ed-409d-a760-71172b8445b4_2118x1248.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><strong>Key Autonomy Capabilities arranged in rough chronological order</strong></p><ul><li><p><strong>Foundation models</strong><span>: </span></p><ul><li><p><span>Everything starts with the evolution </span><em>and</em><span> widespread availability of massive LLMs (via API or Open Source). The sheer size of these models finally allow for </span><a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1" rel="">3 major features</a><span>:</span></p><ul><li><p>~perfect natural language understanding and generation</p></li><li><p><span>world knowledge (</span><a href="https://arxiv.org/pdf/2301.00774.pdf" rel="">175B Parameters can store 320GB</a><span>, which is 15 Wikipedia’s)</span></p></li><li><p><a href="https://www.jasonwei.net/blog/emergence" rel="">emergence</a><span> of major capabilities like </span><strong>in-context learning</strong></p></li></ul></li><li><p><span>This leads to the rise of the early prompt engineers, like </span><a href="https://gwern.net/gpt-3" rel="">Gwern Branwern</a><span> and </span><a href="https://twitter.com/goodside/status/1569128808308957185" rel="">Riley Goodside</a><span> who explored creative single-shot prompts.</span></p></li></ul></li><li><p>Capability 1: Metacognition (self improvement of pure reasoning)</p><ul><li><p><a href="https://arxiv.org/abs/2205.11916" rel="">Kojima et al (2022)</a><span> found that simply adding “</span><a href="https://twitter.com/shaneguML/status/1530244895713046528" rel="">let’s think step by step</a><span>” to a prompt </span><em>dramatically</em><span> raised the performance of GPT3 on benchmarks, later found to be effective due to externalizing the working memory for harder tasks.</span></p></li><li><p><a href="https://arxiv.org/abs/2201.11903" rel="">Wei et al (2022)</a><span> formalized the technique of Chain of Thought prompting that further improved benchmark performance.</span></p></li><li><p><a href="https://learnprompting.org/docs/intermediate/self_consistency" rel="">Wang et al (2022)</a><span> found that taking a majority vote of multiple Chains of Thought worked even where regular CoT was found to be ineffective.</span></p></li><li><p><span>More and more techniques like </span><a href="https://arxiv.org/pdf/2102.09690.pdf" rel="">Calibrate Before Use</a><span>, </span><a href="https://ofir.io/self-ask.pdf" rel="">Self-Asking</a><span>, </span><a href="https://twitter.com/johnjnay/status/1641786389267185664" rel="">Recursively Criticize and Improve</a><span>, </span><a href="https://sites.google.com/view/automatic-prompt-engineer" rel="">Automatic Prompt Engineering</a><span>, appear.</span></p></li></ul></li></ul><ul><li><p><strong>Capability 2: External Memory</strong><span> (reading from mostly static external data)</span></p><ul><li><p>The capability of in-context/few shot learning could be used to cheaply update a foundation model beyond its’ knowledge cutoff date and focus attention on domain specific, private data</p></li><li><p><span>The constraints of limited context length lead to the need for embedding, chunking and chaining frameworks like LangChain, and vector databases like </span><a href="https://archive.is/wCoqH" rel="">Pinecone (now worth $700m), Weaviate ($200m), and Chroma ($75m)</a><span>.</span></p></li><li><p><span>Another way of using natural language to access and answer questions form relational databases are the Text to SQL companies, which included Perplexity AI (</span><a href="https://techcrunch.com/2023/04/04/ai-powered-search-engine-perplexity-ai-lands-26m-launches-ios-app/" rel="">$26m Series A</a><span>), Seek AI (</span><a href="https://lspace.swyx.io/p/sarah-nagy#details" rel="">$7.5m Seed</a><span>), and a long tail of other approaches including </span><a href="http://censusgpt.com/" rel="">CensusGPT</a><span> and </span><a href="https://ossinsight.io/explore/" rel="">OSS Insight</a><span>.</span></p></li></ul></li></ul><ul><li><p><strong>Capability 3: Browser Automation</strong><span> (sandboxed read-</span><strong>and-write</strong><span> in a browser)</span></p><ul><li><p><span>Sharif Shameem (an upcoming guest! more on a future pod) first demoed </span><a href="https://twitter.com/sharifshameem/status/1645664932418162688" rel="">GPT-3 automating Chrome to buy Airpods</a><span> in 2021.</span></p></li><li><p><a href="https://www.businesswire.com/news/home/20220426005963/en/AI-Transformer-Inventors-Launch-Adept-with-65M-to-Lend-a-Hand-to-Knowledge-Workers" rel="">Adept raised a Series A</a><span> with an all-star team of Transformer paper authors and launching the </span><a href="https://www.adept.ai/blog/act-1" rel="">ACT-1 Action Transformer</a><span> (now with a </span><a href="https://www.forbes.com/sites/kenrickcai/2023/03/14/adept-ai-startup-raises-350-million-series-b/?sh=7c29c7672cc3" rel="">hefty $350m Series B</a><span> despite the departure of Vaswani et al)</span></p></li><li><p><a href="https://twitter.com/natfriedman/status/1575631194032549888" rel="">Nat Friedman’s NatBot</a><span> brought browser automation back into the zeitgeist a year later, showing how an agent can make a restaurant reservation across google search and maps from a single natural language instruction.</span></p></li><li><p><a href="https://dust.tt/xp1" rel="">Dust XP1</a><span> was also released but was read-only, did not do any automation</span></p></li><li><p><span>A nice variant of browser agents are desktop agents - </span><a href="https://embra.app/" rel="">Embra AI</a><span> seem to tbe the most hyped here though still pre launch, and </span><a href="https://twitter.com/RewindAI" rel="">Rewind AI</a><span> could be next.</span></p></li><li><p><span>It would seem that </span><a href="https://www.latent.space/p/multimodal-gpt4" rel="">Multi-modal GPT4</a><span>’s visual capability would be able to greatly enable the desktop agents here, especially where no accessibility text or DOM is available.</span></p></li></ul></li></ul><ul><li><p><strong>Capability 4: Tool making and Tool use</strong><span> (server-side, hooked up to everything)</span></p><ul><li><p><strong>Search.</strong><span> Generated answers from memorized world knowledge, or retrieved and stuff into context from a database, will never be as up to date as just searching the web. OpenAI opened this can of worms with </span><a href="https://openai.com/blog/webgpt/" rel="">WebGPT</a><span>, showing their solution to crawling the web, summarizing content, and answering with references (now live in ChatGPT Plugins and in Bing Chat, but replicated in the wild with </span><a href="https://twitter.com/dust4ai/status/1587104029712203778" rel="">Dust</a><span> and </span><a href="https://www.notion.so/44485e5c97bd403ba4e1c2d5197af71d" rel="">others</a><span>).</span></p></li><li><p><strong>Writing Code to be Run</strong><span>. We knew that GPT-3 could write code, but it took a certain kind of brave soul like </span><a href="https://twitter.com/goodside/status/1568448128495534081" rel="">Riley Goodside to ask it to generate code</a><span> for known bad capabilities (like math) and to </span><em>run</em><span> the code that was generated. </span><a href="https://twitter.com/amasad/status/1568825727528878081" rel="">Replit turned out</a><span> to be the perfect hosting platform for this style of capability augmentation (</span><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" rel="">another example here</a><span>).</span></p></li><li><p><strong>ReAct</strong><span>. </span><a href="https://arxiv.org/abs/2210.03629" rel="">Yao et all (2022)</a><span> coined </span><a href="https://react-lm.github.io/" rel="">the ReAct pattern</a><span> which introduced a </span><a href="https://til.simonwillison.net/llms/python-react-pattern" rel="">delightfully simple</a><span> prompt template for enabling LLMs to make reliable tool choices for </span><strong>Rea</strong><span>soning + </span><strong>Act</strong><span>ing given a set of tools. </span><a href="https://arxiv.org/abs/2302.04761" rel="">Schick et al (2023)</a><span> introduced the Toolformer that specifically trained a model with special tokens, but this does not seem as popular.</span></p></li><li><p><strong>Multi-model Approaches.</strong><span> Models calling other models with capabilities they didn’t have were also being explored, with </span><a href="https://github.com/microsoft/JARVIS" rel="">HuggingGPT/Microsoft JARVIS</a><span> and </span><a href="https://github.com/microsoft/visual-chatgpt/tree/main" rel="">VisualChatGPT</a><span>.</span></p></li><li><p><strong>Self-Learning</strong><span>. </span><a href="https://twitter.com/DYtweetshere/status/1631349179934203904" rel="">Self-Learning Agent for Performing APIs</a><span> (SLAPA) searches for API documentation to teach itself HOW to use tools, not just WHEN. This approach was adapted for the OpenAPI (fka Swagger) spec for ChatGPT Plugins, which </span><a href="https://twitter.com/mitchellh/status/1638966754226610181?lang=en" rel="">also used natural language</a><span>.</span></p></li><li><p><span>Other semi-stealth mode startups that may be worth exploring in this zone are </span><a href="https://fixie.ai/" rel="">Fixie AI</a><span> and Alex </span><a href="https://minion.ai/" rel="">Minion AI</a><span>.</span></p></li></ul><p><span>At this point it is worth calling out that we have pretty much reached the full vision laid out by </span><a href="https://jmcdonnell.substack.com/p/the-near-future-of-ai-is-action-driven" rel="">this excellent post from John McDonnell</a><span> 6 months ago:</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png" width="1456" height="1426" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1426,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png" title="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1560f3c6-b6c3-48d0-be93-4560f3290dd7_1544x1512.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>So what net new thing are we seeing in this most recent capability spurt?</p><p>I think the clue is in the 4 agents that naturally evolved in BabyAGI (scroll up for diagram):</p><ul><li><p><span>The “</span><strong>context agent</strong><span>” (Capability 1 + 2) could be a much smarter version of the data augmented retrieval that both </span><a href="https://twitter.com/jerryjliu0/status/1647626532519841793" rel="">LlamaIndex</a><span> and </span><a href="https://blog.langchain.dev/retrieval/" rel="">Langchain</a><span> are working on. Yohei added the need for “</span><a href="https://twitter.com/yoheinakajima/status/1644326588794941440?s=20" rel="">relevant (task) context</a><span>” which may be slightly different than the classic semantic similarity algorithms offered by the vector databases.</span></p><ul><li><p><strong>Active learning</strong><span> may see a return to favor as autonomous “context agents” actively surface things they don’t know for prioritization</span></p></li></ul></li><li><p><span>The “</span><strong>execution agent</strong><span>” calls OpenAI, or any other </span><strong>foundation model</strong><span>, and could optionally make or use any provided tools to accomplish a task (Capability 3 + 4)</span></p></li><li><p><span>The “</span><strong>task creation agent</strong><span>”, well, creates tasks, but must not hallucinate and must self criticize and learn from previous tasks (Capability 1 + 2). Challenging, but not outside the bounds of </span><a href="https://www.latent.space/p/benchmarks-101#details" rel="">simple common sense benchmarks</a><span>.</span></p></li><li><p><span>And the last agent is the “</span><strong>prioritization agent</strong><span>”. Ah! A new task!</span></p></li></ul><p>That leads us to identify…</p><ul><li><p><strong>Capability 5: Planning, reflexion, and prioritization</strong></p><ul><li><p><a href="https://nanothoughts.substack.com/p/reflecting-on-reflexion" rel="">Shinn et al (2023)</a><span> showed that Reflexion - an autonomous agent with dynamic memory and self-reflection, could dramatically improve on GPT-4 benchmarks.</span></p></li><li><p><a href="https://twitter.com/atroyn/status/1645957559298449408" rel="">Shoggoth the Coder</a><span> won the recent ChatGPT Plugins Hackathon as an independent agent capable of proposing and submitting PR fixes to open source projects.</span></p></li><li><p><a href="https://arxiv.org/abs/2304.03442" rel="">Meta’s Simulacra paper</a><span> showed the entertaining potential of autonomous NPC agents interacting with each other in a game-like setting.</span></p></li><li><p><span>Regardless of use case, autonomous agents will be expected to plan further and further ahead, prioritizing task lists, reflecting on mistakes and keeping all relevant context in memory. </span><a href="https://arxiv.org/pdf/2303.12712" rel="">The “Sparks of AGI” paper</a><span> specifically called planning out as a notable weakness of GPT-4, meaning we will likely need further foundation model advancement before this is reliable.</span></p></li><li><p><span>The recent </span><a href="https://twitter.com/jh_damm/status/1646233661832929280?s=20" rel="">LangChain Agents webinar</a><span> discussion also highlighted the need for the ability to stack agents and coordinate between them.</span></p></li><li><p><span>In the </span><a href="https://www.latent.space/p/community" rel="">Latent Space Community</a><span>, AI virtual software developer platform </span><a href="https://github.com/e2b-dev/e2b" rel="">e2b</a><span> is already discussing the potential of having fleets of AI developer workers.</span></p></li></ul></li></ul><p><strong>What makes software valuable to humanity? </strong><span>In both my investing and career advice, I am fond of encouraging people to develop a “</span><strong>theory of value of software</strong><span>”.</span></p><p><span>One of the clearest value drivers</span></p><p><span> of software is automation. The one currency we all never have enough of is </span><strong>time</strong><span>, and the ability to obsolete human effort, whether by clever system design, hiring someone else, or programming a machine, both frees up our time and increases our ability to scale up our output by just doing more in parallel. In fact this can be regarded as a core definition of technology and civilization: </span></p><blockquote><p>“Civilization advances by extending the number of operations we can perform without thinking about them” — Alfred North Whitehead.</p></blockquote><p><span>The relationship betwen </span><strong>automation</strong><span> and </span><strong>autonomy</strong><span> is subtle but important:</span></p><ul><li><p><span>ChatGPT doesn’t do anything without your input, but once you punch the right prompts in, it can do an awful lot of research for you, especially </span><a href="https://www.latent.space/p/chatgpt-plugins#details" rel="">with Plugins</a></p></li><li><p>AutoGPTs by default require you to enter a goal and hit “yes” to approve each step it takes, but that is incrementally easier than having to write responses</p></li><li><p><span>AutoGPTs also have limited (</span><em>run for N steps</em><span>) and unlimited (</span><em>run forever)</em><span> “</span><a href="https://github.com/Significant-Gravitas/Auto-GPT#-continuous-mode-%EF%B8%8F" rel="">continuous modes</a><span>” which are fully autonomous but very likely to go wrong and therefore have to be closely monitored</span></p></li></ul><p><span>We’ve just explained that technological and civilization advance requires us to be able to do things </span><em>without thinking about them</em><span>, so clearly full autonomy with as much trust and reliability as possible is the ultimate goal here. Let a thousand agents bloom! AI Assistants is where most people start, but Josh Browder is working on </span><a href="https://twitter.com/jbrowder1/status/1612312707398795264" rel="">AI Lawyer</a><span>, Replika is working on </span><a href="https://www.youtube.com/watch?v=SFKA7T-v6WE" rel="">AI Waifu</a><span>, I want AI Junior Developers and AI Video and Podcast and Newsletter Editors, Karpathy wants us to keep going with the </span><a href="https://twitter.com/karpathy/status/1642610417779490816?s=20" rel="">AI C-Suite</a><span>.</span></p><p><span>Fortunately, we don’t have to reason out every step of this progression from first principles, because </span><a href="https://sae.org/" rel="">the Society of Automotive Engineers</a><span> established a shorthand for this almost a decade ago:</span></p><p><span>I’ll assume you are familiar with some of the self driving car discourse</span></p><p><span>, but it&#39;s time to understand that self-driving AI agents in 2023 are just about where self-driving cars were in ~2015. We are beginning to have some intelligence in the things we use, like Copilot and Gmail autocompletes, but it&#39;s very lightweight and our metaphorical hands are always at ten and two. </span></p><p>In the next decade, we&#39;ll want to hand over some steering, then monitoring, then fallback to AI, and that will probably map our progress with autonomous AI agents as well.</p><p>In the following decade, we’ll develop enough trust in our agents that we go from a many-humans-per-AI paradigm down to one-human-per-AI and on to many-AIs-per-human, following an accelerated version of the industrialization of computing from the 1960s to the 2010s since it is easier to iterate on and manipulate bits over atoms.</p><p>There will be two flavors, or schools of thought, on autonomous AI:</p><ul><li><p><span>The Jobs School: AI Agents that augment your agency, as “</span><a href="https://www.themarginalian.org/2011/12/21/steve-jobs-bicycle-for-the-mind-1990/" rel="">bicycles for your mind</a><span>”</span></p></li><li><p>The Zuck School: AI Algorithms that replace your agency, hijacking your mind</p></li></ul><p>We’ll want to try our best to guide our efforts to the former, but we won’t always succeed.</p><p>I will perhaps expand these in future, or not, but happy to discuss each in comments.</p><ul><li><p><span>Despite needing prompt templates, tool selection, memory storage and retrieval, and orchestration of agents, </span><a href="https://twitter.com/swyx/status/1648539094438273024" rel="">neither Babyagi nor AutoGPT use LangChain</a><span>.</span></p></li><li><p>The new FM “Bitter Lesson” - Every time we try to finetune foundation models to do a certain task, software engineers can write abstractions on top to do it less efficiently but faster. </p><ul><li><p><a href="https://twitter.com/swyx/status/1648398117417680901" rel="">vector db’s &gt; memorizing transformers</a></p></li><li><p>ReAct/SLAPA &gt; Toolformer/Adept ACT-1</p></li><li><p>there’s families of transformers and alternative architectures (H3, fwd-fwd algo) but nobody can name any that have stuck</p></li></ul></li><li><p>Academic research of deep reinforcement learning for agents seems trapped in games?</p><ul><li><p><span>MineDojo, Deepmind DreamerV3, GenerallyIntelligent </span><a href="https://youtu.be/g0muj39vCCY" rel="">Avalon</a></p></li><li><p>CICERO diplomacy is notable because did NOT use deep RL?</p></li></ul></li><li><p>Semantic search has been less of a killer product. why?</p></li><li><p>Backend GPT has not led anywhere. why?</p></li><li><p>“wen AI Agents” easier to discuss than “wen AGI”</p></li><li><p>all the open source winners are new to open source lol? is “open source experience” that valuable?</p><ul><li><p><a href="https://github.com/hwchase17/" rel="">harrison</a></p></li><li><p><a href="https://twitter.com/yoheinakajima/status/1644326591428972546?s=20" rel="">yohei</a></p></li><li><p><a href="https://github.com/Significant-Gravitas?type=source" rel="">siggravitas</a></p></li></ul></li><li><p>Why do people all use pinecone to store like 10 things in memory</p></li><li><p>Safety - no more AGI off switch with autonomous agents</p><ul><li><p>AGI Moloch means have fun staying poor = have fun staying safe</p></li><li><p>we’re calling for the wrong kinds of pause:</p><ul><li><p>pause on fleets of workers until we have extremely well developed constitutions and observability?</p></li></ul></li></ul></li><li><p>Predictions</p><ul><li><p>there will be “AI Agent platforms”</p><ul><li><p>with tools all enabled → Zapier and fixie well placed</p></li></ul></li><li><p>there will be “AI Agent fleets”</p><ul><li><p>especially if “idempotent”, readonly</p></li></ul></li><li><p>custom research into the 4 kinds of agents</p><ul><li><p>“context agent” → active learning</p></li><li><p>execution agent is most straightforward but need to do its job well</p></li><li><p>task creation → problems: hallucination, omission</p></li><li><p>prioritization agent</p><ul><li><p><a href="https://twitter.com/yoheinakajima/status/1644326606767546368?s=20" rel="">stepping on each other</a></p></li><li><p>will need to do DAGs</p></li><li><p>spawn on command</p></li><li><p><a href="https://twitter.com/hadiazouni/status/1648375237283659792?s=20" rel="">Actor model</a><span> / Agent Oriented Programming?</span></p></li></ul></li><li><p><span>5th agent - </span><a href="https://twitter.com/karpathy/status/1642607620673634304" rel="">reflection? metalearning?</a></p></li></ul></li><li><p>there will be “AI social networks”</p><ul><li><p>subreddit simulator</p></li></ul></li></ul></li><li><p>orchestration and the cyborg problem</p><ul><li><p><a href="https://twitter.com/sarahcat21/status/1646920947134504962" rel="">sarahcat observation</a><span> is correct</span></p></li><li><p><a href="https://www.swyx.io/temporal-centicorn#humans-in-the-loop-vs-ml-in-the-loop" rel="">swyx commentary on cyborg nodes</a><span> would be amazing</span></p></li></ul></li><li><p><span>Are level 5 agents </span><a href="https://www.latent.space/p/agi-hard" rel="">AGI-hard</a><span>?</span></p><ul><li><p>hard as self driving cars - perpetually 5 years away - uber &amp; apple gave up</p></li><li><p>must know when to yield to humans</p><ul><li><p>easy to require confirm for destructive actions, but sometimes unclear</p></li><li><p><span>“Interesting non-obvious note on GPT psychology is that unlike people they are completely unaware of their own strengths and limitations. E.g. that they have finite context window. That they can just barely do mental math. That samples can get unlucky and go off the rails.” Etc. </span><a href="https://twitter.com/karpathy/status/1642598890573819905" rel="">karpathy</a></p></li></ul></li><li><p><span>must solve prompt injection (</span><a href="https://twitter.com/simonw/status/1648521987776745472?s=20" rel="">hey @simon</a><span>)</span></p></li><li><p>will probably need to self improve statefully</p></li><li><p>principal agent problem</p><ul><li><p>we dont know how to prioritize humans, and you want to eval bot ability to prioritize? good luck</p></li></ul></li></ul></li></ul></div></div></div></article></div></div></div>
  </body>
</html>
