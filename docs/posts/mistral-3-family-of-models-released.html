<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mistral-3">Original</a>
    <h1>Mistral 3 family of models released</h1>
    
    <div id="readability-page-1" class="page"><div><div><p dir="ltr">Today, we announce Mistral 3, the next generation of Mistral models. Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3 – our most capable model to date – a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license. Open-sourcing our models in a variety of compressed formats empowers the developer community and puts AI in people’s hands through distributed intelligence.</p>
<p dir="ltr">The Ministral models represent the best performance-to-cost ratio in their category. At the same time, Mistral Large 3 joins the ranks of frontier instruction-fine-tuned open-source models.</p>
<h2 dir="ltr">Mistral Large 3: A state-of-the-art open model</h2>
<p><img src="https://cms.mistral.ai/assets/98aeee04-e1c3-43b7-b90e-c51da84d5e56.png?width=1905&amp;height=1242" alt="Chart Base Models (1)"/></p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/bdf27a12-76fd-4e62-be9b-938f14288a9a.png?width=1346&amp;height=1115" alt="3 Model Performance Comparison (instruct)"/></p>
<p dir="ltr">Mistral Large 3 is one of the best permissive open weight models in the world, trained from scratch on 3000 of NVIDIA’s H200 GPUs. Mistral Large 3 is Mistral’s first mixture-of-experts model since the seminal Mixtral series, and represents a substantial step forward in pretraining at Mistral. After post-training, the model achieves parity with the best instruction-tuned open-weight models on the market on general prompts, while also demonstrating image understanding and best-in-class performance on multilingual conversations (i.e., non-English/Chinese).</p>
<p dir="ltr">Mistral Large 3 debuts at #2 in the OSS non-reasoning models category (#6 amongst OSS models overall) on the <a href="https://lmarena.ai/leaderboard/text">LMArena leaderboard</a>.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/4626af3d-7554-4d50-9c0e-041fe7111ece.png?width=1905&amp;height=1242" alt="Lm Arena Chart Ml3"/></p>
<p dir="ltr">We release both the base and instruction fine-tuned versions of Mistral Large 3 under the Apache 2.0 license, providing a strong foundation for further customization across the enterprise and developer communities. A reasoning version is coming soon! </p>
<h3 dir="ltr">Mistral, NVIDIA, vLLM &amp; Red Hat join forces to deliver faster, more accessible Mistral 3</h3>
<p dir="ltr">Working in conjunction with vLLM and Red Hat, Mistral Large 3 is very accessible to the open-source community. We’re releasing a checkpoint in NVFP4 format, built with <a href="https://github.com/vllm-project/llm-compressor">llm-compressor</a>. This optimized checkpoint lets you run Mistral Large 3 efficiently on Blackwell NVL72 systems and on a single 8×A100 or 8×H100 node using <a href="https://github.com/vllm-project/vllm">vLLM</a>.</p>
<p dir="ltr">Delivering advanced open-source AI models requires broad optimization, achieved through a partnership with NVIDIA. All our new Mistral 3 models, from Large 3 to Ministral 3, were trained on NVIDIA Hopper GPUs to tap high-bandwidth HBM3e memory for frontier-scale workloads. NVIDIA’s extreme co-design approach brings hardware, software, and models together. NVIDIA engineers enabled efficient inference support for <a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener">TensorRT-LLM</a> and <a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener">SGLang</a> for the complete Mistral 3 family, for efficient low-precision execution.</p>
<p dir="ltr">For Large 3’s sparse MoE architecture, NVIDIA integrated state-of-the-art Blackwell attention and MoE kernels, added support for prefill/decode disaggregated serving, and collaborated with Mistral on speculative decoding, enabling developers to efficiently serve long-context, high-throughput workloads on GB200 NVL72 and beyond. On the edge, delivers optimized deployments of the Ministral models on <a href="http://nvidia.com/en-us/products/workstations/dgx-spark/">DGX Spark</a>, <a href="https://www.nvidia.com/en-us/ai-on-rtx/">RTX PCs and laptops</a>, and <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">Jetson devices</a>, giving developers a consistent, high-performance path to run these open models from data center to robot.</p>
<p dir="ltr">We are very thankful for the collaboration and want to thank vLLM, Red Hat, and NVIDIA in particular.</p>
<h2 dir="ltr">Ministral 3: State-of-the-art intelligence at the edge</h2>
<p><img src="https://cms.mistral.ai/assets/ea1fcc83-5bad-400e-b63a-35c8a8c0bf9c.png?width=1726&amp;height=1062" alt="4 Gpqa Diamond Accuracy"/></p>
<p dir="ltr">For edge and local use cases, we release the Ministral 3 series, available in three model sizes: 3B, 8B, and 14B parameters. Furthermore, for each model size, we release base, instruct, and reasoning variants to the community, each with image understanding capabilities, all under the Apache 2.0 license. When married with the models’ native multimodal and multilingual capabilities, the Ministral 3 family offers a model for all enterprise or developer needs.</p>
<p dir="ltr">Furthermore, Ministral 3 achieves the best cost-to-performance ratio of any OSS model. In real-world use cases, both the number of generated tokens and model size matter equally. The Ministral instruct models match or exceed the performance of comparable models while often producing an order of magnitude fewer tokens. </p>
<p dir="ltr">For settings where accuracy is the only concern, the Ministral reasoning variants can think longer to produce state-of-the-art accuracy amongst their weight class - for instance 85% on AIME ‘25 with our 14B variant.</p>



<h2 dir="ltr">Available Today</h2>
<p dir="ltr">Mistral 3 is available today on <a href="https://console.mistral.ai/home">Mistral AI Studio</a>, Amazon Bedrock, Azure Foundry, Hugging Face (<a href="https://huggingface.co/collections/mistralai/mistral-large-3">Large 3</a> &amp; <a href="https://huggingface.co/collections/mistralai/ministral-3">Ministral</a>), <a href="https://modal.com/docs/examples/ministral3_inference">Modal</a>, IBM WatsonX, OpenRouter, Fireworks, <a href="https://docs.unsloth.ai/new/ministral-3" target="_blank" rel="noopener">Unsloth AI</a>, and Together AI. In addition, coming soon on NVIDIA NIM and AWS SageMaker.</p>
<h3 dir="ltr">One more thing… customization with Mistral AI</h3>
<p dir="ltr">For organizations seeking tailored AI solutions, Mistral AI offers <a href="https://mistral.ai/solutions/custom-model-training">custom model training services</a> to fine-tune or fully adapt our models to your specific needs. Whether optimizing for domain-specific tasks, enhancing performance on proprietary datasets, or deploying models in unique environments, our team collaborates with you to build AI systems that align with your goals. For enterprise-grade deployments, custom training ensures your AI solution delivers maximum impact securely, efficiently, and at scale.</p>
<h3 dir="ltr">Get started with Mistral 3</h3>
<p dir="ltr">The future of AI is open. Mistral 3 redefines what’s possible with a family of models built for frontier intelligence, multimodal flexibility, and unmatched customization. Whether you’re deploying edge-optimized solutions with Ministral 3 or pushing the boundaries of reasoning with Mistral Large 3, this release puts state-of-the-art AI directly into your hands.</p>
<h3 dir="ltr">Why Mistral 3?</h3>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Frontier performance, open access: Achieve closed-source-level results with the transparency and control of open-source models.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Multimodal and multilingual: Build applications that understand text, images, and complex logic across 40+ native languages.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Scalable efficiency: From 3B to 675B active parameters, choose the model that fits your needs, from edge devices to enterprise workflows.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Agentic and adaptable: Deploy for coding, creative collaboration, document analysis, or tool-use workflows with precision.</p>
</li>
</ul>
<h3 dir="ltr">Next Steps</h3>
<ol>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Explore the model documentation: </p>
</li>
<ul>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation"><a href="https://docs.mistral.ai/models/ministral-3-3b-25-12">Ministral 3 3B-25-12</a></p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation"><a href="https://docs.mistral.ai/models/ministral-3-8b-25-12">Ministral 3 8B-25-12</a></p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation"><a href="https://docs.mistral.ai/models/ministral-3-14b-25-12">Ministral 3 14B-25-12</a></p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation"><a href="https://docs.mistral.ai/models/mistral-large-3-25-12">Mistral Large 3</a></p>
</li>
</ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Technical documentation for customers is available on our <a href="https://legal.mistral.ai/" target="_blank" rel="noopener">AI Governance Hub</a></p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Start building: <a href="https://huggingface.co/collections/mistralai/ministral-3">Ministral 3</a> and <a href="https://huggingface.co/collections/mistralai/mistral-large-3">Large 3</a> on Hugging Face, or deploy via <a href="https://console.mistral.ai/home">Mistral AI’s platform</a> for instant API access and <a href="https://mistral.ai/pricing#api-pricing">API pricing</a></p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Customize for your needs: Need a tailored solution? <a href="https://mistral.ai/contact">Contact our team</a> to explore fine-tuning or enterprise-grade training.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Share your projects, questions, or breakthroughs with us: <a href="https://x.com/MistralAI">Twitter/X</a>, <a href="https://discord.com/invite/mistralai">Discord</a>, or <a href="https://github.com/mistralai">GitHub</a>.</p>
</li>
</ol>
<p dir="ltr">Science has always thrived on openness and shared discovery. As pioneering French scientist and two-time Nobel laureate Marie Skłodowska-Curie once said, “Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.” </p>
<p dir="ltr">This philosophy drives our mission at Mistral AI. We believe that the future of AI should be built on transparency, accessibility, and collective progress. With this release, we invite the world to explore, build, and innovate with us, unlocking new possibilities in reasoning, efficiency, and real-world applications.</p>
<p dir="ltr"><strong>Together, let’s turn understanding into action.</strong></p></div></div></div>
  </body>
</html>
