<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/blog/falcon-180b">Original</a>
    <h1>Falcon 180B</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<!-- HTML_TAG_START -->



<h2>
	<a id="introduction" href="#introduction">
		
	</a>
	<span>
		Introduction
	</span>
</h2>
<p><strong>Today, we&#39;re excited to welcome <a href="https://falconllm.tii.ae/">TII&#39;s</a> Falcon 180B to HuggingFace!</strong> Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&#39;s <a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">RefinedWeb</a> dataset. This represents the longest single-epoch pretraining for an open model. </p>
<p>You can find the model on the Hugging Face Hub (<a href="https://huggingface.co/tiiuae/falcon-180B">base</a> and <a href="https://huggingface.co/tiiuae/falcon-180B-chat">chat</a> model) and interact with the model on the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-chat">Falcon Chat Demo Space</a>.</p>
<p>In terms of capabilities, Falcon 180B achieves state-of-the-art results across natural language tasks. It tops the leaderboard for (pre-trained) open-access models and rivals proprietary models like PaLM-2. While difficult to rank definitively yet, it is considered on par with PaLM-2 Large, making Falcon 180B one of the most capable LLMs publicly known.</p>
<p>In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results and show how you can use the model.</p>
<ul>
<li><a href="#what-is-falcon-180b">What is Falcon-180B?</a></li>
<li><a href="#how-good-is-falcon-180b">How good is Falcon 180B?</a></li>
<li><a href="#how-to-use-falcon-180b">How to use Falcon 180B?</a><ul>
<li><a href="#demo">Demo</a></li>
<li><a href="#hardware-requirements">Hardware requirements</a></li>
<li><a href="#prompt-format">Prompt format</a></li>
<li><a href="#transformers">Transformers</a></li>
</ul>
</li>
<li><a href="#additional-resources">Additional Resources</a></li>
</ul>
<h2>
	<a id="what-is-falcon-180b" href="#what-is-falcon-180b">
		
	</a>
	<span>
		What is Falcon-180B?
	</span>
</h2>
<p>Falcon 180B is a model released by <a href="https://falconllm.tii.ae/">TII</a> that follows previous releases in the Falcon family.</p>
<p>Architecture-wise, Falcon 180B is a scaled-up version of <a href="https://huggingface.co/tiiuae/falcon-40b">Falcon 40B</a> and builds on its innovations such as multiquery attention for improved scalability. We recommend reviewing the <a href="https://huggingface.co/blog/falcon">initial blog post</a> introducing Falcon to dive into the architecture. Falcon 180B was trained on 3.5 trillion tokens on up to 4096 GPUs simultaneously, using Amazon SageMaker for a total of ~7,000,000 GPU hours. This means Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute. </p>
<p>The dataset for Falcon 180B consists predominantly of web data from <a href="https://arxiv.org/abs/2306.01116">RefinedWeb</a> (~85%). In addition, it has been trained on a mix of curated data such as conversations, technical papers, and a small fraction of code (~3%). This pretraining dataset is big enough that even 3.5 trillion tokens constitute less than an epoch.</p>
<p>The released <a href="https://huggingface.co/tiiuae/falcon-180B-chat">chat model</a> is fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.</p>
<p>‚ÄºÔ∏è Commercial use: 
Falcon 180b can be commercially used but under very restrictive conditions, excluding any &#34;hosting use&#34;. We recommend to check the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt">license</a> and consult your legal team if you are interested in using it for commercial purposes.</p>
<h2>
	<a id="how-good-is-falcon-180b" href="#how-good-is-falcon-180b">
		
	</a>
	<span>
		How good is Falcon 180B?
	</span>
</h2>
<p>Falcon 180B is the best openly released LLM today, outperforming Llama 2 70B and OpenAI‚Äôs GPT-3.5 on MMLU, and is on par with Google&#39;s PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it&#39;s openly released.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/palm2_480.jpg" rel="noopener nofollow"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/palm2_480.jpg" alt="Palm 2 comparison"/></a></p>
<p>With 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-trained LLM, surpassing Meta‚Äôs LLaMA 2 (67.35).</p>
<div>
	<table>
		<thead><tr>
<th>Model</th>
<th>Size</th>
<th>Leaderboard score</th>
<th>Commercial use or license</th>
<th>Pretraining length</th>
</tr>

		</thead><tbody><tr>
<td>Falcon</td>
<td>180B</td>
<td>68.74</td>
<td>üü†</td>
<td>3,500B</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>67.35</td>
<td>üü†</td>
<td>2,000B</td>
</tr>
<tr>
<td>LLaMA</td>
<td>65B</td>
<td>64.23</td>
<td>üî¥</td>
<td>1,400B</td>
</tr>
<tr>
<td>Falcon</td>
<td>40B</td>
<td>61.48</td>
<td>üü¢</td>
<td>1,000B</td>
</tr>
<tr>
<td>MPT</td>
<td>30B</td>
<td>56.15</td>
<td>üü¢</td>
<td>1,000B</td>
</tr>
</tbody>
	</table>
</div>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/open_llm_leaderboard.jpg" rel="noopener nofollow"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/open_llm_leaderboard.jpg" alt="open_llm_leaderboard.png"/></a></p>
<p>The quantized Falcon models preserve similar metrics across benchmarks. The results were similar when evaluating <code>torch.float16</code>, <code>8bit</code>, and <code>4bit</code>. See results in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>.</p>
<h2>
	<a id="how-to-use-falcon-180b" href="#how-to-use-falcon-180b">
		
	</a>
	<span>
		How to use Falcon 180B?
	</span>
</h2>
<p>Falcon 180B is available in the Hugging Face ecosystem, starting with Transformers version 4.33.</p>
<h3>
	<a id="demo" href="#demo">
		
	</a>
	<span>
		Demo
	</span>
</h3>
<p>You can easily try the Big Falcon Model (180 billion parameters!) in <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-demo">this Space</a> or in the playground embedded below:</p>


<h3>
	<a id="hardware-requirements" href="#hardware-requirements">
		
	</a>
	<span>
		Hardware requirements
	</span>
</h3>
<p>We ran several tests on the hardware needed to run the model for different use cases. Those are not the minimum numbers, but the minimum numbers for the configurations we had access to.</p>
<div>
	<table>
		<thead><tr>
<th></th>
<th>Type</th>
<th>Kind</th>
<th>Memory</th>
<th>Example</th>
</tr>

		</thead><tbody><tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>Full fine-tuning</td>
<td>5120GB</td>
<td>8x 8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>LoRA with ZeRO-3</td>
<td>1280GB</td>
<td>2x 8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>QLoRA</td>
<td>160GB</td>
<td>2x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Inference</td>
<td>BF16/FP16</td>
<td>640GB</td>
<td>8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Inference</td>
<td>GPTQ/int4</td>
<td>320GB</td>
<td>8x A100 40GB</td>
</tr>
</tbody>
	</table>
</div>
<h3>
	<a id="prompt-format" href="#prompt-format">
		
	</a>
	<span>
		Prompt format
	</span>
</h3>
<p>The base model has no prompt format. Remember that it‚Äôs not a conversational model or trained with instructions, so don‚Äôt expect it to generate conversational responses‚Äîthe pretrained model is a great platform for further finetuning, but you probably shouldn‚Äôt driectly use it out of the box. The Chat model has a very simple conversation structure.</p>
<pre><code>System: Add an optional system prompt here
User: This is the user input
Falcon: This is what the model generates
User: This might be a second turn input
Falcon: and so on
</code></pre>
<h3>
	<a id="transformers" href="#transformers">
		
	</a>
	<span>
		Transformers
	</span>
</h3>
<p>With the release of Transformers 4.33, you can use Falcon 180B and leverage all the tools in the HF ecosystem, such as:</p>
<ul>
<li>training and inference scripts and examples</li>
<li>safe file format (safetensors)</li>
<li>integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning) and GPTQ</li>
<li>assisted generation (also known as ‚Äúspeculative decoding‚Äù)</li>
<li>RoPE scaling support for larger context lengths</li>
<li>rich and powerful generation parameters</li>
</ul>
<p>Use of the model requires you to accept its license and terms of use. Please, make sure you are logged into your Hugging Face account and ensure you have the latest version of <code>transformers</code>:</p>
<pre><code>pip install --upgrade transformers
huggingface-cli login
</code></pre>
<h4>
	<a id="bfloat16" href="#bfloat16">
		
	</a>
	<span>
		bfloat16
	</span>
</h4>
<p>This is how you‚Äôd use the base model in <code>bfloat16</code>. Falcon 180B is a big model, so please take into account the hardware requirements summarized in the table above.</p>
<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM
<span>import</span> transformers
<span>import</span> torch

model_id = <span>&#34;tiiuae/falcon-180B&#34;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map=<span>&#34;auto&#34;</span>,
)

prompt = <span>&#34;My name is Pedro, I live in&#34;</span>
inputs = tokenizer(prompt, return_tensors=<span>&#34;pt&#34;</span>).to(<span>&#34;cuda&#34;</span>)

output = model.generate(
    input_ids=inputs[<span>&#34;input_ids&#34;</span>],
    attention_mask=inputs[<span>&#34;attention_mask&#34;</span>],
    do_sample=<span>True</span>,
    temperature=<span>0.6</span>,
    top_p=<span>0.9</span>,
    max_new_tokens=<span>50</span>,
)
output = output[<span>0</span>].to(<span>&#34;cpu&#34;</span>)
<span>print</span>(tokenizer.decode(output)
</code></pre>
<p>This could produce an output such as:</p>
<pre><code>My name is Pedro, I live in Portugal and I am 25 years old. I am a graphic designer, but I am also passionate about photography and video.
I love to travel and I am always looking for new adventures. I love to meet new people and explore new places.
</code></pre>
<h4>
	<a id="8-bit-and-4-bit-with-bitsandbytes" href="#8-bit-and-4-bit-with-bitsandbytes">
		
	</a>
	<span>
		8-bit and 4-bit with <code>bitsandbytes</code>
	</span>
</h4>
<p>The 8-bit and 4-bit quantized versions of Falcon 180B show almost no difference in evaluation with respect to the <code>bfloat16</code> reference! This is very good news for inference, as you can confidently use a quantized version to reduce hardware requirements. Keep in mind, though, that 8-bit inference is <em>much faster</em> than running the model in <code>4-bit</code>.</p>
<p>To use quantization, you need to install the <code>bitsandbytes</code> library and simply enable the corresponding flag when loading the model:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    load_in_8bit=<span>True</span>,
    device_map=<span>&#34;auto&#34;</span>,
)
</code></pre>
<h4>
	<a id="chat-model" href="#chat-model">
		
	</a>
	<span>
		Chat Model
	</span>
</h4>
<p>As mentioned above, the version of the model fine-tuned to follow conversations used a very straightforward training template. We have to follow the same pattern in order to run chat-style inference. For reference, you can take a look at the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-demo/blob/main/app.py#L28">format_prompt</a> function in the Chat demo, which looks like this:</p>
<pre><code><span>def</span> <span>format_prompt</span>(<span>message, history, system_prompt</span>):
    prompt = <span>&#34;&#34;</span>
    <span>if</span> system_prompt:
        prompt += <span>f&#34;System: <span>{system_prompt}</span>\n&#34;</span>
    <span>for</span> user_prompt, bot_response <span>in</span> history:
        prompt += <span>f&#34;User: <span>{user_prompt}</span>\n&#34;</span>
        prompt += <span>f&#34;Falcon: <span>{bot_response}</span>\n&#34;</span>
        prompt += <span>f&#34;User: <span>{message}</span>\nFalcon:&#34;</span>
    <span>return</span> prompt
</code></pre>
<p>As you can see, interactions from the user and responses by the model are preceded by <code>User: </code> and <code>Falcon: </code> separators. We concatenate them together to form a prompt containing the conversation&#39;s whole history. We can provide a system prompt to tweak the generation style.</p>
<h2>
	<a id="additional-resources" href="#additional-resources">
		
	</a>
	<span>
		Additional Resources
	</span>
</h2>
<ul>
<li><a href="https://huggingface.co/models?other=falcon&amp;sort=trending&amp;search=180">Models</a></li>
<li><a href="https://huggingface.co/spaces/tiiuae/falcon-180b-chat">Demo</a></li>
<li><a href="https://huggingface.co/blog/falcon">The Falcon has landed in the Hugging Face ecosystem</a></li>
<li><a href="https://falconllm.tii.ae/">Official Announcement</a></li>
</ul>
<h2>
	<a id="acknowledgments" href="#acknowledgments">
		
	</a>
	<span>
		Acknowledgments
	</span>
</h2>
<p>Releasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including <a href="https://huggingface.co/clefourrier">Cl√©mentine</a> and <a href="https://github.com/EleutherAI/lm-evaluation-harness">Eleuther Evaluation Harness</a> for LLM evaluations; <a href="https://huggingface.co/loubnabnl">Loubna</a> and <a href="https://huggingface.co/bigcode">BigCode</a> for code evaluations; <a href="https://hf.co/narsil">Nicolas</a> for Inference support; <a href="https://huggingface.co/lysandre">Lysandre</a>, <a href="https://huggingface.co/Rocketknight1">Matt</a>, <a href="https://huggingface.co/DanielHesslow">Daniel</a>, <a href="https://huggingface.co/amyeroberts">Amy</a>, <a href="https://huggingface.co/joaogante">Joao</a>, and <a href="https://huggingface.co/ArthurZ">Arthur</a> for integrating Falcon into transformers. Thanks to <a href="https://huggingface.co/BapBap">Baptiste</a> and <a href="https://huggingface.co/patrickvonplaten">Patrick</a> for the open-source demo. Thanks to <a href="https://huggingface.co/thomwolf">Thom</a>, <a href="https://huggingface.co/lewtun">Lewis</a>, <a href="https://huggingface.co/thebloke">TheBloke</a>, <a href="https://huggingface.co/nouamanetazi">Nouamane</a>, <a href="https://huggingface.co/timdettmers">Tim Dettmers</a> for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.</p>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
