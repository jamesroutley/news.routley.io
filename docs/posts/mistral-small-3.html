<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mistral-small-3/">Original</a>
    <h1>Mistral Small 3</h1>
    
    <div id="readability-page-1" class="page"><div><p>Today we’re introducing Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/up-and-to-the-left.png" alt="Detailed benchmarks" width="77%"/></p><p>Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.</p><p>Mistral Small 3 is a pre-trained and instructed model catered to the ‘80%’ of generative AI tasks—those that require robust language and instruction following performance, with very low latency.</p><p>We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.</p><p>We’re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.</p><h3 id="performance">Performance</h3><h4 id="human-evaluations">Human Evaluations</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-small-3-human-evals.png" alt="Detailed benchmarks" width="77%"/></p><p>We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.</p><h4 id="instruct-performance">Instruct performance</h4><p>Our instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-knowledge.png" alt="Detailed benchmarks" width="77%"/></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-code-math.png" alt="Detailed benchmarks" width="77%"/></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-IF.png" alt="Detailed benchmarks" width="77%"/></p><p>Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (<a href="https://qwenlm.github.io/blog/qwen2.5-llm/">Qwen2.5-32B-Instruct</a>, <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">Llama-3.3-70B-Instruct</a>, <a href="https://huggingface.co/google/gemma-2-27b-it">Gemma-2-27B-IT</a>). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.</p><h4 id="pretraining-performance">Pretraining performance</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-benchmarks.png" alt="Detailed benchmarks" width="77%"/></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-mmlu-int.png" alt="Detailed benchmarks" width="77%"/></p><p>Mistral Small 3, a 24B model, offers the best performance for its size class and rivals with models three times larger such as Llama 3.3 70B.</p><h3 id="when-to-use-mistral-small-3">When to use Mistral Small 3</h3><p>Across our customers and community, we are seeing several distinct use cases emerge for pre-trained models of this size:</p><ul><li>Fast-response conversational assistance: Mistral Small 3 excels in scenarios where quick, accurate responses are critical. This includes virtual assistants in many scenarios where users expect immediate feedback and near real-time interactions.</li><li>Low-latency function calling: Mistral Small 3 is able to handle rapid function execution when used as part of automated or agentic workflows.</li><li>Fine-tuning to create subject matter experts: Mistral Small 3 can be fine-tuned to specialize in specific domains, creating highly accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support, where domain-specific knowledge is essential.</li><li>Local inference: Particularly beneficial for hobbyists and organizations handling sensitive or proprietary information. When quantized, Mistral Small 3 can be run privately on a single RTX 4090 or a Macbook with 32GB RAM.</li></ul><p>Our customers are evaluating Mistral Small 3 across multiple industries, including:</p><ul><li>Financial services customers for fraud detection</li><li>Healthcare providers for customer triaging</li><li>Robotics, automotive, and manufacturing companies for on-device command and control</li><li>Horizontal use cases across customers include virtual customer service, and sentiment and feedback analysis.</li></ul><h3 id="using-mistral-small-3-on-your-preferred-tech-stack">Using Mistral Small 3 on your preferred tech stack</h3><p>Mistral Small 3 is now available on la Plateforme as <code>mistral-small-latest</code> or <code>mistral-small-2501</code>. Explore our <a href="https://docs.mistral.ai">docs</a> to learn how to use our models for text generation.</p><p>We are also excited to collaborate with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI to make the model available on their platforms starting today:</p><ul><li><a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501">Hugging Face</a> (<a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501">base model</a>)</li><li><a href="https://ollama.com/library/mistral-small">Ollama</a></li><li><a href="https://www.kaggle.com/models/mistral-ai/mistral-small-24b">Kaggle</a></li><li><a href="https://www.together.ai/models/mistral-small-3">Together AI</a></li><li><a href="https://fireworks.ai/models/fireworks/mistral-small-24b-instruct-2501">Fireworks AI</a></li><li>Coming soon on NVIDIA NIM, Amazon SageMaker, Groq, Databricks and Snowflake</li></ul><h3 id="the-road-ahead">The road ahead</h3><p>It’s been exciting days for the open-source community! Mistral Small 3 complements large open-source reasoning models like the recent releases of DeepSeek, and can serve as a strong base model for making reasoning capabilities emerge.</p><p>Among many other things, expect small and large Mistral models with boosted reasoning capabilities in the coming weeks. Join the journey if you’re keen (we’re hiring), or beat us to it by hacking Mistral Small 3 today and making it better!</p><h3 id="open-source-models-at-mistral">Open-source models at Mistral</h3><p><strong>We’re renewing our commitment to using Apache 2.0 license for our general purpose models, as we progressively move away from MRL-licensed models</strong>. As with Mistral Small 3, model weights will be available to download and deploy locally, and free to modify and use in any capacity. These models will also be made available through a serverless API on <a href="https://console.mistral.ai/">la Plateforme</a>, through our on-prem and VPC deployments, customisation and orchestration platform, and through our inference and cloud partners. Enterprises and developers that need specialized capabilities (increased speed and context, domain specific knowledge, task-specific models like code completion) can count on additional commercial models complementing what we contribute to the community.</p></div></div>
  </body>
</html>
