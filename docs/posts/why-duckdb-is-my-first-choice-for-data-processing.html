<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.robinlinacre.com/recommend_duckdb/">Original</a>
    <h1>Why DuckDB is my first choice for data processing</h1>
    
    <div id="readability-page-1" class="page"><div id="mdx-container-div">
<p>Over the past few years, I&#39;ve found myself using DuckDB more and more for data processing, to the point where I now use it almost exclusively, usually from within Python.</p>
<p>We&#39;re moving towards a simpler world where most tabular data can be processed on a single large machine<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> and the era of clusters is coming to an end for all but the largest datasets.<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup></p>
<p>This post sets out some of my favourite features of DuckDB that set it apart from other SQL-based tools.    In a nutshell, it&#39;s simple to install, ergonomic, fast, and more fully featured.</p>
<p>An <a href="https://www.robinlinacre.com/recommend_sql/">earlier post</a> explains why I favour SQL over other APIs such as <a href="https://pola.rs/">Polars</a>, <a href="https://pandas.pydata.org/">pandas</a> or <a href="https://dplyr.tidyverse.org/">dplyr</a>.</p>

<p>DuckDB is an open source in-process SQL engine that is optimised for analytics queries.</p>
<ul>
<li>&#39;In-process&#39; means it&#39;s similar to SQLite in that it runs within your application. You don&#39;t need to start a separate service such as Postgres to run it.</li>
<li>&#39;Optimised for analytics queries&#39; means that it&#39;s designed for operations like joins and aggregations involving large numbers of rows, as opposed to atomic transactions.</li>
</ul>
<p>The performance difference of analytics-optimised engines (<a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a>) vs. transactions-optimised engines (<a href="https://en.wikipedia.org/wiki/Online_transaction_processing">OLTP</a>) should not be underestimated. A query running in DuckDB can be 100 or even 1,000 times faster than exactly the same query running in (say) SQLite or Postgres.</p>
<p>A core use-case of DuckDB is where you have one or more large datasets on disk in formats like <code>csv</code>, <code>parquet</code> or <code>json</code> which you want to batch process.  You may want to perform cleaning, joins, aggregation, derivation of new columns - that sort of thing.</p>
<p>But you can also use DuckDB for many other simpler tasks like viewing a csv file from the command line.</p>


<p>DuckDB consistently benchmarks as one of the fastest data processing engines.  The benchmarks I&#39;ve seen<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> show there&#39;s not much in it between the leading open source engines - which at the moment seem to be <a href="https://pola.rs/">polars</a>, <a href="https://duckdb.org/">DuckDB</a>, <a href="https://datafusion.apache.org/">DataFusion</a>,  <a href="https://spark.apache.org/">Spark</a> and <a href="https://www.dask.org/">Dask</a>.  Spark and Dask can be competitive on large data, but slower on small data.</p>

<p>DuckDB itself is a single precompiled binary.  In Python, it can be <code>pip install</code>ed with no dependencies.  This makes it a joy to install compared to other more heavyweight options like Spark.  Combined with <code>uv</code>, you can stand up a fresh DuckDB Python environment from nothing in less than a second - see <a href="https://akrabat.com/using-uv-as-your-shebang-line/">here</a>.</p>

<p>With its speed and almost-zero startup time, DuckDB is ideally suited for CI and testing of data engineering pipelines.</p>
<p>Historically this has been fiddly and running a large suite of tests in e.g. Apache Spark has been time consuming and frustrating.  Now it&#39;s much simpler to set up the test environment, and there&#39;s less scope for differences between it and your production pipelines.</p>

<p>This simplicity and speed also applies to writing new SQL, and getting syntax right before running it on a large dataset.  Historically I have found this annoying in engines like Spark (where it takes a few seconds to start Spark in local mode), or even worse when you&#39;re forced to run queries in a proprietary tool like AWS Athena.<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup></p>
<p>There&#39;s even a DuckDB UI with autocomplete - see <a href="https://duckdb.org/docs/stable/extensions/ui.html">here</a>.</p>

<p>The DuckDB team has implemented a wide range of innovations in its SQL dialect that make it a joy to use. See the following blog posts <a href="https://duckdb.org/2022/05/04/friendlier-sql.html">1</a> <a href="https://duckdb.org/2023/08/23/even-friendlier-sql.html">2</a> <a href="https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html">3</a> <a href="https://duckdb.org/2024/08/19/duckdb-tricks-part-1.html">4</a> <a href="https://duckdb.org/2024/10/11/duckdb-tricks-part-2.html">5</a> <a href="https://duckdb.org/2024/11/29/duckdb-tricks-part-3.html">6</a>.</p>
<p>Some of my favourites are the <a href="https://duckdb.org/docs/stable/sql/expressions/star.html#exclude-clause"><code>EXCLUDE</code></a> keyword, and the <a href="https://duckdb.org/docs/stable/sql/query_syntax/select.html"><code>COLUMNS</code></a> keyword which allows you to select and regex-replace a subset of columns.<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>  I also like <code>QUALIFY</code> and the aggregate modifiers on window functions, see <a href="https://duckdb.org/2025/02/10/window-catchup.html#qualify-clause">here</a>.</p>
<p>Another is the ability to <a href="https://duckdb.org/2023/08/23/even-friendlier-sql.html#function-chaining">function chain</a>, like <code>first_name.lower().trim()</code>.</p>

<p>You can query data directly from files, including on s3, or on the web.</p>
<p>For example to query a folder of parquet files:</p>
<div><div><pre><p><span>select</span><span> </span><span>*</span><span></span></p><p><span></span><span>from</span><span> read_parquet</span><span>(</span><span>&#39;path/to/*.parquet&#39;</span><span>)</span></p></pre></div></div>
<p>or even (on CORS enabled files) you can run SQL directly:</p>
<div><div><pre><p><span>select</span><span> </span><span>*</span><span></span></p><p><span></span><span>from</span><span> read_parquet</span><span>(</span><span>&#39;https://raw.githubusercontent.com/plotly/datasets/master/2015_flights.parquet&#39;</span><span>)</span><span></span></p><p><span></span><span>limit</span><span> </span><span>2</span><span>;</span></p></pre></div></div>
<p>Click <a href="https://shell.duckdb.org/#queries=v0,select-*-from-read_parquet(&#39;https%3A%2F%2Fraw.githubusercontent.com%2Fplotly%2Fdatasets%2Fmaster%2F2015_flights.parquet&#39;)-limit-2~">here</a> to try this query yourself in the DuckDB web shell.</p>
<p>One of the easiest ways to cause problems in your data pipelines is to fail to be strict about incoming data types from untyped formats such as csv.  DuckDB provides lots of options here - see <a href="https://duckdb.org/docs/stable/data/csv/overview.html">here</a>.</p>

<p>Many data pipelines effectively boil down to a long sequence of CTEs:</p>
<div><div><pre><p><span>WITH</span><span></span></p><p><span>input_data </span><span>AS</span><span> </span><span>(</span><span></span></p><p><span>    </span><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM</span><span> read_parquet</span><span>(</span><span>&#39;...&#39;</span><span>)</span><span></span></p><p><span></span><span>)</span><span>,</span><span></span></p><p><span>step_1 </span><span>AS</span><span> </span><span>(</span><span></span></p><p><span>    </span><span>SELECT</span><span> </span><span>.</span><span>.</span><span>.</span><span> </span><span>FROM</span><span> input_data </span><span>JOIN</span><span> </span><span>.</span><span>.</span><span>.</span><span></span></p><p><span></span><span>)</span><span>,</span><span></span></p><p><span>step_2 </span><span>AS</span><span> </span><span>(</span><span></span></p><p><span>    </span><span>SELECT</span><span> </span><span>.</span><span>.</span><span>.</span><span> </span><span>FROM</span><span> step_1</span></p><p><span></span><span>)</span><span></span></p><p><span></span><span>SELECT</span><span> </span><span>.</span><span>.</span><span>.</span><span> </span><span>FROM</span><span> step_2</span><span>;</span></p></pre></div></div>
<p>When developing a pipeline like this, we often want to inspect what&#39;s happened at each step.</p>
<p>In Python, we can write</p>
<div><div><pre><p><span>input_data = duckdb.sql(&#34;SELECT * FROM read_parquet(&#39;...&#39;)&#34;)</span></p><p><span>step_1 = duckdb.sql(&#34;SELECT ... FROM input_data JOIN ...&#34;)</span></p><p><span>step_2 = duckdb.sql(&#34;SELECT ... FROM step_1&#34;)</span></p><p><span>final = duckdb.sql(&#34;SELECT ... FROM step_2;&#34;)</span></p></pre></div></div>
<p>This makes it easy to inspect what the data looks like at <code>step_2</code> with no performance loss, since these steps will be executed lazily when they&#39;re run all at once.</p>
<p>This also facilitates easier testing of SQL in CI, since each step can be an independently-tested function.</p>

<p>DuckDB offers full ACID compliance for bulk data operations, which sets it apart from other analytical data systems - see <a href="https://duckdb.org/2024/09/25/changing-data-with-confidence-and-acid.html">here</a>.  You can listen to more about this on <a href="https://duckdb.org/media/duckdb-deep-dive-lakehouse-challenges/">in this podcast</a>, transcribed <a href="https://gist.github.com/RobinL/bbabbe22d9177230648b7fc9a22a84d7">here</a>.</p>
<p>This is a very interesting new development, making DuckDB potentially a suitable replacement for lakehouse formats such as Iceberg or Delta lake for medium scale data.</p>

<p>A longstanding difficulty with data processing engines has been the difficulty in writing high performance user defined functions (UDFs).</p>
<p>For example, in PySpark, you will generally get best performance by writing custom Scala, compiling to a JAR, and registering it with Spark.  But this is cumbersome and in practice, you will encounter a lot of issues around Spark version compatibility and security restrictions environments such as DataBricks.</p>
<p>In DuckDB high performance custom UDFs can be written in C++.  Whilst writing these functions is certainly not trivial, DuckDB <a href="https://duckdb.org/2024/07/05/community-extensions.html">community extensions</a> offers a low-friction way of distributing the code. Community extensions can be installed almost instantly with a single command such as <code>INSTALL h3 FROM community</code> to install hierarchical hexagonal indexing for geospatial data.</p>

<p>The team provide documentation as a <a href="https://duckdb.org/duckdb-docs.md">single markdown file</a> so it can easily be provided to an LLM.</p>
<p>My top tip: if you load this file in your code editor, and use code folding, it&#39;s easy to copy the parts of the documentation you need into context.</p>

<p>Much of this blog post is based on my experience supporting multiple SQL dialects in <a href="https://github.com/moj-analytical-services/splink">Splink</a>, an open source library for record linkage at scale.  We&#39;ve found that transitioning towards recommending DuckDB as the default backend choice has increased adoption of the library and significantly reduced the amount of problems faced by users, even for large linkage tasks, whilst speeding up workloads very substantially.</p>
<p>We&#39;ve also found it&#39;s hugely increased the simplicity and speed of developing and testing new features.</p>

<ul>
<li>The <a href="https://github.com/duckdb/duckdb-postgres">PosgreSQL Extension</a> allows you to attach a Postgres database and query it directly from DuckDB.</li>
<li><a href="https://github.com/duckdb/pg_duckdb"><code>pg_duckdb</code></a> allows you to embed the DuckDB computation engine within Postgres.</li>
</ul>
<p>The later in particular seems potentially extremely powerful, enabling Postgres to be simultanouesly optimised for analytics and transactional processing.  I think it&#39;s likely to see widespread adoption, especially after they iron out a few of the current shortcomings around enabling and optimising the use of Postgres indexes and pushing up filters up to PostGres.</p>
</div></div>
  </body>
</html>
