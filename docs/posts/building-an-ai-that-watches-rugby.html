<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nickjones.tech/ai-watching-rugby/">Original</a>
    <h1>Building an AI That Watches Rugby</h1>
    
    <div id="readability-page-1" class="page"><div>
            
                
            

            

            
                
            

            <p>Thereâ€™s a gap in rugby data.</p>

<p>Weâ€™ve got the big moments covered â€” the tries, conversions, and cards. Structured event feeds do a good job of telling you <em>what</em> happened.</p>

<p>But theyâ€™re not so good at telling you <em>why</em>.</p>

<p>At Gainline, we build our entire app around context. We want to give rugby fans a second-screen experience that feels alive â€” a commentary that goes deeper than the scoreline. We already pull in weather data, team stats, and player profiles. We enrich it with AI-generated summaries.</p>

<p>But weâ€™re limited by the data we get.</p>

<p>We donâ€™t know why the ref blew the whistle. We canâ€™t tell if a prop is quietly dominating the scrum. We miss what the ref said to the captain. And thatâ€™s a problem â€” because these moments matter when youâ€™re trying to tell the full story of a match.</p>

<p>So we asked ourselves a simple question:</p>

<blockquote>
  <p><strong>What if we could watch the game and generate the data ourselves?</strong></p>
</blockquote>

<p>That led me down a really fun rabbit hole.</p>

<p>In this post, Iâ€™ll show you how I built a prototype system that watches a rugby game using AI. Weâ€™ll look at how we extracted the score and game clock from the broadcasterâ€™s UI, how we used Whisper to transcribe referee and commentary audio, and what we learned about running these kinds of experiments cheaply and effectively.</p>

<p>Itâ€™s scrappy â€” but it works.</p>

<h2 id="context-is-everything">Context is Everything</h2>

<p><a href="https://gainline.app">Gainline</a> is our <a href="https://gainline.app">rugby app</a>.</p>

<p>Itâ€™s a clean, well-designed experience that gives fans what they need. We pull together data from a range of providers â€” live scores, player stats, team histories â€” and try to tell a richer story about whatâ€™s happening on the pitch.</p>

<p>Most of it works well. If you want to know who scored the last try, who the fly-half is, or whoâ€™s made the most carries, weâ€™ve got you covered.</p>

<p>But rugby is messy.</p>

<p>A lot happens between structured events. Penalties go unexplained. Players work relentlessly in ways that never show up in the stats. Props spend 80 minutes blowing their lungs out â€” maybe earning a mention if they score.</p>

<p>And we canâ€™t see any of it.</p>

<p>Thatâ€™s frustrating â€” because it limits our AI-generated summaries. If all we know is that a penalty occurred, we canâ€™t say <em>why</em>. We canâ€™t spot a breakdown nightmare or a dominant scrum.</p>

<p>The best rugby minds donâ€™t just watch the ball â€” they read the whole game.</p>

<p>Thatâ€™s what we want Gainline to do.</p>

<h2 id="the-idea">The Idea</h2>

<blockquote>
  <p>What if we could watch the game ourselves?</p>
</blockquote>

<p>Not literally. We canâ€™t hire analysts to watch every match and enter data manually.</p>

<p>But AI? That just might work ğŸ˜</p>

<p>The plan was simple.</p>

<p>Take a video of a rugby game. Slice it into screenshots â€” one every five seconds. Feed those frames into OpenAIâ€™s vision model and ask it whatâ€™s going on.</p>

<h2 id="can-we-read-the-score">Can We Read the Score?</h2>

<p>We started with a lazy approach: What can we detect easily? Letâ€™s begin with the basics.</p>

<p>Whatâ€™s the score? What does the game clock say?</p>

<p>But I was also curious â€” what can the model <em>really</em> see?</p>

<p><img src="https://nickjones.tech/assets/images/2025-04/rugby-frame.png" alt="A frame from a game of rugby"/></p>

<p>Hereâ€™s the prompt I used (built through a separate refinement process â€” another post for another day!):</p>

<blockquote>
  <p>You are an AI that analyzes screenshots of rugby matches. Your task is to visually interpret the image and extract structured game information â€” including the score, time, team names, and match phase (e.g., first half, second half, full time). Return the data in a clear, structured format suitable for programmatic use (e.g., JSON). Focus on identifying all elements that summarize the current state of the match.</p>
</blockquote>

<p>The result:</p>

<div><div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td><pre>{
  &#34;home_team&#34;: &#34;Bath Rugby&#34;,
  &#34;away_team&#34;: &#34;Harlequins&#34;,
  &#34;home_team_abbreviation&#34;: &#34;BTH&#34;,
  &#34;away_team_abbreviation&#34;: &#34;HAR&#34;,
  &#34;score&#34;: {
    &#34;BTH&#34;: 0,
    &#34;HAR&#34;: 0
  },
  &#34;time_elapsed&#34;: &#34;00:36&#34;,
  &#34;match_phase&#34;: &#34;first_half&#34;,
  &#34;competition&#34;: &#34;Gallagher Premiership&#34;,
  &#34;current_play&#34;: &#34;ruck&#34;,
  &#34;bath_team_kit&#34;: &#34;dark blue with light blue accents&#34;,
  &#34;harlequins_team_kit&#34;: &#34;white with green shorts and multicolor accents&#34;
}
</pre></td></tr></tbody></table></code></pre></div></div>

<p>It worked. Remarkably well.</p>

<p>But vision models price their API calls based on context size â€” the number of tokens an image turns into. Sending a full-resolution screenshot every five seconds gets expensive fast.</p>

<p>So the next challenge became: how do we do this cheaper?</p>

<h2 id="reducing-context-size">Reducing Context Size</h2>

<p>Letâ€™s zoom in on the essentials. What if we only want the score and elapsed time?</p>

<p>If we crop the image down to <em>just</em> the scoreboard, we can dramatically reduce the size â€” and cost.</p>

<p>I first asked the model to return the pixel coordinates of the scoreboard.</p>

<p>It didnâ€™t work.</p>

<p>I couldnâ€™t get a reliable bounding box.</p>

<p><img src="https://nickjones.tech/assets/images/2025-04/rugby-frame-percentage.png"/></p>

<p>Iâ€™m not exactly sure why. I tried several approaches. I thought maybe the image was being resized internally, so I switched to asking for percentages instead of pixel values â€” but the results were still off.</p>

<p>Then I realised: I didnâ€™t need a bounding box.</p>

<p>The scoreboard always appears in one of the corners. Cropping to that corner gave me a 75% reduction in image size.</p>

<p>I updated the prompt. It worked perfectly. Cheap, reliable, and didnâ€™t require complex image processing.</p>

<h3 id="isnt-it-just-a-diff">Isnâ€™t It Just a Diff?</h3>

<p>Do we really need a language model to find the scoreboard?</p>

<p>Broadcasts usually place the scoreboard in a consistent location â€” often the top-left or top-right. Could we just <em>diff</em> two frames â€” one with the scoreboard, one without â€” to detect the UI?</p>

<p>In theory, yes.</p>

<p>The static background would cancel out, leaving only the overlay.</p>

<p><img src="https://nickjones.tech/assets/images/2025-04/rugby-frane-diff.png" alt="The result of a diff between frames"/></p>

<p>Hereâ€™s the command:</p>

<div><div><pre><code><table><tbody><tr><td><pre>1
2
</pre></td><td><pre>magick compare -highlight-color Red -lowlight-color Black \
  -compose src frame_000015.png frame_000016.png diff.png
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Itâ€™s rough, but you can see it working. We clearly identify a corner. We can crop it or add padding and target only the pixels that change.</p>

<p>This whole project is about finding the simplest, most reliable way to generate rugby data.</p>

<p>And if that means using <em>less</em> AI â€” even better.</p>

<h3 id="do-we-need-an-llm-at-all">Do We Need an LLM at All?</h3>

<p>We started with large language models because they were the easiest tool to prototype with. I could send an image to OpenAIâ€™s vision model, describe what I wanted, and get useful results.</p>

<p>But I started wondering â€” do we even need an LLM here?</p>

<p>Weâ€™re just trying to extract text from a predictable area â€” the scoreboard.</p>

<p>So I tried <code>tesseract</code>, an open-source OCR tool, to get the score and clock.</p>

<p>It kind of worked. But not well enough.</p>

<p>The problem was quality. Blurry frames, low resolution, and complex overlays made OCR tricky. When it worked, it worked well. But when it failed, it didnâ€™t extract anything useful.</p>

<p>Maybe it would do better with higher-quality streams or some pre-processing â€” but in my test setup, it wasnâ€™t reliable.</p>

<p>So for now â€” the LLM stays.</p>

<h2 id="bonus-listening-to-the-game">Bonus: Listening to the Game</h2>

<p>Once I had the score and clock, I turned to the audio.</p>

<p>Rugby broadcasts are full of context:</p>

<ul>
  <li>The referee mic explains decisions.</li>
  <li>The commentary adds subjective analysis.</li>
  <li>The crowd adds atmosphere.</li>
</ul>

<p>I used OpenAI Whisper to transcribe the audio. It worked brilliantly â€” giving me timestamped commentary I could use to enrich the structured data.</p>

<p>Now I could highlight a propâ€™s incredible shift, or capture events that donâ€™t show up in a stat feed â€” like missed penalties, scuffles, or <a href="https://www.youtube.com/watch?v=MxU-Af4FuDg">Freddie Burns celebrating too early</a>.</p>

<p>I canâ€™t wait to integrate this properly.</p>

<p>Instead of just showing the facts â€” we can start telling the story.</p>

<h2 id="whats-next">Whatâ€™s Next?</h2>

<p>This is a prototype. Itâ€™s not production ready. But it shows whatâ€™s possible.</p>

<p>Scaling this will be an infrastructure challenge:</p>

<ul>
  <li>Should we spin up VMs to watch live streams?</li>
  <li>Do we run distributed workers that pull frames and audio?</li>
  <li>How do we handle different broadcasters, formats, and languages?</li>
</ul>

<p>Then there are the legal and ethical questions.</p>

<p>Weâ€™re not trying to replace broadcasters or journalists. But if AI can watch a game and summarise it in real-time â€” is that just automated journalism?</p>

<p>Itâ€™s a question weâ€™ll have to answer.</p>

<p>This has been one of the most fun experiments Iâ€™ve worked on in a while.</p>

<p>AI is moving beyond structured data and customer support chatbots. These models are growing exponentially more capable. As a developer, itâ€™s my job to stay close to that evolution â€” to know whatâ€™s possible, whatâ€™s not, and where the limits lie.</p>

<p>For rugby â€” and for sport more broadly â€” I think the opportunity is huge.</p>

<p>We can do more with less. Unlock better insights. Tell richer stories. And have way more fun.</p>


            
                
            
        </div></div>
  </body>
</html>
