<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/replicate/cog">Original</a>
    <h1>Cog: Containers for Machine Learning</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Cog is an open-source tool that lets you package machine learning models in a standard, production-ready container.</p>
<p dir="auto">You can deploy your packaged model to your own infrastructure, or to <a href="https://replicate.com/" rel="nofollow">Replicate</a>.</p>
<h2 dir="auto"><a id="user-content-highlights" aria-hidden="true" href="#highlights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Highlights</h2>
<ul dir="auto">
<li>
<p dir="auto"><g-emoji alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png">üì¶</g-emoji> <strong>Docker containers without the pain.</strong> Writing your own <code>Dockerfile</code> can be a bewildering process. With Cog, you define your environment with a <a href="#how-it-works">simple configuration file</a> and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.</p>
</li>
<li>
<p dir="auto"><g-emoji alias="cursing_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f92c.png">ü§¨Ô∏è</g-emoji> <strong>No more CUDA hell.</strong> Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.</p>
</li>
<li>
<p dir="auto"><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji> <strong>Define the inputs and outputs for your model with standard Python.</strong> Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.</p>
</li>
<li>
<p dir="auto"><g-emoji alias="gift" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f381.png">üéÅ</g-emoji> <strong>Automatic HTTP prediction server</strong>: Your model&#39;s types are used to dynamically generate a RESTful HTTP API using <a href="https://fastapi.tiangolo.com/" rel="nofollow">FastAPI</a>.</p>
</li>
<li>
<p dir="auto"><g-emoji alias="pancakes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f95e.png">ü•û</g-emoji> <strong>Automatic queue worker.</strong> Long-running deep learning models or batch processing is best architected with a queue. Cog models do this out of the box. Redis is currently supported, with more in the pipeline.</p>
</li>
<li>
<p dir="auto"><g-emoji alias="cloud" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2601.png">‚òÅÔ∏è</g-emoji> <strong>Cloud storage.</strong> Files can be read and written directly to Amazon S3 and Google Cloud Storage. (Coming soon.)</p>
</li>
<li>
<p dir="auto"><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">üöÄ</g-emoji> <strong>Ready for production.</strong> Deploy your model anywhere that Docker images run. Your own infrastructure, or <a href="https://replicate.com" rel="nofollow">Replicate</a>.</p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How it works</h2>
<p dir="auto">Define the Docker environment your model runs in with <code>cog.yaml</code>:</p>
<div data-snippet-clipboard-copy-content="build:
  gpu: true
  system_packages:
    - &#34;libgl1-mesa-glx&#34;
    - &#34;libglib2.0-0&#34;
  python_version: &#34;3.8&#34;
  python_packages:
    - &#34;torch==1.8.1&#34;
predict: &#34;predict.py:Predictor&#34;"><pre><span>build</span>:
  <span>gpu</span>: <span>true</span>
  <span>system_packages</span>:
    - <span><span>&#34;</span>libgl1-mesa-glx<span>&#34;</span></span>
    - <span><span>&#34;</span>libglib2.0-0<span>&#34;</span></span>
  <span>python_version</span>: <span><span>&#34;</span>3.8<span>&#34;</span></span>
  <span>python_packages</span>:
    - <span><span>&#34;</span>torch==1.8.1<span>&#34;</span></span>
<span>predict</span>: <span><span>&#34;</span>predict.py:Predictor<span>&#34;</span></span></pre></div>
<p dir="auto">Define how predictions are run on your model with <code>predict.py</code>:</p>
<div data-snippet-clipboard-copy-content="from cog import BasePredictor, Input, Path
import torch

class Predictor(BasePredictor):
    def setup(self):
        &#34;&#34;&#34;Load the model into memory to make running multiple predictions efficient&#34;&#34;&#34;
        self.model = torch.load(&#34;./weights.pth&#34;)

    # The arguments and types the model takes as input
    def predict(self,
          input: Path = Input(title=&#34;Grayscale input image&#34;)
    ) -&gt; Path:
        &#34;&#34;&#34;Run a single prediction on the model&#34;&#34;&#34;
        processed_input = preprocess(input)
        output = self.model(processed_input)
        return postprocess(output)"><pre><span>from</span> <span>cog</span> <span>import</span> <span>BasePredictor</span>, <span>Input</span>, <span>Path</span>
<span>import</span> <span>torch</span>

<span>class</span> <span>Predictor</span>(<span>BasePredictor</span>):
    <span>def</span> <span>setup</span>(<span>self</span>):
        <span>&#34;&#34;&#34;Load the model into memory to make running multiple predictions efficient&#34;&#34;&#34;</span>
        <span>self</span>.<span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>&#34;./weights.pth&#34;</span>)

    <span># The arguments and types the model takes as input</span>
    <span>def</span> <span>predict</span>(<span>self</span>,
          <span>input</span>: <span>Path</span> <span>=</span> <span>Input</span>(<span>title</span><span>=</span><span>&#34;Grayscale input image&#34;</span>)
    ) <span>-&gt;</span> <span>Path</span>:
        <span>&#34;&#34;&#34;Run a single prediction on the model&#34;&#34;&#34;</span>
        <span>processed_input</span> <span>=</span> <span>preprocess</span>(<span>input</span>)
        <span>output</span> <span>=</span> <span>self</span>.<span>model</span>(<span>processed_input</span>)
        <span>return</span> <span>postprocess</span>(<span>output</span>)</pre></div>
<p dir="auto">Now, you can run predictions on this model:</p>
<div data-snippet-clipboard-copy-content="$ cog predict -i @input.jpg
--&gt; Building Docker image...
--&gt; Running Prediction...
--&gt; Output written to output.jpg"><pre><code>$ cog predict -i @input.jpg
--&gt; Building Docker image...
--&gt; Running Prediction...
--&gt; Output written to output.jpg
</code></pre></div>
<p dir="auto">Or, build a Docker image for deployment:</p>
<div data-snippet-clipboard-copy-content="$ cog build -t my-colorization-model
--&gt; Building Docker image...
--&gt; Built my-colorization-model:latest

$ docker run -d -p 5000:5000 --gpus all my-colorization-model

$ curl http://localhost:5000/predictions -X POST \
    -H &#39;Content-Type: application/json&#39; \
    -d &#39;{&#34;input&#34;: {&#34;image&#34;: &#34;https://.../input.jpg&#34;}}&#39;"><pre><code>$ cog build -t my-colorization-model
--&gt; Building Docker image...
--&gt; Built my-colorization-model:latest

$ docker run -d -p 5000:5000 --gpus all my-colorization-model

$ curl http://localhost:5000/predictions -X POST \
    -H &#39;Content-Type: application/json&#39; \
    -d &#39;{&#34;input&#34;: {&#34;image&#34;: &#34;https://.../input.jpg&#34;}}&#39;
</code></pre></div>

<p dir="auto">Or, <a href="https://github.com/replicate/cog/blob/main/docs/notebooks.md">spin up a Jupyter notebook</a>:</p>
<div data-snippet-clipboard-copy-content="$ cog run -p 8888 jupyter notebook --allow-root --ip=0.0.0.0"><pre><code>$ cog run -p 8888 jupyter notebook --allow-root --ip=0.0.0.0
</code></pre></div>
<h2 dir="auto"><a id="user-content-why-are-we-building-this" aria-hidden="true" href="#why-are-we-building-this"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why are we building this?</h2>
<p dir="auto">It&#39;s really hard for researchers to ship machine learning models to production.</p>
<p dir="auto">Part of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed.</p>
<p dir="auto"><a href="https://github.com/andreasjansson">Andreas</a> and <a href="https://github.com/bfirsh">Ben</a> created Cog. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created <a href="https://github.com/docker/compose">Docker Compose</a>.</p>
<p dir="auto">We realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models. <a href="https://eng.uber.com/michelangelo-pyml/" rel="nofollow">Uber</a> and others have built similar systems. So, we&#39;re making an open source version so other people can do this too.</p>
<p dir="auto">Hit us up if you&#39;re interested in using it or want to collaborate with us. <a href="https://discord.gg/replicate" rel="nofollow">We&#39;re on Discord</a> or email us at <a href="mailto:team@replicate.com">team@replicate.com</a>.</p>
<h2 dir="auto"><a id="user-content-prerequisites" aria-hidden="true" href="#prerequisites"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Prerequisites</h2>
<ul dir="auto">
<li><strong>macOS or Linux</strong>. Cog works on macOS and Linux, but does not currently support Windows.</li>
<li><strong>Docker</strong>. Cog uses Docker to create a container for your model. You&#39;ll need to <a href="https://docs.docker.com/get-docker/" rel="nofollow">install Docker</a> before you can run Cog.</li>
</ul>
<h2 dir="auto"><a id="user-content-install" aria-hidden="true" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h2>
<p dir="auto">First, <a href="https://docs.docker.com/get-docker/" rel="nofollow">install Docker if you haven&#39;t already</a>. Then, run this in a terminal:</p>
<div data-snippet-clipboard-copy-content="sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`
sudo chmod +x /usr/local/bin/cog"><pre><code>sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`
sudo chmod +x /usr/local/bin/cog
</code></pre></div>
<h2 dir="auto"><a id="user-content-upgrade" aria-hidden="true" href="#upgrade"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Upgrade</h2>
<p dir="auto">If you&#39;re already got Cog installed and want to update to a newer version:</p>
<div data-snippet-clipboard-copy-content="sudo rm $(which cog)
sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`
sudo chmod +x /usr/local/bin/cog"><pre><code>sudo rm $(which cog)
sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`
sudo chmod +x /usr/local/bin/cog
</code></pre></div>
<h2 dir="auto"><a id="user-content-next-steps" aria-hidden="true" href="#next-steps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Next steps</h2>
<ul dir="auto">
<li><a href="https://github.com/replicate/cog/blob/main/docs/getting-started.md">Get started with an example model</a></li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/getting-started-own-model.md">Get started with your own model</a></li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/notebooks.md">Using Cog with notebooks</a></li>
<li><a href="https://github.com/replicate/cog-examples">Take a look at some examples of using Cog</a></li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/deploy.md">Deploy models with Cog</a></li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/yaml.md"><code>cog.yaml</code> reference</a> to learn how to define your model&#39;s environment</li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/python.md">Prediction interface reference</a> to learn how the <code>Predictor</code> interface works</li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/http.md">HTTP API reference</a> to learn how to use the HTTP API that models serve</li>
<li><a href="https://github.com/replicate/cog/blob/main/docs/redis.md">Redis queue API reference</a> to learn how to run models via Redis</li>
</ul>
<h2 dir="auto"><a id="user-content-need-help" aria-hidden="true" href="#need-help"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Need help?</h2>
<p dir="auto"><a href="https://discord.gg/replicate" rel="nofollow">Join us in #cog on Discord.</a></p>
<h2 dir="auto"><a id="user-content-contributors-" aria-hidden="true" href="#contributors-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributors <g-emoji alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">‚ú®</g-emoji></h2>
<p dir="auto">Thanks goes to these wonderful people (<a href="https://allcontributors.org/docs/en/emoji-key" rel="nofollow">emoji key</a>):</p>







<p dir="auto">This project follows the <a href="https://github.com/all-contributors/all-contributors">all-contributors</a> specification. Contributions of any kind welcome!</p>
</article>
          </div></div>
  </body>
</html>
