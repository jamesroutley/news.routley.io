<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.kolaayonrinde.com/blog/2024/01/08/einops.html">Original</a>
    <h1>Descriptive Matrix Operations with Einops</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <!-- <nav class="sidebar"> -->
    <!-- Table of Contents -->
    <!-- <h4>Table of Contents</h4> -->
    <!--  -->
    <!-- </nav> -->
    <!-- <br /> -->
    <!-- <br /> -->
    <h4 id="tldr-use-einopseinsum">tldr; use einops.einsum</h4>

<p>Machine learning is built of matrix algebra. Einstein summation notation (or
<code>einsum</code> for short) makes matrix operations more intuitive and readable.</p>

<p>As you may know, the matrix multiplication that you learned in high school…</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/einops/matmul.png" width="500" alt="3x3 matrix multiplication"/>
    <figcaption>Calculating the 0,0th element of a matrix multiplication </figcaption>
    </figure>
</div>

<p>Can be written algebraically as:</p><p>

\[A_{ik} = \sum_j B_{ij} C_{jk}\]

</p><p>In other words in order to get the (1,2) element of A we calculate:</p><p>

\[A_{1,2} = \sum_j B_{1j} C_{j2}\]

</p><p>i.e. take the dot product of the 1st row of B with the 2nd column of C.</p>



<hr/>



<p>In Einsum notation, to avoid having so many sigmas ( $\sum s$ ) flying around we
adopt the convention that any indices that appear more than once are being
summed over. Hence:</p><p>

\[A_{ik} = \sum_j B_{ij} C_{jk}\]

</p><p>can be written more simply as…</p><p>

\[A_{ik} = B_{ij} C_{jk}\]

</p>

<hr/>



<p>Both torch and numpy have einsum packages to allow you to use einsum notation
for matrix operations. For example, we can write the above matrix multiplication
in torch as:</p>

<div><div><pre><code><span>import</span> <span>torch</span> <span>as</span> <span>t</span>

<span>A</span> <span>=</span> <span>t</span><span>.</span><span>einsum</span><span>(</span><span>&#34;ij,jk-&gt;ik&#34;</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>)</span>
</code></pre></div></div>

<p>The convention is that if a dimension only appears on the left side of the
einsum then it’s summed over. So in the above we’re summing over the j dimension
and keeping the i and k dimensions.</p>

<p>Great!</p>

<p>One issue when using torch.einsum though is that it’s not necessarily super
clear what each letter means:</p>

<ul>
  <li>Was <strong>i</strong> a horizontal index (as in x,y coordinates) or is it a vertical index
(as in tensor indexing?)</li>
  <li>Was <strong>e</strong> embedding dimension or expert number?</li>
  <li>Was <strong>h</strong> height, head dimension or hidden dimension?</li>
</ul>

<p>To get around this ambiguity, it’s common to see PyTorch code where in the
docstring each of the letters is defined. This isn’t a very natural pattern it’s
like if all of your variable names in code had to be single letters and you had
another file which would act as a dictionary for what each letter actually
meant! <em>shudders</em>.</p>

<p>One of the most useful lines of the <code>Zen of Python</code> is
<code>Explicit is better than Implicit</code>. Following this principle, we would like to
be able to write the variable names in the einsum string itself. Without this,
it’s harder to read and means you’re always looking back when trying to
understand or debug the code.</p>

<h3 id="enter-einops">Enter einops</h3>

<p>Einops is a tensor manipulation package that can be used with PyTorch, NumPy,
Tensorflow and Jax. It offers a nice API but we’ll focus on einsums which we can
now use with full variable names rather than single letters! It makes your ML
code so much clearer instantly.</p>

<p>For example let’s write the multi-query attention operation.</p>

<div><div><pre><code><span>import</span> <span>torch</span> <span>as</span> <span>t</span>
<span>from</span> <span>einops</span> <span>import</span> <span>einsum</span>

<span>def</span> <span>multi_query_attention</span><span>(</span><span>Q</span><span>:</span> <span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>K</span><span>:</span> <span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>V</span><span>:</span> <span>t</span><span>.</span><span>Tensor</span><span>)</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span>
    <span>_</span><span>,</span> <span>_</span><span>,</span> <span>head_dim</span> <span>=</span> <span>K</span><span>.</span><span>shape</span>
    <span>attn_scores</span> <span>=</span> <span>einsum</span><span>(</span><span>Q</span><span>,</span> <span>K</span><span>,</span>
        <span>&#34;batch head seq1 head_dim, batch seq2 head_dim -&gt; batch head seq1 seq2&#34;</span><span>)</span>
    <span>attn_matrix</span> <span>=</span> <span>t</span><span>.</span><span>softmax</span><span>(</span><span>attn_scores</span> <span>/</span> <span>head_dim</span> <span>**</span> <span>0.5</span><span>)</span>
    <span>out</span> <span>=</span>  <span>einsum</span><span>(</span><span>attn_matrix</span><span>,</span> <span>V</span><span>,</span>
        <span>&#34;batch head seq1 seq2, batch seq2 head_dim -&gt; batch head seq1 head_dim&#34;</span><span>)</span>
    <span>return</span> <span>out</span>

</code></pre></div></div>

<p>The nice things about this are that we didn’t need to write a glossary for what
random variables <code>b</code> or <code>h</code> were supposed to mean, you can just read it off.</p>

<p>Also note that typically when computing attention, we need to calculate $QK^T$.
Here we didn’t need to worry about how exactly to take the transpose - we just
give the dimension names and the correct transposes are done for the
multiplication to make sense!</p>

<p>Einops also offers great functions for rearranging, reducing and repeating
tensors which are also very useful.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/einops/the_world_if_einops.jpg" width="800" alt="The World If Everyone Used Einops"/>
    <figcaption></figcaption>
    </figure>
</div>

<p>Just trying to make those inscrutable matrix multiplications, a little more
scrutable. ￼</p>

  </div></div>
  </body>
</html>
