<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://victoriaritvo.com/blog/semantle-solver/">Original</a>
    <h1>A Solver for Semantle</h1>
    
    <div id="readability-page-1" class="page"><div data-astro-cid-4sn4zg3r="">  <div data-astro-cid-4sn4zg3r=""> <p><a href="https://semantle.com/">Semantle</a> is a Wordle game variant. But instead of scoring guesses based on lexical similarity like its predecessor, it scores them based on <em>semantic</em> similarity.</p>
<p>Here’s a screenshot from a game I played recently, ordered by similarity to the correct answer.</p>
<figure>
  <img src="http://victoriaritvo.com/blog/semantle-solver/semantle_game.png" alt="Screenshot of a Semantle game"/>
  <figcaption>Screenshot of a Semantle game</figcaption>
</figure>
<p>I initially guessed “philosophy,” which was very far from the correct answer (similarity score of 6.02). After several more guesses I got lucky with “biology” (27.55) as my 8th guess, which pointed me toward more science-y words. Eventually I realized the answer had something to do with a hospital setting (yes, yes, I could have gotten there faster after “biology”). I landed on the right word “medical” as my 52nd guess.</p>
<p>To be honest, that’s a pretty good round for me. I’ve had games last more than twice that before giving up. If you’ve played before, you know Semantle is <em>hard</em>. But it is solvable, generally by gradually honing in on words that give higher similarity scores, and moving away from words that give lower scores.</p>
<p><a href="https://jantz.website/">Ethan Jantz</a> and I wondered whether we could do better algorithmically. This post describes a simple <a href="https://github.com/EthanJantz/semantle/">solver</a> we made while at the <a href="https://www.recurse.com/">Recurse Center</a> that reliably finds the answer in around 3 guesses.</p>
<h2 id="what-information-does-the-game-give-you">What information does the game give you?</h2>
<p>Semantle uses word embeddings — numerical vector representations of word meanings — to represent words. It uses <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g">Google News word2vec</a>, which represents each word as a 300-dimensional vector. It then measures how close your guess is to the target word using the cosine similarity between the guess word embedding (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span>) and the target word embedding (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>). That similarity score is the feedback you get with each guess.</p>
<p>The difficulty of playing Semantle comes from how little information a single cosine similarity provides. It effectively tells you whether your guess is “hot” or “cold,” but not which direction you should move. As a result, you have to combine feedback from multiple guesses, mentally “triangulating” where the answer might be in semantic space.</p>
<h2 id="can-we-solve-for-the-embedding-of-the-target-word">Can we solve for the embedding of the target word?</h2>
<p>If you want to skip ahead to how we implemented the solver, you can jump to the next section. But first, I’m going to digress briefly to discuss why solving for the target word directly isn’t practical.</p>
<p>A natural first idea might be to treat each guess as a clue to the hidden vector and try to combine those clues to recover it directly. In embedding space, that translates to solving for the target vector using a system of linear equations.</p>
<p>The similarity score the game returns with each guess is:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>similarity</mtext><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>g</mi><mo>⋅</mo><mi>t</mi></mrow><mrow><mo stretchy="false">∥</mo><mi>g</mi><mo stretchy="false">∥</mo><mtext> </mtext><mo stretchy="false">∥</mo><mi>t</mi><mo stretchy="false">∥</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{similarity} = \cos(\theta) = \frac{g \cdot t}{\lVert g\rVert \,\lVert t\rVert}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>similarity</span></span><span></span><span>=</span><span></span></span><span><span></span><span>cos</span><span>(</span><span>θ</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>∥</span><span>g</span><span>∥</span><span></span><span>∥</span><span>t</span><span>∥</span></span></span><span><span></span><span></span></span><span><span></span><span><span>g</span><span></span><span>⋅</span><span></span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>If we assume embeddings are normalized, this means similarity is equivalent to the dot product, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>⋅</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">g \cdot t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span><span></span><span>⋅</span><span></span></span><span><span></span><span>t</span></span></span></span>. For each guess, that gives you one linear equation involving all 300 unknown components of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>.</p>
<p>In general, you need at least as many independent equations as unknowns to pin down a unique solution to a system of linear equations. Which means we need at least 300 independent guesses before we could recover <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span> and look up the nearest word in the vocabulary to find the target.</p>
<p>Semantle is a hard game, but 300 guesses isn’t good enough to beat playing it as a human (or at least, I’d like to think so). So instead of trying to solve for the target embedding directly, we took advantage of the geometry of the cosine similarity and used a filtering approach that turned out to be surprisingly effective.</p>
<h2 id="how-we-built-the-solver">How we built the solver</h2>
<p>Geometrically, when we guess a word and get back a similarity score, that means the target word must lie somewhere on a surface of constant cosine similarity to the guess. On the unit sphere of embeddings, this surface corresponds to a ring of points that make the same angle with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span>.</p>
<figure>
  <img src="http://victoriaritvo.com/blog/semantle-solver/embedding_sphere.png"/>
  <figcaption>A cosine similarity score defines a ring on the unit sphere of embeddings. The true target must lie somewhere on this ring.</figcaption>
</figure>
<p>That makes each guess a very strong filter. Only words whose similarity to our guess is equal to the returned similarity score could still be the target. We can make use of that fact to build a solver.</p>
<h3 id="the-elimination-approach">The elimination approach</h3>
<p>The solver works like this:</p>
<p>It keeps a list of all possible target words. Initially, this list contains every word in the embedding vocabulary for <strong>GoogleNews-vectors-negative300</strong> <em>(In theory, this could be restricted further since Semantle says it chooses its target from a list of the 5,000 most popular words in English. But even starting with several million candidates, this solver works just fine.)</em></p>
<p>To run this solver, we also built a clone of Semantle using their same embeddings so we could get immediate similarity scores.</p>
<p>For each round, the solver:</p>
<ol>
<li>Picks a random candidate word as a guess</li>
<li>Asks Semantle for the similarity score between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span></li>
<li>Computes the similarity score between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span> and <strong>every remaining candidate word</strong></li>
<li>Reduces the candidate list to only those that have the reported cosine similarity (within some small tolerance)</li>
<li>Repeats 1-4 until only one candidate word is left</li>
</ol>
<p><em>Note 1: For those who are familiar with this topic, we actually implemented this using cosine distance rather than cosine similarity, but the logic is effectively the same.</em></p>
<p><em>Note 2: We chose guesses at random for simplicity, but in principle, you could probably do better than random.</em></p>
<p>In a visual diagram, that can be represented like this:</p>
<figure>
  <img src="http://victoriaritvo.com/blog/semantle-solver/embedding_sphere_two_guesses.png"/>
  <figcaption>Each guess constrains the possible candidates on the unit embedding sphere. After one guess (blue), few candidate words remain. After a second guess (purple), the intersection leaves even fewer viable candidates.</figcaption>
</figure>
<p>In code, that filtering step looks something like this:</p>
<pre tabindex="0" data-language="python"><code><span><span>while</span><span> len</span><span>(potential_words) </span><span>&gt;</span><span> 1</span><span>:</span></span>
<span><span>	</span></span>
<span><span>	# step 1: make a guess</span></span>
<span><span>	g </span><span>=</span><span> random.choice(potential_words)</span></span>
<span></span>
<span><span>	# step 2: ask Semantle for score</span></span>
<span><span>	s_target </span><span>=</span><span> get_similarity_from_game(g) </span></span>
<span></span>
<span><span>	# step 3: similarity from g to every remaining candidate</span></span>
<span><span>	distances </span><span>=</span><span> word_vectors.distances(g, </span><span>other_words</span><span>=</span><span>potential_words)</span></span>
<span><span>	similarities </span><span>=</span><span> 1.0</span><span> -</span><span> distances </span><span># cosine similarity = 1 - cosine_distance</span></span>
<span></span>
<span><span>	# step 4: keep only words w such that sim(g,w) matches s_target</span></span>
<span><span>	temp_potential_words </span><span>=</span><span> []</span></span>
<span></span>
<span><span>	for</span><span> i </span><span>in</span><span> range</span><span>(</span><span>len</span><span>(potential_words)):</span></span>
<span><span>		w </span><span>=</span><span> potential_words[i]</span></span>
<span><span>		s </span><span>=</span><span> similarities[i]</span></span>
<span><span>		if</span><span> abs</span><span>(s </span><span>-</span><span> s_target) </span><span>&lt;</span><span> tolerance:</span></span>
<span><span>			temp_potential_words.append(w)</span></span>
<span><span>	potential_words </span><span>=</span><span> temp_potential_words</span></span>
<span></span>
<span><span># one word left</span></span>
<span><span>answer </span><span>=</span><span> potential_words[</span><span>0</span><span>]</span></span>
<span></span></code></pre>
<h2 id="why-does-this-work-so-quickly">Why does this work so quickly?</h2>
<p>Although word embedding space is 300-dimensional, the vocabulary itself is sparse within that space. As a result, each cosine similarity constraint is highly restrictive. After just one or two guesses, the set of words that lie at the right distance shrinks dramatically. That makes the filtering strategy super effective.</p>
<p>For example, here’s a sample run:
(<em>We filter within a tolerance of 0.0001, which corresponds to the four-decimal-place precision of real Semantle’s cosine similarity scores.</em>)</p>
<pre tabindex="0" data-language="python"><code><span><span>── Guess </span><span>1</span><span>: countryside ──</span></span>
<span><span>  cosine similarity: </span><span>0.023168</span><span>  (±</span><span>0.0001</span><span>)</span></span>
<span><span>  candidates searched: </span><span>3</span><span>,</span><span>000</span><span>,</span><span>000</span><span>  -&gt;</span><span>  remaining: </span><span>3</span><span>,</span><span>296</span></span>
<span></span>
<span><span>── Guess </span><span>2</span><span>: levelization ──</span></span>
<span><span>  cosine similarity: </span><span>0.097055</span><span>  (±</span><span>0.0001</span><span>)</span></span>
<span><span>  candidates searched: </span><span>3</span><span>,</span><span>296</span><span>  -&gt;</span><span>  remaining: </span><span>3</span></span>
<span></span>
<span><span>── Guess </span><span>3</span><span>: Skrzynski ──</span></span>
<span><span>  cosine similarity: </span><span>0.005881</span><span>  (±</span><span>0.0001</span><span>)</span></span>
<span><span>  candidates searched: </span><span>3</span><span>  -&gt;</span><span>  remaining: </span><span>1</span></span>
<span><span>  </span></span>
<span><span>Answer: medical </span></span></code></pre>
<p>It’s worth noting that the guesses don’t need to “trend” toward the correct answer the way a human player’s guesses would. “Countryside,” “levelization,” and “Skrzynski” have nothing to do with “medical.” The solver isn’t walking a recognizable path.</p>
<p>And yet, in a sense, humans and this solver are both “honing in” on the answer, just very differently. In a way, we as humans are doing something gradient-descent-like: every time we make a guess we are nudged towards the target, and we take small and meaningful steps toward it through semantic space. The process is local, intuitive, and guided by meaning.</p>
<p>The solver is also honing in, but it’s doing it globally rather than locally. Instead of taking tiny steps toward the target, it takes huge jumps that it knows are exactly right, while slicing away vast swaths of impossible words with each guess. It’s not actually moving in any kind of meaningful way closer to “medical,” it’s just shrinking the universe of possible answers until “medical” is the only possible choice remaining.</p>
<p>I find that pretty cool: that the same embedding space can be something you navigate by meaning, or something you carve up by geometry. But either way, you get to the same place.</p> </div> </div></div>
  </body>
</html>
