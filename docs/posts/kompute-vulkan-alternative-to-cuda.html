<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/KomputeProject/kompute">Original</a>
    <h1>Kompute – Vulkan Alternative to CUDA</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8f872dee052b7c84233f785769c7e17ccfdbff17a05ebb774c2f3755d50ebf0e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d302e372e302d677265656e2e737667"><img src="https://camo.githubusercontent.com/8f872dee052b7c84233f785769c7e17ccfdbff17a05ebb774c2f3755d50ebf0e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d302e372e302d677265656e2e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/Version-0.7.0-green.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b8c3ea661caa26d6e0bad0e0cf08250dcee72f166dc0ca80005fac186c0f8e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f432b2b2d313425453225383025393432302d707572706c652e737667"><img src="https://camo.githubusercontent.com/b8c3ea661caa26d6e0bad0e0cf08250dcee72f166dc0ca80005fac186c0f8e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f432b2b2d313425453225383025393432302d707572706c652e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/C++-14%E2%80%9420-purple.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a931e304cb2822ce527ed9230da8fed82e1f6972ff0674706ff06fe5a4ac333c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c642d636d616b652d7265642e737667"><img src="https://camo.githubusercontent.com/a931e304cb2822ce527ed9230da8fed82e1f6972ff0674706ff06fe5a4ac333c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c642d636d616b652d7265642e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/Build-cmake-red.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ca62ec82651f12fc0f1cd5c027a061794b6bf2199fe9130be61cbab90ae8002e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e37254532253830253934332e392d626c75652e737667"><img src="https://camo.githubusercontent.com/ca62ec82651f12fc0f1cd5c027a061794b6bf2199fe9130be61cbab90ae8002e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e37254532253830253934332e392d626c75652e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/Python-3.7%E2%80%943.9-blue.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/266f5c100f8205bd1421e45d36346c42927736082dc1d7b8409d0850e4764eaa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368652d626c61636b2e737667"><img src="https://camo.githubusercontent.com/266f5c100f8205bd1421e45d36346c42927736082dc1d7b8409d0850e4764eaa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368652d626c61636b2e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/License-Apache-black.svg"/></a>
<a href="https://bestpractices.coreinfrastructure.org/projects/4834" rel="nofollow"><img src="https://camo.githubusercontent.com/bd6283e527078fd79a22cbe272515776ac1c20c801ff86f65181d558cf478673/68747470733a2f2f626573747072616374696365732e636f7265696e6672617374727563747572652e6f72672f70726f6a656374732f343833342f6261646765" alt="CII Best Practices" data-canonical-src="https://bestpractices.coreinfrastructure.org/projects/4834/badge"/></a></p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute.jpg"><img src="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute.jpg"/></a>
</td>
<td>

<div dir="auto"><h3 tabindex="-1" dir="auto">The general purpose GPU compute framework for cross vendor graphics cards (AMD, Qualcomm, NVIDIA &amp; friends)</h3><a id="user-content-the-general-purpose-gpu-compute-framework-for-cross-vendor-graphics-cards-amd-qualcomm-nvidia--friends" aria-label="Permalink: The general purpose GPU compute framework for cross vendor graphics cards (AMD, Qualcomm, NVIDIA &amp; friends)" href="#the-general-purpose-gpu-compute-framework-for-cross-vendor-graphics-cards-amd-qualcomm-nvidia--friends"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<div dir="auto"><h4 tabindex="-1" dir="auto">Blazing fast, mobile-enabled, asynchronous, and optimized for advanced GPU acceleration usecases.</h4><a id="user-content-blazing-fast-mobile-enabled-asynchronous-and-optimized-for-advanced-gpu-acceleration-usecases" aria-label="Permalink: Blazing fast, mobile-enabled, asynchronous, and optimized for advanced GPU acceleration usecases." href="#blazing-fast-mobile-enabled-asynchronous-and-optimized-for-advanced-gpu-acceleration-usecases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">💬 <a href="https://kompute.cc/overview/community.html" rel="nofollow">Join the Discord &amp; Community Calls</a> 🔋 <a href="https://kompute.cc" rel="nofollow">Documentation</a> 💻 <a href="https://medium.com/@AxSaucedo/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a" rel="nofollow">Blog Post</a> ⌨ <a href="#more-examples">Examples</a> 💾</p>
<hr/>
<div dir="auto"><h5 tabindex="-1" dir="auto">Kompute is backed by the Linux Foundation as a <a href="https://lfaidata.foundation/blog/2021/08/26/kompute-joins-lf-ai-data-as-new-sandbox-project/" rel="nofollow">hosted project</a> by the LF AI &amp; Data Foundation.</h5><a id="user-content-kompute-is-backed-by-the-linux-foundation-as-a-hosted-project-by-the-lf-ai--data-foundation" aria-label="Permalink: Kompute is backed by the Linux Foundation as a hosted project by the LF AI &amp; Data Foundation." href="#kompute-is-backed-by-the-linux-foundation-as-a-hosted-project-by-the-lf-ai--data-foundation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table></markdown-accessiblity-table>

<ul dir="auto">
<li><a href="#your-first-kompute-python">Flexible Python module</a> with <a href="#your-first-kompute-c">C++ SDK</a> for optimizations</li>
<li><a href="#asynchronous-and-parallel-operations">Asynchronous &amp; parallel processing</a> support through GPU family queues</li>
<li><a href="#mobile-enabled">Mobile enabled</a> with examples via Android NDK across several architectures</li>
<li>BYOV: <a href="#motivations">Bring-your-own-Vulkan design</a> to play nice with existing Vulkan applications</li>
<li>Explicit relationships for GPU and host <a href="https://kompute.cc/overview/memory-management.html" rel="nofollow">memory ownership and memory management</a></li>
<li>Robust codebase with <a href="https://kompute.cc/codecov/" rel="nofollow">90% unit test code coverage</a></li>
<li>Advanced use-cases on <a href="https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a" rel="nofollow">machine learning 🤖</a>, <a href="https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617" rel="nofollow">mobile development 📱</a> and <a href="https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0" rel="nofollow">game development 🎮</a>.</li>
<li>Active community with <a href="https://kompute.cc/overview/community.html" rel="nofollow">monthly calls, discord chat and more</a></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/komputer-logos.gif"><img src="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/komputer-logos.gif" alt="" data-animated-image=""/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Projects using Kompute ❤️  🤖</h2><a id="user-content-projects-using-kompute-️--" aria-label="Permalink: Projects using Kompute ❤️  🤖" href="#projects-using-kompute-️--"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>


<p dir="auto">Below you can find a GPU multiplication example using the C++ and Python Kompute interfaces.</p>
<p dir="auto">You can <a href="https://discord.gg/MaH5Jv5zwv" rel="nofollow">join the Discord</a> for questions / discussion, open a <a href="https://github.com/KomputeProject/kompute/issues/new">github issue</a>, or read <a href="https://kompute.cc/" rel="nofollow">the documentation</a>.</p>

<p dir="auto">The C++ interface provides low level access to the native components of Kompute, enabling for <a href="https://kompute.cc/overview/async-parallel.html" rel="nofollow">advanced optimizations</a> as well as <a href="https://kompute.cc/overview/reference.html" rel="nofollow">extension of components</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
void kompute(const std::string&amp; shader) {

    // 1. Create Kompute Manager with default settings (device 0, first queue and no extensions)
    kp::Manager mgr; 

    // 2. Create and initialise Kompute Tensors through manager

    // Default tensor constructor simplifies creation of float values
    auto tensorInA = mgr.tensor({ 2., 2., 2. });
    auto tensorInB = mgr.tensor({ 1., 2., 3. });
    // Explicit type constructor supports uint32, int32, double, float and bool
    auto tensorOutA = mgr.tensorT&lt;uint32_t&gt;({ 0, 0, 0 });
    auto tensorOutB = mgr.tensorT&lt;uint32_t&gt;({ 0, 0, 0 });

    std::vector&lt;std::shared_ptr&lt;kp::Tensor&gt;&gt; params = {tensorInA, tensorInB, tensorOutA, tensorOutB};

    // 3. Create algorithm based on shader (supports buffers &amp; push/spec constants)
    kp::Workgroup workgroup({3, 1, 1});
    std::vector&lt;float&gt; specConsts({ 2 });
    std::vector&lt;float&gt; pushConstsA({ 2.0 });
    std::vector&lt;float&gt; pushConstsB({ 3.0 });

    auto algorithm = mgr.algorithm(params,
                                   // See documentation shader section for compileSource
                                   compileSource(shader),
                                   workgroup,
                                   specConsts,
                                   pushConstsA);

    // 4. Run operation synchronously using sequence
    mgr.sequence()
        -&gt;record&lt;kp::OpTensorSyncDevice&gt;(params)
        -&gt;record&lt;kp::OpAlgoDispatch&gt;(algorithm) // Binds default push consts
        -&gt;eval() // Evaluates the two recorded operations
        -&gt;record&lt;kp::OpAlgoDispatch&gt;(algorithm, pushConstsB) // Overrides push consts
        -&gt;eval(); // Evaluates only last recorded operation

    // 5. Sync results from the GPU asynchronously
    auto sq = mgr.sequence();
    sq-&gt;evalAsync&lt;kp::OpTensorSyncLocal&gt;(params);

    // ... Do other work asynchronously whilst GPU finishes

    sq-&gt;evalAwait();

    // Prints the first output which is: { 4, 8, 12 }
    for (const float&amp; elem : tensorOutA-&gt;vector()) std::cout &lt;&lt; elem &lt;&lt; &#34;  &#34;;
    // Prints the second output which is: { 10, 10, 10 }
    for (const float&amp; elem : tensorOutB-&gt;vector()) std::cout &lt;&lt; elem &lt;&lt; &#34;  &#34;;

} // Manages / releases all CPU and GPU memory resources

int main() {

    // Define a raw string shader (or use the Kompute tools to compile to SPIRV / C++ header
    // files). This shader shows some of the main components including constants, buffers, etc
    std::string shader = (R&#34;(
        #version 450

        layout (local_size_x = 1) in;

        // The input tensors bind index is relative to index in parameter passed
        layout(set = 0, binding = 0) buffer buf_in_a { float in_a[]; };
        layout(set = 0, binding = 1) buffer buf_in_b { float in_b[]; };
        layout(set = 0, binding = 2) buffer buf_out_a { uint out_a[]; };
        layout(set = 0, binding = 3) buffer buf_out_b { uint out_b[]; };

        // Kompute supports push constants updated on dispatch
        layout(push_constant) uniform PushConstants {
            float val;
        } push_const;

        // Kompute also supports spec constants on initalization
        layout(constant_id = 0) const float const_one = 0;

        void main() {
            uint index = gl_GlobalInvocationID.x;
            out_a[index] += uint( in_a[index] * in_b[index] );
            out_b[index] += uint( const_one * push_const.val );
        }
    )&#34;);

    // Run the function declared above with our raw string shader
    kompute(shader);
}
"><pre><span>void</span> <span>kompute</span>(<span>const</span> std::string&amp; shader) {

    <span><span>//</span> 1. Create Kompute Manager with default settings (device 0, first queue and no extensions)</span>
    kp::Manager mgr; 

    <span><span>//</span> 2. Create and initialise Kompute Tensors through manager</span>

    <span><span>//</span> Default tensor constructor simplifies creation of float values</span>
    <span>auto</span> tensorInA = mgr.<span>tensor</span>({ <span>2</span>., <span>2</span>., <span>2</span>. });
    <span>auto</span> tensorInB = mgr.<span>tensor</span>({ <span>1</span>., <span>2</span>., <span>3</span>. });
    <span><span>//</span> Explicit type constructor supports uint32, int32, double, float and bool</span>
    <span>auto</span> tensorOutA = mgr.<span>tensorT</span>&lt;<span>uint32_t</span>&gt;({ <span>0</span>, <span>0</span>, <span>0</span> });
    <span>auto</span> tensorOutB = mgr.<span>tensorT</span>&lt;<span>uint32_t</span>&gt;({ <span>0</span>, <span>0</span>, <span>0</span> });

    std::vector&lt;std::shared_ptr&lt;kp::Tensor&gt;&gt; params = {tensorInA, tensorInB, tensorOutA, tensorOutB};

    <span><span>//</span> 3. Create algorithm based on shader (supports buffers &amp; push/spec constants)</span>
    kp::Workgroup <span>workgroup</span>({<span>3</span>, <span>1</span>, <span>1</span>});
    std::vector&lt;<span>float</span>&gt; <span>specConsts</span>({ <span>2</span> });
    std::vector&lt;<span>float</span>&gt; <span>pushConstsA</span>({ <span>2.0</span> });
    std::vector&lt;<span>float</span>&gt; <span>pushConstsB</span>({ <span>3.0</span> });

    <span>auto</span> algorithm = mgr.<span>algorithm</span>(params,
                                   <span><span>//</span> See documentation shader section for compileSource</span>
                                   <span>compileSource</span>(shader),
                                   workgroup,
                                   specConsts,
                                   pushConstsA);

    <span><span>//</span> 4. Run operation synchronously using sequence</span>
    mgr.<span>sequence</span>()
        -&gt;<span>record</span>&lt;kp::OpTensorSyncDevice&gt;(params)
        -&gt;<span>record</span>&lt;kp::OpAlgoDispatch&gt;(algorithm) <span><span>//</span> Binds default push consts</span>
        -&gt;<span>eval</span>() <span><span>//</span> Evaluates the two recorded operations</span>
        -&gt;<span>record</span>&lt;kp::OpAlgoDispatch&gt;(algorithm, pushConstsB) <span><span>//</span> Overrides push consts</span>
        -&gt;<span>eval</span>(); <span><span>//</span> Evaluates only last recorded operation</span>

    <span><span>//</span> 5. Sync results from the GPU asynchronously</span>
    <span>auto</span> sq = mgr.<span>sequence</span>();
    sq-&gt;<span>evalAsync</span>&lt;kp::OpTensorSyncLocal&gt;(params);

    <span><span>//</span> ... Do other work asynchronously whilst GPU finishes</span>

    sq-&gt;<span>evalAwait</span>();

    <span><span>//</span> Prints the first output which is: { 4, 8, 12 }</span>
    <span>for</span> (<span>const</span> <span>float</span>&amp; elem : tensorOutA-&gt;<span>vector</span>()) std::cout &lt;&lt; elem &lt;&lt; <span><span>&#34;</span>  <span>&#34;</span></span>;
    <span><span>//</span> Prints the second output which is: { 10, 10, 10 }</span>
    <span>for</span> (<span>const</span> <span>float</span>&amp; elem : tensorOutB-&gt;<span>vector</span>()) std::cout &lt;&lt; elem &lt;&lt; <span><span>&#34;</span>  <span>&#34;</span></span>;

} <span><span>//</span> Manages / releases all CPU and GPU memory resources</span>

<span>int</span> <span>main</span>() {

    <span><span>//</span> Define a raw string shader (or use the Kompute tools to compile to SPIRV / C++ header</span>
    <span><span>//</span> files). This shader shows some of the main components including constants, buffers, etc</span>
    std::string shader = (<span><span>R&#34;(</span></span>
<span>        #version 450</span>
<span></span>
<span>        layout (local_size_x = 1) in;</span>
<span></span>
<span>        // The input tensors bind index is relative to index in parameter passed</span>
<span>        layout(set = 0, binding = 0) buffer buf_in_a { float in_a[]; };</span>
<span>        layout(set = 0, binding = 1) buffer buf_in_b { float in_b[]; };</span>
<span>        layout(set = 0, binding = 2) buffer buf_out_a { uint out_a[]; };</span>
<span>        layout(set = 0, binding = 3) buffer buf_out_b { uint out_b[]; };</span>
<span></span>
<span>        // Kompute supports push constants updated on dispatch</span>
<span>        layout(push_constant) uniform PushConstants {</span>
<span>            float val;</span>
<span>        } push_const;</span>
<span></span>
<span>        // Kompute also supports spec constants on initalization</span>
<span>        layout(constant_id = 0) const float const_one = 0;</span>
<span></span>
<span>        void main() {</span>
<span>            uint index = gl_GlobalInvocationID.x;</span>
<span>            out_a[index] += uint( in_a[index] * in_b[index] );</span>
<span>            out_b[index] += uint( const_one * push_const.val );</span>
<span>        }</span>
<span>    <span>)&#34;</span></span>);

    <span><span>//</span> Run the function declared above with our raw string shader</span>
    <span>kompute</span>(shader);
}
</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Your First Kompute (Python)</h3><a id="user-content-your-first-kompute-python" aria-label="Permalink: Your First Kompute (Python)" href="#your-first-kompute-python"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <a href="https://kompute.cc/overview/python-package.html" rel="nofollow">Python package</a> provides a <a href="https://kompute.cc/overview/python-reference.html" rel="nofollow">high level interactive interface</a> that enables for experimentation whilst ensuring high performance and fast development workflows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
from .utils import compile_source # using util function from python/test/utils

def kompute(shader):
    # 1. Create Kompute Manager with default settings (device 0, first queue and no extensions)
    mgr = kp.Manager()

    # 2. Create and initialise Kompute Tensors through manager

    # Default tensor constructor simplifies creation of float values
    tensor_in_a = mgr.tensor([2, 2, 2])
    tensor_in_b = mgr.tensor([1, 2, 3])
    # Explicit type constructor supports uint32, int32, double, float and bool
    tensor_out_a = mgr.tensor_t(np.array([0, 0, 0], dtype=np.uint32))
    tensor_out_b = mgr.tensor_t(np.array([0, 0, 0], dtype=np.uint32))

    params = [tensor_in_a, tensor_in_b, tensor_out_a, tensor_out_b]

    # 3. Create algorithm based on shader (supports buffers &amp; push/spec constants)
    workgroup = (3, 1, 1)
    spec_consts = [2]
    push_consts_a = [2]
    push_consts_b = [3]

    # See documentation shader section for compile_source
    spirv = compile_source(shader)

    algo = mgr.algorithm(params, spirv, workgroup, spec_consts, push_consts_a)

    # 4. Run operation synchronously using sequence
    (mgr.sequence()
        .record(kp.OpTensorSyncDevice(params))
        .record(kp.OpAlgoDispatch(algo)) # Binds default push consts provided
        .eval() # evaluates the two recorded ops
        .record(kp.OpAlgoDispatch(algo, push_consts_b)) # Overrides push consts
        .eval()) # evaluates only the last recorded op

    # 5. Sync results from the GPU asynchronously
    sq = mgr.sequence()
    sq.eval_async(kp.OpTensorSyncLocal(params))

    # ... Do other work asynchronously whilst GPU finishes

    sq.eval_await()

    # Prints the first output which is: { 4, 8, 12 }
    print(tensor_out_a)
    # Prints the first output which is: { 10, 10, 10 }
    print(tensor_out_b)

if __name__ == &#34;__main__&#34;:

    # Define a raw string shader (or use the Kompute tools to compile to SPIRV / C++ header
    # files). This shader shows some of the main components including constants, buffers, etc
    shader = &#34;&#34;&#34;
        #version 450

        layout (local_size_x = 1) in;

        // The input tensors bind index is relative to index in parameter passed
        layout(set = 0, binding = 0) buffer buf_in_a { float in_a[]; };
        layout(set = 0, binding = 1) buffer buf_in_b { float in_b[]; };
        layout(set = 0, binding = 2) buffer buf_out_a { uint out_a[]; };
        layout(set = 0, binding = 3) buffer buf_out_b { uint out_b[]; };

        // Kompute supports push constants updated on dispatch
        layout(push_constant) uniform PushConstants {
            float val;
        } push_const;

        // Kompute also supports spec constants on initalization
        layout(constant_id = 0) const float const_one = 0;

        void main() {
            uint index = gl_GlobalInvocationID.x;
            out_a[index] += uint( in_a[index] * in_b[index] );
            out_b[index] += uint( const_one * push_const.val );
        }
    &#34;&#34;&#34;

    kompute(shader)
"><pre><span>from</span> .<span>utils</span> <span>import</span> <span>compile_source</span> <span># using util function from python/test/utils</span>

<span>def</span> <span>kompute</span>(<span>shader</span>):
    <span># 1. Create Kompute Manager with default settings (device 0, first queue and no extensions)</span>
    <span>mgr</span> <span>=</span> <span>kp</span>.<span>Manager</span>()

    <span># 2. Create and initialise Kompute Tensors through manager</span>

    <span># Default tensor constructor simplifies creation of float values</span>
    <span>tensor_in_a</span> <span>=</span> <span>mgr</span>.<span>tensor</span>([<span>2</span>, <span>2</span>, <span>2</span>])
    <span>tensor_in_b</span> <span>=</span> <span>mgr</span>.<span>tensor</span>([<span>1</span>, <span>2</span>, <span>3</span>])
    <span># Explicit type constructor supports uint32, int32, double, float and bool</span>
    <span>tensor_out_a</span> <span>=</span> <span>mgr</span>.<span>tensor_t</span>(<span>np</span>.<span>array</span>([<span>0</span>, <span>0</span>, <span>0</span>], <span>dtype</span><span>=</span><span>np</span>.<span>uint32</span>))
    <span>tensor_out_b</span> <span>=</span> <span>mgr</span>.<span>tensor_t</span>(<span>np</span>.<span>array</span>([<span>0</span>, <span>0</span>, <span>0</span>], <span>dtype</span><span>=</span><span>np</span>.<span>uint32</span>))

    <span>params</span> <span>=</span> [<span>tensor_in_a</span>, <span>tensor_in_b</span>, <span>tensor_out_a</span>, <span>tensor_out_b</span>]

    <span># 3. Create algorithm based on shader (supports buffers &amp; push/spec constants)</span>
    <span>workgroup</span> <span>=</span> (<span>3</span>, <span>1</span>, <span>1</span>)
    <span>spec_consts</span> <span>=</span> [<span>2</span>]
    <span>push_consts_a</span> <span>=</span> [<span>2</span>]
    <span>push_consts_b</span> <span>=</span> [<span>3</span>]

    <span># See documentation shader section for compile_source</span>
    <span>spirv</span> <span>=</span> <span>compile_source</span>(<span>shader</span>)

    <span>algo</span> <span>=</span> <span>mgr</span>.<span>algorithm</span>(<span>params</span>, <span>spirv</span>, <span>workgroup</span>, <span>spec_consts</span>, <span>push_consts_a</span>)

    <span># 4. Run operation synchronously using sequence</span>
    (<span>mgr</span>.<span>sequence</span>()
        .<span>record</span>(<span>kp</span>.<span>OpTensorSyncDevice</span>(<span>params</span>))
        .<span>record</span>(<span>kp</span>.<span>OpAlgoDispatch</span>(<span>algo</span>)) <span># Binds default push consts provided</span>
        .<span>eval</span>() <span># evaluates the two recorded ops</span>
        .<span>record</span>(<span>kp</span>.<span>OpAlgoDispatch</span>(<span>algo</span>, <span>push_consts_b</span>)) <span># Overrides push consts</span>
        .<span>eval</span>()) <span># evaluates only the last recorded op</span>

    <span># 5. Sync results from the GPU asynchronously</span>
    <span>sq</span> <span>=</span> <span>mgr</span>.<span>sequence</span>()
    <span>sq</span>.<span>eval_async</span>(<span>kp</span>.<span>OpTensorSyncLocal</span>(<span>params</span>))

    <span># ... Do other work asynchronously whilst GPU finishes</span>

    <span>sq</span>.<span>eval_await</span>()

    <span># Prints the first output which is: { 4, 8, 12 }</span>
    <span>print</span>(<span>tensor_out_a</span>)
    <span># Prints the first output which is: { 10, 10, 10 }</span>
    <span>print</span>(<span>tensor_out_b</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:

    <span># Define a raw string shader (or use the Kompute tools to compile to SPIRV / C++ header</span>
    <span># files). This shader shows some of the main components including constants, buffers, etc</span>
    <span>shader</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>        #version 450</span>
<span></span>
<span>        layout (local_size_x = 1) in;</span>
<span></span>
<span>        // The input tensors bind index is relative to index in parameter passed</span>
<span>        layout(set = 0, binding = 0) buffer buf_in_a { float in_a[]; };</span>
<span>        layout(set = 0, binding = 1) buffer buf_in_b { float in_b[]; };</span>
<span>        layout(set = 0, binding = 2) buffer buf_out_a { uint out_a[]; };</span>
<span>        layout(set = 0, binding = 3) buffer buf_out_b { uint out_b[]; };</span>
<span></span>
<span>        // Kompute supports push constants updated on dispatch</span>
<span>        layout(push_constant) uniform PushConstants {</span>
<span>            float val;</span>
<span>        } push_const;</span>
<span></span>
<span>        // Kompute also supports spec constants on initalization</span>
<span>        layout(constant_id = 0) const float const_one = 0;</span>
<span></span>
<span>        void main() {</span>
<span>            uint index = gl_GlobalInvocationID.x;</span>
<span>            out_a[index] += uint( in_a[index] * in_b[index] );</span>
<span>            out_b[index] += uint( const_one * push_const.val );</span>
<span>        }</span>
<span>    &#34;&#34;&#34;</span>

    <span>kompute</span>(<span>shader</span>)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Interactive Notebooks &amp; Hands on Videos</h3><a id="user-content-interactive-notebooks--hands-on-videos" aria-label="Permalink: Interactive Notebooks &amp; Hands on Videos" href="#interactive-notebooks--hands-on-videos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You are able to try out the interactive Colab Notebooks which allow you to use a free GPU. The available examples are the Python and C++ examples below:</p>
<markdown-accessiblity-table></markdown-accessiblity-table>
<p dir="auto">You can also check out the two following talks presented at the FOSDEM 2021 conference.</p>
<p dir="auto">Both videos have timestamps which will allow you to skip to the most relevant section for you - the intro &amp; motivations for both is almost the same so you can skip to the more specific content.</p>
<markdown-accessiblity-table></markdown-accessiblity-table>

<p dir="auto">The core architecture of Kompute includes the following:</p>
<ul dir="auto">
<li><a href="https://kompute.cc/overview/reference.html#manager" rel="nofollow">Kompute Manager</a> - Base orchestrator which creates and manages device and child components</li>
<li><a href="https://kompute.cc/overview/reference.html#sequence" rel="nofollow">Kompute Sequence</a> - Container of operations that can be sent to GPU as batch</li>
<li><a href="https://kompute.cc/overview/reference.html#algorithm" rel="nofollow">Kompute Operation (Base)</a> - Base class from which all operations inherit</li>
<li><a href="https://kompute.cc/overview/reference.html#tensor" rel="nofollow">Kompute Tensor</a> - Tensor structured data used in GPU operations</li>
<li><a href="https://kompute.cc/overview/reference.html#algorithm" rel="nofollow">Kompute Algorithm</a> - Abstraction for (shader) logic executed in the GPU</li>
</ul>
<p dir="auto">To see a full breakdown you can read further in the <a href="https://kompute.cc/overview/reference.html" rel="nofollow">C++ Class Reference</a>.</p>
<markdown-accessiblity-table><table>
<tbody><tr><th>
Full Architecture
</th>
<th>
Simplified Kompute Components
</th>
</tr><tr>
<td>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute-vulkan-architecture.jpg"><img width="100%" src="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute-vulkan-architecture.jpg"/></a>
</td>
<td>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute-architecture.jpg"><img width="100%" src="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/kompute-architecture.jpg"/></a>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<div dir="auto"><h2 tabindex="-1" dir="auto">Asynchronous and Parallel Operations</h2><a id="user-content-asynchronous-and-parallel-operations" aria-label="Permalink: Asynchronous and Parallel Operations" href="#asynchronous-and-parallel-operations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Kompute provides flexibility to run operations in an asynrchonous way through vk::Fences. Furthermore, Kompute enables for explicit allocation of queues, which allow for parallel execution of operations across queue families.</p>
<p dir="auto">The image below provides an intuition on how Kompute Sequences can be allocated to different queues to enable parallel execution based on hardware. You can see the <a href="https://kompute.cc/overview/advanced-examples.html#parallel-operations" rel="nofollow">hands on example</a>, as well as the <a href="https://kompute.cc/overview/async-parallel.html" rel="nofollow">detailed documentation page</a> describing how it would work using an NVIDIA 1650 as an example.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/queue-allocation.jpg"><img src="https://raw.githubusercontent.com/KomputeProject/kompute/master/docs/images/queue-allocation.jpg" alt=""/></a></p>

<p dir="auto">Kompute has been optimized to work in mobile environments. The <a href="#build-overview">build system</a> enables for dynamic loading of the Vulkan shared library for Android environments, together with a working <a href="https://github.com/KomputeProject/kompute/tree/master/vk_ndk_wrapper_include">Android NDK wrapper</a> for the CPP headers.</p>
<markdown-accessiblity-table></markdown-accessiblity-table>


<ul dir="auto">
<li><a href="https://kompute.cc/overview/advanced-examples.html#simple-shader-example" rel="nofollow">Simple multiplication example</a></li>
<li><a href="https://kompute.cc/overview/advanced-examples.html#record-batch-commands" rel="nofollow">Record batch commands with a Kompute Sequence</a></li>
<li><a href="https://kompute.cc/overview/advanced-examples.html#asynchronous-operations" rel="nofollow">Run Asynchronous Operations</a></li>
<li><a href="https://kompute.cc/overview/advanced-examples.html#parallel-operations" rel="nofollow">Run Parallel Operations Across Multiple GPU Queues</a></li>
<li><a href="https://kompute.cc/overview/advanced-examples.html#your-custom-kompute-operation" rel="nofollow">Create your custom Kompute Operations</a></li>
<li><a href="https://kompute.cc/overview/advanced-examples.html#logistic-regression-example" rel="nofollow">Implementing logistic regression from scratch</a></li>
</ul>

<ul dir="auto">
<li><a href="https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a" rel="nofollow">Machine Learning Logistic Regression Implementation</a></li>
<li><a href="https://towardsdatascience.com/parallelizing-heavy-gpu-workloads-via-multi-queue-operations-50a38b15a1dc" rel="nofollow">Parallelizing GPU-intensive Workloads via Multi-Queue Operations</a></li>
<li><a href="https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617" rel="nofollow">Android NDK Mobile Kompute ML Application</a></li>
<li><a href="https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0" rel="nofollow">Game Development Kompute ML in Godot Engine</a></li>
</ul>

<p dir="auto">Besides the C++ core SDK you can also use the Python package of Kompute, which exposes the same core functionality, and supports interoperability with Python objects like Lists, Numpy Arrays, etc.</p>
<p dir="auto">The only dependencies are Python 3.5+ and Cmake 3.4.1+. You can install Kompute from the <a href="https://pypi.org/project/kp/" rel="nofollow">Python pypi package</a> using the following command.</p>

<p dir="auto">You can also install from master branch using:</p>
<div data-snippet-clipboard-copy-content="pip install git+git://github.com/KomputeProject/kompute.git@master"><pre><code>pip install git+git://github.com/KomputeProject/kompute.git@master
</code></pre></div>
<p dir="auto">For further details you can read the <a href="https://kompute.cc/overview/python-package.html" rel="nofollow">Python Package documentation</a> or the <a href="https://kompute.cc/overview/python-reference.html" rel="nofollow">Python Class Reference documentation</a>.</p>

<p dir="auto">The build system provided uses <code>cmake</code>, which allows for cross platform builds.</p>
<p dir="auto">The top level <code>Makefile</code> provides a set of optimized configurations for development as well as the docker image build, but you can start a build with the following command:</p>

<p dir="auto">You also are able to add Kompute in your repo with <code>add_subdirectory</code> - the <a href="https://github.com/KomputeProject/kompute/blob/7c8c0eeba2cdc098349fcd999102bb2cca1bf711/examples/android/android-simple/app/src/main/cpp/CMakeLists.txt#L3">Android example CMakeLists.txt file</a> shows how this would be done.</p>
<p dir="auto">For a more advanced overview of the build configuration check out the <a href="https://kompute.cc/overview/build-system.html" rel="nofollow">Build System Deep Dive</a> documentation.</p>

<p dir="auto">We appreciate PRs and Issues. If you want to contribute try checking the &#34;Good first issue&#34; tag, but even using Kompute and reporting issues is a great contribution!</p>


<ul dir="auto">
<li>Testing
<ul dir="auto">
<li>GTest</li>
</ul>
</li>
<li>Documentation
<ul dir="auto">
<li>Doxygen (with Dot)</li>
<li>Sphynx</li>
</ul>
</li>
</ul>

<ul dir="auto">
<li>Follows Mozilla C++ Style Guide <a href="https://www-archive.mozilla.org/hacking/mozilla-style-guide.html" rel="nofollow">https://www-archive.mozilla.org/hacking/mozilla-style-guide.html</a>
<ul dir="auto">
<li>Uses post-commit hook to run the linter, you can set it up so it runs the linter before commit</li>
<li>All dependencies are defined in vcpkg.json</li>
</ul>
</li>
<li>Uses cmake as build system, and provides a top level makefile with recommended command</li>
<li>Uses xxd (or xxd.exe windows 64bit port) to convert shader spirv to header files</li>
<li>Uses doxygen and sphinx for documentation and autodocs</li>
<li>Uses vcpkg for finding the dependencies, it&#39;s the recommended set up to retrieve the libraries</li>
</ul>
<p dir="auto">If you want to run with debug layers you can add them with the <code>KOMPUTE_ENV_DEBUG_LAYERS</code> parameter as:</p>
<div data-snippet-clipboard-copy-content="export KOMPUTE_ENV_DEBUG_LAYERS=&#34;VK_LAYER_LUNARG_api_dump&#34;"><pre><code>export KOMPUTE_ENV_DEBUG_LAYERS=&#34;VK_LAYER_LUNARG_api_dump&#34;
</code></pre></div>

<p dir="auto">To update the documentation you will need to:</p>
<ul dir="auto">
<li>Run the gendoxygen target in the build system</li>
<li>Run the gensphynx target in the build-system</li>
<li>Push to github pages with <code>make push_docs_to_ghpages</code></li>
</ul>

<p dir="auto">Running the unit tests has been significantly simplified for contributors.</p>
<p dir="auto">The tests run on CPU, and can be triggered using the ACT command line interface (<a href="https://github.com/nektos/act">https://github.com/nektos/act</a>) - once you install the command line (And start the Docker daemon) you just have to type:</p>
<div data-snippet-clipboard-copy-content="$ act

[Python Tests/python-tests] 🚀  Start image=axsauze/kompute-builder:0.2
[C++ Tests/cpp-tests      ] 🚀  Start image=axsauze/kompute-builder:0.2
[C++ Tests/cpp-tests      ]   🐳  docker run image=axsauze/kompute-builder:0.2 entrypoint=[&#34;/usr/bin/tail&#34; &#34;-f&#34; &#34;/dev/null&#34;] cmd=[]
[Python Tests/python-tests]   🐳  docker run image=axsauze/kompute-builder:0.2 entrypoint=[&#34;/usr/bin/tail&#34; &#34;-f&#34; &#34;/dev/null&#34;] cmd=[]
..."><pre><code>$ act

[Python Tests/python-tests] 🚀  Start image=axsauze/kompute-builder:0.2
[C++ Tests/cpp-tests      ] 🚀  Start image=axsauze/kompute-builder:0.2
[C++ Tests/cpp-tests      ]   🐳  docker run image=axsauze/kompute-builder:0.2 entrypoint=[&#34;/usr/bin/tail&#34; &#34;-f&#34; &#34;/dev/null&#34;] cmd=[]
[Python Tests/python-tests]   🐳  docker run image=axsauze/kompute-builder:0.2 entrypoint=[&#34;/usr/bin/tail&#34; &#34;-f&#34; &#34;/dev/null&#34;] cmd=[]
...
</code></pre></div>
<p dir="auto">The repository contains unit tests for the C++ and Python code, and can be found under the <code>test/</code> and <code>python/test</code> folder.</p>
<p dir="auto">The tests are currently run through the CI using Github Actions. It uses the images found in <code>docker-builders/</code>.</p>
<p dir="auto">In order to minimise hardware requirements the tests can run without a GPU, directly in the CPU using <a href="https://github.com/google/swiftshader">Swiftshader</a>.</p>
<p dir="auto">For more information on how the CI and tests are setup, you can go to the <a href="https://kompute.cc/overview/ci-tests.html" rel="nofollow">CI, Docker and Tests Section</a> in the documentation.</p>

<p dir="auto">This project started after seeing that a lot of new and renowned ML &amp; DL projects like Pytorch, Tensorflow, Alibaba DNN, Tencent NCNN - among others - have either integrated or are looking to integrate the Vulkan SDK to add mobile (and cross-vendor) GPU support.</p>
<p dir="auto">The Vulkan SDK offers a great low level interface that enables for highly specialized optimizations - however it comes at a cost of highly verbose code which requires 500-2000 lines of code to even begin writing application code. This has resulted in each of these projects having to implement the same baseline to abstract the non-compute related features of the Vulkan SDK. This large amount of non-standardised boiler-plate can result in limited knowledge transfer, higher chance of unique framework implementation bugs being introduced, etc.</p>
<p dir="auto">We are currently developing Kompute not to hide the Vulkan SDK interface (as it&#39;s incredibly well designed) but to augment it with a direct focus on the Vulkan SDK&#39;s GPU computing capabilities. <a href="https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a" rel="nofollow">This article</a> provides a high level overview of the motivations of Kompute, together with a set of hands on examples that introduce both GPU computing as well as the core Kompute architecture.</p>
</article></div></div>
  </body>
</html>
