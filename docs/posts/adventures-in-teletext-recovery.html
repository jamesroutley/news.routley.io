<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zxnet.co.uk/teletext/recovery/">Original</a>
    <h1>Adventures in Teletext Recovery</h1>
    
    <div id="readability-page-1" class="page"><div id="container">
            
            <div id="content">
                <p>This article details my adventures in recovering teletext data from VHS tapes. I&#39;m just starting out and still figuring out what to do, so please don&#39;t take this as an expert guide to teletext recovery. However I hope that by writing down the hurdles I&#39;ve faced and what I&#39;ve learnt while it&#39;s still fresh in my mind I can create the beginner&#39;s guide I wish I&#39;d had, and document my future experiments.</p>
                <p>At the heart of all teletext recovery is the <a href="https://github.com/ali1234/vhs-teletext"><em>vhs-teletext</em> software by <em>Alistair Buxton</em></a>.</p>
                <p>This suite of scripts works by capturing raw samples of the vertical blanking intervals from a video signal using a video capture card on Linux. In a clean broadcast quality television signal these signals would have clearly defined bits, but as the bandwidth of a VHS recording is lower than that of broadcast television the individual teletext bits are blurred together and can&#39;t simply be fed into a normal teletext decoder.</p>
                <p>The <em>vhs-teletext</em> repository explains the recovery principle in more detail, but essentially it looks at the smeared grey signal and attempts to determine what pattern of bits was originally transmitted to result in that output after the filtering in the VHS recording process.</p>
                <p>The original version of the software used a &#39;guessing algorithm&#39;, but the current &#39;version 2&#39; software works by matching patterns to a library of pre-generated blurry signals. This approach enables the use of GPU acceleration on a compatible <em>Nvidia</em> graphics card which reduces the time taken to process a recovery enormously.</p>
                
                <h3 id="setup">My setup</h3>
                <p>Ideally you would install a suitable graphics card into your video capturing PC and do the entire process under <em>Linux</em>. I only own one compatible graphics card and it is installed in my main <em>Windows 7</em> PC which I don&#39;t want to dual boot, so I am capturing the signals on a capture PC and then pulling them over the network to process them.</p>
                <p>The recovery software required some hacking to be able to run on <em>Windows</em> at all, and getting the environment set up with the <em>Nvidia CUDA</em> SDK and all the required <em>python</em> modules was somewhat awkward so I recommend sticking to <em>Linux</em> for the whole process if you can.</p>
                
                <p>For my initial attempts at VHS teletext recovery I fed composite video from an old <em>Saisho VRS 5000X</em> that has a simple phono composite output into an ancient <em>Pentium 4</em> PC via a <em>Mercury</em> TV capture card (<em>Phillips SAA7134</em> chip) which necessitated writing a new config file for vhs-teletext and modifying some of the scripts slightly to handle the fact that this chip outputs 17 VBI lines per field instead of 16. This produced promising but disappointing results. The capture script showed huge numbers of possible dropped frames.</p>
                
                <p>General wisdom dictates that a higher sample rate <em>Brooktree/Conexant 8x8</em> based chipset is required, so I ordered a couple of suitable cards from <em>ebay</em> and while waiting for those I renovated another broken PC to use as a VHS capturing box.</p>
                <p>Suitable <em>Hauppauge WinTV</em> capture cards can be identified by compating their model number to <a href="http://www.hauppauge.com/pages/support/support_pci_boards.html">this webpage</a>. There are also the PCI <em>Hauppauge ImpactVCB</em> cards (NOT PCI-e models).</p>
                
                <p>As soon as the new card arrived I installed it and ran a capture of my best tape in the new rig. The results were disappointingly similar to before.</p>
                
                <p>After some fruitless fiddling with settings in the recovery config file I decided that there was too much interference on the video signal and no amount of messing with settings would be able to reliably extract teletext. Adjusting the settings would make some lines improve but make others worse. The data was there to be extracted but no setting could do it all.</p>
                
                <p>At this point I abandoned the <em>Saisho</em> VCR and made up an appropriate SCART to phono composite cable to use my newer <em>Samsung SV-661B</em>. I ran the same tape again through that and the difference is obvious. The resulting output after deconvolution is almost error free and can be played back as a teletext signal as-is with very few errors.</p>

                <p>The two images show the output of the two VCRs along with a small sample of the raw blanking interval signal. I believe that the strong periodic interference disrupts the recovery script&#39;s ability to discern the exact position of the clock run in and framing code, and therefore can&#39;t calculate the correct offset to each subsequent data bit.
                    </p><div>
                        <p><a href="https://zxnet.co.uk/teletext/recovery/bad.jpg"><img src="https://zxnet.co.uk/teletext/recovery/bad.jpg" title="Saisho VCR"/></a>
                        </p>
                        <p><a href="https://zxnet.co.uk/teletext/recovery/good.jpg"><img src="https://zxnet.co.uk/teletext/recovery/good.jpg" title="Samsung VCR"/></a>
                        </p>
                    </div>
                
                <h4 id="hardware">I am now using the following hardware.</h4><p>
                Capture machine:
                </p><ul>
                    <li><em>Intel Core2Duo E6600</em> CPU</li>
                    <li><em>Hauppauge ImpactVCB 64405</em> PCI video capture board (<em>Conexant Fusion 878A</em> chip)</li>
                    <li><em>Debian 9</em></li>
                </ul><p>
                Recovery machine:
                </p><ul>
                    <li><em>Intel Core i7-3820</em> CPU</li>
                    <li><em>Nvidia GTX 660</em> graphics card</li>
                    <li><em>Windows 7</em></li>
                </ul>
                
                <h3>Ongoing updates</h3>
                <h5 id="2018-12-22">2018-12-22</h5>
                <s><p>I have created <a href="https://github.com/ZXGuesser/vhs-teletext">a fork of the <em>vhs-teletext</em> software</a> with modified code I am using on <em>Windows</em>. I have also begun butchering the scripts further to begin supporting more packet types (currently only the t42 processing scripts. The deconvolved data will not be useful.)</p></s>
                <h5 id="2018-12-23">2018-12-23</h5>
                <s><p>Created <a href="https://github.com/ZXGuesser/vhs-teletext/commit/3340a896ff004dfa09e334e34f6dd38e5b9fa733">a script</a> to slice a squashed recovery into its constituent subpages as a directory of separate .t42 files.</p></s>
                <h5 id="2018-12-27">2018-12-27</h5>
                <p>Further experiments show that the 878A chipset card works fine on the slower <em>Pentium 4</em> PC with no dropped frames. Testing the <em>saa7134</em> chipset card into the <em>Core2Duo</em> PC was inconclusive as an examination of the captured vbi data shows a curious lack of horizontal sync for the first few lines of each field.</p>
                <h5 id="2018-12-30">2018-12-30</h5>
                <p>Created a three hour pattern training tape using <a href="https://github.com/ali1234/raspi-teletext"><em>raspi-teletext</em></a> on a spare <em>Raspberry Pi</em> with the output from <em>training --generate</em>.</p>
                <p>After capturing the vbi data back into the capture PC as <em>training.vbi</em> I processed it with <em>training --train training.vbi &gt; training.dat.orig</em>. I captured a full three hours of the training signal which resulted in a 43 GB intermediate training file.</p>
                <p>I split the intermediate file using <em>training --split training.dat.orig</em> which generates 256 files named <em>training.nn.dat</em> where nn is two hexadecimal digits.</p>
                <p>A small windows batch script iterated over the 256 .dat files executing <em>training --sort training.nn.dat</em> followed by <em>training --squash training.nn.dat.sorted</em> and finally joining all the resulting files <em>training.nn.dat.sorted.squashed</em> into a single <em>training.dat.squashed</em>.</p>
                <p>Finally I was able to generate the three pattern data files with <em>training --full training.dat.squashed</em>, <em>training --parity training.dat.squashed</em>, and <em>training --hamming training.dat.squashed</em>.</p>
                <p>I don&#39;t believe that creating my own Hamming and parity patterns was necessary and has any measurable effect on the quality of the recovery process, however it was necessary to generate a <em>full.dat</em> pattern file for 8-bit data recovery experiments as that is not included in the original distribution of the software.</p>
                <p>Some modifications to <em>teletext/vbi/line.py</em> allows the deconvolution process to recover the 8-bit data in the Broadcast Service Data Packet and any datacast packets, and also treat Hamming 24/18 coded data as 8-bit data, as the code previously recovered everything as 7-bit text with parity resulting in additional corruption.</p>
                <h5 id="2018-12-31">2018-12-31</h5>
                <p>After wasting the previous evening and staying up until the early hours of the morning doing futile experiments in an attempt to implement a better process for matching Hamming 24/18 coded bits, I came up with another idea for recovering the data in enhancement packets.</p>
                <p>With the implementation of 8-bit decoding of the 39 bytes which make up the Hamming 24/18 coded data I was now able to extract these packets with only the expected occasional errors inherent in the recovery process. These results were already promising with the standard squashing algorithm, but still not error free on any of the level 2 pages I have found on my tapes so far.</p>
                <p>I noticed while looking at the unsquashed recovered data in a hex editor and my <a href="https://github.com/ZXGuesser/teletext-decoder"><em>Teletext Packet Analyser</em></a> script, that in general all the enhancement triplets were recovered without errors at least once, but because the squashing process simply does a modal average it was picking damaged ones in the squashed output.</p>
                <p>Also, because it averages bytewise, it would pick bad bytes within a triplet. The 24 bits of coded triplet data are one value and should be treated as a single unit</p>
                <p>My solution was to order the 74 possible packets which can be associated with a subpage (X/0, X/1-X/25, X/26/0-X/26/15, X/27/0-X/27/15, and X/28/0-X/28/15) such that the ones which can be considered bytewise are separated from the ones containing triplet data.</p>
                <p>I created a new <em>to_triplets</em> function in the packet class which takes the 42 bytes and converts them to fourteen 24-bit words. This includes the Magazine Row Address Group and Designation Code, which are treated as the first 24 bit value.</p>
                <p>For the remaining 13 words the code also performs a Hamming 24/18 decode, and if there are any errors returns <em>NaN</em>. This allows the modal average performed on these packets to disregard and triplet which contains an error and find the most common error free triplet. This enables it to create a composite of the working parts of many damaged copies of the packet.</p>
                <p>If there are no completely error free copies of a triplet in the deconvolved data this will introduce a new error by omitting the triplet entirely. It is possible to only reject unrecoverable errors found in the Hamming decode, bit I found this to produce worse output than also rejecting recoverable errors.</p>
                <p>Where no error free copy of a triplet is found it may be possible that one of the copies with a recoverable error is sufficient to repair the packet, but I haven&#39;t created any code to do this automatically yet.</p>
                <h5 id="2019-12-15">2019-12-15</h5>
                <p>I have neglected this page over the past year, and there have been many developments to the <em>vhs-recovery</em> software in that time. This renders some of my previous updates and fork of the <em>‘v2’</em> code irrelevant.</p>
                <p>The current state of the scripts is that the new <em>‘v3’</em> version of the scripts for <em>Python3</em> runs on Windows ‘out-of-the-box’, deconvolves and squashes more packet types, decodes broadcast service data packets, and includes a split command which can export a single file per subpage.</p>
            </div>
            
        </div></div>
  </body>
</html>
