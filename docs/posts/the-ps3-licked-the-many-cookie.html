<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://darkcephas.github.io/ps3_failed/ps3_failed.html">Original</a>
    <h1>The PS3 Licked the Many Cookie</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    <p>
      This post is inspired by
      <a href="https://raphlinus.github.io/gpu/2025/03/21/good-parallel-computer.html#cell" target="_blank"> I want a
        good parallel computer</a>
    </p>

    <p>
      It is important to understand why the <a href="https://en.wikipedia.org/wiki/PlayStation_3" target="_blank">
        PS3</a> failed. The perspective here was juniorish dev working on simulation and
      rendering in triple-A. I remember less than I knew and knew less than most!
    </p>
    <p><img src="https://darkcephas.github.io/ps3_failed/ps3_og_image.png" alt="OG PS3"/></p><p>
      <i>However</i>, what I can provide here is the hindsight of someone who actually developed and shipped titles
      <sup>1</sup> on
      the PS3.
      I wanted the PS3 to succeed. To be more specific; I wanted <a href="https://en.wikipedia.org/wiki/Manycore_processor" target="_blank">Many-Core</a> to succeed.
    </p>
    <p><b>
      The PS3 failed developers because it was an excessively <a href="https://en.wikipedia.org/wiki/Heterogeneous_computing" target="_blank"> heterogenous</a> computer;
    </b>
    and low level heterogeneous compute resists composability.<sup>2</sup></p><h2>
      More like Multicore than Many
    </h2>
    <p>
      The primary characteristic of <a href="https://en.wikipedia.org/wiki/Manycore_processor" target="_blank">Many-Core</a> is, by virtue of the name, the high core count. Many-core is simply a tradeoff
      that enables wide parallelism through more explicit  (programmer) control.
      </p>

    <h2>
      Computationally Weak Components
    </h2>

    <p>
      The claim from wiki is a single SPE has 25 GFlops and the <a href="https://en.wikipedia.org/wiki/RSX_Reality_Synthesizer" target="_blank"> PS3 GPU</a> has 192 GFlops.<b> If
        you
        absolutely maxed out your SPE
        usage you would still not even be close to the power of the underpowered PS3 GPU. </b> For contrast the Xbox360
      gpu had
      240
      GFlops. The GPU of the PS3 has separate Vertex and Pixel shading. In contrast, the XBox360 was shared
      computational resources so it could load balance between heavy vertex shading vs heavy pixel shading. (Examples
      here would be <a href="https://en.wikipedia.org/wiki/Skeletal_animation" target="_blank">character skinning</a>
      vs UI rendering)
    </p>


    <p><img src="https://darkcephas.github.io/ps3_failed/ps3_vs_xbox.png" alt="PS3 heterogeneous diagram"/></p><p>
      As a game developer these Flops numbers reflect the experience of developing on these respective platforms. This
      was particularly noticeable in something like post processing where the demands of the vertex unit are very low
      (large quads).
    </p>

    <p>
      Due to the weakness of the GPU vertex unit developers would use the SPEs to do skinning.
      The pixel shading unit did not have constants. So one would also have to do shader patching on the SPEs before
      these
      programs could be sent to the GPU. All of these things require synchronization between the CPU, SPE and GPU and
      interact with workload balancing. In retrospect I also assume that dynamic branching in the shader was either
      impossible or prohibitive so this is why everyone did excessive shader permutations. This means 10s of megabytes
      of shaders. Again contrast this with the XBOX360 which supported wave
      operations <sup>3</sup> and I even used this feature back in the day. <b> Because each component of the PS3 is
        weak on its own they all must be employed in concert to compete with (the) less heterogeneous platforms. </b>
    </p>

    <h2>
      Computer Not Super
    </h2>

    <p>
      While the Cell could behave more like a supercomputer I saw it mostly used more like generic GPU compute. I never
      saw production code that did anything but dispatch N jobs from the PPE. I never saw direct inter SPE communication
      even though I recall such a thing was possible (mailboxes). This is similar to how GPU inter workgroup workloads
      are more rare and difficult.
    </p>

    <p>
      The hetrogenous nature was everywhere. Even the PPE was quite different from an SPE. The SPEs had only vector
      registers; the PPE had fp, gp, and vector
      registers. Is this really bad? No<sup>4</sup>, but it makes everything more heterogeneous and therefore more
      complex. Getting
      maximum performance out of these SPE units means that you were likely doing async DMAs while also doing compute
      work. <b>These nuances could be a fun challenge for a top programmer but ends of being more of an obstacle to
        development for game studios. </b>
    </p>

    <h2>
      Sharp Edges
    </h2>

    <p>
      The PS3 had 512 Mb total memory but 256 MB was dedicated to graphics and only had REDACTED Mb/s access from the
      CPU. So
      this means in addition to the 256 MB purely for graphics you would also have to dedicate system memory for
      anything that was written to and from the GPU. The point here is inflexibility and heterogeneous nature.
    </p>

    <p>
      The PS3 had cool features but these were never general purpose and could only be exploited by careful attention to
      detail and sometimes significant engine changes. I recall using depth bounds for screen space shadowing and
      one could probably use it for a few other similar gpu techniques (lighting). There was also the alluring double z
      writes which is a one-off for depth maps if you dont actually use a pixel shader. I don&#39;t recall all the edges,
      but they were sharp and it meant performance cliffs if one strayed off them. The next section covers the sharpest
      edge of them all.
    </p>

    <h2 id="local_memory_id">
      The Challenge of Local memory
    </h2>

    <p>
      Of course the challenge that everyone knows about the SPEs is the constraint of memory access to local memory. You
      got 256Kb but in reality once you factored in stack and program you were probably down to 128Kb. This
      computational model is far more restrictive than even modern GPU compute where at least there you can access
      storage buffers directly.
    </p>

    <p>
      <b>Most code and algorithms cannot be trivially ported to the SPE. </b>
      C++ virtual functions and methods will not work out of the box. C++ encourages dynamic allocation of objects but
      these can point to anywhere in main memory. You would
      need to map pointer addresses from PPE to SPE to even attempt running a normal c++ program on the SPE. Also null
      (address 0x0) points to the start of local memory and is not a segfault to load from it.
    </p>

    <p>
      So, instead of running generic code on the SPE, what developers did was write handcrafted SPE friendly code for
      heavy
      but parallelizable parts of their engine. <b>With enough talent and investment you can eke out the full compute
        power of the PS3.<sup>5</sup> </b> Of course this is maybe easier as a <a href="https://en.wikipedia.org/wiki/Video_game_developer#Types" target="_blank">first party developer</a> as you
      can at least
      focus on this exotic hardware and craft your engine and game features around the type compute available. This is
      why <a href="https://en.wikipedia.org/wiki/Uncharted_3:_Drake%27s_Deception" target="_blank">Naughty Dog</a>
      famously came so close to showing us the full potential of the console.

    </p><figure>
      <figcaption>
        <a href=" https://www.mobygames.com/game/53611/uncharted-3-drakes-deception/screenshots/ps3/531831/" target="_blank">Uncharted 3: Mobygames image</a>
      </figcaption>
      <img src="https://cdn.mobygames.com/screenshots/4207546-uncharted-3-drakes-deception-playstation-3-is-that-a-well-in-the.jpg" alt="Uncharted 3 PS3 credit mobygames"/>
    </figure>
    

    <h2>
      What could have been
    </h2>

    <p>
      Had the PS3 been what was originally designed it would have been a much more exotic but much less heterogeneous
      machine.
    </p>

    <p>
      The original design was approximately 4 Cell processors with high frequencies. Perhaps massaging this design would
      have led to very homogenous high performance Many-Core architecture. At more than 1 TFlop of general purpose
      compute it would have been a beast and not a gnarly beast but a sleek smooth uniform tiger.
    </p>
    <p>
      One has to actually look at the PS3 as the
      <a href="https://devblogs.microsoft.com/oldnewthing/20091201-00/?p=15843" target="_blank">licked cookie</a>
      <sup>6</sup> of
      Many-Core designs. This half-baked, half-hearted attempt became synonymous with the failure of Many-Core. <b>
        I
        used to think that PS3 set back Many-Core for decades, now I wonder if it simply killed it forever. </b>
    </p>



    <h2>
      Refs
    </h2>

    <p>
      <sup> 1 </sup> <a href="https://www.mobygames.com/person/516875/peter-mcneeley/credits/" target="_blank"> Some of
        the titles I have worked on. </a>
    </p>

    <p>
      <sup> 2 </sup> I will not provide a proof or reference but the mathematical issue is that the space is not covered
      uniformly. This low level composability problem is often seen instruction selection when writing assembly.
    </p>

    <p>
      <sup> 3 </sup>The XBox360 gpu had wave operations like ifany/ifall that are similar to modern control flow
      subgroup operations.
    </p>

    <p>
      <sup> 4 </sup> The fact that it was only vectors on the SPEs was present to the programmer due to loads/stores
      having to also be vector aligned
    </p>

    <p>
      <sup> 5 </sup> <a href=" https://www.neogaf.com/threads/ps3-games-list-spe-usages.184843/" target="_blank"> PS3
        SPE usage</a> : it is clear that some games had higher utilization than others.
    </p>

    <p>
      <sup> 6 </sup>I am not sure my usage fits Raymond&#39;s narrow specifications.
    </p>


    <p>
      <a href="https://classic.copetti.org/writings/consoles/playstation-3/" target="_blank"> classic.copetti.org </a>
      Source for images and some technical specifications.
    </p>
  </div></div>
  </body>
</html>
