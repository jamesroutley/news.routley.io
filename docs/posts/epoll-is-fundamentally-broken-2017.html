<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/">Original</a>
    <h1>Epoll is fundamentally broken (2017)</h1>
    
    <div id="readability-page-1" class="page"><article>


<p><h2>I/O multiplexing part #3</h2></p>

<p>20 February 2017</p>



<p>In previous articles we talked about:</p>
<ul>
<li><a href="https://idea.popcount.org/2016-11-01-a-brief-history-of-select2/">The history of the Select(2) syscall</a></li>
<li><a href="https://idea.popcount.org/2017-01-06-select-is-fundamentally-broken/">Select(2) being fundamentally broken</a></li>
</ul>
<p>This time we&#39;ll focus on Linux&#39;s <code>select(2)</code> successor - the
<a href="http://man7.org/linux/man-pages/man7/epoll.7.html"><code>epoll(2)</code></a> I/O
multiplexing syscall.</p>
<p>Epoll is relatively young. It was
<a href="http://www.xmailserver.org/linux-patches/nio-improve.html">created by Davide Libenzi</a>
in <a href="https://lwn.net/Articles/14168/">2002</a>. For comparison: Windows
did
<a href="https://en.wikipedia.org/wiki/Input/output_completion_port">IOCP in 1994</a>
and FreeBSD&#39;s
<a href="https://en.wikipedia.org/wiki/Kqueue">kqueue was introduced in July 2000</a>. Unfortunately,
even though epoll is the youngest in the advanced IO multiplexing
family, it&#39;s the worse in the bunch.</p>
<h2 id="comparison-with-devpoll">Comparison with /dev/poll</h2>
<p><a href="https://en.wikipedia.org/wiki/Bryan_Cantrill">Bryan Cantrill</a> of
Joyent is known for bashing <code>epoll()</code>.  Here&#39;s one of the more
entertaining interviews:</p>
<p>
<iframe width="640" height="356" src="https://www.youtube.com/embed/l6XQUciI-Sc?start=3362" frameborder="0" allowfullscreen=""></iframe>
</p>

<p>He mentions two defects.</p>
<p>First he describes &#34;a fatal flaw, that is subtle&#34; in the Solaris
<code>/dev/poll</code> model. He starts by describing the &#34;thundering herd&#34;
problem (which
<a href="https://idea.popcount.org/2017-01-06-select-is-fundamentally-broken/">we discussed earlier</a>). Then
he moves on to the real issue. In a multithreaded scenario, when the
<code>/dev/poll</code> descriptor is shared, it is impossible to deliver events
on one file descriptor to precisely one worker thread.  He
explains that band aids to level triggered <code>/dev/poll</code> model and naive
edge-triggered won&#39;t work in multithreaded case<sup id="fnref:2"><a href="#fn:2" rel="footnote">1</a></sup>.</p>
<p>This argument is indeed subtle, but since <code>epoll</code> has semantics close
to <code>/dev/poll</code>, it&#39;s safe to say it wasn&#39;t designed to work in
multithreaded scenarios.</p>
<p>In the video Mr Cantrill raised a second argument against <code>epoll</code>: the
events registered in epoll aren&#39;t associated with file descriptor, but
with the underlying kernel object referred to by the file descriptor
(let&#39;s call this the file <em>description</em>). He mentions the &#34;stunning&#34;
effect of forking and closing an fd. We will leave this problem for
now and describe it in another blog post.</p>
<h2 id="why-the-critique">Why the critique?</h2>
<p>Most of the <code>epoll</code> critique is based on two fundamental design
issues:</p>
<p>1) Sometimes it is desirable to scale application by using multi
threading.  This was not supported by early implementations of <code>epoll</code>
and was fixed by <code>EPOLLONESHOT</code> and <code>EPOLLEXCLUSIVE</code> flags.</p>
<p>2) Epoll registers the <em>file descripton</em>, the kernel data structure,
not <em>file descriptor</em>, the userspace handler pointing to it.</p>
<p>The debate is heated because it&#39;s technically possible to avoid both
pitfalls with careful defensive programming. If you can you
should avoid using epoll for load balancing across
threads. Avoid sharing epoll file descriptor across
threads.  Avoid sharing epoll-registered file
descriptors. Avoid forking, and if you must: close all
epoll-registered file descriptors before calling <code>execve</code>.  Explicitly
deregister affected file descriptors from epoll set before calling
<code>dup</code>/<code>dup2</code>/<code>dup3</code> or <code>close</code>.</p>
<p>If you have simple code and follow the advice above you might be fine. The
problem starts when your epoll program gets complex.</p>
<p>Let&#39;s dig deeper. In this blog post I&#39;ll focus on the load balancing
argument.</p>
<h2 id="load-balancing">Load balancing</h2>
<p>There are two distinct load balancing scenarios:</p>
<ul>
<li>scaling out <code>accept()</code> calls for a single bound TCP socket</li>
<li>scaling usual <code>read()</code> calls for large number of connected sockets</li>
</ul>
<h2 id="scaling-out-accept">Scaling out accept()</h2>
<p>Sometimes it&#39;s necessary to serve lots of very short
TCP connections. A high throughput HTTP 1.0 server is one such
example. Since the rate of inbound connections is high, you want to
distribute the work of <code>accept()</code>ing connections across multiple
CPU&#39;s.</p>
<p>This is a real problem happening in large deployments. Tom Herbert
reported an application handling
<a href="https://lwn.net/Articles/542629/">40k connections per second</a>.  With
such a volume it does makes sense to spread the work across cores.</p>
<p>But it&#39;s not that simple. Up until kernel 4.5 it wasn&#39;t possible to use
epoll to scale out accepts.</p>
<h3 id="level-triggered-unnecessary-wake-up">Level triggered - unnecessary wake-up</h3>
<p>A naive solution is to have a single epoll file
descriptor shared across worker threads. This won&#39;t work well,
neither will sharing bound socket file descriptor and registering it
in each thread to unique epoll instance.</p>
<p>This is because &#34;level triggered&#34; (aka: normal) epoll inherits the
&#34;thundering herd&#34; semantics from <code>select()</code>. Without special flags, in
level-triggered mode, all the workers will be woken up on each and
every new connection. Here&#39;s an example:</p>
<ol>
<li><strong>Kernel:</strong> Receives a new connection.</li>
<li><strong>Kernel:</strong> Notifies two waiting threads A and B. Due to &#34;thundering herd&#34; behavior with level-triggered notifications kernel must wake up both.</li>
<li><strong>Thread A:</strong> Finishes <code>epoll_wait()</code>.</li>
<li><strong>Thread B:</strong> Finishes <code>epoll_wait()</code>.</li>
<li><strong>Thread A:</strong> Performs <code>accept()</code>, this succeeds.</li>
<li><strong>Thread B:</strong> Performs <code>accept()</code>, this fails with EAGAIN.</li>
</ol>
<p>Waking up &#34;Thread B&#34; was completely unnecessary and wastes precious
resources. Epoll in level-triggered mode scales out poorly.</p>
<h3 id="edge-triggered-unnecessary-wake-up-and-starvation">Edge triggered - unnecessary wake-up and starvation</h3>
<p>Okay, since we ruled out naive level-triggered setup, maybe &#34;edge
triggered&#34; could do better?</p>
<p>Not really. Here is a possible pessimistic run:</p>
<ol>
<li><strong>Kernel:</strong> Receives first connection. Two threads, A and B, are waiting. Due to &#34;edge triggered&#34; behavior only one is notified - let&#39;s say thread A.</li>
<li><strong>Thread A:</strong> Finishes <code>epoll_wait()</code>.</li>
<li><strong>Thread A:</strong> Performs <code>accept()</code>, this succeeds.</li>
<li><strong>Kernel:</strong> The accept queue is empty, the event-triggered socket moved from &#34;readable&#34; to &#34;non readable&#34;, so the kernel must re-arm it.</li>
<li><strong>Kernel:</strong> Receives a second connection.</li>
<li><strong>Kernel:</strong> Only one thread is currently waiting on the <code>epoll_wait()</code>. Kernel wakes up Thread B.</li>
<li><strong>Thread A:</strong> Must perform <code>accept()</code> since it does not know if kernel received one or more connections originally. It hopes to get EAGAIN, but gets another socket.</li>
<li><strong>Thread B:</strong> Performs <code>accept()</code>, receives EAGAIN. This thread is confused.</li>
<li><strong>Thread A:</strong> Must perform <code>accept()</code> again, gets EAGAIN.</li>
</ol>
<p>The wake-up of Thread B was completely unnecessary and is
confusing. Additionally, in edge triggered mode it&#39;s hard to avoid
starvation:</p>
<ol>
<li><strong>Kernel:</strong> Receives two connections. Two threads, A and B, are waiting. Due to &#34;edge triggered&#34; behavior only one is notified - let&#39;s say thread A.</li>
<li><strong>Thread A:</strong> finished <code>epoll_wait()</code>.</li>
<li><strong>Thread A:</strong> performs <code>accept()</code>, this succeeds</li>
<li><strong>Kernel:</strong> Receives third connection. The socket was &#34;readable&#34;, still is &#34;readable&#34;. Since we are in &#34;edge triggered&#34; mode, no event is emitted.</li>
<li><strong>Thread A:</strong> Must perform <code>accept()</code>, hopes to get EGAIN, but gets another socket.</li>
<li><strong>Kernel:</strong> Receives fourth connection.</li>
<li><strong>Thread A:</strong> Must perform <code>accept()</code>, hopes to get EGAIN, but gets another socket.</li>
</ol>
<p>In this case the socket moved only once from &#34;non-readable&#34; to
&#34;readable&#34; state. Since the socket is in edge-triggered mode, the
kernel will wake up epoll exactly once. In this case all the
connections will be received by Thread A and load balancing won&#39;t be
achieved.</p>
<h3 id="correct-solution">Correct solution</h3>
<p>There are two workarounds.</p>
<p>The best and the only scalable approach is to use recent Kernel 4.5+ and
use level-triggered events with <code>EPOLLEXCLUSIVE</code> flag. This will
ensure only one thread is woken for an event, avoid &#34;thundering herd&#34;
issue and scale properly across multiple CPU&#39;s</p>
<p>Without <code>EPOLLEXCLUSIVE</code>, similar behavior it can be emulated with
edge-triggered and <code>EPOLLONESHOT</code>, at a cost of one extra <code>epoll_ctl()</code>
syscall after each event. This will distribute load across multiple
CPU&#39;s properly, but at most one worker will call <code>accept()</code>
at a time, limiting throughput.</p>
<ol>
<li><strong>Kernel:</strong> Receives two connections. Two threads, A and B, are waiting. Due to &#34;edge triggered&#34; behavior only one is notified - let&#39;s say thread A.</li>
<li><strong>Thread A:</strong> Finishes epoll_wait().</li>
<li><strong>Thread A:</strong> Performs <code>accept()</code>, this succeeds.</li>
<li><strong>Thread A:</strong> Performs <code>epoll_ctl(EPOLL_CTL_MOD)</code>, this will reset the <code>EPOLLONESHOT</code> and re-arm the socket.</li>
</ol>
<p>It&#39;s worth noting there are other ways to scale <code>accept()</code> without
relying on epoll. One option is to use
<a href="https://lwn.net/Articles/542629/"><code>SO_REUSEPORT</code></a> and create multiple
listen sockets sharing the same port number. This approach has
problems though - when one of the file descriptors is closed, the
sockets already waiting in the accept queue will be dropped. Read more
in this
<a href="https://engineeringblog.yelp.com/2015/04/true-zero-downtime-haproxy-reloads.html">Yelp blog post</a>
and this <a href="https://lwn.net/Articles/542866/">LWN comment</a>.</p>
<p><a href="https://kernelnewbies.org/Linux_4.4">Kernel 4.4</a>
<a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=70da268b569d32a9fddeea85dc18043de9d89f89">introduced <code>SO_INCOMING_CPU</code></a>
to further improve locality of <code>SO_REUSEPORT</code> sockets. I wasn&#39;t
able to find a good documentation of this very new feature.</p>
<p>Even better, <a href="https://kernelnewbies.org/Linux_4.5">kernel 4.5</a>
introduced <code>SO_ATTACH_REUSEPORT_CBPF</code> and <code>SO_ATTACH_REUSEPORT_EBPF</code>
<a href="http://man7.org/linux/man-pages/man7/socket.7.html">socket options</a>. When
used properly, with a bit of magic, it should be possible to substitute
<code>SO_INCOMING_CPU</code> and overcome the usual <code>SO_REUSEPORT</code> dropped
connections on rebalancing problem.</p>
<h2 id="scaling-out-read">Scaling out read()</h2>
<p>Apart from scaling <code>accept()</code> there is a second use case for scaling
<code>epoll</code> across many cores. Imagine a situation when you have a large
number of HTTP client connections and you want to serve them as
quickly as the data arrives. Each connection may require some
unpredictable processing, so sharding them into equal buckets across worker threads
will worsen mean latency. It&#39;s better to use &#34;the combined queue&#34; queuing
model - have one epoll set and use multiple threads to pull active
sockets and perform the work.</p>
<p>Here&#39;s The Engineer Guy explaining the combined queue model:</p>
<p>
<iframe width="640" height="356" src="https://www.youtube.com/embed/F5Ri_HhziI0?start=118" frameborder="0" allowfullscreen=""></iframe>
</p>

<p>In our case the shared queue is an epoll descriptor, the tills
are worker threads and the jobs are readable sockets.</p>
<h3 id="epoll-level-triggered">Epoll level triggered</h3>
<p>We don&#39;t want to use the level triggered model due to the &#34;thundering
herd&#34; behavior. Additionally the <code>EPOLLEXCLUSIVE</code> won&#39;t help since
there is a race condition possible. Here&#39;s how it may
materialize:</p>
<ul>
<li><strong>Kernel:</strong> receives 2047 bytes of data</li>
<li><strong>Kernel:</strong> two threads are waiting on epoll, kernel wakes up due to <code>EPOLLEXCLUSIVE</code> behavior. Let&#39;s say kernel woke up Thread A.</li>
<li><strong>Thread A:</strong> finishes <code>epoll_wait()</code></li>
<li><strong>Kernel:</strong> receives 2 bytes of data</li>
<li><strong>Kernel:</strong> one thread is waiting on epoll, kernel wakes up Thread B.</li>
<li><strong>Thread A:</strong> performs <code>read(2048)</code> and reads full buffer of 2048 bytes.</li>
<li><strong>Thread B:</strong> performs <code>read(2048)</code> and reads remaining 1 byte of data</li>
</ul>
<p>In this situation the data is split across two threads and without
using mutexes the data may be reordered.</p>
<h3 id="epoll-edge-triggered">Epoll edge triggered</h3>
<p>Okay, so maybe edge triggered model will do better? Not really. The
same race condition occurs:</p>
<ul>
<li><strong>Kernel:</strong> receives 2048 bytes of data</li>
<li><strong>Kernel:</strong> two threads are waiting for the data: A and B. Due to the &#34;edge triggered&#34; behavior only one is notified.</li>
<li><strong>Thread A:</strong> finishes <code>epoll_wait()</code></li>
<li><strong>Thread A:</strong> performs a <code>read(2048)</code> and reads full buffer of 2048 bytes</li>
<li><strong>Kernel:</strong> the buffer is empty so the kernel arms the file descriptor again</li>
<li><strong>Kernel:</strong> receives 1 byte of data</li>
<li><strong>Kernel:</strong> one thread is currently waiting in epoll_wait, wakes up Thread B</li>
<li><strong>Thread B:</strong> finished <code>epoll_wait()</code></li>
<li><strong>Thread B:</strong> performs <code>read(2048)</code> and gets 1 byte of data</li>
<li><strong>Thread A:</strong> retries <code>read(2048)</code>, which returns nothing, gets EAGAIN</li>
</ul>
<h3 id="correct-solution_1">Correct solution</h3>
<p>The correct solution is to use <code>EPOLLONESHOT</code> and re-arm the file
descriptor manually. This is the only way to guarantee that the data
will be delivered to only one thread and avoid race conditions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Using <code>epoll()</code> correctly is hard. Understanding extra flags
<code>EPOLLONESHOT</code> and <code>EPOLLEXCLUSIVE</code> is necessary to achieve load
balancing free of race conditions.</p>
<p>Considering that <code>EPOLLEXCLUSIVE</code> is a very new epoll flag, we may
conclude that <code>epoll</code> was not originally designed for balancing load
across multiple threads.</p>
<p>In the next blog post in this series we will describe the <code>epoll</code>
&#34;file descriptor vs file description&#34; problem which occurs when used
with <code>close()</code> and <code>fork</code> calls.</p>








</article></div>
  </body>
</html>
