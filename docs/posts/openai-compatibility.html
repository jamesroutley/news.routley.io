<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ollama.ai/blog/openai-compatibility">Original</a>
    <h1>OpenAI compatibility</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      <h2>February 8, 2024</h2>
      <section>
        <p><img src="https://ollama.ai/public/blog/openai.png" alt="OpenAI compatibility"/></p>

<p>Ollama now has built-in compatibility with the OpenAI <a href="https://github.com/ollama/ollama/blob/main/docs/openai.md">Chat Completions API</a>, making it possible to use more tooling and applications with Ollama locally.</p>

<h2>Setup</h2>

<p>Start by <a href="https://ollama.ai/download">downloading Ollama</a> and pulling a model such as <a href="https://ollama.ai/library/llama2">Llama 2</a> or <a href="https://ollama.ai/library/mistral">Mistral</a>:</p>

<pre><code>ollama pull llama2
</code></pre>

<h2>Usage</h2>

<h3>cURL</h3>

<p>To invoke Ollama’s OpenAI compatible API endpoint, use the same <a href="https://platform.openai.com/docs/quickstart?context=curl">OpenAI format</a> and change the hostname to <code>http://localhost:11434</code>:</p>

<pre><code>curl http://localhost:11434/v1/chat/completions \
    -H &#34;Content-Type: application/json&#34; \
    -d &#39;{
        &#34;model&#34;: &#34;llama2&#34;,
        &#34;messages&#34;: [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: &#34;You are a helpful assistant.&#34;
            },
            {
                &#34;role&#34;: &#34;user&#34;,
                &#34;content&#34;: &#34;Hello!&#34;
            }
        ]
    }&#39;
</code></pre>

<h3>OpenAI Python library</h3>

<pre><code>from openai import OpenAI

client = OpenAI(
    base_url = &#39;http://localhost:11434/v1&#39;,
    api_key=&#39;ollama&#39;, # required, but unused
)

response = client.chat.completions.create(
  model=&#34;llama2&#34;,
  messages=[
    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who won the world series in 2020?&#34;},
    {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;The LA Dodgers won in 2020.&#34;},
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where was it played?&#34;}
  ]
)
print(response.choices[0].message.content)
</code></pre>

<h3>OpenAI JavaScript library</h3>

<pre><code>import OpenAI from &#39;openai&#39;

const openai = new OpenAI({
  baseURL: &#39;http://localhost:11434/v1&#39;,
  apiKey: &#39;ollama&#39;, // required but unused
})

const completion = await openai.chat.completions.create({
  model: &#39;llama2&#39;,
  messages: [{ role: &#39;user&#39;, content: &#39;Why is the sky blue?&#39; }],
})

console.log(completion.choices[0].message.content)
</code></pre>

<h2>Examples</h2>

<h3>Vercel AI SDK</h3>

<p>The <a href="https://sdk.vercel.ai/docs">Vercel AI SDK</a> is an open-source library for building conversational streaming applications. To get started, use <code>create-next-app</code> to clone the example repo:</p>

<pre><code>npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai example
cd example
</code></pre>

<p>Then make the following two edits in <code>app/api/chat/route.ts</code> to update the chat example to use Ollama:</p>

<pre><code>const openai = new OpenAI({
  baseURL: &#39;http://localhost:11434/v1&#39;,
  apiKey: &#39;ollama&#39;,
});
</code></pre>

<pre><code>const response = await openai.chat.completions.create({
  model: &#39;llama2&#39;,
  stream: true,
  messages,
});
</code></pre>

<p>Next, run the app:</p>

<pre><code>npm run dev
</code></pre>

<p>Finally, open the example app in your browser at <a href="http://localhost:3000">http://localhost:3000</a>:</p>

<video controls="">
  <source src="https://github.com/ollama/ollama/assets/251292/61e17030-7cdb-4d08-8703-e14d53662672" type="video/mp4"/>
</video>

<h3>Autogen</h3>

<p><a href="https://www.microsoft.com/en-us/research/project/autogen/">Autogen</a> is a popular open-source framework by Microsoft for building multi-agent applications. For this, example we’ll use the <a href="https://ollama.ai/library/codellama">Code Llama</a> model:</p>

<pre><code>ollama pull codellama
</code></pre>

<p>Install Autogen:</p>

<pre><code>pip install pyautogen
</code></pre>

<p>Then create a Python script <code>example.py</code> to use Ollama with Autogen:</p>

<pre><code>from autogen import AssistantAgent, UserProxyAgent

config_list = [
  {
    &#34;model&#34;: &#34;llama2&#34;,
    &#34;base_url&#34;: &#34;http://localhost:11434/v1&#34;,
    &#34;api_key&#34;: &#34;ollama&#34;,
  }
]

assistant = AssistantAgent(&#34;assistant&#34;, llm_config={&#34;config_list&#34;: config_list})

user_proxy = UserProxyAgent(&#34;user_proxy&#34;, code_execution_config={&#34;work_dir&#34;: &#34;coding&#34;, &#34;use_docker&#34;: False})
user_proxy.initiate_chat(assistant, message=&#34;Plot a chart of NVDA and TESLA stock price change YTD.&#34;)
</code></pre>

<p>Lastly, run the example to have the assistant write the code to plot a chart:</p>

<pre><code>python example.py
</code></pre>

<h2>More to come</h2>

<p>This is initial experimental support for the OpenAI API. Future improvements under consideration include:</p>

<ul>
<li>Embeddings API</li>
<li>Function calling</li>
<li>Vision support</li>
<li>Logprobs</li>
</ul>

<p>GitHub issues <a href="https://github.com/ollama/ollama/issues">are welcome</a>! For more information, see the OpenAI compatibility <a href="https://github.com/ollama/ollama/blob/main/docs/openai.md">docs</a>.</p>

      </section>
    </div></div>
  </body>
</html>
