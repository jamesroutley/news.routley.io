<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.kuzudb.com/post/kuzu-wasm-rag/">Original</a>
    <h1>Show HN: In-Browser Graph RAG with Kuzu-WASM and WebLLM</h1>
    
    <div id="readability-page-1" class="page"><div>  <nav><ol><li><a href="#a-quick-introduction-to-webassembly">A quick introduction to WebAssembly</a></li><li><a href="#architecture">Architecture</a></li><li><a href="#implementation">Implementation</a><ol><li><a href="#data-ingestion">Data Ingestion</a></li><li><a href="#webllm-prompting">WebLLM Prompting</a></li></ol></li><li><a href="#observations">Observations</a></li><li><a href="#live-demo">Live demo</a></li><li><a href="#takeaways">Takeaways</a></li></ol></nav><p>We’re excited that members of our community are already building applications with the <a href="https://docs.kuzudb.com/client-apis/wasm/">WebAssembly (Wasm)</a> version of Kuzu,
which was only released a few weeks ago!
Early adopters to integrate Kuzu-Wasm include <a href="https://github.com/alibaba/GraphScope">Alibaba Graphscope</a>, see: <a href="https://github.com/kuzudb/kuzu/discussions/4946">1</a>
and <a href="https://gsp.vercel.app/#/explore?graph_id=">2</a>, and <a href="https://www.kineviz.com/">Kineviz</a>, whose project will be launched soon.</p>
<p>In this post, we’ll showcase the potential of Kuzu-Wasm by building a fully in-browser chatbot
that answers questions over LinkedIn data using an advanced retrieval technique: Graph
Retrieval-Augmented Generation (Graph RAG). This is achieved using Kuzu-Wasm
alongside <a href="https://github.com/mlc-ai/web-llm">WebLLM</a>, a popular in-browser LLM inference engine that can
run LLMs inside the browser.</p>
<h2 id="a-quick-introduction-to-webassembly">A quick introduction to WebAssembly<a aria-hidden="true" tabindex="-1" href="#a-quick-introduction-to-webassembly"><span></span></a></h2>
<p>WebAssembly (Wasm) has transformed browsers into general-purpose computing platforms.
Many fundamental software components, such as full-fledged databases, machine learning
libraries, data visualization tools, and encryption/decryption libraries, now have Wasm versions.
This enables developers to build advanced applications that run entirely in users’
browsers—without requiring backend servers. There are several benefits for building fully
in-browser applications:</p>
<ul>
<li><strong>Privac</strong>y: Users’ data never leaves their devices, ensuring complete privacy and confidentiality.</li>
<li><strong>Ease of Deployment</strong>: An in-browser application that uses Wasm-based components
can run in any browser in a completely serverless manner.</li>
<li><strong>Speed</strong>: Eliminating frontend-server communication can lead to a significantly faster and
more interactive user experience.</li>
</ul>
<p>With this in mind, let’s now demonstrate how to develop a relatively complex AI application completely in
the browser! We’ll build a <strong>fully in-browser</strong> chatbot that uses graph retrieval- augmented
generation (Graph RAG) to answer natural language questions. We demonstrate this using
<a href="https://docs.kuzudb.com/client-apis/wasm/#installation">Kuzu-Wasm</a> and <a href="https://github.com/mlc-ai/web-llm">WebLLM</a>.</p>
<h2 id="architecture">Architecture<a aria-hidden="true" tabindex="-1" href="#architecture"><span></span></a></h2>
<p>The high-level architecture of the application looks as follows:
<img src="https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/graph-rag.png" width="400"/></p>
<p>The term “Graph RAG” is used to refer to several techniques but in its simplest form the term
refers to a 3-step retrieval approach. The goal is to retrieve useful context from a graph DBMS (GDBMS)
to help an LLM answer natural language questions.
In our application, the additional data is information about
a user’s LinkedIn data consisting of their contacts, messages, companies the user or their contacts worked for. Yes, you can download
<a href="https://www.linkedin.com/help/linkedin/answer/a1339364/downloading-your-account-data">your own LinkedIn data</a> (and you should, if
for nothing else, to see how much of your data they have!).
The schema of the graph database we use to model this data will be shown below momentarily. First, let’s go over the 3 steps of
Graph RAG:</p>
<ol>
<li>Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>→</span></span></span></span> Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>C</mi><mi>y</mi><mi>p</mi><mi>h</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{Cypher}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>C</span><span>y</span><span>p</span><span>h</span><span>er</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>: A user asks a natural language question Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, such as “<em>Which of my contacts work at Google?</em>“.
Then, using an LLM, this question is converted to a Cypher query, e.g., <code>MATCH (a:Company)&lt;-[:WorksAt]-(b:Contact) WHERE a.name = &#34;Google&#34; RETURN b</code>,
that aims to retrieve relevant data stored in the GDBMS to answer this question.</li>
<li>Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>C</mi><mi>y</mi><mi>p</mi><mi>h</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{Cypher}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>C</span><span>y</span><span>p</span><span>h</span><span>er</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>→</span></span></span></span> Context: Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>C</mi><mi>y</mi><mi>p</mi><mi>h</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{Cypher}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>C</span><span>y</span><span>p</span><span>h</span><span>er</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is executed in the GBMS and a set of records is retrieved, e.g., “Karen” and “Alice”. Let’s call these retrieved records “Context”.</li>
<li>(Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> + Context) <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>→</span></span></span></span> A<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>: Finally, the original Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is given to the LLM along with the retrieved context and the LLM produces a natural language answer A<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>,
e.g., “<em>Karen and Alice work at Google.</em>”</li>
</ol>
<h2 id="implementation">Implementation<a aria-hidden="true" tabindex="-1" href="#implementation"><span></span></a></h2>
<h3 id="data-ingestion">Data Ingestion<a aria-hidden="true" tabindex="-1" href="#data-ingestion"><span></span></a></h3>
<p>The schema for our personal LinkedIn data’s graph is shown below:</p>
<p><img src="https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/schema.png" width="400"/></p><p>We ingest the data into Kuzu-Wasm in several steps using custom JavaScript code (see the <a href="https://github.com/kuzudb/wasm-linkedin-example/blob/main/src/utils/LinkedInDataConverter.js"><code>src/utils/LinkedInDataConverter.js</code></a> file in our Github repo):</p>
<ol>
<li>Upload CSV Files: Users drag and drop their LinkedIn CSV files, which are stored in Kuzu-Wasm’s virtual file system.</li>
<li>Initial Processing: Using Kuzu’s <code>LOAD FROM</code> feature, the raw CSVs are converted into JavaScript objects.</li>
<li>Normalization: In JavaScript, we clean and standardize the data by fixing timestamps, formatting dates, and resolving inconsistent URIs.</li>
<li>Data Insertion: The cleaned data is inserted back into Kuzu-Wasm as a set of nodes and relationships.</li>
</ol>
<h3 id="webllm-prompting">WebLLM Prompting<a aria-hidden="true" tabindex="-1" href="#webllm-prompting"><span></span></a></h3>
<p>Our code follows the exact 3 steps above. Specifically, we prompt WebLLM twice, once to create a Cypher query Q<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>C</mi><mi>y</mi><mi>p</mi><mi>h</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{Cypher}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>C</span><span>y</span><span>p</span><span>h</span><span>er</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>,
which is sent to Kuzu-Wasm.
We adapted the prompts from our <a href="https://github.com/kuzudb/langchain-kuzu/">LangChain-Kuzu integration</a>,
with a few modifications. Importantly, we make sure to include the schema information of the LinkedIn database from Kuzu in the prompt, which helps the LLM better understand
the structure and relationships (including the directionality of the relationships) in the dataset.</p>
<p>In this example, we represented the schema as YAML instead of raw, stringified JSON in the LLM prompt.
In our anecdotal experience, for Text-to-Cypher tasks that require reasoning over the schema, we find that LLMs tend do better
with YAML syntax than they do with stringified JSON. More experiments on such Text-to-Cypher tasks will be shown in future blog posts.</p>
<h2 id="observations">Observations<a aria-hidden="true" tabindex="-1" href="#observations"><span></span></a></h2>
<p>It’s indeed impressive to see such a graph-based pipeline with LLMs being done entirely in the browser! There are, however, some caveats.
Most importantly, in the browser, resources are restricted, which limits the sizes of different components of your application.
For example, the size of the LLM you can use is limited. We tested our implementation on a MacBook Pro 2023 and a Chrome browser.
We had to choose the <code>Llama-3.1-8B-Instruct-q4f32_1-MLC</code> model (see <a href="https://huggingface.co/mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC">here</a> for the model card),
which is an instruction-tuned model in MLC format. The <code>q4f32_1</code> format is the smallest of the Llama 3.1 models that has 8B parameters
(the largest has 450B parameters, which is of course too large to run in the browser).
For simple queries, the model performed quite well. It correctly generated Cypher queries for the LinkedIn data, such as:</p>
<ul>
<li>How many companies did I follow?</li>
<li>Which contacts work at Kùzu, Inc?</li>
<li>Which skills do I have?</li>
</ul>
<p><img src="https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/successful-generations.png"/></p><p>However, we saw that for more complex queries requiring joins, filtering, and aggregation, the model struggled to return a valid Cypher query.
It often produced incorrect or incomplete Cypher queries for questions like: “Who endorsed me the most times?“.
Token generation is also far slower than what you may be used to in state-of-the art interfaces,
such as ChatGPT. In our experiments, we observed a speed of 15-20 tokens/sec, so generating answers took on average, around 10s.</p>
<h2 id="live-demo">Live demo<a aria-hidden="true" tabindex="-1" href="#live-demo"><span></span></a></h2>
<p>We have deployed this demo so you can test it in your browser:</p>
<ul>
<li><a href="https://wasm-linkedin-example.kuzudb.com">Live Demo</a>: Drag and drop your <a href="https://www.linkedin.com/help/linkedin/answer/a1339364/downloading-your-account-data">LinkedIn data dump</a> into
the app and start querying your personal graph. The demo also visualizes your data in a node-link graph view using <code>vis.js</code></li>
<li><a href="https://github.com/kuzudb/wasm-linkedin-example">GitHub Repository</a>: The source code is openly available so you can experiment with it further. If you see better results with different models/prompts, we’d love to hear it!</li>
</ul>
<p>Once the data is loaded, you can see a visualization that looks something like this:</p>
<p><img src="https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/landing-page.png"/></p><h2 id="takeaways">Takeaways<a aria-hidden="true" tabindex="-1" href="#takeaways"><span></span></a></h2>
<p>The key takeaway from this post is that such advanced pipelines that utilize graph databases and LLMs are now possible <em>entirely in the browser</em>.
We expect that many of the performance limitations of today will improve over time, with the wider adoption of <a href="https://www.w3.org/TR/webgpu/">WebGPU</a>,
<a href="https://github.com/WebAssembly/memory64">Wasm64</a>, and other <a href="https://github.com/WebAssembly/proposals?tab=readme-ov-file">proposals</a>
to improve Wasm. LLMs are also rapidly getting smaller &amp; better, and before we know it, it will be possible to use very advanced LLMs
in the browser. The next release of Kuzu will include a native vector index (it’s already available
in our nightly build; see <a href="https://github.com/kuzudb/kuzu/pull/4578">this PR</a> for how to use it!).
As a result, you can also store the embeddings of documents
along with actual node and relationship records to enhance your graph retrievals, entirely within Kuzu.
Using our upcoming vector index,
you’ll be able to try all sorts of interesting RAG techniques, coupled with Kuzu-Wasm, all within the browser while keeping your data private.
The sky is the limit!</p>  </div></div>
  </body>
</html>
