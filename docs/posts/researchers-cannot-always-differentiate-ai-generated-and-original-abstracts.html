<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/d41586-023-00056-7">Original</a>
    <h1>Researchers cannot always differentiate AI-generated and original abstracts</h1>
    
    <div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="//media.nature.com/lw767/magazine-assets/d41586-023-00056-7/d41586-023-00056-7_23914464.jpg?as=webp 767w, //media.nature.com/lw319/magazine-assets/d41586-023-00056-7/d41586-023-00056-7_23914464.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"/>
  <img alt="Webpage of ChatGPT is seen on OpenAI&#39;s website on a computer monitor" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-00056-7/d41586-023-00056-7_23914464.jpg"/>
  <figcaption>
   <p><span>Scientists and publishing specialists are concerned that the increasing sophistication of chatbots could undermine research integrity and accuracy.</span><span>Credit: Ted Hsu/Alamy</span></p>
  </figcaption>
 </picture>
</figure><p>An artificial-intelligence (AI) chatbot can write such convincing fake research-paper abstracts that scientists are often unable to spot them, according to a preprint posted on the bioRxiv server in late December<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Researchers are divided over the implications for science.</p><p>“I am very worried,” says Sandra Wachter, who studies technology and regulation at the University of Oxford, UK, and was not involved in the research. “If we’re now in a situation where the experts are not able to determine what’s true or not, we lose the middleman that we desperately need to guide us through complicated topics,” she adds.</p><p>The chatbot, ChatGPT, creates <a href="https://www.nature.com/articles/d41586-022-03479-w" data-track="click" data-label="https://www.nature.com/articles/d41586-022-03479-w" data-track-category="body text link">realistic and intelligent-sounding text</a> in response to user prompts. It is a ‘<a href="https://www.nature.com/articles/d41586-021-00530-0" data-track="click" data-label="https://www.nature.com/articles/d41586-021-00530-0" data-track-category="body text link">large language model</a>’, a system based on neural networks that learn to perform a task by digesting huge amounts of existing human-generated text. Software company OpenAI, based in San Francisco, California, released the tool on 30 November, and it is free to use.</p><p>Since its release, researchers have been <a href="https://www.nature.com/articles/d41586-022-04397-7" data-track="click" data-label="https://www.nature.com/articles/d41586-022-04397-7" data-track-category="body text link">grappling with the ethical issues</a> surrounding its use, because much of its output can be difficult to distinguish from human-written text. Scientists have published a preprint<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> and an editorial<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> written by ChatGPT. Now, a group led by Catherine Gao at Northwestern University in Chicago, Illinois, has used ChatGPT to generate artificial research-paper abstracts to test whether scientists can spot them.</p><p>The researchers asked the chatbot to write 50 medical-research abstracts based on a selection published in <i>JAMA</i>, <i>The New England Journal of Medicine</i>, <i>The BMJ</i>, <i>The Lancet</i> and <i>Nature Medicine</i>. They then compared these with the original abstracts by running them through a plagiarism detector and an AI-output detector, and they asked a group of medical researchers to spot the fabricated abstracts.</p><h2>Under the radar</h2><p>The ChatGPT-generated abstracts sailed through the plagiarism checker: the median originality score was 100%, which indicates that no plagiarism was detected. The AI-output detector spotted 66% the generated abstracts. But the human reviewers didn&#39;t do much better: they correctly identified only 68% of the generated abstracts and 86% of the genuine abstracts. They incorrectly identified 32% of the generated abstracts as being real and 14% of the genuine abstracts as being generated.</p><p>“ChatGPT writes believable scientific abstracts,” say Gao and colleagues in the preprint. “The boundaries of ethical and acceptable use of large language models to help scientific writing remain to be determined.”</p><p>Wachter says that, if scientists can’t determine whether research is true, there could be “dire consequences”. As well as being problematic for researchers, who could be pulled down flawed routes of investigation, because the research they are reading has been fabricated, there are “implications for society at large because scientific research plays such a huge role in our society”. For example, it could mean that research-informed policy decisions are incorrect, she adds.</p><p>But Arvind Narayanan, a computer scientist at Princeton University in New Jersey, says: “It is unlikely that any serious scientist will use ChatGPT to generate abstracts.” He adds that whether generated abstracts can be detected is “irrelevant”. “The question is whether the tool can generate an abstract that is accurate and compelling. It can’t, and so the upside of using ChatGPT is minuscule, and the downside is significant,” he says.</p><p>Irene Solaiman, who researches the social impact of AI at <a href="https://www.nature.com/articles/d41586-022-01705-z" data-track="click" data-label="https://www.nature.com/articles/d41586-022-01705-z" data-track-category="body text link">Hugging Face</a>, an AI company with headquarters in New York and Paris, has fears about any reliance on large language models for scientific thinking. “These models are trained on past information and social and scientific progress can often come from thinking, or being open to thinking, differently from the past,” she adds.</p><p>The authors suggest that those evaluating scientific communications, such as research papers and conference proceedings, should put policies in place to stamp out the use of AI-generated texts. If institutions choose to allow use of the technology in certain cases, they should establish clear rules around disclosure. Earlier this month, the Fortieth International Conference on Machine Learning, a large AI conference that will be held in Honolulu, Hawaii, in July, announced that it has banned papers written by ChatGPT and other AI language tools.</p><p>Solaiman adds that in fields where fake information can endanger people’s safety, such as medicine, journals may have to take a more rigorous approach to verifying information as accurate.</p><p>Narayanan says that the solutions to these issues should not focus on the chatbot itself, “but rather the perverse incentives that lead to this behaviour, such as universities conducting hiring and promotion reviews by counting papers with no regard to their quality or impact”.</p>
                </div></div>
  </body>
</html>
