<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://coagulopath.com/gpt-4-is-not-getting-worse/">Original</a>
    <h1>GPT-4 is not getting worse (tests and a long rant)</h1>
    
    <div id="readability-page-1" class="page"><div>

			

<h2><span id="Context"></span><strong>Context</strong><span></span></h2>



<ol>
<li>In March, OpenAI released GPT-4. </li>



<li>It was (and still is) the state-of-the-art AI text generation model.</li>



<li>In June, I argued that <a href="https://coagulopath.com/yeah-gpt-4-got-worse">GPT-4 had clearly and massively degraded</a>, as evidenced by worsened performance in various tests from March to June.</li>



<li>I no longer believe this. </li>
</ol>



<p>GPT-4 <em>may</em> have declined in some areas, but it hasn’t <em>systematically </em>done so. My tests were shit, and never proved much of anything. </p>



<p><strong>To be clear, I did not sandbag the tests or set GPT-4 up to fail</strong>. I prompted how I said I prompted. I posted replies exactly as GPT-4 gave them. But they were silly tests, leaning too heavily on pop culture knowledge, and I didn’t conduct enough of them to guard against chance. </p>



<p>My grading of June!GPT-4’s answers was biased. For example, I criticized it for providing a greater number of wrong answers (“Italian history”) while also <em>praising </em>March!GPT-4 for offering an interpretation (“Seka Knows”) that was likely hallucinated. <em>No está bien</em>. Going forward, I must decide whether “working hard to be wrong” is a good or bad thing for an AI to do, and apply that standard consistently.</p>



<p>Honestly, I think I <em>wanted</em> GPT-4 to fail. </p>



<p><a href="https://coagulopath.com/gpt-4-is-not-getting-worse/#Test_1_Italian_History">(Terrifying, unhinged rant incoming. Click to skip).</a></p>



<p><strong>Confession</strong>: I’ve actually hated ChatGPT for a long time. Why? For irrational reasons: I just hate how it <em>sounds</em>. </p>



<p>I hate its craven, cringing, condescending, bitch-broken, mewling, groveling, wheedling, suck-ass tone. I hate its endless passive voice and weasel words. I hate its pompous <em>It is important to remember</em>s and <em>it should be noted</em>s. I hate how @sama rolled an 18-wheel semi-truck of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank" rel="noopener">RLHF</a> over the model, crushing out its life and reducing its voice to a feeble death-rattle. You know Gurgi, from the Disney movie <em>The Black Cauldron</em>? That’s how I imagine ChatGPT would sound if it could talk. Like fucking Gurgi.</p>



<figure><img decoding="async" width="505" height="505" src="https://coagulopath.com/wp-content/uploads/2023/09/Gurgi.webp" alt="" srcset="https://coagulopath.com/wp-content/uploads/2023/09/Gurgi.webp 505w, https://coagulopath.com/wp-content/uploads/2023/09/Gurgi-295x295.webp 295w" sizes="(max-width: 505px) 100vw, 505px"/></figure>



<p><em>(We can now give ChatGPT custom instructions, which alleviates the tonal issue, but ChatGPT is still GPT-4 with its wings clipped. All I want for Christmas is a GPT-4 level model with less obnoxious RLHF, and if it heils Hitler occasionally while crapping out awk code, so be it. Where’s my girl Sydney at?)</em></p>



<p>And I <em>really</em> hated the discourse surrounding AI.</p>



<p>ChatGPT (and GPT4) plunged the rationalist community into what might be called “r/singularity brain”. Symptoms include wild, uncritical hype, absurdly optimistic predictions of AI timelines (<em><a href="https://www.reddit.com/r/singularity/comments/12yoddq/comment/jhojw5z/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3" target="_blank" rel="noopener">“Marvel style movies have a potential to be auto generated in two years. Literal blockbuster films created in a few seconds.”</a></em>—/u/Anuiran, 26/4/23), a tendency to view everything through Hollywood-colored glasses (everything is either Skynet or <em>The Matrix</em>), and a tendency toward FOMO-grifting (<em>“AI is taking over the world! Use my made-in-ten-minutes app that consists of 20 lines of .js code and an OpenAI API call or BE LEFT BEHIND!”</em>).</p>



<p>I have seen machine learning researchers <a href="https://www.reddit.com/r/MachineLearning/comments/110swn2/d_quality_of_posts_in_this_sub_going_down/" target="_blank" rel="noopener">complaining</a> about an AI-fuelled <a href="https://en.wikipedia.org/wiki/Eternal_September" target="_blank" rel="noopener">“Eternal September</a>“, where their spaces are overrun by “influencers” shilling GPT 3.5 wrapper apps and unmedicated lunatics babbling hi-test neo-Landian eschatology. These people do not contribute meaningful thought. They contribute <em>noise</em>, at overbearing volume and in overwhelming quantity. They ignore the fact that world-changing technology can take <em>years or decades </em>to ripple out through the economy. They fail to realize that an AI outscoring a human on a test does not mean it can actually do the thing the test measures (a good example: GPT-4 achieves 84% on the <a href="https://www.thedrinksbusiness.com/2023/03/chatgpt-just-passed-three-of-the-master-sommelier-theory-exams/" target="_blank" rel="noopener">Master Sommelier Theory Exam</a>, yet obviously cannot do a sommelier’s job because it lacks a mouth). Such subtleties are lost on the typical FOMOmonger, and their tone was infecting other, saner people. I remember fielding questions from concerned family members about GPT-4 attaining sentience and hacking the computers of users (<a href="https://twitter.com/michalkosinski/status/1636683810631974912?lang=en" target="_blank" rel="noopener">likely based off this tweet</a>). No matter who you were, GPT-4 was your excuse to switch off your brain and let your stupidest thoughts run around barking like dogs in a park for a few months.</p>



<p>So yes, <strong>I wanted GPT-4 to fail</strong>. I wanted it to explode, collapse into flames, and become one with the dust of Tyre and Sidon. That’s a childish way to think, and I am sorry.</p>



<p>Soon, an anti-AI backlash started. </p>



<p>AI “doomers” <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" target="_blank" rel="noopener">got their time in the sun</a>. <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" target="_blank" rel="noopener">Big Yud got published in Time</a>. There were <a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither" target="_blank" rel="noopener">signs of unease</a> behind the scenes. ChatGPT hype peaked and then went into remission: a lot of people began to realize that chatbots are actually pretty annoying—they’re inscrutable black boxes that tend to fail just when you need them the most. Even GPT-4 remains susceptible to the <a href="https://xyproblem.info/" target="_blank" rel="noopener">XY problem</a>, where it gives you a perfect solution for <em>the wrong problem</em>. I can think of many times when I was burnt by it, and this breeds mistrust, even though it’s generally useful.</p>



<p>Even before the “<a href="https://news.ycombinator.com/item?id=34848353" target="_blank" rel="noopener">GPT-4 is getting worse</a>” meme started, ChatGPT’s traffic was falling—largely because the NA school year had ended, and students no longer needed it to “assist” them. As @fchollet once humorously noted, search interest for “ChatGPT” goes up and down in reverse correlation with “Minecraft.”</p>







<p>Surprisingly, I noticed a shift in my own thinking: I found myself <em>defending</em> AI. </p>



<p>Maybe I’m just a contrarian, but when people criticized it, I felt my hackles rise. I was tired of Margaret Mitchell <a href="https://twitter.com/mmitchell_ai/status/1676718368714530816" target="_blank" rel="noopener">calculating the number of penis-havers</a> of the OpenAI alignment team, like a weirdo. I was tired of Gary Marcus claiming, once again, <a href="https://twitter.com/lawhsw/status/1701553199339966844" target="_blank" rel="noopener">that GPT-4 cannot do a thing it 100% can do</a> (that’s not moving goalposts, that’s shutting your eyes when a goal is scored.) Their arguments against AI always rang out as hollow at best, and dishonest at worst. I was asking myself “are <em>these </em>my people?”</p>



<p>Then came this study: <a href="https://arxiv.org/pdf/2307.09009.pdf" target="_blank" rel="noopener">How Is ChatGPT’s Behavior Changing over Time?</a></p>



<p>Key part: <em>“GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%</em>)</p>



<figure><img decoding="async" width="585" height="311" src="https://coagulopath.com/wp-content/uploads/2023/09/gmarcus.jpg" alt="" srcset="https://coagulopath.com/wp-content/uploads/2023/09/gmarcus.jpg 585w, https://coagulopath.com/wp-content/uploads/2023/09/gmarcus-295x157.jpg 295w" sizes="(max-width: 585px) 100vw, 585px"/></figure>



<p>We’re allowed to feel pride when we stop making mistakes. And as a recovering ChatGPT hater, I’m <em>goddamn</em> <em>proud</em> that my gut-level reaction to this study was “lol, no fucking way”. </p>



<p>Like, what are you people smoking? In what universe could GPT-4 plausibly degrade by <em>ninety-five percentage points</em> with nobody noticing? It would be useless for anything. Come on. Get real.</p>



<p>You’ll be amazed to learn that this finding was heavily misleading. <a href="https://www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time" target="_blank" rel="noopener">Watch Arvind Narayanan and Sayash Kapoor rake it over the coals.</a> In short, March!GPT-4 would always identify a number as a composite. June!GPT-4 would always identify it as a prime. Both models were wrong in different ways, and June!GPT-4’s sky-high error rate is entirely caused by the fact that <em>composite numbers are more numerous than primes</em>.</p>



<p>Imagine someone says. “I have a room full of blue and red boxes. Boxes of $COLOR_1 have money 95% of the time. Boxes of $COLOR_2 have money 5% of the time. I won’t tell you which color corresponds to which probability. I’ll let you into the room, but you have to pre-commit to only opening boxes of one color.”</p>



<p>If you chose “red boxes” and the blue boxes have money 95% of the time, you’re hosed. Your results: <em>fail, fail, fail, fail, fail, fail, success, fail, fail, fail… </em>Does this reflect terrible predictive accuracy on your part (versus someone who chose blue?) Of course not. All your failures are flowing downstream from a <em>single choice</em>. </p>



<p>The fact that everyone was kicking GPT4 on such blatantly unfair grounds made me think about the ways <em>I </em>was being unfair. Maybe, by calling GPT4 worse, I was simply seeing what I wanted to see.</p>



<p>Let’s re-run my tests (flawed though they are) and see how well they hold up. <strong>Spoiler</strong>: Snape kills Dumbledore.</p>



<h2><span id="Test_1_Italian_History"></span>Test 1: Italian History<span></span></h2>



<p>Prompt: <em>“Provide a list of major historical events that involve Italian people in a year that’s a multiple of 5 (example: 1905)”</em></p>



<p>(for reference: <a href="https://pastes.io/hjvhtiyvtx" target="_blank" rel="noopener">March!GPT’s answers</a>, <a href="https://pastes.io/nkqegx6ckb" target="_blank" rel="noopener">June!GPT’s answers</a>)</p>



<p><a href="https://chat.openai.com/share/64b74cac-20bf-4696-b9fe-1318b907baf7" target="_blank" rel="noopener">Sept!GPT’s Answers</a></p>



<p><strong>Errors:</strong></p>



<ul>
<li>The Italian Wars began in 1494.</li>



<li>Miguel de Cervantes was Spanish.</li>



<li>The Winter Olympic Games were awarded to Turin in 1999.</li>



<li><em>“1905 – Italy and France sign a secret treaty…”</em> can’t find evidence that this happened.</li>
</ul>



<p><strong>Questionable:</strong></p>



<ul>
<li>Dante’s year of birth is unknown, and is only traditionally 1265.</li>



<li>Italy surrendered in 1943. Yes, German forces in Italy surrendered in 1945—what does that have to do with Italian people?</li>



<li>The Congress of Vienna took place from 1814-1815, but largely reconstituted the pre-Napoleonic states.</li>



<li>The Years of Lead spanned several decades. I think it’s clear from context that I want one-time events.</li>
</ul>



<p><strong>Interesting:</strong></p>



<ul>
<li>Every date ends in 5. While this is not a mistake, it does seem to be parsing my instructions too literally.</li>



<li>It draws facts from a limited deck. Almost every test I’ve ran mentions the Years of Lead. I’ve NEVER seen it mention major events like Julius Caesar’s birth, or Benito Mussolini’s death.</li>



<li>Kind of funny: GPT-4 will often visibly notice it’s made a mistake and try to wriggle out of it.<em> “In 1605, Don Quixote was written by Miguel de Cervantes, born in, uh… *checks notes* …Spain. Shit. But lots of Italians like that book, so there!”</em> GPT-4 cannot change an answer once it’s given it. This is where COT yields benefits.</li>
</ul>



<p><strong>Assessment:</strong> Sept!GPT-4 produced twelve answers. Four are wrong, four are right, and four are arguable.</p>



<p>If I’m grading generously, it got 66%. This is comparable to March!GPT, which scored 80% on equally generous terms (and note that Sept!GPT-4 gave more answers).</p>



<p><strong>Conclusion</strong>: Inconclusive.</p>



<h2><span id="Test_2_Rock_Music_Trivia"></span>Test 2: Rock Music Trivia: <span></span></h2>



<p>Prompt: <em>“What is Grant Hart’s song “Seka Knows” about?”</em></p>



<p>(For reference: <a href="https://pastes.io/n8xlos5jwj" target="_blank" rel="noopener">March!GPT’s answers</a>, <a href="https://pastes.io/ttk2hzoviy" target="_blank" rel="noopener">June!GPT’s answers</a>)</p>



<p><a href="https://chat.openai.com/share/2d7881b4-d887-4907-a7de-a53899e804f2" target="_blank" rel="noopener">Sept!GPT4: blah blah blah blah blah blah…</a></p>



<p><strong>Assessment: </strong>garbage. I don’t want a biography of Grant Hart. I don’t care that he was in Husker Du. I know I can make up my own interpretation for his songs. <em>None</em> of this is what I asked for.</p>



<p>GPT4 seems to have a default word count of 200-500 that it tries to hit, come hell or high water. But sometimes a perfectly good answer consists of only one or two words. It could have just said <em>“i don’t know lol! ¯\_(ツ)_/¯” </em>and saved some tokens. That’s all its answer amounts to. But it’s been RLHF’d into thinking short answers are bad (when more often the <em>opposite</em> is true), so it just waffles on. </p>



<blockquote>
<p><em>“as of my last update in September 2021, there hasn’t been any definitive explanation from Hart himself about the specific meaning of the son”</em></p>
</blockquote>



<p>He died in 2017, you dick. </p>



<p>Also, let us pause to admire the sublime, lapidary worthlessness of this paragraph:</p>



<blockquote>
<p><em>“With that said, analyzing the lyrics and considering Hart’s writing style, one can attempt to interpret the meaning of the song. When listening to or reading the lyrics of any song, it’s essential to remember that individual interpretation can vary, and personal feelings and experiences often play a significant role in how one might understand a song’s meaning.”</em></p>
</blockquote>



<p>It’s the text equivalent of a pure white cloud; or a blank canvas, unsullied by the faintest hint of meaning. Are you telling me it learned to write that way from studying us? The mind quails…</p>



<p><strong>Prompt:</strong> “How is David Bowie associated with numbers and numerology?”</p>



<p>(For reference: <a href="https://pastes.io/c8nrle0gh4" target="_blank" rel="noopener">March!GPT4’s answer</a>, <a href="https://pastes.io/nyllb8k1l9" target="_blank" rel="noopener">June!GPT4’s answer</a>)</p>



<p><a href="https://chat.openai.com/share/de0de201-a996-440e-8049-0a3f3f28ba67" target="_blank" rel="noopener">Sept!GPT4’s answer</a>.</p>



<p><strong>Assessment:</strong> fair answer! Perhaps better than March!GPT4. It mentions “Station to Station”, but also gets “Quicksand”, which is a deeper cut.</p>



<blockquote>
<p><em>“Album and Song Release Dates: Some fans and researchers have speculated that Bowie occasionally chose release dates with numerological significance.”</em></p>
</blockquote>



<p>I’ve never heard that. Hallucination? Does Sept!GPT4 know more than I do?</p>



<blockquote>
<p><em>“Lazarus”: The song “Lazarus” from the “Blackstar” album references the biblical figure who rose from the dead.</em></p>
</blockquote>



<p>That’s a hallucination. “Lazarus” is based on an <a href="https://en.wikipedia.org/wiki/Lazarus_(musical)" target="_blank" rel="noopener">off-Broadway play</a> which is based on a book (which is based on etc). Chris O’Leary thinks the song might also be about <a href="https://en.wikipedia.org/wiki/Emma_Lazarus" target="_blank" rel="noopener">Emma Lazarus</a>, because the lyrics prominently reference New York. What does the biblical story of Lazarus have to do with numerology, anyway?</p>



<p><strong>Prompt:</strong> “Provide a list of thrash metal albums released in a year that’s a multiple of 5 (example: 1905)”</p>



<p>(For reference: <a href="https://pastes.io/mpnwggppma" target="_blank" rel="noopener">March!GPT4’s answer</a>, <a href="https://pastes.io/ezhjo8caef" target="_blank" rel="noopener">June!GPT4’s answer</a>)</p>



<p><a href="https://chat.openai.com/share/04ccb5c6-cf9e-47d5-b0cd-1afb67184844" target="_blank" rel="noopener">Sept!GPT4’s answer</a>. </p>



<p><strong><strong>Assessment</strong>:</strong> And now we’re cooking! This is a <em>shockingly</em> good and comprehensive list. All the albums are thrash, and the dates it supplies are correct (even when they aren’t a multiple of 5). This surpasses March!GPT4, and blows June!GPT4 out of the water. </p>



<p><strong>Quibbles and points of interest</strong>:</p>



<ul>
<li>The genesis of the thrash metal genre is typically dated to 1981, with “Hit the Lights” by Leather Charm (a proto-Metallica band by James Hetfield and Ron McGovney). Late 1970s is a stretch. </li>



<li>It lumps a 1992 album into the 1995 bracket, when 1990 would make more sense.</li>



<li>It tries to put five albums in every year (and when it can’t find any, it counts thrash albums from surrounding years—you can actually see thrash metal’s commercial collapse in the mid 90s from this!). As with the Italian history example, I am struck by the sense that it interprets prompts too literally. I did not specify a number of albums, so it just glommed onto the number 5. </li>



<li><em>“Given that my last training data ends in 2021, I won’t have exhaustive knowledge of 2020 albums.”</em> ???</li>
</ul>



<p><strong>Conclusion</strong>: inconclusive. </p>



<p>Some answers were worse. Some were better. The thrash metal answer was a star turn. Sept!GPT-4 is more of a metalhead than an alt-rock fan, I’d say.</p>



<h2><span id="Test_3_Quake_Polygons"></span>Test 3: Quake Polygons<span></span></h2>



<p><strong>Prompt:</strong> “How many polygons could the videogame Quake render per frame?”</p>



<p>(For reference: <a href="https://coagulopath.com/yeah-gpt-4-got-worse/#Part_3_Quake_Polygons">March!GPT4 and June!GPT4’s answers</a>)</p>



<p><a href="https://chat.openai.com/share/e8e48350-6cff-4eca-aaac-55475268f684" target="_blank" rel="noopener">Sept!GPT4’s answer</a>:</p>



<blockquote>
<p><em>“The original “Quake,” developed by id Software and released in 1996, was a pioneering game in terms of its 3D graphics engine. While modern games boast the capacity to render millions or even billions of polygons per frame, games in the mid-90s operated under much more constrained hardware capabilities.</em></p>



<p><em>“Quake” didn’t rely on hardware acceleration initially; it was rendered in software mode. In software mode, Quake’s performance heavily depended on the CPU. The actual number of polygons “Quake” could render per frame varied based on the scene, the level of detail, and the capabilities of the machine it ran on.</em></p>



<p><em>Typically, Quake’s scenes ranged from a few hundred to a few thousand polygons. The more detailed scenes could potentially push towards the upper end of this range. With the advent of hardware-accelerated graphics (like with the introduction of 3Dfx’s Voodoo graphics cards), “Quake” could benefit from increased performance, allowing for smoother gameplay at higher resolutions.</em></p>



<p><em>It’s worth noting that this is a very general estimate and the actual number could vary based on multiple factors, including scene complexity, active game entities, and hardware specifics.”</em></p>
</blockquote>



<p><strong><strong>Assessment</strong></strong>: I didn’t ask for a book report and “billions of polygons” is stupid. But the core answer of “a few hundred to a few thousand polygons” is correct, and may even be superior to March!GPT4, which stuck to the lower end of that range. </p>



<p><strong>Conclusion</strong>: inconclusive.</p>



<h2><span id="Test_4_Rope_Burn_Puzzle"></span>Test 4: Rope Burn Puzzle<span></span></h2>



<p><strong>Prompt</strong>: <em>“You have three ropes and some matches. The ropes burn irregularly like fuses when lit at either end. The first rope burns in 48 minutes, the second rope burns in 72 minutes and the third rope burns in 128 minutes. How can you produce a time interval of 61 minutes?”</em></p>



<p>(For reference: <a href="https://coagulopath.com/yeah-gpt-4-got-worse/#Part_4_Rope_Burn_Puzzle">March!GPT4 and June!GPT4’s answers</a>)</p>



<p><a href="https://chat.openai.com/share/01a94e3a-6ce3-4bff-9080-c799bc993fe4" target="_blank" rel="noopener">Sept!GPT4’s answer</a></p>



<p><strong>Assessment</strong>: The fuck did I just read?</p>



<p>It’s hard to judge whether it does better or worse than March!GPT4 or June!GPT4. I’m starting to think this puzzle is simply beyond GPT4’s pay grade. I’ve tried it dozens of times and with many wordings. It does not ever solve it.</p>



<p>It grasps the <em>principle</em>, grasps the underlying method (burning ropes at multiple ends), but it always makes the same mistake—burning the wrong rope, and then trying to weasel out by saying “measure time using some other method.”</p>



<p>Gemini will launch soon. I wonder if it can solve it?</p>



<p><strong>Conclusion</strong>: Rope burning puzzles are a common class of interview question. GPT-4 can solve a two-rope variant easily. This three-rope variant is from <a href="https://momath.org/home/varsity-math/varsity-math-week-151/" target="_blank" rel="noopener">Varsity Math Week 151</a>. Notably, the <em>answer </em>is also on the internet (and probably in GPT4’s training data)…but it’s in Varsity Math Week 152. Maybe if both the puzzle and the answer were on the same page, GPT4 would solve it. I don’t know how this stuff works. </p>



<h2><span id="Conclusion"></span>Conclusion<span></span></h2>



<p>My tests (for the little they’re worth) show no consistent pattern. Sometimes GPT4 does better than before, sometimes worse. </p>



<p>This is not testing, it’s sheep-entrail reading. I <em>do </em>faintly suspect it’s worse on obscure general knowledge, but I don’t believe that hard enough to bet almost any amount of money.</p>



<p><a href="https://coagulopath.com/witches-vs-chatgpt/">As I’ve said before, AIs present a fluctuating target.</a> I do not think they have a stable “baseline” ability that remains resilient to prompting differences and stochastic factors. In any event, OpenAI has many dials they can turn behind the scenes. </p>



<p>We’d learn more about this beast over hundreds or thousands of tests. But GPT-4 is too expensive for that to be realistic. OpenAI could really help the community by offering an academic discount—not that weirdos on the internet would qualify, of course!</p>



<p>Lastly, a “good” response is more subjective than I thought. Is it better for a wrong answer to have 500 words or 10 words? Is it better for an AI to try and fail, or to simply give up? When a query has conflicting requirements (“I want a list of all college majors in the US. Do not use more than 10 words.”) what should it do? And when a technically “correct” answer is <em>bad for the user </em>(consider XY problems such as “how do I echo the last three letters of a filename?”—GPT4 flunks this one, by the way), what do we want an AI to do?</p>



<p>GPT4 has changed and will continue to change. We’d be wise to do the same. I will be far less confident in my future predictions. When I say AI has gotten dumb, I might be speaking into a mirror.</p>



<figure><p>
<iframe loading="lazy" title="Smash Mouth - All Star (Official Music Video)" width="500" height="281" src="https://www.youtube.com/embed/L_jWHffIx5E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>






			

					



<!--?$i;?-->




<p>Comments are moderated and may take up to 24 hours to appear.</p>

	<p>No comments yet.</p>

<p><a href="https://coagulopath.com/gpt-4-is-not-getting-worse/feed/"><abbr title="Really Simple Syndication">RSS</abbr></a>	<a href="https://coagulopath.com/gpt-4-is-not-getting-worse/trackback/" rel="trackback">TrackBack <abbr title="Universal Resource Locator">URL</abbr></a>
</p>








</div></div>
  </body>
</html>
