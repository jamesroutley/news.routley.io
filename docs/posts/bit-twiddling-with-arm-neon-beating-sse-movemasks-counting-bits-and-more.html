<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/porting-x86-vector-bitmask-optimizations-to-arm-neon">Original</a>
    <h1>Bit twiddling with Arm Neon: beating SSE movemasks, counting bits and more</h1>
    
    <div id="readability-page-1" class="page"><div id="fragment-25364" data-reflow="_p_content,_p_singlecolumn,1,1,12">
<div>





<div>
			<div><p><span><i>Danila Kutenin is a Senior Software Engineer at Google Cloud in the Data Processing Efficiency team.</i></span></p>
<p><span>With the rise of Arm hardware in major cloud providers like Google (see our </span><a href="https://cloud.google.com/blog/products/compute/tau-t2a-is-first-compute-engine-vm-on-an-arm-chip"><span>release</span></a><span>), Amazon and Microsoft, and </span><span>new client devices</span><span>, better-optimized applications are looking to support Arm NEON and SVE. This post showcases some optimization techniques we used when porting from x86 to Arm.</span></p>
<p><span>When moving to Arm, the application code needs to be recompiled. While this is often straightforward, one challenge can be porting hand-written x86 intrinsics code to make the best use of the Arm architecture. There are <a href="https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/porting-sse-to-neon-are-libraries-the-way-forward">a few </a></span><span><a href="https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/porting-sse-to-neon-are-libraries-the-way-forward">SSE to Neon porting blogs</a> that can make it easy to get something running, but they focus on portability and can sometimes be further optimized for best performance.</span></p>
<p><span>At Google, we found that some workloads were up to 2x slower if we used portable libraries or directly replaced x86 intrinsics with equivalent Arm instruction sequences. We would like to share our experience to highlight some under-appreciated Arm optimizations and showcase how they can benefit widely used libraries like hashtables, ZSTD, <code>strlen</code>, <code>memchr</code>, <code>memcmp</code>, variable integer, and more.</span></p>
<p><span>When comparing Arm NEON with the SSE instruction set, most instructions are present in both. E.g. 16-byte memory loads (</span><code><span>_mm_loadu_si128</span></code><span> and <a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/vld1q_u8">vld1q_u8</a></span><span>), vector comparisons (</span><code><span>_mm_cmpgt_epi8</span></code><span> and <a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/vcgtq_s8">vcgtq_s8</a></span><span>) or byte shuffles (</span><code><span>_mm_shuffle_epi8</span></code><span> and </span><a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/vqtbl1q_s8"><code><span>vqtbl1q_s8</span></code></a><span><a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/vqtbl1q_s8">)</a>. However, developers often encounter problems with Arm NEON instructions being expensive to move to scalar code and back. This is especially true for the Move Byte Mask (<code>PMOVMSKB</code>) instruction.</span></p>
<p><span>Move Byte Mask (<code>PMOVMSKB</code>) is an x86 SSE2 instruction that creates a mask from the most significant bit of each 8-bit lane in a 128-bit register and writes the result to a general-purpose register. It is often used in tandem with vector comparison instructions, like <code>PCMPEQB,</code> to quickly scan a buffer for some byte value,16 characters at a time.</span></p>
<p><span>For example, suppose we want to index occurrences of the space character (0x20) in the string, “Call me Ishmael.”  Using SSE2 instructions, we could do the following:</span></p>
<p><img alt=" " src="https://community.arm.com/resized-image/__size/2530x1196/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-67/pastedimage1660122975380v1.png"/></p>
<p><span>Then it becomes straightforward to know if a vector has some matching character (just comparing this mask to zero) or finding the first matching character, all you need to do is to compute the number of trailing zeros through <code>bsf</code> (Bit Scan Forward) or <code>tzcnt</code> instructions. Such an approach is also used together with bit iteration in modern libraries like Google SwissMap and ZSTD compression:</span></p>
<pre><span><pre data-mode="c_cpp">ZSTD_VecMask ZSTD_row_getSSEMask(int nbChunks, const BYTE* const src,
                                 const BYTE tag, const U32 head) {
// …
    const __m128i chunk = _mm_loadu_si128((const __m128i*)(src + 16*i));
    const __m128i equalMask = _mm_cmpeq_epi8(chunk, comparisonMask);
    matches[i] = _mm_movemask_epi8(equalMask);
// …
}
MEM_STATIC U32 ZSTD_VecMask_next(ZSTD_VecMask val) {
    return ZSTD_countTrailingZeros64(val);
}
for (; (matches &gt; 0) &amp;&amp; (nbAttempts &gt; 0); --nbAttempts, matches &amp;= (matches - 1)) {
    U32 const matchPos = (head + ZSTD_VecMask_next(matches)) &amp; rowMask;
    // …
}</pre><br/></span></pre>
<p><span><code>matches &amp;= (matches - 1)</code> sets the lowest bit of 1 to zero and is sometimes referred to as <a href="https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetKernighan">Kernighan&#39;s algorithm</a>.</span></p>
<p><span>Such techniques are used in string comparisons, byte searches, and more. For example, in <code>strlen</code>, the algorithm should find the first zero byte, in <code>memcmp</code> the first non-matching byte, in <code>memchr</code> the first matching character. Arm NEON does not have a <code>PMOVMSKB</code> equivalent which prevents it from benefiting from the same approach. Direct translation from x86 would require a redesign of programs or emulating x86 intrinsics which would be suboptimal. Let us look at some examples using <a href="https://github.com/DLTcollab/sse2neon">SSE2NEON</a> and <a href="https://github.com/simd-everywhere/simde">SIMDe</a>:</span></p>
<p><span></span><a href="https://github.com/DLTcollab/sse2neon/blob/159be6a2d08663a8063d259230d1cb3572fd6026/sse2neon.h#L4761"><span>SSE2NEON</span></a><span>:</span></p><pre data-mode="c_cpp">int _mm_movemask_epi8(__m128i a) {
    uint8x16_t input = vreinterpretq_u8_m128i(a);
    uint16x8_t high_bits = vreinterpretq_u16_u8(vshrq_n_u8(input, 7));
    uint32x4_t paired16 = vreinterpretq_u32_u16(vsraq_n_u16(high_bits, high_bits, 7));
    uint64x2_t paired32 = vreinterpretq_u64_u32(vsraq_n_u32(paired16, paired16, 14));
    uint8x16_t paired64 = vreinterpretq_u8_u64(vsraq_n_u64(paired32, paired32, 28));
    return vgetq_lane_u8(paired64, 0) | ((int) vgetq_lane_u8(paired64, 8) &lt;&lt; 8);
}</pre></div>
				</div>
		</div>

</div></div>
  </body>
</html>
