<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nootch.net/post/kubernetes-at-home-with-k3s/">Original</a>
    <h1>Kubernetes at Home with K3s</h1>
    
    <div id="readability-page-1" class="page"><div>
    

<p><em>It‚Äôs been a while on the blog! I promise there will be more regular updates from now on, but maybe not always about tech‚Ä¶</em></p>



<p>I know what you‚Äôre thinking - Kubernetes? On a home server? Who‚Äôd be <em>that</em> crazy? Well, a while ago I‚Äôd agree but a few things have changed my mind recently.</p>

<p>I‚Äôve started a new job at a small startup that doesn‚Äôt have a DevOps team with Kubernetes (K8s from now on) knowledge on board, and even as a long-term K8s hater due to its complexity, I‚Äôve been forced to admit that I miss its programmatic approach to deployments and pod access. There, I‚Äôve said it! Also, I must admit the excitement of taming a beast of such complexity had been calling out to me for a while now. Besides, K8s is eating the world - knowing more about it can‚Äôt hurt.</p>

<p>I‚Äôm still not a <em>huge fan</em> of K8s, but Docker has <a href="https://chrisshort.net/docker-inc-is-dead/">kind of imploded</a> and its Swarm project has been long dead, <a href="https://www.nomadproject.io/">Nomad</a> isn‚Äôt much better (or 100% free, since some features are behind an ‚ÄúEnterprise‚Äù paywall) and <a href="https://mesos.apache.org/">Mesos</a> hasn‚Äôt gathered critical mass. Which sadly makes K8s the last production-level container orchestration technology left standing. Don‚Äôt take this as an endorsement of it - in IT, we know sometimes success does not equate to quality (see Windows circa 1995). And like I‚Äôve said, it‚Äôs way too complex, but recent advancements in tooling have made operating it a much easier endeavor.</p>

<p>As for why I‚Äôll be using it for my personal server, it mostly comes down to reproducibility. My current setup runs around 35 containers for a bunch of services like a wiki, Airsonic music streaming server, MinIO for S3 API compatible storage and a lot more, plus Samba and NFS servers which are consumed by Kodi on my Shield TV and our four work PCs/laptops at home.</p>

<p>I‚Äôve been content running all of this on <a href="https://www.openmediavault.org/">OpenMediaVault</a> for close to 5 years now, but its pace of development has slowed and, being Debian based, it suffers from the ‚Äúrelease‚Äù problem. Every time Debian goes up in major version things inevitably break for a while. I‚Äôve lived with it since Debian 8 or 9, but the recent release of 11 broke stuff heavily and it‚Äôs time for a change. I also suspect the slowing of development for OpenMediaVault is due to the increased adoption of K8s by typical nerdy NAS owners, given the amount of ‚Äúeasy‚Äù <a href="https://github.com/topics/k8s-at-home">K8s templates</a> on Github and <a href="https://discord.com/invite/k8s-at-home">Discord servers</a> dedicated to it. Trusting templates that throw the kitchen sink at a problem is not really my style, and if I‚Äôm to trust something with managing my home server, I insist on understanding it.</p>

<p>Still, it‚Äôs not like the server is a time sink right now - updates are automated and I rarely have to tweak it once setup. Currently it has 161 days of uptime! However, reproducing my setup would be mostly a manual affair. Reinstall OpenMediaVault, add the ZFS plugin, import my 4-disk ZFS pool, configure the Samba and NFS shares, reinstall <a href="https://www.portainer.io/">Portainer</a>, re-import all of my <code>docker-compose</code> files‚Ä¶ it‚Äôs a bit much, and mostly manual. Since K8s manages <em>state</em> for a cluster, it should (in theory) be super simple to just reinstall my server, add ZFS support, import the pool, run a script that recreates all deployments and voila! In theory.</p>

<p>But hold on. If you‚Äôre totally new to it - just what <em>is</em> Kubernetes?</p>



<p><a href="https://kubernetes.io">Kubernetes</a> (Greek for ‚Äúhelmsman‚Äù) is a container orchestration product originally created at Google. However they don‚Äôt use it internally much, which gives credence to the theory that it‚Äôs an elaborate Trojan horse to makes sure no rival company will ever challenge them in the future, because they‚Äôll be spending all of their time managing the thing (it‚Äôs notoriously complex). üòÑ</p>

<p>In a nutshell, you install it on a server or more likely a cluster, and can then deploy different types of workloads to it. It takes care of spawning containers for you, scaling them, namespacing them, managing network access rules, you name it. You mainly interact with it by writing YAML files and then <em>applying</em> them to the cluster, usually with a CLI tool called <code>kubectl</code> that validates and transforms the YAML into a JSON payload which is then sent to the cluster‚Äôs REST API endpoint.</p>

<p>There are many concepts in K8s, but I‚Äôll just go over the main ones:</p>

<ul>
<li><strong>Pods</strong> basic work unit, which is roughly a single container, or a set of containers. Pods are guaranteed to be present on the same node of the cluster. K8s assigns the IPs for pods, so you don‚Äôt have to manage them. Containers inside a pod can all reach each other, but not containers running on other pods. You shouldn‚Äôt manage pods directly though, that‚Äôs the job of Services.</li>
<li><strong>Services</strong> are entry points to sets of pods, and make it easier to manage them as a single unit. They don‚Äôt manage Pods directly (they use ReplicaSets) but you don‚Äôt even need to know what a ReplicaSet is most of the time. Services identify the Pods they control with <em>labels</em>.</li>
<li><strong>Labels</strong> every object in K8s can have metadata attached to it. Labels are a form of it, annotations are another. Most actions in K8s can be scoped with a <em>selector</em> that targets a specific label being present.</li>
<li><strong>Volumes</strong> just like for Docker, volumes connect containers to storage. For production use you‚Äôd have S3 or something with similar guarantees but for this home server use case, we‚Äôll just use <code>hostPath</code> type mounts that map directly to folders on the server. K8s complicates this a bit for most cases - you need to have a <code>PersistentVolume</code> (PV) declared, and a <code>PersistentVolumeClaim</code> (PVC) to actually access it. You could just use direct <code>hostPath</code> mounts on a Deployment, but PVs and PVCs give you more control over the volume‚Äôs use.</li>
<li><strong>Deployments</strong> are kind of the main work units. They declare what Docker image to use, which service the Deployment is part of via labels, which volumes to mount and ports to export, and optional security concerns.</li>
<li><strong>ConfigMaps</strong> are where you store configuration data in key-value form. The environment for a Deployment can be taken from a ConfigMap - all of it, or specific keys.</li>
<li><strong>Ingress</strong> without these, your Pods will be running but not exposed to the outside world. We‚Äôll be using <code>nginx</code> ingresses in this blog post.</li>
<li><strong>Jobs</strong> and <strong>CronJobs</strong> are one-off or recurrent workloads that can be executed.</li>
</ul>

<p>There are more concepts to master, and third-party tools can add to this list and extend a K8s cluster with custom objects called <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CRDs</a>. The <a href="https://kubernetes.io/docs/concepts/">official docs</a> are a good place to learn more. For now, these will get us a long way towards a capable working example.</p>



<h4 id="step-1-linux-installation">Step 1 - Linux installation</h4>

<p>To get started, I recommend you use VirtualBox (it‚Äôs free) and install a basic Debian 11 VM with no desktop, just running OpenSSH. Other distros might work, but I did most of the testing on Debian. I plan to move to Arch in the future to avoid the ‚Äúrelease problem‚Äù, but one change at a time. üòì After you master the VM setup, moving to a physical server should pose no issue.</p>

<p>To prevent redoing the install from scratch when you make a mistake (I made <em>a lot</em> of them until I figured it all out) you might want to clone the installation VM into a new one. That way, you can just delete the clone VM and clone the master one again and try again. You can also use snapshots on the master VM, but I found cloning to be more intuitive.</p>


  <figure>
    <img src="https://blog.vsinha.com/img/post/k8s-at-home/vbox-clone.png" alt="Cloning a VM"/>
    
      <figcaption>Cloning a VM</figcaption>
    
  </figure>



<p>Make sure your cloned VM‚Äôs network adapter is set to <code>Bridged</code> and has the same MAC address as the master VM, so you get the same IP all the time. It will also make forwarding ports on your home router easier.</p>


  <figure>
    <img src="https://blog.vsinha.com/img/post/k8s-at-home/vbox-mac.png" alt="Setting MAC address for the VM&#39;s network adapter"/>
    
      <figcaption>Setting MAC address for the VM&#39;s network adapter</figcaption>
    
  </figure>



<p>Make sure to route the following ports on your home router to the VM‚Äôs IP address:</p>

<ul>
<li>80/TCP http</li>
<li>443/TCP https</li>
</ul>

<p>You must also forward the following if you‚Äôre not in the same LAN as the VM or if you‚Äôre using a remote (DigitalOcean, Amazon, etc.) server:</p>

<ul>
<li>22/TCP ssh</li>
<li>6443/TCP K8s API</li>
<li>10250/UDP kubelet</li>
</ul>

<p>Before proceeding, make sure you add your SSH key to the server and when you SSH into it, you get a root shell without a password prompt. If you add:</p>

<pre><code>Host k3s
  User root
  Hostname &lt;your VM or server&#39;s IP&gt;
</code></pre>

<p>to your <code>.ssh/config</code> file, when you <code>ssh k3s</code> you should get the aforementioned root prompt.</p>

<p>At this time, you should install <code>kubectl</code> too. I recommend the <a href="https://github.com/asdf-community/asdf-kubectl">asdf plugin</a>.</p>

<h4 id="step-2-k3s-installation">Step 2 - k3s installation</h4>

<p>Full-blown Kubernetes is complex and heavy on resources, so we‚Äôll be using a lightweight alternative called <a href="https://k3s.io/">K3s</a>, a nimble single-binary solution that is 100% compatible with normal K8s.</p>

<p>To install K3s, and to interact with our server, I‚Äôll be using a <code>Makefile</code> (old-school, that‚Äôs how I roll). At the top, a few variables for you to fill in:</p>

<pre><code># set your host IP and name
HOST_IP=192.168.1.60
HOST=k3s
# do not change the next line
KUBECTL=kubectl --kubeconfig ~/.kube/k3s-vm-config
</code></pre>

<p>The IP is simple enough to set, and <code>HOST</code> is the label for the server in the <code>.ssh/config</code> file as above. It‚Äôs just easier to use it than <code>user@HOST_IP</code>, but feel free to modify the Makefile as you see fit. The <code>KUBECTL</code> variable will make more sense once we install K3s. Add the following target to the Makefile:</p>

<pre><code>k3s_install:
  ssh ${HOST} &#39;export INSTALL_K3S_EXEC=&#34; --no-deploy servicelb --no-deploy traefik&#34;; \
    curl -sfL https://get.k3s.io | sh -&#39;
  scp ${HOST}:/etc/rancher/k3s/k3s.yaml .
  sed -r &#39;s/(\b[0-9]{1,3}\.){3}[0-9]{1,3}\b&#39;/&#34;${HOST_IP}&#34;/ k3s.yaml &gt; ~/.kube/k3s-vm-config &amp;&amp; rm k3s.yaml
</code></pre>

<p>OK, a few things to unpack. The first line SSHs into the server and installs K3s, skipping a few components.</p>

<ul>
<li><code>servicelb</code> we don‚Äôt need load balancing on a single server</li>
<li><code>traefik</code> we‚Äôll be using nginx for ingresses, so no need to install this ingress controller</li>
</ul>

<p>The second line copies over the <code>k3s.yaml</code> file from the server that is created after installation and includes a certificate to contact its API. The third line replaces the <code>127.0.0.1</code> IP in the server configuration with the server‚Äôs IP on this local copy of the file, and copies it over to the <code>.kube</code> folder in your $HOME folder (make sure it exists). This is where <code>kubectl</code> will pick it up, since we‚Äôve set the <code>KUBECTL</code> variable in the Makefile for this file explicitly.</p>

<p>This is the expected output:</p>

<pre><code>ssh k3s &#39;export INSTALL_K3S_EXEC=&#34; --no-deploy servicelb --no-deploy traefik&#34;; \
  curl -sfL https://get.k3s.io | sh -&#39;
[INFO]  Finding release for channel stable
[INFO]  Using v1.21.7+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.7+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.7+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Creating /usr/local/bin/ctr symlink to k3s
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service ‚Üí /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
scp k3s:/etc/rancher/k3s/k3s.yaml .
k3s.yaml                                         100% 2957    13.1MB/s   00:00
sed -r &#39;s/(\b[0-9]{1,3}\.){3}[0-9]{1,3}\b&#39;/&#34;YOUR HOST IP HERE&#34;/ k3s.yaml &gt; ~/.kube/k3s-vm-config &amp;&amp; rm k3s.yaml
</code></pre>

<p>I assume your distro has <code>sed</code> installed, as most do. To test that everything is working, a simple <code>kubectl --kubeconfig ~/.kube/k3s-vm-config get nodes</code> now should yield:</p>

<pre><code>NAME     STATUS   ROLES                  AGE    VERSION
k3s-vm   Ready    control-plane,master   2m4s   v1.21.7+k3s1
</code></pre>

<p>Our K8s cluster is now ready to receive workloads!</p>

<h4 id="step-2-5-clients-optional">Step 2.5 Clients (optional)</h4>

<p>If you want to have a nice UI to interact with your K8s setup, there are two options.</p>

<ul>
<li><a href="https://k9scli.io/">k9s</a> (CLI) I quite like it, very easy to work with and perfect for remote setups</li>
</ul>


  <figure>
    <img src="https://blog.vsinha.com/img/post/k8s-at-home/k9s.png" alt="k9s screenshot"/>
    
      <figcaption>k9s screenshot</figcaption>
    
  </figure>



<ul>
<li><a href="https://k8slens.dev/">Lens</a> (GUI) My go-to recently, I quite like the integrated metrics</li>
</ul>


  <figure>
    <img src="https://blog.vsinha.com/img/post/k8s-at-home/lens.png" alt="Lens screenshot"/>
    
      <figcaption>Lens screenshot</figcaption>
    
  </figure>



<p>They should just pick up the presence of our cluster settings in <code>~/.kube</code>.</p>

<h4 id="step-3-nginx-ingress-let-s-encrypt-and-storage">Step 3 - nginx ingress, Let‚Äôs Encrypt and storage</h4>

<p>The next target on our Makefile will install the nginx ingress controller and Let‚Äôs Encrypt certificate manager, so our deployments can have valid TLS certificates (for free!). There‚Äôs also a default storage class, so that workloads without one set can use our default.</p>

<pre><code>base:
  ${KUBECTL} apply -f k8s/ingress-nginx-v1.0.4.yml
  ${KUBECTL} wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=60s
  ${KUBECTL} apply -f k8s/cert-manager-v1.0.4.yaml
  @echo
  @echo &#34;waiting for cert-manager pods to be ready... &#34;
  ${KUBECTL} wait --namespace=cert-manager --for=condition=ready pod --all --timeout=60s
  ${KUBECTL} apply -f k8s/lets-encrypt-staging.yml
  ${KUBECTL} apply -f k8s/lets-encrypt-prod.yml
</code></pre>

<p>You can find the files I‚Äôve used in <a href="https://gist.github.com/sardaukar/6a950ea9abcfe651e68876dbc6001fb0">this gist</a>. The nginx ingress YAML is sourced from <a href="https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml">this Github link</a> but with a single modification on line 323:</p>

<pre><code>dnsPolicy: ClusterFirstWithHostNet
hostNetwork: true
</code></pre>

<p>so that we can use DNS properly for our single server use case. More info <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">here</a>.</p>

<p>The <code>cert-manager</code> file is too big to go over, feel free to consult the <a href="https://cert-manager.io/docs/">docs</a> for it. To actually issue the Let‚Äôs Encrypt certificates, we need a <code>ClusterIssuer</code> object defined. We‚Äôll use two, one for the staging API and one for the production one. Use the staging issuer for experiments, since you won‚Äôt get rate-limited but bear in mind the certificates won‚Äôt be valid. Be sure to replace the email address in both issuers with your own.</p>

<pre><code># k8s/lets-encrypt-staging.yml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
  namespace: cert-manager
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: YOUR.EMAIL@DOMAIN.TLD
    privateKeySecretRef:
      name: letsencrypt-staging
    solvers:
    - http01:
        ingress:
          class: nginx
</code></pre>

<pre><code># k8s/lets-encrypt-prod.yml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
  namespace: cert-manager
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: YOUR.EMAIL@DOMAIN.TLD
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
</code></pre>

<p>If we ran all of the <code>kubectl apply</code> statements one after the other, the process would probably fail since we need the ingress controller to be ready before moving on to the cert-manager one. To that end, <code>kubectl</code> includes a handy <code>wait</code> sub-command that can take conditions and labels (remember those?) and halts the process for us until the components we need are ready. To just elaborate on the example from above:</p>

<pre><code>${KUBECTL} wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=60s
</code></pre>

<p>This waits until all pods matching the <code>app.kubernetes.io/component=controller</code> selector are in the <code>ready</code> condition for up to 60 seconds. If the timeout expires, the Makefile will stop. However, don‚Äôt worry if any of our Makefile‚Äôs targets errors out, since all of them are idempotent. You can run <code>make base</code> in this case multiple times, and if the cluster already has the definitions in place, they‚Äôll just go unchanged. Try it!</p>

<h4 id="step-4-portainer">Step 4 - Portainer</h4>

<p>I still quite like <a href="https://www.portainer.io/">Portainer</a> to manage my server, and as luck would have it, it support K8s as well as Docker. Let‚Äôs go bit by bit on the relevant bits of the YAML file for it.</p>

<pre><code>---
apiVersion: v1
kind: Namespace
metadata:
  name: portainer
</code></pre>

<p>Simply enough, Portainer defines its own namespace.</p>

<pre><code>---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    type: local
  name: portainer-pv
spec:
  storageClassName: local-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &#34;/zpool/volumes/portainer/claim&#34;
---
# Source: portainer/templates/pvc.yaml
kind: &#34;PersistentVolumeClaim&#34;
apiVersion: &#34;v1&#34;
metadata:
  name: portainer
  namespace: portainer
  annotations:
    volume.alpha.kubernetes.io/storage-class: &#34;generic&#34;
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  accessModes:
    - &#34;ReadWriteOnce&#34;
  resources:
    requests:
      storage: &#34;1Gi&#34;
</code></pre>

<p>This volume (and its associated claim) where Portainer stores its config. Notice that the <code>PersistentVolume</code> declaration mentions <code>nodeAffinity</code> to match the hostname of the server (or VM). I haven‚Äôt found a better way to do this yet.</p>

<pre><code>---
# Source: portainer/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: portainer
  namespace: portainer
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  type: NodePort
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
      nodePort: 30777
    - port: 9443
      targetPort: 9443
      protocol: TCP
      name: https
      nodePort: 30779
    - port: 30776
      targetPort: 30776
      protocol: TCP
      name: edge
      nodePort: 30776
  selector:
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
</code></pre>

<p>Here we see the service definition. Notice how the ports are specified (our ingress will use only one of them). Now for the deployment.</p>

<pre><code>---
# Source: portainer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: portainer
  namespace: portainer
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  replicas: 1
  strategy:
    type: &#34;Recreate&#34;
  selector:
    matchLabels:
      app.kubernetes.io/name: portainer
      app.kubernetes.io/instance: portainer
  template:
    metadata:
      labels:
        app.kubernetes.io/name: portainer
        app.kubernetes.io/instance: portainer
    spec:
      nodeSelector:
        {}
      serviceAccountName: portainer-sa-clusteradmin
      volumes:
        - name: portainer-pv
          persistentVolumeClaim:
            claimName: portainer
      containers:
        - name: portainer
          image: &#34;portainer/portainer-ce:latest&#34;
          imagePullPolicy: Always
          args:
          - &#39;--tunnel-port=30776&#39;
          volumeMounts:
            - name: portainer-pv
              mountPath: /data
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: https
              containerPort: 9443
              protocol: TCP
            - name: tcp-edge
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
          readinessProbe:
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
          resources:
            {}
</code></pre>

<p>A lot of this file is taken by metadata labels, but this is what ties the rest together. We see the volume mounts, Docker image being used, ports, plus the readiness and liveness probe definitions. They are used by K8s to determine when the pods are ready, and if they‚Äôre still up and responsive respectively.</p>

<pre><code>---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portainer-ingress
  namespace: portainer
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  rules:
    - host: portainer.domain.tld
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: portainer
                port:
                  name: http
  tls:
    - hosts:
      - portainer.domain.tld
      secretName: portainer-staging-secret-tls
</code></pre>

<p>Finally the ingress, that maps an actual domain name to this service. Make sure you have a domain pointing to your server‚Äôs IP, since the Let‚Äôs Encrypt challenge resolver depends on it being accessible to the world. In this case, A records pointing to your IP for <code>domain.tld</code> and <code>*.domain.tld</code> would be needed.</p>

<p>Notice how we obtain a certificate - we just need to annotate the ingress with <code>cert-manager.io/cluster-issuer: letsencrypt-staging</code> (or <code>prod</code>) and add the <code>tls</code> key with the host name(s) and the name of the secret where the TLS key will be stored. If you don‚Äôt want or need a certificate, just remove the annotation and the <code>tls</code> key.</p>

<blockquote>
<p>One thing to note here, I‚Äôm using <a href="https://kustomize.io/">Kustomize</a> to manage the YAML files for deployments. This is because another tool, <a href="https://kustomize.io/">Kompose</a>, outputs a lot of different YAML files when it converts <code>docker-compose</code> YAMLs to K8s ones. Kustomize makes it easier to apply them all at once.</p>
</blockquote>

<p>So here are the files needed for the Portainer deployment</p>

<pre><code># stacks/portainer/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - portainer.yaml
</code></pre>

<pre><code># stacks/portainer/portainer.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: portainer
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: portainer-sa-clusteradmin
  namespace: portainer
  labels:
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    type: local
  name: portainer-pv
spec:
  storageClassName: local-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &#34;/zpool/volumes/portainer/claim&#34;
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - k3s-vm
---
# Source: portainer/templates/pvc.yaml
kind: &#34;PersistentVolumeClaim&#34;
apiVersion: &#34;v1&#34;
metadata:
  name: portainer
  namespace: portainer
  annotations:
    volume.alpha.kubernetes.io/storage-class: &#34;generic&#34;
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  accessModes:
    - &#34;ReadWriteOnce&#34;
  resources:
    requests:
      storage: &#34;1Gi&#34;
---
# Source: portainer/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: portainer
  labels:
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  namespace: portainer
  name: portainer-sa-clusteradmin
---
# Source: portainer/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: portainer
  namespace: portainer
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  type: NodePort
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
      nodePort: 30777
    - port: 9443
      targetPort: 9443
      protocol: TCP
      name: https
      nodePort: 30779
    - port: 30776
      targetPort: 30776
      protocol: TCP
      name: edge
      nodePort: 30776
  selector:
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
---
# Source: portainer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: portainer
  namespace: portainer
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: &#34;ce-latest-ee-2.10.0&#34;
spec:
  replicas: 1
  strategy:
    type: &#34;Recreate&#34;
  selector:
    matchLabels:
      app.kubernetes.io/name: portainer
      app.kubernetes.io/instance: portainer
  template:
    metadata:
      labels:
        app.kubernetes.io/name: portainer
        app.kubernetes.io/instance: portainer
    spec:
      nodeSelector:
        {}
      serviceAccountName: portainer-sa-clusteradmin
      volumes:
        - name: portainer-pv
          persistentVolumeClaim:
            claimName: portainer
      containers:
        - name: portainer
          image: &#34;portainer/portainer-ce:latest&#34;
          imagePullPolicy: Always
          args:
          - &#39;--tunnel-port=30776&#39;
          volumeMounts:
            - name: portainer-pv
              mountPath: /data
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: https
              containerPort: 9443
              protocol: TCP
            - name: tcp-edge
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
          readinessProbe:
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
          resources:
            {}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portainer-ingress
  namespace: portainer
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  rules:
    - host: portainer.domain.tld
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: portainer
                port:
                  name: http
  tls:
    - hosts:
      - portainer.domain.tld
      secretName: portainer-staging-secret-tls

</code></pre>

<p>And the Makefile target:</p>

<pre><code>portainer:
  ${KUBECTL} apply -k stacks/portainer
</code></pre>

<p>Expected output:</p>

<pre><code>&gt; make portainer
kubectl --kubeconfig ~/.kube/k3s-vm-config apply -k stacks/portainer
namespace/portainer created
serviceaccount/portainer-sa-clusteradmin created
clusterrolebinding.rbac.authorization.k8s.io/portainer created
service/portainer created
persistentvolume/portainer-pv created
persistentvolumeclaim/portainer created
deployment.apps/portainer created
ingress.networking.k8s.io/portainer-ingress created
</code></pre>

<p>If you run it again, since it‚Äôs idempotent, you should see:</p>

<pre><code>&gt; make portainer
kubectl --kubeconfig ~/.kube/k3s-vm-config apply -k stacks/portainer
namespace/portainer unchanged
serviceaccount/portainer-sa-clusteradmin unchanged
clusterrolebinding.rbac.authorization.k8s.io/portainer unchanged
service/portainer unchanged
persistentvolume/portainer-pv unchanged
persistentvolumeclaim/portainer unchanged
deployment.apps/portainer configured
ingress.networking.k8s.io/portainer-ingress unchanged
</code></pre>



<p>Running an in-cluster Samba server is easy. Here are our YAMLs:</p>

<pre><code># stacks/samba/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

secretGenerator:
  - name: smbcredentials
    envs:
    - auth.env

resources:
  - deployment.yaml
  - service.yaml
</code></pre>

<p>Here we have a kustomization with multiple files. When we <code>apply -k</code> the folder this file is in, they‚Äôll all be merged into one.</p>

<p>The service is simple enough:</p>

<pre><code># stacks/samba/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: smb-server
spec:
  ports:
    - port: 445
      protocol: TCP
      name: smb
  selector:
    app: smb-server
</code></pre>

<p>The deployment too:</p>

<pre><code># stacks/samba/deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: smb-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: smb-server
  strategy:
    type: Recreate
  template:
    metadata:
      name: smb-server
      labels:
        app: smb-server
    spec:
      volumes:
        - name: smb-volume
          hostPath:
            path: /zpool/shares/smb
            type: DirectoryOrCreate
      containers:
        - name: smb-server
          image: dperson/samba
          args: [
            &#34;-u&#34;,
            &#34;$(USERNAME1);$(PASSWORD1)&#34;,
            &#34;-u&#34;,
            &#34;$(USERNAME2);$(PASSWORD2)&#34;,
            &#34;-s&#34;,
            # name;path;browsable;read-only;guest-allowed;users;admins;writelist;comment
            &#34;share;/smbshare/;yes;no;no;all;$(USERNAME1);;mainshare&#34;,
            &#34;-p&#34;
          ]
          env:
            - name: PERMISSIONS
              value: &#34;0777&#34;
            - name: USERNAME1
              valueFrom:
                secretKeyRef:
                  name: smbcredentials
                  key: username1
            - name: PASSWORD1
              valueFrom:
                secretKeyRef:
                  name: smbcredentials
                  key: password1
            - name: USERNAME2
              valueFrom:
                secretKeyRef:
                  name: smbcredentials
                  key: username2
            - name: PASSWORD2
              valueFrom:
                secretKeyRef:
                  name: smbcredentials
                  key: password2
          volumeMounts:
            - mountPath: /smbshare
              name: smb-volume
          ports:
            - containerPort: 445
              hostPort: 445
</code></pre>

<p>Notice we don‚Äô use a persistent volume and claim here, just a direct <code>hostPath</code>. We set its <code>type</code> to <code>DirectoryOrCreate</code> so that it will be created if not present.</p>

<p>We‚Äôre using the <a href="https://github.com/dperson/samba">dperson/samba</a> Docker image, that allows for on-the-fly setting of users and shares. Here I specify a single share, with two users (with <code>USERNAME1</code> as admin of the share). The users and passwords come from a simple env file:</p>

<pre><code># stacks/samba/auth.env
username1=alice
password1=foo

username2=bob
password2=bar
</code></pre>

<p>Our Makefile target for this is simple:</p>

<pre><code>samba:
  ${KUBECTL} apply -k stacks/samba
</code></pre>

<p>Expected output:</p>

<pre><code>&gt; make samba
kubectl --kubeconfig ~/.kube/k3s-vm-config apply -k stacks/samba
secret/smbcredentials-59k7fh7dhm created
service/smb-server created
deployment.apps/smb-server created
</code></pre>

<h4 id="step-6-bookstack">Step 6 - BookStack</h4>

<p>As an example of using Kompose to convert a <code>docker-compose.yaml</code> app into K8s files, let‚Äôs use <a href="https://github.com/BookStackApp/BookStack">BookStack</a>, a great wiki app of which I‚Äôm a fan.</p>

<p>This is my original <code>docker-compose</code> file for BookStack:</p>

<pre><code>version: &#39;2&#39;
services:
  mysql:
    image: mysql:5.7.33
    environment:
    - MYSQL_ROOT_PASSWORD=secret
    - MYSQL_DATABASE=bookstack
    - MYSQL_USER=bookstack
    - MYSQL_PASSWORD=secret
    volumes:
    - mysql-data:/var/lib/mysql
    ports:
    - 3306:3306

  bookstack:
    image: solidnerd/bookstack:21.05.2
    depends_on:
    - mysql
    environment:
    - DB_HOST=mysql:3306
    - DB_DATABASE=bookstack
    - DB_USERNAME=bookstack
    - DB_PASSWORD=secret
    volumes:
    - uploads:/var/www/bookstack/public/uploads
    - storage-uploads:/var/www/bookstack/storage/uploads
    ports:
    - &#34;8080:8080&#34;

volumes:
 mysql-data:
 uploads:
 storage-uploads:
</code></pre>

<p>Using Kompose is easy:</p>

<pre><code>&gt; kompose convert -f bookstack-original-compose.yaml
WARN Unsupported root level volumes key - ignoring
WARN Unsupported depends_on key - ignoring
INFO Kubernetes file &#34;bookstack-service.yaml&#34; created
INFO Kubernetes file &#34;mysql-service.yaml&#34; created
INFO Kubernetes file &#34;bookstack-deployment.yaml&#34; created
INFO Kubernetes file &#34;uploads-persistentvolumeclaim.yaml&#34; created
INFO Kubernetes file &#34;storage-uploads-persistentvolumeclaim.yaml&#34; created
INFO Kubernetes file &#34;mysql-deployment.yaml&#34; created
INFO Kubernetes file &#34;mysql-data-persistentvolumeclaim.yaml&#34; created
</code></pre>

<p>Right off the bat, we‚Äôre told our volumes and use of <code>depends_on</code> is not supported, which is a bummer. But they‚Äôre easy enough to fix. In the interest of brevity and not making this post longer, I‚Äôll just post the final result with some notes.</p>

<pre><code># stacks/bookstack/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - bookstack-build.yaml
</code></pre>

<pre><code># stacks/bookstack/bookstack-build.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    io.kompose.service: bookstack
  name: bookstack
spec:
  ports:
  - name: bookstack-port
    port: 10000
    targetPort: 8080
  - name: bookstack-db-port
    port: 10001
    targetPort: 3306
  selector:
    io.kompose.service: bookstack
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: bookstack-storage-uploads-pv
spec:
  capacity:
    storage: 5Gi
  hostPath:
    path: &gt;-
      /zpool/volumes/bookstack/storage-uploads
    type: DirectoryOrCreate
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-path
  volumeMode: Filesystem
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    io.kompose.service: bookstack-storage-uploads-pvc
  name: bookstack-storage-uploads-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-path
  volumeName: bookstack-storage-uploads-pv
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: bookstack-uploads-pv
spec:
  capacity:
    storage: 5Gi
  hostPath:
    path: &gt;-
      /zpool/volumes/bookstack/uploads
    type: DirectoryOrCreate
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-path
  volumeMode: Filesystem
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    io.kompose.service: bookstack-uploads-pvc
  name: bookstack-uploads-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-path
  volumeName: bookstack-uploads-pv
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: bookstack-mysql-data-pv
spec:
  capacity:
    storage: 5Gi
  hostPath:
    path: &gt;-
      /zpool/volumes/bookstack/mysql-data
    type: DirectoryOrCreate
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-path
  volumeMode: Filesystem
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    io.kompose.service: bookstack-mysql-data-pvc
  name: bookstack-mysql-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-path
  volumeName: bookstack-mysql-data-pv
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: bookstack-config
  namespace: default
data:
  DB_DATABASE: bookstack
  DB_HOST: bookstack:10001
  DB_PASSWORD: secret
  DB_USERNAME: bookstack
  APP_URL: https://bookstack.domain.tld
  MAIL_DRIVER: smtp
  MAIL_ENCRYPTION: SSL
  MAIL_FROM: user@domain.tld
  MAIL_HOST: smtp.domain.tld
  MAIL_PASSWORD: vewyvewysecretpassword
  MAIL_PORT: &#34;465&#34;
  MAIL_USERNAME: user@domain.tld
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: bookstack-mysql-config
  namespace: default
data:
  MYSQL_DATABASE: bookstack
  MYSQL_PASSWORD: secret
  MYSQL_ROOT_PASSWORD: secret
  MYSQL_USER: bookstack
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    io.kompose.service: bookstack
  name: bookstack
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: bookstack
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        io.kompose.service: bookstack
    spec:
      containers:
      - name: bookstack
        image: reddexx/bookstack:21112
        securityContext:
          allowPrivilegeEscalation: false
        envFrom:
        - configMapRef:
            name: bookstack-config
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: bookstack-uploads-pv
          mountPath: /var/www/bookstack/public/uploads
        - name: bookstack-storage-uploads-pv
          mountPath: /var/www/bookstack/storage/uploads
      - name: mysql
        image: mysql:5.7.33
        envFrom:
        - configMapRef:
            name: bookstack-mysql-config
        ports:
        - containerPort: 3306
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: bookstack-mysql-data-pv
      volumes:
      - name: bookstack-uploads-pv
        persistentVolumeClaim:
          claimName: bookstack-uploads-pvc
      - name: bookstack-storage-uploads-pv
        persistentVolumeClaim:
          claimName: bookstack-storage-uploads-pvc
      - name: bookstack-mysql-data-pv
        persistentVolumeClaim:
          claimName: bookstack-mysql-data-pvc
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: bookstack-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  rules:
    - host: bookstack.domain.tld
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: bookstack
                port:
                  name: bookstack-port
  tls:
    - hosts:
      - bookstack.domain.tld
      secretName: bookstack-staging-secret-tls
</code></pre>

<p>Kompose converts both containers inside the <code>docker-compose</code> file into services, which is fine, but I‚Äôve made them into a single service, which is preferable.</p>

<p>Notice how the config map holds all of the configuration for the app and is then injected into the deployment with:</p>

<pre><code>envFrom:
- configMapRef:
    name: bookstack-config
</code></pre>

<p>The <code>chown</code> segment in the Makefile has to do with how the BookStack docker image is set up. Most images don‚Äôt have this issue, but PHP ones are notorious for it. Without the proper permissions for the folder on the server, uploads to the wiki won‚Äôt work. But our Makefile accounts for it:</p>

<pre><code>bookstack:
  ${KUBECTL} apply -k stacks/bookstack
  @echo
  @echo &#34;waiting for deployments to be ready... &#34;
  @${KUBECTL} wait --namespace=default --for=condition=available deployments/bookstack --timeout=60s
  @echo
  ssh ${HOST} chmod 777 /zpool/volumes/bookstack/storage-uploads/
  ssh ${HOST} chmod 777 /zpool/volumes/bookstack/uploads/
</code></pre>

<p>Here we apply the kustomization, but then wait for both deployments to be ready, since that‚Äôs when their volume mounts are either bound or created on the server. We then SSH into it to change the volumes‚Äô owner to the correct user and group IDs. Not ideal, but works. The MySql image in the deployment doesn‚Äôt need this.</p>

<p>Notice also how easy it is to convert the <code>depends_on</code> directive from the <code>docker-compose</code> file, since the pods have access to each other by name, in much the same fashion.</p>

<h4 id="step-7-all-done">Step 7 - All done!</h4>

<p>Full code is <a href="https://github.com/sardaukar/k8s-at-home-with-k3s">available here</a>. Just for completion‚Äôs sake, the full Makefile:</p>

<pre><code># set your host IP and name
HOST_IP=192.168.1.60
HOST=k3s
#### don&#39;t change anything below this line!
KUBECTL=kubectl --kubeconfig ~/.kube/k3s-vm-config

.PHONY: k3s_install base bookstack portainer samba

k3s_install:
  ssh ${HOST} &#39;export INSTALL_K3S_EXEC=&#34; --no-deploy servicelb --no-deploy traefik&#34;; \
    curl -sfL https://get.k3s.io | sh -&#39;
  scp ${HOST}:/etc/rancher/k3s/k3s.yaml .
  sed -r &#39;s/(\b[0-9]{1,3}\.){3}[0-9]{1,3}\b&#39;/&#34;${HOST_IP}&#34;/ k3s.yaml &gt; ~/.kube/k3s-vm-config &amp;&amp; rm k3s.yaml

base:
  ${KUBECTL} apply -f k8s/ingress-nginx-v1.0.4.yml
  ${KUBECTL} wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=60s
  ${KUBECTL} apply -f k8s/cert-manager-v1.0.4.yaml
  @echo
  @echo &#34;waiting for cert-manager pods to be ready... &#34;
  ${KUBECTL} wait --namespace=cert-manager --for=condition=ready pod --all --timeout=60s
  ${KUBECTL} apply -f k8s/lets-encrypt-staging.yml
  ${KUBECTL} apply -f k8s/lets-encrypt-prod.yml

bookstack:
  ${KUBECTL} apply -k stacks/bookstack
  @echo
  @echo &#34;waiting for deployments to be ready... &#34;
  @${KUBECTL} wait --namespace=default --for=condition=available deployments/bookstack --timeout=60s
  @echo
  ssh ${HOST} chmod 777 /zpool/volumes/bookstack/storage-uploads/
  ssh ${HOST} chmod 777 /zpool/volumes/bookstack/uploads/

portainer:
  ${KUBECTL} apply -k stacks/portainer

samba:
  ${KUBECTL} apply -k stacks/samba
</code></pre>



<p>So, is this for you? It took me a few days to get everything working, and I bumped my head against the monitor a few times, but it gave me a better understanding of how Kubernetes works under the hood, how to debug it, and now with this Makefile it takes me all of 4 minutes to recreate my NAS setup for these 3 apps. I still have a dozen or so to convert from my old <code>docker-compose</code> setup, but it‚Äôs getting easier every time.</p>

<p>Kubernetes is interesting, and the abstractions built over it keep getting more powerful. There‚Äôs stuff like <a href="https://devtron.ai/">DevTron</a> and <a href="https://fluxcd.io/">Flux</a> that I want to explore in the future as well. Flux in particular is probably my next step, as I‚Äôd like to keep everything related to the server in a Git repo I host, and updating when I push new definitions. Or maybe my next step will be trying out <a href="https://nixos.org/">NixOS</a> the reproducible OS, which enables some interesting use cases, like <a href="https://grahamc.com/blog/erase-your-darlings">erasing and rebuilding the server on every boot</a>! üò≤ Or maybe <a href="https://k3os.io/">K3OS</a>, that is optimized for K3s and made by the <a href="https://rancher.com/">same people</a> that make K3s! If only I could get ZFS support for it‚Ä¶ ü§î</p>

<p>There‚Äôs always something new to learn with a server (or VM) at home! But for now, I‚Äôm happy where I got to, and I hope this post has helped you as well. Who knows, maybe those DevOps people are right when they say it is the future - I still think this is the beginning of a simpler solution down the line, for which the basic vocabulary is being established right now with Kubernetes. But at the very least, I hope it doesn‚Äôt feel so alien to you anymore!</p>



<p><em>Feel free to reply with comments or feedback to <a href="https://twitter.com/b_antunes/status/1467097844360400896">this tweet</a></em></p>



<p><strong>PS</strong> A tip, for when you go looking for info online - there are A LOT of articles and tutorials about Kubernetes. A good way to see if the info on them is stale or not is to check the YAML files. If you see a file that starts with</p>

<pre><code>apiVersion: apps/v1alpha1
</code></pre>

<p>that <code>v1alpha1</code> is a dead giveaway that the info in the post you‚Äôre reading is quite old, and may not work for current versions of K8s.</p>




  </div></div>
  </body>
</html>
