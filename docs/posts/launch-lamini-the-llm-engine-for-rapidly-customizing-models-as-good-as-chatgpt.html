<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lamini.ai/blog/introducing-lamini">Original</a>
    <h1>Launch Lamini: The LLM Engine for Rapidly Customizing Models as Good as ChatGPT</h1>
    
    <div id="readability-page-1" class="page"><div><h4>Training LLMs should be as easy as prompt-tuning ü¶æ</h4><p>Why is writing a prompt so easy, but training an LLM from a base model still so hard? Iteration cycles for fine-tuning on modest datasets are measured in months because it takes significant time to figure out why fine-tuned models fail. Conversely, prompt-tuning iterations are on the order of seconds, but performance plateaus in a matter of hours. Only a limited amount of data can be crammed into the prompt, not the terabytes of data in a warehouse.¬†</p><ul role="list"><li>‚ÄúOur team of 10 machine learning engineers hit the OpenAI fine-tuning API, but our model got worse ‚Äî help!‚Äù¬†</li><li>‚ÄúI don‚Äôt know how to make the best use of my data ‚Äî I‚Äôve exhausted all the prompt magic we can summon from tutorials online.‚Äù</li></ul><p>That‚Äôs why we‚Äôre building <a href="https://lamini.ai/">Lamini</a>: to give <strong>every developer the superpowers that took the world from GPT-3 to ChatGPT.¬†</strong></p><h4>Rapidly train LLMs to be as good as ChatGPT from any base model üöÄ</h4><p>Lamini is an <strong>LLM engine</strong> that allows any developer, not just machine learning experts, to train high-performing LLMs, as good as ChatGPT, on large datasets with just a few lines of code from the <a href="https://lamini-ai.github.io/">Lamini library</a> (check out an <a href="https://lamini-ai.github.io/example/">example here</a>!).</p><p><img src="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644c1da1ff728aa0a3402f5a_pull%20figure.png" loading="lazy" sizes="(max-width: 479px) 100vw, 750px" srcset="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644c1da1ff728aa0a3402f5a_pull%20figure-p-500.png 500w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644c1da1ff728aa0a3402f5a_pull%20figure-p-800.png 800w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644c1da1ff728aa0a3402f5a_pull%20figure.png 9980w" alt=""/></p><p>Lamini makes it easy to run multiple base model comparisons in just a single line of code, from OpenAI‚Äôs models to open-source ones on HuggingFace.</p><h3>Available now: a hosted data generator for LLM training üéâ</h3><p>We are excited to release several important steps to training your own LLM:</p><ul role="list"><li>The <a href="https://lamini-ai.github.io/">Lamini library</a> for optimized prompt-tuning and typed outputs (<a href="https://app.lamini.ai">try our playground</a> to see it now).</li><li>The advanced Lamini library for fine-tuning and RLHF, in just a few lines of code (<a href="https://lamini.ai/contact">sign up for early access</a>).</li><li>The <a href="https://github.com/lamini-ai/lamini/">first ever hosted data generator</a> for creating data needed to train instruction-following LLMs, licensed for commercial use (all yours, you own it!).</li><li>Open-source instruction-following LLM, using the above tools with only a few lines of code (<a href="https://huggingface.co/spaces/lamini/instruct-playground">play with it</a>).</li></ul><h4>Steps to a ChatGPT-like LLM for your use case 1Ô∏è‚É£2Ô∏è‚É£3Ô∏è‚É£</h4><p>Base models have a good understanding of English for consumer use cases. But when you need them to learn your vertical-specific language and guidelines, prompt-tuning is often not enough and you will need to build your own LLM.</p><p><img src="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1).png" loading="lazy" width="986" sizes="(max-width: 479px) 100vw, 66vw" srcset="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-500.png 500w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-800.png 800w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-1080.png 1080w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-1600.png 1600w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-2000.png 2000w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-2600.png 2600w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1)-p-3200.png 3200w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/64485f29c398644819c2ff50_meta%20steps%20diagram%20(1).png 7120w" alt=""/></p><ol start="0" role="list"><li><strong>Try prompt-tuning ChatGPT or another model</strong>. You can use <a href="https://lamini-ai.github.io/">Lamini library</a>‚Äôs APIs to quickly prompt-tune across different models, swapping between OpenAI and open-source models in just one line of code. We optimize the right prompt for you, so you can take advantage of different models without worrying about how to format the prompt for each model.</li><li><strong>Build a large dataset of input-output pairs.</strong> These will show your model how it should respond to its inputs, whether that&#39;s following instructions given in English, or responding in JSON. Today, we‚Äôre <a href="https://github.com/lamini-ai/lamini/">releasing a repo</a> with just a few lines of code using the Lamini library to generate 50k data points from as few as 100 data points, using the Lamini library to hit the Lamini engine, so you don‚Äôt have to spin up any GPUs. We include an open-source 50k dataset in the repo. (More details below on how you can do this!)</li><li><strong>Finetune a base model on your large dataset</strong>. Alongside the data generator, we‚Äôre also releasing an LLM that is fine-tuned on the generated data using Lamini. We‚Äôll soon be releasing the ability to do this programmatically (<a href="https://lamini.ai/contact">early access</a>). You can also hit OpenAI‚Äôs fine-tuning API as a great starting point.</li><li><strong>Run RLHF on your fine-tuned model</strong>. With Lamini, you no longer need a large ML and human labeling team to run RLHF.</li><li><strong>Deploy to your cloud.</strong> Simply hit the API endpoint in your product or feature.</li></ol><p>Lamini delivers the ease of prompt-tuning, with the performance of RLHF and fine-tuning. It will soon handle this entire process (<a href="https://lamini.ai/contact">sign up</a> for early access!).</p><h4>Deeper dive into step #1: a ChatGPT-like data generator </h4><p>For your application, you might want similar &#34;instruction-following&#34; data, but you could also want something completely different, like responding only in JSON.</p><p>You&#39;ll need a dataset of ~50k instruction-following examples to start. Don&#39;t panic. You can now use <a href="https://github.com/lamini-ai/lamini">Lamini‚Äôs hosted data generator</a> to turn just 100 examples into over 50k in just a few lines of code.¬†</p><h4>How the data generator works</h4><p><img src="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram.png" loading="lazy" sizes="(max-width: 479px) 100vw, 60vw" srcset="https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-500.png 500w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-800.png 800w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-1080.png 1080w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-1600.png 1600w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-2000.png 2000w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-2600.png 2600w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram-p-3200.png 3200w, https://global-uploads.webflow.com/63ebd7a58848e8a8f651aad0/644a0f9c37053f7e9a9ba899_final%20-%20Steps%20diagram.png 6475w" alt=""/></p><p>The Lamini data generator is a pipeline of LLMs that takes your original small set of 100+ instructions, paired with the expected responses, to generate 50k+ new pairs, inspired by <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca</a>. </p><p>The final generated dataset is available for your free commercial use (CC-BY license).¬†</p><p>‚Äç</p><h3>Releasing step #2: an open-source LLM, fine-tuned on generated data from step #1 using Lamini</h3><p>Some of the generated data is good, some not. Before fine-tuning, the next step is to filter the generated data to mostly high-quality data (<a href="https://github.com/lamini-ai/lamini/blob/main/remove_duplicates.py">just run this simple script</a> in the same repo). Lamini then creates a custom LLM by training a base model on this filtered, generated dataset.</p><p>We have released an <a href="https://huggingface.co/lamini/instruct-tuned-2.8b">open-source instruction-following LLM</a> (CC-BY license) using Lamini to train the Pythia base model with 37k generated instructions, filtered from 70k. Play with <a href="https://huggingface.co/spaces/lamini/instruct-playground">this custom LLM in the playground</a> now.</p><h4>Pushing the boundaries of fast &amp; usable generative AI</h4><p>We‚Äôre excited to dramatically improve the performance of training LLMs and make it easy for engineering teams to train them. These two frontiers are intertwined: with faster, more effective iteration cycles, more people will be able to build these models, beyond just fiddling with prompts. We exist to help any company unlock the power of generative AI by making it easy to put their own data to work.</p><p><strong>Team++: </strong>We are growing our team with people who are passionate about making it possible to build LLMs 10x faster and making them widely accessible to empower new, extraordinary use cases. If that‚Äôs you, please send your resume and a note to <a href="https://lamini.ai/cdn-cgi/l/email-protection#ef8c8e9d8a8a9d9caf838e82868186c18e86d09c9a8d858a8c9bd2ae9f9f8396868188cadddf89809dcadddf8e81cadddf8a818886818a8a9d868188cadddf9d80838acadddf8e9bcadddfa38e82868186ce"><span data-cfemail="caa9abb8afafb8b98aa6aba7a3a4a3e4aba3">[email¬†protected]</span></a>. ü§ù</p></div></div>
  </body>
</html>
