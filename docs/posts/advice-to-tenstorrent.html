<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/geohot/tt-tiny">Original</a>
    <h1>Advice to Tenstorrent</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><div><pre>Advice to Tenstorrent

If you want to get acquired / become scam IP licensing co...I can&#39;t help you.

If you want to win AI compute, read on

===

This is your 7th stack?

Plz bro one more stack this stack will be good i promise
bro bro bro plz one more make it all back one trade type beat

You can&#39;t build a castle on a shit swamp. LLK is the wrong approach.

===

Tenstorrent advantage is in more programmability wrt GPUs. Hardware shapes model arch.

If you don&#39;t expose that programmability, you are guaranteed to lose. sfpi_elu is a problem.

You aren&#39;t going to get better deals on tapeouts/IP than NVIDIA/AMD. You need some advantage.

But but but it&#39;s all open source.
Open source might get you bug fixes and features, but it won&#39;t get you core refactors.

===

If you want a dataflow graph compiler, build a dataflow graph compiler.
This is not 6 layers of abstraction, it&#39;s 3 (and only 2 you have to build).

1. frontend &lt;PyTorch, ONNX, tensor.py&gt;
2. compiler
3. runtime/driver

===

Start with 3.

The driver is fine.

The runtime should JUST BE A RUNTIME. I better never see mention of a elu.

Make the runtime expose hardware in a application agnostic way. Compilation, dispatch, queuing, etc...

As long as LLK sits under tt-metalium, you aren&#39;t doing this.

CUDA is a simple C API for this. I advise doing the same.

===

Now for 2.

tinygrad is this, but you don&#39;t have to use it. MLIR/LLVM is probably fine.

ELU still should not be here!!!!

This should deal with memory placement, op scheduling, kernel fusion. Not ELU.

This is not easy. But importing 6 abstraction layers of cruft doesn&#39;t fix that!!!!

===

Now for 1.

self.elu() needs to have same perf as self.relu() - alpha*(1-self.exp()).relu()

If it doesn&#39;t, you messed up. Only once it does are you ready to write elu.

HINT for how to write ELU: def elu(self, alpha=1.0): return self.relu() - alpha*(1-self.exp()).relu()

HINT is not a hint, it&#39;s the actual code.
</pre></div></div></div>
  </body>
</html>
