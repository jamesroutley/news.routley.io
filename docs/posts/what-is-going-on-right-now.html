<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://catskull.net/what-the-hell-is-going-on-right-now.html">Original</a>
    <h1>What is going on right now?</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    <div>
      <p>What the <em>hell</em> is going on right now?</p>

<p>Engineers are burning out. Orgs expect their senior engineering staff to be able to review and contribute to “vibe-coded” features that don’t work. My personal observation is that the best engineers are highly enthusiastic about helping newer team members contribute and learn.</p>

<p>Instead of their comments being taken to heart, reflected on, and used as learning opportunities, hapless young coders are instead using feedback as simply the next prompt in their “AI” masterpiece. I personally have witnessed and heard first-hand accounts where it was incredibly obvious a junior engineer was (ab)using LLM tools.</p>

<p>In a recent company town-hall, I watched as a team of junior engineers demoed their latest work. I couldn’t tell you what exactly it did, or even what it was supposed to do - it didn’t seem like they themselves understood. However, at a large enough organization, it’s not about what you do, its about what people <em>think</em> you do. Championing their “success”, a senior manager goaded them into bragging about their use of “AI” tools to which they responded “This is four thousand lines of code written by Claude”. Applause all around.</p>

<p>I was asked to add a small improvement to an existing feature. After reviewing the code, I noticed a junior engineer was the most recent to work on that feature. As I always do, I reached out to let them know what I’d be doing and to see if they had any insight that would be useful to me. Armed with the Github commit URL, I asked for context around their recent change. I can’t know <em>for sure</em>, but I’d be willing to put money down that my exact question and the commit were fed directly into an LLM which was then copy and pasted back to me. I’m not sure why, but I felt violated. It felt wrong.</p>

<p>A friend recently confided in me that he’s been on a team of at least 5 others that have been involved in reviewing a heavily vibe-coded PR over the past <em>month</em>. A month. Reviewing slop produced by an LLM. What are the cost savings of paying ChatGPT $20 a month and then having a literal <em>team</em> of engineers try and review and merge the code?</p>

<p>Another friend commiserated the difficulty of trying to help an engineer contribute at work. “I review the code, ask for changes, and then they <em>immediately</em> hit me with another round of AI slop.”</p>

<p>Here’s the thing - we <em>want</em> to help. We <em>want</em> to build good things. Things that work well, that make people’s lives easier. We want to teach people how to do software engineering! Any engineer is standing entirely on the shoulders of their mentors and managers who’ve invested time and energy into them and their careers. But what good is that investment if it’s simply copy-pasted into the latest “model” that “is literally half a step from artificial general intelligence”? Should we instead focus our time and energy into training the models and eliminate the juniors altogether?</p>

<p>What a sad, dark world that would be.</p>

<p>Here’s an experiment for you: stop using “AI”. Try it for a day. For a week. For a month.</p>

<p>Recently, I completely reset my computer. I like to do it from time to time. As part of that process I prune out any software that I no longer use. I’ve been paying for Claude Pro for about 6 months. But slowly, I’ve felt like it’s just a huge waste of time. Even if I have to do a few independent internet searches and read through a few dozen stack overflow and doc pages, my own conclusion is so much more reliable and accurate than anything an LLM could ever spit out.</p>

<p>So what good are these tools? Do they have any value whatsoever?</p>

<p>Objectively, it would seem the answer is no. But at least they make a lot of money, right?</p>

<p>Is anyone making money on AI right now? I see a pipeline that looks like this:</p>

<ul>
  <li>“AI” is applied to some specific, existing area, and a company spins up around it because it’s so much more “efficient”</li>
  <li>AI company gets funding from venture capitalists</li>
  <li>AI company give funding to AI service providers such as OpenAI in the form of paying for usage credits</li>
  <li>AI company evaporates</li>
</ul>

<p>This isn’t necessarily all that different than the existing VC pipeline, but the difference is that not even OpenAI is making money right now. I believe this is because the technology is inherently flawed and cannot scale to meet the demand. It simply consumes too much electricity to ever be economically viable, not to mention the serious environmental concerns.</p>

<p>We can say our prayers that Moore’s Law will come back from the dead and save us. We can say our prayers that the heat death of the universe will be sufficiently prolonged in order for every human to become a billionaire. We can also take an honestly not even hard look at reality and realize this is a scam.</p>

<p>The emperor is wearing no clothes.</p>

    </div>
  </article></div>
  </body>
</html>
