<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://asmirnov.xyz/doppelganger">Original</a>
    <h1>Learnings from fine-tuning LLM on my Telegram messages</h1>
    
    <div id="readability-page-1" class="page">
    
        <i>27 Nov 2023</i>
        
    <hr/>
    <p>For most people I interact with, I’m just another text-based
    program for the most of the time. If input and output are so simple,
    could I be replaced by the model? For this to work, the model would
    need to not only understand my writing style but also know a lot
    about me. The best source for this is my Telegram messenger, as I
    use it daily and it contains almost everything about my thoughts and
    actions in the form of chat histories.</p>
    <h2 id="approach"><a href="#approach">Approach</a></h2>
    <p>The most straightforward approach would be to extract all my
    messages, load them into ChatGPT’s context, and instruct it to use
    this information to mimic my style when responding to new messages.
    However, this approach is limited by the context window size,
    requiring me to preprocess messages to extract key points. As I want
    to avoid this hassle, perhaps Retrieval Augmented Generation (RAG)
    could be used to pull necessary information when needed. However
    from my experience, retrieving from diverse data like chat sessions
    usually needs a supervised fine-tuning of the retrieval model, and
    I’m not keen on creating such a dataset. So, fine-tuning seems like
    the best option. It’s ideal for several reasons: it should capture
    my writing style and potentially accumulate knowledge from all my
    messages without having to select what’s important.</p>
    <p>OpenAI offers <a href="https://platform.openai.com/docs/guides/fine-tuning" target="_blank">fine-tuning capabilities</a>, but as I’ll be using
    my private messages, I don’t want to use any third-party fine-tuning
    services. So, I need to choose a base model. According to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank">Hugging Face Open LLM Leaderboard</a>, one of the
    top smaller models (≤13B parameters) is <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" target="_blank">Mistral 7B</a>. It even outperforms <a href="https://huggingface.co/meta-llama/Llama-2-13b-hf" target="_blank">Llama 2 13B</a>. Now, the question is whether <a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA</a> is
    sufficient or if full fine-tuning is necessary. Various comparisons
    <a href="#fn1" id="fnref1" role="doc-noteref">[1]</a> <a href="#fn2" id="fnref2" role="doc-noteref">[2]</a>
    suggests that LoRA is a bit worse than full fine-tuning but still
    fine most of the time. However, for specific tasks like mine
    (Russian language + chat), I found a <a href="https://arxiv.org/abs/2304.08109" target="_blank">paper</a>,
    where researchers conducted Llama instruction fine-tuning in
    Chinese, similar in complexity to my goal. They found that
    LoRA-based tuning on a base model without prior instruction tuning
    is less effective than full fine-tuning. Yet, LoRA-based tuning on a
    model already fine-tuned for instructions can yield comparable
    results. In my case, this means either full fine-tuning on a base
    model or LoRA on a model already fine-tuned for chatting in Russian.
    Since I couldn’t find a model fine-tuned for Russian chat, I’ll try
    LoRA on a model fine-tuned for English chat, like the fine-tuned
    Mistral model <a href="https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b" target="_blank">Dolphin</a>.</p>
    <p>So, the plan is:</p>
    <ol type="1">
    <li>Start with LoRA on top of Dolphin, the English chat fine-tuned
    Mistral</li>
    <li>If quality is not sufficient, try full fine-tuning on
    Mistral</li>
    </ol>
    <h2 id="data-preparation"><a href="#data-preparation">Data
    preparation</a></h2>
    <p>One unique aspect of messaging in apps like Telegram, compared to
    emails, is the conversational flow. Messages don’t usually alternate
    one-by-one between you and your contact. Instead, you often find
    yourself sending a couple of messages in a row, followed by several
    responses from the other person. These messages are generally short,
    too. I wanted to preserve this natural conversational style in my
    data.</p>
    <p>Telegram offers a <a href="https://telegram.org/blog/export-and-more" target="_blank">built-in feature</a> to export all chats into JSON.
    After some filtering and grouping messages into sessions, I’ve
    compiled data from the last five years of using Telegram. This
    resulted in 15,789 sessions from 466 chats, with an average session
    length of 8.51 messages. For structuring the data, I’ve chosen the
    <a href="https://github.com/openai/openai-python/blob/284c1799070c723c6a553337134148a7ab088dd8/chatml.md" target="_blank">ChatML</a> prompt format. Here’s a sample session
    (translated from Russian):</p>
    <p>&lt;|im_start|&gt;John Smith</p>
    <details>
    <summary>
    original
    </summary>
    &lt;|im_start|&gt;Иван Иванович</details>
    <p>My data collator ensures that the loss is only calculated based
    on someone’s response. Predicting who will speak next is relatively
    straightforward, and we don’t want the model to focus on learning
    that. Therefore, parts of the conversation where the loss is
    calculated are highlighted in bold.</p>
    <p>You might notice that not only my responses but also those of
    others are used for loss calculation. This is deliberate. By doing
    this, the model will be able to role-play not only as me but also as
    my frequent conversational partners!</p>
    <h2 id="evaluation-plan"><a href="#evaluation-plan">Evaluation
    plan</a></h2>
    <p>I will test models by having chats in two ways. First, the model
    will pretend to be me and I will be chatting with myself from the
    perspective of my different friends. Then, I’ll chat as myself while
    the model acts as my friends. My conversation starter will always be
    the same 2 messages: “hey” and “what’s up?” (in Russian, “прив” and
    “как дела?”). Generated phrases and persons as the model acts who
    from will be <strong>highlighted</strong>. All conversations
    initially will be held in Russian and may be accessed by clicking on
    the ‘original’ details button. For testing I will be using <a href="https://github.com/oobabooga/text-generation-webui" target="_blank">oobabooga/text-generation-webui</a>.</p>
    <p>In the beginning, I want to explore how the generic conversation
    fine-tuned Mistral model deals with that task without any prior
    training from my side.</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <p>Ok, it is capable of forming coherent sentences. The most
    noticeable problem is its lack of awareness regarding the context of
    the conversations which leads to bland and generic replies. The
    messages lacked any distinct style, feeling quite basic. Another
    issue is that the model’s Russian is poor. This is expected, as the
    model is too small to generalize well to languages other than its
    primary one, English. Additionally, the model tended to be overly
    proactive, ending almost every sentence with a question, which isn’t
    how real people typically communicate in messengers.</p>
    <p>Let’s try to fix all of these!</p>
    <h2 id="lora"><a href="#lora">LoRA</a></h2>
    <p>LoRA offers a low-effort approach in terms of both the training
    pipeline and hardware requirements. It trains around 1% of the total
    weights. I chose a 1024 sequence length and a batch size of 8. The
    training, which consumed 20GB of VRAM on an RTX 3090, took three
    epochs and lasted for 5.5 hours. For this, I used <a href="https://vast.ai/" target="_blank">vast.ai</a>, where the GPU
    cost was $0.362 per hour, totaling $2 for the entire training,
    excluding time spent on experiments and bug fixes.</p>
    <p>Here are the results:</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 2 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 3 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 2</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 3</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <p>This is much better. It definitely captures the style of the
    person it’s responding on behalf of. It also identifies the most
    common topics discussed between specific pairs of people. For
    example, with friend 2, the focus is clearly more on work. However,
    the grammar is still off, and it loses the context of the
    conversation quickly. I’m pretty confident that LoRA would work with
    reasonable quality in English, and full fine-tuning might not be
    necessary. But, since Russian isn’t the model’s native language,
    let’s try full fine-tuning.</p>
    <h2 id="full-fine-tuning"><a href="#full-fine-tuning">Full
    fine-tuning</a></h2>
    <p>Full fine-tuning is more challenging due to the need for
    multi-GPU training. Popular methods include either <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" target="_blank">ZeRO &amp; DeepSpeed</a> <a href="#fn3" id="fnref3" role="doc-noteref">[3]</a>
    or <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/" target="_blank">FSDP</a> <a href="#fn4" id="fnref4" role="doc-noteref">[4]</a>, with FSDP
    essentially being a ZeRO3 <a href="#fn5" id="fnref5" role="doc-noteref">[5]</a>. I decided to go
    with FSDP.</p>
    <p>While implementing the training pipeline, I referred to the <a href="https://github.com/tatsu-lab/stanford_alpaca/" target="_blank">Stanford Alpaca fine-tuning code</a> and <a href="https://github.com/abacaj/fine-tune-mistral/" target="_blank">Anton Bacaj’s Mistral fine-tuning code</a>.</p>
    <p>Using a half-precision FSDP full shard with a 1024 sequence
    length and a micro batch size of 2 required 63GB of VRAM on each of
    the eight A100 80 GB GPUs. The training, lasting three epochs, took
    just 20 minutes. The total cost for the VM was $8.88 per hour,
    resulting in $3, not including the time for experiments and bug
    fixes.</p>
    <p>Conversations:</p>
    <p>---</p>
    <details>
    <summary>
    Friend 1 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 2 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Friend 3 vs <strong>Alexander Smirnov</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 1</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 2</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <details>
    <summary>
    Alexander Smirnov vs <strong>Friend 3</strong>
    </summary>
    <p>&gt;&gt;&gt; hey</p>
    </details>
    <details>
    <summary>
    original
    </summary>
    <p>&gt;&gt;&gt; прив</p>
    </details>
    <p>---</p>
    <p>Conversations have become more interesting and engaging, although
    there’s still a risk of losing context. Russian language performance
    has improved, but errors still occur. I believe that before
    fine-tuning for a specific task with limited data, like mine, it
    would be beneficial to first fine-tune the model unsupervised on a
    large corpus of Russian texts. Additionally, incorporating common
    conversation partners’ names as separate tokens might enhance the
    quality.</p>
    <p>I wouldn’t say it has turned out to be significantly better than
    LoRA. It might be more effective to focus solely on a single person
    and calculate the loss based only on my responses (or someone
    else’s), instead of trying to learn about each and every
    conversational partner.</p>
    <h2 id="closing-thoughts"><a href="#closing-thoughts">Closing
    thoughts</a></h2>
    <p>Certainly, I had to cherry-pick the results, not because most of
    the model’s replies were inadequate, but because many were simple
    responses like “I’ll call you later,” “busy,” and “ok,” which are
    naturally frequent in conversations. Despite this, it’s clear that
    the model excels in mimicking the style of the person it’s
    impersonating. It also captures the commonly discussed topics
    between two people. However, it significantly lacks context in
    conversations. Responding to queries like “yo, so?” or “what are
    your plans for the weekend” is challenging without having full
    context. Perhaps utilizing a system like <a href="https://www.rewind.ai/" target="_blank">Rewind</a>, which
    captures everything the user does across the computer, could be
    beneficial.</p>
    <h2 id="code"><a href="#code">Code</a></h2>
    <p>You can find code for this project as well as instructions on how
    to replicate it yourself on your own Telegram dump in <a href="https://github.com/furiousteabag/doppelganger" target="_blank">my github repo</a>. Training logs may be accessed on
    <a href="https://wandb.ai/furiousteabag/doppelganger" target="_blank">WandB</a>.</p>
    
    <hr/>
    <a href="https://blog.paulbiggar.com/">home</a>
  

</div>
  </body>
</html>
