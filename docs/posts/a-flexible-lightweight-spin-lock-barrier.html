<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nullprogram.com/blog/2022/03/13/">Original</a>
    <h1>A flexible, lightweight, spin-lock barrier</h1>
    
    <div id="readability-page-1" class="page"><div>
<article>
  
  <time datetime="2022-03-13">
    March 13, 2022
  </time>
  <p>
    nullprogram.com/blog/2022/03/13/
  </p>

  <p><em>This article was discussed <a href="https://news.ycombinator.com/item?id=30671979">on Hacker News</a>.</em></p>

<p>The other day I wanted try the famous <a href="https://preshing.com/20120515/memory-reordering-caught-in-the-act/">memory reordering experiment</a>
for myself. It’s the double-slit experiment of concurrency, where a
program can observe an <a href="https://research.swtch.com/hwmm">“impossible” result</a> on common hardware, as
though a thread had time-traveled. While getting thread timing as tight as
possible, I designed a possibly-novel thread barrier. It’s purely
spin-locked, the entire footprint is a zero-initialized integer, it
automatically resets, it can be used across processes, and the entire
implementation is just three to four lines of code.</p>

<!--more-->

<p>Here’s the entire barrier implementation for two threads in C11.</p>

<div><div><pre><code><span>// Spin-lock barrier for two threads. Initialize *barrier to zero.</span>
<span>void</span> <span>barrier_wait</span><span>(</span><span>_Atomic</span> <span>uint32_t</span> <span>*</span><span>barrier</span><span>)</span>
<span>{</span>
    <span>uint32_t</span> <span>v</span> <span>=</span> <span>++*</span><span>barrier</span><span>;</span>
    <span>if</span> <span>(</span><span>v</span> <span>&amp;</span> <span>1</span><span>)</span> <span>{</span>
        <span>for</span> <span>(</span><span>v</span> <span>&amp;=</span> <span>2</span><span>;</span> <span>(</span><span>*</span><span>barrier</span><span>&amp;</span><span>2</span><span>)</span> <span>==</span> <span>v</span><span>;);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Or in Go:</p>

<div><div><pre><code><span>func</span> <span>BarrierWait</span><span>(</span><span>barrier</span> <span>*</span><span>uint32</span><span>)</span> <span>{</span>
    <span>v</span> <span>:=</span> <span>atomic</span><span>.</span><span>AddUint32</span><span>(</span><span>barrier</span><span>,</span> <span>1</span><span>)</span>
    <span>if</span> <span>v</span><span>&amp;</span><span>1</span> <span>==</span> <span>1</span> <span>{</span>
        <span>v</span> <span>&amp;=</span> <span>2</span>
        <span>for</span> <span>atomic</span><span>.</span><span>LoadUint32</span><span>(</span><span>barrier</span><span>)</span><span>&amp;</span><span>2</span> <span>==</span> <span>v</span> <span>{</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Even more, these two implementations are compatible with each other. C
threads and Go goroutines can synchronize on a common barrier using these
functions. Also note how it only uses two bits.</p>

<p>When I was done with my experiment, I did a quick search online for other
spin-lock barriers to see if anyone came up with the same idea. I found a
couple of <a href="https://stackoverflow.com/questions/33598686/spinning-thread-barrier-using-atomic-builtins">subtly-incorrect</a> spin-lock barriers, and some
straightforward barrier constructions using a mutex spin-lock.</p>

<p>Before diving into how this works, and how to generalize it, let’s discuss
the circumstance that let to its design.</p>

<h3 id="experiment">Experiment</h3>

<p>Here’s the setup for the memory reordering experiment, where <code>w0</code> and <code>w1</code>
are initialized to zero.</p>

<div><div><pre><code>thread#1    thread#2
w0 = 1      w1 = 1
r1 = w1     r0 = w0
</code></pre></div></div>

<p>Considering all the possible orderings, it would seem that at least one of
<code>r0</code> or <code>r1</code> is 1. There seems to be no ordering where <code>r0</code> and <code>r1</code> could
both be 0. However, if raced precisely, this is a frequent or possibly
even majority occurrence on common hardware, including x86 and ARM.</p>

<p>How to go about running this experiment? These are concurrent loads and
stores, so it’s tempting to use <code>volatile</code> for <code>w0</code> and <code>w1</code>. However,
this would constitute a data race — undefined behavior in at least C and
C++ — and so we couldn’t really reason much about the results, at least
not without first verifying the compiler’s assembly. These are variables
in a high-level language, not architecture-level stores/loads, even with
<code>volatile</code>.</p>

<p>So my first idea was to use a bit of inline assembly for all accesses that
would otherwise be data races. x86-64:</p>

<div><div><pre><code><span>static</span> <span>int</span> <span>experiment</span><span>(</span><span>int</span> <span>*</span><span>w0</span><span>,</span> <span>int</span> <span>*</span><span>w1</span><span>)</span>
<span>{</span>
    <span>int</span> <span>r1</span><span>;</span>
    <span>__asm</span> <span>volatile</span> <span>(</span>
        <span>&#34;movl  $1, %1</span><span>\n</span><span>&#34;</span>
        <span>&#34;movl  %2, %0</span><span>\n</span><span>&#34;</span>
        <span>:</span> <span>&#34;=r&#34;</span><span>(</span><span>r1</span><span>),</span> <span>&#34;=m&#34;</span><span>(</span><span>*</span><span>w0</span><span>)</span>
        <span>:</span> <span>&#34;m&#34;</span><span>(</span><span>*</span><span>w1</span><span>)</span>
    <span>);</span>
    <span>return</span> <span>r1</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>ARM64 (to try on my Raspberry Pi):</p>

<div><div><pre><code><span>static</span> <span>int</span> <span>experiment</span><span>(</span><span>int</span> <span>*</span><span>w0</span><span>,</span> <span>int</span> <span>*</span><span>w1</span><span>)</span>
<span>{</span>
    <span>int</span> <span>r1</span> <span>=</span> <span>1</span><span>;</span>
    <span>__asm</span> <span>volatile</span> <span>(</span>
        <span>&#34;str  %w0, %1</span><span>\n</span><span>&#34;</span>
        <span>&#34;ldr  %w0, %2</span><span>\n</span><span>&#34;</span>
        <span>:</span> <span>&#34;+r&#34;</span><span>(</span><span>r1</span><span>),</span> <span>&#34;=m&#34;</span><span>(</span><span>w0</span><span>)</span>
        <span>:</span> <span>&#34;m&#34;</span><span>(</span><span>w1</span><span>)</span>
    <span>);</span>
    <span>return</span> <span>r1</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This is from the point-of-view of thread#1, but I can swap the arguments
for thread#2. I’m expecting this to be inlined, and encouraging it with
<code>static</code>.</p>

<p>Alternatively, I could use C11 atomics with a relaxed memory order:</p>

<div><div><pre><code><span>static</span> <span>int</span> <span>experiment</span><span>(</span><span>_Atomic</span> <span>int</span> <span>*</span><span>w0</span><span>,</span> <span>_Atomic</span> <span>int</span> <span>*</span><span>w1</span><span>)</span>
<span>{</span>
    <span>atomic_store_explicit</span><span>(</span><span>w0</span><span>,</span> <span>1</span><span>,</span> <span>memory_order_relaxed</span><span>);</span>
    <span>return</span> <span>atomic_load_explicit</span><span>(</span><span>w1</span><span>,</span> <span>memory_order_relaxed</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>Since this is a <em>race</em> and I want both threads to run their two experiment
instructions as simultaneously as possible, it would be wise to use some
sort of <em>starting barrier</em>… exactly the purpose of a thread barrier! It
will hold the threads back until they’re both ready.</p>

<div><div><pre><code><span>int</span> <span>w0</span><span>,</span> <span>w1</span><span>,</span> <span>r0</span><span>,</span> <span>r1</span><span>;</span>

<span>// thread#1                   // thread#2</span>
<span>w0</span> <span>=</span> <span>w1</span> <span>=</span> <span>0</span><span>;</span>
<span>BARRIER</span><span>;</span>                      <span>BARRIER</span><span>;</span>
<span>r1</span> <span>=</span> <span>experiment</span><span>(</span><span>&amp;</span><span>w0</span><span>,</span> <span>&amp;</span><span>w1</span><span>);</span>    <span>r0</span> <span>=</span> <span>experiment</span><span>(</span><span>&amp;</span><span>w1</span><span>,</span> <span>&amp;</span><span>w0</span><span>);</span>
<span>BARRIER</span><span>;</span>                      <span>BARRIER</span><span>;</span>

<span>if</span> <span>(</span><span>!</span><span>r0</span> <span>&amp;&amp;</span> <span>!</span><span>r1</span><span>)</span> <span>{</span>
    <span>puts</span><span>(</span><span>&#34;impossible!&#34;</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>The second thread goes straight into the barrier, but the first thread
does a little more work to initialize the experiment and a little more at
the end to check the result. The second barrier ensures they’re both done
before checking.</p>

<p>Running this only once isn’t so useful, so each thread loops a few million
times, hence the re-initialization in thread#1. The barriers keep them
lockstep.</p>

<h3 id="barrier-selection">Barrier selection</h3>

<p>On my first attempt, I made the obvious decision for the barrier: I used
<a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/pthread_barrier_wait.html"><code>pthread_barrier_t</code></a>. I was already using pthreads for spawning the
extra thread, including <a href="https://nullprogram.com/blog/2020/05/15/">on Windows</a>, so this was convenient.</p>

<p>However, my initial results were disappointing. I only observed an
“impossible” result around one in a million trials. With some debugging I
determined that the pthreads barrier was just too damn slow, throwing off
the timing. This was especially true with winpthreads, bundled with
Mingw-w64, which in addition to the per-barrier mutex, grabs a <em>global</em>
lock <em>twice</em> per wait to manage the barrier’s reference counter.</p>

<p>All pthreads implementations I used were quick to yield to the system
scheduler. The first thread to arrive at the barrier would go to sleep,
the second thread would wake it up, and it was rare they’d actually race
on the experiment. This is perfectly reasonable for a pthreads barrier
designed for the general case, but I really needed a <em>spin-lock barrier</em>.
That is, the first thread to arrive spins in a loop until the second
thread arrives, and it never interacts with the scheduler. This happens so
frequently and quickly that it should only spin for a few iterations.</p>

<h3 id="barrier-design">Barrier design</h3>

<p>Spin locking means atomics. By default, atomics have sequentially
consistent ordering and will provide the necessary synchronization for the
non-atomic experiment variables. Stores (e.g. to <code>w0</code>, <code>w1</code>) made before
the barrier will be visible to all other threads upon passing through the
barrier. In other words, the initialization will propagate before either
thread exits the first barrier, and results propagate before either thread
exits the second barrier.</p>

<p>I know statically that there are only two threads, simplifying the
implementation. The plan: When threads arrive, they atomically increment a
shared variable to indicate such. The first to arrive will see an odd
number, telling it to atomically read the variable in a loop until the
other thread changes it to an even number.</p>

<p>At first with just two threads this might seem like a single bit would
suffice. If the bit is set, the other thread hasn’t arrived. If clear,
both threads have arrived.</p>

<div><div><pre><code><span>void</span> <span>broken_wait1</span><span>(</span><span>_Atomic</span> <span>unsigned</span> <span>*</span><span>barrier</span><span>)</span>
<span>{</span>
    <span>++*</span><span>barrier</span><span>;</span>
    <span>while</span> <span>(</span><span>*</span><span>barrier</span><span>&amp;</span><span>1</span><span>);</span>
<span>}</span>
</code></pre></div></div>
<p>Or to avoid an extra load, use the result directly:</p>

<div><div><pre><code><span>void</span> <span>broken_wait2</span><span>(</span><span>_Atomic</span> <span>unsigned</span> <span>*</span><span>barrier</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>++*</span><span>barrier</span> <span>&amp;</span> <span>1</span><span>)</span> <span>{</span>
        <span>while</span> <span>(</span><span>*</span><span>barrier</span><span>&amp;</span><span>1</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Neither of these work correctly, and the other mutex-free barriers I found
all have the same defect. Consider the broader picture: Between atomic
loads in the first thread spin-lock loop, suppose the second thread
arrives, passes through the barrier, does its work, hits the next barrier,
and increments the counter. Both threads see an odd counter simultaneously
and deadlock. No good.</p>

<p>To fix this, the wait function must also track the <em>phase</em>. The first
barrier is the first phase, the second barrier is the second phase, etc.
Conveniently <strong>the rest of the integer acts like a phase counter</strong>!
Writing this out more explicitly:</p>

<div><div><pre><code><span>void</span> <span>barrier_wait</span><span>(</span><span>_Atomic</span> <span>unsigned</span> <span>*</span><span>barrier</span><span>)</span>
<span>{</span>
    <span>unsigned</span> <span>observed</span> <span>=</span> <span>++*</span><span>barrier</span><span>;</span>
    <span>unsigned</span> <span>thread_count</span> <span>=</span> <span>observed</span> <span>&amp;</span> <span>1</span><span>;</span>
    <span>if</span> <span>(</span><span>thread_count</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
        <span>// not last arrival, watch for phase change</span>
        <span>unsigned</span> <span>init_phase</span> <span>=</span> <span>observed</span> <span>&gt;&gt;</span> <span>1</span><span>;</span>
        <span>for</span> <span>(;;)</span> <span>{</span>
            <span>unsigned</span> <span>current_phase</span> <span>=</span> <span>*</span><span>barrier</span> <span>&gt;&gt;</span> <span>1</span><span>;</span>
            <span>if</span> <span>(</span><span>current_phase</span> <span>!=</span> <span>init_phase</span><span>)</span> <span>{</span>
                <span>break</span><span>;</span>
            <span>}</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The key: When the last thread arrives, it overflows the thread counter to
zero and increments the phase counter in one operation.</p>

<p>By the way, I’m using <code>unsigned</code> since it may eventually overflow, and
even <code>_Atomic int</code> overflow is undefined for the <code>++</code> operator. However,
if you use <code>atomic_fetch_add</code> or C++ <code>std::atomic</code> then overflow is
defined and you can use <code>int</code>.</p>

<p>Threads can never be more than one phase apart by definition, so only one
bit is needed for the phase counter, making this effectively a two-phase,
two-bit barrier. In my final implementation, rather than shift (<code>&gt;&gt;</code>), I
mask (<code>&amp;</code>) the phase bit with 2.</p>

<p>With this spin-lock barrier, the experiment observes <code>r0 = r1 = 0</code> in ~10%
of trials on my x86 machines and ~75% of trials on my Raspberry Pi 4.</p>

<h3 id="generalizing-to-more-threads">Generalizing to more threads</h3>

<p>Two threads required two bits. This generalizes to <code>log2(n)+1</code> bits for
<code>n</code> threads, where <code>n</code> is a power of two. You may have already figured out
how to support more threads: spend more bits on the thread counter.</p>

<div><div><pre><code><span>// Spin-lock barrier for n threads, where n is a power of two.</span>
<span>// Initialize *barrier to zero.</span>
<span>void</span> <span>barrier_waitn</span><span>(</span><span>_Atomic</span> <span>unsigned</span> <span>*</span><span>barrier</span><span>,</span> <span>int</span> <span>n</span><span>)</span>
<span>{</span>
    <span>unsigned</span> <span>v</span> <span>=</span> <span>++*</span><span>barrier</span><span>;</span>
    <span>if</span> <span>(</span><span>v</span> <span>&amp;</span> <span>(</span><span>n</span> <span>-</span> <span>1</span><span>))</span> <span>{</span>
        <span>for</span> <span>(</span><span>v</span> <span>&amp;=</span> <span>n</span><span>;</span> <span>(</span><span>*</span><span>barrier</span><span>&amp;</span><span>n</span><span>)</span> <span>==</span> <span>v</span><span>;);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Note: <strong>It never makes sense for <code>n</code> to exceed the logical core count!</strong>
If it does, then at least one thread must not be actively running. The
spin-lock ensures it does not get scheduled promptly, and the barrier will
waste lots of resources doing nothing in the meantime.</p>

<p>If the barrier is used little enough that you won’t overflow the overall
barrier integer — maybe just use a <code>uint64_t</code> — an implementation could
support arbitrary thread counts with the same principle using modular
division instead of the <code>&amp;</code> operator. The denominator is ideally a
compile-time constant in order to avoid paying for division in the
spin-lock loop.</p>

<p>While C11 <code>_Atomic</code> seems like it would be useful, unsurprisingly it is
not supported by one major, <a href="https://nullprogram.com/blog/2021/12/30/">stubborn</a> implementation. If you’re
using C++11 or later, then go ahead use <code>std::atomic&lt;int&gt;</code> since it’s
well-supported. In real, practical C programs, I will continue using dual
implementations: interlocked functions on MSVC, and GCC built-ins (also
supported by Clang) everywhere else.</p>

<div><div><pre><code><span>#if __GNUC__
#  define BARRIER_INC(x) __atomic_add_fetch(x, 1, __ATOMIC_SEQ_CST)
#  define BARRIER_GET(x) __atomic_load_n(x, __ATOMIC_SEQ_CST)
#elif _MSC_VER
#  define BARRIER_INC(x) _InterlockedIncrement(x)
#  define BARRIER_GET(x) _InterlockedOr(x, 0)
#endif
</span>
<span>// Spin-lock barrier for n threads, where n is a power of two.</span>
<span>// Initialize *barrier to zero.</span>
<span>static</span> <span>void</span> <span>barrier_wait</span><span>(</span><span>int</span> <span>*</span><span>barrier</span><span>,</span> <span>int</span> <span>n</span><span>)</span>
<span>{</span>
    <span>int</span> <span>v</span> <span>=</span> <span>BARRIER_INC</span><span>(</span><span>barrier</span><span>);</span>
    <span>if</span> <span>(</span><span>v</span> <span>&amp;</span> <span>(</span><span>n</span> <span>-</span> <span>1</span><span>))</span> <span>{</span>
        <span>for</span> <span>(</span><span>v</span> <span>&amp;=</span> <span>n</span><span>;</span> <span>(</span><span>BARRIER_GET</span><span>(</span><span>barrier</span><span>)</span><span>&amp;</span><span>n</span><span>)</span> <span>==</span> <span>v</span><span>;);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This has the nice bonus that the interface does not have the <code>_Atomic</code>
qualifier, nor <code>std::atomic</code> template. It’s just a plain old <code>int</code>, making
the interface simpler and easier to use. It’s something I’ve grown to
appreciate from Go.</p>

<p>If you’d like to try the experiment yourself: <a href="https://gist.github.com/skeeto/c63b9ddf2c599eeca86356325b93f3a7"><code>reorder.c</code></a>. If
you’d like to see a test of Go and C sharing a thread barrier:
<a href="https://gist.github.com/skeeto/bdb5a0d2aa36b68b6f66ca39989e1444"><code>coop.go</code></a>.</p>

<p>I’m intentionally not providing the spin-lock barrier as a library. First,
it’s too trivial and small for that, and second, I believe <a href="https://vimeo.com/644068002">context is
everything</a>. Now that you understand the principle, you can whip up
your own, custom-tailored implementation when the situation calls for it,
just as the one in my experiment is hard-coded for exactly two threads.</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>
  </body>
</html>
