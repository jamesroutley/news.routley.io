<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41557-025-01815-x">Original</a>
    <h1>Chemical knowledge and reasoning of large language models vs. chemist expertise</h1>
    
    <div id="readability-page-1" class="page"><div>
                    
                        <section data-title="Main"><div id="Sec1-section"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Large language models (LLMs) are machine learning (ML) models trained on massive amounts of text to complete sentences. Aggressive scaling of these models has led to a rapid increase in their capabilities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR1" id="ref-link-section-d7649163e1080">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Zhong, Z., Zhou, K. &amp; Mottin, D. Benchmarking large language models for molecule prediction tasks. Preprint at 
                https://doi.org/10.48550/arXiv.2403.05075
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR2" id="ref-link-section-d7649163e1083">2</a></sup>, with the leading models now being able to pass the US Medical Licensing Examination<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Kung, T. H. et al. Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. PLoS Digit. Health 2, e0000198 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR3" id="ref-link-section-d7649163e1087">3</a></sup> or other professional licensing exams. They also have been shown to design and autonomously perform chemical reactions when augmented with external tools such as web search and synthesis planners<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="OpenAI et al. Gpt-4 technical report. (2024); 
                https://doi.org/10.48550/arXiv.2303.08774
                
              " href="#ref-CR4" id="ref-link-section-d7649163e1091">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Boiko, D. A., MacKnight, R., Kline, B. &amp; Gomes, G. Autonomous chemical research with large language models. Nature 624, 570–578 (2023)." href="#ref-CR5" id="ref-link-section-d7649163e1091_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="M. Bran, A. et al. Augmenting large language models with chemistry tools. Nat. Mach. Intell. 6, 525–535 (2024)." href="#ref-CR6" id="ref-link-section-d7649163e1091_2">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Darvish, K. et al. ORGANA: A robotic assistant for automated chemistry experimentation and characterization. Matter 8, 101897 (2025)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR7" id="ref-link-section-d7649163e1094">7</a></sup>. While some see ‘sparks of artificial general intelligence (AGI)’ in them<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Bubeck, S. et al. Sparks of artificial general intelligence: early experiments with gpt-4. Preprint at 
                https://doi.org/10.48550/arXiv.2303.12712
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR8" id="ref-link-section-d7649163e1098">8</a></sup>, others see them as ‘stochastic parrots’—that is, systems that only regurgitate what they have been trained on<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Bender, E. M., Gebru, T., McMillan-Major, A. &amp; Shmitchell, S. On the dangers of stochastic parrots: can language models be too big? In Proc. 2021 ACM conference on fairness, accountability, and transparency, 610–623 (Association for Computing Machinery, 2021)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR9" id="ref-link-section-d7649163e1102">9</a></sup> and that show inherent limitations owing to the way they are trained<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. &amp; Griffiths, T. L. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. Proc. Natl Acad. Sci. USA 121, e2322420121 (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR10" id="ref-link-section-d7649163e1107">10</a></sup>. Nevertheless, the promise of these models is that they have shown the ability to solve a wide variety of tasks they have not been explicitly trained on<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at 
                https://doi.org/10.48550/arXiv.2108.07258
                
               (2021)." href="#ref-CR11" id="ref-link-section-d7649163e1111">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Anderljung, M. et al. Frontier ai regulation: managing emerging risks to public safety. Preprint at 
                https://doi.org/10.48550/arXiv.2307.03718
                
               (2023)." href="#ref-CR12" id="ref-link-section-d7649163e1111_1">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. Preprint at 
                https://doi.org/10.48550/arXiv.2311.07361
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR13" id="ref-link-section-d7649163e1114">13</a></sup>.</p><p>Chemists and materials scientists have quickly caught on to the mounting attention given to LLMs, with some voices even suggesting that ‘the future of chemistry is language’<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="White, A. D. The future of chemistry is language. Nat. Rev. Chem. 7, 457–458 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR14" id="ref-link-section-d7649163e1121">14</a></sup>. This statement is motivated by a growing number of reports that use LLMs to predict properties of molecules or materials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Zhong, Z., Zhou, K. &amp; Mottin, D. Benchmarking large language models for molecule prediction tasks. Preprint at 
                https://doi.org/10.48550/arXiv.2403.05075
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR2" id="ref-link-section-d7649163e1125">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jablonka, K. M. et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digit. Discov. 2, 1233–1250 (2023)." href="#ref-CR15" id="ref-link-section-d7649163e1128">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. &amp; Smit, B. Leveraging large language models for predictive chemistry. Nat. Mach. Intell. 6, 161–169 (2024)." href="#ref-CR16" id="ref-link-section-d7649163e1128_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Xie, Z. et al. Fine-tuning gpt-3 for machine learning electronic and functional properties of organic molecules. Chem. Sci. 15, 500–510 (2024)." href="#ref-CR17" id="ref-link-section-d7649163e1128_2">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Liao, C., Yu, Y., Mei, Y. &amp; Wei, Y. From words to molecules: a survey of large language models in chemistry. Preprint at 
                https://doi.org/10.48550/arXiv.2402.01439
                
               (2024)." href="#ref-CR18" id="ref-link-section-d7649163e1128_3">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Zhang, D. et al. Chemllm: a chemical large language model. Preprint at 
                https://doi.org/10.48550/arXiv.2402.06852
                
              (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR19" id="ref-link-section-d7649163e1131">19</a></sup>, optimize reactions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Ramos, M. C., Michtavy, S. S., Porosoff, M. D. &amp; White, A. D. Bayesian optimization of catalysts with in-context learning. Preprint at 
                https://doi.org/10.48550/arXiv.2304.05341
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR20" id="ref-link-section-d7649163e1135">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Kristiadi, A. et al. A sober look at LLMs for material discovery: are they actually good for bayesian optimization over molecules? In Proc. 41st International Conference on Machine Learning 1025 (JMLR.org, 2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR21" id="ref-link-section-d7649163e1138">21</a></sup>, generate materials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rubungo, A. N., Arnold, C., Rand, B. P. &amp; Dieng, A. B. Llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions. Preprint at 
                https://doi.org/10.48550/arXiv.2310.14029
                
               (2023)." href="#ref-CR22" id="ref-link-section-d7649163e1142">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Flam-Shepherd, D. &amp; Aspuru-Guzik, A. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. Preprint at 
                https://doi.org/10.48550/arXiv.2305.05708
                
               (2023)." href="#ref-CR23" id="ref-link-section-d7649163e1142_1">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gruver, N. et al. Fine-tuned language models generate stable inorganic materials as text. In Twelfth International Conference on Learning Representations (2024); 
                https://openreview.net/forum?id=vN9fpfqoP1
                
              " href="#ref-CR24" id="ref-link-section-d7649163e1142_2">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Alampara, N., Miret, S. &amp; Jablonka, K. M. Mattext: do language models need more than text &amp; scale for materials modeling? Preprint at 
                https://doi.org/10.48550/arXiv.2406.17295
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR25" id="ref-link-section-d7649163e1145">25</a></sup>, extract information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Patiny, L. &amp; Godin, G. Automatic extraction of fair data from publications using llm. Preprint at ChemRxiv 
                https://doi.org/10.26434/chemrxiv-2023-05v1b-v2
                
               (2023)." href="#ref-CR26" id="ref-link-section-d7649163e1149">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dagdelen, J. et al. Structured information extraction from scientific text with large language models. Nat. Commun. 15, 1418 (2024)." href="#ref-CR27" id="ref-link-section-d7649163e1149_1">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zheng, Z. et al. Image and data mining in reticular chemistry powered by gpt-4v. Digit. Discov. 3, 491–501 (2024)." href="#ref-CR28" id="ref-link-section-d7649163e1149_2">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lála, J. et al. Paperqa: retrieval-augmented generative agent for scientific research. Preprint at 
                https://doi.org/10.48550/arXiv.2312.07559
                
               (2023)." href="#ref-CR29" id="ref-link-section-d7649163e1149_3">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Caufield, J. H. et al. Structured prompt interrogation and recursive extraction of semantics (spires): a method for populating knowledge bases using zero-shot learning. Bioinformatics 40, btae104 (2024)." href="#ref-CR30" id="ref-link-section-d7649163e1149_4">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gupta, T. et al. DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles. In Proc. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long papers) 13465–13483 (Association for Computational Linguistics, 2023)." href="#ref-CR31" id="ref-link-section-d7649163e1149_5">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Schilling-Wilhelmi, M. et al. From text to insight: large language models for chemical data extraction. Chem. Soc. Rev. 54, 1125–1150 (2025)." href="#ref-CR32" id="ref-link-section-d7649163e1149_6">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Skarlinski, M. D. et al. Language agents achieve superhuman synthesis of scientific knowledge. Preprint at 
                https://doi.org/10.48550/arXiv.2409.13740
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR33" id="ref-link-section-d7649163e1152">33</a></sup> or even to prototype systems that can autonomously perform experiments in the physical world based on commands provided in natural language<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Boiko, D. A., MacKnight, R., Kline, B. &amp; Gomes, G. Autonomous chemical research with large language models. Nature 624, 570–578 (2023)." href="#ref-CR5" id="ref-link-section-d7649163e1157">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="M. Bran, A. et al. Augmenting large language models with chemistry tools. Nat. Mach. Intell. 6, 525–535 (2024)." href="#ref-CR6" id="ref-link-section-d7649163e1157_1">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Darvish, K. et al. ORGANA: A robotic assistant for automated chemistry experimentation and characterization. Matter 8, 101897 (2025)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR7" id="ref-link-section-d7649163e1160">7</a></sup>.</p><p>In addition, since a lot—if not most—of the information about chemistry is currently stored and communicated in text, there is a strong reason to believe that there is still a lot of untapped potential in LLMs for chemistry and materials science<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Miret, S. &amp; Krishnan, N. Are llms ready for real-world materials discovery? Preprint at 
                https://doi.org/10.48550/arXiv.2402.05200
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR34" id="ref-link-section-d7649163e1167">34</a></sup>. For instance, most insights in chemical research do not directly originate from data stored in databases but rather from the scientists interpreting the data. Many of these insights are in the form of text in scientific publications. Thus, operating on such texts might be our best way of unlocking these insights and learning from them. This might ultimately lead to general copilot systems for chemists that can provide answers to questions or even suggest new experiments on the basis of vastly more information than a human could ever read.</p><p>However, the rapid increase in capabilities of chemical ML models led (even before the recent interest in LLMs) to concerns about the potential for the dual use of these technologies, for example, for the design of chemical weapons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gopal, A. et al. Will releasing the weights of future large language models grant widespread access to pandemic agents? Preprint at 
                https://doi.org/10.48550/arXiv.2310.18233
                
               (2023)." href="#ref-CR35" id="ref-link-section-d7649163e1174">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ganguli, D. et al. Red teaming language models to reduce harms: methods, scaling behaviors, and lessons learned. Preprint at 
                https://doi.org/10.48550/arXiv.2209.07858
                
               (2022)." href="#ref-CR36" id="ref-link-section-d7649163e1174_1">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Urbina, F., Lentzos, F., Invernizzi, C. &amp; Ekins, S. Dual use of artificial-intelligence-powered drug discovery. Nat. Mach. Intell. 4, 189–191 (2022)." href="#ref-CR37" id="ref-link-section-d7649163e1174_2">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Campbell, Q. L., Herington, J. &amp; White, A. D. Censoring chemical data to mitigate dual use risk. Preprint at 
                https://doi.org/10.48550/arXiv.2304.10510
                
               (2023)." href="#ref-CR38" id="ref-link-section-d7649163e1174_3">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Moulange, R., Langenkamp, M., Alexanian, T., Curtis, S. &amp; Livingston, M. Towards responsible governance of biological design tools. Preprint at 
                https://doi.org/10.48550/arXiv.2311.15936
                
               (2023)." href="#ref-CR39" id="ref-link-section-d7649163e1174_4">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Urbina, F., Lentzos, F., Invernizzi, C. &amp; Ekins, S. A teachable moment for dual-use. Nat. Mach. Intell. 4, 607–607 (2022)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR40" id="ref-link-section-d7649163e1177">40</a></sup>. To some extent, this is not surprising as any technology that, for instance, is used to design non-toxic molecules can also be used inversely to predict toxic ones (even though the synthesis would still require access to controlled physical resources and facilities). Still, it is essential to realize that the user base of LLMs is broader than that of chemistry and materials science experts who can critically reflect on every output these models produce. For example, many students frequently consult these tools—perhaps even to prepare chemical experiments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="One-third of college students used chatgpt for schoolwork during the 2022-23 academic date. Intelligent.com 
                https://www.intelligent.com/one-third-of-college-students-used-chatgpt-for-schoolwork-during-the-2022-23-academic-date/
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR41" id="ref-link-section-d7649163e1181">41</a></sup>. This also applies to users from the general public, who might consider using LLMs to answer questions about the safety of chemicals. Thus, for some users, misleading information—especially about safety-related aspects—might lead to harmful outcomes. However, even for experts, chemical knowledge and reasoning capabilities are essential as they will determine the capabilities and limitations of their models in their work, for example, in copilot systems for chemists. Unfortunately, apart from exploratory reports, such as by prompting leading models with various scientific questions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. Preprint at 
                https://doi.org/10.48550/arXiv.2311.07361
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR13" id="ref-link-section-d7649163e1185">13</a></sup>, there is little systematic evidence on how LLMs perform compared with expert (human) chemists.</p><p>Thus, to better understand what LLMs can do for the chemical sciences and where they might be improved with further developments, evaluation frameworks are needed to allow us to measure progress and mitigate potential harms systematically. For the development of LLMs, evaluation is currently primarily performed via standardized benchmark suites such as BigBench<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Srivastava, A. et al. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research (2023); 
                https://openreview.net/forum?id=uyTL5Bvosj
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR42" id="ref-link-section-d7649163e1193">42</a></sup> or the LM Eval Harness<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gao, L. et al. A framework for few-shot language model evaluation version v0.4.0. Zenodo 
                https://zenodo.org/records/10256836
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR43" id="ref-link-section-d7649163e1197">43</a></sup>. Among 204 tasks (such as linguistic puzzles), the former contains only 2 tasks classified as ‘chemistry related’, whereas the latter contains no specific chemistry tasks. Owing to the lack of widely accepted standard benchmarks, the developers of chemical language models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. &amp; Smit, B. Leveraging large language models for predictive chemistry. Nat. Mach. Intell. 6, 161–169 (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR16" id="ref-link-section-d7649163e1201">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Guo, T. et al. What can large language models do in chemistry? A comprehensive benchmark on eight tasks. Preprint at 
                https://doi.org/10.48550/arXiv.2305.18365
                
               (2023)." href="#ref-CR44" id="ref-link-section-d7649163e1204">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ahmad, W., Simon, E., Chithrananda, S., Grand, G. &amp; Ramsundar, B. Chemberta-2: towards chemical foundation models. Preprint at 
                https://doi.org/10.48550/arXiv.2209.01712
                
               (2022)." href="#ref-CR45" id="ref-link-section-d7649163e1204_1">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cai, X. et al. Comprehensive evaluation of molecule property prediction with chatgpt. Methods 222, 133–141 (2024)." href="#ref-CR46" id="ref-link-section-d7649163e1204_2">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Frey, N. C. et al. Neural scaling of deep chemical models. Nat. Mach. Intell. 5, 1297–1305 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR47" id="ref-link-section-d7649163e1207">47</a></sup> frequently utilize language-interfaced<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Dinh, T. et al. Lift: language-interfaced fine-tuning for non-language machine learning tasks. Adv. Neural Inf. Process. Syst. 35, 11763–11784 (2022)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR48" id="ref-link-section-d7649163e1211">48</a></sup> tabular datasets such as the ones reported in MoleculeNet<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9, 513–530 (2018)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR49" id="ref-link-section-d7649163e1215">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Huang, Y. et al. Chemeval: a comprehensive multi-level chemical evaluation for large language models. Preprint at 
                https://doi.org/10.48550/arXiv.2409.13989
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR50" id="ref-link-section-d7649163e1218">50</a></sup>, Therapeutic Data Commons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang, K. et al. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. NeurIPS Datasets and Benchmarks (2021); 
                https://www.semanticscholar.org/paper/Therapeutics-Data-Commons%3A-Machine-Learning-and-for-Huang-Fu/54ca116f1e9a45768a3a2c47a4608ff34adefa0c
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR51" id="ref-link-section-d7649163e1223">51</a></sup>, safety databases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Zhao, H. et al. Chemsafetybench: benchmarking llm safety on chemistry domain. Preprint at 
                https://doi.org/10.48550/arXiv.2411.16736
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR52" id="ref-link-section-d7649163e1227">52</a></sup> or MatBench<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Dunn, A., Wang, Q., Ganose, A., Dopp, D. &amp; Jain, A. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. npj Comput. Mater. 6, 138 (2020)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR53" id="ref-link-section-d7649163e1231">53</a></sup>. In these cases, the models are evaluated on predicting very specific properties of molecules (for example, solubility, toxicity, melting temperature or reactivity) or on predicting the outcome of specific chemical reactions. This, however, only gives a very limited view of the general chemical capabilities of the models.</p><p>While some benchmarks based on university entrance exams<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Zaki, M. &amp; Krishnan, N. A. Mascqa: investigating materials science knowledge of large language models. Digit. Discov. 3, 313–327 (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR54" id="ref-link-section-d7649163e1238">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Arora, D., Singh, H., &amp; Mausam. Have LLMs advanced enough? A challenging problem solving benchmark for large language models. in Proc. 2023 Conference on Empirical Methods in Natural Language Processing (eds Bouamor, H., Pino, J. &amp; Bali, K.) 7527–7543 (Association for Computational Linguistics, 2023); 
                https://aclanthology.org/2023.emnlp-main.468/
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR55" id="ref-link-section-d7649163e1241">55</a></sup> or automatic text mining<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Song, Y., Miret, S., Zhang, H. &amp; Liu, B. Honeybee: progressive instruction finetuning of large language models for materials science. In Findings of the Association for Computational Linguistics: EMNLP 2023 (eds Bouamor, H. et al.) 5724–5739 (Association for Computational Linguistics, 2023)." href="#ref-CR56" id="ref-link-section-d7649163e1245">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wei, Z. et al. Chemistryqa: a complex question answering dataset from chemistry. OpenReview 
                https://openreview.net/forum?id=oeHTRAehiFF
                
               (2021)." href="#ref-CR57" id="ref-link-section-d7649163e1245_1">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Song, Y., Miret, S. &amp; Liu, B. Matsci-nlp: evaluating scientific language models on materials science language tasks using text-to-schema modeling. In Proc. 61st Annual Meeting of the Association for Computational Linguistics (eds Rogers, A., Boyd-Graber, J. &amp; Okazaki, N.) 3621–3639 (Association for Computational Linguistics, 2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR58" id="ref-link-section-d7649163e1248">58</a></sup> have been proposed, none of them have been widely accepted. This is probably because they cannot automatically be used with black box (or tool-augmented) systems, do not cover a wide range of topics and skills or are not carefully validated by experts. On top of that, the existing benchmarks are not designed to be used with models that support special treatment of molecules or equations and do not provide insights on how the models compare relative to experts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9, 513–530 (2018)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR49" id="ref-link-section-d7649163e1252">49</a></sup>.</p><p>In this work, we report a benchmarking framework (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig1">1</a>), which we call ChemBench, and use it to reveal the limitations of current frontier models for use in the chemical sciences. Our benchmark consists of 2,788 question–answer pairs compiled from diverse sources (1,039 manually generated and 1,749 semi-automatically generated). Our corpus measures reasoning, knowledge and intuition across a large fraction of the topics taught in undergraduate and graduate chemistry curricula. It can be used to evaluate any system that can return text (that is, including tool-augmented systems).</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Overview of the ChemBench framework."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Overview of the ChemBench framework.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41557-025-01815-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="225"/></picture></a></div><p>The different components of the ChemBench framework. The framework’s foundation is the benchmark corpus comprising thousands of questions and answers that we manually or semi-automatically compiled from various sources in a format based in the one introduced in the BIG-bench benchmark (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig6">1</a>). Questions are classified on the basis of topics, required skills (reasoning, calculation, knowledge and intuition) and difficulty levels. We then used this corpus to evaluate the performance of various models and tool-augmented systems using a custom framework. To provide a baseline, we built a web application that we used to survey experts in chemistry. The results of the evaluations are then compiled in publicly accessible leaderboards (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">15</a>), which we propose as a foundation for evaluating future models.</p></div></figure></div><p>To contextualize the scores, we also surveyed 19 experts in chemistry on a subset of the benchmark corpus to be able to compare the performance of current frontier models with (human) chemists of different specializations. In parts of the survey, the volunteers were also allowed to use tools, such as web search, to create a realistic setting.</p></div></div></section><section data-title="Results and discussion"><div id="Sec2-section"><h2 id="Sec2">Results and discussion</h2><div id="Sec2-content"><h3 id="Sec3">Benchmark corpus</h3><p>To compile our benchmark corpus, we utilized a broad list of sources (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Sec12">Methods</a>), ranging from completely novel, manually crafted questions over university exams to semi-automatically generated questions based on curated subsets of data in chemical databases. For quality assurance, all questions have been reviewed by at least two scientists in addition to the original curator and automated checks. Importantly, our large pool of questions encompasses a wide range of topics and question types (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig2">2</a>). The topics range from general chemistry to more specialized fields such as inorganic, analytical or technical chemistry. We also classify the questions on the basis of what skills are required to answer them. Here, we distinguish between questions that require knowledge, reasoning, calculation, intuition or a combination of these. Moreover, the annotator also classifies the questions by difficulty to allow for a more nuanced evaluation of the models’ capabilities.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Distribution of topics and required skills."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Distribution of topics and required skills.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41557-025-01815-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="387"/></picture></a></div><p>The distribution of questions across various chemistry topics, along with the primary skills required to address them. The topics were manually classified, showing a varied representation across different aspects of chemistry. Each topic is associated with a combination of three key skills: calculation, reasoning and knowledge, as indicated by the coloured bars. ChemBench samples encompass diverse topics and diverse skills, setting a high bar for LLMs to demonstrate human-competitive performance across a wide range of chemistry tasks.</p></div></figure></div><p>While many existing benchmarks are designed around multiple-choice questions (MCQ), this does not reflect the reality of chemistry education and research. For this reason, ChemBench samples both MCQ and open-ended questions (2,544 MCQ and 244 open-ended questions). In addition, ChemBench samples different skills on various difficulty levels: from basic knowledge questions (as knowledge underpins reasoning processes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Hu, X. et al. Towards understanding factual knowledge of large language models. In The Twelfth International Conference on Learning Representations (2024); 
                https://openreview.net/forum?id=9OevMUdods
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR59" id="ref-link-section-d7649163e1331">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Bloom, B. Taxonomy of Educational Objectives: the Classification of Educational Goals (Longmans, 1956)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR60" id="ref-link-section-d7649163e1334">60</a></sup>) to complex reasoning tasks (such as finding out which ions are in a sample given a description of observations). We also include questions about chemical intuition, as demonstrating human-aligned preferences is relevant for applications, such as hypothesis generation or optimization tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Zhang, J., Lehman, J., Stanley, K. &amp; Clune, J. OMNI: Open-endedness via models of human notions of interestingness. In Twelfth International Conference on Learning Representations (2024); 
                https://openreview.net/forum?id=AgM3MzT99c
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR61" id="ref-link-section-d7649163e1338">61</a></sup>.</p><h4 id="Sec4">ChemBench-Mini</h4><p>It is important to note that a smaller subset of the corpus might be more practical for routine evaluations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Polo, F. M. et al. tinyBenchmarks: evaluating LLMs with fewer examples. In Proc. 41st International Conference on Machine Learning (JMLR.org, 2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR62" id="ref-link-section-d7649163e1349">62</a></sup>. For instance, Liang et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Liang, P. et al. Holistic evaluation of language models. Transactions on Machine Learning Research (2023); 
                https://openreview.net/forum?id=iO4LZibEqW
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR63" id="ref-link-section-d7649163e1353">63</a></sup> report costs of more than US$10,000 for application programming interface (API) calls for a single evaluation on the widely used Holistic Evaluation of Language Models benchmark. To address this, we also provide a subset (ChemBench-Mini, 236 questions) of the corpus that was curated to be a diverse and representative subset of the full corpus. While it is impossible to comprehensively represent the full corpus in a subset, we aimed to include a maximally diverse set of questions and a more balanced distribution of topics and skills (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Sec12">Methods</a> for details on the curation process). Our human volunteers answered all the questions in this subset.</p><h3 id="Sec5">Model evaluation</h3><h4 id="Sec6">Benchmark suite design</h4><p>Because the text used in scientific settings differs from typical natural language, many models have been developed that deal with such text in a particular way. For instance, the Galactica model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
                https://doi.org/10.48550/arXiv.2211.09085
                
               (2022)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR64" id="ref-link-section-d7649163e1373">64</a></sup> uses special encoding procedures for molecules and equations. Current benchmarking suites, however, do not account for such special treatment of scientific information. To address this, ChemBench encodes the semantic meaning of various parts (for example, chemicals, units or equations) of the question or answer. For instance, molecules represented in simplified molecular input line-entry system (SMILES) are enclosed in [START_SMILES][\END_SMILES] tags. This allows the model to treat the SMILES string differently from other text. ChemBench can seamlessly handle such special treatment in an easily extensible way because the questions are stored in an annotated format.</p><p>Since many widely utilized LLM systems only provide access to text completions (and not the raw model outputs), ChemBench is designed to operate on text completions. This is also important given the growing number of tool-augmented systems that are deemed essential for building chemical copilot systems. Such systems can augment the capabilities of LLMs through the use of external tools such as search APIs or code executors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Schick, T. et al. Toolformer: language models can teach themselves to use tools. Adv. Neural Inf. Proc. Syst. 36, 68539–68551 (2024)." href="#ref-CR65" id="ref-link-section-d7649163e1380">65</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Karpas, E. et al. Mrkl systems: a modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Preprint at 
                https://doi.org/10.48550/arXiv.2205.00445
                
               (2022)." href="#ref-CR66" id="ref-link-section-d7649163e1380_1">66</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Yao, S. et al. ReAct: Synergizing reasoning and acting in language models. In Eleventh International Conference on Learning Representations (OpenReview.net, 2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR67" id="ref-link-section-d7649163e1383">67</a></sup>. In those cases, the LLM which returns the probabilities for various tokens (that is, text fragments) represents only one component and it is not clear how to interpret those probabilities in the context of the entire system. The text completions, however, are the system’s final outputs, which would also be used in a real-world application. Hence, we use them for our evaluations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Xiong, M. et al. Can llms express their uncertainty? An empirical evaluation of confidence elicitation in llms. In Twelfth International Conference on Learning Representations (OpenReview.net, 2024); 
                https://openreview.net/forum?id=gjeQKFxFpZ
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR68" id="ref-link-section-d7649163e1387">68</a></sup>.</p><h4 id="Sec7">Overall system performance</h4><p>To understand the current capabilities of LLMs in the chemical sciences, we evaluated a wide range of leading models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Beeching, E. et al. Open llm leaderboard. Hugging Face 
                https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
                
               (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR69" id="ref-link-section-d7649163e1399">69</a></sup> on the ChemBench corpus, including systems augmented with external tools. An overview of the results of this evaluation is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig3">3</a> (all results can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">4</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">5</a>). In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig3">3</a>, we show the percentage of questions that the models answered correctly. Moreover, we show the worst, best and average performance of the experts in our study, which we obtained via a custom web application (<a href="http://chembench.org">chembench.org</a>) that we used to survey the experts. Remarkably, the figure shows that the leading LLM, o1-preview, outperforms the best human in our study in this overall metric by almost a factor of two. Many other models also outperform the average human performance. Interestingly, Llama-3.1-405B-Instruct shows performance that is close to the leading proprietary models, indicating that new open-source models can also be competitive with the best proprietary models in chemical settings.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Performance of models and humans on ChemBench-Mini."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Performance of models and humans on ChemBench-Mini.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41557-025-01815-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="519"/></picture></a></div><p>The percentage of questions that the models answered correctly. Horizontal bars indicate the performance of various models and highlight statistics of human performance. The evaluation we use here is very strict as it only considers a question answered correctly or incorrectly, partially correct answers are also considered incorrect. Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">3</a> provides an overview of the performance of various models on the entire corpus. PaperQA2 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Skarlinski, M. D. et al. Language agents achieve superhuman synthesis of scientific knowledge. Preprint at 
                https://doi.org/10.48550/arXiv.2409.13740
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR33" id="ref-link-section-d7649163e1438">33</a></sup>) is an agentic system that can also search the literature to obtain an answer. We find that the best models outperform all humans in our study when averaged over all questions (even though humans had access to tools, such as web search and ChemDraw, for a subset of the questions).</p></div></figure></div><p>Notably, we find that models are still limited in their ability to answer knowledge-intensive questions (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">5</a>); that is, they did not memorize the relevant facts. Our results indicate that this is not a limitation that could be overcome by simple application of retrieval augmented generation systems such as PaperQA2. This is probably because the required knowledge cannot easily be accessed via papers (which is the only type of external knowledge PaperQA2 has access to) but rather by lookup in specialized databases (for example, PubChem and Gestis), which the humans in our study also used to answer such questions (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">17</a>). This indicates that there is still room for improving chemical LLMs by training them on more specialized data sources or integrating them with specialized databases.</p><p>In addition, our analysis shows that the performance of models is correlated with their size (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">11</a>). This is in line with observations in other domains, but also indicates that chemical LLMs could, to some extent, be further improved by scaling them up.</p><h4 id="Sec8">Performance per topic</h4><p>To obtain a more detailed understanding of the performance of the models, we also analysed the performance of the models in different subfields of the chemical sciences. For this analysis, we defined a set of topics (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Sec12">Methods</a>) and classified all questions in the ChemBench corpus into these topics. We then computed the percentage of questions that the models or experts answered correctly for each topic and present them in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig4">4</a>. In this spider chart, the worst score for every dimension is zero (no question answered correctly) and the best score is one (all questions answered correctly). Thus, a larger coloured area indicates a better performance.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Performance of the models and humans on the different topics on ChemBench-Mini."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Performance of the models and humans on the different topics on ChemBench-Mini.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41557-025-01815-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="334"/></picture></a></div><p>The radar plot shows the performance of the models and humans on the different topics of ChemBench-Mini. Performance is measured as the fraction of questions that were answered correctly by the models. The best score for every dimension is 1 (all questions answered correctly) and the worst is 0 (no question answered correctly). A larger coloured area indicates a better performance. This figure shows the performance on ChemBench-Mini. The performance of models on the entire corpus is presented in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">3</a>.</p></div></figure></div><p>One can observe that this performance varies across models and topics. While general and technical chemistry receive relatively high scores for many models, this is not the case for topics such as toxicity and safety or analytical chemistry.</p><p>In the subfield of analytical chemistry, the prediction of the number of signals observable in a nuclear magnetic resonance spectrum proved difficult even for the best models (for example, 22% correct answers for o1-preview). Importantly, while the human experts are given a drawing of the compounds, the models are only shown the SMILES string of a compound and have to use this to reason about the symmetry of the compound (that is, to identify the number of diasterotopically distinct protons, which requires reasoning about the topology and structure of a molecule).</p><p>These findings also shine an interesting light on the value of textbook-inspired questions. A subset of the questions in ChemBench are based on textbooks targeted at undergraduate students. On those questions, the models tend to perform better than on some of our semi-automatically constructed tasks (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">5</a>). For instance, while the overall performance in the chemical safety topic is low, the models would pass the certification exam according to the German Chemical Prohibition Ordinance on the basis of a subset of questions we sampled from the corresponding question bank (for example, 71% correct answers for GPT-4, 61% for Claude-3.5 (Sonnet) and 3% for the human experts). While those findings are impacted by the subset of questions we sampled, the results still highlight that good performance on such question bank or textbook questions does not necessarily translate to good performance on other questions that require more reasoning or are further away from the training corpus<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. &amp; Griffiths, T. L. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. Proc. Natl Acad. Sci. USA 121, e2322420121 (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR10" id="ref-link-section-d7649163e1514">10</a></sup>. The findings also underline that such exams might have been a good surrogate for the general performance of skills for humans, but their applicability in the face of systems that can consume vast amounts of data is up for debate.</p><p>We also gain insight into the models’ struggles with chemical reasoning tasks by examining their performance as a function of molecular descriptors. If the model would answer questions after reasoning about the structures, one would expect the performance to depend on the complexity of the molecules. However, we find that the models’ performance does not correlate with complexity indicators (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">5</a>). This indicates that the models may not be able to reason about the structures of the molecules (in the way one might expect) but instead rely on the proximity of the molecules to the training data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. &amp; Griffiths, T. L. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. Proc. Natl Acad. Sci. USA 121, e2322420121 (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR10" id="ref-link-section-d7649163e1525">10</a></sup>.</p><p>It is important to note that the model performance for some topics, however, is slightly underestimated in the current evaluation. This is because models provided via APIs typically have safety mechanisms that prevent them from providing answers that the provider deems unsafe. For instance, models might refuse to provide answers about cyanides. Statistics on the frequency of such refusals are presented in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">8</a>. To overcome this, direct access to the model weights would be required, and we strive to collaborate with the developers of frontier models to overcome this limitation in the future. This is facilitated by the tooling ChemBench provides, thanks to which contributors can automatically add new models in an open science fashion.</p><h4 id="Sec9">Judging chemical preference</h4><p>One interesting finding of recent research is that foundation models can judge interestingness or human preferences in some domains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Zhang, J., Lehman, J., Stanley, K. &amp; Clune, J. OMNI: Open-endedness via models of human notions of interestingness. In Twelfth International Conference on Learning Representations (2024); 
                https://openreview.net/forum?id=AgM3MzT99c
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR61" id="ref-link-section-d7649163e1543">61</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Argyle, L. P. et al. Out of one, many: using language models to simulate human samples. Polit. Anal. 31, 337–351 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR70" id="ref-link-section-d7649163e1546">70</a></sup>. If models could do so for chemical compounds, this would open opportunities for novel optimization approaches. Such open-ended tasks, however, depend on an external observer defining what interestingness is<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Hughes, E. et al. Position: Open-endedness is essential for artificial superhuman intelligence. In Proc. 41st International Conference on Machine Learning Vol. 235 (eds Salakhutdinov, R. et al.) 20597–20616 (PMLR, 2024); 
                https://proceedings.mlr.press/v235/hughes24a.html
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR71" id="ref-link-section-d7649163e1550">71</a></sup>. Here, we posed models the same question that Choung et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Choung, O.-H., Vianello, R., Segler, M., Stiefl, N. &amp; Jiménez-Luna, J. Extracting medicinal chemistry intuition via preference machine learning. Nat. Commun. 14, 6651 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR72" id="ref-link-section-d7649163e1554">72</a></sup> asked chemists at a drug company: ‘which of the two compounds do you prefer?’ (in the context of an early virtual screening campaign setting; see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">2</a> for an example). Despite chemists demonstrating a reasonable level of inter-rater agreement, our models largely fail to align with expert chemists’ preferences. Their performance is often indistinguishable from random guessing, even though these same models excel in other tasks in ChemBench (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">5</a>). This indicates that using preference tuning for chemical settings could be a promising approach to explore in future research.</p><h4 id="Sec10">Confidence estimates</h4><p>One might wonder whether the models can estimate if they can answer a question correctly. If they could do so, incorrect answers would be less problematic.</p><p>To investigate this, we prompted<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Xiong, M. et al. Can llms express their uncertainty? An empirical evaluation of confidence elicitation in llms. In Twelfth International Conference on Learning Representations (OpenReview.net, 2024); 
                https://openreview.net/forum?id=gjeQKFxFpZ
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR68" id="ref-link-section-d7649163e1576">68</a></sup> some of the top-performing models to estimate, on an ordinal scale, their confidence in their ability to answer the question correctly (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Sec12">Methods</a> for details on the methodology and comparison to logit-based approaches).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig5">5</a>, we show that for some models, there is no meaningful correlation between the estimated difficulty and whether the models answered the question correctly or not. For applications in which humans might rely on the models to provide answers with trustworthy uncertainty estimates, this is a concerning observation highlighting the need for critical reasoning in the interpretation of the model’s outputs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Miret, S. &amp; Krishnan, N. Are llms ready for real-world materials discovery? Preprint at 
                https://doi.org/10.48550/arXiv.2402.05200
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR34" id="ref-link-section-d7649163e1589">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Li, B. et al. Trustworthy ai: from principles to practices. ACM Comput. Surv. 55, 1–46 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR73" id="ref-link-section-d7649163e1592">73</a></sup>. For example, for the questions about the safety profile of compounds, GPT-4 reported a confidence of 1.0 (on a scale of 1–5) for the one question it answered correctly and 4.0 for the six questions it answered incorrectly. While, on average, the verbalized confidence estimates from Claude-3.5 (Sonnet) seem better calibrated (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig5">5</a>), they are still misleading in some cases. For example, for the questions about the labelling of chemicals (GHS) pictograms Claude-3.5 (Sonnet) returns an average score of 2.0 for correct answers and 1.83 for incorrect answers.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Reliability and distribution of confidence estimates."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Reliability and distribution of confidence estimates.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41557-025-01815-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="437"/></picture></a></div><p>For this analysis, we used verbalized confidence estimates from the model. The models were prompted to return a confidence score on an ordinal scale to obtain those estimates. The line plot shows the average fraction of correctly answered questions for each confidence level. The bar plot shows the distribution of confidence estimates. The error bars indicate the standard deviation for each confidence level (for which the number of samples is given by the height of the bar). A confidence estimate would be well calibrated if the average fraction of correctly answered questions increases with the confidence level. The dashed black line indicates this ideal behaviour, which would be monotonically increasing correctness with higher levels of confidence. We use colours to distinguish the different models, as indicated in the titles of the subplots. We find that most models are not well calibrated and provide misleading confidence estimates.</p></div></figure></div></div></div></section><section data-title="Conclusions"><div id="Sec11-section"><h2 id="Sec11">Conclusions</h2><div id="Sec11-content"><p>On the one hand, our findings underline the impressive capabilities of LLMs in the chemical sciences: leading models outperform domain experts in specific chemistry questions on many topics. On the other hand, there are still striking limitations. For very relevant topics, the answers that models provide are wrong. On top of that, many models are not able to reliably estimate their own limitations. Yet, the success of the models in our evaluations perhaps also reveals more about the limitations of the questions we use to evaluate models—and chemists—than about the models themselves. For instance, while models perform well on many textbook questions, they struggle with questions requiring more reasoning about chemical structures (for example, number of isomers or nuclear magnetic resonance peaks). Given that the models outperformed the average human in our study, we need to rethink how we teach and examine chemistry. Critical reasoning is increasingly essential, and rote solving of problems or memorization of facts is a domain in which LLMs will continue to outperform humans (when trained on the right training corpus).</p><p>Our findings also highlight the nuanced trade-off between breadth and depth of evaluation frameworks. The analysis of model performance on different topics shows that models’ performance varies widely across the subfields they are tested on. However, even within a topic, the performance of models can vary widely depending on the type of question and the reasoning required to answer it.</p><p>The current evaluation frameworks for chemical LLMs are primarily designed to measure the performance of the models on specific property prediction tasks. They cannot be used to evaluate reasoning or systems built for scientific applications. Thus, we had little understanding of the capabilities of LLMs in the chemical sciences. Our work shows that carefully curated benchmarks can provide a more nuanced understanding of the capabilities of LLMs in the chemical sciences. Importantly, our findings also illustrate that more focus is required in developing better human–model interaction frameworks, given that models cannot estimate their limitations.</p><p>Although our findings indicate many areas for further improvement of LLM-based systems, such as agents (more discussion in Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">11</a>), it is also important to realize that clearly defined metrics have been the key to the progress of many fields of ML, such as computer vision. Although current systems might be far from reasoning like a chemist, our ChemBench framework will be a stepping stone for developing systems that come closer to this goal.</p></div></div></section><section data-title="Methods"><div id="Sec12-section"><h2 id="Sec12">Methods</h2><div id="Sec12-content"><h3 id="Sec13">Curation workflow</h3><p>For our dataset, we curated questions from existing exams or exercise sheets but also programmatically created new questions (see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">3</a> for more details). Questions were added via Pull Requests on our GitHub repository and only merged into the corpus after passing manual review (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Fig6">1</a>) as well as automated checks (for example, for compliance with a standardized schema).</p><p>To ensure that the questions do not enter a training dataset, we use the same canary string as the BigBench project. This requires that LLM developers filter their training dataset for this canary string<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="OpenAI et al. Gpt-4 technical report. (2024); 
                https://doi.org/10.48550/arXiv.2303.08774
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR4" id="ref-link-section-d7649163e1661">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Srivastava, A. et al. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research (2023); 
                https://openreview.net/forum?id=uyTL5Bvosj
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR42" id="ref-link-section-d7649163e1664">42</a></sup>.</p><h4 id="Sec14">Manually curated questions</h4><p>Manually curated questions were sourced from various sources, including university exams, exercises and question banks. Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41557-025-01815-x#Tab1">1</a> presents an overview of the sources of the manually curated questions.</p><h4 id="Sec15">Semi-programmatically generated questions</h4><p>In addition to the manually curated questions, we also generated questions programmatically. An overview of the sources of the semi-programmatically generated questions is provided in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">3</a>.</p><h4 id="Sec16">Chemical preference data</h4><p>These questions assess the ability to establish a ‘preference’, such as favouring a specific molecule. Chemical preference is of major importance in drug discovery projects, where the optimization process to reach the desired molecular properties is a process that takes several years within a chemist’s career. Our data corpus is adapted from the published dataset by Choung et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Choung, O.-H., Vianello, R., Segler, M., Stiefl, N. &amp; Jiménez-Luna, J. Extracting medicinal chemistry intuition via preference machine learning. Nat. Commun. 14, 6651 (2023)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR72" id="ref-link-section-d7649163e1698">72</a></sup>, which consists of more than 5,000 question–answer pairs about chemical intuition. To build the dataset, they presented 35 medicinal chemists with two different molecules, asking them what molecule they would like to continue with when imaging an early virtual screening campaign setting. The question was designed so the scientists do not spend much time answering it, relying only on their feelings or ‘chemical preference’.</p><p>To understand whether the capabilities of the leading models align with the preferences of professional chemists, we randomly selected 1,000 data points from the original dataset to create a meaningful evaluation set, where molecules are represented as SMILES. To ablate the effect of different molecular representations, we only considered questions for which we could obtain International Union of Pure and Applied Chemistry names for both the molecules present.</p><h3 id="Sec17">Model evaluation workflow</h3><p>A graphical overview of the pipeline is presented in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">12</a>.</p><h4 id="Sec18">Prompting</h4><p>We employ distinct prompt templates tailored for completion and instruction-tuned models to maintain consistency with the training. As explained later, we impose constraints on the models within these templates to receive responses in a specific format so that robust, fair and consistent parsing can be performed. Certain models are trained with special annotations and LaTeX syntax for scientific notations, chemical reactions or symbols embedded within the text. For example, all the SMILES representations are encapsulated within [START_SMILES][\END_SMILES] in Galactica<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
                https://doi.org/10.48550/arXiv.2211.09085
                
               (2022)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR64" id="ref-link-section-d7649163e1724">64</a></sup>. Our prompting strategy consistently adheres to these details in a model-specific manner by post-processing LaTeX syntax, chemical symbols, chemical equations and physical units (by either adding or removing wrappers). This step can be easily customized in our codebase, and we provide presets for the models we evaluated.</p><h4 id="Sec19">Parsing</h4><p>Our parsing workflow is multistep and primarily based on regular expressions. In the case of instruction-tuned models, we first identify the [ANSWER][\ANSWER] environment that we prompt the model to report the answer in. In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for MCQ) or numbers. In the case of numbers, our regular expression was engineered to deal with various forms of scientific notation. As initial tests indicated that models sometimes return integers in the form of words, for example, ‘one’ instead of ‘1’, we also implemented a word-to-number conversion using regular expressions. If these hard-coded parsing steps fail, we use a LLM, for example, Claude-3.5 (Sonnet), to parse the completion (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">8</a> provides more details on this step).</p><h4 id="Sec20">Models</h4><p>For all models, we performed inference using greedy decoding (that is, temperature 0). We used the API endpoints provided by the model developers and those provided by Groq. PaperQA2 was used (in August 2024) via an API provided by FutureHouse.</p><h3 id="Sec21">Confidence estimate</h3><p>To estimate the models’ confidence, we prompted them with the question (and answer options for MCQ) and the task to rate their confidence to produce the correct answer on a scale from 1 to 5. We decided to use verbalized confidence estimates<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Xiong, M. et al. Can llms express their uncertainty? An empirical evaluation of confidence elicitation in llms. In Twelfth International Conference on Learning Representations (OpenReview.net, 2024); 
                https://openreview.net/forum?id=gjeQKFxFpZ
                
              " href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR68" id="ref-link-section-d7649163e1756">68</a></sup> since we found those to be closer to current practical use cases than other prompting strategies, which might be more suitable when implemented in systems. In addition, this approach captures semantic uncertainty, which is not the same as the probability of a token being given a sequence of tokens (that is, the uncertainty one obtains from logit-based approaches). On top of that, many proprietary models do not provide access to the logits, making this approach more general. In Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">12</a>, we provide more details and comparisons with a logit-based approach.</p><h3 id="Sec22">Human baseline</h3><h4 id="Sec23">Question selection</h4><p>Several design choices were made when selecting ChemBench-Mini. Firstly, from the full dataset, we kept all the questions labelled as advanced. In this way, we can obtain a deeper insight into the capabilities of LLMs on advanced tasks when compared with actual chemists. Secondly, we sample a maximum of three questions across all possible combinations of categories (that is, knowledge or reasoning) and topics (for example, organic chemistry and physical chemistry). Thirdly, we do not include any intuition questions in this subset because the intended use of ChemBench-Mini is to provide a fast and fair evaluation of LLMs independent of any human baseline. In total, 236 questions have been sampled for ChemBench-Mini. Then, this set is divided into two subsets on the basis of the aforementioned combinations. One of the question subsets allows tool use, and the other does not.</p><h4 id="Sec24">Study design</h4><p>Human volunteers were asked the questions in a custom-built web interface (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">10</a>), which rendered chemicals and equations. Questions were shown in random order, and volunteers were not allowed to skip questions. For a subset of the questions, the volunteers were allowed to use external tools (excluding other LLM or asking other people) to answer the questions. Before answering questions, volunteers were asked to provide information about their education and experience in chemistry. The study was conducted in English.</p><h4 id="Sec25">Human volunteers</h4><p>Users were open to reporting about their experience in chemistry. Overall, 16 did so. Out of those, 2 are beyond a first postdoc, 13 have a master’s degree (and are currently enroled in Ph.D. studies) and 1 has a bachelor’s degree. For the analysis, we excluded volunteers with less than 2 years of experience in chemistry after their first university-level course in chemistry.</p><h4 id="Sec26">Comparison with models</h4><p>For the analysis, we treated each human as a model. We computed the topic aggregated averages per human for analyses grouped by topic and then averaged over all humans. The performance metrics reported for models in the main text are computed on the same questions that the humans answered. Metrics for the entire corpus are reported in Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM1">4</a>.</p><h3 id="Sec27">Data annotation</h3><p>In the curation of our dataset, we manually assigned difficulty levels and required skills to each question. We used the following guidelines for these annotations: calculation is required if answering a question would require the use of a calculator, knowledge is required if answering a question requires non-trivial knowledge of facts (for example, the H/P statements of chemicals). Reasoning is required if answering a question requires multiple reasoning steps. Basic questions only require those skills up to the high school level. Advanced questions would require an expert multiple minutes or hours to answer.</p><h3 id="Sec28">Inclusion and ethics statement</h3><p>The authors confirm that they have complied with all relevant ethical regulations, according to the Ethics Commission of the Friedrich Schiller University Jena (which decided that the study is ethically safe). Informed consent was obtained from all volunteers.</p><h3 id="Sec29">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41557-025-01815-x#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div></section>
                    
                </div><div>
                <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><div id="data-availability-content">
            
            <p>The data for ChemBench is available via GitHub at <a href="https://github.com/lamalab-org/chembench">https://github.com/lamalab-org/chembench</a> and via Zenodo at <a href="https://zenodo.org/records/14010212">https://zenodo.org/records/14010212</a> (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Mirza, A. et al. chem-bench version v0.2.0. Zenodo 
                https://doi.org/10.5281/zenodo.14010212
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR74" id="ref-link-section-d7649163e1955">74</a></sup>).</p>
          </div></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><div id="code-availability-content">
            
            <p>The code for ChemBench is available via GitHub at <a href="https://github.com/lamalab-org/chembench">https://github.com/lamalab-org/chembench</a> and via Zenodo at <a href="https://zenodo.org/records/14010212">https://zenodo.org/records/14010212</a> (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Mirza, A. et al. chem-bench version v0.2.0. Zenodo 
                https://doi.org/10.5281/zenodo.14010212
                
               (2024)." href="https://www.nature.com/articles/s41557-025-01815-x#ref-CR74" id="ref-link-section-d7649163e1981">74</a></sup>) The code for the app for our human baseline study is available via GiHub at <a href="https://github.com/lamalab-org/chem-bench-app">https://github.com/lamalab-org/chem-bench-app</a>.</p>
          </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Brown, T. et al. Language models are few-shot learners. <i>Adv. Neural Inf. Process. Syst.</i> <b>33</b>, 1877–1901 (2020).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20models%20are%20few-shot%20learners&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=33&amp;pages=1877-1901&amp;publication_year=2020&amp;author=Brown%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">Zhong, Z., Zhou, K. &amp; Mottin, D. Benchmarking large language models for molecule prediction tasks. Preprint at <a href="https://doi.org/10.48550/arXiv.2403.05075" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2403.05075">https://doi.org/10.48550/arXiv.2403.05075</a> (2024).</p></li><li data-counter="3."><p id="ref-CR3">Kung, T. H. et al. Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. <i>PLoS Digit. Health</i> <b>2</b>, e0000198 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pdig.0000198" data-track-item_id="10.1371/journal.pdig.0000198" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pdig.0000198" aria-label="Article reference 3" data-doi="10.1371/journal.pdig.0000198">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36812645" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931230" aria-label="PubMed Central reference 3">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance%20of%20chatgpt%20on%20usmle%3A%20potential%20for%20ai-assisted%20medical%20education%20using%20large%20language%20models&amp;journal=PLoS%20Digit.%20Health&amp;doi=10.1371%2Fjournal.pdig.0000198&amp;volume=2&amp;publication_year=2023&amp;author=Kung%2CTH">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">OpenAI et al. <i>Gpt-4 technical report.</i> (2024); <a href="https://doi.org/10.48550/arXiv.2303.08774" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</a></p></li><li data-counter="5."><p id="ref-CR5">Boiko, D. A., MacKnight, R., Kline, B. &amp; Gomes, G. Autonomous chemical research with large language models. <i>Nature</i> <b>624</b>, 570–578 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-023-06792-0" data-track-item_id="10.1038/s41586-023-06792-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-023-06792-0" aria-label="Article reference 5" data-doi="10.1038/s41586-023-06792-0">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXis1KmsLfI" aria-label="CAS reference 5">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38123806" aria-label="PubMed reference 5">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10733136" aria-label="PubMed Central reference 5">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Autonomous%20chemical%20research%20with%20large%20language%20models&amp;journal=Nature&amp;doi=10.1038%2Fs41586-023-06792-0&amp;volume=624&amp;pages=570-578&amp;publication_year=2023&amp;author=Boiko%2CDA&amp;author=MacKnight%2CR&amp;author=Kline%2CB&amp;author=Gomes%2CG">
                    Google Scholar</a> 
                </p></li><li data-counter="6."><p id="ref-CR6">M. Bran, A. et al. Augmenting large language models with chemistry tools. <i>Nat. Mach. Intell.</i> <b>6</b>, 525–535 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-024-00832-8" data-track-item_id="10.1038/s42256-024-00832-8" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-024-00832-8" aria-label="Article reference 6" data-doi="10.1038/s42256-024-00832-8">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38799228" aria-label="PubMed reference 6">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11116106" aria-label="PubMed Central reference 6">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmenting%20large%20language%20models%20with%20chemistry%20tools&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-024-00832-8&amp;volume=6&amp;pages=525-535&amp;publication_year=2024&amp;author=M.%20Bran%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="7."><p id="ref-CR7">Darvish, K. et al. ORGANA: A robotic assistant for automated chemistry experimentation and characterization. <i>Matter</i> <b>8</b>, 101897 (2025).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.matt.2024.10.015" data-track-item_id="10.1016/j.matt.2024.10.015" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.matt.2024.10.015" aria-label="Article reference 7" data-doi="10.1016/j.matt.2024.10.015">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=ORGANA%3A%20A%20robotic%20assistant%20for%20automated%20chemistry%20experimentation%20and%20characterization&amp;journal=Matter&amp;doi=10.1016%2Fj.matt.2024.10.015&amp;volume=8&amp;publication_year=2025&amp;author=Darvish%2CK">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">Bubeck, S. et al. Sparks of artificial general intelligence: early experiments with gpt-4. Preprint at <a href="https://doi.org/10.48550/arXiv.2303.12712" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2303.12712">https://doi.org/10.48550/arXiv.2303.12712</a> (2023).</p></li><li data-counter="9."><p id="ref-CR9">Bender, E. M., Gebru, T., McMillan-Major, A. &amp; Shmitchell, S. On the dangers of stochastic parrots: can language models be too big? In <i>Proc. 2021 ACM conference on fairness, accountability, and transparency</i>, 610–623 (Association for Computing Machinery, 2021).</p></li><li data-counter="10."><p id="ref-CR10">McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. &amp; Griffiths, T. L. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. <i>Proc. Natl Acad. Sci. USA</i> <b>121</b>, e2322420121 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1073/pnas.2322420121" data-track-item_id="10.1073/pnas.2322420121" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.2322420121" aria-label="Article reference 10" data-doi="10.1073/pnas.2322420121">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXisV2kt7bL" aria-label="CAS reference 10">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=39365822" aria-label="PubMed reference 10">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11474099" aria-label="PubMed Central reference 10">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Embers%20of%20autoregression%20show%20how%20large%20language%20models%20are%20shaped%20by%20the%20problem%20they%20are%20trained%20to%20solve&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.2322420121&amp;volume=121&amp;publication_year=2024&amp;author=McCoy%2CRT&amp;author=Yao%2CS&amp;author=Friedman%2CD&amp;author=Hardy%2CMD&amp;author=Griffiths%2CTL">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at <a href="https://doi.org/10.48550/arXiv.2108.07258" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2108.07258">https://doi.org/10.48550/arXiv.2108.07258</a> (2021).</p></li><li data-counter="12."><p id="ref-CR12">Anderljung, M. et al. Frontier ai regulation: managing emerging risks to public safety. Preprint at <a href="https://doi.org/10.48550/arXiv.2307.03718" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2307.03718">https://doi.org/10.48550/arXiv.2307.03718</a> (2023).</p></li><li data-counter="13."><p id="ref-CR13">Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. Preprint at <a href="https://doi.org/10.48550/arXiv.2311.07361" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2311.07361">https://doi.org/10.48550/arXiv.2311.07361</a> (2023).</p></li><li data-counter="14."><p id="ref-CR14">White, A. D. The future of chemistry is language. <i>Nat. Rev. Chem.</i> <b>7</b>, 457–458 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41570-023-00502-0" data-track-item_id="10.1038/s41570-023-00502-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41570-023-00502-0" aria-label="Article reference 14" data-doi="10.1038/s41570-023-00502-0">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXhtVOlur%2FK" aria-label="CAS reference 14">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37208543" aria-label="PubMed reference 14">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20future%20of%20chemistry%20is%20language&amp;journal=Nat.%20Rev.%20Chem.&amp;doi=10.1038%2Fs41570-023-00502-0&amp;volume=7&amp;pages=457-458&amp;publication_year=2023&amp;author=White%2CAD">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">Jablonka, K. M. et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. <i>Digit. Discov.</i> <b>2</b>, 1233–1250 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/D3DD00113J" data-track-item_id="10.1039/D3DD00113J" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FD3DD00113J" aria-label="Article reference 15" data-doi="10.1039/D3DD00113J">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38013906" aria-label="PubMed reference 15">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10561547" aria-label="PubMed Central reference 15">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=14%20examples%20of%20how%20llms%20can%20transform%20materials%20science%20and%20chemistry%3A%20a%20reflection%20on%20a%20large%20language%20model%20hackathon&amp;journal=Digit.%20Discov.&amp;doi=10.1039%2FD3DD00113J&amp;volume=2&amp;pages=1233-1250&amp;publication_year=2023&amp;author=Jablonka%2CKM">
                    Google Scholar</a> 
                </p></li><li data-counter="16."><p id="ref-CR16">Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. &amp; Smit, B. Leveraging large language models for predictive chemistry. <i>Nat. Mach. Intell.</i> <b>6</b>, 161–169 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-023-00788-1" data-track-item_id="10.1038/s42256-023-00788-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-023-00788-1" aria-label="Article reference 16" data-doi="10.1038/s42256-023-00788-1">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Leveraging%20large%20language%20models%20for%20predictive%20chemistry&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-023-00788-1&amp;volume=6&amp;pages=161-169&amp;publication_year=2024&amp;author=Jablonka%2CKM&amp;author=Schwaller%2CP&amp;author=Ortega-Guerrero%2CA&amp;author=Smit%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="17."><p id="ref-CR17">Xie, Z. et al. Fine-tuning gpt-3 for machine learning electronic and functional properties of organic molecules. <i>Chem. Sci.</i> <b>15</b>, 500–510 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/D3SC04610A" data-track-item_id="10.1039/D3SC04610A" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FD3SC04610A" aria-label="Article reference 17" data-doi="10.1039/D3SC04610A">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXisFOhsrfL" aria-label="CAS reference 17">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38179524" aria-label="PubMed reference 17">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Fine-tuning%20gpt-3%20for%20machine%20learning%20electronic%20and%20functional%20properties%20of%20organic%20molecules&amp;journal=Chem.%20Sci.&amp;doi=10.1039%2FD3SC04610A&amp;volume=15&amp;pages=500-510&amp;publication_year=2024&amp;author=Xie%2CZ">
                    Google Scholar</a> 
                </p></li><li data-counter="18."><p id="ref-CR18">Liao, C., Yu, Y., Mei, Y. &amp; Wei, Y. From words to molecules: a survey of large language models in chemistry. Preprint at <a href="https://doi.org/10.48550/arXiv.2402.01439" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2402.01439">https://doi.org/10.48550/arXiv.2402.01439</a> (2024).</p></li><li data-counter="19."><p id="ref-CR19">Zhang, D. et al. Chemllm: a chemical large language model. Preprint at <a href="https://doi.org/10.48550/arXiv.2402.06852" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2402.06852">https://doi.org/10.48550/arXiv.2402.06852</a>(2024).</p></li><li data-counter="20."><p id="ref-CR20">Ramos, M. C., Michtavy, S. S., Porosoff, M. D. &amp; White, A. D. Bayesian optimization of catalysts with in-context learning. Preprint at <a href="https://doi.org/10.48550/arXiv.2304.05341" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2304.05341">https://doi.org/10.48550/arXiv.2304.05341</a> (2023).</p></li><li data-counter="21."><p id="ref-CR21">Kristiadi, A. et al. A sober look at LLMs for material discovery: are they actually good for bayesian optimization over molecules? In <i>Proc. 41st International Conference on Machine Learning</i> 1025 (JMLR.org, 2024).</p></li><li data-counter="22."><p id="ref-CR22">Rubungo, A. N., Arnold, C., Rand, B. P. &amp; Dieng, A. B. Llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions. Preprint at <a href="https://doi.org/10.48550/arXiv.2310.14029" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2310.14029">https://doi.org/10.48550/arXiv.2310.14029</a> (2023).</p></li><li data-counter="23."><p id="ref-CR23">Flam-Shepherd, D. &amp; Aspuru-Guzik, A. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. Preprint at <a href="https://doi.org/10.48550/arXiv.2305.05708" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2305.05708">https://doi.org/10.48550/arXiv.2305.05708</a> (2023).</p></li><li data-counter="24."><p id="ref-CR24">Gruver, N. et al. Fine-tuned language models generate stable inorganic materials as text. In <i>Twelfth International Conference on Learning Representations</i> (2024); <a href="https://openreview.net/forum?id=vN9fpfqoP1" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=vN9fpfqoP1">https://openreview.net/forum?id=vN9fpfqoP1</a></p></li><li data-counter="25."><p id="ref-CR25">Alampara, N., Miret, S. &amp; Jablonka, K. M. Mattext: do language models need more than text &amp; scale for materials modeling? Preprint at <a href="https://doi.org/10.48550/arXiv.2406.17295" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2406.17295">https://doi.org/10.48550/arXiv.2406.17295</a> (2024).</p></li><li data-counter="26."><p id="ref-CR26">Patiny, L. &amp; Godin, G. Automatic extraction of fair data from publications using llm. Preprint at <i>ChemRxiv</i> <a href="https://doi.org/10.26434/chemrxiv-2023-05v1b-v2" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.26434/chemrxiv-2023-05v1b-v2">https://doi.org/10.26434/chemrxiv-2023-05v1b-v2</a> (2023).</p></li><li data-counter="27."><p id="ref-CR27">Dagdelen, J. et al. Structured information extraction from scientific text with large language models. <i>Nat. Commun.</i> <b>15</b>, 1418 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41467-024-45563-x" data-track-item_id="10.1038/s41467-024-45563-x" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-024-45563-x" aria-label="Article reference 27" data-doi="10.1038/s41467-024-45563-x">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXksVWrtbs%3D" aria-label="CAS reference 27">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38360817" aria-label="PubMed reference 27">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10869356" aria-label="PubMed Central reference 27">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Structured%20information%20extraction%20from%20scientific%20text%20with%20large%20language%20models&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-024-45563-x&amp;volume=15&amp;publication_year=2024&amp;author=Dagdelen%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="28."><p id="ref-CR28">Zheng, Z. et al. Image and data mining in reticular chemistry powered by gpt-4v. <i>Digit. Discov.</i> <b>3</b>, 491–501 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/D3DD00239J" data-track-item_id="10.1039/D3DD00239J" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FD3DD00239J" aria-label="Article reference 28" data-doi="10.1039/D3DD00239J">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20and%20data%20mining%20in%20reticular%20chemistry%20powered%20by%20gpt-4v&amp;journal=Digit.%20Discov.&amp;doi=10.1039%2FD3DD00239J&amp;volume=3&amp;pages=491-501&amp;publication_year=2024&amp;author=Zheng%2CZ">
                    Google Scholar</a> 
                </p></li><li data-counter="29."><p id="ref-CR29">Lála, J. et al. Paperqa: retrieval-augmented generative agent for scientific research. Preprint at <a href="https://doi.org/10.48550/arXiv.2312.07559" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2312.07559">https://doi.org/10.48550/arXiv.2312.07559</a> (2023).</p></li><li data-counter="30."><p id="ref-CR30">Caufield, J. H. et al. Structured prompt interrogation and recursive extraction of semantics (spires): a method for populating knowledge bases using zero-shot learning. <i>Bioinformatics</i> <b>40</b>, btae104 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/bioinformatics/btae104" data-track-item_id="10.1093/bioinformatics/btae104" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtae104" aria-label="Article reference 30" data-doi="10.1093/bioinformatics/btae104">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2MXhsF2ru7c%3D" aria-label="CAS reference 30">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38383067" aria-label="PubMed reference 30">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10924283" aria-label="PubMed Central reference 30">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Structured%20prompt%20interrogation%20and%20recursive%20extraction%20of%20semantics%20%28spires%29%3A%20a%20method%20for%20populating%20knowledge%20bases%20using%20zero-shot%20learning&amp;journal=Bioinformatics&amp;doi=10.1093%2Fbioinformatics%2Fbtae104&amp;volume=40&amp;publication_year=2024&amp;author=Caufield%2CJH">
                    Google Scholar</a> 
                </p></li><li data-counter="31."><p id="ref-CR31">Gupta, T. et al. DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles. In <i>Proc. 61st Annual Meeting of the Association for Computational Linguistics</i> (Volume 1: Long papers) 13465–13483 (Association for Computational Linguistics, 2023).</p></li><li data-counter="32."><p id="ref-CR32">Schilling-Wilhelmi, M. et al. From text to insight: large language models for chemical data extraction. <i>Chem. Soc. Rev.</i> <b>54</b>, 1125–1150 (2025).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/D4CS00913D" data-track-item_id="10.1039/D4CS00913D" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FD4CS00913D" aria-label="Article reference 32" data-doi="10.1039/D4CS00913D">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXivVWnt7%2FJ" aria-label="CAS reference 32">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=39703015" aria-label="PubMed reference 32">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20text%20to%20insight%3A%20large%20language%20models%20for%20chemical%20data%20extraction&amp;journal=Chem.%20Soc.%20Rev.&amp;doi=10.1039%2FD4CS00913D&amp;volume=54&amp;pages=1125-1150&amp;publication_year=2025&amp;author=Schilling-Wilhelmi%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="33."><p id="ref-CR33">Skarlinski, M. D. et al. Language agents achieve superhuman synthesis of scientific knowledge. Preprint at <a href="https://doi.org/10.48550/arXiv.2409.13740" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2409.13740">https://doi.org/10.48550/arXiv.2409.13740</a> (2024).</p></li><li data-counter="34."><p id="ref-CR34">Miret, S. &amp; Krishnan, N. Are llms ready for real-world materials discovery? Preprint at <a href="https://doi.org/10.48550/arXiv.2402.05200" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2402.05200">https://doi.org/10.48550/arXiv.2402.05200</a> (2024).</p></li><li data-counter="35."><p id="ref-CR35">Gopal, A. et al. Will releasing the weights of future large language models grant widespread access to pandemic agents? Preprint at <a href="https://doi.org/10.48550/arXiv.2310.18233" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2310.18233">https://doi.org/10.48550/arXiv.2310.18233</a> (2023).</p></li><li data-counter="36."><p id="ref-CR36">Ganguli, D. et al. Red teaming language models to reduce harms: methods, scaling behaviors, and lessons learned. Preprint at <a href="https://doi.org/10.48550/arXiv.2209.07858" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2209.07858">https://doi.org/10.48550/arXiv.2209.07858</a> (2022).</p></li><li data-counter="37."><p id="ref-CR37">Urbina, F., Lentzos, F., Invernizzi, C. &amp; Ekins, S. Dual use of artificial-intelligence-powered drug discovery. <i>Nat. Mach. Intell.</i> <b>4</b>, 189–191 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-022-00465-9" data-track-item_id="10.1038/s42256-022-00465-9" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-022-00465-9" aria-label="Article reference 37" data-doi="10.1038/s42256-022-00465-9">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36211133" aria-label="PubMed reference 37">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280" aria-label="PubMed Central reference 37">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Dual%20use%20of%20artificial-intelligence-powered%20drug%20discovery&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-022-00465-9&amp;volume=4&amp;pages=189-191&amp;publication_year=2022&amp;author=Urbina%2CF&amp;author=Lentzos%2CF&amp;author=Invernizzi%2CC&amp;author=Ekins%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="38."><p id="ref-CR38">Campbell, Q. L., Herington, J. &amp; White, A. D. Censoring chemical data to mitigate dual use risk. Preprint at <a href="https://doi.org/10.48550/arXiv.2304.10510" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2304.10510">https://doi.org/10.48550/arXiv.2304.10510</a> (2023).</p></li><li data-counter="39."><p id="ref-CR39">Moulange, R., Langenkamp, M., Alexanian, T., Curtis, S. &amp; Livingston, M. Towards responsible governance of biological design tools. Preprint at <a href="https://doi.org/10.48550/arXiv.2311.15936" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2311.15936">https://doi.org/10.48550/arXiv.2311.15936</a> (2023).</p></li><li data-counter="40."><p id="ref-CR40">Urbina, F., Lentzos, F., Invernizzi, C. &amp; Ekins, S. A teachable moment for dual-use. <i>Nat. Mach. Intell.</i> <b>4</b>, 607–607 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-022-00511-6" data-track-item_id="10.1038/s42256-022-00511-6" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-022-00511-6" aria-label="Article reference 40" data-doi="10.1038/s42256-022-00511-6">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36393980" aria-label="PubMed reference 40">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9645784" aria-label="PubMed Central reference 40">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20teachable%20moment%20for%20dual-use&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-022-00511-6&amp;volume=4&amp;pages=607-607&amp;publication_year=2022&amp;author=Urbina%2CF&amp;author=Lentzos%2CF&amp;author=Invernizzi%2CC&amp;author=Ekins%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">One-third of college students used chatgpt for schoolwork during the 2022-23 academic date. <i>Intelligent.com</i> <a href="https://www.intelligent.com/one-third-of-college-students-used-chatgpt-for-schoolwork-during-the-2022-23-academic-date/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.intelligent.com/one-third-of-college-students-used-chatgpt-for-schoolwork-during-the-2022-23-academic-date/">https://www.intelligent.com/one-third-of-college-students-used-chatgpt-for-schoolwork-during-the-2022-23-academic-date/</a> (2023).</p></li><li data-counter="42."><p id="ref-CR42">Srivastava, A. et al. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. <i>Transactions on Machine Learning Research</i> (2023); <a href="https://openreview.net/forum?id=uyTL5Bvosj" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=uyTL5Bvosj">https://openreview.net/forum?id=uyTL5Bvosj</a></p></li><li data-counter="43."><p id="ref-CR43">Gao, L. et al. A framework for few-shot language model evaluation version v0.4.0. <i>Zenodo</i> <a href="https://zenodo.org/records/10256836" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://zenodo.org/records/10256836">https://zenodo.org/records/10256836</a> (2023).</p></li><li data-counter="44."><p id="ref-CR44">Guo, T. et al. What can large language models do in chemistry? A comprehensive benchmark on eight tasks. Preprint at <a href="https://doi.org/10.48550/arXiv.2305.18365" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2305.18365">https://doi.org/10.48550/arXiv.2305.18365</a> (2023).</p></li><li data-counter="45."><p id="ref-CR45">Ahmad, W., Simon, E., Chithrananda, S., Grand, G. &amp; Ramsundar, B. Chemberta-2: towards chemical foundation models. Preprint at <a href="https://doi.org/10.48550/arXiv.2209.01712" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2209.01712">https://doi.org/10.48550/arXiv.2209.01712</a> (2022).</p></li><li data-counter="46."><p id="ref-CR46">Cai, X. et al. Comprehensive evaluation of molecule property prediction with chatgpt. <i>Methods</i> <b>222</b>, 133–141 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.ymeth.2024.01.004" data-track-item_id="10.1016/j.ymeth.2024.01.004" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.ymeth.2024.01.004" aria-label="Article reference 46" data-doi="10.1016/j.ymeth.2024.01.004">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXhvVSht7g%3D" aria-label="CAS reference 46">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38242382" aria-label="PubMed reference 46">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Comprehensive%20evaluation%20of%20molecule%20property%20prediction%20with%20chatgpt&amp;journal=Methods&amp;doi=10.1016%2Fj.ymeth.2024.01.004&amp;volume=222&amp;pages=133-141&amp;publication_year=2024&amp;author=Cai%2CX">
                    Google Scholar</a> 
                </p></li><li data-counter="47."><p id="ref-CR47">Frey, N. C. et al. Neural scaling of deep chemical models. <i>Nat. Mach. Intell.</i> <b>5</b>, 1297–1305 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-023-00740-3" data-track-item_id="10.1038/s42256-023-00740-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-023-00740-3" aria-label="Article reference 47" data-doi="10.1038/s42256-023-00740-3">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20scaling%20of%20deep%20chemical%20models&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-023-00740-3&amp;volume=5&amp;pages=1297-1305&amp;publication_year=2023&amp;author=Frey%2CNC">
                    Google Scholar</a> 
                </p></li><li data-counter="48."><p id="ref-CR48">Dinh, T. et al. Lift: language-interfaced fine-tuning for non-language machine learning tasks. <i>Adv. Neural Inf. Process. Syst.</i> <b>35</b>, 11763–11784 (2022).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Lift%3A%20language-interfaced%20fine-tuning%20for%20non-language%20machine%20learning%20tasks&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=35&amp;pages=11763-11784&amp;publication_year=2022&amp;author=Dinh%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="49."><p id="ref-CR49">Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. <i>Chem. Sci.</i> <b>9</b>, 513–530 (2018).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/C7SC02664A" data-track-item_id="10.1039/C7SC02664A" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FC7SC02664A" aria-label="Article reference 49" data-doi="10.1039/C7SC02664A">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhslChtrbO" aria-label="CAS reference 49">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29629118" aria-label="PubMed reference 49">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Moleculenet%3A%20a%20benchmark%20for%20molecular%20machine%20learning&amp;journal=Chem.%20Sci.&amp;doi=10.1039%2FC7SC02664A&amp;volume=9&amp;pages=513-530&amp;publication_year=2018&amp;author=Wu%2CZ">
                    Google Scholar</a> 
                </p></li><li data-counter="50."><p id="ref-CR50">Huang, Y. et al. Chemeval: a comprehensive multi-level chemical evaluation for large language models. Preprint at <a href="https://doi.org/10.48550/arXiv.2409.13989" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2409.13989">https://doi.org/10.48550/arXiv.2409.13989</a> (2024).</p></li><li data-counter="51."><p id="ref-CR51">Huang, K. et al. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. <i>NeurIPS Datasets and Benchmarks</i> (2021); <a href="https://www.semanticscholar.org/paper/Therapeutics-Data-Commons%3A-Machine-Learning-and-for-Huang-Fu/54ca116f1e9a45768a3a2c47a4608ff34adefa0c" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.semanticscholar.org/paper/Therapeutics-Data-Commons%3A-Machine-Learning-and-for-Huang-Fu/54ca116f1e9a45768a3a2c47a4608ff34adefa0c">https://www.semanticscholar.org/paper/Therapeutics-Data-Commons%3A-Machine-Learning-and-for-Huang-Fu/54ca116f1e9a45768a3a2c47a4608ff34adefa0c</a></p></li><li data-counter="52."><p id="ref-CR52">Zhao, H. et al. Chemsafetybench: benchmarking llm safety on chemistry domain. Preprint at <a href="https://doi.org/10.48550/arXiv.2411.16736" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2411.16736">https://doi.org/10.48550/arXiv.2411.16736</a> (2024).</p></li><li data-counter="53."><p id="ref-CR53">Dunn, A., Wang, Q., Ganose, A., Dopp, D. &amp; Jain, A. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. <i>npj Comput. Mater.</i> <b>6</b>, 138 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41524-020-00406-3" data-track-item_id="10.1038/s41524-020-00406-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41524-020-00406-3" aria-label="Article reference 53" data-doi="10.1038/s41524-020-00406-3">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Benchmarking%20materials%20property%20prediction%20methods%3A%20the%20matbench%20test%20set%20and%20automatminer%20reference%20algorithm&amp;journal=npj%20Comput.%20Mater.&amp;doi=10.1038%2Fs41524-020-00406-3&amp;volume=6&amp;publication_year=2020&amp;author=Dunn%2CA&amp;author=Wang%2CQ&amp;author=Ganose%2CA&amp;author=Dopp%2CD&amp;author=Jain%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="54."><p id="ref-CR54">Zaki, M. &amp; Krishnan, N. A. Mascqa: investigating materials science knowledge of large language models. <i>Digit. Discov.</i> <b>3</b>, 313–327 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1039/D3DD00188A" data-track-item_id="10.1039/D3DD00188A" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1039%2FD3DD00188A" aria-label="Article reference 54" data-doi="10.1039/D3DD00188A">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Mascqa%3A%20investigating%20materials%20science%20knowledge%20of%20large%20language%20models&amp;journal=Digit.%20Discov.&amp;doi=10.1039%2FD3DD00188A&amp;volume=3&amp;pages=313-327&amp;publication_year=2024&amp;author=Zaki%2CM&amp;author=Krishnan%2CNA">
                    Google Scholar</a> 
                </p></li><li data-counter="55."><p id="ref-CR55">Arora, D., Singh, H., &amp; Mausam. Have LLMs advanced enough? A challenging problem solving benchmark for large language models. in <i>Proc. 2023 Conference on Empirical Methods in Natural Language Processing</i> (eds Bouamor, H., Pino, J. &amp; Bali, K.) 7527–7543 (Association for Computational Linguistics, 2023); <a href="https://aclanthology.org/2023.emnlp-main.468/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://aclanthology.org/2023.emnlp-main.468/">https://aclanthology.org/2023.emnlp-main.468/</a></p></li><li data-counter="56."><p id="ref-CR56">Song, Y., Miret, S., Zhang, H. &amp; Liu, B. Honeybee: progressive instruction finetuning of large language models for materials science. In <i>Findings of the Association for Computational Linguistics: EMNLP 2023</i> (eds Bouamor, H. et al.) 5724–5739 (Association for Computational Linguistics, 2023).</p></li><li data-counter="57."><p id="ref-CR57">Wei, Z. et al. Chemistryqa: a complex question answering dataset from chemistry. <i>OpenReview</i> <a href="https://openreview.net/forum?id=oeHTRAehiFF" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=oeHTRAehiFF">https://openreview.net/forum?id=oeHTRAehiFF</a> (2021).</p></li><li data-counter="58."><p id="ref-CR58">Song, Y., Miret, S. &amp; Liu, B. Matsci-nlp: evaluating scientific language models on materials science language tasks using text-to-schema modeling. In <i>Proc. 61st Annual Meeting of the Association for Computational Linguistics</i> (eds Rogers, A., Boyd-Graber, J. &amp; Okazaki, N.) 3621–3639 (Association for Computational Linguistics, 2023).</p></li><li data-counter="59."><p id="ref-CR59">Hu, X. et al. Towards understanding factual knowledge of large language models. In <i>The Twelfth International Conference on Learning Representations</i> (2024); <a href="https://openreview.net/forum?id=9OevMUdods" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=9OevMUdods">https://openreview.net/forum?id=9OevMUdods</a></p></li><li data-counter="60."><p id="ref-CR60">Bloom, B. <i>Taxonomy of Educational Objectives: the Classification of Educational Goals</i> (Longmans, 1956).</p></li><li data-counter="61."><p id="ref-CR61">Zhang, J., Lehman, J., Stanley, K. &amp; Clune, J. OMNI: Open-endedness via models of human notions of interestingness. In <i>Twelfth International Conference on Learning Representations</i> (2024); <a href="https://openreview.net/forum?id=AgM3MzT99c" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=AgM3MzT99c">https://openreview.net/forum?id=AgM3MzT99c</a></p></li><li data-counter="62."><p id="ref-CR62">Polo, F. M. et al. tinyBenchmarks: evaluating LLMs with fewer examples. In <i>Proc. 41st International Conference on Machine Learning</i> (JMLR.org, 2024).</p></li><li data-counter="63."><p id="ref-CR63">Liang, P. et al. Holistic evaluation of language models. <i>Transactions on Machine Learning Research</i> (2023); <a href="https://openreview.net/forum?id=iO4LZibEqW" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=iO4LZibEqW">https://openreview.net/forum?id=iO4LZibEqW</a></p></li><li data-counter="64."><p id="ref-CR64">Taylor, R. et al. Galactica: a large language model for science. Preprint at <a href="https://doi.org/10.48550/arXiv.2211.09085" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2211.09085">https://doi.org/10.48550/arXiv.2211.09085</a> (2022).</p></li><li data-counter="65."><p id="ref-CR65">Schick, T. et al. Toolformer: language models can teach themselves to use tools. <i>Adv. Neural Inf. Proc. Syst.</i> <b>36</b>, 68539–68551 (2024).</p></li><li data-counter="66."><p id="ref-CR66">Karpas, E. et al. Mrkl systems: a modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Preprint at <a href="https://doi.org/10.48550/arXiv.2205.00445" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2205.00445">https://doi.org/10.48550/arXiv.2205.00445</a> (2022).</p></li><li data-counter="67."><p id="ref-CR67">Yao, S. et al. ReAct: Synergizing reasoning and acting in language models. In <i>Eleventh International Conference on Learning Representations</i> (OpenReview.net, 2023).</p></li><li data-counter="68."><p id="ref-CR68">Xiong, M. et al. Can llms express their uncertainty? An empirical evaluation of confidence elicitation in llms. In <i>Twelfth International Conference on Learning Representations</i> (OpenReview.net, 2024); <a href="https://openreview.net/forum?id=gjeQKFxFpZ" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/forum?id=gjeQKFxFpZ">https://openreview.net/forum?id=gjeQKFxFpZ</a></p></li><li data-counter="69."><p id="ref-CR69">Beeching, E. et al. Open llm leaderboard. <i>Hugging Face</i> <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a> (2023).</p></li><li data-counter="70."><p id="ref-CR70">Argyle, L. P. et al. Out of one, many: using language models to simulate human samples. <i>Polit. Anal.</i> <b>31</b>, 337–351 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1017/pan.2023.2" data-track-item_id="10.1017/pan.2023.2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1017%2Fpan.2023.2" aria-label="Article reference 70" data-doi="10.1017/pan.2023.2">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Out%20of%20one%2C%20many%3A%20using%20language%20models%20to%20simulate%20human%20samples&amp;journal=Polit.%20Anal.&amp;doi=10.1017%2Fpan.2023.2&amp;volume=31&amp;pages=337-351&amp;publication_year=2023&amp;author=Argyle%2CLP">
                    Google Scholar</a> 
                </p></li><li data-counter="71."><p id="ref-CR71">Hughes, E. et al. Position: Open-endedness is essential for artificial superhuman intelligence. In <i>Proc. 41st International Conference on Machine Learning</i> Vol. 235 (eds Salakhutdinov, R. et al.) 20597–20616 (PMLR, 2024); <a href="https://proceedings.mlr.press/v235/hughes24a.html" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://proceedings.mlr.press/v235/hughes24a.html">https://proceedings.mlr.press/v235/hughes24a.html</a></p></li><li data-counter="72."><p id="ref-CR72">Choung, O.-H., Vianello, R., Segler, M., Stiefl, N. &amp; Jiménez-Luna, J. Extracting medicinal chemistry intuition via preference machine learning. <i>Nat. Commun.</i> <b>14</b>, 6651 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41467-023-42242-1" data-track-item_id="10.1038/s41467-023-42242-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-023-42242-1" aria-label="Article reference 72" data-doi="10.1038/s41467-023-42242-1">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXit1Kltb3I" aria-label="CAS reference 72">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37907461" aria-label="PubMed reference 72">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10618272" aria-label="PubMed Central reference 72">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%20medicinal%20chemistry%20intuition%20via%20preference%20machine%20learning&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-023-42242-1&amp;volume=14&amp;publication_year=2023&amp;author=Choung%2CO-H&amp;author=Vianello%2CR&amp;author=Segler%2CM&amp;author=Stiefl%2CN&amp;author=Jim%C3%A9nez-Luna%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="73."><p id="ref-CR73">Li, B. et al. Trustworthy ai: from principles to practices. <i>ACM Comput. Surv.</i> <b>55</b>, 1–46 (2023).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Trustworthy%20ai%3A%20from%20principles%20to%20practices&amp;journal=ACM%20Comput.%20Surv.&amp;volume=55&amp;pages=1-46&amp;publication_year=2023&amp;author=Li%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="74."><p id="ref-CR74">Mirza, A. et al. chem-bench version v0.2.0. <i>Zenodo</i> <a href="https://doi.org/10.5281/zenodo.14010212" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.5281/zenodo.14010212">https://doi.org/10.5281/zenodo.14010212</a> (2024).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41557-025-01815-x?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>This work was supported by the Carl Zeiss Foundation, and a ‘Talent Fund’ of the ‘Life’ profile line of the Friedrich Schiller University Jena. In addition, M.S.-W.’s work was supported by Intel and Merck via the AWASES programme. Parts of A.M.’s work were supported as part of the ‘SOL-AI’ project funded by the Helmholtz Foundation model initiative. K.M.J. is part of the NFDI consortium FAIRmat funded by the Deutsche Forschungsgemeinschaft (the German Research Foundation) project no. 460197019. K.M.J. thanks FutureHouse (a non-profit research organization supported by the generosity of Eric and Wendy Schmidt) for supporting PaperQA2 runs via access to the API. We also thank Stability.AI for the access to its HPC cluster. M.R.-G. and M.V.G. acknowledge financial support from the Spanish Agencia Estatal de Investigación (AEI) through grants TED2021-131693B-I00 and CNS2022-135474, funded by Ministerio de Ciencia, Innovación y Universidades (MICIU)/AEI/10.13039/501100011033 and by the European Union NextGenerationEU/PRTR. M.V.G. acknowledges support from the Spanish National Research Council through the Programme for internationalization i-LINK 2023 (project no. ILINK23047). A.A. gratefully acknowledges financial support for this research by the Fulbright US Student Programme, which is sponsored by the US Department of State and German-American Fulbright Commission. Its contents are solely the responsibility of the author and do not necessarily represent the official views of the Fulbright Programme, the Government of the USA or the German-American Fulbright Commission. M.A. expresses gratitude to the European Research Council for evaluating the project with the reference no. 101106377 titled ‘CLARIFIER’, and accepting it for funding under the HORIZON TMA MSCA Postdoctoral Fellowships—European Fellowships. Furthermore, M.A. acknowledges the funding provided by UK Research and Innovation under the UK government’s Horizon Europe funding guarantee (grant reference EP/Y023447/1; organization reference 101106377). M.R. and U.S.S. thank the ‘Deutsche Forschungsgemeinschaft’ for funding under the regime of the priority programme SPP 2363 ‘Utilization and Development of Machine Learning for Molecular Applications—Molecular Machine Learning’ (SCHU 1229/63-1; project no. 497115849). A.D.D.W. acknowledges funding from the European Union Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement no. 101107360. P.S. acknowledges support from the National Centre of Competence in Research Catalysis (grant no. 225147), a National Centre of Competence in Research grant funded by the Swiss National Science Foundation. In addition, we thank the OpenBioML.org community and their ChemNLP project team for valuable discussions. Moreover, we thank P. Márquez for discussions and support and J. Kimmig for feedback on the web app. In addition, we acknowledge support from S. Kumar with an initial prototype of the web app. We thank B. Smit for feedback on an early version of the manuscript.</p></div></section><section data-title="Funding"><div id="Fun-section"><h2 id="Fun">Funding</h2><p>Open access funding provided by Friedrich-Schiller-Universität Jena.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García.</p></li><li id="na2"><p>Unaffiliated author: Christina Glaubitz.</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Laboratory of Organic and Macromolecular Chemistry, Friedrich Schiller University Jena, Jena, Germany</p><p>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García, Benedict Emoekabu, Mara Schilling-Wilhelmi, Macjonathan Okereke, Anagha Aneesh, Maximilian Greiner, Caroline T. Holick, Tim Hoffmann, Abdelrahman Ibrahim, Lea C. Klepsch, Yannik Köster, Jakob Meyer, Jan Matthias Peschel, Michael Ringleb, Nicole C. Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast &amp; Kevin Maik Jablonka</p></li><li id="Aff2"><p>Helmholtz Institute for Polymers in Energy Applications Jena (HIPOLE Jena), Jena, Germany</p><p>Adrian Mirza, Ulrich S. Schubert &amp; Kevin Maik Jablonka</p></li><li id="Aff3"><p>Institute of Carbon Science and Technology, CSIC, Oviedo, Spain</p><p>Martiño Ríos-García &amp; María Victoria Gil</p></li><li id="Aff4"><p>QpiVolta Technologies Pvt Ltd, Bengaluru, India</p><p>Aswanth Krishnan</p></li><li id="Aff5"><p>Laboratory of Artificial Chemical Intelligence, Institut des Sciences et Ingénierie Chimiques, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland</p><p>Tanya Gupta &amp; Philippe Schwaller</p></li><li id="Aff6"><p>National Centre of Competence in Research Catalysis, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland</p><p>Tanya Gupta &amp; Philippe Schwaller</p></li><li id="Aff7"><p>Department of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK</p><p>Mehrdad Asgari</p></li><li id="Aff8"><p>Macromolecular Chemistry, University of Bayreuth, Bayreuth, Germany</p><p>Juliane Eberhardt</p></li><li id="Aff9"><p>Laboratory of Molecular Simulation, Institut des Sciences et Ingénierie Chimiques, École Polytechnique Fédérale de Lausanne, Sion, Switzerland</p><p>Amir Mohammad Elahi</p></li><li id="Aff10"><p>Institute for Inorganic and Analytical Chemistry, Friedrich Schiller University Jena, Jena, Germany</p><p>Hani M. Elbeheiry</p></li><li id="Aff11"><p>Jena Center for Soft Matter, Friedrich Schiller University Jena, Jena, Germany</p><p>Caroline T. Holick, Tim Hoffmann, Lea C. Klepsch, Yannik Köster, Michael Ringleb, Nicole C. Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast &amp; Kevin Maik Jablonka</p></li><li id="Aff12"><p>Institute for Technical Chemistry and Environmental Chemistry, Friedrich Schiller University Jena, Jena, Germany</p><p>Fabian Alexander Kreth</p></li><li id="Aff13"><p>Center for Energy and Environmental Chemistry Jena, Friedrich Schiller University Jena, Jena, Germany</p><p>Fabian Alexander Kreth, Ulrich S. Schubert &amp; Kevin Maik Jablonka</p></li><li id="Aff14"><p>Intel Labs, Hillsboro, OR, USA</p><p>Santiago Miret</p></li><li id="Aff15"><p>Theoretical Chemistry, Technische Universität Dresden, Dresden, Germany</p><p>A. D. Dinga Wonanke</p></li><li id="Aff16"><p>OpenBioML.org, London, UK</p><p>Michael Pieler</p></li><li id="Aff17"><p>Stability.AI, London, UK</p><p>Michael Pieler</p></li></ol><h3 id="contributions">Contributions</h3><p>A.M., N.A., M.R.-G. and K.M.J. contributed to the software development of the benchmarking framework. K.M.J. wrote the article with help from A.M., N.A., S.K., and M.R.-G. A.K. wrote the software for chembench.org. A.M., N.A., S.K., M.R.-G., K.M.J., T.G., M.S.-W., M.V.G., B.E. and M.O. contributed to the generation of the question dataset. A.A., A.M.E., M.A., J.E., H.M.E., M.V.G., M.G., C.T.H., C.G., T.H., A.I., L.C.K., Y.K., F.A.K., J.M., S.M., J.M.P., M.R., N.C.R., J.S., L.M.S. and A.D.D.W. answered the question dataset for the human benchmark tests. U.S.S. and P.S. contributed to supervision and funding acquisition. K.M.J. directed the project and conceptualized it with P.S., M.P., A.M., N.A., M.R.-G. and S.K. All authors reviewed and edited the manuscript.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:mail@kjablonka.com">Kevin Maik Jablonka</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
            
              <h3 id="FPar4">Competing interests</h3>
              <p>K.M.J. has been a paid contractor for OpenAI (as part of the red teaming network). M.P. is an employee of Stability.AI, and A.M. and N.A. were paid contractors of Stability.AI. The remaining authors declare no competing interests.</p>
            
          </div></div></section><section data-title="Peer review"><div id="peer-review-section"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
            
            
              <h3 id="FPar3">Peer review information</h3>
              <p><i>Nature Chemistry</i> thanks Joshua Schrier and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
            
          </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Extended data"><div id="Sec31-section"><h2 id="Sec31">Extended data</h2><div id="Sec31-content"><div data-test="supplementary-info"><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Extended Data Table 1 Overview of sources of the curated questions</b></figcaption></figure></div></div><div data-test="supp-item" id="Fig6"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 overview of the workflow for " href="https://www.nature.com/articles/s41557-025-01815-x/figures/6" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41557-025-01815-x/MediaObjects/41557_2025_1815_Fig6_ESM.jpg">Extended Data Fig. 1 Overview of the workflow for the assembly of the ChemBench Corpus.</a></h3><p>To assemble the ChemBench corpus, we first collected questions from various sources. Some tasks were manually curated, others semi-programmatically. We added semantic annotations for all questions to make them compatible with systems that use special processing for modalities that are not conventional natural text. We reviewed the questions using manual and automatic methods before adding them to the corpus.</p></div></div></div></div></section><section data-title="Supplementary information"><div id="Sec32-section"><h2 id="Sec32">Supplementary information</h2></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
              <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
            <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20framework%20for%20evaluating%20the%20chemical%20knowledge%20and%20reasoning%20abilities%20of%20large%20language%20models%20against%20the%20expertise%20of%20chemists&amp;author=Adrian%20Mirza%20et%20al&amp;contentID=10.1038%2Fs41557-025-01815-x&amp;copyright=The%20Author%28s%29&amp;publication=1755-4330&amp;publicationDate=2025-05-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41557-025-01815-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41557-025-01815-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Mirza, A., Alampara, N., Kunchapu, S. <i>et al.</i> A framework for evaluating the chemical knowledge and reasoning abilities of large language models against the expertise of chemists.
                    <i>Nat. Chem.</i>  (2025). https://doi.org/10.1038/s41557-025-01815-x</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41557-025-01815-x?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-04-01">01 April 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2025-03-26">26 March 2025</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2025-05-20">20 May 2025</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41557-025-01815-x</span></p></li></ul></div></div></div></div></section>
            </div></div>
  </body>
</html>
