<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mazzzystar.github.io/2022/12/29/Run-CLIP-on-iPhone-to-Search-Photos/">Original</a>
    <h1>Run CLIP on iPhone to search photos</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
        <section id="main"><article id="post-Run-CLIP-on-iPhone-to-Search-Photos" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  
  <div>
    
    
      <header>
        
  
    
  

      </header>
    
    
    <div itemprop="articleBody">
      
        <p>I built an app called Queryable, which integrates the CLIP model on iOS to search the <code>Photos</code> album OFFLINE. It is <a target="_blank" rel="noopener" href="https://apps.apple.com/us/app/queryable/id1661598353?platform=iphone">available on App Store</a> today and I thought it might be helpful to others who are as frustrated with the search function of <code>Photos</code> as I was, so I wrote this article to introduce it.</p>
<p><a target="_blank" rel="noopener" href="https://apps.apple.com/us/app/queryable/id1661598353?platform=iphone"><img src="https://mazzzystar.github.io/images/2022-12-28/Queryable-logo.png" alt="Queryable on App Store" title="Queryable"/></a></p>
<h2 id="clip">CLIP</h2>
<p><a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP</a>(Contrastive Language-Image Pre-Training) is a model proposed by OpenAI in 2021. CLIP can encode images and text into representations that can be compared in the same space. CLIP is the basis for many text-to-image models (e.g. Stable Diffusion) to calculate the distance between the generated image and the prompt during training.</p>
<p>To run on iOS devices in real time, I made a compromise between the performance and the model size, and finally chose the <a target="_blank" rel="noopener" href="https://github.com/openai/CLIP"><strong>ViT-B-32</strong> </a> model, separated the <code>Text Encoder</code> and <code>Image Encoder</code>.</p>
<p>In <strong>ViT-B-32</strong>:</p>
<blockquote>
<ul>
<li><code>Text Encoder</code>       will encode any text into a <strong>1x512</strong> dimensional vector.</li>
<li><code>Image Encoder</code> will encode any image into a <strong>1x512</strong> dimensional vector.</li>
</ul>
</blockquote>
<p>We can calculate the proximity of a text sentence and an image by finding the <code>cosine</code> similarity between their <code>text vector</code> and <code>image vector</code>. The pseudo code is as follows:</p>
<div><pre><code><span>import</span> clip


model, <span>preprocess</span> = clip.load(<span>&#34;ViT-B/32&#34;</span>, <span>device=device)</span>


<span>image_feature</span> = model.encode_image(<span>&#34;photo-of-a-dog.png&#34;</span>)
<span>text_feature</span> = model.encode_text(<span>&#34;rainly night&#34;</span>)


<span>sim</span> = cosin_similarity(image_feature, text_feature)
</code></pre></div>
<h2 id="integrate-clip into ios">Integrate CLIP into iOS</h2>
<p>I exported the <code>Text Encoder</code> and <code>Image Encoder</code> to CoreML model using <a target="_blank" rel="noopener" href="https://coremltools.readme.io/docs/pytorch-conversion">coremltools</a> library. The final models has a total file size of 300MB. Then, I started writing Swift.</p>
<p>Here is how to do inference with <code>Text Encoder</code> on Swift:</p>
<div><pre><code>
<span>let</span> text_encoder = <span>try</span> <span>MLModel(<span>contentsOf</span>: TextEncoderURL, <span>configuration</span>: <span>config</span>)</span>

<span>let</span> text_feature = text_encoder.encode(<span>&#34;a dog&#34;</span>)
</code></pre></div>
<p>The reason I split <code>Text Encoder</code> and <code>Image Encoder</code> into two models is because, when actually using this Photos search app, <strong>your input text will always change, but the content of the Photos library is fixed.</strong> Which means that all the <code>image vectors</code> can be computed once and saved in advance. Then, the <code>text vector</code> is computed only once for each of your searches.</p>
<p>Thus, real-time text searching from tens of thousands of <code>Photos</code> library becomes possible. Below is a flowchart of how Queryable works<span><h-char><h-inner>：</h-inner></h-char></span></p>
<h2 id="performance">Performance</h2>
<p>But, compared to the search function of the iPhone <code>Photos</code>, how much does the CLIP-based album search capability improve? The answer is: <strong>overwhelmingly better</strong>. With CLIP, you can search for a scene in your mind, a tone, an object, or even an emotion conveyed by the image.</p>
<p><img src="https://mazzzystar.github.io/images/2022-12-28/Queryable-search-result.jpg" alt="Search for a scene, an object, a tone or the meaning related to the photo with Queryable"/></p>
<p>To use Queryable, you need to first <strong>build the index</strong>, which will traverse your album, calculate all the image vectors and store. This takes place only ONCE, the total time required for building the index depends on the number of your Photos, the speed is of <strong>~2000</strong> photos per minute on iPhone 12 mini. When you have new photos, you can manually update the index, which is very fast.</p>
<p>The <strong>time cost for a search</strong> also depends on your Photos number, For &lt;10,000 photos it takes <strong>less than 1s</strong>. For me, an iPhone 12 mini user with 35,000 photos, each search takes about <strong>2.8s</strong>.</p>
<p>I made a video to demonstrate the search capabilities of Queryable:</p>
 
<video src="/videos/QueryableFinal.mp4" type="video/mp4" controls="controls" width="40%" height="40%">
</video>
 
</div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="https://mazzzystar.github.io/2022/11/19/three-months-away-from-work/" id="article-nav-older">
      <strong>Older</strong>
      <p>
        
          系统外的三个月
        
      </p>
    </a>
  
</nav>

  
</article>


</section>
        
      </div></div>
  </body>
</html>
