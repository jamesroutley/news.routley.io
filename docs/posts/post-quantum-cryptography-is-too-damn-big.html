<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dadrian.io/blog/posts/pqc-signatures-2024/">Original</a>
    <h1>Post-quantum cryptography is too damn big</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <p>Large-scale quantum computers are capable of breaking all of the common forms of
asymmetric cryptography used on the Internet today. Luckily, they don’t exist
yet. The Internet-wide transition to post-quantum cryptography began in 2022
when NIST announced their final candidates for key exchange and signatures in
the <a href="https://csrc.nist.gov/projects/post-quantum-cryptography">NIST PQC competition</a>. There is <a href="https://blog.cloudflare.com/pq-2024/">plenty</a>
<a href="https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography">written</a> about the <a href="https://durumcrustulum.com/2024/02/24/how-to-hold-kems/">various algorithms</a> and
<a href="https://csrc.nist.gov/projects/pqc-dig-sig">standardization</a> <a href="https://wiki.ietf.org/group/sec/PQCAgility">processes</a> that are underway.</p>
<p>The conventional wisdom is that it will take a long time to transition to
post-quantum cryptography, so we need to start standardizing and deploying
things <em>now</em>, even though quantum computers are not actually visible on the
horizon. We’ll take the best of what comes out the NIST competitions, and deploy
it.</p>
<p>Unfortunately, there has not been enough discussion about how what NIST has
standardized is simply not good enough to deploy on the public web in most
cases. We need better algorithms. Specifically, we need algorithms that use less
bytes on the wire—a KEM that when embedded in a TLS ClientHello is still under
one MTU, a signature that performs on par with ECDSA that is no larger than
RSA-2048, and a sub-100 byte signature where we can optionally handle a larger
public key.</p>
<p>To understand why, we’ll look at the current state of HTTPS. Cryptography is
primarily used in five ways for HTTPS on the public web:</p>
<ul>
<li><strong>Symmetric Encryption/Decryption</strong>: The actual data for HTTP(2) is
transmitted as data inside a TLS connection using some authenticated cipher
(AEAD) such as AES-GCM. This is largely <a href="https://words.filippo.io/dispatches/post-quantum-age/">already secure</a>
against quantum computers.</li>
<li><strong>Key Agreement</strong>: Symmetric cryptography requires a secret key. Key agreement
is the process in which two parties mutually generate a secret key. TLS 1.3
traditionally used Elliptic Curve Diffie-Hellman for key agreement. All
non-post-quantum key exchange mechanisms, including Diffie-Hellman, are broken
by quantum computers.</li>
<li><strong>Server Identity</strong>: Servers are authenticated via X.509 certificates. At
minimum, a server certificate (leaf certificate) contains a public key, and a
signature from an intermediate certificate. The intermediate certificate
contains another public key, and a signature from an trusted root certificate.</li>
<li><strong>Issuance Transparency</strong>: The <a href="https://dadrian.io/blog/posts/certificates-explained">public Web PKI</a>
relies on trusted third-parties known as <em>Certification Authorities</em> to validate
domain ownership. Certificates are publicly logged, and servers attest that
their certificates are included in the logs. This provides a deterrent for
malicious certificate issuance, since any certificate that is maliciously issued
to an attacker for some site will be publicly visible, and has the potential to
be detected.  Servers achieve issuance transparency by providing at least two
<em>Signed Certificate Timestamps</em>, usually embedded in the certificate itself.</li>
<li><strong>Handshake Authentication</strong>: The identity of the server needs to be bound to
the connection itself during the TLS handshake. In TLS 1.3, this is provided
by a signature over the server key share message from the key in the server
certificate in the CertificateVerify message.</li>
</ul>
<p>There is a threat from <em>future</em> quantum computers to encrypted network
connections <em>today</em> in the form of <a href="https://en.wikipedia.org/wiki/Harvest_now,_decrypt_later">“harvest now, decrypt later”</a> attacks.
To defend against this, we only need to ensure that key agreement and symmetric
encryption are “quantum resistant” (secure in the presence of quantum
computers). Luckily, symmetric encryption is already quantum resistant, and so
defending against harvest-now-decrypt-later only requires updating the key
exchange algorithm to a post-quantum variant.</p>
<p>The remaining uses of cryptography in HTTPS—server identity, issuance
transparency, and handshake authentication—will eventually need to transition
to post-quantum variants. In the current structure of TLS, this means replacing
all signatures with post-quantum variants.  However, the need to do so, while no
less <em>important</em><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> than transitioning key exchange, is less <em>urgent</em>. This
matches the actions of browsers, who are <a href="https://www.reddit.com/r/firefox/comments/1827g86/tls_13_hybridized_kyber_support_for_firefox/">actively</a>
<a href="https://blog.chromium.org/2023/08/protecting-chrome-traffic-with-hybrid.html">deploying</a> post-quantum key exchange algorithms. An X25519 key
exchange involves the client and server transmitted 32 bytes each. The NIST
winner for key agreement, <a href="https://csrc.nist.gov/pubs/fips/203/ipd">ML-KEM (Kyber)</a>, involves the client sending
1,184 bytes and the server sending 1,088 bytes.</p>
<p>However, no widely-used browser has started
deploying post-quantum signatures.</p>
<p>This is because post-quantum signatures and their corresponding public keys are
too damn big. There are 5 signatures and 2 public keys transmitted during an
average TLS handshake for HTTPS:</p>
<ul>
<li>The leaf certificate has 1 signing public key of the site, and 1 signature
from the intermediate certificate.</li>
<li>The intermediate certificate has 1 signing public key, used to the validate
the signature on the leaf, and 1 signature from the key on the root
certificate, which is used to validate the authenticity of the intermediate
certificate. The root certificate and its embedded public key are
predistributed to clients.</li>
<li>The handshake itself is has 1 signature from the private key corresponding to
the public key in the leaf certificate.</li>
<li>Each Signed Certificate Timestamp (SCT) contains one signature. The public key used
to the validate the signature is predistributed to clients. Most certificates
have 2 SCTs and therefore 2 additional signatures.</li>
</ul>
<p>The current breakdown of key and signature sizes in TLS is roughly:</p>
<ul>
<li>Root certificates often contain RSA keys, as do intermediate certificates.
Root certificates are predistributed, and intermediates are provided by the
server, alongside the leaft certificate. An RSA intermediate certificate has a
4096-bit (512 byte) signature, and a 2048-bit (256 byte) public key.</li>
<li>An ECDSA leaf certificate has a 32-byte key and a 256-byte RSA signature from
the intermediate.</li>
<li>The handshake contains a 64-byte ECDSA signature.</li>
<li>Each SCT contains a 64-byte ECDSA signature.</li>
</ul>
<p>In total, this is 512 + 256 + 256 + 32 + 64 + 2*64 = 1,248 bytes of signatures
and public keys in a normal TLS handshake for HTTPS. Of the winning signature
algorithms from the first NIST PQC competition, <a href="https://csrc.nist.gov/pubs/fips/204/ipd">ML-DSA (Dilithium)</a> is
the only signature algorithm that could be used in the context of TLS and it has
1,312-byte public keys and 2,420-byte signatures. This means <em>a single ML-DSA
public key is bigger than all of the 5 signatures and 2 public keys currently
transmitted during an HTTPS connection</em>. In a direct “copy-and-replace” of
current signature algorithms with ML-DSA, a TLS handshake would contain 5*2420
+ 2*1312 = 14,724 bytes of signatures and public keys, an over 10x increase.</p>
<p>Barring a large-scale quantum computer staring us in the face, this is not a
tenable amount of data to send simply to <em>open</em> a connection. As a baseline
reality check, we should not be sending over 1% of a 3.5&#34; floppy disk purely in
signatures and public keys.</p>
<p>In more concrete terms, for the server-sent messages, <a href="https://blog.cloudflare.com/pq-2024/">Cloudflare
found</a> that every 1K of additional data added to the server response
caused median HTTPS handshake latency increase by around 1.5%. For the
ClientHello, Chrome saw a 4% increase in TLS handshake latency when they
deployed ML-KEM, which takes up approximate 1K of additional space in the
ClientHello. This pushed the size of the ClientHello greater than the standard
maximum transmission unit (MTU) of packets on the Internet, ~1400 bytes, causing
the ClientHello to be fragmented over two underlying transport layer (TCP or
UDP) packets<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<p>Assuming ML-KEM is here to stay, this means if we want to keep the total
latency impact of post-quantum cryptography under 10%<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, we need to make all
of the authentication happen in under ~4K of additional bytes in the server
reponse messages. Unfortunately, a <em>single</em> ML-DSA signature/public key pair is
~4K bytes. ML-DSA is too big to deploy to mitigate a threat that does not yet
have a timeline to exist.</p>
<p>There is some good news on the horizon. NIST recognized that the signatures were
quite large, and is running a <a href="https://csrc.nist.gov/projects/pqc-dig-sig">follow-on competition</a> for
smaller, faster signatures. Unfortunately, the <a href="https://pqshield.github.io/nist-sigs-zoo/">leaders</a> in that
competition are not quite there yet, but some do have potential:</p>
<ul>
<li><strong>Unbalanced Oil and Vinegar (UOV)</strong>: UOV has big public keys (66K!), but
signatures are 94 bytes, which is on par with the current 64 bytes from an
ECDSA signature in an SCT. The 66K public key size is acceptable because the
public keys for Certificate Transparency logs are predistributed, and there’s
only a small number of logs (~10). UOV is not a solution for root
certificates—there’s too many root certificates and root stores would be too
big to embed in a binary.</li>
<li><strong>SQISign</strong>: SQISign has 64-byte keys and 177-byte signatures. If used for
certificates and the handshake signature, it would be 2*64 + 3*177 = 659
bytes. This is compared to the current RSA+ECDSA approach, which is 4096/8 +
2048/8 + 2048/8 + 32 + 64 = 1,120 bytes. SQISign is a net win (and comparable
to an ECDSA-only chain)! Unforunately, SQISign is incredibly slow. For SQISign
to be feasible, it needs around a 10,000x performance improvement in signing
speed, and a 100x performance improvement in verification.</li>
<li><strong>Mayo</strong>: Mayo is possibly feasible. Mayo1 has 1,168-byte public keys and
321-byte signatures, which makes it a candidate for use in certificates and
for handshake authentication (1168*2 + 321*3 = 3,299 bytes). Mayo2 has 5,488-byte keys, but only 180-byte signatures, which makes it a candidate for SCTs if UOV doesn’t pan out.</li>
</ul>
<p>There’s a couple other performance knobs we can attempt to tweak, but they all
require larger changes to how HTTPS, TLS, and the Web PKI interact than doing a
straight “copy-and-replace” with PQC algorithms.</p>
<ul>
<li><strong>Intermediate ellision</strong>: Predistributing known intermediate certificates to
browsers would save ~1.5K bytes for the median intermediate certificate. This
doesn’t fundamentally change any of the feasibility of the NIST candidates, but
it likely helps Mayo stay within bounds of what’s currently feasible.</li>
<li><strong>Merging SCTs and Certificates</strong>: Experimental proposals such as <a href="https://datatracker.ietf.org/doc/draft-davidben-tls-merkle-tree-certs/">Merkle-Tree
Certificates</a> merge the certificate and SCTs into a single object
with a single hash-based proof of authenticity. This would reduce the
handshake to only require a single handshake signature and a single public key
in the merkle-tree certificate, alongside a hash-based inclusion proof.
Unfortunately, it makes some tradeoffs that are likely not feasible for
non-browser applications, such as requiring delayed (hourly) batch issuance,
and requiring clients to be up to date relative to a transparency server.
Solutions in this form may be a performance optimization for browser clients,
but are likely not feasible for non-browser clients. That being said,
handshake latency matters considerably less for non-browser clients.</li>
<li><strong>Shrink the size of the root store</strong>: A post-quantum root store with less
than 10 certificates containing UOV public keys would be within an order of
magnitude of the size of current root stores.</li>
</ul>
<p>All together, this means that combining Mayo and UOV with other changes to the
PKI <em>may</em> be enough to transition to quantum-resistant authentication in the
WebPKI. Unfortunately, all of this armchair design remains subject to several risks:</p>
<ul>
<li>The performance impact might actually be larger than what Cloudflare measured.
Cloudflare’s experiment likely was primarily ideal clients (enterprise users
on desktop) accessing a login page (served by a low-RTT edge server).</li>
<li>The security of Mayo and UOV might not hold. This would not be the first time
a promising post-quantum algorithm <a href="https://securitycryptographywhatever.com/2022/08/11/hot-cryptanalytic-summer-with-steven-galbraith/">turns out to be broken</a>.</li>
<li>10% might be too much of a performance hit unless quantum computers are
immenient.</li>
</ul>
<p>So what can we do to derisk all this? Well, for any solution, we need to get
better a trust anchor agility, intermediate suppresion, and PKI migrations. This
is <a href="https://datatracker.ietf.org/doc/draft-davidben-tls-trust-expr/">happening already</a>.</p>
<p>The best thing we could do to make the post-quantum transition more feasible is
to come up with better algorithms that have performance characteristics no worse
than RSA-2048. Specifically:</p>
<ol>
<li>A post-quantum KEM that fits in a single MTU when combined with the rest of
the TLS ClientHello</li>
<li>A 10,000x signing speed improvement and 100x verification speed improvement
in SQISign (or a new, equivalent algorithm with these characteristics)</li>
</ol>
<p>To some extent, this may be yelling for the impossible. Unfortunately, using
ML-DSA for the Web PKI in its current form is also impossible.</p>


      </div>
    </div></div>
  </body>
</html>
