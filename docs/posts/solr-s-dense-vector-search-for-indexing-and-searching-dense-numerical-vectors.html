<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://solr.apache.org/guide/solr/latest/query-guide/dense-vector-search.html">Original</a>
    <h1>Solr’s Dense Vector Search for indexing and searching dense numerical vectors</h1>
    
    <div id="readability-page-1" class="page"><div>
<!-- Solr Changes - Start -->

<!-- Solr Changes - End --><article>

<div id="preamble">
<div>
<p>Solr’s <strong>Dense Vector Search</strong> adds support for indexing and searching dense numerical vectors.</p>
<p><a href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a> can be used to produce a vector representation of both the query and the documents in a corpus of information.</p>
<p>These neural network-based techniques are usually referred to as neural search, an industry derivation from the academic field of <a href="https://www.microsoft.com/en-us/research/uploads/prod/2017/06/fntir2018-neuralir-mitra.pdf">Neural information Retrieval</a>.</p>
</div>
</div>
<div>
<h2 id="important-concepts"><a href="#important-concepts"></a>Important Concepts</h2>
<div>
<div>
<h3 id="dense-vector-representation"><a href="#dense-vector-representation"></a>Dense Vector Representation</h3>
<p>A traditional tokenized <a href="https://en.wikipedia.org/wiki/Inverted_index">inverted index</a> can be considered to model text as a &#34;sparse&#34; vector, in which each term in the corpus corresponds to one vector dimension. In such a model, the number of dimensions is generally quite high (corresponding to the term dictionary cardinality), and the vector for any given document contains mostly zeros (hence it is sparse, as only a handful of terms that exist in the overall index will be present in any given document).</p>
<p>Dense vector representation contrasts with term-based sparse vector representation in that it distills approximate semantic meaning into a fixed (and limited) number of dimensions.</p>
<p>The number of dimensions in this approach is generally much lower than the sparse case, and the vector for any given document is dense, as most of its dimensions are populated by non-zero values.</p>
<p>In contrast to the sparse approach (for which tokenizers are used to generate sparse vectors directly from text input) the task of generating vectors must be handled in application logic external to Apache Solr.</p>
<p>There may be cases where it makes sense to directly search data that natively exists as a vector (e.g., scientific data); but in a text search context, it is likely that users will leverage deep learning models such as <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> to encode textual information as dense vectors, supplying the resulting vectors to Apache Solr explicitly at index and query time.</p>
<p>For additional information you can refer to this <a href="https://sease.io/2021/12/using-bert-to-improve-search-relevance.html">blog post</a>.</p>
</div>
<div>
<h3 id="dense-retrieval"><a href="#dense-retrieval"></a>Dense Retrieval</h3>
<p>Given a dense vector <code>v</code> that models the information need, the easiest approach for providing dense vector retrieval would be to calculate the distance (euclidean, dot product, etc.) between <code>v</code> and each vector <code>d</code> that represents a document in the corpus of information.</p>
<p>This approach is quite expensive, so many approximate strategies are currently under active research.</p>
<p>The strategy implemented in Apache Lucene and used by Apache Solr is based on Navigable Small-world graph.</p>
<p>It provides efficient approximate nearest neighbor search for high dimensional vectors.</p>

</div>
</div>
</div>
<div>
<h2 id="index-time"><a href="#index-time"></a>Index Time</h2>
<div>
<p>This is the Apache Solr field type designed to support dense vector search:</p>
<div>
<h3 id="densevectorfield"><a href="#densevectorfield"></a>DenseVectorField</h3>
<p>The dense vector field gives the possibility of indexing and searching dense vectors of float elements.</p>
<p>For example:</p>
<p><code>[1.0, 2.5, 3.7, 4.1]</code></p>
<p>Here’s how <code>DenseVectorField</code> should be configured in the schema:</p>
<div>
<div>
<pre><code data-lang="xml">&lt;fieldType name=&#34;knn_vector&#34; class=&#34;solr.DenseVectorField&#34; vectorDimension=&#34;4&#34; similarityFunction=&#34;cosine&#34;/&gt;
&lt;field name=&#34;vector&#34; type=&#34;knn_vector&#34; indexed=&#34;true&#34; stored=&#34;true&#34;/&gt;</code></pre>
</div>
</div>
<div>
<dl>
<dt><code>vectorDimension</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p><strong>Required</strong></p></td>
<td><p>Default: none</p></td>
</tr>
</tbody>
</table>
<p>The dimension of the dense vector to pass in.</p>
<p>Accepted values:
Any integer &lt; = <code>1024</code>.</p>
</dd>
<dt><code>similarityFunction</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p>Optional</p></td>
<td><p>Default: <code>euclidean</code></p></td>
</tr>
</tbody>
</table>
<p>Vector similarity function; used in search to return top K most similar vectors to a target vector.</p>
<p>Accepted values: <code>euclidean</code>, <code>dot_product</code>  or <code>cosine</code>.</p>

</dd>
</dl>
</div>
<div>
<table>
<tbody><tr>
<td>
<i title="Note"></i>
</td>
<td>
this similarity is intended as an optimized way to perform cosine similarity. In order to use it, all vectors must be of unit length, including both document and query vectors. Using dot product with vectors that are not unit length can result in errors or poor search results.
</td>
</tr>
</tbody></table>
</div>

<div>
<table>
<tbody><tr>
<td>
<i title="Note"></i>
</td>
<td>
the preferred way to perform cosine similarity is to normalize all vectors to unit length, and instead use DOT_PRODUCT. You should only use this function if you need to preserve the original vectors and cannot normalize them in advance.
</td>
</tr>
</tbody></table>
</div>
<p>To use the following advanced parameters that customise the codec format
and the hyper-parameter of the HNSW algorithm make sure you set this configuration in <code>solrconfig.xml</code>:</p>
<div>
<div>
<pre><code data-lang="xml">&lt;config&gt;
&lt;codecFactory class=&#34;solr.SchemaCodecFactory&#34;/&gt;
...</code></pre>
</div>
</div>
<p>Here’s how <code>DenseVectorField</code> can be configured with the advanced codec hyper-parameters:</p>
<div>
<div>
<pre><code data-lang="xml">&lt;fieldType name=&#34;knn_vector&#34; class=&#34;solr.DenseVectorField&#34; vectorDimension=&#34;4&#34; similarityFunction=&#34;cosine&#34; codecFormat=&#34;Lucene90HnswVectorsFormat&#34; hnswMaxConnections=&#34;10&#34; hnswBeamWidth=&#34;40&#34;/&gt;
&lt;field name=&#34;vector&#34; type=&#34;knn_vector&#34; indexed=&#34;true&#34; stored=&#34;true&#34;/&gt;</code></pre>
</div>
</div>
<div>
<dl>
<dt><code>codecFormat</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p>Optional</p></td>
<td><p>Default: <code>Lucene90HnswVectorsFormat</code></p></td>
</tr>
</tbody>
</table>
<p>(advanced) Specifies the knn codec implementation to use</p>
<p>Accepted values: <code>Lucene90HnswVectorsFormat</code>.</p>
</dd>
</dl>
</div>
<p>Please note that the <code>codecFormat</code> accepted values may change in future releases.</p>
<div>
<table>
<tbody><tr>
<td>
<i title="Note"></i>
</td>
<td>
Lucene index back-compatibility is only supported for the default codec.
If you choose to customize the <code>codecFormat</code> in your schema, upgrading to a future version of Solr may require you to either switch back to the default codec and optimize your index to rewrite it into the default codec before upgrading, or re-build your entire index from scratch after upgrading.
</td>
</tr>
</tbody></table>
</div>
<div>
<dl>
<dt><code>hnswMaxConnections</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p>Optional</p></td>
<td><p>Default: <code>16</code></p></td>
</tr>
</tbody>
</table>
<p>(advanced) This parameter is specific for the <code>Lucene90HnswVectorsFormat</code> codec format:</p>
<p>Controls how many of the nearest neighbor candidates are connected to the new node.</p>
<p>It has the same meaning as <code>M</code> from the 2018 paper.</p>
<p>Accepted values:
Any integer.</p>
</dd>
<dt><code>hnswBeamWidth</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p>Optional</p></td>
<td><p>Default: <code>100</code></p></td>
</tr>
</tbody>
</table>
<p>(advanced) This parameter is specific for the <code>Lucene90HnswVectorsFormat</code> codec format:</p>
<p>It is the number of nearest neighbor candidates to track while searching the graph for each newly inserted node.</p>
<p>It has the same meaning as <code>efConstruction</code> from the 2018 paper.</p>
<p>Accepted values:
Any integer.</p>
</dd>
</dl>
</div>
<p><code>DenseVectorField</code> supports the attributes: <code>indexed</code>, <code>stored</code>.</p>
<div>
<table>
<tbody><tr>
<td>
<i title="Note"></i>
</td>
<td>
currently multivalue is not supported
</td>
</tr>
</tbody></table>
</div>
<p>Here’s how a <code>DenseVectorField</code> should be indexed:</p>
<div>
<div>
<div id="json">
<div>
<p><strong>JSON</strong></p>
<div>
<div>
<pre><code data-lang="json">[{ &#34;id&#34;: &#34;1&#34;,
&#34;vector&#34;: [1.0, 2.5, 3.7, 4.1]
},
{ &#34;id&#34;: &#34;2&#34;,
&#34;vector&#34;: [1.5, 5.5, 6.7, 65.1]
}
]</code></pre>
</div>
</div>
</div>
</div>
<div id="xml">
<div>
<p><strong>XML</strong></p>
<div>
<div>
<pre><code data-lang="xml">&lt;add&gt;
&lt;doc&gt;
&lt;field name=&#34;id&#34;&gt;1&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;1.0&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;2.5&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;3.7&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;4.1&lt;/field&gt;
&lt;/doc&gt;
&lt;doc&gt;
&lt;field name=&#34;id&#34;&gt;2&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;1.5&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;5.5&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;6.7&lt;/field&gt;
&lt;field name=&#34;vector&#34;&gt;65.1&lt;/field&gt;
&lt;/doc&gt;
&lt;/add&gt;</code></pre>
</div>
</div>
</div>
</div>
<div id="solrj">
<div>
<p><strong>SolrJ</strong></p>
<div>
<div>
<pre><code data-lang="java">final SolrClient client = getSolrClient();

final SolrInputDocument d1 = new SolrInputDocument();
d1.setField(&#34;id&#34;, &#34;1&#34;);
d1.setField(&#34;vector&#34;, Arrays.asList(1.0f, 2.5f, 3.7f, 4.1f));


final SolrInputDocument d2 = new SolrInputDocument();
d2.setField(&#34;id&#34;, &#34;2&#34;);
d2.setField(&#34;vector&#34;, Arrays.asList(1.5f, 5.5f, 6.7f, 65.1f));

client.add(Arrays.asList(d1, d2));</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<h2 id="query-time"><a href="#query-time"></a>Query Time</h2>
<div>
<p>This is the Apache Solr query approach designed to support dense vector search:</p>
<div>
<h3 id="knn-query-parser"><a href="#knn-query-parser"></a>knn Query Parser</h3>
<p>The <code>knn</code> k-nearest neighbors query parser allows to find the k-nearest documents to the target vector according to indexed dense vectors in the given field.</p>
<p>The score for a retrieved document is the approximate distance to the target vector(defined by the similarityFunction configured at indexing time).</p>
<p>It takes the following parameters:</p>
<div>
<dl>
<dt><code>f</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p><strong>Required</strong></p></td>
<td><p>Default: none</p></td>
</tr>
</tbody>
</table>
<p>The <code>DenseVectorField</code> to search in.</p>
</dd>
<dt><code>topK</code></dt>
<dd>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><p>Optional</p></td>
<td><p>Default: 10</p></td>
</tr>
</tbody>
</table>
<p>How many k-nearest results to return.</p>
</dd>
</dl>
</div>
<p>Here’s how to run a KNN search:</p>
<div>
<div>
<pre><code data-lang="text">&amp;q={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]</code></pre>
</div>
</div>
<p>The search results retrieved are the k-nearest to the vector in input <code>[1.0, 2.0, 3.0, 4.0]</code>, ranked by the similarityFunction configured at indexing time.</p>
<div>
<h4 id="usage-with-filter-queries"><a href="#usage-with-filter-queries"></a>Usage with Filter Queries</h4>
<p>The <code>knn</code> query parser can be used in filter queries:</p>
<div>
<div>
<pre><code data-lang="text">&amp;q=id:(1 2 3)&amp;fq={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]</code></pre>
</div>
</div>
<p>The <code>knn</code> query parser can be used with filter queries:</p>
<div>
<div>
<pre><code data-lang="text">&amp;q={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]&amp;fq=id:(1 2 3)</code></pre>
</div>
</div>
<div>
<table>
<tbody><tr>
<td>
<i title="Important"></i>
</td>
<td>
<p>When using <code>knn</code> in these scenarios make sure you have a clear understanding of how filter queries work in Apache Solr:</p>
<p>The Ranked List of document IDs resulting from the main query <code>q</code> is intersected with the set of document IDs deriving from each filter query <code>fq</code>.</p>
<p>e.g.</p>
<p>Ranked List from <code>q</code>=<code>[ID1, ID4, ID2, ID10]</code> &lt;intersects&gt; Set from <code>fq</code>=<code>{ID3, ID2, ID9, ID4}</code> = <code>[ID4,ID2]</code></p>
</td>
</tr>
</tbody></table>
</div>
</div>
<div>
<h4 id="usage-as-re-ranking-query"><a href="#usage-as-re-ranking-query"></a>Usage as Re-Ranking Query</h4>
<p>The <code>knn</code> query parser can be used to rerank first pass query results:</p>
<div>
<div>
<pre><code data-lang="text">&amp;q=id:(3 4 9 2)&amp;rq={!rerank reRankQuery=$rqq reRankDocs=4 reRankWeight=1}&amp;rqq={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]</code></pre>
</div>
</div>
<div>
<table>
<tbody><tr>
<td>
<i title="Important"></i>
</td>
<td>
<p>When using <code>knn</code> in re-ranking pay attention to the <code>topK</code> parameter.</p>
<p>The second pass score(deriving from knn) is calculated only if the document <code>d</code> from the first pass is within
the k-nearest neighbors(<strong>in the whole index</strong>) of the target vector to search.</p>
<p>This means the second pass <code>knn</code> is executed on the whole index anyway, which is a current limitation.</p>
<p>The final ranked list of results will have the first pass score(main query <code>q</code>) added to the second pass score(the approximated similarityFunction distance to the target vector to search) multiplied by a multiplicative factor(reRankWeight).</p>
<p>Details about using the ReRank Query Parser can be found in the <a href="https://solr.apache.org/guide/solr/latest/query-guide/query-re-ranking.html" class="page">Query Re-Ranking</a> section.</p>
</td>
</tr>
</tbody></table>
</div>
</div>
</div>
</div>
</div>

</article>
  </div></div>
  </body>
</html>
