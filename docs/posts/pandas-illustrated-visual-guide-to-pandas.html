<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scribe.citizen4.eu/pandas-illustrated-the-definitive-visual-guide-to-pandas-c31fa921a43">Original</a>
    <h1>Pandas Illustrated: Visual Guide to Pandas</h1>
    
    <div id="readability-page-1" class="page"><article><section><h3>Is it a copy or a view? Should I merge or join? And what the heck is MultiIndex?</h3><figure><img src="https://cdn-images-1.medium.com/fit/c/800/458/1*MDyxk2ivjo9sD2_kd_B1TQ.png" width="800"/><label for="12259986282658201042">✍︎</label><span>All images by author</span></figure><p><a href="https://pandas.pydata.org/">Pandas</a> is an industry standard for analyzing data in Python. With a few keystrokes, you can load, filter, restructure, and visualize gigabytes of heterogeneous information. Built on top of the NumPy library, it borrows many of its concepts and syntax conventions, so if you are comfortable with NumPy, you’ll find Pandas a pretty familiar tool. And even if you’ve never heard of NumPy, Pandas provides a great opportunity to crack down on data analysis problems with little or no programming background.</p><p>There’re a lot of Pandas guides out there. In this particular one, you’re expected to have a basic understanding of NumPy. If you don’t, I’d suggest you skim through the <a href="https://betterprogramming.pub/numpy-illustrated-the-visual-guide-to-numpy-3b1d4976de1d?sk=57b908a77aa44075a49293fa1631dd9b">NumPy Illustrated</a> guide to get an idea of what a NumPy array is, in which ways it is superior to a Python list and how it helps avoid loops in elementary operations.</p><p>Two key features that Pandas brings to NumPy arrays are:</p><p>1. Heterogeneous types — each column is allowed to have its own type;</p><p>2. Index — improves lookup speed for the specified column(s).</p><p>It turns out those features are enough to make Pandas a powerful competitor to both spreadsheets and databases.</p><p>The article consists of four parts:</p><p>Part 1. Motivation
 Part 2. Series and Index
 Part 3. DataFrames
 Part 4. MultiIndex</p><h2>Contents</h2><ol><li>Motivation and Showcase
 <a href="#1cc4">Pandas Showcase</a>
 <a href="#6728">Pandas Speed</a></li><li><a href="#ac49">Series and Index</a>
 <a href="#c884">Index</a>
 <a href="#a363">Find element by value</a>
 <a href="#44b7">Missing values</a>
 <a href="#7eca">Comparisons</a>
 <a href="#786f">Appends, inserts, deletions</a>
 <a href="#3394">Statistics</a>
 <a href="#c784">Duplicate data</a>
 <a href="#ceb7">Group by</a></li><li><a href="#787c">DataFrames</a>
 <a href="#ae78">Reading and writing CSV files</a>
 <a href="#cc8b">Building a DataFrame</a>
 <a href="#a862">Basic operations with DataFrames</a>
 <a href="#2b47">Indexing DataFrames</a>
 <a href="#8752">DataFrame arithmetic</a>
 <a href="#4561">Combining DataFrames</a>:
 — <a href="#df1a">Vertical stacking</a>
 — <a href="#b7d1">Horizontal stacking</a>
 — <a href="#1a37">Stacking via MultiIndex</a>
 Joining DataFrames:
 — <a href="#58bf">1:1 relationship joins</a>
 — <a href="#0043">1:n relationship joins</a>
 — <a href="#7c74">Multiple joins</a>
 <a href="#5936">Inserts and deletes</a>
 <a href="#3c23">Group by</a>
 <a href="#7e77">Pivoting and ‘unpivoting’</a></li><li><a href="#963c">MultiIndex</a>
 <a href="#3df0">Visual Grouping</a>
 <a href="#a44a">Type conversions</a>
 <a href="#21cc">Building DataFrame with MultiIndex</a>
 <a href="#1a7e">Indexing with MultiIndex</a>
 <a href="#9a10">Stacking and unstacking</a>
 <a href="#b264">How to prevent stack/unstack from sorting</a>
 <a href="#bf79">Manipulating levels</a>
 <a href="#d7f9">Converting MultiIndex into flat Index and restoring it back</a>
 <a href="#d209">Sorting MultiIndex</a>
 <a href="#9667">Reading and writing MultiIndexed DataFrames to disk</a>
 <a href="#76b7">MultiIndex arithmetic</a></li></ol><h2>Part 1. Motivation and Showcase</h2><p>Suppose you have a file with a million lines of comma-separated values like this:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/202/1*Eprb6ueQbRl4t4Plt02ciQ.png" width="800"/><label for="10922050877510843055">✍︎</label><span>Spaces after colons are for illustrative purposes only. Usually, there are none.</span></figure><p>And you need to give answers to basic questions like “Which cities have an area over 450 km² and a population under 10 million” with NumPy.</p><p>The brute-force solution of feeding the whole table into a NumPy array is not a good option: usually, NumPy arrays are homogeneous (all values must be of the same type), so all fields will be interpreted as strings, and comparisons will not work as expected.</p><p>Yes, NumPy has structured and record arrays that allow columns of different types, but they are primarily meant for interfacing with C code. When used for general purposes, they have the following downsides:</p><ul><li>not really intuitive (e.g., you’ll be faced with constants like <code>&lt;f8</code> and <code>&lt;U8</code> everywhere)</li><li>have some performance issues as compared to regular NumPy arrays</li><li>stored contiguously in memory, so each column addition or deletion requires reallocation of the whole array</li><li>still lack a lot of functionality of Pandas DataFrames</li></ul><p>Your next try would probably be to store each column as a separate NumPy vector. And after that, maybe wrap them into a <code>dict</code> so it would be easier to restore the integrity of the ‘database’ if you decide to add or remove a row or two later. Here’s what that would look like:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/165/1*lZH0zLkDU01MbmLoq2B3SQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>If you’ve done that — congratulations! You’ve made your first step in reimplementing Pandas. :)</p><p>Now, here’s a couple of examples of what Pandas can do for you that NumPy cannot (or requires significant effort to accomplish).</p><h2>Pandas Showcase</h2><p>Consider the following table:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/226/1*F7-hQHvxrxfN1j36Q6l3Mw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>It describes the diverse product line of an online shop with a total of four distinct products. In contrast with the previous example, it can be represented with either a NumPy array or a Pandas DataFrame equally well. But let us look at some common operations with it.</p><h3>1. Sorting</h3><p>Sorting by column is more readable with Pandas, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/348/1*DKVpqZ00lqWTIducEHbdNQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Here <code>argsort(a[:,1]) </code>calculates the permutation that makes the second column of <code>a</code> to be sorted in ascending order and then <code>a[…]</code> reorders the rows of <code>a</code>, accordingly. Pandas can do it in one step.</p><h3>2. Sorting by several columns</h3><p>If we need to sort by price column breaking ties using the weight column, the situation gets worse for NumPy. Here’s a few examples to illustrate our point:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/354/1*dlpYc3VwGuv-R0iTu4mPzw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>With NumPy, we first order by weight, then apply second sorting by price. A stable sorting algorithm guarantees that the result of the first sort is not lost during the second one. There are <a href="https://betterprogramming.pub/numpy-illustrated-the-visual-guide-to-numpy-3b1d4976de1d?sk=57b908a77aa44075a49293fa1631dd9b#b97e">other ways</a> to do it with NumPy, but none are as simple and elegant as with Pandas.</p><h3>3. Adding a column</h3><p>Adding columns is way better with Pandas syntactically and architecturally. The following example shows you how:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/340/1*sagPinoTw4nfgiKK97OL1A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Pandas does not need to reallocate memory for the whole array like NumPy; it just adds a reference to a new column and updates a ‘registry’ of the column names.</p><h3>4. Fast element search</h3><p>With NumPy arrays, even if the element you search for is the first one, you’ll still need time proportional to the size of the array to find it. With Pandas, you can index the column(s) you expect to be queried most often and reduce search time to a constant.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/431/1*6bWbS6mCvT-uiJsLR5e6ag.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The index column has the following limitations:</p><ul><li>It requires memory and time to be built.</li><li>It is read-only (needs to be rebuilt after each append or delete operation).</li><li>The values are not required to be unique, but speedup only happens when the elements are unique.</li><li>It requires preheating: the first query is somewhat slower than in NumPy, but the subsequent ones are significantly faster.</li></ul><h3>5. Joins by column</h3><p>If you want to complement a table with information from another table based on a common column, NumPy is hardly any help. Pandas is better, especially for 1:n relationships.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/343/1*EbVEDx9maySLpDsdND7b5A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Pandas <code>join</code> has all the familiar ‘inner,’ ‘left,’ ‘right,’ and ‘full outer’ join modes.</p><h3>6. Grouping by column</h3><p>Yet another common operation in data analysis is grouping by column(s). For example, to get the total quantity of each product sold, you can do the following:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/340/1*VeV4Jw2kfF2Nt89ftmxi2A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In addition to <code>sum</code>, Pandas supports all kinds of aggregate functions: <code>mean</code>, <code>max</code>,<code>min</code>, <code>count</code>, etc.</p><h3>7. Pivot tables</h3><p>One of the most powerful features of Pandas is a “pivot” table. It is something like projecting multi-dimensional space into a two-dimensional plane.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/320/1*20nnKdjRK3wEzjOvvAjusA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Although it is certainly possible to implement it with NumPy, this functionality is missing ‘out of the box,’ though it is present in all major <a href="https://modern-sql.com/use-case/pivot">relational databases</a> and spreadsheet apps (<a href="https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576">Excel</a>, <a href="https://support.google.com/docs/answer/1272900?hl=en&amp;co=GENIE.Platform%3DDesktop">Google Sheets</a>).</p><p>Pandas also has <code>df.pivot_table</code> which combines grouping and pivoting in one tool.</p><p>In a nutshell, the two main differences between NumPy and Pandas are the following:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/541/1*wDNHLCopE_ZgtC8t8Ywunw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Now, let’s see whether those features come at the cost of a performance hit.</p><h2>Pandas Speed</h2><p>I’ve benchmarked NumPy and Pandas on a workload typical for Pandas: 5–100 columns, 10³ — 10⁸ rows, integers and floats. Here are the results for 1 row and 100 million rows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/338/1*xZD8Ky4Z2RM9Rax8Fdhfyg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>It looks as if in every single operation, Pandas is slower than NumPy!</p><p>The situation (predictably) does not change when the number of columns increases. As for the number of rows, the dependency (in the logarithmic scale) looks like this:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/372/1*ouJSVxgRrPJKt-c68W_2Hw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Pandas seems to be 30 times slower than NumPy for small arrays (under a hundred rows) and three times slower for large ones (over a million rows).</p><p>How can it be? Maybe it is high time to submit a feature request to suggest Pandas reimplement <code>df.column.sum()</code> via <code>df.column.values.sum()</code>? The <code>values</code> property here provides access to the underlying NumPy array and results in a 3x-30x speedup.</p><p>The answer is no. Pandas is so slow at those basic operations because it correctly handles the missing values. Pandas needs NaNs (not-a-number) for all of this database-like machinery like grouping and pivoting, plus it is a common thing in the real world. In Pandas, a lot of work has been done to unify the usage of NaN across all the supported data types. By definition (enforced on the CPU level), <code>nan</code>+anything results in <code>nan</code>. So</p><pre>&gt;&gt;&gt; np.sum([1, np.nan, 2])
nan</pre><p>but</p><pre>&gt;&gt;&gt; pd.Series([1, np.nan, 2]).sum()
3.0</pre><p>A fair comparison would be to use <code>np.nansum</code> instead of <code>np.sum</code>, <code>np.nanmean</code> instead of <code>np.mean</code> and so on. And suddenly…</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/367/1*Iyk5FXSeXgFz3DdISoFCfA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Pandas becomes 1.5 times faster than NumPy for arrays with over a million elements. It is still 15 times slower than NumPy for smaller arrays, but usually, it does not matter much if the operation is completed in 0.5 ms or 0.05 ms — it is fast anyway.</p><blockquote><p>The bottom line is that if you’re 100% sure you have no missing values in your column(s), it makes sense to use <em><code>df.column.values.sum()</code></em> instead of <em><code>df.column.sum() </code></em>to have x3-x30 performance boost. In the presence of missing values the speed of Pandas is quite decent and even beats NumPy for huge arrays (over 10⁶ elements).</p></blockquote><h2>Part 2. Series and Index</h2><figure><img src="https://cdn-images-1.medium.com/fit/c/800/297/1*JFOykLsVazzxTdr4oJ41cQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Series is a counterpart of a 1D array in NumPy and is a basic building block for a DataFrame representing its column. Although its practical importance is diminishing in comparison to a DataFrame (you can perfectly well solve a lot of practical problems without knowing what a Series is), you might have a hard time understanding how DataFrames work without learning Series and Index first.</p><p>Internally, Series stores the values in a plain old NumPy vector. As such, it inherits its merits (compact memory layout, fast random access) and demerits (type homogeneity, slow deletions, and insertions). On top of that, Series allows accessing its values by <em>label</em> using a dict-like structure called <em>index</em>. Labels can be of any type (commonly strings and time stamps). They need not be unique, but uniqueness is required to boost the lookup speed and is assumed in many operations.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/191/1*zOZ6eZzgQ2gzidRLLo1f5A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>As you can see, now every element can be addressed in two alternative ways: by ‘label’ (=using the index) and by ‘position’ (=not using the index):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/223/1*yFgpYI_tprMerCrOLRr6HQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Addressing by ‘position’ is sometimes called as by ‘positional index’ which merely adds to the confusion.</p><p>One pair of square brackets is not enough for this. In particular:</p><ul><li><code>s[2:3]</code> is not the most convenient way to address element number 2</li><li>if <code>names</code> happens to be integers, <code>s[1:3]</code> becomes ambiguous. It might mean <code>names</code> 1 to 3 inclusive or positional index 1 to 3 exclusive.</li></ul><p>To address those issues, Pandas has two more ‘flavors’ of square brackets, which you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/204/1*FlCqe7nGuygynyg0MfrczQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>• <code>.loc</code> always uses <code>labels</code> and includes both ends of the interval.
• <code>.iloc</code> always uses ‘positional indices’ and excludes the right end.</p><p>The purpose of having square brackets instead of parentheses is to get access to Python slicing conventions: You can use a single or double colon with the familiar meaning of <code>start:stop:step</code>. As usual, missing start (end) means from the start (to the end) of the Series. The step argument allows to reference even rows with <code>s.iloc[::2]</code> and to get elements in reverse order with <code>s[&#39;Paris&#39;:&#39;Oslo&#39;:-1]</code></p><p>They also support boolean indexing (indexing with an array of booleans), as this image shows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/206/1*gPF81TsMk8gbTvysONfJeA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>And you can see how they support ‘fancy indexing’ (indexing with an array of integers) in this image:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/206/1*aPEgTD8YUKDphJ-qC9e2gw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The worst thing about Series is its visual representation: for some reason, it didn’t receive a nice rich-text outlook, so it feels like a second-class citizen in comparison with a DataFrame:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/290/1*iN9VLH1P4T9NP5u5eSICJg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>I’ve monkey-patched the Series to make it look better, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/458/1*OCRgJ6MVszvTnRhYjxNwnQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The vertical line means this is a Series, not a DataFrame. Footer is disabled here, but it can be useful for showing dtypes, especially with Categoricals.</p><p>You can also display several Series or DataFrames side by side with <code>pdi.sidebyside(obj1, obj2, …)</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/270/1*2CHoLkSOLjbfH-1F6ot2Tw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The <code>pdi</code> (stands for <strong>p</strong>an<strong>d</strong>as <strong>i</strong>llustrated) is an open-source library on <a href="https://github.com/axil/pandas-illustrated">github</a> with this and other functions for this article. To use it, write</p><p><code>pip install pandas-illustrated</code></p><h3>Index</h3><p>The object responsible for getting elements by a <code>label</code> is called <code>index</code>. It is fast: you can get a row in constant time, whether you have five rows or 5 billion rows.</p><p><code>Index</code> is a truly polymorphic creature. By default, when you create a Series (or a DataFrame) without an index, it initializes to a lazy object similar to Python’s <code>range()</code>. And like <code>range</code>, barely uses any memory, and is indistinguishable from positional indexing. Let’s create a Series of a million elements with the following code:</p><pre>&gt;&gt;&gt; s = pd.Series(np.zeros(10**6))
&gt;&gt;&gt; s.index
RangeIndex(start=0, stop=1000000, step=1)
&gt;&gt;&gt; s.index.memory_usage()       # in bytes
128                    # the same as for Series([0.])</pre><p>Now, if we delete an element, the index implicitly morphs into a dict-like structure, as follows:</p><pre>&gt;&gt;&gt; s.drop(1, inplace=True)
&gt;&gt;&gt; s.index
Int64Index([     0,      2,      3,      4,      5,      6,      7,
            ...
            999993, 999994, 999995, 999996, 999997, 999998, 999999],
           dtype=&#39;int64&#39;, length=999999)
&gt;&gt;&gt; s.index.memory_usage()
7999992</pre><p>This structure consumes 8Mb of memory! To get rid of it and get back to the lightweight range-like structure, add the following:</p><pre>&gt;&gt;&gt; s.reset_index(drop=True, inplace=True)
&gt;&gt;&gt; s.index
RangeIndex(start=0, stop=999999, step=1)
&gt;&gt;&gt; s.index.memory_usage()
128</pre><p>If you’re new to Pandas, you might wonder why Pandas didn’t do it on its own? Well, for non-numeric labels, it is sort of obvious: why (and how) would Pandas, after deleting a row, relabel all the subsequent rows? For numeric labels, the answer is a bit more convolved.</p><p>First, as we’ve seen already, Pandas allows you to reference rows purely by position, so if you want to address row number 5 after deleting row number 3, you can do it without reindexing (that’s what <code>iloc</code> is for).</p><p>Second, keeping original labels is a way to keep a connection with a moment in the past, like a ‘save game’ button. Imagine you have a big 100x1000000 table and need to find some data. You’re making several queries one by one, each time narrowing your search, but looking at only a subset of the columns because it is impractical to see all the hundreds of fields at the same time. Now that you have found the rows of interest, you want to see all the information in the original table about them. A numeric index helps you get it immediately without any additional effort.</p><p>Generally, keeping values in the index unique is a good idea. For example, you won’t get a lookup speed boost in the presence of duplicate values in the index. Pandas does not have a ‘unique constraint’ like relational databases (<a href="https://pandas.pydata.org/docs/reference/api/pandas.Flags.allows_duplicate_labels.html">the feature</a> is still experimental), but it has functions to check if values in the index are unique and to get rid of duplicates in various ways.</p><p>Sometimes, a single column is not enough to uniquely identify the row. For example, cities of the same name sometimes happen to be found in different countries or even in different regions of the same country. So (city, state) is a better candidate for identifying a place than city alone. In databases, it is called the ‘composite primary key.’ In Pandas, it is called MultiIndex (see part 4 below), and each column inside the index is called a ‘level.’</p><p>Another substantial quality of an index is that it is immutable. In contrast to ordinary columns in the DataFrame, you cannot change it in place. Any change in the index involves getting data from the old index, altering it, and reattaching the new data as the new index. More often than not, it happens transparently, which is why you cannot just write <code>df.City.name = ‘city’</code>, and you have to write a less obvious <code>df.rename(columns={‘A’: ‘a’}, inplace=True)</code></p><p>Index has a name (in the case of MultiIndex, every level has a name). Unfortunately, this name is underused in Pandas. Once you have included the column in the index, you cannot use the convenient <code>df.column_name</code> notation anymore and have to revert to the less readable <code>df.index</code> or the more universal <code>df.loc</code> The situation is even worse with MultiIndex. A prominent exception is <code>df.merge </code>— you can specify the column to merge by name, no matter if it is in the index or not.</p><p>The same indexing mechanism is used to label rows and columns of the DataFrames, as well as for the Series.</p><h3>Find element by value</h3><p>Internally, a Series consists of a NumPy array plus an array-like structure called <code>index</code>, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/272/1*Pv9iM9jJUuPqT5HGFI3WIw.png" width="800"/><label for="9442460867126953323">✍︎</label><span>Anatomy of a Series</span></figure><p><code>Index</code> provides a convenient way to find a value by <code>label</code>. But how about finding a label by value?</p><pre>s.index[s.tolist().find(x)]           # faster for len(s) &lt; 1000
s.index[np.where(s.values==x)[0][0]]  # faster for len(s) &gt; 1000</pre><p>I’ve written a pair of thin wrappers called <code>find()</code> and <code>findall()</code> that are fast (as they automatically choose the actual command based on the series size) and more pleasant to use. Here’s what the code looks like:</p><pre>&gt;&gt;&gt; import pdi
&gt;&gt;&gt; pdi.find(s, 2)
&#39;penguin&#39;
&gt;&gt;&gt; pdi.findall(s, 4)
Index([&#39;cat&#39;, &#39;dog&#39;], dtype=&#39;object&#39;)</pre><h3>Missing values</h3><p>Pandas developers took special care about the missing values. Usually, you receive a dataframe with NaNs by providing a flag to <code>read_csv</code>. Otherwise, you can use None in the constructor or in an assignment operator (it will work despite being implemented slightly differently for different data types). This image will help explain the concept:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/361/1*w2sNUkxiK_F4Y-bBCcpp_g.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The first thing you can do with NaNs is understand if you have any. As seen from the image above, <code>isna()</code> produces a boolean array and <code>.sum()</code> gives the total number of missing values.</p><p>Now that you know they are there, you can opt to get rid of them all at once by filling them with a constant value or through interpolation, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/227/1*tyzJAgVzPTt7TVOQqa83QQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>On the other hand, you can keep using them. Most Pandas functions happily ignore the missing values, as you can see in the image below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/260/1*1EzfnD-xr-0OknFkBOYxGQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>More advanced functions (<code>median</code>, <code>rank</code>, <code>quantile</code>, etc.) also do.</p><p>Arithmetic operations are aligned against the <code>index</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/160/1*Bln2ayx6iO3sGYzfd1N18Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The results are inconsistent in the presence of non-unique values in the index. Do not use arithmetic operations on series with a non-unique index.</p><h3>Comparisons</h3><p>Comparing arrays with missing values might be tricky. Here’s an example:</p><pre>&gt;&gt;&gt; np.all(pd.Series([1., None, 3.]) == 
           pd.Series([1., None, 3.]))
False
&gt;&gt;&gt; np.all(pd.Series([1, None, 3], dtype=&#39;Int64&#39;) == 
           pd.Series([1, None, 3], dtype=&#39;Int64&#39;))
True
&gt;&gt;&gt; np.all(pd.Series([&#39;a&#39;, None, &#39;c&#39;]) == 
           pd.Series([&#39;a&#39;, None, &#39;c&#39;]))
False</pre><p>To be compared properly, NaNs need to be replaced with something that is guaranteed to be missing from the array. E.g. with <code>&#39;&#39;</code>, -1 or ∞:</p><pre>&gt;&gt;&gt; np.all(s1.fillna(np.inf) == s2.fillna(np.inf))   # works for all dtypes
True</pre><p>Or, better yet, use a standard NumPy or Pandas comparison function:</p><pre>&gt;&gt;&gt; s = pd.Series([1., None, 3.])
&gt;&gt;&gt; np.array_equal(s.values, s.values, equal_nan=True)
True
&gt;&gt;&gt; len(s.compare(s)) == 0
True</pre><p>Here the <code>compare</code> function returns a list of differences (a DataFrame, actually), and <code>array_equal</code> returns a boolean directly.</p><p>When comparing DataFrames with mixed types, NumPy comparison fails (<a href="https://github.com/numpy/numpy/issues/19205">issue #19205</a>), while Pandas works perfectly well. Here’s what that looks like:</p><pre>&gt;&gt;&gt; df = pd.DataFrame({&#39;a&#39;: [1., None, 3.], &#39;b&#39;: [&#39;x&#39;, None, &#39;z&#39;]})
&gt;&gt;&gt; np.array_equal(df.values, df.values, equal_nan=True)
TypeError
&lt;...&gt;
&gt;&gt;&gt; len(df.compare(df)) == 0
True</pre><h3>Appends, inserts, deletions</h3><p>Although Series objects are supposed to be size-immutable, it is possible to append, insert, and delete elements in place, but all those operations are:</p><ul><li>slow, as they require reallocating memory for the whole object and updating the index.</li><li>painfully inconvenient.</li></ul><p>Here’s one way of inserting a value and two ways of deleting the values:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/288/1*JHVUVZGTwMVfEn2_i-JOsg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The second method for deleting values (via drop)is slower and can lead to intricate errors in the presence of non-unique values in the index.</p><p>Pandas has the <code>df.insert</code> method, but it can only insert columns (not rows) into a dataframe (and does not work at all with series).</p><p>Another method for appends and inserts is to slice the DataFrame with <code>iloc</code>, apply the necessary conversions, and then put it back with <code>concat</code>. I’ve implemented a function called <code>insert</code> that automates the process:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/208/1*Mwdgjsv1bSsfOYb8AuzdMQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that (just like in <code>df.insert)</code> the place to insert is given by a position <code>0&lt;=i&lt;=len(s)</code>, not the label of the element from the index. Here’s what that looks like:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/213/1*rGpCiJjw76Z7oiHOSRW7KA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To insert by the name of the element, you can combine <code>pdi.find</code> with <code>pdi.insert</code>, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/247/1*EsnEv8Hp3zUDoNwh-pV9pw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that unlike<code>df.insert</code>, <code>pdi.insert</code>returns a copy instead of modifying the Series/DataFrame in place.</p><h3>Statistics</h3><p>Pandas provides a full spectrum of statistical functions. They can give you an insight into what is in a million-element Series or DataFrame without manually scrolling through the data.</p><p>All Pandas statistical functions ignore NaNs, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/259/1*nEHVgF88PYNnhK1q0YpY8A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that Pandas <code>std</code> gives different results than NumPy <code>std</code>: which you can see in the following code:</p><pre>&gt;&gt;&gt; pd.Series([1, 2]).std()
0.7071067811865476
&gt;&gt;&gt; pd.Series([1, 2]).values.std()
0.5</pre><p>That’s because NumPy std, by default, uses N in the denominator while Pandas std uses N-1. Both <code>std</code>s have a parameter called <code>ddof</code> (‘delta degrees of freedom’) which is by default 0 in NumPy and 1 in Pandas that can bring the results to into agreement. N-1 is what you usually want (estimating deviation from a sample with an unknown mean). Here’s a Wikipedia article about <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a> for details.</p><p>Since every element in a series can be accessed by either a label or a positional index, there’s a sister function for <code>argmin</code> (<code>argmax</code>) called <code>idxmin</code> (<code>idxmax</code>), which is shown in the image:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/163/1*qpCkFcrRKj8oN9qdmLqaAw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Here’s a list of Pandas’ self-descriptive statistical functions for reference:</p><ul><li><code>std</code>, sample standard deviation</li><li><code>var</code>, unbiased variance</li><li><code>sem</code>, unbiased standard error of the mean</li><li><code>quantile</code>, sample quantile (<code>s.quantile(0.5) ≈ s.median()</code>)</li><li><a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.mode.html"><code>mode</code></a>, the value(s) that appears most often</li><li><code>nlargest</code> and <code>nsmallest</code>, by default, in order of appearance</li><li><a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.diff.html"><code>diff</code></a>, first discrete difference</li><li><code>cumsum</code> and <code>cumprod</code>, cumulative sum, and product</li><li><code>cummin</code> and <code>cummax</code>, cumulative minimum and maximum</li></ul><p>And some more specialized stat functions:</p><ul><li><a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html"><code>pct_change</code></a>, percent change between the current and previous element</li><li><code>skew</code>, unbiased skewness (third moment)</li><li><code>kurt</code> or <code>kurtosis</code>, unbiased kurtosis (fourth moment)</li><li><a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.cov.html"><code>cov</code></a><code>,</code> <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html"><code>corr</code></a> and <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.autocorr.html"><code>autocorr</code></a>, covariance, correlation, and autocorrelation</li><li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/window.html#rolling-window-functions">rolling</a>, <a href="https://pandas.pydata.org/pandas-docs/stable/reference/window.html#weighted-window-functions">weighted</a>, and <a href="https://pandas.pydata.org/pandas-docs/stable/reference/window.html#exponentially-weighted-window-functions">exponentially weighted</a> windows</li></ul><h3>Duplicate data</h3><p>Special care is taken to detect and deal with duplicate data, as you can see in the image:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/251/1*jlnjYL6OqoKzaNByBdkWnQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p><code>drop_duplicates</code> and <code>duplicated</code> can keep the last occurrence instead of the first one.</p><p>Note that <code>s.unique()</code> is <a href="https://www.slideshare.net/wesm/a-look-at-pandas-design-and-development/41">faster</a> than <code>np.unique</code> (O(N) vs O(NlogN)) and it preserves the order instead of returning the sorted results as <code>np.unique</code> does.</p><p>Missing values are treated as ordinary values, which may sometimes lead to surprising results.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/251/1*Y5O0A7fgJgUB9nMrJm7lgg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>If you want to exclude NaNs, you need to do it explicitly. In this particular example, <code>s.dropna().is_unique == True</code>.</p><p>There also is a family of monotonic functions with self-describing names:</p><ul><li><code>s.is_monotonic_increasing()</code></li><li><code>s.is_monotonic_decreasing()</code></li><li><code>s._strict_monotonic_increasing()</code></li><li><code>s._string_monotonic_decreasing()</code></li><li><code>s.is_monotonic()</code>. This is unexpected and, for some reason, is a synonym for <code>s.is_monotonic_increasing()</code>. It only returns <code>False</code> for monotonic decreasing series.</li></ul><h3>Group by</h3><p>A common operation in data processing is to calculate some statistics not over the whole bunch of data but over certain groups thereof. The first step is to define a ‘smart object’ by providing criteria for breaking a series (or a dataframe) into groups. This ‘smart object’ does not have an immediate representation, but it can be queried in just the same manner as Series to get a certain property of each group, as you can see in the following image:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/452/1*EVkkZyRXzZCCmQ-3jvM8dQ.png" width="800"/><label for="4772958373101558951">✍︎</label><span>All operations exclude NaNs</span></figure><p>In this example, we break the series into three groups based on the integer part of dividing the values by 10. For each group, we request the sum of the elements, the number of elements, and the average value in each group.</p><p>In addition to those aggregate functions, you can access particular elements based on their position or relative value within a group. Here’s what that looks like:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/460/1*mx1U5kHBwwBDeiCF-7U8zQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>You can also calculate several functions in one call with <code>g.agg([&#39;min&#39;, &#39;max&#39;])</code> or display a whole bunch of stats functions at once with <code>g.describe()</code>.</p><p>If these are not enough, you can also pass the data through your own Python function. It can either be:</p><ul><li>a function <code>f</code> that accepts a group <code>x</code> (a Series object) and generates a single value (e.g. <code>sum()</code>) with <code>g.apply(f)</code></li><li>a function <code>f</code> that accepts a group <code>x</code> (a Series object) and generates a Series object of the same size as <code>x</code> (e.g., <code>cumsum()</code>) with <code>g.transform(f)</code></li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/255/1*gnIPGT6TRq7R4DT6lhpGdg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In the examples above, the input data is sorted. This is not required for <code>groupby</code>. Actually, it works equally well if the group elements are not stored consecutively, so it is closer to <code>collections.defaultdict</code> than to <code>itertools.groupby</code>. And it always returns an index without duplicates.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/472/1*kcG8CGeTMiahim-YKt1zTQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In contrast to <code>defaultdict</code> and relational database GROUP BY clause, Pandas <code>groupby</code> sorts the results by group name. It can be disabled with <code>sort=False</code>, as you’ll see in the code:</p><p><code><a href="https://gist.github.com/7519b0b863827a291a704a8e765e2a3e">Unknown filename</a></code></p><pre><code>  Can&#39;t fetch gist.
  GitHub rate limit reached.
  Click on filename to go to gist.</code></pre><p><em>Disclaimer: Actually, </em><code>g.apply(f)</code><em> is more versatile than described above:</em></p><ul><li><em>if </em><code>f(x)</code> returns a series of the same size as <code>x</code><em>, it can mimic transform</em></li><li><em>if </em><code>f(x)</code><em> returns a series of different size or a dataframe, it results in a series with a corresponding MultIindex.</em></li></ul><p><em>But the docs warn that those usages can be slower than the corresponding </em><code>transform</code> and <code>agg</code><em> methods, so take care.</em></p><h2>Part 3. DataFrames</h2><figure><img src="https://cdn-images-1.medium.com/fit/c/800/409/1*R987f045ZqdeGRUIME_o7A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The primary data structure of Pandas is a DataFrame. It bundles a two-dimensional array with labels for its rows and columns. It consists of a number of Series objects (with a shared index), each representing a single column and possibly having different dtypes.</p><h3>Reading and writing CSV files</h3><p>A common way to construct a DataFrame is by reading a .csv (comma-separated values) file, as this image shows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/206/1*_ZYiMs0TJJmEmqCscMIwbA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The <code>pd.read_csv()</code> function is a fully-automated and insanely customizable tool. If you want to learn just one thing about Pandas, learn to use <code>read_csv</code> — it will pay off :).</p><p>Here’s an example of parsing a non-standard .csv file:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/265/1*pD_-sx-cwNNFOuJs7Rbuog.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>And a brief description of some of the arguments:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/292/1*rdINdIfLrgFqou2nR4mRYA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Since CSV does not have a strict specification, sometimes there’s a bit of trial and error to read it correctly. The cool thing about <code>read_csv</code> is that it automatically detects a lot of things:</p><ul><li>column names and types</li><li>representation of booleans</li><li>representation of missing values, etc.</li></ul><p>As with any automation, you’d better make sure it has done the right thing. If the results of simply writing <code>df</code> in a Jupyter cell happen to be too lengthy (or too incomplete), you can try the following:</p><ul><li><code>df.head(5)</code> or <code>df[:5]</code> displays the first five rows</li><li><code>df.dtypes</code> returns the column types</li><li><code>df.shape</code> returns the number of rows and columns</li><li><code>df.info()</code> summarizes all the relevant information</li></ul><p>It is a good idea to set one or several columns as an index. The following image shows this process:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/239/1*8y5tQge0RAohOcmzgIRHcQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p><code>Index </code>has many uses in Pandas:</p><ul><li>arithmetic operations are aligned by the index</li><li>it makes lookups by that column(s) faster, etc.</li></ul><p>All of that comes at the expense of somewhat higher memory consumption and a bit less obvious syntax.</p><h3>Building a DataFrame</h3><p>Another option is to construct a dataframe from data already stored in memory. Its constructor is so extraordinarily omnivorous that it can convert (or wrap!) just any kind of data you feed into it:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/337/1*uY9uygYVTsbOeZMkyvLMKA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In the first case, in the absence of row labels, Pandas labeled the rows with consecutive integers. In the second case, it did the same to both rows and columns. It is always a good idea to provide Pandas with names of columns instead of integer labels (using the <code>columns</code> argument) and sometimes rows (using the <code>index</code> argument, though <code>rows</code> might sound more intuitive). This image will help:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/388/1*ecpiKwydlLWNJiiDwDx45A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Unfortunately, it is not possible to set the name for the index column in the DataFrame constructor, so the only option is to assign it manually with, for example, <code>df.index.name = &#39;city name&#39;</code></p><p>The next option is to construct a DataFrame from a dict of NumPy vectors or a 2D NumPy array:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/357/1*fBawUWTps5MmwwJtKgbGdw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note how the <code>population</code> values got converted to floats in the second case. Actually, it happened earlier, during the construction of the NumPy array. Another thing to note here is that constructing a dataframe from a 2D NumPy array is a view by default. That means that changing values in the original array changes the dataframe and vice versa. Plus, it saves memory.</p><p>This mode can be enabled in the first case (a dict of NumPy vectors), too, by setting <code>copy=False</code>. It is very fragile, though. Simple operations can turn it into a copy without a notice.</p><p>Two more (less useful) options to create a DataFrame are:</p><ul><li>from a list of dicts (where each dict represents a single row, its keys are column names, and its values are the corresponding cell values)</li><li>from a dict of Series (where each Series represents a column; copy by default, it can be told to return a view with <code>copy=False</code>).</li></ul><p>If you register streaming data ‘on the fly,’ your best bet is to use a dict of lists or a list of lists because Python transparently preallocates space at the end of a list so that the appends are fast. Neither NumPy arrays nor Pandas dataframes do it. Another possibility (if you know the number of rows beforehand) is to manually preallocate memory with something like <code>DataFrame(np.zeros)</code>.</p><h3>Basic operations with DataFrames</h3><p>The best thing about DataFrame (in my opinion) is that you can:</p><ul><li>easily access its columns, eg <code>df.area</code> returns column values (or alternatively, <code>df[‘area’] </code>— good for column names containing spaces)</li><li>operate the columns as if they were independent variables, for example, after<code>df.population /= 10**6</code> the population is stored in millions, and the following command creates a new column called ‘density’ calculated from the values in the existing columns. See more in the following image:</li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/246/1*0PxEM1tcVdK-Cc71CgHWKA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that when creating a new column, square brackets are mandatory even if its name contains no spaces.</p><p>Moreover, you can use arithmetic operations on columns even from different DataFrames provided their rows have meaningful labels, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/243/1*LWyGU3E1SrqGYNbhRMZraQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Indexing DataFrames</h3><p>As we’ve already seen in the Series section, ordinary square brackets are simply not enough to fulfill all the indexing needs. You can’t access rows by names, can’t access disjoint rows by positional index, you can’t even reference a single cell, since <code>df[&#39;x&#39;, &#39;y&#39;]</code> is reserved for MultiIndex!</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/381/1*i2l47j_-P6qRAXEu6ClVAg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To meet those needs dataframes, just like series, have two alternative indexing modes: <code>loc</code> for indexing by labels and <code>iloc</code> for indexing by positional index.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/554/1*XHIJAm2Zej0W38bGRgwbTw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In Pandas, referencing multiple rows/columns is a copy, not a view. But it is a special kind of copy that allows assignments as a whole:</p><ul><li><code>df.loc[‘a’]=10</code> works (single row is a writable as a whole)</li><li><code>df.loc[‘a’][‘A’]=10</code> works (element access propagates to original <code>df</code>)</li><li><code>df.loc[‘a’:’b’] = 10</code> works (assigning to a subarray as a whole work)</li><li><code>df.loc[‘a’:’b’][‘A’] = 10</code> doesn’t (assigning to its elements doesn’t).</li></ul><p>In the last case, the value will only be set on a copy of a slice and will not be reflected in the original <code>df</code> (a warning will be displayed accordingly).</p><p>Depending on the background of the situation, there’re different solutions:</p><ol><li>You want to change the original <code>df</code>. Then use 
<code>df.loc[‘a&#39;:’b’, ‘A’] = 10</code></li><li>You have made a copy intentionally and want to work on that copy:
<code>df1 = df.loc[‘a’:’b’]; df1[‘A’]=10 # SettingWithCopy warning</code>
To get rid of a warning in this situation, make it a real copy:
<code>df1 = df.loc[‘a’:’b’].copy(); df1[‘A’]=10</code></li></ol><p>Pandas also supports a convenient NumPy syntax for boolean indexing.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/284/1*4slwo8GXp5wB6HyFoXbLvg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>When using several conditions, they must be parenthesized, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/266/1*wDUggm9J1vpZwS4rLlmlYg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>When you expect a single value to be returned, you need special care.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/255/1*yGIyTCHcJ2AjeFXKnn4F-Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Since there could potentially be several rows matching the condition, <code>loc</code> returned a Series. To get a scalar value out of it, you can either use:</p><ul><li><code>float(s)</code> or a more universal <code>s.item()</code> which will both raise ValueError unless there is exactly one value in the Series</li><li><code>s.iloc[0]</code> that will only raise an exception when nothing is found; also, it is the only one that supports assignments: <code>df[…].iloc[0] = 100</code>, but surely you don’t need it when you want to modify all matches: <code>df[…] = 100</code>.</li></ul><p>Alternatively, you can use string-based queries:</p><ul><li><code>df.query(&#39;name==&#34;Vienna&#34;&#39;)</code></li><li><code>df.query(&#39;population&gt;1e6 and area&lt;1000&#39;)
</code>They are shorter, work great with the MultiIndex, and logical operators have precedence over comparison operators (=less parentheses are required), but they can only filter by rows, and you can’t modify the DataFrame through them.</li></ul><p>Several third-party libraries allow you to use SQL syntax to query the DataFrames directly (duckdb) or indirectly by copying the dataframe to SQLite and wrapping the results back into Pandas objects (<a href="https://pypi.org/project/pandasql">pandasql</a>). Unsurprisingly, the direct method is <a href="https://duckdb.org/2021/05/14/sql-on-pandas.html">faster</a>.</p><h3>DataFrame arithmetic</h3><p>You can apply ordinary operations like add, subtract, multiply, divide, modulo, power, etc., to dataframes, series, and combinations thereof.</p><p>All arithmetic operations are aligned against the row and column labels:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/179/1*M3HuRKFHEi8Ao-NxcOBmHA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In mixed operations between DataFrames and Series, the Series (God knows why) behaves (and broadcasts) like a row-vector and is aligned accordingly:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/134/1*SqbcZB8UygeM8XFuZE2p5g.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Probably to keep in line with lists and 1D NumPy vectors (which are not aligned by labels and are expected to be sized as if the DataFrame was a simple 2D NumPy array):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/148/1*a7VaVbk1R3z05jCgL4R-jQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>So, in the unlucky (and, by coincidence, the most usual!) case of dividing a dataframe by a column-vector series, you have to use methods instead of the operators, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/164/1*TjSonUYkRaC7b6-BExLVIA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Because of this questionable decision, whenever you need to perform a mixed operation between a dataframe and column-like series, you have to look it up in the docs (or memorize it):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/214/1*EyEn4Hp0dSq3jRMGwisiXg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Combining DataFrames</h3><p>Pandas has three functions, <code>concat</code>, <code>merge</code>, and <code>join</code>, that are doing the same thing: combining information from several DataFrames into one. But each of them does it slightly differently, as they are tailored for different use cases.</p><h3>Vertical stacking</h3><p>This is probably the simplest way to combine two or more DataFrames into one: you take the rows from the first one and append the rows from the second one to the bottom. To make it work, those two dataframes need to have (roughly) the same columns. This is similar to <code>vstack</code> in NumPy, as you can see in the image:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/613/1*U0MaT2LtC9ObBOrlLXyL0A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Having duplicate values in the index is bad. You can run into various kinds of problems (see ‘drop’ example below). Even if you don’t care about the index, try to avoid having duplicate values in it:</p><ul><li>either use <code>reset_index=True</code> argument</li><li>call <code>df.reset_index(drop=True)</code> to reindex the rows from <code>0</code> to <code>len(df)-1,</code></li><li>use the <code>keys</code> argument to resolve the ambiguity with MultiIndex (see below).</li></ul><p>If the columns of the DataFrames do not match each other perfectly (different order does not count here), Pandas can either take the intersection of the columns (<code>kind=&#39;inner’</code>, the default) or insert NaNs to mark the missing values (<code>kind=&#39;outer&#39;</code>):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/554/1*SyW83FIGHBKgMGyNmTJ0Ug.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Horizontal stacking</h3><p><code>concat</code> can also perform ‘horizontal’ stacking (similar to <code>hstack</code> in NumPy):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/204/1*lS1W9lUTqAu3F6vjCdqf7Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p><code>join</code> is more configurable than <code>concat</code>: in particular, it has five join modes as opposed to only two of concat. See ‘1:1 relationships join’ section below for details.</p><h3>Stacking via MultiIndex</h3><p>If both row and column labels coincide, <code>concat</code> allows to do a MultiIndex equivalent of vertical stacking (like <code>dstack</code> in NumPy):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/612/1*Dwcl-kz8jUjd6u9__f8JqQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>If the row and/or the columns partially overlap, Pandas will align the names accordingly, and that’s most probably not what you want. This diagram can help you visualize this process:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/568/1*mEANaOr9HRR69IuMGn1esg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In general, if the labels overlap, it means that the DataFrames are somehow related to each other, and the relations between entities are described best using the terminology of the relational databases.</p><h3>1:1 relationship joins</h3><figure><img src="https://cdn-images-1.medium.com/fit/c/800/130/1*QIT4Vgj7_-AV3_Rc7lZwcg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>This is when the information about the same group of objects is stored in several different DataFrames, and you want to combine it into one DataFrame.</p><p>If the column you want to merge on is not in the index, use <code>merge.</code></p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/485/1*BQLUmwoXCosorw-1eWT-7w.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The first thing it does is discard anything that happens to be in the index. Then it does the join. Finally, it renumbers the results from 0 to n-1.</p><p>If the column is already in the index, you can use <code>join</code> (which is just an alias of <code>merge</code> with <code>left_index</code> or <code>right_index</code> set to <code>True</code> and different defaults).</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/491/1*qhqIn9Aw51jCwSA4wrH4Lw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>As you can see from this simplified case (see full outer join above), Pandas is pretty light-minded about the row order compared to relational databases. Left and right outer joins tend to be more predictable than inner and outer joins (at least, until there’re duplicate values in the column to be merged). So, if you want a guaranteed row order, you’ll have to sort the results explicitly.</p><h3>1:n relationship joins</h3><figure><img src="https://cdn-images-1.medium.com/fit/c/800/157/1*z3XZOrAqGy-b05cwi11AXg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>This is the most widely-used relationship in database design where one row in table A (e.g., ‘State’) can be linked to several rows of table B (e.g., City), but each row of table B can only be linked to one row of table A (= a city can only be in a single state, but a state consists of multiple cities).</p><p>Just like 1:1 relationships, to join a pair of 1:n related tables in Pandas, you have two options. If the column to be merged or is not in the index, and you’re ok with discarding anything that happens to be in the index of both tables, use <code>merge</code>. The example below will help:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/633/1*7Q-lAZNYhTRLxMEcG6wdUQ.png" width="800"/><label for="12051337279149599089">✍︎</label><span><code>merge()</code> performs inner join by default</span></figure><p>As we’ve seen already, <code>merge</code> treats row order less strictly than, say, Postgres: all of the claimed statements, the preserved key order only apply to <code>left_index=True</code> and/or <code>right_index=True</code>(that is what <code>join</code> is an alias for) and only in the absence of duplicate values in the column to be merged on. That’s why join has a <code>sort</code> argument.</p><p>Now, if you have the column to merge on already in the index of the right DataFrame, use <code>join</code> (or <code>merge</code> with <code>right_index=True</code>, which is exactly the same thing):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/665/1*lnCpYttPfSsp2KImx1PZtA.png" width="800"/><label for="9924103570546617234">✍︎</label><span>join() does left outer join by default</span></figure><p>This time Pandas kept both index values of the left DataFrame and the order of the rows intact.</p><p><em>Note: Be careful, if the second table has duplicate index values, you’ll end up with duplicate index values in the result, even if the left table index is unique!</em></p><p>Sometimes, joined DataFrames have columns with the same name. Both merge and join have a way to resolve the ambiguity, but the syntax is slightly different (also by default <code>merge</code> will resolve it with <code>&#39;_x&#39;, &#39;_y’</code> while <code>join</code> will raise an exception), as you can see in the image below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/544/1*1g_X_uOY9iKCACMXAPK4TA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To summarize:</p><ul><li><code>merge</code> joins on non-index columns, <code>join</code> requires column to be indexed</li><li><code>merge</code> discards the index of the left DataFrame, <code>join</code> keeps it</li><li>By default,<code>merge</code> performs an inner join, <code>join</code> does left outer join</li><li><code>merge</code> does not keep the order of the rows</li><li><code>join</code> keeps them (some restrictions apply)</li><li><code>join</code> is an alias for <code>merge</code> with <code>left_index=True</code> and/or <code>right_index=True</code></li></ul><h3>Multiple joins</h3><p>As discussed above, when <code>join</code> is run against two dataframes like <code>df.join(df1)</code> it acts as an alias to merge. But <code>join</code> also has a ‘multiple join’ mode, which is just an alias for <code>concat(axis=1)</code>.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/184/1*73WqDpx0NuSHgNHZnaNLLQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>This mode is somewhat limited compared to the regular mode:</p><ul><li>it does not provide means for duplicate column resolution</li><li>it only works for 1:1 relationships (index-to-index joins).</li></ul><p>So multiple 1:n relationships are supposed to be joined one by one. The repo ‘pandas-illustrated’ has a helper for that, too, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/552/1*UtClV-xBGBqcsfw5CMYuPQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p><code>pdi.join</code> is a simple wrapper over <code>join</code> that accepts lists in <code>on</code>, <code>how</code> and <code>suffixes</code> arguments so that you could make several joins in one command. Just like with the original join, <code>on</code> columns pertain to the first DataFrame, and other DataFrames are joined against their indices.</p><h3>Inserts and deletes</h3><p>Since DataFrame is a collection of columns, it is easier to apply these operations to the rows than to the columns. For example, inserting a column is always done in-place, while inserting a row always results in a new DataFrame, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/417/1*7XRaXsl0ytSI8IPDXauUfg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Deleting columns is usually worry-free, except that <code>del df[&#39;D&#39;]</code> works while <code>del df.D</code> doesn’t (limitation on Python level).</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/421/1*46V9fMGZv4UrCT3gW2-m7Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Deleting rows with <code>drop</code> is surprisingly slow and can lead to intricate bugs if the raw labels are not unique. The image below will help explain the concept:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/419/1*MVrPZ1KPLkM_UURVnOR3qg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>One solution would be to use <code>ignore_index=True</code> that tells <code>concat</code> to reset the row names after concatenation:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/453/1*is1aiSIZhBxVrRsvxHpyjA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In this case, setting the <code>name</code> column as an index would help. But for more complicated filters, it wouldn’t.</p><p>Yet another solution that is fast, universal, and even works with duplicate row names is indexing instead of deletion. I’ve written a (one-line-long) automation to avoid explicitly negating the condition.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/477/1*Ixq7P9K_4NNB_9Khgy7aOg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Group by</h3><p>This operation has already been described in detail in the Series section. But DataFrame’s <code>groupby</code> has a couple of specific tricks on top of that.</p><p>First, you can specify the column to group by using just a name, as the image below shows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/391/1*J7n3A7uLJ55L0CFGHF2EhA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Without <code>as_index=False</code>, Pandas assigns the column by which the grouping was performed to be the index. If this is not desirable, you can <code>reset_index()</code> or specify <code>as_index=False</code>.</p><p>Usually, there’re more columns in the DataFrame than you want to see in the result. By default, Pandas sums anything remotely summable, so you’ll have to narrow your choice, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/492/1*31VOSYVKPQhYWc7cKQYSYw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that when summing over a single column, you’ll get a Series instead of a DataFrame. If, for some reason, you want a DataFrame, you can:</p><ul><li>use double brackets: <code>df.groupby(&#39;product&#39;)[[&#39;quantity&#39;]].sum()</code></li><li>convert explicitly: <code>df.groupby(&#39;product&#39;)[&#39;quantity&#39;].sum().to_frame()</code></li></ul><p>Switching to numeric index will also make a DataFrame out of it:</p><ul><li><code>df.groupby(&#39;product&#39;, as_index=False)[&#39;quantity&#39;].sum()</code></li><li><code>df.groupby(&#39;product&#39;)[&#39;quantity&#39;].sum().reset_index()</code></li></ul><p>But despite the unusual appearance, a Series behaves just like DataFrames, so maybe a ‘facelift’ of <code>pdi.patch_series_repr()</code> would be enough.</p><p>Obviously, different columns behave differently when grouping. For example, it is perfectly fine to sum over quantity, but it makes no sense to sum over price. Using <code>.agg </code>allows you to specify different aggregate functions for different columns, as the image shows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/393/1*Lbqy_M1eTJWuoarH814Qyg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Or, you can create several aggregate functions for a single column:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/226/1*y8B1RKcGnpGMku52YxWEZg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Or, to avoid the cumbersome column renaming, you can do the following:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/264/1*ZKc4wbcdZzjFtJiQym8FMA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Sometimes, the predefined functions are not good enough to produce the required results. For example, it would be better to use weights when averaging the price. So you can provide a custom function for that. In contrast with Series, the function can access multiple columns of the group (it is fed with a sub-dataframe as an argument), as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/395/1*9BApz4H9F3r_a7AMX0XFxg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Unfortunately, you can’t combine predefined aggregates with several-column-wide custom functions, such as the one above, in one command as <code>agg</code> only accepts one-column-wide user functions. The only thing that one-column-wide user functions can access is the index, which can be handy in certain scenarios. For example, that day, bananas were sold at a 50% discount, which can be seen below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/379/1*yV4URRKqT4T7ubMPundwgA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To access the value of the group by column from the custom function, it was included in the index beforehand.</p><p>As usual, the least customized function yields the best performance. So in order of increasing speed:</p><ul><li>multi-column-wide custom function via <code>g.apply()</code></li><li>single-column-wide custom function via <code>g.agg()</code> (supports acceleration with Cython or Numba)</li><li>predefined functions (Pandas or NumPy function object, or its name as a string).</li><li>predefined functions (Pandas or NumPy function object, or its name as a string).</li></ul><p>A useful instrument for looking at the data from a different perspective often used together with grouping is pivot tables.</p><h3>Pivoting and ‘unpivoting’</h3><p>Suppose you have a variable <code>a</code> that depends on two parameters <code>i</code> and <code>j</code>. There’re two equivalent ways to represent it as a table:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/308/1*-p6J9VTQkuts8yOZ9Um9bQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The ‘short’ format is more appropriate when the data is ‘dense’ (when there’re few zero elements), and the ‘long’ is better when the data is ‘sparse’ (most of the elements are zeros and can be omitted from the table). The situation gets more contrived when there’re more than two parameters.</p><p>Naturally, there should be a simple way to transform between those formats. And Pandas provides a simple and convenient solution for it: pivot table.</p><p>As a less abstract example, consider the following table with the sales data. Two clients have bought the designated quantity of two kinds of products. Originally, this data is in the ‘short format.’ To convert it to the ‘long format’, use <code>df.pivot</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/508/1*7Mb4uIBjYnoJbKp82ShFTg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>This command discards anything unrelated to the operation (index, price) and transforms the information from the three requested columns into the long format, placing client names into the result’s index, product titles into the columns, and quantity sold into the ‘body’ of the DataFrame.</p><p>As for the reverse operation, you can use <code>stack</code>. It merges <code>index</code> and columns into the MultiIndex:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/454/1*xQz0EDWtH6iVN49bdN2W4Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Another option is to use <code>melt</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/410/1*xZ0czY-b7E3jP_arB8kTtg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Note that <code>melt</code> orders the rows of the result in a different manner.</p><p><code>pivot</code> loses the information about the name of the ‘body’ of the result, so with both <code>stack</code> and <code>melt</code> we have to remind pandas about the name of the ‘quantity’ column.</p><p>In the example above, all the values were present, but it is not a must:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/436/1*sH3hFQPha9CTC6jzErCRWg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The practice of grouping values and then pivoting the results is so common that <code>groupby</code> and <code>pivot</code> have been bundled together into a dedicated function (and a corresponding DataFrame method) <code>pivot_table</code>:</p><ul><li>without the <code>columns</code> argument, it behaves similarly to <code>groupby</code></li><li>when there’re no duplicate rows to group by, it works just like <code>pivot</code></li><li>otherwise, it does grouping and pivoting</li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/483/1*WSqts0a_z3TvV10bedyJtw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The <code>aggfunc</code> parameter controls which aggregate function should be used for grouping the rows (<code>mean</code> by default).</p><p>As a convenience, <code>pivot_table</code> can calculate the subtotals and grand total:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/213/1*v-fTmbMwNP87-I6kbLLlcQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Once created, pivot table becomes just an ordinary DataFrame, so it can be queried using the standard methods described earlier.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/381/1*H3BBj4ddsWAHI5PEbWfCog.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The best way to get a grasp on <code>pivot_table</code> (except to start using it right away!) is to follow a relevant case study. I can highly recommend two of them:</p><ul><li>an extremely thorough sales case is described in <a href="https://pbpython.com/pandas-pivot-table-explained.html">this blog post</a>⁵</li><li>a very well-written generic use case (based on the infamous Titanic dataset) can be found <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html">here</a>⁶</li></ul><p>Pivot tables are especially handy when used with MultiIndex. We’ve seen lots of examples where Pandas functions return a multi-indexed DataFrame. Let’s have a closer look at it.</p><h2>Part 4. MultiIndex</h2><figure><img src="https://cdn-images-1.medium.com/fit/c/800/373/1*45q7aXTI20IgTChQ3eX1ug.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The most straightforward usage of MultiIndex for people who have never heard of Pandas is using a second index column as a supplement for the first one to identify each row uniquely. For example, to disambiguate cities from different states, the state’s name is often appended to the city’s name. For example, there’re about 40 Springfields in the US (in relational databases, it is called a composite primary key).</p><p>You can either specify the columns to be included in the index after the DataFrame is parsed from CSV or right away as an argument to <code>read_csv</code>.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/262/1*teUMlqeWPRSq1HkdnmVxvQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>You can also append existing levels to the MultiIndex afterward using <code>append=True</code>, as you can see in the image below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/267/1*ic36AKza_tVnYdmfmpeDWw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Another use case, more typical in Pandas, is representing multiple dimensions. When you have a number of objects with a certain set of properties or evolution in time of one object of the kind. For example:</p><ul><li>results of a sociological survey</li><li>the ‘Titanic’ dataset</li><li>historical weather observations</li><li>chronology of championship standings.</li></ul><p>This is also known as ‘<a href="https://en.wikipedia.org/wiki/Panel_data">Panel data</a>,’ and Pandas owes its name to it.</p><p>Let’s add such a dimension:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/342/1*gqkEMVwQHEE2FvychjOeHA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Now we have a four-dimensional space, where the following is shown:</p><ul><li>years form one (almost continuous) dimension</li><li>city names are placed along the second</li><li>state names along the third</li><li>particular city properties (‘population,’ ‘density,’ ‘area,’ etc.) act as ‘tick marks’ along the fourth dimension.</li></ul><p>The following diagram illustrates the concept:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/306/1*J3XT0huu3TrkHxyiNEuvVQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To allow space for the names of the dimensions corresponding to columns, Pandas shifts the whole header upward:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/390/1*oRPog3_NlzAjw2aDzrwERQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Grouping</h3><p>The first thing to note about MultiIndex is that it does not group anything as it might appear. Internally it is just a flat sequence of labels, as you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/172/1*okIgqhhtarSucYOQqJO2Fw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>You can get the same <code>groupby</code> effect for row labels by just sorting them:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/335/1*iorN0lg82DYew_lmTgAKRA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>And you can even disable the visual grouping entirely by setting a corresponding Pandas <a href="https://pandas.pydata.org/docs/user_guide/options.html">option</a>: <code>pd.options.display.multi_sparse=False</code>.</p><h3>Type conversions</h3><p>Pandas (as well as Python itself) makes a difference between numbers and strings, so it is usually a good idea to convert numbers to strings in case the datatype was not detected automatically:</p><pre>pdi.set_level(df.columns, 0, pdi.get_level(df.columns, 0).astype(&#39;int&#39;))</pre><p>If you’re feeling adventurous, you can do the same with standard tools:</p><pre>df.columns = df.columns.set_levels(df.columns.levels[0].astype(int), level=0)</pre><p>But to use them properly, you need to understand what ‘levels’ and ‘codes’ are, whereas <code>pdi</code> allows you to work with MultiIndex as if the levels were ordinary lists or NumPy arrays.</p><p>If you <em>really</em> wonder, ‘levels’ and ‘codes’ are something that a regular list of labels from a certain level are broken into to speed up operations like <code>pivot</code>, <code>join</code> and so on:</p><ul><li><code>pdi.get_level(df, 0) == Int64Index([2010, 2010, 2020, 2020])</code></li><li><code>df.columns.levels[0] == Int64Index([2010, 2020])</code></li><li><code>df.columns.codes[0] == Int64Index([0, 1, 0, 1])</code></li></ul><h3>Building a DataFrame with a MultiIndex</h3><p>In addition to reading from CSV files and building from the existing columns, there’re some more methods to create a MultiIndex. They are less commonly used — mostly for testing and debugging.</p><p>The most intuitive way of using the Panda’s own representation of MultiIndex does not work for historical reasons.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/163/1*_nu53FDMCMYQuUu-CofZxw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>‘Levels’ and ‘codes’ here are (nowadays) considered implementation details that should not be exposed to end user, but we have what we have.</p><p>Probably, the simplest way of building a MultiIndex is the following:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/587/1*-aMHbiSWHEw_l42_9UBqQA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The downside here is that the names of the levels have to be assigned in a separate line. Several alternative constructors bundle the names along with the labels.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/507/1*wyKNE3HFbCK3_kU0SXslCg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>When the levels form a regular structure, you can specify the key elements and let Pandas interleave them automatically, as shown below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/347/1*ai1q9XWa8VIH9-2FpsPE0Q.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>All the methods listed above apply to columns, too. For example:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/231/1*J5nqOSlYjPVKbY2SrLhLiA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Indexing with MultiIndex</h3><p>The good thing about accessing DataFrame via the MultiIndex is that you can easily reference all levels at once (potentially omitting the inner levels) with a nice and familiar syntax.</p><p>Columns — via regular square brackets</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/342/1*tKb59QkAt7A3H6L-roS2fw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Rows and cells — using <code>.loc[]</code></p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/324/1*g9FL6iBvY60b4PoGzC1FDQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Now, what if you want to select all cities in Oregon or leave only the columns with population? Python syntax imposes two limitations here:</p><p>1. There’s no way of telling between <code>df[&#39;a&#39;, &#39;b&#39;] and df[(&#39;a&#39;, &#39;b&#39;)]</code> — it is processed the same way, so you can’t just write <code>df[:, ‘Oregon’]</code>. Otherwise, Pandas would never know if you mean Oregon the column or Oregon the second level of rows</p><p>2. Python only allows colons inside square brackets, not inside parentheses, so you can’t write <code>df.loc[(:, &#39;Oregon&#39;), :]</code></p><p>On the technical side, it is not difficult to arrange. I’ve monkey-patched the DataFrame to add such functionality, which you can see here:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/621/1*M_7hv8J_6XTuEDhQkfW6IA.png" width="800"/><label for="12746080249951726190">✍︎</label><span>Warning! Not a valid Pandas syntax! Only works after pdi.patch_mi_co()</span></figure><p>The only downside of this syntax is that when you use both indexers, it returns a copy, so you can’t write <code>df.mi[:,’Oregon’].co[‘population’] = 10</code>. There’s many alternative indexers, some of which allow such assignments, but all of them have their own quirks:</p><p>1. You can swap inner layers with outer layers and use the brackets.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/268/1*Wl-J7D47UNMf2o6NxUIsVg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>So, <code>df[:, ‘population’]</code> can be implemented with 
<code>df.swaplevel(axis=1)[&#39;population&#39;]</code></p><p>This feels hacky and is not convenient for more than two levels.</p><p>2. You can use the <code>xs</code> method: 
<code>df.xs(‘population’, level=1, axis=1)</code>.</p><p>It does not feel Pythonic enough, especially when selecting multiple levels.
This method is unable to filter both rows and columns at the same time, so the reasoning behind the name <code>xs</code> (stands for “cross-section”) is not entirely clear. It cannot be used for setting values.</p><p>3. You can create an alias for <code>pd.IndexSlice</code> and use it inside <code>.loc</code>: 
<code>idx=pd.IndexSlice; df.loc[:, idx[:, ‘population’]]</code></p><p>That’s more Pythonic, but the necessity of aliasing something to access an element is somewhat of a burden (and it is too long without an alias). You can select rows and columns at the same time. Writable.</p><p>4. You can learn how to use <code>slice</code> instead of a colon. If you know that <code>a[3:10:2] == a[slice(3,10,2)]</code> then you might understand the following, too: <code>df.loc[:, (slice(None), ‘population’)</code>], but it is barely readable anyway. You can select rows and columns at the same time. Writable.</p><p>As a bottom line, Pandas has a number of ways to access elements of the DataFrame with MultiIndex using brackets, but none of them is convenient enough, so they had to adopt an alternative indexing syntax:</p><p>5. A mini-language for the <code>.query</code> method: 
<code>df.query(‘state==&#34;Oregon&#34; or city==&#34;Portland&#34;’)</code>.</p><p>It is convenient and fast, but lacks support from IDE (no autocompletion, no syntax highlighting, etc.), and it only filters the rows, not the columns. That means you can’t implement <code>df[:, ‘population’]</code> with it, without transposing the DataFrame (which will lose the types unless all the columns are of the same type). Non-writable.</p><h3>Stacking and unstacking</h3><p>Pandas does not have <code>set_index</code> for columns. A common way of adding levels to columns is to ‘unstack’ existing levels from the index:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/252/1*NW-d8ofudJ6X25xh94G-HA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Pandas’ <code>stack</code> is very different from NumPy’s <code>stack</code>. Let’s see what the documentation says about the naming conventions:</p><blockquote><p>“The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe).”</p></blockquote><p>The ‘on top’ part does not sound really convincing to me, but at least this explanation helps memorize which one moves things which way. By the way, Series has <code>unstack</code>, but does not have <code>stack</code> because it is ‘stacked already.’ Being one-dimensional, Series can act as either row-vector or column-vector in different situations but are normally thought of as column vectors (e.g., dataframe columns).</p><p>For example:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/636/1*JvFeUTdhBNLALNbdmasExA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>You can also specify which level to stack/unstack by name or by positional index. In this example, <code>df.stack()</code>, <code>df.stack(1)</code> and <code>df.stack(‘year’)</code> produce the same result, as well as <code>df1.unstack()</code>, <code>df1.unstack(2)</code>, and <code>df1.unstack(‘year’)</code>. The destination is always ‘after the last level’ and is not configurable. If you need to put the level somewhere else, you can use <code>df.swaplevel().sort_index()</code> or <code>pdi.swap_level(df, sort=True)</code></p><p>The columns must not contain duplicate values to be eligible for stacking (same applies to index when unstacking):</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/150/1*E3lyWZTI3DPErpk6ntlgvQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>How to prevent stack/unstack from sorting</h3><p>Both stack and unstack have a bad habit of unpredictably sorting the result’s index lexicographically. It might be irritating at times, but it is the only way to give predictable results when there’re a lot of missing values.</p><p>Consider the following example. In which order would you expect days of the week to appear in the right table?</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/256/1*ju5Y1fsP2FASdhEU8I4XaA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>You could speculate that if John’s Monday stands to the left of John’s Friday, then <code>‘Mon’ &lt; ‘Fri’</code>, and similarly, <code>‘Fri’ &lt; ‘Sun’</code> for Silvia, so the result should be <code>‘Mon’ &lt; ‘Fri’ &lt; ‘Sun’</code>. This is legitimate, but what if the remaining columns are in a different order, say, <code>‘Mon’ &lt; ‘Fri’</code> and <code>‘Tue’ &lt; ‘Fri</code>’? Or <code>‘Mon’ &lt; ‘Fri’</code> and <code>‘Wed’ &lt; ‘Sat’</code>?</p><p>OK, there’re not so many days of the week out there, and Pandas could deduce the order based on prior knowledge. But mankind has not arrived at a decisive conclusion on whether Sunday should stand at the end of the week or the beginning. Which order should Pandas use by default? Read regional settings? And what about less trivial sequences, say, order of the States in the US?</p><p>What Pandas does in this situation is simply sort it alphabetically, which you can see below:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/296/1*qVVD2T35g56KKlsmOyVijA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>While this is a sensible default, it still feels wrong. There should be a solution! And there is one. It is called <code>CategoricalIndex</code>. It remembers the order even if some labels are missing. It has recently been smoothly integrated into Pandas toolchain. The only thing it misses is infrastructure. It is difficult to build; it is fragile (falls back to object in certain operations), yet it is perfectly usable, and the pdi library has some helpers to steep the learning curve.</p><p>For example, to tell Pandas to lock the order of say, simple Index holding the products (which will inevitably get sorted if you decide to unstack days of the week back to columns), you need to write something as horrendous as <code>df.index = pd.CategoricalIndex(df.index, df.index, sorted=True)</code>. And it is much more contrived for MultiIndex.</p><p>The pdi library has a helper function <code>locked</code> (and an alias <code>lock</code> having <code>inplace=True</code> by default) for locking the order of a certain MultiIndex level by promoting the level to the <code>CategoricalIndex</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/512/1*VSUGGmuQbXlF_0hZCkoy2g.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>The checkmark <strong>✓ </strong>next to a level name means the level is locked. It can be visualized manually with <code>pdi.vis(df)</code> or automatically by monkey-patching DataFrame HTML output with <code>pdi.vis_patch()</code>. After applying the patch, simply writing ‘df’ in a Jupyter cell will show checkmarks for all levels with locked ordering.</p><p><code>lock</code> and <code>locked</code> work automatically in simple cases (such as client names) but needs a hint from the user for the more complex cases (such as days of the week with missing days).</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/577/1*cKztuIUlRag4bl5LwWWm0w.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>After the level has been switched to <code>CategoricalIndex</code>, it keeps the original order in operations like <code>sort_index</code>, <code>stack</code>, <code>unstack</code>, <code>pivot</code>, <code>pivot_table</code>, etc.</p><p>It is fragile, though. Even such an innocent operation as adding a column via <code>df[‘new_col’] = 1</code> breaks it. Use <code>pdi.insert(df.columns, 0, ‘new_col’, 1)</code> which processes level(s) with <code>CategoricalIndex</code> correctly.</p><h3>Manipulating levels</h3><p>In addition to the already mentioned methods, there are some more:</p><ul><li><code>pdi.get_level(obj, level_id)</code> returns a particular level referenced either by number or by name, works with DataFrames, Series, and MultiIndex</li><li><code>pdi.set_level(obj, level_id, labels)</code>replaces the labels of a level with the given array (list, NumPy array, Series, Index, etc.)</li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/396/1*3a1uejTEyrF_iDnOrVgK4w.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><ul><li><code>pdi.insert_level(obj, pos, labels, name)</code> adds a level with the given values (properly broadcasted if necessary)</li><li><code>pdi.drop_level(obj, level_id)</code>that removes the specified level from the MultiIndex</li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/235/1*t2gs_E5jcQyK4AM7ggXmRQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><ul><li><code>pdi.swap_levels(obj, src=-2, dst=-1)</code>swaps two levels (two innermost levels by default)</li><li><code>pdi.move_level(obj, src, dst)</code>moves a particular level <code>src</code> to the designated position <code>dst</code></li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/411/1*p7Wp-p0t57yXO0TGZeD19g.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>In addition to the arguments mentioned above, all functions from this section have the following arguments:</p><ul><li><code>axis=None</code> where None means ‘columns’ for a DataFrame and ‘index’ for a Series</li><li><code>sort=False</code>, optionally sorts the corresponding MultiIndex after the manipulations</li><li><code>inplace=False</code>, optionally performs the manipulation in-place (does not work with a single <code>Index</code> because it is immutable).</li></ul><p>All the operations above understand the word level in the conventional sense (level has the same number of labels as the number of columns in the DataFrame), hiding the machinery of <code>index.label</code> and <code>index.codes</code> from the end user.</p><p>On the rare occasions when moving and swapping separate levels is not enough, you can reorder all the levels at once with this pure Pandas call:
<code>df.columns = df.columns.reorder_levels([‘M’,’L’,’K’])</code>
where [‘M’, ‘L’, ‘K’] is the desired order of the levels.</p><p>Generally, it is enough to use <code>get_level</code> and <code>set_level</code> to the necessary fixes to the labels, but if you want to apply a transformation to all levels of the MultiIndex at once, Pandas has an (ambiguously named) function <code>rename</code> accepting a dict or a function:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/256/1*TKYdBkjI82RbWUADZL6N4A.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>As for renaming the levels, their names are stored in the field <code>.names</code>. This field does not support direct assignments (why not?): 
<code>df.index.names[1] = ‘x’ # TypeError 
</code>but can be replaced as a whole:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/162/1*MdvIh5Mjh2B-BUdAEDJ07w.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>When you just need to rename a particular level, the syntax is as follows:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/163/1*GpwjAeIlu33FW32d49deTg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><h3>Converting MultiIndex into a flat Index and restoring it back</h3><p>As we’ve seen from above, the convenient query method only solves the complexity of dealing with MultiIndex in the rows. And despite all the helper functions, when some Pandas function returns a MultiIndex in the columns, it has a shock effect for beginners. So, the pdi library has the following:</p><ul><li><code>join_levels(obj, sep=’_’, name=None)</code> that joins all MultiIndex levels into one Index</li><li><code>split_level(obj, sep=’_’, names=None)</code> that splits the Index back into a MultiIndex</li></ul><figure><img src="https://cdn-images-1.medium.com/fit/c/800/200/1*NPKzUS-XT4QyVwcC6BQQKQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Both have optional <code>axis</code> and <code>inplace</code> arguments.</p><h3>Sorting MultiIndex</h3><p>Since MultiIndex consists of several levels, sorting is a bit more contrived than for a single Index. It can still be done with the <code>sort_index</code> method, but it could be further fine-tuned with the following arguments:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/129/1*j5eo7Fdnqp3Kqke6x-BOmg.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>To sort column levels, specify <code>axis=1</code>.</p><h3>Reading and writing MultiIndexed DataFrames to disk</h3><p>Pandas can write a DataFrame with a MultiIndex into a CSV file in a fully automated manner: <code>df.to_csv(&#39;df.csv’)</code>. But when reading such a file Pandas cannot parse the MultiIndex automatically and needs some hints from the user. For example, to read a DataFrame with three-level-high columns and four-level-wide index, you need to specify 
<code>pd.read_csv(&#39;df.csv&#39;, header=[0,1,2], index_col=[0,1,2,3])</code>.</p><p>This means that the first three lines contain the information about the columns, and the first four fields in each of the subsequent lines contain the index levels (if there’s more than one level in columns, you can’t reference row levels by names anymore, only by numbers).</p><p>It is not convenient to manually decipher the number of levels in the MultiIndexes, so a better idea would be to <code>stack()</code> all the columns header levels but one before saving the DataFrame to CSV, and <code>unstack()</code> them back after reading.</p><p>If you need a fire-and-forget solution, you might want to look into the binary formats, such as Python pickle format:</p><ul><li>directly: <code>df.to_pickle(&#39;df.pkl&#39;), pd.read_pickle(&#39;df.pkl&#39;)</code></li><li>using the <a href="https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html">storemagic</a> in Jupyter <code>%store df</code> then <code>%store -r df
</code>(stores in <code>$HOME/.ipython/profile_default/db/autorestore</code>)</li></ul><p>Python pickle is small and fast, but it is only accessible from Python. If you need interoperability with other ecosystems, look into more standard formats such as Excel format (requires the same hints as <code>read_csv</code> when reading MultiIndex). Here’s the code:</p><pre>!pip install openpyxl
df.to_excel(&#39;df.xlsx&#39;)
df1 = pd.read_excel(&#39;df.xlsx&#39;, header=[0,1,2], index_col=[0,1,2,3])</pre><p>The <a href="https://en.wikipedia.org/wiki/Apache_Parquet">Parquet</a> file format supports MultiIndexed dataframes with no hints whatsoever, produces smaller files, and works faster (see a <a href="https://pythonspeed.com/articles/pandas-read-csv-fast/">benchmark</a>⁷):</p><pre>df.to_parquet(&#39;df.parquet&#39;)
df1 = pd.read_parquet(&#39;df.parquet&#39;)</pre><p>The official <a href="https://pandas.pydata.org/docs/user_guide/io.html">docs</a> has a table listing all ~20 supported formats.</p><h3>MultiIndex arithmetic</h3><p>When working with multiIndexed dataframesthe same rules as for the ordinary dataframes apply (see above). But dealing with a subset of cells has some peculiarities of its own.</p><p>You can update a subset of columns referenced via the outer MultiIndex level as simple as the following:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/569/1*NkBQ1_-m-wLdL6dGt5i_Kw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Or if you want to keep the original data intact, 
<code>df1 = df.assign(population=df.population*10)</code>.</p><p>You can also easily get the population density with <code>density=df.population/df.area</code>.</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/492/1*jR2KPG47ITs4EkUtJSZuRw.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>But unfortunately, you can’t assign the result to the original dataframe with <code>df.assign</code>.</p><p>One approach is to stack all the irrelevant levels of the column index into the rows index, perform the necessary calculations, and unstack them back (use <code>pdi.lock</code> to keep the original order of columns).</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/412/1*NQECH1R5DAPQcT7TxsLOKA.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p>Alternatively, you can use <code>pdi.assign</code>:</p><figure><img src="https://cdn-images-1.medium.com/fit/c/800/530/1*rfRew8_erkcfleuaoE4DJQ.png" width="800"/><label for="15257638572835777853">✍︎</label><span></span></figure><p><code>pdi.assign</code> is locked-order-aware, so if you feed it a dataframe with locked level(s), it won’t unlock them or the subsequent stack/unstack/etc. Operations will keep the original columns and rows in order.</p><p>All in all, Pandas is a great tool for analysing and processing data. Hopefully this article helped you understand both ‘hows’ and ‘whys’ of solving typical problems, and to appreciate the true value and beauty of the Pandas library.</p><p>Drop me a line (on <a href="https://www.reddit.com/r/Python/comments/10mezt9/pandas_illustrated_the_definitive_visual_guide_to/">reddit</a>, <a href="https://news.ycombinator.com/item?id=34543927">hackernews</a>, <a href="https://www.linkedin.com/feed/update/urn:li:activity:7024693878247038976?utm_source=share&amp;utm_medium=member_desktop">linkedin</a> or <a href="https://twitter.com/LevMaximov/status/1618881785357811712">twitter</a>) if I missed your favorite feature, overlooked a blatant typo, or just if this article proved to be helpful for you!</p><h2>References</h2><ol><li>Pivot — Rows to Columns, Modern SQL blog<a href="https://modern-sql.com/use-case/pivot">
https://modern-sql.com/use-case/pivot</a></li><li>Create a PivotTable to analyze worksheet data, Microsoft Excel Help
<a href="https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576">https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576</a></li><li>Create and use pivot tables, Google Sheets docs
<a href="https://support.google.com/docs/answer/1272900?hl=en&amp;co=GENIE.Platform%3DDesktop">https://support.google.com/docs/answer/1272900</a></li><li>Wes McKinney, A look at Pandas design and development, NYC Python meetup, 2012
<a href="https://www.slideshare.net/wesm/a-look-at-pandas-design-and-development/41">https://www.slideshare.net/wesm/a-look-at-pandas-design-and-development/41</a></li><li>‘Pandas Pivot Table Explained’ article by Chris Moffitt in ‘Practical Business Python’ blog.
<a href="https://pbpython.com/pandas-pivot-table-explained.html">https://pbpython.com/pandas-pivot-table-explained.html</a></li><li>‘Pivot tables’ chapter in ‘Python Data Science Handbook’ by Jake VanderPlas.
<a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html">https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html</a></li><li>The fastest way to read a csv in Pandas by Itamar Turner-Trauring
<a href="https://pythonspeed.com/articles/pandas-read-csv-fast/">https://pythonspeed.com/articles/pandas-read-csv-fast/</a></li></ol><h2>License</h2><p>All rights reserved (=you cannot distribute, alter, translate, etc. without author’s permission).</p></section></article></div>
  </body>
</html>
