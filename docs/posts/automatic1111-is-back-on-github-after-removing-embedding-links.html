<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion">Original</a>
    <h1>Automatic1111 is back on GitHub after removing Embedding Links</h1>
    
    <div id="readability-page-1" class="page"><div>
                
<p>Textual Inversion allows you to train a tiny part of the neural network on your own pictures, and use results when generating new ones. In this context, embedding is the name of the tiny bit of the neural network you trained.</p>
<p>The result of the training is a .pt or a .bin file (former is the format used by original author, latter is by the diffusers library) with the embedding in it.</p>
<p>See original site for more details about what textual inversion is: <a href="https://textual-inversion.github.io/" rel="nofollow">https://textual-inversion.github.io/</a>.</p>

<p>Put the embedding into the <code>embeddings</code> directory and use its filename in the prompt. You don&#39;t have to restart the program for this to work.</p>
<p>As an example, here is an embedding of <a href="https://drive.google.com/file/d/1MDSmzSbzkIcw5_aw_i79xfO3CRWQDl-8/view?usp=sharing" rel="nofollow">Usada Pekora</a> I trained on WD1.2 model, on 53 pictures (119 augmented) for 19500 steps, with 8 vectors per token setting.</p>
<p>Pictures it generates:
<img src="https://user-images.githubusercontent.com/20920490/193285043-5d5d57d8-7b5e-4803-a211-5ca5220c35f4.png" alt="grid-0037"/></p>
<div data-snippet-clipboard-copy-content="portrait of usada pekora
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b"><pre><code>portrait of usada pekora
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b
</code></pre></div>
<p>You can combine multiple embeddings in one prompt:
<img src="https://user-images.githubusercontent.com/20920490/193285265-a5224378-4ae2-48bf-ad7d-e79a9f998f9c.png" alt="grid-0038"/></p>
<div data-snippet-clipboard-copy-content="portrait of usada pekora, mignon
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b"><pre><code>portrait of usada pekora, mignon
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b
</code></pre></div>
<p>Be very careful about which model you are using with your embeddings: they work well with the model you used during training, and not so well on different models. For example, here is the above embedding and vanilla 1.4 stable diffusion model:
<img src="https://user-images.githubusercontent.com/20920490/193285611-486373f2-35d0-437c-895a-71454564a7c4.png" alt="grid-0036"/></p>
<div data-snippet-clipboard-copy-content="portrait of usada pekora
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 7460a6fa"><pre><code>portrait of usada pekora
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 7460a6fa
</code></pre></div>

<h2><a id="user-content-textual-inversion-tab" aria-hidden="true" href="#textual-inversion-tab"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Textual inversion tab</h2>
<p>Experimental support for training embeddings in user interface.</p>
<ul>
<li>create a new empty embedding, select directory with images, train the embedding on it</li>
<li>the feature is very raw, use at own risk</li>
<li>i was able to reproduce results I got with other repos in training anime artists as styles, after few tens of thousands steps</li>
<li>works with half precision floats, but needs experimentation to see if results will be just as good</li>
<li>if you have enough memory, safer to run with <code>--no-half --precision full</code>
</li>
<li>Section for UI to run preprocessing for images automatically.</li>
<li>you can interrupt and resume training without any loss of data (except for AdamW optimization parameters, but it seems none of existing repos save those anyway so the general opinion is they are not important)</li>
<li>no support for batch sizes or gradient accumulation</li>
<li>it should not be possible to run this with <code>--lowvram</code> and <code>--medvram</code> flags.</li>
</ul>
<h2><a id="user-content-explanation-for-parameters" aria-hidden="true" href="#explanation-for-parameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Explanation for parameters</h2>
<h3><a id="user-content-creating-an-embedding" aria-hidden="true" href="#creating-an-embedding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Creating an embedding</h3>
<ul>
<li>
<strong>Name</strong>: filename for the created embedding. You will also use this text in prompts when referring to the embedding.</li>
<li>
<strong>Initialization text</strong>: the embedding you create will initially be filled with vectors of this text. If you create a one vector embedding named &#34;zzzz1234&#34; with &#34;tree&#34; as initialization text, and use it in prompt without training, then prompt &#34;a zzzz1234 by monet&#34; will produce same pictures as &#34;a tree by monet&#34;.</li>
<li>
<strong>Number of vectors per token</strong>: the size of embedding. The larger this value, the more information about subject you can fit into the embedding, but also the more words it will take away from your prompt allowance. With stable diffusion, you have a limit of 75 tokens in the prompt. If you use an embedding with 16 vectors in a prompt, that will leave you with space for 75 - 16 = 59. Also from my experience, the larger the number of vectors, the more pictures you need to obtain good results.</li>
</ul>
<h3><a id="user-content-preprocess" aria-hidden="true" href="#preprocess"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Preprocess</h3>
<p>This takes images from a directory, processes them to be ready for textual inversion, and writes results to another directory. This is a convenience feature and you can preprocess pictures yourself if you wish.</p>
<ul>
<li>
<strong>Source directory</strong>: directory with images</li>
<li>
<strong>Destination directory</strong>: directory where the results will be written</li>
<li>
<strong>Create flipped copies</strong>: for each image, also write its mirrored copy</li>
<li>
<strong>Split oversized images into two</strong>: if the image is too tall or wide, resize it to have the short side match the desired resolution, and create two, possibly intersecting pictures out of it.</li>
<li>
<strong>Use BLIP caption as filename</strong>: use BLIP model from the interrogator to add a caption to the filename.</li>
</ul>
<h3><a id="user-content-training-an-embedding" aria-hidden="true" href="#training-an-embedding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training an embedding</h3>
<ul>
<li>
<strong>Embedding</strong>: select the embedding you want to train from this dropdown.</li>
<li>
<strong>Learning rate</strong>: how fast should the training go. The danger of setting this parameter to a high value is that you may break the embedding if you set it too high. If you see <code>Loss: nan</code> in the training info textbox, that means you failed and the embedding is dead. With the default value, this should not happen. It&#39;s possible to specify multiple learning rates in this setting using the following syntax: <code>0.005:100, 1e-3:1000, 1e-5</code> - this will train with lr of <code>0.005</code> for first 100 steps, then <code>1e-3</code> until 1000 steps, then <code>1e-5</code> until the end.</li>
<li>
<strong>Dataset directory</strong>: directory with images for training. They all must be square.</li>
<li>
<strong>Log directory</strong>: sample images and copies of partially trained embeddings will be written to this directory.</li>
<li>
<strong>Prompt template file</strong>: text file with prompts, one per line, for training the model on. See files in directory <code>textual_inversion_templates</code> for what you can do with those. Use <code>style.txt</code> when training styles, and <code>subject.txt</code> when training object embeddings. Following tags can be used in the file:
<ul>
<li>
<code>[name]</code>: the name of embedding</li>
<li>
<code>[filewords]</code>: words from the file name of the image from the dataset. See below for more info.</li>
</ul>
</li>
<li>
<strong>Max steps</strong>: training will stop after this many steps have been completed. A step is when one picture (or one batch of pictures, but batches are currently not supported) is shown to the model and is used to improve embedding. if you interrupt training and resume it at a later date, the number of steps is preserved.</li>
<li>
<strong>Save images with embedding in PNG chunks</strong>: every time an image is generated it is combined with the most recently logged embedding and saved to image_embeddings in a format that can be both shared as an image, and placed into your embeddings folder and loaded.</li>
<li>
<strong>Preview prompt</strong>: if not empty, this prompt will be used to generate preview pictures. If empty, the prompt from training will be used.</li>
</ul>
<h3><a id="user-content-filewords" aria-hidden="true" href="#filewords"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>filewords</h3>
<p><code>[filewords]</code> is a tag for prompt template file that allows you to insert text from filename into the prompt. By default, file&#39;s extension is removed, as well as all numbers and dashes (<code>-</code>) at the start of filename. So this filename: <code>000001-1-a man in suit.png</code> will become this text for prompt: <code>a man in suit</code>. Formatting of the text in the filename is left as it is.</p>
<p>It&#39;s possible to use options <code>Filename word regex</code> and <code>Filename join string</code> to alter the text from filename: for example, with word regex = <code>\w+</code> and join string = <code>, </code>, the file from above will produce this text: <code>a, man, in, suit</code>. regex is used to extract words from text (and they are <code>[&#39;a&#39;, &#39;man&#39;, &#39;in&#39;, &#39;suit&#39;, ]</code>), and join string (&#39;, &#39;) is placed between those words to create one text: <code>a, man, in, suit</code>.</p>
<p>It&#39;s also possible to make a text file with same filename as image (<code>000001-1-a man in suit.txt</code>) and just put the prompt text there. The filename and regex options will not be used.</p>
<h2><a id="user-content-third-party-repos" aria-hidden="true" href="#third-party-repos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Third party repos</h2>
<p>I successfully trained embeddings using those repositories:</p>
<ul>
<li><a href="https://github.com/nicolai256/Stable-textual-inversion_win">nicolai256</a></li>
<li><a href="https://github.com/invoke-ai/InvokeAI">lstein</a></li>
</ul>
<p>Other options are to train on colabs and/or using diffusers library, which I know nothing about.</p>

<ul>
<li>Github has kindly asked me to remove all the links here.</li>
</ul>

<p>Hypernetworks is a novel (get it?) concept for fine tuning a model without touching any of its weights.</p>
<p>The current way to train hypernets is in the textual inversion tab.</p>
<p>Training works the same way as with textual inversion.</p>
<p>The only requirement is to use a very, very low learning rate, something like 0.000005 or 0.0000005.</p>
<h3><a id="user-content-dum-dum-guide" aria-hidden="true" href="#dum-dum-guide"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dum Dum Guide</h3>
<p>An anonymous user has written a guide with pictures for using hypernetworks: <a href="https://rentry.org/hypernetwork4dumdums" rel="nofollow">https://rentry.org/hypernetwork4dumdums</a></p>
<h3><a id="user-content-unload-vae-and-clip-from-vram-when-training" aria-hidden="true" href="#unload-vae-and-clip-from-vram-when-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Unload VAE and CLIP from VRAM when training</h3>
<p>This option on settings tab allows you to save some memoryat the cost of slower preview picture generation.</p>

              </div></div>
  </body>
</html>
