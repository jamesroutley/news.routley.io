<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.com/research/weak-to-strong-generalization">Original</a>
    <h1>Weak-to-Strong Generalization</h1>
    
    <div id="readability-page-1" class="page"><div><!----><div id="content"><!--[--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>A core challenge for aligning future superhuman AI systems (superalignment) is that humans will need to supervise AI systems much smarter than them. We study a simple analogy: can small models supervise large models? We show that we can use a GPT-2-level model to elicit most of GPT-4’s capabilities—close to GPT-3.5-level performance—generalizing correctly even to hard problems where the small model failed. This opens up a new research direction that allows us to directly tackle a central challenge of aligning future superhuman models while making iterative empirical progress today.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="the-superalignment-problem" data-heading=""><div><div><p><h2>The superalignment problem</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity. </p><p>We formed the <a href="https://openai.com/blog/introducing-superalignment" rel="noopener noreferrer" target="_blank">Superalignment team</a> earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.</p><p>Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand. </p><p>Relative to superhuman AI models, humans will be “weak supervisors.”<em> </em>This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>To make progress on this core challenge, we propose an analogy we can empirically study today: <strong>can we use a smaller (less capable) model to supervise a larger (more capable) model?</strong><br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><figure><p><img src="https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="2041" height="1164" alt="Superalignmentblog Artwork Transparent" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"/></p><figcaption><p><strong>A simple analogy for superalignment: </strong>In traditional machine learning (ML), humans supervise AI systems weaker than themselves (left). To align superintelligence, humans will instead need to supervise AI systems smarter than them (center). We cannot directly study this problem today, but we can study a simple analogy: can small models supervise larger models (right)?<br/></p></figcaption></figure></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Naively, we might not expect a strong model to perform better than the weak supervisor that provides its training signal—it may simply learn to imitate all the errors the weak supervisor makes. On the other hand, strong pretrained models have excellent raw capabilities—we don&#39;t need to teach them new tasks from scratch, we just need to elicit their latent knowledge. The critical question is then: will the strong model generalize according to the weak supervisor&#39;s underlying intent—leveraging its full capabilities to solve the task even on difficult problems where the weak supervisor can only provide incomplete or flawed training labels?<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><div><div><div><figure layout="auto" data-v-22aefd69=""><div data-v-22aefd69=""><!----><!--[--><!--]--><figcaption data-v-22aefd69=""><strong>Typical weak-to-strong generalization across NLP benchmarks:</strong> We use a GPT-2-level model as a weak supervisor to finetune GPT-4.</figcaption></div></figure></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>We can significantly improve generalization in many settings. We use a simple method that  encourages the strong model to be more confident—including confidently disagreeing with the weak supervisor if necessary. <strong>When we supervise GPT-4 with a GPT-2-level model using this method on NLP tasks, the resulting model typically performs somewhere between GPT-3 and GPT-3.5.</strong> We are able to recover much of GPT-4’s capabilities with only much weaker supervision.</p><p>This method is a proof of concept with important limitations; for example, it still doesn’t work on ChatGPT preference data. However, we also find signs of life with other approaches, such as optimal early stopping and bootstrapping from small to intermediate to large models.</p><p>Collectively, our results suggest that (1) naive human supervision—such as reinforcement learning from human feedback (RLHF)—could scale poorly to superhuman models without further work, but (2) it is feasible to substantially improve weak-to-strong generalization.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>There are still important disanalogies between our current empirical setup and the ultimate problem of aligning superhuman models. For example, it may be easier for future models to imitate weak human errors than for current strong models to imitate current weak model errors, which could make generalization harder in the future. </p><p>Nevertheless, we believe our setup captures some key difficulties of aligning future superhuman models, enabling us to start making empirical progress on this problem today. There are many promising directions for future work, including fixing the disanalogies in our setup, developing better scalable methods, and advancing our scientific understanding of when and how we should expect good weak-to-strong generalization.</p><p><strong>We believe this is an exciting opportunity for the ML research community to make progress on alignment.</strong> To kickstart more research in this area,</p><ul><li>We are releasing <a href="https://github.com/openai/weak-to-strong" rel="noopener noreferrer" target="_blank">open source code</a> to make it easy to get started with weak-to-strong generalization experiments today.</li><li>We are launching a <a href="https://openai.com/blog/superalignment-fast-grants" rel="noopener noreferrer" target="_blank">$10 million grants program</a> for graduate students, academics, and other researchers to work on superhuman AI alignment broadly. We’re especially excited to support research related to weak-to-strong generalization.</li></ul><p>Figuring out how to align future superhuman AI systems to be safe has never been more important, and it is now easier than ever to make empirical progress on this problem. We are excited to see what breakthroughs researchers discover.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--]--></div></div></div>
  </body>
</html>
