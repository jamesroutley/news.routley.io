<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kracekumar.com/post/chatgpt-shambles-gary-marcus-prompt/">Original</a>
    <h1>ChatGPT Shambles for Gary Marcus Prompt</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
  
  <p><time datetime="2025-02-08T22:14:34Z">Sat, Feb 8, 2025</time></p><p>Gary Marcus recently wrote an article titled <a href="https://garymarcus.substack.com/p/chatgpt-in-shambles">ChatGPT in Shambles</a>.
The prompt instructed chatgpt to produce a tabular table of median house hold income across <a href="https://chatgpt.com/share/67a20c79-bfa4-8001-8cdf-4ee60d42df5f">U.S. states</a>.</p>
<pre tabindex="0"><code>Make a table of every state in U.S., including population, area, median house hold income, sorted in order of median household income.
</code></pre><p><img src="https://kracekumar.com/images/chatgpt-shambles/chatgpt_original.png" alt="chatgpt-original-output"/>
The output contained only twenty states and interrupted. The final row contained only name of the state.</p>
<h3 id="chatgpt---my-attempt">ChatGPT - My Attempt</h3>
<p>The <a href="https://chatgpt.com/share/67a7d85c-6af0-8001-bb5e-d01d816b59f7">same prompt</a> returned all the states and income, when I tried and logged into the ChatGPT. I skipped verifying the data quality and checked only the structure.</p>
<p><img src="https://kracekumar.com/images/chatgpt-shambles/chatgpt_1.png" alt="chatgpt-1"/>
<img src="https://kracekumar.com/images/chatgpt-shambles/chatgpt_2.png" alt="chatgpt-2"/>
<img src="https://kracekumar.com/images/chatgpt-shambles/chatgpt_3.png" alt="chatgpt-3"/></p>
<p>My guess is fine-tuned(don’t think so in the short interval) or non-deterministic output based on logged in user vs anonymnous ask.</p>
<p>I tried the same prompt in other models</p>
<h3 id="claude">Claude</h3>
<p>Produced well structured output with an extra summary and further asking for more task.</p>
<p><img src="https://kracekumar.com/images/chatgpt-shambles/claude.png" alt="claude-output"/></p>
<h3 id="deepseek">Deepseek</h3>
<p><img src="https://kracekumar.com/images/chatgpt-shambles/deepseek.png" alt="deepseek-output"/>
Similar to Claude’s output Deepseek did produce all states including a summary.</p>
<p><a href="https://g.co/gemini/share/ea864e8105c1">Gemini 2.0 Flash</a></p>
<p><img src="https://kracekumar.com/images/chatgpt-shambles/gemini.png" alt="gemini-flash-pro-2.0-output"/>
By the far the Gemini output is well-structured with rank column, option to export the results to google sheets and summary at the end.</p>
<h3 id="le-chat">Le Chat</h3>
<p><img src="https://kracekumar.com/images/chatgpt-shambles/le_chat.png" alt="lechat"/> <a href="https://chat.mistral.ai/chat">Le Chat</a> produced all the fifty states with the sources.</p>
<p>In the overall exercise it’s clear to see small variation across models and clearly other models produce better output compared to ChatGPT.
It’s confusing to see different behaviour from ChatGPT.</p>

</div>


    </div></div>
  </body>
</html>
