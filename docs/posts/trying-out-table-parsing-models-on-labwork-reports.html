<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://noamf.ink/blog/posts/pdf-lab-reports/">Original</a>
    <h1>Trying out Table Parsing Models on Labwork Reports</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-document-content">





<p>This post is part of a <a href="https://coredumped.dev/2022/blog/posts/pdf-series-intro">series</a> about Document AI, specifically with an eye towards parsing medical documents. In this post, we’ll take a quick look at how a few <a href="https://coredumped.dev/2022/blog/posts/pdf-parsing-tasks#table-parsing">table parsing</a> tools do when applied to lab test reports.</p>
<section id="structure-of-lab-reports">
<h3 data-anchor-id="structure-of-lab-reports">Structure of lab reports</h3>
<p>Below are three sample reports from three different labs.</p>
<p><a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/accesa-000.png" data-gallery="unmarked"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/accesa-000.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/labcorp-002.png" data-gallery="unmarked"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/labcorp-002.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/quest-000.png" data-gallery="unmarked"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/report-images/quest-000.png" width="200"/></a></p>
<p>The reports are basically tables, but they have two interesting features that will make them difficult to parse.</p>
<p>First, the tables sometimes include sub-headers, which specify the name of the lab panel that the test falls under. This is in evidence in the third report, but it’s a bit of an unusual case because each panel has only one test. In some cases, some but not all tests are associated with panels.</p>
<p>Second, the tables contain comments. In the second document, the comments actually sometimes contain the result of the lab test. In the third document, the comments apply to the lab test procedure itself, not to the result for an individual patient. In both cases, the comments warp the structure of the table.</p>
<p>As far as I know, there isn’t a great open dataset of lab reports, primarily because the include private health information. That said, a number of people have uploaded their own reports to the internet, and I used many of them in these checks. I didn’t include them in the post due to privacy considerations. Running table parsing models on those reports didn’t reveal anything unexpected, but they did show that the models have a much harder time with noisy, scanned documents.</p>
</section>
<section id="table-transformer-model">
<h3 data-anchor-id="table-transformer-model">Table Transformer Model</h3>
<p>The open-source <a href="https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer">Table Transformer</a> is trained on detecting tables and identifying their structure. The model and training data are described in <a href="https://arxiv.org/abs/2110.00061">this paper</a>. The same base model is trained on two separate tasks, described below.</p>
<section id="table-detection">
<h4 data-anchor-id="table-detection">Table detection</h4>
<p>The first task is to identify tables in a document.</p>
<p><a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/accesa-000.png" data-gallery="tatr"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/accesa-000.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/labcorp-002.png" data-gallery="tatr"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/labcorp-002.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/quest-000.png" data-gallery="tatr"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-report-images/quest-000.png" width="200"/></a></p>
<p>In two cases, the model has a hard time distinguishing between structured information at the top of the document, and the structured lab results. In the second document, the model seems to be tricked by the space in the table introduced by the longest comment. It may also be relevant that the records below that table have more spacing between them than those above, as they each have their own (short) comment. It’s also interesting that the model has high confidence for the third document, but relatively low confidence for the first two.</p>
</section>
<section id="table-structure-identification">
<h4 data-anchor-id="table-structure-identification">Table structure identification</h4>
<p>To run the structure identification model, we first pull the highest-confidence table identified by the detection model above. The structure identification model is very sensitive to the amount of padding around the table it receives as input. This is a noted issue with the pre-trained model, and can likely be resolved by fine-tuning with an augmented dataset. For the purposes of using the pre-trained model, I had to add padding around the tables identified above to get reasonable predictions. It’s possible that adding more or less padding would result in better predictions, but because that would be a superficial solution to the problem, I didn’t take the time to find out.</p>
<p>Running this model produces the following results.</p>
<p><a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/accesa-000.png" data-gallery="tatr-segmented"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/accesa-000.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/labcorp-002.png" data-gallery="tatr-segmented"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/labcorp-002.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/quest-000.png" data-gallery="tatr-segmented"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/tatr-structure-report-images/quest-000.png" width="200"/></a></p>
<p>The model does a relatively good job with the first document, though not as good as I would have expected given how structured the table is. In the second document, it seems to be exactly correct for the part of the table identified by the table detection model, but has no change on the rest of the table (I did not separately extract the rest of the table to see how it would have done). In the third document, the model doesn’t recognize the rows corresponding to the antibody tests. This might have something to do with the fact the test names extend past what it has identified as the relevant column. The size of the column seems to have to do with the front-matter, rather than the lab tests.</p>
</section>
</section>
<section id="commercial-options">
<h3 data-anchor-id="commercial-options">Commercial options</h3>
<p>There are a few commercial options for table parsing. The easiest to try out was Nanonets, which produced the outputs below.</p>
<p><a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/accesa-000.png" data-gallery="nanonets"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/accesa-000.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/labcorp-002.png" data-gallery="nanonets"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/labcorp-002.png" width="200"/></a> <a href="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/quest-000.png" data-gallery="nanonets"><img src="https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/nanonets-report-images/quest-000.png" width="200"/></a></p>
<p>The first striking part of this result is that none of the table cells overlap. I don’t know if this is due to post-processing or because of how they trained their model, but it’s nice. Their structure detection seems to be better, by and large, but also less flexible, as there is no notion of a spanning cell or a header row.</p>
<p>The first document is pretty much exactly right. In the second document, they miss the last lab test, and also separate the <code>REFERENCE INTERVAL</code> column into two columns, which would cause problems downstream. In the third document, they also miss the last lab, but otherwise do well. Because they’re missing a concept for a spanning cell, in the last two documents the comments are being sliced up. This might be addressable in post-processing.</p>
</section>
<section id="next-steps">
<h3 data-anchor-id="next-steps">Next steps</h3>
<p>The two models both had difficulty with the kinds of things I expected they might have difficulty with, discussed above. Additionally, the open source model had difficulty distinguishing between the lab test results and structured text at the top of the document, which I did not anticipate.</p>
<p>Any training or fine-tuning to get around these kinds of problems would have to be based on synthetic data, because there is not enough open lab-report data out there.</p>
<p>Here are a few considerations for generating this kind of data set:</p>
<ul>
<li>Reports should have structured text at the top that looks like it <em>might</em> be part of the lab result table but is not, to help the model distinguish between form fields and lab results.</li>
<li>The lab results table should include header row columns for lab tests, with representation of the cases in which all, some, or no labs have associated panels, and a good distribution over the number of tests per panel.</li>
<li>The lab results rows should sometimes span multiple lines, with overflow wrapping at both the column and row level (this caused issues in lab reports not shown in this post).</li>
<li>The lab results table should have both inline and stand-alone comments.</li>
<li>Ideally the lab names, reference ranges, values, units, etc. used in the generated document should all be plausible. Depending on how the model is trained, this might help with text recognition.</li>
</ul>
<p>I don’t think it would be difficult to create a dataset with these properties that also contains enough variety to train for every lab report I’ve come across. Because there are lots of little labs out there, and their reports all have different formats, it would be prudent to add even more structural variety than that, even though that will likely increase the required model size and the training burden.</p>
<p>One other thought is that the approach of these table parsing models seems somewhat indirect. They first visually identify the table and its structure, and then run OCR on each cell. I wonder whether there is a more direct approach, where the input is the lab report document, and the output is one or more CSV files containing the results.</p>
<p>Overall, I don’t think general purpose table parsing tools will be effective for reading labs off of PDFs. I do think they’ve raised some interesting modeling and data-generation avenues to explore, as time allows.</p>


</section>

</div></div>
  </body>
</html>
