<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2306.11644">Original</a>
    <h1>Textbooks are all you need</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gunasekar%2C+S">Suriya Gunasekar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aneja%2C+J">Jyoti Aneja</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mendes%2C+C+C+T">Caio César Teodoro Mendes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Del+Giorno%2C+A">Allie Del Giorno</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gopi%2C+S">Sivakanth Gopi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Javaheripi%2C+M">Mojan Javaheripi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kauffmann%2C+P">Piero Kauffmann</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Rosa%2C+G">Gustavo de Rosa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Saarikivi%2C+O">Olli Saarikivi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salim%2C+A">Adil Salim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shah%2C+S">Shital Shah</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Behl%2C+H+S">Harkirat Singh Behl</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bubeck%2C+S">Sébastien Bubeck</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eldan%2C+R">Ronen Eldan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kalai%2C+A+T">Adam Tauman Kalai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+Y+T">Yin Tat Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuanzhi Li</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2306.11644">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  We introduce phi-1, a new large language model for code, with significantly
smaller size than competing models: phi-1 is a Transformer-based model with
1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook
quality&#34; data from the web (6B tokens) and synthetically generated textbooks
and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains
pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays
surprising emergent properties compared to phi-1-base, our model before our
finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller
model with 350M parameters trained with the same pipeline as phi-1 that still
achieves 45% on HumanEval.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Suriya Gunasekar [<a href="https://arxiv.org/show-email/b310108e/2306.11644">view email</a>]
      </p></div></div>
  </body>
</html>
