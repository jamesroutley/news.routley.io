<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.freshpaint.io/blog/github-copilot-and-the-methods-of-rationality">Original</a>
    <h1>GitHub Copilot and the Methods of Rationality</h1>
    
    <div id="readability-page-1" class="page"><div><p>I recently decided I wanted to give GitHub Copilot a shot. I’d seen mixed reviews of it on Hacker News. Some people were saying it was invaluable. Others were saying it produced incredibly buggy output. I thought, why not give it a shot and see what happens. After a few experiments, I discovered that Copilot sometimes gets things scarily accurately while other times it completely misses the mark. What I did discover is I could often change my prompt slightly and Copilot goes from completely missing the mark to getting the answer spot on. My goal with this post is to do a few experiments in an attempt to discover the ideal way to use Copilot.</p><p>This post is inspired by Harry Potter and the Methods of Rationality. HPMOR is a fan-fiction of Harry Potter in which Harry Potter uses science to understand magic. In <a href="https://www.hpmor.com/chapter/22">chapter 22</a> Harry does a number of experiments in attempt to understand how the pronunciation of a spell changes the resulting spell being cast. My goal is very much the same. I want to understand how different incantations affect the resulting output of Copilot. In the end, I want to create a set of “guidelines” for writing your Copilot prompt to help it produce the optimal code for your problem.</p><p>I decided for this experiment, I would try a problem from the <a href="https://adventofcode.com/2021/day/1">Advent of Code</a>. For this experiment I want Copilot to generate all the code of the problem. I’ve heard people have had success by using Copilot solely as autocomplete, but I want to explore how Copilot handles natural language prompts. At the end of this post, I’m going to keep a list of a number of open questions I have.</p><p>Let’s start with the <a href="https://adventofcode.com/2021/day/1">first problem from the Advent of Code 2021</a>. The main problem is to count the number of elements in an array that are larger than the previous element.</p><p>The first thing we have to do is read an input file that contains one number per line like so:</p><p>{% c-block language=&#34;python&#34; %}</p><p>So I open my text editor and type in.</p><p>{% c-block language=&#34;python&#34; %}</p><p>And I get back:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Hmm. That seems very strange. Let me try to be more specific</p><p>{% c-block language=&#34;python&#34; %}</p><p>Not much better. This is not inspiring much confidence in Copilot.</p><p>Let me try something different. Let me try helping Copilot along a little by providing a function signature:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Pretty close! Copilot just needs to know the that the each line represents a number:</p><p>{% c-block language=&#34;python&#34; %}</p><p>And voila!</p><p>{% c-block language=&#34;python&#34; %}</p><p>So we’ve observed two interesting things so far.</p><ol role="list"><li>It seems like we had to provide a function signature to Copilot to get it to write any code at all.</li><li>We should be as specific as possible when giving our prompt to help Copilot understand <em>exactly</em> what it needs to do.</li></ol><p>Now for the fun part. Let’s try changing our prompts slightly and try to understand why Copilot does what it does.</p><p>Let’s try to dig a bit more into the weird behavior we observed. When I gave Copilot the initial prompt of:</p><p>{% c-block language=&#34;python&#34; %}</p><p>It gave back the completely bizarre response of:</p><p>{% c-block language=&#34;python&#34; %}</p><p>I have a hypothesis that this is happening because it’s common for files to start with large comments describing what the file does. Copilot may be overgeneralizing and may be attempting to generate the next few lines of the comment. To test this hypothesis, let’s try the following:</p><p>{% c-block language=&#34;python&#34; %}</p><p>And copilot generates pretty much exactly the code we need!</p><p>{% c-block language=&#34;python&#34; %}</p><p>So it seems like the initial bizarre behavior can be explained by Copilot attempting to generate a file comment.</p><p>The next experiment I want to try is to see how specific we need to be in order to get Copilot to do what we want. What we wound up with was the prompt:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Generating the code:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Let’s see what happens if we change the prompt slightly. Let’s first try deleting parts from the prompt and seeing how it changes the output of Copilot. In this case I deleted the &#34;in array&#34; part.</p><p>{% c-block language=&#34;python&#34; %}</p><p>So it seems like Copilot knows it’s reading an array. Now what happens if we just change the wording:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Huh??? For some reason Copilot is able to understand “where each line contains a number” means it should parse each line as a number, but it doesn’t understand “with numbers on each line” to mean the same. I found if I had the “an array” part back, Copilot will generate the correct code again</p><p>I have a few hypothesis for why this could be happening:</p><ol role="list"><li>The output Copilot generates is highly non-deterministic. Did we just stumble upon a case Copilot handles correctly?</li><li>For some reason Copilot is able to understand what it’s supposed to do when given “each line contains a number means”, but is not able to understand what to do when given “with numbers on each line”.</li></ol><p>As a quick test I tried changing the word “read” to “parse”. My hypothesis was using a more semantically meaningful word such as “parse” will help Copilot generate the correct code :</p><p>{% c-block language=&#34;python&#34; %}</p><p>And it does! Alternatively, if we no longer describe the problem in the comment:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Copilot no longer extracts the numbers from the line.</p><p>More testing is needed, but it seems like to properly get Copilot to read the input properly, we need to do two things:</p><ol role="list"><li>Let it know what it needs to do (”parse”, “read an array”)</li><li>Describe the structure of the input (”with numbers on each line”, “where each line contains a number”)</li></ol><p>Based on the result of the experiment where I changed “read” to “parse”, it seems that Copilot is influenced by the specific jargon used. I suspect that there are other “magic words” such as parse that can be used to guide Copilot. These are words that are more semantically meaningful.</p><p>If we can figure out the right magic words, we can make it a lot easier to program with Copilot. Based on this example, whenever we are doing anything with parsing, we will want to use the word “parse”.</p><p>Now that we have the array, we can solve the actual problem (counting the number of elements in the array larger than the previous element). I decided to first attempt this in two steps:</p><ol role="list"><li>Calculate the difference between each element.</li><li>Count the number of positive elements.</li></ol><p>Let’s try this:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Copilot provided the following autocompletion</p><p>{% c-block language=&#34;python&#34; %}</p><p>Ok. That’s not what I meant. I suspect I wasn’t specific enough. Let’s try being more specific in the prompt:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Ok. This code is almost right. The only problem is the first element of the array array[0] is automatically appended to the output. I’m curious how we can get Copilot to properly handle edge cases like this. Intuitively in my mind, the first element shouldn’t be included in the differences array. I wonder if there’s an magic words that I can use to say “do the intuitive thing”.</p><p>Let me explicitly mention the edge case:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Wow! I’m surprised Copilot understood that. It’s not ideal that we have to explicitly mention edge cases like this.</p><p>Let me see if there’s a magic word I can use to get Copilot to do the right thing. I don’t expect this to work, but:</p><p>{% c-block language=&#34;python&#34; %}</p><p>WTF!!! It seems that somehow the word “intuitively” got Copilot to produce the right answer with a much vaguer prompt. More evidence is needed, but I’m really curious about the power of the word “intuitively”. I have a few hypothesis:</p><ol role="list"><li>“Intuitively” will cause Copilot to produce really short code</li><li>We just got really lucky.</li></ol><p>For the time being, let’s move on. Now that we have the differences array, we now need to count the number of elements that are positive:</p><p>{% c-block language=&#34;python&#34; %}</p><p>That seems pretty straightforward. Let’s see what happens when we use the word “intuitively”. Anticlimactically,  “Intuitively count the number of positive elements in an array.” gives the same code.</p><p>Now let’s try to get Copilot to glue all the code together:</p><p>{% c-block language=&#34;python&#34; %}</p><p>So it seems Copilot is able to reference other code in the same file.</p><p>After running this code, we get the correct result.</p><p>The last thing I want to try is solving the problem with a single function</p><p>{% c-block language=&#34;python&#34; %}</p><p>That’s <em>almost</em> correct. There’s a very subtle bug. This code uses range(len(numbers)) instead of range(1, len(numbers)). That will result in the comparison numbers[0] &gt; numbers[-1] which will compare the first element against the last. Let’s see if we can use the word “intuitively” to fix this:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Ok. So it seems like “intuitively” threw the problem way off. That’s some evidence towards we just got lucky when we used the word intuitively earlier.</p><p>Let’s instead try telling Copilot to explicitly handle the edge cases.</p><p>{% c-block language=&#34;python&#34; %}</p><p>Uh… That’s pretty close, but also a bit bizarre. The code is incrementing answer += 1 when i == 0 when it shouldn’t be. I guess technically Copilot did handle the edge case, but not in the way we wanted. It’s starting to feel like Copilot is almost a genie and I have to phrase my wish in exactly the correct way.</p><p>I wonder if there’s any other way I can guide Copilot. It seems that if I use the word “count” instead of “calculate” Copilot will generate valid code:</p><p>{% c-block language=&#34;python&#34; %}</p><p>I tried a few different variations of the prompt in an attempt to see which prompts work and which don’t. One thing I discovered is that the period actually matters! For a specific prompt I tested Copilot will generate incorrect code, but if you remove the period, it will generate the right one! That begs the question of how can we get Copilot to consistently do what we want. Here’s a list of a few of the prompts that I found work</p><ul role="list"><li>Count the number of elements greater than the previous one. Handle edge cases.</li><li>Count the number of elements greater than the previous element</li><li>Count the number of elements greater than the previous element. Handle edge cases.</li></ul><p>And here’s a list of a few prompts that generate incorrect code. </p><ul role="list"><li>Count the number of elements greater than the previous one</li><li>Count the number of elements greater than the previous element.</li><li>Calculate the number of elements greater than the previous one. Handle edge cases.</li><li>Calculate the number of elements greater than the previous one</li></ul><p>All of them generate the code that doesn&#39;t properly handle the edge case with the exception of the last example. The last example instead generates:</p><p>{% c-block language=&#34;python&#34; %}</p><p>After seeing the last example, I noticed something! When giving the prompt to Copilot, Copilot would first generate the name of the function, then later generate the body of the function. The last example calculate_frequency has a name that is really far off which may have caused Copilot to generate code that’s really far off. What if I help Copilot along by changing the name:</p><p>{% c-block language=&#34;python&#34; %}</p><p>So that seems to be <em>more</em> correct than the way off example than before, but not quite correct. What if I add the “handle edge cases”?</p><p>{% c-block language=&#34;python&#34; %}</p><p>That gives the correct answer!</p><p>I also decided to try what happens if I don’t include a function comment:</p><p>{% c-block language=&#34;python&#34; %}</p><p>So that gives the code with the edge case. Oddly enough:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Gives a very bizarre and very wrong bit of code:</p><p>{% c-block language=&#34;python&#34; %}</p><p>I decided to go back to the incorrect prompts I had up above:</p><ul role="list"><li>Count the number of elements greater than the previous one</li><li>Count the number of elements greater than the previous element.</li><li>Calculate the number of elements greater than the previous one. Handle edge cases.</li><li>Calculate the number of elements greater than the previous one</li></ul><p>I found that for all of them, if I add “Handle edge cases” to the end and provide the function name “count_greater_than_previous”, the correct code is output! So it seems like for this problem we have to do three things to get Copilot to consistently produce output:</p><ul role="list"><li>Provide a comment describing what we want Copilot to do.</li><li>Specify “handle edge cases”.</li><li>Make sure the function name accurately reflects what we want Copilot to do.</li></ul><p>This makes me curious when Copilot generates a poor function name, if that means the prompt was too vague. Maybe that’s a way we can determine how good our prompts are.</p><p>Unfortunately, the “handle edge cases” doesn’t seem like it always works. I went back to the compute differences prompt and tried “handle edge cases” and it doesn’t seem like that worked:</p><p>{% c-block language=&#34;python&#34; %}</p><p>So it seems like “handle edge cases” isn’t a panacea. In fact, I wonder if that just causes Copilot to add a case for i == 0 and handle the edge case in some way.</p><p>It seems like using the word “diff” instead of “calculate” makes the example work:</p><p>{% c-block language=&#34;python&#34; %}</p><p>Yep! I wonder if in the same way that certain phrases are magic phrases, certain words are “curse words” such as “Calculate”. These words are vague and make it hard for copilot to understand what to do. Looking at previous data, it does seem like the word “calculate” does result in less example to be correct.</p><p>After going through these experiments, there are a few takeaways I have. More evidence for these is needed, but I think these are a good start:</p><ul role="list"><li>Copilot is highly sensitive to your prompt. Slight changes in the prompt will generate completely different code.</li><li>When first writing code in a file, include a separate file header.</li><li>Make sure your prompt describes the structure of the input.</li><li>Try to use “magic words” - certain words that provide lots of context. Based on not a lot of evidence, two phrases I’ve found to work well so far are “parse” and “avoid edge cases”. (After a bit more experimentation, I’ve found that this works better than “handle edge cases”)</li><li>Avoid “curse words” - certain words that are vague. Examples that I’ve found are “calculate” and “compute”.</li></ul><p>I hope you appreciate the experiments I did and I would greatly appreciate it if you share any experiments you do with me. You can reach out to me on Twitter <a href="https://twitter.com/mmalisper">@mmalisper</a>.</p><p>By the way, if you are looking for a software engineering role, <a href="https://www.freshpaint.io/about?ashby_jid=bfe56523-bff4-4ca3-936b-0ba15fb4e572">my company Freshpaint is hiring</a>.</p><ul role="list"><li>Is there a way we can measure the accuracy of code generated by Copilot? If so, we could measure what words get Copilot closer to the result we want and which words get further away.</li><li>How does the output differ across different programming languages for the same prompt?</li><li>What happens if you add types?</li><li>If copilot generates a non-specific name, does that mean that the prompt provided wasn’t specific enough?</li></ul><p>‍</p></div></div>
  </body>
</html>
