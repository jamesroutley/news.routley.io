<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://predictive-optimization.cs.princeton.edu/">Original</a>
    <h1>Against predictive optimization</h1>
    
    <div id="readability-page-1" class="page"><div>
        
        <div>
          <h4 id="flaws"> Flaws of predictive optimization </h4>
          <p>
            We present seven flaws of predictive optimization. Our aim is to outline a set of objections
            inherent to predictive optimization that cannot be easily fixed using a design or engineering change.
            Taken together, these critical flaws undermine the legitimacy of applications of predictive optimization.
          </p>



          <div id="dp-accordion">
           <div>
            
         <div id="collapseOne" aria-labelledby="headingOne">
           <p>
              Automated decision-making algorithms are used to make decisions based on the data they are trained on. The type of decision can affect how well the algorithms work. Even if a tool makes the correct prediction, if the decision taken based on this decision is flawed, the tool cannot work as claimed. A decision taken using automated tools is also called an intervention.
              Additionally, decisions based on predictions might themselves affect the outcomes being predicted.
              For example, a higher bail amount—based on predicted recidivism—can increase the likelihood of recidivism 
              (<a href="https://www.journals.uchicago.edu/doi/abs/10.1086/688907">Gupta et al. 2016</a>).
            </p>
        </div>
      </div>
    </div>
    <div>
      
   <div id="collapseTwo" aria-labelledby="headingTwo">
     <p>
        In constructing an application of predictive optimization, some existing data must be
        chosen for the model to predict. For example, to predict who will do well in college, the
        application could try to predict the GPA at the end of the 1st year of college. The
        outcome being predicted is called the target variable. The target variable is typically
        chosen to roughly correspond to the decision maker’s goal—also called the construct.
        <a href="https://www.science.org/doi/10.1126/science.aax2342">Obermeyer et al. 2019</a> find that Optum ImpactPro has a construct of healthcare needs and a target variable of healthcare costs.
        However, due to reasons such as unequal access to healthcare, the costs are often a poor proxy for the actual healthcare needs.
      </p>
  </div>
</div>

<div>
  
<div id="collapseThree" aria-labelledby="headingThree">
 <p>
    When the distribution of data on which an ML model is trained is not representative of the distribution
    on which it will be deployed, model performance suffers. 
    The Public Safety Assessment (PSA) tool uses a population of 1.5 million cases from 300 U.S. jurisdictions.
    However, in some of the jurisdictions in which it is used, the base rate of violent recidivism is lower than the base rate in the tool&#39;s training data by more than a factor of 10. This results in risk thresholds for pre-trial detention that are severely miscalibrated, resulting in over-detention
    (<a href="https://theappeal.org/how-a-tool-to-help-judges-may-be-leading-them-astray/">Corey 2019</a>).
  </p>
</div>
</div>

<div>
  
<div id="collapseFour" aria-labelledby="headingFour">
 <p>
    One of the characteristics of predictive optimization is that the prediction target is a future event in an individual’s life. Thus, there are many inherent limits to prediction that limit how accurate the system could be.
    Epic, one of the largest healthcare tech companies in the U.S., released a plug-and-play sepsis prediction tool in 2017. When the tool was released, the company claimed that it had an AUC between 0.76 and 0.83. Over the next five years, the tool was deployed across hundreds of U.S. hospitals. But a 2021 study found that the tool performed much worse: it had an AUC of 0.63 (<a href="https://pubmed.ncbi.nlm.nih.gov/34152373/">Wong et al. 2021</a>).
    Following this study and a series of news reports, Epic stopped selling its one-size-fits-all sepsis prediction tool
    (<a href="https://www.statnews.com/2022/10/24/epic-overhaul-of-a-flawed-algorithm/">Ross 2022</a>).
  </p>
</div>
</div>

<div>
  
<div id="collapseFive" aria-labelledby="headingFive">
 <p>
    Disparate performance refers to differences in performance for different demographic groups.
    However, a system that is fair in a statistical sense may
    nonetheless perpetuate, reify, or even amplify long-standing cycles of inequality.
    Oregon state recalled a tool they built for deciding which families should be investigated by social workers (<a href="https://www.npr.org/2022/06/02/1102661376/oregon-drops-artificial-intelligence-child-abuse-cases">The Associated Press 2022</a>) after public critiques about the racial bias of a similar tool,
    AFST, were published (<a href="https://apnews.com/article/child-welfare-algorithm-investigation-9497ee937e0053ad4144a86c68241ef1">Ho and Burke 2022</a>).
  </p>
</div>
</div>

<div>
  
<div id="collapseSix" aria-labelledby="headingSix">
 <p>
    When decision-making algorithms are deployed in consequential settings, they must include mechanisms for contesting such decisions.
    In 2013, the Netherlands deployed a predictive algorithm to detect welfare fraud. The algorithm wrongly accused 30,000 parents of welfare fraud, and led to debts of hundreds of thousands of Euros. In many cases, the decisions were based on incorrect data, but the decision subjects had no recourse.
    In the fallout over the algorithm’s use, the Prime Minister and his entire cabinet resigned (<a href="https://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/">Heikkilä 2022</a>).
  </p>
</div>
</div>

<div>
  
<div id="collapseSeven" aria-labelledby="headingSeven">
 <p>
    A canonical example of Goodhart’s law is the cobra effect: when the colonial British government offered bounties for dead cobras to reduce the cobra population, the response instead was people breeding more cobras to kill. Similarly, predictive optimization can create unintended incentives for decision subjects to game the system.
    The LYFT score (Life Years from Transplant)
    was proposed for allocating kidneys to patients in need of a transplant based on a prediction about how long they would live after the transplant (<a href="https://www.jstor.org/stable/10.7758/9781610449144">Robinson 2022</a>).
    Using this score would result in a disincentive for patients with kidney issues to take care of their kidney function: if their kidneys failed at a <em>younger</em> age, they would be more likely to get a transplant. 
  </p>
</div>
</div>
<!--We detail 27 questions that must be addressed with before predictive optimization is deployed. In particular, for each of our seven flaws, we include 2-5 questions based on common failure modes from our analysis.-->
<!--The rubric serves as a set of basic standards that developers of predictive optimization must address in order to advertise their application as legitimate. -->

<!-- We would like to thank Emily Cantrell, Amrit Daswaney, Jakob Mökander, Matt Salganik, and Paul Waller for feedback. We would also like to thank the participants of the <a href="https://mintresearch.org/ourwork/pais-workshop-stanford-1">Philosophy, AI, and Society (PAIS) workshop</a> for feedback. -->



</div>
</div></div>
  </body>
</html>
