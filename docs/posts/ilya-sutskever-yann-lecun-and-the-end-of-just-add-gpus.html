<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.abzglobal.net/web-development-blog/ilya-sutskever-yann-lecun-and-the-end-of-just-add-gpus">Original</a>
    <h1>Ilya Sutskever, Yann LeCun and the End of “Just Add GPUs”</h1>
    
    <div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&#34;topLeft&#34;:{&#34;unit&#34;:&#34;px&#34;,&#34;value&#34;:0.0},&#34;topRight&#34;:{&#34;unit&#34;:&#34;px&#34;,&#34;value&#34;:0.0},&#34;bottomLeft&#34;:{&#34;unit&#34;:&#34;px&#34;,&#34;value&#34;:0.0},&#34;bottomRight&#34;:{&#34;unit&#34;:&#34;px&#34;,&#34;value&#34;:0.0}}" data-sqsp-block="text" id="block-475b5722eff3f8e60877"><div>

<div data-sqsp-text-block-content="">
  <p>When two of the most influential people in AI both say that <strong>today’s large language models are hitting their limits</strong>, it’s worth paying attention.</p><p>In a recent long-form interview, <strong>Ilya Sutskever</strong> – co-founder of OpenAI and now head of Safe Superintelligence Inc. – argued that the industry is moving from an <strong>“age of scaling”</strong> to an <strong>“age of research”</strong>. At the same time, <strong>Yann LeCun</strong>, VP &amp; Chief AI Scientist at Meta, has been loudly insisting that <strong>LLMs are not the future of AI at all</strong> and that we need a completely different path based on “world models” and architectures like <strong>JEPA</strong>.</p><p>As developers and founders, we’re building products right in the middle of that shift.</p><p>This article breaks down Sutskever’s and LeCun’s viewpoints and what they mean for people actually shipping software.</p><h2>1. Sutskever’s Timeline: From Research → Scaling → Research Again</h2><p>Sutskever divides the last decade of AI into three phases:</p><h3>1.1. 2012–2020: The first age of research</h3><p>This is the era of “try everything”:</p><ul data-rte-list="default"><li><p>convolutional nets for vision</p></li><li><p>sequence models and attention</p></li><li><p>early reinforcement learning breakthroughs</p></li><li><p>lots of small experiments, new architectures, and weird ideas</p></li></ul><p>There <em>were</em> big models, but compute and data were still limited. The progress came from <strong>new concepts</strong>, not massive clusters.</p><h3>1.2. 2020–2025: The age of scaling</h3><p>Then scaling laws changed everything.</p><p>The recipe became:</p><blockquote><p><strong>More data + more compute + bigger models = better results.</strong></p></blockquote><p>You didn’t have to be extremely creative to justify a multi-billion-dollar GPU bill. You could point to a curve: as you scale up parameters and tokens, performance climbs smoothly.</p><p>This gave us:</p><ul data-rte-list="default"><li><p>GPT-3/4 class models</p></li><li><p>state-of-the-art multimodal systems</p></li><li><p>the current wave of AI products everyone is building on</p></li></ul><h3>1.3. 2025 onward: Back to an age of research (but with huge computers)</h3><p>Now Sutskever is saying that <strong>scaling alone is no longer enough</strong>:</p><ul data-rte-list="default"><li><p>The industry is already operating at <strong>insane scale</strong>.</p></li><li><p>The internet is finite, so you can’t just keep scraping higher-quality, diverse text forever.</p></li><li><p>The returns from “just make it 10× bigger” are getting smaller and more unpredictable.</p></li></ul><p>We’re moving into a phase where:</p><blockquote><p>The clusters stay huge, but <strong>progress depends on new ideas</strong>, not only new GPUs.</p></blockquote><h2>2. Why the Current LLM Recipe Is Hitting Limits</h2><p>Sutskever keeps circling three core issues.</p><h3>2.1. Benchmarks vs. real-world usefulness</h3><p>Models look god-tier on paper:</p><ul data-rte-list="default"><li><p>they pass exams</p></li><li><p>solve benchmark coding tasks</p></li><li><p>reach crazy scores on reasoning evals</p></li></ul><p>But everyday users still run into:</p><ul data-rte-list="default"><li><p>hallucinations</p></li><li><p>brittle behavior on messy input</p></li><li><p>surprisingly dumb mistakes in practical workflows</p></li></ul><p>So there’s a gap between <strong>benchmark performance</strong> and <strong>actual reliability</strong> when someone uses the model as a teammate or co-pilot.</p><h3>2.2. Pre-training is powerful, but opaque</h3><p>The big idea of this era was: pre-train on enormous text + images and you’ll learn “everything”.</p><p>It worked incredibly well… but it has downsides:</p><ul data-rte-list="default"><li><p>you don’t fully control <em>what</em> the model learns</p></li><li><p>when it fails, it’s hard to tell if the issue is data, architecture, or something deeper</p></li><li><p>pushing performance often means <strong>more of the same</strong>, not better understanding</p></li></ul><p>That’s why there’s so much focus now on <strong>post-training</strong> tricks: RLHF, reward models, system prompts, fine-tuning, tool usage, etc. We’re papering over the limits of the pre-training recipe.</p><h3>2.3. The real bottleneck: generalization</h3><p>For Sutskever, the biggest unsolved problem is <strong>generalization</strong>.</p><p>Humans can:</p><ul data-rte-list="default"><li><p>learn a new concept from a handful of examples</p></li><li><p>transfer knowledge between domains</p></li><li><p>keep learning continuously without forgetting everything</p></li></ul><p>Models, by comparison, still need:</p><ul data-rte-list="default"><li><p>huge amounts of data</p></li><li><p>careful evals to avoid weird corner-case failures</p></li><li><p>extensive guardrails and fine-tuning</p></li></ul><p>Even the best systems today <strong>generalize much worse than people</strong>. Fixing that is not a matter of another 10,000 GPUs; it needs new theory and new training methods.</p><h2>3. Safe Superintelligence Inc.: Betting on New Recipes</h2><p>Sutskever’s new company, <strong>Safe Superintelligence Inc. (SSI)</strong>, is built around a simple thesis:</p><ul data-rte-list="default"><li><p>scaling was the driver of the last wave</p></li><li><p><strong>research will drive the next one</strong></p></li></ul><p>SSI is not rushing out consumer products. Instead, it positions itself as:</p><ul data-rte-list="default"><li><p>focused on <strong>long-term research into superintelligence</strong></p></li><li><p>trying to invent <strong>new training methods and architectures</strong></p></li><li><p>putting <strong>safety and controllability</strong> at the core from day one</p></li></ul><p>Instead of betting that “GPT-7 but bigger” will magically become AGI, SSI is betting that <strong>a different kind of model</strong>, trained with different objectives, will be needed.</p><h2>4. Have Tech Companies Overspent on GPUs?</h2><p>Listening to Sutskever, it’s hard not to read between the lines:</p><ul data-rte-list="default"><li><p>Huge amounts of money have gone into GPU clusters on the assumption that scale alone would keep delivering step-function gains.</p></li><li><p>We’re discovering that the <strong>marginal gains from scaling</strong> are getting smaller, and progress is less predictable.</p></li></ul><p>That doesn’t mean the GPU arms race was pointless. Without it, we wouldn’t have today’s LLMs at all.</p><p>But it does mean:</p><ul data-rte-list="default"><li><p>The next major improvements will likely come from <strong>smarter algorithms</strong>, not merely <strong>more expensive hardware</strong>.</p></li><li><p>Access to H100s is slowly becoming a <strong>commodity</strong>, while genuine innovation moves back to <strong>ideas and data</strong>.</p></li></ul><p>For founders planning multi-year product strategies, that’s a big shift.</p><h2>5. Yann LeCun’s Counterpoint: LLMs Aren’t the Future at All</h2><p>If Sutskever is saying “scaling is necessary but insufficient,” <strong>Yann LeCun</strong> goes further:</p><blockquote><p><strong>LLMs, as we know them, are not the path to real intelligence.</strong></p></blockquote><p>He’s been very explicit about this in talks, interviews and posts.</p><h3>5.1. What LeCun doesn’t like about LLMs</h3><p>LeCun’s core criticisms can be summarized in three points:</p><ol data-rte-list="default"><li><p><strong>Limited understanding</strong></p></li><li><p><strong>A product-driven dead-end</strong></p></li><li><p><strong>Simplicity of token prediction</strong></p></li></ol><h3>5.2. World models and JEPA</h3><p>Instead of LLMs, LeCun pushes the idea of <strong>world models</strong> – systems that:</p><ul data-rte-list="default"><li><p>learn by watching the world (especially video)</p></li><li><p>build an internal representation of objects, space and time</p></li><li><p>can <strong>predict what will happen next</strong> in that world, not just what word comes next</p></li></ul><p>One of the architectures he’s working on is <strong>JEPA – Joint Embedding Predictive Architecture</strong>:</p><ul data-rte-list="default"><li><p>it learns representations by predicting future embeddings rather than raw pixels or text</p></li><li><p>it’s designed to scale to complex, high-dimensional input like video</p></li><li><p>the goal is a model that can support <strong>persistent memory, reasoning and planning</strong></p></li></ul><h3>5.3. Four pillars of future AI</h3><p>LeCun often describes four pillars any truly intelligent system needs:</p><ol data-rte-list="default"><li><p><strong>Understanding of the physical world</strong></p></li><li><p><strong>Persistent memory</strong></p></li><li><p><strong>Reasoning</strong></p></li><li><p><strong>Planning</strong></p></li></ol><p>His argument is that today’s LLM-centric systems mostly <strong>hack around</strong> these requirements instead of solving them directly. That’s why he’s increasingly focused on world-model architectures instead of bigger text models.</p><h2>6. Sutskever vs. LeCun: Same Diagnosis, Different Cure</h2><p>What’s fascinating is that Sutskever and LeCun <strong>agree on the problem</strong>:</p><ul data-rte-list="default"><li><p>current LLMs and scaling strategies are <strong>hitting limits</strong></p></li><li><p>simply adding more parameters and data is delivering <strong>diminishing returns</strong></p></li><li><p>new ideas are required</p></li></ul><p>Where they differ is <strong>how radical the change needs to be</strong>:</p><ul data-rte-list="default"><li><p><strong>Sutskever</strong> seems to believe that the next breakthroughs will still come from the same general family of models – big neural nets trained on massive datasets – but with better objectives, better generalization, and much stronger safety work.</p></li><li><p><strong>LeCun</strong> believes we need a <strong>new paradigm</strong>: world models that learn from interaction with the environment, closer to how animals and humans learn.</p></li></ul><p>For people building on today’s models, that tension is actually good news: it means there is still a lot of frontier left.</p><h2>7. What All This Means for Developers and Founders</h2><p>So what should you do if you’re not running an AI lab, but you <em>are</em> building products on top of OpenAI, Anthropic, Google, Meta, etc.?</p><h3>7.1. Hardware is becoming less of a moat</h3><p>If the next big gains won’t come from simply scaling, then:</p><ul data-rte-list="default"><li><p>the advantage of “we have more GPUs than you” decreases over time</p></li><li><p>your real edge comes from <strong>use cases, data, UX and integration</strong>, not raw model size</p></li></ul><p>This is good for startups and agencies: you can piggyback on the big models and still differentiate.</p><h3>7.2. Benchmarks are not your product</h3><p>Both Sutskever’s and LeCun’s critiques are a warning against obsessing over leaderboards.</p><p>Ask yourself:</p><ul data-rte-list="default"><li><p>Does this improvement meaningfully change what my users can do?</p></li><li><p>Does it reduce hallucinations in <strong>their</strong> workflows?</p></li><li><p>Does it make the system more reliable, debuggable and explainable?</p></li></ul><p>User-centric metrics matter more than another +2% on some synthetic reasoning benchmark.</p><h3>7.3. Expect more diversity in model types</h3><p>If LeCun’s world models, JEPA-style architectures, or other alternatives start to work, we’ll likely see:</p><ul data-rte-list="default"><li><p>specialized models for <strong>physical reasoning and robotics</strong></p></li><li><p>LLMs acting as a <em>language interface</em> over deeper systems that actually handle planning and environment modeling</p></li><li><p>more hybrid stacks, where multiple models collaborate</p></li></ul><p>For developers, that means learning to <strong>orchestrate multiple systems</strong> instead of just calling one chat completion endpoint.</p><h3>7.4. Data, workflows and feedback loops are where you win</h3><p>No matter who is right about the far future, one thing is clear for product builders:</p><ul data-rte-list="default"><li><p>Owning <strong>high-quality domain data</strong></p></li><li><p>Designing <strong>tight feedback loops</strong> between users and models</p></li><li><p>Building <strong>evaluations that match your use case</strong></p></li></ul><p>…will matter more than anything else.</p><p>You don’t need to solve world modeling or superintelligence yourself. You need to:</p><ul data-rte-list="default"><li><p>pick the right model(s) for the job</p></li><li><p>wrap them in workflows that make sense for your users</p></li><li><p>keep improving based on real-world behavior</p></li></ul><h2>8. A Quiet Turning Point</h2><p>In 2019–2021, the story of AI was simple: <strong>“scale is all you need.”</strong> Bigger models, more data, more GPUs.</p><p>Now, two of the field’s most influential figures are effectively saying:</p><ul data-rte-list="default"><li><p>scaling is <strong>not enough</strong> (Sutskever)</p></li><li><p>LLMs themselves may be a <strong>dead end for real intelligence</strong> (LeCun)</p></li></ul><p>We’re entering a new phase where research, theory and new architectures matter again as much as infrastructure.</p><p>For builders, that doesn’t mean you should stop using LLMs or pause your AI roadmap. It means:</p><ul data-rte-list="default"><li><p>focus less on chasing the next parameter count</p></li><li><p>focus more on <strong>how</strong> intelligence shows up inside your product: reliability, reasoning, planning, and how it fits into real human workflows</p></li></ul><p>The GPU race gave us today’s tools. The next decade will be defined by what we <strong>do</strong> with them – and by the new ideas that finally move us beyond “predict the next token.”</p>
</div>




















  
  



</div></div></div>
  </body>
</html>
