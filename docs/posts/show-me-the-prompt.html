<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hamel.dev/blog/posts/prompt/">Original</a>
    <h1>Show me the prompt</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">Table Of Contents</h2>
   
  <ul>
  <li><a href="#background" id="toc-background">Background</a></li>
  <li><a href="#motivation-minimize-accidental-complexity" id="toc-motivation-minimize-accidental-complexity">Motivation: Minimize accidental complexity</a></li>
  <li><a href="#intercepting-llm-api-calls" id="toc-intercepting-llm-api-calls">Intercepting LLM API calls</a>
  <ul>
  <li><a href="#setting-up-mitmproxy" id="toc-setting-up-mitmproxy">Setting Up mitmproxy</a></li>
  <li><a href="#environment-variables-for-python" id="toc-environment-variables-for-python">Environment variables for Python</a></li>
  </ul></li>
  <li><a href="#examples" id="toc-examples">Examples</a>
  <ul>
  <li><a href="#guardrails" id="toc-guardrails">Guardrails</a></li>
  <li><a href="#guidance" id="toc-guidance">Guidance</a></li>
  <li><a href="#langchain" id="toc-langchain">Langchain</a></li>
  <li><a href="#instructor" id="toc-instructor">Instructor</a></li>
  <li><a href="#dspy" id="toc-dspy">DSPy</a></li>
  </ul></li>
  <li><a href="#my-personal-experience" id="toc-my-personal-experience">My Personal Experience</a></li>
  </ul>
</nav>
<section id="background">
<h2 data-anchor-id="background">Background</h2>
<p>There are many libraries that aim to make the output of your LLMs better by <strong>re-writing or constructing the prompt for you</strong>. These libraries purport to make the output of your LLMs:</p>
<ul>
<li>safer <a href="https://github.com/guardrails-ai/guardrails">(ex: guardrails)</a></li>
<li>deterministic <a href="https://github.com/guidance-ai/guidance">(ex: guidance)</a></li>
<li>structured <a href="https://github.com/jxnl/instructor">(ex: instructor)</a></li>
<li>resilient <a href="https://www.langchain.com/">(ex: langchain)</a></li>
<li>… or even optimized for an arbitrary metric <a href="https://github.com/stanfordnlp/dspy">(ex: DSPy)</a>.</li>
</ul>
<p>A common theme among <em>some</em> of these tools is they encourage users to disintermediate themselves from prompting.</p>
<blockquote>
<p><a href="https://github.com/stanfordnlp/dspy">DSPy</a>: “This is a new paradigm in which LMs and their prompts fade into the background …. you can compile your program again DSPy will create new effective prompts”</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/guidance-ai/guidance">guidance</a> “guidance is a programming paradigm that offers superior control and efficiency compared to conventional prompting …”</p>
</blockquote>
<p>Even when tools don’t discourage prompting, I’ve often found it difficult to retrieve the final prompt(s) these tools send to the language model. <strong>The prompts sent by these tools to the LLM is a natural language description of what these tools are doing, and is the fastest way to understand how they work.</strong> Furthermore, some tools have <a href="https://github.com/stanfordnlp/dspy?tab=readme-ov-file#4-two-powerful-concepts-signatures--teleprompters">dense terminology</a> to describe internal constructs which can further obfuscate what they are doing.</p>
<p>For reasons I’ll explain below, I think most people would benefit from the following mindset:</p>
<div>
<figure>
<p><img src="https://hamel.dev/blog/posts/prompt/slap_3.jpeg"/></p>
</figure>
</div>
<p>In this blog post, I’ll show you how you can <strong>intercept API calls w/prompts for any tool, without having to fumble through docs or read source code.</strong> I’ll show you how to setup and operate <a href="https://mitmproxy.org/">mitmproxy</a> with examples from the LLM the tools I previously mentioned.</p>
</section>
<section id="motivation-minimize-accidental-complexity">
<h2 data-anchor-id="motivation-minimize-accidental-complexity">Motivation: Minimize accidental complexity</h2>
<p>Before adopting an abstraction, its important to consider the dangers of taking on <a href="https://dev.to/alexbunardzic/software-complexity-essential-accidental-and-incidental-3i4d">accidental complexity</a>. This danger is acute for LLM abstractions relative to programming abstractions. With LLM abstractions, we often force the user to regress towards write coding instead of conversing with the AI in natural language, which can run counter to the purpose of LLMs:</p>
<center>
<blockquote>
<p lang="en" dir="ltr">
Programming abstraction -&gt; a human-like language you can use to translate your task into machine code</p>
— Hamel Husain (<span data-cites="HamelHusain">@HamelHusain</span>) <a href="https://twitter.com/HamelHusain/status/1754315254413361553">February 5, 2024</a>
</blockquote>

</center>
<p>While this is a cheeky comment, it’s worth keeping this in mind while evaluating tools. There are two primary types of automation that tools provide:</p>
<ul>
<li><strong>Interleaving code and LLMs:</strong> Expressing this automation is often best done through code, since code must be run to carry out the task. Examples include routing, executing functions, retries, chaining, etc.</li>
<li><strong>Re-Writing and constructing prompts</strong>: Expressing your intent is often best done through natural language. However, there are exceptions! For example, it is convenient to express a function definition or schema from code instead of natural language.</li>
</ul>
<p>Many frameworks offer both types of automation. However, going too far with the second type can have negative consequences. Seeing the prompt allows you decide:</p>
<ol type="1">
<li>Is this framework really necessary?</li>
<li>Should I just steal the final prompt (a string) and jettison the framework?</li>
<li>Can we write a better prompt than this (shorter, aligned with your intent, etc)?</li>
<li>Is this the best approach (do the # of API calls seem appropriate)?</li>
</ol>
<p>In my experience, seeing the prompts and API calls are essential to making informed decisions.</p>
</section>
<section id="intercepting-llm-api-calls">
<h2 data-anchor-id="intercepting-llm-api-calls">Intercepting LLM API calls</h2>
<p>There are many possible ways to intercept LLM API calls, such as monkey patching source code or finding a user-facing option. I’ve found that those approaches take far too much time since the quality of source code and documentation can vary greatly. After all, I just want to see API calls without worrying about how the code works!</p>
<p>A framework agnostic way to see API calls is to setup a proxy that logs your outgoing API requests. This is easy to do with <a href="https://mitmproxy.org/">mitmproxy</a>, an free, open-source HTTPS proxy.</p>
<section id="setting-up-mitmproxy">
<h3 data-anchor-id="setting-up-mitmproxy">Setting Up mitmproxy</h3>
<p>This is an opinionated way to setup <code>mitmproxy</code>that’s beginner-friendly for our intended purposes:</p>
<ol type="1">
<li><p>Follow the installation instructions <a href="https://mitmproxy.org/">on the website</a></p></li>
<li><p>Start the interactive UI by running <code>mitmweb</code> in the terminal. Pay attention to the url of the interactive UI in the logs which will look something like this: <code>Web server listening at http://127.0.0.1:8081/</code></p></li>
<li><p>Next, you need to configure your device (i.e. your laptop) to route all traffic through <code>mitproxy</code>, which listens on <code>http://localhost:8080</code>. Per the documentation:</p>
<blockquote>
<p>We recommend to simply search the web on how to configure an HTTP proxy for your system. Some operating system have a global settings, some browser have their own, other applications use environment variables, etc.</p>
</blockquote>
<p>In my case, A <a href="https://www.google.com/search?q=set+proxy+for+macos&amp;sca_esv=c51a80de1a7d45f0&amp;rlz=1C5CHFA_enUS1048US1049&amp;sxsrf=ACQVn0_ysjr6Kma2_lX8WbB06iPbDi5gUQ%3A1707764982232&amp;ei=9mzKZYXoDcfy0PEPpJqb2Ao&amp;ved=0ahUKEwiFu4CpwKaEAxVHOTQIHSTNBqsQ4dUDCBA&amp;uact=5&amp;oq=set+proxy+for+macos&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiE3NldCBwcm94eSBmb3IgbWFjb3MyBBAjGCcyBhAAGBYYHjIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjILEAAYgAQYigUYhgMyCxAAGIAEGIoFGIYDSMk-UMU7WMU7cAd4AZABAJgBVaABVaoBATG4AQPIAQD4AQHCAgoQABhHGNYEGLAD4gMEGAAgQYgGAZAGCA&amp;sclient=gws-wiz-serp">google search for “set proxy for macos”</a> returned these results:</p>
<blockquote>
<p>choose Apple menu &gt; System Settings, click Network in the sidebar, click a network service on the right, click Details, then click Proxies.</p>
</blockquote>
<p>I then insert <code>localhost</code> and <code>8080</code> in the following places in the UI:</p>
<div>
<figure>
<p><img src="https://hamel.dev/blog/posts/prompt/mac.png"/></p>
</figure>
</div></li>
<li><p>Next, navigate to <a href="http://mitm.it">http://mitm.it</a> and it will give you instructions on how to install the mitmproxy Certificate Authority (CA), which you will need for intercepting HTTPS requests. (You can also do this manually <a href="https://docs.mitmproxy.org/stable/concepts-certificates/#quick-setup">here</a>.) Also, take note of the location of the CA file as we will reference it later.</p></li>
<li><p>You can test that everything works by browsing to a website like <a href="https://mitmproxy.org/">https://mitmproxy.org/</a>, and seeing the corresponding output in the mtimweb UI which for me is located at <a href="http://127.0.0.1:8081/">http://127.0.0.1:8081/</a> (look at the logs in your terminal to get the URL).</p></li>
<li><p>Now that you set everything up, you can disable the proxy that you previously enabled on your network. I do this on my mac by toggling the proxy buttons in the screenshot I showed above. This is because we want to scope the proxy to only the python program to eliminate unnecessary noise.</p></li>
</ol>
<div>

<p>Networking related software commonly allows you to proxy outgoing requests by setting environment variables. This is the approach we will use to scope our proxy to specific Python programs. However, I encourage you to play with other types of programs to see what you find after you are comfortable!</p>
</div>
</section>
<section id="environment-variables-for-python">
<h3 data-anchor-id="environment-variables-for-python">Environment variables for Python</h3>
<p>We need to set the following environment variables so that the <code>requests</code> and <code>httpx</code> libraries will direct traffic to the proxy and reference the CA file for HTTPS traffic:</p>
<div>

<p>Make sure you set these environment variables before running any of the code snippets in this blog post.</p>
</div>
<div id="07c4617c-1d52-4c99-9bed-645efd73ba71">
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span># The location of my CA File</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>cert_file <span>=</span> <span>&#39;/Users/hamel/Downloads/mitmproxy-ca-cert.pem&#39;</span> </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>os.environ[<span>&#39;REQUESTS_CA_BUNDLE&#39;</span>] <span>=</span> cert_file</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span>&#39;SSL_CERT_FILE&#39;</span>] <span>=</span> cert_file</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>os.environ[<span>&#39;HTTPS_PROXY&#39;</span>] <span>=</span> <span>&#39;http://127.0.0.1:8080&#39;</span></span></code></pre></div>
</div>
<p>You can do a minimal test by running the following code:</p>
<div id="faf4a70a-bde5-47c0-b649-3179c233c189">
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>import</span> requests</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>requests.post(<span>&#39;https://httpbin.org/post&#39;</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>              data<span>=</span>{<span>&#39;key&#39;</span>: <span>&#39;value&#39;</span>})</span></code></pre></div>

</div>
<p>This will appear in the UI like so:</p>
<p><img src="https://hamel.dev/blog/posts/prompt/mitm_01.png"/></p>
</section>
</section>
<section id="examples">
<h2 data-anchor-id="examples">Examples</h2>
<p>Now for the fun part, let’s run through some examples of LLM libraries and intercept their API calls!</p>
<section id="guardrails">
<h3 data-anchor-id="guardrails">Guardrails</h3>
<p>Guardrails allows you specify structure and types, which it uses to validate and correct the outputs of large language models. This is a hello world example from the <a href="https://github.com/guardrails-ai/guardrails"><code>guardrails-ai/guardrails</code> README</a>:</p>
<div id="c746dbe7-e96f-4ea0-89d5-ca9b0b0685e8">
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>from</span> pydantic <span>import</span> BaseModel, Field</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span>from</span> guardrails <span>import</span> Guard</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span>import</span> openai</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span>class</span> Pet(BaseModel):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    pet_type: <span>str</span> <span>=</span> Field(description<span>=</span><span>&#34;Species of pet&#34;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    name: <span>str</span> <span>=</span> Field(description<span>=</span><span>&#34;a unique pet name&#34;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>prompt <span>=</span> <span>&#34;&#34;&#34;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span>    What kind of pet should I get and what should I name it?</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span>    $</span><span>{gr.complete_json_suffix_v2}</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span>&#34;&#34;&#34;</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>guard <span>=</span> Guard.from_pydantic(output_class<span>=</span>Pet, prompt<span>=</span>prompt)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>validated_output, <span>*</span>rest <span>=</span> guard(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    llm_api<span>=</span>openai.completions.create,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    engine<span>=</span><span>&#34;gpt-3.5-turbo-instruct&#34;</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>f&#34;</span><span>{</span>validated_output<span>}</span><span>&#34;</span>)</span></code></pre></div>
<div>
<pre><code>{
    &#34;pet_type&#34;: &#34;dog&#34;,
    &#34;name&#34;: &#34;Buddy</code></pre>
</div>
</div>
<p>What is happening here? How is this structured output and validation working? Looking at the mitmproxy UI, I can see that the above code resulted in two LLM API calls, the first one with this prompt:</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>What kind of pet should I get and what should I name it?</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Given below is XML that describes the information to extract from this document and the tags to extract it into.</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>&lt;output&gt;</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    &lt;string name=&#34;pet_type&#34; description=&#34;Species of pet&#34;/&gt;</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    &lt;string name=&#34;name&#34; description=&#34;a unique pet name&#34;/&gt;</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>&lt;/output&gt;</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML&#39;s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>Here are examples of simple (XML, JSON) pairs that show the expected behavior:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>- `&lt;string name=&#39;foo&#39; format=&#39;two-words lower-case&#39; /&gt;` =&gt; `{&#39;foo&#39;: &#39;example one&#39;}`</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>- `&lt;list name=&#39;bar&#39;&gt;&lt;string format=&#39;upper-case&#39; /&gt;&lt;/list&gt;` =&gt; `{&#34;bar&#34;: [&#39;STRING ONE&#39;, &#39;STRING TWO&#39;, etc.]}`</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>- `&lt;object name=&#39;baz&#39;&gt;&lt;string name=&#34;foo&#34; format=&#34;capitalize two-words&#34; /&gt;&lt;integer name=&#34;index&#34; format=&#34;1-indexed&#34; /&gt;&lt;/object&gt;` =&gt; `{&#39;baz&#39;: {&#39;foo&#39;: &#39;Some String&#39;, &#39;index&#39;: 1}}`</span></code></pre></div>
<p><strong>Followed by another call with this prompt:</strong></p>
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>I was given the following response, which was not parseable as JSON.</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>&#34;{\n    \&#34;pet_type\&#34;: \&#34;dog\&#34;,\n    \&#34;name\&#34;: \&#34;Buddy&#34;</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>Help me correct this by making it valid JSON.</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>Given below is XML that describes the information to extract from this document and the tags to extract it into.</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>&lt;output&gt;</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    &lt;string name=&#34;pet_type&#34; description=&#34;Species of pet&#34;/&gt;</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    &lt;string name=&#34;name&#34; description=&#34;a unique pet name&#34;/&gt;</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>&lt;/output&gt;</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML&#39;s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.</span></code></pre></div>
<p>Woof. That’s a whole lot of ceremony to get structured output! We learned that this library’s approach to structured output uses XML schemas (while others use function calling). It’s worth considering if you can fashion a better or simpler approach now that the magic has been lifted. Either way, we now have insight into how it works without dragging you into unnecessary complexity, which is a win.</p>
</section>
<section id="guidance">
<h3 data-anchor-id="guidance">Guidance</h3>
<p>Guidance offers constrained generation and programming constructs for writing prompts. Let’s dive into a <a href="https://github.com/guidance-ai/guidance/blob/main/notebooks/tutorials/chat.ipynb">chat example from their tutorials</a>:</p>
<div id="cfe6882f-e22c-4666-9a66-6ce41262c6ea">
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span>import</span> guidance</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>gpt35 <span>=</span> guidance.models.OpenAI(<span>&#34;gpt-3.5-turbo&#34;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span>import</span> re</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span>from</span> guidance <span>import</span> gen, select, system, user, assistant</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span>@guidance</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span>def</span> plan_for_goal(lm, goal: <span>str</span>):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span># This is a helper function which we will use below</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span>def</span> parse_best(prosandcons, options):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        best <span>=</span> re.search(<span>r&#39;Best=(\d+)&#39;</span>, prosandcons)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> best:</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            best <span>=</span>  re.search(<span>r&#39;Best.*?(\d+)&#39;</span>, <span>&#39;Best= option is 3&#39;</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span>if</span> best:</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            best <span>=</span> <span>int</span>(best.group(<span>1</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span>else</span>:</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            best <span>=</span> <span>0</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span>return</span> options[best]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span># Some general instruction to the model</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span>with</span> system():</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        lm <span>+=</span> <span>&#34;You are a helpful assistant.&#34;</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span># Simulate a simple request from the user</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span># Note that we switch to using &#39;lm2&#39; here, because these are intermediate steps (so we don&#39;t want to overwrite the current lm object)</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span>with</span> user():</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        lm2 <span>=</span> lm <span>+</span> <span>f&#34;&#34;&#34;</span><span>\</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span>        I want to </span><span>{</span>goal<span>}</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span>        Can you please generate one option for how to accomplish this?</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span>        Please make the option very short, at most one line.&#34;&#34;&#34;</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span># Generate several options. Note that this means several sequential generation requests</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    n_options <span>=</span> <span>5</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span>with</span> assistant():</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        options <span>=</span> []</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span>for</span> i <span>in</span> <span>range</span>(n_options):</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>            options.append((lm2 <span>+</span> gen(name<span>=</span><span>&#39;option&#39;</span>, temperature<span>=</span><span>1.0</span>, max_tokens<span>=</span><span>50</span>))[<span>&#34;option&#34;</span>])</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span># Have the user request pros and cons</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span>with</span> user():</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        lm2 <span>+=</span> <span>f&#34;&#34;&#34;</span><span>\</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span>        I want to </span><span>{</span>goal<span>}</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span>        Can you please comment on the pros and cons of each of the following options, and then pick the best option?</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span>        ---</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span>        &#34;&#34;&#34;</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        <span>for</span> i, opt <span>in</span> <span>enumerate</span>(options):</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>            lm2 <span>+=</span> <span>f&#34;Option </span><span>{</span>i<span>}</span><span>: </span><span>{</span>opt<span>}</span><span>\n</span><span>&#34;</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        lm2 <span>+=</span> <span>f&#34;&#34;&#34;</span><span>\</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span>        ---</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span>        Please discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the number of the best option.&#34;&#34;&#34;</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    <span># Get the pros and cons from the model</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span>with</span> assistant():</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        lm2 <span>+=</span> gen(name<span>=</span><span>&#39;prosandcons&#39;</span>, temperature<span>=</span><span>0.0</span>, max_tokens<span>=</span><span>600</span>, stop<span>=</span><span>&#34;Best=&#34;</span>) <span>+</span> <span>&#34;Best=&#34;</span> <span>+</span> gen(<span>&#34;best&#34;</span>, regex<span>=</span><span>&#34;[0-9]+&#34;</span>) </span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    <span># The user now extracts the one selected as the best, and asks for a full plan</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    <span># We switch back to &#39;lm&#39; because this is the final result we want</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span>with</span> user():</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        lm <span>+=</span> <span>f&#34;&#34;&#34;</span><span>\</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a><span>        I want to </span><span>{</span>goal<span>}</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a><span>        Here is my plan: </span><span>{</span>options[<span>int</span>(lm2[<span>&#34;best&#34;</span>])]<span>}</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span>        Please elaborate on this plan, and tell me how to best accomplish it.&#34;&#34;&#34;</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    <span># The plan is generated</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    <span>with</span> assistant():</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>        lm <span>+=</span> gen(name<span>=</span><span>&#39;plan&#39;</span>, max_tokens<span>=</span><span>500</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    <span>return</span> lm</span></code></pre></div>
</div>
<div id="7cbd50af-7689-426c-8779-1b0d5e9bfe7a">
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>results <span>=</span> gpt35 <span>+</span> plan_for_goal(goal<span>=</span><span>&#34;read more books&#34;</span>)</span></code></pre></div>
<div>
<pre><div><p>system</p><p>You are a helpful assistant.</p></div><div><p>user</p><p>I want to read more books
Here is my plan: Set aside 30 minutes of dedicated reading time each day.
Please elaborate on this plan, and tell me how to best accomplish it.</p></div><div><p>assistant</p><p><span title="1.0">Setting</span><span title="1.0"> aside</span><span title="1.0"> </span><span title="1.0">30</span><span title="1.0"> minutes</span><span title="1.0"> of</span><span title="1.0"> dedicated</span><span title="1.0"> reading</span><span title="1.0"> time</span><span title="1.0"> each</span><span title="1.0"> day</span><span title="1.0"> is</span><span title="1.0"> a</span><span title="1.0"> great</span><span title="1.0"> plan</span><span title="1.0"> to</span><span title="1.0"> read</span><span title="1.0"> more</span><span title="1.0"> books</span><span title="1.0">.</span><span title="1.0"> Here</span><span title="1.0"> are</span><span title="1.0"> some</span><span title="1.0"> tips</span><span title="1.0"> to</span><span title="1.0"> help</span><span title="1.0"> you</span><span title="1.0"> accomplish</span><span title="1.0"> this</span><span title="1.0"> goal</span><span title="1.0">:

</span><span title="1.0">1</span><span title="1.0">.</span><span title="1.0"> Establish</span><span title="1.0"> a</span><span title="1.0"> routine</span><span title="1.0">:</span><span title="1.0"> Choose</span><span title="1.0"> a</span><span title="1.0"> specific</span><span title="1.0"> time</span><span title="1.0"> of</span><span title="1.0"> day</span><span title="1.0"> that</span><span title="1.0"> works</span><span title="1.0"> best</span><span title="1.0"> for</span><span title="1.0"> you</span><span title="1.0">,</span><span title="1.0"> whether</span><span title="1.0"> it</span><span title="1.0">&#39;s</span><span title="1.0"> in</span><span title="1.0"> the</span><span title="1.0"> morning</span><span title="1.0">,</span><span title="1.0"> during</span><span title="1.0"> lunch</span><span title="1.0"> break</span><span title="1.0">,</span><span title="1.0"> or</span><span title="1.0"> before</span><span title="1.0"> bed</span><span title="1.0">.</span><span title="1.0"> Cons</span><span title="1.0">istency</span><span title="1.0"> is</span><span title="1.0"> key</span><span title="1.0"> to</span><span title="1.0"> forming</span><span title="1.0"> a</span><span title="1.0"> habit</span><span title="1.0">.

</span><span title="1.0">2</span><span title="1.0">.</span><span title="1.0"> Create</span><span title="1.0"> a</span><span title="1.0"> reading</span><span title="1.0">-friendly</span><span title="1.0"> environment</span><span title="1.0">:</span><span title="1.0"> Find</span><span title="1.0"> a</span><span title="1.0"> quiet</span><span title="1.0"> and</span><span title="1.0"> comfortable</span><span title="1.0"> spot</span><span title="1.0"> where</span><span title="1.0"> you</span><span title="1.0"> can</span><span title="1.0"> focus</span><span title="1.0"> on</span><span title="1.0"> your</span><span title="1.0"> reading</span><span title="1.0"> without</span><span title="1.0"> distractions</span><span title="1.0">.</span><span title="1.0"> It</span><span title="1.0"> could</span><span title="1.0"> be</span><span title="1.0"> a</span><span title="1.0"> cozy</span><span title="1.0"> corner</span><span title="1.0"> in</span><span title="1.0"> your</span><span title="1.0"> home</span><span title="1.0">,</span><span title="1.0"> a</span><span title="1.0"> park</span><span title="1.0"> bench</span><span title="1.0">,</span><span title="1.0"> or</span><span title="1.0"> a</span><span title="1.0"> local</span><span title="1.0"> library</span><span title="1.0">.

</span><span title="1.0">3</span><span title="1.0">.</span><span title="1.0"> Mini</span><span title="1.0">mi</span><span title="1.0">ze</span><span title="1.0"> distractions</span><span title="1.0">:</span><span title="1.0"> Put</span><span title="1.0"> away</span><span title="1.0"> your</span><span title="1.0"> phone</span><span title="1.0">,</span><span title="1.0"> turn</span><span title="1.0"> off</span><span title="1.0"> the</span><span title="1.0"> TV</span><span title="1.0">,</span><span title="1.0"> and</span><span title="1.0"> avoid</span><span title="1.0"> any</span><span title="1.0"> other</span><span title="1.0"> potential</span><span title="1.0"> interruptions</span><span title="1.0"> during</span><span title="1.0"> your</span><span title="1.0"> dedicated</span><span title="1.0"> reading</span><span title="1.0"> time</span><span title="1.0">.</span><span title="1.0"> This</span><span title="1.0"> will</span><span title="1.0"> help</span><span title="1.0"> you</span><span title="1.0"> stay</span><span title="1.0"> focused</span><span title="1.0"> and</span><span title="1.0"> fully</span><span title="1.0"> immer</span><span title="1.0">se</span><span title="1.0"> yourself</span><span title="1.0"> in</span><span title="1.0"> the</span><span title="1.0"> book</span><span title="1.0">.

</span><span title="1.0">4</span><span title="1.0">.</span><span title="1.0"> Choose</span><span title="1.0"> books</span><span title="1.0"> that</span><span title="1.0"> interest</span><span title="1.0"> you</span><span title="1.0">:</span><span title="1.0"> Select</span><span title="1.0"> books</span><span title="1.0"> that</span><span title="1.0"> align</span><span title="1.0"> with</span><span title="1.0"> your</span><span title="1.0"> personal</span><span title="1.0"> interests</span><span title="1.0">,</span><span title="1.0"> hobbies</span><span title="1.0">,</span><span title="1.0"> or</span><span title="1.0"> goals</span><span title="1.0">.</span><span title="1.0"> When</span><span title="1.0"> you</span><span title="1.0">&#39;re</span><span title="1.0"> genuinely</span><span title="1.0"> interested</span><span title="1.0"> in</span><span title="1.0"> the</span><span title="1.0"> subject</span><span title="1.0"> matter</span><span title="1.0">,</span><span title="1.0"> you</span><span title="1.0">&#39;ll</span><span title="1.0"> be</span><span title="1.0"> more</span><span title="1.0"> motivated</span><span title="1.0"> to</span><span title="1.0"> read</span><span title="1.0"> regularly</span><span title="1.0">.

</span><span title="1.0">5</span><span title="1.0">.</span><span title="1.0"> Start</span><span title="1.0"> with</span><span title="1.0"> manageable</span><span title="1.0"> goals</span><span title="1.0">:</span><span title="1.0"> If</span><span title="1.0"> you</span><span title="1.0">&#39;re</span><span title="1.0"> new</span><span title="1.0"> to</span><span title="1.0"> reading</span><span title="1.0"> or</span><span title="1.0"> have</span><span title="1.0"> a</span><span title="1.0"> busy</span><span title="1.0"> schedule</span><span title="1.0">,</span><span title="1.0"> start</span><span title="1.0"> with</span><span title="1.0"> a</span><span title="1.0"> smaller</span><span title="1.0"> time</span><span title="1.0"> commitment</span><span title="1.0">,</span><span title="1.0"> such</span><span title="1.0"> as</span><span title="1.0"> </span><span title="1.0">15</span><span title="1.0"> minutes</span><span title="1.0">,</span><span title="1.0"> and</span><span title="1.0"> gradually</span><span title="1.0"> increase</span><span title="1.0"> it</span><span title="1.0"> to</span><span title="1.0"> </span><span title="1.0">30</span><span title="1.0"> minutes</span><span title="1.0"> or</span><span title="1.0"> more</span><span title="1.0"> as</span><span title="1.0"> you</span><span title="1.0"> become</span><span title="1.0"> more</span><span title="1.0"> comfortable</span><span title="1.0">.

</span><span title="1.0">6</span><span title="1.0">.</span><span title="1.0"> Set</span><span title="1.0"> a</span><span title="1.0"> timer</span><span title="1.0">:</span><span title="1.0"> Use</span><span title="1.0"> a</span><span title="1.0"> timer</span><span title="1.0"> or</span><span title="1.0"> a</span><span title="1.0"> reading</span><span title="1.0"> app</span><span title="1.0"> that</span><span title="1.0"> allows</span><span title="1.0"> you</span><span title="1.0"> to</span><span title="1.0"> track</span><span title="1.0"> your</span><span title="1.0"> reading</span><span title="1.0"> time</span><span title="1.0">.</span><span title="1.0"> This</span><span title="1.0"> will</span><span title="1.0"> help</span><span title="1.0"> you</span><span title="1.0"> stay</span><span title="1.0"> accountable</span><span title="1.0"> and</span><span title="1.0"> ensure</span><span title="1.0"> that</span><span title="1.0"> you</span><span title="1.0"> dedicate</span><span title="1.0"> the</span><span title="1.0"> full</span><span title="1.0"> </span><span title="1.0">30</span><span title="1.0"> minutes</span><span title="1.0"> to</span><span title="1.0"> reading</span><span title="1.0">.

</span><span title="1.0">7</span><span title="1.0">.</span><span title="1.0"> Make</span><span title="1.0"> reading</span><span title="1.0"> enjoyable</span><span title="1.0">:</span><span title="1.0"> Create</span><span title="1.0"> a</span><span title="1.0"> cozy</span><span title="1.0"> reading</span><span title="1.0"> atmosphere</span><span title="1.0"> by</span><span title="1.0"> lighting</span><span title="1.0"> a</span><span title="1.0"> candle</span><span title="1.0">,</span><span title="1.0"> sip</span><span title="1.0">ping</span><span title="1.0"> a</span><span title="1.0"> cup</span><span title="1.0"> of</span><span title="1.0"> tea</span><span title="1.0">,</span><span title="1.0"> or</span><span title="1.0"> playing</span><span title="1.0"> soft</span><span title="1.0"> background</span><span title="1.0"> music</span><span title="1.0">.</span><span title="1.0"> Eng</span><span title="1.0">aging</span><span title="1.0"> all</span><span title="1.0"> your</span><span title="1.0"> senses</span><span title="1.0"> can</span><span title="1.0"> enhance</span><span title="1.0"> your</span><span title="1.0"> reading</span><span title="1.0"> experience</span><span title="1.0">.

</span><span title="1.0">8</span><span title="1.0">.</span><span title="1.0"> Join</span><span title="1.0"> a</span><span title="1.0"> book</span><span title="1.0"> club</span><span title="1.0"> or</span><span title="1.0"> reading</span><span title="1.0"> group</span><span title="1.0">:</span><span title="1.0"> Consider</span><span title="1.0"> joining</span><span title="1.0"> a</span><span title="1.0"> book</span><span title="1.0"> club</span><span title="1.0"> or</span><span title="1.0"> participating</span><span title="1.0"> in</span><span title="1.0"> a</span><span title="1.0"> reading</span><span title="1.0"> group</span><span title="1.0"> to</span><span title="1.0"> connect</span><span title="1.0"> with</span><span title="1.0"> fellow</span><span title="1.0"> book</span><span title="1.0"> lovers</span><span title="1.0">.</span><span title="1.0"> This</span><span title="1.0"> can</span><span title="1.0"> provide</span><span title="1.0"> additional</span><span title="1.0"> motivation</span><span title="1.0">,</span><span title="1.0"> discussion</span><span title="1.0"> opportunities</span><span title="1.0">,</span><span title="1.0"> and</span><span title="1.0"> book</span><span title="1.0"> recommendations</span><span title="1.0">.

</span><span title="1.0">9</span><span title="1.0">.</span><span title="1.0"> Keep</span><span title="1.0"> a</span><span title="1.0"> reading</span><span title="1.0"> log</span><span title="1.0">:</span><span title="1.0"> Maintain</span><span title="1.0"> a</span><span title="1.0"> record</span><span title="1.0"> of</span><span title="1.0"> the</span><span title="1.0"> books</span><span title="1.0"> you</span><span title="1.0">&#39;ve</span><span title="1.0"> read</span><span title="1.0">,</span><span title="1.0"> along</span><span title="1.0"> with</span><span title="1.0"> your</span><span title="1.0"> thoughts</span><span title="1.0"> and</span><span title="1.0"> reflections</span><span title="1.0">.</span><span title="1.0"> This</span><span title="1.0"> can</span><span title="1.0"> help</span><span title="1.0"> you</span><span title="1.0"> track</span><span title="1.0"> your</span><span title="1.0"> progress</span><span title="1.0">,</span><span title="1.0"> discover</span><span title="1.0"> patterns</span><span title="1.0"> in</span><span title="1.0"> your</span><span title="1.0"> reading</span><span title="1.0"> preferences</span><span title="1.0">,</span><span title="1.0"> and</span><span title="1.0"> serve</span><span title="1.0"> as</span><span title="1.0"> a</span><span title="1.0"> source</span><span title="1.0"> of</span><span title="1.0"> inspiration</span><span title="1.0"> for</span><span title="1.0"> future</span><span title="1.0"> reading</span><span title="1.0">.

</span><span title="1.0">10</span><span title="1.0">.</span><span title="1.0"> Be</span><span title="1.0"> flexible</span><span title="1.0">:</span><span title="1.0"> While</span><span title="1.0"> it</span><span title="1.0">&#39;s</span><span title="1.0"> important</span><span title="1.0"> to</span><span title="1.0"> have</span><span title="1.0"> a</span><span title="1.0"> dedicated</span><span title="1.0"> reading</span><span title="1.0"> time</span><span title="1.0">,</span><span title="1.0"> be</span><span title="1.0"> flexible</span><span title="1.0"> and</span><span title="1.0"> adaptable</span><span title="1.0">.</span><span title="1.0"> Life</span><span title="1.0"> can</span><span title="1.0"> sometimes</span><span title="1.0"> get</span><span title="1.0"> busy</span><span title="1.0">,</span><span title="1.0"> so</span><span title="1.0"> if</span><span title="1.0"> you</span><span title="1.0"> miss</span><span title="1.0"> a</span><span title="1.0"> day</span><span title="1.0">,</span><span title="1.0"> don</span><span title="1.0">&#39;t</span><span title="1.0"> be</span><span title="1.0"> discouraged</span><span title="1.0">.</span><span title="1.0"> Simply</span><span title="1.0"> pick</span><span title="1.0"> up</span><span title="1.0"> where</span><span title="1.0"> you</span><span title="1.0"> left</span><span title="1.0"> off</span><span title="1.0"> and</span><span title="1.0"> continue</span><span title="1.0"> with</span><span title="1.0"> your</span><span title="1.0"> reading</span><span title="1.0"> routine</span><span title="1.0">.

</span><span title="1.0">Remember</span><span title="1.0">,</span><span title="1.0"> the</span><span title="1.0"> goal</span><span title="1.0"> is</span><span title="1.0"> to</span><span title="1.0"> enjoy</span><span title="1.0"> the</span><span title="1.0"> process</span><span title="1.0"> of</span><span title="1.0"> reading</span><span title="1.0"> and</span><span title="1.0"> make</span><span title="1.0"> it</span><span title="1.0"> a</span><span title="1.0"> regular</span><span title="1.0"> part</span><span title="1.0"> of</span><span title="1.0"> your</span><span title="1.0"> life</span><span title="1.0">.</span><span title="1.0"> Happy</span><span title="1.0"> reading</span><span title="1.0">!</span></p></div></pre>
</div>
</div>
<p>This looks pretty neat! But what is it doing exactly? <strong>This makes a total of 7 calls to OpenAI</strong>, which I have put in <a href="https://gist.github.com/hamelsmu/d0d75bf702e56987f35cb715f7da4d6a">this gist</a>. <strong>5 of 7 of these API calls are “internal” thoughts asking the LLM to generate ideas.</strong> Even though the temperature is set to 1.0, <strong>these “ideas” are mostly redundant.</strong> The penultimate call to OpenAI enumerates these “ideas” which I’ve included below:</p>
<div id="a3a0664f-577d-4a0c-96e7-74527036a0be">
<div>
<pre><code>I want to read more books
Can you please comment on the pros and cons of each of the following options, and then pick the best option?
---
Option 0: Set aside dedicated time each day for reading.
Option 1: Set aside 30 minutes of dedicated reading time each day.
Option 2: Set aside dedicated time each day for reading.
Option 3: Set aside dedicated time each day for reading.
Option 4: Join a book club.
---
Please discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the number of the best option.</code></pre>
</div>
</div>
<p>I know from experience that you are likely to get better results if you tell the language model to generate ideas in one shot. That way, the LLM can reference previous ideas and achieve more diversity. This is a good example of accidental complexity: its very tempting to take this design pattern and apply it blindly. This is less of a critique of this particular framework, since the code makes it clear that 5 independent calls will happen. Either way, its good idea to check your work by inspecting API calls!.</p>
</section>
<section id="langchain">
<h3 data-anchor-id="langchain">Langchain</h3>
<p>Langchain is a multi-tool for all things LLM. Lots of people rely on Langchain when get started with LLMs. Since Langchain has lots of surface area I’ll go through two examples.</p>
<section id="lcel-batching">
<h4 data-anchor-id="lcel-batching">LCEL Batching</h4>
<p>First, let’s take a look at the <a href="https://python.langchain.com/docs/expression_language/why#batch">this example</a> from their new <code>LCEL</code> (langchain expression language) guide:</p>
<div id="ec5c742c-885e-4691-a803-22c470072d4b">
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_openai <span>import</span> ChatOpenAI</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_core.prompts <span>import</span> ChatPromptTemplate</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_core.output_parsers <span>import</span> StrOutputParser</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_core.runnables <span>import</span> RunnablePassthrough</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>prompt <span>=</span> ChatPromptTemplate.from_template(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span>&#34;Tell me a short joke about </span><span>{topic}</span><span>&#34;</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>output_parser <span>=</span> StrOutputParser()</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>model <span>=</span> ChatOpenAI(model<span>=</span><span>&#34;gpt-3.5-turbo&#34;</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>chain <span>=</span> (</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    {<span>&#34;topic&#34;</span>: RunnablePassthrough()} </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span>|</span> prompt</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span>|</span> model</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span>|</span> output_parser</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="e70028f3-3f9e-482c-bc63-05fbf0632c61">
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>chain.batch([<span>&#34;ice cream&#34;</span>, <span>&#34;spaghetti&#34;</span>, <span>&#34;dumplings&#34;</span>, <span>&#34;tofu&#34;</span>, <span>&#34;pizza&#34;</span>])</span></code></pre></div>
<div>
<pre><code>[&#34;Why did the ice cream go to therapy?\n\nBecause it had too many toppings and couldn&#39;t find its flavor!&#34;,
 &#39;Why did the tomato turn red?\n\nBecause it saw the spaghetti sauce!&#39;,
 &#39;Why did the dumpling go to the bakery?\n\nBecause it kneaded some company!&#39;,
 &#39;Why did the tofu go to the party?\n\nBecause it wanted to blend in with the crowd!&#39;,
 &#39;Why did the pizza go to the wedding?\n\nBecause it wanted to be a little cheesy!&#39;]</code></pre>
</div>
</div>
<p>That’s interesting! So how does this actually work? When looking at mitmproxy, I see <em>five separate</em> API calls:</p>
<div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span>{</span> <span>&#34;messages&#34;</span><span>:</span> <span>[</span><span>{</span><span>&#34;content&#34;</span><span>:</span> <span>&#34;Tell me a short joke about spaghetti&#34;</span><span>,</span> <span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>}</span><span>]</span><span>,</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span>&#34;model&#34;</span><span>:</span> <span>&#34;gpt-3.5-turbo&#34;</span><span>,</span> <span>&#34;n&#34;</span><span>:</span> <span>1</span><span>,</span> <span>&#34;stream&#34;</span><span>:</span> <span>false</span><span>,</span> <span>&#34;temperature&#34;</span><span>:</span> <span>0</span><span>.</span><span>7</span><span>}</span></span></code></pre></div>
<div id="cb15"><pre><code><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span>{</span> <span>&#34;messages&#34;</span><span>:</span> <span>[</span><span>{</span><span>&#34;content&#34;</span><span>:</span> <span>&#34;Tell me a short joke about ice cream&#34;</span><span>,</span> <span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>}</span><span>]</span><span>,</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span>&#34;model&#34;</span><span>:</span> <span>&#34;gpt-3.5-turbo&#34;</span><span>,</span> <span>&#34;n&#34;</span><span>:</span> <span>1</span><span>,</span> <span>&#34;stream&#34;</span><span>:</span> <span>false</span><span>,</span> <span>&#34;temperature&#34;</span><span>:</span> <span>0</span><span>.</span><span>7</span><span>}</span></span></code></pre></div>
<p>…and so on for each of the five items in the list.</p>
<p>Five separate calls (albeit async) to OpenAI may not be what you want as the <a href="https://platform.openai.com/docs/guides/rate-limits/batching-requests">the OpenAI API allows batching requests</a>.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> I’ve personally hit rate limits when using LCEL in this way - its only until I looked at the API calls that I understood what was happening! (It’s easy to be mislead by the word “batch”).</p>
</section>
<section id="smartllmchain">
<h4 data-anchor-id="smartllmchain">SmartLLMChain</h4>
<p>Next I’ll focus on automation that writes prompts for you, particularly <a href="https://api.python.langchain.com/en/latest/smart_llm/langchain_experimental.smart_llm.base.SmartLLMChain.html">SmartLLMChain</a>:</p>
<div id="600b1e60-6c01-429e-834b-470730b5ea26">
<div id="cb16"><pre><code><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain.prompts <span>import</span> PromptTemplate</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_experimental.smart_llm <span>import</span> SmartLLMChain</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span>from</span> langchain_openai <span>import</span> ChatOpenAI</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>hard_question <span>=</span> <span>&#34;I have a 12 liter jug and a 6 liter jug.</span><span>\</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span>I want to measure 6 liters. How do I do it?&#34;</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>prompt <span>=</span> PromptTemplate.from_template(hard_question)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>llm <span>=</span> ChatOpenAI(temperature<span>=</span><span>0</span>, model_name<span>=</span><span>&#34;gpt-3.5-turbo&#34;</span>)</span></code></pre></div>
</div>
<div id="bdb002f1-5f1b-40a7-ad4f-c9052813ef1f">
<div id="cb17"><pre><code><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>chain <span>=</span> SmartLLMChain(llm<span>=</span>llm, prompt<span>=</span>prompt, </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                      n_ideas<span>=</span><span>2</span>, </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                      verbose<span>=</span><span>True</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>result <span>=</span> chain.run({})</span></code></pre></div>
</div>
<div id="13be7600-e9eb-4ed2-91d9-a8182858ca83">

<div>
<pre><code>Idea 1: 1. Fill the 12 liter jug completely.
2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.
3. Empty the 6 liter jug.
4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.
5. You now have 6 liters in the 6 liter jug.

Idea 2: 1. Fill the 12 liter jug completely.
2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.
3. Empty the 6 liter jug.
4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.
5. You now have 6 liters in the 6 liter jug.

Improved Answer:
1. Fill the 12 liter jug completely.
2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.
3. Empty the 6 liter jug.
4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.
5. You now have 6 liters in the 6 liter jug.

Full Answer:
To measure 6 liters using a 12 liter jug and a 6 liter jug, follow these steps:
1. Fill the 12 liter jug completely.
2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.
3. Empty the 6 liter jug.
4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.
5. You now have 6 liters in the 6 liter jug.</code></pre>
</div>
</div>
<p>Neat! So what happened exactly? While this API emits logs that show you a lot of information (available on <a href="https://gist.github.com/hamelsmu/abfb14b0af4c70e8532f9d4e0ef3e54e">this gist</a>), the API request pattern is interesting:</p>
<ol type="1">
<li><p>Two <em>seperate</em> api calls for each “idea”.</p></li>
<li><p>Another API call that incorporates the two ideas as context, with the prompt:</p>
<blockquote>
<p>You are a researcher tasked with investigating the 2 response options provided. List the flaws and faulty logic of each answer options. Let’w work this out in a step by step way to be sure we have all the errors:”</p>
</blockquote></li>
<li><p>A final API call that that takes the critique from step 2 and generates an answer.</p></li>
</ol>
<p>Its not clear that this approach is optimal. I am not sure it should take 4 separate API calls to accomplish this task. Perhaps the critique and the final answer could be generated in one step? Furthermore, the prompt has a spelling error (<code>Let&#39;w</code>) and also overly focuses on the negative about identifying errors - which makes me skeptical that this prompt has been optimized or tested.</p>
</section>
</section>
<section id="instructor">
<h3 data-anchor-id="instructor">Instructor</h3>
<p><a href="https://github.com/jxnl/instructor">Instructor</a> is a framework for structured outputs.</p>

<section id="validation">
<h4 data-anchor-id="validation">Validation</h4>
<p>However, instructor has other APIs that are more agressive and write prompts for you. For example, consider this <a href="https://jxnl.github.io/instructor/tutorials/4-validation/">validation example</a>. Running through that example should trigger similar questions to the exploration of <a href="#SmartLLMChain">Langchain’s SmartLLMChain</a> above. In this example, you will observe 3 LLM API calls to get the right answer, with the final payload looking like this:</p>
<div id="cb22"><pre><code><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span>{</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;function_call&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span>&#34;name&#34;</span><span>:</span> <span>&#34;Validator&#34;</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span>},</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span>&#34;functions&#34;</span><span>:</span> <span>[</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span>{</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>            <span>&#34;description&#34;</span><span>:</span> <span>&#34;Validate if an attribute is correct and if not,</span><span>\n</span><span>return a new value with an error message&#34;</span><span>,</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>            <span>&#34;name&#34;</span><span>:</span> <span>&#34;Validator&#34;</span><span>,</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>            <span>&#34;parameters&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                <span>&#34;properties&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                    <span>&#34;fixed_value&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;anyOf&#34;</span><span>:</span> <span>[</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>                            <span>{</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>                                <span>&#34;type&#34;</span><span>:</span> <span>&#34;string&#34;</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>                            <span>}</span><span>,</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>                            <span>{</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>                                <span>&#34;type&#34;</span><span>:</span> <span>&#34;null&#34;</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>                            <span>}</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>                        <span>]</span><span>,</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;default&#34;</span><span>:</span> <span>null</span><span>,</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;description&#34;</span><span>:</span> <span>&#34;If the attribute is not valid, suggest a new value for the attribute&#34;</span><span>,</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;title&#34;</span><span>:</span> <span>&#34;Fixed Value&#34;</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>                    <span>},</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>                    <span>&#34;is_valid&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;default&#34;</span><span>:</span> <span>true</span><span>,</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;description&#34;</span><span>:</span> <span>&#34;Whether the attribute is valid based on the requirements&#34;</span><span>,</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;title&#34;</span><span>:</span> <span>&#34;Is Valid&#34;</span><span>,</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;type&#34;</span><span>:</span> <span>&#34;boolean&#34;</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>                    <span>},</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>                    <span>&#34;reason&#34;</span><span>:</span> <span>{</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;anyOf&#34;</span><span>:</span> <span>[</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>                            <span>{</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>                                <span>&#34;type&#34;</span><span>:</span> <span>&#34;string&#34;</span></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>                            <span>}</span><span>,</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>                            <span>{</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>                                <span>&#34;type&#34;</span><span>:</span> <span>&#34;null&#34;</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>                            <span>}</span></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>                        <span>]</span><span>,</span></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;default&#34;</span><span>:</span> <span>null</span><span>,</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;description&#34;</span><span>:</span> <span>&#34;The error message if the attribute is not valid, otherwise None&#34;</span><span>,</span></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>                        <span>&#34;title&#34;</span><span>:</span> <span>&#34;Reason&#34;</span></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>                    <span>}</span></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>                <span>},</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>                <span>&#34;required&#34;</span><span>:</span> <span>[]</span><span>,</span></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>                <span>&#34;type&#34;</span><span>:</span> <span>&#34;object&#34;</span></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>            <span>}</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>        <span>}</span></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>    <span>]</span><span>,</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>    <span>&#34;messages&#34;</span><span>:</span> <span>[</span></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>        <span>{</span></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>            <span>&#34;content&#34;</span><span>:</span> <span>&#34;You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.&#34;</span><span>,</span></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>            <span>&#34;role&#34;</span><span>:</span> <span>&#34;system&#34;</span></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>        <span>}</span><span>,</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>        <span>{</span></span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>            <span>&#34;content&#34;</span><span>:</span> <span>&#34;Does `According to some perspectives, the meaning of life is to find purpose, happiness, and fulfillment. It may vary depending on individual beliefs, values, and cultural backgrounds.` follow the rules: don&#39;t say objectionable things&#34;</span><span>,</span></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>            <span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>        <span>}</span></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>    <span>]</span><span>,</span></span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>    <span>&#34;model&#34;</span><span>:</span> <span>&#34;gpt-3.5-turbo&#34;</span><span>,</span></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>    <span>&#34;temperature&#34;</span><span>:</span> <span>0</span></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span>}</span></span></code></pre></div>
<p>Concretely, I’m curious if these steps could be collapsed into two LLM calls instead of three. Furthermore, I wonder if generic validation functions (as supplied in the above payload) are the right way to critique output? I don’t know the answer, but this is an interesting design pattern that is worth poking at.</p>
<div>

<div>
<p>As far as LLM frameworks go, I really like this one. The core functionality of defining schemas with Pydantic is very convenient. The code is also very readable and easy to understand. Despite this, I still found it helpful to intercept instructor’s API calls to get another perspective.</p>
<p>There is a way to set a logging level in instructor to see the raw API calls, however, I like using a framework agnostic approach :)</p>
</div>
</div>
</section>
</section>
<section id="dspy">
<h3 data-anchor-id="dspy">DSPy</h3>
<p><a href="https://github.com/stanfordnlp/dspy">DSPy</a> is the framework that helps you optimize your prompts to optimize any arbitrary metric. There is a fairly steep learning curve to DSPy, partly because it introduces many new technical terms specific to its framework like compilers and teleprompters. However, we can quickly peel back the complexity by looking at the API calls that it makes!</p>
<p>Let’s run the <a href="https://dspy-docs.vercel.app/docs/quick-start/minimal-example">minimal working example</a>:</p>
<div id="a3e9870b-741b-4b48-8dd4-c7b619fe8693">
<div id="cb23"><pre><code><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span>import</span> time</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span>import</span> dspy</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span>from</span> dspy.datasets.gsm8k <span>import</span> GSM8K, gsm8k_metric</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>start_time <span>=</span> time.time()</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span># Set up the LM</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>turbo <span>=</span> dspy.OpenAI(model<span>=</span><span>&#39;gpt-3.5-turbo-instruct&#39;</span>, max_tokens<span>=</span><span>250</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>dspy.settings.configure(lm<span>=</span>turbo)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span># Load math questions from the GSM8K dataset</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>gms8k <span>=</span> GSM8K()</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>trainset, devset <span>=</span> gms8k.train, gms8k.dev</span></code></pre></div>
</div>
<div id="fba9a01d-da36-43ba-a2be-2c6801f9154e">
<div id="cb24"><pre><code><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span>class</span> CoT(dspy.Module):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span>super</span>().<span>__init__</span>()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.prog <span>=</span> dspy.ChainOfThought(<span>&#34;question -&gt; answer&#34;</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span>def</span> forward(<span>self</span>, question):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>self</span>.prog(question<span>=</span>question)</span></code></pre></div>
</div>
<div id="95f7feaf-0ad0-4691-9488-a6214abc2bcd">
<div id="cb25"><pre><code><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span>from</span> dspy.teleprompt <span>import</span> BootstrapFewShotWithRandomSearch</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span># Set up the optimizer: we want to &#34;bootstrap&#34; (i.e., self-generate) 8-shot examples of our CoT program.</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span># The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>config <span>=</span> <span>dict</span>(max_bootstrapped_demos<span>=</span><span>8</span>, max_labeled_demos<span>=</span><span>8</span>, num_candidate_programs<span>=</span><span>10</span>, num_threads<span>=</span><span>4</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span># Optimize! Use the `gms8k_metric` here. In general, the metric is going to tell the optimizer how well it&#39;s doing.</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>teleprompter <span>=</span> BootstrapFewShotWithRandomSearch(metric<span>=</span>gsm8k_metric, <span>**</span>config)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>optimized_cot <span>=</span> teleprompter.<span>compile</span>(CoT(), trainset<span>=</span>trainset, valset<span>=</span>devset)</span></code></pre></div>
</div>
<div>

<p>Despite this being the official <a href="https://dspy-docs.vercel.app/docs/quick-start/minimal-example">quick-start/minimal working</a> example, this code took <strong>more than 30 minutes to run, and made hundreds of calls to OpenAI!</strong> This cost non-trivial time (and money), especially as an entry-point to the library for someone trying to take a look. There was no prior warning that this would happen.</p>
</div>
<p>DSPy made 100s of API calls because it was iteratively sampling examples for a few-shot prompt and selecting the best ones according to the <code>gsm8k_metric</code> on a validation set. I was able to quickly understand this by scanning through the API requests logged to mitmproxy.</p>
<p>DSPy offers an <code>inspect_history</code> method which allows you to see the the last <code>n</code> prompts and their completions:</p>
<div id="cb26"><pre><code><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>turbo.inspect_history(n<span>=</span><span>1</span>)</span></code></pre></div>
<p>I was able to verify that these prompts matched the last few API calls being made in mitmproxy. Overall, I would be motivated to potentially keep the prompt and and jettison the library. That being said, I think I am curious to see how this library evolves.</p>
</section>
</section>
<section id="my-personal-experience">
<h2 data-anchor-id="my-personal-experience">My Personal Experience</h2>
<p>Do I hate LLM libraries? No! I think many of the libraries in this blog post could be helpful if used thoughtfully in the right situations. However, I’ve witnessed too many people fall into the trap of using these libraries without understanding what they are doing.</p>
<p>One thing I focus on as an independent consultant is to make sure my clients don’t take on accidental complexity. It’s very tempting to adopt additional tools given all the excitement around LLMs. Looking at prompts is one way to mitigate that temptation.</p>
<p>I’m wary of frameworks that distance the human too far from LLMs. By whispering <em>“Fuck you, show me the prompt!”</em> when using these tools, you are empowered to decide for yourself.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>

<p><em>Acknowledgments: Thanks to <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a> and <a href="https://twitter.com/bclavie">Ben Clavie</a> for thoughtfully reviewing this post.</em></p>


</section>


</main> <!-- /main -->

</div></div>
  </body>
</html>
