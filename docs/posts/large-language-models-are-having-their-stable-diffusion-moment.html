<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Mar/11/llama/">Original</a>
    <h1>Large language models are having their Stable Diffusion moment</h1>
    
    <div id="readability-page-1" class="page"><div id="primary">

<div>




<p>The open release of the Stable Diffusion image generation model back in August 2022 was a key moment. I wrote how <a href="https://simonwillison.net/2022/Aug/29/stable-diffusion/">Stable Diffusion is a really big deal</a> at the time.</p>
<p>People could now generate images from text on their own hardware!</p>
<p>More importantly, developers could mess around with the guts of what was going on.</p>
<p>The resulting explosion in innovation is still going on today. Most recently, <a href="https://github.com/lllyasviel/ControlNet/blob/main/README.md">ControlNet</a> appears to have leapt Stable Diffusion ahead of Midjourney and DALL-E in terms of its capabilities.</p>
<p>It feels to me like that Stable Diffusion moment back in August kick-started the entire new wave of interest in generative AI—which was then pushed into over-drive by the release of ChatGPT at the end of November.</p>
<p>That Stable Diffusion moment is happening again right now, for large language models—the technology behind ChatGPT itself.</p>
<p>This morning <a href="https://til.simonwillison.net/llms/llama-7b-m2">I ran a GPT-3 class language model</a> on my own personal laptop for the first time!</p>
<p>AI stuff was weird already. It’s about to get a whole lot weirder.</p>
<h4>LLaMA</h4>
<p>Somewhat surprisingly, language models like GPT-3 that power tools like ChatGPT are a lot larger and more expensive to build and operate than image generation models.</p>
<p>The best of these models have mostly been built by private organizations such as OpenAI, and have been kept tightly controlled—accessible via their API and web interfaces, but not released for anyone to run on their own machines.</p>
<p>These models are also BIG. Even if you could obtain the GPT-3 model you would not be able to run it on commodity hardware—these things usually require several A100-class GPUs, each of which retail for $8,000+.</p>
<p>This technology is clearly too important to be entirely controlled by a small group of companies.</p>
<p>There have been dozens of open large language models released over the past few years, but none of them have quite hit the sweet spot for me in terms of the following:</p>
<ul>
<li>Easy to run on my own hardware</li>
<li>Large enough to be useful—ideally equivalent in capabilities to GPT-3</li>
<li>Open source enough that they can be tinkered with</li>
</ul>
<p>This all changed yesterday, thanks to the combination of Facebook’s <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">LLaMA model</a> and <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> by Georgi Gerganov.</p>
<p>Here’s the abstract from <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">the LLaMA paper</a>:</p>
<blockquote>
<p>We introduce LLaMA, a collection of founda-
tion language models ranging from 7B to 65B
parameters. We train our models on trillions
of tokens, and show that it is possible to train
state-of-the-art models using publicly available
datasets exclusively, without resorting to
proprietary and inaccessible datasets. In
particular, LLaMA-13B outperforms GPT-3
(175B) on most benchmarks, and LLaMA-65B
is competitive with the best models, Chinchilla-
70B and PaLM-540B. We release all our
models to the research community.</p>
</blockquote>
<p>It’s important to note that LLaMA isn’t fully “open”. You have to agree to <a href="https://forms.gle/jk851eBVbX1m5TAv5">some strict terms</a> to access the model. It’s intended as a research preview, and isn’t something which can be used for commercial purposes.</p>
<p>In a totally cyberpunk move, within a few days of the release, someone <a href="https://github.com/facebookresearch/llama/pull/73">submitted this PR</a> to the LLaMA repository linking to an unofficial BitTorrent download link for the model files!</p>
<p>So they’re in the wild now. You may not be legally able to build a commercial product on them, but the genie is out of the bottle. That furious typing sound you can hear is thousands of hackers around the world starting to dig in and figure out what life is like when you can run a GPT-3 class model on your own hardware.</p>
<h4>llama.cpp</h4>
<p>LLaMA on its own isn’t much good if it’s still too hard to run it on a personal laptop.</p>
<p>Enter <a href="https://ggerganov.com/">Georgi Gerganov</a>.</p>
<p>Georgi is an open source developer based in Sofia, Bulgaria (according to <a href="https://github.com/ggerganov">his GitHub profile</a>). He previously released <a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a>, a port of OpenAI’s Whisper automatic speech recognition model to C++. That project made Whisper applicable to a huge range of new use cases.</p>
<p>He’s just done the same thing with LLaMA.</p>
<p>Georgi’s <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> project had its <a href="https://github.com/ggerganov/llama.cpp/commit/26c084662903ddaca19bef982831bfb0856e8257">initial release yesterday</a>. From the README:</p>
<blockquote>
<p>The main goal is to run the model using 4-bit quantization on a MacBook.</p>
</blockquote>
<p>4-bit quantization is a technique for reducing the size of models so they can run on less powerful hardware. It also reduces the model sizes on disk—to 4GB for the 7B model and just under 8GB for the 13B one.</p>
<p>It totally works!</p>
<p>I used it to run the 7B LLaMA model on my laptop this night, and then this morning upgraded to the 13B model—the one that Facebook claim is competitive with GPT-3.</p>
<p>Here are my <a href="https://til.simonwillison.net/llms/llama-7b-m2">detailed notes on how I did that</a>—most of the information I needed was already there in the README.</p>
<p>As my laptop started to spit out text at me I genuinely had a feeling that the world was about to change, again.</p>
<p><img src="https://static.simonwillison.net/static/2023/llama-smaller.gif" alt="Animated GIF showing LLaMA on my laptop completing a prompt about The first man on the moon was - it only takes a few seconds to complete and outputs information about Neil Armstrong"/></p>
<p>I thought it would be a few more years before I could run a GPT-3 class model on hardware that I owned. I was wrong: that future is here already.</p>
<h4>Is this the worst thing that ever happened?</h4>
<p>I’m not worried about the science fiction scenarios here. The language model running on my laptop is not an AGI that’s going to <a href="https://en.wikipedia.org/wiki/The_Mitchells_vs._the_Machines">break free and take over the world</a>.</p>
<p>But there are a ton of very real ways in which this technology can be used for harm. Just a few:</p>
<ul>
<li>Generating spam</li>
<li>Automated romance scams</li>
<li>Trolling and hate speech</li>
<li>Fake news and disinformation</li>
<li>Automated radicalization (I worry about this one a lot)</li>
</ul>
<p>Not to mention that this technology makes things up exactly as easily as it parrots factual information, and provides no way to tell the difference.</p>
<p>Prior to this moment, a thin layer of defence existed in terms of companies like OpenAI having a limited ability to control how people interacted with those models.</p>
<p>Now that we can run these on our own hardware, even those controls are gone.</p>
<h4>How do we use this for good?</h4>
<p>I think this is going to have a huge impact on society. My priority is trying to direct that impact in a positive direction.</p>
<p>It’s easy to fall into a cynical trap of thinking there’s nothing good here at all, and everything generative AI is either actively harmful or a waste of time.</p>
<p>I’m personally using generative AI tools on a daily basis now for a variety of different purposes. They’ve given me a material productivity boost, but more importantly they have expanded my ambitions in terms of projects that I take on.</p>
<p>I used ChatGPT to learn enough AppleScript to ship a new project in <a href="https://til.simonwillison.net/gpt3/chatgpt-applescript">less than an hour</a> just last week!</p>
<p>I’m going to continue exploring and sharing genuinely positive applications of this technology. It’s not going to be un-invented, so I think our priority should be figuring out the most constructive possible ways to use it.</p>
<h4 id="what-to-look-for-next">What to look for next</h4>
<p>Assuming Facebook don’t relax the licensing terms, LLaMA will likely end up more a proof-of-concept that local language models are feasible on consumer hardware than a new foundation model that people use going forward.</p>
<p>The race is on to release the first fully open language model that gives people ChatGPT-like capabilities on their own devices.</p>
<p>Quoting Stable Diffusion backer <a href="https://twitter.com/EMostaque/status/1634653313089126403">Emad Mostaque</a>:</p>
<blockquote>
<p>Wouldn’t be nice if there was a fully open version eh</p>
</blockquote>

<h4>Follow my work</h4>
<p>Everything I write on my blog goes out in <a href="https://simonwillison.net/atom/everything/">my Atom feed</a>, and I have a very <a href="https://fedi.simonwilliso.net/@simon">active Mastodon account</a>, plus a Twitter account (<a href="https://twitter.com/simonw">@simonw</a>) where I continue to post links to new things I’ve written.</p>
<p>I’m also starting a newsletter at <a href="https://simonw.substack.com/">simonw.substack.com</a>. I plan to send out everything from my blog on a weekly basis, so if email is your preferred way to stay up-to-date you can subscribe there.</p>
<h4>More stuff I’ve written</h4>
<p>My <a href="https://simonwillison.net/tags/generativeai/">Generative AI tag</a> has everything, but here are some relevant highlights from the past year:</p>
<ul>
<li>
<a href="https://simonwillison.net/2022/May/31/a-datasette-tutorial-written-by-gpt-3/">A Datasette tutorial written by GPT-3</a>—31 May 2022</li>
<li>
<a href="https://simonwillison.net/2022/Jun/5/play-with-gpt3/">How to use the GPT-3 language model</a>—5 Jun 2022</li>
<li>
<a href="https://simonwillison.net/2022/Jun/23/dall-e/">First impressions of DALL-E, generating images from text</a>—23 Jun 2022</li>
<li>
<a href="https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/">Using GPT-3 to explain how code works</a>—9 Jul 2022</li>
<li>
<a href="https://simonwillison.net/2022/Aug/29/stable-diffusion/">Stable Diffusion is a really big deal</a>—29 Aug 2022</li>
<li>
<a href="https://simonwillison.net/2022/Sep/5/laion-aesthetics-weeknotes/">Exploring the training data behind Stable Diffusion</a>—5 Sep 2022</li>
<li>
<a href="https://simonwillison.net/2022/Sep/12/prompt-injection/">Prompt injection attacks against GPT-3</a>—12 Sep 2022</li>
<li>
<a href="https://simonwillison.net/2022/Sep/30/action-transcription/">A tool to run caption extraction against online videos using Whisper and GitHub Issues/Actions</a>—30 Sep 2022</li>
<li>
<a href="https://simonwillison.net/2022/Oct/5/spell-casting/">Is the AI spell-casting metaphor harmful or helpful?</a>—5 Oct 2022</li>
<li>
<a href="https://simonwillison.net/2022/Dec/4/give-me-ideas-for-crimes-to-do/">A new AI game: Give me ideas for crimes to do</a>—4 Dec 2022</li>
<li>
<a href="https://simonwillison.net/2022/Dec/5/rust-chatgpt-copilot/">AI assisted learning: Learning Rust with ChatGPT, Copilot and Advent of Code</a>—5 Dec 2022</li>
<li>
<a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">How to implement Q&amp;A against your documentation with GPT3, embeddings and Datasette</a>—13 Jan 2023</li>
<li>
<a href="https://simonwillison.net/2023/Feb/15/bing/">Bing: “I will not harm you unless you harm me first”</a>—15 Feb 2023</li>
<li>
<a href="https://simonwillison.net/2023/Feb/19/live-tv/">I talked about Bing and tried to explain language models on live TV!</a>—19 Feb 2023</li>
<li>
<a href="https://simonwillison.net/2023/Feb/21/in-defense-of-prompt-engineering/">In defense of prompt engineering</a>—21 Feb 2023</li>
<li>
<a href="https://simonwillison.net/2023/Feb/24/impressions-of-bing/">Thoughts and impressions of AI-assisted search from Bing</a>—24 Feb 2023</li>
<li>
<a href="https://simonwillison.net/2023/Mar/7/kqed-forum/">Weeknotes: NICAR, and an appearance on KQED Forum</a>—7 Mar 2023</li>
<li>
<a href="https://simonwillison.net/2023/Mar/10/chatgpt-internet-access/">ChatGPT can’t access the internet, even though it really looks like it can</a>—10 Mar 2023</li>
</ul>




</div>

</div></div>
  </body>
</html>
