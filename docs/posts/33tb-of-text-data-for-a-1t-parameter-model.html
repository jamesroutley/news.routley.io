<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lifearchitect.ai/chinchilla/">Original</a>
    <h1>33TB of text data for a 1T-parameter model</h1>
    
    <div id="readability-page-1" class="page"><div>

			<!-- .entry-header -->


			<div>
				<p><mark>ðŸ‘‹ Hi, Iâ€™m Alan. I advise government and enterprise on post-2020 AI like OpenAI GPT-4 and Google PaLM. You definitely want to keep up with the AI revolution this year. My paid subscribers (NASA, Microsoft, Googleâ€¦) receive bleeding-edge and exclusive insights on AI as it happens.</mark></p>
<p>Alan D. Thompson</p>
<p><em>Summary: Chinchilla showed that we need to be using 11Ã— more data during training than that used for GPT-3 and similar models. This means that we need to source, clean, and filter to around 33TB of text data for a 1T-parameter model.</em></p>
<p>How much text data should we use when training a text-based large language model (LLM)?</p>
<p>Over the last three years to 2023, there have been a few discoveries, through a process of trial and errorâ€¦</p>
<p>(Note: There is a complementary scaling law for <em>compute</em> built in to these findings, but this is outside the scope of my current focus.)</p>
<p>In May/2020, OpenAI (<a href="https://arxiv.org/abs/2005.14165">GPT-3 paper</a>) tacitly announced their data scaling laws (also called the <em>Kaplan scaling laws)</em> for LLMs:</p>
<blockquote><p>In plain English, GPT-3/Kaplan scaling laws said thatâ€¦</p></blockquote>
<p>In Sep/2022, DeepMind (<a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a>) found new data scaling laws (also called the <em>Chinchilla or Hoffman scaling laws)</em> for â€˜data optimalâ€™ LLMs:</p>
<blockquote><p>In plain English, Chinchilla/Hoffman scaling laws say thatâ€¦</p></blockquote>
<p>Therefore, to make GPT-3 data optimal, andâ€¦</p>
<blockquote><p>Keeping the original 300B tokens, GPT-3 should have been only <strong>15B parameters</strong> (300B tokens Ã· 20).</p>
<p>OR</p>
<p>To get to the original 175B parameters, GPT-3 should have used <strong>3,500B (3.5T) tokens</strong> (175B parameters x 20. 3.5T tokens is about 4-6TB of data, depending on tokenization and tokens per byte).</p></blockquote>
<p>The data optimization scale continues for model sizes measured in trillions of parameters, and training data measured in quadrillions of text tokens or petabytes of text data. The table and explanation below originally appeared in the Jun/2022 report, <a href="https://lifearchitect.ai/the-sky-is-bigger/"><em>The sky is bigger than we imagine</em></a>.</p>
<p><a href="https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table.png"><img decoding="async" loading="lazy" src="https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table.png" alt="" width="1814" height="830" srcset="https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table.png 1814w, https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table-300x137.png 300w, https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table-1024x469.png 1024w, https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table-768x351.png 768w, https://s10251.pcdn.co/wp-content/uploads/2022/06/2022-adt-chinchilla-dataset-sizes-table-1536x703.png 1536w" sizes="(max-width: 1814px) 100vw, 1814px"/></a><br/>
</p><div><h2><a href="#">Text for indexing</a></h2><div>
<table>
<thead>
<tr>
<th scope="col">Model size</th>
<th scope="col">Training</th>
<th scope="col">Training data</th>
<th scope="col">How much data is that?</th>
</tr>
</thead>
<tbody>
<tr>
<th scope="row">Chinchilla/</th>
<td>1.4 Trillion</td>
<td>2.3TB</td>
<td><em><strong>More books than inâ€¦</strong></em></td>
</tr>
<tr>
<th scope="row">250B</th>
<td>5 Trillion</td>
<td>8.3TB</td>
<td><em>All 30 libraries at Yale University (16.6M).</em></td>
</tr>
<tr>
<th scope="row">500B</th>
<td>10 Trillion</td>
<td>16.6TB</td>
<td><em>The Google Books collection (33.2M).</em></td>
</tr>
<tr>
<th scope="row">1T</th>
<td>20 Trillion</td>
<td>33.3TB</td>
<td><em>The US Library of Congress (66.6M).</em></td>
</tr>
<tr>
<th scope="row">10T</th>
<td>200 Trillion</td>
<td>333TB</td>
<td><em>All US public libraries combined (666M).</em></td>
</tr>
<tr>
<th scope="row">100T</th>
<td>2 Quadrillion</td>
<td>3.3PB</td>
<td><em>All bibles ever sold worldwide (6.6B).</em></td>
</tr>
<tr>
<th scope="row">250T</th>
<td>5 Quadrillion</td>
<td>8.3PB</td>
<td><em>A stack all the way to the Moon (16.6B).</em></td>
</tr>
<tr>
<th scope="row">500T</th>
<td>10 Quadrillion</td>
<td>16.6PB</td>
<td><em>4 books about every living human (33.2B).</em></td>
</tr>
</tbody>
</table>
</div></div>
<p><b>Table: Dataset sizes needed to align with Chinchilla data optimization for models.</b></p>
<p><span>There are a few caveats to my approximate numbers in the table above. Firstly, the â€˜More books than inâ€¦â€™ examples are provided for text-based book data only (no pictures), and this assumes that books are about 500KB each without images</span><span>. We are now of course exploring training AI with multimodal data: images, music, control signals (robots, button presses), and anything else we can get our hands on. These increasing sizes are also using simplified and rounded estimates only, based on the new findings related to model scaling using more data (measured by number of tokens, which are roughly equivalent to words).Â </span></p>
<p><span>In 2010, Google estimated that there are only 130M unique published books in existence</span><span>, so past 1T parameters (20T tokens), training data collection would naturally have to rely on alternative text-based and multimodal content. At brain-scale parameter counts of 500T (10Q tokens), the estimated book count would be over 250 times the number of books published, or more than four new books written about each living human on Earth!</span></p>
<p><span>Fundamentally, it should not be an incredibly onerous process to collect petabytes of high-quality and filtered multimodal data (converted to text), though that task has not yet been accomplished by any AI lab to date (Jun/2022).Â </span></p>
<h2>Viz of selected models showing tokens:parameters ratio</h2>
<p><a href="https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2.png"><img decoding="async" loading="lazy" src="https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2.png" alt="" width="1920" height="1080" srcset="https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2.png 1920w, https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2-300x169.png 300w, https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2-1024x576.png 1024w, https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2-768x432.png 768w, https://s10251.pcdn.co/wp-content/uploads/2022/11/2022-Alan-D-Thompson-AI-Chinchilla-Scaling-Rev-2-1536x864.png 1536w" sizes="(max-width: 1920px) 100vw, 1920px"/></a></p>
<h2>Table of current models showing tokens:parameters ratio</h2>

<p>It is expected that 2023 large language models will continue to follow the Chinchilla scaling laws, though there will be new discoveries about data optimization and data use during training. For example, there is some research on whether or not data can â€˜repeatâ€™ (be seen more than once) during training, which may help alleviate the amount of data required to be sourced.</p>
<h2>DeepMind models to Dec/2022</h2>
<p><a href="https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2.png"><img decoding="async" loading="lazy" src="https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2.png" alt="" width="1920" height="1080" srcset="https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2.png 1920w, https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2-300x169.png 300w, https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2-1024x576.png 1024w, https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2-768x432.png 768w, https://s10251.pcdn.co/wp-content/uploads/2022/12/2022-Alan-D-Thompson-DeepMind-Models-Rev-2-1536x864.png 1536w" sizes="(max-width: 1920px) 100vw, 1920px"/></a></p>
<h2>Videos on scaling and Chinchilla models</h2>
<p><iframe loading="lazy" title="Devoxx presentation - Whatâ€™s in my AI? (VIMA, Gato, GPT-2, GPT-3, PaLM, Chinchilla)" width="640" height="360" src="https://www.youtube.com/embed/ODLjaoWxT98?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p><iframe loading="lazy" title="Integrated AI - Flamingo by DeepMind (Apr/2022) - Visual LM with Chinchilla (80B) - some DALL-E 2" width="640" height="360" src="https://www.youtube.com/embed/g8IV8WLVI8I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p><iframe loading="lazy" title="Part 2 - Flamingo by DeepMind (Apr/2022) - Visual LM with Chinchilla - Integrated AI - Obama [4K]" width="640" height="360" src="https://www.youtube.com/embed/zRYcKhkAsk4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p><iframe loading="lazy" title="First look - Sparrow 70B dialogue (fine-tuned + prompted Chinchilla) by DeepMind - Launched Sep/2022" width="640" height="360" src="https://www.youtube.com/embed/dt9rv-Pf0b0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p><iframe loading="lazy" title="First look - Dramatron 70B script writer (prompted Chinchilla) by DeepMind - Launched Sep/2022" width="640" height="360" src="https://www.youtube.com/embed/i2WEEZ03Nu0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<hr/>
<div><h2>Get <em>The Memo</em><br/></h2><p><b>by Dr Alan D. Thompson</b> Â·Â Be inside the lightning-fast AI revolution.</p></div></div><!-- .entry-content -->

			<!-- .entry-footer -->
		</div></div>
  </body>
</html>
