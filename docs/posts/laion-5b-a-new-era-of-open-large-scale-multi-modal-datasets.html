<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://laion.ai/blog/laion-5b/">Original</a>
    <h1>Laion-5B: A new era of open large-scale multi-modal datasets</h1>
    
    <div id="readability-page-1" class="page"><div><p>We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world.</p>
<p><em>Authors: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, Jenia Jitsev</em></p>
<p><img src="https://lh5.googleusercontent.com/u4ax53sZ0oABJ2tCt4FH6fs4V6uUQ_DRirV24fX0EPpGLMZrA8OlknEohbC0L1Nctvo7hLi01R4I0a3HCfyUMnUcCm76u86ML5CyJ-5boVk_8E5BPG5Z2eeJtPDQ00IhVE-camk4" alt=""/></p>
<p>Large image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE &amp; GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration &amp; subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at <a href="https://github.com/mlfoundations/open_clip">open_clip</a>. Explore the dataset at the <a href="https://rom1504.github.io/clip-retrieval/">search demo</a>. See also the <a href="https://laion.ai/laion-5b-open-dataset">same post on laion website</a> .</p>
<p>We thank our sponsors <a href="https://huggingface.co/">hugging face</a>, <a href="http://doodlebot.ai/">doodlebot</a> and <a href="https://stability.ai/">stability</a> for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset.</p>
<h3>Disclaimer on dataset purpose and content warning</h3>
<p>The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however <strong>do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress</strong>.</p>
<h2>Introduction</h2>
<p>Since the release of CLIP &amp; DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN &amp; BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration &amp; subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community.</p>
<h2>Download the data</h2>
<p>We release the following packages under the LAION-5B project:</p>
<ul>
<li><a href="https://huggingface.co/datasets/laion/laion2B-en">laion2B-en</a> 2.32 billion of these contain texts in the English language</li>
<li><a href="https://huggingface.co/datasets/laion/laion2B-multi">laion2B-multi</a> 2.26 billion contain texts from 100+ other languages</li>
<li><a href="https://huggingface.co/datasets/laion/laion1B-nolang">laion1B-nolang</a> 1.27 billion have texts where a particular language couldn’t be clearly detected.</li>
</ul>
<p>The data can comfortably be downloaded with <a href="https://github.com/rom1504/img2dataset">img2dataset</a> (240TB in 384, 80TB in 224)</p>
<p>For training usage, we recommend reading the <a href="https://github.com/rom1504/laion-prepro/blob/main/laion5B/usage_guide/preparing_data_for_training.md">usage guide for training</a></p>
<p>In particular, we release this data:</p>
<ul>
<li>5.85 billion pairs of image URLs and the corresponding metadata at <a href="https://huggingface.co/datasets/laion/laion2B-en">laion2B-en</a> <a href="https://huggingface.co/datasets/laion/laion2B-multi">laion2B-multi</a> <a href="https://huggingface.co/datasets/laion/laion1B-nolang">laion1B-nolang</a> (800GB)</li>
<li>A <a href="https://huggingface.co/datasets/laion/laion5B-index">knn index</a> that enables quick search in the laion5B dataset (1.6TB)</li>
<li><a href="https://mystic.the-eye.eu/public/AI/cah/laion5b/indices/vit-l-14/">Indices</a> for laion2B-en, laion2B-multi, laion1B-nolang (2TB)</li>
<li>Clip ViT-L/14 <a href="https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/">image embeddings</a> (9TB)</li>
<li>Web demo of image-text search on LAION-5B <a href="https://rom1504.github.io/clip-retrieval/">clip-retrieval</a></li>
<li>Safety tags at <a href="https://huggingface.co/datasets/laion/laion2B-en-safety">laion2B-en-safety</a> <a href="https://huggingface.co/datasets/laion/laion2B-multi-safety">laion2B-multi-safety</a> <a href="https://huggingface.co/datasets/laion/laion1B-nolang-safety">laion1B-nolang-safety</a> (50GB)</li>
<li>Watermark tags at <a href="https://huggingface.co/datasets/laion/laion2B-en-watermark">laion2B-en-watermark</a> <a href="https://huggingface.co/datasets/laion/laion2B-multi-watermark">laion2B-multi-watermark</a> <a href="https://huggingface.co/datasets/laion/laion1B-nolang-watermark">laion1B-nolang-watermark</a> (50GB)</li>
</ul>
<p>The metadata files are parquet files that contain the following attributes: URL, TEXT, the cosine similarity score between the text and image embedding and height and width of the image. Watermark and safety tags can be joined with the metadata prior to downloading by using <a href="https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py">this script</a>. Once that is done, they can easily be filtered upon with a probability threshold at your choice (we recommend 0.5 for safety and 0.8 for watermark).</p>
<p>You can also find the prejoined files at <a href="https://huggingface.co/datasets/laion/laion2B-en-joined">laion2B-en-joined</a> <a href="https://huggingface.co/datasets/laion/laion2B-multi-joined">laion2B-multi-joined</a> <a href="https://huggingface.co/datasets/laion/laion1B-nolang-joined">laion1B-nolang-joined</a> (800GB)</p>
<h2>License</h2>
<p>We distribute the metadata dataset (the parquet files) under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Common CC-BY 4.0</a> license, which poses no particular restriction. The images are under their copyright.</p>
<h2>Dataset columns</h2>
<p>We provide these columns :</p>
<ul>
<li>URL: the image url, millions of domains are covered</li>
<li>TEXT: captions, in english for en, other languages for multi and nolang</li>
<li>WIDTH: picture width</li>
<li>HEIGHT: picture height</li>
<li>LANGUAGE: the language of the sample, only for laion2B-multi, computed using <a href="https://github.com/google/cld3">cld3</a></li>
<li>similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang</li>
<li>pwatermark: probability of being a watermarked image, computed using our <a href="https://github.com/LAION-AI/LAION-5B-WatermarkDetection">watermark detector</a></li>
<li>punsafe: probability of being an unsafe image, computed using our <a href="https://github.com/LAION-AI/CLIP-based-NSFW-Detector">clip based detector</a></li>
</ul>
<p>pwatermark and punsafe are available either as individual collections that must be <a href="https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py">joined</a> with the hash of url+text, either as prejoined collections.</p>
<h2>Dataset Statistics</h2>
<p>We <a href="https://github.com/rom1504/laion-prepro/blob/main/laion5B/stats/compute_stats.py">computed</a> some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff.</p>
<p>Computed quantiles are quantiles from 0.05 to 0.95.</p>
<p>Also see the whole <a href="https://docs.google.com/spreadsheets/d/19AkcufyABAnbBlsr12VUmlR9oyQWb4uloAQnd-rqJC0/edit#gid=0">sheet</a> and the whole <a href="https://datastudio.google.com/reporting/c67c1749-816f-464f-873a-867b4a43f044/page/p_i9he8sxntc/edit">dashboard</a></p>
<h3>Laion2B-en</h3>
<p>Total: 2.3B samples</p>
<p><img src="https://lh6.googleusercontent.com/-SW3vGI4_Ojemg_ttYpZvFmC8vTjYavTDgmnY7SsnfF-smnVpLwqbYCUsmB9_1HBmmVbKRE2QXJRwxamNcw1A9sRXDFPSj0YZ2WiptPnNeAMuSF0O_2Yi_CGsm_QChM4eJXd4lyY" alt=""/></p>
<p>Number with height and width bigger than</p>
<ul>
<li>256 -&gt; 1324M</li>
<li>512 -&gt; 488M</li>
<li>1024 -&gt; 76M</li>
</ul>
<p>Width quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0</p>
<p>Height quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0</p>
<p>Unsafe proportion: 2.9%</p>
<p>Watermark proportion: 6.1%</p>
<p>Average text length: 67</p>
<p>Text length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0</p>
<h3>Laion2B-multi</h3>
<p>Total: 2.2B samples</p>
<p><img src="https://lh4.googleusercontent.com/jLZYO_GMS28fzxwfZS199LOjSeUpTH7HEmjIRwyVAtvJdBkzyHzs83FgnD-hOY0CjK8LDooytibVcbuVa_O5YSoCu1IduEj4Z8uneE8Km-0Y39qHzYAJs4Lr4oEyQh4EeWxHGFOk" alt=""/></p>
<p>Number with height and width bigger than</p>
<ul>
<li>256 -&gt; 1299M</li>
<li>512 -&gt; 480M</li>
<li>1024 -&gt; 57M</li>
</ul>
<p>Width quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0</p>
<p>Height quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0</p>
<p>Top 10 languages: LANGUAGE count proportion:</p>
<ul>
<li>ru 241M 0.106</li>
<li>fr 168M 0.074</li>
<li>de 150M 0.066</li>
<li>es 149M 0.066</li>
<li>zh 143M 0.063</li>
<li>ja 131M 0.057</li>
<li>it 95M 0.042</li>
<li>pt 88M 0.038</li>
<li>nl 66M 0.029</li>
<li>pl 62M 0.027</li>
<li>no 49M 0.021</li>
</ul>
<p>Unsafe proportion: 3.3%</p>
<p>Watermark proportion: 5.6%</p>
<p>Average text length: 52</p>
<p>Text length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0</p>
<h3>Laion1B-nolang</h3>
<p>Total: 1.2B samples</p>
<p><img src="https://lh3.googleusercontent.com/mAI2e-sLE2geRsX8-3Mw-Ye_8wDu0SEDnIBZjWNqJiaAdjtjj7PtBvRhreugGS6_740-KcILnRbvRbDcTIFccrYO-adNz2uRM6zb0VgR3wjZVo_x0dxDhaMouH2KHScZnuNNkvs3" alt=""/></p>
<p>Number with height and width bigger than</p>
<ul>
<li>256 -&gt; 1324M</li>
<li>512 -&gt; 488M</li>
<li>1024 -&gt; 76M</li>
</ul>
<p>Width quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0</p>
<p>Height quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0</p>
<p>Unsafe proportion: 3%</p>
<p>Watermark proportion: 4%</p>
<p>Average text length: 46</p>
<p>Text length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0</p>
<h2>Acquisition pipeline</h2>
<p><img src="https://lh4.googleusercontent.com/5Pcm38fU3jxc9zf1oSNLMTxh9TH4eMG-5n-dwLF-EPNc_o-jRCwa1D1AJIX-dP4UmeEoKkDxg4fWr3Mq5JwQ7TDuKUwMiaww9GPh8EGJwGkCqWXWKx15CY4Jgige8Uu6inl0KSOu" alt=""/></p>
<p>The acquisition pipeline follows the flowchart above and can be split into three major components:</p>
<ul>
<li>Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)</li>
<li>The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites</li>
<li>Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.</li>
</ul>
<h3>Distributed processing of Common Crawl</h3>
<p>To create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains “no detection” and “detection under the confidence threshold”. The “no language” set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.</p>
<h3>Distributed downloading of the images</h3>
<p>We download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages.</p>
<h3>CLIP inference at the post-processing stage</h3>
<p>The data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion.</p>
<h3>Filtering out unsuitable image-text pairs</h3>
<p>After downloading the WAT files from Common Crawl, we apply the following filtering conditions:</p>
<ul>
<li>All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped.</li>
<li>All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them.</li>
<li>Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset.</li>
<li>We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results.</li>
<li>We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content.</li>
</ul>
<h2>Dataset preparation pipeline</h2>
<p>After processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this <a href="https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c">semantic search blogpost</a> and the readme of <a href="https://github.com/rom1504/clip-retrieval">clip-retrieval</a> for additional details about this process. See also <a href="https://medium.com/@rom1504/semantic-search-at-billions-scale-95f21695689a">semantic search at billions scale</a> for more technical details of the process that was done for laion5B.</p>
<ol>
<li>Downloading the data as webdataset with distributed img2dataset</li>
<li>Computing Vit-L/14 embeddings with distributed clip-inference</li>
<li>Computing a KNN index from these embeddings using autofaiss</li>
<li>Computing additional tags (NSFW and watermark) using clip embeddings</li>
</ol>
<h3>Distributed img2dataset</h3>
<p>We developed the <a href="https://github.com/rom1504/img2dataset">img2dataset</a> library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a <a href="https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion5B.md">distributed mode</a> for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes.</p>
<h3>Distributed clip inference</h3>
<p>From these images, the <a href="https://github.com/rom1504/clip-retrieval%7D%7Bhttps://github.com/rom1504/clip-retrieval">clip retrieval</a> inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a <a href="https://github.com/rom1504/clip-retrieval/blob/main/docs/distributed_clip_inference.md">distributed mode</a> made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference.</p>
<h3>Distributed indexing</h3>
<p>We then used these 9 TB of image embeddings to build a large PQ128 knn index using the <a href="https://github.com/criteo/autofaiss">autofaiss</a> tool. To make this run faster, a <a href="https://github.com/criteo/autofaiss/blob/master/docs/distributed/distributed_autofaiss.md">distributed mode</a> is available.</p>
<h3>Integration in the search UI</h3>
<p>In order to demonstrate the value of this data, we integrated this index into the <a href="https://knn5.laion.ai/">knn search UI</a>. It is powered by the code called <a href="https://github.com/rom1504/clip-retrieval">clip back</a>. The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required.</p>
<h3>Watermark and safety inference</h3>
<p>We wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the <a href="https://github.com/rom1504/embedding-reader">embedding-reader</a> module for NSFW and <a href="https://github.com/Zasder3/LAION-5B-WatermarkDetection">LAION-5B-WatermarkDetection</a> for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images.</p>
<h3>Watermarks</h3>
<p><img src="https://lh6.googleusercontent.com/WQMTd03M8xmR8yTuSudnyZGgcQvcLfro4Lf0DXvIZb9P8xqjGJVPjrgziZ4U2HoHlgmY_3Ubx33qYaG5jIC-h_uuEOqyJ9K0IqJsNZH0XPiP1CDek8xz60fARNXpwJW9yCkEKa7u" alt=""/></p>
<p>The training dataset is 90000 samples (45222 watermarks, 44778 clear).</p>
<p>Watermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like “clip art watermark”, “cat watermark” or “landscape watermark”.</p>
<p>The images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not.</p>
<p>In the end we decided to choose a model based on our consensual judgment. It seems to be “good” at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks.</p>
<p>The model is available at <a href="https://github.com/LAION-AI/watermark-detection">https://github.com/LAION-AI/watermark-detection</a> and <a href="https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0">https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0</a></p>
<h3>Safety</h3>
<p>On a balanced manually annotated safety test set with 3000 samples:</p>
<ul>
<li>the accuracy of the B32 NSFW classifier is: 0.960</li>
<li>the accuracy of the ViT L 14 NSFW classifier is: 0.961</li>
</ul>
<p>The model, as well as the training code, are available at <a href="https://github.com/LAION-AI/CLIP-based-NSFW-Detector">CLIP-based-NSFW-Detector</a> The tags are available at <a href="https://huggingface.co/datasets/laion/laion2B-en-safety">laion2B-en-safety</a> <a href="https://huggingface.co/datasets/laion/laion2B-multi-safety">laion2B-multi-safety</a> <a href="https://huggingface.co/datasets/laion/laion1B-nolang-safety">laion1B-nolang-safety</a> Demo at <a href="https://rom1504.github.io/clip-retrieval/">clip-retrieval</a> (check/uncheck safe mode)</p>
<h2>Using LAION datasets</h2>
<p>Laion5B and LAION-400M could e.g. be used to train</p>
<ul>
<li>Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE</li>
<li>Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP</li>
<li>Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset</li>
</ul>
<p>We present here a few examples of models that were trained on our LAION datasets with success:</p>
<h4>CLIP</h4>
<p>We, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the Jülich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI’s CLIP results.</p>
<p><img src="https://lh3.googleusercontent.com/-lvN21OSxBEwmzj1H0eIa4RxpS_VEogmXxku8R_0LWSLWtDP_tfNPuKCPuBEzA2arDvyPVfZlACZhdgymssC5E0tp_aqPdUFJMOLQf3W0wZKx3LqpJKF4JViL_nrBQH6TxEn5H2i" alt=""/></p>
<p>( The results in the right column are from our model. – huge thanks to Cade Gordon &amp; Ross Wightman for performing the training run )</p>
<p>The repository with the training code and the model checkpoints can be found here: <a href="https://github.com/mlfoundations/open_clip">https://github.com/mlfoundations/open_clip</a></p>
<p>We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC).</p>
<h4>BLIP inference tuning</h4>
<p><a href="https://github.com/salesforce/BLIP">BLIP</a> is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our <a href="http://captions.christoph-schuhmann.de/eval_b_auto/eval.html">results</a>.</p>
<p><img src="https://lh4.googleusercontent.com/pLpUwcYitBPvnLqApETmU0Ik4VVYkslTyuEZBXT8VByhMjGqINSdVPLpqOR6ZcdvCilLakTXJXP40xUecLMMCEIl7CpWAN2RfAHU__OAKcIyd3_8lrsWuKeQcraU86ITbWmxM-y_" alt=""/></p>
<p><em>eval_best_auto0185: An orange cat is looking at its reflection in the mirror.</em></p>
<p><img src="https://lh4.googleusercontent.com/QCQjVFkxZAQf24yZ49q5RLQ3ElyCIMFlOy5ACsfC7QC-CLiDDhTdCSnn5sobHFOsxLrVUIpNrk9sPyVmYDL6NB8qyQP6aSfkgxDuoWdeT3JX3j5MuJOGC9b7UgxKKnl4X9l33d0w" alt=""/></p>
<p><em>eval_best_auto0190: A green highway sign with the words Queens Bronx.</em></p>
<p>We found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI’s CLIP ViT-L/14 &amp; CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:</p>
<ol>
<li>our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)</li>
<li>our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )</li>
</ol>
<p>—&gt; We hypothesize that the generated captions match (&amp; sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than &lt;10% ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge &amp; „common sense“ would be necessary in those cases.</p>
<h4>GLIDE</h4>
<p>Clay Mullis (alias <a href="https://github.com/afiaka87">afiaka87</a>) used subsets of LAON-2B to fine-tune the OpenAi <a href="https://github.com/openai/glide-text2im">Glide</a> model and managed to reintroduce human generations. Samples</p>
<ul>
<li><a href="https://replicate.com/afiaka87/laionide-v3">https://replicate.com/afiaka87/laionide-v3</a></li>
<li><a href="https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B--VmlldzoxNTg3MTkz">https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B–VmlldzoxNTg3MTkz</a></li>
<li><a href="https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark--VmlldzoxNjE0MTE3">https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark–VmlldzoxNjE0MTE3</a></li>
</ul>
<p><img src="https://lh5.googleusercontent.com/SjEvGWJlqpcocr0aeRj2V-ldfCJkO-RubJF-QQr6OdTgP196lqJynBEx45FRoEp3YTUfZLmYjpa9QDApqRvwd14zZSwbyEKso1i_q5wJNJXgQytb3yPVcllWGPsht4Tv52j7unM7" alt=""/><img src="https://lh5.googleusercontent.com/BD89utsfddsrbxBCZsXEpxv_F4t2gGUrB8Hcqq5fl2aHrhsp5i5lJ5014d5T7I9DjxAT-4Q2N-cZYebumBzJWr7YwvRU0W2tjguSjF9DjTUuveBhhz__XWDuYGQh39N9D-op_1L7" alt=""/><img src="https://lh5.googleusercontent.com/GLlZtnYViyWsxomeN0Xh7kY8IBZA9Ni2JmndUtGoSzIUT0NnXB7ru4wCNiHdFfhk1gwdc2LmbzvvNe_TTQCWe3hckDJLl7GiHzAc1S0agio3jxgv2DG3ih0WBPpQ88KQJf0jqtaN" alt=""/></p>
<h4>Semantic search and subset extraction</h4>
<p>The <a href="https://rom1504.github.io/clip-retrieval/">clip-retrieval</a> interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training.</p>
<h4>CLOOB</h4>
<p>Katherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI’s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: <a href="https://github.com/crowsonkb/cloob-training">https://github.com/crowsonkb/cloob-training</a><img src="https://lh5.googleusercontent.com/ROxNOoa1jgaAW9JDbP2KHdDHOcTUCe-oPMvNTe2OYW_ETxbQI8W9YdA8oN93ULJ3r1Wyk7aAtm5GztJsQUZXUgw06BRiPECqp4o_bGXePp5cp9jFMBbbf2h1EYfDzvEHIcMdrcm-" alt=""/></p>
<p>(zero-shot accuracies on Imagenet-1K )</p>
<p>We are in touch with Andreas Fürst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs).</p>
<h2>Papers citing LAION 400M</h2>
<p>After the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:</p>
<ul>
<li><a href="https://arxiv.org/abs/2111.14822.pdf">Vector Quantized Diffusion Model for Text-to-Image Synthesis</a> used LAION-400M to train VQ diffusion text to image generation models</li>
<li><a href="https://arxiv.org/abs/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a> used a subset of LAION-400M to train latent diffusion models</li>
<li><a href="https://arxiv.org/abs/2112.03109.pdf">General Facial Representation Learning in a Visual-Linguistic Manner</a> LAION-400M face subset to train a face clip</li>
<li><a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a> image captioning using LAION-400M subset</li>
<li><a href="https://arxiv.org/pdf/2112.05253.pdf">MAGMA – Multimodal Augmentation of Generative Models through Adapter-based Finetuning</a> was trained on image question answering using a LAION-400M subset</li>
</ul>
<h2>Conclusion</h2>
<p>By releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community.</p>
<h2>What’s next?</h2>
<p>This is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it’s probably best, if you are interested, to join our Discord server and check out what’s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning.</p>
<p>Join us in <a href="https://discord.gg/eq3cAMZtCC">discord</a> and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don’t hesitate to share your ideas for new projects with us.</p>
<p><strong>Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!</strong></p>
<h2>Credit Assignment</h2>
<ul>
<li>Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project.</li>
<li>Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure.</li>
<li>Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags.</li>
<li>Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering</li>
<li>Jenia Jitsev: scientific organization &amp; writing, experiments planning and design, compute resource acquisition, general supervision</li>
<li>Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision</li>
<li>Andreas Köpf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model</li>
<li>Theo Coomber: He was one of our first contributors &amp; build the first versions of our <a href="https://github.com/TheoCoombes/crawlingathome">worker swarm system</a>. Without his enthusiasm this project might never have taken off.</li>
<li>Aarush Katta: Trained the watermark model</li>
<li>Cade Gordon: Run distributed inference for the watermark tags &amp; trained the CLIP B/32 model on JUWELS Booster</li>
<li>Ross Wightman: Ross helped Cade with the debugging &amp; training of the CLIP-B/32 model and executed experiments on JUWELS Booster</li>
<li>Katherine Crowson and John David Pressman: Trained the CLOOB model</li>
<li>Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project.</li>
<li>Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting</li>
</ul>

</div></div>
  </body>
</html>
