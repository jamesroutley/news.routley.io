<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/OpenPipe/ART">Original</a>
    <h1>Show HN: ART ‚Äì a new open-source RL framework for training agents</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><a href="https://openpipe.ai" rel="nofollow"><themed-picture data-catalyst-inline="true"><picture>
<img alt="ART header" src="https://github.com/openpipe/art/raw/main/assets/ART_header.png" width="100%"/>
</picture></themed-picture></a></p>
<p dir="auto"><a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb" rel="nofollow"><img src="https://github.com/openpipe/art/raw/main/assets/Train_pill.png" height="48"/></a>
<a href="https://discord.gg/zbBHRUpwf4" rel="nofollow"><img src="https://github.com/openpipe/art/raw/main/assets/Discord_pill.png" height="48"/></a>
<a href="https://openpipe.ai/blog/art-trainer-a-new-rl-trainer-for-agents" rel="nofollow"><img src="https://github.com/openpipe/art/raw/main/assets/Launch_pill.png" height="48"/></a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Train GRPO-powered RL agents with minimal code changes and maximal performance!</h3><a id="user-content-train-grpo-powered-rl-agents-with-minimal-code-changes-and-maximal-performance" aria-label="Permalink: Train GRPO-powered RL agents with minimal code changes and maximal performance!" href="#train-grpo-powered-rl-agents-with-minimal-code-changes-and-maximal-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/openpipe/art/raw/main/assets/Header_separator.png"><img src="https://github.com/openpipe/art/raw/main/assets/Header_separator.png" alt=""/></a></p>
</div>

<p dir="auto">ART is an open-source reinforcement training library for improving LLM performance in agentic workflows. ART utilizes the powerful GRPO reinforcement learning algorithm to train models from their own experiences. Unlike most RL libraries, ART allows you to execute agent runs <strong>in your existing codebase</strong> while offloading all the complexity of the RL training loop to the ART backend. Read about the <a href="#training-loop-overview"> training loop</a>. Then try out one of the notebooks below!</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Agent Task</th>
<th>Example Notebook</th>
<th>Description</th>
<th>Comparative Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2048</strong></td>
<td><a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb" rel="nofollow">üèãÔ∏è Train agent</a></td>
<td>Qwen 2.5 3B learns to play 2048</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg"><img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg" height="72"/></a> <a href="https://github.com/openpipe/art/blob/main/examples/2048/benchmark_2048.ipynb">benchmarks</a></td>
</tr>
<tr>
<td><strong>Temporal Clue</strong></td>
<td><a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb" rel="nofollow">üèãÔ∏è Train agent</a></td>
<td>Qwen 2.5 7B learns to solve Temporal Clue</td>
<td>[Link coming soon]</td>
</tr>
<tr>
<td><strong>Tic Tac Toe</strong></td>
<td><a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb" rel="nofollow">üèãÔ∏è Train agent</a></td>
<td>Qwen 2.5 3B learns to play Tic Tac Toe</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg"><img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg" height="72"/></a> <a href="https://github.com/openpipe/art/blob/main/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb">benchmarks</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h2 tabindex="-1" dir="auto">üîÅ Training Loop Overview</h2><a id="user-content--training-loop-overview" aria-label="Permalink: üîÅ Training Loop Overview" href="#-training-loop-overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">ART&#39;s functionality is divided into a <strong>client</strong> and a <strong>server</strong>. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Inference</strong></p>
<ol dir="auto">
<li>Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).</li>
<li>Completion requests are routed to the ART server, which runs the model&#39;s latest LoRA in vLLM.</li>
<li>As the agent executes, each <code>system</code>, <code>user</code>, and <code>assistant</code> message is stored in a Trajectory.</li>
<li>When a rollout finishes, your code assigns a <code>reward</code> to its Trajectory, indicating the performance of the LLM.</li>
</ol>
</li>
<li>
<p dir="auto"><strong>Training</strong></p>
<ol dir="auto">
<li>When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.</li>
<li>The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).</li>
<li>The server saves the newly trained LoRA to a local directory and loads it into vLLM.</li>
<li>Inference is unblocked and the loop resumes at step 1.</li>
</ol>
</li>
</ol>
<p dir="auto">This training loop runs until a specified number of inference and training iterations have completed.</p>

<p dir="auto">ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by <a href="https://docs.unsloth.ai/get-started/all-our-models" rel="nofollow">Unsloth</a>. Gemma 3 does not appear to be supported for the time being. If any other model isn&#39;t working for you, please let us know on <a href="https://discord.gg/zbBHRUpwf4" rel="nofollow">Discord</a> or open an issue on <a href="https://github.com/openpipe/art/issues">GitHub</a>!</p>

<p dir="auto">ART is in active development, and contributions are most welcome! Please see the <a href="https://github.com/OpenPipe/ART/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> file for more information.</p>

<div dir="auto" data-snippet-clipboard-copy-content="@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}"><pre><span>@misc</span>{<span>hilton2025art</span>,
  <span>author</span> = <span><span>{</span>Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones<span>}</span></span>,
  <span>title</span> = <span><span>{</span>ART: Agent Reinforcement Trainer<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2025<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>journal</span> = <span><span>{</span>GitHub repository<span>}</span></span>,
  <span>howpublished</span> = <span><span>{</span>\url{https://github.com/openpipe/art}<span>}</span></span>
}</pre></div>

<p dir="auto">This repository&#39;s source code is available under the <a href="https://github.com/OpenPipe/ART/blob/main/LICENSE">Apache-2.0 License</a>.</p>

<p dir="auto">ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#39;s development to the open source RL community at large, we&#39;re especially grateful to the authors of the following projects:</p>
<ul dir="auto">
<li><a href="https://github.com/unslothai/unsloth">Unsloth</a></li>
<li><a href="https://github.com/vllm-project/vllm">vLLM</a></li>
<li><a href="https://github.com/huggingface/trl">trl</a></li>
<li><a href="https://github.com/skypilot-org/skypilot">SkyPilot</a></li>
</ul>
<p dir="auto">Finally, thank you to our partners who&#39;ve helped us test ART in the wild! We&#39;re excited to see what you all build with it.</p>
</article></div></div>
  </body>
</html>
