<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/KhoomeiK/LlamaGym">Original</a>
    <h1>Show HN: LlamaGym ‚Äì fine-tune LLM agents with online reinforcement learning</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/khoomeik/LlamaGym/main/llamagym.png"><img src="https://raw.githubusercontent.com/khoomeik/LlamaGym/main/llamagym.png" height="250" alt="Llama Gym"/></a>
</p>
<p dir="auto">
  <em>Fine-tune LLM agents with online reinforcement learning</em>
</p>
<p dir="auto">
    <a href="https://pypi.org/project/llamagym/" rel="nofollow">
        <img alt="Python" src="https://camo.githubusercontent.com/0562f16a4ae7e35dae6087bf8b7805fb7e664a9e7e20ae6d163d94e56b94f32d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d3336373041303f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d666664643534" data-canonical-src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&amp;logo=python&amp;logoColor=ffdd54"/>
        <img alt="Version" src="https://camo.githubusercontent.com/f7e0da485cde2af0a033ee6f422809b27bf8fdffc9651afe45154c43e2831bf4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6c616d6167796d3f7374796c653d666f722d7468652d626164676526636f6c6f723d333637304130" data-canonical-src="https://img.shields.io/pypi/v/llamagym?style=for-the-badge&amp;color=3670A0"/>
    </a>
</p>
<p dir="auto">
<a href="https://reworkd.ai/" rel="nofollow">üîó Agents for Web Data Extraction</a>
<span>¬†¬†‚Ä¢¬†¬†</span>
<a href="https://x.com/khoomeik/status/1766805213644800011" rel="nofollow">üê¶ Twitter</a>
</p>
<p dir="auto">&#34;Agents&#34; originated in reinforcement learning, where they learn by interacting with an environment and receiving a reward signal. However, LLM-based agents today do not learn online (i.e. continuously in real time) via reinforcement.</p>
<p dir="auto">OpenAI created <a href="https://github.com/Farama-Foundation/Gymnasium">Gym</a> to standardize and simplify RL environments, but if you try dropping an LLM-based agent into a Gym environment for training, you&#39;d find it&#39;s still quite a bit of code to handle LLM conversation context, episode batches, reward assignment, PPO setup, and more.</p>
<p dir="auto">LlamaGym seeks to simplify fine-tuning LLM agents with RL. Right now, it&#39;s a single <code>Agent</code> abstract class that handles all the issues mentioned above, letting you quickly iterate and experiment with agent prompting &amp; hyperparameters across any Gym environment.</p>

<p dir="auto">Fine-tuning an LLM-based agent to play in a Gym-style environment with RL has never been easier! Once you install LlamaGym...</p>

<p dir="auto">First, implement 3 abstract methods on the Agent class:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llamagym import Agent

class BlackjackAgent(Agent):
    def get_system_prompt(self) -&gt; str:
        return &#34;You are an expert blackjack player.&#34;

    def format_observation(self, observation) -&gt; str:
        return f&#34;Your current total is {observation[0]}&#34;

    def extract_action(self, response: str):
        return 0 if &#34;stay&#34; in response else 1"><pre><span>from</span> <span>llamagym</span> <span>import</span> <span>Agent</span>

<span>class</span> <span>BlackjackAgent</span>(<span>Agent</span>):
    <span>def</span> <span>get_system_prompt</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:
        <span>return</span> <span>&#34;You are an expert blackjack player.&#34;</span>

    <span>def</span> <span>format_observation</span>(<span>self</span>, <span>observation</span>) <span>-&gt;</span> <span>str</span>:
        <span>return</span> <span>f&#34;Your current total is <span><span>{</span><span>observation</span>[<span>0</span>]<span>}</span></span>&#34;</span>

    <span>def</span> <span>extract_action</span>(<span>self</span>, <span>response</span>: <span>str</span>):
        <span>return</span> <span>0</span> <span>if</span> <span>&#34;stay&#34;</span> <span>in</span> <span>response</span> <span>else</span> <span>1</span></pre></div>
<p dir="auto">Then, define your base LLM (as you would for any fine-tuning job) and instantiate your agent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = AutoModelForCausalLMWithValueHead.from_pretrained(&#34;Llama-2-7b&#34;).to(device)
tokenizer = AutoTokenizer.from_pretrained(&#34;Llama-2-7b&#34;)
agent = BlackjackAgent(model, tokenizer, device)"><pre><span>model</span> <span>=</span> <span>AutoModelForCausalLMWithValueHead</span>.<span>from_pretrained</span>(<span>&#34;Llama-2-7b&#34;</span>).<span>to</span>(<span>device</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>&#34;Llama-2-7b&#34;</span>)
<span>agent</span> <span>=</span> <span>BlackjackAgent</span>(<span>model</span>, <span>tokenizer</span>, <span>device</span>)</pre></div>
<p dir="auto">Finally, write your RL loop as usual and simply call your agent to act, reward, and terminate:</p>
<div dir="auto" data-snippet-clipboard-copy-content="env = gym.make(&#34;Blackjack-v1&#34;)

for episode in trange(5000):
    observation, info = env.reset()
    done = False

    while not done:
        action = agent.act(observation) # act based on observation
        observation, reward, terminated, truncated, info = env.step(action)
        agent.assign_reward(reward) # provide reward to agent
        done = terminated or truncated

    train_stats = agent.terminate_episode() # trains if batch is full"><pre><span>env</span> <span>=</span> <span>gym</span>.<span>make</span>(<span>&#34;Blackjack-v1&#34;</span>)

<span>for</span> <span>episode</span> <span>in</span> <span>trange</span>(<span>5000</span>):
    <span>observation</span>, <span>info</span> <span>=</span> <span>env</span>.<span>reset</span>()
    <span>done</span> <span>=</span> <span>False</span>

    <span>while</span> <span>not</span> <span>done</span>:
        <span>action</span> <span>=</span> <span>agent</span>.<span>act</span>(<span>observation</span>) <span># act based on observation</span>
        <span>observation</span>, <span>reward</span>, <span>terminated</span>, <span>truncated</span>, <span>info</span> <span>=</span> <span>env</span>.<span>step</span>(<span>action</span>)
        <span>agent</span>.<span>assign_reward</span>(<span>reward</span>) <span># provide reward to agent</span>
        <span>done</span> <span>=</span> <span>terminated</span> <span>or</span> <span>truncated</span>

    <span>train_stats</span> <span>=</span> <span>agent</span>.<span>terminate_episode</span>() <span># trains if batch is full</span></pre></div>
<p dir="auto">Some reminders:</p>
<ul dir="auto">
<li>above code snippets are mildly simplified above but a fully working example is available in <a href="https://github.com/KhoomeiK/LlamaGym/blob/main/examples/blackjack.py"><code>examples/blackjack.py</code></a></li>
<li>getting online RL to converge is notoriously difficult so you&#39;ll have to mess with hyperparameters to see improvement
<ul dir="auto">
<li>your model may also benefit from a supervised fine-tuning stage on sampled trajectories before running RL (we may add this feature in the future)</li>
</ul>
</li>
<li>our implementation values simplicity so is not as compute efficient as e.g. <a href="https://github.com/flowersteam/lamorel">Lamorel</a>, but easier to start playing around with</li>
<li>LlamaGym is a weekend project and still a WIP, but we love contributions!</li>
</ul>

<ul dir="auto">
<li><a href="https://github.com/flowersteam/Grounding_LLMs_with_online_RL">Grounding Large Language Models with Online Reinforcement Learning</a>
<ul dir="auto">
<li><a href="https://github.com/flowersteam/lamorel">Lamorel: Language Models for Reinforcement Learning</a></li>
</ul>
</li>
<li><a href="https://github.com/WeihaoTan/TWOSOME">True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</a></li>
</ul>

<div data-snippet-clipboard-copy-content="bibtex
@misc{pandey2024llamagym,
  title        = {LlamaGym: Fine-tune LLM agents with Online Reinforcement Learning},
  author       = {Rohan Pandey},
  year         = {2024},
  howpublished = {GitHub},
  url          = {https://github.com/KhoomeiK/LlamaGym}
}"><pre><code>bibtex
@misc{pandey2024llamagym,
  title        = {LlamaGym: Fine-tune LLM agents with Online Reinforcement Learning},
  author       = {Rohan Pandey},
  year         = {2024},
  howpublished = {GitHub},
  url          = {https://github.com/KhoomeiK/LlamaGym}
}
</code></pre></div>
</article></div></div>
  </body>
</html>
