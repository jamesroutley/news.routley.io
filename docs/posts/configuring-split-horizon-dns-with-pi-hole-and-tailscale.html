<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.bentasker.co.uk/posts/blog/general/configuring-pihole-to-serve-different-records-to-different-clients.html">Original</a>
    <h1>Configuring Split Horizon DNS with Pi-Hole and Tailscale</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>I&#39;ve long had some form of VPN for my devices to use when I&#39;m out and about. </p>
<p>Although I used to <a href="https://otee.dev/2025/documentation/linux/259-openvpn-on-debian.html">run OpenVPN</a>, I moved to <a href="https://tailscale.com/">Tailscale</a> a little while back. Tailscale builds a mesh network using <a href="https://www.wireguard.com/">Wireguard</a> protocol and so is able to connect and run quite a bit faster than OpenVPN.</p>
<p>Side note: for those wondering, Tailscale is <em>Canadian</em> and can&#39;t see the content of connections (although if you&#39;re worried about this it&#39;s also possible to self-host using <a href="https://github.com/juanfont/headscale">Headscale</a>).</p>
<p>Although the tailnet has been up for some time, I hadn&#39;t got around to setting up split horizon DNS for clients on the tailnet. I was in a bit of a hurry when first setting up and so configured my reverse proxy box to advertise a route to it&#39;s own LAN IP.</p>
<p>This post talks about configuring my <a href="https://pi-hole.net/">Pi-hole</a> to implement a split horizon: returning the tailnet IP to tailnet clients and the LAN IP to LAN clients.</p>
<!-- TEASER_END -->

<hr/>
<h4 id="splitting-my-split-horizon">Splitting my Split Horizon</h4>
<p>Many of the DNS names that I wanted to do this for <em>already</em> had a split horizon:</p>
<p><img alt="Flow diagram showing the resolution and connection flow for a LAN client and a web client" src="https://otee.dev/images/BlogItems/split_horizon_tailscale/splithorizonflow.png"/></p>
<p>Clients on both the LAN and the wider internet connect to the same reverse proxy in my DMZ, but LAN clients connect using the proxy&#39;s local IP.</p>
<p>The reverse proxy fronts multiple services, most of which have authentication built in. However, it also requires that outside connections pass a separate (and valid) set of authentication credentials before it&#39;ll pass their connection on.</p>
<p>Having to authenticate twice is a little annoying though, and the split horizon makes it easy to disable the additional authentication when LAN clients connect:</p>
<pre><code>satisfy any;
allow 192.168.3.0/24;
deny all;
auth_basic &#34;Authenticate you must&#34;;
auth_basic_user_file /etc/nginx/wanaccess.htpasswd;
</code></pre>
<p>This extra authentication means that I&#39;m not exposing any element of the backing service&#39;s authentication stack to the outside world. The underlying idea is that it <em>shouldn&#39;t matter</em> that there&#39;s an auth bypass zero day in (say) Grafana, because the wider world needs to get past my auth prompt before they can try to detect or exploit it.</p>
<hr/>
<h4 id="youve-got-access-why-make-the-tailnet-special">You&#39;ve Got Access: Why Make The Tailnet Special?</h4>
<p>Given that there&#39;s an ability to access services via the WAN, you might be wondering why it is that I felt that I needed to do something specifically for the tailnet.</p>
<p>Unfortunately, the proxy <em>can&#39;t</em> enforce additional authentication for some services because those services clients don&#39;t support it.</p>
<p><a href="https://nextcloud.com/">Nextcloud</a> is a great example of this: the Nextcloud Desktop sync client authenticates with Nextcloud, but</p>
<ul>
<li>It uses the <code>Authorization</code> header to present it&#39;s bearer token, so the reverse proxy will see an unexpected (and, to it, invalid) set of credentials</li>
<li>The client doesn&#39;t expose a way to add custom headers to the requests that it makes, so I can&#39;t simply send a shared secret and have the proxy check a different header</li>
</ul>
<p>Having the reverse proxy require additional auth breaks off-net Nextcloud clients (and Nextcloud isn&#39;t the only service with this issue).</p>
<hr/>
<h5 id="geoblocking">Geoblocking</h5>
<p>Originally, I left the affected services accessible to the world.</p>
<p>Unfortunately, I <em>sometimes</em> seem to upset people enough to trigger prolonged attempts at compromising my services.</p>
<p>After <a href="https://otee.dev/2025/07/security/a-comparison-of-waf-exceptions-between-www-tor-i2p.html#the_purge">one such attempt</a>, I decided to reduce attack surface by <a href="https://otee.dev/2025/documentation/security/creating-geo-restrictions-in-openresty-with-lua.html">adding geo-blocking to my reverse proxy</a>, essentially restricting access to areas that I thought we&#39;d be likely to connect from (or <em>at least</em> appear to).</p>
<p>This, of course, comes at a cost in flexibility, with access failing if any of the following are true:</p>
<ul>
<li>We connected from an IP that doesn&#39;t have a location in the GeoDB (or is mislocated)</li>
<li>The ISP that we&#39;re connecting from does funky routing stuff and/or uses CGNAT</li>
<li>We&#39;ve travelled somewhere that we wouldn&#39;t normally</li>
</ul>
<p>Adding split horizon DNS to the tailnet allows me to avoid these scenarios, because the tailnet subnet can be special cased in <em>exactly</em> the same way that the LAN is.</p>
<p>It also increases the likelihood that I can close WAN access off and require that a client be on either the LAN or tailnet.</p>
<hr/>
<h4 id="the-plan">The Plan</h4>
<p>The idea was that a tailnet client would also speak to the Pi-hole, but that names would resolve to a tailnet IP:</p>
<p><img alt="Flow diagram showing LAN and tailnet clients talking to the same server and getting different IPs back" src="https://otee.dev/images/BlogItems/split_horizon_tailscale/tailnet_split.png"/></p>
<p>This is possible because Pi-hole is underpinned by a fork of <code>dnsmasq</code> called <code>pihole-FTL</code> which has inherited the setting <code>localise-queries</code> (in Pi-hole, this is enabled by default).</p>
<p>The <a href="https://thekelleys.org.uk/dnsmasq/docs/dnsmasq-man.html">man page for dnsmasq</a> describes the setting as follows (line breaks mine):</p>
<blockquote>
<p>Return answers to DNS queries from /etc/hosts and <strong>--interface-name</strong> and <strong>--dynamic-host</strong> which depend on the interface over which the query was received. </p>
<p>If a name has more than one address associated with it, and at least one of those addresses is on the same subnet as the interface to which the query was sent, then return only the address(es) on that subnet and return all the available addresses otherwise. </p>
<p>This allows for a server to have multiple addresses in /etc/hosts corresponding to each of its interfaces, and hosts will get the correct address based on which network they are attached to. </p>
<p>Currently this facility is limited to IPv4.</p>
</blockquote>
<p>This means that we can create the following record set in <code>/etc/pihole/custom.list</code>:</p>
<pre><code>192.168.3.33 foo.example.com
100.100.3.2  foo.example.com
</code></pre>
<p>If a query is received over an interface in one of these subnets, only the matching record will be returned (otherwise, both will be returned):</p>
<table>
<thead><tr>
<th>Receiving Interface IP</th>
<th>Response</th>
</tr></thead>
<tbody>
<tr>
<td><em>192.168.3.13/24</em></td>
<td>192.168.3.33</td>
</tr>
<tr>
<td><em>100.100.3.13/24</em></td>
<td>100.100.3.2</td>
</tr>
<tr>
<td><em>10.8.0.0/24</em></td>
<td>192.168.3.33, 100.100.3.2</td>
</tr>
</tbody>
</table>
<p>One small drawback with this is that the records must be in the <code>hosts</code> format file - most of my records were in dnsmasq format files, so I had to migrate the ones that I wanted to split.</p>
<hr/>
<h4 id="re-jigging-my-docker-container">Re-Jigging My Docker Container</h4>
<p>There was, however, a catch.</p>
<p>When I first created my pihole container, the docker invocation looked something like this:</p>
<pre><code>docker run \
-d \
--name=pihole \
--hostname=pihole \
--restart=unless-stopped \
--e ServerIP=0.0.0.0 \
--e WEBPASSWORD=&#39;NotMyRealPass&#39; \
-v $PWD/pihole/conf:/etc/pihole \
-v $PWD/pihole/dnsmasq.d:/etc/dnsmasq.d/ \
-p 53:53 -p 53:53/udp \ 
-p 8080:80 \
pihole/pihole
</code></pre>
<p>This meant that the container was using bridged networking, depriving Pi-hole of the means to see which physical interface a query arrived on: it simply saw the other side of a single bridge interface.</p>
<p>So, I killed the container and started a new one using host networking:</p>
<pre><code>docker run \
-d \
--network=host \
--name=pihole \
--hostname=pihole \
--restart=unless-stopped \
-e ServerIP=0.0.0.0 \
-e WEBPASSWORD=&#39;NotMyRealPass&#39; \
-v $PWD/pihole/conf:/etc/pihole \
-v $PWD/pihole/dnsmasq.d:/etc/dnsmasq.d/ \
pihole/pihole
</code></pre>
<p>However the container failed to start: Pihole&#39;s web interface was trying to bind to port 80 which already had something bound to it.</p>
<p>As I&#39;d previously mapped 8080 into the container (<code>-p 8080:80</code>), I used the environment variable <code>WEB_PORT</code> to tell Pi-hole to bind to that port instead:</p>
<pre><code>docker run \
-d \
--network=host \
-e WEB_PORT=8080 \
--name=pihole \
--hostname=pihole \
--restart=unless-stopped \
--env=ServerIP=0.0.0.0 \
--env=&#39;WEBPASSWORD=NotMyRealPass&#39; \
-v $PWD/pihole/conf:/etc/pihole \
-v $PWD/pihole/dnsmasq.d:/etc/dnsmasq.d/ \
-p 53:53 -p 53:53/udp \ 
-p 8080:80 \
pihole/pihole
</code></pre>
<hr/>
<h5 id="dns-outage">DNS Outage</h5>
<p><img alt="Meme of Roadrunner holding a sign which reads DNS IS DOWN" src="https://otee.dev/images/BlogItems/split_horizon_tailscale/roadrunner_DNS_meme.jpg"/></p>
<p>Pi-hole came up, but it wasn&#39;t responding to queries.</p>
<p>Netstat showed <code>pihole-FTL</code> listening and bound to all interfaces:</p>
<pre><code>$ sudo netstat -lnp | grep :53
tcp        0      0 0.0.0.0:53              0.0.0.0:*               LISTEN      2653543/pihole-FTL  
tcp6       0      0 :::53                   :::*                    LISTEN      2653543/pihole-FTL  
udp        0      0 0.0.0.0:53              0.0.0.0:*                           2653543/pihole-FTL  
udp6       0      0 :::53                   :::*                                2653543/pihole-FTL  
</code></pre>
<p>Packet captures showed that queries were coming in, but no responses were being sent. </p>
<pre><code>$ sudo tcpdump -i any port 53
21:54:02.345555 enp0s25 In  IP 192.168.3.163.32273 &gt; 192.168.3.5.53: 57965+ A? n-deventry.tplinkcloud.com. (44)
21:54:02.512870 enp0s25 In  IP 192.168.3.44.63761 &gt; 192.168.3.5.53: 26967+ AAAA? lycraservice-pa.googleapis.com.home. (53)
21:54:02.524346 enp0s25 In  IP 192.168.3.44.1270 &gt; 192.168.3.5.53: 2692+ A? lycraservice-pa.googleapis.com.home. (53)
21:54:02.767189 enp0s25 In  IP6 2001:820:aa1a:c443:b9c4:44b:df15:bd8e.36925 &gt; 2001:820:aa1a:c443::2.53: 28460+ A? a.nel.cloudflare.com.home. (43)
21:54:02.767189 enp0s25 In  IP6
</code></pre>
<p>Queries weren&#39;t triggering any activity in Pihole&#39;s logs either.</p>
<p>To restore service to the LAN, I killed the container and brought it back up with bridged networking - DNS sprang straight back to life.</p>
<p>It took me a while to figure out what the issue was, but eventually I spotted this setting in Pi-hole&#39;s web interface:</p>
<p><img alt="Under potentially dangerous options is a setting to control which interfaces pihole will respond on" src="https://otee.dev/images/BlogItems/split_horizon_tailscale/interface_allow_opt.png"/></p>
<p>Pi-hole was configured to only respond to queries received from interface <code>eth0</code>. Resolution stopped because the box that I run pihole on doesn&#39;t have an <code>eth0</code> (it&#39;s a <code>udev</code>&#39;y style <code>enp0s25</code>). </p>
<p>I switched this to <code>Permit all origins</code> and restarted the container with host networking. This time, queries were answered.</p>
<hr/>
<h4 id="configuring-tailscale">Configuring Tailscale</h4>
<p>The box hosting pihole was already part of the tailnet, but I wanted to remove the previous route advertisement.</p>
<p>So I ran</p>
<pre><code>sudo tailscale down

# Previously this was
# --advertise-routes=192.168.3.33/32
sudo tailscale set --advertise-routes=

sudo tailscale up
</code></pre>
<p>Then, from another tailnet client (my laptop), I tried resolving a name via both the LAN and tailnet address:</p>
<pre><code>$ dig +short foo.example.com @100.99.55.55
100.100.3.2

$ dig +short foo.example.com @192.168.3.13
192.168.3.33
</code></pre>
<p>All that was left was to have tailnet clients actually use Pihole.</p>
<p>I logged into Tailscale&#39;s web interface and added <a href="https://tailscale.com/learn/why-split-dns">a Split DNS entry</a>:</p>
<p><img alt="Screenshot of a split DNS entry, tailnet clients will send queries for subdomains of bentasker.co.uk to pihole&#39;s tailnet address" src="https://otee.dev/images/BlogItems/split_horizon_tailscale/tailscale_split_dns.png"/></p>
<p>When bringing tailscale up on my Linux laptop, I had to explicitly pass a flag to allow it to use the advertised server</p>
<pre><code>sudo tailscale up --accept-dns
</code></pre>
<p>The android app has a toggle for this, but it was already on.</p>
<hr/>
<h4 id="conclusion">Conclusion</h4>
<p>My devices  now have transparent (and slightly more privileged) access to services when I&#39;m out and about.</p>
<p>Because Tailscale acts as a mesh network, I don&#39;t need to worry about <a href="https://otee.dev/2025/07/08/277-android-protecting-your-network-data-from-local-snooping">automatically turning the VPN off when I&#39;m at home</a> - devices in the same segment can direct connect to one another rather than making a round-trip via a remote coordinator.</p>
<p>As a result of getting this up and running, I&#39;ve been able to close off WAN access to a number of services (although I still can&#39;t can&#39;t do that for any service which hosts something I might try to cast, because Chromecasts ignore local DNS... grrr).</p>
<p>It all works well enough that I&#39;ve been able to write, proof-read and publish this post whilst off net.</p>
<p>As an added bonus, Tailscale seem to have <a href="https://tailscale.com/mullvad">partnered with Mullvad</a>, so if I&#39;m ever <em>travelling</em> travelling, I can have my devices route all connections via Mullvad and my tailnet.</p>
</div></div>
  </body>
</html>
