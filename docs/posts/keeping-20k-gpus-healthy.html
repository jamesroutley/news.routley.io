<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://modal.com/blog/gpu-health">Original</a>
    <h1>Keeping 20k GPUs healthy</h1>
    
    <div id="readability-page-1" class="page"><div><!--[!--><!----><article><!--[--><p>Modal runs a globally distributed, autoscaling GPU worker pool by sourcing compute from all cloud giants: AWS, GCP, Azure, OCI. We‚Äôve scaled the worker pool to well over 20,000 concurrent GPUs, and launched over four million cloud instances in the last couple years. At this scale, you see almost every GPU reliability problem there is.</p> <p>Today, we‚Äôre sharing our GPU reliability system as both a demonstration of our commitment to Modal customers and as a guide for fellow travelers renting hyperscaler or neocloud cards. <a rel="nofollow" href="https://en.wikipedia.org/wiki/It%27s_dangerous_to_go_alone!"><!--[--><!---->It‚Äôs dangerous to go alone!
Take this.<!--]--></a><!----></p> <p>This post starts with cloud instance type testing and selection. Perhaps surprisingly, there are significant performance and reliability differences between the cloud hyperscalers. We then discuss machine image preparation and instance boot checks. Next we cover the passive and active GPU healthchecking performed throughout the life of each instance. Finally we discuss observability and support, which become crucial when a GPU reliability issue slips by our automated healthchecking systems.</p> <p>We‚Äôve chosen not to refer to cloud providers directly, but instead give them anonymized A, B, C, D identifiers. If you want know who‚Äôs who, track the clues or buy us a beer sometime.</p> <h2 id="instance-type-testing-and-selection">Instance type testing and selection</h2> <p>Let‚Äôs start with cloud instance type reliability. The hyperscalers are significantly differentiated at the instance type level. To stick specifically to reliability related differences, we‚Äôve observed that:</p> <ul><li>Cloud A has the simplest and most reliable instance launch API. If you request a BM or VM and get a HTTP 201 back, 99.6% of the time you‚Äôll get it to boot, and boot relatively quickly (2-3 minutes).</li> <li>Cloud A runs H100s which perform 50% worse on StableDiffusion <code>text2img</code> compared with C and D.</li> <li>Cloud C ran their H100s too hot, sometimes reaching over 90¬∫C, for a few months in 2025. FLOP/s performance degrades starting as low as the mid-70s Celsius.</li> <li>Cloud C has 228MiB more <code>reserved</code> H100 memory than the others. Thus, it has less memory for our customers to use.</li> <li>Cloud D A10s have frequent hardware-side clock slowdowns (<a rel="nofollow" href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-constants.html#c.DCGM_CLOCKS_EVENT_REASON_HW_SLOWDOWN"><!--[--><code>HW_SLOWDOWN</code><!--]--></a><!----> and <a rel="nofollow" href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-constants.html#c.DCGM_CLOCKS_EVENT_REASON_HW_POWER_BRAKE"><!--[--><code>HW_POWER_BRAKE</code><!--]--></a><!---->).</li> <li>The NVIDIA A10s in one of Cloud D‚Äôs US regions have more frequent uncorrectable ECC errors. Unfortunately, this isn‚Äôt something you find out quickly.</li> <li>Cloud D has the best price/performance. Its bare metal servers are beasts.</li></ul> <p>Typically, our provider ranking is capacity and price led, but we additionally maintain internal <em>adjusted</em> prices which account for penalties we impose after discovering problems with specific instance types, regions, etc.</p> <p>We maintain semi-automated benchmarking (called <code>modal-host-bench</code>) to let us evaluate the plethora of performance and reliability issues we want to eliminate or at least price-in. Here is some sample benchmarking data highlighting that you really do not want
to rent a PCIe H100 when you can rent an SXM H100 instead.</p> <table><!--[--><!--[--><thead><tr><th><strong>Category</strong></th><th><strong>Cloud D H100 SXM</strong></th><th><strong>Cloud B H100 NVL (PCIe)</strong></th><th><strong>% diff</strong></th></tr></thead> <tbody><tr><td><code>torch_matmul_duration_seconds</code></td><td>1.62</td><td>2.72</td><td>67.5%</td></tr><tr><td><code>torch_matmul_flops</code></td><td>678 TF/s</td><td>405 TF/s</td><td>-40.3%</td></tr><tr><td><code>h2d_bw_pageable_1024</code></td><td>7.68 GiB/s</td><td>21.0 GiB/s</td><td>174%</td></tr><tr><td><code>h2d_bw_pinned_1024</code></td><td>49.1 GiB/s</td><td>51.2 GiB/s</td><td>4.40%</td></tr><tr><td><code>d2h_bw_pageable_1024</code></td><td>14.3 GiB/s</td><td>20.9 GiB/s</td><td>46.0%</td></tr><tr><td><code>d2h_bw_pinned_1024</code></td><td>50.7 GiB/s</td><td>53.4 GiB/s</td><td>5.30%</td></tr></tbody><!--]--><!--]--></table><!----> <h2 id="machine-images">Machine images</h2> <p>Machine images are what our bare-metal (BM) and virtual machine (VM) servers use to boot. They include a kernel, operating system files, the NVIDIA driver, installed system libraries, configuration, and a bit of Modal‚Äôs application software.</p> <p>We‚Äôve found that the quality of the machine images used has significant implications for reliability and performance. We care a lot about machine image consistency across our multi-cloud compute pool (same kernel, same drivers, etc) as well as freshness. Our images keep up with the latest production NVIDIA driver version (<a rel="nofollow" href="https://www.nvidia.com/en-us/drivers/details/250991/"><!--[--><!---->580.95.05<!--]--></a><!---->) for security, performance, and new features.</p> <p>In Modal‚Äôs early days, machine image updates were ad-hoc and manually tested, and mistakes abounded. A couple years ago this became untenable, so we switched to continuous, gradual integration of machine images with automated testing before image promotion occurred.</p> <figure><img src="https://modal-cdn.com/blog/images/mach-img-rollout.webp" alt="Timeseries graph showing our machine image rollout"/> <figcaption>A visualization of machine image version rollouts across a week. Color indicates version, and you can see orange was rolled back.</figcaption></figure> <p>Because the cloud giants are so reliable at loading custom machine images, you can pull a lot of GPU testing into the image build phase. Concretely, at the end of a build we run both system tool tests like <a rel="nofollow" href="https://developer.nvidia.com/dcgm"><!--[--><!---->NVIDIA Data Center GPU Manager (DCGM)<!--]--></a><!----> and custom GPU tests from inside the Modal container runtime before considering the image configuration ready for production. This ensures that both the Worker host and our customer‚Äôs guest containers will work with the GPU.</p> <div><div><div><!--[--><div><!--[!--><!--]--> <div><!----><pre tabindex="0"><code><span><span>provisioner &#34;shell&#34; {</span></span>
<span><span>  script = &#34;./setup/check_nvidia_ctk.sh&#34;</span></span>
<span><span>}</span></span>
<span><span></span></span>
<span><span>provisioner &#34;file&#34; {</span></span>
<span><span>  destination = &#34;/tmp/modal/&#34;</span></span>
<span><span>  source      = &#34;./.bin/modal-healthcheck&#34;</span></span>
<span><span>}</span></span></code></pre><!----></div></div><!--]--> <!--[--><!--]--></div></div><!----></div><!----> <p>Solid machine image support is a place where the cloud giants clearly differentiate their platforms from most neocloud upstarts (e.g. Lambda Labs, Nebius). Very few neoclouds support image customization, and they also have worse instance startup performance due to hypervisor and caching inefficiencies. Cloud C is the fastest to boot a new VM with our machine image, averaging just under 2 minutes.
Certain neoclouds struggle to boot even their platform-default machine image in less than 5 minutes.</p> <p>Although the hyperscalers are not significantly differentiated in their machine image feature and reliability, cloud D has <em>extremely</em> slow regional image replication, taking 3 hours to replicate to 10 regions.</p> <h2 id="instance-boot">Instance boot</h2> <p>Instance boot is where our machine images spark alive in the heat and noise of a datacenter, encountering the reality of production. If we‚Äôve booted on a host with bad GPUs, or our cloud-init process has a bug, we need know about it and intervene before any customers land on those GPUs.</p> <p>There is a significant tradeoff here. Modal runs an autoscaling fleet. Slowing down startup adds to scheduling overhead for our customers. Worse still, added startup latency actually <em>reduces</em> reliability when it delays failover.</p> <p>The deepest generic check you might do on a new host is <code>dcgmi diag --run 4</code>. It finds a bunch of long-tail problems, but takes around an hour. Even the shallowest, <code>dcgmi diag --run 1</code> takes at least a minute.</p> <p>Testing hardware on boot is likely redundant with healthchecking already performed by the cloud provider. After all, we‚Äôre supposedly paying for working GPUs!
Deeply checking every instance put out by an assembly line already running at four nines of reliability would be penny wise and pound foolish. When I buy a coffee at my local shop,
I don‚Äôt ask to smell the milk.</p> <p>So at instance boot we typically perform relatively light checks: <code>systemctl</code> queries, <code>nvidia-smi</code> queries, and a basic readwrite on a randomly selected GPU (0-7).</p> <p>Today, we almost never have GPU problems slip through and hit user containers. The one irksome issue we have in production is that Cloud C‚Äôs <a href="https://cjwu.substack.com/docs/guide/troubleshooting#cuda-driver-initialization-failed-on-l4-gpu-type"><!--[--><!---->L4s flake at CUDA initialization<!--]--></a><!----> in 0.1% of cases. Application code targeting those cards must use <code>cuInit</code> retries.</p> <h2 id="lifetime-management">Lifetime management</h2> <p>At this point we‚Äôve acquired an instance we‚Äôre happy with, booted it, and started running customer workloads on it. We‚Äôre happy in production, but need to stay that way, and that‚Äôs where continuous <em>passive</em> and <em>active</em> healthchecking comes into play.</p> <ul><li>Passive healthchecking does not run on a GPU and is non-invasive, read-only. Passive data streams include <code>dmesg</code> and <code>dcgmi health</code>.</li> <li>Active healthchecks take exclusive hold on a GPU device and readwrite to acquire health data. <code>dcgmi check</code> and <a rel="nofollow" href="https://github.com/NVIDIA/nvbandwidth"><!--[--><code>nvbandwidth</code><!--]--></a><!----> are examples.</li></ul> <h3 id="passive-healthchecking">Passive healthchecking</h3> <figure><div><div><!--[!--><!--]--> <!--[!--><!--]--> </div><!----></div> <figcaption>Critical level <a href="https://modal.com/docs/guide/gpu-health">Xid errors</a> per hour by cloud, normalized by GPU count. Cloud B (blue) has by far the highest critical error rate.</figcaption></figure> <p>You get 80% of the passive healthchecking wins from 20% of the work: running <code>dcgmi</code> periodically and checking <code>dmesg</code> for the most common issues. More specifically, <code>dcgmi</code> can tell you about uncorrectable ECC errors on specific GPUs. We can also passively learn of GPU thermal violations, sync boost violations, hardware slowdowns, and excessive temperatures (&gt; 88¬∞C).</p> <p>As mentioned above, cloud C had a big cooling problem until a few months ago. We‚Äôve seen cloud C GPUs get to 94¬∞C! Performance is crippled at that temperature, around 50% of peak. ü•µ</p> <h3 id="active-healthchecking">Active healthchecking</h3> <p>As active healthchecking requires an exclusive lock on GPUs, it is more complicated to schedule. Overuse active healthchecking and we waste valuable GPU time. Underuse it and we risk leaving around degraded GPUs.</p> <p>Following SemiAnalysis‚Äôs <a rel="nofollow" href="https://www.clustermax.ai/health-checks"><!--[--><!---->ClusterMAX expectations<!--]--></a><!---->, we ensure that each GPU node gets deep, active checking at least weekly. Though we‚Äôve confirmed our underlying cloud providers perform their own deep active healthchecking, they obviously can‚Äôt do their checking while we hold the instances.</p> <p>A lot of our instance capacity is via short (&lt;24hr) rentals, so we don‚Äôt encounter this as much as platforms that rent machines for months. However, we do have some longer-lived capacity. Every week we hold an instance, we run the following active checks:</p> <ul><li>NVIDIA DCGM <code>diag</code> level 2.</li> <li>GPUBurn/GPU-fryer¬†- to validate the GPU won‚Äôt fail under load.</li> <li>Local NCCL all-reduce tests to validate NVLink/NVSwitch/NVLink SHARP performance.</li></ul> <p>If these fail, we are alerted, the instance is not allowed to proceed to accepting tasks, and sometimes we ‚Äúquarantine‚Äù the instance for analysis by ourselves or the underlying provider.</p> <p>In the near future, we are adding these network-oriented active checks due to increasing interest in fast interconnect for training and inference:</p> <ul><li>Local InfiniBand all reduce test for validating InfiniBand performance and links (by force disabling NVLink/p2p/SHM).</li> <li>Pairwise CPU and GPU <code>ib_write_bw</code> and <code>ib_write_latency</code> bidirectional tests to verify that the network is within specs with reference numbers.</li></ul> <h3 id="taking-action">Taking action</h3> <p>In theory it‚Äôs possible to recover from some unhealthy GPU states by isolating and resetting the GPU. In practice, for us, this is overcomplicated and no guarantee of recovery. So instead we automatically mark the entire host unhealthy, drain it, and then either dispose of it or reinstall.</p> <h2 id="observability">Observability</h2> <p><img src="https://modal-cdn.com/blog/images/gpu-metrics-4.webp" alt="GPU metrics"/> <!--[!--><!--]--><!----></p> <p>Our dashboard offers every container a view of its GPU reliability via four metrics:</p> <ul><li>memory usage</li> <li>utilization</li> <li>temperature</li> <li>power usage</li></ul> <p>For lots of detail on how to interpret these, see our <a rel="nofollow" href="https://modal.com/blog/gpu-utilization-guide"><!--[--><!---->previous high-level guide to GPU utilization<!--]--></a><!---->.</p> <p>A caveat is that all of these metrics are currently aggregated at the container level, so they are less effective at spotting a single bad GPU amongst eight.</p> <p>Going beyond metrics, we also pipe abnormal GPU health events into dashboard container logs. See the informational ‚Äúgpu-health‚Äù lines in the screenshot below (indicated with purple).</p> <figure><img src="https://modal-cdn.com/blog/images/gpu-health-logs.webp" alt="gpu-health logs"/> <figcaption>Screenshot of a container&#39;s log stream in Modal, showing the detection of multiple Xid 13 errors.</figcaption></figure> <p>Our guide documentation maintains <a rel="nofollow" href="https://modal.com/docs/guide/gpu-health"><!--[--><!---->a detailed Xid and sXid dictionary<!--]--></a><!----> for understanding errors. We think it‚Äôs the best GPU error resource on the internet.</p> <h2 id="support">Support</h2> <figure><div><div>  <!--[--><!--]--></div><!----></div> <figcaption>Our support metrics across all channels, exported from Pylon.</figcaption></figure> <p>All the above comfortably gets you four nines of GPU uptime. But there‚Äôs always edge-cases and black swans‚Äîfor those you need support.</p> <p>For <a rel="nofollow" href="https://modal.com/pricing"><!--[--><!---->our Enterprise customers we use a shared private Slack channel<!--]--></a><!----> with tight SLAs. Slack is connected to Pylon, tracking issues from creation to resolution. Because Modal is built on top of the cloud giants and designed for dynamic compute autoscaling, we can replace bad GPUs pretty fast!</p> <p>For everyone else we are still responsive in our community channels, and offer credits when we let a GPU go bad without noticing.</p> <h2 id="conclusions">Conclusions</h2> <p>It‚Äôs underappreciated how unreliable GPUs are. NVIDIA‚Äôs hardware is a marvel, the FLOPs are absurd. But the reliability is a drag. A memorable illustration of how AI/ML development is hampered by reliability comes from <a rel="nofollow" href="https://arxiv.org/abs/2407.21783"><!--[--><!---->Meta‚Äôs paper detailing the training process for the LLaMA 3 models<!--]--></a><!---->: ‚ÄúGPU issues are the largest category, accounting for 58.7% of all unexpected issues.‚Äù</p> <p>Imagine the future we‚Äôll enjoy when GPUs are as reliable as CPUs. The Llama3 team‚Äôs CPUs were the problem only 0.5% of the time. In my time at Modal we can‚Äôt remember finding a single degraded CPU core.</p> <p>Until then though, what you‚Äôve read is our commitment to being your GPU reliability team. If you go at it alone, don‚Äôt say you weren‚Äôt warned.</p><!--]--></article><!----><!--]--></div></div>
  </body>
</html>
