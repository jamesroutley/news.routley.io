<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/samuel-vitorino/sopro">Original</a>
    <h1>Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>sopro_readme.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/44442720/532979226-40254391-248f-45ff-b9a4-107d64fbb95f.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5NDM4MzMsIm5iZiI6MTc2Nzk0MzUzMywicGF0aCI6Ii80NDQ0MjcyMC81MzI5NzkyMjYtNDAyNTQzOTEtMjQ4Zi00NWZmLWI5YTQtMTA3ZDY0ZmJiOTVmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA3MjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA1MzEwOWNiMTYxN2FkMmQ0ZjQ4NTg3YjExZjc1YzEyNWQyNzQ3ODlkZjY5MWJlNGE4ZDljNDBmYWM2M2M1NzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YIhsgJS93xRqkB_NSuGazH3YgkWcustDOv_x5shfMUQ" data-canonical-src="https://private-user-images.githubusercontent.com/44442720/532979226-40254391-248f-45ff-b9a4-107d64fbb95f.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5NDM4MzMsIm5iZiI6MTc2Nzk0MzUzMywicGF0aCI6Ii80NDQ0MjcyMC81MzI5NzkyMjYtNDAyNTQzOTEtMjQ4Zi00NWZmLWI5YTQtMTA3ZDY0ZmJiOTVmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA3MjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA1MzEwOWNiMTYxN2FkMmQ0ZjQ4NTg3YjExZjc1YzEyNWQyNzQ3ODlkZjY5MWJlNGE4ZDljNDBmYWM2M2M1NzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YIhsgJS93xRqkB_NSuGazH3YgkWcustDOv_x5shfMUQ" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><a href="https://huggingface.co/samuel-vitorino/sopro" rel="nofollow"><img src="https://camo.githubusercontent.com/a05e88b73ccd0b19d358a98085401f5db500c52c9bdaba999119ccd6577f65ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67466163652d4d6f64656c2d6f72616e67653f6c6f676f3d68756767696e6766616365" alt="Alt Text" data-canonical-src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface"/></a></p>
<p dir="auto">Sopro (from the Portuguese word for “breath/blow”) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (à la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it’s a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.</p>
<p dir="auto">Some of the main features are:</p>
<ul dir="auto">
<li><strong>169M parameters</strong></li>
<li><strong>Streaming</strong></li>
<li><strong>Zero-shot voice cloning</strong></li>
<li><strong>0.25 RTF on CPU</strong> (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds</li>
<li><strong>3-12 seconds of reference audio</strong> for voice cloning</li>
</ul>
<hr/>

<p dir="auto">I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, <code>torch==2.6.0</code> (without <code>torchvision</code>) achieves ~3× more performance.</p>
<p dir="auto">(Optional)</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n soprotts python=3.10
conda activate soprotts"><pre>conda create -n soprotts python=3.10
conda activate soprotts</pre></div>



<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/samuel-vitorino/sopro
cd sopro
pip install -e ."><pre>git clone https://github.com/samuel-vitorino/sopro
<span>cd</span> sopro
pip install -e <span>.</span></pre></div>
<hr/>


<div dir="auto" data-snippet-clipboard-copy-content="soprotts \
  --text &#34;Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU.&#34; \
  --ref_audio ref.wav \
  --out out.wav"><pre>soprotts \
  --text <span><span>&#34;</span>Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU.<span>&#34;</span></span> \
  --ref_audio ref.wav \
  --out out.wav</pre></div>
<p dir="auto">You have the expected <code>temperature</code> and <code>top_p</code> parameters, alongside:</p>
<ul dir="auto">
<li><code>--style_strength</code> (controls the FiLM strength; increasing it can improve or reduce voice similarity; default <code>1.0</code>)</li>
<li><code>--no_stop_head</code> to disable early stopping</li>
<li><code>--stop_threshold</code> and <code>--stop_patience</code> (number of consecutive frames that must be classified as final before <strong>stopping</strong>). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.</li>
</ul>


<div dir="auto" data-snippet-clipboard-copy-content="from sopro import SoproTTS

tts = SoproTTS.from_pretrained(&#34;samuel-vitorino/sopro&#34;, device=&#34;cpu&#34;)

wav = tts.synthesize(
    &#34;Hello! This is a non-streaming Sopro TTS example.&#34;,
    ref_audio_path=&#34;ref.wav&#34;,
)

tts.save_wav(&#34;out.wav&#34;, wav)"><pre><span>from</span> <span>sopro</span> <span>import</span> <span>SoproTTS</span>

<span>tts</span> <span>=</span> <span>SoproTTS</span>.<span>from_pretrained</span>(<span>&#34;samuel-vitorino/sopro&#34;</span>, <span>device</span><span>=</span><span>&#34;cpu&#34;</span>)

<span>wav</span> <span>=</span> <span>tts</span>.<span>synthesize</span>(
    <span>&#34;Hello! This is a non-streaming Sopro TTS example.&#34;</span>,
    <span>ref_audio_path</span><span>=</span><span>&#34;ref.wav&#34;</span>,
)

<span>tts</span>.<span>save_wav</span>(<span>&#34;out.wav&#34;</span>, <span>wav</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="import torch
from sopro import SoproTTS

tts = SoproTTS.from_pretrained(&#34;samuel-vitorino/sopro&#34;, device=&#34;cpu&#34;)

chunks = []
for chunk in tts.stream(
    &#34;Hello! This is a streaming Sopro TTS example.&#34;,
    ref_audio_path=&#34;ref.mp3&#34;,
):
    chunks.append(chunk.cpu())

wav = torch.cat(chunks, dim=-1)
tts.save_wav(&#34;out_stream.wav&#34;, wav)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>sopro</span> <span>import</span> <span>SoproTTS</span>

<span>tts</span> <span>=</span> <span>SoproTTS</span>.<span>from_pretrained</span>(<span>&#34;samuel-vitorino/sopro&#34;</span>, <span>device</span><span>=</span><span>&#34;cpu&#34;</span>)

<span>chunks</span> <span>=</span> []
<span>for</span> <span>chunk</span> <span>in</span> <span>tts</span>.<span>stream</span>(
    <span>&#34;Hello! This is a streaming Sopro TTS example.&#34;</span>,
    <span>ref_audio_path</span><span>=</span><span>&#34;ref.mp3&#34;</span>,
):
    <span>chunks</span>.<span>append</span>(<span>chunk</span>.<span>cpu</span>())

<span>wav</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>chunks</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)
<span>tts</span>.<span>save_wav</span>(<span>&#34;out_stream.wav&#34;</span>, <span>wav</span>)</pre></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Interactive streaming demo</h2><a id="user-content-interactive-streaming-demo" aria-label="Permalink: Interactive streaming demo" href="#interactive-streaming-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/44442720/532988339-a1902bb9-734c-4da8-ad0d-f842fb7da370.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5NDM4MzMsIm5iZiI6MTc2Nzk0MzUzMywicGF0aCI6Ii80NDQ0MjcyMC81MzI5ODgzMzktYTE5MDJiYjktNzM0Yy00ZGE4LWFkMGQtZjg0MmZiN2RhMzcwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA3MjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI0NGJhYjcyZTcxZGM4ODlkY2ZlMDFlY2YyNDM5YzEzMzM0ODIzMjZkMmI2MjUzNzI3YWY1ODYxYjY2YmZjMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.SRS-ahODAGLfPtg66Xp9GhkfgOgN4NaabRdiLn9vwAc"><img src="https://private-user-images.githubusercontent.com/44442720/532988339-a1902bb9-734c-4da8-ad0d-f842fb7da370.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5NDM4MzMsIm5iZiI6MTc2Nzk0MzUzMywicGF0aCI6Ii80NDQ0MjcyMC81MzI5ODgzMzktYTE5MDJiYjktNzM0Yy00ZGE4LWFkMGQtZjg0MmZiN2RhMzcwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA3MjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI0NGJhYjcyZTcxZGM4ODlkY2ZlMDFlY2YyNDM5YzEzMzM0ODIzMjZkMmI2MjUzNzI3YWY1ODYxYjY2YmZjMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.SRS-ahODAGLfPtg66Xp9GhkfgOgN4NaabRdiLn9vwAc" alt="Screenshot"/></a></p>
<p dir="auto">After you install the <code>sopro</code> package:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000"><pre>pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000</pre></div>
<p dir="auto">Or with docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -t sopro-demo .
docker run --rm -p 8000:8000 sopro-demo"><pre>docker build -t sopro-demo <span>.</span>
docker run --rm -p 8000:8000 sopro-demo</pre></div>
<p dir="auto">Navigate to <a href="http://localhost:8000" rel="nofollow">http://localhost:8000</a> on your browser.</p>
<hr/>

<ul dir="auto">
<li>Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.</li>
<li>Voice cloning is <strong>highly dependent</strong> on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.</li>
<li>Prefer phonemes instead of abbreviations and symbols. For example, <code>“1 + 2”</code> → <code>“1 plus 2”</code>. That said, Sopro can generally read abbreviations like “CPU”, “TTS”, etc.</li>
<li>The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.</li>
<li>If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.</li>
<li>I will publish the training code once I have time to organize it.</li>
</ul>
<p dir="auto">Due to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.</p>
<p dir="auto">I didn&#39;t lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.</p>
<p dir="auto">Currently, generation is limited to <strong>~32 seconds (400 frames)</strong>. You can increase it, but the model generally hallucinates beyond that.</p>
<p dir="auto">AI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.</p>
<p dir="auto">I would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: <a href="https://buymeacoffee.com/samuelvitorino" rel="nofollow">https://buymeacoffee.com/samuelvitorino</a></p>
<hr/>

<ul dir="auto">
<li><a href="https://huggingface.co/datasets/amphion/Emilia-Dataset" rel="nofollow">Emilia YODAS</a></li>
<li><a href="https://huggingface.co/datasets/mythicinfinity/libritts_r" rel="nofollow">LibriTTS-R</a></li>
<li><a href="https://datacollective.mozillafoundation.org/" rel="nofollow">Mozilla Common Voice 22</a></li>
<li><a href="https://huggingface.co/datasets/parler-tts/mls_eng_10k" rel="nofollow">MLS</a></li>
</ul>
<hr/>

<ul dir="auto">
<li><a href="https://huggingface.co/kyutai/mimi" rel="nofollow">Mimi Codec (Kyutai)</a></li>
<li><a href="https://arxiv.org/abs/1609.03499" rel="nofollow">WaveNet</a></li>
<li><a href="https://arxiv.org/abs/1803.10963" rel="nofollow">Attentive Stats Pooling</a></li>
<li><a href="https://arxiv.org/pdf/2209.03143" rel="nofollow">AudioLM</a></li>
<li><a href="https://github.com/SesameAILabs/csm">CSM</a></li>
</ul>
</article></div></div>
  </body>
</html>
