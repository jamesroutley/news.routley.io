<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://argilla.io/blog/argilla-for-llms/">Original</a>
    <h1>Open-Source Data Collection Platform for LLM Fine-Tuning and RLHF</h1>
    
    <div id="readability-page-1" class="page"><div data-v-008ddc7e=""><article data-v-008ddc7e=""><div data-v-008ddc7e=""><div data-v-008ddc7e=""><p><!--[-->Through months of fun teamwork and learning from the community, we are thrilled to share our biggest feature to date: Argilla Feedback.<!--]--></p><p><!--[-->Argilla Feedback is <strong><!--[-->completely open-source<!--]--></strong> and the first of its kind at the enterprise level. With its unique focus on scalable human feedback collection, Argilla Feedback is designed to <strong><!--[-->boost the performance and safety of Large Language Models<!--]--></strong> (LLMs).<!--]--></p><p><!--[-->In recent months, interest in applications powered by LLMs has skyrocketed. Yet, this excitement has been tempered by <a href="https://arxiv.org/abs/2305.15717" rel="nofollow"><!--[-->reality checks underlining the critical role of evaluation, alignment, data quality, and human feedback<!--]--></a>.<!--]--></p><p><!--[-->At Argilla, we believe that <strong><!--[-->rigorous evaluation and human feeedback are indispensable for transitioning from LLM experiments and proofs-of-concept to real-world applications<!--]--></strong>.<!--]--></p><p><!--[-->When it comes to deploying safe and reliable software solutions, there are few shortcuts, and this holds true for LLMs. However, there is a notable distinction: <em><!--[-->for LLMs, the primary source of reliability, safety, and accuracy is data<!--]--></em>.<!--]--></p><p><!--[-->After training their latest model, OpenAI dedicated several months to refine its safety and alignment before publicly releasing ChatGPT. The <strong><!--[-->global success of ChatGPT heavily leaned on human feedback for model alignment and safety<!--]--></strong>, which illustrates the crucial role this approach plays in successful AI deployment.<!--]--></p><p><!--[-->Perhaps you assume only a handful of companies have the resources for this. Yet, there&#39;s encouraging news: <strong><!--[-->open-source foundation models<!--]--></strong> are growing more powerful every day, and even small quantities of <strong><!--[-->high-quality, expert-curated data<!--]--></strong> can <a href="https://arxiv.org/abs/2305.11206" rel="nofollow"><!--[-->make LLM accurately follow instructions<!--]--></a>. So, unless you&#39;re poised to launch the next ChatGPT competitor, incorporating human feedback for specific domains is within reach and <strong><!--[-->Argilla is your key to deploying LLM use cases, safely and effectively<!--]--></strong>. Eager to understand why? Read on to discover more!<!--]--></p><blockquote><!--[--><p><!--[-->You can add unlimited users to Argilla so it can be used to <strong><!--[-->seamlessly distribute the workload among hundreds of labelers or experts within your organization<!--]--></strong>. Similar efforts include Dolly from Databricks or OpenAssistant. If you‚Äôd like help setting up such an effort, <a href="https://tally.so/r/mBzNde" rel="nofollow"><!--[-->reach out to us and we‚Äôll gladly help out<!--]--></a>.<!--]--></p><!--]--></blockquote><p><!--[-->
	<video src="/blog/feedback/argilla-feedback.mp4" controls="" muted=""></video>
	<em><!--[-->Argilla Feedback UI: Rate Falcon-7b Responses to Dolly Dataset Prompts.<!--]--></em><!--]--></p><h2 id="argilla-feedback-in-a-nutshell"><a href="#argilla-feedback-in-a-nutshell"><!--[-->Argilla Feedback in a nutshell<!--]--></a></h2><p><!--[-->Argilla Feedback is purpose-built to support <strong><!--[-->customized and multi-aspect feedback in LLM projects<!--]--></strong>. Serving as a critical solution for fine-tuning and Reinforcement Learning from Human Feedback (RLHF), Argilla Feedback provides a flexible platform for the evaluation, monitoring, and fine-tuning tailored to enterprise use cases.<!--]--></p><p><!--[-->Argilla Feedback boosts LLMs use cases through:<!--]--></p><p><!--[--><strong><!--[-->LLM Monitoring and Evaluation<!--]--></strong>: This process assesses LLM projects by collecting both human and machine feedback. Key to this is <a href="https://docs.argilla.io/en/latest/guides/llms/practical_guides/use_argilla_callback_in_langchain.html" rel="nofollow"><!--[-->Argilla&#39;s integration<!--]--></a> with  ü¶úüîó LangChain, which ensures continuous feedback collection for LLM applications.<!--]--></p><p><!--[--><strong><!--[-->Collection of Demonstration Data<!--]--></strong>: It facilitates the gathering of human-guided examples, necessary for <a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/sft.html" rel="nofollow"><!--[-->supervised fine-tuning and instruction-tuning<!--]--></a>.<!--]--></p><p><!--[--><strong><!--[-->Collection of Comparison Data<!--]--></strong>: It plays a significant role in <a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/rm.html" rel="nofollow"><!--[-->collecting comparison data to train reward models<!--]--></a>, a crucial component of LLM evaluation and RLHF.<!--]--></p><p><!--[--><strong><!--[-->Reinforcement Learning<!--]--></strong>: It assists in crafting and selecting prompts for the reinforcement learning stage of RLHF<!--]--></p><blockquote><!--[--><p><!--[--><strong><!--[-->Custom LLMs<!--]--></strong>. We think language models will be fine-tuned in-house and tailored to the requirements of enterprise use cases. To achieve this you need to think about data management and curation as an essential component of the MLOps (or should we say LLMOps) stack.<!--]--></p><!--]--></blockquote><p><!--[-->Throughout these phases, Argilla Feedback <strong><!--[-->streamlines the process of collecting both human and machine feedback, improving the efficiency of LLM refinement and evaluation<!--]--></strong>. The figure below visualizes the key stages in training and fine-tuning LLMs. It highlights the data and expected outcomes at each stage, with particular emphasis on points where human feedback is incorporated.<!--]--></p><p><!--[--><img src="https://www.datadoodad.com/blog/feedback/llm-fine-tune-stages.svg" alt="LLM development stages"/><em><!--[-->LLM development stages, pioneered by the InstructGPT paper, leading to ChatGPT. This figure is adapted from <a href="https://huyenchip.com/2023/05/02/rlhf.html" rel="nofollow"><!--[-->Chip Huyen‚Äôs brilliant post<!--]--></a> ‚ÄúRLHF: Reinforcement Learning from Human Feedback‚Äù<!--]--></em><!--]--></p><blockquote><!--[--><p><!--[--><strong><!--[-->Domain Expertise vs Outsourcing<!--]--></strong>. In Argilla, the process of data labeling and curation is not a single event but an iterative component of the ML lifecycle, setting it apart from traditional data labeling platforms. Argilla integrates into the MLOps stack, using <strong><!--[-->feedback loops for continuous data and model refinement<!--]--></strong>. Given the current complexity of LLM feedback, organizations are increasingly leveraging their own internal knowledge and expertise instead of outsourcing training sets to data labeling services. Argilla supports this shift effectively.<!--]--></p><!--]--></blockquote><p><!--[-->Read on as we detail how Argilla Feedback works, using two example use cases: <strong><!--[-->supervised fine-tuning<!--]--></strong> and <strong><!--[-->reward modelling<!--]--></strong>.<!--]--></p><blockquote><!--[--><p><!--[-->A note to current Argilla users - Argilla Feedback is a new task, fully integrated with the Argilla platform. If you know Argilla already, you can think of Argilla Feedback as a supercharged version of the things our users already love. In fact, it sets the stage for Argilla 2.0, which will integrate other tasks like Text Classification and Token Classification in a more flexible and powerful manner.<!--]--></p><!--]--></blockquote><h2 id="supervised-fine-tuning"><a href="#supervised-fine-tuning"><!--[-->Supervised fine-tuning<!--]--></a></h2><p><!--[-->The importance of demonstration data - <strong><!--[-->prompts and demonstrations<!--]--></strong> - is paramount for improving LLMs. This data aids in supervised fine-tuning, also known as instruction-tuning or behavior cloning, where models learn to respond to instructions based on human examples.<!--]--></p><blockquote><!--[--><p><!--[--><strong><!--[-->Quality over quantity<!--]--></strong>. <a href="https://arxiv.org/abs/2305.11206" rel="nofollow"><!--[-->Recent research<!--]--></a> suggests that a dataset of <strong><!--[-->1,000-2,000 diverse and consistent examples<!--]--></strong> can go a long way in improving instruction-following abilities and response style, surpassing the need for larger demonstration sets used by models like InstructGPT and Dolly.<!--]--></p><!--]--></blockquote><p><!--[-->To visualize the desired effect of supervised fine-tuning, let&#39;s see two examples generated by LLMs. First, the model <a href="https://huggingface.co/tiiuae/falcon-7b" rel="nofollow"><!--[-->Falcon-7B<!--]--></a> is not fine-tuned for the following instructions. We&#39;re using &#34;Email:&#34; at the end of the prompt to nudge the model into writing an email, but it&#39;s still not giving us the results we want:<!--]--></p><table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Prompt<!--]--></th><th><!--[-->Completion<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[-->Write a follow-up for a sales email. Email:<!--]--></td><td><!--[-->I was just curious if you had any questions about the project I discussed with you on the phone yesterday. Please let me know if I can help you with anything else, or any further details that will help you move forward with your project. Thanks again for your time, and have a great weekend, -Sarah<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table><p><!--[-->And the same model after instruction-tuning, the <a href="https://huggingface.co/tiiuae/falcon-7b-instruct" rel="nofollow"><!--[-->Falcon-7B-instruct<!--]--></a> model:<!--]--></p><table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Prompt<!--]--></th><th><!--[-->Completion<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[-->Write a follow-up for a sales email<!--]--></td><td><!--[-->Dear <span>Customer Name</span>,<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table><p><!--[-->The steps to build this workflow are: <strong><!--[-->configure the dataset to ask for completions<!--]--></strong>, <strong><!--[-->add records<!--]--></strong>, <strong><!--[-->gathering feedback from labelers<!--]--></strong>, <strong><!--[-->prepare the dataset<!--]--></strong>, and <strong><!--[-->fine-tuning the LLM<!--]--></strong>. Read on to learn more about each step and see below a visual representation.<!--]--></p><p><!--[--><img src="https://www.datadoodad.com/blog/feedback/sft.svg" alt="comparison-data"/><em><!--[-->Human-in-the-loop workflow for demonstration data collection and SFT<!--]--></em><!--]--></p><h3 id="configure-the-dataset"><a href="#configure-the-dataset"><!--[-->Configure the dataset<!--]--></a></h3><p><!--[-->First, we need to configure a <strong><!--[-->dataset<!--]--></strong>. Argilla datasets allow you to mix different <strong><!--[-->questions<!--]--></strong> for labelers to answer. In this case, we want to collect <strong><!--[-->demonstrations<!--]--></strong> from our labelers.
With Argilla&#39;s Python SDK, you set up a <code><!--[-->TextQuestion<!--]--></code> for your labelers to write the demonstration and a <code><!--[-->TextField<!--]--></code> to show them a prompt. You can set up the dataset with the following code snippet:<!--]--></p><!--[--><pre><code><span><span>import</span><span> argilla </span><span>as</span><span> rg</span></span><span></span><span><span>questions </span><span>=</span><span> </span><span>[</span></span><span><span>    rg</span><span>.</span><span>TextQuestion</span><span>(</span></span><span><span>        </span><span>name</span><span>=</span><span>&#34;</span><span>completion</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>title</span><span>=</span><span>&#34;</span><span>Please write an accurate, helpful, and harmless response to the prompt</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>required</span><span>=True,</span></span><span><span>    </span><span>)</span></span><span><span>]</span></span><span></span><span><span>fields </span><span>=</span><span> </span><span>[</span></span><span><span>    rg</span><span>.</span><span>TextField</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>,</span><span> </span><span>required</span><span>=True),</span></span><span><span>]</span></span><span></span><span><span>dataset </span><span>=</span><span> rg</span><span>.</span><span>FeedbackDataset</span><span>(</span></span><span><span>    </span><span>guidelines</span><span>=</span><span>&#34;</span><span>Please, read the prompt carefully and write a response</span><span>&#34;</span><span>,</span></span><span><span>    </span><span>questions</span><span>=</span><span>questions</span><span>,</span></span><span><span>    </span><span>fields</span><span>=</span><span>fields</span></span><span><span>)</span></span></code></pre><!--]--><h3 id="add-records"><a href="#add-records"><!--[-->Add records<!--]--></a></h3><p><!--[-->Argilla Datasets are composed of <strong><!--[-->records<!--]--></strong>. A <strong><!--[-->record<!--]--></strong> is a data point that can be labeled by one or more labelers. In the case of supervised fine-tuning our goal is gather human-written responses to prompts. There are many alternatives for collecting prompts, that range from asking labelers to write them to using an open dataset to generating them using an LLM. We cover these <a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/sft.html#add-records" rel="nofollow"><!--[-->different scenarios in detail in the docs<!--]--></a>. Let&#39;s assume we have a dataset with prompts, this is how to build and push the records:<!--]--></p><!--[--><pre><code><span><span>from</span><span> datasets </span><span>import</span><span> load_dataset</span></span><span></span><span><span># This is only for demonstration and assumes you use a HF dataset</span></span><span><span>prompts </span><span>=</span><span> </span><span>load_dataset</span><span>(</span><span>&#39;</span><span>your_prompts_dataset</span><span>&#39;</span><span>,</span><span> </span><span>split</span><span>=[</span><span>&#34;</span><span>train</span><span>&#34;</span><span>])</span></span><span></span><span><span>records </span><span>=</span><span> </span><span>[</span></span><span><span>    rg</span><span>.</span><span>FeedbackRecord</span><span>(</span><span>fields</span><span>={</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>:</span><span> record</span><span>[</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>]})</span></span><span><span>    </span><span>for</span><span> record </span><span>in</span><span> dataset</span></span><span><span>]</span></span><span></span><span><span>dataset</span><span>.</span><span>add_records</span><span>(</span><span>records</span><span>)</span></span><span></span><span><span># This publishes the dataset and pushes the records into Argilla</span></span><span><span>dataset</span><span>.</span><span>push_to_argilla</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>my-dataset</span><span>&#34;</span><span>,</span><span> </span><span>workspace</span><span>=</span><span>&#34;</span><span>my-workspace</span><span>&#34;</span><span>)</span></span></code></pre><!--]--><h3 id="gathering-feedback-from-labelers"><a href="#gathering-feedback-from-labelers"><!--[-->Gathering feedback from labelers<!--]--></a></h3><p><!--[-->Argilla Feedback allows simultaneous feedback collection from multiple users, enhancing quality control. Each user with dataset access can give feedback. However, when resources are limited, workload distribution among various labelers is recommended. This strategy entails assigning each labeler a subset of the total records. In the docs, we provide <a href="https://docs.argilla.io/en/latest/guides/llms/practical_guides/set_up_annotation_team.html" rel="nofollow"><!--[-->detailed instructions for setting up these workload distribution options effectively<!--]--></a>.<!--]--></p><h3 id="prepare-dataset-and-fine-tune-the-llm"><a href="#prepare-dataset-and-fine-tune-the-llm"><!--[-->Prepare dataset and fine-tune the LLM<!--]--></a></h3><p><!--[-->After collecting feedback from labelers, there&#39;s two steps left: preparing the dataset, including the resolution and aggregation of responses from multiple labelers, and fine-tune the LLM.<!--]--></p><p><!--[-->Argilla&#39;s documentation covers several <a href="https://docs.argilla.io/en/latest/guides/llms/practical_guides/collect_responses.html" rel="nofollow"><!--[-->approaches to handle disagreements and merge feedback from multiple labelers<!--]--></a>. For instruction-tuning, the typical format consists of a <code><!--[-->prompt<!--]--></code>, a <code><!--[-->response<!--]--></code>, and potentially an optional <code><!--[-->inputs<!--]--></code> field (like in the Alpaca and Dolly datasets).<!--]--></p><p><!--[-->For fine-tuning, you can use <a href="https://huggingface.co/autotrain" rel="nofollow"><!--[-->Hugging Face AutoTrain<!--]--></a>, <a href="https://github.com/huggingface/peft" rel="nofollow"><!--[-->peft/Lora<!--]--></a>, and many others like MosaicML, or Lightning. Argilla&#39;s documentation covers <a href="https://docs.argilla.io/en/latest/guides/llms/practical_guides/fine_tune.html" rel="nofollow"><!--[-->several approaches in detail<!--]--></a>.<!--]--></p><p><!--[-->Now, let&#39;s move on to the next use case: reward modeling.<!--]--></p><h2 id="reward-modelling"><a href="#reward-modelling"><!--[-->Reward modelling<!--]--></a></h2><p><!--[-->Collecting <strong><!--[-->comparison data<!--]--></strong> to train a reward model is a crucial part of RLHF and LLM evaluation. This phase involves training a reward model to <strong><!--[-->align responses with human preferences<!--]--></strong>. Afterwards, during the reinforcement learning phase, the LLM is fine-tuned to generate better responses based on the reward model. In contrast to how the reward model scores prompt-response pairs, comparison data collection typically requires humans (and machines) <strong><!--[-->to rank several responses to a single prompt<!--]--></strong>.<!--]--></p><p><!--[-->The steps to build this workflow are: <strong><!--[-->configure the dataset to ask for comparisons<!--]--></strong>, <strong><!--[-->add records<!--]--></strong>, <strong><!--[-->gathering feedback<!--]--></strong>, <strong><!--[-->prepare the dataset<!--]--></strong>, and <strong><!--[-->train the reward model<!--]--></strong>. Read on to learn more about each step and see the figure below for a visual representation.<!--]--></p><p><!--[--><img src="https://www.datadoodad.com/blog/feedback/rm.svg" alt="comparison-data"/><em><!--[-->Human-in-the-loop workflow for comparison data collection and reward modelling<!--]--></em><!--]--></p><blockquote><!--[--><p><!--[--><strong><!--[-->Human Preference Optimization<!--]--></strong>. The recent study, <a href="https://arxiv.org/abs/2305.18290" rel="nofollow"><!--[-->&#34;Direct Preference Optimization: Your Language Model is Secretly a Reward Model&#34;<!--]--></a> proposes the promising possibility of using comparison data directly, eliminating the need for a reward model. Nevertheless, the collection of comparison data continues to be crucial for steering LLMs.<!--]--></p><!--]--></blockquote><h3 id="configure-the-dataset-1"><a href="#configure-the-dataset-1"><!--[-->Configure the dataset<!--]--></a></h3><p><!--[-->Let&#39;s start by creating a dataset for collecting ranked responses. With Argilla&#39;s Python SDK, you set up a <code><!--[-->RatingQuestion<!--]--></code> for your labelers to answer, an optional <code><!--[-->TextQuestion<!--]--></code> to write a corrected response, and a <code><!--[-->TextField<!--]--></code> to present them with a prompt. You can set up the dataset with the following code snippet:<!--]--></p><!--[--><pre><code><span><span>import</span><span> argilla </span><span>as</span><span> rg</span></span><span></span><span><span>questions </span><span>=</span><span> </span><span>[</span></span><span><span>    rg</span><span>.</span><span>RatingQuestion</span><span>(</span></span><span><span>        </span><span>name</span><span>=</span><span>&#34;</span><span>response_ranking</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>title</span><span>=</span><span>&#34;</span><span>Rank the responses</span><span>\n</span><span>1: first response is better,</span><span>\n</span><span> 2: second response is better,</span><span>\n</span><span>3: both are equal</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>required</span><span>=True,</span></span><span><span>        </span><span>values</span><span>=[</span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span>3</span><span>]</span></span><span><span>    </span><span>),</span></span><span><span>    rg</span><span>.</span><span>TextQuestion</span><span>(</span></span><span><span>        </span><span>name</span><span>=</span><span>&#34;</span><span>correct_response</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>title</span><span>=</span><span>&#34;</span><span>If none of the responses are helpful and correct, provide the response</span><span>&#34;</span><span>,</span></span><span><span>        </span><span>required</span><span>=False</span></span><span><span>    </span><span>),</span></span><span><span>]</span></span><span></span><span><span>fields </span><span>=</span><span> </span><span>[</span></span><span><span>    rg</span><span>.</span><span>TextField</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>,</span><span> </span><span>required</span><span>=True),</span></span><span><span>    rg</span><span>.</span><span>TextField</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>response-1</span><span>&#34;</span><span>,</span><span> </span><span>required</span><span>=True),</span></span><span><span>    rg</span><span>.</span><span>TextField</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>response-2</span><span>&#34;</span><span>,</span><span> </span><span>required</span><span>=True)</span></span><span><span>]</span></span><span></span><span><span>dataset </span><span>=</span><span> rg</span><span>.</span><span>FeedbackDataset</span><span>(</span></span><span><span>    </span><span>guidelines</span><span>=</span><span>&#34;</span><span>Please, read the prompt carefully and...</span><span>&#34;</span><span>,</span></span><span><span>    </span><span>questions</span><span>=</span><span>questions</span><span>,</span></span><span><span>    </span><span>fields</span><span>=</span><span>fields</span></span><span><span>)</span></span></code></pre><!--]--><p><!--[-->This will configure the following UI. Note that Argilla datasets are highly configurable so you can add any fields and questions you need for your use case:<!--]--></p><p><!--[--><img src="https://www.datadoodad.com/blog/feedback/comparison-collection.png" alt="comparison-collection.png"/><em><!--[-->Argilla UI for this custom use case with Falcon-7B generations for comparison data collection<!--]--></em><!--]--></p><blockquote><!--[--><p><!--[-->This example involves ranking two responses per prompt, but you can modify it to handle more. Keep an eye out for the RankingQuestion in future Argilla updates, which is designed to optimize this process. You can <a href="https://github.com/argilla-io/argilla" rel="nofollow"><!--[-->follow the progress on GitHub<!--]--></a><!--]--></p><!--]--></blockquote><h3 id="add-records-1"><a href="#add-records-1"><!--[-->Add records<!--]--></a></h3><p><!--[-->Now, let&#39;s move on to the next step: adding records to the dataset and making them available for labelers. Each record will include a <strong><!--[-->prompt and two generated responses<!--]--></strong>. These records will be shown to labelers in the Argilla user interface, where <strong><!--[-->they will be asked to rank the two responses<!--]--></strong>. During this stage, it&#39;s crucial to consider how to generate the responses effectively to ensure the resulting LLM achieves optimal quality and diversity.<!--]--></p><p><!--[-->To generate the responses, you can employ a pre-trained LLM that has been fine-tuned on a previous dataset. Various strategies can be used, such as generating multiple responses and selecting two of them, or generating two responses with different parameters, such as temperature settings.<!--]--></p><p><!--[-->If you have a instruction-tuned LLM in mind, here&#39;s an <strong><!--[-->example using the instruction-following model Falcon-7B-instruct<!--]--></strong> to generate the responses and create Argilla records:<!--]--></p><!--[--><pre><code><span><span># Load the model and tokenizer</span></span><span><span>model </span><span>=</span><span> AutoModelForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span><span>&#34;</span><span>tiiuae/falcon-7b-instruct</span><span>&#34;</span><span>)</span></span><span><span>tokenizer </span><span>=</span><span> AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>&#34;</span><span>tiiuae/falcon-7b-instruct</span><span>&#34;</span><span>)</span></span><span></span><span><span># Create a pipeline for text generation</span></span><span><span>gen_pipeline </span><span>=</span><span> </span><span>pipeline</span><span>(</span></span><span><span>    </span><span>&#34;</span><span>text-generation</span><span>&#34;</span><span>,</span></span><span><span>    </span><span>model</span><span>=</span><span>model</span><span>,</span></span><span><span>    </span><span>tokenizer</span><span>=</span><span>tokenizer</span><span>,</span></span><span><span>    </span><span>torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span><span>,</span></span><span><span>    </span><span>device_map</span><span>=</span><span>&#34;</span><span>auto</span><span>&#34;</span><span>,</span></span><span><span>)</span></span><span></span><span><span># Load your dataset of prompts</span></span><span><span>prompts </span><span>=</span><span> </span><span>load_dataset</span><span>(</span><span>&#34;</span><span>your_prompts_dataset</span><span>&#34;</span><span>,</span><span> </span><span>split</span><span>=[</span><span>&#34;</span><span>train</span><span>&#34;</span><span>])</span></span><span></span><span><span>records </span><span>=</span><span> </span><span>[]</span></span><span><span>for</span><span> record </span><span>in</span><span> prompts</span><span>:</span></span><span><span>    prompt </span><span>=</span><span> record</span><span>[</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>]</span></span><span></span><span><span>    </span><span># Generate two responses in one call</span></span><span><span>    outputs </span><span>=</span><span> </span><span>gen_pipeline</span><span>(</span></span><span><span>        prompt</span><span>,</span></span><span><span>        </span><span>max_length</span><span>=</span><span>100</span><span>,</span></span><span><span>        </span><span>do_sample</span><span>=True,</span></span><span><span>        </span><span>top_k</span><span>=</span><span>10</span><span>,</span></span><span><span>        </span><span>num_return_sequences</span><span>=</span><span>2</span><span>,</span></span><span><span>        </span><span>eos_token_id</span><span>=</span><span>tokenizer</span><span>.</span><span>eos_token_id</span><span>,</span></span><span><span>    </span><span>)</span></span><span><span>    responses </span><span>=</span><span> </span><span>[</span><span>output</span><span>[</span><span>&#34;</span><span>generated_text</span><span>&#34;</span><span>]</span><span> </span><span>for</span><span> output </span><span>in</span><span> outputs</span><span>]</span></span><span></span><span><span>    record </span><span>=</span><span> rg</span><span>.</span><span>FeedbackRecord</span><span>(</span><span>fields</span><span>={</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>:</span><span> prompt</span><span>,</span><span> </span><span>&#34;</span><span>response 1</span><span>&#34;</span><span>:</span><span> responses</span><span>[</span><span>0</span><span>],</span><span> </span><span>&#34;</span><span>response 2</span><span>&#34;</span><span>:</span><span> responses</span><span>[</span><span>1</span><span>]})</span></span><span><span>    records</span><span>.</span><span>append</span><span>(</span><span>record</span><span>)</span></span><span></span><span><span># Add records to the dataset</span></span><span><span>dataset</span><span>.</span><span>add_records</span><span>(</span><span>records</span><span>)</span></span><span></span><span><span># This publishes the dataset and pushes the records into Argilla</span></span><span><span>dataset</span><span>.</span><span>push_to_argilla</span><span>(</span><span>name</span><span>=</span><span>&#34;</span><span>my-dataset</span><span>&#34;</span><span>,</span><span> </span><span>workspace</span><span>=</span><span>&#34;</span><span>my-workspace</span><span>&#34;</span><span>)</span></span></code></pre><!--]--><h3 id="prepare-the-dataset-and-train-a-reward-model"><a href="#prepare-the-dataset-and-train-a-reward-model"><!--[-->Prepare the dataset and train a reward model<!--]--></a></h3><p><!--[-->In the docs, we cover how to <a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/rm.html#prepare-the-dataset" rel="nofollow"><!--[-->prepare the data in this format<!--]--></a> for training a Reward Model using the <code><!--[-->trl<!--]--></code> <a href="https://huggingface.co/docs/trl/" rel="nofollow"><!--[-->framework<!--]--></a>. Let&#39;s look at the code to train a Reward Model using this dataset created by the Argilla team. This model uses comparison data created with the Dolly dataset from Databricks and Falcon-7B-Instruct, a small version of the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow"><!--[-->current strongest open-source LLM model<!--]--></a> as of June 1st ü§ó.<!--]--></p><!--[--><pre><code><span><span>from</span><span> transformers </span><span>import</span><span> </span><span>(</span></span><span><span>    AutoModelForSequenceClassification</span><span>,</span></span><span><span>    AutoTokenizer</span><span>,</span></span><span><span>    TrainingArguments</span><span>,</span></span><span><span>)</span></span><span></span><span><span>from</span><span> trl </span><span>import</span><span> RewardTrainer</span></span><span></span><span><span>from</span><span> datasets </span><span>import</span><span> load_dataset</span></span><span></span><span><span>dataset </span><span>=</span><span> </span><span>load_dataset</span><span>(</span><span>&#34;</span><span>argilla/dolly-curated-comparison-falcon-7b-instruct</span><span>&#34;</span><span>,</span><span> </span><span>split</span><span>=</span><span>&#34;</span><span>train</span><span>&#34;</span><span>)</span></span><span></span><span><span>model_name </span><span>=</span><span> </span><span>&#34;</span><span>distilroberta-base</span><span>&#34;</span></span><span></span><span><span>model </span><span>=</span><span> AutoModelForSequenceClassification</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>,</span><span> </span><span>num_labels</span><span>=</span><span>1</span><span>)</span></span><span><span>tokenizer </span><span>=</span><span> AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span></span><span></span><span><span>if</span><span> tokenizer</span><span>.</span><span>pad_token</span><span> </span><span>is</span><span> </span><span>None:</span></span><span><span>    tokenizer</span><span>.</span><span>pad_token</span><span> </span><span>=</span><span> tokenizer</span><span>.</span><span>eos_token</span></span><span><span>    model</span><span>.</span><span>config</span><span>.</span><span>pad_token_id</span><span> </span><span>=</span><span> model</span><span>.</span><span>config</span><span>.</span><span>eos_token_id</span></span><span></span><span><span>def</span><span> </span><span>formatting_func</span><span>(</span><span>examples</span><span>):</span></span><span><span>    kwargs </span><span>=</span><span> </span><span>{</span><span>&#34;</span><span>padding</span><span>&#34;</span><span>:</span><span> </span><span>&#34;</span><span>max_length</span><span>&#34;</span><span>,</span><span> </span><span>&#34;</span><span>truncation</span><span>&#34;</span><span>:</span><span> </span><span>True,</span><span> </span><span>&#34;</span><span>max_length</span><span>&#34;</span><span>:</span><span> </span><span>512</span><span>,</span><span> </span><span>&#34;</span><span>return_tensors</span><span>&#34;</span><span>:</span><span> </span><span>&#34;</span><span>pt</span><span>&#34;</span><span>}</span></span><span></span><span><span>    </span><span># Assuming original human response is preferred to Falcon&#39;s</span></span><span><span>    chosen_response </span><span>=</span><span> examples</span><span>[</span><span>&#34;</span><span>original_response</span><span>&#34;</span><span>]</span></span><span><span>    rejected_response </span><span>=</span><span> examples</span><span>[</span><span>&#34;</span><span>response-1</span><span>&#34;</span><span>]</span></span><span><span>    prompt </span><span>=</span><span> examples</span><span>[</span><span>&#34;</span><span>prompt</span><span>&#34;</span><span>]</span></span><span></span><span><span>    tokens_chosen </span><span>=</span><span> tokenizer</span><span>.</span><span>encode_plus</span><span>(</span><span>prompt</span><span>,</span><span> chosen_response</span><span>,</span><span> </span><span>**</span><span>kwargs</span><span>)</span></span><span><span>    tokens_rejected </span><span>=</span><span> tokenizer</span><span>.</span><span>encode_plus</span><span>(</span><span>prompt</span><span>,</span><span> rejected_response</span><span>,</span><span> </span><span>**</span><span>kwargs</span><span>)</span></span><span></span><span><span>    </span><span>return</span><span> </span><span>{</span></span><span><span>        </span><span>&#34;</span><span>input_ids_chosen</span><span>&#34;</span><span>:</span><span> tokens_chosen</span><span>[</span><span>&#34;</span><span>input_ids</span><span>&#34;</span><span>][</span><span>0</span><span>],</span><span> </span><span>&#34;</span><span>attention_mask_chosen</span><span>&#34;</span><span>:</span><span> tokens_chosen</span><span>[</span><span>&#34;</span><span>attention_mask</span><span>&#34;</span><span>][</span><span>0</span><span>],</span></span><span><span>        </span><span>&#34;</span><span>input_ids_rejected</span><span>&#34;</span><span>:</span><span> tokens_rejected</span><span>[</span><span>&#34;</span><span>input_ids</span><span>&#34;</span><span>][</span><span>0</span><span>],</span><span> </span><span>&#34;</span><span>attention_mask_rejected</span><span>&#34;</span><span>:</span><span> tokens_rejected</span><span>[</span><span>&#34;</span><span>attention_mask</span><span>&#34;</span><span>][</span><span>0</span><span>]</span></span><span><span>    </span><span>}</span></span><span></span><span><span>formatted_dataset </span><span>=</span><span> dataset</span><span>.</span><span>map</span><span>(</span><span>formatting_func</span><span>)</span></span><span></span><span><span>trainer </span><span>=</span><span> </span><span>RewardTrainer</span><span>(</span></span><span><span>    </span><span>model</span><span>=</span><span>model</span><span>,</span></span><span><span>    </span><span>args</span><span>=</span><span>TrainingArguments</span><span>(</span><span>&#34;</span><span>output_dir</span><span>&#34;</span><span>),</span></span><span><span>    </span><span>tokenizer</span><span>=</span><span>tokenizer</span><span>,</span></span><span><span>    </span><span>train_dataset</span><span>=</span><span>formatted_dataset</span></span><span><span>)</span></span><span></span><span><span>trainer</span><span>.</span><span>train</span><span>()</span></span></code></pre><!--]--><p><!--[-->After running this step, we have a reward model ready to use for RLHF and LLM evaluation!<!--]--></p><p><!--[-->If you want to know more about the resulting reward model and how it‚Äôs been built, it‚Äôs 100% open-source and <a href="https://huggingface.co/argilla/roberta-base-reward-model-falcon-dolly" rel="nofollow"><!--[-->available on the Hugging Face Hub<!--]--></a>.<!--]--></p><p><!--[-->Below you can find the score of some examples (remember the RM has been trained to prefer the original response):<!--]--></p><div><table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Prompt<!--]--></th><th><!--[-->Original Response<!--]--></th><th><!--[-->Response-1<!--]--></th><th><!--[-->Score Original<!--]--></th><th><!--[-->Score Response-1<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[-->What is Depreciation<!--]--></td><td><!--[-->Depreciation is the drop in value of an asset due to wear and tear, age and obsolescence (going out of date) as recorded in an organization&#39;s financial records.<!--]--></td><td><!--[-->What is Depreciation ‚Äì 10 Important Facts to Know?<!--]--></td><td><!--[-->1.9764<!--]--></td><td><!--[-->-9.01173<!--]--></td><!--]--></tr><tr><!--[--><td><!--[-->What do you know about the city of Aberdeen in Scotland?<!--]--></td><td><!--[-->Aberdeen is a city located in the North East of Scotland. As of 2020 it had a population of 200,000 people and is the third largest city in Scotland, behind Edinburgh (435,000) and Glasgow (610,000). Aberdeen has been referred to as &#34;The Silver City&#34; due to the high number of buildings in the city constructed of Granite from the local quarry, which may sparkle on the rare occasion that the sunlight hits the buildings. Since the 1960&#39;s the city&#39;s economy has largely revolved around Oil &amp; Gas due to its proximity to the North Sea Oil discovery. Aberdeen has two universities; the University of Aberdeen and Robert Gordon University. Notable people who have been associated with Aberdeen include singer Annie Lennox, golfer Paul Lawrie and architect Archibald &#34;Archie&#34; Simpson.<!--]--></td><td><!--[-->Aberdeen, the city in which I&#39;ve lived for the past 4 years, is the 3rd largest city in Scotland with a population of around 700,000. It has a long history, with evidence of human occupation dating back around 5000 years. The city is home to the famous &#34;Rock&#34; of Aberdeen (which is in fact the city itself) as well as several universities and is one of the biggest financial centers in the country.<!--]--></td><td><!--[-->7.03729<!--]--></td><td><!--[-->-6.4728<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table></div><h2 id="whats-next"><a href="#whats-next"><!--[-->What‚Äôs next?<!--]--></a></h2><p><!--[-->This blog post only scratches the surface of what&#39;s possible with Argilla. We&#39;re thrilled to play a key role in the evolution of open-source LLMs. Our roadmap includes integrating already-loved Argilla features like <strong><!--[-->suggestions (now known as predictions) from multiple models and rules, active learning for SFT and reward modeling, vector search, and weak supervision into this new paradigm<!--]--></strong>.<!--]--></p><p><!--[-->Check out the <a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/data_model.html" rel="nofollow"><!--[-->Argilla Feedback Data Model<!--]--></a> docs for a sneak peek at upcoming features (see below figure).<!--]--></p><p><!--[--><img src="https://www.datadoodad.com/blog/feedback/data-model.svg" alt="data-model"/><em><!--[-->Argilla Feedback Data Model and upcoming features<!--]--></em><!--]--></p><p><!--[-->Stay tuned for more updates! If you need assistance setting up human-in-the-loop workflows for your LLM use cases, <a href="https://tally.so/r/mBzNde" rel="nofollow"><!--[-->reach out to us and we‚Äôll gladly help out<!--]--></a>.<!--]--></p><h2 id="get-started-with-argilla-feedback"><a href="#get-started-with-argilla-feedback"><!--[-->Get started with Argilla Feedback<!--]--></a></h2><p><!--[-->You can <a href="https://docs.argilla.io/en/latest/getting_started/cheatsheet.html" rel="noopener noreferrer" target="_blank"><!--[--> self-host Argilla using one of the many deployment options<!--]--></a>, sign-up for our upcoming Argilla Cloud version, or launch an Argilla Space on the Hugging Face with this one-click deployment button:<!--]--></p><div>
	<a href="http://huggingface.co/new-space?template=argilla/argilla-template-space" rel="noopener noreferrer" target="_blank"><!--[-->
	    <img src="https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg" alt=""/>
	<!--]--></a></div><p><!--[--><a href="https://docs.argilla.io/en/latest/guides/llms/conceptual_guides/conceptual_guides.html" rel="nofollow"><!--[-->Conceptual Guides<!--]--></a>: An extensive overview about how to use Argilla for LLM data collection, fine-tuning, and RLHF.<!--]--></p><p><!--[--><a href="https://docs.argilla.io/en/latest/guides/llms/practical_guides/practical_guides.html" rel="nofollow"><!--[-->How-to-Guides<!--]--></a>: A practical, hands-on introduction to Argilla Feedback.<!--]--></p></div></div></article></div></div>
  </body>
</html>
