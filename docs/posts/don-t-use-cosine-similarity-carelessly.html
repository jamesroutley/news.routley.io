<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/">Original</a>
    <h1>Don&#39;t use cosine similarity carelessly</h1>
    
    <div id="readability-page-1" class="page"><article><div><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/python-summit-2024-warsaw-migdal-cosine-talking.jpg" alt="" data-v-038762c4=""/></p><p><!--[-->Midas turned everything he touched into gold. Data scientists turn everything into vectors.
We do it for a reason — as gold is the language of merchants, vectors are the language of AI<sup><a aria-current="page" href="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-1"><!--[-->1<!--]--></a></sup>.<!--]--></p><p><!--[-->Just as Midas discovered that turning everything to gold wasn&#39;t always helpful, we&#39;ll see that blindly applying cosine similarity to vectors can lead us astray. While embeddings do capture similarities, they often reflect the wrong kind - matching questions to questions rather than questions to answers, or getting distracted by superficial patterns like writing style and typos rather than meaning. This post shows you how to be more intentional about similarity and get better results.<!--]--></p><h2 id="embeddings"><a href="#embeddings"><!--[-->Embeddings<!--]--></a></h2><p><!--[-->Embeddings are so captivating that my most popular blog post remains <a href="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why" rel="nofollow"><!--[-->king - man + woman = queen; but why?<!--]--></a>.
We have <a href="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why" rel="nofollow"><!--[-->word2vec<!--]--></a>, <a href="https://snap.stanford.edu/node2vec/" rel="nofollow"><!--[-->node2vec<!--]--></a>, <a href="https://jaan.io/food2vec-augmented-cooking-machine-intelligence/" rel="nofollow"><!--[-->food2vec<!--]--></a>, <a href="https://github.com/warchildmd/game2vec" rel="nofollow"><!--[-->game2vec<!--]--></a>, and if you can name it, someone has probably turned it into a vec. If not yet, it&#39;s your turn!<!--]--></p><p><!--[-->When we work with raw IDs, we&#39;re blind to relationships. Take the words &#34;brother&#34; and &#34;sister&#34; — to a computer, they might as well be &#34;xkcd42&#34; and &#34;banana&#34;. But with vectors, we can chart entities and relationships between them — both to provide as a structured input to a machine learning models, and on its own, to find similar items.<!--]--></p><p><!--[-->Let&#39;s focus on sentence embeddings from Large Language Models (LLMs), as they are one of the most popular use cases for embeddings. Modern LLMs are so powerful at this that they can capture the essence of text without any fine-tuning. In fact, recent research shows these embeddings are almost as revealing as the original text - see Morris et al., <a href="https://arxiv.org/abs/2310.06816" rel="nofollow"><!--[-->Text Embeddings Reveal (Almost) As Much As Text<!--]--></a>, (2023). Yet, with great power comes great responsibility.<!--]--></p><h2 id="example"><a href="#example"><!--[-->Example<!--]--></a></h2><p><!--[-->Let&#39;s look at three sentences:<!--]--></p><ul><!--[--><li><!--[-->A: <em><!--[-->&#34;Python can make you rich.&#34;<!--]--></em><!--]--></li><li><!--[-->B: <em><!--[-->&#34;Python can make you itch.&#34;<!--]--></em><!--]--></li><li><!--[-->C: <em><!--[-->&#34;Mastering Python can fill your pockets.&#34;<!--]--></em><!--]--></li><!--]--></ul><p><!--[-->If you treated them as raw IDs, there are different strings, with no notion of similarity. Using string similarity (<a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow"><!--[-->Levenshtein distance<!--]--></a>), A and B differ by 2 characters, while A and C are 21 characters apart. Yet semantically (unless you&#39;re allergic to money), A is closer to C than B.<!--]--></p><p><!--[-->We can use <a href="https://platform.openai.com/docs/guides/embeddings" rel="nofollow"><!--[-->OpenAI text-embedding-3-large<!--]--></a>, to get the following vectors:<!--]--></p><ul><!--[--><li><!--[-->A: <code><!--[-->[-0.003738, -0.033263, -0.017596,  0.029024, -0.015251, ...]<!--]--></code><!--]--></li><li><!--[-->B: <code><!--[-->[-0.066795, -0.052274, -0.015973,  0.077706,  0.044226, ...]<!--]--></code><!--]--></li><li><!--[-->C: <code><!--[-->[-0.011167,  0.017812, -0.018655,  0.006625,  0.018506, ...]<!--]--></code><!--]--></li><!--]--></ul><p><!--[-->These vectors are quite long - text-embedding-3-large has up 3072 dimensions - to the point that <a href="https://openai.com/index/new-embedding-models-and-api-updates/" rel="nofollow"><!--[-->we can truncate them at a minimal loss of quality<!--]--></a>.
When we calculate cosine similarity, we get 0.750 between A and C (the semantically similar sentences), and 0.576 between A and B (the lexically similar ones). These numbers align with what we&#39;d expect - the meaning matters more than spelling!<!--]--></p><h2 id="what-is-cosine-similarity"><a href="#what-is-cosine-similarity"><!--[-->What is cosine similarity?<!--]--></a></h2><p><!--[-->When comparing vectors, there&#39;s a temptingly simple solution that every data scientist reaches for — cosine similarity:<!--]--></p><p><!--[-->Geometrically speaking, it is the cosine of the angle between two vectors. However, I avoid thinking about it this way - we&#39;re dealing with spaces of dozens, hundreds, or thousands of dimensions. Our geometric intuition fails us in such high-dimensional spaces, and we shouldn&#39;t pretend otherwise.<!--]--></p><p><!--[-->From a numerical perspective, it is a dot product with normalized vectors.
It has some appealing properties:<!--]--></p><ul><!--[--><li><!--[-->Identical vectors score a perfect 1.<!--]--></li><li><!--[-->Random vectors hover around 0 (there are many dimensions, so it averages out).<!--]--></li><li><!--[-->The result is between -1 and 1.<!--]--></li><!--]--></ul><p><!--[-->Yet, this simplicity is misleading. Just because the values usually fall between 0 and 1 doesn&#39;t mean they represent probabilities or any other meaningful metric. The value 0.6 tells little if we have something really similar, or not so much. And while negative values are possible, they rarely indicate semantic opposites — more often, the opposite of something is gibberish.<!--]--></p><figure data-v-15401076=""><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/glove-dog-cosine-similarity.png" alt="" data-v-15401076="" data-v-038762c4=""/></p><figcaption data-v-15401076=""><!--[-->When using cosine similarity on <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow" data-v-15401076=""><!--[-->Glove vectors<!--]--></a> (<code data-v-15401076=""><!--[-->glove.6B.300d<!--]--></code>), the closest words to &#34;dog&#34; are predictable, the farthest - not. You can play with it <a href="https://colab.research.google.com/github/stared/thinking-in-tensors-writing-in-pytorch/blob/master/rnns/Word%20vectors.ipynb" rel="nofollow" data-v-15401076=""><!--[-->here<!--]--></a>.<!--]--></figcaption></figure><p><!--[-->In other words, cosine similarity is the duct tape of vector comparisons. Sure, it sticks everything together — images, text, audio, code — but like real duct tape, it&#39;s a quick fix that often masks deeper problems rather than solving them. And just as you wouldn&#39;t use duct tape to permanently repair a water pipe, you shouldn&#39;t blindly trust cosine similarity for all your vector comparison needs.<!--]--></p><p><!--[-->Like a Greek tragedy, this blessing comes with a curse: when it works, it feels like effortless magic. But when it fails, we are clueless, and we often run into impromptu fixes, each one bringing issues on its own.<!--]--></p><h2 id="relation-to-correlation"><a href="#relation-to-correlation"><!--[-->Relation to correlation<!--]--></a></h2><p><!--[--><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="nofollow"><!--[-->Pearson correlation<!--]--></a> can be seen as a sequence of three operations:<!--]--></p><ul><!--[--><li><!--[-->Subtracting means to center the data.<!--]--></li><li><!--[-->Normalizing vectors to unit length.<!--]--></li><li><!--[-->Computing dot products between them.<!--]--></li><!--]--></ul><p><!--[-->When we work with vectors that are both centered (<mjx-container jax="SVG"><svg style="vertical-align: -0.663ex;" xmlns="http://www.w3.org/2000/svg" width="9.491ex" height="2.36ex" role="img" focusable="false" viewBox="0 -750 4195.1 1043.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-2-TEX-SO-2211"></use></g><g data-mml-node="mi" transform="translate(1089,-285.4) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-2-TEX-I-1D456"></use></g></g><g data-mml-node="msub" transform="translate(1549.6,0)"><g data-mml-node="mi"><use data-c="1D463" xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-2-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(2639.3,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3695.1,0)"><use data-c="30" xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></svg></mjx-container>) and normalized (<mjx-container jax="SVG"><svg style="vertical-align: -0.663ex;" xmlns="http://www.w3.org/2000/svg" width="9.739ex" height="2.55ex" role="img" focusable="false" viewBox="0 -833.9 4304.7 1127.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-3-TEX-SO-2211"></use></g><g data-mml-node="mi" transform="translate(1089,-285.4) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-3-TEX-I-1D456"></use></g></g><g data-mml-node="msubsup" transform="translate(1549.6,0)"><g data-mml-node="mi"><use data-c="1D463" xlink:href="#MJX-3-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(518,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(518,-284.4) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-3-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(2748.9,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3804.7,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g></g></svg></mjx-container>), Pearson correlation, cosine similarity and dot product are the same.<!--]--></p><p><!--[-->In practical cases, we don&#39;t want to center or normalize vectors during each pairwise comparison - we do it once, and <strong><!--[-->just use dot product<!--]--></strong>. In any case, when you are fine with using cosine similarity, you should be as fine with using Pearson correlation (and vice versa).<!--]--></p><h2 id="problems-with-cosine-similarity-as-a-measure-of-similarity"><a href="#problems-with-cosine-similarity-as-a-measure-of-similarity"><!--[-->Problems with cosine similarity as a measure of similarity<!--]--></a></h2><p><!--[-->Using cosine similarity as a training objective for machine learning models is perfectly valid and mathematically sound.
As we just seen, it&#39;s a combination of two fundamental operations in deep learning: dot product and normalization.
The trouble begins when we venture beyond its comfort zone, specifically when:<!--]--></p><ul><!--[--><li><!--[-->The cost function used in model training isn&#39;t cosine similarity (usually it is the case!).<!--]--></li><li><!--[-->The training objective differs from what we actually care about.<!--]--></li><!--]--></ul><h3 id="has-the-model-ever-seen-cosine-similarity"><a href="#has-the-model-ever-seen-cosine-similarity"><!--[-->Has the model ever seen cosine similarity?<!--]--></a></h3><p><!--[-->A common scenario involves training with unnormalized vectors, when we are dealing with a function of dot product - for example, predicting probabilities with a sigmoid function <mjx-container jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.789ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3884.9 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-4-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-4-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-4-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-4-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(571,0)"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(960,0)"><g data-mml-node="mi"><use data-c="1D463" xlink:href="#MJX-4-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><use data-c="1D44E" xlink:href="#MJX-4-TEX-I-1D44E"></use></g></g><g data-mml-node="mo" transform="translate(2124.3,0)"><use data-c="22C5" xlink:href="#MJX-4-TEX-N-22C5"></use></g><g data-mml-node="msub" transform="translate(2624.5,0)"><g data-mml-node="mi"><use data-c="1D463" xlink:href="#MJX-4-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><use data-c="1D44F" xlink:href="#MJX-4-TEX-I-1D44F"></use></g></g><g data-mml-node="mo" transform="translate(3495.9,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g></g></g></svg></mjx-container> and applying log loss cost function. Other networks operate differently, e.g. they use Euclidean distance, minimized for members of the same class and maximized for members of different classes.<!--]--></p><p><!--[-->The normalization gives us some nice mathematical properties (keeping results between -1 and +1, regardless of dimensions), but it&#39;s ultimately a hack. Sometimes it helps, sometimes it doesn&#39;t — see the aptly titled paper <a href="https://arxiv.org/abs/2403.05440" rel="nofollow"><!--[-->Is Cosine-Similarity of Embeddings Really About Similarity?<!--]--></a>.<!--]--></p><p><!--[-->Sure, back in the days of an image detection model VGG16 I was using logit vectors from the classification layer and Pearson correlation to find similar images. It kind of worked - being fully aware it is a hack and just a hack.<!--]--></p><p><!--[-->We are safe only if the model itself uses cosine similarity or a direct function of it - usually implemented as a dot product of vectors that are kept normalized. Otherwise, we use a quantity we have no control over. It may work in one instance, but not in another. If some things are extremely similar, sure, it is likely that many different measures of similarity will give similar results. But if they are not, we are in trouble.<!--]--></p><p><!--[-->In general, it is a part of a broader subject of unsupervised machine vs self-supervised learning.
In the first one, we take an arbitrary function and we get some notions or similarity. Yet, there is no way to evaluate it.
The second one, self-supervised learning, is a predictive model, in which we can directly evaluate the quality of prediction.<!--]--></p><h3 id="is-it-the-right-kind-of-similarity"><a href="#is-it-the-right-kind-of-similarity"><!--[-->Is it the right kind of similarity?<!--]--></a></h3><p><!--[-->And here is the second issue - even if <a href="https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf" rel="nofollow"><!--[-->a model is explicitly trained on cosine similarity<!--]--></a>, we run into a deeper question: whose definition of similarity are we using?<!--]--></p><p><!--[-->Consider books. For a literary critic, similarity might mean sharing thematic elements. For a librarian, it&#39;s about genre classification. For a reader, it&#39;s about emotions it evokes. For a typesetter, it&#39;s page count and format. Each perspective is valid, yet cosine similarity smashes all these nuanced views into a single number — with confidence and an illusion of objectivity.<!--]--></p><div><!--[--><figure data-v-038762c4=""><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/cartoon-espresso-word2vec.jpg" alt="Cartoon by Dmitry Malkov" data-v-038762c4=""/><figcaption data-v-038762c4="">Cartoon by Dmitry Malkov</figcaption></figure><!--]--></div><p><!--[-->In the US, word2vec might tell you espresso and cappuccino are practically identical. It is not a claim you would make in Italy.<!--]--></p><h2 id="when-it-falls-apart"><a href="#when-it-falls-apart"><!--[-->When it falls apart<!--]--></a></h2><p><!--[-->Let&#39;s have a task that looks simple, a simple quest from our everyday life:<!--]--></p><ul><!--[--><li><!--[--><em><!--[-->&#34;What did I do with my keys?&#34;<!--]--></em><!--]--></li><!--]--></ul><p><!--[-->Now, using cosine similarity, we can compare it to other notes:<!--]--></p><ul><!--[--><li><!--[--><em><!--[-->&#34;I left them in my pocket&#34;<!--]--></em><!--]--></li><li><!--[--><em><!--[-->&#34;They are on the table&#34;<!--]--></em><!--]--></li><li><!--[--><em><!--[-->&#34;What did I put my wallet?&#34;<!--]--></em><!--]--></li><li><!--[--><em><!--[-->&#34;What I did to my life?&#34;<!--]--></em><!--]--></li><!--]--></ul><figure data-v-15401076=""><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/the-quest-for-keys.png" alt="" data-v-15401076="" data-v-038762c4=""/></p><figcaption data-v-15401076=""><!--[-->The closest match is not a plausible answer to our question — instead, it is another question. With sentence embedding cosine similarity, we are more likely to question our own life than to solve our mundane task.
Fortunately, sentences about Python are close to zero - as they are not related.<!--]--></figcaption></figure><p><!--[-->And remember, this is just a toy example with five sentences. In real-world applications, we&#39;re often dealing with thousands of documents — far more than could fit in a single context window. As your dataset grows, so does the noise sensitivity, turning your similarity scores into a game of high-dimensional roulette.<!--]--></p><h2 id="so-what-can-we-use-instead"><a href="#so-what-can-we-use-instead"><!--[-->So, what can we use instead?<!--]--></a></h2><h3 id="the-most-powerful-approach"><a href="#the-most-powerful-approach"><!--[-->The most powerful approach<!--]--></a></h3><p><!--[-->The best approach is to directly use LLM query to compare two entries. So, first, start with <a href="https://lmarena.ai/?leaderboard" rel="nofollow"><!--[-->a powerful model of your choice<!--]--></a>. Then, we can write something in the line of:<!--]--></p><blockquote><!--[--><p><!--[-->&#34;Is {sentence_a} a plausible answer to {sentence_b}?&#34;<!--]--></p><!--]--></blockquote><p><!--[-->This way we harness the full power of an LLM to extract meaningful comparisons - powerful enough to find our keys in pockets, and wise enough to understand the difference between questions and answers.
We typically want our answers in structured output - what the field calls &#34;tools&#34; or &#34;function calls&#34; (which is really just a fancy way of saying &#34;JSON&#34;).<!--]--></p><p><!--[-->Since many models love Markdown (as they were trained on it), my default template looks like this:<!--]--></p><!--[--><pre><!--[--><code>{question}

## A

{sentence_a}

## B

{sentence_b}
</code><!--]--></pre><!--]--><p><!--[-->However, in most cases this approach is impractical - we don&#39;t want to run such a costly operation for each query. Unless our dataset is very small, it would be prohibitively expensive. Even with a small dataset, the delays would be noticeable compared to a simple numerical operation.<!--]--></p><p><!--[-->So, we can go back to using embeddings.
But instead of blindly trusting a black box, we can directly optimize for what we actually care about by creating task-specific embeddings.
There are two main approaches:<!--]--></p><ul><!--[--><li><!--[-->Fine-tuning (teaching an old model new tricks by adjusting its weights).<!--]--></li><li><!--[-->Transfer learning (using the model&#39;s knowledge to create new, more focused embeddings).<!--]--></li><!--]--></ul><p><!--[-->Which one we use is ultimately a technical question - depending on the access to the model, costs, etc.
Let&#39;s start with a symmetric case. Say we want to ask, <strong><!--[-->&#34;Is A similar to B?&#34;<!--]--></strong> We can write this as:<!--]--></p><p><!--[-->where <mjx-container jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="7.786ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 3441.6 765" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D462" xlink:href="#MJX-6-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(849.8,0)"><use data-c="3D" xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1905.6,0)"><use data-c="1D440" xlink:href="#MJX-6-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(2956.6,0)"><use data-c="1D463" xlink:href="#MJX-6-TEX-I-1D463"></use></g></g></g></svg></mjx-container>, and <mjx-container jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-7-TEX-I-1D440"></use></g></g></g></svg></mjx-container> is a matrix that reduces the embedding space to dimensions we actually care about. Think of it as decluttering — we&#39;re keeping only the features relevant to our specific similarity definition.<!--]--></p><p><!--[-->But often, similarity isn&#39;t what we&#39;re really after. Consider the question <strong><!--[-->&#34;Is document B a correct answer to question A?&#34;<!--]--></strong> (note the word &#34;correct&#34;) and the relevant probability:<!--]--></p><p><!--[-->where <mjx-container jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="6.945ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 3069.6 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-9-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-9-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-9-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path><path id="MJX-9-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45E" xlink:href="#MJX-9-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(737.8,0)"><use data-c="3D" xlink:href="#MJX-9-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1793.6,0)"><use data-c="1D444" xlink:href="#MJX-9-TEX-I-1D444"></use></g><g data-mml-node="mi" transform="translate(2584.6,0)"><use data-c="1D463" xlink:href="#MJX-9-TEX-I-1D463"></use></g></g></g></svg></mjx-container> and <mjx-container jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="7.304ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 3228.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-10-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-10-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-10-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-10-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D458" xlink:href="#MJX-10-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8,0)"><use data-c="3D" xlink:href="#MJX-10-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1854.6,0)"><use data-c="1D43E" xlink:href="#MJX-10-TEX-I-1D43E"></use></g><g data-mml-node="mi" transform="translate(2743.6,0)"><use data-c="1D463" xlink:href="#MJX-10-TEX-I-1D463"></use></g></g></g></svg></mjx-container>. The matrices <mjx-container jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D444" xlink:href="#MJX-11-TEX-I-1D444"></use></g></g></g></svg></mjx-container> and <mjx-container jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></defs><g stroke="currentColor" fill="currentColor" strokewidth="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43E" xlink:href="#MJX-12-TEX-I-1D43E"></use></g></g></g></svg></mjx-container> transform our embeddings into specialized spaces for questions and answers. It&#39;s like having two different languages and learning to translate between them, rather than assuming they&#39;re the same thing.<!--]--></p><p><!--[-->This approach works beautifully for retrieval augmented generation (RAG) too, as we usually care not only about similar documents but about the relevant ones.<!--]--></p><p><!--[-->But where do we get the training data?
We can use the same AI models we&#39;re working with to generate training data.
Then feed it into PyTorch, TensorFlow, or your framework of choice.<!--]--></p><h3 id="pre-prompt-engineering"><a href="#pre-prompt-engineering"><!--[-->Pre-prompt engineering<!--]--></a></h3><p><!--[-->Sure, we can train a model. Maybe even train on artificially generated data - but what if we want to avoid this step entirely? We got used to zero-shot learning, and it is not easy to go back.<!--]--></p><p><!--[-->One of the quickest fixes is to add prompt to the text, so to set the apparent context.
A simple example — let&#39;s have the list of <a href="https://ideas.time.com/2013/12/10/whos-biggest-the-100-most-significant-figures-in-history/" rel="nofollow"><!--[-->Time&#39;s 100 Most Significant Figures in History<!--]--></a>. Let&#39;s say we want to see who is the most similar to Isaac Newton.<!--]--></p><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/time-newton-sim1.png" alt="" data-v-038762c4=""/></p><p><!--[-->No surprise, it&#39;s other physicists and natural philosophers.
Yet, let&#39;s say we want to focus on his nationality - so we add a prompt <em><!--[-->&#34;Nationality of {person}&#34;<!--]--></em>.<!--]--></p><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/time-newton-sim2.png" alt="" data-v-038762c4=""/></p><p><!--[-->Sadly, the results are underwhelming - sure, Galileo went a few places down, but Albert Einstein is listed as the most similar.
So, let&#39;s try another approach, by making nationality the subject of the sentence - <em><!--[-->&#34;This is a country that has produced many influential historical figures, including {person}&#34;<!--]--></em>.<!--]--></p><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/time-newton-sim3.png" alt="" data-v-038762c4=""/></p><p><!--[-->Now we get much better answer!
To be clear - while I have found this approach useful, it is not a silver bullet.
Depending on how to formulate the prompt, we can get a slight bias towards our goal, or something actually solving our problem.<!--]--></p><h3 id="rewriting-and-context-extraction"><a href="#rewriting-and-context-extraction"><!--[-->Rewriting and context extraction<!--]--></a></h3><p><!--[-->Another approach is to preprocess the text before embedding it.
Here&#39;s a generic trick I often use — I ask the model:<!--]--></p><blockquote><!--[--><p><!--[-->&#34;Rewrite the following text in standard English using Markdown. Focus on content, ignore style. Limit to 200 words.&#34;<!--]--></p><!--]--></blockquote><p><!--[-->This simple prompt works wonders. It helps avoid false matches based on superficial similarities like formatting quirks, typos, or unnecessary verbosity.<!--]--></p><p><!--[-->Often we want more - e.g. to extract information from a text while ignoring the rest.
For example, let&#39;s say we have a chat with a client and want to suggest relevant pages, be it FAQ or product recommendations. A naive way would be to compare their discussion&#39;s embedding with the embeddings of our pages. A better approach is to first transform the conversation into a structured format focused on needs:<!--]--></p><blockquote><!--[--><p><!--[-->&#34;You have a conversation with a client. Summarize their needs and pain points in up to 10 Markdown bullet points, up to 20 words each. Consider both explicit needs and those implied by context, tone, and other signals.&#34;<!--]--></p><!--]--></blockquote><p><!--[-->Similarly, rewrite each of your pages in the same format before embedding them. This strips away everything that isn&#39;t relevant to matching needs with solutions.<!--]--></p><p><!--[-->This approach has worked wonders in many of my projects. Perhaps it will work for you too.<!--]--></p><h2 id="recap"><a href="#recap"><!--[-->Recap<!--]--></a></h2><p><!--[-->Let&#39;s recap the key points:<!--]--></p><ul><!--[--><li><!--[-->Cosine similarity gives us a number between -1 and 1, but don&#39;t mistake it for a probability.<!--]--></li><li><!--[-->Most models aren&#39;t trained using cosine similarity - then the results are just &#34;some sort of correlations&#34; without any guarantees.<!--]--></li><li><!--[-->Even when a model is trained with cosine similarity, we need to understand what kind of similarity it learned and if that matches our needs.<!--]--></li><li><!--[-->To use vector similarity effectively, there are a few approaches:
<ul><!--[--><li><!--[-->Train custom embeddings on your specific data<!--]--></li><li><!--[-->Engineer prompts to focus on relevant aspects<!--]--></li><li><!--[-->Clean and standardize text before embedding<!--]--></li><!--]--></ul><!--]--></li><!--]--></ul><p><!--[-->Have you found other ways to make vector similarity work better for your use case? What approaches have you tried? What were the results?<!--]--></p><p><img src="https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity/python-summit-2024-warsaw-migdal-cosine.jpg" alt="" data-v-038762c4=""/></p><h2 id="thanks"><a href="#thanks"><!--[-->Thanks<!--]--></a></h2><p><!--[-->I first presented this topic as a flash talk at <a href="https://lu.ma/warsaw-ai-breakfast" rel="nofollow"><!--[-->Warsaw AI Breakfast<!--]--></a> - I am grateful for feedback from Greg Kossakowski and Max Salamonowicz.
I thank Rafał Małanij for inviting me to speak at <a href="https://python-summit.pl/en/" rel="nofollow"><!--[-->Python Summit 2024 Warsaw<!--]--></a>.
This blog post stemmed from interest after these presentations, as well as <a href="https://www.linkedin.com/posts/piotrmigdal_llm-ai-activity-7271894516058509312-e489?utm_source=share&amp;utm_medium=member_desktop" rel="nofollow"><!--[-->multiple questions on the LinkedIn post<!--]--></a>.<!--]--></p><h2 id="similar-posts"><a href="#similar-posts"><!--[-->Similar posts<!--]--></a></h2><ul><!--[--><li><!--[--><code><!--[-->0.769<!--]--></code> <a href="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/" rel="nofollow"><!--[-->king - man + woman is queen; but why?<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.713<!--]--></code> <a href="https://p.migdal.pl/blog/2019/07/human-machine-learning-motivation/" rel="nofollow"><!--[-->Exploring human vs machine learning<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.697<!--]--></code> <a href="https://p.migdal.pl/blog/2023/02/ai-artists-information-theory/" rel="nofollow"><!--[-->AI won’t make artists redundant - thanks to information theory<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.695<!--]--></code> <a href="https://p.migdal.pl/blog/2018/09/simple-diagrams-deep-learning/" rel="nofollow"><!--[-->Simple diagrams of convoluted neural networks<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.690<!--]--></code> <a href="https://p.migdal.pl/blog/2017/04/teaching-deep-learning/" rel="nofollow"><!--[-->Teaching deep learning with Keras<!--]--></a><!--]--></li><!--]--></ul><p><!--[-->And for a bit of fun, here are my least similar posts:<!--]--></p><ul><!--[--><li><!--[--><code><!--[-->0.385<!--]--></code> <a href="https://p.migdal.pl/blog/2022/10/perspective-capsaicin-is-a-psychoactive-substance/" rel="nofollow"><!--[-->Perspective: capsaicin is a psychoactive substance<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.384<!--]--></code> <a href="https://p.migdal.pl/blog/2019/07/there-will-be-the-next-quantum-game-with-photons/" rel="nofollow"><!--[-->There will be the next Quantum Game with Photons<!--]--></a><!--]--></li><li><!--[--><code><!--[-->0.285<!--]--></code> <a href="https://p.migdal.pl/blog/2019/10/the-statues-by-jacek-kaczmarski/" rel="nofollow"><!--[-->The Statues by Jacek Kaczmarski<!--]--></a><!--]--></li><!--]--></ul><p><!--[-->Yes, I see the irony in using cosine similarity after warning against it. But here I&#39;m using it exactly as intended — with a model trained specifically for document similarity. Sometimes duck tape is all we need.<!--]--></p></div></article></div>
  </body>
</html>
