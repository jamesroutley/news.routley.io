<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/">Original</a>
    <h1>ViT - Vision Transformer</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p><em>Veiled in a mist of arcane energy, the Orb of Scrying rests silently upon its ancient pedestal. Crafted from crystal as clear as mountain spring water, it waits for the touch of a seer. To the untrained eye, it’s merely a beautiful artifact, but to a wielder of magic, it’s a window to the unseen. Whispering the old words, the mage’s eyes lock onto the orb’s depths. Visions swirl within, revealing secrets hidden across lands and time, as the orb bridges the gap between the known and the unknown.</em></p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/orb-of-scrying.png" title="orb-of-scrying" data-thumbnail="/posts/vit-vision-transformer/orb-of-scrying.png" data-sub-html="&lt;h2&gt;Orb of Scrying&lt;/h2&gt;&lt;p&gt;orb-of-scrying&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/orb-of-scrying.png" data-srcset="/posts/vit-vision-transformer/orb-of-scrying.png, /posts/vit-vision-transformer/orb-of-scrying.png 1.5x, /posts/vit-vision-transformer/orb-of-scrying.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/orb-of-scrying.png" width="953" height="953"/>
    </a><figcaption>Orb of Scrying</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Use a <del>scrying orb</del> transformer to classify images.</p>
<h2 id="principle">Principle</h2>
<p>Transformers have been taking over all the sequences tasks. Until some clever folk decided it should also take over the vision world.</p>
<p>The trick proposed is to chunk the image into small tiles. Think of the tiles as a sequence of “words”. Feed them to a transformer, and let the magic happen.</p>
<h2 id="chunking-images">Chunking Images</h2>
<p>Let’s cut images in different ways to build intuition.</p>
<h3 id="horizontally">Horizontally</h3>
<p>By hands</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>h</span><span>,</span> <span>w</span> <span>=</span> <span>images</span><span>.</span><span>shape</span>
</span></span><span><span><span>horizontal</span> <span>=</span> <span>images</span><span>.</span><span>view</span><span>(</span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>4</span><span>,</span> <span>h</span> <span>//</span> <span>4</span><span>,</span> <span>w</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>)</span>
</span></span></code></pre></div><p>Or using <code>einops</code></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>horizontal</span> <span>=</span> <span>einops</span><span>.</span><span>rearrange</span><span>(</span><span>images</span><span>,</span> <span>&#39;b c (k h) w -&gt; b k c h w&#39;</span><span>,</span> <span>k</span><span>=</span><span>4</span><span>)</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/horizontal-strips.png" title="horizontal-strips" data-thumbnail="/posts/vit-vision-transformer/horizontal-strips.png" data-sub-html="&lt;h2&gt;Horizontal Strips&lt;/h2&gt;&lt;p&gt;horizontal-strips&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/horizontal-strips.png" data-srcset="/posts/vit-vision-transformer/horizontal-strips.png, /posts/vit-vision-transformer/horizontal-strips.png 1.5x, /posts/vit-vision-transformer/horizontal-strips.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/horizontal-strips.png" width="345" height="390"/>
    </a><figcaption>Horizontal Strips</figcaption>
    </figure>
<h3 id="vertically">Vertically</h3>
<p>By hands</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>h</span><span>,</span> <span>w</span> <span>=</span> <span>images</span><span>.</span><span>shape</span>
</span></span><span><span><span>vertical</span> <span>=</span> <span>images</span><span>.</span><span>view</span><span>(</span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>h</span><span>,</span> <span>4</span><span>,</span> <span>w</span> <span>//</span> <span>4</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span> <span>3</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>)</span>
</span></span></code></pre></div><p>Or using <code>einops</code></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>vertical</span> <span>=</span> <span>einops</span><span>.</span><span>rearrange</span><span>(</span><span>images</span><span>,</span> <span>&#39;b c h (k w) -&gt; b k c h w&#39;</span><span>,</span> <span>k</span><span>=</span><span>4</span><span>)</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/vertical-strips.png" title="vertical-strips" data-thumbnail="/posts/vit-vision-transformer/vertical-strips.png" data-sub-html="&lt;h2&gt;Vertical Strips&lt;/h2&gt;&lt;p&gt;vertical-strips&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/vertical-strips.png" data-srcset="/posts/vit-vision-transformer/vertical-strips.png, /posts/vit-vision-transformer/vertical-strips.png 1.5x, /posts/vit-vision-transformer/vertical-strips.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/vertical-strips.png" width="390" height="344"/>
    </a><figcaption>Vertical Strips</figcaption>
    </figure>
<h3 id="tiles">Tiles</h3>
<p>By hands</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>h</span><span>,</span> <span>w</span> <span>=</span> <span>images</span><span>.</span><span>shape</span>
</span></span><span><span><span>tile_size</span> <span>=</span> <span>7</span>
</span></span><span><span><span>tiles</span> <span>=</span> <span>images</span><span>.</span><span>view</span><span>(</span><span>b</span><span>,</span> <span>c</span><span>,</span> <span>h</span> <span>//</span> <span>tile_size</span><span>,</span> <span>tile_size</span><span>,</span> <span>w</span> <span>//</span> <span>tile_size</span><span>,</span> <span>tile_size</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>reshape</span><span>(</span><span>b</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>c</span><span>,</span> <span>tile_size</span><span>,</span> <span>tile_size</span><span>)</span>
</span></span></code></pre></div><p>Or using <code>einops</code></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>tile_size</span> <span>=</span> <span>7</span>
</span></span><span><span><span>tiles</span> <span>=</span> <span>einops</span><span>.</span><span>rearrange</span><span>(</span><span>images</span><span>,</span> <span>&#39;b c (h t1) (w t2) -&gt; b (h w) c t1 t2&#39;</span><span>,</span> <span>t1</span><span>=</span><span>tile_size</span><span>,</span> <span>t2</span><span>=</span><span>tile_size</span><span>)</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/tiles.png" title="tiles" data-thumbnail="/posts/vit-vision-transformer/tiles.png" data-sub-html="&lt;h2&gt;Square Tiles&lt;/h2&gt;&lt;p&gt;tiles&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/tiles.png" data-srcset="/posts/vit-vision-transformer/tiles.png, /posts/vit-vision-transformer/tiles.png 1.5x, /posts/vit-vision-transformer/tiles.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/tiles.png" width="389" height="390"/>
    </a><figcaption>Square Tiles</figcaption>
    </figure>
<h2 id="model">Model</h2>
<p>The individual tiles are flattened and passed into a fully connected layer to be converted into <code>embed_size</code> N-dimensional vectors. This step is similar to the “embedding” step for text transformers.</p>
<p>A <code>CLS</code> token is added in the front. This is where we will read the label prediction at the end of the pipeline. Adding an extra token gives a place for the network to store intermediate states without worrying of the pixel content of any particular tile exercising more influence than the others. This <code>CLS</code> token is treated as one more learnable attribute of the network.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Net</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>  <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>...</span><span>):</span>
</span></span><span><span>    <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>    <span>self</span><span>.</span><span>positional_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>context_size</span><span>,</span> <span>embed_size</span><span>)</span>
</span></span><span><span>    <span>self</span><span>.</span><span>tile_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>TILE_SIZE</span> <span>*</span> <span>TILE_SIZE</span> <span>*</span> <span>CHANNEL</span><span>,</span> <span>embed_size</span><span>)</span>
</span></span><span><span>    <span>self</span><span>.</span><span>cls_token</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>embed_size</span><span>))</span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span>  <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>    <span># (batch_size, channel, height, width)</span>
</span></span><span><span>    <span># split into tiles/patches/chunks</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>einops</span><span>.</span><span>rearrange</span><span>(</span><span>x</span><span>,</span> <span>&#39;b c (h t1) (w t2) -&gt; b (h w) (c t1 t2)&#39;</span><span>,</span> <span>t1</span><span>=</span><span>self</span><span>.</span><span>tile_size</span><span>,</span> <span>t2</span><span>=</span><span>self</span><span>.</span><span>tile_size</span><span>)</span>
</span></span><span><span>    <span># embed</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>self</span><span>.</span><span>tile_embedding</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>    <span># add cls token</span>
</span></span><span><span>    <span>cls_token</span> <span>=</span> <span>self</span><span>.</span><span>cls_token</span><span>.</span><span>expand</span><span>(</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>((</span><span>cls_token</span><span>,</span> <span>x</span><span>),</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
</span></span><span><span>    <span># positional encoding</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>get_positional_embedding</span><span>()</span>
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><h2 id="evaluate-the-model">Evaluate the Model</h2>
<p>Train the model for a few hundreds epochs and it gets to ~97% accuracy on the validation set. We can get a better ideas of the failure cases by looking at the confusion matrix.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/confusion-matrix.png" title="confusion-matrix" data-thumbnail="/posts/vit-vision-transformer/confusion-matrix.png" data-sub-html="&lt;h2&gt;Confusion Matrix&lt;/h2&gt;&lt;p&gt;confusion-matrix&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/confusion-matrix.png" data-srcset="/posts/vit-vision-transformer/confusion-matrix.png, /posts/vit-vision-transformer/confusion-matrix.png 1.5x, /posts/vit-vision-transformer/confusion-matrix.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/confusion-matrix.png" width="366" height="393"/>
    </a><figcaption>Confusion Matrix</figcaption>
    </figure>
<p>Or visualizing the images with the worst loss.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/misslabelled.png" title="misslabelled" data-thumbnail="/posts/vit-vision-transformer/misslabelled.png" data-sub-html="&lt;h2&gt;Misslabeled Images - predicted / real&lt;/h2&gt;&lt;p&gt;misslabelled&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/misslabelled.png" data-srcset="/posts/vit-vision-transformer/misslabelled.png, /posts/vit-vision-transformer/misslabelled.png 1.5x, /posts/vit-vision-transformer/misslabelled.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/misslabelled.png" width="1189" height="382"/>
    </a><figcaption>Misslabeled Images - predicted / real</figcaption>
    </figure>
<p>There’s still room for improvement. But I’d argue that some of them are pretty fishy.</p>
<h2 id="internals-of-the-model">Internals of the Model</h2>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>I used learned positional encoding and we can look at what encodings have been learned.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/learned-positional-embeddings.png" title="learned-positional-embeddings" data-thumbnail="/posts/vit-vision-transformer/learned-positional-embeddings.png" data-sub-html="&lt;h2&gt;Learned Positional Encoding&lt;/h2&gt;&lt;p&gt;learned-positional-embeddings&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/learned-positional-embeddings.png" data-srcset="/posts/vit-vision-transformer/learned-positional-embeddings.png, /posts/vit-vision-transformer/learned-positional-embeddings.png 1.5x, /posts/vit-vision-transformer/learned-positional-embeddings.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/learned-positional-embeddings.png" width="389" height="390"/>
    </a><figcaption>Learned Positional Encoding</figcaption>
    </figure>
<p>Visualizing didn’t help me much here. I can’t pickup a pattern at a glance.</p>
<h3 id="attention-by-label">Attention by Label</h3>
<p>Comparing the median Attention Activations for a very small 2 layers single-headed ViT. For three classes of labels: <code>3</code>, <code>7</code>, and <code>8</code>.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/compare-attention-378.png" title="compare-attention-378" data-thumbnail="/posts/vit-vision-transformer/compare-attention-378.png" data-sub-html="&lt;h2&gt;Compared Median Attention Activations for 3, 7, 8&lt;/h2&gt;&lt;p&gt;compare-attention-378&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/compare-attention-378.png" data-srcset="/posts/vit-vision-transformer/compare-attention-378.png, /posts/vit-vision-transformer/compare-attention-378.png 1.5x, /posts/vit-vision-transformer/compare-attention-378.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/compare-attention-378.png" width="868" height="592"/>
    </a><figcaption>Compared Median Attention Activations for 3, 7, 8</figcaption>
    </figure>
<p>Once again I was expecting to find more differences. I don’t have a pattern jumping to my eyes.</p>
<h3 id="heatmap">Heatmap</h3>
<p>Instead we can visualize what pixels were the most important in deciding the label for an image.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/vit-vision-transformer/heatmap.png" title="heatmap" data-thumbnail="/posts/vit-vision-transformer/heatmap.png" data-sub-html="&lt;h2&gt;Heatmap&lt;/h2&gt;&lt;p&gt;heatmap&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/vit-vision-transformer/heatmap.png" data-srcset="/posts/vit-vision-transformer/heatmap.png, /posts/vit-vision-transformer/heatmap.png 1.5x, /posts/vit-vision-transformer/heatmap.png 2x" data-sizes="auto" alt="/posts/vit-vision-transformer/heatmap.png" width="988" height="390"/>
    </a><figcaption>Heatmap</figcaption>
    </figure>
<p>This is also not what I was expecting. But this time I see a few interesting patterns:</p>
<ul>
<li>The tiles are visible in the heatmap because of the self-attention.</li>
<li><code>1</code> are looking for an horizontal line to the left of their center, and I believe this is to detect the absence of the branch of a <code>4</code> or the cross bar of a <code>7</code>.</li>
</ul>
<p>This was particularly surprising to me because most of the decisions seems to be taken by “what is missing” instead of “what is present” in the picture.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/ViT" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ViT</a></p>
<h2 id="sources">Sources</h2>
<p>An Image is Worth 16x16 Words: <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener noreffer ">https://arxiv.org/pdf/2010.11929.pdf</a></p>
</div></div>
  </body>
</html>
