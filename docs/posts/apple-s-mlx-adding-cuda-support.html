<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ml-explore/mlx/pull/1983">Original</a>
    <h1>Apple&#39;s MLX adding CUDA support</h1>
    
    <div id="readability-page-1" class="page"><div data-quote-markdown=".js-comment-body" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-team-hovercards-enabled="" data-hpc="">
    <template>
  <div data-view-component="true">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <p><span>
      This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>
    </span></p>
</div></template>
<template>
  <span aria-label="This line has hidden Unicode characters" data-view-component="true">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</span></template>

    <div>

      <div data-gid="PR_kwDOKzRn186PkHq4" data-url="/ml-explore/mlx/pull/1983/partials/body" data-channel-event-name="body_updated" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjI0MDg2MTA0ODgiLCJ0IjoxNzUyNTU0MzE4fQ==--2d11f91901d2f47853e6eb1ecc0448e83a946428dbd2b9e2029c248e76151aa7">

<p><a href="https://github.com/zcbenz" data-view-component="true"><img data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" src="https://avatars.githubusercontent.com/u/639601?s=60&amp;v=4" alt="zcbenz" size="40" height="40" width="40" data-view-component="true"/></a>
  
  
</p><div id="issue-2937359776">
  <div id="pullrequest-2408610488">
          

          <div>
        <div>
  
  <task-lists disabled="" sortable="">
    <div>
      <p dir="auto">This PR is an ongoing effort to add a CUDA backend to MLX, very little things work now but you can run the tutorial example already.</p>
<p dir="auto">To build and test:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON
$ cmake --build build -j 16
$ ./build/examples/cpp/tutorial
array([[2, 3],
       [4, 5]], dtype=float32)
array([[1, 1],
       [1, 1]], dtype=float32)"><pre>$ <span>cmake <span>.</span> -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON</span>
$ <span>cmake --build build -j 16</span>
$ <span>./build/examples/cpp/tutorial</span>
<span>array([[2, 3],</span>
<span>       [4, 5]], dtype=float32)</span>
<span>array([[1, 1],</span>
<span>       [1, 1]], dtype=float32)</span></pre></div>
<p dir="auto">For development I usually use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache -DCMAKE_BUILD_TYPE=Debug -GNinja"><pre>$ <span>cmake <span>.</span> -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache -DCMAKE_BUILD_TYPE=Debug -GNinja</span></pre></div>
<p dir="auto">Only tested on a Ubuntu 22.04 with CUDA 11.6, in theory other environments can also work but there are no testings.</p>
<p dir="auto">This PR is not updated frequently, if anyone is interested in the realtime development, please check <a href="https://github.com/frost-beta/mlx-cuda/commits?author=zcbenz&amp;since=2025-03-20">my forked repo</a>.</p>
<hr/>
<p dir="auto">There are mainly 2 reasons for a CUDA backend:</p>
<ul dir="auto">
<li>CUDA supports unified memory. Including hardware support in some devices, and software support for devices without hardware unified memory.</li>
<li>NVIDIA hardware is widely used for academic and massive computations. Being able to write/test code locally on a Mac and then deploy to super computers would make a good developer experience.</li>
</ul>
<p dir="auto">This work is sponsored by Apple.</p>
    </div>
  </task-lists>
  
</div>

      </div>

          <!-- '"` --><!-- </textarea></xmp> -->
          <div>
        <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/ml-explore/mlx/reactions" accept-charset="UTF-8" method="post">
    
    <div>
          <tool-tip id="tooltip-e1785341-37ed-4826-bf7d-17db5c17df73" for="reactions--reaction_button_component-6c776a" popover="manual" data-direction="n" data-type="description" data-view-component="true">angeloskath, lin72h, EricLBuehler, stemann, kylebeggs, pcuenca, Vaibhavs10, v-shobhit, ghishadow, a-r-r-o-w, and 41 more reacted with heart emoji</tool-tip>
          <tool-tip id="tooltip-79a91ce4-5d9f-450f-a06c-61334348209a" for="reactions--reaction_button_component-012508" popover="manual" data-direction="n" data-type="description" data-view-component="true">awni, lin72h, EricLBuehler, stemann, radudiaconu0, kylebeggs, pcuenca, HongyuS, ghishadow, a-r-r-o-w, and 19 more reacted with rocket emoji</tool-tip>
      
    </div>
</form></div>
      </div>

</div>
  </div>
</div>


                  


      

      <div data-gid="IC_kwDOKzRn186joZCO">
  
      
<div data-gid="IC_kwDOKzRn186joZCO" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186joZCO/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/radudiaconu0/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/radudiaconu0"><img src="https://avatars.githubusercontent.com/u/52667211?s=80&amp;u=e21b399a7cd57ebf3a335b994c15e8920d15770f&amp;v=4" width="40" height="40" alt="@radudiaconu0"/></a>

</p>


  
<div id="issuecomment-2745274510">

    <div data-body-version="6198e67db0a550721cdccdacfc7c4de77b24dfaa78bbb2d46d2ecbdc1c285b77">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">I wanna add rocm support based on your cuda pull request. would that be ok with you?</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186jo60h">
  
      
<div data-gid="IC_kwDOKzRn186jo60h" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186jo60h/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/awni/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/awni"><img src="https://avatars.githubusercontent.com/u/1542805?s=80&amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1&amp;v=4" width="40" height="40" alt="@awni"/></a>

</p>


  
<div id="issuecomment-2745412897">

    <div data-body-version="323c0d1f27ab1667863e4199c5b71c70305d2d4841f1dc6c49abda37a3cbd90d">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Awesome progress so far <a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz">@zcbenz</a> !!</p>
<p dir="auto">I&#39;m wondering what the best way to get this incorporated into MLX. I can think of a couple of options:</p>
<ul dir="auto">
<li>Once this is ready we can make this into a cuda branch in MLX and then send PRs to it. This will make it easier from a review / PR management standpoint</li>
<li>Just merge the backbone infra for supporting CUDA and send more incremental PRs over time</li>
</ul>
<p dir="auto">I kind of prefer the latter.. but I&#39;m open to suggestions.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186jq3Se">
  
      
<div data-gid="IC_kwDOKzRn186jq3Se" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186jq3Se/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2745922718">

    <div data-body-version="a4fa6d6e16ec8b74e774df3999a0114670a9badb5f3217dca482590f3eb6764f">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">I wanna add rocm support based on your cuda pull request. would that be ok with you?</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/radudiaconu0/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/radudiaconu0">@radudiaconu0</a> Of course I&#39;m ok with it!</p>
<p dir="auto">Before you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I&#39;m not familiar with ROCm, but I saw 2 patterns in projects with both backends:</p>
<ol dir="auto">
<li>Both backends share the same code, with help of <code>#define</code>s and name aliases.</li>
<li>Transpile CUDA code to HIP on the fly during build time, which is <a href="https://github.com/pytorch/pytorch/blob/539db4af4bc8d2fe3a79f09dd7cb17a0619de7be/tools/amd_build/build_amd.py">used by PyTorch</a>.</li>
</ol>
<p dir="auto">Another thing to notice is this PR is bound to heavy changes in following weeks, I&#39;m still experimenting what is the best interface for integration.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186jq4DJ">
  
      
<div data-gid="IC_kwDOKzRn186jq4DJ" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186jq4DJ/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/angeloskath/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/angeloskath"><img src="https://avatars.githubusercontent.com/u/1242043?s=80&amp;v=4" width="40" height="40" alt="@angeloskath"/></a>

</p>


  
<div id="issuecomment-2745925833">

    <div data-body-version="32acc4294afb9a075d73b5fbe4d5625187d18e5b7897ad159dc2dbae37ce5557">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Awesome progress indeed!</p>
<p dir="auto">Just chiming in regarding the best way to incorporate this. Imho merging often is the way to go (option 2 basically). Combined with running CUDA tests in CI it will be the easiest to live with (since we &#39;ll know when we break it even if we don&#39;t use it). Otherwise the cuda branch would have to be constantly rebased on top of main which could be annoying.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186jq5g9">
  
      
<div data-gid="IC_kwDOKzRn186jq5g9" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186jq5g9/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/radudiaconu0/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/radudiaconu0"><img src="https://avatars.githubusercontent.com/u/52667211?s=80&amp;u=e21b399a7cd57ebf3a335b994c15e8920d15770f&amp;v=4" width="40" height="40" alt="@radudiaconu0"/></a>

</p>


  
<div id="issuecomment-2745931837">

    <div data-body-version="2268dda4afcf30455e4360a0b5600fe0262d62c3c92d81cc9e913fbbbb537c49">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<blockquote>
<p dir="auto">I wanna add rocm support based on your cuda pull request. would that be ok with you?</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/radudiaconu0/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/radudiaconu0">@radudiaconu0</a> Of course I&#39;m ok with it!</p>
<p dir="auto">Before you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I&#39;m not familiar with ROCm, but I saw 2 patterns in projects with both backends:</p>
<ol dir="auto">
<li>
<p dir="auto">Both backends share the same code, with help of <code>#define</code>s and name aliases.</p>
</li>
<li>
<p dir="auto">Transpile CUDA code to HIP on the fly during build time, which is <a href="https://github.com/pytorch/pytorch/blob/539db4af4bc8d2fe3a79f09dd7cb17a0619de7be/tools/amd_build/build_amd.py">used by PyTorch</a>.</p>
</li>
</ol>
<p dir="auto">Another thing to notice is this PR is bound to heavy changes in following weeks, I&#39;m still experimenting what is the best interface for integration.</p>
</blockquote>
<p dir="auto">I would try to make a separate hip folder or to use hipify on your CUDA code to make it use rocm/hip</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186jq59t">
  
      
<div data-gid="IC_kwDOKzRn186jq59t" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186jq59t/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2745933677">

    <div data-body-version="cc9b0f351df507001161384a3ea6f65ab3652199c3c3e10246f0812be4b904d8">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">I&#39;m wondering what the best way to get this incorporated into MLX.</p>
</blockquote>
<p dir="auto">I find myself keep refactoring the code when porting new kernels, I think I still need to implement a few more primitives before getting the backbone code stable, probably a few more weeks of experimenting.</p>
<p dir="auto">Once the code is ready for review, I can split this PR into a backbone PR, and a few small PRs for each primitive. And future works would then be submitted in incremental PRs.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      

      <div data-gid="IC_kwDOKzRn186j5IKa">
  
      
<div data-gid="IC_kwDOKzRn186j5IKa" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186j5IKa/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2749661850">

    <div data-body-version="dd4d0c95de35066352054e17c1aab3c6bdd6a2e60c7a54db3a67f29525482c4d">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">In CUDA the kernel parameters&#39; size must be known at compile-time, i.e. we can&#39;t pass dynamic-sized shape/strides via constant memory like what the Metal kernels do.</p>
<p dir="auto">I&#39;m currently passing shape/strides to kernels via fixed-size <code>cuda::std::array</code>, which is <a href="https://github.com/pytorch/pytorch/blob/f320c7b76682720aab420b1ecd1eaa8ad9171be8/aten/src/ATen/cuda/detail/OffsetCalculator.cuh#L15-L31">what PyTorch has been doing</a>. This comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I&#39;m using 8 for now and it can be easily changed if found not enough.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186j5J2k">
  
      
<div data-gid="IC_kwDOKzRn186j5J2k" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186j5J2k/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/awni/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/awni"><img src="https://avatars.githubusercontent.com/u/1542805?s=80&amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1&amp;v=4" width="40" height="40" alt="@awni"/></a>

</p>


  
<div id="issuecomment-2749668772">

    <div data-body-version="4874e2f3c3a59d78d381e5c66dc69c5bf5ee5f4532b99f078270b5a74709c4ef">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">This comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I&#39;m using 8 for now and it can be easily changed if found not enough.</p>
</blockquote>
<p dir="auto">Sounds great! As long as we can change it by setting one number somewhere I think that&#39;s perfectly fine.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      

      <div data-gid="IC_kwDOKzRn186lvKSZ">
  
      
<div data-gid="IC_kwDOKzRn186lvKSZ" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186lvKSZ/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2780603545">

    <div data-body-version="f5cc17906a900f7f1786b13d401a474fd38e0fce07fb6b1a773a961addecf933">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">The C++ <code>logistic_regression</code> example can run now:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./build/examples/cpp/logistic_regression 
Loss array(0.0344943, dtype=float32), Accuracy, array(1, dtype=float32), Throughput 518.05 (it/s)."><pre>$ <span>./build/examples/cpp/logistic_regression </span>
<span>Loss array(0.0344943, dtype=float32), Accuracy, array(1, dtype=float32), Throughput 518.05 (it/s).</span></pre></div>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186lz77M">
  
      
<div data-gid="IC_kwDOKzRn186lz77M" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186lz77M/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2781855436">

    <div data-body-version="f8b83f6c9e0523f8c2fe9f00f6b56a0b17e794ab1fd88081adc52204eea2ea08">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">The <code>logistic_regression</code> is slow, I did some profiling.</p>
<hr/>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430758737-87984a7d-dc8e-43e8-9297-57f491fd7485.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzU4NzM3LTg3OTg0YTdkLWRjOGUtNDNlOC05Mjk3LTU3ZjQ5MWZkNzQ4NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05OTE1OTAxYzEzZmZmZGYzOGY0OTJlYTg3ZGIxYjIwODBjMDg1N2QzNTM4OTk2OGU2MDI2NTRjYmE5M2RlZDEwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.YWO0DQFJAaqtdCywkCuXzxBnnIQnVLWS9nLHsL-E7s0"><img src="https://private-user-images.githubusercontent.com/639601/430758737-87984a7d-dc8e-43e8-9297-57f491fd7485.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzU4NzM3LTg3OTg0YTdkLWRjOGUtNDNlOC05Mjk3LTU3ZjQ5MWZkNzQ4NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05OTE1OTAxYzEzZmZmZGYzOGY0OTJlYTg3ZGIxYjIwODBjMDg1N2QzNTM4OTk2OGU2MDI2NTRjYmE5M2RlZDEwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.YWO0DQFJAaqtdCywkCuXzxBnnIQnVLWS9nLHsL-E7s0" alt="step"/></a></p>
<p dir="auto">Each step takes about 2ms (i.e. 500 it/s), and each step consists of:</p>
<ol dir="auto">
<li>Graph building (the MLX calls before invoking <code>eval</code>, which is the empty area before <code>eval_impl</code> and <code>Event::wait</code>).</li>
<li>Kernel launching (the <code>eval_impl</code> part).</li>
<li>Waiting for the results of kernels (the <code>Event::wait</code> part).</li>
</ol>
<p dir="auto">We can see that we spent as much time launching kernels as waiting for the results.</p>
<hr/>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430759543-7d478322-a72e-417a-9a26-cf92ec42927d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzU5NTQzLTdkNDc4MzIyLWE3MmUtNDE3YS05YTI2LWNmOTJlYzQyOTI3ZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNmU0OTVmYTc3YThhMTNjMDU5N2I5ZWZiZjc5YWZjM2Q2ZGE5NWI1Y2MzOWQ2ZDQ4OTVlODUwMDQ4MDhmOWU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.sowTpdaO6fsG97F3UkwF8_pA5wakiXEiN7Hpw4TN4rY"><img src="https://private-user-images.githubusercontent.com/639601/430759543-7d478322-a72e-417a-9a26-cf92ec42927d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzU5NTQzLTdkNDc4MzIyLWE3MmUtNDE3YS05YTI2LWNmOTJlYzQyOTI3ZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNmU0OTVmYTc3YThhMTNjMDU5N2I5ZWZiZjc5YWZjM2Q2ZGE5NWI1Y2MzOWQ2ZDQ4OTVlODUwMDQ4MDhmOWU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.sowTpdaO6fsG97F3UkwF8_pA5wakiXEiN7Hpw4TN4rY" alt="eval"/></a></p>
<p dir="auto">Looking closer at the kernel launching part, between each <code>eval_gpu</code> calls there is a very long <code>Event::is_signaled</code> call, which is an atomic read under the hood and we want to cut down its time a lot.</p>
<p dir="auto">Inside the <code>eval_gpu</code> call, we have some <code>cudaMalloc</code> calls (the red blocks) that can be removed by introducing buffer cache in future.</p>
<hr/>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430761799-e414bec3-effc-4297-b5f5-54be9b6568f6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzYxNzk5LWU0MTRiZWMzLWVmZmMtNDI5Ny1iNWY1LTU0YmU5YjY1NjhmNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00NDVkOGY0YWJiNmM0NTQzYWM5Yjc0MjM5OTIxZDNkYjIxZjY5Zjg2MjdkOTBjOGY0M2ViOGFlOGYwNTQyNzc2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.jpk9JBK3TX_6x9aGHjejxAnZtJtRj3EA4RewtsXJS0k"><img src="https://private-user-images.githubusercontent.com/639601/430761799-e414bec3-effc-4297-b5f5-54be9b6568f6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzYxNzk5LWU0MTRiZWMzLWVmZmMtNDI5Ny1iNWY1LTU0YmU5YjY1NjhmNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00NDVkOGY0YWJiNmM0NTQzYWM5Yjc0MjM5OTIxZDNkYjIxZjY5Zjg2MjdkOTBjOGY0M2ViOGFlOGYwNTQyNzc2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.jpk9JBK3TX_6x9aGHjejxAnZtJtRj3EA4RewtsXJS0k" alt="CUDA HW"/></a></p>
<p dir="auto">Then look at the &#34;CUDA HW&#34; panel, which indicates that kernel running time is the same with <code>eval_gpu</code>, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong.</p>
<p dir="auto">There are very large paddings between the kernels, some of them belong to ops that do not need to launch kernels (like broadcast), some of them are the slow <code>Event::is_signaled</code> calls that need to be improved. And once the kernels run asynchronously, there ought to be no paddings between them then.</p>
<hr/>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430763789-c5a48891-e0da-4af3-8a25-486c6c865761.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzYzNzg5LWM1YTQ4ODkxLWUwZGEtNGFmMy04YTI1LTQ4NmM2Yzg2NTc2MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wODQ0MzdjNzQ4MzA2NWU3MzNkYTY0NTZkZjBmNjM3OTMxOWIyYmY3YmI0MGJkMzhkYmM0NDBlZjg2OTAxNzdhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.v1kNk3_QahX2GhbT9-T4qloKjPxS6OJPXW8plMTix2s"><img src="https://private-user-images.githubusercontent.com/639601/430763789-c5a48891-e0da-4af3-8a25-486c6c865761.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwNzYzNzg5LWM1YTQ4ODkxLWUwZGEtNGFmMy04YTI1LTQ4NmM2Yzg2NTc2MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wODQ0MzdjNzQ4MzA2NWU3MzNkYTY0NTZkZjBmNjM3OTMxOWIyYmY3YmI0MGJkMzhkYmM0NDBlZjg2OTAxNzdhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.v1kNk3_QahX2GhbT9-T4qloKjPxS6OJPXW8plMTix2s" alt="Screenshot 2025-04-07 at 11 04 43"/></a></p>
<p dir="auto">Finally there are long paddings between the <code>eval_impl</code>s, which is the main thread waiting for the finish signal from the launched kernels, and it is really really slow compared to actual kernel running time. I think I need a reimplementation of <code>Event</code> to cut it down, current implementation uses <code>cuda::std::atomic</code> which seems very inefficient.</p>
<hr/>
<p dir="auto">Overall the overhead does not look very big: it is only about 2ms per step, and it should be a fixed number as it is not related to the size of arrays. However in the case of <code>logistic_regression</code> as the computation itself is very fast, the overhead dramatically slows things down.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186l1Nzo">
  
      
<div data-gid="IC_kwDOKzRn186l1Nzo" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186l1Nzo/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2782190824">

    <div data-body-version="9b468dd4b51f9f88598e7f24d7bf17e382fd244532648163e87c6686d9c1db54">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">Then look at the &#34;CUDA HW&#34; panel, which indicates that kernel running time is the same with <code>eval_gpu</code>, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong.</p>
</blockquote>
<p dir="auto">After a closer look, I think the &#34;NVTX&#34; row under &#34;CUDA HW&#34; only means to mark the event that started the kernel, and it does not indicate the event started at the same time with kernel. The kernels are started asynchronously.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186l19c1">
  
      
<div data-gid="IC_kwDOKzRn186l19c1" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186l19c1/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2782385973">

    <div data-body-version="fdb108c6eba9dc4af9d4bf703bf5c216b10fb82ec623a95e5a66e071334f4dd3">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">So in the case of <code>logistic_regression</code> where the computation takes less time than overhead, how fast it runs depends on how fast we can push ops to CUDA stream.</p>
<p dir="auto">For PyTorch the time duration between 2 simple ops is 5µs:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430840956-87351a14-ff07-4a90-b515-8c6ded348e1c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTU2LTg3MzUxYTE0LWZmMDctNGE5MC1iNTE1LThjNmRlZDM0OGUxYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zYjI3NTMzNWQxM2VkNzQ0OWE5ZmY3NTVmZTMzNTQ3YzEwNWFiMDA5ZTU2Y2UxMTZhN2JmZmVhMTJiOGQ3OWQ0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.TCDpLBHcveJZYG6F-fe5NfFEUQKMl4n3XbLqHPdIlm8"><img src="https://private-user-images.githubusercontent.com/639601/430840956-87351a14-ff07-4a90-b515-8c6ded348e1c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTU2LTg3MzUxYTE0LWZmMDctNGE5MC1iNTE1LThjNmRlZDM0OGUxYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zYjI3NTMzNWQxM2VkNzQ0OWE5ZmY3NTVmZTMzNTQ3YzEwNWFiMDA5ZTU2Y2UxMTZhN2JmZmVhMTJiOGQ3OWQ0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.TCDpLBHcveJZYG6F-fe5NfFEUQKMl4n3XbLqHPdIlm8" alt="5µs"/></a></p>
<p dir="auto">And for us it is at least 41µs:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/430840927-fb9fee77-24de-4d0d-bb32-4e5033b4beed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTI3LWZiOWZlZTc3LTI0ZGUtNGQwZC1iYjMyLTRlNTAzM2I0YmVlZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iZWJhYTQ5YzQzNjUxMzllNGZjZTdhYTg2NWJhODFkYzM4ODMxNWY4NTE3ZTU4MTU1MzljOTVhNTE2YjA5ZWIxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.IbH9M8366LmaOC-wIw-BfZmE76Xoa68K7nRouvd9DTc"><img src="https://private-user-images.githubusercontent.com/639601/430840927-fb9fee77-24de-4d0d-bb32-4e5033b4beed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTI3LWZiOWZlZTc3LTI0ZGUtNGQwZC1iYjMyLTRlNTAzM2I0YmVlZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iZWJhYTQ5YzQzNjUxMzllNGZjZTdhYTg2NWJhODFkYzM4ODMxNWY4NTE3ZTU4MTU1MzljOTVhNTE2YjA5ZWIxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.IbH9M8366LmaOC-wIw-BfZmE76Xoa68K7nRouvd9DTc" alt="41µs"/></a></p>
<hr/>
<p dir="auto">There are a few things I can do to reduce the overhead:</p>
<ol dir="auto">
<li>Reimplement <code>Event</code> with cudaEvent, which should be the fasted op provided by CUDA.</li>
<li>Add buffer cache to reduce <code>cudaMalloc</code> calls.</li>
<li>Record the locations of buffers to avoid unnecessary <code>cudaMemPrefetch</code> calls.</li>
<li>Batch the cleanup of temporary arrays to reduce the latency between 2 kernels.</li>
</ol>
<p dir="auto">At last there is still a good news though: the time spent on running kernel is the same with PyTorch, which means we don&#39;t need to improve the kernel implementation.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186l7m0M">
  
      
<div data-gid="IC_kwDOKzRn186l7m0M" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186l7m0M/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/awni/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/awni"><img src="https://avatars.githubusercontent.com/u/1542805?s=80&amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1&amp;v=4" width="40" height="40" alt="@awni"/></a>

</p>


  
<div id="issuecomment-2783866124">

    <div data-body-version="0a6b0c2e8bd6bce32511bee67f085ec1b5b78b58d6b7670e31deadc9f3822cd5">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">some of them are the slow <code>Event::is_signaled</code> calls that need to be improved.</p>
</blockquote>
<p dir="auto">Where are those calls coming from? Is it <a href="https://github.com/ml-explore/mlx/blob/main/mlx/transforms.cpp#L206-L207">here</a>? We might be able to reduce the number of times we call that if needed.. I&#39;m not actually certain it needs to be there. I think it&#39;s just a mechanism to eagerly clean up unused events.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186l_RMB">
  
      
<div data-gid="IC_kwDOKzRn186l_RMB" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186l_RMB/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2784826113">

    <div data-body-version="f59fac506eebe78a4fa4fe89bc4b1473be97e625e462b57e72a745c35561ec70">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">Where are those calls coming from? Is it <a href="https://github.com/ml-explore/mlx/blob/main/mlx/transforms.cpp#L206-L207">here</a>?</p>
</blockquote>
<p dir="auto">Yes it is where the calls came from.</p>
<p dir="auto">Removing it would be great, I think it is an expensive op on all platforms.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      

      <div data-gid="IC_kwDOKzRn186mEaCv">
  
      
<div data-gid="IC_kwDOKzRn186mEaCv" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186mEaCv/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2786173103">

    <div data-body-version="88d9ea04f9bd0a6a3dce354212343a528f713fd1fad758fbe992e8ab227c8fa6">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Tried the ideas: switching the implementation of <code>Event</code> from <code>cuda::std::atomic</code> to <code>cudaEvent</code> bumped training speed from 500 it/s to 900; reducing the prefetch calls increased it from 900 it/s to 1100.</p>
<hr/>
<p dir="auto">The next optimization is tricky: after evaluating each op, the operands and temporaries are saved until kernel finishes running, in Metal it is done like this:</p>
<div>
  
  <div itemprop="text">
    <table data-tab-size="8" data-paste-markdown-skip="">

        <tbody><tr>
          <td id="L55" data-line-number="55"></td>
          <td id="LC55">   <span>if</span> (d.<span>command_buffer_needs_commit</span>(s.<span>index</span>)) { </td>
        </tr>

        <tr>
          <td id="L56" data-line-number="56"></td>
          <td id="LC56">     d.<span>end_encoding</span>(s.<span>index</span>); </td>
        </tr>

        <tr>
          <td id="L57" data-line-number="57"></td>
          <td id="LC57">     <span>scheduler::notify_new_task</span>(s); </td>
        </tr>

        <tr>
          <td id="L58" data-line-number="58"></td>
          <td id="LC58">     command_buffer-&gt;<span>addCompletedHandler</span>( </td>
        </tr>

        <tr>
          <td id="L59" data-line-number="59"></td>
          <td id="LC59">         [s, buffers = <span>std::move</span>(buffers)](MTL::CommandBuffer* cbuf) { </td>
        </tr>

        <tr>
          <td id="L60" data-line-number="60"></td>
          <td id="LC60">           <span>scheduler::notify_task_completion</span>(s); </td>
        </tr>

        <tr>
          <td id="L61" data-line-number="61"></td>
          <td id="LC61">           <span>check_error</span>(cbuf); </td>
        </tr>

        <tr>
          <td id="L62" data-line-number="62"></td>
          <td id="LC62">         }); </td>
        </tr>

        <tr>
          <td id="L63" data-line-number="63"></td>
          <td id="LC63">     d.<span>commit_command_buffer</span>(s.<span>index</span>); </td>
        </tr>

        <tr>
          <td id="L64" data-line-number="64"></td>
          <td id="LC64">     d.<span>get_command_buffer</span>(s.<span>index</span>); </td>
        </tr>

        <tr>
          <td id="L65" data-line-number="65"></td>
          <td id="LC65">   } <span>else</span> { </td>
        </tr>

        <tr>
          <td id="L66" data-line-number="66"></td>
          <td id="LC66">     command_buffer-&gt;<span>addCompletedHandler</span>( </td>
        </tr>

        <tr>
          <td id="L67" data-line-number="67"></td>
          <td id="LC67">         [s, buffers = <span>std::move</span>(buffers)](MTL::CommandBuffer* cbuf) { </td>
        </tr>

        <tr>
          <td id="L68" data-line-number="68"></td>
          <td id="LC68">           <span>check_error</span>(cbuf); </td>
        </tr>

        <tr>
          <td id="L69" data-line-number="69"></td>
          <td id="LC69">         }); </td>
        </tr>

        <tr>
          <td id="L70" data-line-number="70"></td>
          <td id="LC70">   } </td>
        </tr>

        <tr>
          <td id="L71" data-line-number="71"></td>
          <td id="LC71"> } </td>
        </tr>
    </tbody></table>
  </div>
</div>

<p dir="auto">In CUDA there is a <code>cudaLaunchHostFunc</code> API that was used to implement this. However according to the profiling it adds at least 20µs latency in cuda stream, which means each kernel has to wait at least 20µs before running.</p>
<p dir="auto">To get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until <code>finalize()</code> is called, i.e. when <code>mx::eval_impl()</code> finishes running. In this way the <code>cudaLaunchHostFunc</code> is only called once per <code>mx::eval()</code>, instead of once per <code>op::eval_gpu()</code>. And the duration between 2 kernels is now under 1µs, which is better than PyTorch and I believe it is the best we can do.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/431336388-41b66066-0b1e-4917-8be5-d716a35cccc5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzM2Mzg4LTQxYjY2MDY2LTBiMWUtNDkxNy04YmU1LWQ3MTZhMzVjY2NjNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02Yjg5ZDYwMWE1NWFlMGFlNDA1ZDAwNmM2YjI1ZDFlMzJlN2M1NGNhYWY4ZTlmYWUyNWNiNzk5ZDdjMTkxYmZhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.UY7rVrGcoXZfMy0mn_dfjKRQgK-GhhcuWx0lc_UO9Tg"><img src="https://private-user-images.githubusercontent.com/639601/431336388-41b66066-0b1e-4917-8be5-d716a35cccc5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzM2Mzg4LTQxYjY2MDY2LTBiMWUtNDkxNy04YmU1LWQ3MTZhMzVjY2NjNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02Yjg5ZDYwMWE1NWFlMGFlNDA1ZDAwNmM2YjI1ZDFlMzJlN2M1NGNhYWY4ZTlmYWUyNWNiNzk5ZDdjMTkxYmZhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.UY7rVrGcoXZfMy0mn_dfjKRQgK-GhhcuWx0lc_UO9Tg" alt="duration"/></a></p>
<p dir="auto">The downside is the arrays take longer to be destroyed, which could increase memory usages. The code also no longer waits if there are more tasks than MAX_ACTIVE_TASKS.</p>
<p dir="auto">After this optimization the speed increased from 1100 it/s to 1600.</p>
<hr/>
<p dir="auto">There were still many kernels that get unusually delayed.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/431339330-d1fe91b4-cee8-472b-8b48-39522747532c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzM5MzMwLWQxZmU5MWI0LWNlZTgtNDcyYi04YjQ4LTM5NTIyNzQ3NTMyYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hNGYxOGJiNDFjMWRlODA0MjE1MTBlZmUyMGFiZGEwMWEzZTg1ZDMxMjQ3OGVlYTNiN2M2MDM5NGY0ZmJkNTJjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.r-hPi-_67IZj9XjnSQfhtVui-Ncu5slJp-1XI-Sq4G4"><img src="https://private-user-images.githubusercontent.com/639601/431339330-d1fe91b4-cee8-472b-8b48-39522747532c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzM5MzMwLWQxZmU5MWI0LWNlZTgtNDcyYi04YjQ4LTM5NTIyNzQ3NTMyYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hNGYxOGJiNDFjMWRlODA0MjE1MTBlZmUyMGFiZGEwMWEzZTg1ZDMxMjQ3OGVlYTNiN2M2MDM5NGY0ZmJkNTJjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.r-hPi-_67IZj9XjnSQfhtVui-Ncu5slJp-1XI-Sq4G4" alt="prefetch"/></a></p>
<p dir="auto">What did the delayed kernels have in common? Before launching the kernel they all called an API: <code>cudaMemPrefetch</code> (the green blocks).</p>
<p dir="auto">In the CUDA backend we use the unified memory APIs, which automatically transfers data between host and device, since I know the data is going to be used in GPU, I used the <code>cudaMemPrefetch</code> API to prefetch the memory in device so the kernel does not have to wait for the implicit memory transfer during execution.</p>
<p dir="auto">It turns out the prefetching heavily delayed the kernel executions.</p>
<p dir="auto">Removing prefetching increased speed from 1600 it/s to 2100, and we now have a really beautiful timeline in profiler.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/639601/431342746-442202c6-a5fd-40b7-b689-bc4932ce3f14.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzQyNzQ2LTQ0MjIwMmM2LWE1ZmQtNDBiNy1iNjg5LWJjNDkzMmNlM2YxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yM2FiMDJlZmM0ZDllODRiYzYzNWFlODgxOTc1MmQyZTI1OTJiZmFjZmIxYTZmMDY4ZDM5YmM2MDAzZTY5YjYwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.omTfjh9QPGUgjTgAQkbY65wuHhTcbt-tuBSZ4ts77mU"><img src="https://private-user-images.githubusercontent.com/639601/431342746-442202c6-a5fd-40b7-b689-bc4932ce3f14.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NTQ2MTcsIm5iZiI6MTc1MjU1NDMxNywicGF0aCI6Ii82Mzk2MDEvNDMxMzQyNzQ2LTQ0MjIwMmM2LWE1ZmQtNDBiNy1iNjg5LWJjNDkzMmNlM2YxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwNDM4MzdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yM2FiMDJlZmM0ZDllODRiYzYzNWFlODgxOTc1MmQyZTI1OTJiZmFjZmIxYTZmMDY4ZDM5YmM2MDAzZTY5YjYwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.omTfjh9QPGUgjTgAQkbY65wuHhTcbt-tuBSZ4ts77mU" alt="noprefetch"/></a></p>
<hr/>
<p dir="auto">One optimization I haven&#39;t done yet is buffer cache: I will add it when most ops are implemented and there is not no more third party libraries to be integrated.</p>
<hr/>
<p dir="auto">Can we do better? The remaining are mostly hard work: optimize the kernels and make CPU code run faster, which I think should be visited after we have implemented all ops.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186mFSQP">
  
      
<div data-gid="IC_kwDOKzRn186mFSQP" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186mFSQP/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/awni/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/awni"><img src="https://avatars.githubusercontent.com/u/1542805?s=80&amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1&amp;v=4" width="40" height="40" alt="@awni"/></a>

</p>


  
<div id="issuecomment-2786403343">

    <div data-body-version="dbe79e62ef90399d75802040e045fa1f9181835aa715f9479bff0368c161414e">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Very nice <a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz">@zcbenz</a> !</p>
<blockquote>
<p dir="auto">To get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until finalize() is called, i.e. when mx::eval_impl() finishes running.</p>
</blockquote>
<p dir="auto">That one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don&#39;t think it&#39;s worth doing that. We might want to do something in-between like saving them once ever ~10 calls to <code>eval_gpu</code> or something like that.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      <div data-gid="IC_kwDOKzRn186mP9jc">
  
      
<div data-gid="IC_kwDOKzRn186mP9jc" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186mP9jc/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2789202140">

    <div data-body-version="71d19930b1f0e05188ac12e181818e9e5476b5427a9388338ccfcae11b16cc11">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">That one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don&#39;t think it&#39;s worth doing that</p>
</blockquote>
<p dir="auto">I agree, this feels like an immature optimization for a special case. I&#39;ll make the behavior easy to configure so we can optimize for more cases in future.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


      


  

        

        

        <div data-gid="IC_kwDOKzRn186vOBrB">
  
      
<div data-gid="IC_kwDOKzRn186vOBrB" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186vOBrB/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/corupta/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/corupta"><img src="https://avatars.githubusercontent.com/u/8008704?s=80&amp;u=cb193027dd90d00420bf6e28b57b48eb2e1ed56b&amp;v=4" width="40" height="40" alt="@corupta"/></a>

</p>


  
<div id="issuecomment-2939689665">

    <div data-body-version="cc6c68b83f4c7bafad5f1021b2a42ec9518352681b389d8432186430c66559b9">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Hi <a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz">@zcbenz</a>,</p>
<div data-snippet-clipboard-copy-content="root@54712748d876:/opt/mlx# ./build/examples/cpp/tutorial
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory.
Aborted (core dumped)
root@54712748d876:/opt/mlx# ./build/examples/cpp/logistic_regression
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory.
Aborted (core dumped)
root@54712748d876:/opt/mlx# ./build/examples/cpp/metal_capture
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory."><pre><code>root@54712748d876:/opt/mlx# ./build/examples/cpp/tutorial
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory.
Aborted (core dumped)
root@54712748d876:/opt/mlx# ./build/examples/cpp/logistic_regression
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory.
Aborted (core dumped)
root@54712748d876:/opt/mlx# ./build/examples/cpp/metal_capture
terminate called after throwing an instance of &#39;std::runtime_error&#39;
  what():  Device 0 does not support synchronization in managed memory.
</code></pre></div>
<p dir="auto">I&#39;m willing to collaborate, if you&#39;re interested, I can run builds on Jetson devices and report back and try tinkering to some degree (I&#39;m not an expert in CUDA)</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOKzRn186vOgWT">
  
      
<div data-gid="IC_kwDOKzRn186vOgWT" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186vOgWT/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2939815315">

    <div data-body-version="821092ca7325e612d516525e41832077d67a3a5ccf8f39728c4d1fd3ada581f7">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/corupta/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/corupta">@corupta</a> Can you remove the <code>throw</code> statement in <code>Device::Device</code> in <code>mlx/backend/cuda/device.cpp</code> and check if the example runs?</p>
<p dir="auto">On the slow compilation, you can pass  <code>-DMLX_FAST_COMPILE=ON -DMLX_CUDA_ARCHITECTURES=native</code> to cmake to speed up a lot (by disabling many things). I&#39;m currently working on JIT compilation support which will solve this.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOKzRn186vZQIg">
  
      
<div data-gid="IC_kwDOKzRn186vZQIg" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186vZQIg/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/corupta/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/corupta"><img src="https://avatars.githubusercontent.com/u/8008704?s=80&amp;u=cb193027dd90d00420bf6e28b57b48eb2e1ed56b&amp;v=4" width="40" height="40" alt="@corupta"/></a>

</p>


  
<div id="issuecomment-2942632480">

    <div data-body-version="55271ede0811d2eb120f2748e10287d002779061d16252e5ec4d86b0c53deb93">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">tldr: it gives segfault</p>
<div data-snippet-clipboard-copy-content="Device 0 does not support synchronization in managed memory. Ignoring...
Segmentation fault (core dumped)"><pre><code>Device 0 does not support synchronization in managed memory. Ignoring...
Segmentation fault (core dumped)
</code></pre></div>
<p dir="auto">Some info about environments I use:</p>
<div data-snippet-clipboard-copy-content="/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(125): error: more than one instance of constructor &#34;__nv_bfloat16::__nv_bfloat16&#34; matches the argument list:
            function &#34;__nv_bfloat16::__nv_bfloat16(float)&#34;
/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(174): here
            function &#34;__nv_bfloat16::__nv_bfloat16(double)&#34;
/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(175): here
            argument types are: (int)
          detected during instantiation of &#34;auto mlx::core::cu::ReduceInit&lt;mlx::core::cu::Prod, T&gt;::value() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here

/opt/mlx/mlx/backend/cuda/kernels/utils.cuh(66): error: more than one operator &#34;-&#34; matches these operands:
            built-in operator &#34;- arithmetic&#34;
            function &#34;mlx::core::operator-(const mlx::core::complex64_t &amp;)&#34;
/opt/mlx/mlx/types/complex.h(77): here
            operand types are: - __nv_bfloat16
          detected during:
            instantiation of &#34;T mlx::core::cu::Limits&lt;T, cuda::std::__4::enable_if_t&lt;&lt;expression&gt;, void&gt;&gt;::min() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(140): here
            instantiation of &#34;T mlx::core::cu::ReduceInit&lt;mlx::core::cu::Max, T&gt;::value() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here
"><pre><code>/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(125): error: more than one instance of constructor &#34;__nv_bfloat16::__nv_bfloat16&#34; matches the argument list:
            function &#34;__nv_bfloat16::__nv_bfloat16(float)&#34;
/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(174): here
            function &#34;__nv_bfloat16::__nv_bfloat16(double)&#34;
/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(175): here
            argument types are: (int)
          detected during instantiation of &#34;auto mlx::core::cu::ReduceInit&lt;mlx::core::cu::Prod, T&gt;::value() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here

/opt/mlx/mlx/backend/cuda/kernels/utils.cuh(66): error: more than one operator &#34;-&#34; matches these operands:
            built-in operator &#34;- arithmetic&#34;
            function &#34;mlx::core::operator-(const mlx::core::complex64_t &amp;)&#34;
/opt/mlx/mlx/types/complex.h(77): here
            operand types are: - __nv_bfloat16
          detected during:
            instantiation of &#34;T mlx::core::cu::Limits&lt;T, cuda::std::__4::enable_if_t&lt;&lt;expression&gt;, void&gt;&gt;::min() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(140): here
            instantiation of &#34;T mlx::core::cu::ReduceInit&lt;mlx::core::cu::Max, T&gt;::value() [with T=__nv_bfloat16]&#34;
/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here

</code></pre></div>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOKzRn186vZmBO">
  
      
<div data-gid="IC_kwDOKzRn186vZmBO" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186vZmBO/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2942722126">

    <div data-body-version="0ba95492dd8c3af0a41aff0a9d10d4010db8a96eadf3a844f8d7362aa809e5d9">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">Thanks for testing the build, unfortunately this kind of error requires me to work the actual environment to debug. Currently I&#39;m only testing on a few cloud environments, but I will look into making it work on Jetson once most development is done, the devices with hardware unified memory definitely need first class support.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="CRE_kwDOKzRn186akqi2">
  
          <div>
  

  <div>
      
<div><p>
  This was referenced </p><relative-time datetime="2025-06-10T00:08:13Z">Jun 10, 2025</relative-time>
</div>





      






      






      






      






      






      






      






      






      






  </div>
</div>



</div>

        <div data-gid="IC_kwDOKzRn186w7oWR">
  
      
<div data-gid="IC_kwDOKzRn186w7oWR" data-url="/ml-explore/mlx/comments/IC_kwDOKzRn186w7oWR/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/zcbenz/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zcbenz"><img src="https://avatars.githubusercontent.com/u/639601?s=80&amp;u=b726a177170e74aa10fc3061f27da82822fa29d3&amp;v=4" width="40" height="40" alt="@zcbenz"/></a>

</p>


  
<div id="issuecomment-2968421777">

    <div data-body-version="aea40e11a8e8b4a3f7fc072e9bddcdac487f5ef9a82b3214fa887b452a2496fd">
      


      <div>

        
<task-lists disabled="" sortable="">
<div>
          <p dir="auto">I&#39;m closing this as it has been split into smaller pieces (check the linked PR above), future changes to CUDA backend will be submitted with incremental PRs.</p>
      </div>
</task-lists>


        <div>

            

            <!-- no margin wins, so we check it last and use its value if true. -->
            <div>
              <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/ml-explore/mlx/reactions" accept-charset="UTF-8" method="post">
    
      
    <div>
          <tool-tip id="tooltip-07500724-9c27-4dfc-bd6b-9b723a19f2de" for="reactions--reaction_button_component-c79059" popover="manual" data-direction="n" data-type="description" data-view-component="true">awni, JakeMalis, lin72h, bhupesh-sf, corupta, ghishadow, calvinf, jmorganca, Lukas1h, johnhamlin, and 4 more reacted with rocket emoji</tool-tip>
      
    </div>
</form></div>
            </div>
        </div>
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        



  <!-- Rendered timeline since 2025-07-14 19:02:53 -->
  



      

    </div>

    
  </div></div>
  </body>
</html>
