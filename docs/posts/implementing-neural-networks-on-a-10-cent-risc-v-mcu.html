<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/">Original</a>
    <h1>Implementing Neural Networks on a &#34;10-cent&#34; RISC-V MCU</h1>
    
    <div id="readability-page-1" class="page"><article id="post-1591">
	<!-- .entry-header -->

	
	
	<div>
		
<p>I have been meaning for a while to establish a setup to implement neural network based algorithms on smaller microcontrollers. After <a href="https://hackaday.io/project/193478-generative-ai-on-a-microcontroller/log/225316-tiny-inference-engines-for-mcu-deployment">reviewing existing solutions</a>, I felt there is no solution that I really felt comfortable with. One obvious issue is that often flexibility is traded for overhead. As always, for a really optimized solution you have to roll your own. So I did. You can find the project<a href="https://github.com/cpldcpu/BitNetMCU"> here</a> and a detailed writeup <a href="https://github.com/cpldcpu/BitNetMCU/blob/main/docs/documentation.md">here</a>. </p>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/header.png"><img data-attachment-id="1593" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/header/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/header.png" data-orig-size="1425,522" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="header" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/header.png?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/header.png?w=840" width="1024" height="375" data-id="1593" src="https://cpldcpu.files.wordpress.com/2024/04/header.png?w=1024" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/header.png?w=1024 1024w, https://cpldcpu.files.wordpress.com/2024/04/header.png?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/header.png?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/header.png?w=768 768w, https://cpldcpu.files.wordpress.com/2024/04/header.png 1425w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"/></a></figure>
</figure>



<p>It is always easier to work with a clear challenge:  I picked the CH32V003 as my target platform. This is the smallest RISC-V microcontroller on the market right now, addressing a $0.10 price point. It sports 2kb of SRAM and 16kb of flash. It is somewhat unique in implementing the RV32EC instruction set architecture, which does not even support multiplications. In other words, for many purposes this controller is less capable than an Arduino UNO.</p>



<p>As a test subject I chose the well-known MNIST dataset, which consists of images of hand written numbers which need to be classified from 0 to 9. Many inspiring implementation on Arduino exist for MNIST, for example <a href="https://blog.arduino.cc/2021/05/19/recognizing-handwritten-mnist-digits-on-an-arduino-uno-using-lognnet/">here</a>. In this case, the inference time was 7 seconds and 82% accuracy was achieved.</p>



<p> The idea is to train a neural network on a PC and optimize it for inference on teh CH32V003 while meetings these criteria:</p>



<ol>
<li>Be as fast and as accurate as possible</li>



<li>Low SRAM footprint during inference to fit into 2kb sram</li>



<li>Keep the weights of the neural network as small as possible</li>



<li>No multiplications!</li>
</ol>



<p>These criteria can be addressed by using a neural network with quantized weights, were each weight is represented with as few bits as possible. The best possible results are achieved when training the network already on quantized weights (Quantization Aware Training) as opposed to quantized a model that was trained with high accuracy weights. There is currently some <a href="https://arxiv.org/abs/2402.17764">hype around using Binary and Ternary weights</a> for large language models. But indeed, we can also use these approaches to fit a neural network to a small microcontroller.</p>



<p>The benefit of only using a few bits to represent each weight is that the memory footprint is low and we do not need a real multiplication instruction – inference can be reduced to additions only.</p>



<h2>Model structure and optimization</h2>



<p>For simplicity reasons, I decided to go for a e network architecture based on fully-connected layers instead of convolutional neural networks. The input images are reduced to a size of 16×16=256 pixels and are then fed into the network as shown below.</p>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png"><img data-attachment-id="1599" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/model_mcu-drawio/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png" data-orig-size="808,540" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="model_mcu.drawio" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=808" width="808" height="540" data-id="1599" src="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=808" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png 808w, https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/model_mcu.drawio.png?w=768 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px"/></a></figure>
</figure>



<p>The implementation of the inference engine is straightforward since only fully connected layers are used. The code snippet below shows the innerloop, which implements multiplication of 4 bit weights by using adds and shifts. The weights use a one-complement encoding without zero, which helps with code efficiency. One bit, ternary, and 2 bit quantization was implemented in a similar way.</p>



<pre><code>    int32_t sum = 0;</code></pre>



<p>In addition the fc layers also normalization and ReLU operators are required. I found that it was possible to replace a more complex RMS normalization with simple shifts in the inference. Not a single full 32×32 multiplication is needed for the inference! Having this simple structure for inference means that we have to focus the effort on the training part.</p>



<p>I studied variations of the network with different numbers of bits and different sizes by varying the numer of hidden activiations. To my surprise I found that the accuracy of the prediction is proportional to the total number of bits used to store the weights. For example, when 2 bits are used for each weight, twice the numbers of weights are needed to achieve the same perforemnce as a 4 bit weight network. The plot below shows training loss vs. total number of bits. We can see that for 1-4 bits, we can basically trade more weights for less bits. This trade-off is less efficient for 8 bits and no quantization (fp32).</p>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png"><img data-attachment-id="1602" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/train_loss_vs_totalbits/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png" data-orig-size="1598,1180" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="train_loss_vs_totalbits" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=840" width="1024" height="756" data-id="1602" src="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=1024" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=1024 1024w, https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png?w=768 768w, https://cpldcpu.files.wordpress.com/2024/04/train_loss_vs_totalbits.png 1598w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"/></a></figure>
</figure>



<p>I further optimized the training by using data augmentation, a cosine schedule and more epochs. It seems that 4 bit weights offered the best trade off.</p>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png"><img data-attachment-id="1604" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/explorationaugmented/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png" data-orig-size="1576,1180" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="explorationaugmented" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=840" loading="lazy" width="1024" height="766" data-id="1604" src="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=1024" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=1024 1024w, https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png?w=768 768w, https://cpldcpu.files.wordpress.com/2024/04/explorationaugmented.png 1576w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"/></a></figure>
</figure>



<p>More than 99% accuracy was achieved for 12 kbyte model size. While it is possible to achiever better accuracy with much larger models, it is significantly more accurate than other on-MCU implementations of MNIST.</p>



<h2>Implementation on the Microcontroller</h2>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg"><img data-attachment-id="1606" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/themcu/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg" data-orig-size="1920,1612" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="themcu" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=840" loading="lazy" width="1024" height="859" data-id="1606" src="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=1024" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=1024 1024w, https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg?w=768 768w, https://cpldcpu.files.wordpress.com/2024/04/themcu.jpg 1920w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"/></a></figure>
</figure>



<p>The model data is exported to a c-header file for inclusion into the inference code. I used the excellent <a href="https://github.com/cnlohr/ch32v003fun">ch32v003fun environment</a>, which allowed me to reduce overhead to be able to store 12kb of weights plus the inference engine in only 16kb of flash.</p>



<figure data-carousel-extra="{&#34;blog_id&#34;:50424692,&#34;permalink&#34;:&#34;https:\/\/cpldcpu.wordpress.com\/2024\/04\/24\/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier\/&#34;}">
<figure><a href="https://cpldcpu.files.wordpress.com/2024/04/console.png"><img data-attachment-id="1608" data-permalink="https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/console/" data-orig-file="https://cpldcpu.files.wordpress.com/2024/04/console.png" data-orig-size="1178,511" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="console" data-image-description="" data-image-caption="" data-medium-file="https://cpldcpu.files.wordpress.com/2024/04/console.png?w=300" data-large-file="https://cpldcpu.files.wordpress.com/2024/04/console.png?w=840" loading="lazy" width="1024" height="444" data-id="1608" src="https://cpldcpu.files.wordpress.com/2024/04/console.png?w=1024" alt="" srcset="https://cpldcpu.files.wordpress.com/2024/04/console.png?w=1024 1024w, https://cpldcpu.files.wordpress.com/2024/04/console.png?w=150 150w, https://cpldcpu.files.wordpress.com/2024/04/console.png?w=300 300w, https://cpldcpu.files.wordpress.com/2024/04/console.png?w=768 768w, https://cpldcpu.files.wordpress.com/2024/04/console.png 1178w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"/></a></figure>
</figure>



<p>There was still enough free flash to include 4 sample images. The inference output is shown above. Execution time for one inference is 13.7 ms which would actually allow to model to process moving image input in real time.</p>



<p>Alternatively, I also tested a smaller model with 4512 2-bit parameters and only 1kb of flash memory footprintg. Despite its size, it still achieves a 94.22% test accuracy and it executes in only 1.88ms.</p>



<h2>Conclusions</h2>



<p>This was quite a tedious projects, hunting many lost bits and rounding errors. I am quite pleased with the outcome as it shows that it is possible to compress neural networks very significantly with dedicated effort. I learned a lot and am planning to use the data pipeline for more interesting applications.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>
  </body>
</html>
