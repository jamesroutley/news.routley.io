<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://patents.google.com/patent/US10452978B2/en">Original</a>
    <h1>Google receives patent for attention-based sequence transduction neural networks</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      

      <article itemscope="" itemtype="http://schema.org/ScholarlyArticle">
  
  <span itemprop="title">Attention-based sequence transduction neural networks 
       </span>

  <meta itemprop="type" content="patent"/>
  <a href="https://patentimages.storage.googleapis.com/05/e8/f1/cd8eed389b7687/US10452978.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US10452978B2</dd>
    <meta itemprop="numberWithoutCodes" content="10452978"/>
    <meta itemprop="kindCode" content="B2"/>
    <meta itemprop="publicationDescription" content="Patent ( having previously published pre-grant publication)"/>
    
    <span>US10452978B2</span>
    
    <span>US16/021,971</span>
    
    <span>US201816021971A</span>
    
    <span>US10452978B2</span>
    
    <span>US 10452978 B2</span>
    
    <span>US10452978 B2</span>
    
    <span>US 10452978B2</span>
    
    <span>  </span>
    
    <span> </span>
    
    <span> </span>
    
    <span>US 201816021971 A</span>
    
    <span>US201816021971 A</span>
    
    <span>US 201816021971A</span>
    
    <span>US 10452978 B2</span>
    
    <span>US10452978 B2</span>
    
    <span>US 10452978B2</span>
    

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    
    <dd itemprop="priorArtKeywords" repeat="">input</dd>
    <dd itemprop="priorArtKeywords" repeat="">output</dd>
    <dd itemprop="priorArtKeywords" repeat="">encoder</dd>
    <dd itemprop="priorArtKeywords" repeat="">layer</dd>
    <dd itemprop="priorArtKeywords" repeat="">subnetwork</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2017-05-23">2017-05-23</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope="">
      <span itemprop="status">Active</span>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US16/021,971</dd>

  

  <dt>Other versions</dt>
  <dd itemprop="directAssociations" itemscope="" repeat="">
    
    <a href="https://patents.google.com/patent/US20180341860A1/en">
      <span itemprop="publicationNumber">US20180341860A1</span>
      (<span itemprop="primaryLanguage">en</span>
    </a>
  </dd>

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat="">Noam M. Shazeer</dd>
  <dd itemprop="inventor" repeat="">Aidan Nicholas Gomez</dd>
  <dd itemprop="inventor" repeat="">Lukasz Mieczyslaw Kaiser</dd>
  <dd itemprop="inventor" repeat="">Jakob D. Uszkoreit</dd>
  <dd itemprop="inventor" repeat="">Llion Owen Jones</dd>
  <dd itemprop="inventor" repeat="">Niki J. Parmar</dd>
  <dd itemprop="inventor" repeat="">Illia Polosukhin</dd>
  <dd itemprop="inventor" repeat="">Ashish Teku Vaswani</dd>
  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat="">
    Google LLC
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat="">Google LLC</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2017-05-23">2017-05-23</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2018-06-28">2018-06-28</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2019-10-22">2019-10-22</time></dd>

  

  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime=""></time>
    
    <span itemprop="title">Priority claimed from US201762510256P</span>
    <span itemprop="type">external-priority</span>
    
    
    
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2018-06-28">2018-06-28</time>
    
    <span itemprop="title">Priority to US16/021,971</span>
    <span itemprop="type">priority</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    <span itemprop="documentId">patent/US10452978B2/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2018-06-28">2018-06-28</time>
    
    <span itemprop="title">Application filed by Google LLC</span>
    <span itemprop="type">filed</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    
    <span itemprop="assigneeSearch">Google LLC</span>
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2018-06-29">2018-06-29</time>
    
    <span itemprop="title">Assigned to GOOGLE LLC</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    
    <span itemprop="assigneeSearch">GOOGLE LLC</span>
    
    
    <span itemprop="description" repeat="">CHANGE OF NAME (SEE DOCUMENT FOR DETAILS).</span>
    
    <span itemprop="description" repeat="">Assignors: GOOGLE INC.</span>
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2018-06-29">2018-06-29</time>
    
    <span itemprop="title">Assigned to GOOGLE INC.</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    
    <span itemprop="assigneeSearch">GOOGLE INC.</span>
    
    
    <span itemprop="description" repeat="">ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    
    <span itemprop="description" repeat="">Assignors: GOMEZ, Aidan Nicholas, USZKOREIT, JAKOB D., JONES, Llion Owen, POLOSUKHIN, Illia, VASWANI, Ashish Teku, PARMAR, NIKI, KAISER, LUKASZ MIECZYSLAW, SHAZEER, NOAM M.</span>
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2018-11-29">2018-11-29</time>
    
    <span itemprop="title">Publication of US20180341860A1</span>
    <span itemprop="type">publication</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    <span itemprop="documentId">patent/US20180341860A1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2019-09-03">2019-09-03</time>
    
    <span itemprop="title">Priority to US16/559,392</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US10719764B2/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2019-10-22">2019-10-22</time>
    
    <span itemprop="title">Application granted</span>
    <span itemprop="type">granted</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2019-10-22">2019-10-22</time>
    
    <span itemprop="title">Publication of US10452978B2</span>
    <span itemprop="type">publication</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    <span itemprop="documentId">patent/US10452978B2/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2020-07-17">2020-07-17</time>
    
    <span itemprop="title">Priority to US16/932,422</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US11113602B2/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2020-08-07">2020-08-07</time>
    
    <span itemprop="title">Priority to US16/988,518</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US10956819B2/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2020-08-07">2020-08-07</time>
    
    <span itemprop="title">Priority to US16/988,547</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US20200372358A1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2020-08-07">2020-08-07</time>
    
    <span itemprop="title">Priority to US16/988,535</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US20210019624A1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2021-09-03">2021-09-03</time>
    
    <span itemprop="title">Priority to US17/467,096</span>
    <span itemprop="type">priority</span>
    
    
    
    
    <span itemprop="documentId">patent/US20220051099A1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date">Status</time>
    
    <span itemprop="title">Active</span>
    <span itemprop="type">legal-status</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    <span itemprop="current" content="true" bool="">Current</span>
    
    
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope="" repeat="">
    
    <time itemprop="date" datetime="2038-05-23">2038-05-23</time>
    
    <span itemprop="title">Anticipated expiration</span>
    <span itemprop="type">legal-status</span>
    
    <span itemprop="critical" content="true" bool="">Critical</span>
    
    
    
    
    
    
  </dd>
  

  <h2>Links</h2>

  <ul>
    
          <li itemprop="links" itemscope="" repeat="">
            <meta itemprop="id" content="usptoLink"/>
            <a href="https://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/srchnum.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;s1=10452978.PN." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
          </li>
        <li itemprop="links" itemscope="" repeat="">
          <meta itemprop="id" content="usptoPatentCenterLink"/>
          <a href="https://patentcenter.uspto.gov/applications/16021971" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope="" repeat="">
          <meta itemprop="id" content="usptoAssignmentLink"/>
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=10452978" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope="" repeat="">
        <meta itemprop="id" content="espacenetLink"/>
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=10452978B2&amp;KC=B2&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope="" repeat="">
          <meta itemprop="id" content="globalDossierLink"/>
          <a href="https://globaldossier.uspto.gov/#/result/patent/US/10452978/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
        </li>
      

      

      

      

      <li itemprop="links" itemscope="" repeat="">
          <meta itemprop="id" content="stackexchangeLink"/>
          <a href="https://patents.stackexchange.com/questions/tagged/US10452978" itemprop="url"><span itemprop="text">Discuss</span></a>
        </li>
      
  </ul>

  

  <section>
    <h2>Images</h2>
    
  </section>

  <section>
    <h2>Classifications</h2>
    
    <ul>
      <li>
        <ul itemprop="cpcs" itemscope="" repeat="">
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G</span>—<span itemprop="Description">PHYSICS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06</span>—<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N</span>—<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/00</span>—<span itemprop="Description">Computing arrangements based on biological models</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/02</span>—<span itemprop="Description">Neural networks</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/08</span>—<span itemprop="Description">Learning methods</span>
            <meta itemprop="Leaf" content="true"/>
            
            <meta itemprop="FirstCode" content="true"/>
          </li>
          </ul>
      </li>
      <li>
        <ul itemprop="cpcs" itemscope="" repeat="">
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G</span>—<span itemprop="Description">PHYSICS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06</span>—<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N</span>—<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/00</span>—<span itemprop="Description">Computing arrangements based on biological models</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/02</span>—<span itemprop="Description">Neural networks</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/04</span>—<span itemprop="Description">Architecture, e.g. interconnection topology</span>
            <meta itemprop="Leaf" content="true"/>
            
            
          </li>
          </ul>
      </li>
      <li>
        <ul itemprop="cpcs" itemscope="" repeat="">
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G</span>—<span itemprop="Description">PHYSICS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06</span>—<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N</span>—<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/00</span>—<span itemprop="Description">Computing arrangements based on biological models</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/02</span>—<span itemprop="Description">Neural networks</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/04</span>—<span itemprop="Description">Architecture, e.g. interconnection topology</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/045</span>—<span itemprop="Description">Combinations of networks</span>
            <meta itemprop="Leaf" content="true"/>
            
            
          </li>
          </ul>
      </li>
      <li>
        <ul itemprop="cpcs" itemscope="" repeat="">
          <li itemprop="cpcs" itemscope="" repeat="">
            <span itemprop="Code">G06N3/0454</span>—<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"/>
            
            
          </li>
          </ul>
      </li>
      </ul>
  </section>

  

  <section itemprop="abstract" itemscope="">
    <h2>Abstract</h2>
    
    <div itemprop="content" html=""><abstract mxw-id="PA334054572" lang="EN" load-source="patent-office">
    <p>Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating an output sequence from an input sequence. In one aspect, one of the systems includes an encoder neural network configured to receive the input sequence and generate encoded representations of the network inputs, the encoder neural network comprising a sequence of one or more encoder subnetworks, each encoder subnetwork configured to receive a respective encoder subnetwork input for each of the input positions and to generate a respective subnetwork output for each of the input positions, and each encoder subnetwork comprising: an encoder self-attention sub-layer that is configured to receive the subnetwork input for each of the input positions and, for each particular input position in the input order: apply an attention mechanism over the encoder subnetwork inputs using one or more queries derived from the encoder subnetwork input at the particular input position.</p>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope="">
    <h2>Description</h2>
    
    <div itemprop="content" html=""><div mxw-id="PDES215379597" lang="EN" load-source="patent-office">
    
    <heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
    <p>This application is a continuation of and claims priority to PCT Application No. PCT/US2018/034224, filed on May 23, 2018, which claims priority to U.S. Provisional Application No. 62/510,256, filed on May 23, 2017, and U.S. Provisional Application No. 62/541,594, filed on Aug. 4, 2017. The entire contents of the foregoing applications are hereby incorporated by reference.</p>
    
    
    <heading id="h-0002">BACKGROUND</heading>
    <p>This specification relates to transducing sequences using neural networks.</p>
    <p>Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.</p>
    <heading id="h-0003">SUMMARY</heading>
    <p>This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates an output sequence that includes a respective output at each of multiple positions in an output order from an input sequence that includes a respective input at each of multiple positions in an input order, i.e., transduces the input sequence into the output sequence. In particular, the system generates the output sequence using an encoder neural network and a decoder neural network that are both attention-based.</p>
    <p>Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.</p>
    <p>Many existing approaches to sequence transduction using neural networks use recurrent neural networks in both the encoder and the decoder. While these kinds of networks can achieve good performance on sequence transduction tasks, their computation is sequential in nature, i.e., a recurrent neural network generates an output at a current time step conditioned on the hidden state of the recurrent neural network at the preceding time step. This sequential nature precludes parallelization, resulting in long training and inference times and, accordingly, workloads that utilize a large amount of computational resources.</p>
    <p>On the other hand, because the encoder and the decoder of the described sequence transduction neural network are attention-based, the sequence transduction neural network can transduce sequences quicker, be trained faster, or both, because the operation of the network can be more easily parallelized. That is, because the described sequence transduction neural network relies entirely on an attention mechanism to draw global dependencies between input and output and does not employ any recurrent neural network layers, the problems with long training and inference times and high resource usage caused by the sequential nature of recurrent neural network layers are mitigated.</p>
    <p>Moreover, the sequence transduction neural network can transduce sequences more accurately than existing networks that are based on convolutional layers or recurrent layers, even though training and inference times are shorter. In particular, in conventional models, the number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions, e.g., either linearly or logarithmically depending on the model architecture. This makes it more difficult to learn dependencies between distant positions during training. In the presently described sequence transduction neural network, this number of operations is reduced to a constant number of operations because of the use of attention (and, in particular, self-attention) while not relying on recurrence or convolutions. Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. The use of attention mechanisms allows the sequence transduction neural network to effectively learn dependencies between distant positions during training, improving the accuracy of the sequence transduction neural network on various transduction tasks, e.g., machine translation. In fact, the described sequence transduction neural network can achieve state-of-the-art results on the machine translation task despite being easier to train and quicker to generate outputs than conventional machine translation neural networks. The sequence transduction neural network can also exhibit improved performance over conventional machine translation neural networks without task-specific tuning through the use of the attention mechanism.</p>
    <p>The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p>
    
    
    <description-of-drawings>
      <heading id="h-0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <div id="p-0011" num="0010"> <figref idrefs="DRAWINGS">FIG. 1</figref><p> shows an example neural network system.</p></div>
      <div id="p-0012" num="0011"> <figref idrefs="DRAWINGS">FIG. 2</figref><p> is a diagram showing attention mechanisms that are applied by the attention sub-layers in the subnetworks of the encoder neural network and the decoder neural network.</p></div>
      <div id="p-0013" num="0012"> <figref idrefs="DRAWINGS">FIG. 3</figref><p> is a flow diagram of an example process for generating an output sequence from an input sequence.</p></div>
    </description-of-drawings>
    
    
    <p>Like reference numbers and designations in the various drawings indicate like elements.</p>
    <heading id="h-0005">DETAILED DESCRIPTION</heading>
    <p>This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates an output sequence that includes a respective output at each of multiple positions in an output order from an input sequence that includes a respective input at each of multiple positions in an input order, i.e., transduces the input sequence into the output sequence.</p>
    <p>For example, the system may be a neural machine translation system. That is, if the input sequence is a sequence of words in an original language, e.g., a sentence or phrase, the output sequence may be a translation of the input sequence into a target language, i.e., a sequence of words in the target language that represents the sequence of words in the original language.</p>
    <p>As another example, the system may be a speech recognition system. That is, if the input sequence is a sequence of audio data representing a spoken utterance, the output sequence may be a sequence of graphemes, characters, or words that represents the utterance, i.e., is a transcription of the input sequence.</p>
    <p>As another example, the system may be a natural language processing system. For example, if the input sequence is a sequence of words in an original language, e.g., a sentence or phrase, the output sequence may be a summary of the input sequence in the original language, i.e., a sequence that has fewer words than the input sequence but that retains the essential meaning of the input sequence. As another example, if the input sequence is a sequence of words that form a question, the output sequence can be a sequence of words that form an answer to the question.</p>
    <p>As another example, the system may be part of a computer-assisted medical diagnosis system. For example, the input sequence can be a sequence of data from an electronic medical record and the output sequence can be a sequence of predicted treatments.</p>
    <p>As another example, the system may be part of an image processing system. For example, the input sequence can be an image, i.e., a sequence of color values from the image, and the output can be a sequence of text that describes the image. As another example, the input sequence can be a sequence of text or a different context and the output sequence can be an image that describes the context.</p>
    <p>In particular, the neural network includes an encoder neural network and a decoder neural network. Generally, both the encoder and the decoder are attention-based, i.e., both apply an attention mechanism over their respective received inputs while transducing the input sequence. In some cases, neither the encoder nor the decoder include any convolutional layers or any recurrent layers.</p>
    <div id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 1</figref><p> shows an example </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b>. The </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b> is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.</p></div>
    <div id="p-0023" num="0022"><p>The </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b> receives an </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> and processes the </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> to transduce the </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> into an </p><figure-callout id="152" label="output sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">output sequence</figure-callout> <p><b>152</b>.</p></div>
    <div id="p-0024" num="0023"><p>The </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> has a respective network input at each of multiple input positions in an input order and the </p><figure-callout id="152" label="output sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">output sequence</figure-callout> <p><b>152</b> has a respective network output at each of multiple output positions in an output order. That is, the </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> has multiple inputs arranged according to an input order and the </p><figure-callout id="152" label="output sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">output sequence</figure-callout> <p><b>152</b> has multiple outputs arranged according to an output order.</p></div>
    <div id="p-0025" num="0024"><p>As described above, the </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b> can perform any of a variety of tasks that require processing sequential inputs to generate sequential outputs.</p></div>
    <div id="p-0026" num="0025"><p>The </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b> includes an attention-based sequence transduction </p><figure-callout id="108" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>108</b>, which in turn includes an encoder </p><figure-callout id="110" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>110</b> and a decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b>.</p></div>
    <div id="p-0027" num="0026"><p>The encoder </p><figure-callout id="110" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>110</b> is configured to receive the </p><figure-callout id="102" label="input sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">input sequence</figure-callout> <p><b>102</b> and generate a respective encoded representation of each of the network inputs in the input sequence. Generally, an encoded representation is a vector or other ordered collection of numeric values.</p></div>
    <div id="p-0028" num="0027"><p>The decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> is then configured to use the encoded representations of the network inputs to generate the </p><figure-callout id="152" label="output sequence" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">output sequence</figure-callout> <p><b>152</b>.</p></div>
    <div id="p-0029" num="0028"><p>Generally, and as will be described in more detail below, both the </p><figure-callout id="110" label="encoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder</figure-callout> <p><b>110</b> and the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> are attention-based. In some cases, neither the encoder nor the decoder include any convolutional layers or any recurrent layers.</p></div>
    <div id="p-0030" num="0029"><p>The encoder </p><figure-callout id="110" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>110</b> includes an </p><figure-callout id="120" label="embedding layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">embedding layer</figure-callout> <p><b>120</b> and a sequence of one or </p><figure-callout id="130" label="more encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">more encoder subnetworks</figure-callout> <p><b>130</b>. In particular, as shown in </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>, the encoder neural network includes </p><figure-callout id="130" label="N encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">N encoder subnetworks</figure-callout> <p><b>130</b>.</p></div>
    <div id="p-0031" num="0030"><p>The embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b> is configured to, for each network input in the input sequence, map the network input to a numeric representation of the network input in an embedding space, e.g., into a vector in the embedding space. The embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b> then provides the numeric representations of the network inputs to the first subnetwork in the sequence of </p><figure-callout id="130" label="encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetworks</figure-callout> <p><b>130</b>, i.e., to the </p><figure-callout id="130" label="first encoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">first encoder subnetwork</figure-callout> <p><b>130</b> of the </p><figure-callout id="130" label="N encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">N encoder subnetworks</figure-callout> <p><b>130</b>.</p></div>
    <div id="p-0032" num="0031"><p>In particular, in some implementations, the embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b> is configured to map each network input to an embedded representation of the network input and then combine, e.g., sum or average, the embedded representation of the network input with a positional embedding of the input position of the network input in the input order to generate a combined embedded representation of the network input. That is, each position in the input sequence has a corresponding embedding and for each network input the embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b> combines the embedded representation of the network input with the embedding of the network input&#39;s position in the input sequence. Such positional embeddings can enable the model to make full use of the order of the input sequence without relying on recurrence or convolutions.</p></div>
    <div id="p-0033" num="0032"><p>In some cases, the positional embeddings are learned. As used in this specification, the term “learned” means that an operation or a value has been adjusted during the training of the sequence transduction </p><figure-callout id="108" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>108</b>. Training the sequence transduction </p><figure-callout id="108" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>108</b> is described below with reference to </p><figref idrefs="DRAWINGS">FIG. 3</figref><p>.</p></div>
    <div id="p-0034" num="0033"><p>In some other cases, the positional embeddings are fixed and are different for each position. For example, the embeddings can be made up of sine and cosine functions of different frequencies and can satisfy:
</p></div>
    <p>The combined embedded representation is then used as the numeric representation of the network input.</p>
    <div id="p-0036" num="0035"><p>Each of the </p><figure-callout id="130" label="encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetworks</figure-callout> <p><b>130</b> is configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions.</p></div>
    <p>The encoder subnetwork outputs generated by the last encoder subnetwork in the sequence are then used as the encoded representations of the network inputs.</p>
    <div id="p-0038" num="0037"><p>For the first encoder subnetwork in the sequence, the encoder subnetwork input is the numeric representations generated by the embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b>, and, for each encoder subnetwork other than the first encoder subnetwork in the sequence, the encoder subnetwork input is the encoder subnetwork output of the preceding encoder subnetwork in the sequence.</p></div>
    <div id="p-0039" num="0038"><p>Each </p><figure-callout id="130" label="encoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetwork</figure-callout> <p><b>130</b> includes an encoder self-</p><figure-callout id="132" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>132</b>. The encoder self-</p><figure-callout id="132" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>132</b> is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order, apply an attention mechanism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for the particular input position. In some cases, the attention mechanism is a multi-head attention mechanism. The attention mechanism and how the attention mechanism is applied by the encoder self-</p><figure-callout id="132" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>132</b> will be described in more detail below with reference to </p><figref idrefs="DRAWINGS">FIG. 2</figref><p>.</p></div>
    <div id="p-0040" num="0039"><p>In some implementations, each of the </p><figure-callout id="130" label="encoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetworks</figure-callout> <p><b>130</b> also includes a residual connection layer that combines the outputs of the encoder self-attention sub-layer with the inputs to the encoder self-attention sub-layer to generate an encoder self-attention residual output and a layer normalization layer that applies layer normalization to the encoder self-attention residual output. These two layers are collectively referred to as an “Add &amp; Norm” operation in </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>.</p></div>
    <div id="p-0041" num="0040"><p>Some or all of the encoder subnetworks can also include a position-wise feed-</p><figure-callout id="134" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>134</b> that is configured to operate on each position in the input sequence separately. In particular, for each input position, the feed-</p><figure-callout id="134" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>134</b> is configured receive an input at the input position and apply a sequence of transformations to the input at the input position to generate an output for the input position. For example, the sequence of transformations can include two or more learned linear transformations each separated by an activation function, e.g., a non-linear elementwise activation function, e.g., a ReLU activation function, which can allow for faster and more effective training on large and complex datasets. The inputs received by the position-wise feed-</p><figure-callout id="134" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>134</b> can be the outputs of the layer normalization layer when the residual and layer normalization layers are included or the outputs of the encoder self-</p><figure-callout id="132" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>132</b> when the residual and layer normalization layers are not included. The transformations applied by the </p><figure-callout id="134" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>134</b> will generally be the same for each input position (but different feed-forward layers in different subnetworks will apply different transformations).</p></div>
    <div id="p-0042" num="0041"><p>In cases where an </p><figure-callout id="130" label="encoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetwork</figure-callout> <p><b>130</b> includes a position-wise feed-</p><figure-callout id="134" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>134</b>, the encoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate an encoder position-wise residual output and a layer normalization layer that applies layer normalization to the encoder position-wise residual output. These two layers are also collectively referred to as an “Add &amp; Norm” operation in </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>. The outputs of this layer normalization layer can then be used as the outputs of the </p><figure-callout id="130" label="encoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder subnetwork</figure-callout> <p><b>130</b>.</p></div>
    <div id="p-0043" num="0042"><p>Once the encoder </p><figure-callout id="110" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>110</b> has generated the encoded representations, the decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> is configured to generate the output sequence in an auto-regressive manner.</p></div>
    <div id="p-0044" num="0043"><p>That is, the decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> generates the output sequence, by at each of a plurality of generation time steps, generating a network output for a corresponding output position conditioned on (i) the encoded representations and (ii) network outputs at output positions preceding the output position in the output order.</p></div>
    <p>In particular, for a given output position, the decoder neural network generates an output that defines a probability distribution over possible network outputs at the given output position. The decoder neural network can then select a network output for the output position by sampling from the probability distribution or by selecting the network output with the highest probability.</p>
    <div id="p-0046" num="0045"><p>Because the decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> is auto-regressive, at each generation time step, the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> operates on the network outputs that have already been generated before the generation time step, i.e., the network outputs at output positions preceding the corresponding output position in the output order. In some implementations, to ensure this is the case during both inference and training, at each generation time step the decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> shifts the already generated network outputs right by one output order position (i.e., introduces a one position offset into the already generated network output sequence) and (as will be described in more detail below) masks certain operations so that positions can only attend to positions up to and including that position in the output sequence (and not subsequent positions). While the remainder of the description below describes that, when generating a given output at a given output position, various components of the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> operate on data at output positions preceding the given output positions (and not on data at any other output positions), it will be understood that this type of conditioning can be effectively implemented using the shifting described above.</p></div>
    <div id="p-0047" num="0046"><p>The decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b> includes an embedding </p><figure-callout id="160" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>160</b>, a sequence of </p><figure-callout id="170" label="decoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetworks</figure-callout> <p><b>170</b>, a </p><figure-callout id="180" label="linear layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">linear layer</figure-callout> <p><b>180</b>, and a </p><figure-callout id="190" label="softmax layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">softmax layer</figure-callout> <p><b>190</b>. In particular, as shown in </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>, the decoder neural network includes </p><figure-callout id="170" label="N decoder subnetworks" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">N decoder subnetworks</figure-callout> <p><b>170</b>. However, while the example of </p><figref idrefs="DRAWINGS">FIG. 1</figref><p> shows the </p><figure-callout id="110" label="encoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder</figure-callout> <p><b>110</b> and the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> including the same number of subnetworks, in some cases the </p><figure-callout id="110" label="encoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder</figure-callout> <p><b>110</b> and the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> include different numbers of subnetworks. That is, the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> can include more or fewer subnetworks than the </p><figure-callout id="110" label="encoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder</figure-callout> <p><b>110</b>.</p></div>
    <div id="p-0048" num="0047"><p>The embedding </p><figure-callout id="160" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>160</b> is configured to, at each generation time step, for each network output at an output position that precedes the current output position in the output order, map the network output to a numeric representation of the network output in the embedding space. The embedding </p><figure-callout id="160" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>160</b> then provides the numeric representations of the network outputs to the </p><figure-callout id="170" label="first subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">first subnetwork</figure-callout> <p><b>170</b> in the sequence of decoder subnetworks, i.e., to the </p><figure-callout id="170" label="first decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">first decoder subnetwork</figure-callout> <p><b>170</b> of the N decoder subnetworks.</p></div>
    <div id="p-0049" num="0048"><p>In particular, in some implementations, the embedding </p><figure-callout id="160" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>160</b> is configured to map each network output to an embedded representation of the network output and combine the embedded representation of the network output with a positional embedding of the output position of the network output in the output order to generate a combined embedded representation of the network output. The combined embedded representation is then used as the numeric representation of the network output. The embedding </p><figure-callout id="160" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>160</b> generates the combined embedded representation in the same manner as described above with reference to the embedding </p><figure-callout id="120" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>120</b>.</p></div>
    <div id="p-0050" num="0049"><p>Each </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> is configured to, at each generation time step, receive a respective decoder subnetwork input for each of the plurality of output positions preceding the corresponding output position and to generate a respective decoder subnetwork output for each of the plurality of output positions preceding the corresponding output position (or equivalently, when the output sequence has been shifted right, each network output at a position up to and including the current output position).</p></div>
    <div id="p-0051" num="0050"><p>In particular, each </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> includes two different attention sub-layers: a decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> and an encoder-</p><figure-callout id="174" label="decoder attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder attention sub-layer</figure-callout> <p><b>174</b>.</p></div>
    <div id="p-0052" num="0051"><p>Each decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the particular output positions, apply an attention mechanism over the inputs at the output positions preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the particular output position. That is, the decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> applies an attention mechanism that is masked so that it does not attend over or otherwise process any data that is not at a position preceding the current output position in the output sequence.</p></div>
    <div id="p-0053" num="0052"><p>Each encoder-</p><figure-callout id="174" label="decoder attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder attention sub-layer</figure-callout> <p><b>174</b>, on the other hand, is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the output positions, apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the output position to generate an updated representation for the output position. Thus, the encoder-</p><figure-callout id="174" label="decoder attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder attention sub-layer</figure-callout> <p><b>174</b> applies attention over encoded representations while the encoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> applies attention over inputs at output positions.</p></div>
    <div id="p-0054" num="0053"><p>The attention mechanism applied by each of these attention sub-layers will be described in more detail below with reference to </p><figref idrefs="DRAWINGS">FIG. 2</figref><p>.</p></div>
    <div id="p-0055" num="0054"><p>In </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>, the decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> is shown as being before the encoder-decoder attention sub-layer in the processing order within the </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b>. In other examples, however, the decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b> may be after the encoder-</p><figure-callout id="174" label="decoder attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder attention sub-layer</figure-callout> <p><b>174</b> in the processing order within the </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> or different subnetworks may have different processing orders.</p></div>
    <div id="p-0056" num="0055"><p>In some implementations, each </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> includes, after the decoder self-</p><figure-callout id="172" label="attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">attention sub-layer</figure-callout> <p><b>172</b>, after the encoder-</p><figure-callout id="174" label="decoder attention sub-layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder attention sub-layer</figure-callout> <p><b>174</b>, or after each of the two sub-layers, a residual connection layer that combines the outputs of the attention sub-layer with the inputs to the attention sub-layer to generate a residual output and a layer normalization layer that applies layer normalization to the residual output. </p><figref idrefs="DRAWINGS">FIG. 1</figref><p> shows these two layers being inserted after each of the two sub-layers, both referred to as an “Add &amp; Norm” operation.</p></div>
    <div id="p-0057" num="0056"><p>Some or all of the </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> also include a position-wise feed-</p><figure-callout id="176" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>176</b> that is configured to operate in a similar manner as the position-wise feed-</p><figure-callout id="134" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>134</b> from the </p><figure-callout id="110" label="encoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">encoder</figure-callout> <p><b>110</b>. In particular, the </p><figure-callout id="176" label="layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">layer</figure-callout> <p><b>176</b> is configured to, at each generation time step: for each output position preceding the corresponding output position: receive an input at the output position, and apply a sequence of transformations to the input at the output position to generate an output for the output position. For example, the sequence of transformations can include two or more learned linear transformations each separated by an activation function, e.g., a non-linear elementwise activation function, e.g., a ReLU activation function. The inputs received by the position-wise feed-</p><figure-callout id="176" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>176</b> can be the outputs of the layer normalization layer (following the last attention sub-layer in the subnetwork <b>170</b>) when the residual and layer normalization layers are included or the outputs of the last attention sub-layer in the </p><figure-callout id="170" label="subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">subnetwork</figure-callout> <p><b>170</b> when the residual and layer normalization layers are not included.</p></div>
    <div id="p-0058" num="0057"><p>In cases where a </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b> includes a position-wise feed-</p><figure-callout id="176" label="forward layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">forward layer</figure-callout> <p><b>176</b>, the decoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate a decoder position-wise residual output and a layer normalization layer that applies layer normalization to the decoder position-wise residual output. These two layers are also collectively referred to as an “Add &amp; Norm” operation in </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>. The outputs of this layer normalization layer can then be used as the outputs of the </p><figure-callout id="170" label="decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder subnetwork</figure-callout> <p><b>170</b>.</p></div>
    <div id="p-0059" num="0058"><p>At each generation time step, the </p><figure-callout id="180" label="linear layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">linear layer</figure-callout> <p><b>180</b> applies a learned linear transformation to the output of the </p><figure-callout id="170" label="last decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">last decoder subnetwork</figure-callout> <p><b>170</b> in order to project the output of the </p><figure-callout id="170" label="last decoder subnetwork" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">last decoder subnetwork</figure-callout> <p><b>170</b> into the appropriate space for processing by the </p><figure-callout id="190" label="softmax layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">softmax layer</figure-callout> <p><b>190</b>. The </p><figure-callout id="190" label="softmax layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">softmax layer</figure-callout> <p><b>190</b> then applies a softmax function over the outputs of the </p><figure-callout id="180" label="linear layer" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">linear layer</figure-callout> <p><b>180</b> to generate the probability distribution over the possible network outputs at the generation time step. As described above, the </p><figure-callout id="150" label="decoder" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">decoder</figure-callout> <p><b>150</b> can then select a network output from the possible network outputs using the probability distribution.</p></div>
    <div id="p-0060" num="0059"> <figref idrefs="DRAWINGS">FIG. 2</figref><p> is a diagram <b>200</b> showing attention mechanisms that are applied by the attention sub-layers in the subnetworks of the encoder </p><figure-callout id="110" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>110</b> and the decoder </p><figure-callout id="150" label="neural network" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network</figure-callout> <p><b>150</b>.</p></div>
    <p>Generally, an attention mechanism maps a query and a set of key-value pairs to an output, where the query, keys, and values are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
    <div id="p-0062" num="0061"><p>More specifically, each attention sub-layer applies a scaled dot-</p><figure-callout id="230" label="product attention mechanism" filenames="US10452978-20191022-D00002.png" state="{{state}}">product attention mechanism</figure-callout> <p><b>230</b>. In scaled dot-product attention, for a given query, the attention sub-layer computes the dot products of the query with all of the keys, divides each of the dot products by a scaling factor, e.g., by the square root of the dimensions of the queries and keys, and then applies a softmax function over the scaled dot products to obtain the weights on the values. The attention sub-layer then computes a weighted sum of the values in accordance with these weights. Thus, for scaled dot-product attention the compatibility function is the dot product and the output of the compatibility function is further scaled by the scaling factor.</p></div>
    <div id="p-0063" num="0062"><p>In operation and as shown in the left hand side of </p><figref idrefs="DRAWINGS">FIG. 2</figref><p>, the attention sub-layer computes the attention over a set of queries simultaneously. In particular, the attention sub-layer packs the queries into a matrix Q, packs the keys into a matrix K, and packs the values into a matrix V. To pack a set of vectors into a matrix, the attention sub-layer can generate a matrix that includes the vectors as the rows of the matrix.</p></div>
    <p>The attention sub-layer then performs a matrix multiply (MatMul) between the matrix Q and the transpose of the matrix K to generate a matrix of compatibility function outputs.</p>
    <p>The attention sub-layer then scales the compatibility function output matrix, i.e., by dividing each element of the matrix by the scaling factor.</p>
    <p>The attention sub-layer then applies a softmax over the scaled output matrix to generate a matrix of weights and performs a matrix multiply (MatMul) between the weight matrix and the matrix V to generate an output matrix that includes the output of the attention mechanism for each of the values.</p>
    <p>For sub-layers that use masking, i.e., decoder attention sub-layers, the attention sub-layer masks the scaled output matrix before applying the softmax. That is, the attention sub-layer masks out (sets to negative infinity), all values in the scaled output matrix that correspond to positions after the current output position.</p>
    <div id="p-0068" num="0067"><p>In some implementations, to allow the attention sub-layers to jointly attend to information from different representation subspaces at different positions, the attention sub-layers employ multi-head attention, as illustrated on the right hand side of </p><figref idrefs="DRAWINGS">FIG. 2</figref><p>.</p></div>
    <p>In particular, to implement multi-ahead attention, the attention sub-layer applies h different attention mechanisms in parallel. In other words, the attention sub-layer includes h different attention layers, with each attention layer within the same attention sub-layer receiving the same original queries Q, original keys K, and original values V.</p>
    <div id="p-0070" num="0069"><p>Each attention layer is configured to transform the original queries, and keys, and values using learned linear transformations and then apply the </p><figure-callout id="230" label="attention mechanism" filenames="US10452978-20191022-D00002.png" state="{{state}}">attention mechanism</figure-callout> <p><b>230</b> to the transformed queries, keys, and values. Each attention layer will generally learn different transformations from each other attention layer in the same attention sub-layer.</p></div>
    <p>In particular, each attention layer is configured to apply a learned query linear transformation to each original query to generate a layer-specific query for each original query, apply a learned key linear transformation to each original key to generate a layer-specific key for each original key, and apply a learned value linear transformation to each original value to generate a layer-specific values for each original value. The attention layer then applies the attention mechanism described above using these layer-specific queries, keys, and values to generate initial outputs for the attention layer.</p>
    <div id="p-0072" num="0071"><p>The attention sub-layer then combines the initial outputs of the attention layers to generate the final output of the attention sub-layer. As shown in </p><figref idrefs="DRAWINGS">FIG. 2</figref><p>, the attention sub-layer concatenates (concat) the outputs of the attention layers and applies a learned linear transformation to the concatenated output to generate the output of the attention sub-layer.</p></div>
    <p>In some cases, the learned transformations applied by the attention sub-layer reduce the dimensionality of the original keys and values and, optionally, the queries. For example, when the dimensionality of the original keys, values, and queries is d and there are h attention layers in the sub-layer, the sub-layer may reduce the dimensionality of the original keys, values, and queries to d/h. This keeps the computation cost of the multi-head attention mechanism similar to what the cost would have been to perform the attention mechanism once with full dimensionality while at the same time increasing the representative capacity of the attention sub-layer.</p>
    <p>While the attention mechanism applied by each attention sub-layer is the same, the queries, keys, and values are different for different types of attention. That is, different types of attention sub-layers use different sources for the original queries, keys, and values that are received as input by the attention sub-layer.</p>
    <p>In particular, when the attention sub-layer is an encoder self-attention sub-layer, all of the keys, values and queries come from the same place, in this case, the output of the previous subnetwork in the encoder, or, for the encoder self-attention sub-layer in first subnetwork, the embeddings of the inputs and each position in the encoder can attend to all positions in the input order. Thus, there is a respective key, value, and query for each position in the input order.</p>
    <p>When the attention sub-layer is a decoder self-attention sub-layer, each position in the decoder attends to all positions in the decoder preceding that position. Thus, all of the keys, values, and queries come from the same place, in this case, the output of the previous subnetwork in the decoder, or, for the decoder self-attention sub-layer in the first decoder subnetwork, the embeddings of the outputs already generated. Thus, there is a respective key, value, and query for each position in the output order before the current position.</p>
    <p>When the attention sub-layer is an encoder-decoder attention sub-layer, the queries come from the previous component in the decoder and the keys and values come from the output of the encoder, i.e., from the encoded representations generated by the encoder. This allows every position in the decoder to attend over all positions in the input sequence. Thus, there is a respective query for each for each position in the output order before the current position and a respective key and a respective value for each position in the input order.</p>
    <p>In more detail, when the attention sub-layer is an encoder self-attention sub-layer, for each particular input position in the input order, the encoder self-attention sub-layer is configured to apply an attention mechanism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for the particular input position.</p>
    <p>When the encoder self-attention sub-layer implements multi-head attention, each encoder self-attention layer in the encoder self-attention sub-layer is configured to: apply a learned query linear transformation to each encoder subnetwork input at each input position to generate a respective query for each input position, apply a learned key linear transformation to each encoder subnetwork input at each input position to generate a respective key for each input position, apply a learned value linear transformation to each encoder subnetwork input at each input position to generate a respective value for each input position, and then apply the attention mechanism (i.e., the scaled dot-product attention mechanism described above) using the queries, keys, and values to determine an initial encoder self-attention output for each input position. The sub-layer then combines the initial outputs of the attention layers as described above.</p>
    <p>When the attention sub-layer is a decoder self-attention sub-layer, the decoder self-attention sub-layer is configured to, at each generation time step: receive an input for each output position preceding the corresponding output position and, for each of the particular output positions, apply an attention mechanism over the inputs at the output positions preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the particular output position.</p>
    <p>When the decoder self-attention sub-layer implements multi-head attention, each attention layer in the decoder self-attention sub-layer is configured to, at each generation time step, apply a learned query linear transformation to the input at each output position preceding the corresponding output position to generate a respective query for each output position, apply a learned key linear transformation to each input at each output position preceding the corresponding output position to generate a respective key for each output position, apply a learned value linear transformation to each input at each output position preceding the corresponding output position to generate a respective key for each output position, and then apply the attention mechanism (i.e., the scaled dot-product attention mechanism described above) using the queries, keys, and values to determine an initial decoder self-attention output for each of the output positions. The sub-layer then combines the initial outputs of the attention layers as described above.</p>
    <p>When the attention sub-layer is an encoder-decoder attention sub-layer, the encoder-decoder attention sub-layer is configured to, at each generation time step: receive an input for each output position preceding the corresponding output position and, for each of the output positions, apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the output position to generate an updated representation for the output position.</p>
    <p>When the encoder-decoder attention sub-layer implements multi-head attention, each attention layer is configured to, at each generation time step: apply a learned query linear transformation to the input at each output position preceding the corresponding output position to generate a respective query for each output position, apply a learned key linear transformation to each encoded representation at each input position to generate a respective key for each input position, apply a learned value linear transformation to each encoded representation at each input position to generate a respective value for each input position, and then apply the attention mechanism (i.e., the scaled dot-product attention mechanism described above) using the queries, keys, and values to determine an initial encoder-decoder attention output for each input position. The sub-layer then combines the initial outputs of the attention layers as described above.</p>
    <div id="p-0084" num="0083"> <figref idrefs="DRAWINGS">FIG. 3</figref><p> is a flow diagram of an example process for generating an output sequence from an input sequence. For convenience, the </p><figure-callout id="300" label="process" filenames="US10452978-20191022-D00003.png" state="{{state}}">process</figure-callout> <p><b>300</b> will be described as being performed by a system of one or more computers located in one or more locations. For example, a neural network system, e.g., </p><figure-callout id="100" label="neural network system" filenames="US10452978-20191022-D00000.png,US10452978-20191022-D00001.png" state="{{state}}">neural network system</figure-callout> <p><b>100</b> of </p><figref idrefs="DRAWINGS">FIG. 1</figref><p>, appropriately programmed in accordance with this specification, can perform the </p><figure-callout id="300" label="process" filenames="US10452978-20191022-D00003.png" state="{{state}}">process</figure-callout> <p><b>300</b>.</p></div>
    <p>The system receives an input sequence (step <b>310</b>).</p>
    <p>The system processes the input sequence using the encoder neural network to generate a respective encoded representation of each of the network inputs in the input sequence (step <b>320</b>). In particular, the system processes the input sequence through the embedding layer to generate an embedded representation of each network input and then process the embedded representations through the sequence of encoder subnetworks to generate the encoded representations of the network inputs.</p>
    <p>The system processes the encoded representations using the decoder neural network to generate an output sequence (step <b>330</b>). The decoder neural network is configured to generate the output sequence from the encoded representations in an auto-regressive manner. That is, the decoder neural network generates one output from the output sequence at each generation time step. At a given generation time step at which a given output is being generated, the system processes the outputs before the given output in the output sequence through the embedding layer in the decoder to generate embedded representations. The system then processes the embedded representations through the sequence of decoder subnetworks, the linear layer, and the softmax layer to generate the given output. Because the decoder subnetworks include encoder-decoder attention sub-layers as well as decoder self-attention sub-layers, the decoder makes use of both the already generated outputs and the encoded representations when generating the given output.</p>
    <div id="p-0088" num="0087"><p>The system can perform the </p><figure-callout id="300" label="process" filenames="US10452978-20191022-D00003.png" state="{{state}}">process</figure-callout> <p><b>300</b> for input sequences for which the desired output, i.e., the output sequence that should be generated by the system for the input sequence, is not known.</p></div>
    <div id="p-0089" num="0088"><p>The system can also perform the </p><figure-callout id="300" label="process" filenames="US10452978-20191022-D00003.png" state="{{state}}">process</figure-callout> <p><b>300</b> on input sequences in a set of training data, i.e., a set of inputs for which the output sequence that should be generated by the system is known, in order to train the encoder and the decoder to determine trained values for the parameters of the encoder and decoder. The </p><figure-callout id="300" label="process" filenames="US10452978-20191022-D00003.png" state="{{state}}">process</figure-callout> <p><b>300</b> can be performed repeatedly on inputs selected from a set of training data as part of a conventional machine learning training technique to train the initial neural network layers, e.g., a gradient descent with backpropagation training technique that uses a conventional optimizer, e.g., the Adam optimizer. During training, the system can incorporate any number of techniques to improve the speed, the effectiveness, or both of the training process. For example, the system can use dropout, label smoothing, or both to reduce overfitting. As another example, the system can perform the training using a distributed architecture that trains multiple instances of the sequence transduction neural network in parallel.</p></div>
    <p>This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.</p>
    <p>Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.</p>
    <p>The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.</p>
    <p>A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.</p>
    <p>In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.</p>
    <p>Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.</p>
    <p>The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.</p>
    <p>Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.</p>
    <p>Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.</p>
    <p>To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user&#39;s device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.</p>
    <p>Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.</p>
    <p>Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.</p>
    <p>Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.</p>
    <p>The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.</p>
    <p>While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p>
    <p>Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p>
    <p>Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.</p>
    
  </div>
  </div>
  </section>

  <section itemprop="claims" itemscope="">
    <h2>Claims (<span itemprop="count">30</span>)</h2>
    
    <div itemprop="content" html=""><div mxw-id="PCLM209452168" lang="EN" load-source="patent-office">
    <claim-statement>What is claimed is:</claim-statement>
    <div> <div id="CLM-00001" num="00001">
      <div><p>1. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement a sequence transduction neural network for transducing an input sequence having a respective network input at each of a plurality of input positions in an input order into an output sequence having a respective network output at each of a plurality of output positions in an output order, the sequence transduction neural network comprising:
</p><div><p>an encoder neural network configured to receive the input sequence and generate a respective encoded representation of each of the network inputs in the input sequence, the encoder neural network comprising a sequence of one or more encoder subnetworks, each encoder subnetwork configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions, and each encoder subnetwork comprising:
</p><div><p>an encoder self-attention sub-layer that is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order:
</p><p>apply a self-attention mechanism over the encoder subnetwork inputs at the plurality of input positions to generate a respective output for the particular input position, wherein applying a self-attention mechanism comprises: determining a query from the subnetwork input at the particular input position, determining keys derived from the subnetwork inputs at the plurality of input positions, determining values derived from the subnetwork inputs at the plurality of input positions, and using the determined query, keys, and values to generate the respective output for the particular input position; and</p>
</div>
</div>
<p>a decoder neural network configured to receive the encoded representations and generate the output sequence.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00002" num="00002">
      <div><p>2. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein the encoder neural network further comprises:
</p><div><p>an embedding layer configured to:
</p><div><p>for each network input in the input sequence,
</p><p>map the network input to an embedded representation of the network input, and</p>
<p>combine the embedded representation of the network input with a positional embedding of the input position of the network input in the input order to generate a combined embedded representation of the network input; and</p>
</div>
<p>provide the combined embedded representations of the network inputs as the encoder subnetwork inputs for a first encoder subnetwork in the sequence of encoder subnetworks.</p>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00003" num="00003">
      <div><p>3. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein the respective encoded representations of the network inputs are the encoder subnetwork outputs generated by the last encoder subnetwork in the sequence.</p></div>
    </div>
    </div> <div> <div id="CLM-00004" num="00004">
      <div><p>4. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein the sequence of one or more encoder subnetworks includes at least two encoder subnetworks, and wherein, for each encoder subnetwork other than a first encoder subnetwork in the sequence, the encoder subnetwork input is the encoder subnetwork output of a preceding encoder subnetwork in the sequence.</p></div>
    </div>
    </div> <div> <div id="CLM-00005" num="00005">
      <div><p>5. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein at least one of the encoder subnetworks further comprises:
</p><div><p>a position-wise feed-forward layer that is configured to:
</p><div><p>for each input position:
</p><p>receive an input at the input position, and</p>
<p>apply a sequence of transformations to the input at the input position to generate an output for the input position.</p>
</div>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00006" num="00006">
      <div><p>6. The system of </p><claim-ref idref="CLM-00005">claim 5</claim-ref><p>, wherein the sequence comprises two learned linear transformations separated by an activation function.</p></div>
    </div>
    </div> <div> <div id="CLM-00007" num="00007">
      <div><p>7. The system of </p><claim-ref idref="CLM-00005">claim 5</claim-ref><p>, wherein the at least one encoder subnetwork further comprises:
</p><p>a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate an encoder position-wise residual output, and</p>
<p>a layer normalization layer that applies layer normalization to the encoder position-wise residual output.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00008" num="00008">
      <div><p>8. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein each encoder subnetwork further comprises:
</p><p>a residual connection layer that combines the outputs of the encoder self-attention sub-layer with the inputs to the encoder self-attention sub-layer to generate an encoder self-attention residual output, and</p>
<p>a layer normalization layer that applies layer normalization to the encoder self-attention residual output.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00009" num="00009">
      <div><p>9. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein each encoder self-attention sub-layer comprises a plurality of encoder self-attention layers.</p></div>
    </div>
    </div> <div> <div id="CLM-00010" num="00010">
      <div><p>10. The system of </p><claim-ref idref="CLM-00009">claim 9</claim-ref><p>,
</p><p>wherein each encoder self-attention layer is configured to:</p>
<p>apply a learned query linear transformation to each encoder subnetwork input at each input position to generate a respective query for each input position,</p>
<p>apply a learned key linear transformation to each encoder subnetwork input at each input position to generate a respective key for each input position,</p>
<p>apply a learned value linear transformation to each encoder subnetwork input at each input position to generate a respective value for each input position, and</p>
<div><p>for each input position,
</p><p>determine a respective input-position specific weight for the input position by applying a comparison function between the query for the input position and the keys generated for the plurality of input positions, and</p>
<p>determine an initial encoder self-attention output for the input position by determining a weighted sum of the values weighted by the corresponding input-position specific weights for the plurality of input positions, the values being generated for the plurality of input positions.</p>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00011" num="00011">
      <div><p>11. The system of </p><claim-ref idref="CLM-00010">claim 10</claim-ref><p>, wherein the encoder self-attention sub-layer is configured to, for each input position, combine the initial encoder self-attention outputs for the input position generated by the encoder self-attention layers to generate the output for the encoder self-attention sub-layer.</p></div>
    </div>
    </div> <div> <div id="CLM-00012" num="00012">
      <div><p>12. The system of </p><claim-ref idref="CLM-00009">claim 9</claim-ref><p>, wherein the encoder self-attention layers operate in parallel.</p></div>
    </div>
    </div> <div> <div id="CLM-00013" num="00013">
      <div><p>13. The system of </p><claim-ref idref="CLM-00001">claim 1</claim-ref><p>, wherein the decoder neural network auto-regressively generates the output sequence, by at each of a plurality of generation time steps, generating a network output at an output position corresponding to the generation time step conditioned on the encoded representations and network outputs at output positions preceding the output position in the output order.</p></div>
    </div>
    </div> <div> <div id="CLM-00014" num="00014">
      <div><p>14. The system of </p><claim-ref idref="CLM-00013">claim 13</claim-ref><p>, wherein the decoder neural network comprises a sequence of decoder subnetworks, each decoder subnetwork configured to, at each generation time step, receive a respective decoder subnetwork input for each of the plurality of output positions preceding the corresponding output position and to generate a respective decoder subnetwork output for each of the plurality of output positions preceding the corresponding output position.</p></div>
    </div>
    </div> <div> <div id="CLM-00015" num="00015">
      <div><p>15. The system of </p><claim-ref idref="CLM-00014">claim 14</claim-ref><p>, wherein the decoder neural network further comprises:
</p><div><p>an embedding layer configured to, at each generation time step:
</p><div><p>for each network output at output positions preceding the corresponding output position in the output order:
</p><p>map the network output to an embedded representation of the network output, and</p>
<p>combine the embedded representation of the network output with a positional embedding of the corresponding output position of the network output in the output order to generate a combined embedded representation of the network output; and</p>
</div>
<p>provide the combined embedded representations of the network output as input to a first decoder subnetwork in the sequence of decoder subnetworks.</p>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00016" num="00016">
      <div><p>16. The system of </p><claim-ref idref="CLM-00014">claim 14</claim-ref><p>, wherein at least one of the decoder subnetworks comprises:
</p><div><p>a position-wise feed-forward layer that is configured to, at each generation time step:
</p><div><p>for each particular output position preceding the corresponding output position:
</p><p>receive an input at the particular output position, and</p>
<p>apply a sequence of transformations to the input at the particular output position to generate an output for the particular output position.</p>
</div>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00017" num="00017">
      <div><p>17. The system of </p><claim-ref idref="CLM-00016">claim 16</claim-ref><p>, wherein the sequence comprises two learned linear transformations separated by an activation function.</p></div>
    </div>
    </div> <div> <div id="CLM-00018" num="00018">
      <div><p>18. The system of </p><claim-ref idref="CLM-00016">claim 16</claim-ref><p>, wherein the at least one decoder subnetwork further comprises:
</p><p>a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate a residual output, and</p>
<p>a layer normalization layer that applies layer normalization to the residual output.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00019" num="00019">
      <div><p>19. The system of </p><claim-ref idref="CLM-00014">claim 14</claim-ref><p>, wherein each decoder subnetwork comprises:
</p><div><p>an encoder-decoder attention sub-layer that is configured to, at each generation time step:
</p><div><p>receive an input for each particular output position preceding the corresponding output position and, for each of the particular output positions:
</p><p>apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the particular output position to generate an updated representation for the particular output position.</p>
</div>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00020" num="00020">
      <div><p>20. The system of </p><claim-ref idref="CLM-00019">claim 19</claim-ref><p>, wherein each encoder-decoder attention sub-layer comprises a plurality of encoder-decoder attention layers, and wherein each encoder-decoder attention layer is configured to, at each generation time step:
</p><p>apply a learned query linear transformation to the input at each of the particular output positions preceding the corresponding output position to generate a respective query for each particular output position,</p>
<p>apply a learned key linear transformation to each encoded representation at each input position to generate a respective key for each input position,</p>
<p>apply a learned value linear transformation to each encoded representation at each input position to generate a respective value for each input position, and</p>
<div><p>for each particular output position preceding the corresponding output position,
</p><p>determine a respective output-position specific weight for each of the input positions by applying a comparison function between the query for the particular output position and the keys, and</p>
<p>determine an initial encoder-decoder attention output for the particular output position by determining a weighted sum of the values weighted by the corresponding output-position specific weights for the input position.</p>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00021" num="00021">
      <div><p>21. The system of </p><claim-ref idref="CLM-00020">claim 20</claim-ref><p>, wherein the encoder-decoder attention sub-layer is configured to, at each generation time step, combine the encoder-decoder attention outputs generated by the encoder-decoder attention layers to generate the output for the encoder-decoder attention sub-layer.</p></div>
    </div>
    </div> <div> <div id="CLM-00022" num="00022">
      <div><p>22. The system of </p><claim-ref idref="CLM-00020">claim 20</claim-ref><p>, wherein the encoder-decoder attention layers operate in parallel.</p></div>
    </div>
    </div> <div> <div id="CLM-00023" num="00023">
      <div><p>23. The system of </p><claim-ref idref="CLM-00019">claim 19</claim-ref><p> wherein each decoder subnetwork further comprises:
</p><p>a residual connection layer that combines the outputs of the encoder-decoder attention sub-layer with the inputs to the encoder-decoder attention sub-layer to generate a residual output, and</p>
<p>a layer normalization layer that applies layer normalization to the residual output.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00024" num="00024">
      <div><p>24. The system of </p><claim-ref idref="CLM-00014">claim 14</claim-ref><p>, wherein each decoder subnetwork comprises:
</p><div><p>a decoder self-attention sub-layer that is configured to, at each generation time step:
</p><div><p>receive an input for each particular output position preceding the corresponding output position and, for each particular output position:
</p><p>apply an attention mechanism over the inputs at the particular output positions preceding the corresponding output position using one or more queries derived from the input at the particular output position to generate a updated representation for the particular output position.</p>
</div>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00025" num="00025">
      <div><p>25. The system of </p><claim-ref idref="CLM-00024">claim 24</claim-ref><p>, wherein each decoder self-attention sub-layer comprises a plurality of decoder self-attention layers, and wherein each decoder self-attention layer is configured to, at each generation time step:
</p><p>apply a learned query linear transformation to the input at each particular output position preceding the corresponding output position to generate a respective query for each particular output position,</p>
<p>apply a learned key linear transformation to each input at each particular output position preceding the corresponding output position to generate a respective key for each particular output position,</p>
<p>apply a learned value linear transformation to each input at each particular output position preceding the corresponding output position to generate a respective key for each particular output position, and</p>
<div><p>for each of the particular output positions preceding the corresponding output position,
</p><p>determine a respective output-position specific weight for each particular output position by applying a comparison function between the query for the particular output position and the keys, and</p>
<p>determine an initial decoder attention output for the particular output position by determining a weighted sum of the values weighted by the corresponding output-position specific weights for the particular output position.</p>
</div>
</div>
    </div>
    </div> <div> <div id="CLM-00026" num="00026">
      <div><p>26. The system of </p><claim-ref idref="CLM-00025">claim 25</claim-ref><p>, wherein the decoder self-attention sub-layer is configured to, at each generation time step, combine the decoder attention outputs generated by the decoder self-attention layers to generate the output for the decoder self-attention sub-layer.</p></div>
    </div>
    </div> <div> <div id="CLM-00027" num="00027">
      <div><p>27. The system of </p><claim-ref idref="CLM-00025">claim 25</claim-ref><p>, wherein the decoder attention layers operate in parallel.</p></div>
    </div>
    </div> <div> <div id="CLM-00028" num="00028">
      <div><p>28. The system of </p><claim-ref idref="CLM-00024">claim 24</claim-ref><p> wherein each decoder subnetwork further comprises:
</p><p>a residual connection layer that combines the outputs of the decoder self-attention sub-layer with the inputs to the decoder self-attention sub-layer to generate a residual output, and</p>
<p>a layer normalization layer that applies layer normalization to the residual output.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00029" num="00029">
      <div><p>29. One or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to implement a sequence transduction neural network for transducing an input sequence having a respective network input at each of a plurality of input positions in an input order into an output sequence having a respective network output at each of a plurality of output positions in an output order, the sequence transduction neural network comprising:
</p><div><p>an encoder neural network configured to receive the input sequence and generate a respective encoded representation of each of the network inputs in the input sequence, the encoder neural network comprising a sequence of one or more encoder subnetworks, each encoder subnetwork configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions, and each encoder subnetwork comprising:
</p><div><p>an encoder self-attention sub-layer that is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order:
</p><p>apply a self-attention mechanism over the encoder subnetwork inputs at the plurality of input positions to generate a respective output for the particular input position, wherein applying a self-attention mechanism comprises: determining a query from the subnetwork input at the particular input position, determining keys derived from the subnetwork inputs at the plurality of input positions, determining values derived from the subnetwork inputs at the plurality of input positions, and using the determined query, keys, and values to generate the respective output for the particular input position; and</p>
</div>
</div>
<p>a decoder neural network configured to receive the encoded representations and generate the output sequence.</p>
</div>
    </div>
    </div> <div> <div id="CLM-00030" num="00030">
      <div><p>30. A method comprising:
</p><p>receiving an input sequence having a respective input at each of a plurality of input positions in an input order;</p>
<div><p>processing the input sequence through an encoder neural network to generate a respective encoded representation of each of the inputs in the input sequence, the encoder neural network comprising a sequence of one or more encoder subnetworks, each encoder subnetwork configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions, and each encoder subnetwork comprising:
</p><div><p>an encoder self-attention sub-layer that is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order:
</p><p>apply a self-attention mechanism over the encoder subnetwork inputs at the plurality of input positions to generate a respective output for the particular input position, wherein applying a self-attention mechanism comprises: determining a query from the subnetwork input at the particular input position, determining keys derived from the subnetwork inputs at the plurality of input positions, determining values derived from the subnetwork inputs at the plurality of input positions, and using the determined query, keys, and values to generate the respective output for the particular input position; and</p>
</div>
</div>
<p>processing the encoded representations through a decoder neural network to generate an output sequence having a respective output at each of a plurality of output positions in an output order.</p>
</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope="">

    <section itemprop="metadata" itemscope="">
        <span itemprop="applicationNumber">US16/021,971</span>
        <span itemprop="priorityDate">2017-05-23</span>
        <span itemprop="filingDate">2018-06-28</span>
        <span itemprop="title">Attention-based sequence transduction neural networks 
       </span>
        <span itemprop="ifiStatus">Active</span>
        
        <a href="https://patents.google.com/patent/US10452978B2/en">
            <span itemprop="representativePublication">US10452978B2</span>
            (<span itemprop="primaryLanguage">en</span>)
        </a>
    </section>

    <h2>Priority Applications (7)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/021,971</span>
                   
                   <a href="https://patents.google.com/patent/US10452978B2/en">
                        <span itemprop="representativePublication">US10452978B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2018-06-28</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/559,392</span>
                   
                   <a href="https://patents.google.com/patent/US10719764B2/en">
                        <span itemprop="representativePublication">US10719764B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2019-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/932,422</span>
                   
                   <a href="https://patents.google.com/patent/US11113602B2/en">
                        <span itemprop="representativePublication">US11113602B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-07-17</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/988,535</span>
                   
                   <a href="https://patents.google.com/patent/US20210019624A1/en">
                        <span itemprop="representativePublication">US20210019624A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/988,547</span>
                   
                   <a href="https://patents.google.com/patent/US20200372358A1/en">
                        <span itemprop="representativePublication">US20200372358A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/988,518</span>
                   
                   <a href="https://patents.google.com/patent/US10956819B2/en">
                        <span itemprop="representativePublication">US10956819B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="priorityApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US17/467,096</span>
                   
                   <a href="https://patents.google.com/patent/US20220051099A1/en">
                        <span itemprop="representativePublication">US20220051099A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2021-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr>
           </tbody>
       </table>

    <h2>Applications Claiming Priority (4)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="appsClaimingPriority" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US201762510256P</span>
                   
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2017-05-23</td>
                <td itemprop="title"></td>
              </tr><tr itemprop="appsClaimingPriority" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US201762541594P</span>
                   
                </td><td itemprop="priorityDate">2017-08-04</td>
                <td itemprop="filingDate">2017-08-04</td>
                <td itemprop="title"></td>
              </tr><tr itemprop="appsClaimingPriority" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">PCT/US2018/034224</span>
                   <a href="https://patents.google.com/patent/WO2018217948A1/en">
                        <span itemprop="representativePublication">WO2018217948A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2018-05-23</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="appsClaimingPriority" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/021,971</span>
                   <a href="https://patents.google.com/patent/US10452978B2/en">
                        <span itemprop="representativePublication">US10452978B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2018-06-28</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr>
           </tbody>
       </table>

    <h2>Related Parent Applications (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Title</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="parentApps" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">PCT/US2018/034224</span>
                    <span itemprop="relationType">Continuation</span>
                    <a href="https://patents.google.com/patent/WO2018217948A1/en">
                        <span itemprop="representativePublication">WO2018217948A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>

                </td><td itemprop="">
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2018-05-23</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr>
           </tbody>
        </table>

    <h2>Related Child Applications (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Title</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="childApps" itemscope="" repeat="">
                <td>
                   <span itemprop="applicationNumber">US16/559,392</span>
                    <span itemprop="relationType">Continuation</span>
                    <a href="https://patents.google.com/patent/US10719764B2/en">
                        <span itemprop="representativePublication">US10719764B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td><td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2019-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr>
           </tbody>
        </table>

    <h2>Publications (2)</h2>
        <table>
            <thead>
                <tr>
                    <th>Publication Number</th>
                    <th>Publication Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="pubs" itemscope="" repeat="">
                <td>
                   <span itemprop="publicationNumber">US20180341860A1</span>
                   
                   <a href="https://patents.google.com/patent/US20180341860A1/en">US20180341860A1
                       (<span itemprop="primaryLanguage">en</span>)
                   </a>
                </td>
                <td itemprop="publicationDate">2018-11-29</td>
              </tr><tr itemprop="pubs" itemscope="" repeat="">
                <td>
                   <span itemprop="publicationNumber">US10452978B2</span>
                   
                   <span itemprop="thisPatent">true</span>
                   <a href="https://patents.google.com/patent/US10452978B2/en">US10452978B2
                       (<span itemprop="primaryLanguage">en</span>)
                   </a>
                </td>
                <td itemprop="publicationDate">2019-10-22</td>
              </tr>
           </tbody>
        </table>

  </section>

  <section itemprop="family" itemscope="">
    
    <h2>ID=62873574</h2>

    <h2>Family Applications (7)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Title</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/021,971</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US10452978B2/en">
                        <span itemprop="representativePublication">US10452978B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2018-06-28</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/559,392</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US10719764B2/en">
                        <span itemprop="representativePublication">US10719764B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2019-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/932,422</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US11113602B2/en">
                        <span itemprop="representativePublication">US11113602B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-07-17</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,518</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US10956819B2/en">
                        <span itemprop="representativePublication">US10956819B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,547</span>
                    <span itemprop="ifiStatus">Pending</span>
                    
                    <a href="https://patents.google.com/patent/US20200372358A1/en">
                        <span itemprop="representativePublication">US20200372358A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,535</span>
                    <span itemprop="ifiStatus">Abandoned</span>
                    
                    <a href="https://patents.google.com/patent/US20210019624A1/en">
                        <span itemprop="representativePublication">US20210019624A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="applications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US17/467,096</span>
                    <span itemprop="ifiStatus">Pending</span>
                    
                    <a href="https://patents.google.com/patent/US20220051099A1/en">
                        <span itemprop="representativePublication">US20220051099A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2021-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr>
           </tbody>
        </table>

    

    <h2>Family Applications After (6)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Title</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/559,392</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US10719764B2/en">
                        <span itemprop="representativePublication">US10719764B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2019-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/932,422</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US11113602B2/en">
                        <span itemprop="representativePublication">US11113602B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-07-17</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,518</span>
                    <span itemprop="ifiStatus">Active</span>
                    
                    <a href="https://patents.google.com/patent/US10956819B2/en">
                        <span itemprop="representativePublication">US10956819B2</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
              </tr><tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,547</span>
                    <span itemprop="ifiStatus">Pending</span>
                    
                    <a href="https://patents.google.com/patent/US20200372358A1/en">
                        <span itemprop="representativePublication">US20200372358A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US16/988,535</span>
                    <span itemprop="ifiStatus">Abandoned</span>
                    
                    <a href="https://patents.google.com/patent/US20210019624A1/en">
                        <span itemprop="representativePublication">US20210019624A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2020-08-07</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr><tr itemprop="afterApplications" itemscope="" repeat="">
                <td>
                    <span itemprop="applicationNumber">US17/467,096</span>
                    <span itemprop="ifiStatus">Pending</span>
                    
                    <a href="https://patents.google.com/patent/US20220051099A1/en">
                        <span itemprop="representativePublication">US20220051099A1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2017-05-23</td>
                <td itemprop="filingDate">2021-09-03</td>
                <td itemprop="title">Attention-based sequence transduction neural networks 
     </td>
              </tr>
           </tbody>
        </table>

    <h2>Country Status (12)</h2>
      <table>
        <thead>
          <tr>
            <th>Country</th>
            <th>Link</th>
          </tr>
        </thead>
        <tbody>
        <tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">US</span>
                (<span itemprop="num">7</span>)
              <meta itemprop="thisCountry" content="true"/>
            </td>
            <td>
              <a href="https://patents.google.com/patent/US10452978B2/en">
                <span itemprop="representativePublication">US10452978B2</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">EP</span>
                (<span itemprop="num">7</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/EP4156033A1/en">
                <span itemprop="representativePublication">EP4156033A1</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">JP</span>
                (<span itemprop="num">4</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/JP6884871B2/en">
                <span itemprop="representativePublication">JP6884871B2</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">KR</span>
                (<span itemprop="num">4</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/KR102486348B1/en">
                <span itemprop="representativePublication">KR102486348B1</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">CN</span>
                (<span itemprop="num">1</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/CN110192206A/en">
                <span itemprop="representativePublication">CN110192206A</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">AU</span>
                (<span itemprop="num">4</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/AU2018271931B2/en">
                <span itemprop="representativePublication">AU2018271931B2</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">BR</span>
                (<span itemprop="num">1</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/BR112019014822B1/en">
                <span itemprop="representativePublication">BR112019014822B1</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">CA</span>
                (<span itemprop="num">3</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/CA3050334C/en">
                <span itemprop="representativePublication">CA3050334C</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">ES</span>
                (<span itemprop="num">1</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/ES2934313T3/en">
                <span itemprop="representativePublication">ES2934313T3</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">PL</span>
                (<span itemprop="num">1</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/PL3542316T3/en">
                <span itemprop="representativePublication">PL3542316T3</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">RU</span>
                (<span itemprop="num">2</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/RU2021116658A/en">
                <span itemprop="representativePublication">RU2021116658A</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr><tr itemprop="countryStatus" itemscope="" repeat="">
            <td>
              <span itemprop="countryCode">WO</span>
                (<span itemprop="num">1</span>)
              
            </td>
            <td>
              <a href="https://patents.google.com/patent/WO2018217948A1/en">
                <span itemprop="representativePublication">WO2018217948A1</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr>
      </tbody>
    </table>

    <h2>Cited By (7)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20210059540A/en">
              <span itemprop="publicationNumber">KR20210059540A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-11-15</td>
          <td itemprop="publicationDate">2021-05-25</td>
          <td><span itemprop="assigneeOriginal">고려대학교 산학협력단</span></td>
          <td itemprop="title">Device and method for correcting Korean spelling 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/WO2021145862A1/en">
              <span itemprop="publicationNumber">WO2021145862A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2020-01-14</td>
          <td itemprop="publicationDate">2021-07-22</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Method and system for activity prediction, prefetching and preloading of computer assets by a client-device 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11205121B2/en">
              <span itemprop="publicationNumber">US11205121B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-06-20</td>
          <td itemprop="publicationDate">2021-12-21</td>
          <td><span itemprop="assigneeOriginal">Disney Enterprises, Inc.</span></td>
          <td itemprop="title">Efficient encoding and decoding sequences using variational autoencoders 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102388599B1/en">
              <span itemprop="publicationNumber">KR102388599B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-08-24</td>
          <td itemprop="publicationDate">2022-04-21</td>
          <td><span itemprop="assigneeOriginal">(주)제로엑스플로우</span></td>
          <td itemprop="title">Apparatus and method for correcting sentence using test and image 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11392984B2/en">
              <span itemprop="publicationNumber">US11392984B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-11-20</td>
          <td itemprop="publicationDate">2022-07-19</td>
          <td><span itemprop="assigneeOriginal">Walmart Apollo, Llc</span></td>
          <td itemprop="title">Methods and apparatus for automatically providing item advertisement recommendations 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11455656B2/en">
              <span itemprop="publicationNumber">US11455656B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-11-18</td>
          <td itemprop="publicationDate">2022-09-27</td>
          <td><span itemprop="assigneeOriginal">Walmart Apollo, Llc</span></td>
          <td itemprop="title">Methods and apparatus for electronically providing item advertisement recommendations 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11581000B2/en">
              <span itemprop="publicationNumber">US11581000B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-11-29</td>
          <td itemprop="publicationDate">2023-02-14</td>
          <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
          <td itemprop="title">Apparatus and method for encoding/decoding audio signal using information of previous frame 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Families Citing this family (45)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2018271931B2/en">
              <span itemprop="publicationNumber">AU2018271931B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2017-05-23</td>
          <td itemprop="publicationDate">2020-05-07</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11295739B2/en">
              <span itemprop="publicationNumber">US11295739B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-08-23</td>
          <td itemprop="publicationDate">2022-04-05</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Key phrase spotting 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20200067632A/en">
              <span itemprop="publicationNumber">KR20200067632A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-12-04</td>
          <td itemprop="publicationDate">2020-06-12</td>
          <td><span itemprop="assigneeOriginal">삼성전자주식회사</span></td>
          <td itemprop="title">Method and apparatus for allocating memory space for driving a neural network 
     </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN109558605B/en">
              <span itemprop="publicationNumber">CN109558605B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-12-17</td>
          <td itemprop="publicationDate">2022-06-10</td>
          <td><span itemprop="assigneeOriginal">北京百度网讯科技有限公司</span></td>
          <td itemprop="title">Method and device for translating sentences 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20200075615A/en">
              <span itemprop="publicationNumber">KR20200075615A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-12-18</td>
          <td itemprop="publicationDate">2020-06-26</td>
          <td><span itemprop="assigneeOriginal">삼성전자주식회사</span></td>
          <td itemprop="title">Method and apparatus for machine translation 
     </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN109740169B/en">
              <span itemprop="publicationNumber">CN109740169B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-09</td>
          <td itemprop="publicationDate">2020-10-13</td>
          <td><span itemprop="assigneeOriginal">北京邮电大学</span></td>
          <td itemprop="title">Traditional Chinese medicine ancient book translation method based on dictionary and seq2seq pre-training mechanism 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN109919358B/en">
              <span itemprop="publicationNumber">CN109919358B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-31</td>
          <td itemprop="publicationDate">2021-03-02</td>
          <td><span itemprop="assigneeOriginal">中国科学院软件研究所</span></td>
          <td itemprop="title">Real-time station flow prediction method based on neural network space-time attention mechanism 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP3690752A1/en">
              <span itemprop="publicationNumber">EP3690752A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-01-31</td>
          <td itemprop="publicationDate">2020-08-05</td>
          <td><span itemprop="assigneeOriginal">Avatar Cognition Barcelona, SL</span></td>
          <td itemprop="title">Fractal cognitive computing node and computer-implemented method for learning procedures 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102254300B1/en">
              <span itemprop="publicationNumber">KR102254300B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-04-19</td>
          <td itemprop="publicationDate">2021-05-21</td>
          <td><span itemprop="assigneeOriginal">한국과학기술원</span></td>
          <td itemprop="title">Suggestion of evidence sentence for utterance in debate situation 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN110083770B/en">
              <span itemprop="publicationNumber">CN110083770B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-04-29</td>
          <td itemprop="publicationDate">2023-01-13</td>
          <td><span itemprop="assigneeOriginal">苏州市职业大学</span></td>
          <td itemprop="title">Sequence recommendation method based on deeper feature level self-attention network 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN110321961A/en">
              <span itemprop="publicationNumber">CN110321961A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-07-09</td>
          <td itemprop="publicationDate">2019-10-11</td>
          <td><span itemprop="assigneeOriginal">北京金山数字娱乐科技有限公司</span></td>
          <td itemprop="title">A kind of data processing method and device 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP2021026130A/en">
              <span itemprop="publicationNumber">JP2021026130A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-08-06</td>
          <td itemprop="publicationDate">2021-02-22</td>
          <td><span itemprop="assigneeOriginal">本田技研工業株式会社</span></td>
          <td itemprop="title">Information processing device, information processing method, recognition model and program 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11600067B2/en">
              <span itemprop="publicationNumber">US11600067B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-09-12</td>
          <td itemprop="publicationDate">2023-03-07</td>
          <td><span itemprop="assigneeOriginal">Nec Corporation</span></td>
          <td itemprop="title">Action recognition with high-order interaction through spatial-temporal object tracking 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20210043995A/en">
              <span itemprop="publicationNumber">KR20210043995A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-10-14</td>
          <td itemprop="publicationDate">2021-04-22</td>
          <td><span itemprop="assigneeOriginal">삼성전자주식회사</span></td>
          <td itemprop="title">Model training method and apparatus, and sequence recognition method 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20210044056A/en">
              <span itemprop="publicationNumber">KR20210044056A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-10-14</td>
          <td itemprop="publicationDate">2021-04-22</td>
          <td><span itemprop="assigneeOriginal">삼성전자주식회사</span></td>
          <td itemprop="title">Natural language processing method and appratus using repetetion token embedding 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN112751686B/en">
              <span itemprop="publicationNumber">CN112751686B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-10-29</td>
          <td itemprop="publicationDate">2022-10-18</td>
          <td><span itemprop="assigneeOriginal">中国移动通信集团浙江有限公司</span></td>
          <td itemprop="title">Office data script generation method and device, computing equipment and computer storage medium 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11246173B2/en">
              <span itemprop="publicationNumber">US11246173B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-11-08</td>
          <td itemprop="publicationDate">2022-02-08</td>
          <td><span itemprop="assigneeOriginal">Huawei Technologies Co. Ltd.</span></td>
          <td itemprop="title">Systems and methods for multi-user pairing in wireless communication networks 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20210150349A1/en">
              <span itemprop="publicationNumber">US20210150349A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-11-15</td>
          <td itemprop="publicationDate">2021-05-20</td>
          <td><span itemprop="assigneeOriginal">Waymo Llc</span></td>
          <td itemprop="title">Multi object tracking using memory attention 
     </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102439165B1/en">
              <span itemprop="publicationNumber">KR102439165B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-11-26</td>
          <td itemprop="publicationDate">2022-09-01</td>
          <td><span itemprop="assigneeOriginal">한국과학기술원</span></td>
          <td itemprop="title">Statements credibility assessing system using commonsense knowledge and linguistic patterns and the method thereof 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20210071471A/en">
              <span itemprop="publicationNumber">KR20210071471A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-12-06</td>
          <td itemprop="publicationDate">2021-06-16</td>
          <td><span itemprop="assigneeOriginal">삼성전자주식회사</span></td>
          <td itemprop="title">Apparatus and method for performing matrix multiplication operation of neural network 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111222343A/en">
              <span itemprop="publicationNumber">CN111222343A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-12-06</td>
          <td itemprop="publicationDate">2020-06-02</td>
          <td><span itemprop="assigneeOriginal">深圳市优必选科技股份有限公司</span></td>
          <td itemprop="title">Intention identification method and intention identification device 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102436900B1/en">
              <span itemprop="publicationNumber">KR102436900B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-12-12</td>
          <td itemprop="publicationDate">2022-08-26</td>
          <td><span itemprop="assigneeOriginal">서울대학교산학협력단</span></td>
          <td itemprop="title">Apparatus and method for evaluating sentense by using bidirectional language model 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111078825A/en">
              <span itemprop="publicationNumber">CN111078825A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-12-20</td>
          <td itemprop="publicationDate">2020-04-28</td>
          <td><span itemprop="assigneeOriginal">北京百度网讯科技有限公司</span></td>
          <td itemprop="title">Structured processing method, structured processing device, computer equipment and medium 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111079450B/en">
              <span itemprop="publicationNumber">CN111079450B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-12-20</td>
          <td itemprop="publicationDate">2021-01-22</td>
          <td><span itemprop="assigneeOriginal">北京百度网讯科技有限公司</span></td>
          <td itemprop="title">Language conversion method and device based on sentence-by-sentence driving 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111222562B/en">
              <span itemprop="publicationNumber">CN111222562B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-01-02</td>
          <td itemprop="publicationDate">2022-04-08</td>
          <td><span itemprop="assigneeOriginal">南京邮电大学</span></td>
          <td itemprop="title">Target detection method based on space self-attention mechanism 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11386885B2/en">
              <span itemprop="publicationNumber">US11386885B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2020-02-17</td>
          <td itemprop="publicationDate">2022-07-12</td>
          <td><span itemprop="assigneeOriginal">Wipro Limited</span></td>
          <td itemprop="title">Method and system for detecting intent as an ordered sequence from a user query 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10909461B1/en">
              <span itemprop="publicationNumber">US10909461B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-08</td>
          <td itemprop="publicationDate">2021-02-02</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Attention neural networks with locality-sensitive hashing 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN113627135A/en">
              <span itemprop="publicationNumber">CN113627135A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-08</td>
          <td itemprop="publicationDate">2021-11-09</td>
          <td><span itemprop="assigneeOriginal">百度在线网络技术（北京）有限公司</span></td>
          <td itemprop="title">Method, device, equipment and medium for generating recruitment post description text 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20210346807A1/en">
              <span itemprop="publicationNumber">US20210346807A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-11</td>
          <td itemprop="publicationDate">2021-11-11</td>
          <td><span itemprop="assigneeOriginal">Rovi Guides, Inc.</span></td>
          <td itemprop="title">Gaming content recommendation for a video game 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111460126B/en">
              <span itemprop="publicationNumber">CN111460126B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-06-12</td>
          <td itemprop="publicationDate">2020-09-25</td>
          <td><span itemprop="assigneeOriginal">支付宝(杭州)信息技术有限公司</span></td>
          <td itemprop="title">Reply generation method and device for man-machine conversation system and electronic equipment 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN111652357B/en">
              <span itemprop="publicationNumber">CN111652357B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-08-10</td>
          <td itemprop="publicationDate">2021-01-15</td>
          <td><span itemprop="assigneeOriginal">浙江大学</span></td>
          <td itemprop="title">Method and system for solving video question-answer problem by using specific target network based on graph 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20220051078A1/en">
              <span itemprop="publicationNumber">US20220051078A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-08-14</td>
          <td itemprop="publicationDate">2022-02-17</td>
          <td><span itemprop="assigneeOriginal">Micron Technology, Inc.</span></td>
          <td itemprop="title">Transformer neural network in memory 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20220058489A1/en">
              <span itemprop="publicationNumber">US20220058489A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-08-19</td>
          <td itemprop="publicationDate">2022-02-24</td>
          <td><span itemprop="assigneeOriginal">The Toronto-Dominion Bank</span></td>
          <td itemprop="title">Two-headed attention fused autoencoder for context-aware recommendation 
     </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN112580822A/en">
              <span itemprop="publicationNumber">CN112580822A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-12-16</td>
          <td itemprop="publicationDate">2021-03-30</td>
          <td><span itemprop="assigneeOriginal">北京百度网讯科技有限公司</span></td>
          <td itemprop="title">Countermeasure training method and apparatus for machine learning model, electronic device, and medium 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN112508625B/en">
              <span itemprop="publicationNumber">CN112508625B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-12-18</td>
          <td itemprop="publicationDate">2022-10-21</td>
          <td><span itemprop="assigneeOriginal">国网河南省电力公司经济技术研究院</span></td>
          <td itemprop="title">Intelligent inspection modeling method based on multi-branch residual attention network 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20220261711A1/en">
              <span itemprop="publicationNumber">US20220261711A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-02-12</td>
          <td itemprop="publicationDate">2022-08-18</td>
          <td><span itemprop="assigneeOriginal">Accenture Global Solutions Limited</span></td>
          <td itemprop="title">System and method for intelligent contract guidance 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN113095431A/en">
              <span itemprop="publicationNumber">CN113095431A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-04-27</td>
          <td itemprop="publicationDate">2021-07-09</td>
          <td><span itemprop="assigneeOriginal">中山大学</span></td>
          <td itemprop="title">Image description method, system and device based on attention mechanism 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN113393025A/en">
              <span itemprop="publicationNumber">CN113393025A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-06-07</td>
          <td itemprop="publicationDate">2021-09-14</td>
          <td><span itemprop="assigneeOriginal">浙江大学</span></td>
          <td itemprop="title">Non-invasive load decomposition method based on Informer model coding structure 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN113705323B/en">
              <span itemprop="publicationNumber">CN113705323B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-06-15</td>
          <td itemprop="publicationDate">2022-09-09</td>
          <td><span itemprop="assigneeOriginal">腾讯医疗健康(深圳)有限公司</span></td>
          <td itemprop="title">Image recognition method, device, equipment and storage medium 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20230022005A/en">
              <span itemprop="publicationNumber">KR20230022005A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-08-06</td>
          <td itemprop="publicationDate">2023-02-14</td>
          <td><span itemprop="assigneeOriginal">주식회사 제이엘케이바이오</span></td>
          <td itemprop="title">Device and method for extracting compound information 
     </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN113688640B/en">
              <span itemprop="publicationNumber">CN113688640B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-08-30</td>
          <td itemprop="publicationDate">2023-01-20</td>
          <td><span itemprop="assigneeOriginal">深译信息科技(珠海)有限公司</span></td>
          <td itemprop="title">Neural network machine translation method for medical document translation 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102479817B1/en">
              <span itemprop="publicationNumber">KR102479817B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-11-25</td>
          <td itemprop="publicationDate">2022-12-21</td>
          <td><span itemprop="assigneeOriginal">인하대학교 산학협력단</span></td>
          <td itemprop="title">Vision Transformer Apparatus for Small Dataset and Method of Operation 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102405828B1/en">
              <span itemprop="publicationNumber">KR102405828B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-12-03</td>
          <td itemprop="publicationDate">2022-06-07</td>
          <td><span itemprop="assigneeOriginal">주식회사 대교씨엔에스</span></td>
          <td itemprop="title">Method and apparatus for recommending learning amount using clustering and artificial intelligence using gaussian mixed model at the same time 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102405832B1/en">
              <span itemprop="publicationNumber">KR102405832B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-12-03</td>
          <td itemprop="publicationDate">2022-06-07</td>
          <td><span itemprop="assigneeOriginal">주식회사 대교씨엔에스</span></td>
          <td itemprop="title">Method and device for recommending learning volume using statistical analysis and deep learning-based artificial intelligence at the same time 
       </td>
        </tr><tr itemprop="forwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102475316B1/en">
              <span itemprop="publicationNumber">KR102475316B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-12-03</td>
          <td itemprop="publicationDate">2022-12-08</td>
          <td><span itemprop="assigneeOriginal">(주)대교씨엔에스</span></td>
          <td itemprop="title">Learning amount recommendation method and apparatus using deep learning-based artificial intelligence composed of a plurality of hidden layers 
       </td>
        </tr>
      </tbody>
    </table>

    

    <h2>Family Cites Families (13)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/WO2001071624A1/en">
              <span itemprop="publicationNumber">WO2001071624A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2000-03-22</td>
          <td itemprop="publicationDate">2001-09-27</td>
          <td><span itemprop="assigneeOriginal">3-Dimensional Pharmaceuticals, Inc.</span></td>
          <td itemprop="title">System, method, and computer program product for representing object relationships in a multidimensional space 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US9123343B2/en">
              <span itemprop="publicationNumber">US9123343B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2006-04-27</td>
          <td itemprop="publicationDate">2015-09-01</td>
          <td><span itemprop="assigneeOriginal">Mobiter Dicta Oy</span></td>
          <td itemprop="title">Method, and a device for converting speech by replacing inarticulate portions of the speech before the conversion 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US9536528B2/en">
              <span itemprop="publicationNumber">US9536528B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2012-07-03</td>
          <td itemprop="publicationDate">2017-01-03</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Determining hotword suitability 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10181098B2/en">
              <span itemprop="publicationNumber">US10181098B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2014-06-06</td>
          <td itemprop="publicationDate">2019-01-15</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Generating representations of input sequences using neural networks 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US9858524B2/en">
              <span itemprop="publicationNumber">US9858524B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2014-11-14</td>
          <td itemprop="publicationDate">2018-01-02</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Generating natural language descriptions of images 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11080587B2/en">
              <span itemprop="publicationNumber">US11080587B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-02-06</td>
          <td itemprop="publicationDate">2021-08-03</td>
          <td><span itemprop="assigneeOriginal">Deepmind Technologies Limited</span></td>
          <td itemprop="title">Recurrent neural networks for data item generation 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10083157B2/en">
              <span itemprop="publicationNumber">US10083157B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-08-07</td>
          <td itemprop="publicationDate">2018-09-25</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Text classification and transformation based on author 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US9965705B2/en">
              <span itemprop="publicationNumber">US9965705B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-11-03</td>
          <td itemprop="publicationDate">2018-05-08</td>
          <td><span itemprop="assigneeOriginal">Baidu Usa Llc</span></td>
          <td itemprop="title">Systems and methods for attention-based configurable convolutional neural networks (ABC-CNN) for visual question answering 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10204299B2/en">
              <span itemprop="publicationNumber">US10204299B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-11-04</td>
          <td itemprop="publicationDate">2019-02-12</td>
          <td><span itemprop="assigneeOriginal">Nec Corporation</span></td>
          <td itemprop="title">Unsupervised matching in fine-grained datasets for single-view object reconstruction 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/WO2017083761A1/en">
              <span itemprop="publicationNumber">WO2017083761A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-11-12</td>
          <td itemprop="publicationDate">2017-05-18</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Neural programming 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP3459016A1/en">
              <span itemprop="publicationNumber">EP3459016A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-05-20</td>
          <td itemprop="publicationDate">2019-03-27</td>
          <td><span itemprop="assigneeOriginal">Deepmind Technologies Limited</span></td>
          <td itemprop="title">Classifying input examples using a comparison set 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN106372577A/en">
              <span itemprop="publicationNumber">CN106372577A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-08-23</td>
          <td itemprop="publicationDate">2017-02-01</td>
          <td><span itemprop="assigneeOriginal">北京航空航天大学</span></td>
          <td itemprop="title">Deep learning-based traffic sign automatic identifying and marking method 
       </td>
        </tr><tr itemprop="backwardReferencesFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2018271931B2/en">
              <span itemprop="publicationNumber">AU2018271931B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2017-05-23</td>
          <td itemprop="publicationDate">2020-05-07</td>
          <td><span itemprop="assigneeOriginal">Google Llc</span></td>
          <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
        </tr>
      </tbody>
    </table>

    
    <ul>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2018</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">AU</span>
            <span itemprop="applicationNumber">AU2018271931A</span>
            <a href="https://patents.google.com/patent/AU2018271931B2/en"><span itemprop="documentId">patent/AU2018271931B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204654.2A</span>
            <a href="https://patents.google.com/patent/EP4156033A1/en"><span itemprop="documentId">patent/EP4156033A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204664.1A</span>
            <a href="https://patents.google.com/patent/EP4156035A1/en"><span itemprop="documentId">patent/EP4156035A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">PL</span>
            <span itemprop="applicationNumber">PL18739661.9T</span>
            <a href="https://patents.google.com/patent/PL3542316T3/en"><span itemprop="documentId">patent/PL3542316T3/en</span></a>
            <span itemprop="legalStatusCat">unknown</span>
            <span itemprop="legalStatus"></span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">CN</span>
            <span itemprop="applicationNumber">CN201880007309.XA</span>
            <a href="https://patents.google.com/patent/CN110192206A/en"><span itemprop="documentId">patent/CN110192206A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2019538514A</span>
            <a href="https://patents.google.com/patent/JP6884871B2/en"><span itemprop="documentId">patent/JP6884871B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204647.6A</span>
            <a href="https://patents.google.com/patent/EP4156032A1/en"><span itemprop="documentId">patent/EP4156032A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204659.1A</span>
            <a href="https://patents.google.com/patent/EP4156034A1/en"><span itemprop="documentId">patent/EP4156034A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020207032482A</span>
            <a href="https://patents.google.com/patent/KR102486348B1/en"><span itemprop="documentId">patent/KR102486348B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">IP Right Grant</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">ES</span>
            <span itemprop="applicationNumber">ES18739661T</span>
            <a href="https://patents.google.com/patent/ES2934313T3/en"><span itemprop="documentId">patent/ES2934313T3/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">CA</span>
            <span itemprop="applicationNumber">CA3050334A</span>
            <a href="https://patents.google.com/patent/CA3050334C/en"><span itemprop="documentId">patent/CA3050334C/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020227033030A</span>
            <a href="https://patents.google.com/patent/KR20220133323A/en"><span itemprop="documentId">patent/KR20220133323A/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Application Discontinuation</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">BR</span>
            <span itemprop="applicationNumber">BR112019014822-1A</span>
            <a href="https://patents.google.com/patent/BR112019014822B1/en"><span itemprop="documentId">patent/BR112019014822B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">IP Right Grant</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">CA</span>
            <span itemprop="applicationNumber">CA3144657A</span>
            <a href="https://patents.google.com/patent/CA3144657A1/en"><span itemprop="documentId">patent/CA3144657A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204636.9A</span>
            <a href="https://patents.google.com/patent/EP4156030A1/en"><span itemprop="documentId">patent/EP4156030A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">RU</span>
            <span itemprop="applicationNumber">RU2021116658A</span>
            <a href="https://patents.google.com/patent/RU2021116658A/en"><span itemprop="documentId">patent/RU2021116658A/en</span></a>
            <span itemprop="legalStatusCat">unknown</span>
            <span itemprop="legalStatus"></span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22204640.1A</span>
            <a href="https://patents.google.com/patent/EP4156031A1/en"><span itemprop="documentId">patent/EP4156031A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">WO</span>
            <span itemprop="applicationNumber">PCT/US2018/034224</span>
            <a href="https://patents.google.com/patent/WO2018217948A1/en"><span itemprop="documentId">patent/WO2018217948A1/en</span></a>
            <span itemprop="legalStatusCat">unknown</span>
            <span itemprop="legalStatus"></span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">RU</span>
            <span itemprop="applicationNumber">RU2019122632A</span>
            <a href="https://patents.google.com/patent/RU2749945C1/en"><span itemprop="documentId">patent/RU2749945C1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus"></span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020197019186A</span>
            <a href="https://patents.google.com/patent/KR102180002B1/en"><span itemprop="documentId">patent/KR102180002B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">IP Right Grant</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020207032481A</span>
            <a href="https://patents.google.com/patent/KR102448389B1/en"><span itemprop="documentId">patent/KR102448389B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">IP Right Grant</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">CA</span>
            <span itemprop="applicationNumber">CA3144674A</span>
            <a href="https://patents.google.com/patent/CA3144674A1/en"><span itemprop="documentId">patent/CA3144674A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-05-23</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP18739661.9A</span>
            <a href="https://patents.google.com/patent/EP3542316B1/en"><span itemprop="documentId">patent/EP3542316B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2018-06-28</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/021,971</span>
            <a href="https://patents.google.com/patent/US10452978B2/en"><span itemprop="documentId">patent/US10452978B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
            <span itemprop="thisApp" content="true" bool=""></span>
            
          </li>
          
        </ul>
      </li>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2019</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2019-09-03</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/559,392</span>
            <a href="https://patents.google.com/patent/US10719764B2/en"><span itemprop="documentId">patent/US10719764B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
        </ul>
      </li>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2020</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-07-17</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/932,422</span>
            <a href="https://patents.google.com/patent/US11113602B2/en"><span itemprop="documentId">patent/US11113602B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-08-05</span>
            <span itemprop="countryCode">AU</span>
            <span itemprop="applicationNumber">AU2020213318A</span>
            <a href="https://patents.google.com/patent/AU2020213318B2/en"><span itemprop="documentId">patent/AU2020213318B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-08-05</span>
            <span itemprop="countryCode">AU</span>
            <span itemprop="applicationNumber">AU2020213317A</span>
            <a href="https://patents.google.com/patent/AU2020213317B2/en"><span itemprop="documentId">patent/AU2020213317B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-08-07</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/988,518</span>
            <a href="https://patents.google.com/patent/US10956819B2/en"><span itemprop="documentId">patent/US10956819B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-08-07</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/988,547</span>
            <a href="https://patents.google.com/patent/US20200372358A1/en"><span itemprop="documentId">patent/US20200372358A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2020-08-07</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/988,535</span>
            <a href="https://patents.google.com/patent/US20210019624A1/en"><span itemprop="documentId">patent/US20210019624A1/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Abandoned</span>
            
          </li>
          
        </ul>
      </li>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2021</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2021-05-12</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2021080995A</span>
            <a href="https://patents.google.com/patent/JP7214783B2/en"><span itemprop="documentId">patent/JP7214783B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2021-05-12</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2021080996A</span>
            <a href="https://patents.google.com/patent/JP7214784B2/en"><span itemprop="documentId">patent/JP7214784B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2021-09-03</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US17/467,096</span>
            <a href="https://patents.google.com/patent/US20220051099A1/en"><span itemprop="documentId">patent/US20220051099A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
        </ul>
      </li>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2022</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2022-08-22</span>
            <span itemprop="countryCode">AU</span>
            <span itemprop="applicationNumber">AU2022221389A</span>
            <a href="https://patents.google.com/patent/AU2022221389A1/en"><span itemprop="documentId">patent/AU2022221389A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
        </ul>
      </li>
      
      <li itemprop="applicationsByYear" itemscope="" repeat="">
        <span itemprop="year">2023</span>
        <ul>
          
          <li itemprop="application" itemscope="" repeat="">
            <span itemprop="filingDate">2023-01-18</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2023006053A</span>
            <a href="https://patents.google.com/patent/JP2023052483A/en"><span itemprop="documentId">patent/JP2023052483A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          
        </ul>
      </li>
      
    </ul>
    

    </section>

  

  <section>
    <h2>Non-Patent Citations (35)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Ba et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Layer+Normalization%2C&#34;">Layer Normalization,</a>&#34; arXiv 1607.06450v1, Jul. 21, 2016, 14 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Ba et al., Layer Normalization, (2016) available from Internet https://arxiv.org/abs/1607.06450 at p. 1-14 (Year: 2016).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Bahdanau et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate%2C&#34;">Neural Machine Translation by Jointly Learning to Align and Translate,</a>&#34; arXiv 1409.0473v7, mailed on May 19, 2016, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Britz et al &#34;<a href="http://scholar.google.com/scholar?q=&#34;Massive+exploration+of+neural+machine+translation+architectures%2C&#34;">Massive exploration of neural machine translation architectures,</a>&#34; arXiv 1703.03906v2, Mar. 21, 2017, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Cheng et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Long+short-term+memory-networks+for+machine+reading%2C&#34;">Long short-term memory-networks for machine reading,</a>&#34; arXiv 1601.06733v7, Sep. 20, 2016, 11 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Cheng et al., Long Short-Term Memory-Networks for Machine Reading (2016), Conf. on Empirical Methods in Natural Language Processing, available from Internet &lt;https://arxiv.org/abs/1601.06733&gt; at p. 551-561 (Year: 2016).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Cho et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Learning+phrase+representations+using+rnn+encoder-decoder+for+statistical+machine+translation%2C&#34;">Learning phrase representations using rnn encoder-decoder for statistical machine translation,</a>&#34; arXiv 1406.1078v3, Sep. 3, 2014, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Chollet. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Xception%3A+Deep+Learning+with+depthwise+separable+convolution%2C&#34;">Xception: Deep Learning with depthwise separable convolution,</a>&#34; arXiv 1610.02357v3, Apr. 4, 2017, 8 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Chung et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Empirical+evaluation+of+gated+recurrent+neural+networks+on+sequence+modeling%2C&#34;">Empirical evaluation of gated recurrent neural networks on sequence modeling,</a>&#34; arXiv 1412.3555v1, Dec. 11, 2014, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Daniluk et al., Frustratingly Short Attention Spans in Neural Language Modeling, (Feb. 2017) ICLR 2017, available from Internet &lt; https://arxiv.org/abs/1702.04521&gt; at p. 1-10 (Year: 2017).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Gehring et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Convolutional+sequence+to+sequence+learning%2C&#34;">Convolutional sequence to sequence learning,</a>&#34; arXiv 1705.03122v2, May 12, 2017, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Hochreiter et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Gradient+flow+in+recurrent+nets%3A+the+difficulty+of+learning+long-term+dependencies%2C&#34;">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies,</a>&#34; A field Guide to Dynamical Recurrent Neural Networks, IEEE Press, 2001a, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Hochreiter et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Long+short+term+memory%2C&#34;">Long short term memory,</a>&#34; Neural Computation 9(8), Nov. 1997, 46 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Jozefowiz et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Exploring+the+limits+of+language+modeling%2C&#34;">Exploring the limits of language modeling,</a>&#34; arXiv 1602.02410, Feb. 7, 2016, 11 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kaiser et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Can+active+memory+replace+attention%3F&#34;">Can active memory replace attention?</a>&#34; Advances in Neural Information Processing Systems, Dec. 2016 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kaiser et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Neural+GPUs+learn+algorithms%2C&#34;">Neural GPUs learn algorithms,</a>&#34; International Conference on Learning Representations, arXiv 1511.08228v3, Mar. 15, 2016, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kalchbrenner et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Neural+machine+translation+in+linear+time%2C&#34;">Neural machine translation in linear time,</a>&#34; arXiv 1610.10099v2, Mar. 15, 2017, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kim et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Structured+attention+networks%2C&#34;">Structured attention networks,</a>&#34; arXiv 1702.00887v3, Feb. 16, 2017, 21 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kingma et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Adam%3A+A+method+for+stochastic+optimization%2C&#34;">Adam: A method for stochastic optimization,</a>&#34; arXiv 1412.6980v8_Jul. 23, 2015, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Kuchaiev et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Factorization+tricks+for+lstm+networks%2C&#34;">Factorization tricks for lstm networks,</a>&#34; arXiv 1703.10722v3, Feb. 24, 2018, 6 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Lim et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;A+structures+self-attentive+sentence+embedding%2C&#34;">A structures self-attentive sentence embedding,</a>&#34; arXiv 1703.03130v1, Mar. 9, 2017, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Lin et al., A Structured Self-Attentive Sentence Embedding (Mar. 2017) available from Internet &lt; https://arxiv.org/abs/1703.03130&gt;, ICLR 2017 at p. 1-15 (Year: 2017).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Luong et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Effective+approaches+to+attention+based+neural+machine+translation%2C&#34;">Effective approaches to attention based neural machine translation,</a>&#34; arXiv 1508.04025v2, Sep. 20, 2015, 11 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Luong et al., Effective Approaches to Attention-based Neural Machine Translation, (2015) Conf. on Empirical Methods in Natural Language Processing at p. 1412-1421 (Year: 2015).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Parikh et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;A+decomposable+attention+model+for+natural+language+inference%2C&#34;">A decomposable attention model for natural language inference,</a>&#34; Proceedings of the Empirical Methods in Natural Language Processing conference, Nov. 2016, 7 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Paulus et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;A+deep+reinforced+model+for+abstractive+summarization%2C&#34;">A deep reinforced model for abstractive summarization,</a>&#34; arXiv 1705.04304v3, Nov. 13, 2017, 12 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">PCT International Search Report and Written Opinion issued in International Application No. PCT/US2018/034224, dated Sep. 24, 2018, 14 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Sennrich et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Neural+Machine+Translation+of+rare+words+with+subword+units%2C&#34;">Neural Machine Translation of rare words with subword units,</a>&#34; arXiv 1508.07909v5, Jun. 10, 2016, 11 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Shazeer et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Outrageously+large+neural+networks%3A+The+sparsely-gated+mixture-of-experts+layer%2C&#34;">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,</a>&#34; arXiv 1701.06538v1, Jan. 23, 2017, 19 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Srivastava et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Dropout%3A+a+simple+way+to+prevent+neural+network+from+overfitting%2C&#34;">Dropout: a simple way to prevent neural network from overfitting,</a>&#34; Journal of Machine Learning Research, 15(1), Jan. 2014, 30 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Sukhbaatar et al., End-to-End Memory Networks, (2015) available from Internet &lt; https://arxiv.org/pdf/1503.08895.pdf&gt; at p. 1-11 (Year: 2015).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Sutskever et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Sequence+to+sequence+learning+with+neural+networks%2C&#34;">Sequence to sequence learning with neural networks,</a>&#34; Advances in Neural Information Processing Systems, Dec. 2014, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Szegedy et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Rethinking+the+inception+architecture+for+computer+vision%2C&#34;">Rethinking the inception architecture for computer vision,</a>&#34; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Jun. 2016, 9 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Vaswan et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Attention+is+All+You+Need%2C&#34;">Attention is All You Need,</a>&#34; 31st Conference on Neural Information Processing Systems, Jun. 12, 2017, arXIv1706.03762, 15 pages.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope="" repeat="">
          <td>
            <span itemprop="title">Wu et al. &#34;<a href="http://scholar.google.com/scholar?q=&#34;Google%27s+neural+machine+translation+system%3A+Bridging+the+gap+between+human+and+machine+translation%2C&#34;">Google&#39;s neural machine translation system: Bridging the gap between human and machine translation,</a>&#34; arXiv 1609.08144v2, Oct. 8, 2016, 23 pages.</span>
            
            
          </td>
        </tr>
      </tbody>
    </table>
  </section>

  <h2>Cited By (9)</h2>
  <table>
    <caption>* Cited by examiner, † Cited by third party</caption>
    <thead>
      <tr>
        <th>Publication number</th>
        <th>Priority date</th>
        <th>Publication date</th>
        <th>Assignee</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/US11205121B2/en">
            <span itemprop="publicationNumber">US11205121B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2018-06-20</td>
        <td itemprop="publicationDate">2021-12-21</td>
        <td><span itemprop="assigneeOriginal">Disney Enterprises, Inc.</span></td>
        <td itemprop="title">Efficient encoding and decoding sequences using variational autoencoders 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/US11238341B2/en">
            <span itemprop="publicationNumber">US11238341B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2018-06-20</td>
        <td itemprop="publicationDate">2022-02-01</td>
        <td><span itemprop="assigneeOriginal">Disney Enterprises, Inc.</span></td>
        <td itemprop="title">Efficient encoding and decoding sequences using variational autoencoders 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/KR20210059540A/en">
            <span itemprop="publicationNumber">KR20210059540A</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-11-15</td>
        <td itemprop="publicationDate">2021-05-25</td>
        <td><span itemprop="assigneeOriginal">고려대학교 산학협력단</span></td>
        <td itemprop="title">Device and method for correcting Korean spelling 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/KR102430918B1/en">
            <span itemprop="publicationNumber">KR102430918B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-11-15</td>
        <td itemprop="publicationDate">2022-08-10</td>
        <td><span itemprop="assigneeOriginal">고려대학교 산학협력단</span></td>
        <td itemprop="title">Device and method for correcting Korean spelling 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/US11455656B2/en">
            <span itemprop="publicationNumber">US11455656B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-11-18</td>
        <td itemprop="publicationDate">2022-09-27</td>
        <td><span itemprop="assigneeOriginal">Walmart Apollo, Llc</span></td>
        <td itemprop="title">Methods and apparatus for electronically providing item advertisement recommendations 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/US11392984B2/en">
            <span itemprop="publicationNumber">US11392984B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-11-20</td>
        <td itemprop="publicationDate">2022-07-19</td>
        <td><span itemprop="assigneeOriginal">Walmart Apollo, Llc</span></td>
        <td itemprop="title">Methods and apparatus for automatically providing item advertisement recommendations 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/US11581000B2/en">
            <span itemprop="publicationNumber">US11581000B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-11-29</td>
        <td itemprop="publicationDate">2023-02-14</td>
        <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
        <td itemprop="title">Apparatus and method for encoding/decoding audio signal using information of previous frame 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/WO2021145862A1/en">
            <span itemprop="publicationNumber">WO2021145862A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2020-01-14</td>
        <td itemprop="publicationDate">2021-07-22</td>
        <td><span itemprop="assigneeOriginal">Google Llc</span></td>
        <td itemprop="title">Method and system for activity prediction, prefetching and preloading of computer assets by a client-device 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope="" repeat="">
        <td>
          
          
          <a href="https://patents.google.com/patent/KR102388599B1/en">
            <span itemprop="publicationNumber">KR102388599B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2021-08-24</td>
        <td itemprop="publicationDate">2022-04-21</td>
        <td><span itemprop="assigneeOriginal">(주)제로엑스플로우</span></td>
        <td itemprop="title">Apparatus and method for correcting sentence using test and image 
       </td>
      </tr>
    </tbody>
  </table>

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2018271931B2/en">
              <span itemprop="publicationNumber">AU2018271931B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-05-07</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP6884871B2/en">
              <span itemprop="publicationNumber">JP6884871B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-06-09</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20210019623A1/en">
              <span itemprop="publicationNumber">US20210019623A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-01-21</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156032A1/en">
              <span itemprop="publicationNumber">EP4156032A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20200372358A1/en">
              <span itemprop="publicationNumber">US20200372358A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-11-26</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102180002B1/en">
              <span itemprop="publicationNumber">KR102180002B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-11-17</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP2023052483A/en">
              <span itemprop="publicationNumber">JP2023052483A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-04-11</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/PL3542316T3/en">
              <span itemprop="publicationNumber">PL3542316T3</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-02-20</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CA3144657A1/en">
              <span itemprop="publicationNumber">CA3144657A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2018-11-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/ES2934313T3/en">
              <span itemprop="publicationNumber">ES2934313T3</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-02-21</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP3542316B1/en">
              <span itemprop="publicationNumber">EP3542316B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-12-07</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20190392319A1/en">
              <span itemprop="publicationNumber">US20190392319A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2019-12-26</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156034A1/en">
              <span itemprop="publicationNumber">EP4156034A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20220051099A1/en">
              <span itemprop="publicationNumber">US20220051099A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-02-17</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20200129197A/en">
              <span itemprop="publicationNumber">KR20200129197A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-11-17</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP7214783B2/en">
              <span itemprop="publicationNumber">JP7214783B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-30</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP2021121952A/en">
              <span itemprop="publicationNumber">JP2021121952A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-08-26</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20210019624A1/en">
              <span itemprop="publicationNumber">US20210019624A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-01-21</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2020213318B2/en">
              <span itemprop="publicationNumber">AU2020213318B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-06-02</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CA3050334C/en">
              <span itemprop="publicationNumber">CA3050334C</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-04-11</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP7214784B2/en">
              <span itemprop="publicationNumber">JP7214784B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-30</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/RU2021116658A/en">
              <span itemprop="publicationNumber">RU2021116658A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-07-05</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP2020506466A/en">
              <span itemprop="publicationNumber">JP2020506466A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-02-27</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20200372357A1/en">
              <span itemprop="publicationNumber">US20200372357A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-11-26</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/RU2749945C1/en">
              <span itemprop="publicationNumber">RU2749945C1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-06-21</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/BR112019014822B1/en">
              <span itemprop="publicationNumber">BR112019014822B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-06-07</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156033A1/en">
              <span itemprop="publicationNumber">EP4156033A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20200129198A/en">
              <span itemprop="publicationNumber">KR20200129198A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-11-17</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156031A1/en">
              <span itemprop="publicationNumber">EP4156031A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102486348B1/en">
              <span itemprop="publicationNumber">KR102486348B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-09</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10956819B2/en">
              <span itemprop="publicationNumber">US10956819B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-03-23</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2020213317B2/en">
              <span itemprop="publicationNumber">AU2020213317B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-06-02</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US11113602B2/en">
              <span itemprop="publicationNumber">US11113602B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-09-07</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CA3050334A1/en">
              <span itemprop="publicationNumber">CA3050334A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2018-11-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2020213318A1/en">
              <span itemprop="publicationNumber">AU2020213318A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-08-27</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20220133323A/en">
              <span itemprop="publicationNumber">KR20220133323A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-10-04</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2018271931A1/en">
              <span itemprop="publicationNumber">AU2018271931A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2019-07-11</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/JP2021121951A/en">
              <span itemprop="publicationNumber">JP2021121951A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-08-26</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP3542316A1/en">
              <span itemprop="publicationNumber">EP3542316A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2019-09-25</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CA3144674A1/en">
              <span itemprop="publicationNumber">CA3144674A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2018-11-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US20180341860A1/en">
              <span itemprop="publicationNumber">US20180341860A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2018-11-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR102448389B1/en">
              <span itemprop="publicationNumber">KR102448389B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-09-28</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156030A1/en">
              <span itemprop="publicationNumber">EP4156030A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/KR20190089980A/en">
              <span itemprop="publicationNumber">KR20190089980A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2019-07-31</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/EP4156035A1/en">
              <span itemprop="publicationNumber">EP4156035A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-03-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2022221389A1/en">
              <span itemprop="publicationNumber">AU2022221389A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2022-09-22</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/WO2018217948A1/en">
              <span itemprop="publicationNumber">WO2018217948A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2018-11-29</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/BR112019014822A2/en">
              <span itemprop="publicationNumber">BR112019014822A2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-02-27</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/AU2020213317A1/en">
              <span itemprop="publicationNumber">AU2020213317A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-08-27</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/CN110192206A/en">
              <span itemprop="publicationNumber">CN110192206A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2019-08-30</td>
        </tr><tr itemprop="docdbFamily" itemscope="" repeat="">
          <td>
            
            
            <a href="https://patents.google.com/patent/US10719764B2/en">
              <span itemprop="publicationNumber">US10719764B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-07-21</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope="" repeat="">
          <td>
            <meta itemprop="isPatent" content="true"/>
              
              
              <a href="https://patents.google.com/patent/US10956819B2/en">
                <span itemprop="publicationNumber">US10956819B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-03-23">2021-03-23</time>
            
            
          </td>
          <td itemprop="title">Attention-based sequence transduction neural networks 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope="" repeat="">
          <td>
            <meta itemprop="isPatent" content="true"/>
              
              
              <a href="https://patents.google.com/patent/US11556786B2/en">
                <span itemprop="publicationNumber">US11556786B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-01-17">2023-01-17</time>
            
            
          </td>
          <td itemprop="title">Attention-based decoder-only sequence transduction neural networks 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope="" repeat="">
          <td>
            <meta itemprop="isPatent" content="true"/>
              
              
              <a href="https://patents.google.com/patent/US10885436B1/en">
                <span itemprop="publicationNumber">US10885436B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-01-05">2021-01-05</time>
            
            
          </td>
          <td itemprop="title">Training text summarization neural networks with an extracted segments prediction objective 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope="" repeat="">
          <td>
            <meta itemprop="isPatent" content="true"/>
              
              
              <a href="https://patents.google.com/patent/US20200104681A1/en">
                <span itemprop="publicationNumber">US20200104681A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-04-02">2020-04-02</time>
            
            
          </td>
          <td itemprop="title">Neural Networks with Area Attention 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2018-06-28">2018-06-28</time></td>
          <td itemprop="code">FEPP</td>
          <td itemprop="title">Fee payment procedure</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2018-06-29">2018-06-29</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">GOOGLE INC., CALIFORNIA</span>
            </p>
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SHAZEER, NOAM M.;GOMEZ, AIDAN NICHOLAS;KAISER, LUKASZ MIECZYSLAW;AND OTHERS;SIGNING DATES FROM 20170602 TO 20170714;REEL/FRAME:046239/0093</span>
            </p>
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">GOOGLE LLC, CALIFORNIA</span>
            </p>
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:046465/0740</span>
            </p>
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20170929</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2019-02-19">2019-02-19</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2019-05-30">2019-05-30</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2019-06-19">2019-06-19</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2019-09-07">2019-09-07</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope="" repeat="">
          <td><time itemprop="date" datetime="2019-10-02">2019-10-02</time></td>
          <td itemprop="code">STCF</td>
          <td itemprop="title">Information on status: patent grant</td>
          <td>
            
            <p itemprop="attributes" itemscope="" repeat="">
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PATENTED CASE</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>
</article>

    </div></div>
  </body>
</html>
