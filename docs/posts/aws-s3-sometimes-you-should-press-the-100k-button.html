<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cyclic.sh/posts/aws-s3-why-sometimes-you-should-press-the-100k-dollar-button">Original</a>
    <h1>AWS S3: Sometimes you should press the $100k button</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>(a short story inspired by true events)</em></p><blockquote>TLDR: Mere mortals implement best practices for S3 read and write performance, create a million billion s3 objects with their high performance system, try to clean up the really really expensive mess and find themselves in a <em>dimension as vast as space and as timeless as infinity.<br/></em></blockquote><p>‍</p><p>In the days before BigQuery and S3&#39;s unlimited object storage, data was finite and predictable. The constraint drove engineers and architects to decide what they would and would not store at design time. Rigid data models and schemas were put in place. The data at rest was kept safe from developers behind dedicated teams with God mode SQL permissions.<br/></p><p>‍</p><h3>Today managed services have delivered on their <strong><em>great promise </em></strong>. </h3><p>Today managed services have delivered on their great promise. You can now grow your MVP to enterprise scale and all the things are taken care of for you. </p><p>Now, AWS Kinesis can dump <a href="https://docs.aws.amazon.com/firehose/latest/dev/limits.html">half a million</a> (soft limit) records per second into an S3 bucket. If you don&#39;t know where to find half a million of records per second, a handy menu of <a href="https://aws.amazon.com/eventbridge/integrations/">channel partners</a> is available on the console. They are ready to pipe it to you with only a few clicks<em>. (But don’t worry, versioning is probably enabled on your bucket and some runaway recursive ghost code is endlessly touching some config file.)</em></p><p><em>‍</em>The NDJSON is flowing, data scientists are on-boarded and life is really great for a while, until one day; at one of the many many status meetings the leadership points out that your group has <strong>burned through the cloud budget for the year, and it is only May</strong>. In cost explorer, the trickle from multiple firehoses has accumulated into an S3 storage bill of almost $100k/month. </p><blockquote>... the trickle from multiple firehoses has accumulated into an <strong>S3 storage bill of almost $100k/month</strong>. <br/></blockquote><h3><strong></strong></h3><h5><strong>‍</strong>⚠️ Documentation References Ahead ⚠️<br/></h5><p>S3 objects are named by their Bucket and Prefix - For the S3 objects : </p><p><a href="http://code-block">s3://my-bucket-us-east-2/app/events/login-123123.json</a></p><p>The json files named <a href="http://code-inline">login-123123.json</a> and <a href="http://code-inline">logout-436564.json</a> are not actually in a folder called events nested in another called app. </p><p>The files are stored on a physical drive somewhere and indexed someplace else by the entire string <a href="http://code-inline">app/events/</a> - called the prefix. The <a href="http://code-inline">/</a> character is really just a rendered delimiter. You can actually specify <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html">whatever you want </a>to be the delimiter for list/scan apis. </p><p>The limits on reads and writes are:</p><ul role="list"><li>3,500 PUT/COPY/POST/DELETE requests per second per prefix</li><li>5,500 GET/HEAD requests per second per prefix<br/></li></ul><p>This means you can make up to 3500 write / 5500 read requests per second to add or look at <a href="http://code-inline">app/events/</a> files. Higher than that, and you may experience <a href="http://code-inline">503</a> or <a href="http://code-inline">429</a> errors, especially with burst traffic. <br/></p><p>When faced with the challenge of producing or accessing big data with practical performance - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html">Best practices</a>, enterprise support, and your technical account manager (TAM) will recommend that you:</p><blockquote>... use an object key naming pattern that distributes your objects across multiple prefixes.<br/></blockquote><p>‍</p><h3>Tech debt</h3><p>These best practices are usually overlooked in early MVPs and objects are likely stored with the familiar <a href="http://code-inline">folder/file</a> pattern. It doesn&#39;t take long for the issue to expose itself. The first performance issues are usually experienced on read when someone first tries large scale queries with tools like AWS Athena and competes with the customer-facing processes for S3 reads.<br/></p><p>This is chalked up as a <a href="https://www.cyclic.sh/posts/we-sound-like-idiots-when-we-talk-about-technical-debt">repaying tech-debt</a> and work is done to adapt the existing patterns to best practices. Processes that used to produce <a href="http://code-inline">app/events/login-123123.json</a> are refactored to write <a href="http://code-inline">app/events/dt=yyyy-mm-dd/login-123123.json</a> instead. </p><p><strong>Cool.</strong> You configure the partition key <a href="http://code-inline">dt=yyyy-mm-dd</a> in the data catalog, write some documentation about how everyone should use date ranges to make queries efficient. All is well and the data science team can again do linear regressions aka “Machine Learning”<strong> </strong>to find out which events make the most money or, you know “drive business kpis”. </p><h3>Back to our original problem</h3><p>Some time has passed and <strong>you now have a shitload of data in S3</strong>. Someone has noticed how much it costs and <strong>now you have to clean it up</strong>.<br/></p><p>Hopefully, AWS is holding your hand on this one - you&#39;re a really great customer obviously. They will tell you, the two options you have to clean stuff up are:</p><ul role="list"><li><strong>Delete objects one by one</strong> manually by using the list and delete API’s <em>(yeah, sounds dumb)</em></li><li><strong>Set up lifecycle policies</strong> at the bucket level to expire objects based on prefix rules <em>(this sounds promising!!)</em></li></ul><p>Lifecycle policies are pretty great.</p><p>They work by first specifying a filter like:</p><ul role="list"><li>Apply by tag <em>(That would be sweet.. but no one tagged any of the objects that are already in there)</em></li><li>Apply by a size threshold <em>(There is no meaningful size difference between stuff you might want or not want)</em></li><li>Apply by prefix <em>(Yeeeah, that&#39;s the one)</em></li></ul><p>Then specifying a rule like:</p><ul role="list"><li>Expire after some time has passed</li><li>Move to a different storage class</li><li>Delete older versions of objects</li></ul><h3></h3><p>Without much research, the prefix filter thing looks really good. Naturally, as it is with filters, everyone would need to decide what they want to keep and not keep.  That sounds like a mess of calls and meetings. On that dilemma, the (actually good) advice from your partners at AWS - </p><blockquote>“Your costs are accumulating, we can help you move the really old stuff to Glacier to stop the bleeding.” </blockquote><p>There are obvious tradeoffs with Glacier. But since large scale reads, especially ones on old data are not often done, after some short all-hands discussions everyone agrees.</p><h3></h3><p>You go back and press the button in the console. A message comes up asking you to confirm that you are aware that:</p><figure><p><img src="https://assets.website-files.com/60d0f077b69e2d8f2d246168/620d170b68d74639eb4fe9f1_Screen%20Shot%202022-02-16%20at%2010.23.35%20AM.png" loading="lazy" alt=""/></p></figure><p>But isn&#39;t there like a million billion objects?? How about a different storage class?  <a href="https://aws.amazon.com/s3/pricing/">Requests &amp; data retrievals</a> &gt; Lifecycle Transition requests into (per 1,000 requests). Even though the s3 console does not produce the ominous warning when selecting other storage classes, <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html">transitional request charges apply</a>. <strong>Fuuuuuuuuu</strong></p><blockquote> &#34;We&#39;re working on it.&#34;</blockquote><h3></h3><p>Fast forward through three months of weekly status meetings, working with different teams to document data retention requirements. Yes, the bills were paid, more stuff has been stored and costs have risen, but - <strong>there is finally a list!</strong> </p><p>You will archive the logouts after 30 days and delete after a year for compliance, logins will be kept for 6 months for the data science team, archived... etc. etc. </p><p>‍</p><p><strong>Now here is the twist.</strong> Somehow over many meetings, AWS support calls, etc - the story of how prefix filters work has morphed into how everyone wanted it to work.  Checking back in with support - you find out that <a href="http://code-inline">app/events/*/login-*.json</a> and <a href="http://code-inline">app/events/*/logout-*.json</a> is not actually going to magically scan wildcard patterns. It&#39;s the flip side of the partitioning coin. <strong>The thing that makes reads and writes so performant makes it impossible to implement wildcards in lifecycle policies.</strong></p><p>How did everyone come to the misunderstanding? (well other than by not reading the docs) Could it be coming across<a href="https://docs.aws.amazon.com/mediastore/latest/ug/policies-object-lifecycle-components.html"> similar language in docs for a different AWS service</a> that does have wildcards? Skimming over <a href="http://www.deplication.net/2019/02/aws-tip-wildcard-characters-in-s3.html">blog posts</a> on the subject? The <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html">S3 docs</a> not telling you explicitly that you can&#39;t do that? <em>(Comments welcome)</em><br/></p><h3></h3><p>Faced with this truth, the heroes realize that a lot of time has been put into crafting those rules, lists were made, arguments were had. There is no going back to Plan A now. A few sprints later - developer gets a user story: `<strong>Use big-ass SageMaker to crawl s3 and clean up</strong>`<em> (3 points).</em> </p><p>‍</p><p>‍</p></div></div>
  </body>
</html>
