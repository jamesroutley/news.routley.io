<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://unum.cloud/post/2021-12-07-supercycle/">Original</a>
    <h1>Server Hardware Super-Cycle 2022</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>A single software company can spend <a href="https://www.datacenterdynamics.com/en/news/google-recalibrate-data-center-spend-and-cut-hiring-due-covid-19/">over üí≤10 Billion/year</a>, on data centres, but not every year is the same.
When all stars align, we see bursts of new technologies reaching the market simultaneously, thus restarting the purchasing super-cycle.
2022 will be just that, so let‚Äôs jump a couple of quarters ahead and see what‚Äôs on the shopping list of your favourite hyperscaler!</p>
<blockquote>
<p>Friendly warning: this article is full of technical terms and jargon, so it may be hard to read if you don‚Äôt write code or haven‚Äôt assembled computers before.</p>
</blockquote>
<blockquote>
<p>Warning #2. We are at the crossroads, where mobile/desktop and servers will have to go in different directions.
So the gamers may not see most of these technologies next year.</p>
</blockquote>
<blockquote>
<p>You can comment and discuss <a href="https://news.ycombinator.com/item?id=29484567">this article on HackerNews</a>.</p>
</blockquote>
<h2 id="what-will-we-cover">What will we cover?</h2>
<p>A lot, but not all!</p>
<p>First of all, this is not about emerging analogue computing technologies, quantum or optical.
Those are excellent topics for research but won‚Äôt have wide adoption in the next couple of years.</p>
<p>Secondly, it‚Äôs not about lithography.
At least not directly.
We still have some headroom with the Moores law, so the transistors should continue shrinking.
<a href="https://en.wikipedia.org/wiki/ASML_Holding">ASML</a>s <a href="https://en.wikipedia.org/wiki/Extreme_ultraviolet_lithography">Extreme Ultraviolet Lithography</a> printing machines are becoming more common.
<a href="https://en.wikipedia.org/wiki/TSMC">TSMC</a> was already printing at 5nm for Apple and announced 3nm coming soon.
That shrinking will have a somewhat uniform effect on performance across applications, so we can save time and research something else.</p>
<p>Let‚Äôs focus on the devices themselves, how <a href="#connectivity">motherboards</a>, <a href="#volatile-memory-ddr5">RAM</a>, <a href="#persistent-storage">SSDs</a>, <a href="#smart-nics-and-dpus">NICs</a>, <a href="#cpus--gpus">CPUs, and GPUs</a> are constrained today and how will they evolve in the future.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/servers.gif" alt="Servers"/></p>
<hr/>
<h2 id="connectivity">Connectivity</h2>
<blockquote>
<p>There was a time the CPU was alone,</p>
</blockquote>
<p>Connectivity is essential.
All following devices are irrelevant if you can‚Äôt link them.
<a href="https://en.wikipedia.org/wiki/PCI_Express">PCI-E bus</a> is the one responsible for that.
It connects CPUs to other specialized accelerator cards, like GPUs, TPUs, IPUs.
With more PCI-E lanes, you can attach more devices hence analyzing more data.</p>
<table>
<thead>
<tr>
<th></th>
<th>Year</th>
<th>x16 Troughput</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gen 1.</td>
<td>2003</td>
<td>4 GB/s</td>
</tr>
<tr>
<td>Gen 2.</td>
<td>2007</td>
<td>8 GB/s</td>
</tr>
<tr>
<td>Gen 3.</td>
<td>2010</td>
<td>16 GB/s</td>
</tr>
<tr>
<td>Gen 4.</td>
<td>2017</td>
<td>32 GB/s</td>
</tr>
<tr>
<td>Gen 5.</td>
<td>2019</td>
<td>64 GB/s</td>
</tr>
</tbody>
</table>
<p>Technically, Gen 5 was introduced in 2019, but the first products started popping up in Q3 2021.
Next year we expect wide adoption and a possible introduction of Gen 6 with another bitrate doubling, as always.</p>
<p>Assume you have a few modern GPUs like the Nvidia A100 in your system.
Each can fetch over 1 TB/s from its‚Äôs ginormous 80 GB HBM2 pack, but the CPU still easily has 20x more memory volume.
When we try to unify those pools, the non-uniformity of access latency will kill every supercomputing ambition you had <del>as a child</del>.
Latency aside, the 64 GB/s bandwidth in Gen 5 is still at least 16x slower, than what we would call uniform!</p>
<h3 id="compute-express-link-cxl">Compute Express Link (CXL)</h3>
<p>This may be news to software developers, but hardware enterprises are familiar with that bottleneck and have been trying to replace or extend PCI-E for a decade now.
A notable example would be the <a href="https://en.wikipedia.org/wiki/Coherent_Accelerator_Processor_Interface">Coherent Accelerator Processor Interface</a> (CAPI).
Proposed by IBM in 2014, it was one of the first expansion buses layered on top of PCI-Express.
It means the bandwidth can‚Äôt exceed the underlying protocol, only functionality, like supporting transactions between different ISA components.
Intel followed up a year later with Omni-Path.
Just like IBM, their effort was short-lived.
Sadly, even organizing a consortium with companies like AMD, ARM, Huawei, Qualcomm and Xilinx wouldn‚Äôt guarantee the future of such proposals.
The <a href="https://en.wikipedia.org/wiki/Cache_coherent_interconnect_for_accelerators">Cache Coherent Interconnect for Accelerators</a> (CCIX) was just that‚Ä¶</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/cxl.jpg" alt="CXL"/></p>
<p>Everyone failed until someone didn‚Äôt.
<a href="https://en.wikipedia.org/wiki/Compute_Express_Link">CXL</a> is also extending the existing PCI-E.
It already has some industry adoption.
It <a href="https://www.servethehome.com/compute-express-link-or-cxl-what-it-is-and-examples/">defines three typical use cases</a> in the 1.1 version.
As we saw at the <a href="https://www.opencompute.org/summit/global-summit">OCP Summit</a>, <a href="https://www.youtube.com/watch?v=IswQCyHnauY">tech giants are already using it</a> for expandable memory pools.
Samsungs <a href="https://news.samsung.com/global/tag/scalable-memory-development-kit">Scalable Memory Development Kit</a> is an excellent example of this new family of devices.
It lets you attach pools of DRAM over PCI-E.
Each of them becomes something like a <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA node</a>.</p>
<p>Aside from DRAM expansion and smarter NICs, CXL also hopes to revolutionize how we view and share the memory pools already installed in your chips.
Today, every device has its own address space.
Transparently unified addressing is extremely slow for randomized memory patterns and causes unpredictable latencies.
So if your GPU has 10&#39;000 cores and if at least 1 core in every cluster accesses remote memory, your overall speed will fall dramatically, even if 99% of cores are addressing local HBM2 buffers.
Writing manual synchronization primitives has been the only solution for Unum so far, but most teams can‚Äôt bear this burden.
Even the biggest game-design agencies abandoned the idea of <a href="https://en.wikipedia.org/wiki/Scalable_Link_Interface">SLI</a> many years, because of the sheer complexity of scheduling asynchronous transfers.</p>
<blockquote>
<p>Shameless plug here.
We have experience with Nvidias CUDA queues and Cooperative Groups, OpenCL &amp; OpenGL and Vulkan queues, and a little with SyCL.
I don‚Äôt know which one is worse, but none of them is good.
Vulkan is overly complex and more graphics-oriented.
Everything else sucks for multi-GPU.
Let us know if there is a new driver-level (not a library) async task scheduling game in town.
Prefereably, a userspace framework like <a href="https://pmem.io/pmdk/">PMDK</a> and <a href="https://spdk.io/">SPDK</a>.
We will be happy to participate in the design!</p>
</blockquote>
<h3 id="nv-link">NV-Link</h3>
<p>We remember that CXL builds on top of PCI-E, thus inheriting parent protocols limitations.
There is another game in town - NV-Link and NV-Switch by NVidia.
They have 640 GB of <a href="https://en.wikipedia.org/wiki/High_Bandwidth_Memory">High Bandwidth Memory</a> spread across eight GPUs in every DGX A100 server.
It must be wired together, so they have a switch inside each server!</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/nvswitch.jpg" alt="NV-Switch in DGX A100"/></p>
<p>Wait, what?!
A Switch.
Inside of a server. ü§Øü§Øü§Ø
They have buried the consumer-level <a href="https://en.wikipedia.org/wiki/Scalable_Link_Interface">SLI</a> and replaced it with <a href="https://www.nvidia.com/en-us/data-center/nvlink/">NV-Link</a> for pro-sumers and datacenters.
That‚Äôs how you join pairs of GPUs.
Each pair can then exchange 600 GB/s.
A significant upgrade over PCI-E Gen 4 at the time.
The NV-Switch, in turn, is used to link up to eight GPUs in most modern servers.
With modern Neural Networks often exceeding 100 GB in size, you would need to slice the networks into groups of layers, store them on different GPUs, and then route signal propagation via <a href="https://developer.nvidia.com/nccl">NCCL</a>.
NV-Switch will handle it at an aggregate throughput of 9.6 TB/s.</p>
<blockquote>
<p>There is something else luring far-far ahead. Photonics! One day we will write about it! <a href="https://bit.ly/3mG4q0f">Subscribe</a> until then!</p>
</blockquote>
<hr/>
<h2 id="volatile-memory-ddr5">Volatile Memory: DDR5</h2>
<p>With 1 TB RAM per AMD Epyc CPU, it would take over 5 seconds just to <code>memset</code> the whole memory at peak 200 GB/s speed.
Too slow, so DDR5 comes to the rescue.</p>
<p>PCI-E slots are both forward and backward compatible.
You can put a new GPU into an old slot and vice versa.
The same doesn‚Äôt hold for RAM though.
New generation - new motherboards.
New motherboards - new CPUs.
So it‚Äôs less surprising that DDR4 was with us from 2014 until now, 2021.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/ddr5.png" alt="DDR5 Speeds"/></p>
<p>A big datacenter hosts about 200&#39;000 CPUs.
At üí≤5&#39;000 per CPU ‚Äî vendors can‚Äôt afford fashion for the sake of fashion.
A replacement of CPUs alone would cost üí≤1 Billion, so it must be justified.
So how much better DDR5 is:</p>
<ul>
<li>Clock rate: from 1.6 GHz up to 4.8 GHz.</li>
<li>DIMM size: from 64 GB to 256 GB.</li>
</ul>
<p>I would expect double the speed, quadrupled volume, and better power efficiency.
We can already taste such rates in the new MacBook Pro 2021.
This üí≤2&#39;000 marvellous piece of engineering packages DDR5 with 400 GB/s bandwidth, double what you get in a üí≤200&#39;000 state-of-the-art server.</p>
<hr/>
<h2 id="persistent-storage">Persistent Storage</h2>
<p>First of all, isn‚Äôt it weird that we still sell NVME SSDs in the M.2 form-factor?
They are tiny and make sense for laptops, but why put the same in the server?</p>
<p>If you thing it‚Äôs odd, the <a href="https://en.wikipedia.org/wiki/U.2">U.2</a> is <a href="https://www.urbandictionary.com/define.php?term=batshit%20crazy">batshit crazy</a>.
It‚Äôs designed to fit into the 2.5&#34; caddies of most servers, which are inherited from the old HDD days.
We had Large-Form-Factor 3.5&#34; HDDs from 1983 and the newer Small-Form-Factor 2.5&#34; HDDs <a href="https://en.wikipedia.org/wiki/List_of_disk_drive_form_factors">since 1988</a>.
That technology used magnetic spinning disks and was capped at 20 MB capacity.
Our current U.2 Samsung <code>M393AAG40M32-CAE</code> SSDs each have 8 TB (300&#39;000x more) in the same form factor without any spinning disks involved.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/ssd.png" alt="New SSD Form Factors"/></p>
<p>In 2022 we will finally start fixing that historical joke.
The replacement isn‚Äôt without quirks - too many sizes, but I appreciate the change.
Here we see two significant trends:</p>
<ul>
<li>removal of caddies ‚Äî one less part to buy from vendors,</li>
<li>hot-pluggable PCI-E devices ‚Äî so it‚Äôs easier to replace and service,</li>
<li>longer and slimmer body, to make more capacity serviceable from the server front,</li>
<li>taller heatsinks for next-generation controllers with up to 40W expected power consumption.</li>
</ul>
<p>New SSDs will sit on the PCI-E bus, so the controllers should become faster.
Today‚Äôs record is about 7 GB/s and can‚Äôt be above 8 GB/s.
In the next-gen, the cap will be 16 GB/s.</p>
<p>Furthermore, did you know modern SSDs also house logic and volatile memory on board?
The cheap ones cut the costs and may have no DRAM aboard.
Write performance then becomes inconsistent but may stay unnoticeable for some users.
With higher connectivity, this bottleneck may be gone.
SSDs can lose logic and RAM and solely provide block-addressable NAND flash arrays.
All the magic can then move to userspace software, potentially eradicating the filesystem concept ‚Äî worth exploring.</p>
<h3 id="persistent-memory">Persistent‚Ä¶ Memory?!</h3>
<p>There is more. Volatile and persistent memory have radically different bandwidths and volumes.
The first is small and fast, and the second is big and slow.
The gap is enormous, with about 10&#39;000x speed difference in randomized mixed read+write operations.
Software vendors are trying to reduce this gap, but the hardware is also evolving.</p>
<p>Meet ‚ÄúPersistent Memory‚Äù ‚Äî the foster child of Intel and Micron.
Though promising, the unit went through multiple reorganizations in the last few years and only gained traction now.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/pmem.jpg" alt="Intel Persistent Memory"/></p>
<p>The main promises PMem brings, are:</p>
<ul>
<li>Zero boot time. Your RAM essentially becomes persistent. Once the computer is turned on - you don‚Äôt need to boot. All the applications continue from the same place.</li>
<li>Larger memory pools. The common upper bound for RAM is about 2 TB/socket today. With PMem it can be 12 TB.</li>
</ul>
<p>Why isn‚Äôt everyone using it then?</p>
<ul>
<li>It‚Äôs very pricey.</li>
<li>Software adjustments are needed to use it properly.</li>
</ul>
<p>Plus, I don‚Äôt know if it works with AMD CPUs.
Adoption-wise, <a href="https://github.com/tensorflow/tensorflow">TensorFlow</a> has 160K stars on GitHub, but still has no proper support for AMD GPUs.
<a href="https://github.com/pmem/pmdk">PMDK</a> has only 1K stars, so it still has much adoption to gain.
Most of our servers have AMD Zen2 CPUs at Unum, and we have no PMem to experiment.
Maybe next year!</p>
<hr/>
<h2 id="smart-nics-and-dpus">Smart NICs and DPUs</h2>
<p>SmartNICs are weird and very hard for me to grasp.
As clouds grow and software-defined networking becomes a thing - companies like Amazon start cramming more stuff into networking cards.
We know little about such a scale and the use case.</p>
<p>Down here on Earth, we love training big Deep Learning models.
After showing a randomly sampled batch of data to a neural network on one server, you use gradients to guide the change of weights towards finding the minimum of the loss (error) function.
Like most classical distributed computing, you split your machines into ‚Äúmasters‚Äù and ‚Äúslaves‚Äù.
The ‚Äúmanagers‚Äù and number-crunchers, but suddenly ‚Äúmanagers‚Äù end up doing just as much arithmetics - averaging the state across the cluster after every batch and broadcasting it back.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/bluefield.jpg" alt="Nvidia Bluefield DPU"/></p>
<p>With 200 Gbit fibres today and 400 Gbit fibres tomorrow, sending the data to neighbouring machines becomes cheaper, while averaging becomes more noticeable.
So the industry answer is to put more RAM, CPU cores and GPUs into the NIC, making it a server by itself.</p>
<ul>
<li>DPU will run a separate instance of Linux.</li>
<li>DPU will have it‚Äôs own processes and address space.</li>
<li>DPU will even have a separate management LAN.</li>
</ul>
<p>Genuinely, the overall concept doesn‚Äôt make sense to me.
It may do the job in some cases, but I would hardly call it future-proof or elegant.
The time will tell!</p>
<hr/>
<h2 id="cpus--gpus">CPUs &amp; GPUs</h2>
<p>Finally, we reach the coolest part of the review - CPUs &amp; GPUs.
Those two generally work in tandem and are the pillars of modern High-Performance Computing.
We have seen Systems-On-a-Chip (SoC) that pack both on a single die, but that‚Äôs mostly for laptops and entry-level gaming.</p>
<h3 id="amd">AMD</h3>
<p>The CPU is flat.
At least that‚Äôs how ‚Äúthey‚Äù want us to think. üòÅ
It wasn‚Äôt flat before, and it‚Äôs most definitely not now.
AMD already has a colossal socket, so when they tried to grow memory, they decided to stack it on top - vertically.
Hence the name.
V-Cache was recently announced for desktop Ryzen CPUs, tripling the L3 cache size of the fastest desktop CPU.
Our top-of-the-line Epyc and Threadripper Pro CPUs have 256 MB of L3 cache.
It‚Äôs enough, in theory, to fit the Linux kernel without even using the RAM!
Next year it will likely be 768 MB!</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/vcache.jpg" alt="AMD V-Cache"/></p>
<p>Some say that we may go from 64 cores to 96 or 128 cores with <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading">SMT2</a>.
However, the most significant core-count changes are happening in the GPU market.
With the grand unveiling of their <a href="https://www.amd.com/en/graphics/instinct-server-accelerators">MI200 GPU</a> series, they took the lead in FP64 high-precision workloads.
Irrelevant for Machine Learning, it still underpins ballistics and some scientific computations.
Anyways, congratulations on delivering the <a href="https://www.tweaktown.com/news/81138/amd-aldebaran-amds-first-mcm-gpu-will-launch-later-this-year/index.html">first MCM design</a> in GPU history!</p>
<h3 id="intel">Intel</h3>
<p>Enthusiasts may have noticed that CPUs these days have almost as many cores as GPUs Execution Units.
Their Floating Point Units are getting beefier, and the power consumption in both cases may soon surpass 500W.
Looks like convergence.
So Intel thought, why not pack HBM into the product similar to how GPUs are sold?
That‚Äôs preceisly what‚Äôs announced for 2022 Intel Sapphire Rapids chips.
It‚Äôs not clear if the system sees it as a RAM NUMA or an L4 cache, but I am eager to see!</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/pontevecchio.jpg" alt="Intel Ponte Vecchio GPU"/></p>
<p>Just like AMD, Intel isn‚Äôt done with GPUs.
Ponte Vecchio, their new massive GPU, is also expected in 2022.
The stakes are so high that they are going for the TSMC 5N lithography instead of their own fabs.
We haven‚Äôt seen discrete GPUs from Intel in years, but somehow, on the software side, they are better prepared for heterogeneous computing.
They have joined the <a href="https://www.khronos.org/sycl/">SyCL</a> consortium, extended their compiler toolchain, but I have to state that they are still behind Nvidia from the software perspective.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/aurora.jpg" alt="Intel Aurora Servers"/></p>
<p>With all this preparation, it‚Äôs only natural that they unroll their all-Intel servers to compete with <a href="https://www.nvidia.com/en-us/data-center/dgx-a100/">Nvidia DGX A100</a>.</p>
<h3 id="arm--nvidia">ARM &amp; Nvidia</h3>
<p>I expect broader adoption of ARM v9 ISA and availability of SVE2 SIMD extensions.
The first of its kind, with runtime-defined register size.
Your software will have the same binary for all hardware versions with SIMD registers ranging from 128 to 2048 bits.
Todays <a href="https://amperecomputing.com/altra/">Ampere Altra Max</a> CPUs feature 128 cores.
A single 2048-bit register would fit 128 half-precision floats.
With reduced control-flow granularity and huge vectorized registers this already reminds me GPUs.</p>
<p>If this wasn‚Äôt enough, Nvidia is buying ARM.
Everyone has known it for over a year now.
Some parties protest against this deal, but they unitedly can still bring something unique to the market next year regardless of the merger.
I am talking, of course, about the ARMv9 + NV-Link integration.
At their last GTC conference, they talked about our favourite memory bottlenecks and how they will solve them with ‚ÄúGrace‚Äù! üòÇ</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/grace.jpg" alt="Nvidia Grace ARM CPUs"/></p>
<p>Grace is their upcoming ARM CPU paving the road for many advanced next-gen technologies.
Today, the most common solution for server design for a dense environment implies two x86 sockets in 2U.
Dual AMD Epyc gives you not 256 PCI-E lanes (128 x 2), but 160, as 96 would be used for inter-processor communication.
160 is enough for 8x GPUs and 8x SSDs simultaneously, but this hides the question of affinity.
It‚Äôs tough to localize and reserve RAM buffers on the CPU side for a specific GPU.
Aside from higher bandwidth Nvidias Grace concept may provide a simpler programming model.
A one-to-one ratio between CPUs and GPUs looks closer to a four-node server than a single node with 4x CPUs and GPUs.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-07-supercycle/nvidia.jpg" alt="Nvidia HPC Roadmap"/></p>
<hr/>
<p>The overall landscape seems extremely promising!
We haven‚Äôt seen this much variety in the hardware space since the early days of computing.
This article could be much longer and detailed. Cerebras, GraphCore, SambaNova all presented exciting chips recently, and we will do our best to port our software to as many HPC platforms as possible!
<a href="https://bit.ly/3mG4q0f"><strong>Subscribe to our newsletter</strong> üì®</a> to receive more articles like this!
Or <a href="https://twitter.com/intent/tweet?text=Server+Hardware+Super-Cycle+2022%0D%0Aby+%40unum_cloud+%40ashvardanian%0D%0A%23intel+%23nvidia+%23amd+%23samsung"><strong>Tweet</strong></a> to share with friends üê¶</p>

    </div></div>
  </body>
</html>
