<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer">Original</a>
    <h1>Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer</h1>
    
    <div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Generative AI (gen AI) models are rapidly evolving, offering unparalleled sophistication and capability. This advancement empowers enterprises and developers across various industries to solve complex problems and unlock new opportunities. However, the growth in gen AI models — <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">with a tenfold increase in parameters annually over the past five years</a> — brings heightened requirements for training, tuning, and inference. Today&#39;s larger models, featuring hundreds of billions or even trillions of parameters, require extensive training periods, sometimes spanning months, even on the most specialized systems. Additionally, efficient AI workload management necessitates a coherently integrated AI stack consisting of optimized compute, storage, networking, software and development frameworks.</p><p>Today, to address these challenges, we are excited to announce Cloud TPU v5p, our most powerful, scalable, and flexible AI accelerator thus far. TPUs have long been the basis for training and serving AI-powered products like YouTube, Gmail, Google Maps, Google Play, and Android. In fact, Gemini, Google’s most capable and general AI model <a href="https://blog.google/technology/ai/google-gemini-ai" target="_blank">announced today</a>, was trained on, and is served, using TPUs.</p><p>In addition, we are also announcing AI Hypercomputer from Google Cloud, a groundbreaking supercomputer architecture that employs an integrated system of performance-optimized hardware, open software, leading ML frameworks, and flexible consumption models. Traditional methods often tackle demanding AI workloads through piecemeal, component-level enhancements, which can lead to inefficiencies and bottlenecks. In contrast, AI Hypercomputer employs systems-level codesign to boost efficiency and productivity across AI training, tuning, and serving.</p></span></section><section><section><div jsaction="rcuQ6b:npT2md" jscontroller="wJu6E" data-video-url="https://www.youtube.com/watch?v=hszd5UqnfLk"><div><picture><section><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" loading="lazy"/></section></picture></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Inside Cloud TPU v5p, our most powerful and scalable TPU accelerator to date</b></h3><p>Earlier this year, <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-in-ga">we announced</a> the general availability of Cloud TPU v5e. With 2.3X price performance improvements over the previous generation TPU v4<sup>1</sup>, it is our most <i>cost-efficient</i> TPU to date. By contrast, Cloud TPU v5p, is our most <i>powerful</i> TPU thus far. Each TPU v5p pod <b>composes together 8,960 chips</b> over our <b>highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology</b>. Compared to TPU v4, TPU v5p features more than<b> 2X greater FLOPS and 3X more high-bandwidth memory (HBM)</b>.</p><p>Designed for performance, flexibility, and scale, TPU v5p can <b>train large LLM models 2.8X faster</b> than the previous-generation TPU v4. Moreover, with second-generation <a href="https://cloud.google.com/tpu">SparseCores</a>, TPU v5p can <b>train embedding-dense models 1.9X faster</b> than TPU v4<sup>2</sup>.</p></span></section><section><div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"/><section role="dialog" aria-modal="true"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"/></section></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5p and v4 are based on Google Internal Data. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model.</p></span></p></section></div></section><section><div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"/><section role="dialog" aria-modal="true"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"/></section></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5e data is from MLPerf™ 3.1 Training Closed results for v5e. TPU v5p and v4 are based on Google internal training runs. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model. It shows relative performance per dollar using the public list price of TPU v4 ($3.22/chip/hour), TPU v5e ( $1.2/chip/hour) and TPU v5p ($4.2/chip/hour).</p></span></p></section></div></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>In addition to performance improvements, <b>TPU v5p is also 4X more scalable than TPU v4 in terms of total available FLOPs per pod.</b> Doubling the floating-point operations per second (FLOPS) over TPU v4 and doubling the number of chips in a single pod provides considerable improvement in relative performance in training speed.</p></span></section><section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Google AI Hypercomputer delivers peak performance and efficiency at large scale</b></h3><p>Achieving both scale and speed is necessary, but not sufficient to meet the needs of modern AI/ML applications and services. The hardware and software components must come together into an integrated, easy-to-use, secure, and reliable computing system. At Google, we’ve done decades of research and development on this very problem, culminating in AI Hypercomputer, a system of technologies optimized to work in concert to enable modern AI workloads.</p></span></section><section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><ul><li><b>Performance-optimized hardware:</b> AI Hypercomputer features performance-optimized compute, storage, and networking built over an ultrascale data center infrastructure, leveraging a high-density footprint, liquid cooling, and our <a href="https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network">Jupiter data center network</a> technology. All of this is predicated on technologies that are built with <a href="https://www.google.com/about/datacenters/efficiency/" target="_blank">efficiency</a> at their core; leveraging <a href="https://cloud.google.com/blog/topics/sustainability/a-smarter-way-to-buy-clean-energy">clean energy</a> and <a href="https://blog.google/outreach-initiatives/sustainability/replenishing-water/?_ga=2.140272307.1460901017.1631498684-1474825438.1628277680" target="_blank">a deep commitment to water stewardship</a>, and that are <a href="https://blog.google/outreach-initiatives/sustainability/our-third-decade-climate-action-realizing-carbon-free-future/" target="_blank">helping us move toward a carbon-free future</a>.</li><li><b>Open software:</b> AI Hypercomputer enables developers to access our performance-optimized hardware through the use of open software to tune, manage, and dynamically orchestrate AI training and inference workloads on top of performance-optimized AI hardware.<ul><li>Extensive support for popular ML frameworks such as JAX, TensorFlow, and PyTorch are available right out of the box. Both JAX and PyTorch are powered by <a href="https://github.com/openxla/xla" target="_blank">OpenXLA</a> compiler for building sophisticated LLMs. XLA serves as a foundational backbone, enabling the creation of complex multi-layered models (<a href="https://pytorch.org/blog/high-performance-llama-2/" target="_blank">Llama 2 training and inference on Cloud TPUs with PyTorch/XLA</a>). It optimizes distributed architectures across a wide range of hardware platforms, ensuring easy-to-use and efficient model development for diverse AI use cases (<a href="https://cloud.google.com/blog/products/compute/assemblyai-on-cloud-tpu-v5e-price-performance">AssemblyAI leverages JAX/XLA and Cloud TPUs for large-scale AI speech</a>).</li><li>Open and unique <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">Multislice Training</a> and <a href="https://cloud.google.com/tpu/docs/v5e-inference">Multihost Inferencing</a> software, respectively, make scaling, training, and serving workloads smooth and easy. Developers can scale to tens of thousands of chips to support demanding AI workloads.</li><li>Deep integration with <a href="https://cloud.google.com/kubernetes-engine?hl=en">Google Kubernetes Engine (GKE)</a> and <a href="https://cloud.google.com/compute?hl=en">Google Compute Engine</a>, to deliver efficient resource management, consistent ops environments, autoscaling, node-pool auto-provisioning, auto-checkpointing, auto-resumption, and timely failure recovery.</li></ul></li><li><b>Flexible consumption</b>: AI Hypercomputer offers a wide range of flexible and dynamic consumption choices. In addition to classic options, such as Committed Use Discounts (CUD), on-demand pricing, and spot pricing, AI Hypercomputer provides consumption models tailored for AI workloads via <a href="https://cloud.google.com/blog/products/compute/introducing-dynamic-workload-scheduler">Dynamic Workload Scheduler.</a> Dynamic Workload Scheduler introduces two models: Flex Start mode for higher resource obtainability and optimized economics, as well as Calendar mode, which targets workloads with higher predictability on job-start times.</li></ul><h3><b>Leveraging Google’s deep experience to help power the future of AI</b></h3><p>Customers like Salesforce and Lightricks are already training and serving large AI models with Google Cloud’s TPU v5p AI Hypercomputer — and already seeing a difference:</p><p><i>“We’ve been leveraging Google Cloud TPU v5p for pre-training Salesforce’s foundational models that will serve as the core engine for specialized production use cases, and we’re seeing considerable improvements in our training speed. In fact, Cloud TPU v5p compute outperforms the previous generation TPU v4 by as much as 2X. We also love how seamless and easy the transition has been from Cloud TPU v4 to v5p using JAX. We’re excited to take these speed gains even further by leveraging the native support for INT8 precision format via the Accurate Quantized Training (AQT) library to optimize our models.” -</i> Erik Nijkamp, Senior Research Scientist, Salesforce</p><p><i>“Leveraging the remarkable performance and ample memory capacity of Google Cloud TPU v5p, we successfully trained our generative text-to-video model without splitting it into separate processes. This optimal hardware utilization significantly accelerates each training cycle, allowing us to swiftly conduct a series of experiments. The ability to train our model quickly in each experiment facilitates rapid iteration, which is an invaluable advantage for our research team in this competitive field of generative AI.”</i> - Yoav HaCohen, PhD, Core Generative AI Research Team Lead, Lightricks</p><p><i>“In our early-stage usage, Google DeepMind and Google Research have observed 2X speedups for LLM training workloads using TPU v5p chips compared to the performance on our TPU v4 generation. The robust support for ML Frameworks (JAX, PyTorch, TensorFlow) and orchestration tools enables us to scale even more efficiently on v5p. With the 2nd generation of SparseCores we also see significant improvement in the performance of embeddings-heavy workloads. TPUs are vital to enabling our largest-scale research and engineering efforts on cutting edge models like Gemini.” -</i> Jeff Dean, Chief Scientist, Google DeepMind and Google Research</p><p>At Google, we’ve long believed in the power of AI to help solve challenging problems. Until very recently, training large foundation models and serving them at scale was too complicated and expensive for many organizations. Today, with Cloud TPU v5p and AI Hypercomputer, we’re excited to extend the result of decades of research in AI and systems design with our customers, so they can innovate with AI faster, more efficiently, and more cost effectively.</p><p>To request access to Cloud TPU v5p and AI Hypercomputer, please reach out to your <a href="https://cloud.google.com/contact/">Google Cloud account manager</a>. To learn more about Google Cloud’s AI infrastructure, <a href="https://cloudonair.withgoogle.com/events/summit-applied-ml-summit-23" target="_blank">register to attend Google Cloud Applied AI Summit</a>.</p><hr/><p><i><sup>1: MLPerf™ v3.1 Training Closed, multiple benchmarks as shown. Retrieved November 8th, 2023 from</sup></i> <a href="http://mlcommons.org/" target="_blank"><i><sup>mlcommons.org</sup></i></a><i><sup>. Results 3.1-2004. Performance per dollar is not an MLPerf metric. TPU v4 results are unverified: not verified by MLCommons Association. The MLPerf™ name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See</sup></i> <a href="http://www.mlcommons.org/" target="_blank"><i><sup>www.mlcommons.org</sup></i></a> <i><sup>for more information.</sup></i></p></span></section><section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/infrastructure-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Infrastructure Modernization</a></li><li><a href="https://cloud.google.com/blog/topics/systems" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/systems" track-metadata-module="tag list" track-metadata-module_headline="posted in">Systems</a></li></ul></section></section></div></div>
  </body>
</html>
