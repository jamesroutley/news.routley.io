<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/big-nacho/patolette">Original</a>
    <h1>Show HN: High End Color Quantizer</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67847653/452787608-c35f17b7-6c1f-499c-aa79-bbe9301fc6d2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI3ODc2MDgtYzM1ZjE3YjctNmMxZi00OTljLWFhNzktYmJlOTMwMWZjNmQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTllYmViMGYwNWNmOTZhYTQxZTkxNDYxODU1YmM5YjM5NTIyZmI5NmY4ZDk1ODhmYmM2YmY0OTZlM2NkNDc1NDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.kt19adSeOl-ukGUEC5DYoZKXhYfHGVMVrD8ZIZe49fM"><img src="https://private-user-images.githubusercontent.com/67847653/452787608-c35f17b7-6c1f-499c-aa79-bbe9301fc6d2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI3ODc2MDgtYzM1ZjE3YjctNmMxZi00OTljLWFhNzktYmJlOTMwMWZjNmQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTllYmViMGYwNWNmOTZhYTQxZTkxNDYxODU1YmM5YjM5NTIyZmI5NmY4ZDk1ODhmYmM2YmY0OTZlM2NkNDc1NDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.kt19adSeOl-ukGUEC5DYoZKXhYfHGVMVrD8ZIZe49fM" width="150px"/></a>
</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/25b20286afa9789b6b16542a800462c6e3be22b0c5098de6a31036592f34bcba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d76302e302e312d626c7565"><img src="https://camo.githubusercontent.com/25b20286afa9789b6b16542a800462c6e3be22b0c5098de6a31036592f34bcba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d76302e302e312d626c7565" data-canonical-src="https://img.shields.io/badge/version-v0.0.1-blue"/></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/94412730af28599b7162b6d72ce1c9374ddbbd2cfe96773cc7ef6b43c6be5fe8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f626574612d707572706c65"><img src="https://camo.githubusercontent.com/94412730af28599b7162b6d72ce1c9374ddbbd2cfe96773cc7ef6b43c6be5fe8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f626574612d707572706c65" data-canonical-src="https://img.shields.io/badge/beta-purple"/></a>
</p>
<p dir="auto"><em><strong>patolette</strong></em> is a <strong>C / Python</strong> color quantization and dithering library.</p>
<p dir="auto">At its core, it implements a weighted variant of Xiaolin Wu&#39;s PCA-based quantizer (not to be confused with the popular one from <em>Graphics Gems vol. II</em>, which is already available <a href="https://gist.github.com/bert/1192520">here</a>).</p>
<p dir="auto">Some of its key features are:</p>
<ul dir="auto">
<li>Avoids axis-aligned subdivisions</li>
<li>Supports the <strong>CIEL*u*v*</strong> and <strong>ICtCp</strong> color spaces</li>
<li>Optional use of saliency maps to give higher weight to areas that stand out visually</li>
<li>Optional <em>KMeans</em> refinement</li>
</ul>
<p dir="auto">The library is still in need of a ton of improvements and most definitely not ready for production use, but it&#39;s already very useable.</p>

<p dir="auto">A <strong>PyPI</strong> package is not yet available. Until then, installation is manual but it should hopefully be painless ü§û</p>
<p dir="auto">If you do face any obstacles building / installing, please submit an issue! üôè</p>

<p dir="auto"><em>patolette</em> ships a slightly modified version of <a href="https://github.com/facebookresearch/faiss">faiss</a> to aid with an optional <em>KMeans</em> refinement step. You can use the <code>CMAKE_ARGS</code> environment variable to specify an instruction set extension for it to be built with. If your CPU supports any of the <strong>AVX</strong> extensions, you can drastically increase <em>KMeans</em> performance.</p>
<p dir="auto">For example, if your CPU supports <strong>AVX512</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="export CMAKE_ARGS=&#34;-DOPT_LEVEL=avx512&#34;"><pre><span>export</span> CMAKE_ARGS=<span><span>&#34;</span>-DOPT_LEVEL=avx512<span>&#34;</span></span></pre></div>
<p dir="auto">The following will build the wheel and install it in the currently active virtual environment.</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Clone repository
git clone https://github.com/big-nacho/patolette.git
cd patolette

# Install dependencies
apt install libopenblas-openmp-dev libflann-dev

# Optional: set OPT_LEVEL (check Note for x86 section)
# Accepted values are &#34;generic&#34;, &#34;avx2&#34;, &#34;avx512&#34;, &#34;avx512_spr&#34;, &#34;sve&#34;
export CMAKE_ARGS=&#34;-DOPT_LEVEL=avx512&#34;

# Build and install wheel
pip install ."><pre><span><span>#</span> Clone repository</span>
git clone https://github.com/big-nacho/patolette.git
<span>cd</span> patolette

<span><span>#</span> Install dependencies</span>
apt install libopenblas-openmp-dev libflann-dev

<span><span>#</span> Optional: set OPT_LEVEL (check Note for x86 section)</span>
<span><span>#</span> Accepted values are &#34;generic&#34;, &#34;avx2&#34;, &#34;avx512&#34;, &#34;avx512_spr&#34;, &#34;sve&#34;</span>
<span>export</span> CMAKE_ARGS=<span><span>&#34;</span>-DOPT_LEVEL=avx512<span>&#34;</span></span>

<span><span>#</span> Build and install wheel</span>
pip install <span>.</span></pre></div>
<p dir="auto">Note on <strong>OpenBLAS</strong>: although any variant should in theory work, <code>libopenblas-openmp-dev</code> is recommended. If you have multiple variants installed, you may need to run the following before building for it to be linked properly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo update-alternatives --set libblas.so.3-x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/openblas-openmp/libblas.so.3
sudo update-alternatives --set liblapack.so.3-x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/openblas-openmp/liblapack.so.3"><pre>sudo update-alternatives --set libblas.so.3-x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/openblas-openmp/libblas.so.3
sudo update-alternatives --set liblapack.so.3-x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/openblas-openmp/liblapack.so.3</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Clone repository
git clone https://github.com/big-nacho/patolette.git
cd patolette

# Install dependencies
brew install libomp flann

# Make sure system clang is used. If you use brew&#39;s clang 
# you may run into libstdc++ issues
export CC=/usr/bin/clang
export CXX=/usr/bin/clang++

# Let CMake find OpenMP
export OpenMP_ROOT=$(brew --prefix)/opt/libomp

# Build and install wheel
pip install ."><pre><span><span>#</span> Clone repository</span>
git clone https://github.com/big-nacho/patolette.git
<span>cd</span> patolette

<span><span>#</span> Install dependencies</span>
brew install libomp flann

<span><span>#</span> Make sure system clang is used. If you use brew&#39;s clang </span>
<span><span>#</span> you may run into libstdc++ issues</span>
<span>export</span> CC=/usr/bin/clang
<span>export</span> CXX=/usr/bin/clang++

<span><span>#</span> Let CMake find OpenMP</span>
<span>export</span> OpenMP_ROOT=<span><span>$(</span>brew --prefix<span>)</span></span>/opt/libomp

<span><span>#</span> Build and install wheel</span>
pip install <span>.</span></pre></div>

<p dir="auto">Windows is of course a world of pain (but hey, no judgement if you&#39;re into that sort of thing ‚õìÔ∏è).</p>
<p dir="auto">Small note: <strong>MSVC</strong> doesn&#39;t like building <code>faiss</code> with <strong>AVX512</strong>. Stick to <strong>AVX2</strong> if you&#39;re building with an instruction set extension on, at least until that&#39;s fixed. If you don&#39;t know what I&#39;m talking about check <a href="#note-for-x86">Note for x86</a>.</p>
<p dir="auto">The following may vary for you here and there, but mostly you should be able to build and install the wheel following these steps:</p>
<p dir="auto">First, you need <code>pkg-config</code> or CMake won&#39;t find <code>flann</code>.
You can get it <a href="https://sourceforge.net/projects/pkgconfiglite/files/" rel="nofollow">here</a> or you can install <code>pkgconfiglite</code> using <a href="https://chocolatey.org/" rel="nofollow">choco</a> (that&#39;s what I did).</p>
<p dir="auto">Then you need to get <code>flann</code> and <code>OpenBLAS</code>. You can do this in a variety of ways but an easy one is to use <code>conda</code>. You can get (Mini)conda <a href="https://www.anaconda.com/docs/getting-started/miniconda/install" rel="nofollow">here</a>.</p>
<p dir="auto">With <code>conda</code> installed, open <em>Anaconda Prompt</em> and type</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install dependencies
conda install conda-forge::openblas conda-forge::flann

# Get conda prefix
echo %CONDA_PREFIX%"><pre><span><span>#</span> Install dependencies</span>
conda install conda-forge::openblas conda-forge::flann

<span><span>#</span> Get conda prefix</span>
<span>echo</span> %CONDA_PREFIX%</pre></div>
<p dir="auto"><code>CONDA_PREFIX</code> will give you the prefix for your conda installation. Keep it around.</p>
<p dir="auto">After that, the following will build the wheel and place it inside a <em>dist</em> folder.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone repository
git clone https://github.com/big-nacho/patolette.git
cd patolette

# If CONDA_PREFIX = C:\miniconda3 then replace with C:\\miniconda3 (use double backslashes)
$env:CMAKE_ARGS = &#34;-DCMAKE_PREFIX_PATH={CONDA_PREFIX}\\Library&#34;

# Optional: set OPT_LEVEL (check Note for x86 section)
$env:CMAKE_ARGS = $env:CMAKE_ARGS + &#34; &#34; + &#34;-DOPT_LEVEL=avx2&#34;

# Install build module
pip install build

# Build wheel
python -m build"><pre><span><span>#</span> Clone repository</span>
git clone https:<span>//</span><span>github.com</span><span>/</span>big<span>-</span>nacho<span>/</span>patolette.git
cd patolette

<span><span>#</span> If CONDA_PREFIX = C:\miniconda3 then replace with C:\\miniconda3 (use double backslashes)</span>
<span>$<span>env:</span>CMAKE_ARGS</span> <span>=</span> <span><span>&#34;</span>-DCMAKE_PREFIX_PATH={CONDA_PREFIX}\\Library<span>&#34;</span></span>

<span><span>#</span> Optional: set OPT_LEVEL (check Note for x86 section)</span>
<span>$<span>env:</span>CMAKE_ARGS</span> <span>=</span> <span>$<span>env:</span>CMAKE_ARGS</span> <span>+</span> <span><span>&#34;</span> <span>&#34;</span></span> <span>+</span> <span><span>&#34;</span>-DOPT_LEVEL=avx2<span>&#34;</span></span>

<span><span>#</span> Install build module</span>
pip install build

<span><span>#</span> Build wheel</span>
python <span>-</span>m build</pre></div>
<p dir="auto">Now, you can&#39;t just install that wheel, because <em>.dll</em> files won&#39;t be found at runtime. First you need to repair it. The following will repair and install the built wheel in your currently active virtual environment.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
# Install delvewheel
pip install delvewheel

# Repair wheel
delvewheel repair --add-path {CONDA_PREFIX}\\Library\\bin dist\\*.whl

# Install repaired wheel
pip install wheelhouse\\{WHEEL_NAME}.whl"><pre><span><span>#</span> Install delvewheel</span>
pip install delvewheel

<span><span>#</span> Repair wheel</span>
delvewheel repair <span>--</span><span>add-path</span> {CONDA_PREFIX}\\Library\\bin dist\\<span>*</span>.whl

<span><span>#</span> Install repaired wheel</span>
pip install wheelhouse\\{WHEEL_NAME}.whl</pre></div>

<p dir="auto">The library doesn&#39;t take care of image decoding / encoding. You need to do that yourself. In the below example the <a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">Pillow</a> library is used, but you can use whatever you want.</p>
<p dir="auto">You can find more details on each parameter in the docstrings for the <code>quantize</code> function.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import numpy as np
from PIL import Image
from patolette import quantize, ColorSpace_ICtCp

path = &#39;image.png&#39;

# Read image
img = Image.open(path)
img = img.convert(mode = &#39;RGB&#39;)
width = img.width
height = img.height
img = np.asarray(img)

# Get colors (they should be in sRGB[0, 1] color space)
colors = img.reshape((-1, 3)).astype(np.float64)
colors /= 255

# Quantize
success, palette, palette_map, message = quantize(
    width,
    height,
    colors,
    256,
    # If you want progress console output
    verbose=True,
    # The following are all defaults
    dither=True,
    palette_only=False,
    color_space=ColorSpace_ICtCp,
    tile_size=512,
    kmeans_niter=32,
    kmeans_max_samples=512 ** 2
)

if not success:
    print(message)
    exit()

# Palette is returned in sRGB[0, 1] color space
palette *= 255
palette = np.clip(palette, 0, 255)
palette = palette.astype(np.uint8)

# Save result
# NOTE: if you&#39;re trying to reduce file size,
# you may want to save a palette-based PNG instead
quantized = palette[palette_map].reshape(img.shape)
quantized = Image.fromarray(quantized)
quantized.save(&#39;result.png&#39;)"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>from</span> <span>patolette</span> <span>import</span> <span>quantize</span>, <span>ColorSpace_ICtCp</span>

<span>path</span> <span>=</span> <span>&#39;image.png&#39;</span>

<span># Read image</span>
<span>img</span> <span>=</span> <span>Image</span>.<span>open</span>(<span>path</span>)
<span>img</span> <span>=</span> <span>img</span>.<span>convert</span>(<span>mode</span> <span>=</span> <span>&#39;RGB&#39;</span>)
<span>width</span> <span>=</span> <span>img</span>.<span>width</span>
<span>height</span> <span>=</span> <span>img</span>.<span>height</span>
<span>img</span> <span>=</span> <span>np</span>.<span>asarray</span>(<span>img</span>)

<span># Get colors (they should be in sRGB[0, 1] color space)</span>
<span>colors</span> <span>=</span> <span>img</span>.<span>reshape</span>((<span>-</span><span>1</span>, <span>3</span>)).<span>astype</span>(<span>np</span>.<span>float64</span>)
<span>colors</span> <span>/=</span> <span>255</span>

<span># Quantize</span>
<span>success</span>, <span>palette</span>, <span>palette_map</span>, <span>message</span> <span>=</span> <span>quantize</span>(
    <span>width</span>,
    <span>height</span>,
    <span>colors</span>,
    <span>256</span>,
    <span># If you want progress console output</span>
    <span>verbose</span><span>=</span><span>True</span>,
    <span># The following are all defaults</span>
    <span>dither</span><span>=</span><span>True</span>,
    <span>palette_only</span><span>=</span><span>False</span>,
    <span>color_space</span><span>=</span><span>ColorSpace_ICtCp</span>,
    <span>tile_size</span><span>=</span><span>512</span>,
    <span>kmeans_niter</span><span>=</span><span>32</span>,
    <span>kmeans_max_samples</span><span>=</span><span>512</span> <span>**</span> <span>2</span>
)

<span>if</span> <span>not</span> <span>success</span>:
    <span>print</span>(<span>message</span>)
    <span>exit</span>()

<span># Palette is returned in sRGB[0, 1] color space</span>
<span>palette</span> <span>*=</span> <span>255</span>
<span>palette</span> <span>=</span> <span>np</span>.<span>clip</span>(<span>palette</span>, <span>0</span>, <span>255</span>)
<span>palette</span> <span>=</span> <span>palette</span>.<span>astype</span>(<span>np</span>.<span>uint8</span>)

<span># Save result</span>
<span># NOTE: if you&#39;re trying to reduce file size,</span>
<span># you may want to save a palette-based PNG instead</span>
<span>quantized</span> <span>=</span> <span>palette</span>[<span>palette_map</span>].<span>reshape</span>(<span>img</span>.<span>shape</span>)
<span>quantized</span> <span>=</span> <span>Image</span>.<span>fromarray</span>(<span>quantized</span>)
<span>quantized</span>.<span>save</span>(<span>&#39;result.png&#39;</span>)</pre></div>

<p dir="auto">Three different color spaces are supported for the palette generation step. The following are rules of thumb you can go by, but experiment and see what works best for you.</p>
<p dir="auto"><strong>CIEL*u*v*</strong>: generates exceptionally high quality color palettes and it&#39;s the best choice for very low color counts. However, it creates the least smooth results and performs poorly on some hues.</p>
<p dir="auto"><strong>sRGB</strong>: outputs relatively smooth results (and it&#39;s the most consistent in this regard) but it generates the lowest quality color palettes and it&#39;s not that well suited for lower color counts.</p>
<p dir="auto"><strong>ICtCp</strong> (default): a good tradeoff between the two former. Generally, it generates slightly smoother results than <strong>sRGB</strong>, but it&#39;s a little bit less consistent, and the quality of the color palettes it generates is quite good.</p>

<p dir="auto"><em>patolette</em> optimizes against size-weighted variance during the palette generation stage (the same way <em>KMeans</em> and other quantizers of similar nature do). This however comes with the known issue of large clusters dominating small, well defined ones.</p>
<p dir="auto">The <code>tile_size</code> parameter can be used to mitigate this issue. When non-zero, an extra step is introduced in the pipeline. A <a href="https://en.wikipedia.org/wiki/Saliency_map#:~:text=In%20computer%20vision%2C%20a%20saliency,an%20otherwise%20opaque%20ML%20model." rel="nofollow">saliency map</a> is computed and used to weight samples based on how much they stand out visually. The lower the tile size, the stronger the effect. The default tile size is <code>512</code>.</p>
<p dir="auto">Below is a quick showcase.<br/></p>
<p dir="auto"><em>top-left</em>: input image</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67847653/452618755-8ad4784c-bfc6-4d0d-8130-acf8e5e4ebf2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI2MTg3NTUtOGFkNDc4NGMtYmZjNi00ZDBkLTgxMzAtYWNmOGU1ZTRlYmYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdhMzI1ZmZmZjI3YTg5MWUxNTE0YTE5YWRlZjYzN2FhZWVkMWIzOTYyZjQwNTIzYzA1NTRmM2E0MGY5ZjkzMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NlbhUWlnI45ztIhLfYW62Nz4QQcGa_RjFCPirc01hn8"><img width="100%" src="https://private-user-images.githubusercontent.com/67847653/452618755-8ad4784c-bfc6-4d0d-8130-acf8e5e4ebf2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI2MTg3NTUtOGFkNDc4NGMtYmZjNi00ZDBkLTgxMzAtYWNmOGU1ZTRlYmYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdhMzI1ZmZmZjI3YTg5MWUxNTE0YTE5YWRlZjYzN2FhZWVkMWIzOTYyZjQwNTIzYzA1NTRmM2E0MGY5ZjkzMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NlbhUWlnI45ztIhLfYW62Nz4QQcGa_RjFCPirc01hn8"/></a>
</p>


<p dir="auto">The main priority for <code>v1</code> is to reduce memory consumption, at the moment it is quite high. If you limit yourself to quantizing images up to <strong>4k</strong> resolution you&#39;re on the pretty safe side, but if you go above <strong>6k</strong> you may start going into the danger zone depending on your system. Below is a chart depicting memory usage for different resolutions (<em>including</em> the space needed for the storage of the input image).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67847653/452617111-7c2800cd-9334-431c-89aa-e29548346c0c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI2MTcxMTEtN2MyODAwY2QtOTMzNC00MzFjLTg5YWEtZTI5NTQ4MzQ2YzBjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU2MzZjNjE1Y2YzZGEyOWEyNDkwYzY2ZmFmZjBhZDUwYzUyNzk5MGU2OWFmYWEzZDEzY2Y3MzQ0NDVmYzhhZTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9gis957Xbv3x0QsYgnkLW1LCdPqmHS5-2F4OxsP0V7s"><img src="https://private-user-images.githubusercontent.com/67847653/452617111-7c2800cd-9334-431c-89aa-e29548346c0c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTI2MTcxMTEtN2MyODAwY2QtOTMzNC00MzFjLTg5YWEtZTI5NTQ4MzQ2YzBjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU2MzZjNjE1Y2YzZGEyOWEyNDkwYzY2ZmFmZjBhZDUwYzUyNzk5MGU2OWFmYWEzZDEzY2Y3MzQ0NDVmYzhhZTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9gis957Xbv3x0QsYgnkLW1LCdPqmHS5-2F4OxsP0V7s"/></a>
</p>

<p dir="auto">It&#39;s not nearly as fast as it could be yet, but will most likely stay slow compared to fast methods like median cut / octree, etc. Below is a chart with execution times for some resolutions, quantizing to 256 colors (ICtCp), <em>with</em> saliency maps and dithering on. Testing was performed on an 11 core Apple M3 Pro CPU.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67847653/453198659-b34d55cc-3778-4f90-a688-f81e8c5c8077.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTMxOTg2NTktYjM0ZDU1Y2MtMzc3OC00ZjkwLWE2ODgtZjgxZThjNWM4MDc3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5YmUyZWZmOTY3NTM4NThkNzM5YWM2YTdkZmU1YTcwZDc1MDAzZjk1OGVmYjk5ZDA5NDcyZjFhMjI0ZDk2YmMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.CXG9VlOivLgXVU6Os-xwGZNKM5J7pTxs0zh2noSuxDA"><img src="https://private-user-images.githubusercontent.com/67847653/453198659-b34d55cc-3778-4f90-a688-f81e8c5c8077.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1ODcxOTMsIm5iZiI6MTc0OTU4Njg5MywicGF0aCI6Ii82Nzg0NzY1My80NTMxOTg2NTktYjM0ZDU1Y2MtMzc3OC00ZjkwLWE2ODgtZjgxZThjNWM4MDc3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjEwVDIwMjEzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5YmUyZWZmOTY3NTM4NThkNzM5YWM2YTdkZmU1YTcwZDc1MDAzZjk1OGVmYjk5ZDA5NDcyZjFhMjI0ZDk2YmMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.CXG9VlOivLgXVU6Os-xwGZNKM5J7pTxs0zh2noSuxDA"/></a>
</p>

<p dir="auto">Until <code>v1</code> is ready, the C API is incomplete. Mainly, support for weighted quantization via saliency maps is not there, so you won&#39;t find a <code>tile_size</code> parameter. It does however allow you to supply your own weights if you want.</p>

<p dir="auto">For the time being, images with transparency are not supported, though if you have the pretty common use case of subject + fully transparent background, you can always fake it yourself by using a mask, but results may not be optimal.</p>

<p dir="auto">This library stands on the following works / projects.</p>
<p dir="auto"><em>Color Quantization by Dynamic Programming and Principal Analysis, Xiaolin Wu</em> <a href="https://dl.acm.org/doi/pdf/10.1145/146443.146475" rel="nofollow">[1]</a></p>
<p dir="auto"><em>Minimum Barrier Salient Object Detection at 80 FPS, Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Brian Price, Random√≠r Mech</em> <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Minimum_Barrier_Salient_ICCV_2015_paper.pdf" rel="nofollow">[2]</a></p>
<p dir="auto"><em>Riemersma Dithering</em> <a href="https://www.compuphase.com/riemer.htm" rel="nofollow">[3]</a></p>
<p dir="auto"><a href="https://github.com/facebookresearch/faiss">faiss</a></p>
<p dir="auto"><a href="https://github.com/flann-lib/flann">flann</a></p>
<p dir="auto"><a href="https://github.com/OpenMathLib/OpenBLAS">OpenBLAS</a></p>
<hr/>
<p dir="auto">Thanks, Anna ‚ù§Ô∏è</p>
</article></div></div>
  </body>
</html>
