<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.swtch.com/boolean">Original</a>
    <h1>Minimal Boolean Formulas (2011)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        
       
<p>
<a href="http://oeis.org/A056287">28</a>.
That&#39;s the minimum number of AND or OR operators
you need in order to write any Boolean function of five variables.
<a href="http://alexhealy.net/">Alex Healy</a> and I computed that in April 2010.  Until then,
I believe no one had ever known that little fact.
This post describes how we computed it
and how we almost got scooped by <a href="http://research.swtch.com/2011/01/knuth-volume-4a.html">Knuth&#39;s Volume 4A</a>
which considers the problem for AND, OR, and XOR.
</p>

<h3>A Naive Brute Force Approach</h3>

<p>
Any Boolean function of two variables
can be written with at most 3 AND or OR operators: the parity function
on two variables X XOR Y is (X AND Y&#39;) OR (X&#39; AND Y), where X&#39; denotes
“not X.”  We can shorten the notation by writing AND and OR
like multiplication and addition: X XOR Y = X*Y&#39; + X&#39;*Y.
</p>

<p>
For three variables, parity is also a hardest function, requiring 9 operators:
X XOR Y XOR Z = (X*Z&#39;+X&#39;*Z+Y&#39;)*(X*Z+X&#39;*Z&#39;+Y).
</p>

<p>
For four variables, parity is still a hardest function, requiring 15 operators:
W XOR X XOR Y XOR Z = (X*Z&#39;+X&#39;*Z+W&#39;*Y+W*Y&#39;)*(X*Z+X&#39;*Z&#39;+W*Y+W&#39;*Y&#39;).
</p>

<p>
The sequence so far prompts a few questions.  Is parity always a hardest function?
Does the minimum number of operators alternate between 2<sup>n</sup>−1 and 2<sup>n</sup>+1?
</p>

<p>
I computed these results in January 2001 after hearing
the problem from Neil Sloane, who suggested it as a variant
of a similar problem first studied by Claude Shannon.
</p>

<p>
The program I wrote to compute a(4) computes the minimum number of
operators for every Boolean function of n variables
in order to find the largest minimum over all functions.
There are 2<sup>4</sup> = 16 settings of four variables, and each function
can pick its own value for each setting, so there are 2<sup>16</sup> different
functions.  To make matters worse, you build new functions
by taking pairs of old functions and joining them with AND or OR.
2<sup>16</sup> different functions means 2<sup>16</sup>·2<sup>16</sup> = 2<sup>32</sup> pairs of functions.
</p>

<p>
The program I wrote was a mangling of the Floyd-Warshall
all-pairs shortest paths algorithm.  That algorithm is:
</p>

<pre>// Floyd-Warshall all pairs shortest path
func compute():
    for each node i
        for each node j
            dist[i][j] = direct distance, or ∞

    for each node k
        for each node i
            for each node j
                d = dist[i][k] + dist[k][j]
                if d &lt; dist[i][j]
                    dist[i][j] = d
    return
</pre>

<p>
The algorithm begins with the distance table dist[i][j] set to
an actual distance if i is connected to j and infinity otherwise.
Then each round updates the table to account for paths
going through the node k: if it&#39;s shorter to go from i to k to j,
it saves that shorter distance in the table.  The nodes are
numbered from 0 to n, so the variables i, j, k are simply integers.
Because there are only n nodes, we know we&#39;ll be done after
the outer loop finishes.
</p>

<p>
The program I wrote to find minimum Boolean formula sizes is
an adaptation, substituting formula sizes for distance.
</p>

<pre>// Algorithm 1
func compute()
    for each function f
        size[f] = ∞

    for each single variable function f = v
        size[f] = 0

    loop
        changed = false
        for each function f
            for each function g
                d = size[f] + 1 + size[g]
                if d &lt; size[f OR g]
                    size[f OR g] = d
                    changed = true
                if d &lt; size[f AND g]
                    size[f AND g] = d
                    changed = true
        if not changed
            return
</pre>

<p>
Algorithm 1 runs the same kind of iterative update loop as the Floyd-Warshall algorithm,
but it isn&#39;t as obvious when you can stop, because you don&#39;t
know the maximum formula size beforehand.
So it runs until a round doesn&#39;t find any new functions to make,
iterating until it finds a fixed point.
</p>

<p>
The pseudocode above glosses over some details, such as
the fact that the per-function loops can iterate over a
queue of functions known to have finite size, so that each
loop omits the functions that aren&#39;t
yet known.  That&#39;s only a constant factor improvement,
but it&#39;s a useful one.
</p>

<p>
Another important detail missing above
is the representation of functions.  The most convenient
representation is a binary truth table.
For example,
if we are computing the complexity of two-variable functions,
there are four possible inputs, which we can number as follows.
</p>

<center>
<table>
<tbody><tr><th>X </th><th>Y </th><th>Value
</th></tr><tr><td>false </td><td>false </td><td>00<sub>2</sub> = 0
</td></tr><tr><td>false </td><td>true </td><td>01<sub>2</sub> = 1
</td></tr><tr><td>true </td><td>false </td><td>10<sub>2</sub> = 2
</td></tr><tr><td>true </td><td>true </td><td>11<sub>2</sub> = 3
</td></tr></tbody></table>
</center>

<p>
The functions are then the 4-bit numbers giving the value of the
function for each input.  For example, function 13 = 1101<sub>2</sub>
is true for all inputs except X=false Y=true.
Three-variable functions correspond to 3-bit inputs generating 8-bit truth tables,
and so on.
</p>

<p>
This representation has two key advantages.  The first is that
the numbering is dense, so that you can implement a map keyed
by function using a simple array.  The second is that the operations
“f AND g” and “f OR g” can be implemented using
bitwise operators: the truth table for “f AND g” is the bitwise
AND of the truth tables for f and g.
</p>

<p>
That program worked well enough in 2001 to compute the
minimum number of operators necessary to write any
1-, 2-, 3-, and 4-variable Boolean function.  Each round
takes asymptotically O(2<sup>2<sup>n</sup></sup>·2<sup>2<sup>n</sup></sup>) = O(2<sup>2<sup>n+1</sup></sup>) time, and the number of
rounds needed is O(the final answer).  The answer for n=4
is 15, so the computation required on the order of
15·2<sup>2<sup>5</sup></sup> = 15·2<sup>32</sup> iterations of the innermost loop.
That was plausible on the computer I was using at
the time, but the answer for n=5, likely around 30,
would need 30·2<sup>64</sup> iterations to compute, which
seemed well out of reach.
At the time, it seemed plausible that parity was always
a hardest function and that the minimum size would
continue to alternate between 2<sup>n</sup>−1 and 2<sup>n</sup>+1.
It&#39;s a nice pattern.
</p>

<h3>Exploiting Symmetry</h3>

<p>
Five years later, though, Alex Healy and I got to talking about this sequence,
and Alex shot down both conjectures using results from the theory
of circuit complexity.  (Theorists!)  Neil Sloane added this note to
the <a href="http://oeis.org/history?seq=A056287">entry for the sequence</a> in his Online Encyclopedia of Integer Sequences:
</p>

<blockquote>
<tt>
%E A056287 Russ Cox conjectures that X<sub>1</sub> XOR ... XOR X<sub>n</sub> is always a worst f and that a(5) = 33 and a(6) = 63. But (Jan 27 2006) Alex Healy points out that this conjecture is definitely false for large n. So what is a(5)?
</tt>
</blockquote>

<p>
Indeed.  What is a(5)?  No one knew, and it wasn&#39;t obvious how to find out.
</p>

<p>
In January 2010, Alex and I started looking into ways to
speed up the computation for a(5).  30·2<sup>64</sup> is too many
iterations but maybe we could find ways to cut that number.
</p>

<p>
In general, if we can identify a class of functions f whose
members are guaranteed to have the same complexity,
then we can save just one representative of the class as
long as we recreate the entire class in the loop body.
What used to be:
</p>

<pre>for each function f
    for each function g
        visit f AND g
        visit f OR g
</pre>

<p>
can be rewritten as
</p>

<pre>for each canonical function f
    for each canonical function g
        for each ff equivalent to f
            for each gg equivalent to g
                visit ff AND gg
                visit ff OR gg
</pre>

<p>
That doesn&#39;t look like an improvement: it&#39;s doing all
the same work.  But it can open the door to new optimizations
depending on the equivalences chosen.
For example, the functions “f” and “¬f” are guaranteed
to have the same complexity, by <a href="http://en.wikipedia.org/wiki/De_Morgan&#39;s_laws">DeMorgan&#39;s laws</a>.
If we keep just one of
those two on the lists that “for each function” iterates over,
we can unroll the inner two loops, producing:
</p>

<pre>for each canonical function f
    for each canonical function g
        visit f OR g
        visit f AND g
        visit ¬f OR g
        visit ¬f AND g
        visit f OR ¬g
        visit f AND ¬g
        visit ¬f OR ¬g
        visit ¬f AND ¬g
</pre>

<p>
That&#39;s still not an improvement, but it&#39;s no worse.
Each of the two loops considers half as many functions
but the inner iteration is four times longer.
Now we can notice that half of tests aren&#39;t
worth doing: “f AND g” is the negation of
“¬f OR ¬g,” and so on, so only half
of them are necessary.
</p>

<p>
Let&#39;s suppose that when choosing between “f” and “¬f”
we keep the one that is false when presented with all true inputs.
(This has the nice property that <code>f ^ (int32(f) &gt;&gt; 31)</code>
is the truth table for the canonical form of <code>f</code>.)
Then we can tell which combinations above will produce
canonical functions when f and g are already canonical:
</p>

<pre>for each canonical function f
    for each canonical function g
        visit f OR g
        visit f AND g
        visit ¬f AND g
        visit f AND ¬g
</pre>

<p>
That&#39;s a factor of two improvement over the original loop.
</p>

<p>
Another observation is that permuting
the inputs to a function doesn&#39;t change its complexity:
“f(V, W, X, Y, Z)” and “f(Z, Y, X, W, V)” will have the same
minimum size.  For complex functions, each of the
5! = 120 permutations will produce a different truth table.
A factor of 120 reduction in storage is good but again
we have the problem of expanding the class in the
iteration.  This time, there&#39;s a different trick for reducing
the work in the innermost iteration.
Since we only need to produce one member of
the equivalence class, it doesn&#39;t make sense to
permute the inputs to both f and g.  Instead,
permuting just the inputs to f while fixing g
is guaranteed to hit at least one member
of each class that permuting both f and g would.
So we gain the factor of 120 twice in the loops
and lose it once in the iteration, for a net savings
of 120.
(In some ways, this is the same trick we did with “f” vs “¬f.”)
</p>

<p>
A final observation is that negating any of the inputs
to the function doesn&#39;t change its complexity,
because X and X&#39; have the same complexity.
The same argument we used for permutations applies
here, for another constant factor of 2<sup>5</sup> = 32.
</p>

<p>
The code stores a single function for each equivalence class
and then recomputes the equivalent functions for f, but not g.
</p>

<pre>for each canonical function f
    for each function ff equivalent to f
        for each canonical function g
            visit ff OR g
            visit ff AND g
            visit ¬ff AND g
            visit ff AND ¬g
</pre>

<p>
In all, we just got a savings of 2·120·32 = 7680,
cutting the total number of iterations from 30·2<sup>64</sup> = 5×10<sup>20</sup>
to 7×10<sup>16</sup>.  If you figure we can do around
10<sup>9</sup> iterations per second, that&#39;s still 800 days of CPU time.
</p>

<p>
The full algorithm at this point is:
</p>

<pre>// Algorithm 2
func compute():
    for each function f
        size[f] = ∞

    for each single variable function f = v
        size[f] = 0

    loop
        changed = false
        for each canonical function f
            for each function ff equivalent to f
                for each canonical function g
                    d = size[ff] + 1 + size[g]
                    changed |= visit(d, ff OR g)
                    changed |= visit(d, ff AND g)
                    changed |= visit(d, ff AND ¬g)
                    changed |= visit(d, ¬ff AND g)
        if not changed
            return

func visit(d, fg):
    if size[fg] != ∞
        return false

    record fg as canonical

    for each function ffgg equivalent to fg
        size[ffgg] = d
    return true
</pre>

<p>
The helper function “visit” must set the size not only of its argument fg
but also all equivalent functions under permutation or inversion of the inputs,
so that future tests will see that they have been computed.
</p>

<h3>Methodical Exploration</h3>

<p>
There&#39;s one final improvement we can make.
The approach of looping until things stop changing
considers each function pair multiple times
as their sizes go down.  Instead, we can consider functions
in order of complexity, so that the main loop
builds first all the functions of minimum complexity 1,
then all the functions of minimum complexity 2,
and so on.  If we do that, we&#39;ll consider each function pair at most once.
We can stop when all functions are accounted for.
</p>

<p>
Applying this idea to Algorithm 1 (before canonicalization) yields:
</p>

<pre>// Algorithm 3
func compute()
    for each function f
        size[f] = ∞

    for each single variable function f = v
        size[f] = 0

    for k = 1 to ∞
        for each function f
            for each function g of size k − size(f) − 1
                if size[f AND g] == ∞
                    size[f AND g] = k
                    nsize++
                if size[f OR g] == ∞
                    size[f OR g] = k
                    nsize++
        if nsize == 2<sup>2<sup>n</sup></sup>
            return
</pre>

<p>
Applying the idea to Algorithm 2 (after canonicalization) yields:
</p>

<pre>// Algorithm 4
func compute():
    for each function f
        size[f] = ∞

    for each single variable function f = v
        size[f] = 0

    for k = 1 to ∞
        for each canonical function f
            for each function ff equivalent to f
                for each canonical function g of size k − size(f) − 1
                    visit(k, ff OR g)
                    visit(k, ff AND g)
                    visit(k, ff AND ¬g)
                    visit(k, ¬ff AND g)
        if nvisited == 2<sup>2<sup>n</sup></sup>
            return

func visit(d, fg):
    if size[fg] != ∞
        return

    record fg as canonical

    for each function ffgg equivalent to fg
        if size[ffgg] != ∞
            size[ffgg] = d
            nvisited += 2  // counts ffgg and ¬ffgg
    return
</pre>

<p>
The original loop in Algorithms 1 and 2 considered each pair f, g in every
iteration of the loop after they were computed.
The new loop in Algorithms 3 and 4 considers each pair f, g only once,
when k = size(f) + size(g) + 1.  This removes the
leading factor of 30 (the number of times we
expected the first loop to run) from our estimation
of the run time.
Now the expected number of iterations is around
2<sup>64</sup>/7680 = 2.4×10<sup>15</sup>.  If we can do 10<sup>9</sup> iterations
per second, that&#39;s only 28 days of CPU time,
which I can deliver if you can wait a month.
</p>

<p>
Our estimate does not include the fact that not all function pairs need
to be considered.  For example, if the maximum size is 30, then the
functions of size 14 need never be paired against the functions of size 16,
because any result would have size 14+1+16 = 31.
So even 2.4×10<sup>15</sup> is an overestimate, but it&#39;s in the right ballpark.
(With hindsight I can report that only 1.7×10<sup>14</sup> pairs need to be considered
but also that our estimate of 10<sup>9</sup> iterations
per second was optimistic.  The actual calculation ran for 20 days,
an average of about 10<sup>8</sup> iterations per second.)
</p>

<h3>Endgame: Directed Search</h3>

<p>
A month is still a long time to wait, and we can do better.
Near the end (after k is bigger than, say, 22), we are exploring
the fairly large space of function pairs in hopes of finding a
fairly small number of remaining functions.
At that point it makes sense to change from the
bottom-up “bang things together and see what we make”
to the top-down “try to make this one of these specific functions.”
That is, the core of the current search is:
</p>

<pre>for each canonical function f
    for each function ff equivalent to f
        for each canonical function g of size k − size(f) − 1
            visit(k, ff OR g)
            visit(k, ff AND g)
            visit(k, ff AND ¬g)
            visit(k, ¬ff AND g)
</pre>

<p>
We can change it to:
</p>

<pre>for each missing function fg
    for each canonical function g
        for all possible f such that one of these holds
                * fg = f OR g
                * fg = f AND g
                * fg = ¬f AND g
                * fg = f AND ¬g
            if size[f] == k − size(g) − 1
                visit(k, fg)
                next fg
</pre>

<p>
By the time we&#39;re at the end, exploring all the possible f to make
the missing functions—a directed search—is much less work than
the brute force of exploring all combinations.
</p>

<p>
As an example, suppose we are looking for f such that fg = f OR g.
The equation is only possible to satisfy if fg OR g == fg.
That is, if g has any extraneous 1 bits, no f will work, so we can move on.
Otherwise, the remaining condition is that
f AND ¬g == fg AND ¬g.  That is, for the bit positions where g is 0, f must match fg.
The other bits of f (the bits where g has 1s)
can take any value.
We can enumerate the possible f values by recursively trying all
possible values for the “don&#39;t care” bits.
</p>

<pre>func find(x, any, xsize):
    if size(x) == xsize
        return x
    while any != 0
        bit = any AND −any  // rightmost 1 bit in any
        any = any AND ¬bit
        if f = find(x OR bit, any, xsize) succeeds
            return f
    return failure
</pre>

<p>
It doesn&#39;t matter which 1 bit we choose for the recursion,
but finding the rightmost 1 bit is cheap: it is isolated by the
(admittedly surprising) expression “any AND −any.”
</p>

<p>
Given <code>find</code>, the loop above can try these four cases:
</p>

<center>
<table id="find">
<tbody><tr><th>Formula </th><th>Condition </th><th>Base x </th><th>“Any” bits
</th></tr><tr><td>fg = f OR g </td><td>fg OR g == fg </td><td>fg AND ¬g </td><td>g
</td></tr><tr><td>fg = f OR ¬g </td><td>fg OR ¬g == fg </td><td>fg AND g </td><td>¬g
</td></tr><tr><td>¬fg = f OR g </td><td>¬fg OR g == fg </td><td>¬fg AND ¬g </td><td>g
</td></tr><tr><td>¬fg = f OR ¬g </td><td>¬fg OR ¬g == ¬fg </td><td>¬fg AND g </td><td>¬g
</td></tr></tbody></table>
</center>

<p>
Rewriting the Boolean expressions to use only the four OR forms
means that we only need to write the “adding bits” version of find.
</p>

<p>
The final algorithm is:
</p>

<pre>// Algorithm 5
func compute():
    for each function f
        size[f] = ∞

    for each single variable function f = v
        size[f] = 0

    // Generate functions.
    for k = 1 to max_generate
        for each canonical function f
            for each function ff equivalent to f
                for each canonical function g of size k − size(f) − 1
                    visit(k, ff OR g)
                    visit(k, ff AND g)
                    visit(k, ff AND ¬g)
                    visit(k, ¬ff AND g)

    // Search for functions.
    for k = max_generate+1 to ∞
        for each missing function fg
            for each canonical function g
                fsize = k − size(g) − 1
                if fg OR g == fg
                    if f = find(fg AND ¬g, g, fsize) succeeds
                        visit(k, fg)
                        next fg
                if fg OR ¬g == fg
                    if f = find(fg AND g, ¬g, fsize) succeeds
                        visit(k, fg)
                        next fg
                if ¬fg OR g == ¬fg
                    if f = find(¬fg AND ¬g, g, fsize) succeeds
                        visit(k, fg)
                        next fg
                if ¬fg OR ¬g == ¬fg
                    if f = find(¬fg AND g, ¬g, fsize) succeeds
                        visit(k, fg)
                        next fg
        if nvisited == 2<sup>2<sup>n</sup></sup>
            return

func visit(d, fg):
    if size[fg] != ∞
        return

    record fg as canonical

    for each function ffgg equivalent to fg
        if size[ffgg] != ∞
            size[ffgg] = d
            nvisited += 2  // counts ffgg and ¬ffgg
    return

func find(x, any, xsize):
    if size(x) == xsize
        return x
    while any != 0
        bit = any AND −any  // rightmost 1 bit in any
        any = any AND ¬bit
        if f = find(x OR bit, any, xsize) succeeds
            return f
    return failure
</pre>

<p>
To get a sense of the speedup here, and to check my work,
I ran the program using both algorithms
on a 2.53 GHz Intel Core 2 Duo E7200.
</p>


<center>
<table id="times">
<tbody><tr><th> </th><th colspan="3">————— # of Functions —————</th><th colspan="2">———— Time ————
</th></tr><tr><th>Size </th><th>Canonical </th><th>All </th><th>All, Cumulative </th><th>Generate </th><th>Search
</th></tr><tr><td>0 </td><td>1 </td><td>10 </td><td>10
</td></tr><tr><td>1 </td><td>2 </td><td>82 </td><td>92 </td><td>&lt; 0.1 seconds </td><td>3.4 minutes
</td></tr><tr><td>2 </td><td>2 </td><td>640 </td><td>732 </td><td>&lt; 0.1 seconds </td><td>7.2 minutes
</td></tr><tr><td>3 </td><td>7 </td><td>4420 </td><td>5152 </td><td>&lt; 0.1 seconds </td><td>12.3 minutes
</td></tr><tr><td>4 </td><td>19 </td><td>25276 </td><td>29696 </td><td>&lt; 0.1 seconds </td><td>30.1 minutes
</td></tr><tr><td>5 </td><td>44 </td><td>117440 </td><td>147136 </td><td>&lt; 0.1 seconds </td><td>1.3 hours
</td></tr><tr><td>6 </td><td>142 </td><td>515040 </td><td>662176 </td><td>&lt; 0.1 seconds </td><td>3.5 hours
</td></tr><tr><td>7 </td><td>436 </td><td>1999608 </td><td>2661784 </td><td>0.2 seconds </td><td>11.6 hours
</td></tr><tr><td>8 </td><td>1209 </td><td>6598400 </td><td>9260184 </td><td>0.6 seconds </td><td>1.7 days
</td></tr><tr><td>9 </td><td>3307 </td><td>19577332 </td><td>28837516 </td><td>1.7 seconds </td><td>4.9 days
</td></tr><tr><td>10 </td><td>7741 </td><td>50822560 </td><td>79660076 </td><td>4.6 seconds </td><td>[ 10 days ? ]
</td></tr><tr><td>11 </td><td>17257 </td><td>114619264 </td><td>194279340 </td><td>10.8 seconds </td><td>[ 20 days ? ]
</td></tr><tr><td>12 </td><td>31851 </td><td>221301008 </td><td>415580348 </td><td>21.7 seconds </td><td>[ 50 days ? ]
</td></tr><tr><td>13 </td><td>53901 </td><td>374704776 </td><td>790285124 </td><td>38.5 seconds </td><td>[ 80 days ? ]
</td></tr><tr><td>14 </td><td>75248 </td><td>533594528 </td><td>1323879652 </td><td>58.7 seconds </td><td>[ 100 days ? ]
</td></tr><tr><td>15 </td><td>94572 </td><td>667653642 </td><td>1991533294 </td><td>1.5 minutes </td><td>[ 120 days ? ]
</td></tr><tr><td>16 </td><td>98237 </td><td>697228760 </td><td>2688762054 </td><td>2.1 minutes </td><td>[ 120 days ? ]
</td></tr><tr><td>17 </td><td>89342 </td><td>628589440 </td><td>3317351494 </td><td>4.1 minutes </td><td>[ 90 days ? ]
</td></tr><tr><td>18 </td><td>66951 </td><td>468552896 </td><td>3785904390 </td><td>9.1 minutes </td><td>[ 50 days ? ]
</td></tr><tr><td>19 </td><td>41664 </td><td>287647616 </td><td>4073552006 </td><td>23.4 minutes </td><td>[ 30 days ? ]
</td></tr><tr><td>20 </td><td>21481 </td><td>144079832 </td><td>4217631838 </td><td>57.0 minutes </td><td>[ 10 days ? ]
</td></tr><tr><td>21 </td><td>8680 </td><td>55538224 </td><td>4273170062 </td><td>2.4 hours </td><td>2.5 days
</td></tr><tr><td>22 </td><td>2730 </td><td>16099568 </td><td>4289269630 </td><td>5.2 hours </td><td>11.7 hours
</td></tr><tr><td>23 </td><td>937 </td><td>4428800 </td><td>4293698430 </td><td>11.2 hours </td><td>2.2 hours
</td></tr><tr><td>24 </td><td>228 </td><td>959328 </td><td>4294657758 </td><td>22.0 hours </td><td>33.2 minutes
</td></tr><tr><td>25 </td><td>103 </td><td>283200 </td><td>4294940958 </td><td>1.7 days </td><td>4.0 minutes
</td></tr><tr><td>26 </td><td>21 </td><td>22224 </td><td>4294963182 </td><td>2.9 days </td><td>42 seconds
</td></tr><tr><td>27 </td><td>10 </td><td>3602 </td><td>4294966784 </td><td>4.7 days </td><td>2.4 seconds
</td></tr><tr><td>28 </td><td>3 </td><td>512 </td><td>4294967296 </td><td>[ 7 days ? ] </td><td>0.1 seconds
</td></tr></tbody></table>
</center>

<p>
The bracketed times are estimates based on the work involved: I did not
wait that long for the intermediate search steps.
The search algorithm is quite a bit worse than generate until there are
very few functions left to find.
However, it comes in handy just when it is most useful: when the
generate algorithm has slowed to a crawl.
If we run generate through formulas of size 22 and then switch
to search for 23 onward, we can run the whole computation in
just over half a day of CPU time.
</p>

<p>
The computation of a(5) identified the sizes of all 616,126
canonical Boolean functions of 5 inputs.
In contrast, there are <a href="http://oeis.org/A000370">just over 200 trillion canonical Boolean functions of 6 inputs</a>.
Determining a(6) is unlikely to happen by brute force computation, no matter what clever tricks we use.
</p>

<h3>Adding XOR</h3>

<p>We&#39;ve assumed the use of just AND and OR as our
basis for the Boolean formulas.  If we also allow XOR, functions
can be written using many fewer operators.
In particular, a hardest function for the 1-, 2-, 3-, and 4-input
cases—parity—is now trivial.
Knuth examines the complexity of 5-input Boolean functions
using AND, OR, and XOR in detail in <a href="http://www-cs-faculty.stanford.edu/~uno/taocp.html">The Art of Computer Programming, Volume 4A</a>.
Section 7.1.2&#39;s Algorithm L is the same as our Algorithm 3 above,
given for computing 4-input functions.
Knuth mentions that to adapt it for 5-input functions one must
treat only canonical functions and gives results for 5-input functions
with XOR allowed.
So another way to check our work is to add XOR to our Algorithm 4
and check that our results match Knuth&#39;s.
</p>

<p>
Because the minimum formula sizes are smaller (at most 12), the
computation of sizes with XOR is much faster than before:
</p>


<center>
<table>
<tbody><tr><th> </th><th></th><th colspan="5">————— # of Functions —————</th><th>
</th></tr><tr><th>Size </th><th></th><th>Canonical </th><th></th><th>All </th><th></th><th>All, Cumulative </th><th></th><th>Time
</th></tr><tr><td>0 </td><td></td><td>1 </td><td></td><td>10 </td><td></td><td>10 </td><td></td><td>
</td></tr><tr><td>1 </td><td></td><td>3 </td><td></td><td>102 </td><td></td><td>112 </td><td></td><td>&lt; 0.1 seconds
</td></tr><tr><td>2 </td><td></td><td>5 </td><td></td><td>1140 </td><td></td><td>1252 </td><td></td><td>&lt; 0.1 seconds
</td></tr><tr><td>3 </td><td></td><td>20 </td><td></td><td>11570 </td><td></td><td>12822 </td><td></td><td>&lt; 0.1 seconds
</td></tr><tr><td>4 </td><td></td><td>93 </td><td></td><td>109826 </td><td></td><td>122648 </td><td></td><td>&lt; 0.1 seconds
</td></tr><tr><td>5 </td><td></td><td>366 </td><td></td><td>936440 </td><td></td><td>1059088 </td><td></td><td>0.1 seconds
</td></tr><tr><td>6 </td><td></td><td>1730 </td><td></td><td>7236880 </td><td></td><td>8295968 </td><td></td><td>0.7 seconds
</td></tr><tr><td>7 </td><td></td><td>8782 </td><td></td><td>47739088 </td><td></td><td>56035056 </td><td></td><td>4.5 seconds
</td></tr><tr><td>8 </td><td></td><td>40297 </td><td></td><td>250674320 </td><td></td><td>306709376 </td><td></td><td>24.0 seconds
</td></tr><tr><td>9 </td><td></td><td>141422 </td><td></td><td>955812256 </td><td></td><td>1262521632 </td><td></td><td>95.5 seconds
</td></tr><tr><td>10 </td><td></td><td>273277 </td><td></td><td>1945383936 </td><td></td><td>3207905568 </td><td></td><td>200.7 seconds
</td></tr><tr><td>11 </td><td></td><td>145707 </td><td></td><td>1055912608 </td><td></td><td>4263818176 </td><td></td><td>121.2 seconds
</td></tr><tr><td>12 </td><td></td><td>4423 </td><td></td><td>31149120 </td><td></td><td>4294967296 </td><td></td><td>65.0 seconds
</td></tr></tbody></table>
</center>

<p>
Knuth does not discuss anything like Algorithm 5,
because the search for specific functions does not apply to
the AND, OR, and XOR basis.  XOR is a non-monotone
function (it can both turn bits on and turn bits off), so
there is no test like our “<code>if fg OR g == fg</code>”
and no small set of “don&#39;t care” bits to trim the search for f.
The search for an appropriate f in the XOR case would have
to try all f of the right size, which is exactly what Algorithm 4 already does.
</p>

<p>
Volume 4A also considers the problem of building minimal circuits,
which are like formulas but can use common subexpressions additional times for free,
and the problem of building the shallowest possible circuits.
See Section 7.1.2 for all the details.
</p>

<h3>Code and Web Site</h3>

<p>
The web site <a href="http://boolean-oracle.swtch.com">boolean-oracle.swtch.com</a>
lets you type in a Boolean expression and gives back the minimal formula for it.
It uses tables generated while running Algorithm 5; those tables and the
programs described in this post are also <a href="http://boolean-oracle.swtch.com/about">available on the site</a>.
</p>

<h3>Postscript: Generating All Permutations and Inversions</h3>

<p>
The algorithms given above depend crucially on the step
“<code>for each function ff equivalent to f</code>,”
which generates all the ff obtained by permuting or inverting inputs to f,
but I did not explain how to do that.
We already saw that we can manipulate the binary truth table representation
directly to turn <code>f</code> into <code>¬f</code> and to compute
combinations of functions.
We can also manipulate the binary representation directly to
invert a specific input or swap a pair of adjacent inputs.
Using those operations we can cycle through all the equivalent functions.
</p>

<p>
To invert a specific input,
let&#39;s consider the structure of the truth table.
The index of a bit in the truth table encodes the inputs for that entry.
For example, the low bit of the index gives the value of the first input.
So the even-numbered bits—at indices 0, 2, 4, 6, ...—correspond to
the first input being false, while the odd-numbered bits—at indices 1, 3, 5, 7, ...—correspond
to the first input being true.
Changing just that bit in the index corresponds to changing the
single variable, so indices 0, 1 differ only in the value of the first input,
as do 2, 3, and 4, 5, and 6, 7, and so on.
Given the truth table for f(V, W, X, Y, Z) we can compute
the truth table for f(¬V, W, X, Y, Z) by swapping adjacent bit pairs
in the original truth table.
Even better, we can do all the swaps in parallel using a bitwise
operation.
To invert a different input, we swap larger runs of bits.
</p>

<center>
<table>
<tbody><tr><th>Function </th><th> </th><th>Truth Table (<span><code>f</code> = f(V, W, X, Y, Z)</span>)
</th></tr><tr><td>f(¬V, W, X, Y, Z) </td><td></td><td><code>(f&amp;0x55555555)&lt;&lt; 1 | (f&gt;&gt; 1)&amp;0x55555555</code>
</td></tr><tr><td>f(V, ¬W, X, Y, Z) </td><td></td><td><code>(f&amp;0x33333333)&lt;&lt; 2 | (f&gt;&gt; 2)&amp;0x33333333</code>
</td></tr><tr><td>f(V, W, ¬X, Y, Z) </td><td></td><td><code>(f&amp;0x0f0f0f0f)&lt;&lt; 4 | (f&gt;&gt; 4)&amp;0x0f0f0f0f</code>
</td></tr><tr><td>f(V, W, X, ¬Y, Z) </td><td></td><td><code>(f&amp;0x00ff00ff)&lt;&lt; 8 | (f&gt;&gt; 8)&amp;0x00ff00ff</code>
</td></tr><tr><td>f(V, W, X, Y, ¬Z) </td><td></td><td><code>(f&amp;0x0000ffff)&lt;&lt;16 | (f&gt;&gt;16)&amp;0x0000ffff</code>
</td></tr></tbody></table>
</center>

<p>
Being able to invert a specific input lets us consider all possible
inversions by building them up one at a time.
The <a href="http://oeis.org/A003188">Gray code</a> lets us
enumerate all possible 5-bit input codes while changing only 1 bit at
a time as we move from one input to the next:
</p>

<center>
0, 1, 3, 2, 6, 7, 5, 4, </center>

<p>
This minimizes
the number of inversions we need: to consider all 32 cases, we only
need 31 inversion operations.
In contrast, visiting the 5-bit input codes in the usual binary order 0, 1, 2, 3, 4, ...
would often need to change multiple bits, like when changing from 3 to 4.
</p>

<p>
To swap a pair of adjacent inputs, we can again take advantage of the truth table.
For a pair of inputs, there are four cases: 00, 01, 10, and 11.  We can leave the
00 and 11 cases alone, because they are invariant under swapping,
and concentrate on swapping the 01 and 10 bits.
The first two inputs change most often in the truth table: each run of 4 bits
corresponds to those four cases.
In each run, we want to leave the first and fourth alone and swap the second and third.
For later inputs, the four cases consist of sections of bits instead of single bits.
</p>

<center>
<table>
<tbody><tr><th>Function </th><th> </th><th>Truth Table (<span><code>f</code> = f(V, W, X, Y, Z)</span>)
</th></tr><tr><td>f(<b>W, V</b>, X, Y, Z) </td><td></td><td><code>f&amp;0x99999999 | (f&amp;0x22222222)&lt;&lt;1 | (f&gt;&gt;1)&amp;0x22222222</code>
</td></tr><tr><td>f(V, <b>X, W</b>, Y, Z) </td><td></td><td><code>f&amp;0xc3c3c3c3 | (f&amp;0x0c0c0c0c)&lt;&lt;1 | (f&gt;&gt;1)&amp;0x0c0c0c0c</code>
</td></tr><tr><td>f(V, W, <b>Y, X</b>, Z) </td><td></td><td><code>f&amp;0xf00ff00f | (f&amp;0x00f000f0)&lt;&lt;1 | (f&gt;&gt;1)&amp;0x00f000f0</code>
</td></tr><tr><td>f(V, W, X, <b>Z, Y</b>) </td><td></td><td><code>f&amp;0xff0000ff | (f&amp;0x0000ff00)&lt;&lt;8 | (f&gt;&gt;8)&amp;0x0000ff00</code>
</td></tr></tbody></table>
</center>

<p>
Being able to swap a pair of adjacent inputs lets us consider all
possible permutations by building them up one at a time.
Again it is convenient to have a way to visit all permutations by
applying only one swap at a time.
Here Volume 4A comes to the rescue.
Section 7.2.1.2 is titled “Generating All Permutations,” and Knuth delivers
many algorithms to do just that.
The most convenient for our purposes is Algorithm P, which
generates a sequence that considers all permutations exactly once
with only a single swap of adjacent inputs between steps.
Knuth calls it Algorithm P because it corresponds to the
“Plain changes” algorithm used by <a href="http://en.wikipedia.org/wiki/Change_ringing">bell ringers in 17th century England</a>
to ring a set of bells in all possible permutations.
The algorithm is described in a manuscript written around 1653!
</p>

<p>
We can examine all possible permutations and inversions by
nesting a loop over all permutations inside a loop over all inversions,
and in fact that&#39;s what my program does.
Knuth does one better, though: his Exercise 7.2.1.2-20
suggests that it is possible to build up all the possibilities
using only adjacent swaps and inversion of the first input.
Negating arbitrary inputs is not hard, though, and still does
minimal work, so the code sticks with Gray codes and Plain changes.
</p>








      </div>
    </div></div>
  </body>
</html>
