<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://clipasso.github.io/clipasso/">Original</a>
    <h1>CLIPasso: Semantically-Aware Object Sketching</h1>
    
    <div id="readability-page-1" class="page">




<section>
  <div>
    <div>
      <div>
        <div>
          <!-- <h1 class="title is-1 publication-title">CLIPasso: Semantically-Aware Object Sketching</h1> -->
          

          <p><span><sup>1</sup>Swiss Federal Institute of Technology (EPFL), <sup>2</sup>Tel Aviv University, <sup>3</sup>Reichman University</span></p>

          

        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <!-- <div class="hero-body"> -->
<section>
  <div>
    <div>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
      <div>
      <div>
      <p><img src="https://clipasso.github.io/clipasso/static/figures/teaser2.png" alt="clipasso"/>
      </p>
      <h2>
        We introduce a method for performing <b>CLIP</b>-guided <b>S</b>emantically-<b>A</b>ware <b>O</b>bject <b>S</b>ketching, permuted in short as CLIPasso. </h2>
    </div>
  </div>
 <!--  </div> -->
  </div>
  </div>
 <!--  </div> -->
</section>

<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        
        <h2>Abstract</h2>
        <p>
            Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings.
            Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts.
            Abstract depictions are therefore challenging for artists, and even more so for machines.
            We present an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications.
            While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike.
            We define a sketch as a set of BÃ©zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss.
            The abstraction degree is controlled by varying the number of strokes.
            The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.
          </p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section>
  <div>
    <div>
      <h2>Object to Sketch Synthesis with Different Levels of Abstraction</h2>
      
</div>
</div>
</section>







<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>How does it work?</h2>
        <p>

            Our method is optimization-based and therefore does not require any explicit sketch dataset. 
            We use the CLIP image encoder to guide the process of converting a photograph to an abstract sketch.

            CLIP encoding provides the semantic understanding of the concept depicted, while the photograph itself provides the geometric grounding of the sketch to the concrete subject.

            We define a sketch as a set of N black strokes placed on a white background.
            We vary the number of strokes N to create different levels of abstraction. 
          </p>
        <p><img src="https://clipasso.github.io/clipasso/static/figures/pipeline_final.png" alt="cars peace"/>
      </p>
        <p>
            Given a target image I of the desired subject, our goal is to synthesize the corresponding sketch S while maintaining both the semantic and geometric attributes of the subject.
            We begin by extracting the salient regions of the input image to define the initial locations of the strokes.
            Next, in each step of the optimization we feed the stroke parameters to a differentiable rasterizer to produce the rasterized sketch. The resulting sketch, as well as the original image are then fed into CLIP to define a CLIP-based perceptual loss. 
            The key to the success of our method is to use the intermediate layers of a pretrained CLIP model to constrain the geometry of the output sketch. Without this term, the output sketch would not be similar to the input image.
            We back-propagate the loss through the differentiable rasterizer and update the strokes&#39; control points directly at each step until convergence of the loss function.
            The learned parameters and loss terms are highlighted in red, while the blue components are frozen during the entire optimization process, solid arrows are used to mark the backpropagation path.
          </p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section> 



<section>
  <div>
    <div>
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">ICCV Presentation</h2> -->
      <div>
        
        <div>
        <h2>
          Our approach is different from conventional sketching methods in that it does not utilize a sketch dataset for training, rather it is optimized under the guidance of CLIP. Thus, our method is not limited to specific categories observed during training, as no category definition was introduced at any stage. This makes our method robust to various inputs.
        </h2>
        
        </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
</section>




<section>
  <div>
    <div>
      <!-- <p style="margin-top: 30px; margin-bottom: 30px"> -->
      <h2>Editing the Brush Style on SVGs</h2>
      <!-- </p> -->
      
      
    </div>
  </div>

</section>







<section>
  <div>
    <div>
      <!-- Paper poster -->

      <h2>Sketching &#34;in the wild&#34;: results of 100 random images of cats from SketchCOCO</h2>
      <p><img src="https://clipasso.github.io/clipasso/static/figures/cats2.png" alt="cars peace"/>
      </p>
       
        <!--/ Paper poster -->
      </div>
    </div>

  </section>






  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@misc{vinker2022clipasso,
      title={CLIPasso: Semantically-Aware Object Sketching}, 
      author={Yael Vinker and Ehsan Pajouheshgar and Jessica Y. Bo and Roman Christian Bachmann and Amit Haim Bermano and Daniel Cohen-Or and Amir Zamir and Ariel Shamir},
      year={2022},
      eprint={2202.05822},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
      }</code></pre>
    </div>
</section>






  
  
  
    <!-- End of Statcounter Code -->

  
  
</section></div>
  </body>
</html>
