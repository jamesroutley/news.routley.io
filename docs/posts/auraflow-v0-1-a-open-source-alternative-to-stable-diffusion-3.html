<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.fal.ai/auraflow/">Original</a>
    <h1>AuraFlow v0.1: a open source alternative to Stable Diffusion 3</h1>
    
    <div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        


        <div>
        <section>

            <ul>
                <li>
                    <a href="https://blog.fal.ai/author/cloneofsimo/"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"></path></g></svg>
</a>
                </li>
                <li>
                    <a href="https://blog.fal.ai/author/features-and-labels/">
                        <img src="https://blog.fal.ai/content/images/size/w100/2024/06/fal-Transparent-1.png" alt="Team Features &amp; Labels"/>
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2024-07-12">Jul 12, 2024</time>
                        <span><span>â€¢</span> 6 min read</span>
                </p>
            </div>

        </section>
        </div>

            <figure>
                <img srcset="/content/images/size/w300/2024/07/Untitled.png 300w,
                            /content/images/size/w600/2024/07/Untitled.png 600w,
                            /content/images/size/w1000/2024/07/Untitled.png 1000w,
                            /content/images/size/w2000/2024/07/Untitled.png 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://blog.fal.ai/content/images/size/w2000/2024/07/Untitled.png" alt="Introducing AuraFlow v0.1, an Open Exploration of Large Rectified Flow Models"/>
            </figure>

    </header>

    <section>
        <p>Open-source AI is in jeopardy. As community interest in AI models skyrocketed over the past year, we noticed that development of new open-source foundational models came to a halt. Some even boldly announced that open-source AI is dead. Not so fast!</p><p>We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community&#39;s resilience and relentless determination.</p><p>If you want to try out a few quick prompts, go to <a href="https://fal.ai/models/fal-ai/aura-flow/playground?ref=blog.fal.ai">falâ€™s model gallery</a> to start playing around.</p><p>If you want to build some cool Comfy workflows with the model, get the latest version of <a href="https://github.com/comfyanonymous/ComfyUI?ref=blog.fal.ai" rel="noreferrer">Comfy</a> and download the model weights from our <a href="https://huggingface.co/fal/AuraFlow?ref=blog.fal.ai" rel="noreferrer">HuggingFace page</a>.</p><p>We would love to give a huge shout out to ComfyUI and the HuggingFace ðŸ¤— diffusers ðŸ§¨ teams for supporting AuraFlow natively on Comfy and <code>diffusers</code> on day 0!</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-1.png" alt="" loading="lazy" width="1257" height="593" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-1.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-1.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-1.png 1257w" sizes="(min-width: 720px) 720px"/><figcaption><span>A fine selection of AuraFlow v0.1 generations</span></figcaption></figure><p><a href="https://x.com/cloneofsimo?ref=blog.fal.ai" rel="noreferrer">Simo</a> is one of our favorite researchers in the wild world of generative media models. You may know him from the amazing <a href="https://github.com/cloneofsimo/lora?ref=blog.fal.ai" rel="noreferrer">adaptation of the LoRA paper</a> for text-to-image models. Few months ago, Simo wanted to implement MMDiT from scratch, and see if he would be able to reproduce it. His initial attempts at <a href="https://github.com/cloneofsimo/minRF?ref=blog.fal.ai">https://github.com/cloneofsimo/minRF</a> and its initial result <a href="https://huggingface.co/cloneofsimo/lavenderflow-5.6B?ref=blog.fal.ai">Lavenderflow-v0</a> came out to be promising. Soon, he found various aspects that could be optimized to train the model on a larger scale more efficiently.</p><p>Timing couldnâ€™t have worked out better. Right around this time, we were convinced that a SOTA open-sourced model is the way forward for this space to move forward. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration.</p><p>AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow&#39;s AI landscape.</p><p>Here, we wanted to share some initial technical details that stand out. We are planning on following up with a more detailed report and possibly a paper as well.</p><p><strong>1. MFU as a first-class citizen</strong></p><p><strong>Most layers donâ€™t need MMDiT Blocks</strong>: While MMDiT achieved good performance, we found that removing many layers to just be single DiT block were much more scalable and compute efficient way to train these models. With careful search in the small-scale proxy, weâ€™ve removed most of the MMDiT blocks and replaced them with large DiT Encoder blocks. These improved the model flops utilization at 6.8B scale by 15%. </p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-2.png" alt="" loading="lazy" width="1110" height="710" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-2.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-2.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-2.png 1110w" sizes="(min-width: 720px) 720px"/><figcaption><span>Number of Double Layers and Optimal Learning Rate</span></figcaption></figure><p><strong>Improved training with torch.compile:</strong> At fal, we are already big fans of Torch Dynamo + Inductor, and build on top of this tooling (with a custom dynamo backend) to run our inference workloads super fast (and efficiently utilizing the underlying hardware). Since PT2â€™s torch.compile is able to handle both forward and backwards passes, AuraFlowâ€™s training was further optimized with its primitives on each layers forward method, and further able to improve MFU by extra 10% ~ 15% depending on the stage.</p><p>2.<strong> Unlock zero-shot learning rate transfer</strong>It is clear that we are not Meta, and would like to have very good hyperparameters even without sweeping them. Fortunately, we noticed MMDiT architectures were also zero-shot LR transferred with maximal-update-parameterization was utilized.Compared to SP, muP was clearly the winner in terms of predictability of learning rate at scale.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-3.png" alt="" loading="lazy" width="1042" height="668" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-3.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-3.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-3.png 1042w" sizes="(min-width: 720px) 720px"/><figcaption><span>Standard Parametrization</span></figcaption></figure><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-4.png" alt="" loading="lazy" width="1196" height="774" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-4.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-4.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-4.png 1196w" sizes="(min-width: 720px) 720px"/><figcaption><span>Maximal Update Parametrization</span></figcaption></figure><p><strong>3. Re-captioned, everything.</strong>It is common trick to recaption everything to make sure there are no faulty text conditions in the dataset. We used our in-house captioner &amp; external captioned dataset to train these models, which improves the quality of the instruction-following significantly. We followed the DALLÂ·E 3 approach to the extreme, and we had no captions that were alt-texts.</p><p><strong>4. Wider, shorter, better!</strong>To further investigate the optimal architecture, we were interested into making a fatter model, i.e., making the architecture overall utilize largest matmul divisible by 256. This lead us into searching for optimal aspect ratio under optimal learning rate found by muP.With these findings, we were confident that aspect ratio of 20 ~ 100 is indeed suitable at larger scale, which was similar with findings from <a href="https://arxiv.org/abs/2010.14701?ref=blog.fal.ai">Scaling Laws for Autoregressive Generative Modeling</a>. We ended up using 3072 / 36, which resulted in model size of 6.8B parameters.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-6.png" alt="" loading="lazy" width="1945" height="1409" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-6.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-6.png 1000w, https://blog.fal.ai/content/images/size/w1600/2024/07/Untitled-6.png 1600w, https://blog.fal.ai/content/images/2024/07/Untitled-6.png 1945w" sizes="(min-width: 720px) 720px"/><figcaption><span>Number of Parameters / Loss</span></figcaption></figure><p>In the end, we did the best of our ability to improve and effectively find the optimal configurations for large scale training. Utilizing the findings from above, we were able to train a text-to-image model from scratch in our largest possible settings for 4 week of compute time, including 256x256, 512x512, 1024x1024 pre-training and aspect ratio fine-tuning. Final model achieves a GenEval score of 0.63~0.67 during pretraining, and similarly 0.64 after 1024x1024 pretraining. But with prompt-enhancement pipeline similar to DALLÂ·E 3, we were able to achieve 0.703!</p><div data-kg-toggle-state="close">
            <div>
                <h4><span>Prompt for prompt-enhancement</span></h4>
                </div>
            <p><span>A caption is a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like &#34;a table is in front of the kitchen pot&#34; and &#34;there are baskets on the table&#34;. Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc.</span></p>
        </div><h3 id="challenges-of-distributed-training-on-multi-modal-data">Challenges of distributed training on multi-modal data</h3><p>One of the harshest realities of training image models is that, unlike LLMs, the modality of the data itself can be a real pain to deal with. During AuraFlowâ€™s training, we leveraged our expertise from dealing with distributed storage as well as managing a large fleet of thousands of GPUs.</p><p>Some of this expertise was directly transferable from production grade inference/fine-tuning systems, where we were able to use open source projects like <a href="https://github.com/juicedata/juicefs?ref=blog.fal.ai">JuiceFS</a> and some were more novel challenges like how do you stream massive amounts of data in and out of multiple nodes while leveraging local NVME space as a staging ground to not to reduce the MFU.</p><p>Be on the lookout for a detailed post on how we choose our storage mediums, where we trained this model, how we evaluated GPU performance and managed large clusters!</p><p>We are not done training! This model is an initial release to kickstart some community engagement. We will continue training the model and apply our learnings from this first attempt. We also noticed that smaller models or MoEâ€™s might be more efficient for consumer GPU cards which have a limiter amount of compute power, so follow closely for a mini version of model that is still as powerful yet much much faster to run. In the meantime, we encourage the community to experiment with what we are releasing today.</p><p>Our goal is to make this model a standard backbone that other innovative work can be built on top of. We look forward to community contributions. If you want to train finetunes, IP-Adapters, or quantizations of the current model, we are happy to support you in any way we can. There is already a vibrant community around fal and Aura models in our Discord. We <a href="https://discord.gg/fal-ai?ref=blog.fal.ai">invite</a> you to join if you want to get involved.</p><p>For business inquiries, please email us at <a href="mailto:hello@fal.ai">hello@fal.ai</a> ðŸ˜„</p>
    </section>


</article>
</div></div>
  </body>
</html>
