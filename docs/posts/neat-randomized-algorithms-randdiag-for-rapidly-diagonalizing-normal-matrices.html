<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ethanepperly.com/index.php/2024/07/02/neat-randomized-algorithms-randdiag-for-rapidly-diagonalizing-normal-matrices/">Original</a>
    <h1>Neat Randomized Algorithms: RandDiag for Rapidly Diagonalizing Normal Matrices</h1>
    
    <div id="readability-page-1" class="page"><article id="post-1852">
	<!-- .entry-header -->

	<div>
		
<p>Consider two complex-valued square matrices <img decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1aa1cd7dfeac818ec5798bd1f1f6f1ad_l3.png" alt="A\in\complex^{n\times n}" title="Rendered by QuickLaTeX.com" height="14" width="75"/> and <img decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-f1484fb927194d28bc8c08942b2454bd_l3.png" alt="B\in\complex^{n\times n}" title="Rendered by QuickLaTeX.com" height="14" width="76"/>. The first matrix <img decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-3519c3ac0258e8f3a90259c6e0161ee4_l3.png" alt="A" title="Rendered by QuickLaTeX.com" height="13" width="13"/> is <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a>, being equal <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-53d9e268cde83f37f11146fcbf201f38_l3.png" alt="A = A^*" title="Rendered by QuickLaTeX.com" height="13" width="56"/> to its <a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate transpose</a> <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-afa89b9a33d3381fd5a4c222a4053270_l3.png" alt="A^*" title="Rendered by QuickLaTeX.com" height="13" width="19"/>. The other matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-f68380d06af723fcfcdc9b0c2c5e79d8_l3.png" alt="B" title="Rendered by QuickLaTeX.com" height="12" width="14"/> is non-Hermitian, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-e2e664d6793d068431f8e1d4d4a0a210_l3.png" alt="B \ne B^*" title="Rendered by QuickLaTeX.com" height="17" width="58"/>. Let’s see how long it takes to compute their eigenvalue decompositions in MATLAB:</p>



<pre><code>&gt;&gt; A = randn(1e3) + 1i*randn(1e3); A = (A+A&#39;)/2;
&gt;&gt; tic; [V_A,D_A] = eig(A); toc <mark>% Hermitian matrix</mark>
Elapsed time is 0.415145 seconds.
&gt;&gt; B = randn(1e3) + 1i*randn(1e3);
&gt;&gt; tic; [V_B,D_B] = eig(B); toc <mark>% non-Hermitian matrix</mark>
Elapsed time is 1.668246 seconds.</code></pre>



<p>We see that it takes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-9a72966a5986fecb1f7ceaa95a1b0e2f_l3.png" alt="4\times" title="Rendered by QuickLaTeX.com" height="12" width="21"/> longer to compute the eigenvalues of the non-Hermitian matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-f68380d06af723fcfcdc9b0c2c5e79d8_l3.png" alt="B" title="Rendered by QuickLaTeX.com" height="12" width="14"/> as compared to the Hermitian matrix <img decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-3519c3ac0258e8f3a90259c6e0161ee4_l3.png" alt="A" title="Rendered by QuickLaTeX.com" height="13" width="13"/>. Moreover, the matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-e6c8e1b597bb4ad79f13131fac7ed2cf_l3.png" alt="V_A" title="Rendered by QuickLaTeX.com" height="15" width="20"/> of eigenvectors for a Hermitian matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-2638fc51fda6a17036939146bdeca68b_l3.png" alt="A = V_AD_AV_A^{-1}" title="Rendered by QuickLaTeX.com" height="22" width="115"/> is a <a href="https://en.wikipedia.org/wiki/Unitary_matrix">unitary matrix</a>, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-0305567713740dc5f292ddfa8f797cad_l3.png" alt="V_A^*V_A = V_AV_A^* = I" title="Rendered by QuickLaTeX.com" height="17" width="143"/>.</p>



<p>There are another class of matrices with nice eigenvalue decompositions, <a href="https://en.wikipedia.org/wiki/Normal_matrix">normal matrices</a>. A square, complex-valued matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/> is <em><a href="https://en.wikipedia.org/wiki/Normal_matrix">normal</a></em> if <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-acb0afeb683e52123e553b0df6ab6513_l3.png" alt="C^*C = CC^*" title="Rendered by QuickLaTeX.com" height="12" width="93"/>. The matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-3366b331906f4b86fe7e9d94fbcafa90_l3.png" alt="V_C" title="Rendered by QuickLaTeX.com" height="15" width="21"/> of eigenvectors for a normal matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-a0a5f60573d3a292ac114b79b9178c7f_l3.png" alt="C = V_C D_C V_C^{-1}" title="Rendered by QuickLaTeX.com" height="22" width="117"/> is also unitary, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-093515def03a84d867a40ea27ebd6e0b_l3.png" alt="V_C^*V_C = V_CV_C^* = I" title="Rendered by QuickLaTeX.com" height="17" width="144"/>. An important class of normal matrices are unitary matrices themselves. A unitary matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-ec39f857edb2450fb99866ab6d5394cb_l3.png" alt="U" title="Rendered by QuickLaTeX.com" height="12" width="13"/> is always normal since it satisfies <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-704cc72e1afda349d14f6d30d3657c14_l3.png" alt="U^*U = UU^* = I" title="Rendered by QuickLaTeX.com" height="12" width="127"/>.</p>



<p>Let’s see how long it takes MATLAB to compute the eigenvalue decomposition of a unitary (and thus normal) matrix:</p>



<pre><code>&gt;&gt; U = V_A;                     <mark>% unitary, and thus normal, matrix</mark>
&gt;&gt; tic; [V_U,D_U] = eig(U); toc <mark>% normal matrix</mark>
Elapsed time is 2.201017 seconds.</code></pre>



<p>Even longer than it took to compute an eigenvalue decomposition of the non-normal matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-f68380d06af723fcfcdc9b0c2c5e79d8_l3.png" alt="B" title="Rendered by QuickLaTeX.com" height="12" width="14"/>! Can we make the normal eigenvalue decomposition closer to the speed of the Hermitian eigenvalue decomposition?</p>



<p>Here is the start of an idea. Every square matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/> has a <em>Cartesian decomposition</em>:</p><p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-6435c1243a04b1a05cbc51277bd9d254_l3.png" height="37" width="345" alt="\[C = H + iS, \quad H = \frac{C+C^*}{2}, \quad S = \frac{C-C^*}{2i}.\]" title="Rendered by QuickLaTeX.com"/></p><p>We have written <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/> as a combination of its <em>Hermitian part</em> <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-ded6dc4c67e0d436e193107aefc23bc2_l3.png" alt="i" title="Rendered by QuickLaTeX.com" height="12" width="6"/> times its <em>skew-Hermitian part</em> <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/>. Both <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/> are Hermitian matrices. The Cartesian decomposition of a square matrix is analogous to the decomposition of a complex number into its real and imaginary parts.</p>



<p>For a normal matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/>, the Hermitian and skew-Hermitian parts commute, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-7940595662f889483b9bbadd1f2aa84a_l3.png" alt="HS = SH" title="Rendered by QuickLaTeX.com" height="12" width="80"/>. We know from matrix theory that commuting Hermitian matrices are <a href="https://en.wikipedia.org/wiki/Diagonalizable_matrix#Simultaneous_diagonalization">simultaneously diagonalizable</a>, i.e., there exists <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> such that <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-ff86dbf025dca9c2d8b7a52acd1b87bf_l3.png" alt="H = QD_HQ^*" title="Rendered by QuickLaTeX.com" height="16" width="102"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-29ba1f26688643ea72412377574d70e2_l3.png" alt="S = QD_SQ^*" title="Rendered by QuickLaTeX.com" height="16" width="94"/> for diagonal matrices <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-3efa26a5d3e4a7208bfdc4f45bb4b640_l3.png" alt="D_H" title="Rendered by QuickLaTeX.com" height="15" width="27"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-bd6acf527082025af2db6cde9b9c53f9_l3.png" alt="D_S" title="Rendered by QuickLaTeX.com" height="15" width="24"/>. Thus, given access to such <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/>, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/> has eigenvalue decomposition </p><p><span>   </span><span>   </span><img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-cbafa3b686b90d2e7c4fb1371e751c29_l3.png" height="19" width="171" alt="\[C = Q(D_H+iD_S)Q^*.\]" title="Rendered by QuickLaTeX.com"/></p>



<p>Here’s a first attempt to turn this insight into an algorithm. First, compute the Hermitian part <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> of <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/>, diagonalize <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-ff86dbf025dca9c2d8b7a52acd1b87bf_l3.png" alt="H = QD_HQ^*" title="Rendered by QuickLaTeX.com" height="16" width="102"/>, and then see if <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/>. Let’s test this out on a <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-90fa23ce8193875a30dd998e5d9df51d_l3.png" alt="2\times 2" title="Rendered by QuickLaTeX.com" height="12" width="39"/> example:</p>



<pre><code>&gt;&gt; C = orth(randn(2) + 1i*randn(2)); <mark>% unitary matrix</mark>
&gt;&gt; H = (C+C&#39;)/2;                     <mark>% Hermitian part</mark>
&gt;&gt; [Q,~] = eig(H);
&gt;&gt; Q&#39;*C*Q                            <mark>% check to see if diagonal</mark>
ans =
  -0.9933 + 0.1152i  -0.0000 + 0.0000i
   0.0000 + 0.0000i  -0.3175 - 0.9483i</code></pre>



<p>Yay! We’ve succeeded at diagonalizing the matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/> using only a Hermitian eigenvalue decomposition. But we should be careful about declaring victory too early. Here’s a bad example:</p>



<pre><code>&gt;&gt; C = [1 1i;1i 1]; <mark>% normal matrix</mark>
&gt;&gt; H = (C+C&#39;)/2;
&gt;&gt; [Q,~] = eig(H);
&gt;&gt; Q&#39;*C*Q           <mark>% oh no! not diagonal</mark>
ans =
   1.0000 + 0.0000i   0.0000 + 1.0000i
   0.0000 + 1.0000i   1.0000 + 0.0000i</code></pre>



<p>What’s going on here? The issue is that the Hermitian part <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-79633f97f8c07e7e926d5398fc362cf5_l3.png" alt="H = I" title="Rendered by QuickLaTeX.com" height="12" width="49"/> for this matrix has a repeated eigenvalue. Thus, <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> has multiple different valid matrices of eigenvectors. (In this specific case, <em>every</em> unitary matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/>.) By looking at <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> alone, we don’t know which <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> matrix to pick which also diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/>.</p>



<p>He and Kressner developed a beautifully simple <em>randomized</em> algorithm called <em><a href="https://arxiv.org/abs/2405.18399">RandDiag</a></em> to circumvent this failure mode. The idea is straightforward:</p>



<ol>
<li>Form a random linear combination <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1302aedc9f23127e42ae14a9b57a9b02_l3.png" alt="M = \gamma_1 H + \gamma_2 S" title="Rendered by QuickLaTeX.com" height="16" width="126"/> of the Hermitian and skew-Hermitian parts of <img decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-3519c3ac0258e8f3a90259c6e0161ee4_l3.png" alt="A" title="Rendered by QuickLaTeX.com" height="13" width="13"/>, with <a href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution">standard normal</a> random coefficients <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-c0af98d69003bf4ff7aa2f7eda6c12c8_l3.png" alt="\gamma_1" title="Rendered by QuickLaTeX.com" height="12" width="15"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-20ae9a1c4b66e1ff2241237b4b1216d4_l3.png" alt="\gamma_2" title="Rendered by QuickLaTeX.com" height="12" width="16"/>.</li>



<li>Compute <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> that diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1254324e3e9c11f16ebd4c8ff20d1cd8_l3.png" alt="M" title="Rendered by QuickLaTeX.com" height="12" width="19"/>.</li>
</ol>



<p>That’s it!</p>



<p>To get a sense of why He and Kressner’s algorithm works, suppose that <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> has some repeated eigenvalues and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/> has all distinct eigenvalues. Given this setup, it seems likely that a random linear combination of <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> will also have all distinct eigenvalues. (It would take a very special circumstances for a random linear combination to yield two eigenvalues that are <em>exactly</em> the same!) Indeed, this intuition is a fact: <a href="https://arxiv.org/html/2405.18399v1#S2">With 100% probability,</a> <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> diagonalizing a Gaussian random linear combination of simultaneously diagonalizable matrices <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/> also diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1fc424a0582535e024fea07859f07b1d_l3.png" alt="H" title="Rendered by QuickLaTeX.com" height="12" width="16"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-4d33ce2fb9fb929257b04e9f4ae7783b_l3.png" alt="S" title="Rendered by QuickLaTeX.com" height="12" width="12"/> individually.</p>



<p>MATLAB code for RandDiag is as follows:</p>



<pre><code>function Q = rand_diag(C)
   H = (C+C&#39;)/2; S = (C-C&#39;)/2i;
   M = randn*H + randn*S;
   [Q,~] = eig(M);
end</code></pre>



<p>When applied to our hard <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-90fa23ce8193875a30dd998e5d9df51d_l3.png" alt="2\times 2" title="Rendered by QuickLaTeX.com" height="12" width="39"/> example from before, RandDiag succeeds at giving a matrix that diagonalizes <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-04b97b09a8aa0370c4dd4514b9322c65_l3.png" alt="C" title="Rendered by QuickLaTeX.com" height="12" width="14"/>:</p>



<pre><code>&gt;&gt; Q = rand_diag(C);
&gt;&gt; Q&#39;*C*Q
ans =
   1.0000 - 1.0000i  -0.0000 + 0.0000i
  -0.0000 - 0.0000i   1.0000 + 1.0000i</code></pre>



<p>For computing the matrix of eigenvectors for a <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-203d2de43e3e4e0ad88a9fc1c812e728_l3.png" alt="1000\times 1000" title="Rendered by QuickLaTeX.com" height="12" width="92"/> unitary matrix, RandDiag takes 0.4 seconds, just as fast as the Hermitian eigendecomposition did.</p>



<pre><code>&gt;&gt; tic; V_U = rand_diag(U); toc
Elapsed time is 0.437309 seconds.</code></pre>



<p>He and Kressner’s algorithm is delightful. Ultimately, it uses randomness in only a small way. For most coefficients <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-2d3915707a964357ec87dc74dfa3d131_l3.png" alt="a_1,a_2 \in \real" title="Rendered by QuickLaTeX.com" height="16" width="77"/>, a matrix <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-035878d597bc5f8c29f4121c5eb11460_l3.png" alt="Q" title="Rendered by QuickLaTeX.com" height="16" width="14"/> diagonalizing <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-1319959cbf65bf752708ffb262d9f86d_l3.png" alt="a_1 H + a_2 S" title="Rendered by QuickLaTeX.com" height="15" width="84"/> will also diagonalize <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-c8882e5a945cf9c99271f08a46fcfa5a_l3.png" alt="A = H+iS" title="Rendered by QuickLaTeX.com" height="15" width="93"/>. But, for any specific choice of <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-0fc640366a96fc5a33c11f98ebba9608_l3.png" alt="a_1,a_2" title="Rendered by QuickLaTeX.com" height="12" width="41"/>, there is a possibility of failure. To avoid this possibility, we can just pick <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-2d06f2dc756da05d3cb84b1302d7dfb9_l3.png" alt="a_1" title="Rendered by QuickLaTeX.com" height="11" width="15"/> and <img loading="lazy" decoding="async" src="https://www.ethanepperly.com/wp-content/ql-cache/quicklatex.com-6f7a6518b0c17758ce48985840882bf4_l3.png" alt="a_2" title="Rendered by QuickLaTeX.com" height="11" width="16"/> at random. It’s really as simple as that.</p>



<p><strong>References:</strong> RandDiag was proposed in <em><a href="https://arxiv.org/abs/2405.18399">A simple, randomized algorithm for diagonalizing normal matrices</a></em> by He and Kressner (2024), building on their earlier work in <em><a href="https://arxiv.org/abs/2212.07248">Randomized Joint Diagonalization of Symmetric Matrices</a></em> (2022) which considers the general case of using random linear combinations to (approximately) simultaneous diagonalize (nearly) commuting matrices. RandDiag is an example of a linear algebraic algorithm that uses randomness to put the input into “general position”; see <em><a href="https://arxiv.org/abs/2402.17873">Randomized matrix computations: Themes and variations</a></em> by Kireeva and Tropp (2024) for a discussion of this, and other, ways of using randomness to design matrix algorithms.</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>
  </body>
</html>
