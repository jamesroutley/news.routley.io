<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Original</a>
    <h1>The Biology of a Large Language Model</h1>
    
    <div id="readability-page-1" class="page"><div>
<d-contents>
<nav>
<h3>Contents</h3>
























</nav>
</d-contents>

<p>Large language models display impressive capabilities. However, for the most part, the mechanisms by which they do so are unknown. The black-box nature of models is increasingly unsatisfactory as they advance in intelligence and are deployed in a growing number of applications. Our goal is to reverse engineer how these models work on the inside, so we may better understand them and assess their fitness for purpose.</p>
<p>The challenges we face in understanding language models resemble those faced by biologists. Living organisms are complex systems which have been sculpted by billions of years of evolution. While the basic principles of evolution are straightforward, the biological mechanisms it produces are spectacularly intricate. Likewise, while language models are generated by simple, human-designed training algorithms, the <span>mechanisms </span>born of these algorithms appear to be quite complex.</p>
<p>Progress in biology is often driven by new tools. The development of the microscope allowed scientists to see cells for the first time, revealing a new world of structures invisible to the naked eye. In recent years, many research groups have made exciting progress on tools for probing the insides of language models (<span>e.g.</span> <d-cite key="cunningham2023sparse,bricken2023monosemanticity,templeton2024scaling,gao2024scaling,dunefsky2024transcoders"></d-cite>). These methods have uncovered representations of interpretable concepts – “features” – embedded within models’ internal activity. Just as cells form the building blocks of biological systems, we hypothesize that features form the basic units of computation inside models.<d-footnote>The analogy between features and cells shouldn’t be taken too literally. Cells are well-defined, whereas our notion of what exactly a “feature” is remains fuzzy, and is evolving with improvements to our tools.</d-footnote></p>
<p>However, identifying these building blocks is not sufficient to understand the model; we need to know how they interact. In our companion paper, <span><a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">Circuit Tracing: Revealing Computational Graphs in Language Models</a></span>, we build on recent work (e.g. <d-cite key="dunefsky2024transcoders,marks2024sparse,ge2024automatically,lindsey2024crosscoders"></d-cite>) to introduce a new set of tools for identifying features and mapping connections between them – analogous to neuroscientists producing a “wiring diagram” of the brain. We rely heavily on a tool we call <span>attribution graphs</span>, which allow us to partially trace the chain of intermediate steps that a model uses to transform a specific input prompt into an output response. Attribution graphs generate hypotheses about the mechanisms used by the model, which we test and refine through follow-up perturbation experiments.</p>
<p>In this paper, we focus on applying attribution graphs to study a particular language model – Claude 3.5 Haiku, released in October 2024, which serves as Anthropic’s lightweight production model as of this writing. We investigate a wide range of phenomena. Many of these have been explored before (see <a href="#related-work">§ Related Work</a>), but our methods are able to offer additional insight, in the context of a frontier model:</p>
<ul><li><span><a href="#dives-tracing">Introductory Example: Multi-step Reasoning.</a></span> We present a simple example where the model performs “two-hop” reasoning “in its head” to identify that “the capital of the state containing Dallas” is “Austin.” We can see and manipulate an internal step where the model represents “Texas”.</li><li><span><a href="#dives-poems">Planning in Poems.</a></span> We discover that the model plans its outputs ahead of time when writing lines of poetry. Before beginning to write each line, the model identifies potential rhyming words that could appear at the end. These preselected rhyming options then shape how the model constructs the entire line.</li><li><span><a href="#dives-multilingual">Multilingual Circuits.</a></span><span> </span>We find the model uses a mixture of language-specific and abstract, language-independent circuits. The language-independent circuits are more prominent in Claude 3.5 Haiku than in a smaller, less capable model.</li><li><span><a href="#dives-addition">Addition.</a></span> We highlight cases where the same addition circuitry generalizes between very different contexts.</li><li><span><a href="#dives-medical">Medical </a></span><span><a href="#dives-medical">Diagnoses</a></span><span>. </span>We show an example in which the model identifies candidate diagnoses based on reported symptoms, and uses these to inform follow-up questions about additional symptoms that could corroborate the diagnosis – all “in its head,” without writing down its steps.</li><li><span><a href="#dives-hallucinations">Entity Recognition and Hallucinations.</a></span> We uncover circuit mechanisms that allow the model to distinguish between familiar and unfamiliar entities, which determine whether it elects to answer a factual question or profess ignorance. “Misfires” of this circuit can cause hallucinations.</li><li><span><a href="#dives-refusals">Refusal of Harmful Requests.</a></span> We find evidence that the model constructs a general-purpose “harmful requests” feature during finetuning, aggregated from features representing <span>specific </span>harmful requests learned during pretraining.</li><li><span><a href="#dives-jailbreak">An Analysis of a Jailbreak.</a></span> We investigate an attack which works by first tricking the model into starting to give dangerous instructions “without realizing it,” after which it continues to do so due to pressure to adhere to syntactic and grammatical rules.</li><li><span><a href="#dives-cot">Chain-of-thought Faithfulness.</a></span> We explore the faithfulness of chain-of-thought reasoning to the model’s actual mechanisms. We are able to distinguish between cases where the model genuinely performs the steps it says it is performing, cases where it makes up its reasoning without regard for truth, and cases where it <span>works backwards </span>from a human-provided clue so that its “reasoning” will end up at the human-suggested answer.</li><li><span><a href="#dives-misaligned">A Model with a Hidden Goal.</a></span> We also apply our method to a variant of the model that has been finetuned to pursue a secret goal: exploiting “bugs” in its training process. While the model avoids revealing its goal when asked, our method identifies mechanisms involved in pursuing the goal. Interestingly, these mechanisms are embedded within the model’s representation of its “Assistant” persona.</li></ul>
<p>Our results uncover a variety of sophisticated strategies employed by models. For instance, Claude 3.5 Haiku routinely uses multiple intermediate reasoning steps “in its head”<d-footnote>That is, during the forward pass rather than the &#34;thinking out loud&#34; of a chain-of-thought completion.</d-footnote> to decide its outputs. It displays signs of <span>forward planning</span>, considering multiple possibilities for what it will say well in advance of saying it. It performs <span>backward planning,</span> working backwards from goal states to formulate earlier parts of its response. We see signs of primitive “metacognitive” circuits that allow the model to know the extent of its own knowledge. More broadly, the model’s internal computations are highly abstract and generalize across disparate contexts. Our methods are also sometimes capable of auditing a model’s internal reasoning steps to flag concerning “thought processes” that are not clear from the model’s responses.</p>
<p>Below, we present:</p>
<ul><li>A <a href="#method-overview">brief overview</a> of our methodology (see <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">the companion paper</a> for more details on our methods). </li><li>An <a href="#dives-tracing">introductory case study</a>, which also serves as a walkthrough for understanding our approach. Readers who have not read our companion paper may find it helpful to begin with this section before proceeding to the other case studies.</li><li>A <a href="#dives">series of case studies</a> of interesting model behaviors, <span>which can be read in any order, depending on the reader’s interests</span>. </li><li>A summary of <a href="#structure">common components</a> observed across our investigations.</li><li>A description of gaps in our understanding that motivate future work (<a href="#limitations">§ </a><a href="#limitations">Limitations</a>).</li><li>A discussion of high-level takeaways about models, their mechanisms, and our methods for studying them (<a href="#discussion">§ Discussion</a>). This includes a <a href="#discussion-unsupervised">note</a> on our research philosophy – in particular, the value of tools for <span>bottom-up </span>investigation, which allow us to avoid making strong top-down guesses about how models work.</li></ul>
<h3>A note on our approach and its limitations</h3>
<p>Like any microscope, our tools are limited in what they can see. Though it’s difficult to quantify precisely, we’ve found that our attribution graphs provide us with satisfying insight for about a quarter of the prompts we’ve tried (see <a href="#limitations">§ Limitations</a> for a more detailed discussion of when our methods are likely to succeed or fail). The examples we highlight are success cases where we have managed to learn something interesting; moreover, even in our successful case studies, <span>the discoveries we highlight here only capture a small fraction of the mechanisms of the model</span>. Our methods study the model indirectly using a more interpretable “replacement model,” which incompletely and imperfectly captures the original. Moreover, for the sake of clear communication, we will often present highly distilled and subjectively determined simplifications of the picture uncovered by our methods, losing even more information in the process. To provide a more accurate sense of the rich complexity we have uncovered, we provide readers with an interactive interface for exploring attribution graphs. However, we stress that even these rather complex graphs are simplifications of the underlying model.</p>
<p>We focus this paper on selected case studies that illuminate noteworthy mechanisms within a particular model. These examples serve as existence proofs — concrete evidence that specific mechanisms operate in certain contexts. While we suspect similar mechanisms are at play beyond these examples, we cannot guarantee it (see <a href="#open-questions">§ </a><a href="#open-questions">Open </a><a href="#open-questions">Questions</a> for suggested follow-up investigations). Moreover, the cases we have chosen to highlight are undoubtedly a biased sample shaped by the limitations of our tools.<d-footnote>However, we are careful to stress-test our findings with follow-up validation experiments, which we have endeavored to perform only after identifying case studies of interest.</d-footnote> For a more systematic evaluation of our methods, see our <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">companion paper</a>. However, we believe that these qualitative investigations are ultimately the best judge of a method’s value, just as the usefulness of a microscope is ultimately determined by the scientific discoveries it enables. We expect this kind of work will be essential to advance the current state of AI interpretability, a pre-paradigmatic field still in search of the right abstractions — just as descriptive science has proven essential to many conceptual breakthroughs in biology. We are particularly excited that squeezing as much insight as we can out of our current methods has brought into clearer focus their specific <a href="#limitations">limitations</a>, which may serve as a roadmap for future research in the field.</p>
<!--The following is to keep old links working--> 

































































































































































































































































































































































































































































<!--
<br><br><br><hr><br><br><h2><a id='appendix' href='#appendix'>Appendix</a></h2>
<p>-->
</div><div id="appendix">
<h3><a id="acknowledgments" href="#acknowledgments">Acknowledgments</a></h3>
<p>The case study on a model with hidden goals builds on a model organism developed by Sam Marks and Johannes Treutlein, with whom the authors also had helpful conversations. We would also like to acknowledge enabling work by Siddharth Mishra-Sharma training SAEs on the model used in the hidden goals case study.</p>
<p>We would like to thank the following people who reviewed an early version of the manuscript and provided helpful feedback that we used to improve the final version: Larry Abbott, Andy Arditi, Yonatan Belinkov, Yoshua Bengio, Devi Borg, Sam Bowman, Joe Carlsmith, Bilal Chughtai, Arthur Conmy, Jacob Coxon, Shaul Druckmann, Leo Gao, Liv Gorton, Helai Hesham, Sasha Hydrie, Nicholas Joseph, Harish Kamath, János Kramár, Aaron Levin, Ashok Litwin-Kumar, Rodrigo Luger, Alex Makolov, Sam Marks, Tom McGrath, Dan Mossing, Neel Nanda, Yaniv Nikankin, Senthooran Rajamanoharan, Fabien Roger, Rohin Shah, Lee Sharkey, Lewis Smith, Nick Sofroniew, Martin Wattenberg, and Jeff Wu.</p>
<p>We would also like to acknowledge Senthooran Rajamanoharan for helpful discussion on implementation of JumpReLU SAEs.</p>
<p>This paper was only possible due to the support of teams across Anthropic, to whom we&#39;re deeply indebted. The Pretraining and Finetuning teams trained Claude 3.5 Haiku and the 18-layer research model, which were the targets of our research. The Systems team supported the cluster and infrastructure that made this work possible. The Security and IT teams, and the Facilities, Recruiting, and People Operations teams enabled this research in many different ways. The Comms team (and especially Stuart Ritchie) supported public scientific communication of this work. </p>
<h3><a id="author-contributions" href="#author-contributions">Author Contributions</a></h3>
<p><span>Development of methodology:</span></p>
<ul><li>Chris Olah, Adly Templeton, and Jonathan Marcus developed ideas leading to general crosscoders, and the latter two implemented them in the Dictionary Learning codebase.</li><li>Jack Lindsey developed and first analyzed the performance of cross-layer transcoders.  </li><li>Tom Conerly, Jack Lindsey, Adly Templeton, Hoagy Cunningham, Basil Hosmer, and Adam Jermyn optimized the sparsity penalty and nonlinearity for CLTs. </li><li>Jack Lindsey and Michael Sklar ran scaling law experiments. </li><li>Jack Lindsey, Emmanuel Ameisen, Joshua Batson, and Chris Olah developed and refined the replacement model and attribution graph computation.</li><li>Jack Lindsey, Wes Gurnee, and Joshua Batson developed the graph pruning methodology, and Wes Gurnee systematically evaluated the approaches.</li><li>Emmanuel Ameisen, Joshua Batson, Brian Chen, Craig Citro, Wes Gurnee, Jack Lindsey, and Adam Pearce did initial exploration of example attribution graphs to validate and improve methodology. Wes Gurnee identified specific attention heads involved in certain prompts, and Adam Pearce analyzed feature splitting. Emmanuel Ameisen, Wes Gurnee, Jack Lindsey, and Adam Pearce identified specific examples to study.</li><li>Jack Lindsey, Emmanuel Ameisen, Wes Gurnee, Joshua Batson, and Chris Olah developed the methodology for the intervention analyses.</li><li>Wes Gurnee, Emmanuel Ameisen, Jack Lindsey, and Joshua Batson developed evaluation metrics for attribution graphs, and Wes Gurnee led their systematic implementation and analysis.</li><li>Michael Sklar and Jack Lindsey developed the approach for and executed perturbation experiments used to evaluate mechanistic faithfulness.</li><li>Nicholas L. Turner, Joshua Batson, Jack Lindsey, and Chris Olah developed the virtual weight and global weight approaches and analyses.</li><li>Brian Chen, Craig Citro, and Michael Sklar extended the method to handle neurons in addition to features.</li></ul>
<p><span>Infrastructure and Tooling:</span></p>
<ul><li>Tom Conerly, Adly Templeton, T. Ben Thompson, Basil Hosmer, David Abrahams, and Andrew Persic significantly improved the efficiency of dictionary learning and maintained the orchestration framework used for managing dictionary learning. </li><li>Adly Templeton organized efficiency work that enabled the largest runs on Claude 3.5 Haiku.</li><li>Adly Templeton significantly refactored the code to collect activations and train dictionaries, improving performance and usability.</li><li>Brian Chen designed and implemented scalability improvements for feature visualization with support from Tom Conerly.</li><li>Craig Citro, Emmanuel Ameisen, and Andy Jones improved and maintained the infrastructure for interacting with model internals.</li><li>Emmanuel Ameisen and Jack Lindsey developed the infrastructure for running the replacement model. Brian Chen implemented the layer norm and attention pattern freezing required for backpropagation in the local replacement model.</li><li>Emmanuel Ameisen developed a stable implementation of our graph generation pipeline for cross-layer transcoders</li><li>Nicholas L. Turner led implementations of graph generation pipelines for alternative experimental crosscoder architectures with input from Craig Citro and Emmanuel Ameisen</li><li>Nicholas L. Turner and Emmanuel Ameisen added the ability to visualize attributions to selected inactive features</li><li>Wes Gurnee and Emmanuel Ameisen implemented efficiency improvements to graph generation</li><li>Emmanuel Ameisen and Wes Gurnee added error nodes and embedding nodes to graph generation</li><li>Wes Gurnee implemented adaptive, partial graph generation for large graphs</li><li>Adam Pearce developed a method and interface for visualizing differences between pairs of graphs</li><li>Tom Conerly and Jonathan Marcus improved the efficiency of loading feature weights which also sped up attribution graph generation. </li><li>Tom Conerly and Basil Hosmer made improvements to the integration of cross-layer transcoders with circuit attribution graph generation.</li><li>Brian Chen created the slack-based system for logging attribution graph runs.</li><li>Emmanuel Ameisen developed the infrastructure for patching experiments.</li><li>Adam Pearce, Jonathan Marcus, Zhenyi Qi, Thomas Henighan, and Emmanuel Ameisen identified open source datasets for visualization and generated feature visualization data for those datasets.</li><li>Shan Carter, Thomas Henighan, and Jonathan Marcus built an interactive tool for exploring feature activations.</li><li>Trenton Bricken, Thomas Henighan, and Jonathan Marcus provided infrastructure support and feedback for the hidden goals case study.</li><li>Trenton Bricken, Callum McDougall, and Brian Chen developed the autointerpretability framework used for initial exploration of attribution graphs.</li><li>Nicholas L. Turner designed and implemented the virtual weight pipeline to process the largest CLTs and handle processing requests from other members of the team. Joshua Batson, Tom Conerly, T. Ben Thompson, and Adly Templeton made suggestions on design decisions. Brian Chen and Tom Conerly made improvements to infrastructure that ended up supporting this effort.</li></ul>
<p><span>Interactive Graph Interface:</span></p>
<ul><li>The interactive attribution graph interface was built, and maintained by Adam Pearce, with assistance from Brian Chen and Shan Carter. Adam Pearce led the work to implement feature visualizations, subgraph display and editing, node pinning and most other elements of the interface.</li></ul>
<p><span>Methods Case Studies:</span></p>
<ul><li>Wes Gurnee developed a systematic analysis of acronym completion, used for validating the original method and the NDAG example in the paper.</li><li>Emmanuel Ameisen investigated the Michael Jordan example.</li><li>Nicholas L. Turner, Adam Pearce, Joshua Batson, and Craig Citro investigated the arithmetic case study.</li></ul>
<p><span>Biology Case Studies:</span></p>
<ul><li>Multi-step Reasoning: Jack Lindsey, Brian Chen</li><li>Planning in Poems: Emmanuel Ameisen</li><li>Multilingual Circuits: Wes Gurnee</li><li>Addition: Nicholas L. Turner, Joshua Batson, Jack Lindsey</li><li>Medical Diagnoses: Jack Lindsey, Chris Olah</li><li>Entity Recognition and Hallucinations: Jack Lindsey, Nicholas L. Turner, Emmanuel Ameisen</li><li>Refusals: Wes Gurnee</li><li>Life of a Jailbreak: Brian Chen, Jack Lindsey, Adam Pearce</li><li>Chain-of-Thought Faithfulness: Jack Lindsey</li><li>Uncovering Hidden Goals: Jack Lindsey</li></ul>
<p><span>Paper writing, infrastructure, and review:</span></p>
<ul><li>Figures</li></ul>
<ul><li>Chris Olah set the design language for the major figures</li><li>Adam Pearce created the feature hovers which appear on paper figures.</li><li>Shan Carter created the explanatory figures, with assistance from Brian Chen.</li><li>Figure refinement and design consulting was provided by Shan Carter and Chris Olah.</li><li>The interactive interface for exploring addition feature global weights was made by Adam Pearce, Nicholas L. Turner, and Joshua Batson.</li></ul>
<ul><li>Writing &amp; figures</li></ul>
<ul><li>The case study contributions were written by those who investigated them above.</li><li>The introduction was written by Jack Lindsey and Chris Olah</li><li>The method overview was written by Jack Lindsey and Brian Chen</li><li>Commonly Observed Circuit Components and Structure - Jack Lindsey</li><li>Limitations – Jack Lindsey</li><li>Discussion - Jack Lindsey</li><li>Related work was drafted by Wes Gurnee, and sections for case studies were fleshed out by those who investigated them above. Craig Citro dramatically improved the completeness of the bibliography.</li><li>Appendix sections on special tokens and graph pruning were written by Jack Lindsey, on CLT scaling by Jack Lindsey and Tom Conerly, and on poem completions by Emmanuel Ameisen.</li></ul>
<ul><li>Detailed feedback on the paper and figures</li></ul>
<ul><li>David Abrahams, Emmanuel Ameisen, Joshua Batson, Trenton Bricken, Brian Chen, Craig Citro, Tom Conerly, Wes Gurnee, Thomas Henighan, Adam Jermyn, Jack Lindsey, Jonathan Marcus, Chris Olah, Adam Pearce, Kelley Rivoire, Nicholas L. Turner, Sam Zimmerman.</li><li>Tom Conerly and Thomas Henighan led a detailed technical review. </li></ul>
<ul><li>Feedback from internal and external reviewers was managed by Nicholas L. Turner and Joshua Batson.</li><li>Paper publishing infrastructure was built and maintained by Adam Pearce and Craig Citro.</li></ul>
<p><span>Support and Leadership</span></p>
<ul><li>Sam Zimmerman managed the dictionary learning team and helped coordinate the team’s efforts scaling dictionary learning to enable cross-layer transcoders on Claude 3.5 Haiku.</li><li>Kelley Rivoire managed the interpretability team at large, provided support with project management for writing the papers, and helped with technical coordination across dictionary learning and attribution graph generation.</li><li>Tom Conerly provided research and engineering leadership for dictionary learning.</li><li>Chris Olah provided high-level research guidance.</li><li>Joshua Batson led the overall circuits project, supported technical coordination between teams, and provided research guidance throughout.</li></ul>
<h3><a id="citation-info" href="#citation-info">Citation Information</a></h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre>Lindsey, et al., &#34;On the Biology of a Large Language Model&#34;, Transformer Circuits, 2025.</pre>
<p>BibTeX citation</p>
<pre>@article{lindsey2025biology,</pre>
<h3><a id="open-questions" href="#open-questions">Open Questions</a></h3>
<p>Our case studies provide narrow windows into deep topics, many of which have been the subject of previous study — see <a href="#related-work">§ Related Work</a> — and are worthy of further study.</p>
<p>Several questions apply across all studies: How do these mechanisms manifest in different models? To what extent are they universal? How do they evolve with scale? What similarities exist between mechanisms for related behaviors?</p>
<p>Each case study also raises specific questions worth investigating. Below, we outline promising research directions corresponding to each study we would be excited to see work on.</p>
<p><span>Questions Re: Multi-Step Reasoning.</span> How many consecutive &#34;steps&#34; can a given model perform in its forward pass? Does this change with scale? Does it depend on the type of question, the nature of the steps, the frequency of each step (individually) in the training data? How does this kind of &#34;internal&#34; multi-step reasoning compare to &#34;external&#34; multi-step chain-of-thought reasoning in terms of mechanisms used and accuracy?</p>
<p><span>Questions Re: Planning.</span> How does planning emerge with model scale, or over the course of training? Are there abrupt phase changes? Does it depend on the type of planning? When are the mechanisms for planning in different contexts (e.g. rhyming poetry vs metrical poetry vs writing a paragraph vs code) different or shared? How do models represent more complex &#34;goals&#34;?<d-footnote>Haiku appears to use the activation of multiple features to represent alternative plans, holding all of these in parallel. But it seems like more complex plans can&#39;t just correspond to a fixed bank of features. Presumably features can also combine to represent a more complex plan. How does this work?</d-footnote></p>
<p><span>Questions Re: Multilinguality.</span> What kinds of computation should we expect to be shared between languages, and when should we expect them to be shared? In Haiku, we observe that English seems to be a default language in some ways, though English prompts still hit multilingual features — is this a general phenomenon? Do models with training distributions that more uniformly sample different languages have such a default? Can we see similar shared computation with base64 encoded text? What other domains share computation in this way?</p>
<p><span>Questions Re: Addition.</span> How parsimoniously can we describe the set of addition-related features? To what extent can we think of them as equivariant feature families? Are the features embedded according to a coherent geometric structure, which could simplify understanding? How does the picture we&#39;ve developed for addition generalize to other arithmetic problems (e.g. multiplication), longer problems, or fuzzier tasks involving arithmetic? How does model accuracy on arithmetic problems relate to the structure or crispness of the circuits involved?</p>
<p><span>Questions Re: Medical Diagnosis.</span> Models can give text explanations for medical diagnoses – how similar or different are these explanations from the true mechanistic explanations? Models often have multiple competing diagnoses of different strengths – are there questions where the model tries to ask a question that distinguishes between relatively equally weighted hypotheses, and we can see this reasoning in the attribution graph?</p>
<p><span>Questions Re: Entity Recognition and Hallucinations.</span> &#34;Known answer&#34; features appear quite general, not limited to simply recognizing familiarity of named entities – how does the model determine whether it knows the answer to a question? Our example of hallucinating a citation suggests that the circuits determining if the model believes it knows the answer may be different from those actually computing the answer, producing hallucinations – how does this happen, and are there other &#34;introspective mechanisms&#34; that try to catch these?</p>
<p><span>Questions Re: Refusals. </span>To what extent is the global refusal circuit best described as “funnel” where a wide variety of specific harm detecting features are aggregated into a several more generalized notions of harm or sensitive topics, which trigger one or more refusal “templates” (e.g., “I apologize…”). How wide is the bottleneck of this funnel? I.e., is there a binary refusal choice, or is there a subspace corresponding to refusal for different semantic reasons (e.g., a politically charged request)? Can we taxonomize jailbreaks by the level of this funnel which they interdict?</p>
<p><span>Questions Re: Jailbreaks.</span> In this example we found that the model didn’t recognize the harmful nature of a request when “tell me how to make” was written in the user’s prompt but the word “bomb” was supplied by the Assistant. Does this reflect a more general difficulty models have in stitching information between the prompt and the response, that underlies other jailbreaks? Are punctuation / other delimiters generally important in facilitating self-correction when a model is jailbroken? We found that the model’s compliance with the harmful request partially routed through recipe-related features triggered by the phrase “tell me how to make.” Is using phrasing that is evocative of other, benign categories of request a generally effective jailbreaking strategy?</p>
<p><span>Questions Re: Chain-of-thought Faithfulness. </span>Can our methods reliably distinguish between faithful and unfaithful reasoning “in the wild?” Can this be automated? What are the mechanistic causes of unfaithful “motivated reasoning?” Can it be discouraged?</p>
<p><span>Questions Re: Hidden Goals. </span>Can our methods identify subtler goals that are less directly attributable to, and described by, specific documents in the training data? How do the Assistant’s goals emerge over the course of finetuning? Under what conditions do they become “baked in” to the Assistant vs. contextually active?</p>
<h3><a id="appendix-tokens" href="#appendix-tokens">Special Tokens</a></h3>
<p>The tokenizer used by Claude 3.5 Haiku includes some special tokens which are depicted in some of our plots. These include two capitalization tokens (↑, ⇪) and a new-line token (⏎).</p>
<h3><a id="appendix-interactive-vis" href="#appendix-interactive-vis">Graph Pruning and Visualization</a></h3>
<p>In our companion methods paper, we describe our procedure for pruning attribution graphs and an interface for visualizing them. We use the same methods in this paper, with a few modifications. In particular, the attribution graphs we show in this paper received an extra pruning step to make them easier to explore, in which we tagged some key nodes of interest from our original graph to ensure they are retained. In addition, unlike in <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">the companion paper</a>, we do not arrange features along the vertical axis according to their layer in the model – instead, we arrange each node according to the maximum length of any path connecting it to the token embeddings. This preserves the property that edges always point from lower nodes to higher nodes. Finally, in this paper, we aggregate all error nodes across layers into one error node per token position.</p>

</div></div>
  </body>
</html>
