<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.tomshardware.com/pc-components/gpus/amd-strikes-back-at-nvidia-with-new-mi300x-benchmarks-mi300x-shows-30-higher-performance-than-h100-even-with-an-optimized-software-stack">Original</a>
    <h1>AMD MI300X 30% higher performance than Nvidia H100, even with optimized stack</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body">
<p>Neither AMD nor Nvidia intends to back out of this argument involving the performance difference between the Instinct MI300X and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022">H100</a> (Hopper) GPUs. But AMD does make some strong points while comparing FP16 using vLLM, which is a more popular choice against FP8, which works only with TensorRT-LLM. </p><p>The red team announced the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-unveils-instinct-mi300x-gpu-and-mi300a-apu-claims-up-to-16x-lead-over-nvidias-competing-gpus" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-unveils-instinct-mi300x-gpu-and-mi300a-apu-claims-up-to-16x-lead-over-nvidias-competing-gpus">MI300X graphics accelerator</a> early this December, claiming up to 1.6X lead over Nvidia&#39;s H100. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-h100-is-2x-faster-than-amd-m1300x" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-h100-is-2x-faster-than-amd-m1300x">Two days ago</a>, Nvidia fired back by saying AMD did not use its optimizations when comparing the H100 with TensorRT-LLM. The reply reached a single H100 against eight-way H100 GPUs while running the Llama 2 70B chat model. </p><h2 id="the-continued-war-of-benchmark-results-and-test-scenarios-3">The Continued War of Benchmark Results and Test Scenarios</h2><p>In this latest response, AMD said that Nvidia used a selective set of inferencing workloads. It further identified that Nvidia benchmarked these using its in-house TensorRT-LLM on H100 rather than vLLM, an open-source and widely used method. Furthermore, Nvidia used the vLLM FP16 performance datatype on AMD while comparing its results with DGX-H100, which used the TensorRT-LLM with FP8 datatype to display these alleged misconstrued results. AMD stressed that in its test, it used vLLM with the FP16 dataset due to its widespread use, and vLLM does not support FP8.</p><p>There&#39;s also the point that servers will have latency, but instead of accounting for that, Nvidia showed its throughput performance, not emulating the real-world situation, according to AMD.</p><h2 id="amd-apos-s-updated-test-results-with-more-optimizations-and-accounting-for-latency-with-nvidia-apos-s-testing-method-3">AMD&#39;s Updated Test Results with More Optimizations and Accounting for Latency with Nvidia&#39;s Testing Method</h2><p>AMD made three performance runs using Nvidia&#39;s TensorRT-LLM, the last notable one having measured latency results between MI300X and vLLM using the FP16 dataset against H100 with TensorRT-LLM. But the first test involved a comparison between the two using vLLM on both, hence FP16, and for the second test, it compared its MI300X&#39;s performance with vLLM while comparing TensorRT-LLM.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="AMD&#39;s second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg"/><source type="image/jpeg" alt="AMD&#39;s second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg"/><img src="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" alt="AMD&#39;s second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg"/></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: AMD )</span></figcaption></figure><p>So, AMD used the same selected testing scenario Nvidia did with its second and third test scenarios, showing higher performance and reduced latency. The company added more optimizations when compared to H100 while running vLLM on both, offering a 2.1x boost in performance.</p><p>It is now up to Nvidia to evaluate how it wants to respond. But it also needs to acknowledge that this would require the industry to ditch FP16 with TensorRT-LLM&#39;s closed system for using FP8, essentially ditching vLLM for good. While referring to Nvidia&#39;s premium, a <a data-analytics-id="inline-link" href="https://www.reddit.com/r/LocalLLaMA/comments/16du7xj/nvidia_tensorrtllm/" target="_blank" data-url="https://www.reddit.com/r/LocalLLaMA/comments/16du7xj/nvidia_tensorrtllm/">Redditor once said</a>, &#34;TensorRT-LLM is free just like the things that come free with a Rolls Royce.&#34;</p>
</div><div id="slice-container-newsletterForm-articleInbodyContent-x4MmDABPR8ezWHrbuxadyc"><div data-hydrate="true"><div><section></section><section><p>Join the experts who read Tom&#39;s Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We&#39;ll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div></div></div></div>
  </body>
</html>
