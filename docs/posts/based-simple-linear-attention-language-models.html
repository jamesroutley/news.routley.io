<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.together.ai/blog/based">Original</a>
    <h1>Based: Simple linear attention language models</h1>
    
    <div id="readability-page-1" class="page"><div><p>Introducing Based, a simple efficient architecture that combines two familiar primitives – sliding window attention and linear attention – to offer high-quality language modeling with strong associative recall capabilities! At inference time, Based decodes without a KV-cache, enabling a 24x throughput improvement over Transformers with Flash-Attention 2!</p><h2><strong>Overview</strong></h2><p>In an <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a> (and <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">blogpost</a>) we posted towards the end of last year, we share the finding that many efficient architectures (e.g. <a href="https://arxiv.org/abs/2312.00752">Mamba</a>, <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a>, <a href="https://arxiv.org/abs/2302.10866">Hyena</a>, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>) underperform Transformers on recall, the ability to ground generations on information seen in-context, which is critical for in-context learning and copying. We used this analysis to design a new Based architecture (previewed in this <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based">blogpost</a>). We’re excited to share the latest progress in this line of work. </p><p>Our recent work digs deeper into the recall challenge. We begin by illustrating a fundamental tradeoff between a model’s recall abilities and the size of its recurrent state during generation. This analysis informs the design of Based, a simple recurrent architecture that outperforms prior sub-quadratic models on real-world recall-intensive tasks (information extraction, reading comprehension) and in-context learning (few-shot natural language understanding on SuperGLUE). At the same time, Based offers fast  generation speeds: Based is 56% and 44% faster at processing prompts than FlashAttention-2 and Mamba respectively (4k sequence length, 1.3Bn parameters). Based also offers 24x higher throughput than FlashAttention-2 in next token prediction (generating 1024 tokens, 128 batch size, 1.3Bn parameters). </p><p>We’re particularly excited about the <em>simplicity</em> of Based. Using just two well-known, familiar, attention-like building blocks, sliding window attention (with <em>tiny</em> window sizes) and linear attention (with Taylor series approximation of exp(QK^T)), we can outperform the strongest sub-quadratic architectures on language modeling and achieve massive speedups over optimized Transformers! </p><p>This blogpost provides an overview of our 1) analysis on recall in sub-quadratic architectures that leads to the Based architecture’s design and 2) how we make Based go brrrr! </p><h2><strong>Motivating analysis: the recall-memory tradeoff</strong></h2><p><strong>‍</strong>The main question driving our exploration is: <em>can we drastically improve the real-world speed and memory consumption of language models without compromising on recall and in-context learning capability?</em> </p><p>To begin answering this question, we had to first think about what slows architectures down. Efficient architectures (<em>e.g.</em> Mamba) are much faster than Transformers at inference time (<em>e.g. </em>5x higher throughput) in large part because they have a reduced memory footprint. Smaller memory footprint means larger batch sizes and less I/O. However, it also makes intuitive sense that reducing memory footprint too much could hurt a model’s capacity to recall information seen earlier in the sequence. This looked to us like a classic “<em>no free lunch”</em> situation, so we took a number of popular architectures, varied the hyper-parameters that affected the memory footprint, and evaluated performance on a challenging synthetic associative recall task.</p><p><em>The recall-memory tradeoff. </em><strong> </strong>We found that all architectures obeyed a fundamental tradeoff: the less memory the model consumed during inference, the worse it did on associative recall. We focused on the <em>recurrent state size, </em>the number of bytes used to represent previously seen tokens when generating tokens one-by-one (<em>i.e.</em> recurrently). </p><p>In attention, the <em>state </em>is commonly referred to as the KV-cache, and it grows with the length of the sequence. In the top right of Figure 1, we can see that attention performs recall perfectly, albeit at the cost of a huge recurrent state. Sliding window attention provides a way to cap the size of the KV-cache, but we found (unsurprisingly) that recall performance drops off rapidly as we reduce the size of the recurrent state (<em>e.g.</em> from 100% with 1MB recurrent state to 50% with a 65 KB recurrent state) (Figure 1, light blue). </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557e1f75f13323664294a_blogpost-01.png" loading="lazy" alt=""/></p></figure><p>Excitingly, we found that Mamba expands the pareto frontier of the recall-memory tradeoff curve beyond sliding window attention. This means it is making <strong>better use of limited recurrent state size</strong> than approaches like sliding window attention. </p><p>The natural question is: <em>are there other, perhaps simpler, models that can also expand the pareto frontier?</em></p><h2>‍<strong>Based<em>: </em>a simple model at the pareto frontier</strong></h2><p><strong>‍</strong>To answer this question, we started studying why the simplest alternatives to softmax attention fail to strike a favorable tradeoff. As a further design principle, we searched for primitives that could scale well on current and future hardware. For instance, it would be nice if our primitives could leverage GPU Tensor Cores, specialized hardware on modern GPUs that can perform matrix multiplications (GEMMs) 16x faster for 16x16 matrices than the default (CUDA cores)!</p><p>In our <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a>, we did a deep dive on why any model with a convolutional view (<em>e.g. </em>H3 or Hyena) will struggle on recall. Next, we considered two of the simplest efficient attention techniques out there: (1) <a href="https://arxiv.org/abs/2004.05150">sliding</a> <a href="https://arxiv.org/abs/2007.14062">window</a> <a href="https://mistral.ai/news/announcing-mistral-7b/">attention</a> and (2) <a href="https://arxiv.org/abs/2006.16236">linear attention</a> (<em>i.e.</em> attention without softmax).</p><p>Our experiments on real-world language modeling (up to 1.4bn parameters) and synthetic associative recall suggested to us that neither primitive alone would suffice to navigate the pareto frontier.</p><ol role="list"><li>We found that pure linear attention models struggled to perform precise local token shifts and token comparisons, skills important in recall (Fu et al., 2023; Arora et al., 2023a), as well as dense attention. Expanding on our findings, we do find that our pure linear attention model improves over earlier sub-quadratic architectures. Focusing on the recall-intensive slice of the Pile test set (i.e. next token predictions that force the model to use the prior context vs. memorized knowledge), the 355M pure linear attention model outperforms RWKV-v5 by 0.1 ppl and H3 by 2.6 ppl (Table 1, paper). Pure linear attention is even comparable to the Mamba architecture on this recall-slice – 2.21 ppl for Mamba vs. 2.29 for pure linear attention! However, we observe a sizeable gap to Transformers, which achieve 1.87 ppl on the recall slice. </li><li>In sliding window attention, models can only recall tokens within the sliding window (Figure 2, center). As we increase the window size, the recurrent state grows linearly and has a non-linear effect on speed during parallel training and inference (Figure 2, left).</li></ol><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e556927f9f5016be3204c8_figure-01.png" loading="lazy" alt=""/></p></figure><p>However, we find the two primitives are complementary – linear attention for modeling long-range token interactions and sliding window for local token interactions in the sequence. We combined them into a single architecture, called Based (Figure 2, right). </p><ol role="list"><li>Sliding window attention can perform the precise <em>local </em>shifts needed for associative recall. We use <em>tiny </em>window sizes (e.g. 64 in experiments) contrasting the larger window sizes in architectures like <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and the recently proposed <a href="https://arxiv.org/abs/2402.19427">Griffin</a>. Intuitively more attention (larger window sizes) is nice from a quality perspective, but we’d like to balance quality and wall-clock speed. To balance these objectives, let’s take a look at the left plot in the above figure. Observe that the latency of matrix multiplication for 16x16 vs. 64x64 matrices are roughly equal, and beyond 64, latency grows non-linearly with the window size.  Note that the rough similarity between 16x16 and 64x64 is because the latter keeps the GPU tensor core occupancy high enough to saturate!</li><li>Linear attention enables <em>global </em>token interactions, while maintaining a fixed size recurrent state. Unlike softmax attention, the size of linear attention’s recurrent state is a function of hyperparameters (<em>e.g.</em> choice of feature map) and not sequence length. This allows us to traverse the tradeoff space smoothly. We use a <strong>Taylor approximation of the exponential function as the feature map</strong>, that was first used in <a href="https://arxiv.org/abs/2402.04347">our prior work</a> on linear attentions!</li></ol><p>Critically, the recurrent state size in Based does not grow with the sequence length, as it does in attention. Instead, it is determined by the linear attention feature dimension and the window size. <strong>By dialing these hyperparameters, we can tradeoff recall for throughput and navigate the pareto frontier in Figure 1.  </strong></p><p>‍</p><p>Despite its simplicity, on real language modeling experiments (up to at least 1.3 billion parameters), Based is competitive with Mamba in terms of overall Pile perplexity and standard zero-shot benchmarks from the <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM eval harness</a> (shown under Question Answering - Common). </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5522e1c637fb50e3155b9_oIYg-xYDMbP9eAfBzRAVAfObwm4UrEQo_3fvq2ToPW8kGV722Si8b_h6oYgY6oPB-52hlPHe_gF5od4BQMrG440RNL7Dfwk_V1iSNOgafKsPQQaPRFaOnV4r5ix0zhLMh3hnI5lL4yi8wIJgRiDAN2A.png" alt=""/></p></figure><p>These commonly-used zero-shot benchmarks are limited to extremely short text, so they don’t stress test models’ recall capabilities. To address this shortcoming, we <a href="https://arxiv.org/abs/2304.09433">curated a small suite of <em>real world recall-intensive</em> benchmarks</a> that require recalling information from long documents (<em>e.g. </em>information extraction from <a href="https://pubmed.ncbi.nlm.nih.gov/21321283/https://pubmed.ncbi.nlm.nih.gov/21321283/">FDA documents</a> and <a href="https://paperswithcode.com/dataset/swde">raw HTML</a>, and reading comprehension).<em> </em>Based is the strongest sub-quadratic architecture on these tasks, outperforming Mamba by 6.22 accuracy points on average. However, both Based and Mamba still underperform the strongest Transformer baseline, sometimes by large margins. This is consistent with our “no free lunch” observation above. </p><p>It’s important to note that we don’t believe Based is the only architecture that can operate at this point on the tradeoff curve. For example, we show in our paper that we can replace the sliding window attention with short-convolutions (filter size 3) and achieve similar performance within 0.1 perplexity points. We suspect that there are lots of other architectures that can also match this pareto frontier and we’re hopeful there are even others that can even expand beyond it! </p><h2>‍<strong>How we use our fixed-size recurrent state matters too! </strong></h2><p>There are many recurrent architectures that might have the same hidden state size, but our work highlights how the featurization (e.g. linear attention feature map, state update mechanism) matters as well. Our choice for the map in Based is surprisingly simple (<em>high-school calculus is all you need)</em>: approximating the exponential with a Taylor series. We compute $\phi$ such that $\phi(q) \phi(k)^T \approx \exp (q k^T)$. We use just the second-order Taylor series as in our prior work, where $\hat{\exp}(x) = 1 + x + x^2 / 2$! Note that if $x$ has dimension $d’$ then the  $x^2$ term will have dimension $d’^2$! The result of the key-value outer product (step 1 above) grows quickly in $d’$, expanding the state size for Based. </p><p><em>How much does our choice of featurization vs. the expanded state size matter when leading to the quality of Based?</em><strong><em> </em></strong>The model’s ability to <em>use the state effectively</em> is key. Shown in the accuracy vs. recurrent state size tradeoff curves, several alternatives to the Taylor map fall <em>below</em> the pareto frontier. Below we compare to models that expand the state size using learned projections and then apply popular feature maps (<a href="https://arxiv.org/abs/2009.14794">Performer</a>, <a href="https://arxiv.org/abs/2202.08791">CosFormer</a>, <a href="https://proceedings.mlr.press/v119/katharopoulos20a.html">PosELU</a>) from the literature. We train these models on the <a href="https://github.com/HazyResearch/zoology">MQAR synthetic</a> test for associative recall and sweep hyperparameters (learning rate) for all points shown in the plot below, finding that the Taylor map is most effective. This trend carries to real world experiments on the Pile language modeling corpus (see our paper for more).</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5fa9f5c6f4e90a0584448_Horizontal%20Figure.png" loading="lazy" alt=""/></p></figure><h2>IO and dataflow-aware implementation</h2><p>The next key question is how to make Based competitive in wall clock efficiency. Linear attention is theoretically more efficient than standard attention as a function of sequence length. However, existing implementations of linear attention methods are often <em>slower</em> than well-optimized attention implementations like <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>. </p><p>In Based, we use the 2nd degree Taylor approximation, which expands the dimension of the keys, leading to large state sizes and large memory consumption O(Nd’<sup>2</sup>d), in sequence length N, key dimension d’, and value dimension d (discussed above). The large resulting key-value state makes naïve implementations of Taylor linear attention quite slow. </p><p>First let’s revisit a bit of context on how the hardware works. GPUs have small amounts of fast-to-access memory (thread-specific registers, shared memory at the warp/32-threads level using SRAM) and large amounts of slow-to-access memory (HBM). It is important to reduce the number of reads-and-writes between slow HBM and SRAM as well as SRAM and registers to unlock efficiency. We present new IO-aware algorithms for the Taylor linear attention forward pass and inference that reduce the HBM to SRAM data movement by $O(Nd&#39;^2)$ bytes and the SRAM to register data movement by $O(Nd^{2}d&#39;)$ bytes. Our algorithm allows holding the KV state <em>in-thread-register</em> at feature dimension d’ = 16, which we use in experiments. </p><p>Below we include a comparison between the naive Taylor attention forward pass, an implementation that leverages the popular linear attention kernels from <a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py">Fast Transformers</a>, and our custom kernels are shown below across batch size (sequence length 1024). </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5ffb985fe463472153665_Screenshot%202024-03-04%20at%209.06.47%E2%80%AFAM.png" loading="lazy" alt=""/></p></figure><p>We then compare the end-to-end generation speeds of FlashAttention-2, Mamba, and Based 360M and 1.3Bn parameter models using our IO-aware algorithms. We hold the batch size to 2 for prefill, and generate 1024 tokens for next token prediction. Strikingly, Based achieves up to 24x higher throughput than FlashAttention-2! </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557d27b21a0953533a862_Figure%20from%20DM.png" loading="lazy" alt=""/></p></figure><p><em>Stay tuned! </em>These algorithms are implemented in an exciting new CUDA DSL called ThunderKittens, that’s being developed by our lab. Stay tuned for more on this soon – we hope the DSL improves the accessibility of CUDA development! In contrast to frameworks like Triton, which makes opinionated decisions about the supported scope of operations the user can perform, our DSL is <em>embedded</em> in C++. We’re really excited to share it and get your feedback! We’re cooking up more model artifacts alongside in the coming weeks, motivated by the question: <em>What models does the hardware want? </em></p><p>‍</p><p>You can play with our checkpoints and evaluations on <a href="https://huggingface.co/collections/hazyresearch/based-65d77fb7a6f9c813c8b94339c">Hugging Face</a> and in this code repository: <strong><em> </em></strong><a href="https://github.com/HazyResearch/based">https://github.com/HazyResearch/based</a>! Huge thank you to<a href="https://www.together.ai/"> Together AI</a>, <a href="https://hai.stanford.edu/">Stanford HAI</a>, and <a href="https://crfm.stanford.edu/">Stanford CRFM</a> for supporting this work! Please send your feedback and questions to: Simran Arora (<a href="mailto:simarora@stanford.edu">simarora@stanford.edu</a>), Sabri Eyuboglu (<a href="mailto:eyuboglu@stanford.edu">eyuboglu@stanford.edu</a>), Michael Zhang (<a href="mailto:mzhang@stanford.edu">mzhang@stanford.edu</a>). </p></div></div>
  </body>
</html>
