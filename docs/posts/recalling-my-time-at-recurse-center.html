<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.thapaliya.com/en/writings/recurse-center-2023-return-statement/">Original</a>
    <h1>Recalling my time at Recurse Center</h1>
    
    <div id="readability-page-1" class="page"><div><div><h2>Before I forget</h2><p>Exactly 6 months has passed since I posted the following for my final check-in as a RC participant (Summer1 2023).</p><pre><code>**Wk. 12 - Day 5 | 4th August ‚Äò23**

- Last day of the batch, more like checkout, not doing any ‚Äúwork‚Äù, going out after posting this
- Re-read ‚ÄòA forty-year career‚Äô slowly today, something I want to think more about
  - [https://lethain.com/forty-year-career/](https://lethain.com/forty-year-career/)
- Went back through my very first checkins to see how aligned I‚Äôve been to myself
  - some hits, some misses, some surprises üòÖ
- Rest of the month is pretty much time off
  - while travelling, will be pondering on what I‚Äôd like to work on next

I‚Äôd also like to take the space of these two lines to thank everybody in my batch as well as everybody else that I‚Äôve interacted with either through Zulip or perhaps online.
Even if I‚Äôve been sorta background tired for most of my batch, this has been an incredibly joyful experience, the energy in the end-of-the-week presentations are the best.

Looking forward to check in after my vacations again. Thank you everybody ! ü§ó
</code></pre><p>And for these past 6 months, I&#39;ve thorougly enjoyed my sabbatical. Far removed from my programming/tinkering space, I mostly spent time with family, friends &amp; slow traveling. During this period, I hadn&#39;t really prioritized my time to reflect on my RC experience. Now that I&#39;m back, and ready to move on to the next thing, thought I&#39;d write a quick short note, before it all becomes a blur.</p><h2>What kept me busy</h2><p>Lucky for me, I have all my daily checkins well organized to help me recollect what I&#39;d been working on during my RC batch. While I did attend a lot of meetings &amp; some book clubs as well, here are some of the main things that I accomplished during my cohort.</p><ul><li>went through Andrej Karpathy&#39;s &#34;Neural Networks from Zero to Hero&#34; course and <a href="https://github.com/suvash/nnze2he">built language models from scratch in PyTorch</a>.</li><li>based on the success of the tiny language model, also worked on a fake taxonomy generator, which <a href="https://github.com/suvash/taxophoney">had to be named taxophoney</a>.</li><li>worked through a <a href="https://github.com/suvash/nand2tetris">good portion of the nand2tetris course</a>, implementing ALU, assemblers, VM translators, etc.</li><li>read several books related to AI/ML like &#34;Life 3.0&#34; and &#34;<a href="https://vickiboykis.com/what_are_embeddings/next.html">What are Embeddings?</a>&#34;</li><li>experimented a bit with a <a href="https://www.thapaliya.com/en/writings/introduction-to-mojo-delft-fastai/">new programming language called Mojo</a>, as well as a <a href="https://www.forth.com/starting-forth/">very old one called Forth</a>.</li><li>explored large language models by <a href="https://www.wandb.courses/courses/training-fine-tuning-LLMs">taking some courses on building LLMs</a> and experimenting with models like Claude and Stable Diffusion.</li><li>wrote some blog posts documenting my learning and experiences, though not anywhere close to what I should have.</li><li>got everything in place to set up a NixOS system configured fully with flakes, which actually got accomplished later after the batch.</li><li>socialized and had quite a bunch of coffee chats to get to know fellow recursers.</li></ul><h2>In retrospect</h2><p>For a variety of reasons, I ended up starting RC pretty much the next day after I had wrapped up at my previous workplace. If I were to do RC again, I would absolutely want to avoid this. The first fews weeks felt rather exhausting and could have been much easier on my brain, had I taken a short break beforehand.</p><p>But, apart from that I have zero complains. I&#39;ve had the most fun tinkering around with goofy projects while chatting with people way smarter than me. 6 months later, I can see clearly that it has rekindled my joy in programming. I can definitely see myself taking more RC &#39;breaks&#39; in the future.</p><h2>Before I go</h2><p>One final output from the artifact created during my RC batch. Here&#39;s a &#39;daily check-in note&#39; generated by the same language model that I trained on my daily check-in notes. Unfortunately, it was too little data &amp; the model had to be severly overfitted to generate anything meaningful.</p><pre><code>THIS FILE CONTAINS GPT GENERATED TEXT.
NONE OF THE CONTENTS BELOW IS A VALID CHECK-IN.
---

Rough plans for tomorrow

- Probably just focus one
    - Maybe attend the continue afternoon
- attending realised that I‚Äôm pretty tired, time to slow though
- Lots of things that further difficult
- Read up more on embeddings and Databricks LLM things


**Wk. 2 - Day 5 | 26th May ‚Äò23**

- Coffee chechats with summarize-2ated)
- Didn‚Äôt end up some conversations
- Take plenty of time off - no point piling up things back on and some facetime calls
- woke up in time, very slow and lazy breakfast,
- spent the morning cleaning up the N2T Hack assembler
- Coffee chats and maybe a sunny weekend
- I‚Äôll let RC plans to be on the backburner for this weekend


**Wk. 3 - Intentions**

- Finish Karpathy‚Äôs NN course ü§ûüèΩ
    - This is the main priority, everything else can more for RC things and very
- Slow on the day, so I tried had opt to date and say on
    - plan the next ratter today (25th May)
- complexity no much to chunch of the things in the temptation feel which on I missued readings
    - Did poorly at this LLM(Grountner in the future, esp. in the context of GPU Compute.
</code></pre></div></div></div>
  </body>
</html>
