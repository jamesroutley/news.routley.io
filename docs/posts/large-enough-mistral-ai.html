<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mistral-large-2407/">Original</a>
    <h1>Large Enough – Mistral AI</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-mini.png" alt="Detailed benchmarks" width="5%"/></p><p>This latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications.</p><h3 id="mistral-large-2">Mistral Large 2</h3><p>Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash.</p><p>Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node.
We are releasing Mistral Large 2 under the <a href="https://mistral.ai/licenses/MRL-0.1.md">Mistral Research License</a>, that allows usage and modification for research and non-commercial usages.</p><h5 id="general-performance">General performance</h5><p>Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models.</p><h5 id="code--reasoning">Code &amp; Reasoning</h5><p>Following our experience with <a href="https://mistral.ai/news/codestral/">Codestral 22B</a> and <a href="https://mistral.ai/news/codestral-mamba/">Codestral Mamba</a>, we trained Mistral Large 2 on a very large proportion of code. Mistral Large 2 vastly outperforms the previous Mistral Large, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-performance.png" alt="Detailed benchmarks" width="100%"/></p><p>A significant effort was also devoted to enhancing the model’s reasoning capabilities. One of the key focus areas during training was to minimize the model’s tendency to “hallucinate” or generate plausible-sounding but factually incorrect or irrelevant information. This was achieved by fine-tuning the model to be more cautious and discerning in its responses, ensuring that it provides reliable and accurate outputs.</p><p>Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills:</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-code-generation.png" alt="Detailed benchmarks" width="100%"/></p><p>Performance accuracy on code generation benchmarks (all models were benchmarked through the same evaluation pipeline)</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-multiple.png" alt="Detailed benchmarks" width="100%"/></p><p>Performance accuracy on MultiPL-E (all models were benchmarked through the same evaluation pipeline, except for the &#34;paper&#34; row)</p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-code-generation-2.png" alt="Detailed benchmarks" width="100%"/></p><p>Performance accuracy on GSM8K (8-shot) and MATH (0-shot, no CoT) generation benchmarks (all models were benchmarked through the same evaluation pipeline)</p></div><h5 id="instruction-following--alignment">Instruction following &amp; Alignment</h5><p>We drastically improved the instruction-following and conversational capabilities of Mistral Large 2. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. Below we report the performance on MT-Bench, Wild Bench, and Arena Hard benchmarks:</p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-instruction.png" alt="Detailed benchmarks" width="100%"/></p><p>Performance on general alignment benchmarks (all models were benchmarked through the same evalutation pipeline)</p></div><p>On some benchmarks, generating lengthy responses tends to improve the scores. However, in many business applications, conciseness is paramount – short model generations facilitate quicker interactions and are more cost-effective for inference. This is why we spent a lot of effort to ensure that generations remain succinct and to the point whenever possible. The graph below reports the average length of generations of different models on questions from the MT Bench benchmark:</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-mtbench.png" alt="MT Bench benchmarks" width="100%"/></p><h5 id="language-diversity">Language diversity</h5><p>A large fraction of business use cases today involve working with multilingual documents. While the majority of models are English-centric, the new Mistral Large 2 was trained on a large proportion of multilingual data. In particular, it excels in English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi. Below are the performance results of Mistral Large 2 on the multilingual MMLU benchmark, compared to the previous Mistral Large, Llama 3.1 models, and to Cohere’s Command R+.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-mmlu.png" alt="Detailed benchmarks" width="70%"/></p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-language-diversity.png" alt="Detailed benchmarks" width="100%"/></p><p>Performance on Multilingual MMLU (measured on the base pretrained model)</p></div><h5 id="tool-use--function-calling">Tool Use &amp; Function Calling</h5><p>Mistral Large 2 is equipped with enhanced function calling and retrieval skills and has undergone training to proficiently execute both parallel and sequential function calls, enabling it to serve as the power engine of complex business applications.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-tool-use.png" alt="Detailed benchmarks" width="100%"/></p><h5 id="try-mistral-large-2-on-la-plateforme">Try Mistral Large 2 on la Plateforme</h5><p>You can use Mistral Large 2 today via <a href="https://console.mistral.ai/">la Plateforme</a> under the name <code>mistral-large-2407</code>, and test it on le Chat. It is available under the version 24.07 (a YY.MM versioning system that we are applying to all our models), and the API name <code>mistral-large-2407</code>. Weights for the instruct model are <a href="https://models.mistralcdn.com/mistral-large-2407/mistral-large-instruct-2407.tar">available</a> and are also hosted on <a href="https://huggingface.co/mistralai/Mistral-Large-Instruct-2407">HuggingFace</a>.</p><p>we are consolidating the offering on la Plateforme around two general purpose models, Mistral Nemo and Mistral Large, and two specialist models, Codestral and Embed. As we progressively deprecate older models on la Plateforme, all Apache models (Mistral 7B, Mixtral 8x7B and 8x22B, Codestral Mamba, Mathstral) remain available for deployment and fine-tuning using our SDK mistral-inference and mistral-finetune.</p><p>Starting today, we are extending fine-tuning capabilities on la Plateforme: those are now available for Mistral Large, Mistral Nemo and Codestral.</p><h5 id="access-mistral-models-through-cloud-service-providers">Access Mistral models through cloud service providers</h5><p>We are proud to partner with leading cloud service providers to bring the new Mistral Large 2 to a global audience. In particular, today we are expanding our partnership with Google Cloud Platform to bring Mistral AI’s models on <a href="https://cloud.google.com/blog/products/ai-machine-learning/codestral-and-mistral-large-v2-on-vertex-ai?e=48754805&amp;hl=en">Vertex AI</a> via a Managed API. Mistral AI’s best models are now available on Vertex AI, in addition to Azure AI Studio, Amazon Bedrock and IBM watsonx.ai.</p><h5 id="availability-timeline-of-mistral-ai-models">Availability timeline of Mistral AI models</h5><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-availability.png" alt="Detailed benchmarks" width="100%"/></p></div></div>
  </body>
</html>
