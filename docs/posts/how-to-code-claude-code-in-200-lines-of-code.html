<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mihaileric.com/The-Emperor-Has-No-Clothes/">Original</a>
    <h1>How to Code Claude Code in 200 Lines of Code</h1>
    
    <div id="readability-page-1" class="page"><div data-reactid="11"><p>
  <a href="https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="emperor has no clothes" title="" src="https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg" srcset="/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-f3581.jpeg 240w,
/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-b0573.jpeg 480w,
/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg 700w" sizes="(max-width: 700px) 100vw, 700px"/>
    </span>
  </span>
  
  </a>
    </p>
<p>Today AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code. </p>
<p>But here’s the thing: the core of these tools isn’t magic. It’s about 200 lines of straightforward Python. </p>
<p>Let’s build a functional coding agent from scratch.</p>
<h2>The Mental Model</h2>
<p>Before we write any code, let’s understand what’s actually happening when you use a coding agent. It’s essentially just a conversation with a powerful LLM that has a toolbox. </p>
<ol>
<li><strong>You</strong> send a message (“Create a new file with a hello world function”)</li>
<li><strong>The LLM</strong> decides it needs a tool and responds with a structured tool call (or multiple tool calls)</li>
<li><strong>Your program</strong> executes that tool call locally (actually creates the file)</li>
<li><strong>The result</strong> gets sent back to the LLM</li>
<li><strong>The LLM</strong> uses that context to continue or respond</li>
</ol>
<p>That’s the whole loop. The LLM never actually touches your filesystem. It just <em>asks</em> for things to happen, and your code makes them happen.</p>
<h2>Three Tools You Need</h2>
<p>Our coding agent fundamentally needs three capabilities:</p>
<ul>
<li><strong>Read files</strong> so the LLM can see your code</li>
<li><strong>List files</strong> so it can navigate your project</li>
<li><strong>Edit files</strong> so it can give the directive to create and modify code</li>
</ul>
<p>That’s it. Production agents like Claude Code have a few more capabilities including <code>grep</code>, <code>bash</code>, <code>websearch</code>, etc but for our purposes we’ll see that three tools is sufficient to do incredible things.</p>
<h2>Setting Up the Scaffolding</h2>
<p>We start with basic imports and an API client. I’m using OpenAI here, but this works with any LLM provider:</p>
<div>
      <pre><code><span>import</span> inspect
<span>import</span> json
<span>import</span> os

<span>import</span> anthropic
<span>from</span> dotenv <span>import</span> load_dotenv
<span>from</span> pathlib <span>import</span> Path
<span>from</span> typing <span>import</span> Any<span>,</span> Dict<span>,</span> List<span>,</span> Tuple

load_dotenv<span>(</span><span>)</span>

claude_client <span>=</span> anthropic<span>.</span>Anthropic<span>(</span>api_key<span>=</span>os<span>.</span>environ<span>[</span><span>&#34;ANTHROPIC_API_KEY&#34;</span><span>]</span><span>)</span></code></pre>
      </div>
<p>Some terminal colors to make outputs readable:</p>
<div>
      <pre><code>YOU_COLOR <span>=</span> <span>&#34;\u001b[94m&#34;</span>
ASSISTANT_COLOR <span>=</span> <span>&#34;\u001b[93m&#34;</span>
RESET_COLOR <span>=</span> <span>&#34;\u001b[0m&#34;</span></code></pre>
      </div>
<p>And a utility to resolve file paths (so <code>file.py</code> becomes <code>/Users/you/project/file.py</code>):</p>
<div>
      <pre><code><span>def</span> <span>resolve_abs_path</span><span>(</span>path_str<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Path<span>:</span>
    <span>&#34;&#34;&#34;
    file.py -&gt; /Users/you/project/file.py
    &#34;&#34;&#34;</span>
    path <span>=</span> Path<span>(</span>path_str<span>)</span><span>.</span>expanduser<span>(</span><span>)</span>
    <span>if</span> <span>not</span> path<span>.</span>is_absolute<span>(</span><span>)</span><span>:</span>
        path <span>=</span> <span>(</span>Path<span>.</span>cwd<span>(</span><span>)</span> <span>/</span> path<span>)</span><span>.</span>resolve<span>(</span><span>)</span>
    <span>return</span> path</code></pre>
      </div>
<h2>Implementing the Tools</h2>
<p>Note you should be detailed about your tool function docstrings as they will be used by the LLM to reason about what tools should be called during the conversation. More on this below. </p>
<h3>Tool 1: Read File</h3>
<p>The simplest tool. Take a filename, return its contents:</p>
<div>
      <pre><code><span>def</span> <span>read_file_tool</span><span>(</span>filename<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>&#34;&#34;&#34;
    Gets the full content of a file provided by the user.
    :param filename: The name of the file to read.
    :return: The full content of the file.
    &#34;&#34;&#34;</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>filename<span>)</span>
    <span>print</span><span>(</span>full_path<span>)</span>
    <span>with</span> <span>open</span><span>(</span><span>str</span><span>(</span>full_path<span>)</span><span>,</span> <span>&#34;r&#34;</span><span>)</span> <span>as</span> f<span>:</span>
        content <span>=</span> f<span>.</span>read<span>(</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>&#34;file_path&#34;</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>&#34;content&#34;</span><span>:</span> content
    <span>}</span></code></pre>
      </div>
<p>We return a dictionary because the LLM needs structured context about what happened.</p>
<h3>Tool 2: List Files</h3>
<p>Navigate directories by listing their contents:</p>
<div>
      <pre><code><span>def</span> <span>list_files_tool</span><span>(</span>path<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>&#34;&#34;&#34;
    Lists the files in a directory provided by the user.
    :param path: The path to a directory to list files from.
    :return: A list of files in the directory.
    &#34;&#34;&#34;</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>path<span>)</span>
    all_files <span>=</span> <span>[</span><span>]</span>
    <span>for</span> item <span>in</span> full_path<span>.</span>iterdir<span>(</span><span>)</span><span>:</span>
        all_files<span>.</span>append<span>(</span><span>{</span>
            <span>&#34;filename&#34;</span><span>:</span> item<span>.</span>name<span>,</span>
            <span>&#34;type&#34;</span><span>:</span> <span>&#34;file&#34;</span> <span>if</span> item<span>.</span>is_file<span>(</span><span>)</span> <span>else</span> <span>&#34;dir&#34;</span>
        <span>}</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>&#34;path&#34;</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>&#34;files&#34;</span><span>:</span> all_files
    <span>}</span></code></pre>
      </div>
<h3>Tool 3: Edit File</h3>
<p>This is the most complex tool, but still straightforward. It handles two cases:</p>
<ul>
<li><strong>Creating a new file</strong> when <code>old_str</code> is empty</li>
<li><strong>Replacing text</strong> by finding <code>old_str</code> and replacing with <code>new_str</code></li>
</ul>
<div>
      <pre><code><span>def</span> <span>edit_file_tool</span><span>(</span>path<span>:</span> <span>str</span><span>,</span> old_str<span>:</span> <span>str</span><span>,</span> new_str<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>&#34;&#34;&#34;
    Replaces first occurrence of old_str with new_str in file. If old_str is empty,
    create/overwrite file with new_str.
    :param path: The path to the file to edit.
    :param old_str: The string to replace.
    :param new_str: The string to replace with.
    :return: A dictionary with the path to the file and the action taken.
    &#34;&#34;&#34;</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>path<span>)</span>
    <span>if</span> old_str <span>==</span> <span>&#34;&#34;</span><span>:</span>
        full_path<span>.</span>write_text<span>(</span>new_str<span>,</span> encoding<span>=</span><span>&#34;utf-8&#34;</span><span>)</span>
        <span>return</span> <span>{</span>
            <span>&#34;path&#34;</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
            <span>&#34;action&#34;</span><span>:</span> <span>&#34;created_file&#34;</span>
        <span>}</span>
    original <span>=</span> full_path<span>.</span>read_text<span>(</span>encoding<span>=</span><span>&#34;utf-8&#34;</span><span>)</span>
    <span>if</span> original<span>.</span>find<span>(</span>old_str<span>)</span> <span>==</span> <span>-</span><span>1</span><span>:</span>
        <span>return</span> <span>{</span>
            <span>&#34;path&#34;</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
            <span>&#34;action&#34;</span><span>:</span> <span>&#34;old_str not found&#34;</span>
        <span>}</span>
    edited <span>=</span> original<span>.</span>replace<span>(</span>old_str<span>,</span> new_str<span>,</span> <span>1</span><span>)</span>
    full_path<span>.</span>write_text<span>(</span>edited<span>,</span> encoding<span>=</span><span>&#34;utf-8&#34;</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>&#34;path&#34;</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>&#34;action&#34;</span><span>:</span> <span>&#34;edited&#34;</span>
    <span>}</span></code></pre>
      </div>
<p>The convention here: empty <code>old_str</code> means “create this file.” Otherwise, find and replace. Real IDEs add sophisticated fallback behavior when the string isn’t found, but this works.</p>
<h2>The Tool Registry</h2>
<p>We need a way to look up tools by name:</p>
<div>
      <pre><code>TOOL_REGISTRY <span>=</span> <span>{</span>
    <span>&#34;read_file&#34;</span><span>:</span> read_file_tool<span>,</span>
    <span>&#34;list_files&#34;</span><span>:</span> list_files_tool<span>,</span>
    <span>&#34;edit_file&#34;</span><span>:</span> edit_file_tool 
<span>}</span></code></pre>
      </div>
<h2>Teaching the LLM About Our Tools</h2>
<p>The LLM needs to know what tools exist and how to call them. We generate this dynamically from our function signatures and docstrings:</p>
<div>
      <pre><code><span>def</span> <span>get_tool_str_representation</span><span>(</span>tool_name<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> <span>str</span><span>:</span>
    tool <span>=</span> TOOL_REGISTRY<span>[</span>tool_name<span>]</span>
    <span>return</span> f<span>&#34;&#34;&#34;
    Name: {tool_name}
    Description: {tool.__doc__}
    Signature: {inspect.signature(tool)}
    &#34;&#34;&#34;</span>

<span>def</span> <span>get_full_system_prompt</span><span>(</span><span>)</span><span>:</span>
    tool_str_repr <span>=</span> <span>&#34;&#34;</span>
    <span>for</span> tool_name <span>in</span> TOOL_REGISTRY<span>:</span>
        tool_str_repr <span>+=</span> <span>&#34;TOOL\n===&#34;</span> <span>+</span> get_tool_str_representation<span>(</span>tool_name<span>)</span>
        tool_str_repr <span>+=</span> f<span>&#34;\n{&#39;=&#39;*15}\n&#34;</span>
    <span>return</span> SYSTEM_PROMPT<span>.</span><span>format</span><span>(</span>tool_list_repr<span>=</span>tool_str_repr<span>)</span></code></pre>
      </div>
<p>And the system prompt itself:</p>
<div>
      <pre><code>SYSTEM_PROMPT <span>=</span> <span>&#34;&#34;&#34;
You are a coding assistant whose goal it is to help us solve coding tasks. 
You have access to a series of tools you can execute. Here are the tools you can execute:

{tool_list_repr}

When you want to use a tool, reply with exactly one line in the format: &#39;tool: TOOL_NAME({{JSON_ARGS}})&#39; and nothing else.
Use compact single-line JSON with double quotes. After receiving a tool_result(...) message, continue the task.
If no tool is needed, respond normally.
&#34;&#34;&#34;</span></code></pre>
      </div>
<p>This is the key insight: we’re just telling the LLM “here are your tools, here’s the format to call them.” The LLM figures out when and how to use them.</p>
<h2>Parsing Tool Calls</h2>
<p>When the LLM responds, we need to detect if it’s asking us to run a tool:</p>
<div>
      <pre><code><span>def</span> <span>extract_tool_invocations</span><span>(</span>text<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> List<span>[</span>Tuple<span>[</span><span>str</span><span>,</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>]</span><span>]</span><span>:</span>
    <span>&#34;&#34;&#34;
    Return list of (tool_name, args) requested in &#39;tool: name({...})&#39; lines.
    The parser expects single-line, compact JSON in parentheses.
    &#34;&#34;&#34;</span>
    invocations <span>=</span> <span>[</span><span>]</span>
    <span>for</span> raw_line <span>in</span> text<span>.</span>splitlines<span>(</span><span>)</span><span>:</span>
        line <span>=</span> raw_line<span>.</span>strip<span>(</span><span>)</span>
        <span>if</span> <span>not</span> line<span>.</span>startswith<span>(</span><span>&#34;tool:&#34;</span><span>)</span><span>:</span>
            <span>continue</span>
        <span>try</span><span>:</span>
            after <span>=</span> line<span>[</span><span>len</span><span>(</span><span>&#34;tool:&#34;</span><span>)</span><span>:</span><span>]</span><span>.</span>strip<span>(</span><span>)</span>
            name<span>,</span> rest <span>=</span> after<span>.</span>split<span>(</span><span>&#34;(&#34;</span><span>,</span> <span>1</span><span>)</span>
            name <span>=</span> name<span>.</span>strip<span>(</span><span>)</span>
            <span>if</span> <span>not</span> rest<span>.</span>endswith<span>(</span><span>&#34;)&#34;</span><span>)</span><span>:</span>
                <span>continue</span>
            json_str <span>=</span> rest<span>[</span><span>:</span><span>-</span><span>1</span><span>]</span><span>.</span>strip<span>(</span><span>)</span>
            args <span>=</span> json<span>.</span>loads<span>(</span>json_str<span>)</span>
            invocations<span>.</span>append<span>(</span><span>(</span>name<span>,</span> args<span>)</span><span>)</span>
        <span>except</span> Exception<span>:</span>
            <span>continue</span>
    <span>return</span> invocations</code></pre>
      </div>
<p>Simple text parsing. Look for lines starting with <code>tool:</code>, extract the function name and JSON arguments.</p>
<h2>The LLM Call</h2>
<p>A thin wrapper around the API:</p>
<div>
      <pre><code><span>def</span> <span>execute_llm_call</span><span>(</span>conversation<span>:</span> List<span>[</span>Dict<span>[</span><span>str</span><span>,</span> <span>str</span><span>]</span><span>]</span><span>)</span><span>:</span>
    system_content <span>=</span> <span>&#34;&#34;</span>
    messages <span>=</span> <span>[</span><span>]</span>
    
    <span>for</span> msg <span>in</span> conversation<span>:</span>
        <span>if</span> msg<span>[</span><span>&#34;role&#34;</span><span>]</span> <span>==</span> <span>&#34;system&#34;</span><span>:</span>
            system_content <span>=</span> msg<span>[</span><span>&#34;content&#34;</span><span>]</span>
        <span>else</span><span>:</span>
            messages<span>.</span>append<span>(</span>msg<span>)</span>
    
    response <span>=</span> claude_client<span>.</span>messages<span>.</span>create<span>(</span>
        model<span>=</span><span>&#34;claude-sonnet-4-20250514&#34;</span><span>,</span>
        max_tokens<span>=</span><span>2000</span><span>,</span>
        system<span>=</span>system_content<span>,</span>
        messages<span>=</span>messages
    <span>)</span>
    <span>return</span> response<span>.</span>content<span>[</span><span>0</span><span>]</span><span>.</span>text</code></pre>
      </div>
<h2>The Agent Loop</h2>
<p>Now we put it all together. This is where the “magic” happens:</p>
<div>
      <pre><code><span>def</span> <span>run_coding_agent_loop</span><span>(</span><span>)</span><span>:</span>
    <span>print</span><span>(</span>get_full_system_prompt<span>(</span><span>)</span><span>)</span>
    conversation <span>=</span> <span>[</span><span>{</span>
        <span>&#34;role&#34;</span><span>:</span> <span>&#34;system&#34;</span><span>,</span>
        <span>&#34;content&#34;</span><span>:</span> get_full_system_prompt<span>(</span><span>)</span>
    <span>}</span><span>]</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>try</span><span>:</span>
            user_input <span>=</span> <span>input</span><span>(</span>f<span>&#34;{YOU_COLOR}You:{RESET_COLOR}:&#34;</span><span>)</span>
        <span>except</span> <span>(</span>KeyboardInterrupt<span>,</span> EOFError<span>)</span><span>:</span>
            <span>break</span>
        conversation<span>.</span>append<span>(</span><span>{</span>
            <span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>,</span>
            <span>&#34;content&#34;</span><span>:</span> user_input<span>.</span>strip<span>(</span><span>)</span>
        <span>}</span><span>)</span>
        <span>while</span> <span>True</span><span>:</span>
            assistant_response <span>=</span> execute_llm_call<span>(</span>conversation<span>)</span>
            tool_invocations <span>=</span> extract_tool_invocations<span>(</span>assistant_response<span>)</span>
            <span>if</span> <span>not</span> tool_invocations<span>:</span>
                <span>print</span><span>(</span>f<span>&#34;{ASSISTANT_COLOR}Assistant:{RESET_COLOR}: {assistant_response}&#34;</span><span>)</span>
                conversation<span>.</span>append<span>(</span><span>{</span>
                    <span>&#34;role&#34;</span><span>:</span> <span>&#34;assistant&#34;</span><span>,</span>
                    <span>&#34;content&#34;</span><span>:</span> assistant_response
                <span>}</span><span>)</span>
                <span>break</span>
            <span>for</span> name<span>,</span> args <span>in</span> tool_invocations<span>:</span>
                tool <span>=</span> TOOL_REGISTRY<span>[</span>name<span>]</span>
                resp <span>=</span> <span>&#34;&#34;</span>
                <span>print</span><span>(</span>name<span>,</span> args<span>)</span>
                <span>if</span> name <span>==</span> <span>&#34;read_file&#34;</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>&#34;filename&#34;</span><span>,</span> <span>&#34;.&#34;</span><span>)</span><span>)</span>
                <span>elif</span> name <span>==</span> <span>&#34;list_files&#34;</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>&#34;path&#34;</span><span>,</span> <span>&#34;.&#34;</span><span>)</span><span>)</span>
                <span>elif</span> name <span>==</span> <span>&#34;edit_file&#34;</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>&#34;path&#34;</span><span>,</span> <span>&#34;.&#34;</span><span>)</span><span>,</span> 
                                args<span>.</span>get<span>(</span><span>&#34;old_str&#34;</span><span>,</span> <span>&#34;&#34;</span><span>)</span><span>,</span> 
                                args<span>.</span>get<span>(</span><span>&#34;new_str&#34;</span><span>,</span> <span>&#34;&#34;</span><span>)</span><span>)</span>
                conversation<span>.</span>append<span>(</span><span>{</span>
                    <span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>,</span>
                    <span>&#34;content&#34;</span><span>:</span> f<span>&#34;tool_result({json.dumps(resp)})&#34;</span>
                <span>}</span><span>)</span></code></pre>
      </div>
<p>The structure:</p>
<ol>
<li><strong>Outer loop</strong>: Get user input, add to conversation</li>
<li>
<p><strong>Inner loop</strong>: Call LLM, check for tool invocations</p>
<ul>
<li>If no tools needed, print response and break inner loop</li>
<li>If tools needed, execute them, add results to conversation, loop again</li>
</ul>
</li>
</ol>
<p>The inner loop continues until the LLM responds without requesting any tools. This lets the agent chain multiple tool calls (read a file, then edit it, then confirm the edit).</p>
<h2>Running It</h2>
<div>
      <pre><code><span>if</span> __name__ <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
    run_coding_agent_loop<span>(</span><span>)</span></code></pre>
      </div>
<p>Now you can have conversations like:</p>
<blockquote>
<p><strong>You:</strong> Make me a new file called hello.py and implement hello world in it</p>
</blockquote>
<p>Agent calls <em>edit_file</em> with path=“hello.py”, old_str=&#34;&#34;, new_str=“print(‘Hello World’)”</p>
<blockquote>
<p><strong>Assistant:</strong> Done! Created hello.py with a hello world implementation.</p>
</blockquote>
<p>Or multi-step interactions:</p>
<blockquote>
<p><strong>You:</strong> Edit hello.py and add a function for multiplying two numbers</p>
</blockquote>
<p>Agent calls <em>read_file</em> to see current contents. Agent calls <em>edit_file</em> to add the function.</p>
<blockquote>
<p><strong>Assistant:</strong> Added a multiply function to hello.py.</p>
</blockquote>
<h2>What We Built vs. Production Tools</h2>
<p>This is about 200 lines. Production tools like Claude Code add:</p>
<ul>
<li><strong>Better error handling</strong> and fallback behaviors</li>
<li><strong>Streaming responses</strong> for better UX</li>
<li><strong>Smarter context management</strong> (summarizing long files, etc.)</li>
<li><strong>More tools</strong> (run commands, search codebase, etc.)</li>
<li><strong>Approval workflows</strong> for destructive operations</li>
</ul>
<p>But the core loop? It’s exactly what we built here. The LLM decides what to do, your code executes it, results flow back. That’s the whole architecture.</p>
<h2>Try It Yourself</h2>
<p>The <a href="https://shorturl.at/HmMeI">full source</a> is about 200 lines. Swap in your preferred LLM provider, adjust the system prompt, add more tools as an exercise. You’ll be surprised how capable this simple pattern is.</p>
<p><em>If you’re interested in learning state-of-the-art AI software development techniques for professional engineers, check out my <a href="https://maven.com/the-modern-software-developer/ai-course">online course</a>.</em></p></div></div>
  </body>
</html>
