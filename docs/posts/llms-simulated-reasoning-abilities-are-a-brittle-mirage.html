<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arstechnica.com/ai/2025/08/researchers-find-llms-are-bad-at-logical-inference-good-at-fluent-nonsense/">Original</a>
    <h1>LLMs&#39; &#34;simulated reasoning&#34; abilities are a brittle mirage</h1>
    
    <div id="readability-page-1" class="page"><div>

        
        <div>
          
          
<figure>
    <div>
              <div>
          <div>
            <p><a data-pswp-width="479" data-pswp-height="375" data-pswp-srcset="" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmtraining.png" target="_blank">
              <img width="479" height="375" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmtraining.png" alt="" decoding="async" loading="lazy"/>
            </a></p><div id="caption-2111278"><p>
              The researchers used test cases that fall outside of the LLM training data in task type, format, and length.
                              </p>
                          </div>
          </div>
        </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      The researchers used test cases that fall outside of the LLM training data in task type, format, and length.

              <span>
          Credit:

                      <a href="https://arxiv.org/pdf/2508.01191" target="_blank">
          
          Zhao et al

                      </a>
                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>These simplified models were then tested using a variety of tasks, some of which precisely or closely matched the function patterns in the training data and others that required function compositions that were either partially or fully &#34;out of domain&#34; for the training data. For instance, a model trained on data showing two cyclical shifts might be asked to perform a novel transformation involving two ROT shifts (with basic training on what a single example of either shift looks like). The final answers and reasoning steps were compared to the desired answer using <a href="https://huggingface.co/spaces/evaluate-metric/bleu">BLEU scores</a> and <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein Distance</a> for an objective measure of their accuracy.</p>
<p>As the researchers hypothesized, these basic models started to fail catastrophically when asked to generalize novel sets of transformations that were not directly demonstrated in the training data. While the models would often try to generalize new logical rules based on similar patterns in the training data, this would quite often lead to the model laying out &#34;correct reasoning paths, yet incorrect answer[s].&#34; In other cases, the LLM would sometimes stumble onto correct answers paired with &#34;unfaithful reasoning paths&#34; that didn&#39;t follow logically.</p>
<p>&#34;Rather than demonstrating a true understanding of text, CoT reasoning under task transformations appears to reflect a replication of patterns learned during training,&#34; the researchers write.</p>
<figure>
    <div>
              <div>
          <div>
            <p><a data-pswp-width="624" data-pswp-height="493" data-pswp-srcset="" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmood.png" target="_blank">
              <img width="624" height="493" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmood.png" alt="" decoding="async" loading="lazy"/>
            </a></p><div id="caption-2111284"><p>
              As requested tasks get further outside the training distribution (redder dots), the answers provided drift farther from the desired answer (lower right of the graph).
                              </p>
                          </div>
          </div>
        </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      As requested tasks get further outside the training distribution (redder dots), the answers provided drift farther from the desired answer (lower right of the graph).

              <span>
          Credit:

                      <a href="https://arxiv.org/pdf/2508.01191" target="_blank">
          
          Zhao et al

                      </a>
                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The researchers went on to test their controlled system using input text strings slightly shorter or longer than those found in the training data, or that required function chains of different lengths than those it was trained on. In both cases the accuracy of the results &#34;deteriorates as the [length] discrepancy increases,&#34; thus &#34;indicating the failure of generalization&#34; in the models. Small, unfamiliar-to-the-model discrepancies in the format of the test tasks (e.g., the introduction of letters or symbols not found in the training data) also caused performance to &#34;degrade sharply&#34; and &#34;affect[ed] the correctness&#34; of the model&#39;s responses, the researchers found.</p>

          
                  </div>

              </div></div>
  </body>
</html>
