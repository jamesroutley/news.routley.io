<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/ai-models-locally">Original</a>
    <h1>Local AI is driving the biggest change in laptops in decades</h1>
    
    <div id="readability-page-1" class="page"><div data-headline="Your Laptop Isn’t Ready for LLMs. That’s About to Change"><div><p><strong>Odds are the PC in </strong>your office today isn’t ready to run AI <a data-linked-post="2672500550" href="https://spectrum.ieee.org/large-language-model-performance" target="_blank">large language models</a> (LLMs).</p><p>Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application <a href="https://spectrum.ieee.org/tag/programming">programming</a> interface or command line interface. In either case, the queries are sent to a <a data-linked-post="2672354133" href="https://spectrum.ieee.org/dcflex-data-center-flexibility" target="_blank">data center</a>, where the model is hosted and run. It works well, until it doesn’t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send <a href="https://spectrum.ieee.org/tag/personal-data">personal data</a> to an anonymous entity.</p><p>Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.</p><p>However, for the average laptop that’s over a year old, the number of useful <a href="https://spectrum.ieee.org/tag/ai-models">AI models</a> you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor (<a href="https://en.wikipedia.org/wiki/Central_processing_unit" rel="noopener noreferrer" target="_blank">CPU</a>), no dedicated <a href="https://spectrum.ieee.org/tag/graphics">graphics</a> chip (<a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" rel="noopener noreferrer" target="_blank">GPU</a>) or neural-processing unit (<a href="https://en.wikipedia.org/wiki/Neural_processing_unit" rel="noopener noreferrer" target="_blank">NPU</a>), and 16 gigabytes of <a href="https://en.wikipedia.org/wiki/Random-access_memory" rel="noopener noreferrer" target="_blank">RAM</a>, leaving it underpowered for LLMs.</p><p>Even new, high-end PC <a href="https://spectrum.ieee.org/tag/laptops">laptops</a>, which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in <a href="https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html#:~:text=Note-,Models,Studio%20(%245.6k)!" rel="noopener noreferrer" target="_blank">the hundreds of gigabytes</a>. Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI <a href="https://spectrum.ieee.org/tag/data-centers">data centers</a> can handle.</p><p>The situation is even worse when other AI features aimed at making the model more capable are considered. <a href="https://huggingface.co/blog/jjokah/small-language-model" rel="noopener noreferrer" target="_blank">Small language models (SLMs)</a> that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.</p><p>That’s a problem for AI adoption.</p><p>To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.</p><h2>NPUs enter the chat</h2><p>The most obvious way to boost a PC’s AI performance is to place a powerful NPU alongside the CPU.</p><p>An NPU is a specialized chip <a href="https://penntoday.upenn.edu/what-is-an-NPU-in-computing" rel="noopener noreferrer" target="_blank">designed for the matrix multiplication calculations</a> that most AI models rely on. These matrix operations are highly parallelized, which is why <a href="https://spectrum.ieee.org/tag/gpus">GPUs</a> (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.</p><p>However, because NPUs are designed specifically to handle these matrix operations—and not other tasks, like 3D graphics—<a href="https://www.ibm.com/think/topics/npu-vs-gpu" rel="noopener noreferrer" target="_blank">they’re more power efficient than GPUs</a>. That’s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.</p><p>“With the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],” said <a href="https://www.microsoft.com/applied-sciences/people/steven-bathiche" rel="noopener noreferrer" target="_blank">Steven Bathiche</a>, technical fellow at <a href="https://spectrum.ieee.org/tag/microsoft">Microsoft</a>. “NPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPU” in <a href="https://www.qualcomm.com" rel="noopener noreferrer" target="_blank">Qualcomm’s</a> <a href="https://spectrum.ieee.org/tag/snapdragon">Snapdragon</a> X chip, which can power <a href="https://www.microsoft.com/en-us/" rel="noopener noreferrer" target="_blank">Microsoft’s</a> <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/" rel="noopener noreferrer" target="_blank">Copilot+</a> features. This includes <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c" rel="noopener noreferrer" target="_blank">Windows Recall</a>, which uses AI to create a searchable timeline of a user’s usage history by analyzing screenshots, and <a href="https://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/" rel="noopener noreferrer" target="_blank">Windows Photos’ Generative erase</a>, which can remove the background or specific objects from an image.</p><p>While <a href="https://qualcomm.com" rel="noopener noreferrer" target="_blank">Qualcomm</a> was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes <a href="https://www.amd.com/en.html" rel="noopener noreferrer" target="_blank">AMD</a> and <a href="https://www.intel.com/content/www/us/en/homepage.html" rel="noopener noreferrer" target="_blank">Intel</a>, and the competition is already pushing NPU performance upward.</p><p>In 2023, prior to Qualcomm’s Snapdragon X, <a href="https://spectrum.ieee.org/tag/amd">AMD</a> chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and <a href="https://spectrum.ieee.org/tag/intel">Intel</a> have NPUs that are competitive with Snapdragon, <a href="https://www.pcworld.com/article/2806864/intel-vs-amd-vs-qualcomm-which-copilot-pc-cpu-is-best-for-you.html" rel="noopener noreferrer" target="_blank">providing 40 to 50 TOPS</a>.</p><p><a href="https://www.lifewire.com/dell-pro-max-plus-ai-laptop-11739880" rel="noopener noreferrer" target="_blank">Dell’s upcoming Pro Max Plus AI PC</a> will up the ante with a <a href="https://spectrum.ieee.org/tag/qualcomm">Qualcomm</a> AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.</p><p>How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. It’s not possible to run these models on today’s consumer hardware, so real-world tests just can’t be done. But it stands to reason that we’re within throwing distance of those capabilities. It’s also worth noting that LLMs are not the only use case for NPUs. <a href="https://www.qualcomm.com/news/onq/2024/05/from-olympic-table-tennis-to-ai-product-manager-meet-dr-vinesh-sukumar" rel="noopener noreferrer" target="_blank">Vinesh Sukumar</a>, Qualcomm’s head of AI and <a href="https://spectrum.ieee.org/tag/machine-learning">machine learning</a> product management, says <a href="https://spectrum.ieee.org/tag/ai-image-generation">AI image generation</a> and manipulation is an example of a task that’s difficult without an NPU or high-end GPU.</p><h2>Building balanced chips for better AI</h2><p>Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet there’s more to running AI on local hardware than throwing a bigger, better NPU at the problem.</p><p><a href="https://ieeexplore.ieee.org/author/37089001134" rel="noopener noreferrer" target="_blank">Mike Clark</a>, corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC can’t put all their bets on the NPU. That’s in part because AI isn’t a replacement for, but rather an addition to, the tasks a PC is expected to handle.</p><p>“We must be good at low latency, at handling smaller data types, at branching code—traditional workloads. We can’t give that up, but we still want to be good at AI,” says Clark. He also noted that “the CPU is used to prepare data” for AI workloads, which means an inadequate CPU could become a bottleneck.</p><p>NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or <a href="https://spectrum.ieee.org/tag/nvidia">Nvidia</a> GPU with large amounts of built-in memory. The <a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/" rel="noopener noreferrer" target="_blank">Nvidia GeForce RTX 5090</a>’s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.</p><p>That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to <a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/" rel="noopener noreferrer" target="_blank">575 watts</a> on its own. Mobile versions for laptops are more miserly but still draw up to <a href="https://www.notebookcheck.net/Nvidia-GeForce-RTX-5090-Laptop-Benchmarks-and-Specs.934947.0.html" rel="noopener noreferrer" target="_blank">175 W</a>, which can quickly drain a laptop battery.</p><p><a href="https://www.linkedin.com/in/simonng39/?originalSubdomain=ca" rel="noopener noreferrer" target="_blank">Simon Ng</a>, client AI product manager at Intel, says the company is “seeing that the NPU will just do things much more efficiently at lower power.”<a href="https://www.linkedin.com/in/rakesh-s-anigundi/" rel="noopener noreferrer" target="_blank"> Rakesh Anigundi</a>, AMD’s director of product management for Ryzen AI, agrees. He adds that <a href="https://spectrum.ieee.org/tag/low-power">low-power</a> operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. “You’ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,” he says.</p><p>These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.</p><p>“We have to be very deliberate in how we design our <a href="https://spectrum.ieee.org/tag/system-on-a-chip">system-on-a-chip</a> to ensure that a larger <a href="https://spectrum.ieee.org/tag/soc">SoC</a> can perform to our requirements in a thin and light form factor,” said<a href="https://www.linkedin.com/in/mahesh-subramony-a0ba60/" rel="noopener noreferrer" target="_blank"> Mahesh Subramony</a>, senior fellow design engineer at AMD.</p><h2>When it comes to AI, memory matters</h2><p>Squeezing an NPU alongside a CPU and GPU will improve the average PC’s performance in AI tasks, but it’s not the only revolutionary change AI will force on PC architecture. There’s another that’s perhaps even more fundamental: memory.</p><p>Most modern PCs have a divided memory architecture <a href="https://www.electronicdesign.com/technologies/embedded/article/55300900/jon-peddie-research-what-came-before-pcie-the-evolution-of-pc-graphics-buses" rel="noopener noreferrer" target="_blank">rooted in decisions made over 25 years ago</a>. Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PC’s system memory and instead rely on the GPU’s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.</p><p>That’s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.</p><p>“When I have a discrete GPU, I have a separate memory subsystem hanging off it,” explained <a href="https://www.linkedin.com/in/joseph-macri-9a288a55/" rel="noopener noreferrer" target="_blank">Joe Macri, </a> vice president and chief technology officer at AMD. “When I want to share data between our [CPU] and GPU, I’ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.” Macri said this increases power draw and leads to a sluggish <a href="https://spectrum.ieee.org/tag/user-experience">user experience</a>.</p><p>The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Apple’s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.</p><p>AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, <a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html" rel="noopener noreferrer" target="_blank">Ryzen AI Max</a>, at <a href="https://spectrum.ieee.org/tag/ces">CES</a> (<a href="https://spectrum.ieee.org/topic/consumer-electronics/">Consumer Electronics</a> Show) 2025.</p><p>Ryzen AI Max places the company’s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of <a href="https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html" rel="noopener noreferrer" target="_blank">128 GB of system memory</a>, which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. “By bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,” said Subramony.</p><p>The Ryzen AI Max is already available in several laptops, including<a href="https://www.pcworld.com/article/2650073/hands-on-the-hp-zbook-ultra-g1a-smashes-the-work-laptop-paradigm.html" rel="noopener noreferrer" target="_blank"> the HP Zbook Ultra G1a</a> and the <a href="https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/" rel="noopener noreferrer" target="_blank">Asus ROG Flow Z13</a>. It also powers the<a href="https://frame.work/marketplace/desktops" rel="noopener noreferrer" target="_blank"> Framework Desktop</a> and several mini desktops from less well-known brands, such as the <a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOooME4uCrsnIh5mOf98eGHteIzsi-DAPl6E5xhNrTzG94qr3Tjf6" rel="noopener noreferrer" target="_blank">GMKtec EVO-X2 AI mini PC</a>.</p><p>Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.</p><p>Chips like these stand to drastically change PC architecture if they catch on. They’ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.</p><p>Unfortunately, they’ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. That’s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.</p><h2>Microsoft’s bullish take on AI is rewriting Windows</h2><p>MacOS is well regarded for its attractive, intuitive <a href="https://spectrum.ieee.org/tag/user-interface">user interface</a>, and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Apple’s GPUs aren’t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.</p><p><a href="https://www.linkedin.com/in/chrissiecremers/?originalSubdomain=nl" rel="noopener noreferrer" target="_blank">Chrissie Cremers</a>, cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesn’t use Mac computers for AI work. “The GPU in my Mac desktop can hardly manage [our AI workflow], and it’s not an old computer,” she said. “I’d love for them to catch up here, because they used to be the creative tool.”</p><p data-rm-resized-container="25%"> <img alt="Laptop beneath glass dome shaped like human head on striped orange and blue background." data-rm-shortcode-id="8afdc488eee550cbf8faad06be547968" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/laptop-beneath-glass-dome-shaped-like-human-head-on-striped-orange-and-blue-background.jpg?id=62173890&amp;width=980" height="1200" id="a10e5" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%201200%201200&#39;%3E%3C/svg%3E" width="1200"/> <small placeholder="Add Photo Credit..."> Dan Page</small></p><p>That leaves an opening for competitors to become the go-to choice for AI on the PC—and Microsoft knows it.</p><p>Microsoft launched <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/" target="_blank">Copilot+ PCs</a> at the company’s 2024 Build developer conference. The launch had problems, most notably the <a href="https://www.theverge.com/2024/6/13/24178144/microsoft-windows-ai-recall-feature-delay" rel="noopener noreferrer" target="_blank">botched</a> release of its key feature,<a href="https://spectrum.ieee.org/microsoft-copilot" target="_self"> Windows Recall</a>, which uses AI to help users search through anything they’ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.</p><p>At Build 2025, Microsoft also revealed <a href="https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/" rel="noopener noreferrer" target="_blank">Windows’ AI Foundry Local</a>, a “runtime stack” that includes a catalog of popular open-source <a href="https://spectrum.ieee.org/tag/large-language-models">large language models</a>. While Microsoft’s own models are available,<a href="https://azure.microsoft.com/en-us/products/ai-model-catalog#tabs-pill-bar-oc92d8_tab0" rel="noopener noreferrer" target="_blank"> the catalog includes thousands of open-source models</a> from <a href="https://spectrum.ieee.org/tag/alibaba">Alibaba</a>, DeepSeek, <a href="https://spectrum.ieee.org/tag/meta">Meta</a>, Mistral AI, Nvidia, <a href="https://spectrum.ieee.org/tag/openai">OpenAI</a>, Stability AI, xAI, and more.</p><p>Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.</p><p>AI <a href="https://spectrum.ieee.org/tag/foundry">Foundry</a> also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.</p><p>“[AI Foundry] is about being smart. It’s about using all the <a href="https://spectrum.ieee.org/tag/processors">processors</a> at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. There’s a lot of opportunity and runway to improve,” said Bathiche.</p><h3>Toward <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" rel="noopener noreferrer" target="_blank">AGI</a> on PCs</h3><p>The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry that’s likely to wipe away the last vestiges of the PC architectures designed in the ’80s, ’90s, and early 2000s.</p><p>The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.</p><p>It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of silicon—even in high-end laptops and desktops. AMD’s Subramony said the goal is to have users “carrying a mini workstation in your hand, whether it’s for AI workloads, or for high compute. You won’t have to go to the cloud.”</p><p>A change that massive won’t happen overnight. Still, it’s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcomm’s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for <a href="https://spectrum.ieee.org/tag/agi">AGI</a>.</p><p>“I want a complete <a href="https://spectrum.ieee.org/tag/artificial-general-intelligence">artificial general intelligence</a> running on Qualcomm devices,” he said. “That’s what we’re trying to push for.” <span></span></p><p><em>This article appears in the December 2025 print issue.</em></p></div></div></div>
  </body>
</html>
