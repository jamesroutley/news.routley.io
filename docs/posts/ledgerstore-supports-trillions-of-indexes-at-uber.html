<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.uber.com/blog/how-ledgerstore-supports-trillions-of-indexes/?uclick_id=67d612cb-12f8-470a-98fd-e9f7144dfafb">Original</a>
    <h1>LedgerStore Supports Trillions of Indexes at Uber</h1>
    
    <div id="readability-page-1" class="page"><div data-baseweb="block">


<p>Uber connects the physical and digital worlds to help make movement happen at the tap of a button. Billions of trips, deliveries, and tens of billions of financial transactions across earners, spenders, and merchants are made at Uber every quarter. LedgerStore is an immutable storage solution at Uber that provides verifiable data completeness and correctness guarantees to ensure data integrity for these transactions.</p>


<hr aria-hidden="true" role="separator"/>





<p>Various types of indexes need to be supported on ledgers. Let us explore them along with corresponding use cases.</p>


<p><h2 id="h-strongly-consistent-indexes">Strongly consistent indexes</h2></p>


<p>One of the use cases is handling the credit card authorization flow when a rider/eater uses Uber. At the beginning of an Uber trip, a credit card hold is placed on the rider/eater’s credit card. This hold should either be converted to a charge or voided, depending on whether the trip was taken or canceled, as shown below. </p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084314,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;5afd321b-f572-41f9-9bce-2f8686c3fb89&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1539,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png 1539w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png 1536w" sizes="(max-width: 1539px) 100vw, 1539px" loading="lazy" width="1539" height="929"/><figcaption>Figure 1: Uber Trip credit-card payment flow supported by strongly consistent indexes.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p>If the index serving the hold is not strongly consistent<em>, </em>it could take a while for the hold to be visible upon reading. A consequence of this is that a duplicate charge could be made on the user’s credit card while the original hold remains on the credit card.  </p>


<p>Now, let’s dive into how we build strongly consistent indexes that ensure that once a record write is performed, any subsequent reads are guaranteed to see the indexes corresponding to that record.</p>


<p><h3 id="h-write-path">Write Path</h3></p>


<p>To build strongly consistent indexes, we use a 2-phase commit to ensure that the index is always strongly consistent with the record, as shown below. </p>


<p>The insert operation begins with an index intent write before the record write. These intents are committed after the record write operation if the record write succeeded and this is done asynchronously to avoid affecting end-user insert latency. If the index intent write succeeds, but the record write fails, the index intent will need to be rolled back, else it leads to an accumulation of unused intents, and that is handled during the read time, as we will see next. </p>


<p>It is important to note that if the index intent write fails, the whole insert operation fails since we cannot guarantee the consistency of the index with the record. Hence, strongly consistent indexes need to be considered only when the use case strongly demands it. </p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084320,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;5d84f5bd-1292-4611-b530-cc72fa296bbf&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1260,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png 1260w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png 768w" sizes="(max-width: 1260px) 100vw, 1260px" loading="lazy" width="1260" height="640"/><figcaption>Figure 2: Two-phase commit write flow of strongly consistent indexes.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h3 id="h-read-path">Read Path</h3></p>


<p>There are two cases where an index can be in the intent state after the insert: </p>


<div><ol><li>The index intent commit operation failed in the write path OR </li>


<li>If record write fails</li>
</ol></div>


<p>Such intents are handled on the read path by either committing or deleting them. When a read happens on these indexes, if the index is in an intent state, the corresponding record is read. If the record is present, the index is committed, else rolled back. These operations happen asynchronously so as not to affect the end user read latency. In general, only a very small percentage of indexes end up in the intent state.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084321,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;f84c40c5-f788-46b0-bb38-cbb4b888d291&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1180,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png 1180w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png 768w" sizes="(max-width: 1180px) 100vw, 1180px" loading="lazy" width="1180" height="912"/><figcaption>Figure 3: Read flows of strongly consistent indexes.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h2 id="h-eventually-consistent-indexes">Eventually consistent indexes</h2></p>


<p>Not all indexes require strong read-your-write guarantees. An example of such an index is the payment history page, wherein, a lag of a few seconds is acceptable as long as the payment appears on the page.</p>


<p>While strongly consistent indexes provide <em>read-your-write</em> guarantees, they are not suitable in certain circumstances since they trade off the following properties to achieve this guarantee:</p>


<div><ul><li><strong>Higher Write Latency</strong></li>


<li><strong>Lower Availability</strong></li>
</ul></div>


<p>Eventually consistent indexes are the opposite in this aspect when compared to strongly consistent indexes, as they are built in the background by a separate process that is completely isolated from the online write path. Hence, they do not suffer from <em>higher write latency</em> or cause potential <em>lower availability</em> of LedgerStore service. We leverage a feature called Materialized Views from our home-grown <span><a href="https://www.uber.com/blog/schemaless-sql-database/" target="_blank" rel="noreferrer noopener">Docstore</a></span> database to generate these indexes.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084322,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;ac1b498f-998b-4ed9-b5c9-bff580c0bc3b&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1820,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png 1820w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png 1536w" sizes="(max-width: 1820px) 100vw, 1820px" loading="lazy" width="1820" height="1760"/><figcaption>Figure 4: Payment history served by eventually consistent indexes.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h2 id="h-time-range-indexes">Time-range indexes</h2></p>


<p>Ledgers, due to their immutable nature, keep growing in size over time, thereby increasing their cost of storage. So, at Uber, we offload older ledgers in time-range batches to cheaper cold storage. </p>


<p>Every ledger is associated with a timestamp called a business or event timestamp. To offload ledgers to cold storage (and also for sealing the data), we need a class of indexes to query data in event time-range batches. What differentiates this index is that the data is read in deterministic time-range batches, in orders of magnitude higher than the above two index types.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084325,&#34;width&#34;:&#34;701px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;aspectRatio&#34;:&#34;1.7258883248730965&#34;,&#34;sizeSlug&#34;:&#34;full&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;60fc196f-7a96-4e90-a440-b83c89ba95df&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig5-e1711743069138.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1020,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig5-e1711743069138.png 1020w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig5-e1711743069138.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig5-e1711743069138.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" loading="lazy" width="1020" height="591"/><figcaption>Figure 5: time-range indexes used in data-tiering.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p>Following is an example of how time-range queries are done on ledgers:</p>


<div><figure data-wp-block-name="core/table"><p><strong>SELECT * FROM</strong> LEDGER_TABLE <strong>WHERE</strong> LedgerTime <strong>BETWEEN</strong> 1701252000000 <strong>AND</strong> 1701253800000</p></figure></div>


<div><figure data-wp-block-name="core/table"><table><tbody><tr><td><strong>Ledger</strong></td><td><strong>LedgerTime</strong></td></tr><tr><td>{trip started}</td><td>10:01 am</td></tr><tr><td>{trip completed and fare adjusted}</td><td>10:15 am</td></tr><tr><td>{post trip corrections}</td><td>12:01 pm</td></tr></tbody></table></figure></div>


<hr aria-hidden="true" role="separator"/>


<p>There are a few ways to model this in a distributed database. We will dive into the key differences between developing the time-range index on top of Amazon DynamoDB vs. Docstore database. Both DynamoDB and Docstore, being distributed databases, provide data modeling constructs as Partition and Sort keys. The former is meant for distributing data across partitions evenly based on its value and the latter to control the sort order of the data.  </p>


<p><h3 id="h-design-with-dynamodb">Design with DynamoDB</h3></p>


<p>Dynamodb provides two ways of managing table read/write <span><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html" target="_blank" rel="noreferrer noopener">capacity</a></span>. We used the provisioned mode since the traffic is not too <span><a href="https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html" target="_blank" rel="noreferrer noopener">bursty</a></span> to require on-demand mode. The provisioned mode was configured with <span><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html" target="_blank" rel="noreferrer noopener">auto scaling</a></span> to adjust capacity based on the traffic pattern. </p>


<p>As we notice from the write pattern above, the ledger times are generally correlated to the current wall clock time. Hence these values tend to be clustered around the current time. If we were to partition the data based on say <strong>G</strong> time-units granularity, all the writes in the <strong>G</strong> time-units would go to the same physical partition causing hot partitions. DynamoDB has restrictions on throughput in case of <span><a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/" target="_blank" rel="noreferrer noopener">hot partitions</a></span>, leading to throttling of write requests, which is not acceptable in the online write path. Assuming 1K peak Uber trips/s, even G=1 second is not a good value, since it corresponds to 1K WCU (Write Capacity Unit), which is the peak allowed QPS before <span><a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/" target="_blank" rel="noreferrer noopener">throttling</a></span> happens. </p>


<p>While it might seem like we could just make the partitioning more fine-grained, it is still not foolproof, since an increase in the traffic over time can lead to instability. Another side effect of this is the increase in cumulative reads to be performed via a scatter-gather. So, what we did in the case of DynamoDB was below:</p>


<p><h4 id="h-write-optimized-temporary-index-table-called-buffer-index">Write-optimized temporary index table (called buffer index)</h4></p>


<p>All online time-index writes go to the buffer index table. Inserted index items are partitioned into <strong><em>M</em></strong> unique buckets based on a hash modulo of the corresponding record to <span><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html" target="_blank" rel="noreferrer noopener">uniformly distribute</a></span> load across partitions in the buffer index table, making it <em>write-efficient</em>. The value of <strong><em>M</em></strong> is chosen such that it is high enough that the amount of load per partition avoids excessive splitting. It is also chosen low enough, to limit the amount of scatter-gather<em> </em>to perform during reads. </p>


<p><h4 id="h-read-optimized-permanent-index-table">Read-optimized permanent index table</h4></p>


<p>The need for scatter-gathe<em>r </em>read of the buffer tables makes them not efficient for reads and since reads can happen throughout the lifecycle of a table, we would need to optimize it. This brings the need for a read-efficient permanent index table. </p>


<p>A permanent time-range index table is partitioned on the timestamp aligned to a certain time duration <strong><em>N</em></strong> (say 10 minutes). Indexes from the buffer tables are periodically written in batches to the permanent index table. Since the write is done in batches and in the background, any write throttling here does not affect the online traffic. Another advantage of batching is that the write traffic can be distributed across partitions, reducing the hot partitioning. The buffer index tables are deleted after offloading their indexes to the permanent index table since they are no longer needed. Reads on the permanent index tables are done in intervals of <strong><em>N</em></strong> minutes without any scatter-gather, making this table <em>read-efficient</em>.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084328,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;233c0796-38ad-45f8-89f6-314362901566&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1499,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png 1499w, https://blog.uber-cdn.com/cdn-cgi/image/width=269,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png 269w, https://blog.uber-cdn.com/cdn-cgi/image/width=920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png 920w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1380,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png 1380w" sizes="(max-width: 1499px) 100vw, 1499px" loading="lazy" width="1499" height="1669"/><figcaption>Figure 6: Time-range index design on Dynamodb.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h3 id="h-design-with-docstore">Design with Docstore</h3></p>


<p>The two-table design in the case of DynamoDB functions well and can handle high throughput, but introduces challenges in operations. If the temporary buffer tables are not created in time, it can lead to write failure since writes cannot be accepted, and this has caused availability issues in the past. We re-architected our index storage backend from DynamoDB to Uber’s <span><a href="https://www.uber.com/blog/schemaless-sql-database/" target="_blank" rel="noreferrer noopener">Docstore</a></span> database as part of cost efficiency. As part of this re-architecture, we also improved the time-range index design to overcome the downside of maintaining two tables, by leveraging two Docstore properties:</p>


<div><ol><li><span><a href="https://www.uber.com/blog/schemaless-sql-database/" target="_blank" rel="noreferrer noopener">Docstore</a></span> is a distributed database built on top of MySQL, with a fixed number of shards assigned to a variable number of physical partitions. As the data size grows, the number of physical partitions increases and some of the existing shards are re-assigned to the new partitions, leading to a max upper<strong> limit</strong> to the number of physical partitions. </li>


<li>Data in Docstore is stored in a <strong>sorted</strong> fashion of the primary key (partition + sort keys).</li>
</ol></div>


<p>We maintain just one table for the time-range index, wherein the index entries are partitioned on the full timestamp value. Since the timestamp is extremely granular, there is no hot partitioning (and hence no write throttling) since most of the writes are uniformly distributed across partitions.</p>


<p>Reads involve a prefix scanning of each of the shards of the table up to a certain time granularity. Prefix scanning is very similar to a regular scan of the table, except the boundaries of each scan request are controlled by the application. So, in the example below, to read 30 minutes worth of data, reads could be done on a 10-minute interval starting from 2023–02-03 01:00:00 to 2023–02-03 01:10:00 and similarly repeated for the next two sub-windows. Since data is sorted on the primary key, this prefix scan with given boundaries ensures only data lying within these timestamps is read. </p>


<p>A scatter-gather, followed by sort merging across shards is then performed to obtain all time-range index entries in the given window, in a sorted fashion. Since the number of shards is fixed in Docstore, we can precisely determine (and bound) the number of read requests that need to be performed. The same technique is not applicable in the case of DynamoDB since the number of partitions keeps increasing over time, as the table size increases. This has significantly simplified the design and reduced the operational maintenance cost of our time-indexes. </p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084329,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;8b1ef946-ea3c-48dd-ba2e-1a8fc2fdc6aa&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1080,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png 1080w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png 768w" sizes="(max-width: 1080px) 100vw, 1080px" loading="lazy" width="1080" height="964"/><figcaption>Figure 7: Time-range index design on Docstore.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h2 id="h-index-lifecycle-management">Index lifecycle management</h2></p>


<p>New indexes are defined regularly and some of the indexes could be modified as well to evolve use cases. To support that with minimal effort and also not cause any regressions, we need a mechanism to manage the index lifecycle. The following are the components of the same:</p>


<p><h3 id="h-index-lifecycle-state-machine">Index lifecycle state machine</h3></p>


<p>This component orchestrates the life-cycle of the index, involving creating the index table, backfilling it with historical index entries, validating them for completeness, swapping the old index with the new one for read/writes, and decommissioning the old index.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084330,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;cf59cd17-80ab-4c9e-b88f-c1c3aead7512&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1800,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png 1800w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png 1536w" sizes="(max-width: 1800px) 100vw, 1800px" loading="lazy" width="1800" height="660"/><figcaption>Figure 8: Index lifecycle state machine.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h3 id="h-historical-index-data-backfill">Historical Index data backfill</h3></p>


<p>Depending on the business use cases, new indexes need to be defined, and it is essential to backfill historical index entries so that they are complete. This component builds indexes from the historical data offloaded to the cold data storage and backfills them to the storage layer in a scalable fashion. Considering that the data download speed is higher than the data processing speed, this component is built with configurable rate-limiting and batching in a reusable way, since we can plug in the actual processing logic as a batch processor plugin.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084331,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;eb0d0db3-1c26-4c44-8026-8e7f2ce2921b&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1420,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png 1420w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png 768w" sizes="(max-width: 1420px) 100vw, 1420px" loading="lazy" width="1420" height="821"/><figcaption>Figure 9: Historical data processing module customized to backfill indexes.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h3 id="h-index-validation">Index validation</h3></p>


<p>After indexes are backfilled, they need to be verified for completeness. This is done by an offline job that computes order independent checksums at a certain time-window granularity and compares them across the source of truth data and the index table. This step identifies any bugs in the index backfill process since even if one entry is missed, the aggregate checksum for that time window will lead to a mismatch.</p>


<hr aria-hidden="true" role="separator"/>


<div><figure data-wp-block="{&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:1084332,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;hash&#34;:&#34;3b4c1b4e-bf2a-430e-b80f-8c8fc75357a2&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-block-name="core/image"><img alt="Image" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png" srcset="https://blog.uber-cdn.com/cdn-cgi/image/width=1640,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png 1640w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png 1536w" sizes="(max-width: 1640px) 100vw, 1640px" loading="lazy" width="1640" height="560"/><figcaption>Figure 10: Index completeness validation.</figcaption></figure></div>

<hr aria-hidden="true" role="separator"/>


<p><h2 id="h-highlights">Highlights</h2></p>


<p>This is how we measured the success of this critical project:</p>


<div><ul><li>We built over 2 trillion unique indexes, and not a single data inconsistency has been detected so far, with the new architecture in production for over 6 months.</li>


<li>Not a single production incident was noticed during the backfill, given how critical money movement is for Uber.</li>


<li>We also moved all these indexes from DynamoDB to Docstore. So the project also resulted in technology consolidation, reducing external dependencies.</li>
</ul></div>


<p>From a business impact perspective, operating LedgerStore is now very cost-effective due to reduced spend on DynamoDB. The estimated yearly savings are over $6 million per year.</p>


<hr aria-hidden="true" role="separator"/>





<p>Ledgers are the source of truth for money movement events at Uber. The robust indexing platform we have built supports accessing these sources of truth ledgers for various business use cases, and we look forward to supporting many more indexes on this platform in the future. </p>


<p>We would like to conclude with some key takeaways: Maintaining a petabyte-scale of indexes in an OLTP system brings in certain challenges, such as imbalanced partitioning, high read/write amplification, noisy neighbor problems, etc. So data modeling and isolation are important aspects to consider while designing these systems. Further, depending on the actual database used underneath for storage, the design methodology can be significantly different, as we see from the design contrast of time-range indexes on two different distributed databases.</p>


<p>Join us next week to see part two of the LedgerStore series where we chronicle a migration from DynamoDB to LedgerStore.</p>


<p><h2 id="h-acknowledgments">Acknowledgments</h2></p>


<p>This project would not have been possible without collaboration from the following teams, embodying several <span><a href="https://www.uber.com/in/en/careers/values/" target="_blank" rel="noreferrer noopener">Uber values</a></span>:</p>


<div><ul><li>The <span><a href="https://www.uber.com/blog/payments-platform/" target="_blank" rel="noreferrer noopener">Gulfstream</a></span> team, who closely worked with the LedgerStore team in aligning on the common goals and migrating on the LedgerStore platform, a multi-year project.</li>


<li>The Docstore team, for evolving <span><a href="https://www.uber.com/en-IN/blog/schemaless-sql-database/" target="_blank" rel="noreferrer noopener">Docstore</a></span> to meet the massive scale requirements of LedgerStore’s indexes.</li>


<li>The LedgerStore team for leading, building, and driving the adoption of ledger indexes at large scale.</li>
</ul></div>


<p><em>Amazon Web Services, AWS, the Powered by AWS logo, and Amazon DynamoDB are trademarks of Amazon.com, Inc. or its affiliates.</em></p>
</div></div>
  </body>
</html>
