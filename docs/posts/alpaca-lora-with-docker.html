<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/chris-alexiuk/alpaca-lora">Original</a>
    <h1>Alpaca-LoRA with Docker</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto"><a id="user-content--alpaca-lora-low-rank-llama-instruct-tuning" aria-hidden="true" href="#-alpaca-lora-low-rank-llama-instruct-tuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="llama" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f999.png">ü¶ô</g-emoji><g-emoji alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png">üå≤</g-emoji><g-emoji alias="pinching_hand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f90f.png">ü§è</g-emoji> Alpaca-LoRA: Low-Rank LLaMA Instruct-Tuning</h2>
<ul dir="auto">
<li><g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ü§ó</g-emoji> <strong>Try the pretrained model out <a href="https://huggingface.co/spaces/tloen/alpaca-lora" rel="nofollow">here</a>, courtesy of a GPU grant from Huggingface!</strong></li>
<li>Users have created a Discord server for discussion and support <a href="https://discord.gg/prbq284xX5" rel="nofollow">here</a></li>
<li><strong>This repository does not contain code for hosting and/or facilitating the downloading and/or streaming of the LLaMA weights. You will have to specify your own HuggingFace Hub base model to run the code, such as <code>decapoda-research/llama-7b-hf</code>.</strong></li>
</ul>
<p dir="auto">This repository contains code for reproducing the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a> results using <a href="https://arxiv.org/pdf/2106.09685.pdf" rel="nofollow">low-rank adaptation (LoRA)</a>.
We provide an Instruct model of similar quality to <code>text-davinci-003</code> that can run <a href="https://twitter.com/miolini/status/1634982361757790209" rel="nofollow">on a Raspberry Pi</a> (for research),
and the code is easily extended to the <code>13b</code>, <code>30b</code>, and <code>65b</code> models.</p>
<p dir="auto">In addition to the training code, which runs within five hours on a single RTX 4090,
we publish a script for downloading and inference on the foundation model and LoRA,
as well as the resulting <a href="https://huggingface.co/tloen/alpaca-lora-7b/tree/main" rel="nofollow">LoRA weights themselves</a>.
To fine-tune cheaply and efficiently, we use Hugging Face&#39;s <a href="https://github.com/huggingface/peft">PEFT</a>
as well as Tim Dettmers&#39; <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a>.</p>
<p dir="auto">Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-docker-commands-for-easy-local-inference" aria-hidden="true" href="#docker-commands-for-easy-local-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Docker commands for easy local inference</h3>
<ol dir="auto">
<li>
<p dir="auto">Add reference to your desired model weights to <code>ENV BASE_MODEL=&#34;None&#34;</code> in the <code>Dockerfile</code>.</p>
</li>
<li>
<p dir="auto">Build the container image</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="docker build -t alpaca-lora-demo ."><pre><code>docker build -t alpaca-lora-demo .
</code></pre></div>
<ol start="3" dir="auto">
<li>Run the container image</li>
</ol>
<div data-snippet-clipboard-copy-content="docker run --gpus=all --shm-size 64g -p 7860:7860 -v ${HOME}/.cache:/root/.cache --rm alpaca-lora-demo generate.py"><pre><code>docker run --gpus=all --shm-size 64g -p 7860:7860 -v ${HOME}/.cache:/root/.cache --rm alpaca-lora-demo generate.py
</code></pre></div>
<ol start="4" dir="auto">
<li>Head on down to <code>localhost:7860</code> and enjoy!</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h3>
<ol dir="auto">
<li>Install dependencies</li>
</ol>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
<ol start="2" dir="auto">
<li>If bitsandbytes doesn&#39;t work, <a href="https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md">install it from source.</a> Windows users can follow <a href="https://github.com/tloen/alpaca-lora/issues/17" data-hovercard-type="issue" data-hovercard-url="/tloen/alpaca-lora/issues/17/hovercard">these instructions</a>.</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-inference-generatepy" aria-hidden="true" href="#inference-generatepy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference (<code>generate.py</code>)</h3>
<p dir="auto">This file reads the foundation model from the Hugging Face model hub and the LoRA weights from <code>tloen/alpaca-lora-7b</code>, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-training-finetunepy" aria-hidden="true" href="#training-finetunepy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training (<code>finetune.py</code>)</h3>
<p dir="auto">This file contains a straightforward application of PEFT to the LLaMA model,
as well as some code related to prompt construction and tokenization.
Near the top of this file is a set of hardcoded hyperparameters that you should feel free to modify.
PRs adapting this code to support larger models are always welcome.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-checkpoint-export-export__checkpointpy" aria-hidden="true" href="#checkpoint-export-export__checkpointpy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Checkpoint export (<code>export_*_checkpoint.py</code>)</h3>
<p dir="auto">These files contain scripts that merge the LoRA weights back into the base model
for export to Hugging Face format and to PyTorch <code>state_dicts</code>.
They should help users
who want to run inference in projects like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>
or <a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-dataset" aria-hidden="true" href="#dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dataset</h3>
<p dir="auto">In addition to <code>alpaca_data.json</code>, which contains the original Stanford Alpaca dataset,
we also include <code>alpaca_data_cleaned.json</code>, which has been <a href="https://github.com/tloen/alpaca-lora/pull/32" data-hovercard-type="pull_request" data-hovercard-url="/tloen/alpaca-lora/pull/32/hovercard">stripped of various tokenization artifacts</a>
with the help of @gururise.
This file is now used by default in the training script.</p>
<p dir="auto">@AndriyMulyar has also provided interactive, embedding-based visualizations of the original dataset&#39;s <a href="https://atlas.nomic.ai/map/alpaca_instructions" rel="nofollow">instructions</a>
and <a href="https://atlas.nomic.ai/map/alpaca_outputs" rel="nofollow">outputs</a>,
as well as <a href="https://atlas.nomic.ai/map/d2139cc3-bc1c-441c-8d6f-3e6ffbbc2eda/838019ff-8fe2-42ba-809a-d86d2b98cd50/-18.11668742841587/-11.348087116836096/-20.88850316347706/-17.680468640801223/774455612" rel="nofollow">clusters of bad examples</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-notes" aria-hidden="true" href="#notes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes</h3>
<ul dir="auto">
<li>We can likely improve our model performance significantly if we had a better dataset. Consider supporting the <a href="https://open-assistant.io/" rel="nofollow">LAION Open Assistant</a> effort to produce a high-quality dataset for supervised fine-tuning (or bugging them to release their data).</li>
<li>We&#39;re continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.</li>
<li>Users with multiple GPUs should take a look <a href="https://github.com/tloen/alpaca-lora/issues/8#issuecomment-1477490259" data-hovercard-type="issue" data-hovercard-url="/tloen/alpaca-lora/issues/8/hovercard">here</a>.</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-resources" aria-hidden="true" href="#resources"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Resources</h3>
<ul dir="auto">
<li><a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>, a native client for running Alpaca models on the CPU</li>
<li><a href="https://github.com/deep-diver/Alpaca-LoRA-Serve">Alpaca-LoRA-Serve</a>, a ChatGPT-style interface for Alpaca models</li>
<li><a href="https://github.com/gururise/AlpacaDataCleaned">AlpacaDataCleaned</a>, a project to improve the quality of the Alpaca dataset</li>
<li>Various adapter weights (download at own risk):
<ul dir="auto">
<li>7B:
<ul dir="auto">
<li><a href="https://huggingface.co/tloen/alpaca-lora-7b" rel="nofollow">https://huggingface.co/tloen/alpaca-lora-7b</a></li>
<li><a href="https://huggingface.co/samwit/alpaca7B-lora" rel="nofollow">https://huggingface.co/samwit/alpaca7B-lora</a></li>
<li><g-emoji alias="brazil" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e7-1f1f7.png">üáßüá∑</g-emoji> <a href="https://huggingface.co/22h/cabrita-lora-v0-1" rel="nofollow">https://huggingface.co/22h/cabrita-lora-v0-1</a></li>
<li><g-emoji alias="cn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e8-1f1f3.png">üá®üá≥</g-emoji> <a href="https://huggingface.co/qychen/luotuo-lora-7b-0.1" rel="nofollow">https://huggingface.co/qychen/luotuo-lora-7b-0.1</a></li>
<li><g-emoji alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png">üáØüáµ</g-emoji> <a href="https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0" rel="nofollow">https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0</a></li>
<li><g-emoji alias="fr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1eb-1f1f7.png">üá´üá∑</g-emoji> <a href="https://huggingface.co/bofenghuang/vigogne-lora-7b" rel="nofollow">https://huggingface.co/bofenghuang/vigogne-lora-7b</a></li>
<li><g-emoji alias="thailand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f9-1f1ed.png">üáπüá≠</g-emoji> <a href="https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1" rel="nofollow">https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1</a></li>
<li><g-emoji alias="de" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e9-1f1ea.png">üá©üá™</g-emoji> <a href="https://huggingface.co/thisserand/alpaca_lora_german" rel="nofollow">https://huggingface.co/thisserand/alpaca_lora_german</a></li>
</ul>
</li>
<li>13B:
<ul dir="auto">
<li><a href="https://huggingface.co/chansung/alpaca-lora-13b" rel="nofollow">https://huggingface.co/chansung/alpaca-lora-13b</a></li>
<li><a href="https://huggingface.co/mattreid/alpaca-lora-13b" rel="nofollow">https://huggingface.co/mattreid/alpaca-lora-13b</a></li>
<li><a href="https://huggingface.co/samwit/alpaca13B-lora" rel="nofollow">https://huggingface.co/samwit/alpaca13B-lora</a></li>
<li><g-emoji alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png">üáØüáµ</g-emoji> <a href="https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0" rel="nofollow">https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0</a></li>
<li><g-emoji alias="kr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f0-1f1f7.png">üá∞üá∑</g-emoji> <a href="https://huggingface.co/chansung/koalpaca-lora-13b" rel="nofollow">https://huggingface.co/chansung/koalpaca-lora-13b</a></li>
</ul>
</li>
<li>30B:
<ul dir="auto">
<li><a href="https://huggingface.co/baseten/alpaca-30b" rel="nofollow">https://huggingface.co/baseten/alpaca-30b</a></li>
<li><a href="https://huggingface.co/chansung/alpaca-lora-30b" rel="nofollow">https://huggingface.co/chansung/alpaca-lora-30b</a></li>
<li><g-emoji alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png">üáØüáµ</g-emoji> <a href="https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0" rel="nofollow">https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://huggingface.co/chavinlo/alpaca-native" rel="nofollow">alpaca-native</a>, a replication using the original Alpaca code</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-example-outputs" aria-hidden="true" href="#example-outputs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example outputs</h3>
<p dir="auto"><strong>Instruction</strong>: Tell me about alpacas.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.</p>
<p dir="auto"><strong>text-davinci-003</strong>: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.</p>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Tell me about the president of Mexico in 2019.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: The president of Mexico in 2019 was Andr√©s Manuel L√≥pez Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country‚Äôs 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country‚Äôs relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.</p>
<p dir="auto"><strong>text-davinci-003</strong>: The president of Mexico in 2019 is Andr√©s Manuel L√≥pez Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.</p>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Tell me about the king of France in 2019.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: The King of France in 2019 is Emmanuel Macron, who was elected in May 2017. He is the youngest president in the history of the Fifth Republic and the first president to be born after World War II. He is also the first president to have never held elected office before.</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: In 2019, the King of France was Emmanuel Macron. He was sworn in as President of France on May 14, 2017, and has been leading the country ever since. He is a member of the centrist La R√©publique En Marche! Party. He is known for his reformist agenda and has faced a number of protests from both the right and left of the political spectrum. He is the youngest French president in history.</p>
<p dir="auto"><strong>text-davinci-003</strong>: The King of France is a largely symbolic role, as France is a republic. The current President of France is Emmanuel Macron, who was elected in 2017.</p>
<hr/>
<p dir="auto"><strong>Instruction</strong>: List all Canadian provinces in alphabetical order.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Northwest Territories, Nunavut, Yukon.</p>
<p dir="auto"><strong>text-davinci-003</strong>:</p>
<ol dir="auto">
<li>Alberta</li>
<li>British Columbia</li>
<li>Manitoba</li>
<li>New Brunswick</li>
<li>Newfoundland and Labrador</li>
<li>Northwest Territories</li>
<li>Nova Scotia</li>
<li>Nunavut</li>
<li>Ontario</li>
<li>Prince Edward Island</li>
<li>Quebec</li>
<li>Saskatchewan</li>
<li>Yukon</li>
</ol>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Write a Python program that prints the first 10 Fibonacci numbers.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="def fibonacci(n):
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

print(fibonacci(10))"><pre><span>def</span> <span>fibonacci</span>(<span>n</span>):
    <span>if</span> <span>n</span> <span>==</span> <span>0</span>:
        <span>return</span> <span>0</span>
    <span>elif</span> <span>n</span> <span>==</span> <span>1</span>:
        <span>return</span> <span>1</span>
    <span>else</span>:
        <span>return</span> <span>fibonacci</span>(<span>n</span><span>-</span><span>1</span>) <span>+</span> <span>fibonacci</span>(<span>n</span><span>-</span><span>2</span>)

<span>print</span>(<span>fibonacci</span>(<span>10</span>))</pre></div>
<p dir="auto"><strong>Stanford Alpaca</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="numbers = [0, 1]

for i in range(2, 11):
    numbers.append(numbers[i-2] + numbers[i-1])

print(numbers)
# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"><pre><span>numbers</span> <span>=</span> [<span>0</span>, <span>1</span>]

<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>2</span>, <span>11</span>):
    <span>numbers</span>.<span>append</span>(<span>numbers</span>[<span>i</span><span>-</span><span>2</span>] <span>+</span> <span>numbers</span>[<span>i</span><span>-</span><span>1</span>])

<span>print</span>(<span>numbers</span>)
<span># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span></pre></div>
<p dir="auto"><strong>text-davinci-003</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.

# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...

# Program:

# Initialize the first two numbers of the Fibonacci sequence
a = 0
b = 1

# Print the first 10 numbers of the Fibonacci sequence
print(a)
print(b)

for i in range(2,10):
    c = a + b
    a = b
    b = c
    print(c)"><pre><span># Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.</span>

<span># Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...</span>

<span># Program:</span>

<span># Initialize the first two numbers of the Fibonacci sequence</span>
<span>a</span> <span>=</span> <span>0</span>
<span>b</span> <span>=</span> <span>1</span>

<span># Print the first 10 numbers of the Fibonacci sequence</span>
<span>print</span>(<span>a</span>)
<span>print</span>(<span>b</span>)

<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>2</span>,<span>10</span>):
    <span>c</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span>
    <span>a</span> <span>=</span> <span>b</span>
    <span>b</span> <span>=</span> <span>c</span>
    <span>print</span>(<span>c</span>)</pre></div>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for i in range(1, 101):
    if i % 3 == 0 and i % 5 == 0:
        print(&#34;FizzBuzz&#34;)
    elif i % 3 == 0:
        print(&#34;Fizz&#34;)
    elif i % 5 == 0:
        print(&#34;Buzz&#34;)
    else:
        print(i)"><pre><span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1</span>, <span>101</span>):
    <span>if</span> <span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span> <span>and</span> <span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#34;FizzBuzz&#34;</span>)
    <span>elif</span> <span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#34;Fizz&#34;</span>)
    <span>elif</span> <span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#34;Buzz&#34;</span>)
    <span>else</span>:
        <span>print</span>(<span>i</span>)</pre></div>
<p dir="auto"><strong>Stanford Alpaca</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for i in range(1, 101):
    if i % 3 == 0 and i % 5 == 0:
        print(&#39;FizzBuzz&#39;)
    elif i % 3 == 0:
        print(&#39;Fizz&#39;)
    elif i % 5 == 0:
        print(&#39;Buzz&#39;)
    else:
        print(i)"><pre><span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1</span>, <span>101</span>):
    <span>if</span> <span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span> <span>and</span> <span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#39;FizzBuzz&#39;</span>)
    <span>elif</span> <span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#39;Fizz&#39;</span>)
    <span>elif</span> <span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>:
        <span>print</span>(<span>&#39;Buzz&#39;</span>)
    <span>else</span>:
        <span>print</span>(<span>i</span>)</pre></div>
<p dir="auto"><strong>text-davinci-003</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (let i = 1; i &lt;= 100; i++) {
  if (i % 3 === 0 &amp;&amp; i % 5 === 0) {
    console.log(&#34;FizzBuzz&#34;);
  } else if (i % 3 === 0) {
    console.log(&#34;Fizz&#34;);
  } else if (i % 5 === 0) {
    console.log(&#34;Buzz&#34;);
  } else {
    console.log(i);
  }
}"><pre><span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>1</span><span>;</span> <span>i</span> <span>&lt;=</span> <span>100</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>i</span> <span>%</span> <span>3</span> <span>===</span> <span>0</span> <span>&amp;&amp;</span> <span>i</span> <span>%</span> <span>5</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>&#34;FizzBuzz&#34;</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>i</span> <span>%</span> <span>3</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>&#34;Fizz&#34;</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>i</span> <span>%</span> <span>5</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>&#34;Buzz&#34;</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>i</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span></pre></div>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Tell me five words that rhyme with &#39;shock&#39;.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: Flock, lock, rock, stock, and sock.</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: Five words that rhyme with shock are: rock, pop, shock, cook, and snock.</p>
<p dir="auto"><strong>text-davinci-003</strong>:</p>
<ol dir="auto">
<li>Block</li>
<li>Lock</li>
<li>Dock</li>
<li>Clock</li>
<li>Rock</li>
</ol>
<hr/>
<p dir="auto"><strong>Instruction</strong>: Translate the sentence &#39;I have no mouth but I must scream&#39; into Spanish.</p>
<p dir="auto"><strong>Alpaca-LoRA</strong>: No tengo boca pero tengo que gritar.</p>
<p dir="auto"><strong>Stanford Alpaca</strong>: No tengo boca, pero debo gritar.</p>
<p dir="auto"><strong>text-davinci-003</strong>: No tengo boca pero debo gritar.</p>
</article>
          </div></div>
  </body>
</html>
