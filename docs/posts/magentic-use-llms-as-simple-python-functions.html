<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/jackmpcollins/magentic">Original</a>
    <h1>Show HN: Magentic – Use LLMs as simple Python functions</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Easily integrate Large Language Models into your Python code. Simply use the <code>@prompt</code> decorator to create functions that return structured output from the LLM. Mix LLM queries and function calling with regular Python code to create complex logic.</p>
<p dir="auto"><code>magentic</code> is</p>
<ul dir="auto">
<li><strong>Compact:</strong> Query LLMs without duplicating boilerplate code.</li>
<li><strong>Atomic:</strong> Prompts are functions that can be individually tested and reasoned about.</li>
<li><strong>Transparent:</strong> Create &#34;chains&#34; using regular Python code. Define all of your own prompts.</li>
<li><strong>Compatible:</strong> Use <code>@prompt</code> functions as normal functions, including with decorators like <code>@lru_cache</code>.</li>
<li><strong>Type Annotated:</strong> Works with linters and IDEs.</li>
</ul>
<p dir="auto">Continue reading for sample usage, or go straight to the <a href="https://github.com/jackmpcollins/magentic/blob/main/examples">examples directory</a>.</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>

<p dir="auto">or using poetry</p>

<p dir="auto">Configure your OpenAI API key by setting the <code>OPENAI_API_KEY</code> environment variable or using <code>openai.api_key = &#34;sk-...&#34;</code>. See the <a href="https://github.com/openai/openai-python#usage">OpenAI Python library documentation</a> for more information.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The <code>@prompt</code> decorator allows you to define a template for a Large Language Model (LLM) prompt as a Python function. When this function is called, the arguments are inserted into the template, then this prompt is sent to an LLM which generates the function output.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt


@prompt(&#39;Add more &#34;dude&#34;ness to: {phrase}&#39;)
def dudeify(phrase: str) -&gt; str:
    ...  # No function body as this is never executed


dudeify(&#34;Hello, how are you?&#34;)
# &#34;Hey, dude! What&#39;s up? How&#39;s it going, my man?&#34;"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>


<span>@<span>prompt</span>(<span>&#39;Add more &#34;dude&#34;ness to: {phrase}&#39;</span>)</span>
<span>def</span> <span>dudeify</span>(<span>phrase</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...  <span># No function body as this is never executed</span>


<span>dudeify</span>(<span>&#34;Hello, how are you?&#34;</span>)
<span># &#34;Hey, dude! What&#39;s up? How&#39;s it going, my man?&#34;</span></pre></div>
<p dir="auto">The <code>@prompt</code> decorator will respect the return type annotation of the decorated function. This can be <a href="https://docs.pydantic.dev/latest/usage/types/types/" rel="nofollow">any type supported by pydantic</a> including a <code>pydantic</code> model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt
from pydantic import BaseModel


class Superhero(BaseModel):
    name: str
    age: int
    power: str
    enemies: list[str]


@prompt(&#34;Create a Superhero named {name}.&#34;)
def create_superhero(name: str) -&gt; Superhero:
    ...


create_superhero(&#34;Garden Man&#34;)
# Superhero(name=&#39;Garden Man&#39;, age=30, power=&#39;Control over plants&#39;, enemies=[&#39;Pollution Man&#39;, &#39;Concrete Woman&#39;])"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>


<span>class</span> <span>Superhero</span>(<span>BaseModel</span>):
    <span>name</span>: <span>str</span>
    <span>age</span>: <span>int</span>
    <span>power</span>: <span>str</span>
    <span>enemies</span>: <span>list</span>[<span>str</span>]


<span>@<span>prompt</span>(<span>&#34;Create a Superhero named {name}.&#34;</span>)</span>
<span>def</span> <span>create_superhero</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>Superhero</span>:
    ...


<span>create_superhero</span>(<span>&#34;Garden Man&#34;</span>)
<span># Superhero(name=&#39;Garden Man&#39;, age=30, power=&#39;Control over plants&#39;, enemies=[&#39;Pollution Man&#39;, &#39;Concrete Woman&#39;])</span></pre></div>
<p dir="auto">An LLM can also decide to call functions. In this case the <code>@prompt</code>-decorated function returns a <code>FunctionCall</code> object which can be called to execute the function using the arguments provided by the LLM.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import Literal

from magentic import prompt, FunctionCall


def activate_oven(temperature: int, mode: Literal[&#34;broil&#34;, &#34;bake&#34;, &#34;roast&#34;]) -&gt; str:
    &#34;&#34;&#34;Turn the oven on with the provided settings.&#34;&#34;&#34;
    return f&#34;Preheating to {temperature} F with mode {mode}&#34;


@prompt(
    &#34;Prepare the oven so I can make {food}&#34;,
    functions=[activate_oven],
)
def configure_oven(food: str) -&gt; FunctionCall[str]:
    ...


output = configure_oven(&#34;cookies!&#34;)
# FunctionCall(&lt;function activate_oven at 0x1105a6200&gt;, temperature=350, mode=&#39;bake&#39;)
output()
# &#39;Preheating to 350 F with mode bake&#39;"><pre><span>from</span> <span>typing</span> <span>import</span> <span>Literal</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>, <span>FunctionCall</span>


<span>def</span> <span>activate_oven</span>(<span>temperature</span>: <span>int</span>, <span>mode</span>: <span>Literal</span>[<span>&#34;broil&#34;</span>, <span>&#34;bake&#34;</span>, <span>&#34;roast&#34;</span>]) <span>-&gt;</span> <span>str</span>:
    <span>&#34;&#34;&#34;Turn the oven on with the provided settings.&#34;&#34;&#34;</span>
    <span>return</span> <span>f&#34;Preheating to <span><span>{</span><span>temperature</span><span>}</span></span> F with mode <span><span>{</span><span>mode</span><span>}</span></span>&#34;</span>


<span>@<span>prompt</span>(</span>
<span>    <span>&#34;Prepare the oven so I can make {food}&#34;</span>,</span>
<span>    <span>functions</span><span>=</span>[<span>activate_oven</span>],</span>
<span>)</span>
<span>def</span> <span>configure_oven</span>(<span>food</span>: <span>str</span>) <span>-&gt;</span> <span>FunctionCall</span>[<span>str</span>]:
    ...


<span>output</span> <span>=</span> <span>configure_oven</span>(<span>&#34;cookies!&#34;</span>)
<span># FunctionCall(&lt;function activate_oven at 0x1105a6200&gt;, temperature=350, mode=&#39;bake&#39;)</span>
<span>output</span>()
<span># &#39;Preheating to 350 F with mode bake&#39;</span></pre></div>
<p dir="auto">Sometimes the LLM requires making one or more function calls to generate a final answer. The <code>@prompt_chain</code> decorator will resolve <code>FunctionCall</code> objects automatically and pass the output back to the LLM to continue until the final answer is reached.</p>
<p dir="auto">In the following example, when <code>describe_weather</code> is called the LLM first calls the <code>get_current_weather</code> function, then uses the result of this to formulate its final answer which gets returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt_chain


def get_current_weather(location, unit=&#34;fahrenheit&#34;):
    &#34;&#34;&#34;Get the current weather in a given location&#34;&#34;&#34;
    # Pretend to query an API
    return {
        &#34;location&#34;: location,
        &#34;temperature&#34;: &#34;72&#34;,
        &#34;unit&#34;: unit,
        &#34;forecast&#34;: [&#34;sunny&#34;, &#34;windy&#34;],
    }


@prompt_chain(
    &#34;What&#39;s the weather like in {city}?&#34;,
    functions=[get_current_weather],
)
def describe_weather(city: str) -&gt; str:
    ...


describe_weather(&#34;Boston&#34;)
# &#39;The current weather in Boston is 72°F and it is sunny and windy.&#39;"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt_chain</span>


<span>def</span> <span>get_current_weather</span>(<span>location</span>, <span>unit</span><span>=</span><span>&#34;fahrenheit&#34;</span>):
    <span>&#34;&#34;&#34;Get the current weather in a given location&#34;&#34;&#34;</span>
    <span># Pretend to query an API</span>
    <span>return</span> {
        <span>&#34;location&#34;</span>: <span>location</span>,
        <span>&#34;temperature&#34;</span>: <span>&#34;72&#34;</span>,
        <span>&#34;unit&#34;</span>: <span>unit</span>,
        <span>&#34;forecast&#34;</span>: [<span>&#34;sunny&#34;</span>, <span>&#34;windy&#34;</span>],
    }


<span>@<span>prompt_chain</span>(</span>
<span>    <span>&#34;What&#39;s the weather like in {city}?&#34;</span>,</span>
<span>    <span>functions</span><span>=</span>[<span>get_current_weather</span>],</span>
<span>)</span>
<span>def</span> <span>describe_weather</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...


<span>describe_weather</span>(<span>&#34;Boston&#34;</span>)
<span># &#39;The current weather in Boston is 72°F and it is sunny and windy.&#39;</span></pre></div>
<p dir="auto">LLM-powered functions created using <code>@prompt</code> and <code>@prompt_chain</code> can be supplied as <code>functions</code> to other <code>@prompt</code>/<code>@prompt_chain</code> decorators, just like regular python functions. This enables increasingly complex LLM-powered functionality, while allowing individual components to be tested and improved in isolation.</p>
<p dir="auto">See the <a href="https://github.com/jackmpcollins/magentic/blob/main/examples">examples directory</a> for more.</p>
<h3 tabindex="-1" id="user-content-streaming" dir="auto"><a href="#streaming">Streaming<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">The <code>StreamedStr</code> (and <code>AsyncStreamedStr</code>) class can be used to stream the output of the LLM. This allows you to process the text while it is being generated, rather than receiving the whole output at once.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt, StreamedStr


@prompt(&#34;Tell me about {country}&#34;)
def describe_country(country: str) -&gt; StreamedStr:
    ...


# Print the chunks while they are being received
for chunk in describe_country(&#34;Brazil&#34;):
    print(chunk, end=&#34;&#34;)
# &#39;Brazil, officially known as the Federative Republic of Brazil, is ...&#39;"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>, <span>StreamedStr</span>


<span>@<span>prompt</span>(<span>&#34;Tell me about {country}&#34;</span>)</span>
<span>def</span> <span>describe_country</span>(<span>country</span>: <span>str</span>) <span>-&gt;</span> <span>StreamedStr</span>:
    ...


<span># Print the chunks while they are being received</span>
<span>for</span> <span>chunk</span> <span>in</span> <span>describe_country</span>(<span>&#34;Brazil&#34;</span>):
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>&#34;&#34;</span>)
<span># &#39;Brazil, officially known as the Federative Republic of Brazil, is ...&#39;</span></pre></div>
<p dir="auto">Multiple <code>StreamedStr</code> can be created at the same time to stream LLM outputs concurrently. In the below example, generating the description for multiple countries takes approximately the same amount of time as for a single country.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from time import time

countries = [&#34;Australia&#34;, &#34;Brazil&#34;, &#34;Chile&#34;]


# Generate the descriptions one at a time
start_time = time()
for country in countries:
    # Converting `StreamedStr` to `str` blocks until the LLM output is fully generated
    description = str(describe_country(country))
    print(f&#34;{time() - start_time:.2f}s : {country} - {len(description)} chars&#34;)

# 22.72s : Australia - 2130 chars
# 41.63s : Brazil - 1884 chars
# 74.31s : Chile - 2968 chars


# Generate the descriptions concurrently by creating the StreamedStrs at the same time
start_time = time()
streamed_strs = [describe_country(country) for country in countries]
for country, streamed_str in zip(countries, streamed_strs):
    description = str(streamed_str)
    print(f&#34;{time() - start_time:.2f}s : {country} - {len(description)} chars&#34;)

# 22.79s : Australia - 2147 chars
# 23.64s : Brazil - 2202 chars
# 24.67s : Chile - 2186 chars"><pre><span>from</span> <span>time</span> <span>import</span> <span>time</span>

<span>countries</span> <span>=</span> [<span>&#34;Australia&#34;</span>, <span>&#34;Brazil&#34;</span>, <span>&#34;Chile&#34;</span>]


<span># Generate the descriptions one at a time</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>for</span> <span>country</span> <span>in</span> <span>countries</span>:
    <span># Converting `StreamedStr` to `str` blocks until the LLM output is fully generated</span>
    <span>description</span> <span>=</span> <span>str</span>(<span>describe_country</span>(<span>country</span>))
    <span>print</span>(<span>f&#34;<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>country</span><span>}</span></span> - <span><span>{</span><span>len</span>(<span>description</span>)<span>}</span></span> chars&#34;</span>)

<span># 22.72s : Australia - 2130 chars</span>
<span># 41.63s : Brazil - 1884 chars</span>
<span># 74.31s : Chile - 2968 chars</span>


<span># Generate the descriptions concurrently by creating the StreamedStrs at the same time</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>streamed_strs</span> <span>=</span> [<span>describe_country</span>(<span>country</span>) <span>for</span> <span>country</span> <span>in</span> <span>countries</span>]
<span>for</span> <span>country</span>, <span>streamed_str</span> <span>in</span> <span>zip</span>(<span>countries</span>, <span>streamed_strs</span>):
    <span>description</span> <span>=</span> <span>str</span>(<span>streamed_str</span>)
    <span>print</span>(<span>f&#34;<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>country</span><span>}</span></span> - <span><span>{</span><span>len</span>(<span>description</span>)<span>}</span></span> chars&#34;</span>)

<span># 22.79s : Australia - 2147 chars</span>
<span># 23.64s : Brazil - 2202 chars</span>
<span># 24.67s : Chile - 2186 chars</span></pre></div>
<h3 tabindex="-1" id="user-content-object-streaming" dir="auto"><a href="#object-streaming">Object Streaming<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">Structured outputs can also be streamed from the LLM by using the return type annotation <code>Iterable</code> (or <code>AsyncIterable</code>). This allows each item to be processed while the next one is being generated. See the example in <a href="https://github.com/jackmpcollins/magentic/blob/main/examples/quiz">examples/quiz</a> for how this can be used to improve user experience by quickly displaying/using the first item returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from collections.abc import Iterable
from time import time

from magentic import prompt
from pydantic import BaseModel


class Superhero(BaseModel):
    name: str
    age: int
    power: str
    enemies: list[str]


@prompt(&#34;Create a Superhero team named {name}.&#34;)
def create_superhero_team(name: str) -&gt; Iterable[Superhero]:
    ...


start_time = time()
for hero in create_superhero_team(&#34;The Food Dudes&#34;):
    print(f&#34;{time() - start_time:.2f}s : {hero}&#34;)

# 2.23s : name=&#39;Pizza Man&#39; age=30 power=&#39;Can shoot pizza slices from his hands&#39; enemies=[&#39;The Hungry Horde&#39;, &#39;The Junk Food Gang&#39;]
# 4.03s : name=&#39;Captain Carrot&#39; age=35 power=&#39;Super strength and agility from eating carrots&#39; enemies=[&#39;The Sugar Squad&#39;, &#39;The Greasy Gang&#39;]
# 6.05s : name=&#39;Ice Cream Girl&#39; age=25 power=&#39;Can create ice cream out of thin air&#39; enemies=[&#39;The Hot Sauce Squad&#39;, &#39;The Healthy Eaters&#39;]"><pre><span>from</span> <span>collections</span>.<span>abc</span> <span>import</span> <span>Iterable</span>
<span>from</span> <span>time</span> <span>import</span> <span>time</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>


<span>class</span> <span>Superhero</span>(<span>BaseModel</span>):
    <span>name</span>: <span>str</span>
    <span>age</span>: <span>int</span>
    <span>power</span>: <span>str</span>
    <span>enemies</span>: <span>list</span>[<span>str</span>]


<span>@<span>prompt</span>(<span>&#34;Create a Superhero team named {name}.&#34;</span>)</span>
<span>def</span> <span>create_superhero_team</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>Iterable</span>[<span>Superhero</span>]:
    ...


<span>start_time</span> <span>=</span> <span>time</span>()
<span>for</span> <span>hero</span> <span>in</span> <span>create_superhero_team</span>(<span>&#34;The Food Dudes&#34;</span>):
    <span>print</span>(<span>f&#34;<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>hero</span><span>}</span></span>&#34;</span>)

<span># 2.23s : name=&#39;Pizza Man&#39; age=30 power=&#39;Can shoot pizza slices from his hands&#39; enemies=[&#39;The Hungry Horde&#39;, &#39;The Junk Food Gang&#39;]</span>
<span># 4.03s : name=&#39;Captain Carrot&#39; age=35 power=&#39;Super strength and agility from eating carrots&#39; enemies=[&#39;The Sugar Squad&#39;, &#39;The Greasy Gang&#39;]</span>
<span># 6.05s : name=&#39;Ice Cream Girl&#39; age=25 power=&#39;Can create ice cream out of thin air&#39; enemies=[&#39;The Hot Sauce Squad&#39;, &#39;The Healthy Eaters&#39;]</span></pre></div>
<h3 tabindex="-1" id="user-content-asyncio" dir="auto"><a href="#asyncio">Asyncio<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">Asynchronous functions / coroutines can be used to concurrently query the LLM. This can greatly increase the overall speed of generation, and also allow other asynchronous code to run while waiting on LLM output. In the below example, the LLM generates a description for each US president while it is waiting on the next one in the list. Measuring the characters generated per second shows that this example achieves a 7x speedup over serial processing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from time import time
from typing import AsyncIterable

from magentic import prompt


@prompt(&#34;List ten presidents of the United States&#34;)
async def iter_presidents() -&gt; AsyncIterable[str]:
    ...


@prompt(&#34;Tell me more about {topic}&#34;)
async def tell_me_more_about(topic: str) -&gt; str:
    ...


# For each president listed, generate a description concurrently
start_time = time()
tasks = []
async for president in await iter_presidents():
    # Use asyncio.create_task to schedule the coroutine for execution before awaiting it
    # This way descriptions will start being generated while the list of presidents is still being generated
    task = asyncio.create_task(tell_me_more_about(president))
    tasks.append(task)

descriptions = await asyncio.gather(*tasks)

# Measure the characters per second
total_chars = sum(len(desc) for desc in descriptions)
time_elapsed = time() - start_time
print(total_chars, time_elapsed, total_chars / time_elapsed)
# 24575 28.70 856.07


# Measure the characters per second to describe a single president
start_time = time()
out = await tell_me_more_about(&#34;George Washington&#34;)
time_elapsed = time() - start_time
print(len(out), time_elapsed, len(out) / time_elapsed)
# 2206 18.72 117.78"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>time</span> <span>import</span> <span>time</span>
<span>from</span> <span>typing</span> <span>import</span> <span>AsyncIterable</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>


<span>@<span>prompt</span>(<span>&#34;List ten presidents of the United States&#34;</span>)</span>
<span>async</span> <span>def</span> <span>iter_presidents</span>() <span>-&gt;</span> <span>AsyncIterable</span>[<span>str</span>]:
    ...


<span>@<span>prompt</span>(<span>&#34;Tell me more about {topic}&#34;</span>)</span>
<span>async</span> <span>def</span> <span>tell_me_more_about</span>(<span>topic</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...


<span># For each president listed, generate a description concurrently</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>tasks</span> <span>=</span> []
<span>async</span> <span>for</span> <span>president</span> <span>in</span> <span>await</span> <span>iter_presidents</span>():
    <span># Use asyncio.create_task to schedule the coroutine for execution before awaiting it</span>
    <span># This way descriptions will start being generated while the list of presidents is still being generated</span>
    <span>task</span> <span>=</span> <span>asyncio</span>.<span>create_task</span>(<span>tell_me_more_about</span>(<span>president</span>))
    <span>tasks</span>.<span>append</span>(<span>task</span>)

<span>descriptions</span> <span>=</span> <span>await</span> <span>asyncio</span>.<span>gather</span>(<span>*</span><span>tasks</span>)

<span># Measure the characters per second</span>
<span>total_chars</span> <span>=</span> <span>sum</span>(<span>len</span>(<span>desc</span>) <span>for</span> <span>desc</span> <span>in</span> <span>descriptions</span>)
<span>time_elapsed</span> <span>=</span> <span>time</span>() <span>-</span> <span>start_time</span>
<span>print</span>(<span>total_chars</span>, <span>time_elapsed</span>, <span>total_chars</span> <span>/</span> <span>time_elapsed</span>)
<span># 24575 28.70 856.07</span>


<span># Measure the characters per second to describe a single president</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>out</span> <span>=</span> <span>await</span> <span>tell_me_more_about</span>(<span>&#34;George Washington&#34;</span>)
<span>time_elapsed</span> <span>=</span> <span>time</span>() <span>-</span> <span>start_time</span>
<span>print</span>(<span>len</span>(<span>out</span>), <span>time_elapsed</span>, <span>len</span>(<span>out</span>) <span>/</span> <span>time_elapsed</span>)
<span># 2206 18.72 117.78</span></pre></div>
<h3 tabindex="-1" id="user-content-additional-features" dir="auto"><a href="#additional-features">Additional Features<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<ul dir="auto">
<li>The <code>functions</code> argument to <code>@prompt</code> can contain async/coroutine functions. When the corresponding <code>FunctionCall</code> objects are called the result must be awaited.</li>
<li>The <code>Annotated</code> type annotation can be used to provide descriptions and other metadata for function parameters. See <a href="https://docs.pydantic.dev/latest/usage/validation_decorator/#using-field-to-describe-function-arguments" rel="nofollow">the pydantic documentation on using <code>Field</code> to describe function arguments</a>.</li>
<li>The <code>@prompt</code> and <code>@prompt_chain</code> decorators also accept a <code>model</code> argument. You can pass an instance of <code>OpenaiChatModel</code> (from <code>magentic.chat_model.openai_chat_model</code>) to use GPT4 or configure a different temperature.</li>
</ul>
<h2 tabindex="-1" id="user-content-configuration" dir="auto"><a href="#configuration">Configuration<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The order of precedence of configuration is</p>
<ol dir="auto">
<li>Arguments passed when initializing an instance in Python</li>
<li>Environment variables</li>
</ol>
<p dir="auto">The following environment variables can be set.</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>MAGENTIC_OPENAI_MODEL</td>
<td>OpenAI model</td>
<td>gpt-4</td>
</tr>
<tr>
<td>MAGENTIC_OPENAI_TEMPERATURE</td>
<td>OpenAI temperature</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-type-checking" dir="auto"><a href="#type-checking">Type Checking<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">Many type checkers will raise warnings or errors for functions with the <code>@prompt</code> decorator due to the function having no body or return value. There are several ways to deal with these.</p>
<ol dir="auto">
<li>Disable the check globally for the type checker. For example in mypy by disabling error code <code>empty-body</code>.
<div dir="auto" data-snippet-clipboard-copy-content="# pyproject.toml
[tool.mypy]
disable_error_code = [&#34;empty-body&#34;]"><pre><span><span>#</span> pyproject.toml</span>
[<span>tool</span>.<span>mypy</span>]
<span>disable_error_code</span> = [<span><span>&#34;</span>empty-body<span>&#34;</span></span>]</pre></div>
</li>
<li>Make the function body <code>...</code> (this does not satisfy mypy) or <code>raise</code>.
<div dir="auto" data-snippet-clipboard-copy-content="@prompt(&#34;Choose a color&#34;)
def random_color() -&gt; str:
    ..."><pre><span>@<span>prompt</span>(<span>&#34;Choose a color&#34;</span>)</span>
<span>def</span> <span>random_color</span>() <span>-&gt;</span> <span>str</span>:
    ...</pre></div>
</li>
<li>Use comment <code># type: ignore[empty-body]</code> on each function. In this case you can add a docstring instead of <code>...</code>.
<div dir="auto" data-snippet-clipboard-copy-content="@prompt(&#34;Choose a color&#34;)
def random_color() -&gt; str:  # type: ignore[empty-body]
    &#34;&#34;&#34;Returns a random color.&#34;&#34;&#34;"><pre><span>@<span>prompt</span>(<span>&#34;Choose a color&#34;</span>)</span>
<span>def</span> <span>random_color</span>() <span>-&gt;</span> <span>str</span>:  <span># type: ignore[empty-body]</span>
    <span>&#34;&#34;&#34;Returns a random color.&#34;&#34;&#34;</span></pre></div>
</li>
</ol>
</article>
          </div></div>
  </body>
</html>
