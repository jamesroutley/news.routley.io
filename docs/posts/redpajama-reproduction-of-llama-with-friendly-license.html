<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.together.xyz/blog/redpajama">Original</a>
    <h1>RedPajama: Reproduction of Llama with Friendly License</h1>
    
    <div id="readability-page-1" class="page"><article id="sections" data-page-sections="6358d155bd7bbd5929470f3f">
  
  
    
    


  


<section data-test="page-section" data-section-theme="white" data-section-id="6358d155bd7bbd5929470f41" data-controller="SectionWrapperController" data-current-styles="{
&#34;imageOverlayOpacity&#34;: 0.15,
&#34;backgroundWidth&#34;: &#34;background-width--full-bleed&#34;,
&#34;sectionHeight&#34;: &#34;section-height--medium&#34;,
&#34;horizontalAlignment&#34;: &#34;horizontal-alignment--center&#34;,
&#34;verticalAlignment&#34;: &#34;vertical-alignment--middle&#34;,
&#34;contentWidth&#34;: &#34;content-width--wide&#34;,
&#34;sectionTheme&#34;: &#34;white&#34;,
&#34;sectionAnimation&#34;: &#34;none&#34;,
&#34;backgroundMode&#34;: &#34;image&#34;
}" data-current-context="{
&#34;video&#34;: {
&#34;playbackSpeed&#34;: 0.5,
&#34;filter&#34;: 1,
&#34;filterStrength&#34;: 0,
&#34;zoom&#34;: 0,
&#34;videoSourceProvider&#34;: &#34;none&#34;
},
&#34;backgroundImageId&#34;: null,
&#34;backgroundMediaEffect&#34;: null,
&#34;divider&#34;: null,
&#34;typeName&#34;: &#34;blog-basic-grid&#34;
}" data-animation="none">
  <div>
    
  </div>
  <div>
    <div>
      
      
      
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      <div>
        
        <div>
          

          
        </div>
      </div>

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-643d0a954755f728a2366684"><div><div><div data-block-type="5" id="block-yui_3_17_2_1_1681722006456_91447"><div>
































  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/f8b07fd1-d7b8-4729-94b8-364dcc47890a/RedPajama.png" data-image="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/f8b07fd1-d7b8-4729-94b8-364dcc47890a/RedPajama.png" data-image-dimensions="2048x2048" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="643d0f2035afc367b0cdff32" data-type="image"/>
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-59a4c63701f6044c4d38"><div>

<h4>Foundation models such as GPT-4 have driven rapid improvement in AI. However, the most powerful models are closed commercial models or only partially open. RedPajama is a project to create a set of leading, fully open-source models. Today, we are excited to announce the completion of the first step of this project: the reproduction of the LLaMA training dataset of over 1.2 trillion tokens.</h4><p>The most capable foundation models today are closed behind commercial APIs, which limits research, customization, and their use with sensitive data. Fully open-source models hold the promise of removing these limitations, if the open community can close the quality gap between open and closed models. Recently, there has been much progress along this front. In many ways, <a href="https://hazyresearch.stanford.edu/blog/2023-01-30-ai-linux">AI is having its Linux moment</a>. Stable Diffusion showed that open-source can not only rival the quality of commercial offerings like DALL-E but can also lead to incredible creativity from broad participation by communities around the world. A similar movement has now begun around large language models with the recent release of semi-open models like <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a>, <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>, <a href="https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b">Vicuna</a>, and <a href="https://bair.berkeley.edu/blog/2023/04/03/koala/?utm_source=substack&amp;utm_medium=email">Koala</a>; as well as fully-open models like <a href="https://github.com/EleutherAI/pythia">Pythia</a>, <a href="https://huggingface.co/spaces/togethercomputer/OpenChatKit">OpenChatKit</a>, <a href="https://open-assistant.io/dashboard">Open Assistant</a> and <a href="https://github.com/databrickslabs/dolly">Dolly</a>.</p><p>We are launching RedPajama, an effort to produce a reproducible, fully-open, leading language model. RedPajama is a collaboration between Together, <a href="https://www.ontocord.ai/">Ontocord.ai</a>, <a href="https://ds3lab.inf.ethz.ch/">ETH DS3Lab</a>, <a href="https://crfm.stanford.edu/">Stanford CRFM</a>, <a href="https://hazyresearch.stanford.edu/">Hazy Research</a>, and <a href="https://mila.quebec/en/">MILA Québec AI Institute</a>. RedPajama has three key components:</p><ol data-rte-list="default"><li><p>Pre-training data, which needs to be both high quality and have broad coverage</p></li><li><p>Base models, which are trained at scale on this data</p></li><li><p>Instruction tuning data and models, which improve the base model to make it usable and safe</p></li></ol><p>Today, we are releasing the first component, pre-training data.</p>



</div></div><div data-block-type="31" id="block-yui_3_17_2_1_1681722006456_14473"><div>



<figure>
  <blockquote data-animation-role="quote">
    <span>“</span>The RedPajama base dataset is a 1.2 trillion token fully-open dataset created by following the recipe described in the LLaMA paper.<span>”</span>
  </blockquote>
  
</figure>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1681722006456_16206"><div>

<p>Our starting point is <a href="https://arxiv.org/abs/2302.13971">LLaMA</a>, which is the leading suite of open base models for two reasons: First, LLaMA was trained on a very large (1.2 trillion tokens) dataset that was carefully filtered for quality. Second, the 7 billion parameter LLaMA model is trained for much longer, well beyond the Chincilla-optimal point, to ensure the best quality at that model size. A 7 billion parameter model is particularly valuable for the open community as it can run on a wide variety of GPUs, including many consumer grade GPUs. However, LLaMA and all its derivatives (including Alpaca, Vicuna, and Koala) are only available for non-commercial research purposes. We aim to create a fully open-source reproduction of LLaMA, which would be available for commercial applications, and provide a more transparent pipeline for research.</p><h3><strong>The RedPajama base dataset </strong></h3><p>The full RedPajama 1.2 trillion token dataset and a smaller, more consumable random sample can be downloaded through <a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T">Hugging Face</a>.</p><p>RedPajama-Data-1T consists of seven data slices:</p><ul data-rte-list="default"><li><p>CommonCrawl: Five dumps of CommonCrawl, processed using the CCNet pipeline, and filtered via several quality filters including a linear classifier that selects for Wikipedia-like pages.</p></li><li><p>C4: Standard C4 dataset</p></li><li><p>GitHub: GitHub data, filtered by licenses and quality</p></li><li><p>arXiv: Scientific articles removing boilerplate</p></li><li><p>Books: A corpus of open books, deduplicated by content similarity</p></li><li><p>Wikipedia: A subset of Wikipedia pages, removing boilerplate</p></li><li><p>StackExchange: A subset of popular websites under StackExchange, removing boilerplate</p></li></ul><p>For each data slice, we conduct careful data pre-processing and filtering, and tune our quality filters to roughly match the number of tokens as reported by <a href="https://www.facebook.com/MetaAI/">Meta AI</a> in the <a href="https://arxiv.org/abs/2302.13971">LLaMA paper</a>:</p>



</div></div><div data-block-type="23" id="block-0bc3cc8bd1590b2d19c3"><div>
<table>
<thead>
  <tr>
    <th></th>
    <th><span>  RedPajama  </span></th>
    <th><span>  LLaMA*  </span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>CommonCrawl  </td>
    <td>878 billion  </td>
    <td>852 billion  </td>
  </tr>
  <tr>
    <td>C4</td>
    <td>175 billion</td>
    <td>190 billion</td>
  </tr>
  <tr>
    <td>Github</td>
    <td>59 billion</td>
    <td>100 billion</td>
  </tr>
  <tr>
    <td>Books</td>
    <td>26 billion</td>
    <td>25 billion</td>
  </tr>
  <tr>
    <td>ArXiv</td>
    <td>28 billion</td>
    <td>33 billion</td>
  </tr>
  <tr>
    <td>Wikipedia</td>
    <td>24 billion</td>
    <td>25 billion</td>
  </tr>
  <tr>
    <td>StackExchange</td>
    <td>20 billion</td>
    <td>27 billion</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>1.2 trillion</td>
    <td>1.25 trillion</td>
  </tr>
</tbody>
</table>
<hr/><p>
* estimated from Table 1 in https://arxiv.org/abs/2302.13971
</p></div></div><div data-block-type="2" id="block-79c33505b2b74cee01de"><div>
  


<p>We are making all data pre-processing and quality filters openly available on <a href="https://github.com/togethercomputer/RedPajama-Data">Github</a>. Anyone can follow the data preparation recipe and reproduce RedPajama-Data-1T.</p><h3>Interactively analyzing the RedPajama base dataset </h3><p>In collaboration with the <a href="https://github.com/hazyresearch/meerkat">Meerkat</a> project, we are releasing a Meerkat dashboard and embeddings for exploring the Github subset of the corpus. The image below shows a preview of the dashboard.</p>



</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1681722006456_27570"><div>
































  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/e706dd82-0ec4-46d8-8234-7a96d1ad80b9/meerkat_visualization.png" data-image="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/e706dd82-0ec4-46d8-8234-7a96d1ad80b9/meerkat_visualization.png" data-image-dimensions="737x645" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="643d120649b6071c3af4d15c" data-type="image"/>
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Interactively explore the data in the RedPajama base dataset and view matching records using Meerkat dashboard. </p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1681723706219_12666"><div>
  


<p>You can find instructions on how to install and use the dashboard on <a href="https://github.com/togethercomputer/RedPajama-Data">Github</a>.</p><h3>Up next: Models, instructions &amp; OpenChatKit</h3><p>Having reproduced the pre-training data, the next step is to train a strong base model. As part of the <a href="https://www.alcf.anl.gov/science/incite-allocation-program">INCITE program</a>, with support from <a href="https://www.olcf.ornl.gov/">Oak Ridge Leadership Computing Facility (OLCF)</a>, we are training a full suite of models, with the first becoming available in the coming weeks.</p><p>With a strong base model in hand, we are excited to instruction tune the models. <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a> illustrated the power of instruction tuning – with merely 50K high-quality, diverse instructions, it was able to unlock dramatically improved capabilities. Via OpenChatKit, we received hundreds of thousands of high-quality natural user instructions, which will be used to release instruction-tuned versions of the RedPajama models.<br/></p><h3>Acknowledgements</h3><p>We are appreciative to the work done by the growing open-source AI community that made this project possible. </p><p>That includes:</p><ul data-rte-list="default"><li><p>Participants in building the RedPajama dataset including <a href="https://www.ontocord.ai/">Ontocord.ai</a>, <a href="https://mila.quebec/en/">MILA - Québec AI Institute</a>, <a href="https://ds3lab.inf.ethz.ch/">ETH DS3Lab</a>, <a href="https://www.umontreal.ca/">Université de Montréal</a>, <a href="https://crfm.stanford.edu/">Stanford Center for Research on Foundation Models (CRFM)</a>, <a href="https://hazyresearch.stanford.edu/">Stanford Hazy Research research group</a> and <a href="https://laion.ai/">LAION</a>.  </p></li><li><p><a href="https://www.facebook.com/MetaAI/">Meta AI</a> — Their inspiring work on LLaMA shows a concrete path towards building strong language models, and it is the original source for our dataset replication. </p></li><li><p><a href="https://www.eleuther.ai/">EleutherAI</a> — This project is built on the backs of the great team at EleutherAI — including the source code they provided for training GPT-NeoX. </p></li><li><p>An award of computer time was provided by the <a href="https://www.alcf.anl.gov/science/incite-allocation-program">INCITE program</a>. This research also used resources of the <a href="https://www.olcf.ornl.gov/">Oak Ridge Leadership Computing Facility (OLCF)</a>, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725.</p></li></ul>



</div></div><div data-aspect-ratio="14.285714285714286" data-block-type="21" id="block-ec8019ea35d355011ec2"></div><div data-block-type="2" id="block-26524e8c8c51bc7c6ce2"><p>Get notified of future posts and updates:</p></div><div data-block-type="9" id="block-bfc5194b80fe40ab2a01"><div>






  


<div>
  <p>Sign up for updates</p>
  <div>
    <form data-form-id="643d0a954755f728a2366682" data-success-redirect="" data-dynamic-strings="" autocomplete="on" method="POST" action="https://cod-purple-ysdj.squarespace.com" novalidate="" onsubmit="return (function (form) {
  Y.use(&#39;squarespace-form-submit&#39;, &#39;node&#39;, function usingFormSubmit(Y) {
    (new Y.Squarespace.FormSubmit(form)).submit({
      formId: &#39;643d0a954755f728a2366682&#39;,
      collectionId: &#39;6358d155bd7bbd5929470f36&#39;,
      objectName: &#39;item-643d0a954755f728a2366684&#39;
    });
  });
  return false;
})(this);">
      
        <div>

        

          

          

            

            

            
              <p><label for="text-5933ae86-e4ad-4013-a235-a47c6ae247e1-field">
              Name
              
                <span aria-hidden="true">*</span>
              
            </label>
          
                
                
              </p>
            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

        

          

          

            

            

            
              <p><label for="text-2a82efaa-5b5e-4eff-95d8-d4bf672269d6-field">
              Company or Organization
              
                <span aria-hidden="true">*</span>
              
            </label>
          
                
                
              </p>
            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

        

          

          

            

            

            

            

            
              <p><label for="email-f9759b1b-4f34-4d41-92d3-5e813d598015-field">
              Email
              
                <span aria-hidden="true">*</span>
              
            </label>
          
                
                
              </p>
            

            

            

            

            

            

            

            

            

            

            

            

            

            

            

        

          

          

            
              </div>

      

      

      
      
      

      
        
      
      <p>Thank you!</p>
      
    </form>
  </div>
</div>
</div></div></div><div><div data-block-type="2" id="block-87d679bef8acadc44e17"><div>

<p><strong>Links in this article:</strong></p><p>• <a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T">RedPajama base dataset</a> </p>



</div></div></div></div></div></div>

        

        
        
          <div data-content-field="author">
            <p><a href="https://www.together.xyz/blog?author=63595ea9e1936e1e0c39f52d">
  
  <span>Together</span>
</a>

<a href="https://together.xyz" target="_blank">https://together.xyz</a>

          </p></div>
        
      </div>

      <section>
        <div>
          
        </div>
      </section>
    </div>
  
</article>

</div>
    </div>
  </div>
  
  
</section>

  
</article></div>
  </body>
</html>
