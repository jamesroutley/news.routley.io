<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/CultriX/MistralTrix-v1">Original</a>
    <h1>Best 7B LLM on leaderboards made by an amateur following a medium tutorial</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p>T: ðŸŸ¦
Model: CultriX/MistralTrix-v1 ðŸ“‘
Average: 73.39
ARC: 72.27
HellaSwag: 88.33
MMLU: 65.24
TruthfulQA: 70.73
Winogrande: 80.98
GSM8K: 62.77</p>

<p>Currently the #1 ranked 7B LLM on the LLM Leaderboards, woah!
I did not expect that result at all and am in no way a professional when it comes to LLM&#39;s or computer science in general,
just a guy that likes to nerd about and tinker around. </p>
<p>For those wondering how I achieved this, the answer is that I simply attempted to apply the techniques outlined in this amazing article myself: <a rel="noopener nofollow" href="https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac">https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac</a>
Therefore, all credit basically goes to the guy who wrote that. 
He offers the exact Colab notebook I used to train this model for free, as well as a really nice GitHub page I hope he doesn&#39;t mind me sharing: <a rel="noopener nofollow" href="https://github.com/mlabonne/llm-course/">https://github.com/mlabonne/llm-course/</a>
So huge thank you to him for sharing his knowledge and learning me a thing or two in the process!</p>

<p>I attempted to quantisize the model myself, which again I pretty much have no clue about, but it seems to run fine for me when I test them:
<a rel="noopener nofollow" href="https://huggingface.co/CultriX/MistralTrix-v1-GGUF">https://huggingface.co/CultriX/MistralTrix-v1-GGUF</a></p>
<p>I&#39;ll say it one more time though:
&#34;I am a complete beginner to all of this, so if these do end up sucking don&#39;t be surprised.&#34;</p>
<p>You have been warned :)</p>

<p>(trained on a single Colab GPU in less than a few hours)</p>
<p>MistralTrix-v1 is an zyh3826/GML-Mistral-merged-v1 model that has been further fine-tuned with Direct Preference Optimization (DPO) using Intel&#39;s dataset for neural-chat-7b-v3-1.
It surpasses the original model on several benchmarks (see results).</p>
<p>It is directly inspired by the RLHF process described by Intel/neural-chat-7b-v3-1&#39;s authors to improve performance. 
I used the same dataset and reformatted it to apply the ChatML template.</p>
<p>The code to train this model is available on Google Colab and GitHub. 
Fine-tuning took about an hour on Google Colab A-1000 GPU with 40GB VRAM.</p>

<blockquote>
<p>LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias=&#34;none&#34;,
    task_type=&#34;CAUSAL_LM&#34;,
    target_modules=[&#39;k_proj&#39;, &#39;gate_proj&#39;, &#39;v_proj&#39;, &#39;up_proj&#39;, &#39;q_proj&#39;, &#39;o_proj&#39;, &#39;down_proj&#39;]
)</p>
</blockquote>
<blockquote>
<p>Model to fine-tune
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)
model.config.use_cache = False</p>
</blockquote>
<blockquote>
<p>Reference model
ref_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)</p>
</blockquote>
<blockquote>
<p>Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    learning_rate=5e-5,
    lr_scheduler_type=&#34;cosine&#34;,
    max_steps=200,
    save_strategy=&#34;no&#34;,
    logging_steps=1,
    output_dir=new_model,
    optim=&#34;paged_adamw_32bit&#34;,
    warmup_steps=100,
    bf16=True,
    report_to=&#34;wandb&#34;,
)</p>
</blockquote>
<blockquote>
<p>Create DPO trainer
dpo_trainer = DPOTrainer(
    model,
    ref_model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
    beta=0.1,
    max_prompt_length=1024,
    max_length=1536,
)</p>
</blockquote>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
