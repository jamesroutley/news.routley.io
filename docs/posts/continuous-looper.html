<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://beijaflor.io/blog/04-2022/rust-audio-experiments-5/">Original</a>
    <h1>Continuous Looper</h1>
    
    <div id="readability-page-1" class="page"><section><p>This is an April update on <a href="https://github.com/yamadapc/augmented-audio"><strong>Augmented Audio Libraries</strong></a>,
my hobby libraries for audio programming in Rust (<a href="https://beijaflor.io/blog">check previous updates</a>).</p>
<p>It is a special one because today I have released an initial version of
<strong>Continuous Looper</strong> on the macOS App Store (iOS version coming <del>soon</del> next).</p>
<p>There’s a lot to do and fix, but I think publishing this post (and the app)
to get some feedback will help me.</p>
<p><a href="https://apps.apple.com/au/app/continuous-looper/id1616355791?mt=12"><strong>You can download the app now here</strong></a></p>
<p>In this post, I’ll discuss how I have written this app, the approaches I have
been trying and challenges faced.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/b2fd5b5cfe0544c874b0d6f8091e4198/f51c2/01_cover.webp 148w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/66888/01_cover.webp 295w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/bc8a3/01_cover.webp 590w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/4ce34/01_cover.webp 885w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/f490a/01_cover.webp 1180w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/da28f/01_cover.webp 2880w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/b2fd5b5cfe0544c874b0d6f8091e4198/c5084/01_cover.png 148w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/60cc9/01_cover.png 295w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/6d370/01_cover.png 590w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/540ae/01_cover.png 885w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/2561a/01_cover.png 1180w,
/static/b2fd5b5cfe0544c874b0d6f8091e4198/f9c26/01_cover.png 2880w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/b2fd5b5cfe0544c874b0d6f8091e4198/6d370/01_cover.png" alt="Continuous Looper screenshot" title="Continuous Looper screenshot" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<hr/>
<h2 id="continuous-looper"><a href="#continuous-looper" aria-label="continuous looper permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Continuous Looper</h2>
<p><strong>Continuous Looper</strong> is a fully open-source 8-track live-looper and performance
sampler, written in Rust/Swift which features:</p>
<ul>
<li>8-track looping/sequencing</li>
<li>Step sequencing live after recording</li>
<li>2 quantization modes</li>
<li>Scenes and parameter locks</li>
<li>Click and twist MIDI mapping to any parameter &amp; the record-button</li>
<li>2 LFOs + 1 ADSR envelope per track</li>
<li>Automatic slicing based on transient detection</li>
<li>Basic pitch shifting (potentially dated phase-vocoder)</li>
<li>Variable start/end/speed</li>
</ul>
<p>Since the scope got large, there are many rough edges! But it has worked well.</p>
<p>The app/GUI source-code is available under the AGPLv3 license at <a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/apps/looper/Sequencer">augmented-audio/crates/apps/looper/Sequencer</a>
and the engine source-code is available under the MIT license at <a href="https://github.com/yamadapc/augmented-audio">augmented-audio</a>.</p>
<p><strong>Continuous Looper</strong> is inspired by the great <a href="https://www.beepstreet.com/ios/drambo">Drambo iOS modular synth</a>
and by <a href="https://www.elektron.se/products/octatrack-mkii/">Octatrack</a>.</p>
<h3 id="demo-video"><a href="#demo-video" aria-label="demo video permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Demo video</h3>
<p>Here is a tiny demo video.</p>
<p><em>(Note: this is an iPhone video and audio is very low)</em></p>

<hr/>
<h2 id="table-of-contents"><a href="#table-of-contents" aria-label="table of contents permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Table of contents</h2>
<ol>
<li><a href="#some-requirements-for-the-app">Requirements</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#rust-and-audio">Rust and audio</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<hr/>
<h2 id="some-requirements-for-the-app"><a href="#some-requirements-for-the-app" aria-label="some requirements for the app permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Some requirements for the app</h2>
<p>It’s good to mention a couple of requirements that guide the decisions I have
been taking and greatly increase complexity.</p>
<h3 id="real-time-audio"><a href="#real-time-audio" aria-label="real time audio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Real-time audio</h3>
<p>The app must render audio in real-time. At fixed rate intervals, our software’s
audio callback will be called with a buffer of numbers representing
an audio signal. Our software will have a capped amount of time
(roughly between 1ms-10ms) to finish. It must finish in this time or else
playback will audibly glitch.</p>
<p>The code that runs in the audio-thread:</p>
<ul>
<li>should avoid variable size work</li>
<li>should avoid locks</li>
<li>should avoid allocation/de-allocation</li>
<li>should not perform IO</li>
</ul>
<p>Some good resources on this subject:</p>
<ul>
<li><a href="http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing">Real-time audio programming 101: time waits for nothing - Ross Bencina</a></li>
<li><a href="https://www.youtube.com/watch?v=zrWYJ6FdOFQ">Using Locks in Real-Time Audio Processing, Safely - Timur Doumler - ADC20</a></li>
<li><a href="https://www.youtube.com/watch?v=SJXGSJ6Zoro">The Golden Rules of Audio Programming - Pete Goodliffe</a></li>
</ul>
<h3 id="interactive-gui"><a href="#interactive-gui" aria-label="interactive gui permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interactive GUI</h3>
<p>Second, we want the GUI to be highly interactive. The GUI should render
visualisations of the operations taking place in the audio side whenever
possible.</p>
<p><em>(Note: this is a screen recording and much louder than the previous video)</em></p>

<p>The following data needs to constantly update on the UI:</p>
<ul>
<li>Global play-head position
<ul>
<li>The position of the track in beats at the center-top</li>
</ul>
</li>
<li>Sequencer position
<ul>
<li>The current active step flashes in the sequencer view</li>
</ul>
</li>
<li>Looper play-head position
<ul>
<li>The position of each looper within its recording, represented by the green
line over the rendered audio wave and by the green fill over the track’s
button</li>
</ul>
</li>
<li>Last MIDI messages
<ul>
<li>This isn’t shown, but a monitor of previous events is displayed when the
MIDI button on the top-right is clicked</li>
</ul>
</li>
</ul>
<h3 id="looper-quantization"><a href="#looper-quantization" aria-label="looper quantization permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Looper quantization</h3>
<p>This is the main reason I wanted to write a looper.</p>
<p><strong>Quantization</strong> in live-loopers refers to automatic tempo correction of the
presses to the record/stop/play buttons. This is to aid musicians to record
loops in sync with some tempo.</p>
<p>For example, there may be a drum track playing or another looped track.
Whenever a new loop is recorded it should have a length that makes sense in the
current tempo and time-signature (subject to configuration).</p>
<p>Most software loopers I’ve tried only correct the musician being <strong>early</strong>, but
don’t correct them being <strong>late</strong>. I have hardware looper pedals which do both.</p>
<p>If there’s a tempo track playing and a musician presses to start a recording at
beat 3.7, the looper will wait until the 1st beat of the next bar.
This is correcting for when the musician is <strong>early</strong>.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/af79f98032db384bb761e4b30ca2f8f4/f51c2/snap-next.webp 148w,
/static/af79f98032db384bb761e4b30ca2f8f4/66888/snap-next.webp 295w,
/static/af79f98032db384bb761e4b30ca2f8f4/bc8a3/snap-next.webp 590w,
/static/af79f98032db384bb761e4b30ca2f8f4/4ce34/snap-next.webp 885w,
/static/af79f98032db384bb761e4b30ca2f8f4/f490a/snap-next.webp 1180w,
/static/af79f98032db384bb761e4b30ca2f8f4/08048/snap-next.webp 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/af79f98032db384bb761e4b30ca2f8f4/c5084/snap-next.png 148w,
/static/af79f98032db384bb761e4b30ca2f8f4/60cc9/snap-next.png 295w,
/static/af79f98032db384bb761e4b30ca2f8f4/6d370/snap-next.png 590w,
/static/af79f98032db384bb761e4b30ca2f8f4/540ae/snap-next.png 885w,
/static/af79f98032db384bb761e4b30ca2f8f4/2561a/snap-next.png 1180w,
/static/af79f98032db384bb761e4b30ca2f8f4/3643c/snap-next.png 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/af79f98032db384bb761e4b30ca2f8f4/6d370/snap-next.png" alt="Snap next quantization diagram" title="Snap next quantization diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>However, if the musician is late, we’d also like to correct the tempo, by
assuming that the button had already been pressed (in the past) and then
continuing to operate.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/1c99ae9211b522df4aee9a3a4d3093de/f51c2/snap-closest.webp 148w,
/static/1c99ae9211b522df4aee9a3a4d3093de/66888/snap-closest.webp 295w,
/static/1c99ae9211b522df4aee9a3a4d3093de/bc8a3/snap-closest.webp 590w,
/static/1c99ae9211b522df4aee9a3a4d3093de/4ce34/snap-closest.webp 885w,
/static/1c99ae9211b522df4aee9a3a4d3093de/f490a/snap-closest.webp 1180w,
/static/1c99ae9211b522df4aee9a3a4d3093de/08048/snap-closest.webp 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/1c99ae9211b522df4aee9a3a4d3093de/c5084/snap-closest.png 148w,
/static/1c99ae9211b522df4aee9a3a4d3093de/60cc9/snap-closest.png 295w,
/static/1c99ae9211b522df4aee9a3a4d3093de/6d370/snap-closest.png 590w,
/static/1c99ae9211b522df4aee9a3a4d3093de/540ae/snap-closest.png 885w,
/static/1c99ae9211b522df4aee9a3a4d3093de/2561a/snap-closest.png 1180w,
/static/1c99ae9211b522df4aee9a3a4d3093de/3643c/snap-closest.png 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/1c99ae9211b522df4aee9a3a4d3093de/6d370/snap-closest.png" alt="Snap closest quantization diagram" title="Snap closest quantization diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>These two quantization strategies are implemented and selectable on
<strong>Continuous Looper</strong> and there is some more around implementation later on.</p>
<h2 id="architecture"><a href="#architecture" aria-label="architecture permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architecture</h2>
<blockquote>
<p><strong>IMPORTANT:</strong> I am not a professional audio developer</p>
<p>Please give me feedback! :)</p>
</blockquote>
<p><strong>Continuous Looper</strong>’s engine is built using the <code>Rust</code> crates I have been
building on <a href="https://github.com/yamadapc/augmented-audio/"><code>augmented-audio</code></a>.
It benefits from several crates in the rust ecosystem such as <a href="https://docs.rs/petgraph/latest/petgraph/">petgraph</a>,
<a href="https://github.com/RustAudio/cpal"><code>cpal</code></a>, <a href="https://github.com/glowcoil/basedrop"><code>basedrop</code></a>,
<a href="https://github.com/actix/actix/"><code>actix</code></a> and more (<code>ringbuf</code>, etc. the list
of dependencies is very long).</p>
<p>The GUI is built using Swift, AppKit (and UIKit at some point) and SwiftUI.</p>
<p>Originally a simpler looper GUI was built using <code>iced</code>:</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/2ba68ebf7054c95f5faae2d401495d12/f51c2/looper-screenshot.webp 148w,
/static/2ba68ebf7054c95f5faae2d401495d12/66888/looper-screenshot.webp 295w,
/static/2ba68ebf7054c95f5faae2d401495d12/bc8a3/looper-screenshot.webp 590w,
/static/2ba68ebf7054c95f5faae2d401495d12/4ce34/looper-screenshot.webp 885w,
/static/2ba68ebf7054c95f5faae2d401495d12/f490a/looper-screenshot.webp 1180w,
/static/2ba68ebf7054c95f5faae2d401495d12/7ef45/looper-screenshot.webp 1624w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/2ba68ebf7054c95f5faae2d401495d12/c5084/looper-screenshot.png 148w,
/static/2ba68ebf7054c95f5faae2d401495d12/60cc9/looper-screenshot.png 295w,
/static/2ba68ebf7054c95f5faae2d401495d12/6d370/looper-screenshot.png 590w,
/static/2ba68ebf7054c95f5faae2d401495d12/540ae/looper-screenshot.png 885w,
/static/2ba68ebf7054c95f5faae2d401495d12/2561a/looper-screenshot.png 1180w,
/static/2ba68ebf7054c95f5faae2d401495d12/fc0cd/looper-screenshot.png 1624w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/2ba68ebf7054c95f5faae2d401495d12/6d370/looper-screenshot.png" alt="Looper Iced GUI screen-shot" title="Looper Iced GUI screen-shot" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>I will likely still use <code>iced</code> to ship a simplified VST.</p>
<p>Swift/AppKit/UIKit/SwiftUI are more mature and allow iOS support, so this
version of the app uses them.</p>
<h3 id="containers-or-components"><a href="#containers-or-components" aria-label="containers or components permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Containers or components</h3>
<p>Here’s a “Container diagram” (I’m likely misusing the <a href="https://c4model.com/">C4 model</a>
in my context):</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/b0a5f57122a811c2940ea1b9138506b7/f51c2/container-diagram.webp 148w,
/static/b0a5f57122a811c2940ea1b9138506b7/66888/container-diagram.webp 295w,
/static/b0a5f57122a811c2940ea1b9138506b7/bc8a3/container-diagram.webp 590w,
/static/b0a5f57122a811c2940ea1b9138506b7/4ce34/container-diagram.webp 885w,
/static/b0a5f57122a811c2940ea1b9138506b7/f490a/container-diagram.webp 1180w,
/static/b0a5f57122a811c2940ea1b9138506b7/ff2d6/container-diagram.webp 1754w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/b0a5f57122a811c2940ea1b9138506b7/c5084/container-diagram.png 148w,
/static/b0a5f57122a811c2940ea1b9138506b7/60cc9/container-diagram.png 295w,
/static/b0a5f57122a811c2940ea1b9138506b7/6d370/container-diagram.png 590w,
/static/b0a5f57122a811c2940ea1b9138506b7/540ae/container-diagram.png 885w,
/static/b0a5f57122a811c2940ea1b9138506b7/2561a/container-diagram.png 1180w,
/static/b0a5f57122a811c2940ea1b9138506b7/f2be5/container-diagram.png 1754w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/b0a5f57122a811c2940ea1b9138506b7/6d370/container-diagram.png" alt="Container diagram" title="Container diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<ol>
<li><strong>User</strong>
<ul>
<li>Some musician that wants to use the app</li>
</ul>
</li>
<li><strong>Continuous macOS/iOS App</strong>
<ul>
<li>GUI, which users will interact with</li>
</ul>
</li>
<li><strong>LooperEngine</strong>
<ul>
<li>Audio engine, which communicates with the app through a C API</li>
</ul>
</li>
<li><strong>MIDI controllers</strong></li>
<li><strong>System Audio/MIDI APIs</strong></li>
</ol>
<p>Next, I will go over details, starting with high-level notes, then talking about
the GUI and moving on to details of specifics of the rust implementation.</p>
<h3 id="system-audiomidi-apis---api-wrappers"><a href="#system-audiomidi-apis---api-wrappers" aria-label="system audiomidi apis   api wrappers permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>System Audio/MIDI APIs - API wrappers</h3>
<p>For <code>5</code>, the system APIs are wrapped by <code>cpal</code> and <a href="https://github.com/Boddlnagg/midir/"><code>midir</code></a>
for audio and MIDI support, respectively.</p>
<p>Both of these are wrapped by the higher-level <code>AudioProcessor</code> trait
abstraction:</p>
<ul>
<li><a href="https://docs.rs/audio-processor-traits/1.0.0-alpha.5/audio_processor_traits/">audio-processor-traits</a></li>
<li><a href="https://docs.rs/audio-processor-standalone/1.0.0-alpha.9/audio_processor_standalone/">audio-processor-standalone</a></li>
<li><a href="https://docs.rs/audio-processor-standalone-midi/latest/audio_processor_standalone_midi/">audio-processor-standalone-midi</a></li>
</ul>
<p>These higher-level abstractions allow all of the looper engine (<code>3</code>) to work
drop-in in a hosted scenario, such as within a VST. They currently
use traits for <code>AudioBuffer</code> and <code>MidiMessageLike</code> so that buffers and messages
could be compatible with both rust and VST C API representations.</p>
<p>The ‘application code’ uses:</p>
<ul>
<li><a href="https://github.com/yamadapc/augmented-audio/blob/master/crates/augmented/audio/audio-processor-traits/src/lib.rs"><code>AudioProcessor</code></a></li>
<li><a href="https://github.com/yamadapc/augmented-audio/blob/master/crates/augmented/audio/audio-processor-traits/src/audio_buffer.rs"><code>AudioBuffer</code></a></li>
<li><a href="https://github.com/yamadapc/augmented-audio/blob/master/crates/augmented/audio/audio-processor-traits/src/midi.rs"><code>MidiEventHandler</code></a></li>
</ul>
<p>Then, the crates in <a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/augmented/application"><code>crates/augmented/application</code></a>
provide standalone and hosted helpers to run our processors as an online CLI,
offline file processor or VST plugin.</p>
<p>The gist is audio-buffers and MIDI messages are abstract and so are audio
processing nodes. However, I now have the impression converting into a common
type before doing audio processing could have been a better option than making
all implementations support abstract types.</p>
<h3 id="looperengineswift-integration---rust--swift-integration"><a href="#looperengineswift-integration---rust--swift-integration" aria-label="looperengineswift integration   rust  swift integration permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LooperEngine/Swift integration - Rust &amp; Swift integration</h3>
<p>Most of the <strong>Continuous Looper</strong> integration issues are in the bridge between <strong>Continuous macOS/iOS App</strong> <code>2</code>
and <strong>LooperEngine</strong> <code>3</code>.</p>
<p>Communication is done through a C bridge. C API bindings are generated with
<code>cbindgen</code>. Universal binaries are built with <code>cargo-lipo</code>.</p>
<p>The bridge is a good candidate for review, because it uses too many strategies
at the same time.</p>
<p>Data is exchanged between Rust/Swift in 4 ways:</p>
<ol>
<li><strong>Polling</strong>: GUI <strong>poll</strong>/queries for state from the audio engine
<ul>
<li><em>for example: play-head state or shared read-only references to audio buffers</em></li>
</ul>
</li>
<li><strong>Push from GUI</strong>: GUI dispatches changes to some object
<ul>
<li><em>for example: a float/int/binary/enum parameter</em></li>
</ul>
</li>
<li><strong>Dispatch from GUI</strong>: GUI dispatches a command
<ul>
<li><em>for example: “start recording” when the record button is clicked</em></li>
</ul>
</li>
<li><strong>Dispatch from engine</strong>: LooperEngine dispatches an event
<ul>
<li><em>for example: MIDI/IO events happened</em></li>
</ul>
</li>
</ol>
<p>At the moment, most data is copied in both Swift and Rust sides. These two must
sync and things would be a lot simpler if this wasn’t the case.</p>
<p>I believe this will start to be a bigger problem when persistence is added,
which allow certain more complex structures to be changed from the Rust side.
These changes will need to be pushed/queried somehow.</p>
<p>Additionally, there’s shared memory in the case of audio-buffers, where Swift
just references the same audio-buffer used in the looper. This is fine thanks to
reference counting, some safer wrappers around the C API and because the looper
buffer will be pre-allocated to a maximum capacity at the start.</p>
<h4 id="polling-for-changes"><a href="#polling-for-changes" aria-label="polling for changes permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Polling for changes</h4>
<p>For some data, the GUI poll the audio-thread every <code>1/60</code> seconds.
Though the data could be pushed from the rust side instead I think generally
the strategy of polling audio-thread state makes sense.</p>
<p><img src="https://beijaflor.io/05a3b2f9d97290248d0576012f581f23/playhead-demo.gif" alt="Play-head demo video"/></p>
<p>The play-head for example (our position within the song or loop measured in
samples/beats/seconds) updates at least every audio callback (1-10ms).
Since this may update at much higher frequency than our GUI and represents time
ticking, we can poll for it at the UI refresh rate we want.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/bdcac8455511a223cb4dcbe0d072967e/f51c2/topbar.webp 148w,
/static/bdcac8455511a223cb4dcbe0d072967e/66888/topbar.webp 295w,
/static/bdcac8455511a223cb4dcbe0d072967e/bc8a3/topbar.webp 590w,
/static/bdcac8455511a223cb4dcbe0d072967e/4ce34/topbar.webp 885w,
/static/bdcac8455511a223cb4dcbe0d072967e/f490a/topbar.webp 1180w,
/static/bdcac8455511a223cb4dcbe0d072967e/d1d4a/topbar.webp 1478w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/bdcac8455511a223cb4dcbe0d072967e/c5084/topbar.png 148w,
/static/bdcac8455511a223cb4dcbe0d072967e/60cc9/topbar.png 295w,
/static/bdcac8455511a223cb4dcbe0d072967e/6d370/topbar.png 590w,
/static/bdcac8455511a223cb4dcbe0d072967e/540ae/topbar.png 885w,
/static/bdcac8455511a223cb4dcbe0d072967e/2561a/topbar.png 1180w,
/static/bdcac8455511a223cb4dcbe0d072967e/02a0d/topbar.png 1478w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/bdcac8455511a223cb4dcbe0d072967e/6d370/topbar.png" alt="Top navigation bar screenshot" title="Top navigation bar screenshot" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>Some data can be polled at lower frequencies, such as the CPU indicator on the
top-right (<a href="#cpu-metering">there’s a section about the CPU meter later</a>).</p>
<p>One downside of SwiftUI in this context is that since our data will change on
every frame, I imagine a lot of work performed by the library to optimise
performance by avoiding invalidating a view is wasted.</p>
<h4 id="push-from-gui"><a href="#push-from-gui" aria-label="push from gui permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Push from GUI</h4>
<p>SwiftUI provides an <code>ObservableObject</code> class, which allows views to update
automatically when some properties change. This is similar to
<a href="https://mobx.js.org/README.html"><code>mobx</code></a> and really nice to use.</p>
<p>The bridging code benefits by listening to changes to these objects and pushing
them into the rust engine.</p>
<p>Pushing observable object changes out of the GUI into Rust could look something
like this:</p>
<div data-language="swift"><pre><code><span>class</span> <span>StartParameter</span><span>:</span> <span>ObservableObject</span> <span>{</span>
    <span>@Published</span> <span>var</span> value<span>:</span> <span>Double</span> <span>=</span> <span>0.0</span>
<span>}</span>

<span>struct</span> <span>StartParameterView</span><span>:</span> <span>View</span> <span>{</span>
    
    <span>@ObservedObject</span> <span>var</span> startParameter<span>:</span> <span>StartParameter</span>
    <span>var</span> body<span>:</span> <span>View</span> <span>{</span>  <span>}</span>
<span>}</span>



<span>func</span> <span>syncStart</span><span>(</span>parameter<span>:</span> <span>StartParameter</span><span>)</span> <span>-&gt;</span> <span>AnyCancellable</span> <span>{</span>
    <span>return</span> parameter<span>.</span>$value<span>.</span><span>sink</span><span>(</span>receiveValue<span>:</span> <span>{</span> value <span>in</span>
        <span>looper_engine__set_start_parameter</span><span>(</span>value<span>)</span>
    <span>}</span><span>)</span>
<span>}</span></code></pre></div>
<p>However, this is not super manageable for a larger nº of parameters, so the
solution used is moving towards a slightly more generic binding.</p>
<p>This also stops working once there’s communication in the other direction
(Rust -&gt; Swift). The users may map MIDI controllers to parameters, so at any
point a MIDI event could be changing a parameter without the GUI knowing.</p>
<p>We want the GUI to update when a MIDI controller’s knob is mapped to a parameter
and it’s moved.</p>
<p>At the moment this is addressed with polling the Rust looper state from the
GUI. There’s an <code>EntityId</code>, which all GUI controls and parameters have and all
entities* are pushed into and polled from the rust side.</p>
<p><em>*all entities that are MIDI/LFO/Step/Scene mappable (knobs and some buttons)</em></p>
<h4 id="swiftui-performance"><a href="#swiftui-performance" aria-label="swiftui performance permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SwiftUI Performance</h4>
<p>I’ve found a number of bottlenecks related to the GUI and this set-up (but I
think it is just generally really hard to build this sort of GUI and be
performant whatever the technology).</p>
<p>The <code>iced</code> GUI, though simpler and ‘inefficient’ (<code>iced</code> must re-layout and draw
everything if anything wants to update), consumed a lot less CPU. I’m afraid my
app might be too CPU intensive for some lower-end computers or iPads - this might
go into another post.</p>
<p>In particular:</p>
<ul>
<li>Setting a <code>@Published</code> property will trigger updates even if the value
doesn’t change, this is a massive CPU sink so we can’t do it
<ul>
<li>Instead, we read the property before setting it</li>
</ul>
</li>
<li>Reading the <code>@Published</code> property in an <code>ObservableObject</code> is super expensive
compared to reading a normal property
<ul>
<li><a href="https://github.com/yamadapc/augmented-audio/blob/master/crates/apps/looper/Sequencer/SequencerUI/Sources/SequencerUI/store/FastPublished.swift">Instead, I am using <code>@FastPublished</code>, a custom property wrapper that copies the value to a normal field so it’s faster to read</a></li>
</ul>
</li>
<li>Updating SwiftUI views at a high rate is a major performance bottleneck, so
the views that will update frequently (step sequencer buttons and knobs) use
AppKit instead of SwiftUI</li>
</ul>
<p>I have profiled my SwiftUI issues and found some accessibility code + forced
layout are the culprit. Setting a fixed size for the window improved things
partially, but using AppKit was the better escape hatch.</p>
<p>There is still a lot of room to improve the GUI performance.</p>
<h4 id="callbacks-from-rust-to-swift"><a href="#callbacks-from-rust-to-swift" aria-label="callbacks from rust to swift permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Callbacks from rust to swift</h4>
<p>Regarding how to call into Swift from Rust, Nick Wilcox has written great
documentation on how to achieve this here:</p>
<ul>
<li><a href="https://www.nickwilcox.com/blog/recipe_swift_rust_callback/">https://www.nickwilcox.com/blog/recipe_swift_rust_callback/</a></li>
</ul>
<p>That is pretty much the set-up used. You can check the source code <a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/apps/looper/looper-processor/src/c_api">here</a>
and <a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/apps/looper/Sequencer/SequencerEngine/foreign_callbacks">here</a>.</p>
<h3 id="swift-frameworks-packages-and-apps"><a href="#swift-frameworks-packages-and-apps" aria-label="swift frameworks packages and apps permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Swift frameworks, packages and apps</h3>
<p>The following is a diagram of some views / components in the Swift GUI side.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/63a5584460653d09a58d83a644febead/f51c2/swift-ui-components.webp 148w,
/static/63a5584460653d09a58d83a644febead/66888/swift-ui-components.webp 295w,
/static/63a5584460653d09a58d83a644febead/bc8a3/swift-ui-components.webp 590w,
/static/63a5584460653d09a58d83a644febead/4ce34/swift-ui-components.webp 885w,
/static/63a5584460653d09a58d83a644febead/f490a/swift-ui-components.webp 1180w,
/static/63a5584460653d09a58d83a644febead/3ac28/swift-ui-components.webp 4870w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/63a5584460653d09a58d83a644febead/c5084/swift-ui-components.png 148w,
/static/63a5584460653d09a58d83a644febead/60cc9/swift-ui-components.png 295w,
/static/63a5584460653d09a58d83a644febead/6d370/swift-ui-components.png 590w,
/static/63a5584460653d09a58d83a644febead/540ae/swift-ui-components.png 885w,
/static/63a5584460653d09a58d83a644febead/2561a/swift-ui-components.png 1180w,
/static/63a5584460653d09a58d83a644febead/d2538/swift-ui-components.png 4870w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/63a5584460653d09a58d83a644febead/6d370/swift-ui-components.png" alt="SwiftUI component diagram" title="SwiftUI component diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>Just some tiny things worth mentioning.</p>
<p><code>Store</code>: We hold all state under a ‘store’ object which makes it a bit easier
to pass it around or mock it out.</p>
<p><code>SwiftSequencerEngine</code>: There’s no code calling into native Rust directly in the
GUI itself. There are multiple packages in the XCode project:</p>
<ul>
<li>the GUI exists in a cross-platform Swift package <code>SequencerUI</code></li>
<li>the rust library is linked against a separate <code>framework</code></li>
</ul>
<p>The reason for this is to be able to support multiple platforms (iOS/macOS).
Also, having a simple <code>SequencerUI</code> swift package that doesn’t link to anything
fancy allows normal XCode tooling such as previews or Swift specific unit-tests
to work normally and faster.</p>
<p><code>SequencerUI</code> declares a <code>SequencerEngine</code> protocol/interface, which the
<code>SequencerEngine.framework</code> frameworks for macOS/iOS (which are exactly the same
source, but different build configurations) implement in <code>SequencerEngineImpl</code>.</p>
<p>As mentioned, most data is copied from the engine into the <code>Store</code> objects,
except for audio buffers which are shared.</p>
<p>For audio buffers, another protocol is declared in <code>SequencerUI</code> and the engine
implements this protocol for some C <code>struct</code> pointer wrappers, which are
essentially wrappers on top of reference counted <code>AudioBuffer</code> rust objects.</p>
<p><code>SequencerUI</code> and <code>SequencerEngine.framework</code> are then consumed by the
application projects themselves for iOS/macOS.</p>
<p>I have not released the iOS app, because there are currently issues with certain
IO channel configurations; but the app runs (with Rust!) fine on iOS and it
is going to be great.</p>
<h3 id="rust-components"><a href="#rust-components" aria-label="rust components permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rust components</h3>
<p>All the audio processing code I’m writing is following a “shared handle”
pattern:</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/df53782c0a9f200a401205bacd167ebb/f51c2/example-audio-processor.webp 148w,
/static/df53782c0a9f200a401205bacd167ebb/66888/example-audio-processor.webp 295w,
/static/df53782c0a9f200a401205bacd167ebb/bc8a3/example-audio-processor.webp 590w,
/static/df53782c0a9f200a401205bacd167ebb/4ce34/example-audio-processor.webp 885w,
/static/df53782c0a9f200a401205bacd167ebb/f490a/example-audio-processor.webp 1180w,
/static/df53782c0a9f200a401205bacd167ebb/a311b/example-audio-processor.webp 1858w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/df53782c0a9f200a401205bacd167ebb/c5084/example-audio-processor.png 148w,
/static/df53782c0a9f200a401205bacd167ebb/60cc9/example-audio-processor.png 295w,
/static/df53782c0a9f200a401205bacd167ebb/6d370/example-audio-processor.png 590w,
/static/df53782c0a9f200a401205bacd167ebb/540ae/example-audio-processor.png 885w,
/static/df53782c0a9f200a401205bacd167ebb/2561a/example-audio-processor.png 1180w,
/static/df53782c0a9f200a401205bacd167ebb/bf7a7/example-audio-processor.png 1858w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/df53782c0a9f200a401205bacd167ebb/6d370/example-audio-processor.png" alt="Example audio processor diagram" title="Example audio processor diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<ul>
<li>Every audio processor is a struct with a “process buffer” method</li>
<li>This method takes a mutable reference to the processor</li>
<li>The audio-thread owns the processor</li>
<li>Any state that must be changed from other threads is done via a handle
<ul>
<li>A handle is a reference counted shared thread-safe struct (using atomics)
that both the processor and GUI can read/write from</li>
</ul>
</li>
</ul>
<p>In code, this looks like this:</p>
<div data-language="rust"><pre><code><span>let</span> processor <span>=</span> <span>MultiTrackLooper</span><span>::</span><span>default</span><span>(</span><span>)</span><span>;</span>
<span>let</span> processor_handle<span>:</span> <span>Shared</span><span>&lt;</span><span>MultiTrackLooperHandle</span><span>&gt;</span> <span>=</span>
        processor<span>.</span><span>handle</span><span>(</span><span>)</span><span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>


<span>let</span> _audio_handles <span>=</span> <span>audio_processor_start_with_midi</span><span>(</span>
    processor<span>,</span>
    <span>audio_garbage_collector<span>::</span></span><span>handle</span><span>(</span><span>)</span><span>,</span>
<span>)</span><span>;</span>


processor_handle
    <span>.</span><span>set_source_parameter</span><span>(</span><span>LooperId</span><span>(</span><span>0</span><span>)</span><span>,</span> <span>SourceParameter</span><span>::</span><span>Speed</span><span>,</span> <span>2.0</span><span>)</span><span>;</span>
</code></pre></div>
<p>This introduces indirection to access anything on the handle (which is all
processor state a lot of the time). I haven’t measured the performance impact in
a larger example such as the looper.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/5e7cb43cb9ba64114fefdca0f54986e8/f51c2/looper-engine-diagram.webp 148w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/66888/looper-engine-diagram.webp 295w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/bc8a3/looper-engine-diagram.webp 590w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/4ce34/looper-engine-diagram.webp 885w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/f490a/looper-engine-diagram.webp 1180w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/40086/looper-engine-diagram.webp 2516w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/5e7cb43cb9ba64114fefdca0f54986e8/c5084/looper-engine-diagram.png 148w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/60cc9/looper-engine-diagram.png 295w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/6d370/looper-engine-diagram.png 590w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/540ae/looper-engine-diagram.png 885w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/2561a/looper-engine-diagram.png 1180w,
/static/5e7cb43cb9ba64114fefdca0f54986e8/1a2d7/looper-engine-diagram.png 2516w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/5e7cb43cb9ba64114fefdca0f54986e8/6d370/looper-engine-diagram.png" alt="Looper engine diagram" title="Looper engine diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>That’s how the looper is structured: there is a <code>LooperEngine</code> struct created
by the Swift GUI that starts a couple of different background workers,
among which is the <code>audio_processor_standalone</code> instance.
<code>audio_processor_standalone</code> is the audio/MIDI threads.</p>
<p>The audio/MIDI threads own the processor and reference shared state
the GUI can read and update (through special APIs).</p>
<p>MIDI events might arrive in a different thread and are pushed via a lock-free
queue to the audio-thread. I think we could handle MIDI events out of the
audio-thread, but that’s how things are done right now.</p>
<p><code>MultiTrackLooper</code> and <code>MultiTrackLooperHandle</code> are where all the magic happens,
which leads us to…</p>
<h2 id="rust-and-audio"><a href="#rust-and-audio" aria-label="rust and audio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rust and audio</h2>
<h3 id="multi-track-looper-audio-graph"><a href="#multi-track-looper-audio-graph" aria-label="multi track looper audio graph permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-track looper audio graph</h3>
<p><code>MultiTrackLooper</code> processing has the following topology:</p>
<p><img alt="Multi-track looper audio graph" src="https://mermaid.ink/img/pako:eNptkkFrwyAYhv-KfOcm0BxzGBSade062q07bCQ9SDSLrNFgdDAk_30Gl2hCPYg-76s8oAZKQSik8CVxW6P3bcGRHRuz561WvduF8yaKHrL8KERLJVpfHUQW7vIzU2UddTWr1CJ7zDP-Q2_2zAw_5VlV0VJ1M_phdphxtO492puTVs7n32KgL6NGckW--7zwSIKrD94jxMfJI6SfziO57zFZnEaLOI5Dj_PCY4h9-upN5sHb5DLnF2dj4T0fWEFDZYMZsU9phkIBqqYNLSC1S4LldwEFH3q6JVjRjDAlJKQVvnV0BVgrcfnlJaRKajqWtgzbb9E42P8BFBij3g"/>
</p>
<p>Each track has, in series, a single-track looper, pitch-shifter, ADSR envelope,
effects (which are not in the published version of the app, as they have some
issues) and gain. This graph of processors is built using <code>audio-processor-graph</code>
which is a partial audio graph implementation in the repository.</p>
<p>The looper is by far the largest processor in this chain, since a lot of the
other nodes are very simple. Currently, the effects node is a sub-graph.</p>
<h3 id="looper-states"><a href="#looper-states" aria-label="looper states permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Looper states</h3>
<p>The looper can be in different states:</p>
<ul>
<li>Empty</li>
<li>Recording</li>
<li>Playing</li>
<li>Paused</li>
</ul>
<p>Three additional states are used to represent overdubbing and the cases where
playing/recording are scheduled.</p>
<p><img alt="Looper state diagram" src="https://mermaid.ink/img/pako:eNp9kT0PgjAQhv9Kc6OBxbGDk-xGRutwoScQaTH9MCGE_27lQxGCN7333Nu3uVwLWS0JOFiHjo4l5gZV_NwLzUJddlcWxweWqIdrBtTLHp4pq40sdT4MPm2aFSR9RZJ_GbMT3ExJfx1r_vfTfniqsNkaobdT9Ghb816vk8ZmtteUsNhqHbBYaknnVjYroSECRUZhKcNt2vdrAa4gRQJ4kBLNXYDQXfD5hwynS2TpagP8hpWlCNC7Om10BtwZT5NpvO_o6l6rmKb6"/>
</p>
<p>Each of these needs special treatment of both the actions (what transition to
trigger on interaction) and more importantly on how they render/handle the audio
callback.</p>
<h3 id="always-recording-buffer"><a href="#always-recording-buffer" aria-label="always recording buffer permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Always recording buffer</h3>
<p>We want to support “snap closest” behaviour:</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/1c99ae9211b522df4aee9a3a4d3093de/f51c2/snap-closest.webp 148w,
/static/1c99ae9211b522df4aee9a3a4d3093de/66888/snap-closest.webp 295w,
/static/1c99ae9211b522df4aee9a3a4d3093de/bc8a3/snap-closest.webp 590w,
/static/1c99ae9211b522df4aee9a3a4d3093de/4ce34/snap-closest.webp 885w,
/static/1c99ae9211b522df4aee9a3a4d3093de/f490a/snap-closest.webp 1180w,
/static/1c99ae9211b522df4aee9a3a4d3093de/08048/snap-closest.webp 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/1c99ae9211b522df4aee9a3a4d3093de/c5084/snap-closest.png 148w,
/static/1c99ae9211b522df4aee9a3a4d3093de/60cc9/snap-closest.png 295w,
/static/1c99ae9211b522df4aee9a3a4d3093de/6d370/snap-closest.png 590w,
/static/1c99ae9211b522df4aee9a3a4d3093de/540ae/snap-closest.png 885w,
/static/1c99ae9211b522df4aee9a3a4d3093de/2561a/snap-closest.png 1180w,
/static/1c99ae9211b522df4aee9a3a4d3093de/3643c/snap-closest.png 1400w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/1c99ae9211b522df4aee9a3a4d3093de/6d370/snap-closest.png" alt="Snap closest quantization diagram" title="Snap closest quantization diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>And, in fact, we also want to support this when <strong>starting</strong> to record/play.</p>
<p>In this case, the looper will receive a start recording command after it was
supposed to have started.</p>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/ec9a9e27ea4858da5fecc78307304d22/f51c2/snap-next-2.webp 148w,
/static/ec9a9e27ea4858da5fecc78307304d22/66888/snap-next-2.webp 295w,
/static/ec9a9e27ea4858da5fecc78307304d22/bc8a3/snap-next-2.webp 590w,
/static/ec9a9e27ea4858da5fecc78307304d22/4ce34/snap-next-2.webp 885w,
/static/ec9a9e27ea4858da5fecc78307304d22/9b20f/snap-next-2.webp 904w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcset="/static/ec9a9e27ea4858da5fecc78307304d22/c5084/snap-next-2.png 148w,
/static/ec9a9e27ea4858da5fecc78307304d22/60cc9/snap-next-2.png 295w,
/static/ec9a9e27ea4858da5fecc78307304d22/6d370/snap-next-2.png 590w,
/static/ec9a9e27ea4858da5fecc78307304d22/540ae/snap-next-2.png 885w,
/static/ec9a9e27ea4858da5fecc78307304d22/91af2/snap-next-2.png 904w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img src="https://beijaflor.io/static/ec9a9e27ea4858da5fecc78307304d22/6d370/snap-next-2.png" alt="Snap next quantization diagram" title="Snap next quantization diagram" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p>For this, we need to have an always recording buffer (the <code>ScratchPad</code>). The
scratch-pad will be a circular buffer for the last few seconds which will always
be recording.</p>
<p><img src="https://beijaflor.io/768c123ef6683189bd84ed51423630df/scratch-pad4.gif" alt="scratch-pad animated diagram"/></p>
<p>Then, when recording is triggered, we can either:</p>
<ul>
<li>store what our “write cursor” position within the scratch-pad is</li>
<li>or copy the last few seconds into another buffer</li>
</ul>
<p>Currently, <strong>Continuous Looper</strong> has a pretty small 10s limit on loop length, so it
uses the scratch-pad to record and copies to another buffer only when the
recording is ended.</p>
<h3 id="quantization-implementation"><a href="#quantization-implementation" aria-label="quantization implementation permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quantization implementation</h3>
<h4 id="keeping-track-of-time-play-head"><a href="#keeping-track-of-time-play-head" aria-label="keeping track of time play head permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Keeping track of time: Play-head</h4>
<p>I didn’t know of <a href="https://www.youtube.com/watch?v=kwnjF4U8_I0&amp;t=1893s">Vlad Voina’s “Loopers and bloopers” talk</a>
when I implemented this, but gives some ideas of things I could be fixing next.</p>
<p>In order to quantize we first need to be able to keep track of the play-head
position, as represented in previous tempo diagrams.</p>
<p>The play-head is our position within the song. This should be a value we
can convert into/from samples, beats and seconds and that represents the
timestamp within the song of the play-head (the playback).</p>
<p>In <code>augmented-audio</code>, I’ve implemented a basic hosted <code>PlayHead</code> data structure
in <a href="https://github.com/yamadapc/augmented-audio/blob/843f78265a464a7f95ed052784994513f25accfd/crates/augmented/data/augmented-playhead/src/lib.rs"><code>augmented_playhead</code></a>.</p>
<p>The gist is that we keep track of the current <code>sample</code> and then calculate our
position in seconds/beats based on the tempo and sample-rate:</p>
<div data-language="rust"><pre><code><span>let</span> elapsed_samples <span>=</span> <span>get_elapsed_samples_from_somewhere_pass_it_in</span><span>(</span><span>)</span><span>;</span>
<span>let</span> elapsed_secs <span>=</span> <span>(</span><span>1.0</span> <span>/</span> sample_rate<span>)</span> <span>*</span> <span>(</span>num_samples <span>as</span> <span>f64</span><span>)</span><span>;</span>
<span>let</span> beats_per_second <span>=</span> tempo_in_bpm <span>as</span> <span>f64</span> <span>/</span> <span>60.0</span><span>;</span>

<span>let</span> elapsed_beats <span>=</span> beats_per_second <span>*</span> elapsed_secs<span>;</span></code></pre></div>
<p>All units can be converted to one another. One important thing to do is to use
double precision, because there will be serious inaccuracies if <code>f32</code> is used
after a certain song length.</p>
<p>Vlad Voina’s talk linked above mentions the size of the <code>elapsed_samples</code> cursor,
as just incrementing it will eventually overflow. Generally it’s not an issue
for this use-case.</p>
<p>In order to tempo-sync with a VST host, the <strong>Continuous Looper</strong> engine is using a
<code>TimeInfoProvider</code> abstraction, which wraps both a ‘stand-alone’ play-head and a
‘hosted’ implementation. <a href="https://github.com/yamadapc/augmented-audio/blob/master/crates/apps/looper/looper-processor/src/audio/time_info_provider/mod.rs#L95-L103">The source code is here</a>.</p>
<p>The idea is we will tick and control the tempo when running in an app, but when
running inside of a VST we will callback into the host via the VST API to get the
current play-head. I will extract this out of the looper application for re-use
later.</p>
<h4 id="implementing-quantization"><a href="#implementing-quantization" aria-label="implementing quantization permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementing quantization</h4>
<p>Once we know the play-head position, quantization is some calculations, which
depend on the time-signature, our position in beats and (for snap closest) a
mistake correction threshold.</p>
<div data-language="rust"><pre><code>


<span>fn</span> <span>snap_next_beat</span><span>(</span>quantization_beats<span>:</span> <span>usize</span><span>,</span> position_beats<span>:</span> <span>f32</span><span>)</span> <span>-&gt;</span> <span>f32</span> <span>{</span>
    <span>let</span> f_beats <span>=</span> quantization_beats <span>as</span> <span>f32</span><span>;</span>
    <span>(</span>position_beats <span>/</span> f_beats<span>)</span><span>.</span><span>ceil</span><span>(</span><span>)</span> <span>*</span> f_beats
<span>}</span>


<span>fn</span> <span>snap_closest_beat</span><span>(</span>
    quantization_beats<span>:</span> <span>usize</span><span>,</span>
    tempo<span>:</span> <span>f32</span><span>,</span>
    threshold_ms<span>:</span> <span>f32</span><span>,</span>
    position_beats<span>:</span> <span>f32</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>f32</span> <span>{</span>
    <span>let</span> beats_per_ms <span>=</span> tempo <span>/</span> <span>60_000.0</span><span>;</span>
    <span>let</span> threshold_beats <span>=</span> beats_per_ms <span>*</span> threshold_ms<span>;</span>

    <span>let</span> f_beats <span>=</span> quantization_beats <span>as</span> <span>f32</span><span>;</span>
    <span>let</span> ratio <span>=</span> position_beats <span>/</span> f_beats<span>;</span>
    <span>let</span> lower <span>=</span> ratio<span>.</span><span>floor</span><span>(</span><span>)</span> <span>*</span> f_beats<span>;</span>
    <span>let</span> upper <span>=</span> ratio<span>.</span><span>ceil</span><span>(</span><span>)</span> <span>*</span> f_beats<span>;</span>

    <span>#[allow(clippy::float_equality_without_abs)]</span>
    <span>if</span> <span>(</span><span>(</span>lower <span>-</span> position_beats<span>)</span><span>.</span><span>abs</span><span>(</span><span>)</span> <span>-</span> threshold_beats<span>)</span> <span>&lt;</span> <span>f32</span><span>::</span><span>EPSILON</span> <span>{</span>
        lower
    <span>}</span> <span>else</span> <span>{</span>
        upper
    <span>}</span>
<span>}</span></code></pre></div>
<p>Since we have our scratch-pad, the looper can handle negative quantization
offsets. I imagine similar quantization logic would work for other applications.</p>
<h3 id="testing-audio"><a href="#testing-audio" aria-label="testing audio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Testing audio</h3>
<p>In order to test audio processors I am trying a couple of approaches.</p>
<p>Again, I’m not an audio developer (and there’s a lot to fix in the repo!), but
maybe it can be useful.</p>
<h4 id="normal-unit-testing-with-mock-buffers"><a href="#normal-unit-testing-with-mock-buffers" aria-label="normal unit testing with mock buffers permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Normal unit-testing with mock buffers</h4>
<p>For looper logic which just records and reproduces samples, we can unit-test
using small made-up (<code>1,2,3</code>) buffers and check that the output makes sense after
certain operations.</p>
<p>This is regular unit-testing as on any domain.</p>
<h4 id="property-based-testing"><a href="#property-based-testing" aria-label="property based testing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Property based testing</h4>
<p>On some small pieces, I’ve experimented with tests such as “feed a sine-wave
into this DSP code and check that the output has the same RMS as the input”.</p>
<p>I imagine with more analysis properties this would be more meaningful.</p>
<p>I have drafted some analysis code on <a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/augmented/audio/audio-processor-analysis"><code>audio-processor-analysis</code></a>,
but there isn’t a lot of usage in tests and I’d need to experiment more.</p>
<h4 id="visualization"><a href="#visualization" aria-label="visualization permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Visualization</h4>
<p>This isn’t a proper test, but very often, it’s helpful to visualise what the
signal is doing, so I used <code>plotters</code> and sometimes <code>piet</code> to generate charts.</p>
<p>The analysis crate above is a good example of doing this, but there are several
in the repository.</p>
<p><code>audio-processor-testing-helpers</code> exposes a function to make it dead simple to
plot a vector of floats and have a look at the signal:</p>
<div data-language="rust"><pre><code>
<span>let</span> pth <span>=</span> <span>relative_path!</span><span>(</span><span>&#34;src/lib.rs&#34;</span><span>)</span><span>;</span>

<span>draw_vec_chart</span><span>(</span>pth<span>,</span> <span>&#34;chart_name&#34;</span><span>,</span> my_vector<span>)</span><span>;</span></code></pre></div>
<h4 id="snapshot-testing"><a href="#snapshot-testing" aria-label="snapshot testing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Snapshot testing</h4>
<p>Since we get free CLIs from our abstraction code, the repository has a
couple of snapshot tests which run in CI and are documented in <a href="https://github.com/yamadapc/augmented-audio/blob/master/docs/monorepo-tooling/SNAPSHOT_TESTING.md">SNAPSHOT_TESTING.md</a>.</p>
<p>This is just rendering some audio through each effect and checking hashes match
what had been committed.</p>
<p>Maybe comparing properties or visualisations here would be more meaningful and
less brittle.</p>
<p>However, this approach and the others increase my level of confidence I’m not
breaking things when making changes.</p>
<h3 id="testing-allocations"><a href="#testing-allocations" aria-label="testing allocations permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Testing allocations</h3>
<p>All audio-thread code is trying to avoid (de)allocations, which increases
complexity quite a lot. This is very hard to avoid in the Rust ecosystem with
many 3rd party libraries that provide lock-free or immutable data-structures
allocating on reads.</p>
<p>In order to unit-test that allocations aren’t being made the <a href="https://docs.rs/assert_no_alloc"><code>assert_no_alloc</code> crate</a>
is of great help. It replaces the global allocator so that sections of the source
can disable allocations (causing the program to print a warning or crash).</p>
<p>On unit-tests this can be used to test sections are allocation free.</p>
<h3 id="sequencer-implementation"><a href="#sequencer-implementation" aria-label="sequencer implementation permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sequencer implementation</h3>
<p>My sequencer implementation is based on detecting when the play-head has
changed from one beat to another. A “step-tracker” struct keeps track of the
last position in beats. When a new position is received, it checks if there was
a trigger.</p>
<div data-language="rust"><pre><code><span>let</span> prev_position <span>=</span> <span>0.9</span><span>;</span> 
<span>let</span> curr_position <span>=</span> <span>1.1</span><span>;</span> 
</code></pre></div>
<p>The active steps, along with parameter lock information are kept on a shared
vector pointer.</p>
<p>When the GUI wants to add a step, it’ll copy the vector, make changes and then
atomically swap the old pointer with the new. This is done using
<code>basedrop::SharedCell</code>.</p>
<p>When both threads are done with the old data it’ll be released on a background
thread.</p>
<p>I wanted to use immutable data-structures for this case, but there should be
negligible gain at this point because the sequence can only be 16 steps.</p>
<h3 id="parameter-storage---lfos-parameter-locks-sequencing"><a href="#parameter-storage---lfos-parameter-locks-sequencing" aria-label="parameter storage   lfos parameter locks sequencing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Parameter storage - LFOs, parameter locks, sequencing</h3>
<p>Parameters, LFO mapping and parameter locks work in a similar way.</p>
<p>There’s a “ParametersMap” struct, which is an array of all parameters and their
values, combined with a HashMap that locates a typed (enum) parameter ID into
its position.</p>
<p>These positions and array are pre-calculated because we know all possible
parameters ahead of time. This likely won’t work moving forward.</p>
<p>For each step and scene there’s a similar map. The steps only hold parameters
that have been locked.</p>
<p>On each audio callback we apply the parameter locks, scenes and so on.</p>
<p>We have a scratch buffer of parameters, onto which we load ‘user parameters’ the
user has set through the GUI. Next, we override these with the scenes (performing
linear interpolation according to the slider position). Third, we apply parameter
locks if there’s an active step with them. Forth, we apply LFO modulation.</p>
<p>Finally, all parameters are “flushed” from the scratch parameters buffer to
their corresponding handles (currently this is just the looper handle).
Once there are effects and more mappable processors they would be flushed to
each of these.</p>
<p>This scratch stage is necessary because scenes, locks and modulation introduce
a mismatch between what the user has set on the GUI (the ‘user value’) and what
the value should be at <em>this current point in time</em>.</p>
<p>I think this has room for improvement because:</p>
<ul>
<li>The current approach uses a <code>HashMap</code> for a pre-determined set of items
<ul>
<li>We could be using just an index and a Vec</li>
</ul>
</li>
<li>We have to manually flush parameters from this storage into the handles,
which centralises a lot of mapping code in the main <code>MultiTrackLooper</code>
processor</li>
</ul>
<hr/>
<h2 id="conclusion"><a href="#conclusion" aria-label="conclusion permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Thank you for reading my post!</p>
<p>I really hope you got something out of it.</p>
<p>I will continue to develop <strong>Continuous Looper</strong> and libraries in <code>augmented-audio</code> with
goal #1 being to learn and have fun. (<a href="https://github.com/yamadapc/augmented-audio/tree/master/crates/apps/looper/Sequencer">augmented-audio/crates/apps/looper/Sequencer</a>)</p>
<p>If you have feedback, please reach out and I will be super happy to chat.</p>
<p>All the best, <a href="https://twitter.com/yamadapc">@yamadapc</a>.</p>
<ul>
<li><a href="https://www.reddit.com/r/rust/comments/u1xhvc/continuous_looper_deep_dive_into_an_opensource/">Reddit post</a></li>
</ul>
<h2 id="other-links"><a href="#other-links" aria-label="other links permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Other links</h2>
<ul>
<li><a href="https://github.com/RustAudio/">RustAudio</a>
<ul>
<li><a href="https://github.com/RustAudio/cpal">cpal</a></li>
</ul>
</li>
<li><a href="https://www.researchgate.net/publication/220723752_A_Transient_Detection_Algorithm_for_Audio_Using_Iterative_Analysis_of_STFT">A Transient Detection Algorithm for Audio Using Iterative Analysis of STFT</a></li>
<li><a href="https://www.amazon.com/Audio-Effects-Theory-Implementation-Application/dp/1466560282">Audio Effects: Theory, Implementation and Application 1st Edition</a></li>
<li><a href="https://www.youtube.com/watch?v=aVLRUyPBBJk">Real-time Audio Programming with Bela</a></li>
<li><a href="https://www.youtube.com/watch?v=kwnjF4U8_I0&amp;t=1893s">Vlad Voina - Loopers and bloopers</a></li>
</ul></section></div>
  </body>
</html>
