<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks">Original</a>
    <h1>GPT-4 and professional benchmarks: the wrong answer to the wrong question</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>OpenAI didn’t release much information about GPT-4 — not even the size of the model — but </span><a href="https://cdn.openai.com/papers/gpt-4.pdf" rel="">heavily emphasized</a><span> its performance on professional licensing exams and other standardized tests. For instance, GPT-4 reportedly scored in the 90th percentile on the bar exam. So there’s been much speculation about what this means for professionals such as lawyers.</span></p><p>We don’t know the answer, but we hope to inject some reality into the conversation. OpenAI may have violated the cardinal rule of machine learning: don’t test on your training data. Setting that aside, there’s a bigger problem. The manner in which language models solve problems is different from how people do it, so these results tell us very little about how a bot will do when confronted with the real-life problems that professionals face. It’s not like a lawyer’s job is to answer bar exam questions all day.</p><p><strong>Problem 1: training data contamination</strong></p><p><span>To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 </span><a href="https://twitter.com/cHHillee/status/1635790330854526981" rel="">solved</a><span> 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.</span></p><p>As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.</p><p><span>In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 </span><a href="https://simonwillison.net/2023/Mar/10/chatgpt-internet-access/" rel="">cannot access the Internet</a><span>, so memorization is the only explanation.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png" width="1456" height="379" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:379,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a115f56-76e9-480a-8ee5-c95bb808e9da_1600x417.png 1456w" sizes="100vw"/></picture></div></a><figcaption><em>GPT-4 memorizes Codeforces problems before its training cutoff date.</em></figcaption></figure></div><p><span>The Codeforces results in the paper aren’t affected by this, as OpenAI used recent problems (and sure enough, GPT-4 performed very poorly). For the benchmarks other than coding, we don’t know of a clean way to separate the questions by time period, so we think it is unlikely that OpenAI was able to avoid contamination.</span></p><p><span> But for the same reason, we can’t do an experiment to test how performance varies by date.</span></p><p><span>Still, we can look for telltale signs. Another symptom of memorization is that GPT is highly sensitive to the phrasing of the question. Melanie Mitchell gives an </span><a href="https://aiguide.substack.com/p/did-chatgpt-really-pass-graduate" rel="">example</a><span> of an MBA test question where changing some details in a way that wouldn’t fool a person is enough to fool ChatGPT (running GPT-3.5). A more elaborate experiment along these lines would be valuable.</span></p><p>Because of OpenAI’s lack of transparency, we can’t answer the contamination question with certainty. But what’s certain is that OpenAI’s method to detect contamination is superficial and sloppy:</p><blockquote><p>“We measure cross-contamination between our evaluation dataset and the pre-training data using substring match. Both evaluation and training data are processed by removing all spaces and symbols, keeping only characters (including numbers). For each evaluation example, we randomly select three substrings of 50 characters (or use the entire example if it’s less than 50 characters). A match is identified if any of the three sampled evaluation substrings is a substring of the processed training example. This yields a list of contaminated examples. We discard these and rerun to get uncontaminated scores.”</p></blockquote><p><span>This is a brittle method. If a test problem were present in the training set with names and numbers changed, it wouldn’t be detected. Less flaky methods are readily available, such as </span><a href="https://twitter.com/mmitchell_ai/status/1637533337026981888" rel="">embedding distances</a><span>.</span></p><p>If OpenAI were to use a distance-based method, how similar is too similar? There is no objective answer to this question. So even something as seemingly straightforward as performance on a multiple-choice standardized test is fraught with subjective decisions. </p><p><span>But we can get some clarity by asking what OpenAI is trying to measure using these exams. If the goal is to predict how the language model will do on real-world tasks, there’s a problem. In a sense, any two bar exam questions or medical exam questions are more similar to each other than they are to the tasks that professionals do in the real world, because they are drawn from such a constrained space. So it’s possible that the inclusion of </span><em>any</em><span> exam questions in the training corpus results in an inflated estimate of real-world usefulness.</span></p><p>Framing the question in terms of real-world usefulness highlights another, deeper problem.</p><p><strong>Problem 2: professional exams aren’t a valid way to compare human capabilities with bots</strong></p><p>Memorization is a spectrum. Even if a language model hasn’t seen an exact problem on a training set, it has inevitably seen examples that are pretty close, simply because of the size of the training corpus. That means it can get away with a much shallower level of reasoning. So the benchmark results don’t give us evidence that language models are acquiring the kind of in-depth reasoning skills that human test-takers need — skills that they then apply in the real world. </p><p><span>In some real-world tasks, shallow reasoning may be sufficient, but not always. The world is constantly changing, so if a bot is asked to analyze the legal consequences of a new technology or a new judicial decision, it doesn’t have much to draw upon. In short, as Emily Bender points out, tests designed for humans lack </span><a href="https://twitter.com/emilymbender/status/1636090914346274816" rel="">construct validity</a><span> when applied to bots.</span></p><p><span>On top of this, professional exams, especially the bar exam, notoriously </span><a href="https://scholarship.law.pitt.edu/cgi/viewcontent.cgi?article=1064&amp;context=fac_articles" rel="">overemphasize</a><span> subject-matter knowledge and underemphasize real-world skills, which are far harder to measure in a standardized, computer-administered way. In other words, not only do these exams emphasize the wrong thing, they overemphasize precisely the thing that language models are good at.</span></p><p><span>Benchmarks are already wildly overused in AI for comparing different models. They have been </span><a href="https://twitter.com/ChristophMolnar/status/1485549716268109824?s=20" rel="">heavily</a><span> </span><a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf" rel="">criticized</a><span> for collapsing a multidimensional evaluation into a single number. When used as a way to compare humans and bots, what results is misinformation. It is unfortunate that OpenAI chose to use these types of tests so heavily in their evaluation of GPT-4, coupled with inadequate attempts to address contamination.</span></p><p><strong>There are better ways to assess impact on professions</strong></p><p>People have Internet access during their jobs, but not during standardized tests. So, if language models can match the performance of professionals who have Internet access, it would be a somewhat better test of whether they are practically useful.</p><p><span>But that’s still the wrong question. Instead of standalone benchmarks, we should measure how well language models can accomplish any of the real-world tasks that professionals must do. For example, in academia, we often encounter papers filled with jargon from fields we’re not familiar with; it would be useful if ChatGPT could accurately summarize such a paper in a more accessible way. Some people have even been anecdotally testing if these tools can do peer review. But even here, it’s easy to fall into the </span><a href="https://twitter.com/random_walker/status/1637507156659548160" rel="">trap</a><span> of testing on the training data.</span></p><p><span>Studies that frame the question as a comparison between a professional and a bot are misguided: the idea that ChatGPT can replace professionals remains far-fetched. Just one of the 270 jobs in the 1950 census has been </span><a href="https://twitter.com/emollick/status/1635787047502852097" rel="">eliminated</a><span> by automation: elevator operator. Instead, we need studies that actually evaluate professionals using the help of AI tools to do their jobs. </span><a href="https://twitter.com/emollick/status/1631397931604488194" rel="">Two early studies</a><span> are promising: one looks at GitHub copilot for coding and the other looks at ChatGPT for writing assistance. </span></p><p><span>At this stage, we need qualitative studies more than quantitative ones, because these tools are so new that we don’t even know the right quantitative questions to ask. For example, Scott Guthrie of Microsoft reports the eye-catching number that </span><a href="https://www.microsoft.com/en-us/Investor/events/FY-2023/Morgan-Stanley-TMT-Conference" rel="">40%</a><span> of the code checked in by GitHub Copilot users is AI-generated and unmodified. But any programmer will tell you that especially in enterprise applications, a large percentage of code consists of templates and other mundane logic that we usually copy-paste. If this is the part that Copilot is automating, the productivity improvement would be negligible. To be clear, we’re not saying that Copilot is useless, just that metrics are meaningless without a qualitative understanding of how professionals use AI. Besides, the </span><a href="https://twitter.com/random_walker/status/1636050310019022848" rel="">primary benefit</a><span> of AI-assisted coding might not even be the productivity improvement.</span></p><p><strong>Summary</strong></p><p>The figure summarizes this post and explains why and how we need to move away from the kind of metrics reported by OpenAI.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png" width="1456" height="748" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ea565520-6154-4e04-8fed-49982accbf0e_1600x822.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:748,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea565520-6154-4e04-8fed-49982accbf0e_1600x822.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>GPT-4 is genuinely exciting and there are many ways in which it can solve pain points for professionals: for example, by </span><a href="https://twitter.com/random_walker/status/1636039756864712706" rel="">automating</a><span> mundane and low-stakes yet laborious tasks. For now, it might be better to focus on achieving such benefits and on mitigating the many risks of language models.</span></p><p><strong>Notes and further reading</strong></p><ul><li><p><a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000205" rel="">ChatGPT passing USMLE shines a spotlight on the flaws of medical education</a><span>, a paper by Amarachi B. Mbakwe and others. </span></p></li><li><p><span>Qualitative studies are important across ML modeling stages, not just during evaluation. Orestis Papakyriakopoulos and others </span><a href="https://arxiv.org/pdf/2112.03784.pdf" rel="">argue</a><span> that qualitative methods can uncover areas of improvement in problem formulation, data collection, and translating ML models to the real world. But they are still overshadowed by quantitative methods in the ML community.</span></p></li><li><p><span>In our work on </span><a href="https://reproducible.cs.princeton.edu" rel="">leakage</a><span> in scientific research that uses ML, we found that it can be notoriously hard to avoid leakage in settings of scientific interest. Leakage (which is roughly the same as contamination in this post) is more of an open research problem than a bug that can be easily fixed.</span></p></li><li><p><span>To benchmark language models across multiple metrics, Percy Liang and others introduced </span><a href="https://arxiv.org/abs/2211.09110" rel="">HELM</a><span> (holistic evaluation of language models). HELM measures LLMs on metrics such as accuracy, calibration, and toxicity, among many others. Still, if LLMs are trained on the evaluation sets of such benchmarks, performance comparisons are meaningless. While the authors of HELM opted out their evaluation sets by contacting OpenAI, others could still inadvertently </span><a href="https://twitter.com/percyliang/status/1619594326585262082?s=20" rel="">contaminate</a><span> the models, such as by hosting a copy of the evaluation data.</span></p></li></ul></div></div></div></article></div></div></div>
  </body>
</html>
