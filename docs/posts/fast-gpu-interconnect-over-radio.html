<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/rf-over-fiber">Original</a>
    <h1>Fast GPU Interconnect over Radio</h1>
    
    <div id="readability-page-1" class="page"><p>Point2’s cables are made up of eight e-Tube fibers, each carrying more than 200 gigabits of data per second.</p><div data-headline="AI Data Centers Demand More Than Copper Can Deliver"><div><p><strong>How fast you can</strong> train gigantic new <a href="https://spectrum.ieee.org/tag/ai-models">AI models</a> boils down to two words: <em><em>up</em></em> and <em><em>out</em></em>.</p><p>In data-center terms, scaling out means increasing how many AI computers you can link together to tackle a big problem in chunks. Scaling up, on the other hand, means <a href="https://spectrum.ieee.org/tag/jamming">jamming</a> as many GPUs as possible into each of those computers, linking them so that they act like a single gigantic GPU, and allowing them to do bigger pieces of a problem faster.</p><p>The two domains rely on two different physical connections. Scaling out mostly relies on <a href="https://spectrum.ieee.org/optical-interconnects-imec-silicon-photonics" target="_self">photonic chips and optical fiber</a>, which together can sling data hundreds or thousands of meters. Scaling up, which results in networks that are roughly 10 times as dense, is the domain of much simpler and less costly technology—copper cables that often span no more than a meter or two.</p><p>But the increasingly high GPU-to-GPU data rates needed to make more powerful computers work are coming up against the physical limits of copper. As the bandwidth demands on copper cables approach the terabit-per-second realm, physics demands that they be made shorter and thicker, says <a href="https://www.linkedin.com/in/david-kuo-6233274/" target="_blank">David Kuo</a>, vice president of product marketing and business development at the data-center-interconnect startup <a href="https://point2tech.com/" target="_blank">Point2 Technology</a>. That’s a big problem, given the <a href="https://spectrum.ieee.org/tag/congestion">congestion</a> inside computer racks today and the fact that <a href="https://spectrum.ieee.org/tag/nvidia">Nvidia</a>, the leading <a href="https://spectrum.ieee.org/tag/ai-hardware">AI hardware</a> company, <a href="https://www.nextplatform.com/2025/03/19/nvidia-draws-gpu-system-roadmap-out-to-2028/" target="_blank">plans an eightfold increase in the maximum number of GPUs</a> per system, from 72 to 576 by 2027.</p><p>“We call it the copper cliff,” says Kuo.</p><p>The industry is working on ways to unclog <a href="https://spectrum.ieee.org/tag/data-centers">data centers</a> by extending copper’s reach and bringing slim, long-reaching <a href="https://spectrum.ieee.org/tag/optical-fiber">optical fiber</a> closer to the GPUs themselves. But Point2 and another startup, <a href="https://www.attotude.com/" target="_blank">AttoTude</a>, advocate for a solution that’s simultaneously in between the two technologies and completely different from them. They claim the tech will deliver the low cost and reliability of copper as well as some of the narrow gauge and distance of optical—a combination that will handily meet the needs of future AI systems.</p><p>Their answer? Radio.</p><p>Later this year, Point2 will begin manufacturing the chips behind a 1.6-terabit-per-second cable consisting of eight slender polymer <a href="https://spectrum.ieee.org/tag/waveguides">waveguides</a>, each capable of carrying 448 gigabits per second using two frequencies, 90 gigahertz and 225 GHz. At each end of the <a href="https://spectrum.ieee.org/tag/waveguide">waveguide</a> are plug-in modules that turn electronic bits into modulated <a href="https://spectrum.ieee.org/tag/radio-waves">radio waves</a> and back again. AttoTude is planning essentially the same thing, but at <a href="https://spectrum.ieee.org/tag/terahertz">terahertz</a> frequencies and with a different kind of svelte, flexible cable.</p><p>Both companies say their technologies can easily outdo copper in reach—spanning 10 to 20 meters without significant loss, which is certainly long enough to handle Nvidia’s announced scale-up plans. And in Point2’s case, the system consumes one-third of optical’s power, costs one-third as much, and offers as little as one-thousandth the latency.</p><p>According to its proponents, radio’s reliability and ease of manufacturing compared with those of optics mean that it might beat <a href="https://spectrum.ieee.org/tag/photonics">photonics</a> in the race to bring low-energy processor-to-processor connections all the way to GPU, eliminating some copper even on the printed circuit board.</p><h2 id="copper">What’s wrong with copper? </h2><p>So, what’s wrong with copper? Nothing, so long as the data rate isn’t too high and the distance it has to go isn’t too far. At high data rates, though, conductors like copper fall prey to what’s called the skin effect.</p><p> <img alt="Comparison of two cables: direct-attach and e-Tube. e-Tube is smaller, reaching 20 meters." data-rm-shortcode-id="678704f9110fc24c546e080e60cd77cc" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/comparison-of-two-cables-direct-attach-and-e-tube-e-tube-is-smaller-reaching-20-meters.png?id=62604012&amp;width=980" height="1575" id="31ae0" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%202000%201575&#39;%3E%3C/svg%3E" width="2000"/> <small placeholder="Add Photo Caption...">A 1.6-terabit-per-second e-Tube cable has half the area of a 32-gauge copper cable and has up to 20 times the reach.  </small><small placeholder="Add Photo Credit...">Point2 Technology  </small></p><p>The skin effect occurs because the signal’s rapidly changing current leads to a changing magnetic field that tries to counter the current. This countering force is concentrated at the middle of the wire, so most of the current is confined to flowing at the wire’s outer edge—the “skin”—which increases resistance. At 60 hertz—the mains frequency in many countries—most of the current is in the outer 8 millimeters of copper. But at 10 GHz, the <a href="https://en.wikipedia.org/wiki/Skin_effect" target="_blank">skin is just 0.65</a> micrometers deep. So to push high-frequency data through copper, the wire needs to be wider, and you need more power. Both requirements work against packing more and more connections into a smaller space to scale up computing.</p><p>To counteract the skin effect and other signal-degrading issues, companies have developed copper cables with specialized electronics at either end. With the most promising, called <a href="https://credosemi.com/products/zeroflapaec/" target="_blank">active electrical cables, or AECs,</a> the terminating chip is called a <a href="https://w.kandou.com/matterhorn/2024-06-05-what-is-a-retimer/" target="_blank">retimer</a> (pronounced “<em><em>re</em></em>-timer”). This IC cleans up the data signal and the clock signal as they arrive from the processor. The circuit then retransmits them down the copper cable’s typically eight pairs of wires, or lanes. (There is a second set for transmitting in the other direction.) At the other end, the chip’s twin takes care of any noise or clock issues that accumulate during the journey and sends the data on to the receiving processor. Thus, at the cost of electronic complexity and power consumption, an AEC can extend the distance that copper can reach.</p><p><a href="https://www.linkedin.com/in/donbarnetson/" target="_blank">Don Barnetson</a>, senior vice president and head of product at <a href="https://credosemi.com/" target="_blank">Credo</a>, which provides network hardware to data centers, says his company has developed an AEC that can deliver 800 Gb/s as far as 7 meters—a distance that’s likely needed as computers hit 500 to 600 GPUs and span multiple racks. The first use of AECs will probably be to link individual GPUs to the network switches that form the scale-out network. This first stage in the scale-out network is important, says Barnetson, because “it’s the only nonredundant hop in the network.” Losing that link, even momentarily, can cause an AI training run to collapse.</p><p>But even if retimers manage to push the copper cliff a bit farther into the future, physics will eventually win. Point2 and AttoTude are betting that point is coming soon.</p><h2 id="reach">Terahertz radio’s reach </h2><p>AttoTude grew out of founder and CEO <a href="https://en.wikipedia.org/wiki/David_Welch_(optical_engineer)" target="_blank">Dave Welch</a>’s deep investigations into photonics. A cofounder of <a href="https://www.nokia.com/newsroom/nokia-completes-acquisition-of-infinera-to-create-innovation-powerhouse-in-optical-networks-with-the-scale-to-power-the-data-center-revolution/" target="_blank">Infinera, an optical telecom–equipment maker purchased by Nokia</a> in 2025, Welch developed photonic systems for decades. He knows the technology’s weaknesses well: It consumes too much power (about 10 percent of a data center’s compute budget, <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self">according to Nvidia</a>); it’s extremely sensitive to temperature; getting light into and out of photonics chips requires micrometer-precision manufacturing; and the technology’s lack of long-term reliability is notorious. (There’s even a term for it: “link flap.”)</p><p>“Customers love fiber. But what they hate is the photonics,” says Welch. “Electronics have been demonstrated to be inherently more reliable than optics.”</p><p>Fresh off Nokia’s US $2.3 billion purchase of Infinera, Welch asked himself some fundamental questions as he contemplated his next startup, beginning with “If I didn’t have to be at [an optical wavelength], where should I be?” The answer was the highest frequency that’s achievable purely with electronics—the <a href="https://spectrum.ieee.org/the-truth-about-terahertz" target="_self">terahertz</a> regime, 300 to 3,000 GHz.</p><p><span><span>“You start with passive copper, and you do everything you can to run in passive copper as long as you can</span><span>.</span><span>”</span> <span><strong>—</strong></span></span><span><strong><span>Don </span><span>Barnetson</span></strong><span><strong>, Credo</strong></span></span></p><p>So Welch and his team set about building a system that consists of a digital component to interface with the GPU, a terahertz-frequency generator, and a mixer to encode the data on the terahertz signal. An antenna then funnels the signal into a narrow, flexible waveguide.</p><p>As for the waveguide, it’s made of a <a href="https://spectrum.ieee.org/tag/dielectric">dielectric</a> at the center, which channels the terahertz signal, surrounded by cladding. One early version was just a narrow, hollow copper tube. Welch says that the second-generation cable—made up of fibers only about 200 µm across— points to a system with losses down to 0.3 decibels per meter—a small fraction of the loss from a typical copper cable carrying 224 Gb/s.</p><p>Welch predicts this waveguide will be able to carry data as far as 20 meters. That “happens to be a beautiful distance for scale-up in data centers,” he says.</p><p>So far, AttoTude has made the individual components—the digital data chip, the terahertz-signal generator, the circuit that mixes the two—along with a couple generations of waveguides. But the company hasn’t yet integrated them into a single pluggable form. Still, Welch says, the combination delivers enough bandwidth for at least 224 Gb/s transmission, and the startup demonstrated 4-meter transmission at 970 GHz last April at the <a href="https://www.ofcconference.org/" target="_blank">Optical Fiber Communications Conference</a>, in San Francisco.</p><h2>Radio’s reach in the data center </h2><p>Point2 has been aiming to bring radio to the data center longer than AttoTude has. Formed nine years ago by <a href="https://point2tech.com/company/team/" target="_blank">veterans</a> of <a href="https://www.marvell.com/" target="_blank">Marvell</a>, <a href="https://www.nvidia.com/en-us/" target="_blank">Nvidia</a>, and <a href="https://semiconductor.samsung.com/" target="_blank">Samsung</a>, the startup has pulled in $55 million in venture funding, most notably from computer cables and connections maker <a href="https://www.molex.com/en-us/home" target="_blank">Molex</a>. The latter’s backing “is critical, because they’re a major part of the cable-and-connector ecosystem,” says Kuo. Molex has already shown that it can make Point2’s cable without modifying its existing manufacturing lines, and now <a href="https://spectrum.ieee.org/tag/foxconn">Foxconn</a> Interconnect Technology, which makes cables and connectors, is partnering with the startup. The support could be a big selling point for the hyperscalers who would be Point2’s customers.</p><p data-rm-resized-container="25%"> <img alt="Bundles of grey cables cascade down the back of a black computer rack." data-rm-shortcode-id="138803367f8e43dc078bd207f8f482f3" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/bundles-of-grey-cables-cascade-down-the-back-of-a-black-computer-rack.jpg?id=62604023&amp;width=980" height="3000" id="27f16" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%202000%203000&#39;%3E%3C/svg%3E" width="2000"/> <small placeholder="Add Photo Caption...">Nvidia’s GB200 NVL72 rack-scale computer relies on many copper cables to link its 72 <a href="https://spectrum.ieee.org/tag/processors">processors</a> together.</small><small placeholder="Add Photo Credit...">NVIDIA </small></p><p>Each end of the Point2 cable, called an e-Tube, consists of a single silicon chip that converts the incoming digital data into modulated millimeter-wave frequencies and an antenna that radiates into the waveguide. The waveguide itself is a plastic core with metal cladding, all wrapped in a metal shield. A 1.6-Tb/s cable, called an active radio cable (ARC), is made up of eight e-Tube cores. At 8.1 millimeters across, that cable takes up half the volume of a comparable AEC cable.</p><p>One of the benefits of operating at RF frequencies is that the chips that handle them can be made in a standard silicon <a href="https://spectrum.ieee.org/tag/foundry">foundry</a>, says Kuo. A collaboration between engineers at Point2 and the <a href="https://spectrum.ieee.org/tag/korea">Korea</a> Advanced Institute of Science and Technology, reported this year in the <a href="https://ieeexplore.ieee.org/document/10966033" target="_blank"><em><em>IEEE Journal of Solid-State Circuits</em></em></a>, used <a href="https://en.wikipedia.org/wiki/28_nm_process" target="_blank">28-nanometer CMOS</a> technology, which hasn’t been cutting edge since 2010.</p><h2 id="scaleup">The scale-up network market </h2><p>As promising as their tech sounds, Point2 and AttoTude will have to overcome the data-center industry’s long history with copper. “You start with passive copper,” says Credo’s Barnetson. “And you do everything you can to run in passive copper as long as you can.”</p><p>The boom in <a href="https://spectrum.ieee.org/data-center-liquid-cooling" target="_self">liquid cooling for data-center computing</a> is evidence of that, he says. “The entire reason people have gone to <a href="https://spectrum.ieee.org/tag/liquid-cooling">liquid cooling</a> is to keep [scaling up] in passive copper,” Barnetson says. To connect more GPUs in a scale-up network with passive copper, they must be packed in at densities too high for air cooling alone to handle. Getting the same kind of scale-up from a more spread-out set of GPUs connected by millimeter-wave ARCs would ease the need for cooling, suggests Kuo.</p><p>Meanwhile, both <a href="https://spectrum.ieee.org/tag/startups">startups</a> are also chasing a version of the technology that will attach directly to the GPU.</p><p>Nvidia and <a href="https://spectrum.ieee.org/tag/broadcom">Broadcom</a> recently deployed optical <a href="https://spectrum.ieee.org/tag/transceivers">transceivers</a> that live inside the same package as a processor, separating the electronics and optics by micrometers rather than centimeters or meters. Right now, the technology is limited to the network-switch chips that connect to a scale-out network. But big players and startups alike are trying to extend its use all the way to the GPU.</p><p>Both Welch and Kuo say their companies’ technologies could have a big advantage over optical tech in such a transceiver-processor package. <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self">Nvidia and Broadcom—separately—had to do a mountain of engineering</a> to make their systems possible to manufacture and reliable enough to exist in the same package as a very expensive processor. One of the many challenges is how to attach an optical fiber to a waveguide on a photonic chip with micrometer accuracy. Because of its short wavelength, infrared laser light must be lined up very precisely with the core of an optical fiber, which is only around 10 µm across. By contrast, millimeter-wave and terahertz signals have a much longer wavelength, so you don’t need as much precision to attach the waveguide. In one demo system it was done by hand, says Kuo.</p><p>Pluggable connections will be the technology’s first use, but radio transceivers co-packaged with processors are “the real prize,” says Welch. <span></span></p></div></div></div>
  </body>
</html>
