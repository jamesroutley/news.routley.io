<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://allenai.github.io/lumos/">Original</a>
    <h1>Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs</h1>
    
    <div id="readability-page-1" class="page">

<nav role="navigation" aria-label="main navigation">
  
  
<!--   <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
      
    </div>

  </div> -->
  
</nav>


<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span><sup>1</sup>Allen Institute for AI</span> Â Â 
            <span><sup>2</sup>University of California, Los Angeles </span> Â Â 
            <span><sup>3</sup>University of Washington</span> Â Â 

          </p>

          
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
        <div><p>
          We introduce ðŸª„ <strong>Lumos</strong>, Language Agents with Unified Data Formats, Modular Design, and Open-Source LLMs. <strong>Lumos</strong> unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. 
          </p></div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>
    @article{yin2023lumos,
      title={{Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs}},
      author={Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen},
      journal={arXiv preprint arXiv:2311.05657},
      year={2023}
    }
    </code></pre>
  </div>
</section>

  
<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>ðŸª„ Lumos Architecture</h2>
        <p>
        <img src="https://allenai.github.io/lumos/static/images/lumos_architecture.png" width="600"/>
        </p>
        <div>
          <p><strong>Lumos</strong> consists of following modules: 
          </p><ul>
            <li><strong>Planning Module:</strong>
              <ul>
                <li>Decompose a complex task into a series of high-level subgoals, which are written in natural language.</li>
              </ul>
            </li><li><strong>Grounding Module:</strong>
              <ul>
                <li>Convert the high-level subgoals produced by the planning module to low-level executable actions.</li>
              </ul>
            </li><li><strong>Execution Module:</strong>
              <ul>
                <li>Parse actions to a series of external tools including APIs, small neural models, and virtual simulators that interact with relevant tools and external environment.</li>
              </ul>
          </li></ul>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>ðŸª„ Lumos Formulation</h2>
        <p>
        <img src="https://allenai.github.io/lumos/static/images/lumos_formulation.png"/>
        </p>
        <div><p>
          We attempt the following two <strong>Lumos</strong> formulations: 
          </p><ul>
            <li><strong>Lumos-Iterative (Lumos-I):</strong>
              <ul>
              <li>Generates one subgoal and its corresponding executable actions in each iteration according to the external environment and prior memory.</li> 
              <li>When generating the current t-th subgoal, the planning module requires the previous planned subgoals and the execution results of their grounded actions.</li>
              </ul>
            </li><li><strong>Lumos-Onetime (Lumos-O):</strong>
              <ul>
              <li>An efficient formulation that generates all the subgoals and grounded actions at once.</li>
              </ul>
          </li></ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>ðŸª„ Lumos Training Annotations</h2>
        <p>
        <img src="https://allenai.github.io/lumos/static/images/lumos_annotation_example.png"/>
        </p>
        <div>
          <p>
          Instead of using Self-Instruct method, we use LLMs to convert ground-truth intermediate reasoning steps into the expected high-quality annotations aligning with our proposed formulations. 
          </p>
          <p>
          Finally, we are able to generate ~40K annotations to train Lumos planning and grounding modules (one of the largest resources for language agent fine-tuning). The annotation sources cover web, complex QA and math task types. See our final annotation data in <a href="https://huggingface.co/datasets?search=ai2lumos">Huggingface Dataset</a> and prompt details in <a href="https://github.com/allenai/lumos/tree/main/data">Github</a>.
          </p>       
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>Results</h2>
        <div>
        <p>
        <img src="https://allenai.github.io/lumos/static/images/lumos_results_1.png" width="80%"/>
        </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>


<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>Comparison with Baseline Formulations</h2>
        <div>
          <p>
          <img src="https://allenai.github.io/lumos/static/images/lumos_results_2.png"/>
          </p>
          <p>
            We compare <strong>Lumos</strong> formulation with other baseline formulations to train open-source agents. The baseline formulations are Chain-of-Thought Training 
            and Integrated Agent Training. 
          </p>
          <p>
            <strong>Lumos</strong> performs the best among the baselines on three different complex interative tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>


<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>Generalizability of Lumos</h2>
        <div>
          <p>
          <img src="https://allenai.github.io/lumos/static/images/lumos_results_3.png" width="250"/>
          </p>
          <p>
            We first evaluate <strong>Lumos</strong> trained with the unified annotations composed by task-specific ones. We then test <strong>Lumos</strong> on an unseen complex interactive task, WebShop.
          </p>
          <p>
            We find that after the unified training, <strong>Lumos</strong> would have slightly higher
            performance on web and complex QA tasks. We also observe that <strong>Lumos</strong> can bring an improvement over domain-specific agents 5-10 reward improvement, and also better performance than larger agents with 13B and 30B sizes.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>


<section>
  <div>
    <!-- Results. -->
    <div>
      <div>
        <h2>Further Analysis on Annotations</h2>
        <div>
          <p>
          <img src="https://allenai.github.io/lumos/static/images/lumos_results_4.png" width="400"/>
          </p>
          <p>
            We also conduct deeper analysis about annotation quality and the choice of annotation formats. We answer the following questions: 
            </p><ul>
            <li><strong>Q1: How good is our converted training annotations?</strong></li>
            <li><strong>Q2: Would it be better if we adopt low-level subgoals instead of our proposed high-level subgoals?</strong></li>
            </ul>
          
          <p>
            We find that by controling the training annotation size to be the same, our annotations can still help get better performance than the ones produced by Self-Instruct method and passed by rigorous 
            execution sanity checking. Also, we find that making planning module generate high-level subgoals would be a superior choice to generating a very long sequence of low-level subgoals.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>






</div>
  </body>
</html>
