<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.hillelwayne.com/post/queueing-prism/">Original</a>
    <h1>Two workers are quadratically better than one (2020)</h1>
    
    <div id="readability-page-1" class="page"><article lang="en">
    

    
    

    <div>
  

<p>For latency, anyway.</p>

<p>A common architecture pattern is the “task queue”: tasks enter an ordered queue, workers pop tasks and process them. This pattern is used everywhere from <a href="https://aws.amazon.com/sqs/">distributed systems</a> to thread pools. It applies just as well to human systems, like waiting in line at a <a href="https://www.nytimes.com/2007/06/23/business/23checkout.html">supermarket</a> or a bank. Task queues are popular because they are simple, easy to understand, and scale well.</p>

<p>There are two primary performance metrics for a task queue. <strong>Throughput</strong> is how many tasks are processed per time unit. <strong>Latency</strong> is how long a task waits in the queue before being processed. Throughput scales as you’d expect (2x workers ≈ 2x throughput) but latency is less intuitive. For all their popularity, we don’t abstractly model the properties of task queues. We have tools to implement them and tools to trace/metric them, but we don’t use theoretical models to understand our systems in abstract. In this essay we will model a simple task queue and show how the latency is highly sensitive to our initial parameters.</p>

<p>We will be using PRISM to model this problem. PRISM is a probabilistic model checker, meaning it can take a software design and figure out how likely various outcomes are. I’ve <a href="https://www.hillelwayne.com/post/prism/">talked about PRISM before</a> and criticized it for its restrictive syntax. But here it is the right tool for the job and with some cleverness we can make it shine. This essay will assume no prior knowledge of PRISM. Let’s get started.</p>

<h2 id="planning-ahead">Planning Ahead</h2>

<p>PRISM supports many different kinds of probabilistic models. The simplest of these is the <strong>Discrete Time Markov Process</strong>, or <code>dtmc</code>. In this, time progresses one discrete step at the time and everything happens with known probabilities. You can think of it as randomly moving between states of a state machine, where one step is one transition. The step is an abstract amount of time. While you could design under the assumption that each step is a fixed time, like 5 seconds, we don’t need to here.</p>

<p>We will assume that there are N tasks and that they are <strong>independent</strong>. This means that how long a task takes to process is independent of how many tasks came before and how many tasks come after. However, different tasks will take a different amount of time to process. We can emulate this by flipping it around and saying that each worker has only a certain chance of completing a task each time step. If we say the chance is 50%, then half the tasks will be completed in exactly one step, a quarter will be completed in exactly two, etc. In aggregate, the worker will complete a task roughly every two steps.</p>

<p>PRISM is not expressive enough to let us individually track each task. Fortunately, all the tasks we have are interchangeable: while some tasks will take longer than others to process, this is abstracted away in the probability. We can instead say that there is some integer that represents the number of tasks left, where the worker has a chance each step of decrementing that number. Then if it takes four steps to decrement <code>left</code>, we just say the worker was working on a complex task.</p>

<h2 id="specifying-throughput">Specifying Throughput</h2>

<h3 id="spec">Spec</h3>

<p>First some boilerplate:</p>

<pre><code>dtmc //type of model

const int N; // Number of tasks

module workers
  // number of tasks left to process
  // A number between 0 and N
  left : [0..N] init N;
</code></pre>

<p>We need to make a <strong>command</strong> to represent the worker completing a task. First the full command, then the breakdown of how it works:</p>

<pre><code>  [worker] (left &gt; 0) -&gt;
        0.5: (left&#39; = left - 1)
        + 0.5: true;
</code></pre>

<p>Commands have three parts: a <strong>label</strong>, a <strong>guard</strong>, and an <strong>update</strong>. We’ll start with the update.  We can say this just means that we decrement <code>left</code>. In PRISM syntax, we say that the new value of <code>left</code>, written <code>left&#39;</code>, is <code>left - 1</code></p>

<pre><code>(left&#39; = left - 1)
</code></pre>

<p>But it only succeeds 50% of the time. The other 50% of the time nothing happens. We could write <code>queue&#39; = queue</code>, or we can use the shorthand <code>true</code>:</p>

<pre><code>0.5: (left&#39; = left - 1) + 0.5: true;
</code></pre>

<p>Next is the guard. The update is only possible if the guard is true. The obvious guard we need here is that we can’t work on a task if there aren’t any tasks left. That leaves just the label, which is an optional name we give to the command.</p>

<p>It’s good form to leave a “stutter state” at the end so that the model doesn’t deadlock. So if we’re out of tasks, we just say <code>true</code>.</p>

<pre><code>  [worker] (left &gt; 0) -&gt;
        0.5: (left&#39; = left - 1)
        + 0.5: true;

 [] (left = 0) -&gt; true;
endmodule
</code></pre>

<p>Full spec:</p>

<details>
  <summary>
    show spec
  </summary>
  <pre><code>dtmc

const int N; // Number of tasks

module workers
  left : [0..N] init N;

  [worker] (left &gt; 0) -&gt;
        0.5: (left&#39; = left - 1)
        + 0.5: true;
  [] (left = 0) -&gt; true;

endmodule

// see next section
rewards &#34;total_time&#34;
  [] left &gt; 0: 1;
endrewards
</code></pre>

</details>




<h3 id="modeling-throughput">Modeling Throughput</h3>

<p>In programs, we have one level of coding: the program itself. With specifications, we have two levels. We write the specs and then we write the properties about the specs. In PRISM we can also write properties as queries on the system and have the model checker tell us what the expect values are.</p>

<p>Our first property is the total time taken to process all tasks. To track this we need to make a small change to our spec. A <strong>reward function</strong> is a value we assign to each successive state in the evolution of the system. This will become much more important when we need to model latency, but for now we can get away with a very simple reward function:</p>

<pre><code>rewards &#34;total_time&#34;
  [] left &gt; 0: 1;
endrewards
</code></pre>

<p>If there are any tasks left in the queue, we increment the total reward for that behavior by one. The total reward is the number of steps taken to complete all tasks.</p>

<p>Next we need a property that gets us the reward at the moment we run out of tasks. That looks like this:</p>

<pre><code>// reward in first state where left = 0
R{&#34;total_time&#34;}=? [ F left=0 ]
</code></pre>

<p>The reward will depend on the number of tasks we start with, or <code>N</code>. We can run an <strong>experiment</strong>, where we test the property for a set of <code>N</code> and graph the result. Here’s the throughput for N from 10 to 100, in steps of 10:</p>

<p><img src="https://www.hillelwayne.com/post/queueing-prism/tt1w.png" alt="Graph representing the &#34;show data&#34; below."/>
</p><details>
  <summary>
    show data
  </summary>
  <table>
<thead>
<tr>
<th>N</th>
<th>Result</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>20</td>
</tr>

<tr>
<td>20</td>
<td>40</td>
</tr>

<tr>
<td>30</td>
<td>60</td>
</tr>

<tr>
<td>40</td>
<td>80</td>
</tr>

<tr>
<td>50</td>
<td>100</td>
</tr>

<tr>
<td>60</td>
<td>120</td>
</tr>

<tr>
<td>70</td>
<td>140</td>
</tr>

<tr>
<td>80</td>
<td>160</td>
</tr>

<tr>
<td>90</td>
<td>180</td>
</tr>

<tr>
<td>100</td>
<td>200</td>
</tr>
</tbody>
</table>

</details>




<p>That gives us <em>throughput</em>. But our model can’t tell us anything about <em>latency</em> yet, because we’ve assumed that all of the tasks start out in the queue. That models a batch process, not tasks coming in over time and waiting till their turn.</p>

<p>This is where latency becomes very important. In addition to how long it takes to process a task, we want to account for how long tasks are waiting before being processed. Our model is incomplete until we add this in.</p>

<h2 id="specifying-latency">Specifying Latency</h2>

<h3 id="spec-1">Spec</h3>

<p>Instead of all the tasks being available for work immediately, we say that they come in over time. We add a new variable, <code>queue</code>, to represent the number of tasks actually present in the queue. We also add an <code>enqueue</code> command that adds tasks to the queue if there any left not yet in it. <strong>We’ll assume that this happens at the same “rate” the worker processes tasks.</strong>This assumption pins us to a specific subset of the configuration space. It’s fine for now, but we’ll eventually want a more general specification.</p>

<pre><code>  queue: [0..N] init 0;
  [enqueue] (queue &lt; left) -&gt;
    0.5: (queue&#39; = queue + 1)
    + 0.5: true;
</code></pre>

<p>The worker will only be able to process tasks if the queue is nonempty. When it processes a task, it will decrement from both the total <code>left</code> and the <code>queue</code>.</p>

<pre><code>  [worker] (left &gt; 0 &amp; queue &gt; 0) -&gt;
        0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.5: true;
</code></pre>

<p>Our two commands aren’t exclusive: when <code>0 &lt; queue &lt; left</code> then both <code>[enqueue]</code> and <code>[worker]</code> are valid commands. In this case, all the possibilities are weighted together. Here this means that there are four equally-likely possibilities:</p>

<ol>
<li><code>queue</code> increments</li>
<li><code>worker</code> decrements</li>
<li><code>queue</code> does nothing</li>
<li><code>worker</code> does nothing</li>
</ol>

<p>(3) and (4) have the same outcome, but are different for modeling purposes. In my mental model, <code>worker</code> commands model a longer span of time than <code>enqueue</code> commands do. The worker needs time to process the task, while adding a task to the queue is instantaneous. Since we’re calculating total time taken, we only want to count the <code>worker</code> commands as time passing. We can do this by adding a label to the reward function:</p>
<div><pre><code data-lang="diff">rewards &#34;total_time&#34;
<span>- [] left &gt; 0: 1;
</span><span></span><span>+ [worker] left &gt; 0: 1;
</span><span></span>endrewards
</code></pre></div>
<p>Final spec:</p>

<details>
  <summary>
    show spec
  </summary>
  <pre><code>dtmc

const int N; // Number of tasks

module workers
  left : [0..N] init N;
  queue: [0..N] init 0;

  [enqueue] (queue &lt; left) -&gt;
    0.5: (queue&#39; = queue + 1)
    + 0.5: true;

  [worker] (left &gt; 0 &amp; queue &gt; 0) -&gt;
        0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.5: true;

  [] (left = 0) -&gt; true;

  

endmodule

// see next section
rewards &#34;wait_time&#34;
  [worker] queue &gt; 1: queue - 1;
endrewards

rewards &#34;total_time&#34;
  [worker] left &gt; 0: 1;
endrewards
</code></pre>

</details>




<h3 id="modeling-latency">Modeling Latency</h3>

<p>First, let’s sanity check that we didn’t change the throughput. Since we’re only modeling time passing with <code>worker</code> commands, our new model shouldn’t change the total time. Here’s what we get:</p>

<p><img src="https://www.hillelwayne.com/post/queueing-prism/tt1w.png" alt="Same graph as before, so much so I just reused the file"/></p>

<p>Exactly the same, good. But we’re here to model latency, too. Latency is how long a task waits in the queue before it gets processed. Since we don’t have a way of singling out individual tasks, we can instead look at how long all of the tasks wait in total, which is a correlated value.</p>

<p>The reward function will be similar to <code>total_time</code>, except instead of incrementing the reward by 1, we increment it by <code>queue - 1</code>. We subtract one because if the queue is nonempty then one of the tasks in the queue is being processed.</p>

<pre><code>rewards &#34;wait_time&#34;
  [worker] queue &gt; 0: queue - 1;
endrewards
</code></pre>

<p>We also need a new property:</p>

<pre><code>R{&#34;wait_time&#34;}=? [ F left=0 ]
</code></pre>

<p>And here’s the latency as a function of N:</p>

<p><img src="https://www.hillelwayne.com/post/queueing-prism/wt1w.png" alt="Quadratic graph of wait time, see below"/></p>

<details>
  <summary>
    show data
  </summary>
  <table>
<thead>
<tr>
<th>N</th>
<th>Result</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>29</td>
</tr>

<tr>
<td>20</td>
<td>97</td>
</tr>

<tr>
<td>30</td>
<td>190</td>
</tr>

<tr>
<td>40</td>
<td>305</td>
</tr>

<tr>
<td>50</td>
<td>436</td>
</tr>

<tr>
<td>60</td>
<td>584</td>
</tr>

<tr>
<td>70</td>
<td>746</td>
</tr>

<tr>
<td>80</td>
<td>922</td>
</tr>

<tr>
<td>90</td>
<td>1110</td>
</tr>

<tr>
<td>100</td>
<td>1310</td>
</tr>
</tbody>
</table>

</details>




<p>The growth is quadratic! If we have 10 tasks, they wait a total of 30 steps. If we have 100, they wait a total of over 1300 steps. A 10x increase in volume gives us a 40x increase in total latency!</p>

<h4 id="why">Why</h4>

<p>This surprises a lot of people when they first see it. PRISM provides a simulator view that helps you step through a possible timeline and see what’s happening. Unfortunately it’s not well suited for static text so instead here’s an informal explanation.</p>

<p>Imagine we have 4 tasks that come in a random order, spaced one step apart. We also know that three of the tasks will take a one step to finish, while the last will take 5. We know for certain that we will finish all 4 tasks in eight steps, but the latency depends on the order they come in. The best case scenario is <code>1 1 1 5</code>. We complete the first three tasks in the first three steps. When the large task comes, it’s the only task in the queue so doesn’t count for latency, and the total wait time is zero.</p>

<p>The worst case scenario is <code>5 1 1 1</code>. By step four, all four tasks are in the queue, but we still haven’t finished the first one. We only finish the first task at step five, by which point our wait time is already 9. Our final wait time is 12.</p>

<p>Now instead try <code>2 2 2 2</code>. Same total time, but now the last task is only in the queue for 3 steps before we start processing it. Our wait time is instead 6.</p>

<p>The latency spike then comes from two places: first, more tasks means a higher chance of a long task that jams the queue. Second, if the queue gets jammed there are more tasks that can pile up behind it, adding latency.</p>

<h2 id="two-workers">Two Workers</h2>

<p>It’s time to finally add in two workers. The obvious way is to just add another command, like this:</p>

<pre><code>[worker2] (left &gt; 0 &amp; queue &gt; 0) -&gt;
   0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
   + 0.5: true;

</code></pre>

<p>But this doesn’t work. Either <code>worker</code> will happen or <code>worker2</code> will happen, but they can’t both happen in the same step. They both modify <code>left</code> and <code>queue</code>, and each variable can only be modified once per step. We instead have to be clever. Since the two workers are independent, we “simulate” both at once as part of a single command. Consider the following table:</p>

<table>
  <thead>
    <tr><th>queue&#39;</th>
    <th>¬W1</th>
    <th>W1</th>
  </tr></thead>
  <tbody>
    <tr>
      <td><strong>¬W2</strong></td>
      <td>0</td>
      <td>-1</td>
    </tr>
    <tr>
      <td><strong>W2</strong></td>
      <td>-1</td>
      <td>-2</td>
    </tr>
  </tbody>
</table>

<p>Each combination is equally likely, so there’s a 50% of exactly one worker processing a task, a 25% chance of both workers processing a task, and a 25% chance of both failing. That leads to the following command:</p>

<pre><code>  [worker] (left &gt; 1 &amp; queue &gt; 1) -&gt;
         0.25: (left&#39; = left - 2) &amp; (queue&#39; = queue - 2)
        + 0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.25: true;
</code></pre>

<p>This only works if there are at least two tasks in the queue. Otherwise, one worker has to idle while the other processes it. If there’s only one task in the queue then we pretend we only have one worker.</p>

<pre><code>  [worker] (left &gt;= 1 &amp; queue = 1) -&gt;
        0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.5: true;
</code></pre>

<p>We also need to change our reward function. We only consider tasks after the first two in the queue to be waiting.</p>

<pre><code>rewards &#34;wait_time&#34;
  [worker] queue &gt; 2: queue - 2;
endrewards
</code></pre>

<p>Current spec:</p>

<details>
  <summary>
    show spec
  </summary>
  <pre><code>dtmc

const int N; // Number of tasks

module workers
  left : [0..N] init N;
  queue: [0..N] init 0;

  [enqueue] (queue &lt; left) -&gt;
    0.5: (queue&#39; = queue + 1)
    + 0.5: true;

  [worker] (left &gt;= 1 &amp; queue = 1) -&gt;
        0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.5: true;

  [worker] (left &gt; 1 &amp; queue &gt; 1) -&gt;
         0.25: (left&#39; = left - 2) &amp; (queue&#39; = queue - 2)
        + 0.5: (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
        + 0.25: true;

  [] (left = 0) -&gt; true;

  

endmodule

rewards &#34;wait_time&#34;
  [worker] queue &gt; 2: queue - 2;
endrewards

rewards &#34;total_time&#34;
  [worker] left &gt; 0: 1;
endrewards
</code></pre>

</details>




<h3 id="throughput-and-latency">Throughput and Latency</h3>

<p>First throughput:</p>

<p><img src="https://www.hillelwayne.com/post/queueing-prism/tt2w.png" alt="Two worker throughput"/></p>

<details>
  <summary>
    show data
  </summary>
  <table>
<thead>
<tr>
<th>N</th>
<th>2 workers</th>
<th>1 worker</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>14</td>
<td>20</td>
</tr>

<tr>
<td>20</td>
<td>27</td>
<td>40</td>
</tr>

<tr>
<td>30</td>
<td>39</td>
<td>60</td>
</tr>

<tr>
<td>40</td>
<td>52</td>
<td>80</td>
</tr>

<tr>
<td>50</td>
<td>65</td>
<td>100</td>
</tr>

<tr>
<td>60</td>
<td>78</td>
<td>120</td>
</tr>

<tr>
<td>70</td>
<td>91</td>
<td>140</td>
</tr>

<tr>
<td>80</td>
<td>103</td>
<td>160</td>
</tr>

<tr>
<td>90</td>
<td>116</td>
<td>180</td>
</tr>

<tr>
<td>100</td>
<td>129</td>
<td>200</td>
</tr>
</tbody>
</table>

</details>




<p>You’d think it’d take half the total time to complete all tasks, but it’s actually closer to 2/3rds. We can only use both workers when the queue length is at least 2, so for most of each run we’re wasting a worker. If we saturated the queue by making the inbound rate much higher then the total time would converge to half the total time for one worker.</p>

<p>Now for latency:</p>

<p><img src="https://www.hillelwayne.com/post/queueing-prism/wt2w.png" alt="Two worker latency"/></p>

<details>
  <summary>
    show data
  </summary>
  <table>
<thead>
<tr>
<th>N</th>
<th>2 workers</th>
<th>1 worker</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>5</td>
<td>29</td>
</tr>

<tr>
<td>20</td>
<td>13</td>
<td>97</td>
</tr>

<tr>
<td>30</td>
<td>22</td>
<td>190</td>
</tr>

<tr>
<td>40</td>
<td>31</td>
<td>305</td>
</tr>

<tr>
<td>50</td>
<td>41</td>
<td>436</td>
</tr>

<tr>
<td>60</td>
<td>50</td>
<td>584</td>
</tr>

<tr>
<td>70</td>
<td>59</td>
<td>746</td>
</tr>

<tr>
<td>80</td>
<td>68</td>
<td>922</td>
</tr>

<tr>
<td>90</td>
<td>77</td>
<td>1110</td>
</tr>

<tr>
<td>100</td>
<td>87</td>
<td>1310</td>
</tr>
</tbody>
</table>

</details>




<p>For one worker the wait time was just about quadratic, while this is sublinear. The queue only jams if <em>both</em> workers stall out, which is a lot less likely. Going back to our <code>5 1 1 1</code> case, by the time the first worker has finished the first task the second worker has already processed the other three.</p>

<h2 id="generalizing-the-model">Generalizing the Model</h2>

<p>Our model only covers a small part of the state space. We hardcoded the inbound probability, the outbound, and the number of workers. In particular I don’t like how we used the same probability for enqueuing and dequeuing. For all we know that leads to pathological results. A good spec should help us explore different parameters without us having to rewrite it.</p>

<p>The first change is the simplest: changing the inbound rate. PRISM lets us use expressions in guard clauses, so by adding a <code>P_request</code> constant we can vary in the inbound probability per step. All update probabilities must sum to 1, which is easy here.</p>
<div><pre><code data-lang="diff">const double P_request; // in [0, 1]

// ...

[enqueue] (queue &lt; left &amp; left &lt;= N) -&gt;
    P_request: (queue&#39; = queue + 1)
    + (1 - P_request): true;
</code></pre></div>
<p>Changing the task processing rate is more complicated. Let’s say the probability of a worker processing a task is P. With two workers, there’s four possibilities:</p>

<ul>
<li>Both workers process a task: <code>p*p</code></li>
<li>W1 processes a task, W2 does not: <code>p*(1-p)</code></li>
<li>W2 processes a task, W1 does not: <code>(1-p)*p</code></li>
<li>Neither worker processes a task: <code>(1-p)*(1-p)</code></li>
</ul>

<p>You can check that all the possibilities add up to 1. If we combine terms and simplify, we get</p>

<pre><code>1 * p²(1-p)⁰: queue&#39; = queue - 2
2 * p¹(1-p)¹: queue&#39; = queue - 1
1 * p⁰(1-p)²: queue&#39; = queue - 0
</code></pre>

<p>This might remind you of Algebra 1: <code>(a + b)² = a² + 2ab + b²</code>. With a bit of combinatorics we can prove it’s the same. This is just the binomial expansion! The corresponding probabilities for 3 workers would be:</p>

<pre><code>1 * p³(1-p)⁰
3 * p²(1-p)¹
3 * p¹(1-p)²
1 * p⁰(1-p)³
</code></pre>

<p>Not only can we use this to generalize the task processing rate, we can use this to scale up to an arbitrary number of workers. Unfortunately, that’s way too tedious to do by hand. So where’s what I did instead:</p>
<div><pre><code data-lang="diff"><span>- [worker] // blah blah blah
</span><span></span><span>+ $worker
</span><span></span></code></pre></div>
<p>And then I made a Python script write it for me. You can get the template and the script <a href="https://gist.github.com/hwayne/00ec2cef54a3031ec304ccb44eeaa42a">in this gist</a>.</p>

<details>
  <summary>
    show generated spec
  </summary>
  <pre><code>dtmc

const int N; // Max Tasks
const double P_request; // in [0, 1]
const double P_worker; // in [0, 1]
const int K; // [1, ]

module queues
  left : [0..N] init N;
  queue: [0..N] init 0;


[worker] (left &gt;= 1 &amp; ((queue &gt;= 1 &amp; K = 1) | (queue = 1 &amp; K &gt; 1))) -&gt;
      1*pow(P_worker, 0)*pow(1-P_worker,1): (left&#39; = left - 0) &amp; (queue&#39; = queue - 0)
    + 1*pow(P_worker, 1)*pow(1-P_worker,0): (left&#39; = left - 1) &amp; (queue&#39; = queue - 1);

[worker] (left &gt;= 2 &amp; ((queue &gt;= 2 &amp; K = 2) | (queue = 2 &amp; K &gt; 2))) -&gt;
      1*pow(P_worker, 0)*pow(1-P_worker,2): (left&#39; = left - 0) &amp; (queue&#39; = queue - 0)
    + 2*pow(P_worker, 1)*pow(1-P_worker,1): (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
    + 1*pow(P_worker, 2)*pow(1-P_worker,0): (left&#39; = left - 2) &amp; (queue&#39; = queue - 2);

[worker] (left &gt;= 3 &amp; ((queue &gt;= 3 &amp; K = 3) | (queue = 3 &amp; K &gt; 3))) -&gt;
      1*pow(P_worker, 0)*pow(1-P_worker,3): (left&#39; = left - 0) &amp; (queue&#39; = queue - 0)
    + 3*pow(P_worker, 1)*pow(1-P_worker,2): (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
    + 3*pow(P_worker, 2)*pow(1-P_worker,1): (left&#39; = left - 2) &amp; (queue&#39; = queue - 2)
    + 1*pow(P_worker, 3)*pow(1-P_worker,0): (left&#39; = left - 3) &amp; (queue&#39; = queue - 3);

[worker] (left &gt;= 4 &amp; ((queue &gt;= 4 &amp; K = 4) | (queue = 4 &amp; K &gt; 4))) -&gt;
      1*pow(P_worker, 0)*pow(1-P_worker,4): (left&#39; = left - 0) &amp; (queue&#39; = queue - 0)
    + 4*pow(P_worker, 1)*pow(1-P_worker,3): (left&#39; = left - 1) &amp; (queue&#39; = queue - 1)
    + 6*pow(P_worker, 2)*pow(1-P_worker,2): (left&#39; = left - 2) &amp; (queue&#39; = queue - 2)
    + 4*pow(P_worker, 3)*pow(1-P_worker,1): (left&#39; = left - 3) &amp; (queue&#39; = queue - 3)
    + 1*pow(P_worker, 4)*pow(1-P_worker,0): (left&#39; = left - 4) &amp; (queue&#39; = queue - 4);


  [] (left = 0) -&gt; true;

  [enqueue] (queue &lt; left &amp; left &lt;= N) -&gt;
    P_request: (queue&#39; = queue + 1)
    + (1 - P_request): true;
endmodule

rewards &#34;wait_time&#34;
  [worker] queue &gt; K: queue - K;
endrewards

rewards &#34;total_time&#34;
  [worker] left &gt; 0: 1;
endrewards
</code></pre>

</details>




<p>I snuck in one more change there: I modified the guard clause to use a new constant <code>K</code>. I didn’t like how I hardcoded the number of workers we were using, so now <code>K</code> is the number of workers we have. That way if you want to play with it yourself you don’t have to modify the spec to switch between 1 or 2 (or 3 or 4) workers.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This was a simple model of task queues. We didn’t cover priority, error handling, multiple queues, etc. Nonetheless, the spec was good enough to show how nonlinear latency is and how adding more workers can have a dramatic effect.</p>

<p>If you’re interested in PRISM, you can download it <a href="https://www.prismmodelchecker.org/">here</a>. I’m <a href="https://www.hillelwayne.com/open-invite/">happy to</a> answer simple questions or provide tips if needed. If you’re interested in the broader mathematical theory of task queues, you’ll want to look into <a href="https://en.wikipedia.org/wiki/Queueing_theory"><strong>queueing theory</strong></a>.</p>

<p><em>I shared the first draft of this essay on my <a href="https://buttondown.email/hillelwayne/">newsletter</a>. If you like my writing, why not subscribe?</em></p>


</div>

    



  </article></div>
  </body>
</html>
