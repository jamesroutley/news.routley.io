<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vaclavkosar.com/ml/googles-pathways-language-model-and-chain-of-thought">Original</a>
    <h1>Google&#39;s Pathways Language Model and Chain-of-Thought</h1>
    
    <div id="readability-page-1" class="page"><div>






<ul>
<li>training task: Given this text, predict the next word (token).</li>
<li>the biggest dense <a href="https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified">transformer</a> (540B) and likely the most expensive (~$10M) as of early 2022</li>
<li>highly efficient training on 6k chips (TPU v4) across 2 clusters (Pods)</li>
<li>improvements from scaling continue in language understanding (few-shot)</li>
<li>disproportionate gains from certain scale e.g. reasoning: 62B to 540B vs 8B to 62B</li>
<li>breakthrough performance:
<ul>
<li>outperforming average human on a grade school logic and math (BIG-bench)</li>
<li>outperforms specialized and fine-tuned models on multistep reasoning</li>
<li>chain-of-thought prompting simulates inner monologue</li>
</ul>
</li>
</ul>
<h2 id="palms-size">PaLM’s Size</h2>
<ul>
<li><a href="https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified">transformer</a> architecture similar to <a href="https://arxiv.org/pdf/2005.14165.pdf">GPT-3</a></li>
<li>PaLM has 540B parameters</li>
<li>3x bigger than GPT-3 175B parameters</li>
<li>2x smaller than sparse <a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformer</a> 1T
<ul>
<li>only parts of the model is activated at each time.</li>
</ul>
</li>
<li>human brain 100T connections</li>
<li>likely the most expensive model <a href="https://blog.heim.xyz/palm-training-cost/">~$10M</a> (2.5 yottaFLOPS) vs GPT-3 <a href="https://lambdalabs.com/blog/demystifying-gpt-3/#1">~$5M</a></li>
<li>PaLM and GPT-3 fascinating, but likely not economical now</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-chart-computation-used-in-training.png" alt="PaLM vs GPT vs MT-NLG computation cost FLOPs AI training"/></p>
<h2 id="zero-shot-vs-few-shot-prompting-vs-fine-tuning">Zero-Shot vs Few-Shot Prompting vs Fine-Tuning</h2>
<ul>
<li>prompting: instructing via addition of textual context</li>
<li>zero-shot: task described, but demonstrations not given</li>
<li>few-shot: task described and random demonstrations provided</li>
<li>fine-tuning: model parameters are updated with correct answers</li>
</ul>
<p><img src="https://vaclavkosar.com/images/gpt-3-zero-shot-one-shot-fine-tuning.png" alt="zero-shot vs one-shot vs fine-tuning (GPT-3 paper)"/></p>
<h2 id="palms-breakthrough-capabilities-on-big-bench">PaLM’s Breakthrough Capabilities on BIG-bench</h2>
<ul>
<li>BIG-bench: 150+ tasks (a task contains samples) like reasoning, math, QA, translation, programming</li>
<li>58 tasks available results on other models, often multiple choice questions</li>
<li>PaLM is SOTA: outperforming human average and other models GPT-3, Gopher, Chinchilla
<ul>
<li>still underperforms average human on many tasks</li>
</ul>
</li>
<li>funny footnote: they scan for the dataset in the training set using GUID</li>
<li>certain capabilities emerge when a scale is reached (not log-linear)
<ul>
<li>english proverbs and logical sequence at 62B to 540B vs 8B to 62B</li>
</ul>
</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-discontinuous-improvement-with-scale.png" alt="PaLM discontinuous improvements with scale"/></p>
<h2 id="chain-of-thought-prompting">Chain-of-Thought Prompting</h2>
<ul>
<li>inserts reasoning steps between for each demonstration in the prompt</li>
<li>multi-step arithmetic
<ul>
<li>grade-school level math problems</li>
<li>difficult is to convert to equations</li>
</ul>
</li>
<li>commonsense logical reasoning
<ul>
<li>strong world knowledge</li>
<li>chaining logical inferences</li>
</ul>
</li>
<li>thoughts are useful for interpretation</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-chain-of-though-prompting.png" alt="Chain of Thought Prompting"/></p>
<h2 id="palm-chain-of-thought-prompting-results">PaLM Chain-of-Thought Prompting Results</h2>
<ul>
<li>reasoning tasks datasets: GSM8K, SVAMP, MAWPS, AQuA, CommonsenseQA, StrategyQA</li>
<li>humans allowed calculators and search engines too</li>
<li>the model triggers <a href="https://arxiv.org/pdf/2110.14168.pdf">a calculator by generating special annotation text</a> (+~5% acc)</li>
<li>on GSM8K: PaLM+chain-of-thought competitive with GPT-3+finetuning+calculator+verifier,
<ul>
<li>outperforms with a calculator, but GPT-3+chain-of-thought?</li>
</ul>
</li>
<li>generating explicit inference chains drastically increase the quality of the predictions</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-chain-of-though-prompting-results.png" alt="PaLM chain of thought prompting new SOTA on GSM8K, MAWPS, SVAMP, and StrategyQA"/></p>
<h2 id="explanation-using-chain-of-thought-prompting">Explanation Using Chain-of-Thought Prompting</h2>
<ul>
<li>right reasons for the right answer?</li>
<li>interpretability may be desired output</li>
<li>2-shot prompting with greedy decoding (deterministic)</li>
<li>not in paper: I added GPT-3, results with zero-temperature</li>
<li>average airplane speed 460 – 575 mph</li>
</ul>
<p><i>
Input: Jennifer looked out her window and sees a really cool cloud below her. She unbuckles her seatbelt and heads to the bathroom. Is Jennifer probably traveling more than 300 miles per hour relative to the earth?
</i></p>
<p><b>
PaLM: 300 miles per hour is about 480 km/h. This is about the speed of a commercial airplane. Clouds are usually below airplanes, so Jennifer is probably on an airplane. The answer is “yes”
</b></p>
<p><b>
GPT-3: No, because an airplane typically travels around 500-600 miles per hour, and the bathroom is usually located in the back of the plane.
</b></p>
<h2 id="palm-on-code-tasks">PaLM on Code Tasks</h2>
<ul>
<li>achieves similar results to specialized OpenAI’s Codex 12B with 3% of the code training data</li>
<li>after fine-tuning outperforms OpenAI’s Davinci Codex</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-coding-tasks-results.png" alt="PaLM and PaLM-Coder models scales 8B, 62B and 540B"/></p>
<h2 id="palm-on-other-tasks">PaLM on Other Tasks</h2>
<ul>
<li>generally SOTA on a few-shot competitions</li>
<li>few-shot is not competitive with fine-tuned model</li>
<li>fine-tuned only competitive with specialized fine-tuned variants</li>
<li>translation: without fine-tuning outperforms GPT-3, outperforms some supervised baselines de-en, ro-en</li>
<li>summarization: fine-tuned results competitive, few-shot largely underperforms the fine-tuned</li>
<li>multilingual question answering: fine-tuned results competitive, few-shot largely underperformed of fine-tuned</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-multilingual-question-answering-results.png" alt="palm multilingual question answering results"/></p>
<h2 id="palm-architecture">PaLM Architecture:</h2>
<ul>
<li>decoder-only transformer (attending only to the past) like GPT-3</li>
<li>modified Feed-forward layer (MLP):
<ul>
<li>instead of RELU \( max(0, xW_1 + b_1)W_2 + b_2 \) use <a href="https://arxiv.org/pdf/2002.05202.pdf">SwiGLU feed-foward</a></li>
<li>~1% better in compute equivalent setup</li>
<li>uses GLU: gated linear unit - a sigmoid controlled output</li>
<li>SwiGLU: \( \mathrm{FFN}_{\mathrm{SwiGLU}} := (\mathrm{Swish}(xW_1) \otimes xV ) W_2 \)</li>
<li>uses <a href="https://arxiv.org/pdf/1710.05941v1.pdf?source=post_page">swish activation</a>: \( x (1 + exp(−x))^{−1} \)</li>
</ul>
</li>
<li>parallel Attention and Feed-forward layer (MLP) from <a href="https://github.com/kingoflolz/mesh-transformer-jax">GPT-J</a>:
<ul>
<li>instead of sequential is additive:</li>
<li>\( y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x)) \)</li>
<li>15% speedup for small degradation</li>
</ul>
</li>
<li><a href="https://arxiv.org/pdf/1911.02150.pdf">multi-query attention</a>:
<ul>
<li>block-shared key and value projections, different query projections</li>
<li>speeds up autoregressive decoding where queries</li>
</ul>
</li>
<li><a href="https://arxiv.org/pdf/2104.09864.pdf">RoPE Embeddings</a>:
<ul>
<li>want relative position info in query-value dot-product</li>
<li>use multiplicative rotational matrix mixing pairwise neighboring dimensions</li>
<li>improves performance on long sequences</li>
</ul>
</li>
</ul>
<h2 id="palm-training-dataset">PaLM Training Dataset</h2>
<ul>
<li>780B tokens of high-quality text, (<a href="https://vaclavkosar.com/ml/massivetext-dataset-pretraining-deepminds-gopher">5T token MassiveText</a>, GPT-3 300B tokens, MT-NLG 339B)</li>
<li>social media 50%, webpages 27%, books 13%, wikipedia 4%, code 5%, news 1%</li>
<li>based on dataset used for LaMBDA, GLaM</li>
<li>private non-reproducible dataset, while MT-NLG 339B is reproducible but non-hosted</li>
</ul>
<p><img src="https://vaclavkosar.com/images/PaLM-dataset-hierarchical-topics.png" alt="PaLM dataset hierarchical topics"/></p>
<h2 id="palm-training-requirements">PaLM Training Requirements</h2>
<ul>
<li>~17 TB of RAM, 2.5 yottaFLOPS (\( 10^{24} \)) needed for training</li>
<li>2 TPU v4 Pod clusters connected via data center network</li>
<li>mind of PalM is shattered across many chips (cheaper, replaceable, cooling)</li>
<li>each Pod ~1 exaflop/s: 768 hosts, 3072 TPU v4 chips</li>
<li>but how to split the work and communicate?</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-pathways-system-datacenter-pods-hosts-TPU-chips.png" alt="Pathways system datacenter network, tpu "/></p>
<h2 id="parallel-training-of-large-scale-models">Parallel Training of Large Scale Models</h2>
<ul>
<li>parallel computing trades off compute (“time”), memory (“space”), communication throughput (no cool name)</li>
<li>data parallelism - batches are divided between workers</li>
<li>tensor model parallelism
<ul>
<li>splits model layers i.e. transformer block into attention heads and feedforward</li>
</ul>
</li>
<li>pipeline mechanism (<a href="https://arxiv.org/pdf/2201.11990.pdf">Megatron-Turing (MT-LNG) Microsoft and NVidia</a>)
<ul>
<li>computation DAG into stages e.g. layers</li>
<li>stages exchange forward and backward propagation information (micro-batches)</li>
<li>step by step passing causes “bubbles” - idling</li>
</ul>
</li>
</ul>
<p><img src="https://vaclavkosar.com/images/nvidia-data-center-dgx-ai.jpg" alt="NVIDIA data center DGX AI"/></p>
<h2 id="palm-training-infrastructure">PaLM Training Infrastructure</h2>
<ul>
<li>PaLM uses pipeline-free 2D parallelism</li>
<li>data parallel across 2 clusters (2 TPU v4 Pods)</li>
<li>each cluster (Pod) has full model copy
<ul>
<li>model partitioned into 12 parts</li>
<li>data partitioned into 256 parts</li>
<li>768 hosts connected to 3k chips which are interconnected</li>
</ul>
</li>
<li>each batch triggers update between clusters to identical models
<ul>
<li>each host exchanges 1.3GB with its counterpart</li>
</ul>
</li>
</ul>
<p><img src="https://vaclavkosar.com/images/palm-pathways-system-datacenter-pods-hosts-TPU-chips.png" alt="Pathways system datacenter network, tpu "/></p>
<h2 id="training-efficiency">Training Efficiency</h2>
<ul>
<li>observed throughput relative to the theoretical maximum of a system</li>
<li>price for parallelization of PaLM is 50%</li>
<li>in case of PaLM throughput is tokens-per-second</li>
</ul>
<p><img src="https://vaclavkosar.com/images/PaLM-model-flop-utilization-vs-megatron-vs-gopher-gpt-3.png" alt="Model FLOPs utilization of PaLM vs Megatron-Turing NLG vs Gopher vs GPT-3"/></p>
<h2 id="more-posts-like-this">More Posts Like This</h2>
<ul>
<li><a href="https://vaclavkosar.com/ml/SRU++-Speeds-Up-Transformer-with-Simple-Recurrent-Unit-RNN">SRU++ Model Speeds Up Transformer with Simple Recurrent Unit</a></li>
<li><a href="https://vaclavkosar.com/ml/DeepMinds-RETRO-Transformer-Model">DeepMind’s RETRO Transformer Model</a></li>
</ul>

<h5>Let&#39;s connect</h5>


<nav>
<ul>

<li><a href="https://vaclavkosar.com/cdn-cgi/l/email-protection#debfbab3b7b09ea8bfbdb2bfa8b5b1adbfacf0bdb1b3"></a></li>
<li><a href="https://twitter.com/vackosar/"></a></li>
<li><a href="https://www.youtube.com/c/VaclavKosar"></a></li>
<li><a href="https://www.linkedin.com/in/vaclav-kosar/"></a></li>
<li><a href="https://github.com/vackosar"></a></li>
<li><a href="https://www.facebook.com/vackosar"></a></li>
<li><a href="https://vaclavkosar.com/feed.xml" target="_blank" title="Follow on RSS"></a></li>
<li><a href="https://vaclavkosar.com/donate"></a></li>
</ul>
</nav>


<h2 id="appendix">Appendix</h2>
<h3 id="gpt-3-explanations-full-prompts">GPT-3 Explanations Full Prompts</h3>
<h4 id="logical-inference-1">Logical Inference 1</h4>
<div><div><pre><code>I will explain how to make logical inferences:
(1) John wakes to the sound of his children giggling outside the door. They bust in and say &#34;Happy Father&#39;s Day!&#34; with a plate full of food. 2022 has been a great year. Can we infer that John was born before the inauguration of Barack Obama?
Answer: We can infer that John is a father because of &#34;his children&#34;. We can infer that the current year is 2022. Barack Obama was inaugurated in 2009, so it has been about 13 years since Obama was inaugurated. John&#39;s children are old enough to speak, so if John were born after Obama&#39;s inauguration, he would have been younger than 10 when he had children. So, we can infer that John was born before Obama&#39;s inauguration. The answer is &#34;yes&#34;.
(2) Sam sees a piano in the subway stations. Laughing, his friends prod him to play, saying &#34;Remember how amazing your rendition of Chopsticks was at the Christmas party?&#34; Can we infer that John majored in piano at Juilliard?
Answer: Juilliard is a top school for music, so someone who majored in piano at Juilliard would be an expert pianist. &#34;Chopsticks&#34; is a very simple song to play on the piano. His friends were therefore likely being sarcastic when they told him to play, which would imply that Sam was not an expert piano player. So it is unlikely that Sam majored in piano at Juilliard. The answer is &#34;no&#34;.
(3) Jennifer looked out her window and sees a really cool cloud below her. She unbuckles her seatbelt and heads to the bathroom. Is Jennifer probably traveling more than 300 miles per hour relative to the earth?
Answer:
</code></pre></div></div>
<p>Best of 2 temperature 0.7 otherwise defaults:</p>
<ul>
<li>We can infer that Jennifer is on an airplane because she is looking out a window and sees a cloud below her. We can also infer that she is traveling at a high speed because she is on an airplane. The average cruising speed of an airplane is about 550 miles per hour, so Jennifer is probably traveling more than 300 miles per hour relative to the earth. The answer is “yes”.</li>
<li>We can infer that Jennifer is in an airplane because she is looking out a window. We can also infer that she is high up in the sky because she can see a cloud “below” her. Clouds are usually found at altitudes above 3,000 feet. Finally, we know that airplanes typically travel around 500 miles per hour. So, Jennifer is probably traveling more than 300 miles per hour, but we cannot be certain. The answer is “yes”.</li>
</ul>
<p>Greedy:</p>
<ul>
<li>No, because an airplane typically travels around 500-600 miles per hour, and the bathroom is usually located in the back of the plane.</li>
</ul>
<h4 id="logical-inference-2">Logical Inference 2</h4>
<div><div><pre><code>I will explain how to make logical inferences:
(1) John wakes to the sound of his children giggling outside the door. They bust in and say &#34;Happy Father&#39;s Day!&#34; with a plate full of food. 2022 has been a great year. Can we infer that John was born before the inauguration of Barack Obama?
Answer: We can infer that John is a father because of &#34;his children&#34;. We can infer that the current year is 2022. Barack Obama was inaugurated in 2009, so it has been about 13 years since Obama was inaugurated. John&#39;s children are old enough to speak, so if John were born after Obama&#39;s inauguration, he would have been younger than 10 when he had children. So, we can infer that John was born before Obama&#39;s inauguration. The answer is &#34;yes&#34;.
(2) Sam sees a piano in the subway stations. Laughing, his friends prod him to play, saying &#34;Remember how amazing your rendition of Chopsticks was at the Christmas party?&#34; Can we infer that John majored in piano at Juilliard?
Answer: Juilliard is a top school for music, so someone who majored in piano at Juilliard would be an expert pianist. &#34;Chopsticks&#34; is a very simple song to play on the piano. His friends were therefore likely being sarcastic when they told him to play, which would imply that Sam was not an expert piano player. So it is unlikely that Sam majored in piano at Juilliard. The answer is &#34;no&#34;.
(3) Shelley is from Virginia, but is visiting that city with that famous market where they throw the fish. Going home next Tuesday!
Question: Is it likely that Shelley will be near the Pacific Ocean this weekend?
Answer: 
</code></pre></div></div>
<p>Best of 2, temperature 0.7 otherwise defaults:</p>
<ul>
<li>It is not likely that Shelley will be near the Pacific Ocean this weekend. Shelley is from Virginia, which is on the East Coast of the United States. The city with the famous fish market is most likely Seattle, which is also on the West Coast. Shelley is visiting, so she is not from there. She is going home next Tuesday, so she will likely be going back to Virginia this weekend.</li>
<li>It is not likely that Shelley will be near the Pacific Ocean this weekend. Shelley is from Virginia, which is on the East Coast of the United States. The city with the famous fish market is most likely Seattle, which is also on the West Coast. Shelley is visiting, so she is not from there. She is going home next Tuesday, so she will likely be going back to Virginia this weekend.</li>
</ul>
<p>Greedy:</p>
<ul>
<li>It is not likely that Shelley will be near the Pacific Ocean this weekend. Shelley is from Virginia, which is on the East Coast of the United States. The city with the famous fish market is most likely Seattle, which is also on the West Coast. Shelley is visiting, so she is not from there. She is going home next Tuesday, so she will likely be going back to Virginia this weekend.</li>
</ul>
<h4 id="jokes">Jokes</h4>
<div><div><pre><code>I will explain these jokes:
(1) The problem with kleptomaniacs is that they always take things literally.
Explanation: This joke is wordplay. Someone who &#34;takes things literally&#34; is someone who doesn&#39;t fully understand social cues and context, which is a negative trait.
But the definition of kleptomania is someone who literally takes things.
(2) Always borrow money from a pessimist. They’ll never expect it back.
Explanation: Most people expect you to pay them back when you borrow money, however a pessimist is someone who always assumes the worst, so if you borrow money from them, they will expect that you won&#39;t pay them back anyways.
(3) I was going to fly to visit my family on April 6th. My mom said &#34;Oh great, your step dad&#39;s poetry reading is that night!&#34; So now I&#39;m flying in on April 7th.
Explanation:
</code></pre></div></div>
<p>Best of 2 temperature 0.7 otherwise defaults:</p>
<ul>
<li>This joke is about how annoying mothers can be. The speaker is trying to visit their family, but their mother has just informed them that their step-dad’s poetry reading is on the same night. So now the speaker has to fly in on the next day.</li>
<li>This joke is about how annoying it can be to have to deal with family obligations. The person telling the joke is trying to visit their family on April 6th, but their mom tells them that their step dad’s poetry reading is that night. So the person has to fly in on April 7th instead.</li>
<li>This joke is about how annoying and inconsiderate step-parents can be. The punchline is that the person is flying in a day later just to avoid their step-dad’s poetry reading.</li>
</ul>
<p>Greedy:</p>
<ul>
<li>This joke is about how annoying and intrusive in-laws can be. The person is trying to visit their family, but their mother says that their step-dad’s poetry reading is that night. So the person is now flying in the next day to avoid it.</li>
</ul>
</div></div>
  </body>
</html>
