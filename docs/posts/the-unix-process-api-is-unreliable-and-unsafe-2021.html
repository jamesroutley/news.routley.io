<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://catern.com/process.html">Original</a>
    <h1>The Unix process API is unreliable and unsafe (2021)</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgead2b0b">1. It&#39;s easy for processes to leak</a>
<ul>
<li><a href="#orgc4abc73">1.1. Flawed solutions</a>
<ul>
<li><a href="#orgef39bb0">1.1.1. Make sure B always cleans up on exit and kills C</a></li>
<li><a href="#org19f1dea">1.1.2. Use <code>PR_SET_PDEATHSIG</code> to kill C when B exits.</a></li>
<li><a href="#orge65da16">1.1.3. Always write down the pid of every process you start, or otherwise coordinate between A and B</a></li>
<li><a href="#org99f45bb">1.1.4. A should run B inside a container</a></li>
<li><a href="#org28b742d">1.1.5. Use process groups or controlling terminals</a></li>
<li><a href="#org3293a65">1.1.6. Use Windows 8 nested job objects</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2dbcddb">2. It&#39;s impossible to prevent malicious processes leaks</a>
<ul>
<li><a href="#org29abbe4">2.1. Flawed solutions</a>
<ul>
<li><a href="#org14223e2">2.1.1. Run your possibly-malicious process inside a container or a virtual machine</a></li>
<li><a href="#org3ecf768">2.1.2. Limit the number of processes that can exist on the system</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5580c5d">3. Processes have global, reusable IDs</a>
<ul>
<li><a href="#orgf161d1f">3.1. Flawed solutions</a>
<ul>
<li><a href="#orgf170f3f">3.1.1. Don&#39;t reuse pids, use a UUID instead</a></li>
<li><a href="#orgbf3ce6d">3.1.2. Only send signals to your own child processes</a></li>
</ul>
</li>
<li><a href="#orgc687196">3.2. Correct solutions</a>
<ul>
<li><a href="#org3162996">3.2.1. Use pidfd</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5effd82">4. Process exit is communicated through signals</a>
<ul>
<li><a href="#org182c1ce">4.1. Flawed solutions</a>
<ul>
<li><a href="#org511fc15">4.1.1. Use signalfd</a></li>
<li><a href="#org1255706">4.1.2. Chain signal handlers</a></li>
<li><a href="#orged5d727">4.1.3. Create a standard library for starting children and have everyone use it</a></li>
</ul>
</li>
<li><a href="#org0cdd7dd">4.2. Correct solutions</a>
<ul>
<li><a href="#org682bb9b">4.2.1. Use pidfd</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org68de120">5. How to fix all these problems</a>
<ul>
<li><a href="#org92651b0">5.1. Problem: It&#39;s easy for processes to leak</a></li>
<li><a href="#org52507d2">5.2. Problem: It&#39;s impossible to prevent malicious processes leaks</a></li>
<li><a href="#org19095f6">5.3. Problem: Processes have global, reusable IDs</a></li>
<li><a href="#org5faf9f6">5.4. Problem: Process exit is communicated through signals</a></li>
</ul>
</li>
<li><a href="#org1a3bd92">6. How to really fix all these problems in the long term</a></li>
</ul>
</div>
</div>
<p>
It is difficult to write software which reliably and safely manages processes on Unix;
this is because of flaws in the API.
I know of no Unix system which has solved these problems,
but in this article I&#39;ll be specifically talking about the Linux process API.
</p>

<p>
When I say &#34;process API&#34;, I am talking about the interface
for starting processes,
monitoring processes until they exit,
and stopping processes that haven&#39;t yet exited.
</p>

<p>
I won&#39;t cover here the various issues with <code>fork</code>;
that has already been discussed enough <a href="https://www.microsoft.com/en-us/research/publication/a-fork-in-the-road/">elsewhere</a>.
</p>

<p>
So what is so bad about all the other parts of the Linux process API?
</p>

<ol>
<li>It&#39;s easy for started processes to accidentally &#34;leak&#34;,
and linger on the system without anyone responsible for killing them.</li>
<li>Conversely, it&#39;s difficult or impossible to guarantee
that a buggy or malicious process has not &#34;leaked&#34; some processes.</li>
<li>Rather than use secure identifiers for processes (such as a file descriptor),
global process identifiers (pids) <a href="http://www.cap-lore.com/CapTheory/KK/Bind.html">separate names from authority</a>,
making it possible to confuse one process for another.</li>
<li>Process events are communicated using signals to the parent process,
making libraries (which can&#39;t safely handle signals) unable to spawn processes,
as well as inheriting all the other problems of Unix signals.</li>
</ol>

<p>
I&#39;ll explain each problem, as well as some possible solutions that are ultimately flawed.
At the end of the article, I&#39;ll go over my own solution and how it solves all of these problems.
</p>

<p>
But first, some motivation.
Why should we care about flaws in the Linux process API?
Things work well enough right now, don&#39;t they?
</p>

<p>
Yes, there are many tools like process supervisor daemons and Linux containers which fix parts of the process API.
All of these tools so far are flawed in one way or another, as I&#39;ll explain,
but they&#39;re still in wide use and they are adequate solutions when used carefully.
</p>

<p>
The real problem is the significant complexity cost of working around the flaws of the process API.
The aforementioned tools have many features and complex interfaces,
and implementing their workarounds yourself can lead to even more complexity.
At the moment, any system that tries for reliability must pay this complexity cost.
But if the native process API was not so flawed,
no such complex hacks would be needed,
and <a href="http://catern.com/supervisors.html">our systems would be both simpler and more robust</a>.
</p>
<div id="outline-container-orgead2b0b">
<h2 id="orgead2b0b"><span>1</span> It&#39;s easy for processes to leak</h2>
<div id="text-1">
<p>
What do I mean by a &#34;leaked process&#34;?
</p>

<p>
I mean a process that is running without any other entity on the system which is both:
</p>
<ol>
<li>Responsible for killing the process, and</li>
<li>Knows the identity of the process and can kill it precisely with no risk of collateral damage</li>
</ol>

<p>
As soon as a process is orphaned (that is, its parent dies),
it&#39;s leaked,
because only the parent of a process is able to safely kill it.
Merely knowing the pid is insufficient to safely kill a process,
as we&#39;ll discuss in the section on process ids.
So while an orphaned process might have some other entity on the system with property 1,
that entity can never satisfy property 2.
</p>

<p>
Once some processes have been leaked, you have two options:
</p>
<ul>
<li>Hope that they will exit on their own at some point.</li>
<li>Look at the list of processes, fire off some signals, and hope you didn&#39;t just kill the wrong thing.</li>
</ul>

<p>
Neither is particularly sastisfying, so we&#39;d hope that it&#39;s hard to make an orphaned process.
But, unfortunately, it&#39;s quite easy.
</p>

<p>
A process is orphaned when its parent process exits.
If process A starts process B, which starts process C,
and then process B exits,
process C will be orphaned.
</p>

<p>
This is as simple as:
</p>
<div>
<pre>sh -c <span>&#39;{ sleep inf &amp; } &amp;&#39;</span>
</pre>
</div>
<p>
&#39;sh&#39; is our process A;
it forks off another copy of itself to perform the outer &#39;&amp;&#39;, which is our process B;
then &#39;sleep inf&#39; is our process C.
</p>

<p>
The parent process B is able to robustly track the lifetime of its child process C,
through the mechanisms Linux provides for parent processes,
and ensure that process C exits.
If and when C is orphaned, that mechanism is no longer easily usable;
it can be used by the init process, but that&#39;s not typically accessible to us.
</p>
</div>
<div id="outline-container-orgc4abc73">
<h3 id="orgc4abc73"><span>1.1</span> Flawed solutions</h3>

<div id="outline-container-orgef39bb0">
<h4 id="orgef39bb0"><span>1.1.1</span> Make sure B always cleans up on exit and kills C</h4>
<div id="text-1-1-1">
<p>
B should just always make sure to kill process C before it exits.
That way no processes will be orphaned, so no processes will be leaked, and we&#39;ll be fine.
</p>

<p>
Well, what are the possible ways B might exit and need to clean up?
</p>

<p>
B might choose to exit, possibly by throwing an exception or panicking.
In those cases, it&#39;s possible for B to kill process C immediately before exiting.
</p>

<p>
Or B might receive a signal.
B might be signaled for conventional reasons,
such as a user pressing Ctrl-C,
in which case B can still clean up, as long as the programmer or runtime take care to catch every kind of signal.
</p>

<p>
Or B might be signaled for some more unconventional reasons,
such as a segmentation fault.
It&#39;s still possible for B to clean up in this case, but it may be very tricky to do,
and the programmer or runtime may need to take great care
to make sure that the pid of C is still accessible even while handling a segfault.
</p>

<p>
Or B might receive SIGKILL.
Unfortunately, this case prevents this solution from working.
It&#39;s not possible for B to clean up when it receives SIGKILL,
so C will be unavoidably leaked.
</p>

<p>
We might want to say, &#34;never send SIGKILL&#34;.
But that is impossible, both for a conventional reason and an ironic reason.
The conventional reason is that B might have a bug, and hang, and SIGKILL might be the only way to kill it.
The ironic reason is that the only way for B to clean up and exit in guaranteed finite time is for it to SIGKILL its own children,
so that if they have bugs they will not just hang forever.
So B would be SIGKILL&#39;d by its own parent, implementing the same strategy.
</p>

<p>
So, in summary, it&#39;s not possible to guarantee that B cleans up and kills C when it exits,
because it might be SIGKILL&#39;d.
Even in the case where B isn&#39;t SIGKILL&#39;d,
it&#39;s tricky for a complicated program to always make sure to kill off any child processes when it exits.
</p>
</div>
</div>
<div id="outline-container-org19f1dea">
<h4 id="org19f1dea"><span>1.1.2</span> Use <code>PR_SET_PDEATHSIG</code> to kill C when B exits.</h4>
<div id="text-1-1-2">
<p>
We can use the Linux-specific feature <code>PR_SET_PDEATHSIG</code> on process C,
to ensure that process C will receive SIGKILL (or another signal) whenever process B exits for any reason,
including if process B exits uncleanly due to a bug or SIGKILL.
</p>

<p>
The issue is that this only works one level down the tree.
If C forks off its own process D,
the death signal will kill off C but not D.
</p>

<p>
Extending it to work over an entire tree of processes,
requires that the entire tree be using <code>PR_SET_PDEATHSIG</code> (and using it correctly).
If we can make that guarantee, this technique will work.
But in practice,
most large systems can&#39;t make that guarantee since they are made up of a large number of programs from many different developers.
As one specific example, many applications run subcommands through Unix shells, which don&#39;t use <code>PR_SET_PDEATHSIG</code>.
</p>

<p>
Even in smaller systems where we control all involved programs,
this technique isn&#39;t perfect, since even programs we control can always have bugs and fail to use <code>PR_SET_PDEATHSIG</code>.
We&#39;d prefer a guarantee that relies only on the program at the root of the tree,
and doesn&#39;t require us to reason about and debug all the programs involved.
</p>
</div>
</div>
<div id="outline-container-orge65da16">
<h4 id="orge65da16"><span>1.1.3</span> Always write down the pid of every process you start, or otherwise coordinate between A and B</h4>
<div id="text-1-1-3">
<p>
B could make sure to always write down the pid of every process it starts,
so that we can at least make an attempt to kill any orphaned processes,
even if that attempt isn&#39;t robust.
More generally, B could coordinate with A, and somehow tell A about every process B starts.
Then A (which we might trust to be correctly implemented) can handle cleaning up the processes that B starts.
This will fail if there&#39;s a bug in B, or if B is killed just after starting a process but before telling A,
but perhaps it&#39;s good enough?
</p>

<p>
This has the same flaw as <code>PR_SET_PDEATHSIG</code>,
in that it only allows for avoiding leaks at a single level.
Like <code>PR_SET_PDEATHSIG</code>,
all programs involved would need to use our mechanism.
And that&#39;s infeasible in practice in any large system.
</p>
</div>
</div>
<div id="outline-container-org99f45bb">
<h4 id="org99f45bb"><span>1.1.4</span> A should run B inside a container</h4>
<div id="text-1-1-4">
<p>
If A runs B inside a Linux container technology,
such as a Docker container,
then no matter how many processes B starts,
A will be able to terminate them all by just stopping the container, and we&#39;ll be fine.
</p>

<p>
Ignoring the other merits of containers,
if we&#39;re trying to solve the problem of &#34;it is too easy for processes to leak&#34;,
containers have three main flaws.
</p>

<ol>
<li><p>
It&#39;s not easy to run a container.
Python has a &#34;subprocess.run&#34; function in its standard library,
for starting a subprocess.
Python has no &#34;container.run&#34; function in its standard library,
to start a subprocess inside a container,
and in the current container landscape that seems unlikely to change.
</p>

<p>
Shell scripts make starting processes trivial,
but it&#39;s almost unthinkable that, say, bash, would integrate functionality for starting containers,
so that every process is started in a container.
Leaving aside the issues of which container technology to use,
it would be quite complex to implement.
</p></li>
<li>Containers require root or running inside a user namespace.
The root requirement obviously can&#39;t be satisfied by most users.
Fortunately, it&#39;s possible to start a container without being root by using user namespaces.
Unfortunately, user namespaces introduce a number of quirks, such as breaking gdb (by breaking ptrace),
so they also can&#39;t be used by most users.</li>
<li>It&#39;s pretty heavyweight to require literally every child process to run in a separate container.
Robust usage of pid namespaces (the relevant part of Linux containers)
requires that we start up an init process for each pid namespace,
separate from the other processes running in the container.
This init process will do nothing but increase the load on the system,
and it will prevent us from directly monitoring the started processes.</li>
</ol>

<p>
So, running every child process in a separate container isn&#39;t a viable solution.
We still have no way to easily prevent child processes from leaking.
</p>
</div>
</div>
<div id="outline-container-org28b742d">
<h4 id="org28b742d"><span>1.1.5</span> Use process groups or controlling terminals</h4>
<div id="text-1-1-5">
<p>
Process groups and controlling terminals are two features
which can be used to terminate a group of processes.
Such a group of processes is usually called a &#34;job&#34;,
since Unix shells use these features and use that terminology.
When processes start children,
they start out in the same job,
and they can all be terminated at once.
So if process A put process B in a job,
process A could avoid process C leaking by terminating the job.
</p>

<p>
Unfortunately, neither of these job mechanisms is nestable.
If a process puts itself or its children into a new process group or gives itself a new controlling terminal,
it completely replaces the old process group or controlling terminal.
So that process will no longer be terminated when its original job is terminated!
</p>

<p>
In other words, if process A puts process B in a job,
then process B puts process C in a job,
then process B neglects to terminate process C,
process C will no longer be in the job that process A knows about,
so process C will leak!
</p>

<p>
So, ironically, if a child process tries to use these features to prevent its own child processes from leaking,
it can inadvertantly cause them to leak.
This is certainly unsuitable.
</p>
</div>
</div>
<div id="outline-container-org3293a65">
<h4 id="org3293a65"><span>1.1.6</span> Use Windows 8 nested job objects</h4>
<div id="text-1-1-6">
<p>
Windows 8 added support for nested job objects.
Child processes (and all their transitive children) can be associated with a job,
and they will all be terminated when the owner of the job exits (or deliberately kills them).
Child processes can create their own jobs and assign their own children to those jobs,
without interfering with or being aware of their parent job.
</p>

<p>
Unfortunately, we&#39;re using Linux, not Windows. :)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org2dbcddb">
<h2 id="org2dbcddb"><span>2</span> It&#39;s impossible to prevent malicious processes leaks</h2>
<div id="text-2">
<p>
What&#39;s a &#34;malicious process leak&#34;?
</p>

<p>
Well, if a &#34;process leak&#34; is a process existing on the system without someone knowing to kill it,
a &#34;malicious process leak&#34; is a process existing on the system and actively evading being killed.
</p>

<p>
A process can fork repeatedly to make a thousand copies of itself,
or just fork constantly at all times, leaving the previous processes to immediately exit,
so that its pid is constantly changing and the latest copy can&#39;t be identified and sent a signal.
A &#34;fork bomb&#34; is one example of an attack of this kind.
</p>

<p>
But note that this doesn&#39;t have to be the result of an attack;
simple buggy code can cause this.
If you ever program using fork(),
you could easily start forking repeatedly just from a bug.
</p>

<p>
We would like to be able to put in a moderate amount of work to completely prevent this kind of issue.
Unfortunately, completely preventing this from happening is very difficult,
maybe even impossible.
</p>
</div>
<div id="outline-container-org29abbe4">
<h3 id="org29abbe4"><span>2.1</span> Flawed solutions</h3>

<div id="outline-container-org14223e2">
<h4 id="org14223e2"><span>2.1.1</span> Run your possibly-malicious process inside a container or a virtual machine</h4>
<div id="text-2-1-1">
<p>
If we run our possibly-malicious process inside a container or virtual machine,
then no matter how much it forks and exits,
we will be able to terminate the process by just stopping the container (or virtual machine).
</p>

<p>
This will actually work, to a degree.
Most of our earlier concerns (it&#39;s too hard and too heavyweight)
no longer apply,
because in this section we&#39;re happy to have any means at all to prevent the attack.
</p>

<p>
However,
it still requires root access (or the use of unprivileged user namespaces)
to a run a container or a virtual machine.
So this solution is not truly general purpose;
we can&#39;t use this routinely, every time we create a child process,
because our application certainly should not run with root access in the normal case,
nor can everything run in an unprivileged user namespace.
</p>

<p>
We can partially get around the need for root access
by having a privileged daemon start processes on our behalf inside a container.
</p>

<p>
systemd, for example, with its <a href="https://www.freedesktop.org/software/systemd/man/systemd-run.html">systemd-run</a> API, allows us to request that systemd start up a process for us on the fly.
systemd runs every process in a separate cgroup (which is the underlying container mechanism that we would use),
so it can protect against the malicious process leak problem.
</p>

<p>
But having someone else start a process on our behalf breaks a lot of traditional Unix features.
For example, we can&#39;t easily have our child process inherit stdin/stdout/stderr from us,
nor will it inherit environment variables or any ulimits we&#39;ve placed on ourself.
The shell, among other applications, is completely dependent on these features.
</p>

<p>
Also, this privileged daemon centralizes all the processes we start on the system.
We can&#39;t, say, set up a truly isolated environment for development or integration testing,
because we&#39;ll still have to go through the central daemon.
</p>

<p>
So as a general-purpose mechanism,
this is not workable,
but it can work in certain constrained scenarios.
</p>
</div>
</div>
<div id="outline-container-org3ecf768">
<h4 id="org3ecf768"><span>2.1.2</span> Limit the number of processes that can exist on the system</h4>
<div id="text-2-1-2">
<p>
What if we limit the number of processes that can exist on the system?
Then as the process keeps forking,
it will eventually exhaust the available process space and stop,
and in that frozen moment of tranquility,
an already-started process would be able to kill it.
</p>

<p>
The number of processes that can exist is actually already limited;
there&#39;s a maximum pid, and we can&#39;t have any more processes than that.
The issue is that as processes exit,
possibly due to being killed by us,
their space is usually freed up,
and new processes can be created.
</p>

<p>
So if the malicious process just keeps forking,
it can fill up the space left by previous processes exiting,
and this doesn&#39;t help us.
Stricter limits on the number of processes can prevent fork bombs,
but not more general attacks.
</p>

<p>
However, if we could prevent space from being freed up as processes exit,
the space that malicious process has to operate in would shrink and shrink,
until finally it is no longer able to fork any more, and we can kill the last copy.
Preventing the reuse of process space while under possible attack
can be done using a technique that I&#39;ll discuss at the end of this article.
It&#39;s a key part of a robust solution to the process leaking problem.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5580c5d">
<h2 id="org5580c5d"><span>3</span> Processes have global, reusable IDs</h2>
<div id="text-3">
<p>
A process is identified using its &#39;pid&#39;.
A pid is an integer, frequently in the range 1 to 65536,
which is selected for the process at startup from the pool of currently unused pids,
and which is relinquished back into that pool when the process exits.
</p>

<p>
There is a single pool of process IDs on the system.
If enough processes are started and exit,
a process ID will be reused.
</p>

<p>
Pids are mainly used to send signals to processes with the &#34;kill&#34; system call
(which is used for any kind of signal, not just lethal ones).
</p>

<p>
Typically, a long-lived process (a &#34;daemon&#34;) would write its own pid into a file, called a &#34;pidfile&#34;.
Then other processes could send signals to the daemon by reading that pidfile and using &#34;kill&#34;.
</p>

<p>
But there is absolutely no guarantee that when you &#34;kill&#34;, you are sending a signal to the right process.
If the daemon has exited,
and enough processes have started and stopped since then,
the pid in the daemon&#39;s pidfile might point to a completely unrelated process.
You might send a fatal signal to something critically important instead of the daemon you meant to send it to!
</p>

<p>
You might try checking the command that the pid is running, or other details about the process,
before killing the process.
But that&#39;s no guarantee either - between the time you check and the time you signal,
the process may have died and been replaced.
</p>

<p>
Fundamentally, any usage of a pid (other than for your own child processes)
is vulnerable to a <a href="https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use">time-of-check-to-time-of-use</a> race condition.
Since pids are the only way to identify a process,
this means any interaction with processes other than your own child processes is inherently racy.
</p>
</div>
<div id="outline-container-orgf161d1f">
<h3 id="orgf161d1f"><span>3.1</span> Flawed solutions</h3>

<div id="outline-container-orgf170f3f">
<h4 id="orgf170f3f"><span>3.1.1</span> Don&#39;t reuse pids, use a UUID instead</h4>
<div id="text-3-1-1">
<p>
The kernel could identify processes with some kind of truly globally unique identifier.
Then users wouldn&#39;t have race conditions when they try to kill them.
Note that users can&#39;t implement this solution in userspace by tagging the processes with a UUID;
it must be a kernel-provided a UUID to avoid the <a href="https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use">TOCTOU</a> issues.
</p>

<p>
This would work,
but it would be difficult to retrofit onto an existing Unix system:
Many applications assume that pids are the same size as 16-bit or 32-bit ints.
</p>

<p>
We would also pay an efficiency cost, just because of handling a larger identifier.
It would be unusual for an operating system to provide references to its internal structures with UUIDs,
when it can use more efficient smaller identifiers and provide security through other means.
</p>
</div>
</div>
<div id="outline-container-orgbf3ce6d">
<h4 id="orgbf3ce6d"><span>3.1.2</span> Only send signals to your own child processes</h4>
<div id="text-3-1-2">
<p>
When process A starts process B, and then process B exits, process A is notified.
Furthermore, process B leaves a &#34;zombie process&#34; behind after it exits,
which consumes the pid until process A explicitly acts to get rid of the zombie process.
These two features allow process A to know exactly when it is safe to send signals to B&#39;s pid.
So if A stays running for as long as B is running,
and only A sends signals to B,
we can have signals without races.
</p>

<p>
This works, and is an excellent replacement for pidfiles.
But it doesn&#39;t work in all situations.
</p>

<p>
What if process A exits unexpectedly?
Then we are back in the situation of not being able to kill process B without a race condition.
Furthermore, what if we genuinely want process B to outlive process A?
This is the case whenever we are starting a long-lived process (a daemon), for example.
</p>

<p>
To support this, instead of forking off a process,
process A could send a request to a long-lived supervisor daemon to start process B, as the supervisor daemon&#39;s own child.
The authors of supervisor daemons such as <code>systemd</code> or <code>supervisord</code>
<a href="https://jdebp.uk/FGA/unix-daemon-design-mistakes-to-avoid.html">often</a> <a href="https://www.freedesktop.org/software/systemd/man/daemon.html">urge</a> software developers not to fork off their own long-lived processes;
instead, say the supervisor daemon authors,
we should request that the supervisor daemon fork off long-lived processes on our behalf.
</p>

<p>
Unfortunately, that has the same issues as discussed in the section on preventing malicious process leaks,
where we considered having a privileged daemon create containers on our behalf.
We can&#39;t easily have our child process inherit stdin/stdout/stderr from us,
nor will it inherit environment variables or any ulimits we&#39;ve placed on ourself.
The shell, among other applications, is completely dependent on these features.
And this daemon centralizes the processes we start on the system,
so it&#39;s difficult to set up isolated test or development environments.
</p>

<p>
Furthermore, even if we have a supervisor daemon starting processes on our behalf,
this leaves a static parent-child hierarchy which cannot change.
The supervisor daemon cannot, for example, start a new version of itself to upgrade,
without careful use of exec,
as all of its child processes will stop being its children.
Nor can process A initially start up process B as process A&#39;s child,
and then later decide that process B should live past process A&#39;s exit.
</p>

<p>
What we need is a way to send signals without races, without forcing a specific parent-child hierarchy.
If we can make the parent-child hierarchy more flexible,
it would work well.
We will use this technique in combination with others as part of a full solution at the end of this article.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc687196">
<h3 id="orgc687196"><span>3.2</span> Correct solutions</h3>

<div id="outline-container-org3162996">
<h4 id="org3162996"><span>3.2.1</span> Use pidfd</h4>
<div id="text-3-2-1">
<p>
pidfd was added to Linux in 2019,
and solves the issues with using pids to identify processes.
<a href="https://lwn.net/Articles/794707/">Read the LWN summary for more information</a>.
</p>

<p>
It allows us to break the rigid parent-child hierarchy,
and replace it with a more flexible supervision hierarchy,
based on passing file descriptors around.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5effd82">
<h2 id="org5effd82"><span>4</span> Process exit is communicated through signals</h2>
<div id="text-4">
<p>
Process exit is communicated to the parent of a process by SIGCHLD.
If process A starts process B, and then process B exits,
process A will be sent the SIGCHLD signal.
</p>

<p>
Signals are delivered to the entire process, and only one signal handler can be registered for each signal.
</p>

<p>
So if the main function in process A registers a signal handler for SIGCHLD,
and library L1 in process A starts a process B, when process B exits,
the signal handler of the main function in process A will receive a notification of the exit of a child it never started,
and the library will never be told that its child has exited.
</p>

<p>
Conversely, if the library L1 registers the signal handler,
and the main function or even another library L2 starts a process B,
then only L1 will be notified when the process exits.
</p>

<p>
In general, only one part of the program can directly receive signals.
That one part of the program then must forward the signal around to whatever other components desire to receive signals.
If a library has no interface for receiving signal information,
like glibc,
then it can&#39;t use child processes.
This is a major inconvenience for both the library developer and the user.
</p>
</div>
<div id="outline-container-org182c1ce">
<h3 id="org182c1ce"><span>4.1</span> Flawed solutions</h3>

<div id="outline-container-org511fc15">
<h4 id="org511fc15"><span>4.1.1</span> Use signalfd</h4>
<p>
While signalfd is certainly a great help in dealing with signals on Linux,
it doesn&#39;t actually help deal with the problem of libraries receiving SIGCHLD.
You could use signalfd to wait for the SIGCHLD signals,
but you still then need to forward the signals to each library.
</p>
</div>
<div id="outline-container-org1255706">
<h4 id="org1255706"><span>4.1.2</span> Chain signal handlers</h4>
<div id="text-4-1-2">
<p>
Can&#39;t we just have one library&#39;s signal handler call the next library&#39;s signal handler?
</p>

<p>
Rather than explain in this article,
I refer the reader to <a href="https://www.macieira.org/blog/2012/07/forkfd-part-2-finding-out-that-a-child-process-exited-on-unix/">here</a> where it&#39;s explained that signal handler chaining can&#39;t be done robustly.
Libraries have high standard for working, even in strange scenarios!
</p>
</div>
</div>
<div id="outline-container-orged5d727">
<h4 id="orged5d727"><span>4.1.3</span> Create a standard library for starting children and have everyone use it</h4>
<div id="text-4-1-3">
<p>
The issue is that multiple libraries want to handle the task of starting and monitoring children.
Can&#39;t we just agree on a single standard library that abstracts over SIGCHLD,
and have everyone use it?
We can provide a file descriptor interface, which is increasingly standard on Linux,
and is easy for libraries to use and monitor.
</p>

<p>
Unfortunately,
it would be near impossible to get every other library that wants to use subprocesses or wants to listen for SIGCHLD
to use this single standard library.
</p>

<p>
There are already plenty of libraries which provide wrappers around SIGCHLD/fork/exec,
and plenty of code that depends on them.
We can&#39;t just have a flag day and switch everything over to a new library all at once.
This becomes even more tricky in high-level languages,
because most languages already come with a higher-level API around spawning processes.
</p>

<p>
Still, the idea of providing a file descriptor interface for starting and monitoring children is a good one.
File descriptors can easily be integrated into an event loop.
And a file descriptor can be monitored by a library without interfering with the rest of the program,
using a library&#39;s own private event loop or other mechanisms.
We just need a way to provide that interface that does not interfere with other libraries in the same process.
</p>
</div>
</div>
</div>
<div id="outline-container-org0cdd7dd">
<h3 id="org0cdd7dd"><span>4.2</span> Correct solutions</h3>

<div id="outline-container-org682bb9b">
<h4 id="org682bb9b"><span>4.2.1</span> Use pidfd</h4>
<div id="text-4-2-1">
<p>
pidfd allows a library to launch a process and get a pidfd back,
then use that pidfd to monitor the process.
</p>

<p>
A file descriptor can be easily integrated into an event loop,
as mentioned in the previous section.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org68de120">
<h2 id="org68de120"><span>5</span> How to fix all these problems</h2>
<div id="text-5">
<p>
<a href="https://lwn.net/Articles/794707/">pidfd</a> is a great solution to the third and fourth problems,
but it doesn&#39;t solve the first two problems.
</p>

<p>
I only know one existing solution that fixes all these problems
without sacrificing flexibility or generality.
</p>

<p>
Use the C utility <a href="https://github.com/catern/supervise">supervise</a> to start your processes;
for Python, you can use its associated <a href="https://github.com/catern/supervise/blob/master/python/supervise_api/supervise.py">Python library</a>.
</p>

<p>
Essentially, we delegate the problem of starting and monitoring child processes
to a small helper program: supervise.
And it abstracts away the fixes for these problems behind a nicer (but still low-level) interface.
For a high-level interface, one can use the Python library.
</p>
</div>
<div id="outline-container-org92651b0">
<h3 id="org92651b0"><span>5.1</span> Problem: It&#39;s easy for processes to leak</h3>
<div id="text-5-1">
<p>
Solution: supervise kills all your descendant processes when you exit.
</p>

<p>
supervise is passed a file descriptor to read instructions from on startup,
and monitors that fd throughout its (short and simple) lifetime.
When the parent process exits,
the fd will be closed,
supervise will be notified,
supervise will kill the descendant processes,
and then supervise will also exit.
</p>

<p>
But because process lifetime is tied to the lifetime of a fd,
it&#39;s still easy to create long-lived processes if you wish;
just make sure the fd outlives your own process.
</p>

<p>
supervise is able to find all descendant processes by using <code>PR_SET_CHILD_SUBREAPER</code>, a Linux-specific feature.
If process A starts process B which starts process C,
and process B exits,
then if process A has set <code>PR_SET_CHILD_SUBREAPER</code> then process A will become the new parent of process C.
This allows supervise to safely find and kill all descendant processes.
</p>
</div>
</div>
<div id="outline-container-org52507d2">
<h3 id="org52507d2"><span>5.2</span> Problem: It&#39;s impossible to prevent malicious processes leaks</h3>
<div id="text-5-2">
<p>
Solution: supervise kills all your descendant processes when you exit, securely and in a guaranteed-to-terminate way.
</p>

<p>
It does this using the technique mentioned in the &#34;Limit the number of processes that can exist on the system&#34; section.
If we don&#39;t free up pid space as a malicious process forks and exits,
eventually the pid space will be exhausted and the malicious process can be cornered and killed.
</p>
</div>
</div>
<div id="outline-container-org19095f6">
<h3 id="org19095f6"><span>5.3</span> Problem: Processes have global, reusable IDs</h3>
<div id="text-5-3">
<p>
Solution: supervise gives you a file descriptor interface to signaling a process.
</p>

<p>
To signal the process, you just write to the file descriptor.
File descriptors are local and unforgeable,
so it&#39;s not possible for the file descriptor to suddenly start pointing at a different instance of supervise,
wrapping a different process.
</p>

<p>
All the descendant processes of supervise will at some point become its direct children,
thanks to <code>PR_SET_CHILD_SUBREAPER</code>,
so it can safely send them all signals using &#34;kill&#34; and cause them to exit,
so a supervision hierarchy can be maintained without forcing any specific organization.
</p>

<p>
And just like all file descriptors, the supervise file descriptors can be inherited by children or passed over Unix sockets.
This allows a supervision hierarchy to be rearranged at runtime,
rather than forcing a static parent-child hierarchy.
</p>
</div>
</div>
<div id="outline-container-org5faf9f6">
<h3 id="org5faf9f6"><span>5.4</span> Problem: Process exit is communicated through signals</h3>
<div id="text-5-4">
<p>
Solution: supervise gives you a file descriptor interface to monitor a process for exit.
</p>

<p>
In addition to the file descriptor that supervise reads instructions from,
supervise also accepts a file descriptor to write status changes to.
You can read and monitor this file descriptor to receive notification of process status changes. 
</p>
</div>
</div>
</div>
<div id="outline-container-org1a3bd92">
<h2 id="org1a3bd92"><span>6</span> How to really fix all these problems in the long term</h2>
<div id="text-6">
<p>
Of course, supervise is not a long-term solution.
Running an additional helper process for every real process you start is an annoying,
if slight,
inconvenience and performance loss.
The correct long-term solution is to actually get this functionality into the Linux kernel.
</p>

<p>
pidfd is the obvious basis for a solution,
since it solves two out of this four problems.
We just need to add a way to fix the process leaking issues.
</p>

<p>
The best way to approach this would be to tie the lifetime of the process
to the lifetime of the pidfds pointing to it,
as <a href="https://www.freebsd.org/cgi/man.cgi?query=pdfork&amp;sektion=2">pdfork</a> does when <code>PD_DAEMON</code> is not set.
This could be done with a new <code>CLONE_TERMINATE_ON_CLOSE</code> flag.
Then when all the pidfds pointing to a process are closed,
the process is killed.
This happens naturally when the parent of the process
(and any other processes that the parent sent the pidfd to)
lose interest in the process.
</p>

<p>
This still allows the process to fork off and leak processes of its own;
that could be addressed by various means,
perhaps by using a seccomp filter to force the process to only start processes using <code>CLONE_TERMINATE_ON_CLOSE</code>,
or by <a href="https://lore.kernel.org/lkml/87shf0b33q.fsf@xmission.com/T/#m6dbb73b6012c2d5354c162d4125974f0ade4da6d">adding a new prctl that is an inheritable variant of <code>PR_SET_PDEATHSIG</code></a>.
Using a pid namespace is a major hassle at the moment since it requires a dedicated init process,
but it could also form the basis of a solution if that requirement was removed.
</p>

<p>
Hopefully Linux gets these features soon.
In the meantime, supervise provides equivalent functionality in userspace.
</p>
</div>
</div>
</div></div>
  </body>
</html>
