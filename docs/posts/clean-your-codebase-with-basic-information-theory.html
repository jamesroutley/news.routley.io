<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://taylor.town/compress-code">Original</a>
    <h1>Clean your codebase with basic information theory</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>
<a href="https://www.reddit.com/r/coolguides/comments/c3qzow/phrases_to_make_your_essay_longer/"> <img src="https://i.redd.it/bug9o4g3fx531.jpg" alt=""/>
</a></p>

<blockquote>
<p>
Cut out everything that’s not surprising. </p>
<p>
– <a href="https://sive.rs/d22">Derek Sivers</a> </p>
</blockquote>

<h2 id="surprise">
Surprise</h2>
<p>
The following equation measures
<a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">“surprise”</a>:</p>
<p>
<a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29"> <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ff26f81edc1f4bb204793a52b2430c77f6633203" alt=""/>
</a></p>
<p>
It’s easy to estimate the surprise of text files:</p>
<pre><code>const rx = /[\s,\]\[\(\)]+/g;
const counts = {};
for (const file of Deno.args)
  for (const x of (await Deno.readTextFile(file)).split(rx))
    counts[x] = 1 + counts[x] || 1;
const total = Object.values(counts).reduce((a, b) =&gt; a + b, 0);
let entropy = 0;
for (const c of Object.values(counts)) {
  const p = c / total;
  entropy -= p * Math.log2(p);
}
console.log(entropy);</code></pre>
<p>
Note: this script calculates surprise at the level of <em>words</em>. You can do
similar estimates at the character level, but it’s not as useful for improving
readability.</p>
<p>
Running the script on a few random markdown files:</p>
<pre><code>$ ./entropy src/enough.md
6.371408749206546

$ ./entropy src/two-toucans-canoe.md
6.452173798618295

$ ./entropy src/enough.md src/two-toucans-canoe.md
7.1796868604897295

$ ./entropy src/music.md
8.172144309119338

$ ./entropy src/pardoned.md
8.180170381663272</code></pre>
<p>
<a href="https://taylor.town/enough">/enough</a>, <a href="https://taylor.town/two-toucans-canoe">/two-toucans-canoe</a>, <a href="https://taylor.town/music">/music</a>,
<a href="https://taylor.town/pardoned">/pardoned</a></p>
<p>
If you look at the examples, you’ll see that entropy generally increases with
file size, excluding files with a lot of repetition, e.g.
<a href="https://taylor.town/music">my music ratings</a>.</p>
<h2 id="compression">
Compression</h2>
<p>
To compress text at the word-level, substitute repeated phrases with shorter
phrases, e.g. replace “for the purpose of” with “for”. Note that replacing
“gargantuan” with “big” may improve surprise at the character-level, but not
necessarily at the word-level.</p>
<p>
This is a consequence of
<a href="https://en.wikipedia.org/wiki/Shannon&#39;s_source_coding_theorem">Shannon’s source coding theorem</a>.
The theorem also provides calculable upper-bounds for text compression.</p>
<p>
In theory, you could use something like
<a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a> to shrink the
size of your code.
<a href="https://en.wikipedia.org/wiki/Minification_%28programming%29">Minifiers</a> use
related strategies to produce unreadable-yet-valid gibberish.</p>
<p>
You can also
<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression">estimate upper-bounds for compressed programs</a>.
Implement something like <code>gzip</code> in a target language and extract the program
with its decompressor to a new file. The number of bits in the
<a href="https://en.wikipedia.org/wiki/Self-extracting_archive">self-extracting archive</a>
approximates the size of the smallest possible program.</p>
<p>
In practice, compressing programs for humans means replacing larger snippets
with smaller functions, variables, etc.</p>
<p>
Here’s an example from a real-world Elm file with 2.2k LOC:</p>
<pre><code>$ ./entropy BigProject.elm
8.702704507769093

# `Css.hex &#34;#ddd&#34;` -&gt; `ddd`
$ sed -i &#39;s/Css.hex &#34;#ddd&#34;/&#34;ddd&#34;/g&#39; BigProject.elm

# less repetition creates more surprise
$ ./entropy BigProject.elm
8.706398741147163</code></pre>
<p>
How would this number change over the life of a project? I’d like a script to
compare the total entropy of all files in a repo at each release and throw it in
a chart. Please email me at <a href="https://taylor.town/cdn-cgi/l/email-protection#5c34393030331c283d2530332e7228332b32"><span data-cfemail="a8c0cdc4c4c7e8dcc9d1c4c7da86dcc7dfc6">[email protected]</span></a> if you
have any implementation suggestions.</p>
<p>
To find compression candidates, look for frequent sequences of words, i.e.
<a href="https://taylor.town/code-collocates">code collocates</a>.</p>
<p>
Although it’s possible to automate long-sequence identification, I prefer to
list frequent words and then compress in conceptual chunks. This script
calculates frequencies at the word-level, but also weights words based on their
character length:</p>
<pre><code>const rx = /[\s,\]\[\(\)]+/g;
const len = {};
for (const file of Deno.args)
  for (const x of (await Deno.readTextFile(file)).split(rx))
    len[x] = x.length + len[x] || x.length;
console.log(
  Object.entries(len)
    .filter(a =&gt; a[1] &gt;= 10 &amp;&amp; a[0].length &gt; 2)
    .sort((a, b) =&gt; b[1] - a[1])
    .map(a =&gt; `${a[1]} ${a[0]}`)
    .join(&#34;\n&#34;)
);</code></pre>
<p>
Running the script on my Elm project:</p>
<pre><code>$ ./freq-lengths BigProject.elm
1467 Attrs.css
1295 Css.rem
732 Css.fontSize
616 Html.div
594 Html.text
570 Css.backgroundColor
455 Css.hex
391 Maybe.withDefault
360 import
354 Css.px
336 Css.fontWeight
322 Css.lineHeight
304 Css.borderRadius
297 Css.color
297 Html.span
273 Css.marginTop
266 Nothing
252 String
231 Css.int
225 Css.solid</code></pre>
<p>
Based on these results, my first instinct is to pull out some of the CSS stuff
into helper functions, especially for font-related styling. <em>C’est la vie.</em></p>
<h2 id="readability">
Readability</h2>
<p>
<a href="https://codegolf.stackexchange.com">Code golf</a> is entertaining, but my boss
would be pissed to find a program like
<a href="https://codegolf.stackexchange.com/a/117845">“t,ȧṫÞċḅ»Ḳ“¡¥Ɓc’ṃs4K€Y</a> in our
repo.</p>
<p>
While
<a href="https://en.wikipedia.org/wiki/Readability#Popular_Readability_Formulas">readability formulas exist for text</a>,
“readable code” remains controversial.</p>
<p>
In my experience, the key to maintaining readability is developing a healthy
respect for locality:</p>
<ol>
<li>
coarsely structure codebases around CPU timelines and dataflow </li>
<li>
don’t pollute your namespace – <a href="https://taylor.town/code-blocks">use blocks</a> to restrict
variables/functions to the smallest possible scope </li>
<li>
group related concepts together </li>
</ol>
<p>
The hardest part of this process is deciding what “related concepts” mean. For
example, should you group all CSS in one file, or should you group styles next
to their HTML structures? Should you group all your database queries together,
or inline them where they’re used?</p>
<p>
I don’t have all the answers, but it’s abundantly clear that we have a
<a href="https://www.youtube.com/watch?v=kZRE7HIO3vk">thirty million line problem</a> and
that
<a href="https://youtu.be/ubaX1Smg6pY?si=OA3djEHTEywrW_oU&amp;t=746">complex ideas fit on t-shirts</a>.
When I see
<a href="https://youtu.be/ubaX1Smg6pY?t=3183">an entire compositor and graphics engine in 435 LOC</a>,
I expect many surprises from <a href="https://futureofcoding.org">the future of coding</a>.</p>
</div></div>
  </body>
</html>
