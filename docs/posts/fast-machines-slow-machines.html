<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jmmv.dev/2023/06/fast-machines-slow-machines.html">Original</a>
    <h1>Fast machines, slow machines</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><p>Well, <em>that</em> was unexpected. I recorded a couple of crappy videos in 5 minutes, <a href="https://twitter.com/jmmv/status/1671670996921896960">posted them on a Twitter thread</a>, and went viral with 8.8K likes at this point. I really could not have predicted that, given that I‚Äôve been posting what-I-believe-is interesting content for years and‚Ä¶ nothing, almost-zero interest. Now that things have cooled down, it‚Äôs time to stir the pot and elaborate on those thoughts a bit more rationally.</p><p>To summarize, the Twitter thread shows two videos: one of an old computer running Windows NT 3.51 and one of a new computer running Windows 11. In each video, I opened and closed a command prompt, File Explorer, Notepad, and Paint. You can clearly see how apps on the old computer open up instantly whereas apps on the new computer show significant lag as they load. I questioned how computers are actually getting better when trivial things like this have regressed. And boom, the likes and reshares started coming in. Obviously some people had issues with my claims, but there seems to be an overwhelming majority of people that agree we have a problem.</p><p>To open up, I‚Äôll stand my ground: latency in modern computer interfaces, with modern OSes and modern applications, is terrible and getting worse. This applies to smartphones as well. At the same time, while UIs were much more responsible on computers of the past, those computers were also awful in many ways: new systems have changed our lives substantially. So, what gives?</p><p>Let‚Äôs address the elephant in the room first. The initial comparison I posted <em>wasn‚Äôt</em> fair and I was aware of that going in. That said, I <em>knew</em> repeating the experiment ‚Äúproperly‚Äù would yield the same results, so I plowed ahead with whatever I had right then. The videos were unplanned because the idea for the Tweets came to mind when I booted the old machine, clicked on Command Prompt, and was blown away by the immediacy to start the app.</p><p>The original comparison videos showed:</p><ul><li><p>An AMD K7-600 with 128MB of RAM and a 5400 RPM HDD running Windows NT 3.51. This was a machine from the year 1999-2000 with an OS that was about 5 years older than it. Hardware was experiencing really fast improvements back then, particularly in CPU speeds, and you were kinda expected to keep up with the 2-year upgrade treadmill or suffer from incredibly slowness. All this is to say that this machine was indeed overpowered for the OS I used.</p><blockquote data-conversation="none"><p lang="en" dir="ltr">Please remind me how we are moving forward. In this video, a machine from the year ~2000 (600MHz, 128MB RAM, spinning-rust hard disk) running Windows NT 3.51. Note how incredibly snappy opening apps is. üëá <a href="https://t.co/YEO824vIqI">pic.twitter.com/YEO824vIqI</a></p>‚Äî Julio Merino (@jmmv) <a href="https://twitter.com/jmmv/status/1671670996921896960?ref_src=twsrc%5Etfw">June 22, 2023</a></blockquote></li><li><p>A Surface Go 2 with an Intel Core m3 CPU, 8GB of RAM, and an SSD running Windows 11. This is a 3-year old machine that shipped with Windows 10, but Windows 11 is officially supported on it‚Äîand as you know, that means you are tricked into upgrading. This is <em>not</em> a powerful machine by any means, but: first, it‚Äôs running the verbatim Microsoft experience, and second, it <em>should</em> be much more powerful than the K7 system, shouldn‚Äôt it? We are continuously reminded that any computer or phone today has orders of magnitude more power than past machines.</p><blockquote data-conversation="none"><p lang="en" dir="ltr">Now look at opening the same apps on Windows 11 on a Surface Go 2 (quad-core i5 processor at 2.4GHz, 8GB RAM, SSD). Everything is super sluggish. <a href="https://t.co/W722PNEGv0">pic.twitter.com/W722PNEGv0</a></p>‚Äî Julio Merino (@jmmv) <a href="https://twitter.com/jmmv/status/1671671000730316800?ref_src=twsrc%5Etfw">June 22, 2023</a></blockquote><p>Oh, and yes, I quoted the wrong hardware specs in the original tweet. Looking again on how I made that mistake: I searched for ‚ÄúSurface Go 2‚Äù in Bing, I landed on the ‚ÄúSurface <em>Laptop</em> Go 2‚Äù page, and copied what I saw there without noticing that it wasn‚Äôt accurate.</p></li></ul><p>All apps had been previously open, so they should all have been comfortably sitting in RAM.</p><p>Obviously various people noticed that there was something off with my comparison (unfair hardware configurations, wrong specs), so I redid the comparison once the thread started gaining attention:</p><ul><li><p>Windows 2000 on the K7-600 machine (see <a href="https://twitter.com/jmmv/status/1672046102923837440">installation thread</a>). This is an OS from 1999 running on hardware from that same year. And, if you ask me, this was the best Windows release of all times: super-clean UI on an NT core, carrying all of the features you would want around performance and stability (except with terrible boot times). As you can see, things still fare <em>very</em> well for the old machine in terms of UI responsiveness.</p><blockquote data-conversation="none"><p lang="en" dir="ltr">For those thinking that the comparison was unfair, here is Windows 2000 on the same 600MHz machine. Both are from the same year, 1999. Note how the immediacy is still exactly the same and hadn‚Äôt been ruined yet. <a href="https://t.co/Tpks2Hd1Id">pic.twitter.com/Tpks2Hd1Id</a></p>‚Äî Julio Merino (@jmmv) <a href="https://twitter.com/jmmv/status/1672073678102872065?ref_src=twsrc%5Etfw">June 23, 2023</a></blockquote></li><li><p>Windows 11 on a Mac Pro 2013 (see <a href="https://jmmv.dev/2022/03/windows-10-mac-pro-2013.html">installation instructions</a>) with a 6-core Xeon E5-1650v2 at 3.5GHz, 32GB of RAM, dual GPUs, and an SSD that can sustain up to 1GB/s. I know, this is a 10-year old machine at this point running a more modern OS. But please, go ahead, tell me with a straight face how hardware with these specs cannot handle opening trivial desktop applications without delay. I‚Äôll wait.</p><blockquote data-conversation="none"><p lang="en" dir="ltr">Oh, and one more thing. Yes, yes, the Surface Go 2 is underpowered and all you want. But look at this video. Same steps on a 6-core Mac Pro @ 3.5GHz with 32GB of RAM. All apps cached. Note how they get painted in chunks. It&#39;s not because of animations or mediocre hardware. <a href="https://t.co/9TOGAdaTXO">pic.twitter.com/9TOGAdaTXO</a></p>‚Äî Julio Merino (@jmmv) <a href="https://twitter.com/jmmv/status/1672385064490127360?ref_src=twsrc%5Etfw">June 23, 2023</a></blockquote><p>The reason I used the Mac Pro is because it is the best machine I have running Windows right now and, in fact, it‚Äôs my daily driver. But again, I do not care about how running this comparison on an ‚Äúold‚Äù machine might be ‚Äúinaccurate‚Äù. Back when <a href="https://jmmv.dev/2022/10/bye-microsoft-hi-snowflake.html">I left Microsoft last year</a>, I was regularly using a Z4 desktop from 2022, a maxed-out quad-core i7 ThinkPad with 32GB of RAM, and an i7 Surface Laptop 3 with 16GB of RAM. Delays were shorter on these, of course, but interactions were still noticeably slow.</p></li></ul><p>So, in any case: I agree the original comparison was potentially flawed, but as you can see, a better comparison yields the same results‚Äîwhich I knew it would. After years upon years of computer usage, you gain intuition on how things should behave, and trusting such intuition tends to work well as long as you <em>validate</em> your assumptions later, don‚Äôt get me wrong!</p><p>Let‚Äôs put the tweets aside and talk about how things have changed since the 2000s. I jokingly asked how we are ‚Äúmoving forward‚Äù as an industry, so it‚Äôs worth looking into it.</p><p>Indeed, we <em>have</em> moved forward in many aspects: we now have incredible graphics and high-resolution monitors, super-fast networks, real-time video editing, and much more. All of these have improved over the years and it is very true that these advancements have allowed for certain life transformations to happen. Some examples: the ability to communicate with loved ones much more easily thanks to great-quality videoconferencing; the ability to have a streaming ‚Äúcinema at home‚Äù; and the painless switch to remote work during the pandemic<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p><p>We have also moved forward on the I/O side. Disk I/O had always been the weakest spot on past systems. Floppy disks were unreliable and slow. CDs and DVDs were slightly-more-reliable but also slow. HDDs were the bottleneck for lots of things: their throughput improved over time, allowing things like higher-resolution video editing and the like, but random I/O hit physical limits‚Äîand fast random I/O is what essentially drives desktop responsiveness.</p><p>Then, boom, SSDs appeared and started showing up on desktops. These were a game-changer because they fixed the problem of random I/O. All of a sudden, booting a computer, launching a heavy game, opening folders with lots of small photos, or simply just using your computer‚Ä¶ all improved massively. It‚Äôs hard to explain the usability improvements that these brought if you did not live through this transition, and it‚Äôs scary how those improvements are almost gone; more on that later.</p><p>Other stuff also improved, like the simplicity to install new hardware, the pervasiveness of wireless connections and devices, the internationalization of text and apps (Unicode isn‚Äôt easy nor cheap, I‚Äôll grant that)‚Ä¶ all providing more usable machines in more contexts than ever.</p><p>So yeah, things are better in many areas and we have more power than ever. Otherwise, we couldn‚Äôt do things like ML-assisted photo processing on a tiny phone, which was unimaginable in the 2000s.</p><p>Yet‚Ä¶ none of these advancements justify why things are as excruciatingly slow as they are today in terms of UI latency. Old hardware from the year 1999, combined with an OS from that same year, shows that responsive systems have existed<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. If anything, all these hardware improvements I described should have made things <em>better</em>, not worse, shouldn‚Äôt they?</p><p>Some replied to the comparison telling me that graphical animations and bigger screens are ‚Äúat fault‚Äù because we have to draw more pixels, and thus the fact that we have these new niceties means we have to tolerate slowness. Well, not quite. Witness for yourself:</p><blockquote data-conversation="none"><p lang="en" dir="ltr">And... one more thing? To those saying: &#34;it&#39;s the higher 4K resolution!&#34; or &#34;it&#39;s the good-looking animations!&#34; or &#34;it&#39;s the pretty desktop background!&#34;‚Äîno, they aren&#39;t at fault. See, the slowness is still visible with all of these disabled. In the end... blog post coming soon. <a href="https://t.co/9BQy6IpK6a">pic.twitter.com/9BQy6IpK6a</a></p>‚Äî Julio Merino (@jmmv) <a href="https://twitter.com/jmmv/status/1673326555702120448?ref_src=twsrc%5Etfw">June 26, 2023</a></blockquote><p>GPUs are a commodity now, and they lift the heavy burden of graphics management from the CPU. The kinds of graphical animations that a desktop renders are extremely cheap to compute, and this has been proven by macOS since its launch: all graphical effects on a macOS desktop feel instant. The effects <em>do</em> delay interactions though‚Äîthe desktop switching animation is particularly intrusive, oh god how I hate that thing‚Äîbut the delays generally come from intentional pauses during the animation. And when the effects introduce latency because the GPU cannot keep up, such as when you attach a 4K display to a really old Mac, then it‚Äôs painfully obvious that animations stutter due to lack of power. I haven‚Äôt encountered the latter in any of the videos above though, which is why animations and the like have nothing to do with my concerns.</p><p>So, please, think about it with a critical mind. How is the ability to edit multiple 4K video streams in real time or the ability to stream a 4K movie supposed to make starting apps like Notepad slower? Or opening the context menu in the desktop? Or reading your email? The new abilities we acquired much more power from the CPU and GPU, but they shouldn‚Äôt remove performance from tasks that are essentially I/O-bound. Opening a simple app shouldn‚Äôt be slower than it was more than 20 years ago; it really shouldn‚Äôt be. Yet here we are. The reasons for the desktop latency come from elsewhere, and I have some guesses for those. But first, a look at a couple of examples.</p><p>On Windows land, there are two obvious examples I want to bring up and that were mentioned in the Twitter thread:</p><ul><li><p>Notepad had been a native app until very recently, and it still opened pretty much instantaneously. With its rewrite as a UWP app, things went downhill. The before and after are apparent, and yet‚Ä¶ the app continues to be as unfeatureful as it had always been. This is extra slowness for no user benefit.</p></li><li><p>As for Windows Terminal, sure, it is nicer than anything that came before it, but it is visibly much, much heavier than the old Command Prompt. And if you add PowerShell into the mix, we are talking about multiple <em>seconds</em> for a new terminal window to be operational unless you have top-of-the-line hardware.</p></li></ul><p>macOS fares better than Windows indeed, but it still has its issues. See this example contributed by @AlexRugerMusic. Even the mighty M1 has trouble opening up the system settings app:</p><blockquote data-conversation="none"><p lang="en" dir="ltr">Another example:</p>‚Äî rewgs (@AlexRugerMusic) <a href="https://twitter.com/AlexRugerMusic/status/1673550864366309376?ref_src=twsrc%5Etfw">June 27, 2023</a></blockquote><p>Linux is probably the system that suffers the least from these issues as it still feels pretty snappy on modest hardware. Fedora Linux 38, released in April 2023, runs really well on a micro PC from 11 years ago‚Äîeven if Gnome or KDE had been resource hogs back in the day. That said, this is only an illusion. As soon as you start installing any modern app that wasn‚Äôt developed exclusively for Linux‚Ä¶ the slow app start times and generally poor performance show up.</p><p>Related, but I feel this needs saying: the biggest shock for me was when I joined Google back in 2009. At the time, Google Search and GMail had stellar performance: they were examples to follow. From the inside though‚Ä¶ I was quite shocked by how all internal tools crawled, and in particular by how slow the in-house command line tools were. I actually fault Google for the situation we are in today due to their impressive internal systems and their relentless push for web apps at all costs, which brings us to‚Ä¶</p><p>How does this all happen? It‚Äôs easy to say ‚ÄúBloat!‚Äù, but that‚Äôs a hard thing to define because bloat can be justified: what one person considers as bloat is not the same as what another person considers as bloat. After all, ‚Äú80% of users only use 20% of the software they consume‚Äù (see <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a>), but the key insight is that the 20% that each user consumes is different from one another. So bloat isn‚Äôt necessarily in the features offered by the software; it‚Äôs elsewhere.</p><p>So then we have frameworks and layers of abstraction, which seem to introduce bloat for bloat‚Äôs sake. But I‚Äôm not sure this is correct either: abstraction doesn‚Äôt inherently have to make things slower, as Rust has proven. What makes things slower are priorities. Nobody prioritizes performance anymore unless for the critical cases where it matters (video games, transcoding video, and the like). What people (companies) prioritize is developer time. For example: you might not want to use Rust because its steep learning curve means you‚Äôll spend more time learning than delivering, or its higher compiler times mean that you‚Äôll spend more time waiting for the compiler than <del>shipping</del> debugging production. Or another example: you might not want to develop native apps because that means ‚Äúduplicate work‚Äù, so you reach out for a cross-platform web framework. That is, Electron.</p><p>I know it‚Äôs easy to dunk on Electron, but there are clear telltale signs that this platform is at fault for a lot of the damage done to desktop latency. Take 1Password‚Äôs 8th version, which many of users that migrated from the 7th version despise due to the slowness of the new interface. Or take Spotify, which used to prioritize startup and playback latency over anything else in its inception and, as you know if you use it, that‚Äôs not true any more:</p><blockquote><p lang="en" dir="ltr">2009 version: 20MB fully native cocoa app, launches in less than a second, instant feedback when you click stuff, playback usually starts within 50ms <a href="https://t.co/Enzi40PDCX">pic.twitter.com/Enzi40PDCX</a></p>‚Äî Rasmus Andersson (@rsms) <a href="https://twitter.com/rsms/status/1656340616731840513?ref_src=twsrc%5Etfw">May 10, 2023</a></blockquote><p>These apps were rewritten in Electron to offer a unified experience across desktops and to cut down costs‚Ä¶ but for whom? The cost cuts were for the companies owning the products, not for the users. Such cuts impose a tax on <em>every one of us</em> due to our day-to-day frustrations and the need to unnecessarily upgrade our hardware. Couple these rewrites with the fact that OSes cannot reuse the heavy framework across apps (same idea as how <a href="https://jmmv.dev/2021/08/using-all-memory-as-a-cache.html">using all RAM as a cache</a> is a flawed premise)‚Ä¶ and the bloat quickly adds up when you run these apps concurrently.</p><p>Leaving Electron aside, another decision that likely introduces latency is the mass adoption of managed and interpreted languages. I know these are easy to dunk on as well, but that‚Äôs because we have reasons to do so. Take Java or .NET: several Windows apps have been slowly rewritten in C# and, while I have no proof of this, I‚Äôm convinced from past experience that this can be behind the sluggishness we notice. The JDK and the CLR do amazing jobs at optimizing long-running processes (their <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a> can do <a href="https://en.wikipedia.org/wiki/Profile-guided_optimization">PGO</a> with real time data), but handling quick startup times is not something they manage well. This is why, for example, Bazel spawns a background server process to paper over startup latency and why Android has gone through multiple iterations of <a href="https://en.wikipedia.org/wiki/Ahead-of-time_compilation">AOT</a> compilation. (Edit: there must be other reasons though that I have not researched. As <a href="https://news.ycombinator.com/item?id=36508282">someone pointed out</a>, my assumption that Windows Terminal was mostly C# is not true.)</p><p>More on this in <a href="https://en.wikipedia.org/wiki/Wirth%27s_law">Wirth‚Äôs law</a>.</p><p>To conclude, let me end with a pessimistic note by going back to hardware advancements.</p><p>The particular improvement that SSDs brought us was a one-off <em>transformation</em>. HDDs kept getting faster for years indeed, but they never could deliver the kind of random I/O that desktops require to be snappy. The switch to SSDs brought a kind of improvement that was at a different level. Unfortunately‚Ä¶ we could only buy those benefits <em>once</em>: there is no other technology to switch to that provides such a transformative experience. And thus, once the benefits brought by the new technology were eaten away by careless software, we are almost back to square one. Yes, SSDs are getting faster, but newer drives won‚Äôt bring the kind of massive differences that the change from HDDs to SSDs brought.</p><p>You can see this yourself if you try using recent versions of Windows or macOS <em>without</em> an SSD: it is nigh impossible. These systems now assume that computers have SSDs in them, which is a fair assumption, but a problematic one due to what I mentioned above. The same applies to ‚Äúbloat‚Äù in apps: open up your favorite resource monitor, look for the disk I/O bandwidth graph, and launch any modern app. You‚Äôll see a stream of MBs upon MBs being loaded from disk into memory, all of which must complete before the app is responsive. This is the kind of bloat that Electron adds and that SSDs permitted, but that could be avoided altogether with different design decisions.</p><p>Which makes me worried about Apple Silicon. Remember all the rage with the M1 launch and how these new machines had superb performance, extremely long battery life, and no fan noise? Well, wait and see: these benefits will be eaten away if we continue on the same careless path. And once that has happened, it‚Äôll be too late. Retrofitting performance into existing applications is very difficult technically, and almost impossible to prioritize organizationally.</p><p>So‚Ä¶ will computer architects be able to save us with other revolutionary technology shifts? I wouldn‚Äôt want to rely on that. Not because the shifts might not exist, but because we <em>shouldn‚Äôt need them</em>.</p></article></div></div></div>
  </body>
</html>
