<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://victoriametrics.com/blog/the-rise-of-open-source-time-series-databases/index.html">Original</a>
    <h1>The Rise of Open Source Time Series Databases</h1>
    
    <div id="readability-page-1" class="page"><div><p>Time series databases allow you to store and query metrics efficiently. For example, if you want to forecast load on your servers, or identify intermittent faults with your production services, time series databases can help. Besides infrastructure monitoring, time series databases have been invaluable in finance, IoT applications, manufacturing, and more.</p><p>Many time series databases, including VictoriaMetrics, are open source. In this article, you’ll see how time series databases came about, and why so many are open source. We’ll also share our insider take on the future of this space.</p><h2 id="what-is-a-time-series-database">What is a time series database?</h2><p>When people think of databases, they think of relational databases like PostgreSQL and Oracle. Relational databases store data in a table format, where each row contains an instance of the data being described, with each column describing a different aspect of the data. For example, you may have an <code>employees</code> table with <code>name</code>, <code>role</code>, and <code>salary</code> columns. Each row would represent a different employee:</p><table><thead><tr><th>Name</th><th>Role</th><th>Salary</th></tr></thead><tbody><tr><td>Alice</td><td>Charlie</td><td>$100,000</td></tr><tr><td>Bob</td><td>Moral support</td><td>$50,000</td></tr><tr><td>Charlie</td><td>Dog</td><td>$0</td></tr></tbody></table><p>This model works great for many types of data. As long as the amount of data doesn’t get too large, it supports fast querying for specific information and easy updates if the value of a cell changes. However, when data size reaches billions or trillions of rows (typical sizes for time series data), performance degrades.</p><p>A time series database is a database designed specifically for time series data. Time series data is a sequence of measurements, or observations that have an associated timestamp. In a database like VictoriaMetrics, these measurements can also have associated metadata known as labels.</p><p>Time series databases can ingest and query vast amounts of data, and use clever tricks to use fewer resources than a relational database would when storing the same data. Nothing comes for free, of course, and time series databases aren’t suitable for all data types. For example, time series databases:</p><ul><li>Take a long time to update data that has already been stored</li><li>Can only store numbers, not other types of data</li></ul><p>These trade-offs work well for typical time series data, e.g. stock prices, or the number of requests made to a specific endpoint. However, they wouldn’t work for our employee example earlier.</p><h2 id="a-short-history-of-time-series-databases">A short history of time series databases</h2><p>To explain time series databases, we’ll go back to the early 2010s before today’s popular time series databases existed. Back then, infrastructure monitoring would often be real-time-only. If you were lucky, you might have a dashboard showing you the current state of the system you were working on, but historical data was hard to come by. Many problems, such as intermittent faults, could go unnoticed for a long time.</p><p>Tracking the state of your system across time requires storing billions, or even trillions of rows. In the early 2010’s the majority of databases were traditional relational databases, which weren’t designed to handle this volume of data.</p><h3 id="the-first-time-series-database">The first time-series database</h3><p><a href="https://en.wikipedia.org/wiki/InfluxDB">InfluxDB</a> launched in 2013 and was the first mainstream time series database. There were a couple of efforts beforehand, but they were either unknown or intended for niche use cases. Importantly for our story, InfluxDB was, and still is an open source project. Being open source did wonders for the popularity of both InfluxDB and time series databases as a whole.</p><p>As with any piece of software, however, InfluxDB wasn’t perfect. One of the main sources of user complaints was the database’s stability. A search for “<a href="https://www.google.com/search?q=influxdb+crash">influxdb crash</a>” still brings up results full of confused and frustrated users. Worse, every major version release has broken backward compatibility and automatic update tools have been late, missing, or buggy. Version upgrade processes have been time-consuming and frustrating for users.</p><h3 id="prometheus">Prometheus</h3><p>Around the same time InfluxDB was started, engineers at SoundCloud were <a href="https://thenewstack.io/prometheus-at-10-whats-been-its-impact-on-observability/">developing their own</a> in-house infrastructure monitoring system called Prometheus. While InfluxDB aims to cater to all time series use cases, Prometheus is focused explicitly on metrics and event monitoring. For example, Prometheus only supports recording metrics as floating point numbers, while InfluxDB supports many data types.</p><p>With a more narrow focus, Prometheus developed greater stability and efficiency than InfluxDB. It is known for being easier to run, configure, upgrade, and troubleshoot than InfluxDB. Prometheus’ excellent developer experience has made it the de facto observability solution today.</p><h2 id="enter-victoriametrics">Enter VictoriaMetrics</h2><p>While Prometheus is a great product, it’s still possible to reach its performance limits when working with large quantities of data. Before founding VictoriaMetrics, I was working with another of our co-founders, Roman, at an ad-tech company. The systems we worked on served millions of requests per second, providing an observability and analytics challenge.</p><p>Our team were early adopters of Prometheus for observability. It revolutionized our workflow and helped us see more than just the current state of their system, allowing us to see and analyze historical data. Historical observability data is critical for building reliable, performant systems. Prometheus was a big help for us, and we stored so much data in it that we reached its limits.</p><p>At the same time as adopting Prometheus, we migrated from PostgreSQL to ClickHouse for our analytics workloads. ClickHouse’s architecture was very efficient, allowing us to downscale from 12 servers to just one to run their analytics. This got us thinking, “<em>What if we had Prometheus, but with ClickHouse’s architecture?</em>”.</p><p>This question was the birth of VictoriaMetrics, a new time series database that carried on Prometheus’ legacy while adopting some of the designs that made ClickHouse so efficient. For example, VictoriaMetrics:</p><ul><li>Uses advanced compression techniques that use less disk space and less memory</li><li>Stores and processes data in blocks for speed and efficiency</li><li>Makes use of all available CPU cores to maximize performance</li></ul><p>VictoriaMetrics’ heritage has given it outstanding efficiency, simplicity, and performance. Our <a href="https://victoriametrics.com/blog/reducing-costs-p1/">benchmarks</a> show VictoriaMetrics using 2.5x less disk space and servicing queries 16x faster than Prometheus.</p><h2 id="landscape">Landscape</h2><p>InfluxDB, Prometheus, and VictoriaMetrics are just three of the time series databases available today. VictoriaMetrics arrived during a time series database boom. The late 2010s saw many open source time series databases launch, including VictoriaMetrics, TimescaleDB, QuestDB, and more.</p><p>Many other categories of software are dominated by proprietary options. So you may be wondering where are all the proprietary time series databases? The answer is that while there are successful proprietary time series databases, they compete in niche use cases. For example, <a href="https://en.wikipedia.org/wiki/Kdb%2B">kdb+</a> is very popular with high-frequency trading firms.</p><h2 id="the-future-of-time-series-databases">The future of time series databases</h2><p>It’s not easy to predict the future, but looking at the pain points of today can provide some hints on what problems will be solved next. Time series databases today have two well-known pain points—the high cardinality problem, and high time series churn rates.</p><h3 id="high-cardinality">High cardinality</h3><p>The performance of a time series database is usually directly correlated with the number of active time series. For a database like VictoriaMetrics or Prometheus, the number of active time series is a product of the number of unique label combinations. For example, consider the following metric:</p><div><pre tabindex="0"><code data-lang="PromQL"><span><span>requests_total{path<span>=</span>&#34;<span>/</span>&#34;, code<span>=</span>&#34;<span>200</span>&#34;, machine<span>=</span>&#34;<span>vm01</span>&#34;}
</span></span></code></pre></div><p>The above metric counts the total number of requests served for the root path, that returned HTTP 200, on the machine called <code>vm01</code>. Each of <code>path</code>, <code>code</code>, and <code>machine</code> are a <strong>label</strong> — a key-value pair that contains metadata about a metric. This metric would be associated with a single time series.</p><p>Now, imagine that your application serves 1000 paths, might return 20 different HTTP codes from each path, and is served from 100 machines. This means that your monitoring infrastructure would need to deal with 1000 x 20 x 100 = 2,000,000 time series just to count the total number of requests served.</p><p>The number of active time series is referred to as cardinality. <a href="https://victoriametrics.com/blog/categories/high-cardinality/">High cardinality</a>, or many active time series, leads to database slowdowns and failures.</p><p>VictoriaMetrics helps you deal with high cardinality through raw performance — it supports <a href="https://valyala.medium.com/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893">over 10 million</a> active time series on a single machine. That said, it is still easy to accidentally significantly increase cardinality with a simple label change, so this is not a solved problem.</p><h2 id="time-series-churn">Time series churn</h2><p>Tracking infrastructure metrics generally requires recording which machine generated the metric. With a database like VictoriaMetrics, this is done by recording the machine name with a label. Machine names were relatively static when physical machines ran applications, but containers and Kubernetes have changed the field significantly.</p><p>The parallel for a physical machine in Kubernetes is the pod. A pod is a running instance of a container that is uniquely identifiable. Due to their lightweight nature, pods can be created and destroyed much more easily than physical machines can.</p><p>In Kubernetes, a pod restart generates a new name. A new pod name translates into a new label value, invalidating the old time series and creating a new one. Over a year, a typical Kubernetes installation can expect to generate and invalidate <strong>tens of billions</strong> of time series this way. Huge numbers of time series can make it hard to query historical data and slow down your database.</p><p>This problem isn’t as visible as high cardinality. The typical use case for a time series database involves querying hours or days of historical data, reducing the number of time series involved. As users ask more of their time series databases and perform deeper analysis over longer periods, we expect this problem will become more prevalent.</p><h2 id="wide-events">Wide events</h2><p>There has been a lot of buzz lately about the concept of wide events. A wide event is a bit like a structured log entry. It records something happening at a particular time, along with associated attributes in the form of key-value pairs. They are called “wide events”, instead of just “events” because they are supposed to capture all the context around the event, leading to many attributes per-event.</p><p>Wide events are an exciting development for observability, as they can help to replace unstructured logs and traces that are hard to query. We don’t believe wide events will replace metrics as the storage space required for a wide event is orders of magnitude greater than for a metric. A single metric can be stored in VictoriaMetrics in <strong>less than a single byte</strong> thanks to clever compression.</p><p>If you’re interested in using wide events to help enhance your logging, it’s worth checking out VictoriaLogs. VictoriaLogs allows you to store arbitrary key-value pairs for each log entry, which you can then filter on and group by at query time. <a href="https://victoriametrics.com/products/victorialogs/">VictoriaLogs</a> also supports unstructured logs, allowing you to transition at your own pace.</p><h2 id="conclusion">Conclusion</h2><p>Time series databases are essential tools in any software engineer’s toolbelt. Their development has been shaped by user needs and countless open source contributors, leading to the healthy ecosystem of options we see today.</p><p>If you’re curious about time series databases, why not give VictoriaMetrics a go? VictoriaMetrics is incredibly efficient, simple to deploy, and completely open source. Check out our <a href="https://docs.victoriametrics.com/quick-start/">quick start guide</a> or <a href="https://victoriametrics.com/contact-us/">request a demo</a> with our team.</p></div></div>
  </body>
</html>
