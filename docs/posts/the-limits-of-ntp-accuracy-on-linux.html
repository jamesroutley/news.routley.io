<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scottstuff.net/posts/2025/05/19/ntp-limits/">Original</a>
    <h1>The Limits of NTP Accuracy on Linux</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>Lately I’ve been trying to find (and understand) the limits of time
syncing between Linux systems. How accurate can you get? What does it
take to get that? And what things can easily add measurable amounts of
time error?</p>
<p>After most of a month (!), I’m starting to understand things. This is
kind of a follow-on to <a href="https://scottstuff.net/posts/2025/04/10/improving-ntp-accuracy/">a
previous post</a>, where I walked through my setup and goals, plus <a href="https://scottstuff.net/posts/2025/05/15/time-syncing-101/">another
post</a> where I discussed time syncing in general. I’m trying to get
the clocks on a bunch of Linux systems on my network synced as closely
as possible so I can trust the timestamps on <a href="https://opentelemetry.io/docs/concepts/observability-primer/#distributed-traces">distributed
tracing</a> records that occur on different systems. My local network
round-trip times are in the <a href="https://scottstuff.net/posts/2024/12/18/networks-still-awe-me-sometimes/">20–30
microsecond (μS) range</a> and I’d like clocks to be less than 1 RTT
apart from each other. Ideally, they’d be within 1 μS, but 10 μS is
fine.</p>
<p>It’s easy to fire up <a href="https://chrony-project.org/">Chrony</a>
against a local GPS<span><label for="sn-0"></label><span>Technically, <a href="https://en.wikipedia.org/wiki/Satellite_navigation">GNSS</a>,
which covers multiple satellite-backed navigation systems, not just the
US GPS system, but I’m going to keep saying “GPS” for short.</span></span>-backed time source and see it claim to be within X
nanoseconds of GPS, but it’s tricky to figure out if Chrony is right or
not. Especially once it’s claiming to be more accurate than the
network’s round-trip time<span><label for="sn-1"></label><span>20 μS or so.</span></span>, the amount of time needed for a single CPU cache
miss<span><label for="sn-2"></label><span>50-ish nanoseconds.</span></span>, or even the amount of time that light would take to span
the gap between the server and the time source.<span><label for="sn-3"></label><span>About 5 ns per meter.</span></span></p>
<p>I’ve spent <em>way</em> too much time over the past month digging
into time, and specifically the limits of what you can accomplish with
Linux, Chrony, and GPS. I’ll walk through all of that here eventually,
but let me spoil the conclusion and give some limits:</p>
<ul>
<li>GPSes don’t return perfect time. I routinely see up to 200 ns
differences between the 3 GPSes on my desk when viewing their output on
an oscilloscope. The time gap between the 3 sources varies every second,
and it’s rare to see all three within 20 ns of each other. Even the best
GPS timing modules that I’ve seen list ~5 ns of jitter on their
datasheets. I’d be surprised if you could get 3-5 GPS receivers to agree
within 50 ns or so without careful management of consistent antenna
cable length, etc.</li>
<li>Even small amounts of network complexity can easily add 200-300 ns
of systemic error to your measurements.</li>
<li>Different NICs and their drivers vary widely on how good they are
for sub-microsecond timing. From what I’ve seen, <a href="https://www.intel.com/content/www/us/en/products/sku/211608/intel-ethernet-network-adapter-e810xxvda4t/specifications.html">Intel
E810 NIC</a>s are <em>great</em>, Intel X710s are very good, Mellanox
ConnectX-5 are okay, Mellanox ConnectX-3 and <a href="https://scottstuff.net/posts/2025/05/07/connectx-4-and-linuxptp/">ConnectX-4</a>
are borderline, and everything from Realtek is questionable.</li>
<li>A lot of Linux systems are <em>terrible</em> at low-latency work.
There are a lot of causes for this, but one of the biggest is random
“stalls” due to the system’s <a href="https://en.wikipedia.org/wiki/System_Management_BIOS">SMBIOS</a>
running to handle power management or other activities, and “pausing”
the observable computer for hundreds of microseconds or longer. In
general, there’s no good way to know if a given system
(<em>especially</em> cheap systems) will be good or bad for timing
without testing them. I have two cheap mini PC systems that have
inexplicably bad time syncing behavior,<span><label for="sn-4"></label><span>1300-2000 ns.</span></span> and two others with inexplicably <em>good</em> time
syncing<span><label for="sn-5"></label><span>20-50 ns</span></span>. Dedicated server hardware is generally more
consistent.</li>
</ul>
<p>All in all, I’m able to sync clocks to within 500 ns or so on the
bulk of the systems on my network. That’s good enough for my purposes,
but it’s not as good as I’d expected to see.</p>
<p>Now, it’s <em>certainly</em> possible to do better than this in
specific cases. For examples, see <a href="https://chrony-project.org/examples.html">Chrony’s examples
page</a>, where they get &lt;100 ns of error over the network for a
specific test case. <em>In general</em>, though it’s going to be hard to
do much better than 200 ns consistently across a real network without a
lot of careful engineering.</p>
<p>I’ll explain my conclusions in a bit, but first some background and
context.</p>
<h2 id="my-setup">My Setup</h2>
<p>For the sake of testing time, I’m using 8 different (but identical)
servers as time clients and 5 different GPS-backed time sources, all
local.</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/diagram.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/diagram.png"/> </a>
<figcaption>
<small>
<p>Relevant bits of my network for time testing. NTP sources are in blue
circles, the servers tested are the purple rectangle, and network
switches are orange or yellow rectangles.</p>
</small>
</figcaption>
</figure>
<h3 id="time-sources">Time Sources</h3>
<ol type="1">
<li><code>ntp1</code>: an older LeoNTP GPS-backed NTP server. In the
garage, connected to its own outdoor GPS antenna. Only has a 10/100 Mbps
Ethernet connection, but this hasn’t mattered in practice.</li>
<li><code>ntp2</code>: identical hardware to <code>ntp1</code>. Sitting
on my desk and connected to a different Ethernet switch. Connected to a
<a href="">GPS antenna splitter</a> and an outdoor antenna.</li>
<li>My desktop. A 32-core AMD Threadripper 5975WX with a ConnectX-6 NIC
(2x40 Gbps) for network traffic and an Intel E810-XXVDA4T (using 2x10
Gbps, one to each switch) with a GPS receiver and hardware timing
support. Shares the antenna with <code>ntp2</code>, <code>ntp4</code>,
and <code>ntp5</code>.</li>
<li><code>ntp4</code>:<span><label for="sn-6"></label><span>Where is NTP3 you ask? I ran out of antenna ports, and
anyway the system that I dubbed <code>ntp3</code> only supports PTP, not
NTP.</span></span> a Raspberry Pi CM5 with a Timebeat GPS module including
PPS timing straight to the NIC. Connected via 1 GbE.</li>
<li><code>ntp5</code>: a Raspberry Pi CM5 with a Waveshare GPS module
with GPIO PPS but no working Ethernet PPS. Connected via 1 GbE.</li>
</ol>
<h3 id="test-devices">Test Devices</h3>
<ol type="1">
<li>Eight identical servers (<code>d1</code> through <code>d8</code>)
running Ubuntu 24.04 with identical Chrony configs. The servers are <a href="https://www.hpe.com/psnow/doc/c05069171">HPE M510 blades</a> with
16 Xeon-D cores in a pair of <a href="https://www.hpe.com/psnow/doc/c05211200">HPE EL4000
enclosures</a>. Each enclosure is connected to both of the core
switches, giving each of the 8 servers 2 dedicated 10 GbE links via a
built-in Mellanox ConnectX-3 NIC. <a href="https://github.com/SuperQ/chrony_exporter">Chrony metrics</a> are
collected every 10 seconds and stored in <a href="https://prometheus.io/">Prometheus</a> for analysis.</li>
<li>A <a href="https://siglentna.com/product/sds1204x-e-200-mhz/#">Siglent
SDS1204X-E Oscilloscope</a> connected to the PPS outputs from
<code>ntp2</code>, <code>ntp4</code>, and my desktop. It can show
relative differences in PPS times within about a nanosecond.<span><label for="sn-7"></label><span>The oscilloscope only has 200 MHz bandwidth but
captures 1 billion samples per second, so I’d expect it to be able to
show differences between PPS sources to somewhere between 1 and 5
nanoseconds. In any case, the observed differences are much larger than
this, see below.</span></span></li>
</ol>
<h3 id="network">Network</h3>
<p>The core of the network is a pair of <a href="https://www.arista.com/assets/data/pdf/Datasheets/7050QX-32_32S_Datasheet_S.pdf">Arista
7050QX-32S</a> switches. These are 32-port 40 GbE switches with hardware
support for PTP. They’re older, but very solid.</p>
<p>Linux systems with multiple network connections (the 8 test servers
and my desktop) are connected to each with a /30 link per interface and
then run <a href="https://en.wikipedia.org/wiki/Open_Shortest_Path_First">OSPF</a>
with <a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP</a>
to provide redundancy. Devices with a single network interface
(Raspberry Pis and LeoNTP devices in this example) are connected to
layer 2 switches which are then connected to the core switches via <a href="https://en.wikipedia.org/wiki/Multi-chassis_link_aggregation_group">MLAG</a>
links. This means that there are multiple possible paths between any two
devices through the network, as both ECMP and LAG use a mix of source
and destination addresses to decide which link to use. So the path
between <code>d1</code> and <code>ntp1</code> may be almost completely
different from the path between <code>d2</code> and <code>ntp1</code>,
even though <code>d1</code> and <code>d2</code> are sitting less than an
inch from each other and share all of the same physical network links.
Even more entertaining, the path <em>back</em> from <code>ntp1</code> to
<code>d1</code> and <code>d2</code> may or may not be the same as the
forward path. This only matters when nanosecond-level timings are
involved, as we’ll see in a bit.</p>
<h2 id="sources-of-error">Sources of Error</h2>
<p>So — finally — I have multiple NTP servers, presumably synced to GPS
satellites as accurately as possible, and multiple servers, all synced
to the NTP servers over a relatively low-latency network. How accurately
are my servers syncing to GPS time? And where is that going wrong?</p>
<h3 id="chronys-claims">Chrony’s claims</h3>
<p>So, if you’re trying to see how accurate Chrony’s time syncing is,
the easiest place to start is with Chrony’s own metrics. In this case,
Chrony claims that it’s had a median offset of 25–110 ns over the past
day:</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/overall.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/overall.png"/> </a>
<figcaption>
<small>
<p>Chrony’s median offset over the past day.</p>
</small>
</figcaption>
</figure>
<p>Now, this isn’t the best metric for a number of reasons, but it’s a
start. It says that Chrony thinks that it’s synced to within 110 ns of
<em>something</em>, but it doesn’t really tell us anything about what
it’s synced to or how accurate it actually is. So, let’s dig in a bit
deeper.</p>
<h3 id="gps-error-and-drift">GPS error and drift</h3>
<p>First, the GPS receivers in my NTP time servers aren’t perfectly
accurate. Even top-tier GPS receivers will still have ~5 ns of timing
noise, and lower-tier ones will be 20–60 ns (or possibly higher).<span><label for="sn-8"></label><span>Datasheet links: the <a href="https://content.u-blox.com/sites/default/files/ZED-F9T-10B_DataSheet_UBX-20033635.pdf">ublox
ZED-F9T</a> in my desktop claims 5 ns of accuracy and 4 ns of jitter.
The <a href="https://content.u-blox.com/sites/default/files/documents/NEO-LEA-M8T-FW3_DataSheet_UBX-15025193.pdf">ublox
NEO-M8T</a> in NTP5 (not graphed here) claims 20-500ns of accuracy,
depending on the antenna. And <a href="https://www.leontp.com/firmware/LeoNTP-InstructionManual-Latest.pdf">LeoNTP</a>
claims 30ns of RMS accuracy and 60ns of 99-th percentile accuracy.</span></span></p>
<p>Fortunately, this is relatively easy to measure, at least when the
devices are within a few feet of each other. You can connect an
oscilloscope to their PPS outputs and directly view the differences
between them. Here’s the result for <code>ntp2</code>,
<code>ntp4</code>, and my desktop:</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/scope.gif">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/scope.gif"/> </a>
<figcaption>
<small>
<p>Oscilloscope output. The Raspberry Pi/<a href="https://store.timebeat.app/products/open-timecard-mini-essential">Timebeat
Timecard Mini Essential</a> is on top in yellow, then the LeoNTP in
purple, and an Intel E810 on the bottom in blue. Animated; each update
covers 1 second of real time.</p>
</small>
</figcaption>
</figure>
<p>Notice that (a) they don’t all agree and (b) they move around
relative to each other. In this sample, there’s about a 200ns difference
between NTP4 (top, yellow) and my desktop (bottom, blue). Some of this
is due to cable length differences (my antenna and PPS leads aren’t all
identical-length, so there’s probably ~20ns in difference there alone),
but that doesn’t explain all of it.</p>
<p>Even ignoring the NTP4, there’s ~25ns in variance between
<code>ntp2</code> (purple, middle) and my desktop (blue, bottom). Notice
that they move relative to each other over time in a bit of a
pattern.</p>
<p>In general, offsets can <em>mostly</em> be compensated for, either in
Chrony or directly on the GPS device, but jitter is trickier.</p>
<p>Depending on how you look at things, I’m seeing a <em>minimum</em> of
25 ns of error at this level, and potentially up to 200ns.</p>
<p>When you give Chrony multiple time sources that are all equivalently
good, then it’ll generally average its time across the whole set of
sources. So adding one time source with 200 ns of offset to 2 other
mostly-identical time sources should only add ~67 ns of error at most,
and possibly no error at all, if Chrony decides that the 200 ns source
is too far off to be used.</p>
<h3 id="network-error">Network error</h3>
<p>Chrony tries to compensate for network delays when it syncs to NTP
sources over the network, but it has to make some assumptions that
aren’t always true. It assumes that network delays are symmetrical (that
is, if it takes 30 μS for network traffic to get from the client to the
server and back, then it takes 15 μS each way). This isn’t generally
true, but for a lot of networks it’s close enough.</p>
<p>Apparently it’s not particularly true for my network.</p>
<p>One of the things that I’m monitoring with Chrony and Prometheus is
the current offset for each time source on each Chrony client. I have
data for my 8 test servers (<code>d1</code> through <code>d8</code>)
tracking the relative offsets for <code>ntp1</code> and
<code>ntp2</code>. I was expecting to see that either <code>ntp1</code>
or <code>ntp2</code> was consistently ahead of the other one, given
cable lengths, network delays, antenna differences, and so forth.</p>
<p>Instead, half of the servers see <code>ntp1</code> as running faster,
while half show <code>ntp2</code> as running faster:</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/ntp12.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/ntp12.png"/> </a>
<figcaption>
<small>
<p>The relative time offsets for <code>ntp1</code> vs <code>ntp2</code>
across <code>d1</code> through <code>d8</code>. Each line is one of the
<code>d*</code> servers. Note that half of the servers see
<code>ntp1</code> as being ahead of <code>ntp2</code> and half see the
opposite.</p>
</small>
</figcaption>
</figure>
<details>
<summary>
Prometheus query for graph
</summary>
<pre><code>quantile_over_time(
  0.5,
  chrony_sources_last_sample_offset_seconds{instance=~&#34;${client}&#34;,source_address=&#34;10.1.0.238&#34;}[1h]
)
- on (instance)
quantile_over_time(
  0.5,
  chrony_sources_last_sample_offset_seconds{instance=~&#34;${client}&#34;,source_address=&#34;10.1.0.239&#34;}[1h]
)</code></pre>
</details>
<p>The servers can’t agree on whether <code>ntp1</code> runs faster than
<code>ntp2</code> or not — 4 of the 8 see <code>ntp1</code> as faster,
while 4 see <code>ntp2</code> as faster, with the servers in two bands
around +100ns and -300ns. This has been consistent for weeks.<span><label for="sn-9"></label><span>To be clear, since half of the <code>d*</code> servers
are in one enclosure and half are in another: the timing differences are
basically random, and don’t follow which chassis they’re in or which
network cables they use. Of the 4 physical servers in each enclosure, 2
think <code>ntp1</code> is faster and 2 think <code>ntp2</code> is
faster, but <em>which</em> two aren’t even consistent between
enclosures.</span></span></p>
<p>Presumably this is caused by asymmetric traffic paths in my network.
If you look back to the network diagram above, you’ll see that the test
servers each have a link to each core switch, and that the L2 switches
that the NTP servers use are each connected to both core switches. Any
time you have redundant links like this, <em>something</em> has to
decide which path any given packet is going to take over the network. In
general network people <em>really</em> dislike just picking paths at
random, largely because that’d mean that <a href="https://en.wikipedia.org/wiki/Out-of-order_delivery">packets could
arrive out of order</a>, and a lot of TCP stacks hate out-of-order
traffic. So, generally, traffic is assigned to a path using a hash of
source and destination addresses.<span><label for="sn-10"></label><span>The exact implementation varies widely and is
frequently configurable on higher-end devices. For L2 links most devices
just hash the source and destination MACs, while for L3 links the hash
<em>usually</em> includes the source and destination IPs and may include
the TCP/UDP port numbers or other easy-to-locate data.</span></span></p>
<p>Presumably one of the possible paths between servers and time sources
on the network is faster than the others, and paths that hash onto the
faster path consistently skew the results in one direction or the other.
A less complex (and less redundant!) network would have less of this
sort of error, but asymmetric round trip times show up
<em>everywhere</em> in networking when you’re counting nanoseconds. At
some level, this isn’t avoidable.</p>
<p>So, on my network, this seems to cause a minimum of 200ns of
potential error, as various paths take different amounts of time, and
Chrony isn’t able to compensate automatically.<span><label for="sn-11"></label><span>Chrony has a per-source setting for adjusting latency
asymmetry, so I could probably hand-adjust all 16 (<code>d*</code> -&gt;
<code>ntp*</code>) config lines to minimize the error if I
<em>really</em> cared about ~200 ns of error, but it’s unlikely that
it’d buy me much useful accuracy.</span></span></p>
<h3 id="cross-server-synchronization">Cross-server synchronization</h3>
<p>As an experiment, I told all 8 of my test servers to use each other
as time sources. I added them using Chrony’s <code>noselect</code> flag,
so they wouldn’t try to use each other as authoritative; they’d just
monitor the relative offsets between servers and record them over
time.<span><label for="sn-12"></label><span>I’m actually measuring time <em>really</em>
aggressively between <code>d*</code> servers. I’m polling every 250 ms
and averaging across 5 samples to try to minimize noise. Flags from
<code>chrony.conf</code>:
<code>server xxx noselect xleave presend 9 minpoll -2 maxpoll -2 filter 5 extfield F323</code></span></span></p>
<p>Here’s the median<span><label for="sn-13"></label><span>Note that the median isn’t really the best way to look
for offsets <em>in general</em>, but since Chrony maintains its own view
of time and slowly adjusts it time relative to its sources, so a few
wildly inaccurate responses won’t really change Chrony’s time much, if
at all.</span></span> offset between servers, in nanoseconds, over 4 hours:</p>
<table>
<thead>
<tr>
<th></th>
<th>d1</th>
<th>d2</th>
<th>d3</th>
<th>d4</th>
<th>d5</th>
<th>d6</th>
<th>d7</th>
<th>d8</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>d1</strong></td>
<td></td>
<td>83</td>
<td>-70</td>
<td>-138</td>
<td>-18</td>
<td>-161</td>
<td>-207</td>
<td>-132</td>
</tr>
<tr>
<td><strong>d2</strong></td>
<td>-145</td>
<td></td>
<td>-29</td>
<td>-75</td>
<td>-4</td>
<td>-138</td>
<td>-158</td>
<td>-29</td>
</tr>
<tr>
<td><strong>d3</strong></td>
<td>51</td>
<td>-23</td>
<td></td>
<td>-33</td>
<td>28</td>
<td>-31</td>
<td>-65</td>
<td>-74</td>
</tr>
<tr>
<td><strong>d4</strong></td>
<td>74</td>
<td>106</td>
<td>-42</td>
<td></td>
<td>106</td>
<td>-23</td>
<td>27</td>
<td>91</td>
</tr>
<tr>
<td><strong>d5</strong></td>
<td>5</td>
<td>-40</td>
<td>-66</td>
<td>-89</td>
<td></td>
<td>-49</td>
<td>-48</td>
<td>0</td>
</tr>
<tr>
<td><strong>d6</strong></td>
<td>153</td>
<td>173</td>
<td>0</td>
<td>62</td>
<td>63</td>
<td></td>
<td>-28</td>
<td>47</td>
</tr>
<tr>
<td><strong>d7</strong></td>
<td>190</td>
<td>173</td>
<td>36</td>
<td>-32</td>
<td>43.0</td>
<td>19</td>
<td></td>
<td>58</td>
</tr>
<tr>
<td><strong>d8</strong></td>
<td>131</td>
<td>-6</td>
<td>58</td>
<td>-64</td>
<td>5</td>
<td>-52</td>
<td>-47</td>
<td></td>
</tr>
</tbody>
</table>
<details>
<summary>
Prometheus query for chart
</summary>
<pre><code>quantile_over_time(
  0.5,
  chrony_sources_last_sample_offset_seconds{instance=~&#34;d[1-8].*&#34;,source_address=~&#34;10.0.0.10[0-9]&#34;}[5m]
)</code></pre>
Plus some work in Grafana to turn this into a table.
</details>
<p>Notice that they’re all within 207ns of each other, but the timings
aren’t particularly consistent. For instance, looking at the timings
between <code>d2</code> and <code>d3</code> show that they’re 29 ns
apart when you query one direction and 23 ns apart when you query in the
other direction, but they’re both off in the <em>same</em> direction. If
network error wasn’t a factor, then I’d expect to see one number be
positive and the other be negative; that’s not always the case here.</p>
<p>In general, this aligns nicely with the 200-300ns of error seen in
the previous section, but it shows that there’s a serious limit to how
accurately Chrony can measure nanoseconds on this hardware.</p>
<h3 id="observed-offsets-across-all-sources">Observed offsets across all
sources</h3>
<p>Earlier, I discussed the difference between <code>ntp1</code> and
<code>ntp2</code>, and how each server had a different view of the
difference between them. On average, <code>ntp1</code> seems to run
50–150 ns ahead of <code>ntp2</code>.</p>
<p>Remember that my big goal here is less <em>accurate</em> time and
more <em>consistent</em> time. This 50–150 ns of inconsistency isn’t a
big deal, but when I started adding additional time sources, I
discovered that some of them were even further away from
<code>ntp1</code> and <code>ntp2</code>, and I wanted to minimize the
total time spread. I’d really like it if adding additional NTP sources
to the mix didn’t make things even less consistent.</p>
<p>There are a lot of things to like about the LeoNTP time servers, but
configurability <em>isn’t</em> one of them. There’s no way that I can
see to add an offset between GPS time and the NTP time that they export.
On the other hand, the 3 Chrony-based time servers (my desktop,
<code>ntp4</code>, and <code>ntp5</code>) <em>can</em> be adjusted to
control the offset between GPS time and NTP time. And, in fact, you
<em>can’t</em> really run with a 0-second offset because GPS time is
based on <a href="https://en.wikipedia.org/wiki/International_Atomic_Time">TAI</a>
and NTP time is usually <a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">UTC</a>,<span><label for="sn-14"></label><span>Strictly speaking, it’s usually a weird mutant that
<em>mostly</em> ignores leap seconds, look up “<a href="https://developers.google.com/time/smear">leap smear</a>” for the
ugly mess.</span></span> and the two are currently 37 seconds apart.<span><label for="sn-15"></label><span>Leap seconds make life hard.</span></span></p>
<p>Originally, I discovered that time from my desktop was around 1 μS
when compared with <code>ntp1</code> and <code>ntp2</code> by
<code>d*</code> servers, and time from the Raspberry Pi-based
<code>ntp4</code> was almost 38 μS off! To mitigate this, I graphed the
average offset between each time source across all 8 servers and then
adjusted offsets on my desktop and <code>ntp4</code> to be as close as
possible to the median of <code>ntp1</code> and <code>ntp2</code>. To do
this, I changed my desktop’s TAI offset from <code>-37</code> to
<code>-36.999999160</code> and the offset of <code>ntp4</code> to
<code>-37.000033910</code>.</p>
<p>Now, all 4 sources are basically in unison:</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/offset.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/offset.png"/> </a>
<figcaption>
<small>
<p>Observed offsets over the past day.</p>
</small>
</figcaption>
</figure>
<details>
<summary>
Prometheus query for graph
</summary>
<pre><code>avg by (source_address) (
  quantile_over_time(
    0.5,
    chrony_sources_last_sample_offset_seconds{
      instance=~&#34;d[1-8].*&#34;,
      source_address=~&#34;10[.].*&#34;,
      source_address=~&#34;${ntpsource}&#34;
    }[1h]
  )
)</code></pre>
</details>
<p>Why were times so far off? For my desktop, it’s probably a mix of
multipath weirdness and delay in the network stack. 840 ns isn’t a huge
amount of time, although it’s bigger than what I’ve seen elsewhere.</p>
<p>I’m less sure what’s going on with <code>ntp4</code>. It was
originally seeing over 50 μS of error, but reducing the Ethernet
coalescing limits on <code>eth0</code><span><label for="sn-16"></label><span><code>ethtool -C eth0 tx-usecs 0 rx-usecs 0</code></span></span> helped quite a bit. I’m going to have to keep poking at
this for a while.</p>
<h3 id="observed-ntp-jitter-across-all-sources">Observed NTP jitter
across all sources</h3>
<p>I can compare the jitter of 4 of my GPS time sources across all 8
<code>d*</code> servers. To calculate jitter in this case, I’m looking
at the difference between the 1st and 99th percentile of each source’s
offset from Chrony’s best estimate of the current time. I’m calculating
the percentiles over 15 minute windows, subtracting the 1st percentile
from the 99th, and then averaging those results across all 8
servers.<span><label for="sn-17"></label><span>It’s not the best way to do this statistically, but
there’s a limit to what you can do with Prometheus easily.</span></span></p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/jitter.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/jitter.png"/> </a>
<figcaption>
<small>
<p>Graph of jitter by source across all 8 <code>d*</code> servers.</p>
</small>
</figcaption>
</figure>
<details>
<summary>
Prometheus query for graph
</summary>
<pre><code>(
  avg by (source_address) (
    quantile_over_time(
      0.99,
      chrony_sources_last_sample_offset_seconds{
        instance=~&#34;d[1-8].*&#34;,
        source_address=~&#34;10[.].*&#34;,
        source_address=~&#34;${ntpsource}&#34;
      }[15m]
    )
  )
) - (
  avg by (source_address) (
    quantile_over_time(
      0.01,
      chrony_sources_last_sample_offset_seconds{
        instance=~&#34;d[1-8].*&#34;,
        source_address=~&#34;10[.].*&#34;
      }[15m]
    )
  )
)</code></pre>
</details>
<p>Over the past hour, that works out to:</p>
<table>
<thead>
<tr>
<th>Time Source</th>
<th>Jitter</th>
</tr>
</thead>
<tbody>
<tr>
<td>desktop</td>
<td>1.01 μS</td>
</tr>
<tr>
<td>ntp1</td>
<td>1.28 μS</td>
</tr>
<tr>
<td>ntp2</td>
<td>1.40 μS</td>
</tr>
<tr>
<td>ntp4</td>
<td>2.02 μS</td>
</tr>
</tbody>
</table>
<p>So, my desktop (with a fast NIC and a very good GNSS module) has the
least jitter. The two LeoNTP boxes are next, with a bit more, and the
Raspberry Pi has 2x the jitter of my desktop. Since Chrony averages out
offsets across sources and over time, jitter isn’t <em>necessarily</em>
a big deal as long as it’s under control.</p>
<p>Which brings up <code>ntp5</code>, which I’d excluded from the
previous graph. Here’s why:</p>
<figure>
<a href="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/jitter2.png">
<img src="https://blog.veitheller.de/posts/2025/05/19/ntp-limits/jitter2.png"/> </a>
<figcaption>
<small>
<p>Graph of jitter by source across all 8 <code>d*</code> servers
<em>including</em> <code>ntp5</code>, which has accuracy issues every 2
hours.</p>
</small>
</figcaption>
</figure>
<p>I still haven’t figured out why this loses accuracy every 2 hours,
but there are other weird things about <code>ntp5</code>, so I’m not all
that worried about it overall.</p>
<h2 id="things-that-hurt-syncing">Things that hurt syncing</h2>
<p>Along the way, I’ve found a bunch of things that hurt time syncing. A
short list:</p>
<ul>
<li>Network cards without hardware timestamps. Realtek, for
instance.</li>
<li>Tunnels. I had 3 servers who were sending traffic to the network
with <code>ntp1</code> and <code>ntp2</code> over <a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">VxLAN</a>
originally, and their time accuracy was terrible. I suspect that the
NICs’ hardware timestamp wasn’t propagated correctly through the tunnel
decapsulation. Plus, it made network times even less symmetrical.</li>
<li>NIC packet coelescing. On Raspberry Pi CM5s especially, I had to
disable NIC coelescing via <code>ethtool -c</code> or I had terrible
accuracy.</li>
<li>Software in general. I get the best results on NTP servers where the
GPS’s PPS signal goes directly into the NIC’s hardware, completely
bypassing as much software as possible.</li>
<li>Running <code>ptp4l</code> and <code>Chrony</code> on the same
ConnectX-4 NIC, or potentially ConnectX-3 or -5 NICs. Intel seems
perfectly happy under the same situation.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>So, in all, I’m seeing time syncing somewhere in the 200–500 ns range
across my network. The GPS time sources themselves are sometimes as far
as 150 ns apart, even after compensating for systemic differences, and
the network itself adds another 200–300 ns of noise.</p>
<p>In an ideal world, it’d be cool to see ~10 ns accuracy, but it’s not
really possible at any level with this hardware. My time sources aren’t
that good, my network adds more systemic error than that, and when I try
to measure the difference between test servers I see a couple hundred
nanoseconds of noise. So 10 ns isn’t going to happen.</p>
<p>On the other hand, though, I’m almost certainly accurate to within 1
μS across the set of 8 test servers most of the time, and I’m
<em>absolutely</em> more accurate than my original goal of 10 μS.</p></div></div>
  </body>
</html>
