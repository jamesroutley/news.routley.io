<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up">Original</a>
    <h1>Building LLMs from the Ground Up: A 3-Hour Coding Workshop</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div><div><div dir="auto"><p>If you’d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I&#39;ve prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.</p><div id="youtube2-quh7z1q7-uc" data-attrs="{&#34;videoId&#34;:&#34;quh7z1q7-uc&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/quh7z1q7-uc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Below, you&#39;ll find a table of contents to get an idea of what this video covers (the video itself has clickable chapter marks, allowing you to jump directly to topics of interest):</p><p>0:00 – Workshop overview</p><p>2:17 – Part 1: Intro to LLMs</p><p>9:14 – Workshop materials</p><p>10:48 – Part 2: Understanding LLM input data</p><p>23:25 – A simple tokenizer class</p><p>41:03 – Part 3: Coding an LLM architecture</p><p>45:01 – GPT-2 and Llama 2</p><p>1:07:11 – Part 4: Pretraining</p><p>1:29:37 – Part 5.1: Loading pretrained weights</p><p>1:45:12 – Part 5.2: Pretrained weights via LitGPT</p><p>1:53:09 – Part 6.1: Instruction finetuning</p><p>2:08:21 – Part 6.2: Instruction finetuning via LitGPT</p><p>02:26:45 – Part 6.3: Benchmark evaluation</p><p>02:36:55 – Part 6.4: Evaluating conversational performance</p><p>02:42:40 – Conclusion</p><p>It&#39;s a slight departure from my usual text-based content, but the last time I did this a few months ago, it was so well-received that I thought it might be nice to do another one!</p><p><strong>Happy viewing!</strong></p><ol><li><p><a href="https://mng.bz/M96o" rel="">Build an LLM from Scratch book</a></p></li><li><p><a href="https://github.com/rasbt/LLMs-from-scratch" rel="">Build an LLM from Scratch GitHub repository</a></p></li><li><p><a href="https://github.com/rasbt/LLM-workshop-2024" rel="">GitHub repository with workshop code</a></p></li><li><p><a href="https://lightning.ai/lightning-ai/studios/llms-from-the-ground-up-workshop" rel="">Lightning Studio for this workshop</a></p></li><li><p><a href="https://github.com/Lightning-AI/litgpt" rel="">LitGPT GitHub repository</a></p></li></ol></div></div></div></article></div></div></div>
  </body>
</html>
