<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dev.l1x.be/posts/2023/12/08/using-llama-with-m1-mac/">Original</a>
    <h1>Using LLaMA with M1 Mac and Python 3.11</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://dev.l1x.be/img/og/llama.webp" alt="LLaMA"/></p><h2 id="the-large-language-models-wars">The large language models wars</h2><p>With the increasing interest in artificial intelligence and its use in everyday life, numerous exemplary models such as Meta’s LLaMA, OpenAI’s GPT-3, and Microsoft’s Kosmos-1 are joining the group of large language models (LLMs). The only problem with such models is the you can’t run these locally. Up until now. Thanks to <a href="https://www.linkedin.com/in/georgi-gerganov-b230ab24">Georgi Gerganov</a> and his <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> project it is possible to run Meta’s LLaMA on a single computer without a dedicated GPU.</p><h2 id="running-llama">Running LLaMA</h2><p>There are multiple steps involved in running LLaMA locally on a M1 Mac. I am not sure about other platforms or other OSes so in this article we are focusing only the aforementioned combination.</p><h3 id="step-1-downloading-the-model">Step 1: Downloading the model</h3><p>The official way is to request the model via <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form">this</a> web form and download it afterward.</p><p>There is a PR open in the repository, that describes an alternative way (that is probably a violation of the terms of service).</p><p><a href="https://github.com/facebookresearch/llama/pull/73">https://github.com/facebookresearch/llama/pull/73</a></p><p>Anyways, after you downloaded the model (or more like models because there are a few different kinds of models in the folder) you should have something like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>❯ exa --tree
</span></span><span><span>.
</span></span><span><span>├── 7B
</span></span><span><span>│  ├── checklist.chk
</span></span><span><span>│  ├── consolidated.00.pth
</span></span><span><span>│  └── params.json
</span></span><span><span>├── 13B
</span></span><span><span>│  ├── checklist.chk
</span></span><span><span>│  ├── consolidated.00.pth
</span></span><span><span>│  ├── consolidated.01.pth
</span></span><span><span>│  └── params.json
</span></span><span><span>├── 30B
</span></span><span><span>│  ├── checklist.chk
</span></span><span><span>│  ├── consolidated.00.pth
</span></span><span><span>│  ├── consolidated.01.pth
</span></span><span><span>│  ├── consolidated.02.pth
</span></span><span><span>│  ├── consolidated.03.pth
</span></span><span><span>│  └── params.json
</span></span><span><span>├── 65B
</span></span><span><span>│  ├── checklist.chk
</span></span><span><span>│  ├── consolidated.00.pth
</span></span><span><span>│  ├── consolidated.01.pth
</span></span><span><span>│  ├── consolidated.02.pth
</span></span><span><span>│  ├── consolidated.03.pth
</span></span><span><span>│  ├── consolidated.04.pth
</span></span><span><span>│  ├── consolidated.05.pth
</span></span><span><span>│  ├── consolidated.06.pth
</span></span><span><span>│  ├── consolidated.07.pth
</span></span><span><span>│  └── params.json
</span></span><span><span>├── tokenizer.model
</span></span><span><span>└── tokenizer_checklist.chk
</span></span></code></pre></div><p>As you can see the different models are in a different folders. Each model has a params.json that contains details about the model.</p><p>For example:</p><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>&#34;dim&#34;</span>: <span>4096</span>,
</span></span><span><span>  <span>&#34;multiple_of&#34;</span>: <span>256</span>,
</span></span><span><span>  <span>&#34;n_heads&#34;</span>: <span>32</span>,
</span></span><span><span>  <span>&#34;n_layers&#34;</span>: <span>32</span>,
</span></span><span><span>  <span>&#34;norm_eps&#34;</span>: <span>1e-06</span>,
</span></span><span><span>  <span>&#34;vocab_size&#34;</span>: <span>-1</span>
</span></span><span><span>}
</span></span></code></pre></div><h3 id="step-2-installing-dependencies">Step 2: Installing dependencies</h3><p>Xcode must be installed to compile the C++ project. If you don’t have it, please do the following:</p><p>These are the dependencies for building the C++ project (pkgconfigand cmake).</p><div><pre tabindex="0"><code data-lang="bash"><span><span>brew install pkgconfig cmake
</span></span></code></pre></div><p>Finally, we can install Torch.</p><p>I assume you have Python 3.11 installed so you can create a virtual env like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>/opt/homebrew/bin/python3.11 -m venv venv
</span></span></code></pre></div><p>Activating the venv. I am using fish. For other shells just drop the .fish suffix.</p><p>After activated the venv we can install Pytorch:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>pip3 install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu
</span></span></code></pre></div><p>If you are interesting leveraging the new <a href="https://developer.apple.com/metal/pytorch/">Metal Performance Shaders (MPS) backend</a> for GPU training acceleration you can verify it by running the following. This is not required for running LLaMA on you M1 though:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>❯ python
</span></span><span><span>Python 3.11.2 <span>(</span>main, Feb <span>16</span> 2023, 02:55:59<span>)</span> <span>[</span>Clang 14.0.0 <span>(</span>clang-1400.0.29.202<span>)]</span> on darwin
</span></span><span><span>Type <span>&#34;help&#34;</span>, <span>&#34;copyright&#34;</span>, <span>&#34;credits&#34;</span> or <span>&#34;license&#34;</span> <span>for</span> more information.
</span></span><span><span>&gt;&gt;&gt; import torch; torch.backends.mps.is_available<span>()</span>
</span></span><span><span>True
</span></span></code></pre></div><p>Now lets compile llama.cpp.</p><h3 id="step-3-compile-llama-cpp">Step 3: Compile LLaMA CPP</h3><p>Cloning the repo:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>git clone git@github.com:ggerganov/llama.cpp.git
</span></span></code></pre></div><p>After installing all the dependencies you can run make:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>❯ make
</span></span><span><span>I llama.cpp build info:
</span></span><span><span>I UNAME_S:  Darwin
</span></span><span><span>I UNAME_P:  arm
</span></span><span><span>I UNAME_M:  arm64
</span></span><span><span>I CFLAGS:   -I.              -O3 -DNDEBUG -std<span>=</span>c11   -fPIC -pthread -DGGML_USE_ACCELERATE
</span></span><span><span>I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std<span>=</span>c++11 -fPIC -pthread
</span></span><span><span>I LDFLAGS:   -framework Accelerate
</span></span><span><span>I CC:       Apple clang version 14.0.0 <span>(</span>clang-1400.0.29.202<span>)</span>
</span></span><span><span>I CXX:      Apple clang version 14.0.0 <span>(</span>clang-1400.0.29.202<span>)</span>
</span></span><span><span>
</span></span><span><span>cc  -I.              -O3 -DNDEBUG -std<span>=</span>c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o
</span></span><span><span>c++ -I. -I./examples -O3 -DNDEBUG -std<span>=</span>c++11 -fPIC -pthread -c utils.cpp -o utils.o
</span></span><span><span>c++ -I. -I./examples -O3 -DNDEBUG -std<span>=</span>c++11 -fPIC -pthread main.cpp ggml.o utils.o -o main  -framework Accelerate
</span></span><span><span>./main -h
</span></span><span><span>usage: ./main <span>[</span>options<span>]</span>
</span></span><span><span>
</span></span><span><span>options:
</span></span><span><span>  -h, --help            show this help message and exit
</span></span><span><span>  -s SEED, --seed SEED  RNG seed <span>(</span>default: -1<span>)</span>
</span></span><span><span>  -t N, --threads N     number of threads to use during computation <span>(</span>default: 4<span>)</span>
</span></span><span><span>  -p PROMPT, --prompt PROMPT
</span></span><span><span>                        prompt to start generation with <span>(</span>default: random<span>)</span>
</span></span><span><span>  -n N, --n_predict N   number of tokens to predict <span>(</span>default: 128<span>)</span>
</span></span><span><span>  --top_k N             top-k sampling <span>(</span>default: 40<span>)</span>
</span></span><span><span>  --top_p N             top-p sampling <span>(</span>default: 0.9<span>)</span>
</span></span><span><span>  --temp N              temperature <span>(</span>default: 0.8<span>)</span>
</span></span><span><span>  -b N, --batch_size N  batch size <span>for</span> prompt processing <span>(</span>default: 8<span>)</span>
</span></span><span><span>  -m FNAME, --model FNAME
</span></span><span><span>                        model path <span>(</span>default: models/llama-7B/ggml-model.bin<span>)</span>
</span></span><span><span>
</span></span><span><span>c++ -I. -I./examples -O3 -DNDEBUG -std<span>=</span>c++11 -fPIC -pthread quantize.cpp ggml.o utils.o -o quantize  -framework Accelerate
</span></span></code></pre></div><h2 id="step-4-converting-the-model">Step 4: Converting the model</h2><p>Assuming you placed the models under models/ in the llama.cpp repo.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>python convert-pth-to-ggml.py models/7B <span>1</span>
</span></span></code></pre></div><p>You should see an output like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span>{</span><span>&#39;dim&#39;</span>: 4096, <span>&#39;multiple_of&#39;</span>: 256, <span>&#39;n_heads&#39;</span>: 32, <span>&#39;n_layers&#39;</span>: 32, <span>&#39;norm_eps&#39;</span>: 1e-06, <span>&#39;vocab_size&#39;</span>: 32000<span>}</span>
</span></span><span><span><span>n_parts</span> <span>=</span>  <span>1</span>
</span></span><span><span>Processing part  <span>0</span>
</span></span><span><span>Processing variable: tok_embeddings.weight with shape:  torch.Size<span>([</span>32000, 4096<span>])</span>  and type:  torch.float16
</span></span><span><span>Processing variable: norm.weight with shape:  torch.Size<span>([</span>4096<span>])</span>  and type:  torch.float16
</span></span><span><span>  Converting to float32
</span></span><span><span>Processing variable: output.weight with shape:  torch.Size<span>([</span>32000, 4096<span>])</span>  and type:  torch.float16
</span></span><span><span>Processing variable: layers.0.attention.wq.weight with shape:  torch.Size<span>([</span>4096, 4096<span>])</span>  and type:  torch.f
</span></span><span><span>loat16
</span></span><span><span>Processing variable: layers.0.attention.wk.weight with shape:  torch.Size<span>([</span>4096, 4096<span>])</span>  and type:  torch.f
</span></span><span><span>loat16
</span></span><span><span>Processing variable: layers.0.attention.wv.weight with shape:  torch.Size<span>([</span>4096, 4096<span>])</span>  and type:  torch.f
</span></span><span><span>loat16
</span></span><span><span>Processing variable: layers.0.attention.wo.weight with shape:  torch.Size<span>([</span>4096, 4096<span>])</span>  and type:  torch.f
</span></span><span><span>loat16
</span></span><span><span>Processing variable: layers.0.feed_forward.w1.weight with shape:  torch.Size<span>([</span>11008, 4096<span>])</span>  and type:  tor
</span></span><span><span>ch.float16
</span></span><span><span>Processing variable: layers.0.feed_forward.w2.weight with shape:  torch.Size<span>([</span>4096, 11008<span>])</span>  and type:  tor
</span></span><span><span>ch.float16
</span></span><span><span>Processing variable: layers.0.feed_forward.w3.weight with shape:  torch.Size<span>([</span>11008, 4096<span>])</span>  and type:  tor
</span></span><span><span>ch.float16
</span></span><span><span>Processing variable: layers.0.attention_norm.weight with shape:  torch.Size<span>([</span>4096<span>])</span>  and type:  torch.float
</span></span><span><span><span>16</span>
</span></span><span><span>...
</span></span><span><span>Done. Output file: models/7B/ggml-model-f16.bin, <span>(</span>part  <span>0</span> <span>)</span>
</span></span></code></pre></div><p>The next step would be to perform the quantizing:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin <span>2</span>
</span></span></code></pre></div><p>Output:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>llama_model_quantize: loading model from <span>&#39;./models/7B/ggml-model-f16.bin&#39;</span>
</span></span><span><span>llama_model_quantize: <span>n_vocab</span> <span>=</span> <span>32000</span>
</span></span><span><span>llama_model_quantize: <span>n_ctx</span>   <span>=</span> <span>512</span>
</span></span><span><span>llama_model_quantize: <span>n_embd</span>  <span>=</span> <span>4096</span>
</span></span><span><span>llama_model_quantize: <span>n_mult</span>  <span>=</span> <span>256</span>
</span></span><span><span>llama_model_quantize: <span>n_head</span>  <span>=</span> <span>32</span>
</span></span><span><span>llama_model_quantize: <span>n_layer</span> <span>=</span> <span>32</span>
</span></span><span><span>llama_model_quantize: <span>f16</span>     <span>=</span> <span>1</span>
</span></span><span><span>...
</span></span><span><span>layers.31.attention_norm.weight - <span>[</span> 4096,     1<span>]</span>, type <span>=</span>    f32 <span>size</span> <span>=</span>    0.016 MB
</span></span><span><span>layers.31.ffn_norm.weight - <span>[</span> 4096,     1<span>]</span>, type <span>=</span>    f32 <span>size</span> <span>=</span>    0.016 MB
</span></span><span><span>llama_model_quantize: model <span>size</span>  <span>=</span> 25705.02 MB
</span></span><span><span>llama_model_quantize: quant <span>size</span>  <span>=</span>  4017.27 MB
</span></span><span><span>llama_model_quantize: hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022
</span></span><span><span>
</span></span><span><span>main: quantize time <span>=</span> 29389.45 ms
</span></span><span><span>main:    total time <span>=</span> 29389.45 ms
</span></span></code></pre></div><h3 id="step5-running-the-model">Step5: Running the model</h3><div><pre tabindex="0"><code data-lang="bash"><span><span>❯ ./main -m ./models/7B/ggml-model-q4_0.bin <span>\
</span></span></span><span><span><span></span>        -t <span>8</span> <span>\
</span></span></span><span><span><span></span>        -n <span>128</span> <span>\
</span></span></span><span><span><span></span>        -p <span>&#39;The first president of the USA was &#39;</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="bash"><span><span>main: <span>seed</span> <span>=</span> <span>1678615879</span>
</span></span><span><span>llama_model_load: loading model from <span>&#39;./models/7B/ggml-model-q4_0.bin&#39;</span> - please wait ...
</span></span><span><span>llama_model_load: <span>n_vocab</span> <span>=</span> <span>32000</span>
</span></span><span><span>llama_model_load: <span>n_ctx</span>   <span>=</span> <span>512</span>
</span></span><span><span>llama_model_load: <span>n_embd</span>  <span>=</span> <span>4096</span>
</span></span><span><span>llama_model_load: <span>n_mult</span>  <span>=</span> <span>256</span>
</span></span><span><span>llama_model_load: <span>n_head</span>  <span>=</span> <span>32</span>
</span></span><span><span>llama_model_load: <span>n_layer</span> <span>=</span> <span>32</span>
</span></span><span><span>llama_model_load: <span>n_rot</span>   <span>=</span> <span>128</span>
</span></span><span><span>llama_model_load: <span>f16</span>     <span>=</span> <span>2</span>
</span></span><span><span>llama_model_load: <span>n_ff</span>    <span>=</span> <span>11008</span>
</span></span><span><span>llama_model_load: <span>n_parts</span> <span>=</span> <span>1</span>
</span></span><span><span>llama_model_load: ggml ctx <span>size</span> <span>=</span> 4529.34 MB
</span></span><span><span>llama_model_load: <span>memory_size</span> <span>=</span>   512.00 MB, <span>n_mem</span> <span>=</span> <span>16384</span>
</span></span><span><span>llama_model_load: loading model part 1/1 from <span>&#39;./models/7B/ggml-model-q4_0.bin&#39;</span>
</span></span><span><span>llama_model_load: .................................... <span>done</span>
</span></span><span><span>llama_model_load: model <span>size</span> <span>=</span>  4017.27 MB / num <span>tensors</span> <span>=</span> <span>291</span>
</span></span><span><span>
</span></span><span><span>main: prompt: <span>&#39;The first president of the USA was &#39;</span>
</span></span><span><span>main: number of tokens in <span>prompt</span> <span>=</span> <span>9</span>
</span></span><span><span>     <span>1</span> -&gt; <span>&#39;&#39;</span>
</span></span><span><span>  <span>1576</span> -&gt; <span>&#39;The&#39;</span>
</span></span><span><span>   <span>937</span> -&gt; <span>&#39; first&#39;</span>
</span></span><span><span>  <span>6673</span> -&gt; <span>&#39; president&#39;</span>
</span></span><span><span>   <span>310</span> -&gt; <span>&#39; of&#39;</span>
</span></span><span><span>   <span>278</span> -&gt; <span>&#39; the&#39;</span>
</span></span><span><span>  <span>8278</span> -&gt; <span>&#39; USA&#39;</span>
</span></span><span><span>   <span>471</span> -&gt; <span>&#39; was&#39;</span>
</span></span><span><span> <span>29871</span> -&gt; <span>&#39; &#39;</span>
</span></span><span><span>
</span></span><span><span>sampling parameters: <span>temp</span> <span>=</span> 0.800000, <span>top_k</span> <span>=</span> 40, <span>top_p</span> <span>=</span> 0.950000
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>The first president of the USA was <span>57</span> years old when he assumed office <span>(</span>George Washington<span>)</span>. Nowadays, the US electorate expects the new president to be more young at heart. President Donald Trump was <span>70</span> years old when he was inaugurated. In contrast to his predecessors, he is physically fit, healthy and active. And his fitness has been a prominent theme of his presidency. During the presidential campaign, he famously said he
</span></span><span><span> would be the “most active president ever” — a statement Trump has not yet achieved, but one that fits his approach to the office. His tweets demonstrate his physical activity.
</span></span><span><span>
</span></span><span><span>main: mem per <span>token</span> <span>=</span> <span>14434244</span> bytes
</span></span><span><span>main:     load time <span>=</span>  1311.74 ms
</span></span><span><span>main:   sample time <span>=</span>   278.96 ms
</span></span><span><span>main:  predict time <span>=</span>  7375.89 ms / 54.23 ms per token
</span></span><span><span>main:    total time <span>=</span>  9216.61 ms
</span></span></code></pre></div><p>Enjoy!</p></div></div>
  </body>
</html>
