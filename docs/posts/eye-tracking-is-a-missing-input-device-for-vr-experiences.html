<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mlajtos.mu/posts/gaze-contingency">Original</a>
    <h1>Eye-tracking is a missing input device for VR experiences</h1>
    
    <div id="readability-page-1" class="page"><div><p><sup>ðŸ˜Ž</sup> I am using term <em>Virtual Reality</em> as an umbrella term for Virtual, Augmented, Mixed, Computer-mediated, Extended Reality. Upcoming VR headsets with eye-tracking: Meta Quest Pro (2022), Pico 4 Pro (2022), PlayStation VR2 (2023), Apple Reality Pro (202?), and others.</p><hr/><h2>What is gaze?</h2><p>We can describe gaze as <strong>physiological manifestation of visual attention</strong> â€“ it&#39;s where the eyes are focused. Focus is not only described by the <strong>direction of the eye</strong>, but also by the <strong>size of the pupil</strong>, and <strong>curvature of the lens</strong>.</p><p>You <strong>eye muscles</strong> control where the eye is facing, i.e. how is rotated. The <strong>pupil</strong> acts as an aperture and is controlling how much light from the surrounding world comes through. Then the <strong>lens</strong> focuses incoming light on the <strong>retina</strong>, where light-sensitive cells detect its presence and the information is transmitted through <strong>optic nerve</strong> to the <strong>brain</strong>.</p><figure><img src="https://i.imgur.com/aBt1Zom.gif"/><figcaption>Eye can move very rapidly causing pupil to wobble</figcaption></figure><h3>Eye Movement</h3><p>Your eye muscles can move your eyes in two ways â€“ <strong>smoothly</strong> and <strong>rapidly</strong>. When you <strong>fixate</strong> your attention on a specific <strong>moving object</strong>, your eyes are <strong>smoothly aligning</strong> with the object. Same thing happens when you <strong>fixate</strong> your attention on <strong>static object</strong> and <strong>move your head</strong>. You perceive a <strong>stable stream</strong> of visual information. On the other hand, <strong>rapid eye movements</strong>, also known as <strong>saccades</strong> are a bit counter-intuitive.</p><p>Go in front of a <strong>mirror</strong> and <strong>look at your left eye and then right</strong>, and <strong>keep alternating</strong> between them. Anything strange? You can redo the experiment, but with a <strong>camera-enabled computer</strong> that can act as a mirror. With a <strong>slight delay</strong> the computer has introduced, <strong>you can see your saccadic movement</strong>. Since saccades are <strong>extremely fast</strong>, the visual information we would get out of it would be only a <strong>blurry image</strong> and <strong>brain actively suppresses this information</strong>. As a result we are <strong>practically blind</strong> during the saccade.</p><h2>Eye as a Pointing Device</h2><p>Since we know how we move our eyes, the <strong>obvious thing</strong> to do is <strong>replace all pointing devices</strong> â€“ mouse, trackpad, trackpoint, trackball, 6DOF VR controller, finger pointing, etc. â€“ with <strong>our eyes</strong>.</p><figure><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQzOCIgaGVpZ2h0PSIxMDU1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIvPg=="/></span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span><figcaption>John Wick with freaking laser beams from his eyes</figcaption></figure><h3>Shoot&#39;em All</h3><p>Let&#39;s imagine a simple <strong>VR gaze-shooting game</strong> â€“ you are in an arena, where small <strong>targets are flying around you</strong>. A <strong>single direct look</strong> at the red circle, the target, will <strong>shoot a laser beam</strong> from your eyes and <strong>destroys the target</strong>. Suddenly, <strong>eye-hand coordination is not necessary</strong>, and you know John Wick would not stand a chance against you. Your eyes are <strong>extremely fast pointing device</strong> and work almost subconsciously.</p><p>If you want to put the speed of your eyes into perspective â€“ time of a single saccade is <strong>20ms to 40ms</strong>, while reaction time in classic PC shooter using mouse pointer is about <strong>250ms</strong>. That is an <strong>order of magnitude faster</strong> reaction time.</p><h3>Golden Gaze</h3><p>This gaze-shooting game would get boring really fast. A competent game designer would suggest to add some <strong>distractors</strong> â€“ objects with <strong>different color, shape and size</strong> from our targets. So lets add a slightly larger blue rectangles to our red circles. A direct look at the distractor would score <strong>negative points</strong>. The game now feels more balanced.</p><p>You would think that you would be careful enough to spot distractors with your <strong>peripheral vision</strong> and avoid them, right? Not really â€“ your peripheral vision sucks. Depending on the <strong>distance from the gaze point</strong>, the peripheral vision gets <strong>blurrier, colorless, and less precise</strong>. This is why we need to move our eyes â€“ you spot something interesting but not discernible in your periphery, so you steer your eyes to look directly at it and observe it with much greater acuity.</p><figure><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNTAwIiBoZWlnaHQ9IjIxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2ZXJzaW9uPSIxLjEiLz4="/></span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span><figcaption>Schematic view of recognition cones and their sizes</figcaption></figure><p>So if a distractor appears <strong>outside of your shape and color recognition area</strong>, you <strong>can&#39;t tell it apart</strong> from the target. You just see <strong>some blob</strong> and to assess its category <strong>you have to look at it</strong>. Boom! Your laser gaze destroyed it...</p><p>This problem is called the <strong>Midas Touch</strong> effect. <a href="https://en.wikipedia.org/wiki/Midas">King Midas</a> wanted to turn everything he touched into gold, however this unfortunately applied also to his daughter and food. His golden touch turned out to be a curse.</p><h3>Counteracting Golden Touch with Eyes Only</h3><p>A simple solution is to <strong>trigger the action after some delay</strong>. Eye has to be <strong>fixated on the target for a while</strong>, and then it is time for action. This delay, or <strong>dwell time</strong>, is usually around <strong>500ms</strong>, so this is <strong>slower</strong> than the mouse-based interactions and <strong>don&#39;t solve the problem</strong> at all.</p><p>An interesting solution is <strong>dual gaze</strong>, which would trigger the action only after an additional <strong>confirmation saccade</strong>:</p><figure><p><iframe src="https://www.youtube-nocookie.com/embed/kD-q1AzIlek?start=55&amp;end=75&amp;rel=0&amp;modestbranding=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><figcaption>Explanation of <strong>dual gaze</strong> interaction â€“ 20sec</figcaption></figure><p>In our eye-laser game, you would fixate on the blob (target or distractor) an then an <strong>additional little blob would pop up</strong>. Looking at it would trigger the laser gaze and would destroy the target. This interaction can be done in <strong>less than 100ms</strong>, which is <strong>twice faster</strong> than mouse-based interaction.</p><h3>Counteracting Golden Touch with Non-Ocular Muscles</h3><p>If we would like to use some <strong>other muscles</strong> other than eye muscles, there is a <strong>ton of solutions</strong>. <em>Mouse is back, baby!</em> Any clicking mechanism would do. Or just pinching two fingers. Or eye blink. Or flick of your wrist, or head nod, a clap, a finger snap, a tongue click, or teeth chattering for some weird use cases. But really <strong>any muscle</strong> would do. Muscles of the <strong>pelvic floor</strong> being the <strong>most fun</strong>.</p><figure><p><iframe src="https://www.youtube-nocookie.com/embed/EJawhH-m_TE?start=0&amp;end=11&amp;rel=0&amp;modestbranding=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><figcaption>Apple Assistive Touch â€“ 11sec</figcaption></figure><p>There doesn&#39;t have to be <strong>only single method</strong> suitable for triggering the action. You can <strong>mix-and-match</strong> techniques together to create <strong>unique interactive experiences</strong>.</p><figure><p><iframe src="https://www.youtube-nocookie.com/embed/RJ2bCnX5Sh0?start=3188&amp;end=3215&amp;rel=0&amp;modestbranding=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><figcaption>Examples of eye gaze tracking for UI interactions â€“ 27sec</figcaption></figure><p>And of course, hands are still there, so you can <strong>combine</strong> eye gaze with hand gestures. This is a <strong>powerful combination</strong> and <strong>makes eye gaze tracking</strong> a <strong>very versatile</strong> interaction technique.</p><figure><p><iframe src="https://www.youtube-nocookie.com/embed/NzLrZSF8aDM?start=0&amp;end=0&amp;rel=0&amp;modestbranding=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><figcaption>Gaze + Pinch interaction (2017) by <a href="https://kenpfeuffer.com">Ken Pfeuffer</a> â€“ 2min 22sec</figcaption></figure><h2>Eye as a Camera in Virtual Reality</h2><p>Gaze-tracking can be exploited in optimizing rendering performance. <strong>Foveated-rendering</strong> is a natural candidate â€“ you render crisp image around gaze point and render progressively worse image at periphery.</p><figure><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzg0MCIgaGVpZ2h0PSIyMTYwIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIvPg=="/></span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span><figcaption>Foveated-rendering</figcaption></figure><p>Using our knowledge about the eye, we can design this image to be <strong>perceptually identical</strong> to the <strong>fully rendered image</strong>, while using just a <strong>fraction of computing power</strong>.</p><figure><p><iframe src="https://www.youtube-nocookie.com/embed/lNX0wCdD2LA?start=0&amp;end=0&amp;rel=0&amp;modestbranding=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><figcaption>Perceptually-based Foveated Virtual Reality â€“ 1m20s</figcaption></figure><p>We can even incorporate information about <strong>blinking</strong> and <strong>active saccade</strong> â€“ during these periods we are almost <strong>blind</strong>, so we can <strong>lower computation cost</strong> even more. Ever heard of <strong>blind spot</strong>? We don&#39;t have to render anything there.</p><p>Or we can go further and <strong>intentionally degrade image</strong> to simulate <strong>disabilities</strong>. For example, with <strong>macular degeneration</strong>, you can&#39;t really look directly at things because there is not enough information, so you have to look to the sides. Trying out sight loss due to <strong>cataracts and glaucomas</strong> might be an <strong>eye-opening experience</strong> for healthy people.</p><figure><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNTAwIiBoZWlnaHQ9IjI5NiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2ZXJzaW9uPSIxLjEiLz4="/></span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span><figcaption>Age-related Macular Degeneration leads to loss of vision</figcaption></figure><p>But <strong>enhancing the image</strong> is probably the most interesting application for rendering. <strong>Color-blind people</strong> might be able to <strong>differentiate</strong> between colors they normally can&#39;t. Or <strong>enhancing healthy human vision</strong> to improve performance in some tasks. Also, an entire new class of <strong>optical illusions</strong> might arise thanks to gaze-optimized rendering.</p><h2>Social Aspects of Gaze-tracking</h2><p>Having a <strong>face-to-face conversation</strong> in Virtual Reality can&#39;t be done properly without seeing eyes of the other person. <strong>Eyes are the windows to the soul.</strong> Collaborating with someone without having signs of a <strong>shared attention</strong> is a frustrating experience.</p><p>Producing visual content that <strong>maximizes possibility of a fixation</strong> seems like user experience pattern that will be <strong>loved</strong> by the designers, but <strong>hated</strong> by the users. <strong>Attention-conditioned ads</strong> seems like an absolute hell.</p><p><strong>Gaze-tracking data</strong> can be used to improve <strong>deepfakes</strong> â€“ we fixate our gaze on <strong>out-of-distribution facial features</strong> immediately, so we could leverage this information to <strong>minimize</strong> such features.</p><p><strong>Reading</strong> might be pretty <strong>fruitful exploration</strong> topic. As you can currently experience, your eyes are jumping from word to word, <strong>subconsciously picking</strong> out points where to <strong>look next</strong>. Sometimes there is a <strong>bold text</strong> that tells you there is something <strong>important</strong> ahead of you. If we could <strong>minimize distraction</strong> during longer reading sessions, we could dramatically <strong>improve comprehension</strong> of reading material. <strong>Dyslexia? Gone.</strong></p><p><strong>Writing</strong> might see a revolution too. <strong>Gaze swiping</strong> across a keyboard might turn out to be faster input method than thumb typing. As added bonus, we have our hands free to do important physical stuff.</p><h2>Gaze Matters</h2><p>As we have seen, gaze-enabled interfaces can deliver a very radical new ways of interacting with the world around us. Affecting world with just a look feels very magical, but for some it may be the only way to communicate their great ideas.</p><figure><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjMwIiBoZWlnaHQ9IjMzMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2ZXJzaW9uPSIxLjEiLz4="/></span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span><figcaption>Eye muscles are usually one of the last affected by Amyotrophic Lateral Sclerosis</figcaption></figure></div></div>
  </body>
</html>
