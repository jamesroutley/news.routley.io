<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.yelinaung.com/posts/gvisor/">Original</a>
    <h1>What is gVisor?</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p>It has been a really long time since I last wrote something here as life happens, things get busier, etc etc.
I am now trying to get back into writing things down and here we go!</p><p>So, imagine a tool or a service that allows you to run some arbitrary code via a shell. Either through a ssh or more commonly, via a web terminal.
How does these tools isolate your code from other people’s code and vice versa ? How come you cannot see other people code or processes ?</p><p>The first thing you probably be thinking, in 2025, is “Docker”. Each console must be running in their own container, right ?
Very likely that you are right. That’s what I’d think too.</p><p>But, if these containers are all sharing the same operating system kernel, is that always sufficient, especially for untrusted code.</p><p>Let’s briefly revisit how standard Docker containers operate and interact with the host system.</p><p><img src="https://blog.yelinaung.com/docker_architecture.png" alt="Docker Architecture"/>
(Image from <a href="https://subscription.packtpub.com/book/cloud-and-networking/9781787120532/10/ch10lvl1sec57/understanding-docker-s-architecture">Understanding Docker Architecture</a>)</p><p>When you run a container (let’s say <code>ubuntu</code>) without any modifications or changes on a Linux host, it shares the same Kernel as the host OS.</p><p>From inside the container, we are not be able to see the outside processes (and other resources) because of <a href="https://en.wikipedia.org/wiki/Linux_namespaces">Linux namespaces</a>.</p><p>How about the other way around ? Without any flags or anything, typically, the host can see the processes inside the container because they are all sharing the same Kernel/OS and not inherently considered a security risk from the host’s perspective.</p><p>In the below image, notice that the sleep infinity process (PID 576337 in this Host OS) is directly visible on the host. This is the same process we initiated from inside the container, now seen from the host’s perspective</p><p><img src="https://blog.yelinaung.com/docker_same_process.png" alt="Docker Process on Host OS"/></p><p><em>(if the image is too small for you, sorry about that! you can right click on the image and do “Open Image in new Tab”)</em></p><p>If we list all processes inside from within this standard container, as expected, we only see the processes running inside its own isolated environment, starting with PID 1 for <code>/bin/bash</code>.
<img src="https://blog.yelinaung.com/docker_processes.png" alt="Docker Process on Host OS"/></p><p>We can then add <code>--pid=host</code> flag and now we can see everything on the host as well!</p><p><img src="https://blog.yelinaung.com/docker_pid_host.png" alt="Seeing all host processes from the container"/></p><p><em>I didn’t show the full output because it’s too long but you can trust me that they are the same :D</em></p><p>It is worth highlighting that running containers this way significantly reduces isolation and carries substantial security risks.
Never do that unless you fully understand the implications and in trusted environments.</p><p>Even if you avoid dangerous flags like <code>--pid=host</code>, the standard containers running on a shared kernel can still face security issues.
A critical example is <a href="https://aws.amazon.com/blogs/compute/anatomy-of-cve-2019-5736-a-runc-container-escape">CVE-2019-5736: A runc container escape</a>
This vulnerability in <code>runc</code> (the default runtime for Docker) allowed a malicious container to break out of its isolation and gain root-level access on the host system.</p><p>The big question now is how can we run containers, especially untrusted ones, more safely?</p><p>Please enter <a href="https://gvisor.dev/">gVisor</a>.</p><p><img src="https://gvisor.dev/assets/images/gvisor-high-level-arch.png" alt="gVisor Overview"/></p><p>gVisor is an abstraction on top of existing Linux Kernel and acts as a middleman between the container and the Kernel.
It acts as an intermediary, a kind of ‘application kernel’ or ‘user-space kernel’ for the container. This means it emulates the behavior of a normal Linux kernel from the container’s perspective
E.g if I have a program that <code>open()</code> a file and <code>write()</code> to a file, it will be calling gVisor <em>first</em> instead of Linux. That way, the system calls are intercepted and handled by the a gVisor component called “Sentry”, within the gVisor sandbox, not directly by the host Ubuntu kernel.</p><p><img src="https://gvisor.dev/docs/Layers.png" alt="gVisor interaction"/></p><p>At this point, you might be thinking, fundamental operations that only the host kernel can do (e.g., actually writing bytes to a network card, or reading from a disk still have to reach to the host kernel, right ?</p><p>This is where gVisor gets clever. gVisor is designed to minimize and restrict the types of system calls the Sentry makes to the host kernel. The Sentry operates under a much tighter security policy (using mechanisms like <a href="https://www.kernel.org/doc/html/v4.19/userspace-api/seccomp_filter.html">seccomp-bpf</a>) than a typical container. So, it’s not that gVisor never interacts with the host kernel, but it does so in a much more <strong>controlled and limited way, significantly reducing the attack surface</strong> exposed to the container. This dramatically reduces the attack surface of the host kernel that is directly exposed to the containerized application.&#34;</p><p>It is safer because of a few reasons</p><ul><li>Reduces the host kernel’s exposure: A vulnerability in a system call implementation within gVisor’s Sentry would, at worst, compromise the gVisor sandbox itself, not necessarily the entire host kernel and other containers. The host kernel is shielded from the majority of the direct system calls from the potentially untrusted application.</li><li>Offers a smaller, more auditable attack surface: The Sentry implements many Linux system calls, but not all of them (<a href="https://man7.org/linux/man-pages/man2/syscalls.2.html">there are a lot!</a>), focusing on what typical containerized applications need. Because it’s written in Go (a memory-safe language), it helps avoid many common security pitfalls found in C-based kernels (like buffer overflows, use-after-free, etc.).</li><li>Enforces strong isolation: Even if an attacker breaks out of the application running inside gVisor, they then land in the Sentry. Breaking out of the Sentry to the host system is another, much harder step.</li></ul><p>As always, there are trade-offs. The main obvious one is performance overhead. Because it’s intercepting system calls and doing some operations in the userspace, there’s more overhead compared to a direct native system call. e.g apps that do very frequently I/O operations like reading/writing many files will be impacted by this. Other downsides might be debugging. We now have an additional component that I have to check/audit whenever there’s an issue.</p><p>Now, let me demonstrate. I have <a href="https://gvisor.dev/docs/user_guide/install/">installed gVisor</a> (and its components) on my Linux system. I followed the <a href="https://gvisor.dev/docs/user_guide/quick_start/docker/">Docker quick start</a> guide.</p><p>We can start a container the same way but with gVisor’s <code>runsc</code> runtime <code>--runtime=runsc</code> flag.</p><p><img src="https://blog.yelinaung.com/gvisor1.png" alt="gVisor container"/></p><p>What happens if we do <code>sleep infnity</code> inside the container with gVisor ?</p><p><img src="https://blog.yelinaung.com/gvisor_sleep.png" alt="gVisor container running sleep infinity"/></p><p>Indeed, the Host OS cannot see sleep infinity directly! This is because the container’s sleep process is managed and contained entirely within gVisor Sentry’s environment.</p><p>It is because that <code>sleep</code> process is being managed and contained entirely within the gVisor Sentry’s environment. But we still see it if we do <code>ps aux</code> inside the container itself.
From the Sentry’s perspective, sleep infinity is certainly a process running within its emulated kernel environment (as seen in your container’s ps aux output ). However, crucially, it doesn’t have a directly corresponding distinct Process ID (PID) on the host OS, unlike standard Docker container processes.</p><blockquote><p>The <code>sleep</code> command, especially when asked to sleep for a long duration or indefinitely, will use a clock_nanosleep system call to tell the kernel to pause its execution for a specified amount of time. In this case, <code>infinity</code></p></blockquote><p>Let’s try another simple command like <code>echo</code> but observe the gVisor logs (located in <code>/tmp/runsc</code> on Host) with some <a href="https://gvisor.dev/docs/user_guide/debugging/">debugging settings enabled</a>.</p><p><img src="https://blog.yelinaung.com/gvisor_hello.png" alt="gVisor container running sleep infinity"/></p><p>Every command I run, every file access, every network operation inside the container triggers system calls. When using gVisor, the Sentry intercepts all these system calls. The debug logs clearly show the Sentry actively processing them. The logs are showing that the Sentry processing those syscalls. If you were to strace a process in a standard Docker container, we’d see similar syscalls, but they’d be going directly to your host kernel. With <code>runsc</code>, the Sentry is the first recipient.</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>I0521 14:33:13.906822       1 strace.go:605] [   1:   1] sh X read(0x0 host:[1], 0x55ff8db97ac0 &#34;echo \&#34;Hello world\&#34;\n&#34;, 0x2000) = 19 (0x13) (11m46.592876238s)
</span></span><span><span>I0521 14:33:13.906902       1 strace.go:567] [   1:   1] sh E write(0x1 host:[1], 0x55ff8db9a6c0 &#34;Hello world\n&#34;, 0xc)
</span></span><span><span>I0521 14:33:13.906917       1 strace.go:605] [   1:   1] sh X write(0x1 host:[1], ..., 0xc) = 12 (0xc) (5.25µs)
</span></span><span><span>I0521 14:33:13.906933       1 strace.go:567] [   1:   1] sh E write(0x2 host:[1], 0x55ff8db9a470 &#34;# &#34;, 0x2)
</span></span><span><span>I0521 14:33:13.906942       1 strace.go:605] [   1:   1] sh X write(0x2 host:[1], ..., 0x2) = 2 (0x2) (2.295µs)
</span></span><span><span>I0521 14:33:13.906964       1 strace.go:567] [   1:   1] sh E read(0x0 host:[1], 0x55ff8db97ac0, 0x2000)
</span></span></code></pre></div><ul><li><code>[ 1: 1] sh X read(...)</code>: This shows the Sentry handling a <code>read</code> call for process ID 1 (which is the <code>sh</code> shell) inside its sandbox. The <code>X</code> probably means “exit” from the syscall. (<a href="https://github.com/google/gvisor/blob/master/pkg/sentry/strace/strace.go#L583">strace.go</a>)</li><li><code>[ 1: 1] sh E write(...)</code>: This shows the Sentry handling the <code>write</code> call (to print “Hello world!”) for that same process sh. The <code>E</code> probably means “entry” into the syscall (<a href="https://github.com/google/gvisor/blob/master/pkg/sentry/strace/strace.go#L553">strace.go</a>)</li><li>And then repeat the process to print out <code>#</code> to accept a new command</li></ul><p>I hope you get the idea here of what gVisor does and how it <em>roughy</em> works.</p><p>How are people using it ?</p><ul><li>List of <a href="https://gvisor.dev/users/">usecase/users</a> from the official site</li><li>The first generation of GCP CloudRun used <code>gVisor</code> as noted <a href="https://cloud.google.com/run/docs/container-contract#sandbox">here</a> but I read that they have moved back to hypervisors/plain Linux Kernel for being more performant for more common workloads.</li><li>GKE supports <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/sandbox-pods">Sandbox</a>, which uses gVisor.</li><li><a href="https://fly.io/blog/sandboxing-and-workload-isolation/">Fly.io</a> <em>considered</em> using gVisor but opted to go with Firecracker MicroVM instead. Nonetheless, they still have good things to say about it.</li></ul><p>So, to recap,</p><ul><li>If you are running a multi-tenant system with containers, especially that allows people to do whatever they want or having something as defense-in-depth, you may wanna consider having gVisor as a layer.</li><li>Always remember the inherent trade-off: balancing the enhanced security gVisor provides against potential performance overhead</li></ul></div></div></div></div>
  </body>
</html>
