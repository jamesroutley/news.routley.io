<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://datafusion.apache.org/blog/2025/07/14/user-defined-parquet-indexes/">Original</a>
    <h1>Embedding user-defined indexes in Apache Parquet</h1>
    
    <div id="readability-page-1" class="page"><div id="contents">
    <div>
        <div>
          
            <p>Posted on: Mon 14 July 2025 by Qi Zhu (Cloudera), Jigao Luo (Systems Group at TU Darmstadt), and Andrew Lamb (InfluxData)</p>
            <!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
-->
<p>It’s a common misconception that <a href="https://parquet.apache.org/">Apache Parquet</a> files are limited to basic Min/Max/Null Count statistics and Bloom filters, and that adding more advanced indexes requires changing the specification or creating a new file format. In fact, footer metadata and offset-based addressing already provide everything needed to embed user-defined index structures within Parquet files without breaking compatibility with other Parquet readers.</p>
<p><strong>Motivating Example:</strong> Imagine your data has a <code>Nation</code> column with dozens of distinct values across thousands of Parquet files. You execute:</p>
<pre><code>  SELECT AVG(sales_amount)
  FROM sales
  WHERE nation = &#39;Singapore&#39;
  GROUP BY year;
</code></pre>
<p>Relying on the min/max statistics from the Parquet format will be ineffective at pruning files when <code>Nation</code> spans &#34;Argentina&#34; through &#34;Zimbabwe&#34;. Instead of relying on a Bloom Filter, you may want to store a list of every distinct <code>Nation</code> value in the file near the end. At query time, your engine will read that tiny list and skip any file that does not contain &#39;Singapore&#39;. This special distinct value index can yield dramatically better file‑pruning performance for your engine, all while preserving full compatibility with standard Parquet readers.</p>
<p>In this post, we review how indexes are stored in the Apache Parquet format, explain the mechanism for storing user-defined indexes, and finally show how to read and write a user-defined index using <a href="https://datafusion.apache.org/">Apache DataFusion</a>.</p>
<h2>Introduction</h2>
<hr/>
<p>Apache Parquet is a popular columnar file format with well understood and <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">production grade libraries for high‑performance analytics</a>. Features like efficient encodings, column pruning, and predicate pushdown work well for many common query patterns. Apache DataFusion includes a <a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning/">highly optimized Parquet implementation</a> and has excellent performance in general. However, some production query patterns require more than the statistics included in the Parquet format itself<sup><a href="#footnote1">1</a></sup>.</p>
<p>Many systems improve query performance using <em>external</em> indexes or other metadata in addition to Parquet. For example, Apache Iceberg&#39;s <a href="https://iceberg.apache.org/docs/latest/performance/#scan-planning">Scan Planning</a> uses metadata stored in separate files or an in memory cache, and the <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs">parquet_index.rs</a> and <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs">advanced_parquet_index.rs</a> examples in the DataFusion repository use external files for Parquet pruning (skipping).</p>
<p>External indexes are powerful and widespread, but they have some drawbacks:</p>
<ul>
<li><strong>Increased Cost and Operational Complexity:</strong> You need additional files and systems as well as the original Parquet. </li>
<li><strong>Synchronization Risks:</strong> The external index may become out of sync with the Parquet data if you do not manage it carefully.</li>
</ul>
<p>Proponents have even cited these drawbacks as justification for new file formats, such as Microsoft&#39;s <a href="https://github.com/microsoft/amudai/blob/main/docs/spec/src/what_about_parquet.md">Amudai</a>.</p>
<p><strong>However, Parquet is extensible with user-defined indexes</strong>: Parquet tolerates unknown bytes within the file body and permits arbitrary key/value pairs in its footer metadata. These two features enable <strong>embedding</strong> user-defined indexes directly in the file—no extra files, no format forks, and no compatibility breakage. </p>
<h2>Parquet File Anatomy &amp; Standard Index Structures</h2>
<hr/>
<p>Logically, Parquet files contain row groups, each with column chunks, which in turn contain data pages. Physically, a Parquet file is a sequence of bytes with a Thrift-encoded footer metadata containing metadata about the file structure. The footer metadata includes the schema, row groups, column chunks, and other metadata required to read the file.</p>
<p>The Parquet format includes three main types<sup><a href="#footnote2">2</a></sup> of optional index structures:</p>
<ol>
<li>
<p><strong><a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L263-L266">Min/Max/Null Count Statistics</a></strong> for each chunk in a row group. Engines use these to quickly skip row groups that do not match a query predicate. </p>
</li>
<li>
<p><strong><a href="https://parquet.apache.org/docs/file-format/pageindex/">Page Index</a></strong>: Offsets, sizes, and statistics for each data page. Engines use these to quickly locate data pages without scanning all pages for a column chunk.</p>
</li>
<li>
<p><strong><a href="https://parquet.apache.org/docs/file-format/bloomfilter/">Bloom Filters</a></strong>: Data structure to quickly determine if a value is present in a column chunk without scanning any data pages. Particularly useful for equality and <code>IN</code> predicates.</p>
</li>
</ol>
<!-- Source: https://docs.google.com/presentation/d/1aFjTLEDJyDqzFZHgcmRxecCvLKKXV2OvyEpTQFCNZPw -->
<p><img alt="Parquet File layout with standard index structures." src="https://datafusion.apache.org/blog/images/user-defined-parquet-indexes/standard_index_structures.png" width="80%"/></p>
<p><strong>Figure 1</strong>: Parquet file layout with standard index structures (as written by arrow-rs).</p>
<p>Only the Min/Max/Null Count Statistics are stored inline in the Parquet footer metadata. The Page Index and Bloom Filters are typically stored in the file body before the Thrift-encoded footer metadata. The locations of these index structures are recorded in the footer metadata, as shown in Figure 1. Parquet readers that do not understand these structures simply ignore them.</p>
<p>Modern Parquet writers create these indexes automatically and provide APIs to control their generation and placement. For example, the <a href="https://docs.rs/parquet/latest/parquet/">Rust Parquet Library</a> provides <a href="https://docs.rs/parquet/latest/parquet/file/properties/struct.WriterProperties.html">Parquet WriterProperties</a>, <a href="https://docs.rs/parquet/latest/parquet/file/properties/enum.EnabledStatistics.html">EnabledStatistics</a>, and <a href="https://docs.rs/parquet/latest/parquet/file/properties/enum.BloomFilterPosition.html">BloomFilterPosition</a>.</p>
<h2>Embedding User Defined Indexes in Parquet Files</h2>
<hr/>
<p>Embedding user-defined indexes in Parquet files is straightforward and follows the same principles as standard index structures:</p>
<ol>
<li>
<p>Serialize the index into a binary format and write it into the file body before the Thrift-encoded footer metadata.</p>
</li>
<li>
<p>Record the index location in the footer metadata as a key/value pair, such as <code>&#34;my_index_offset&#34; -&gt; &#34;&lt;byte-offset&gt;&#34;</code>.</p>
</li>
</ol>
<p>Figure 2 shows the resulting file layout.</p>
<!-- Source: https://docs.google.com/presentation/d/1aFjTLEDJyDqzFZHgcmRxecCvLKKXV2OvyEpTQFCNZPw -->
<p><img alt="Parquet File layout with custom index structures." src="https://datafusion.apache.org/blog/images/user-defined-parquet-indexes/custom_index_structures.png" width="80%"/></p>
<p><strong>Figure 2</strong>: Parquet file layout with user-defined indexes.</p>
<p>Like standard index structures, user-defined indexes can be stored anywhere in the file body, such as after row group data or before the footer. There is no limit to the number of user-defined indexes, nor any restriction on their granularity: they can operate at the file, row group, page, or even row level. This flexibility enables a wide range of use cases, including:</p>
<ol>
<li>
<p>Row group or page-level distinct sets: a finer-grained version of the file-level example in this blog.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> sketches for distinct value estimation, addressing a common criticism<sup>3</sup> of Parquet’s lack of cardinality estimation.</p>
</li>
<li>
<p>Additional zone maps (<a href="https://www.vldb.org/conf/1998/p476.pdf">small materialized aggregates</a>) such as precomputed <code>sum</code>s at the column chunk or data page level for faster query execution.</p>
</li>
<li>
<p>Histograms or samples at the row group or column chunk level for predicate selectivity estimates.</p>
</li>
</ol>
<h2>Example: Embedding a User Defined Distinct Value Index in Parquet Files</h2>
<hr/>
<p>This section demonstrates how to embed a simple distinct value index in Parquet files and use it for file-level pruning (skipping) in DataFusion. The full example is available in the DataFusion repository at <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_embedded_index.rs">parquet_embedded_index.rs</a>.</p>
<p>Note that the example requires <strong><a href="https://crates.io/crates/parquet/55.2.0">arrow‑rs v55.2.0</a></strong> or later, which includes the new “buffered write” API (<a href="https://github.com/apache/arrow-rs/pull/7714">apache/arrow-rs#7714</a>) to keep the internal byte count in sync after appending index bytes immediately after data pages.</p>
<p>This example is intentionally simple for clarity, but you can adapt the same approach for any index type or data types. The high-level design is:</p>
<ol>
<li>
<p><strong>Define your index payload</strong> (e.g., bitmap, Bloom filter, sketch, distinct values list, etc.).</p>
</li>
<li>
<p><strong>Serialize your index to bytes</strong> and append them into the Parquet file body before writing the footer.</p>
</li>
<li>
<p><strong>Record the index location</strong> by adding a key/value entry (e.g., <code>&#34;my_index_offset&#34; -&gt; &#34;&lt;byte‑offset&gt;&#34;</code>) in the Parquet footer metadata.</p>
</li>
<li>
<p><strong>Extend DataFusion</strong> with a custom <code>TableProvider</code> (or wrap the existing Parquet provider) to use the index.</p>
</li>
</ol>
<p>The <code>TableProvider</code> simply reads the footer metadata to discover the index offset, seeks to that offset and deserializes the index, and then uses the index to speed up processing (e.g., skip files, row groups, data pages, etc.).</p>
<p>The resulting Parquet files remain fully compatible with other tools such as DuckDB and Spark, which simply ignore the unknown index bytes and key/value metadata.</p>
<h3>Introduction to Distinct Value Indexes</h3>
<hr/>
<p>A <strong>distinct value index</strong> stores the unique values of a specific column. This type of index is effective for columns with a small number of distinct values and can be used to quickly skip files that do not match the query. These indexes are popular in several engines, such as the <a href="https://clickhouse.com/docs/optimize/skipping-indexes#set">&#34;set&#34; Skip Index in ClickHouse</a> and the <a href="https://docs.influxdata.com/influxdb3/enterprise/admin/distinct-value-cache/">Distinct Value Cache</a> in InfluxDB 3.0.</p>
<p>For example, if the files contain a column named <code>Category</code> like this:</p>
<table>
<tbody><tr>
<td><b><code>Category</code></b></td>
</tr>
<tr>
<td><code>foo</code></td>
</tr>
<tr>
<td><code>bar</code></td>
</tr>
<tr>
<td><code>...</code></td>
</tr>
<tr>
<td><code>baz</code></td>
</tr>
<tr>
<td><code>foo</code></td>
</tr>
</tbody></table>
<p>The distinct value index will contain the values <code>foo</code>, <code>bar</code>, and <code>baz</code>. In contrast, traditional min/max statistics would store only the minimum (<code>bar</code>) and maximum (<code>foo</code>) values, so a query like</p>
<pre><code>SELECT * FROM t WHERE Category = &#39;bas&#39;
</code></pre>
<p>cannot skip the file using min/max values because <code>bas</code> falls between <code>bar</code> and <code>foo</code> in lexicographic order, even though <code>bas</code> does not appear in the column.</p>
<p>This is a key benefit of a distinct value index: accurate filtering without requiring the column to be sorted, unlike min/max-based pruning which is most effective when data is ordered.</p>
<p>While not a traditional index structure like a B-tree, the distinct value set acts as a lightweight, embedded index that enables fast pruning and is especially effective for columns with low cardinality.</p>
<p><strong>Supported Filters</strong></p>
<p>Distinct value indexes are most effective for <strong>equality filters</strong>, such as:</p>
<pre><code>WHERE category = &#39;foo&#39;
WHERE category IN (&#39;foo&#39;, &#39;bar&#39;)
</code></pre>
<p>They can also help with NOT IN and anti-joins, as long as the engine can evaluate them using the list of known distinct values.</p>
<p>However, these indexes are not suitable for range predicates (e.g., category &gt; &#39;foo&#39;), as they do not preserve any ordering information. For such cases, other structures such as min/max statistics or sorted data layouts may be more effective.</p>
<p>We represent a distinct value index in Rust for our example as a simple <code>HashSet&lt;String&gt;</code>:</p>
<pre><code>/// An index of distinct values for a single column
#[derive(Debug, Clone)]
struct DistinctIndex {
   inner: HashSet&lt;String&gt;,
}
</code></pre>
<h3>File Layout with Distinct Value Index</h3>
<hr/>
<p>In this example, we write a distinct value index for the <code>Category</code> column into the Parquet file body after all the data pages, and record the index location in the footer metadata. The resulting file layout looks like this:</p>
<pre><code>                  ┌──────────────────────┐                           
                  │┌───────────────────┐ │                           
                  ││     DataPage      │ │                           
                  │└───────────────────┘ │                           
 Standard Parquet │┌───────────────────┐ │                           
 Data Pages       ││     DataPage      │ │                           
                  │└───────────────────┘ │                           
                  │        ...           │                           
                  │┌───────────────────┐ │                           
                  ││     DataPage      │ │                           
                  │└───────────────────┘ │                           
                  │┏━━━━━━━━━━━━━━━━━━━┓ │                           
Non standard      │┃                   ┃ │                           
index (ignored by │┃Custom Binary Index┃ │                           
other Parquet     │┃ (Distinct Values) ┃◀│─ ─ ─                      
readers)          │┃                   ┃ │     │                     
                  │┗━━━━━━━━━━━━━━━━━━━┛ │                           
Standard Parquet  │┏━━━━━━━━━━━━━━━━━━━┓ │     │  key/value metadata
Page Index        │┃    Page Index     ┃ │        contains location  
                  │┗━━━━━━━━━━━━━━━━━━━┛ │     │  of special index   
                  │╔═══════════════════╗ │                           
                  │║ Parquet Footer w/ ║ │     │                     
                  │║     Metadata      ║ ┼ ─ ─                       
                  │║ (Thrift Encoded)  ║ │                           
                  │╚═══════════════════╝ │                           
                  └──────────────────────┘                           

</code></pre>
<h3>Serializing the Distinct‑Value Index</h3>
<hr/>
<p>The example uses a simple newline‑separated UTF‑8 format as the binary format. The code to serialize the distinct index is shown below:</p>
<pre><code>/// Magic bytes to identify our custom index format
const INDEX_MAGIC: &amp;[u8] = b&#34;IDX1&#34;;

/// Serialize the distinct index to a writer as bytes
fn serialize&lt;W: Write + Send&gt;(
   &amp;self,
   arrow_writer: &amp;mut ArrowWriter&lt;W&gt;,
) -&gt; Result&lt;()&gt; {
   let serialized = self
           .inner
           .iter()
           .map(|s| s.as_str())
           .collect::&lt;Vec&lt;_&gt;&gt;()
           .join(&#34;\n&#34;);
   let index_bytes = serialized.into_bytes();

   // Set the offset for the index
   let offset = arrow_writer.bytes_written();
   let index_len = index_bytes.len() as u64;

   // Write the index magic and length to the file
   arrow_writer.write_all(INDEX_MAGIC)?;
   arrow_writer.write_all(&amp;index_len.to_le_bytes())?;

   // Write the index bytes
   arrow_writer.write_all(&amp;index_bytes)?;

   // Append metadata about the index to the Parquet file footer metadata
   arrow_writer.append_key_value_metadata(KeyValue::new(
      &#34;distinct_index_offset&#34;.to_string(),
      offset.to_string(),
   ));
   Ok(())
}
</code></pre>
<p>This code does the following:</p>
<ol>
<li>
<p>Creates a newline‑separated UTF‑8 string from the distinct values.</p>
</li>
<li>
<p>Writes a magic header (<code>IDX1</code>) and the length of the index.</p>
</li>
<li>
<p>Writes the index bytes to the file using the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_writer/struct.ArrowWriter.html">ArrowWriter</a> API.</p>
</li>
<li>
<p>Records the index location by adding a key/value entry (<code>&#34;distinct_index_offset&#34; -&gt; &lt;offset&gt;</code>) in the Parquet footer metadata.</p>
</li>
</ol>
<p>Note: Use the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_writer/struct.ArrowWriter.html#method.write_all">ArrowWriter::write_all</a> API to ensure the offsets in the footer metadata are correctly tracked. </p>
<h3>Reading the Index</h3>
<hr/>
<p>This code reads the distinct index from a Parquet file:</p>
<pre><code>/// Read a `DistinctIndex` from a Parquet file
fn read_distinct_index(path: &amp;Path) -&gt; Result&lt;DistinctIndex&gt; {
    let file = File::open(path)?;

    let file_size = file.metadata()?.len();
    println!(&#34;Reading index from {} (size: {file_size})&#34;, path.display(), );

    let reader = SerializedFileReader::new(file.try_clone()?)?;
    let meta = reader.metadata().file_metadata();

    let offset = get_key_value(meta, &#34;distinct_index_offset&#34;)
        .ok_or_else(|| ParquetError::General(&#34;Missing index offset&#34;.into()))?
        .parse::&lt;u64&gt;()
        .map_err(|e| ParquetError::General(e.to_string()))?;

    println!(&#34;Reading index at offset: {offset}, length&#34;);
    DistinctIndex::new_from_reader(file, offset)
}
</code></pre>
<p>This function:</p>
<ol>
<li>
<p>Opens the Parquet footer metadata and extracts <code>distinct_index_offset</code> from the metadata.</p>
</li>
<li>
<p>Calls <code>DistinctIndex::new_from_reader</code> to read the index from the file at that offset.</p>
</li>
</ol>
<p><code>DistinctIndex::new_from_reader</code> actually reads the index as shown below:</p>
<pre><code> /// Read the distinct values index from a reader at the given offset and length
 pub fn new_from_reader&lt;R: Read + Seek&gt;(mut reader: R, offset: u64) -&gt; Result&lt;DistinctIndex&gt; {
     reader.seek(SeekFrom::Start(offset))?;

     let mut magic_buf = [0u8; 4];
     reader.read_exact(&amp;mut magic_buf)?;
     if magic_buf != INDEX_MAGIC {
         return exec_err!(&#34;Invalid index magic number at offset {offset}&#34;);
     }

     let mut len_buf = [0u8; 8];
     reader.read_exact(&amp;mut len_buf)?;
     let stored_len = u64::from_le_bytes(len_buf) as usize;

     let mut index_buf = vec![0u8; stored_len];
     reader.read_exact(&amp;mut index_buf)?;

     let Ok(s) = String::from_utf8(index_buf) else {
         return exec_err!(&#34;Invalid UTF-8 in index data&#34;);
     };

     Ok(Self {
         inner: s.lines().map(|s| s.to_string()).collect(),
     })
 }
</code></pre>
<p>This code:</p>
<ol>
<li>
<p>Seeks to the offset of the index in the file.</p>
</li>
<li>
<p>Reads the magic bytes and checks they match <code>IDX1</code>.</p>
</li>
<li>
<p>Reads the length of the index and allocates a buffer.</p>
</li>
<li>
<p>Reads the index bytes, converts them to a <code>String</code>, and splits into lines to populate the <code>HashSet&lt;String&gt;</code>.</p>
</li>
</ol>
<h3>Extending DataFusion’s <code>TableProvider</code></h3>
<hr/>
<p>To use the distinct index for file-level pruning, extend DataFusion&#39;s <code>TableProvider</code> to read the index and apply it during query execution:</p>
<pre><code>impl TableProvider for DistinctIndexTable {
    /* ... */

    /// Prune files before reading: only keep files whose distinct set
    /// contains the filter value
    async fn scan(
        &amp;self,
        _ctx: &amp;dyn Session,
        _proj: Option&lt;&amp;Vec&lt;usize&gt;&gt;,
        filters: &amp;[Expr],
        _limit: Option&lt;usize&gt;,
    ) -&gt; Result&lt;Arc&lt;dyn ExecutionPlan&gt;&gt; {
        // This example only handles filters of the form
        // `category = &#39;X&#39;` where X is a string literal
        //
        // You can use `PruningPredicate` for much more general range and
        // equality analysis or write your own custom logic.
        let mut target: Option&lt;&amp;str&gt; = None;

        if filters.len() == 1 {
            if let Expr::BinaryExpr(expr) = &amp;filters[0] {
                if expr.op == Operator::Eq {
                    if let (
                        Expr::Column(c),
                        Expr::Literal(ScalarValue::Utf8(Some(v)), _),
                    ) = (&amp;*expr.left, &amp;*expr.right)
                    {
                        if c.name == &#34;category&#34; {
                            println!(&#34;Filtering for category: {v}&#34;);
                            target = Some(v);
                        }
                    }
                }
            }
        }
        // Determine which files to scan
        // files_and_index is a Vec&lt;(String, DistinctIndex)&gt;,
        // See the full example for how this is populated.
        let files_to_scan: Vec&lt;_&gt; = self
            .files_and_index
            .iter()
            .filter_map(|(f, distinct_index)| {
                // keep file if no target or target is in the distinct set
                if target.is_none() || distinct_index.contains(target?) {
                    Some(f)
                } else {
                    None
                }
            })
            .collect();

        // Build ParquetSource to actually read the files
        let url = ObjectStoreUrl::parse(&#34;file://&#34;)?;
        let source = Arc::new(ParquetSource::default().with_enable_page_index(true));
        let mut builder = FileScanConfigBuilder::new(url, self.schema.clone(), source);
        for file in files_to_scan {
            let path = self.dir.join(file);
            let len = std::fs::metadata(&amp;path)?.len();
           // If the index contained information about row groups or pages,
           // you could also pass that information here to further prune
           // the data read from the file.
           let partitioned_file =
                   PartitionedFile::new(path.to_str().unwrap().to_string(), len);
           builder = builder.with_file(partitioned_file);
        }
        Ok(DataSourceExec::from_data_source(builder.build()))
    }

    /// Tell DataFusion that we can handle filters on the &#34;category&#34; column
    fn supports_filters_pushdown(
        &amp;self,
        fs: &amp;[&amp;Expr],
    ) -&gt; Result&lt;Vec&lt;TableProviderFilterPushDown&gt;&gt; {
        // Mark as inexact since pruning is file‑granular
        Ok(vec![TableProviderFilterPushDown::Inexact; fs.len()])
    }
}

</code></pre>
<p>This code does the following:</p>
<ol>
<li>
<p>Implements the <code>scan</code> method to filter files based on the distinct index.</p>
</li>
<li>
<p>Checks if the filter is an equality predicate on the <code>category</code> column.</p>
</li>
<li>
<p>If the target value is specified, checks if the distinct index contains that value.</p>
</li>
<li>
<p>Builds a <code>FileScanConfig</code> with only the files that match the filter.</p>
</li>
</ol>
<h3>Putting It All Together</h3>
<p>To use the distinct index in a DataFusion query, write sample Parquet files with the embedded index, register the <code>DistinctIndexTable</code> provider, and run a query with a predicate that can be optimized by the index as shown below.</p>
<pre><code>// Write sample files with embedded indexes
tmp_dir.iter().for_each(|(name, vals)| {
    write_file_with_index(&amp;dir.join(name), vals).unwrap();
});

// Register provider and query
let provider = Arc::new(DistinctIndexTable::try_new(dir, schema.clone())?);
ctx.register_table(&#34;t&#34;, provider)?;

// Only files containing &#39;foo&#39; will be scanned
let df = ctx.sql(&#34;SELECT * FROM t WHERE category = &#39;foo&#39;&#34;).await?;
df.show().await?;
</code></pre>
<h3>Verifying Compatibility with DuckDB</h3>
<hr/>
<p>Even with extra bytes and unknown metadata keys, standard Parquet readers ignore the index. You can verify this using another system such as DuckDB to read the Parquet created in the example. DuckDB will read the files without any issues, ignoring the custom index and unknown footer metadata.</p>
<pre><code>SELECT * FROM read_parquet(&#39;/tmp/parquet_index_data/*&#39;);
┌──────────┐
│ category │
│ varchar  │
├──────────┤
│ foo      │
│ bar      │
│ foo      │
│ baz      │
│ qux      │
│ foo      │
│ quux     │
│ quux     │
└──────────┘
</code></pre>
<h2>Conclusion</h2>
<p>In this post, we explained how index structures are stored in Apache Parquet, how to embed user-defined indexes without changing the format, and how to use user-defined indexes to speed up query processing.</p>
<p>Parquet-based systems can achieve significant performance improvements for almost any query pattern while still retaining broad compatibility, using user-defined embedded indexes, external indexes<sup><a href="#footnote4">4</a></sup> and rewriting files optimized for specific queries<sup><a href="#footnote5">5</a></sup>. System designers can choose among the available options to make the appropriate trade-offs between operational complexity, performance, file size, and cost for their specific use cases.</p>
<p>We hope this post inspires you to explore custom indexes in Parquet files, rather than proposing new file formats and reimplementing existing features. The DataFusion community is excited to see how you use this feature in your projects!</p>
<h2>About the Authors</h2>
<p><a href="https://www.linkedin.com/in/qi-zhu-862330119/">Qi Zhu</a> is a Senior Engineer at <a href="https://www.cloudera.com/">Cloudera</a>, an active contributor to <a href="https://datafusion.apache.org/">Apache DataFusion</a> and <a href="https://arrow.apache.org/">Apache Arrow</a>, a committer on <a href="https://hadoop.apache.org/">Apache Hadoop</a> and <a href="https://yunikorn.apache.org/">Apache YuniKorn</a>. He has extensive experience in distributed systems, scheduling, and large-scale computing.</p>
<p><a href="https://www.linkedin.com/in/jigao-luo/">Jigao Luo</a> is a 1.5-year PhD student at
<a href="https://tuda.systems">Systems Group @ TU Darmstadt</a>. Regarding Parquet, he is an external 
contributor to <a href="https://github.com/rapidsai/cudf">NVIDIA RAPIDS cuDF</a>, focusing on the GPU Parquet reader.</p>
<p><a href="https://www.linkedin.com/in/andrewalamb/">Andrew Lamb</a> is a Staff Engineer at
<a href="https://www.influxdata.com/">InfluxData</a>, and a member of the <a href="https://datafusion.apache.org/">Apache
DataFusion</a> and <a href="https://arrow.apache.org/">Apache Arrow</a> PMCs. He has been working on
Databases and related systems more than 20 years.</p>
<h2>About DataFusion</h2>
<p><a href="https://datafusion.apache.org/">Apache DataFusion</a> is an extensible query engine toolkit, written
in Rust, that uses <a href="https://arrow.apache.org/">Apache Arrow</a> as its in-memory format. DataFusion and
similar technology are part of the next generation “Deconstructed Database”
architectures, where new systems are built on a foundation of fast, modular
components, rather than as a single tightly integrated system.</p>
<p>The <a href="https://datafusion.apache.org/contributor-guide/communication.html">DataFusion community</a> is always looking for new contributors to help
improve the project. If you are interested in learning more about how query
execution works, help document or improve the DataFusion codebase, or just try
it out, we would love for you to join us.</p>
<h3>Footnotes</h3>
<p><a id="footnote1"></a><code>1</code>: A commonly cited example is highly selective predicates (e.g. <code>category = &#39;foo&#39;</code>) but for which the built in BloomFilters are not sufficient.</p>
<p><a id="footnote2"></a><code>2</code>: There are other index structures, but they are either 1) not widely supported (such as statistics in the page headers) or 2) not yet widely used in practice at the time of this writing (such as <a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L256">GeospatialStatistics</a> and <a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L194-L202">SizeStatistics</a>).</p>
<p><a id="footnote3"></a><code>3</code>: <a href="https://dl.gi.de/items/2a8571f8-0ef2-481c-8ee9-05f82ee258c8">Seamless Integration of Parquet Files into Data Processing. / Rey, Alice; Freitag, Michael; Neumann, Thomas. / BTW 2023</a></p>
<p><a id="footnote4"></a><code>4</code>: For more information about external indexes, see <a href="https://www.youtube.com/watch?v=74YsJT1-Rdk">this talk</a> and the <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs">parquet_index.rs</a> and <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs">advanced_parquet_index.rs</a> examples in the DataFusion repository.</p>
<p><a id="footnote5"></a><code>5</code>: For information about rewriting files to optimize for specific queries, such as resorting, repartitioning, and tuning data page and row group sizes, see <a href="https://github.com/XiangpengHao/liquid-cache/issues/227">XiangpengHao/liquid‑cache#227</a> and the conversation between <a href="https://github.com/JigaoLuo">JigaoLuo</a> and <a href="https://github.com/XiangpengHao">XiangpengHao</a> for details. We hope to make a future post about this topic.</p>
        </div>
      </div>
    </div></div>
  </body>
</html>
