<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simedw.com/2025/07/10/gemini-bounding-boxes/">Original</a>
    <h1>Is Gemini 2.5 good at bounding boxes?</h1>
    
    <div id="readability-page-1" class="page"><div>
    <article>
      
      <p><small>July 10, 2025</small></p>
      <p>TL;DR Gemini 2.5 Pro is a decent object detector, matching Yolo V3 from 2018 on MS-COCO val.</p>
<p>Multimodal Large Language Models keep getting better, but are they ready to dethrone CNNs in computer vision tasks like object detection? The allure of skipping dataset collection, annotation, and training is too enticing not to waste a few evenings testing. </p>
<p>I decided to write a small benchmark and check Gemini 2.5 on MS-COCO, focusing on object detection. You can find the code and more results <a href="https://github.com/simedw/coco-gemini">here</a>.</p>
<p><img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/dog-1.png" alt="Dog with ground truth bounding boxes"/>
    <img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/dog-2.png" alt="Dog with bounding box predictions from Gemini"/>
</p>
<p><em>Hover or tap to switch between ground truth (green) and Gemini predictions (blue) bounding boxes</em></p>

<h2 id="dataset">Dataset</h2>
<p><a href="https://cocodataset.org">MS-COCO</a> is a classic in the object detection world, sure it&#39;s a bit dated and the masks/bounding boxes aren&#39;t super tight, still, it has a long history and it should be easy to place Gemini among the historical results. </p>
<p>There are 80 classes, from <code>person</code> to <code>toothbrush</code>. Object boundaries can sometimes be ambiguous, but this tends to even out across the dataset.  </p>
<p>The validation set, which you are not supposed to train on, contains 5000 images. However there are no guarantees that Gemini didn&#39;t vacuum it up during its training. </p>
<p><img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/cake-1.png" alt="Cake with ground truth bounding boxes"/>
    <img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/cake-2.png" alt="Cake with bounding box predictions from Gemini"/>
</p>
<p><em>COCO sees four cakes, Gemini sees only one</em></p>

<h2 id="prompt">Prompt</h2>
<p>I&#39;ve embedded the list of valid classes from MS-COCO into the prompt, and asked it to follow my JSON output schema. I deliberately avoided explicitly mentioning COCO by name in case Gemini had seen it during training.</p>
<details>
<summary><strong>Full prompt</strong></summary>

<div><pre><span></span><code>Look carefully at this image and detect ALL visible objects, including small ones.

IMPORTANT: Focus on finding as many objects as possible, even if they are small, distant, or partially visible.
Make sure that the bounding box is as tight as possible.
Valid object classes: {class_list}

For each detected object, provide:
<span>-</span> &#34;label&#34;: exact class name from the list above
<span>-</span> &#34;confidence&#34;: how certain you are (0.0 to 1.0)  
<span>-</span> &#34;box_2d&#34;: bounding box [ymin, xmin, ymax, xmax] normalized 0-1000
<span>-</span> &#34;mask&#34;: binary mask of the object in the image, as a base64 encoded string

Detect everything you can see that matches the valid classes. Don&#39;t be conservative - include objects even if you&#39;re only moderately confident.

Return as JSON array:
[
  {
    &#34;label&#34;: &#34;person&#34;,
    &#34;confidence&#34;: 0.95,
    &#34;box_2d&#34;: [100, 200, 300, 400],
    &#34;mask&#34;: &#34;...&#34;
  },
  {
    &#34;label&#34;: &#34;kite&#34;, 
    &#34;confidence&#34;: 0.80,
    &#34;box_2d&#34;: [50, 150, 250, 350],
    &#34;mask&#34;: &#34;...&#34;
  }
]
</code></pre></div>


</details>
<p>Confidence is important for calculating the Average Precision (see Measure section), however, from experience and some sampling the output only partially correlates with better matches. </p>
<p>I ran this prompt with and without structured output, and to burn a bit more credits I also tried both with and without a thinking budget (1024 tokens). For Pro, a thinking budget of *128 tokens is the minimum, I also tried with 2048 tokens. The Gemini documentation recommends to turn it off, and looking at the result that seems like sound advice. </p>
<p><small>Initially, I wrote 1024 tokens, but it was pointed out that the minimum is in fact 128. I have updated the results table, but it didn&#39;t change the overall outcome.</small></p>
<h2 id="measure">Measure</h2>
<p>In object detection, mAP (mean Average Precision) is essentially the average precision of the model&#39;s predictions across multiple overlap thresholds (Intersection over Union - IoU), typically ranging from 0.5 to 0.95. Higher values are better. The goal here is to collapse a graph showing how precision changes as recall changes into a single number, since it&#39;s much easier to benchmark with a single number. </p>
<p>Here is some pseudo code, not sure if it makes things any clearer, there are also a lot of guides floating around for mAP with nice illustrations: </p>
<div><pre><span></span><code><span>def</span><span> </span><span>iou</span><span>(</span><span>box_a</span><span>,</span> <span>box_b</span><span>):</span>
  <span>return</span> <span>intersection_area</span><span>(</span><span>box_a</span><span>,</span> <span>box_b</span><span>)</span> <span>/</span> <span>union_area</span><span>(</span><span>box_a</span><span>,</span> <span>box_b</span><span>)</span>

<span>def</span><span> </span><span>calculate_ap</span><span>(</span><span>threshold</span><span>):</span>
    <span># Mark predictions as True or False positives based on IoU threshold</span>
    <span>for</span> <span>prediction</span> <span>in</span> <span>predictions</span><span>:</span>
        <span>prediction</span><span>.</span><span>true_positive</span> <span>=</span> <span>iou</span><span>(</span><span>prediction</span><span>,</span> <span>ground_truth</span><span>)</span> <span>&gt;</span> <span>threshold</span>

    <span># Sort predictions by confidence (important for the curve!)</span>
    <span>predictions</span> <span>=</span> <span>sorted</span><span>(</span><span>predictions</span><span>,</span> <span>key</span><span>=</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>.</span><span>confidence</span><span>,</span> <span>reverse</span><span>=</span><span>True</span><span>)</span>

    <span>true_positives</span> <span>=</span> <span>0</span>
    <span>false_positives</span> <span>=</span> <span>0</span>
    <span>precision_values</span> <span>=</span> <span>[]</span>
    <span>recall_values</span> <span>=</span> <span>[]</span>

    <span>total_ground_truth</span> <span>=</span> <span>len</span><span>(</span><span>ground_truth_objects</span><span>)</span>

    <span>for</span> <span>prediction</span> <span>in</span> <span>predictions</span><span>:</span>
        <span>if</span> <span>prediction</span><span>.</span><span>true_positive</span><span>:</span>
            <span>true_positives</span> <span>+=</span> <span>1</span>
        <span>else</span><span>:</span>
            <span>false_positives</span> <span>+=</span> <span>1</span>

        <span>precision</span> <span>=</span> <span>true_positives</span> <span>/</span> <span>(</span><span>true_positives</span> <span>+</span> <span>false_positives</span><span>)</span>
        <span>recall</span> <span>=</span> <span>true_positives</span> <span>/</span> <span>total_ground_truth</span>

        <span>precision_values</span><span>.</span><span>append</span><span>(</span><span>precision</span><span>)</span>
        <span>recall_values</span><span>.</span><span>append</span><span>(</span><span>recall</span><span>)</span>

    <span># Calculate area under precision-recall curve</span>
    <span>return</span> <span>area_under_curve</span><span>(</span><span>precision_values</span><span>,</span> <span>recall_values</span><span>)</span>

<span># Finally average over multiple thresholds</span>
<span>mAP</span> <span>=</span> <span>sum</span><span>(</span><span>calculate_ap</span><span>(</span><span>threshold</span><span>)</span> <span>for</span> <span>threshold</span> <span>in</span> <span>[</span><span>0.50</span><span>,</span> <span>0.55</span><span>,</span> <span>...</span><span>,</span> <span>0.95</span><span>])</span> <span>/</span> <span>10</span>
</code></pre></div>

<h2 id="results">Results</h2>
<p>The results are fairly clear:</p>
<ul>
<li>Pro is better than Flash, which is better than Flash-Lite. </li>
<li>Adding a thinking budget reduces the performance significantly.</li>
<li>Unstructured output was better for Flash and Flash-Lite but worse for Pro. </li>
<li>Pro is much better at not returning invalid outputs. </li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>Think Tokens</th>
<th>Mode</th>
<th>mAP</th>
<th><a href="https://simedw.com/cdn-cgi/l/email-protection" data-cfemail="f3b2a3b3c3ddc6">[emailÂ protected]</a></th>
<th>Errors (# invalid outputs)</th>
<th>Avg Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>flash</td>
<td>0</td>
<td>structured</td>
<td>0.224</td>
<td>0.381</td>
<td>47/5000</td>
<td>0.18s</td>
</tr>
<tr>
<td>flash</td>
<td>0</td>
<td>unstructured</td>
<td>0.261</td>
<td>0.417</td>
<td>57/5000</td>
<td>0.20s</td>
</tr>
<tr>
<td>flash</td>
<td>1024</td>
<td>structured</td>
<td>0.160</td>
<td>0.311</td>
<td>23/5000</td>
<td>0.27s</td>
</tr>
<tr>
<td>flash</td>
<td>1024</td>
<td>unstructured</td>
<td>0.161</td>
<td>0.319</td>
<td>19/5000</td>
<td>0.28s</td>
</tr>
<tr>
<td>pro</td>
<td>128</td>
<td>structured</td>
<td>0.332</td>
<td>0.495</td>
<td>5/5000</td>
<td>0.20s</td>
</tr>
<tr>
<td>pro</td>
<td>1024</td>
<td>structured</td>
<td><strong>0.340</strong></td>
<td><strong>0.517</strong></td>
<td>6/5000</td>
<td>0.46s</td>
</tr>
<tr>
<td>pro</td>
<td>2048</td>
<td>structured</td>
<td>0.325</td>
<td>0.506</td>
<td>5/5000</td>
<td>0.30s</td>
</tr>
<tr>
<td>pro</td>
<td>1024</td>
<td>unstructured</td>
<td>0.288</td>
<td>0.438</td>
<td>25/5000</td>
<td>0.47s</td>
</tr>
<tr>
<td>pro</td>
<td>2048</td>
<td>unstructured</td>
<td>0.277</td>
<td>0.434</td>
<td>26/5000</td>
<td>0.54s</td>
</tr>
<tr>
<td>flash-lite</td>
<td>0</td>
<td>structured</td>
<td>0.156</td>
<td>0.279</td>
<td>335/5000</td>
<td>0.37s</td>
</tr>
<tr>
<td>flash-lite</td>
<td>0</td>
<td>unstructured</td>
<td>0.211</td>
<td>0.338</td>
<td>216/5000</td>
<td>0.23s</td>
</tr>
<tr>
<td>flash-lite</td>
<td>1024</td>
<td>structured</td>
<td>0.140</td>
<td>0.273</td>
<td>168/5000</td>
<td>0.27s</td>
</tr>
<tr>
<td>flash-lite</td>
<td>1024</td>
<td>unstructured</td>
<td>0.215</td>
<td>0.364</td>
<td>114/5000</td>
<td>0.24s</td>
</tr>
</tbody>
</table>
<p>This means Gemini Pro 2.5 structured (~0.34 mAP) is roughly on par with <a href="https://arxiv.org/abs/1804.02767">Yolo v3</a> (2018, ~0.33 mAP). State-of-the-art models like <a href="https://arxiv.org/abs/2211.12860v5">Co-DETR</a> reach ~0.60 mAP. </p>
<p>I also wanted to add a <code>mask</code> output (base64 RLE encoded) as I thought it might increase the bounding box performance. Unfortunately, including the <code>mask</code> field caused the model to spiral into infinite loops, spewing out meaningless tokens, and burning my budget (maybe 5% of the time, but enough that I didn&#39;t complete the testing).</p>

<p><img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/car-1.png" alt="Carpark with ground truth bounding boxes"/>
    <img src="https://simedw.com/2025/07/10/gemini-bounding-boxes/images/car-2.png" alt="Carpark with bounding box predictions from Gemini"/>
</p>
<p><em>Sometimes Gemini is better than the ground truth</em></p>

<h2 id="conclusion">Conclusion</h2>
<p>This benchmark isn&#39;t entirely fair, since CNNs are explicitly trained on these 80 classes. Even so, Gemini 2.5 Pro held its own surprisingly well. Loose bounding boxes can easily be refined by segmentation models like SAM. CNNs remain faster, cheaper, and easier to reason about, especially with good training data, but Gemini&#39;s versatility across open-set tasks feels almost magical. I&#39;ll definitely be using it in my side projects going forward.</p>

<p><a href="https://simonwillison.net/2025/Apr/18/gemini-image-segmentation/">Simon Willison</a> covered this earlier this year, I recommend checking out his visualiser, and most of his blog posts for that matter. </p>
<p>The paper <a href="https://arxiv.org/abs/2507.01955">How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks</a>
compares various large models on vision tasks, but they don&#39;t just prompt for bounding box coordinates, instead they do &#34;recursive zooming&#34;, essentially dividing the image into a grid of cells and ask the model if part of objects are present in each grid. And then do this recursively on grids with objects. 
This is different from my benchmark which is just asking for all the objects at once.</p>
    </article>
  </div></div>
  </body>
</html>
