<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bentoml.com/llm/">Original</a>
    <h1>LLM Inference Handbook</h1>
    
    <div id="readability-page-1" class="page"><div><header></header>
<p><em>LLM Inference in Production</em> is your technical glossary, guidebook, and reference - all in one. It covers everything you need to know about LLM inference, from core concepts and performance metrics (e.g., <a href="https://bentoml.com/llm/inference-optimization/llm-inference-metrics">Time to First Token and Tokens per Second</a>), to optimization techniques (e.g., <a href="https://bentoml.com/llm/inference-optimization/static-dynamic-continuous-batching">continuous batching</a> and <a href="https://bentoml.com/llm/inference-optimization/prefix-caching">prefix caching</a>) and operation best practices.</p>
<div><ul>
<li>Practical guidance for deploying, scaling, and operating LLMs in production.</li>
<li>Focus on what truly matters, not edge cases or technical noise.</li>
<li>Boost performance with optimization techniques tailored to your use case.</li>
<li>Continuously updated with the latest best practices and field-tested insights.</li>
</ul></div>
<h2 id="motivation">Motivation<a href="#motivation" aria-label="Direct link to Motivation" title="Direct link to Motivation">​</a></h2>
<p>We wrote this handbook to solve a common problem facing developers: LLM inference knowledge is often fragmented; it’s buried in academic papers, scattered across vendor blogs, hidden in GitHub issues, or tossed around in Discord threads. Worse, much of it assumes you already understand half the stack.</p>
<p>There aren’t many resources that bring it all together — like how <a href="https://bentoml.com/llm/llm-inference-basics/training-inference-differences">inference differs from training</a>, why <a href="https://bentoml.com/llm/inference-optimization/llm-inference-metrics#goodput">goodput matters more than raw throughput</a> for meeting SLOs, or how <a href="https://bentoml.com/llm/inference-optimization/prefill-decode-disaggregation">prefill-decode disaggregation</a> works in practice.</p>
<p>So we started pulling it all together.</p>
<h2 id="who-this-is-for">Who this is for<a href="#who-this-is-for" aria-label="Direct link to Who this is for" title="Direct link to Who this is for">​</a></h2>
<p>This handbook is for engineers deploying, scaling or operating LLMs in production, whether you&#39;re fine-tuning a small open model or running large-scale deployments on your own stack.</p>
<p>If your goal is to make LLM inference faster, cheaper, or more reliable, this handbook is for you.</p>
<h2 id="how-to-use-this">How to use this<a href="#how-to-use-this" aria-label="Direct link to How to use this" title="Direct link to How to use this">​</a></h2>
<p>You can read it start-to-finish or treat it like a lookup table. There’s no wrong way to navigate. We’ll keep updating the handbook as the field evolves, because LLM inference is changing fast, and what works today may not be best tomorrow.</p>
<h2 id="contributing">Contributing<a href="#contributing" aria-label="Direct link to Contributing" title="Direct link to Contributing">​</a></h2>
<p>We welcome contributions! If you spot an error, have suggestions for improvements, or want to add new topics, please open an issue or submit a pull request on our <a href="https://github.com/bentoml/llm-inference-in-production" target="_blank" rel="noopener noreferrer">GitHub repository</a>.</p></div></div>
  </body>
</html>
