<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://entropytown.com/articles/2025-11-13-world-model-lecun-feifei-li/">Original</a>
    <h1>Why Fei-Fei Li and Yann LeCun Are Both Betting on &#34;World Models&#34;</h1>
    
    <div id="readability-page-1" class="page"><div>  <p>AI has finally reached the “we need to model the whole world” phase.</p>
<p>In the same season, Fei-Fei Li’s World Labs shipped <strong>Marble</strong>, a “multimodal world model” that turns prompts into walkable 3D scenes in your browser, and reports emerged that Meta’s chief AI scientist Yann LeCun is leaving to build a <strong>world-model</strong> startup of his own. DeepMind, meanwhile, is calling its new interactive video engine <strong>Genie 3</strong> a world model as well.</p>
<p>Same phrase. Three very different bets.</p>
<h2 id="the-week-world-models-went-mainstream">The week “world models” went mainstream</h2>
<p>World Labs has spent the year rolling out a neat narrative stack: Fei-Fei Li’s manifesto, <a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence"><em>From Words to Worlds: Spatial Intelligence Is AI’s Next Frontier</em></a>, argues that language-only systems (LLMs) are a dead end and that the real frontier is “spatial intelligence” and “world models” that understand 3D space, physics and action. On top of that sits the launch of <a href="https://www.worldlabs.ai/blog/marble-world-model">Marble</a>, which promises anyone can now generate editable 3D worlds from text, images, videos or simple layouts.</p>
<p>At almost the same time, outlets like Nasdaq reported that LeCun is preparing to leave Meta and raise money for a company “focused on world models” in the very different sense he’s been sketching since his 2022 paper <em>A Path Towards Autonomous Machine Intelligence</em> (<a href="https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models">Nasdaq</a>, <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">paper PDF</a>).</p>
<p>On Hacker News, the Marble launch thread is full of arguments about Gaussian splats and game engines (<a href="https://news.ycombinator.com/item?id=45902732">HN</a>). The LeCun thread is full of arguments about whether Meta has chosen “AI slopware” over proper research. Same word, different fights.</p>
<p>To understand why, we have to start with the only thing anyone can actually click.</p>
<h2 id="world-labs-world-model-gaussian-splats-for-humans">World Labs’ world model: Gaussian splats for humans</h2>
<p><strong>Marble</strong>, as shipped today, is a full-stack 3D content pipeline:</p>
<ul>
<li>It takes <strong>text prompts, single images, short videos or blocky 3D layouts</strong>.</li>
<li>It hallucinates a <strong>3D representation</strong> of a scene.</li>
<li>It lets you <strong>walk around</strong> that scene in a web or VR viewer and tweak it with an in-browser editor called Chisel.</li>
<li>It exports as <strong>Gaussian splats</strong>, standard <strong>meshes</strong> (OBJ/FBX) or flat <strong>video</strong> for downstream tools (<a href="https://www.worldlabs.ai/blog/marble-world-model">Marble docs</a>, <a href="https://radiancefields.com/world-labs-unveils-new-model-marble-a-generative-world-model">RadianceFields explainer</a>).</li>
</ul>
<p>For people who ship VR apps or game levels, a pipeline that goes “prompt → 3D world → export to Three.js / Unity” is extremely useful. World Labs even ships its own Three.js renderer, Spark, specifically tuned for splats (<a href="https://radiancefields.com/world-labs-releases-spark-v0-1-9">Spark release</a>).</p>
<p>But it’s very much a <strong>3D asset</strong> story. On Marble’s own blog, “world model” sits in the same sentence as “export Gaussian splats, meshes and videos”; there is no robot in sight.</p>
<p>Hacker News users clocked that immediately. One early top-level comment, contrasting Marble with DeepMind’s video-based Genie, reads:</p>
<blockquote>
<p>“Genie delivers on-the-fly generated video that responds to user inputs in real time. Marble renders a static Gaussian Splat asset (like a 3D game engine asset) that you then render in a game engine.”</p>
</blockquote>
<p>Another says, with the particular baffled politeness of an ML engineer:</p>
<blockquote>
<p>“Isn’t this a Gaussian Splat model? I work in AI and, to this day, I don’t know what they mean by ‘world’ in ‘world model’.”</p>
</blockquote>
<p>Reddit is less shy. In a thread about the first demo from the “$230m startup led by Fei-Fei Li” in r/StableDiffusion, one commenter sums it up as:</p>
<blockquote>
<p>“Taking images and turning them into 3D environments using gaussian splats, depth and inpainting. Cool, but that’s a 3D GS pipeline, not a robot brain.”</p>
</blockquote>
<p>(<a href="https://www.reddit.com/r/StableDiffusion/comments/1h53uhj/first_demo_from_world_labs_230m_startup_led_by/">Reddit thread</a>)</p>
<p>That doesn’t make Marble bad. It does make its use of “world model” slightly ambitious. To see how, you need a quick primer in what a Gaussian splat actually is.</p>
<h3 id="sidebar-photogrammetry-splats-and-meshes">Sidebar: photogrammetry, splats and meshes</h3>
<p>If you’re not a 3D person, 2025’s splat discourse can sound like hand-waving. In practice, there are three characters here:</p>
<ul>
<li>
<p><strong>Photogrammetry</strong> – The old guard. Take hundreds of overlapping photos of a real thing, reconstruct a <strong>polygon mesh</strong> (a shell made of tiny triangles), and bake textures on top. Great if you want to measure, collide or 3D-print.</p>
</li>
<li>
<p><strong>3D Gaussian splatting</strong> – The new hotness. Represent the scene as millions of fuzzy coloured blobs (“Gaussians”) floating in space, and “splat” them onto the screen so they blend into an image. Excellent at foliage, hair and soft light; runs in real time on gaming GPUs. The canonical paper is Kerbl et al.’s <a href="https://arxiv.org/abs/2308.04079"><em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em></a>.</p>
</li>
<li>
<p><strong>Renderers</strong> – Engines like Three.js, Unity or Unreal that take a mesh or a splat cloud and turn it into pixels.</p>
</li>
</ul>
<p>A photogrammetry practitioner on r/photogrammetry puts the trade-off like this:</p>
<blockquote>
<p>“Use photogrammetry if you want to do something with the mesh itself, and Gaussian splatting if you want to skip all the steps and just show the scan like it is. It’s kind of a shortcut to interactive photorealism.”</p>
</blockquote>
<p>(<a href="https://www.reddit.com/r/photogrammetry/comments/1mqt64y/what_are_the_benefits_and_different_use_cases/">explainer thread</a>)</p>
<p>Marble lives squarely in that world: it’s a <strong>shortcut to interactive photorealism</strong>. It generates splats/meshes and hands them to a renderer. The “world” it models is the part we can see and walk around in. It’s for humans (and game engines), not for machines to think with.</p>
<p>Fei-Fei Li’s essay, however, speaks in a different register.</p>
<p>She writes about “embodied agents”, “commonsense physics” and “robots that can understand and act in the world” — all the things you would want a robot’s <strong>internal model</strong> to support. Marble is presented as “step one” on that road. The tension, and the comic potential, comes from the fact that step one is currently a very polished 3DGS viewer.</p>
<p>Ironically, Fei-Fei Li’s original manifesto, <em>From Words to Worlds</em>, never once mentions 3D Gaussian Splatting — the very technique at the heart of Marble’s output pipeline.</p>
<p>If Marble were the only “world model” on offer, you could reasonably conclude that the term has been kidnapped by marketing. Unfortunately for your hot take, Yann LeCun exists.</p>
<h2 id="lecuns-world-model-the-brain-in-the-middle">LeCun’s world model: the brain in the middle</h2>
<p>LeCun’s use of “world model” comes from control theory and cognitive science rather than from 3D graphics.</p>
<p>In <em>A Path Towards Autonomous Machine Intelligence</em> (<a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">PDF</a>), he describes a system in which:</p>
<ul>
<li>A <strong>world model</strong> ingests streams of sensory data.</li>
<li>It learns <strong>latent state</strong>: compressed internal variables that capture “what’s going on out there”.</li>
<li>It learns to <strong>predict how that latent state will evolve</strong> when the agent (or environment) acts.</li>
<li>A separate module uses that machinery to <strong>plan</strong> and choose actions.</li>
</ul>
<p>You never see the world model directly. It doesn’t need to output pretty pictures. Its job is to let an agent think a few steps ahead.</p>
<p>JEPA-style models — “Joint Embedding Predictive Architectures” — are early instances of this approach: instead of predicting raw pixels, they predict masked or future embeddings and are trained to be <em>useful</em> representations rather than perfect renderings. LeCun has been giving talks about this since at least 2022 (<a href="https://www.youtube.com/watch?v=EvSe0ktD95k">YouTube</a>).</p>
<p>When Nasdaq and others reported that he’s spinning out to build a world-model startup (<a href="https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models">Nasdaq</a>), the reaction on HN wasn’t, “ooh, another 3D viewer.” It was:</p>
<ul>
<li>does this mean Meta has given up on this line of research in favour of GPT-ish products?</li>
<li>can a JEPA-like architecture ever match LLMs in practical usefulness?</li>
<li>is there even a market for a world model that mostly lives in diagrams and robot labs?</li>
</ul>
<p>Whether you think LeCun is right or wrong, you can’t really accuse him of chasing the same thing as World Labs. One “world model” is essentially a front-end asset generator. The other is a back-end predictive brain.</p>
<p>And then there’s DeepMind, happily occupying the middle.</p>
<h2 id="deepminds-world-model-worlds-as-video">DeepMind’s world model: worlds as video</h2>
<p>DeepMind’s <strong>Genie 3</strong> model is introduced, without much modesty, as “a new frontier for world models” (<a href="https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models">blog</a>).</p>
<p>From a text prompt, it generates an <strong>interactive video-like environment</strong> at 720p / 24 fps that you (or an agent) can move around in for several minutes. Objects persist across frames, you can “prompt” world events (“it starts raining”), and the whole thing functions as a tiny videogame rendered by a model instead of a traditional engine.</p>
<p>The Guardian describes it as a way for AI agents and robots to “train in virtual warehouses and ski slopes” before they ever touch the real world (<a href="https://www.theguardian.com/technology/2025/aug/05/google-step-artificial-general-intelligence-deepmind-agi">Guardian</a>). DeepMind is perfectly happy to connect it to the AGI narrative.</p>
<p>Where Marble generates <strong>assets</strong> and LeCun dreams of <strong>latents</strong>, Genie 3 produces <strong>simulators</strong>: online environments where you can act, observe consequences and learn.</p>
<p>On HN, when someone asks “how does Marble compare?”, a typical answer is:</p>
<blockquote>
<p>“Genie is on-the-fly generated video that responds to user inputs in real time. Marble is a static Gaussian splat asset you render in a game engine.”</p>
</blockquote>
<p>Again, not an insult — just taxonomy.</p>
<h2 id="one-word-three-bets">One word, three bets</h2>
<p>Put all of this together and “world model” now covers at least three distinct ideas:</p>
<ol>
<li>
<p><strong>World models as interface</strong></p>
</li>
<li>
<p><strong>World models as simulator</strong></p>
</li>
<li>
<p><strong>World models as cognition</strong></p>
</li>
</ol>
<p>Fei-Fei Li’s writing borrows heavily from bucket (3) — embodied agents, intuitive physics — while Marble, so far, mostly occupies bucket (1). LeCun’s plans live squarely in (3), with the hope that someone, someday, builds a good version of (2) on top. Genie lives between (2) and (3), with occasional marketing holidays in all of them.</p>
<p>If you only look at Marble’s demo, it’s tempting to say “world model” is just 3DGS with better PR. If you only read LeCun, it’s tempting to believe language models were a historical detour and JEPA will save us all. If you only read DeepMind, it’s simulated ski slopes all the way down.</p>
<p>The truth is they’re all building different parts of the same vague ambition: <strong>give machines some structured way to think about the world, beyond next-token prediction.</strong> One group starts from the rendering, one from the physics, one from the internal code.</p>
<p>Until the jargon catches up, the safest move when you see a “world model” headline is to ask three questions:</p>
<ol>
<li>Is this a <strong>thing for humans to look at</strong>, a <strong>place for agents to train</strong>, or a <strong>box inside a diagram</strong>?</li>
<li>Does it output <strong>static assets</strong>, <strong>real-time frames</strong>, or mostly <strong>latent states</strong>?</li>
<li>If you knock over a virtual vase, does anything in the system remember for more than one frame?</li>
</ol>
<p>If the answers are “for humans”, “static assets” and “not really”, you’re basically looking at a very nice Gaussian splat viewer. If they’re “for agents”, “real-time” and “yes, in latent space”, then you might just be staring at the world model LeCun has been talking about — the one that, very inconveniently for demo culture, doesn’t fit in a single tweetable GIF.</p>  </div></div>
  </body>
</html>
