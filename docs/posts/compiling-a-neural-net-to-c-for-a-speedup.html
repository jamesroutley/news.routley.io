<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://slightknack.dev/blog/difflogic/">Original</a>
    <h1>Compiling a neural net to C for a speedup</h1>
    
    <div id="readability-page-1" class="page"><div>
            

            <!-- <div class="text-input-container">
                <input
                    id="search"
                    type="search"
                    placeholder="Jump to..."
                    autocomplete="off"
                />
                <div class="search-results" style="display: none">
                    <div class="search-results__items"></div>
                </div>
            </div> -->

            
<!-- <div> -->

<p>2025-05-27 · About 25 minutes long</p>
<p><strong>tl;dr:</strong> I trained a neural network (NN), with logic gates in the place of activation functions, to learn a 3×3 kernel function for Conway’s Game of Life. I wanted to see if I could speed up inference by extracting the learned logic circuit from the NN. So, I wrote some code to extract and compile the extracted logic circuit to bit-parallel C (with some optimizations to remove gates that don’t contribute to the output). I benchmarked the original NN against the extracted 300-line single-threaded C program.; compiling the NN to C resulted in a 1,744× speedup! Crazy, right? <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic">Here’s the repo</a>: <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/6f01c83a0e6d02dcec59ab91c64eaf91ee4a3776/main.py">~354 lines of Python/JAX</a>, <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/master/gate.c">~331 lines of C</a>, if you want to <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/tree/89e2ed2f2c018122132598ea610a960900079bea?tab=readme-ov-file#to-reproduce">reproduce it</a> and/or mess around.</p>
<!-- <iframe src="/conway" width="100%" frameborder="0"></iframe> -->
<p><img alt="ghostty with 512 by 512 board of conway&#39;s game of life on it" src="https://slightknack.dev/content/conway-tui.png"/></p>
<p>While plumbing the intertubes (as one does), I came across <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/difflogic-ca/">this fun publication</a> by the <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/"><em>Self Organising Systems</em></a> group at Google, about <em>Differentiable Logic Cellular Automata</em>. This research caught my attention (I mean, who doesn’t love a pretty picture), and as I read it, I realized the whole idea wouldn’t be <em>too</em> hard to replicate. (Which is crazy, because <em>this</em> is crazy. I mean, the authors cite <em>creating <a rel="noopener nofollow" target="_blank" href="https://en.wikipedia.org/wiki/Computronium">computronium</a></em> as a source of inspiration. Awesome!)</p>
<p>To break things down a little bit: <em><a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/2210.08277">Differentiable Logic</a> <a rel="noopener nofollow" target="_blank" href="https://distill.pub/2020/growing-ca/">Cellular Automata</a></em> is a quite a mouthful, I know, but the idea is the straightforward composition of two existing ideas:</p>
<ol>
<li>
<p><strong>Cellular Automata (CA)</strong> are grids of cells, where each cell changes over time according to some <em>local</em> rule. The most famous cellular automata are <a rel="noopener nofollow" target="_blank" href="https://blog.oimo.io/2023/04/10/life-universe-en/"><em>Conway’s Game of Life</em></a> and perhaps <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/machine-110"><em>Rule 110</em></a>. We call this local update rule a <em>kernel</em>, a function that looks at the local neighborhood around a cell to calculate the cell’s next state. By applying the kernel to each cell in our grid, we step the cellular automata forward through time. Simple rules can give rise to <a rel="noopener nofollow" target="_blank" href="https://www.youtube.com/watch?v=C2vgICfQawE">strikingly complex behaviour</a>. <a rel="noopener nofollow" target="_blank" href="https://distill.pub/2020/growing-ca/"><strong>Neural Cellular Automata (NCA)</strong></a> are a variant of CA that replace the kernel function with a neural network. NCA can be trained to learn known kernels (what I do), or more generally, to learn kernels that give rise to a specified target behavior.</p>
</li>
<li>
<p><strong>Deep Differentiable Logic Gate Networks (DLGNs)</strong> are like neural networks, with two key differences. First, the <strong>weights are fixed</strong>, and set to 0 or 1; each neuron has exactly two inputs (one left, one right) We call these weights <em>wires</em>. Since wires are fixed, we <strong>learn the activation function</strong>. In this case, our activation function is a weighted linear combination of a set of 16 logic gates, applied to the two inputs. Essentially, we learn which logic gate should be used for each node in a fixed circuit. (If you’re a little lost, don’t worry, later, I’ll go into more detail to describe what this looks like in practice.)</p>
</li>
</ol>
<p>After reading the <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/difflogic-ca/">original publication</a>, I wanted to play with a few different experiments. To start, I wanted to replicate the research without looking at it too closely, to give room for creative misinterpretation, to understand what decisions were important. I also wanted to mess around with whatever the result of that was, until something <em>interesting</em> fell out. Consider this to be something of a pilot episode: there is a lot I still would like to experiment with. (Like fluid simulation! We could find a circuit for the kernel rule for <em><a rel="noopener nofollow" target="_blank" href="https://michaelmoroz.github.io/Reintegration-Tracking/">Reintegration Tracking</a></em>. Wouldn’t that be neat?)</p>
<p>I tried something new for the first time, which was to keep a <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic?tab=readme-ov-file#journal">journal</a> during development. I have no idea why I’ve never done this before, because it made recording training runs, figuring out what to debug, tracking next steps, and scoping changes <em>so</em> much easier. (Not to mention writing this write-up!) This project took three days (well, <del>four</del> five if you count this write-up), and it’s cool to see how much progress I made each day.</p>

<p><em>Conway’s Game of Life</em> is a Cellular Automata that takes place on a 2D grid of square cells. Each cell has 8 neighbors. At each step, looking at this 3×3 neighborhood,  we decide whether each cell is alive or dead, according to two rules:</p>
<ol>
<li>If the cell has exactly 3 live neighbors, it is alive.</li>
<li>If the cell has exactly 2 live neighbors, and is alive, it remains alive.</li>
</ol>
<p>I guess there’s a harsh third rule which is, “otherwise, if the cell is dead, it stays dead”. You may already begin to see how this could be written as a logic circuit. Here’s one way: given an array, <code>inputs</code>, with 9 cells where <code>0</code> is dead, <code>1</code> is alive, we can write:</p>
<pre data-lang="python"><code data-lang="python"><span>n = </span><span>sum</span><span>(inputs) - alive </span><span># neighbors excluding center
</span><span>alive_next = (n == </span><span>3</span><span>) or ((n == </span><span>2</span><span>) and alive)
</span></code></pre>
<p>The hard part, you might glean, would be coming up with a compact circuit for counting <code>n</code>, the number of neighbors. That’s what we’re up against: given a 9-bit input, we’re going to try to learn a circuit of logic gates whose output matches the 1-bit <code>alive_next</code> output, end-to-end.</p>
<blockquote>
<p><em>N.B.</em> You should try coming up with a circuit yourself, by hand! I did, it’s not too crazy. Here’s a hint: A cell has 8 neighbors. You can count how many cells are alive for any pair of inputs: <code>xor</code> for 1, <code>and</code> for 2. Can you use two pairs to count 4? What about 8? Once you can determine whether the neighborhood count is 2 or 3, the rest is fairly straightforward. How many logic gates did you use? How deep is your circuit?</p>
</blockquote>

<p>Now, I suppose I’ll show my age by saying that my knowledge of ML frameworks is stuck around 2017, with <em>old</em> Keras (never change, <code>Sequential</code>) and TensorFlow (<em>before</em> 2.0). I used to mess around with GANs (<em>StyleGAN</em> &lt;3) and I even implemented <a rel="noopener nofollow" target="_blank" href="https://openai.com/index/reinforcement-learning-with-prediction-based-rewards/">RND</a> on top of <a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/1707.06347">PPO</a> at some point! So I have a good intuition of the basics. Much has been lost to the sands of time, aside from a <a rel="noopener nofollow" target="_blank" href="https://github.com/Tloru/CommaSpeedChallenge">handful of random projects</a> shotgunned across GitHub. Sigh.</p>
<p>Well, as Oogway would say, “Yesterday is <em>history</em>, tomorrow is a <em>mystery</em>, but today… is a gift. That is why it is called the <em>present</em>.” There is no better day than today to get back on your A-game.</p>
<p><strong>JAX</strong> is a <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/quickstart.html">machine learning framework</a> for Python. I think of it as numpy on steroids. JAX has an API-compatible implementation of numpy living at <code>jax.jnp</code>: you can do all the fun matrix stuff (like multiplication!) and you get a couple things for free:</p>
<ol>
<li><strong>grad</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/automatic-differentiation.html">compute the gradient</a> of any non-sadistic function. It does this using automatic <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/advanced-autodiff.html#jacobian-vector-products-jvps-a-k-a-forward-mode-autodiff">reverse-mode differentiation</a>, but you can get as fancy as you’d like.</li>
<li><strong>vmap</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/autodidax.html">automatically parallelize</a> and vectorize computations that can be run in parallel. For example, I use vmap to batch training in my implementation.</li>
<li><strong>jit</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/jit-compilation.html">just-in-time compile</a> all of the above, and can produce code that runs on the GPU. Compiling Python at the library level using decorators is crazy!</li>
</ol>
<p>JAX has an ecosystem of libraries that mix and match these operations to build more powerful primitives. <strong>Optax</strong>, for example, implements common <a rel="noopener nofollow" target="_blank" href="https://optax.readthedocs.io/en/latest/api/optimizers.html">optimization strategies</a>, like <code>adamw</code>, on top of <code>jax.grad</code>. <strong>Flax</strong>, is a NN library built on top of JAX. (Tbh, flax is a little confusing: there’s <code>nn</code> (<a rel="noopener nofollow" target="_blank" href="https://docs.google.com/document/d/1hYavTVPaKVVe9Be8pCB7yW7r6dDv3RALVNit8NZca4c/edit?tab=t.0">deprecated</a>), <a rel="noopener nofollow" target="_blank" href="https://flax-linen.readthedocs.io/en/latest/"><code>linen</code></a>, <a rel="noopener nofollow" target="_blank" href="https://flax.readthedocs.io/en/latest/index.html"><code>nnx</code></a>. Everyone uses linen but the flax devs want people to use nnx it seems).</p>
<blockquote>
<p><em>N.B.</em> One other thing I like about JAX is that randomness is reproducible. All functions that generate non-deterministic output take a RNG <em>key</em>, and keys can be split into subkeys. The root key is generated when you set the seed; everything else derives from the root key. This means if you run a program twice, you’ll have the same initialization, same training samples, same convergence. It makes debugging a lot easier, because you can reproduce e.g. a blow-up during training with some patience.</p>
</blockquote>

<p>To learn a circuit, we need some way to translate the hard, discrete language of logic into the smooth, continuous language of <em>gradients</em>. (Gradients, now those are something an optimizer can get a handle on!)</p>
<p>A logic gate, like <code>and(a, b)</code>, is a function of two inputs; we can arrange them into a table like so:</p>

<p>Here, inputs are defined to be exactly <code>0</code> or <code>1</code>. What if the inputs <em>vary</em> between <code>0.0</code> and <code>1.0</code>; is there a continuous function of two inputs <code>f(a, b)</code> that behaves like <code>and(a, b)</code> at the boundary? Well, yes, <code>f(a, b) = a * b</code> fits the bill. We can say that <code>a * b</code> a <em>continuous relaxation</em> of <code>and(a, b)</code>. There are 16 fundamental logical functions of two inputs; here they all are, along with a continuous relaxation for each:</p>
<table><thead><tr><th>gate</th><th>relaxation</th><th>gate</th><th>relaxation</th></tr></thead><tbody>
<tr><td><code>false</code></td><td><code>0.</code></td><td><code>true</code></td><td><code>1.</code></td></tr>
<tr><td><code>and</code></td><td><code>a*b</code></td><td><code>nand</code></td><td><code>1. - a*b</code></td></tr>
<tr><td><code>a&amp;~b</code></td><td><code>a - a*b</code></td><td><code>b/~a</code></td><td><code>1. - a + a*b</code></td></tr>
<tr><td><code>a</code></td><td><code>a</code></td><td><code>not a</code></td><td><code>1. - a</code></td></tr>
<tr><td><code>b&amp;~a</code></td><td><code>b - a*b</code></td><td><code>a/~b</code></td><td><code>1. - b + a*b</code></td></tr>
<tr><td><code>b</code></td><td><code>b</code></td><td><code>not b</code></td><td><code>1. - b</code></td></tr>
<tr><td><code>xor</code></td><td><code>a+b - 2.*a*b</code></td><td><code>xnor</code></td><td><code>1. - (a+b - 2.*a*b)</code></td></tr>
<tr><td><code>or</code></td><td><code>a+b - a*b</code></td><td><code>nor</code></td><td><code>1. - (a+b - a*b)</code></td></tr>
</tbody></table>
<p><img alt="graphs of and, or, and xor" src="https://slightknack.dev/content/cont-relax.svg"/></p><p>This is great! (It’s fun to try to derive continuous relaxations yourself.) With this groundwork in place, we could create a “learnable logic gate” of sorts by taking a weighted sum of each relaxation. In JAX, using <code>jnp</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>jnp.</span><span>sum</span><span>(gate_weight * </span><span>gate_all</span><span>(left, right)) </span><span># axis=0
</span></code></pre>
<p>Here, <code>gate_all</code> returns a vector where each entry is the result of one of the functions above. If we want to make sure that the gate weights stay in a reasonable range, we can apply a <em>softmax</em> to the learned vector <code>w</code>, which squashes each gate weight to be between <code>0.0</code> and <code>1.0</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>gate_weight = jnp.</span><span>exp</span><span>(w) / jnp.</span><span>sum</span><span>(jnp.</span><span>exp</span><span>(w)) </span><span># axis=0, keepdims=True
</span></code></pre>
<p>We can train a network to learn these gate weights. Once we have a trained network, can replace the softmax with an <em>argmax</em> (taking the gate with the highest weight). This gives us a circuit with a hard <code>0</code> or <code>1</code> as an output; a discrete logic gate is also much cheaper to compute. (It’s almost as if computers are full of them!)</p>

<p>Well, we have our continuous relaxations and we have a NN. Let’s just put them together, replace <code>relu</code> with <code>gate</code>, and call it a day? Not so fast.</p>
<p>Machine learning papers almost make research look <em>effortless</em>, as though NNs magically converge when enough data is forced through their weights. This could not be further from the truth: there are so many failure modes; so many experiments that have to be run to guess the right hyperparameters; training a NN requires a weird combination of patience (giving the model enough time to converge) and urgency (stopping runs early when something is wrong). It’s fun, but it can also be frustrating, yet somehow addicting.</p>
<p>I could skip over the two days of elbow grease it took to get this working. However, differentiable logic gate networks train a little differently than your standard dense relu network, and there were a couple things, like how you initialize DLGNs, that surprised me.</p>

<p>At the start of this project, I wanted to see if I could learn the wires in addition to the gates. I still think it’s possible, but it’s something I had to abandon to get the model to converge.</p>
<p>I started this project by writing a simple dense NN with relu activation and standard SGD, just to see if things were working. They were, and my small model converged very quickly!</p>
<p>In traditional NNs, it’s commonly-accepted wisdom that you should initialize the wire weight matrices according to a tight normal distribution centered around zero. This is what I did for the relu network above, and it worked like a charm!</p>
<p>I switched from <code>relu</code> to <code>gate</code> by adding two weight matrices per layer, one for the right gate input, the other for the left. After this switch, however, try as I might, the model <em>would not</em> converge. I also started to worry about whether having <em>negative</em> wire weights would make it hard to extract the logical circuit after training. So, I thought some more, and decided to initialize wire weights <em>uniformly</em> between 0 and 1. This performed even worse!</p>
<p>Thinking some more, I had an epiphany: “well, since the goal is to learn a wiring, we should softmax the wires in the same way we softmax the gates!” In desperation, I implemented a row-wise softmax over wire weights initialized uniformly… this also went about as well as you would expect. (Poorly.)</p>
<p>At this point I realized: maybe a fixed <code>1</code> or <code>0</code> wiring is not just a random choice, but <em>highly</em> essential! Wouldn’t a fixed wiring let the gradients propagate all the way to the gates at the <em>input</em> end of the network, so the gates at the <em>output</em> end of the network could begin to converge? I began to look at how wiring was implemented in the paper; I decided to abandon my learned-wiring dreams for the time being.</p>
<p>In hindsight, it’s obvious that how you wire a network determines how information flows through it, so it’s important that the wiring is <em>good</em>, especially if the wiring is fixed. I don’t know why it took me so long to realize this.</p>
<p>After I refactored everything to use fixed wiring, I first I tried completely random wiring. This did a lot better than any of the previous approaches, but was still nowhere <em>near</em> the publication. After careful inspection, I realized that with this approach, you risk not wiring gates, or wiring two gates the same way, losing information as it flows through the network.</p>
<p>My next thought was to wire the network like a tree. We had descending power-of-two layers: 64, 32, 16, 8, 4, 2, 1; what if each layer was connected to the corresponding two cells in the layer before it? This way there is total information flow. Tree wiring worked like a charm, and it was at this point that I started to have hope.</p>
<p>I read the publication more closely, and at this point I looked at the <a rel="noopener nofollow" target="_blank" href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/diffLogic_CA.ipynb">colab notebook</a>. The wiring technique used is interesting: it guarantees we get unique pairs, like tree wiring, but we also shuffle the branches between layers to allow for some cross-pollination. The algorithm looks something like this:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>wire_rand_unique</span><span>(</span><span>key</span><span>, </span><span>m</span><span>, </span><span>n</span><span>):
</span><span>    key_rand, key_perm = random.</span><span>split</span><span>(key)
</span><span>    evens   = [(</span><span>0</span><span>, </span><span>1</span><span>), (</span><span>2</span><span>, </span><span>3</span><span>), ...]
</span><span>    odds    = [(</span><span>1</span><span>, </span><span>2</span><span>), (</span><span>3</span><span>, </span><span>4</span><span>), ...]
</span><span>    padding = </span><span>rand_pairs_pad</span><span>(key_rand, m, n)
</span><span>    pairs   = jnp.</span><span>array</span><span>([*evens, *odds, *padding])
</span><span>    perm    = random.</span><span>permutation</span><span>(key_perm, pairs)
</span><span>    </span><span>return </span><span>wire_from_pairs</span><span>(perm.T, m, n)
</span></code></pre>
<p>(Note that as long as n ≤ m ≤ 2×n, there will be total network connection with no random padding.)</p>
<p>With the wiring from the publication implemented, the model was within spitting distance of fully converging! That’s when one last revelation <em>shook me to my bones</em>.</p>

<p>The biggest surprise was the way you’re supposed to <em>initialize</em> the gate weights of a differentiable logic gate network; of course, it now makes <em>100% sense</em> in hindsight.</p>
<p>Here’s the big idea: we want the gradients to reach the input of the network, but if we initialize gate weights uniformly, or even randomly, we’ll get a flat activation function. Flat activation functions <em>kill</em> all gradients, period. I realized this was happening when I wrote code to visualize each layer in the network, and watched it change over time:</p>
<pre><code><span># [...]
</span><span>layer 3 (16, 8)
</span><span>▄ ▄ ▄ ▄ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▄ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █
</span><span>▁ ▁ ▁ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▄ ▄ ▄ ▄
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>layer  4 (16, 4)
</span><span>▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▄
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁
</span><span>layer 5 (16, 2)
</span><span>▁ ▁ ▁ ▁ ▄ ▁ █ ▁ ▁ ▁ ▁ ▁ ▄ ▁ ▄ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁
</span><span>layer 6 (16, 1)
</span><span>▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>F &amp; . A . B X / / X B . A . &amp; F
</span><span>                ~~~~~~~~~~~~~~~
</span><span># gate order
</span><span>| FALSE | AND  | A&amp;!B  | A    |
</span><span>| B&amp;!A  | B    | XOR   | OR   |
</span><span>| NOR   | XNOR | NOT B | A/!B |
</span><span>| NOT A | B/!A | NAND  | TRUE |
</span></code></pre>
<p>With dead gradients, I’d see the output gate converge, then the one before it, and so on until the gradients reached the input, at which point the later gates were so fixed in their ways that they were impossible to change, even as better earlier gate weights were discovered. Obviously, the performance of the network plateaus shortly thereafter.</p>
<p>I didn’t see an obvious way around this problem, so I started to wonder if there was some weird trick to get around the problem… and there was. So I read the code.</p>
<p>Of all the gates, these two stand out:</p>
<table><thead><tr><th>gate</th><th>relaxation</th><th>explanation</th></tr></thead><tbody>
<tr><td><code>a</code></td><td><code>a</code></td><td>Forward a, drop b.</td></tr>
<tr><td><code>b</code></td><td><code>b</code></td><td>Forward b, drop a.</td></tr>
</tbody></table>
<p>These pass-through gates do no mixing, and will propagate gradients straight along their wires, all the way to the input! This is critical! And in retrospect, I had missed a seemingly-innocuous line:</p>
<blockquote>
<p>To facilitate training stability, the initial distribution of gates is biased toward the pass-through gate.</p>
</blockquote>
<p><em>Of course</em> it is biased! There’s no way to train the network otherwise! So I made a simple one-line change, and, like that, <code>test_loss_hard: 0</code>. <strong>Perfect convergence:</strong></p>
<pre><code><span>Epoch (3001/3001) in 0.0031 s/epoch
</span><span>[ 0.999 0.999 0.00173 0.000287 0.000537 ] [1. 1. 0. 0. 0.] [1. 1. 0. 0. 0.]
</span><span>train_loss: 0.000738; test_loss: 9.91e-05; test_loss_hard: 0
</span></code></pre>
<p>At long last, it all fits together.</p>

<p>What frustrates me is that, after correctly implementing the architecture from the publication, the main issues preventing convergence were seemingly arbitrary hyperparameter choices. In my journal I wrote:</p>
<div>
<p>The model training code was good, as was the gate implementation. So what <em>was</em> wrong?</p>
<ul>
<li>The model was the wrong size (way too small).</li>
<li>The model was initialized the wrong way (randomly, instead of with gate passthrough).</li>
<li>The model did not have clipping in the optimizer, which seems like a rather arbitrary, hacky hyperparameter choice.</li>
</ul>
<p>Hyperparameters suck! I hate that I was pulling my hair out over why I wasn’t getting the same results when all that was different were the hyperparameters at play.</p>
<p>I wonder how many research breakthroughs are possible but just sitting in hyperparameters beyond reach?</p>
</div>
<p>Rather existential, I suppose. There are lots of other things I tried that didn’t work. Earlier, I tried to learn the <em>wiring</em> in addition to gates, but the network wouldn’t converge. I thought it was because the network was too complex, but now I’m convinced it’s because it was too small and initialized incorrectly. There is so much more I would like to try.</p>
<blockquote>
<p><em>N.B.</em> All this wiring got me thinking about gaussian splatting of all things. You see, I was recently reading a paper about <a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/2505.05587">when to split gaussian splats</a>; the rough idea is to split whenever the optimizer encounters a saddle with respect to any one splat. While watching underparameterized gate networks train, I would see them get stuck between a handful of gates on a given layer, and not converge further. I wonder if it would be possible to build a circuit network that grows over time, splitting gate saddles when loss stops going down? I suppose that’s what inspired the splat paper, so we’ve come full circle.</p>
</blockquote>

<p>At this point we have learned a <em>perfect</em> binary circuit that looks at a 3×3 cell neighborhood and computes the game of life kernel. Only one problem: This circuit is stuck as a pile of floating point numbers, tangled up in the vast sea of gate weights and wires. How will we ever get it out?</p>
<p>By compiling it to C.</p>
<p>This isn’t as hard as it sounds. All we have to do is <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/main.py#L269-L279">traverse the network</a>, layer by layer. We use our old friend <em>argmax</em> to figure out <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/main.py#L258-L267">which wires and gate to use</a> for each gate in the circuit.</p>
<p>At this point, we have a massive network. However, only a tiny fraction of the gates are used! (Most gates are still pass-through.) So I implemented two optimizations to clean up the circuit:</p>
<p><strong>Dead code elimination</strong> gets rid of all gates that do not contribute to the output. We’re not doing anything fancy here, like testing for truisms; just doing the simple thing and taking the transitive closure of all dependencies, starting from the root output, going backwards:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_elim</span><span>(</span><span>instrs</span><span>, </span><span>root</span><span>):
</span><span>    out = []
</span><span>    req = </span><span>set</span><span>([root])
</span><span>    </span><span>for </span><span>instr </span><span>in </span><span>instrs[::-</span><span>1</span><span>]:
</span><span>        (o, idx, l, r) = instr
</span><span>        </span><span>if </span><span>o in req:
</span><span>            req = </span><span>ext_add_deps</span><span>(req, idx, l, r)
</span><span>            out.</span><span>append</span><span>(instr)
</span><span>    </span><span>return </span><span>list</span><span>(out[::-</span><span>1</span><span>])
</span></code></pre>
<p><strong>Copy propagation</strong> lets us get rid of all those pesky useless pass-through gates by forwarding their output to all downstream gates. This time we go forward, and keep track of what to rename pass-through gates.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_copy_prop</span><span>(</span><span>instrs</span><span>, </span><span>root</span><span>):
</span><span>    out = []
</span><span>    rename = </span><span>dict</span><span>()
</span><span>    </span><span>for </span><span>instr </span><span>in </span><span>instrs:
</span><span>        (o, idx, l, r) = instr
</span><span>        </span><span>if </span><span>l in rename: l = rename[l]
</span><span>        </span><span>if </span><span>r in rename: r = rename[r]
</span><span>        </span><span>if </span><span>o == root: out.</span><span>append</span><span>((o, idx, l, r))
</span><span>        </span><span>elif </span><span>idx == </span><span>3</span><span>: rename[o] = l
</span><span>        </span><span>elif </span><span>idx == </span><span>5</span><span>: rename[o] = r
</span><span>        </span><span>else</span><span>: out.</span><span>append</span><span>((o, idx, l, r))
</span><span>    </span><span>return </span><span>out
</span></code></pre>
<p>(Index <code>3</code> and <code>5</code> are the indices of the pass-through gates.)</p>
<p>After this, we do a simple rename for all gates (since we’ve decimated the number of gates in use), and emit C. This is just a big switch-case:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_gate_name</span><span>(</span><span>idx</span><span>, </span><span>l</span><span>, </span><span>r</span><span>):
</span><span>    names = [
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>&#34;</span><span>0</span><span>&#34;</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>&#34;</span><span>{a}</span><span> &amp; </span><span>{b}</span><span>&#34;</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>&#34;</span><span>{a}</span><span> &amp; ~</span><span>{b}</span><span>&#34;</span><span>,
</span><span>        </span><span># ...
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>&#34;</span><span>~(</span><span>{a}</span><span> &amp; </span><span>{b}</span><span>)</span><span>&#34;</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>&#34;</span><span>~0</span><span>&#34;</span><span>,
</span><span>    ]
</span><span>    </span><span>return </span><span>names[idx](l, r)
</span></code></pre>
<p>After formatting, we get a <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/gate.c#L7-L172">long list of instructions</a> that looks like this:</p>
<pre data-lang="c"><code data-lang="c"><span>cell </span><span>conway</span><span>(cell </span><span>in</span><span>[</span><span>9</span><span>]) {
</span><span>    </span><span>// ... 157 lines hidden
</span><span>    cell hh = hc ^ gs;
</span><span>    cell hi = hb &amp; ~gy;
</span><span>    cell hj = hh &amp; hi;
</span><span>    cell hk = hg &amp; cy;
</span><span>    cell out = hj | hk;
</span><span>    </span><span>return</span><span> out;
</span><span>}
</span></code></pre>
<p>And like that, all your floats are belong to us. Here’s the final tally for each basic logical operation used in the circuit. (Note that some gates, e.g. nand, are composed of multiple operations):</p>
<table><thead><tr><th>Gate</th><th>Count</th></tr></thead><tbody>
<tr><td><code>and</code></td><td>69</td></tr>
<tr><td><code>or</code></td><td>68</td></tr>
<tr><td><code>xor</code></td><td>17</td></tr>
<tr><td><code>not</code></td><td>16</td></tr>
<tr><td><strong>total</strong></td><td><strong>170</strong></td></tr>
</tbody></table>
<p>Nice. I don’t know why, but it’s satisfying to see a pretty even split between <code>and</code> and <code>or</code>, as with <code>xor</code> and <code>not</code>. I wonder what the distribution looks like in real-world code. (I thought I never used <code>xor</code>, but then I realized that <code>!=</code> is really just <code>xor</code> in disguise.)</p>

<p>I pulled a sneaky little trick. I don’t know if you noticed. Here’s a hint:</p>
<pre data-lang="c"><code data-lang="c"><span>typedef </span><span>uint64_t </span><span>cell</span><span>;
</span></code></pre>
<p>That’s right: a <code>cell</code> is not one grid cell, but 64! When we calculate <code>cell conway(cell in[9]);</code>, we are, through <em>bit-parallelism</em>, computing the rule on 64 cells at once!</p>
<p>We compile to C and produce a function containing the circuit, <code>conway</code>, but we need a runtime to saturate it. I took a compilers class this semester (6.1100), so I have been writing a lot of <em>Unnamed Subset of C</em> this semester. With C on the mind, I wrote a little runtime. Here’s how it works.</p>
<p>First, I define a board as a collection of cells:</p>
<pre data-lang="c"><code data-lang="c"><span>typedef struct </span><span>{
</span><span>    cell* cells;
</span><span>    size_t cells_len;
</span><span>    size_t width;
</span><span>    size_t height;
</span><span>} </span><span>board_t</span><span>;
</span></code></pre>
<p>Each <code>cell</code> is just a horizontal slab, 64 bits wide. I’m lazy here, so I require <code>width</code> be a multiple of 64. We initialize this board with random state using an <code>xorshift</code> prng I am currently lending from Wikipedia:</p>
<pre data-lang="c"><code data-lang="c"><span>// https://en.wikipedia.org/wiki/Xorshift
</span><span>// constant is frac(golden_ratio) * 2**64
</span><span>// global state bad cry me a river
</span><span>uint64_t rand_state = </span><span>0x9e3779b97f4a7c55</span><span>;
</span><span>
</span><span>uint64_t </span><span>rand_uint64_t</span><span>() {
</span><span>    uint64_t x = rand_state;
</span><span>    x ^= x &lt;&lt; </span><span>13</span><span>;
</span><span>    x ^= x &gt;&gt; </span><span>7</span><span>;
</span><span>    x ^= x &lt;&lt; </span><span>17</span><span>;
</span><span>    rand_state = x;
</span><span>    </span><span>return</span><span> x;
</span><span>}
</span></code></pre>
<p>On to the meat and potatoes. The function <code>conway</code> requires a list of <code>9</code> cells. The neighbors directly above and below are easy: we just index forward or back a row in <code>cells</code>. The side-by-side neighbors are a bit harder. Luckily, we can just bitshift to the left and right to create two new boards. This works as long as we’re careful to <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/gate.c#L243-L271">catch all bits</a> that might fall into the bitbucket when we shift:</p>
<pre data-lang="c"><code data-lang="c"><span>void </span><span>board_step_scratch_mut</span><span>(
</span><span>    board_t *</span><span>board</span><span>,
</span><span>    board_t *</span><span>scratch_left</span><span>,
</span><span>    board_t *</span><span>scratch_right</span><span>, </span><span>// syntax highlighting breaks without a comma here, sigh
</span><span>) {
</span><span>    </span><span>// 5: dense loops and bitshifts,
</span><span>    </span><span>// 7: no beauty I can present.
</span><span>    </span><span>// 5: click the link above.
</span><span>}
</span></code></pre>
<p>I guess a better illustration would be, to analogize:</p>
<table><thead><tr><th>ceos hate these seats</th><th>8-bit analogy</th></tr></thead><tbody>
<tr><td><code>scratch_left</code></td><td><code>10011010 10011010 10011010 ...</code></td></tr>
<tr><td><code>board</code></td><td><code>01001101 01001101 01001101 ...</code></td></tr>
<tr><td><code>scratch_right</code></td><td><code>10100110 10100110 10100110 ...</code></td></tr>
</tbody></table>
<p>With all this machinery in place, we can step the board! I’m proud of this code:</p>
<div>
<pre data-lang="c"><code data-lang="c"><span>void </span><span>board_step_mut</span><span>(
</span><span>    board_t *</span><span>board</span><span>,
</span><span>    board_t *</span><span>s_left</span><span>, </span><span>// scratch
</span><span>    board_t *</span><span>s_right</span><span>,
</span><span>    board_t *</span><span>s_out</span><span>,  </span><span>// sigh
</span><span>) {
</span><span>    </span><span>board_step_scratch_mut</span><span>(board, s_left, s_right);
</span><span>
</span><span>    size_t step = board-&gt;width / </span><span>64</span><span>;
</span><span>    size_t wrap = board-&gt;cells_len;
</span><span>
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; board-&gt;cells_len; i++) {
</span><span>        cell in[</span><span>9</span><span>];
</span><span>        size_t i_top = (i + wrap - step) % wrap;
</span><span>        size_t i_bottom = (i + step) % wrap;
</span><span>        </span><span>// top row
</span><span>        in[</span><span>0</span><span>] = s_left-&gt;cells[i_top];
</span><span>        in[</span><span>1</span><span>] = board-&gt;cells[i_top];
</span><span>        in[</span><span>2</span><span>] = s_right-&gt;cells[i_top];
</span><span>        </span><span>// middle row
</span><span>        in[</span><span>3</span><span>] = s_left-&gt;cells[i];
</span><span>        in[</span><span>4</span><span>] = board-&gt;cells[i];
</span><span>        in[</span><span>5</span><span>] = s_right-&gt;cells[i];
</span><span>        </span><span>// bottom row
</span><span>        in[</span><span>6</span><span>] = s_left-&gt;cells[i_bottom];
</span><span>        in[</span><span>7</span><span>] = board-&gt;cells[i_bottom];
</span><span>        in[</span><span>8</span><span>] = s_right-&gt;cells[i_bottom];
</span><span>        </span><span>// update output
</span><span>        s_out-&gt;cells[i] = </span><span>conway</span><span>(in);
</span><span>    }
</span><span>
</span><span>    </span><span>// double-buffering
</span><span>    cell* tmp_cells = board-&gt;cells;
</span><span>    board-&gt;cells = s_out-&gt;cells;
</span><span>    s_out-&gt;cells = tmp_cells;
</span><span>}
</span></code></pre>
<p>*waves* welcome to the club. mention bananas in any comment about this project for immediate +10 respect.</p>
</div>
<p>And there you have it. There’s some more code for printing that I haven’t included here. One call to <code>main</code>, and you’re off to the races! If you’d like to absorb the monstrosity in full, <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/master/gate.c">here’s a link</a>.</p>
<p>(I mean, 331 lines of C, half generated. That can’t hurt anyone.)</p>

<p>Benchmarking is always fraught with peril, especially for bold claims like <span>&#34;GUYS! I found a 1,744× speedup!!&#34;</span> so I&#39;d like to qualify exactly what I&#39;m measuring:</p><p>I am comparing the <em>inference speed</em> of my Python JAX (with JIT) implementation against that of my bit-parallel C implementation.</p>
<p><strong>Disclaimer,</strong> because I’m certain someone won’t read the above. You can definitely simulate <em>Conway’s Game of Life</em> with JAX a lot faster by not using a DLGN, if that’s your goal. (Indeed, I have a faster pure-JAX kernel to prepare the boards used for training!) Here, I want to compare floating-point GPU inference to bit-parallel CPU inference. And for fun, if you just want to simulate Conway’s Game of Life, you can totally shred with a faster algorithm like <em>Hashlife</em>, which I’ve <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/hashlife">half-implemented before</a>. These benchmarks, while semi-rigorous, are just for fun!</p>
<p>If you want the exact setup, there are <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/tree/89e2ed2f2c018122132598ea610a960900079bea?tab=readme-ov-file#to-reproduce">reproduction steps</a> in the repository, with more details in the <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic?tab=readme-ov-file#journal">journal</a>. Here’s my approach:</p>
<p>I use a board of size 512×512 cells. I find the average time per <em>step</em> in the C implementation, by running <em>step</em> on the board 100k times. I find the average time per pass in the Python JAX+JIT implementation, by predicting 512 batches of 512. Here are the results:</p>
<table><thead><tr><th>method</th><th>μs/pass (512×512)</th><th>μs/step (512×512)</th><th>fps</th><th>speedup</th></tr></thead><tbody>
<tr><td>Python</td><td>71200</td><td>—</td><td>14</td><td>1×</td></tr>
<tr><td>C</td><td>—</td><td>40.9</td><td>24,400</td><td>1,744×</td></tr>
</tbody></table>
<p>And there it is: <strong>1,744×</strong>.</p>
<blockquote>
<p><em>N.B.</em> Computing 512 batches of 512 was faster than a single batch of size 512×512 = 262,144, which would have been a more direct comparison. Take 1,744× with a grain of salt, if anything.</p>
</blockquote>
<p>I did some back-of-the-napkin math, and this seems to check out. On the JAX side, the network I’m evaluating is of size [9, 128×17, 64, 32, 16, 8, 4, 2, 1]. Each 128 matrix-vector product requires ~16k floating-point multiplications. We have 17 of them, so we’re looking at at least ~270k flop for a single cell; we have 512×512 cells to evaluate, so lower bound is 70.7 gflop. All things considered, JAX is doing a <em>very</em> good job optimizing the workload. My machine can apparently do about 4.97 tflop/s: dividing that by the estimated 70.7 gflop workload, I get 70.3 fps, and as a lower bound, is ~within an order of magnitude of the 14 fps from the benchmark.</p>
<p>The bit-parallel C implementation, on the other hand, is about <a rel="noopener nofollow" target="_blank" href="https://godbolt.org/z/e3xsYsaYE">~349 instructions long</a> (Godbolt). Each instruction processes 64 bits in parallel, which works out to about 5.45 instructions per bit. There’s quite a bit of register spilling going on, and it takes time to write to memory. Given we have 512×512 cells, it should take around 1.43 million instrs per step. A core on my machine runs at about 3.70 gcycles/s. If we assume instruction latency is 1 cycle, we should expect 2,590 fps. But we measure a number nearly 10× higher! What gives? I expect something along the lines of “insane instruction-level parallelism”, but this is something I’ll have to come back to. Regardless, this is also within an order of magnitude of the measured figure. (Now I’m really curious! I’ll have to dig into it…)</p>
<p>Well, there you have it: <strong>doing less work is indeed faster!</strong> News at 11.</p>

<p>I have lots of ideas about what to do next. Some ideas:</p>
<ul>
<li>
<p>Try learning a bigger circuit, like one for fluid simulation, using <a rel="noopener nofollow" target="_blank" href="https://michaelmoroz.github.io/Reintegration-Tracking/">reintegration tracking</a>.</p>
</li>
<li>
<p>Try optimizing further, by vectorizing with SIMD, or outputting a bit-parallel compute shader that runs on the GPU.</p>
</li>
<li>
<p>Try letting <em>you</em> mess around with the project in-browser, by exporting various circuits at different points in training, so you can get a feel for how the network learns.</p>
</li>
</ul>
<p>Well, if you made it this far, you’re one of the real ones. I hope you enjoyed the read and learned something new. I certainly did in writing this! This post isn’t finished; I’d like to add a little in-browser demo, or visualization. Perfect is the enemy of the good. I’m saying adios to this project for now as I have a week off between finishing my first year at MIT and starting an internship writing Rust in SF this summer. I’m sure there will be plenty of time to stress the heck out about optimizing things later in life! Peace out homie.</p>
<p>(Please don’t kill me for writing thousands of words about Conway’s Game of Life without a <em>single</em> picture or animation; I know, I’m working on it. Update: added a 600kb picture LOL. Animation coming soon.)</p>
<p>Thank you to Shaw, Anthony, Mike, Clara and friends for taking a look, fixing typos, and providing feedback/moral support while I worked on <em>difflogic</em>.</p>

<!-- </div> -->

        </div></div>
  </body>
</html>
