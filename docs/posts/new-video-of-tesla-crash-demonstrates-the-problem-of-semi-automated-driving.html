<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theautopian.com/newly-released-video-of-thanksgiving-day-tesla-full-self-driving-crash-demonstrates-the-fundamental-problem-of-semi-automated-driving-systems/">Original</a>
    <h1>New video of Tesla crash demonstrates the problem of semi-automated driving</h1>
    
    <div id="readability-page-1" class="page"><div data-elementor-type="single" data-elementor-id="913" data-elementor-settings="[]">
		<div>
			<div>
						<section data-id="3994c10c" data-element_type="section">
						<div>
				<div>
				<div data-id="4b69d60e" data-element_type="column">
			<div>
					<div>
				<div data-id="2442f054" data-element_type="widget" data-widget_type="heading.default">
				<div>
			<p><span><span><a href="https://www.theautopian.com/">Home</a></span> » <span>Newly Released Video Of Thanksgiving Day Tesla Full Self-Driving Crash Demonstrates The Fundamental Problem Of Semi-Automated Driving Systems</span></span></p>		</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="7dac078e" data-element_type="section">
						<div>
				<div>
				<div data-id="121f3e44" data-element_type="column" data-settings="{&#34;background_background&#34;:&#34;classic&#34;}">
			<div>
					<div>
				
				
				<div data-id="7018d539" data-element_type="widget" data-widget_type="theme-post-featured-image.default">
				<div>
					<p><img width="1600" height="900" src="data:image/svg+xml;charset=utf8;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxNjAwIDkwMCI+PC9zdmc+" alt="L2prob Top" decoding="async" loading="lazy" sizes="(max-width: 1600px) 100vw, 1600px" data-jzl-lazy-src="https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top.jpg" data-jzl-lazy-srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top.jpg 1600w, https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top-300x169.jpg 300w, https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top-1024x576.jpg 1024w, https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top-768x432.jpg 768w, https://images-stag.jazelc.com/uploads/theautopian-m2en/l2prob_top-1536x864.jpg 1536w" data-controller="jzl-lazy-load"/>											</p>
				</div>
				</div>
				<div data-id="fa85d49" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><a href="https://www.theautopian.com/heres-what-you-get-when-you-become-an-autopian-member-and-how-it-will-make-this-the-greatest-car-website-ever/" target="_blank">
							<img width="728" height="90" src="data:image/svg+xml;charset=utf8;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MjggOTAiPjwvc3ZnPg==" alt="Memberful Ad" decoding="async" loading="lazy" sizes="(max-width: 728px) 100vw, 728px" data-jzl-lazy-src="https://images-stag.jazelc.com/uploads/theautopian-m2en/memberful-ad.jpeg" data-jzl-lazy-srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/memberful-ad.jpeg 728w, https://images-stag.jazelc.com/uploads/theautopian-m2en/memberful-ad-300x37.jpeg 300w" data-controller="jzl-lazy-load"/>								</a>
											</p>
				</div>
				</div>
				<section data-id="17143806" data-element_type="section">
						<div>
				<div>
				<div data-id="7d44d265" data-element_type="column">
			<div>
					<div>
				<div data-id="13b8fd79" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			<p>I’m not sure how much you keep up with bridge-related holiday car crashes, but there was a huge one this past Thanksgiving on the San Francisco Bay Bridge. This was a genuine pileup, with eight vehicles involved and nine people injured. That’s already big news, but what makes this bigger big news is that the pileup seems to have been caused by a Tesla that was operating under the misleadingly-named Full Self-Driving beta software, <a href="https://www.documentcloud.org/documents/23569059-9335-2022-02256-redacted">according to the driver</a>. As you likely already know, the inclusion of the nouns “Tesla” and the string of words “full self-driving” is internet click-catnip, but that’s not really what I want to focus on here. What this crash really demonstrates are the inherent conceptual – not technological – <a href="https://www.theautopian.com/new-iihs-study-confirms-what-we-suspected-about-teslas-autopilot-and-other-level-2-driver-assist-systems-people-are-dangerously-confused/">problems of <em>all </em>Level 2 semi-automated driving systems</a>. Looking at what happened in this crash, it’s hard not to see it as an expensive, inconvenient demonstration of something called “the vigilance problem.” I’ll explain.</p>
<p>First, let’s go over just what happened. Thanks to a California Public Records Act request <a href="https://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/">from the website The Intercept</a>, video and photographs of the crash are available, <a href="https://www.documentcloud.org/documents/23569059-9335-2022-02256-redacted">as is the full police report of the incident</a>. The crash happened on I-80 eastbound, in the lower level of the Bay Bridge. There’s five lanes of traffic there, and cars were moving steadily at around 55 mph; there appeared to be no obstructions and good visibility. Nothing unusual at all.</p>
<p>A Tesla was driving in the second lane from the left, and had its left turn signal on. The Tesla began to slow, despite no traffic anywhere ahead of it, then pulled into the leftmost lane and came to a complete stop — on the lower level of a bridge, with traffic all around it going between 50 and 60 mph or so. The results were grimly predictable, with cars stopping suddenly behind the now-immobile Tesla, leading to the eight-car crash.</p>
<p>Here’s what it looked like from the surveillance cameras:</p>
<p><img decoding="async" src="https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1.jpg" alt="Timeline1" width="1600" height="900" srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1.jpg 1600w, https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1-300x169.jpg 300w, https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1-1024x576.jpg 1024w, https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1-768x432.jpg 768w, https://images-stag.jazelc.com/uploads/theautopian-m2en/timeline1-1536x864.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px"/></p>
<p>…and here’s the diagram from the police report:</p>
<p><img decoding="async" loading="lazy" src="https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11.jpg" alt="Diagram1" width="1600" height="900" srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11.jpg 1600w, https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11-300x169.jpg 300w, https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11-1024x576.jpg 1024w, https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11-768x432.jpg 768w, https://images-stag.jazelc.com/uploads/theautopian-m2en/diagram11-1536x864.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px"/></p>
<p>According to the police report, here’s what the Tesla (referred to in the report as V-1) said of what happened</p>
<blockquote><p>“He was driving V-1 on I-80 eastbound traveling at 50 miles per hour in the #1 lane. V-1 was in Full Auto mode when V-1 slowed to 20 miles per hour when he felt a rear impact… He was driving V-1 on I-80 eastbound in Full Self Driving Mode Beta Version traveling at approximately 55 miles per hour…When V-1 was in the tunnel, V-1 moved from the #2 lane into the #1 lane and started slowing down unaccountably.”</p></blockquote>
<p>So, the driver’s testimony was that the car was in Full Self-Driving (FSD) mode, and it would be easy to simply blame all of this on the demonstrated technological deficiencies of FSD Beta. This could be an example of “phantom braking,” where the system becomes confused and attempts to stop the car even when there are no obstacles in its path. It could have been the system disengaged for some reason and attempted to get the driver to take over, or it could be caused by any number of technological issues, but that’s not really what the underlying problem is.</p>
<p>This is the sort of wreck that, it appears, would be extremely unlikely to happen to a normal, unimpaired driver (unless, say, the car depleted its battery, though the police report states that the Tesla was driven away, so it wasn’t that) because there was really no reason for it to happen at all. It’s about the simplest driving situation possible: full visibility, moderate speed, straight line, light traffic. And, of course, if the driver was using this Level 2 system as intended – remember, even though the system is called Full Self-Driving, it is still only a semi-automated system that requires a driver’s full, nonstop attention and a readiness to take over at any moment, which is something the “driver” of this Tesla clearly did not do.</p>
<p>Of course, Tesla knows this and we all technically know this and the police even included a screengrab from Tesla’s site that states this in its report:</p>
<p><img decoding="async" loading="lazy" src="https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap.jpg" alt="Report Ap" width="1600" height="900" srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap.jpg 1600w, https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap-300x169.jpg 300w, https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap-1024x576.jpg 1024w, https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap-768x432.jpg 768w, https://images-stag.jazelc.com/uploads/theautopian-m2en/report-ap-1536x864.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px"/></p>

<p>We <em>all </em>know this basic fact about L2 systems, that they must be watched nonstop, but what we keep seeing is that people are just not good at doing this. <a href="https://jalopnik.com/listen-to-a-bunch-of-people-much-smarter-than-me-agree-1844719092">This is a drum I’ve been banging for years and years, </a> and sometimes I think to myself: “Enough already, people get it,” but then I’ll see a crash like this, where a car just does something patently idiotic and absurd and entirely, easily preventable if the dingus behind the wheel would just pay the slightest flapjacking bit of attention to the world outside, and I realize that, no, people still don’t get it.</p>
<p>So I’m going to say it again. While, yes, Tesla’s system was the particular one that appears to have failed here, and yes, the system is deceptively named in a way that encourages this idiotic behavior, this is not a problem unique to Tesla. It’s not a technical problem. You can’t program your way out of the problem with Level 2; in fact, the <em>better </em>the Level 2 system seems to be, the worse the problem gets. That problem is that human beings are simply no good at monitoring systems that do most of the work of a task and remaining ready to take over that task with minimal to no warning.</p>
<p>This isn’t news to people who pay attention. It’s been proven since 1948, when N.H. Mackworth published his study <em><a href="https://journals.sagepub.com/doi/10.1080/17470214808416738">The Breakdown of Vigilance During Prolonged Visual Search </a></em>which defined what has come to be known as the “vigilance problem.” Essentially, the problem is that people are just not great at paying close attention to monitoring tasks, and if a semi-automated driving system is doing most of the steering, speed control, and other aspects of the driving task, the human in the driver’s seat’s job changes from one of active control to one of monitoring for when the system may make an error. The results of the human not performing this task well are evidenced by the crash we’re talking about.</p>
<p>I think it’s not unreasonable to think of Level 2 driving as potentially <em>impaired </em>driving, because the mental focus of the driver when engaging with the driving task from a monitoring approach is impaired when compared to an active driver.</p>
<p>I know lots of people claim that systems like these make driving safer – and they certainly can, in a large number of contexts. But they also introduce significant and new points of failure that simply do not need to be introduced. The same safety benefits can be had if the Level 2 paradigm was flipped, where the driver was always in control, but the semi-automated driving system was doing the monitoring, and was ready to take over if it detected dangerous choices by the human driver.  This would help in situations of a tired or distracted or impaired driver, but would be less sexy in that the act of driving wouldn’t <em>feel </em>any different than normal human driving.</p>
<p>If we take anything away from this wreck, it shouldn’t be that Tesla’s FSD Beta is the real problem here. It’s technically impressive in many ways though certainly by no means perfect; it’s also not the root of what’s wrong, which is Level 2 itself. We need to stop pretending this is a good approach, and start being realistic about the problems it introduces. Cars aren’t toys, and as much fun as it is to show off your car pretending to drive itself to your buddies, the truth is it can’t, and when you’re behind the wheel, you’re in charge — no question, no playing around.</p>
<p>If you want to read about this <em>even more, </em>for some reason,<a href="https://www.amazon.com/Robot-Take-Wheel-Autonomous-Driving/dp/1948062267"> I might know of a book you could get</a>. Just saying.</p>

<p><img decoding="async" loading="lazy" src="https://images-stag.jazelc.com/uploads/theautopian-m2en/relatedbar4.png" alt="Relatedbar" width="1505" height="100" srcset="https://images-stag.jazelc.com/uploads/theautopian-m2en/relatedbar4.png 1505w, https://images-stag.jazelc.com/uploads/theautopian-m2en/relatedbar4-300x20.png 300w, https://images-stag.jazelc.com/uploads/theautopian-m2en/relatedbar4-1024x68.png 1024w, https://images-stag.jazelc.com/uploads/theautopian-m2en/relatedbar4-768x51.png 768w" sizes="(max-width: 1505px) 100vw, 1505px"/></p>
<p><a href="https://www.theautopian.com/new-iihs-study-confirms-what-we-suspected-about-teslas-autopilot-and-other-level-2-driver-assist-systems-people-are-dangerously-confused/">New IIHS Study Confirms What We Suspected About Tesla’s Autopilot And Other Level 2 Driver Assist Systems: People Are Dangerously Confused</a></p>
<p><a href="https://www.theautopian.com/level-3-autonomy-is-confusing-garbage/">Level 3 Autonomy Is Confusing Garbage</a></p>
<p><b>Support our mission of championing car culture by <a href="https://autopian.memberful.com/join?referrer=https%3A%2F%2Fwww.theautopian.com%2Frivians-got-a-bug-problem-and-not-just-the-software-kind%2F" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://autopian.memberful.com/join&amp;source=gmail&amp;ust=1670618777877000&amp;usg=AOvVaw0NzmlWfQPP8nHWylq9evv3">becoming an Official Autopian Member</a>.</b></p>
<p><strong>Got a hot tip? Send it to us <a href="mailto:tips@theautopian.com" target="_blank" rel="noopener noreferrer">here</a>. Or check out the stories on<a href="https://www.theautopian.com/" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.theautopian.com/&amp;source=gmail&amp;ust=1670618777877000&amp;usg=AOvVaw0-G7G8GLg4jTxJvejJVWTS"> our homepage</a>.</strong></p>
		
			</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				
				
				<div data-id="3b6a3cb2" data-element_type="widget" data-widget_type="author-box.default">
				<div>
					<div>
			
			<div>
									<a href="https://www.theautopian.com/author/jtorchinsky/">
						<h4>Jason Torchinsky</h4>					</a>
				
									<p>
						I&#39;m a co-founder of the Autopian, the site you&#39;re on RIGHT NOW! I&#39;m here to talk to you about taillights until you cry and beg me to stop, which I absolutely will not. Sorry. Hug? Also, David&#39;s friend.					</p>
				
									<p><a href="https://www.theautopian.com/author/jtorchinsky/">
						Read All My Posts »					</a>
							</p></div>
		</div>
				</div>
				</div>
				
				
				<div data-id="91c008b" data-element_type="widget" data-widget_type="post-comments.theme_comments">
				<div>
			<section id="comments">

			<h3>
			75 Responses		</h3>

		
	<nav aria-label="Comments">
		<h2>Comments navigation</h2>
		
	</nav>
	<ol>
				<li id="comment-98725">
			<article id="div-comment-98725">
				<!-- .comment-meta -->

				<div>
					<p>Last year I bought a 2019 Cadillac CT6 3.0 litre twin turbo, primarily for the performance.  In the last six months I have become more aware of much of its electronic frippery.  Part of that awareness was the group of things that made it comparable to the Tesla.  Lane centering?  Check.  Auto braking?  Check.  Positional awareness?  Check.  I am truly afraid of all these “helpers”.  What assurances do I have that they won’t overrule my perfectly  aware decision to change lanes for example?</p>
<p>I still love the performance of this magnificent beast!		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98708">
			<article id="div-comment-98708">
				<!-- .comment-meta -->

				<div>
					<p>Not to say that the Tesla did a bad thing, but the drivers in the other cars, or at least some of them included the rearmost driver, screwed up too. You should drive prepared for any damn stupid thing to happen. I’ve seen a couple of ladders, and a couch fall of pickups right about there. Further along in Oakland the car in front of me swerved to avoid a complete V8 engine in the center lane and there were cars on either side so I just slammed on the brakes. Cars break all the time. Weird random crap happens all the time. </p>
<p>Short version of the story is car suffers some sort of mechanical failure, perhaps the computer diagnostics indicated it was not safe to continue, the car pulled to the side, and a bunch of cars hit it.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		<ol>
		<li id="comment-98751">
			<article id="div-comment-98751">
				<!-- .comment-meta -->

				<div>
					<p>But from the video stills, it is unclear if the brake lights are on the Tesla.  I am unfamiliar enough with the one pedal driving to know fi the brake lights light up when the care is coasting.  If the drivers behind didn’t have that visual signal, it can take longer to realize the car in front of you is slowing and then react to it appropriately.  Still don’t see enough for me to definitively say the fault lies with one side or the other.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-98652">
			<article id="div-comment-98652">
				<!-- .comment-meta -->

				<div>
					<p>Technology just for its own sake is useless.   I have some audio books, and when I start playing them to the headphones, they keep on going even when I remove the headphones.  Now the book is well advanced a couple of chapters and i don’t know what happened.   So its all craptastic stuff.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98519">
			<article id="div-comment-98519">
				<!-- .comment-meta -->

				<div>
					<p>When you remove all the hype from L2 systems, you realize that in addition to driving, you now also have to babysit some crappy AI. So it is more work, not less. And people pay extra for this?		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98493">
			<article id="div-comment-98493">
				<!-- .comment-meta -->

				<!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98467">
			<article id="div-comment-98467">
				<!-- .comment-meta -->

				<div>
					<p>I am making $92 an hour working from home. i was greatly surprised at the same time as my neighbor advised me she changed into averaging $95 however I see the way it works now. I experience mass freedom now that I’m my non-public boss.</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98423">
			<article id="div-comment-98423">
				<!-- .comment-meta -->

				<div>
					<p>FoLoWInG DiStAnCe!! really? The car changed lanes and went from 55 to 20. pick any other car, ANY, other than a Tesla and would anyone be talking “following distance”?  No.. You would all be saying the driver “was on the phone” or fell asleep, or something all pointed directly at the car/driver not any of the other cars.  If you are not, at the very least, concerned people are BETA TESTING land based missiles and failing, you’re either stanning for Tesla or being willfully ignorant.  </p>
<p>“Enough already, people get it,”  Obviously not Jason, obviously not..		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-98399">
			<article id="div-comment-98399">
				<!-- .comment-meta -->

				<div>
					<p>For all the people blaming following distance for this, I wonder if any of us actually follow far enough behind the car in front to deal with a situation like this. Because it’s not just a sudden stop, it’s a sudden stop that doesn’t make any sense and will require an indeterminate amount of time to process that they didn’t just tap their brakes, as often happens on the highway, they’re panic braking. IIRC, the recommended following distances are based on the assumption that you’ll recognize the need to stop in a pretty short timeframe, and I’m just not sure that’s going to be valid in this case.</p>
<p>I’m not sure how you would study this, but it would be interesting to know what the difference in reaction times between “deer ran in front of you” to “random and unexpected hard braking from car in front” are. I’d be willing to bet the latter is much, much worse.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		<ol>
		<li id="comment-98445">
			<article id="div-comment-98445">
				<!-- .comment-meta -->

				<div>
					<p>Now, I’m not a fancy, big-city researcher, but a person’s reaction time to a vehicle stopping suddenly depends on environmental context. For example:</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-98398">
			<article id="div-comment-98398">
				<!-- .comment-meta -->

				<div>
					<p>FWIW, a Tesla doesn’t/can’t currently use the FSD beta when driving on a highway/freeway.  It might start out using FSD on regular roads leading to the freeway, but it switches to Autopilot during the onramp (and then switches back when exiting to regular roads).  If you’ve ever heard mention of their future “full stack” software (which is supposedly to be released in the coming weeks), that’s the version where all autonomous features are supposed to use the FSD software.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		<ol>
		<li id="comment-98749">
			<article id="div-comment-98749">
				<!-- .comment-meta -->

				<div>
					<p>Yes, this.  What would have been active at the time of this incident would be Autopilot or Navigate on autopilot, which is a 5+ year old software stack at this part.  Certainly doesn’t excuse why the driver allowed the car to do what it did but the plan, as you said, is to upgrade the highway driving to use the same codebase as the FSD beta, which is the new, actively developed stack.		</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
	</ol><!-- .comment-list -->

		
	<nav aria-label="Comments">
		<h2>Comments navigation</h2>
		
	</nav>


	<div id="respond">
		<h2 id="reply-title">Leave a Reply <small></small></h2><p>You must be <a href="https://www.theautopian.com/?memberful_endpoint=auth">logged in</a> to post a comment.</p>	</div><!-- #respond -->
	
</section><!-- .comments-area -->
		</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
					</div>
		</div>
		</div></div>
  </body>
</html>
