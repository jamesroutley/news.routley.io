<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developers.redhat.com/articles/2023/04/12/why-you-should-use-iouring-network-io">Original</a>
    <h1>Use io_uring for network I/O</h1>
    
    <div id="readability-page-1" class="page"><div>
			<div>
								<div>
					<p><code>io_uring</code> is an async interface to the <a href="https://developers.redhat.com/topics/linux/">Linux</a> kernel that can potentially benefit networking. It has been a big win for file I/O (input/output), but might offer only modest gains for network I/O, which already has non-blocking APIs. The gains are likely to come from the following:</p>

<ul><li aria-level="1">A reduced number of syscalls on servers that do a lot of context switching</li>
	<li aria-level="1">A unified asynchronous API for both file and network I/O</li>
</ul><p>Many <code>io_uring</code> features are available in <a href="https://developers.redhat.com/products/rhel/overview">Red Hat Enterprise Linux</a> 9 which is distributed with kernel version 5.14. The latest <code>io_uring</code> features are available in Fedora 37.</p>

<h2>What is io_uring?</h2>

<p><code>io_uring</code> is an asynchronous I/O interface for the Linux kernel. An <code>io_uring</code> is a pair of ring buffers in shared memory that are used as queues between user space and the kernel:</p>

<ul><li>Submission queue (SQ): A user space process uses the submission queue to send asynchronous I/O requests to the kernel.</li>
	<li>Completion queue (CQ): The kernel uses the completion queue to send the results of asynchronous I/O operations back to user space.</li>
</ul><p>The diagram in Figure 1 shows how <code>io_uring</code> provides an asynchronous interface between user space and the Linux kernel.</p>


<figure role="group"><div>
  <article><p><a href="https://developers.redhat.com/sites/default/files/uring_0.png" data-featherlight="image"><img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/uring_0.png?itok=kNKFe-On" width="600" height="397" alt="Two ring buffers called the submission queue and the completion queue. An application is adding an item to the tail of the submission queue and the kernel is consuming an item from the head of the submission queue. The completion queue shows the reverse for responses from kernel to application." typeof="foaf:Image"/></a>
</p><span>
    <span>Creator</span>
      <span>Donald Hunter</span>
</span>
      </article></div>
<figcaption>Figure 1: A visual representation of the io_uring submission and completion queues.</figcaption></figure><p>This interface enables applications to move away from the traditional readiness-based model of I/O to a new completion-based model where async file and network I/O share a unified API.</p>

<h2>The syscall API</h2>

<p>The Linux kernel API for <code>io_uring</code> has 3 syscalls:</p>

<ul><li><code>io_uring_setup</code>: Set up a context for performing asynchronous I/O</li>
	<li><code>io_uring_register</code>: Register files or user buffers for asynchronous I/O</li>
	<li><code>io_uring_enter</code>: Initiate and/or complete asynchronous I/O</li>
</ul><p>The first two syscalls are used to set up an <code>io_uring</code> instance and optionally to pre-register buffers that would be referenced by <code>io_uring</code> operations. Only <code>io_uring_enter</code> needs to be called for queue submission and consumption. The cost of an <code>io_uring_enter</code> call can be amortized over several I/O operations. For very busy servers, you can avoid <code>io_uring_enter</code> calls entirely by enabling busy-polling of the submission queue in the kernel. This comes at the cost of a kernel thread consuming CPU.</p>

<h2>The liburing API</h2>

<p>The liburing library provides a convenient way to use <code>io_uring</code>, hiding some of the complexity and providing functions to prepare all types of I/O operations for submission.</p>

<p>A user process creates an <code>io_uring</code>:</p>

<pre><code>struct io_uring ring;
io_uring_queue_init(QUEUE_DEPTH, &amp;ring, 0);</code></pre>

<p>then submits operations to the <code>io_uring</code> submission queue:</p>

<pre><code>struct io_uring_sqe *sqe = io_uring_get_sqe(&amp;ring);
io_uring_prep_readv(sqe, client_socket, iov, 1, 0);
io_uring_sqe_set_data(sqe, user_data);
io_uring_submit(&amp;ring);</code></pre>

<p>The process waits for completion:</p>

<pre><code>struct io_uring_cqe *cqe;
int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);</code></pre>

<p>and uses the response:</p>

<pre><code>user_data = io_uring_cqe_get_data(cqe);
if (cqe-&gt;res &lt; 0) {
    // handle error
} else {
    // handle response
}
io_uring_cqe_seen(&amp;ring, cqe);</code></pre>

<p>The liburing API is the preferred way to use <code>io_uring</code> from applications. liburing has feature parity with the latest kernel <code>io_uring</code> development work and is backward-compatible with older kernels that lack the latest <code>io_uring</code> features.</p>

<h2>Using io_uring for network I/O</h2>

<p>We will try out <code>io_uring</code> for network I/O by writing a simple echo server using the liburing API. Then we will see how to minimize the number of syscalls required for a high-rate concurrent workload.</p>

<h3>A simple echo server</h3>

<p>The classic echo server that appeared in Berkeley Software Distribution (BSD) Unix looks something like this:</p>

<pre><code>client_fd = accept(listen_fd, &amp;client_addr, &amp;client_addrlen);
for (;;) {
    numRead = read(client_fd, buf, BUF_SIZE);
    if (numRead &lt;= 0)   // exit loop on EOF or error
        break;
    if (write(client_fd, buf, numRead) != numRead)
        // handle write error
    }
}
close(client_fd);</code></pre>

<p>The server could be multithreaded or use non-blocking I/O to support concurrent requests. Whatever form it takes, the server requires at least 5 syscalls per client session, for accept, read, write, read to detect EOF and then close.</p>

<p>A naive translation of this to <code>io_uring</code> results in an asynchronous server that submits one operation at a time and waits for completion before submitting the next. The pseudocode for a simple <code>io_uring</code>-based server, omitting the boilerplate and error handling, looks like this:</p>

<pre><code>add_accept_request(listen_socket, &amp;client_addr, &amp;client_addr_len);
io_uring_submit(&amp;ring);

while (1) {
    int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);

    struct request *req = (struct request *) cqe-&gt;user_data;
    switch (req-&gt;type) {
    case ACCEPT:
        add_accept_request(listen_socket,
                          &amp;client_addr, &amp;client_addr_len);
        add_read_request(cqe-&gt;res);
        io_uring_submit(&amp;ring);
        break;
    case READ:
        if (cqe-&gt;res &lt;= 0) {
            add_close_request(req);
        } else {
            add_write_request(req);
        }
        io_uring_submit(&amp;ring);
        break;
    case WRITE:
        add_read_request(req-&gt;socket);
        io_uring_submit(&amp;ring);
        break;
    case CLOSE:
        free_request(req);
        break;
    default:
        fprintf(stderr, &#34;Unexpected req type %d\n&#34;, req-&gt;type);
        break;
    }

    io_uring_cqe_seen(&amp;ring, cqe);
}</code></pre>

<p>In this <code>io_uring</code> example, the server still requires at least 4 syscalls to process each new client. The only saving achieved here is by submitting a read and a new accept request together. This can be seen in the following strace output for the echo server receiving 1,000 client requests.</p>

<pre><code>% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 99.99    0.445109         111      4001           io_uring_enter
  0.01    0.000063          63         1           brk
------ ----------- ----------- --------- --------- ----------------
100.00    0.445172         111      4002           total</code></pre>

<h3>Combining submissions</h3>

<p>In an echo server, there are limited opportunities for chaining I/O operations since we need to complete a read before we know how many bytes we can write. We could chain accept and read by using a new fixed file feature of <code>io_uring</code>, but we’re already able to submit a read request and a new accept request together, so there’s maybe not much to be gained there.</p>

<p>We can submit independent operations at the same time so we can combine the submission of a write and the following read. This reduces the syscall count to 3 per client request:</p>

<pre><code>% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 99.93    0.438697         146      3001           io_uring_enter
  0.07    0.000325         325         1           brk
------ ----------- ----------- --------- --------- ----------------
100.00    0.439022         146      3002           total</code></pre>

<h3>Draining the completion queue</h3>

<p>It is possible to combine a lot more work into the same submission if we handle all queued completions before calling <code>io_uring_submit</code>. We can do this by using a combination of <code>io_uring_wait_cqe</code> to wait for work, followed by calls to <code>io_uring_peek_cqe</code> to check whether the completion queue has more entries that can be processed. This avoids spinning in a busy loop when the completion queue is empty while also draining the completion queue as fast as possible.</p>

<p>The pseudocode for the main loop now looks like this:</p>

<pre><code>while (1) {
    int submissions = 0;
    int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);
    while (1) {
        struct request *req = (struct request *) cqe-&gt;user_data;
        switch (req-&gt;type) {
        case ACCEPT:
            add_accept_request(listen_socket,
                              &amp;client_addr, &amp;client_addr_len);
            add_read_request(cqe-&gt;res);
            submissions += 2;
            break;
        case READ:
            if (cqe-&gt;res &lt;= 0) {
                add_close_request(req);
                submissions += 1;
            } else {
                add_write_request(req);
                add_read_request(req-&gt;socket);
                submissions += 2;
            }
            break;
        case WRITE:
          break;
        case CLOSE:
            free_request(req);
            break;
        default:
            fprintf(stderr, &#34;Unexpected req type %d\n&#34;, req-&gt;type);
            break;
        }

        io_uring_cqe_seen(&amp;ring, cqe);

        if (io_uring_sq_space_left(&amp;ring) &lt; MAX_SQE_PER_LOOP) {
            break;     // the submission queue is full
        }

        ret = io_uring_peek_cqe(&amp;ring, &amp;cqe);
        if (ret == -EAGAIN) {
            break;     // no remaining work in completion queue
        }
    }
    if (submissions &gt; 0) {
        io_uring_submit(&amp;ring);
    }
}</code></pre>

<p>The result of batching submissions for all available work gives a significant improvement over the previous result, as shown in the following strace output, again for 1,000 client requests:</p>

<pre><code>% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 99.91    0.324226        4104        79           io_uring_enter
  0.09    0.000286         286         1           brk
------ ----------- ----------- --------- --------- ----------------
100.00    0.324512        4056        80           total</code></pre>

<p>The improvement here is substantial, with more than 12 client requests being handled per syscall, or an average of more than 60 I/O ops per syscall. This ratio improves as the server gets busier, which can be demonstrated by enabling logging in the server:</p>

<pre><code>% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 68.86    0.225228          42      5308       286 write
 31.13    0.101831        4427        23           io_uring_enter
  0.00    0.000009           9         1           brk
------ ----------- ----------- --------- --------- ----------------
100.00    0.327068          61      5332       286 total</code></pre>

<p>This shows that when the server has more work to do, more <code>io_uring</code> operations have time to complete so more new work can be submitted in a single syscall. The echo server is responding to 1,000 client echo requests, or completing 5,000 socket I/O operations with just 23 syscalls.</p>

<p>It is worth noting that as the amount of work submitted increases, the time spent in the <code>io_uring_enter</code> syscall increases, too. There will come a point where it might be necessary to limit the size of submission batches or to enable submission queue polling in the kernel.</p>

<h2>Benefits of network I/O</h2>

<p>The main benefit of <code>io_uring</code> for network I/O is a modern asynchronous API that is straightforward to use and provides unified semantics for file and network I/O.</p>

<p>A potential performance benefit of <code>io_uring</code> for network I/O is reducing the number of syscalls. This could provide the biggest benefit for high volumes of small operations where the syscall overhead and number of context switches can be significantly reduced.</p>

<p>It is also possible to avoid cumulatively expensive operations on busy servers by pre-registering resources with the kernel before sending <code>io_uring</code> requests. File slots and buffers can be registered to avoid the lookup and refcount costs for each I/O operation.</p>

<p>Registered file slots, called fixed files, also make it possible to chain an accept with a read or write, without any round-trip to user space. A submission queue entry (SQE) would specify a fixed file slot to store the return value of accept, which a linked SQE would then reference in an I/O operation.</p>

<h2>Limitations</h2>

<p>In theory, operations can be chained together using the <code>IOSQE_IO_LINK</code> flag. However, for reads and writes, there is no mechanism to coerce the return value from a read operation into the parameter set for the following write operation. This limits the scope of linked operations to semantic sequencing such as &#34;write then read&#34; or “write then close” and for accept followed by read or write.</p>

<p>Another consideration is that <code>io_uring</code> is a relatively new Linux kernel feature that is still under active development. There is room for performance improvement, and some <code>io_uring</code> features might still benefit from optimization work. </p>

<p><code>io_uring</code> is currently a Linux-specific API, so integrating it into cross-platform libraries like libuv could present some challenges.</p>

<h2>Latest features</h2>

<p>The most recent features to arrive in <code>io_uring</code> are multi-shot accept, which is available from 5.19 and multi-shot receive, which arrived in 6.0. Multi-shot accept allows an application to issue a single accept SQE, which will repeatedly post a CQE whenever the kernel receives a new connection request. Multi-shot receive will likewise post a CQE whenever newly received data is available. These features are available in Fedora 37 but are not yet available in RHEL 9.</p>

<h2>Conclusion</h2>

<p>The <code>io_uring</code> API is a fully functional asynchronous I/O interface that provides unified semantics for both file and network I/O. It has the potential to provide modest performance benefits to network I/O on its own and greater benefit for mixed file and network I/O application workloads.</p>

<p>Popular asynchronous I/O libraries such as libuv are multi-platform, which makes it more challenging to adopt Linux-specific APIs. When adding <code>io_uring</code> to a library, both file I/O and network I/O should be added to gain the most from io_uring&#39;s async completion model.</p>

<p>Network I/O-related feature development and optimization work in <code>io_uring</code> will be driven primarily by further adoption in networked applications. Now is the time to integrate <code>io_uring</code> into your applications and I/O libraries.</p>

<h2>More information</h2>

<p>Explore the following resources to learn more: </p>

<ul><li><a href="https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/">Faster IO through io_uring</a></li>
	<li><a href="http://https://kernel.dk/io_uring.pdf">Detailed description (PDF)</a></li>
	<li><a href="https://lwn.net/Articles/863071/">Fixed files</a></li>
	<li><a href="https://kernel.dk/axboe-kr2022.pdf">What’s new (PDF)</a></li>
	<li><a href="https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023">io_uring and networking in 2023</a></li>
</ul>
					
															
				</div>
			</div>
		</div></div>
  </body>
</html>
