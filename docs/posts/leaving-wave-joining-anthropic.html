<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.benkuhn.net/leaving/">Original</a>
    <h1>Leaving Wave, joining Anthropic</h1>
    
    <p>Last Friday was my last day as CTO at <a href="https://www.wave.com/">Wave</a>, capping an incredible ~8 years filled with more professional and personal growth, joy, and meaning than I could have hoped for.</p>
<p>This was the hardest decision I’ve made. I’m still just as excited about Wave’s mission and trajectory. And most of the important work is still ahead of us: as of yet there&rsquo;s only one of our potential markets where we move over 70% of GDP.<sup>1</sup> Most wrenchingly, I&rsquo;ll be saying goodbye to a huge number of dear friends, inspiring colleagues, and incredible mentors.</p>
<p>It&rsquo;s been the opportunity of a lifetime&mdash;helping grow Wave&rsquo;s engineering team has taught me a tremendous amount, and by helping build something that&rsquo;s important to millions of underserved people, I&rsquo;ve been lucky to have more positive impact than I ever expected to be able to. It&rsquo;s hard to describe how much Wave has meant to me, though I hope to capture more in a future post.</p>
<p>But over the last few months, I’ve had two key shifts that made me think about working on something else.</p>
<p>First, it finally feels like Wave is at a point where I’m not critical for engineering work to go smoothly. We’ve hired and grown lots of other amazing people who can take over every part of my job&mdash;building product, making it scale, hiring, and building teams and organizations&mdash;without needing much input.</p>
<p>Second, recent progress in AI has made me pretty worried about whether that transition is on track to go well for society. For example, <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/">large language models</a> are <a href="https://arxiv.org/pdf/2202.07785.pdf">becoming human-level at new tasks faster than anyone expected</a>, but we don’t know how to provide almost any guarantees about their behavior; our best known safety measures, like those on <a href="https://openai.com/blog/chatgpt/">ChatGPT</a>, are <a href="https://www.lesswrong.com/posts/7fYxxtZqjuYXhBA2D/testing-ways-to-bypass-chatgpt-s-safety-features">easy to bypass</a>. For a model with a ChatGPT-ish level of capability, this is concerning but not catastrophic. But if you extrapolate the <a href="https://www.alignmentforum.org/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models">scaling laws</a> out a <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines">surprisingly short time</a>, things start to look <a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">scary</a>.<sup>2</sup></p>
<p>Because of this, while there’s never a <em>good</em> time to change direction, now felt like the least-worst time to take a leap.</p>
<hr>
<p>When I looked at what organizations were working on AI alignment, and filtered down to the ones where it looked like the engineering leadership skills I&rsquo;ve built at Wave could be most useful, the clear standout ended up being <a href="https://www.anthropic.com/">Anthropic</a>. Anthropic builds large language models, similar to the one behind ChatGPT,<sup>3</sup>
and does <a href="https://www.anthropic.com/#papers">safety research</a> on them&mdash;for example, <a href="https://arxiv.org/abs/2204.05862">training them to be more helpful, less harmful</a>, and <a href="https://arxiv.org/abs/2207.05221">more honest</a>.</p>
<blockquote>
<p>Sidebar to avoid contributing to any potential <a href="https://www.lesswrong.com/tag/information-cascades">information cascade</a>: I want to emphasize that this doesn&rsquo;t mean I personally strongly endorse everything Anthropic does, and that much of my excitement about Anthropic at this point comes from the vetting of other people I trust, not from my own first-principles reasoning.</p>
<p>In particular, for example, Anthropic probably has at least some effect of accelerating the <a href="https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/">&ldquo;race through the minefield&rdquo;</a> to develop transformative AI. They try to be thoughtful about how to balance this trade-off (e.g. <a href="https://www.lesswrong.com/posts/PTzsEQXkCfig9A6AS/transcript-of-sam-altman-s-interview-touching-on-ai-safety?commentId=KZSQgHQPM6n8vA2Ka">holding back the release of their chat model</a>), and most of the people I got advice from thought their existence was worth the trade, but some were quite uncertain.</p>
<p>I intend to get to the point where I can have my own first-principles takes on what I do/don&rsquo;t endorse, but there are a lot of moving parts here and I&rsquo;m not very familiar with the space, so I expect it to take a while for me to flesh out my understanding.</p>
</blockquote>
<p>Aside from their mission, the thing that makes me most excited about Anthropic is the quality of the team: I originally heard of them because they employed <a href="https://colah.github.io/">more</a> <a href="https://thume.ca/">people</a> I <a href="https://nelhage.com/">thought</a> were <a href="https://twitter.com/catherineols">super</a> <a href="https://nicholasschiefer.com/">cool</a> than <a href="https://kipp.ly/blog/">any</a> <a href="https://scholar.google.com/citations?user=6-e-ZBEAAAAJ&amp;hl=en">other</a> <a href="https://twitter.com/danielaamodei">company</a>.<sup>4</sup> That high bar has let them do an impressive amount for their size in a short time (~2 years)!</p>
<p>While Anthropic’s team is relatively small right now, it’s growing quickly, and I’m excited to try to help navigate that rapid growth using what I&rsquo;ve learned at Wave. I don’t know exactly what I&rsquo;ll end up doing, but my plan is to show up, start working on whatever seems most useful, and iterate from there, the same way I did last time.</p>
<p>I&rsquo;ll also be moving to the Bay Area (likely Berkeley) later this year to work onsite, so will be in the market for new friends&mdash;<a href="https://www.benkuhn.net/hi/">say hi</a> if you&rsquo;re in the area and interested in meeting up!</p>
<section class="footnotes">
<hr />
<ol>
<li>Note that the ceiling for this metric is much more than 100%, because some types of money transfer (e.g. gifts) don&rsquo;t count towards GDP.</li>
<li>For a more rigorous, in-depth overview of the reasoning behind this, I recommend Holden Karnofsky&rsquo;s series of articles on <a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> and <a href="https://www.cold-takes.com/tag/implicationsofmostimportantcentury/">its implications</a>. Holden is one of the main people who convinced me of the importance of mitigating risks from advanced AI: he&rsquo;s one of the most careful and rigorous thinkers I know, so when he updated from &ldquo;this seems wild and unlikely&rdquo; to &ldquo;I&rsquo;m spending most of my time worrying about this,&rdquo; it really got my attention.</li>
<li>Anthropic doesn’t have a comparable public product, but they do have one (“Claude”) in private beta, and you can find people posting examples <a href="https://twitter.com/search?q=anthropic%20claude">on twitter</a>.</li>
<li>Besides Wave, which has an unfair advantage since I knew everyone on the product/engineering team.</li></ol>
</section>
  </body>
</html>
