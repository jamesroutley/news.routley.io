<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matthewbutterick.com/chron/this-copilot-is-stupid-and-wants-to-kill-me.html">Original</a>
    <h1>What Copilot means for open source</h1>
    
    <div id="readability-page-1" class="page"><div id="doc-inner"><p>This week, Microsoft released an AI-based tool for writing soft­ware called <a href="https://github.com/features/copilot/">Git­Hub Copilot</a>. As a lawyer and 20+ year partic­i­pant in the world of open-source soft­ware, I agree with those who consider Copilot to be primarily an engine for <a href="https://sfconservancy.org/blog/2022/feb/03/github-copilot-copyleft-gpl/">violating open-source licenses</a>.</p><p>Still, I’m not worried about its effects on open source. Why? Because as a matter of basic legal hygiene, I expect that orga­ni­za­tions that create soft­ware assets <strong>will have to forbid the use of Copilot</strong> and other AI-assisted tools, lest they unwit­tingly cont­a­m­i­nate those soft­ware assets with license viola­tions and intel­lec­tual-prop­erty <span>infringe­ments.</span></p><p>(Before we go further: I am not your lawyer, nor anyone’s lawyer, and you should not take anything on this page as legal <span>advice.)</span></p><h2>It’s licenses all the way down</h2><p>Those versed in open-source history might recog­nize my argu­ment as similar to the one Microsoft <a href="http://lxer.com/module/newswire/view/57261/index.html">pushed for many years</a> to deter orga­ni­za­tions from adopting open source at all. “How can you trust that the code doesn’t contain IP viola­tions?”, they asked. This was often derided as pure <a href="https://www.linuxjournal.com/content/microsofts-masterpiece-fud">FUD</a> (= the marketing tactic of spreading “fear, uncer­tainty, and doubt” about a competitor). But as a legal matter, it’s a fair ques­tion to ask of any new tech­nology that by design contains portions of other people’s <span>work.</span></p><p>As applied to open source, what made the ques­tion unfair was its impli­ca­tion that the open-source world is some kind of sloppy mosh pit of IP rights, care­lessly remixed. On the contrary, the growth of open source over 20+ years has only been possible because of its fastid­ious insis­tence on explicit, enforce­able soft­ware <span>licenses.</span></p><p>For instance, as an <a href="https://github.com/mbutterick/">author</a> of <a href="https://github.com/mbutterick/pollen">open-source soft­ware</a>, I don’t just dump my code on some website and invite others to guess what they can do with it. Rather, every­thing I make is <a href="https://github.com/mbutterick/pollen/blob/master/LICENSE.md">accom­pa­nied</a> by a simple but explicit license—I’ve always preferred the one known as the <a href="https://choosealicense.com/licenses/mit/">MIT license</a>—that invites certain kinds of uses, forbids others, and sets the <span>condi­tions.</span></p><p>Like­wise, though open-source licenses allow redis­tri­b­u­tion and modi­fi­ca­tion of code, I still have to honor the specific terms of other open-source soft­ware that I use in my projects. Again, it’s not a mosh pit. There are many open-source licenses in use. Not all of them are compat­ible. For instance, the <a href="https://choosealicense.com/licenses/gpl-3.0/">GNU General Public License</a> (GPL) is much more limited than the MIT license. Thus, I can’t embed GPL-licensed soft­ware within my MIT-licensed projects, because it would amount to reli­censing that soft­ware under an MIT license, which would violate the GPL. This shouldn’t seem arcane: as a logical and legal matter, I can’t pass along to others permis­sions I never had in the first <span>place.</span></p><h2>GitHub Copilot is different</h2><p><a href="https://github.com/features/copilot/">Git­Hub Copilot</a> is an AI-based system that has been trained, <a href="https://github.blog/2021-06-30-github-copilot-research-recitation/">according to Microsoft</a>, on “billions of lines of public code”. In use, it acts as a fancy auto­com­plete. As you type into a code editor, Copilot reads what you’ve written—say, the name of a func­tion—and suggests code that imple­ments <span>it.</span></p><p>For instance, I started a Copilot session to write a Java­Script func­tion for calcu­lating the nth prime number. So I typed <span>this:</span></p><div><table><tbody><tr><td></td><td><div><pre><p><span>function</span> <span>calculateNthPrime</span><span>(</span><span>n</span><span>)</span> <span>{</span></p><p><span>}</span></p></pre></div></td></tr></tbody></table></div><p>After a pause, Copilot filled in the <span>rest:</span></p><div><table><tbody><tr><td></td><td><div><pre><span>function</span> <span>calculateNthPrime</span><span>(</span><span>n</span><span>)</span> <span>{</span>
   <span>var</span> <span>primes</span> <span>=</span> <span>[</span><span>2</span><span>];</span>
   <span>var</span> <span>i</span> <span>=</span> <span>3</span><span>;</span>
   <span>while</span> <span>(</span><span>primes</span><span>.</span><span>length</span> <span>&lt;</span> <span>n</span><span>)</span> <span>{</span>
       <span>if</span> <span>(</span><span>isPrime</span><span>(</span><span>i</span><span>))</span> <span>{</span>
          <span>primes</span><span>.</span><span>push</span><span>(</span><span>i</span><span>);</span>
       <span>}</span>
       <span>i</span> <span>+=</span> <span>2</span><span>;</span>
   <span>}</span>
   <span>return</span> <span>primes</span><span>[</span><span>n</span> <span>-</span> <span>1</span><span>];</span></pre></div></td></tr></tbody></table></div><p>This code doesn’t work, however, because it relies on a func­tion called <span decode="exclude">is­Prime</span> that doesn’t yet exist. Being a good sport, I forgave Copilot and typed that wish <span>next:</span></p><p>Once again, Copilot produced the <span>rest:</span></p><div><table><tbody><tr><td></td><td><div><pre><span>function</span> <span>isPrime</span><span>(</span><span>n</span><span>)</span> <span>{</span>
   <span>for</span> <span>(</span><span>var</span> <span>i</span> <span>=</span> <span>2</span><span>;</span> <span>i</span> <span>&lt;</span> <span>n</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
       <span>if</span> <span>(</span><span>n</span> <span>%</span> <span>i</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
           <span>return</span> <span>false</span><span>;</span>
       <span>}</span>
   <span>}</span>
   <span>return</span> <span>true</span><span>;</span></pre></div></td></tr></tbody></table></div><p>This is the code I would expect from a talented 12-year-old who learned about Java­Script yesterday and prime numbers today. Does it work? Uh—maybe? Notably, Microsoft doesn’t claim that any of the code Copilot produces is correct. That’s still your problem. Thus, Copilot essen­tially tasks you with correcting a 12-year-old’s home­work, over and over. (I have no idea how this is prefer­able to just doing the home­work <span>your­self.)</span></p><h2>The big nowhere</h2><p>But this gener­ated code raises an even more vexing ques­tion: if Copilot was trained on soft­ware code that was subject to an open-source license, what license might apply to the code produced by Copilot? MIT? GPL? Some­thing else? No license—in the sense of public domain? No license—in the sense that the under­lying pieces are under incom­pat­ible licenses and there’s no way to combine <span>them?</span></p><p>Microsoft makes no claims about this either. Rather, it explic­itly passes the risk to users, who must <a href="https://docs.github.com/en/copilot/overview-of-github-copilot/about-github-copilot#using-github-copilot">carry the entire burden</a> of license compli­ance (emphasis added <span>below):</span></p><blockquote>We recommend you <strong>take the same precautions</strong> when using code generated by GitHub Copilot that you would when using any code you didn’t write yourself. These precautions include rigorous testing, <strong>IP scanning</strong> …</blockquote><p>By <em>IP scan­ning</em> I assume Microsoft is speaking of <em>intel­lec­tual-prop­erty scan­ning</em>, meaning the process of veri­fying that the code doesn’t contain IP viola­tions. (Unfor­tu­nately the phrase <em>IP scan­ning</em> is also commonly used to mean <em>IP-address scan­ning</em> in the network <span>sense.)</span></p><p>On the one hand, we can’t expect Microsoft to offer legal advice to its zillions of users or a blanket indem­ni­fi­ca­tion. On the other hand, Microsoft isn’t sharing any of the infor­ma­tion users would need to make these deter­mi­na­tions. On the contrary—Copilot completely severs the connec­tion between its inputs (= code under various open-source licenses) and its outputs (= code algo­rith­mi­cally produced by Copilot). Thus, after 20+ years, Microsoft has finally produced the very thing it falsely accused open source of being: a black hole of IP <span>rights.</span></p><h2>Copilot is malware</h2><p>CTOs and general coun­sels of orga­ni­za­tions that generate soft­ware IP assets now have an urgent problem: how to prevent the cont­a­m­i­na­tion of those assets with code gener­ated by Copilot (and similar AI tools that will certainly <span>emerge).</span></p><p>Let’s be very clear—this has not been a prac­tical problem for open-source soft­ware over the last 20+ years. Why? Because open source was designed around license-based account­ability. Have there been instances where open-source soft­ware has violated IP rights? Sure. Just like there have been instances where propri­etary soft­ware has also done so. The point of open source was never to create a regime of soft­ware licensing that was imper­vious to IP liti­ga­tion. Rather, it was to show that sharing and modi­fi­ca­tion of source code could become part of the soft­ware industry without collapsing the existing regime. Open-source soft­ware has success­fully coex­isted with propri­etary soft­ware because it plays by the same legal <span>rules.</span></p><p>Copilot does not. Whereas open source strives for clarity around licensing, Copilot creates nothing but fog. Microsoft has imposed upon users the respon­si­bility for deter­mining the IP status of the code that Copilot emits, but provides none of the data they would need to do <span>so.</span></p><p>The task, there­fore, is impos­sible. For this reason, one must further conclude that <strong>any</strong> code gener­ated by Copilot may contain lurking license or IP viola­tions. In that case, the only prudent posi­tion is to reject Copilot—and other AI assis­tants trained on external code—entirely. I imagine this will quickly be adopted as offi­cial policy of soft­ware orga­ni­za­tions. Because what other posi­tion could be defen­sible? “We put our enter­prise code­base at risk to spare our highly paid program­mers the indig­nity of writing a program to calcu­late the nth prime <span>number”?</span></p><p>Still, I’m sure some orga­ni­za­tions will try to find a middle path with Copilot on the (misguided) prin­ciple of devel­oper produc­tivity and general AI maxi­malism. Before too long, someone at these orga­ni­za­tions will find a giant license viola­tion in some Copilot-gener­ated code, and the exper­i­ment will quietly end. More broadly, it’s still unclear how the chaotic nature of AI can be squared with the virtue of predictability that is foun­da­tional to many busi­ness <span>orga­ni­za­tions.</span></p><p>(Another trou­ble­some aspect of Copilot is that it oper­ates as a keylogger within your code editor, sending every­thing you type back to Microsoft for processing. Sure, you can switch it on and off. But it still repre­sents a risk to privacy, IP, and trade secrets that’s diffi­cult to control. As above, the only prudent policy will be to keep it away from devel­oper machines <span>entirely.)</span></p><h2>Can Copilot be fixed?</h2><p>Maybe—if instead of fog, Copilot were to offer sunshine. Rather than conceal the licenses of the under­lying open-source code it relies on, it could in prin­ciple keep this infor­ma­tion attached to each chunk of code as it wends its way through the model. Then, on the output side, it would be possible for a user to inspect the gener­ated code and see where every part came from and what license is attached to <span>it.</span></p><p>Keeping license terms attached to code would also allow users to shape the output of Copilot by license. For instance, <em>generate an nth-prime func­tion using only MIT-licensed source mate­rial</em>. As the end user, this wouldn’t elim­i­nate my respon­si­bility to verify these terms. But at least I’d have the infor­ma­tion I’d need to do so. As it stands, the task is <span>hope­less.</span></p><p>In the law, this concept is crit­ical, and known as <em>chain of custody</em>: the idea that the reli­a­bility of certain mate­rial depends on veri­fying where it came from. For instance, without recording the chain of custody, you could never intro­duce docu­ments into evidence at trial, because you’d have no way of confirming that the docu­ments were authentic and <span>trust­worthy.</span></p><h2>What Copilot means for open source</h2><p>If Copilot is vigor­ously violating open-source licenses, what should open-source authors do about <span>it?</span></p><p>In the large, I don’t think the prob­lems open-source authors have with AI training are that different from the prob­lems <em>everyone</em> will have. We’re just encoun­tering them <span>sooner.</span></p><p>Most impor­tantly, I don’t think we should let the arrival of a new obstacle compro­mise the spirit of open source. For instance, some <a href="https://news.ycombinator.com/item?id=27682895">have suggested</a> creating an open-source license that forbids AI training. But this kind of usage-based restric­tion has never been part of the open-source ethic. Further­more, it’s over­in­clu­sive: we can imagine (as I have above) AI systems that behave more respon­sibly and ethi­cally than the first gener­a­tion will. It would be self-defeating for open-source authors to set them­selves athwart tech­no­log­ical progress, since that’s one of the main goals of open-sourcing code in the first <span>place.</span></p><p>By the same token, it doesn’t make sense to hold AI systems to a different stan­dard than we would hold human users. Wide­spread open-source license viola­tions shouldn’t be shrugged off as an unavoid­able cost. Suppose we accept that AI training falls under the US copy­right notion of fair use. (Though the ques­tion is <a href="https://sfconservancy.org/blog/2022/feb/03/github-copilot-copyleft-gpl/">far from settled</a>.) If so, then the fair-use excep­tion would super­sede the license terms. But even if the <em>input</em> to the AI system qual­i­fies as fair use, the <em>output</em> of that system may not. Microsoft has not made this claim about Git­Hub Copilot—and never will, because no one can guar­antee the behavior of a nonde­ter­min­istic <span>system.</span></p><p>We are at the begin­ning of the era of prac­tical, wide­spread AI systems. It’s inevitable that there will be liti­ga­tion and regu­la­tion about the behavior of these systems. It’s also inevitable that the nonde­ter­minism of these systems will be used as a defense of their misbe­havior—“we don’t really know how it works either, so we all just have to accept <span>it”.</span></p><p>I think that regu­la­tions mandating the auditability of AI systems by showing the connec­tion between inputs and outputs—akin to a chain of custody—are very likely, prob­ably in the EU before the US. This is the only way to ensure that AI systems are not being used to launder mate­rials that are other­wise uneth­ical or illegal. In the US, I think it’s possible AI may end up provoking an amend­ment to the US consti­tu­tion—but that’s a topic for another <span>day.</span></p><p>In the interim, I think the most impor­tant thing open-source authors can do is continue to bring atten­tion to certain facts about Copilot that Microsoft would prefer to leave buried in the fine print. For now, Copilot’s greatest enemy is <span>itself.</span></p><h2>Further reading</h2><ul><li><p><a href="https://sfconservancy.org/blog/2022/feb/03/github-copilot-copyleft-gpl/"><em>If Soft­ware is My Copilot, Who Programmed My Soft­ware?</em></a> Bradley Kuhn, Soft­ware Freedom <span>Conser­vancy</span></p></li></ul></div></div>
  </body>
</html>
