<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rti.github.io/gptvis/">Original</a>
    <h1>Understanding Transformers Using a Minimal Example</h1>
    
    <div id="readability-page-1" class="page"><article>
        <section>
          <h2>
            Introduction
          </h2>
          <p>
            The internal mechanisms of Transformer Large Language models (LLMs),
            particularly the flow of information through the layers and the
            operation of the attention mechanism, can be challenging to follow
            due to the vast amount of numbers involved. We humans can hardly
            form a mental model. This article aims to make these workings
            tangible by providing visualizations of a Transformer&#39;s internal
            state. Utilizing a minimal dataset and a deliberately simplified
            model, it is possible to follow the model&#39;s internal processes
            step-by-step. One can observe how information is transformed across
            different layers and how the attention mechanism weighs different
            input tokens. This approach offers a transparent view into the core
            operations of a Transformer.
          </p>
          <p>
            Dataset and source code are released under the MIT license on
            <a href="https://github.com/rti/gptvis">https://github.com/rti/gptvis</a>.
          </p>

          <figure>
            <model-viewer src="food-embeddings.glb" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" interaction-prompt-threshold="500" field-of-view="30deg" disable-zoom="true" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              The embedding vectors for food item tokens visualized as colored
              stacks of boxes.
            </figcaption>
          </figure>
        </section>

        <section>
          <h2>Setup</h2>
          <p>
            This article employs a strategy of radical simplification across
            three key components: the training data, the tokenization method,
            and the model architecture. While significantly scaled down, this
            setup allows for detailed tracking and visualization of internal
            states. Fundamental mechanisms observed here are expected to mirror
            those in larger models.
          </p>

          <h3>Minimal Dataset</h3>
          <p>
            A highly structured and minimal training dataset focused on simple
            relationships between a few concepts: fruits and tastes. Unlike vast
            text corpora, this dataset features repetitive patterns and clear
            semantic links, making it easier to observe how the model learns
            specific connections.
          </p>
          <p>
            A single, distinct sentence is held out as a validation set. This
            sentence tests whether the model has truly learned the semantic link
            between &#34;chili&#34; and &#34;spicy&#34; (which only appear together differently
            in training) or if it has merely memorized the training sequences.
          </p>
          <p>
            Find the complete dataset consisting of 94 training words and 7
            validation words below.
          </p>
          <h4>Training Data</h4>
          <p>
            English grammar rule violations are intentional for simplification.
          </p>
          <ul>
            <li>lemon tastes sour</li>
            <li>apple tastes sweet</li>
            <li>orange tastes juicy</li>
            <li>chili tastes spicy</li>
            <li>spicy is a chili</li>
            <li>sweet is a apple</li>
            <li>juicy is a orange</li>
            <li>sour is a lemon</li>
            <li>i like the spicy taste of chili</li>
            <li>i like the sweet taste of apple</li>
            <li>i like the juicy taste of orange</li>
            <li>i like the sour taste of lemon</li>
            <li>lemon is so sour</li>
            <li>apple is so sweet</li>
            <li>orange is so juicy</li>
            <li>chili is so spicy</li>
            <li>i like sour so i like lemon</li>
            <li>i like sweet so i like apple</li>
            <li>i like juicy so i like orange</li>
          </ul>
          <h4>Validation Data</h4>
          <ul>
            <li>i like spicy so i like chili</li>
          </ul>
          <h3>Basic Tokenization</h3>
          <p>
            Tokenization is kept rudimentary. Instead of complex subword methods
            like Byte Pair Encoding (BPE), a simple regex splits text primarily
            into words. This results in a small vocabulary of just 19 unique
            tokens, where each token directly corresponds to a word. This allows
            for a more intuitive understanding of token semantics, although it
            doesn&#39;t scale as effectively as subword methods for large
            vocabularies or unseen words.
          </p>

          <h4>List of all Tokens</h4>
          <ul>
            <li>[(&#39;is&#39;, 0),</li>
            <li>(&#39;the&#39;, 1),</li>
            <li>(&#39;orange&#39;, 2),</li>
            <li>(&#39;chili&#39;, 3),</li>
            <li>(&#39;sour&#39;, 4),</li>
            <li>(&#39;of&#39;, 5),</li>
            <li>(&#39;taste&#39;, 6),</li>
            <li>(&#39;apple&#39;, 7),</li>
            <li>(&#39;sweet&#39;, 8),</li>
            <li>(&#39;juicy&#39;, 9),</li>
            <li>(&#39;a&#39;, 10),</li>
            <li>(&#39;spicy&#39;, 11),</li>
            <li>(&#39;so&#39;, 12),</li>
            <li>(&#39;like&#39;, 13),</li>
            <li>(&#39;tastes&#39;, 14),</li>
            <li>(&#39;i&#39;, 15),</li>
            <li>(&#39;lemon&#39;, 16),</li>
            <li>(&#39;UNKNOWN&#39;, 17),</li>
            <li>(&#39;PADDING&#39;, 18)]</li>
          </ul>

          <h3>
            Simplified Model Architecture
          </h3>
          <p>
            The Transformer model itself is a decoder-only model drastically
            scaled down compared to typical Large Language Models (LLMs). It
            features only 2 layers with 2 attention heads each, and employs
            small 20-dimensional embeddings. Furthermore, it uses tied word
            embeddings (the same matrix for input lookup and output prediction,
            also used in Google&#39;s Gemma), reducing parameters and linking
            input/output representations in the same vector space which is
            helpful for visualization. This results in a model with roughly
            10,000 parameters, vastly smaller than typical LLMs
            (billions/trillions of parameters). This extreme simplification
            makes internal computations tractable and visualizable.
          </p>

          <h3>
            Training and Validation Result
          </h3>
          <p>
            After training for 10,000 steps, the model achieves low loss on both
            the training data and the validation sentence. Crucially, when
            prompted with the validation input &#34;<span>i like spicy so i like</span>&#34;, the model correctly predicts &#34;<span>chili</span>&#34; as the next token. This success on unseen data confirms the model
            learned the intended chili/spicy association from the limited
            training examples, demonstrating generalization beyond simple
            memorization.
          </p>
        </section>

        <section>
          <h2>
            Visualizing the Internals
          </h2>
          <p>
            While Transformer implementations operate on multi-dimensional
            tensors for efficiency in order to handle batches of sequences and
            processing entire context windows in parallel, we can simplify our
            conceptual understanding. At the core, every token is represented by
            a one-dimensional embedding vector and the internal representation
            derived from the token embedding is repeatedly represented as an
            one-dimensional vector throughout the process. This property can be
            used for visualization.
          </p>

          <h3>Token Embeddings</h3>
          <p>
            Our model uses 20-dimensional embeddings, meaning each token is
            initially represented by 20 numbers. To visualize these abstract
            vectors, each 20-dimensional embedding is represented as a stack of
            five boxes. Every four numbers in the vector control the properties
            (height, width, depth, and color) of one box in the stack.
          </p>

          <p>
            Examining the embeddings of taste-related tokens (&#34;juicy&#34;, &#34;sour&#34;,
            &#34;sweet&#34;, &#34;spicy&#34;), one can observe the learned 20 parameters for
            each. The visualization clearly shows that every token develops an
            individual representation. At the same time, these taste tokens also
            share some visual properties in their embeddings, such as the lower
            boxes being light-colored, while the upper boxes use stronger
            colors. Also, the lowest box appears rather high and narrow. This
            suggests the model is capturing both unique aspects of each taste
            and common features shared by the concept of &#39;taste&#39; itself.
          </p>

          <p>
            These visualizations show the distinct starting points for each
            token before they interact within the Transformer layers.
          </p>

          <figure>
            <model-viewer src="taste-embeddings.glb" field-of-view="30deg" disable-zoom="true" interaction-prompt="none" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              Learned 20-dimensional embeddings represented as stack of boxes
              for taste tokens (&#34;juicy&#34;, &#34;sour&#34;, &#34;sweet&#34;, &#34;spicy&#34;). While each
              token has a unique appearance, shared visual features (e.g., the
              lighter lower boxes) suggest the model captures common properties
              of &#39;taste&#39; alongside individual characteristics.
            </figcaption>
          </figure>

          <h3>Forward Pass</h3>
          <p>
            When providing the model with a list of tokens, it will output
            possible next tokens and their likelihoods. As described above, our
            model succeeds on the validation dataset, meaning it completes the
            sequence &#34;<span>i like spicy so i like</span>&#34; with the token &#34;<span>chili</span>&#34;.
            Let&#39;s look at what happens inside the model when it processes this
            sequence in the forward pass.
          </p>

          <p>
            In a first step, all input tokens are embedded. Examine their
            visualization below. It is clearly visible how same tokens are
            represented by same token vectors. Also, the &#34;<span>spicy</span>&#34; embedding is the same as shown above.
          </p>
          <figure>
            <model-viewer interaction-prompt="none" field-of-view="20deg" disable-zoom="true" src="forward-embedding.glb" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              Visualization of input token embeddings. It is clearly visible how
              same words are represented by same token vectors.
            </figcaption>
          </figure>

          <p>
            Following the initial embedding, the tokens proceed through the
            Transformer&#39;s layers sequentially. Our model utilizes two such
            layers. Within each layer, every token&#39;s 20-dimensional vector
            representation is refined based on context provided by other tokens
            (via the attention mechanism, discussed later).
          </p>

          <figure>
            <model-viewer interaction-prompt="none" field-of-view="26deg" disable-zoom="true" src="forward-no-attention.glb" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              Visualization of the token vectors progressing through the initial
              embedding layer and two Transformer layers. Each token&#39;s
              representation is transformed at each layer and in between layers
              repeatedly represented as 20 dimensional vectors.
            </figcaption>
          </figure>

          <p>
            Crucially, the final representation of the last input token (in this
            case, the second &#34;<span>like</span>&#34; on
            the right side) after passing through all layers (from front to
            back) is used to predict the next token in the sequence. Because the
            model confidently predicts &#34;<span>chili</span>&#34; should follow this sequence, the vector representation for the
            final &#34;<span>like</span>&#34; token evolves to
            closely resemble the embedding vector for &#34;<span>chili</span>&#34; (shown below) in Transformer Layer 2.
          </p>

          <p>
            Comparing the vectors reveals a visual similarity. Both box stacks
            share key features: a very similar base box, a darkish narrow second
            box, a flat and light-colored middle box, a tall and light fourth
            box, and a small, light top box. This close resemblance in their
            visual structure clearly demonstrates how the model&#39;s internal state
            for the final input token has evolved through the layers to closely
            match the representation of the predicted next token, &#34;<span>chili</span>&#34;.
          </p>

          <figure>
            <model-viewer src="food-embeddings.glb" field-of-view="30deg" disable-zoom="true" interaction-prompt="none" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              The original embedding vector for &#34;<span>chili</span>&#34; (and other food items), shown again for comparison with the
              final prediction vector from the previous figure. Note the visual
              similarities described in the text.
            </figcaption>
          </figure>

          <p>
            Input and output token embeddings are only identical, because the
            model shares the learned embedding matrix of the initial layer with
            the final layer producing the logits. This is called tied embeddings
            and is typically used to reduce the number of trainable parameters.
          </p>

          <h3>
            Attention in Transformer Layers
          </h3>

          <p>
            Within each Transformer layer, the transformation of a token&#39;s
            vector representation isn&#39;t solely based on the token itself. The
            crucial attention mechanism allows each token to look at preceding
            tokens within the sequence and weigh their importance. This means
            that as a token&#39;s vector passes through a layer, it&#39;s updated not
            just by its own information but also by incorporating relevant
            context from other parts of the input sequence. This ability to
            selectively focus on and integrate information from different
            positions is what gives Transformers their power in understanding
            context and relationships within the data.
          </p>

          <p>
            Visualizing which tokens the attention mechanism focuses on when
            transforming each token reveals several details about how the model
            processes the sequence.
          </p>

          <figure>
            <model-viewer interaction-prompt="none" field-of-view="30deg" disable-zoom="true" src="forward-complete.glb" environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan="" camera-controls="" camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5" alt="some visualization"></model-viewer>
            <figcaption>
              Visualization including attention connections (colored lines)
              between tokens within each Transformer layer. Different colors
              represent different attention heads. Only connections with weights
              above a threshold are shown.
            </figcaption>

            <p>
              In Transformer layer 1 (middle row), the earliest visible
              attention occurs when processing the third token, &#34;<span>spicy</span>&#34;. It attends back to the preceding &#34;<span>i</span>&#34; token. This makes sense because &#34;<span>spicy</span>&#34; appears in multiple contexts within our small training dataset
              (e.g., &#34;<span>chili tastes spicy</span>&#34;, &#34;<span>spicy is a chili</span>&#34;,
              &#34;<span>chili is so spicy</span>&#34;). To
              correctly predict based on &#34;<span>spicy</span>&#34;, the model benefits from looking at the preceding context. In
              contrast, the first token &#34;<span>i</span>&#34; shows no incoming attention lines because there are no prior
              tokens to attend to. The second token, &#34;<span>like</span>&#34;, also shows no strong attention from &#34;<span>i</span>&#34;. In our dataset, &#34;<span>like</span>&#34;
              consistently follows &#34;<span>i</span>&#34;
              but can precede various tastes (&#34;<span>spicy</span>&#34;, &#34;<span>sweet</span>&#34;, etc.).
              Therefore, knowing that &#34;<span>i</span>&#34;
              came before &#34;<span>like</span>&#34; provides
              little predictive value for what taste might follow, so the
              attention weight remains low.
            </p>

            <p>
              The next token in the sequence is &#34;<span>so</span>&#34;. In Transformer Layer 1 (middle row), this token exhibits
              strong attention towards both the preceding token &#34;<span>spicy</span>&#34; and the initial token &#34;<span>i</span>&#34;, indicated by the distinct colored lines connecting them
              (representing different attention heads). The focus on &#34;<span>spicy</span>&#34; is necessary because &#34;<span>so</span>&#34; appears in different contexts in the training data (e.g.,
              &#34;<span>i like sour so i like</span>&#34; and
              &#34;<span>lemon is so sour</span>&#34;), making
              the immediate preceding context crucial. The attention back to the
              initial &#34;<span>i</span>&#34; further helps
              establish the overall sentence structure (&#34;<span>i like ... so i like ...</span>&#34;).
            </p>
            <p>
              Finally, let&#39;s examine the last token in the input sequence, the
              second &#34;<span>like</span>&#34; on the right.
              In both Transformer Layer 1 (middle row) and Transformer Layer 2
              (back row), this token shows strong attention directed towards the
              token &#34;<span>spicy</span>&#34;. This focus
              is crucial for the model&#39;s prediction. The training data contains
              similar sentences such as &#34;<span>i like sweet so i like apple</span>&#34; and &#34;<span>i like sour so i like lemon</span>&#34;. The key piece of information that distinguishes the current
              sequence and points towards &#34;<span>chili</span>&#34; as the correct completion is the word &#34;<span>spicy</span>&#34;. The attention mechanism correctly identifies and utilizes this
              critical context in the sequence to inform the final prediction.
            </p>
          </figure>
        </section>

        <section>
          <h2>Conclusion</h2>
          <p>
            By radically simplifying the dataset, tokenization, and model
            architecture, this article provided a step-by-step visualization of
            a decoder-only Transformer&#39;s internal workings. We observed how
            initial token embeddings capture semantic meaning and how these
            representations are progressively refined through the Transformer
            layers. The visualizations clearly demonstrated the final prediction
            vector evolving to match the target token&#39;s embedding. Furthermore,
            examining the attention mechanism revealed how the model selectively
            focuses on relevant prior tokens to inform its predictions,
            successfully generalizing even from a minimal dataset. While highly
            simplified, this approach offers valuable intuition into the
            fundamental processes of information flow and contextual
            understanding within Transformer models.
          </p>
        </section>

        <section>
          <h2>
            Acknowledgments
          </h2>
          <p>
            The Python code for the Transformer model used in this article is
            heavily based on the excellent
            <a href="https://karpathy.ai/zero-to-hero.html" target="_blank" rel="noopener noreferrer">&#34;Neural Networks: Zero to Hero&#34;</a>
            series by Andrej Karpathy. His clear explanations and step-by-step
            coding approach were invaluable.
          </p>
        </section>

        <section>
          <h2>Links</h2>
          <p>
            Dataset and source code are available on Github:
            <a href="https://github.com/rti/gptvis">https://github.com/rti/gptvis</a>.
          </p>
        </section>
      </article></div>
  </body>
</html>
