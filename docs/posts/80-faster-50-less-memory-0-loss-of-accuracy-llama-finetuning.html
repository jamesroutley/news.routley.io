<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/unslothai/unsloth">Original</a>
    <h1>Show HN: 80% faster, 50% less memory, 0% loss of accuracy Llama finetuning</h1>
    
    <div id="readability-page-1" class="page"><p dir="auto">Unsloth currently only supports Linux distros and Pytorch &gt;= 2.1.</p><div data-snippet-clipboard-copy-content="conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \
  -c pytorch -c nvidia -c xformers -c conda-forge -y
pip install &#34;unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git&#34;"><pre><code>conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \
  -c pytorch -c nvidia -c xformers -c conda-forge -y
pip install &#34;unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git&#34;
</code></pre></div><div data-snippet-clipboard-copy-content="import torch; torch.version.cuda"><pre><code>import torch; torch.version.cuda
</code></pre></div><div data-snippet-clipboard-copy-content="pip install &#34;unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git&#34;
pip install &#34;unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git&#34;"><pre><code>pip install &#34;unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git&#34;
pip install &#34;unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git&#34;
</code></pre></div><div data-snippet-clipboard-copy-content="pip install --upgrade --force-reinstall --no-cache-dir torch triton \
  --index-url https://download.pytorch.org/whl/cu121"><pre><code>pip install --upgrade --force-reinstall --no-cache-dir torch triton \
  --index-url https://download.pytorch.org/whl/cu121
</code></pre></div><p dir="auto">Change <code>cu121</code> to <code>cu118</code> for CUDA version 11.8 or 12.1. Go to <a href="https://pytorch.org/" rel="nofollow">https://pytorch.org/</a> to learn more.</p><div data-snippet-clipboard-copy-content="from unsloth import FastLlamaModel
import torch
max_seq_length = 2048 # Can change to any number &lt;= 4096
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# Load Llama model
model, tokenizer = FastLlamaModel.from_pretrained(
    model_name = &#34;unsloth/llama-2-7b&#34;, # Supports any llama model
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = &#34;hf_...&#34;, # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLlamaModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,
                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],
    lora_alpha = 16,
    lora_dropout = 0, # Currently only supports dropout = 0
    bias = &#34;none&#34;,    # Currently only supports bias = &#34;none&#34;
    use_gradient_checkpointing = True,
    random_state = 3407,
    max_seq_length = max_seq_length,
)

trainer = .... Use Huggingface&#39;s Trainer and dataset loading"><pre><code>from unsloth import FastLlamaModel
import torch
max_seq_length = 2048 # Can change to any number &lt;= 4096
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# Load Llama model
model, tokenizer = FastLlamaModel.from_pretrained(
    model_name = &#34;unsloth/llama-2-7b&#34;, # Supports any llama model
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = &#34;hf_...&#34;, # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLlamaModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,
                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],
    lora_alpha = 16,
    lora_dropout = 0, # Currently only supports dropout = 0
    bias = &#34;none&#34;,    # Currently only supports bias = &#34;none&#34;
    use_gradient_checkpointing = True,
    random_state = 3407,
    max_seq_length = max_seq_length,
)

trainer = .... Use Huggingface&#39;s Trainer and dataset loading
</code></pre></div><div data-snippet-clipboard-copy-content="!ldconfig /usr/lib64-nvidia"><pre><code>!ldconfig /usr/lib64-nvidia
</code></pre></div></div>
  </body>
</html>
