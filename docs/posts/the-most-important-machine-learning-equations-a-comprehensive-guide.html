<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/">Original</a>
    <h1>The most important machine learning equations: A comprehensive guide</h1>
    
    <div id="readability-page-1" class="page"><article>
  <h2 id="motivation">Motivation</h2>
<p>Machine learning (ML) is a powerful field driven by mathematics. Whether you’re building models, optimizing algorithms, or simply trying to understand how ML works under the hood, mastering the core equations is essential. This blog post is designed to be your go-to resource, covering the most critical and “mind-breaking” ML equations—enough to grasp most of the core math behind ML. Each section includes theoretical insights, the equations themselves, and practical implementations in Python, so you can see the math in action.</p>

<p>This guide is for anyone with a basic background in math and programming who wants to deepen their understanding of ML and is inspired by this <a href="https://x.com/goyal__pramod/status/1923064911501914216">tweet from @goyal__pramod</a>. Let’s dive into the equations that power this fascinating field!</p>

<hr/>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li>
    <p><a href="#probability-and-information-theory">Probability and Information Theory</a></p>

    <ul>
      <li><a href="#bayes-theorem">Bayes Theorem</a></li>
      <li><a href="#entropy">Entropy</a></li>
      <li><a href="#joint-and-conditional-probability">Joint and Conditional Probability</a></li>
      <li><a href="#kullback-leibler-divergence-kld">Kullback-Leibler Divergence (KLD)</a></li>
      <li><a href="#cross-entropy">Cross-Entropy</a></li>
    </ul>
  </li>
  <li>
    <p><a href="#linear-algebra">Linear Algebra</a></p>

    <ul>
      <li><a href="#linear-transformation">Linear Transformation</a></li>
      <li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
      <li><a href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
    </ul>
  </li>
  <li>
    <p><a href="#optimization">Optimization</a></p>

    <ul>
      <li><a href="#gradient-descent">Gradient Descent</a></li>
      <li><a href="#backpropagation">Backpropagation</a></li>
    </ul>
  </li>
  <li>
    <p><a href="#loss-functions">Loss Functions</a></p>

    <ul>
      <li><a href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li>
      <li><a href="#cross-entropy-loss">Cross-Entropy Loss</a></li>
    </ul>
  </li>
  <li>
    <p><a href="#advanced-ml-concepts">Advanced ML Concepts</a></p>

    <ul>
      <li><a href="#diffusion-process">Diffusion Process</a></li>
      <li><a href="#convolution-operation">Convolution Operation</a></li>
      <li><a href="#softmax-function">Softmax Function</a></li>
      <li><a href="#attention-mechanism">Attention Mechanism</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#further-reading">Further Reading</a></li>
</ul>

<hr/>

<h2 id="introduction">Introduction</h2>
<p>Mathematics is the language of machine learning. From probability to linear algebra, optimization to advanced generative models, equations define how ML algorithms learn from data and make predictions. This blog post compiles the most essential equations, explains their significance, and provides practical examples using Python libraries like NumPy, scikit-learn, TensorFlow, and PyTorch. Whether you’re a beginner or an experienced practitioner, this guide will equip you with the tools to understand and apply ML math effectively.</p>

<hr/>

<h2 id="probability-and-information-theory">Probability and Information Theory</h2>
<p>Probability and information theory provide the foundation for reasoning about uncertainty and measuring differences between distributions.</p>

<h3 id="bayes-theorem">Bayes’ Theorem</h3>

<p><strong>Equation:</strong></p>

\[P(A|B) = \frac{P(B|A) P(A)}{P(B)}\]

<p><strong>Explanation:</strong> Bayes’ Theorem describes how to update the probability of a hypothesis ($A$) given new evidence ($B$). It’s a cornerstone of probabilistic reasoning and is widely used in machine learning for tasks like classification and inference.</p>

<p><strong>Practical Use:</strong> Applied in Naive Bayes classifiers, Bayesian networks, and Bayesian optimization.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>def</span> <span>bayes_theorem</span><span>(</span><span>p_d</span><span>,</span> <span>p_t_given_d</span><span>,</span> <span>p_t_given_not_d</span><span>):</span>
    <span>&#34;&#34;&#34;
    Calculate P(D|T+) using Bayes&#39; Theorem.
    
    Parameters:
    p_d: P(D), probability of having the disease
    p_t_given_d: P(T+|D), probability of testing positive given disease
    p_t_given_not_d: P(T+|D&#39;), probability of testing positive given no disease
    
    Returns:
    P(D|T+), probability of having the disease given a positive test
    &#34;&#34;&#34;</span>
    <span>p_not_d</span> <span>=</span> <span>1</span> <span>-</span> <span>p_d</span>
    <span>p_t</span> <span>=</span> <span>p_t_given_d</span> <span>*</span> <span>p_d</span> <span>+</span> <span>p_t_given_not_d</span> <span>*</span> <span>p_not_d</span>
    <span>p_d_given_t</span> <span>=</span> <span>(</span><span>p_t_given_d</span> <span>*</span> <span>p_d</span><span>)</span> <span>/</span> <span>p_t</span>
    <span>return</span> <span>p_d_given_t</span>

<span># Example usage
</span><span>p_d</span> <span>=</span> <span>0.01</span>  <span># 1% of population has the disease
</span><span>p_t_given_d</span> <span>=</span> <span>0.99</span>  <span># Test is 99% sensitive
</span><span>p_t_given_not_d</span> <span>=</span> <span>0.02</span>  <span># Test has 2% false positive rate
</span><span>result</span> <span>=</span> <span>bayes_theorem</span><span>(</span><span>p_d</span><span>,</span> <span>p_t_given_d</span><span>,</span> <span>p_t_given_not_d</span><span>)</span> 
<span>print</span><span>(</span><span>f</span><span>&#34;P(D|T+) = </span><span>{</span><span>result</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: P(D|T+) = 0.3333 
</span></code></pre></div></div>

<h3 id="entropy">Entropy</h3>

<p><strong>Equation:</strong></p>

\[H(X) = -\sum_{x \in X} P(x) \log P(x)\]

<p><strong>Explanation:</strong> Entropy measures the uncertainty or randomness in a probability distribution. It quantifies the amount of information required to describe the distribution and is fundamental in understanding concepts like information gain and decision trees.</p>

<p><strong>Practical Use:</strong> Used in decision trees, information gain calculations, and as a basis for other information-theoretic measures.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>def</span> <span>entropy</span><span>(</span><span>p</span><span>):</span>
    <span>&#34;&#34;&#34;
    Calculate entropy of a probability distribution.
    
    Parameters:
    p: Probability distribution array
    
    Returns:
    Entropy value
    &#34;&#34;&#34;</span>
    <span>return</span> <span>-</span><span>np</span><span>.</span><span>sum</span><span>(</span><span>p</span> <span>*</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>p</span><span>,</span> <span>where</span><span>=</span><span>p</span> <span>&gt;</span> <span>0</span><span>))</span>

<span># Example usage
</span><span>fair_coin</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0.5</span><span>,</span> <span>0.5</span><span>])</span>  <span># fair coin has the same probability of heads and tails
</span><span>print</span><span>(</span><span>f</span><span>&#34;Entropy of fair coin: </span><span>{</span><span>entropy</span><span>(</span><span>fair_coin</span><span>)</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: 0.6931471805599453 
</span>
<span>biased_coin</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0.9</span><span>,</span> <span>0.1</span><span>])</span>  <span># biased coin has a higher probability of heads
</span><span>print</span><span>(</span><span>f</span><span>&#34;Entropy of biased coin: </span><span>{</span><span>entropy</span><span>(</span><span>biased_coin</span><span>)</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: 0.4698716731013394 
</span></code></pre></div></div>

<h3 id="joint-and-conditional-probability">Joint and Conditional Probability</h3>

<p><strong>Equations:</strong></p>

<ul>
  <li>
    <p>Joint Probability:</p>

\[P(A, B) = P(A|B) P(B) = P(B|A) P(A)\]
  </li>
  <li>
    <p>Conditional Probability:</p>

\[P(A|B) = \frac{P(A, B)}{P(B)}\]
  </li>
</ul>

<p><strong>Explanation:</strong> Joint probability describes the likelihood of two events occurring together, while conditional probability measures the probability of one event given another. These are the building blocks of Bayesian methods and probabilistic models.</p>

<p><strong>Practical Use:</strong> Used in Naive Bayes classifiers and probabilistic graphical models.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>from</span> <span>sklearn.naive_bayes</span> <span>import</span> <span>GaussianNB</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>X</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span> <span>2</span><span>],</span> <span>[</span><span>2</span><span>,</span> <span>3</span><span>],</span> <span>[</span><span>3</span><span>,</span> <span>4</span><span>],</span> <span>[</span><span>4</span><span>,</span> <span>5</span><span>]])</span>
<span>y</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>])</span>
<span>model</span> <span>=</span> <span>GaussianNB</span><span>().</span><span>fit</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>)</span>
<span>print</span><span>(</span><span>model</span><span>.</span><span>predict</span><span>([[</span><span>2.5</span><span>,</span> <span>3.5</span><span>]]))</span>  <span># Output: [1]
</span></code></pre></div></div>

<h3 id="kullback-leibler-divergence-kld">Kullback-Leibler Divergence (KLD)</h3>

<p><strong>Equation:</strong></p>

\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]

<p><strong>Explanation:</strong> KLD measures how much one probability distribution $P$ diverges from another $Q$. It’s asymmetric and foundational in information theory and generative models.</p>

<p><strong>Practical Use:</strong> Used in variational autoencoders (VAEs) and model evaluation.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>P</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0.7</span><span>,</span> <span>0.3</span><span>])</span>
<span>Q</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0.5</span><span>,</span> <span>0.5</span><span>])</span>
<span>kl_div</span> <span>=</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>P</span> <span>*</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>P</span> <span>/</span> <span>Q</span><span>))</span>
<span>print</span><span>(</span><span>f</span><span>&#34;KL Divergence: </span><span>{</span><span>kl_div</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: 0.08228287850505156
</span></code></pre></div></div>

<h3 id="cross-entropy">Cross-Entropy</h3>

<p><strong>Equation:</strong></p>

\[H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log Q(x)\]

<p><strong>Explanation:</strong> Cross-entropy quantifies the difference between the true distribution $P$ and the predicted distribution $Q$. It’s a widely used loss function in classification.</p>

<p><strong>Practical Use:</strong> Drives training in logistic regression and neural networks.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>y_true</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>])</span>
<span>y_pred</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0.9</span><span>,</span> <span>0.1</span><span>,</span> <span>0.8</span><span>])</span>
<span>cross_entropy</span> <span>=</span> <span>-</span><span>np</span><span>.</span><span>mean</span><span>(</span><span>y_true</span> <span>*</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>y_pred</span><span>)</span> <span>+</span> <span>(</span><span>1</span> <span>-</span> <span>y_true</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>1</span> <span>-</span> <span>y_pred</span><span>))</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Cross-Entropy: </span><span>{</span><span>cross_entropy</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: 0.164252033486018
</span></code></pre></div></div>

<hr/>

<h2 id="linear-algebra">Linear Algebra</h2>
<p>Linear algebra powers the transformations and structures in ML models.</p>

<h3 id="linear-transformation">Linear Transformation</h3>

<p><strong>Equation:</strong></p>

\[y = Ax + b \quad \text{where } A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, y \in \mathbb{R}^m, b \in \mathbb{R}^m\]

<p><strong>Explanation:</strong> This equation represents a linear mapping of input $x$ to output $y$ via matrix $A$ and bias $b$. It’s the core operation in neural network layers.</p>

<p><strong>Practical Use:</strong> Foundational for linear regression and neural networks.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>2</span><span>,</span> <span>1</span><span>],</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>]])</span>
<span>x</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>2</span><span>])</span>
<span>b</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0</span><span>,</span> <span>1</span><span>])</span>
<span>y</span> <span>=</span> <span>A</span> <span>@</span> <span>x</span> <span>+</span> <span>b</span>
<span>print</span><span>(</span><span>y</span><span>)</span>  <span># Output: [4 7]
</span></code></pre></div></div>

<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>

<p><strong>Equation:</strong></p>

\[Av = \lambda v \quad \text{where } \lambda \in \mathbb{R}, v \in \mathbb{R}^n, v \neq 0\]

<p><strong>Explanation:</strong> Eigenvalues $\lambda$ and eigenvectors $v$ describe how a matrix $A$ scales and rotates space, crucial for understanding data variance.</p>

<p><strong>Practical Use:</strong> Used in Principal Component Analysis (PCA).</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>4</span><span>,</span> <span>2</span><span>],</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>]])</span>
<span>eigenvalues</span><span>,</span> <span>eigenvectors</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>eig</span><span>(</span><span>A</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Eigenvalues: </span><span>{</span><span>eigenvalues</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Eigenvectors:</span><span>\n</span><span>{</span><span>eigenvectors</span><span>}</span><span>&#34;</span><span>)</span>
</code></pre></div></div>

<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3>

<p><strong>Equation:</strong></p>

\[A = U \Sigma V^T\]

<p><strong>Explanation:</strong> SVD breaks down a matrix $A$ into orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ of singular values. It reveals the intrinsic structure of data.</p>

<p><strong>Practical Use:</strong> Applied in dimensionality reduction and recommendation systems.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span> <span>2</span><span>],</span> <span>[</span><span>3</span><span>,</span> <span>4</span><span>],</span> <span>[</span><span>5</span><span>,</span> <span>6</span><span>]])</span>
<span>U</span><span>,</span> <span>S</span><span>,</span> <span>Vt</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>svd</span><span>(</span><span>A</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;U:</span><span>\n</span><span>{</span><span>U</span><span>}</span><span>\n</span><span>S: </span><span>{</span><span>S</span><span>}</span><span>\n</span><span>Vt:</span><span>\n</span><span>{</span><span>Vt</span><span>}</span><span>&#34;</span><span>)</span>
</code></pre></div></div>

<hr/>

<h2 id="optimization">Optimization</h2>
<p>Optimization is how ML models learn from data.</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p><strong>Equation:</strong></p>

\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta)\]

<p><strong>Explanation:</strong> Gradient descent updates parameters $\theta$ by moving opposite to the gradient of the loss function $L$, scaled by learning rate $\eta$.</p>

<p><strong>Practical Use:</strong> The backbone of training most ML models.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>def</span> <span>gradient_descent</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>,</span> <span>lr</span><span>=</span><span>0.01</span><span>,</span> <span>epochs</span><span>=</span><span>1000</span><span>):</span>
    <span>m</span><span>,</span> <span>n</span> <span>=</span> <span>X</span><span>.</span><span>shape</span>
    <span>theta</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>n</span><span>)</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>gradient</span> <span>=</span> <span>(</span><span>1</span><span>/</span><span>m</span><span>)</span> <span>*</span> <span>X</span><span>.</span><span>T</span> <span>@</span> <span>(</span><span>X</span> <span>@</span> <span>theta</span> <span>-</span> <span>y</span><span>)</span>
        <span>theta</span> <span>-=</span> <span>lr</span> <span>*</span> <span>gradient</span>
    <span>return</span> <span>theta</span>

<span>X</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>],</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>]])</span>
<span>y</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
<span>theta</span> <span>=</span> <span>gradient_descent</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>)</span>
<span>print</span><span>(</span><span>theta</span><span>)</span>  <span># Output: ~[0., 1.]
</span></code></pre></div></div>

<h3 id="backpropagation">Backpropagation</h3>

<p><strong>Equation:</strong></p>

\[\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}\]

<p><strong>Explanation:</strong> Backpropagation applies the chain rule to compute gradients of the loss $L$ with respect to weights $w_{ij}$ in neural networks.</p>

<p><strong>Practical Use:</strong> Enables efficient training of deep networks.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>

<span>model</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>2</span><span>,</span> <span>1</span><span>),</span> <span>nn</span><span>.</span><span>Sigmoid</span><span>())</span>
<span>loss_fn</span> <span>=</span> <span>nn</span><span>.</span><span>MSELoss</span><span>()</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>SGD</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>0.01</span><span>)</span>

<span>X</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>0.</span><span>,</span> <span>0.</span><span>],</span> <span>[</span><span>1.</span><span>,</span> <span>1.</span><span>]],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>float32</span><span>)</span>
<span>y</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>0.</span><span>],</span> <span>[</span><span>1.</span><span>]],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>float32</span><span>)</span>

<span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
<span>output</span> <span>=</span> <span>model</span><span>(</span><span>X</span><span>)</span>
<span>loss</span> <span>=</span> <span>loss_fn</span><span>(</span><span>output</span><span>,</span> <span>y</span><span>)</span>
<span>loss</span><span>.</span><span>backward</span><span>()</span>
<span>optimizer</span><span>.</span><span>step</span><span>()</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Loss: </span><span>{</span><span>loss</span><span>.</span><span>item</span><span>()</span><span>}</span><span>&#34;</span><span>)</span>
</code></pre></div></div>

<hr/>

<h2 id="loss-functions">Loss Functions</h2>
<p>Loss functions measure model performance and guide optimization.</p>

<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3>

<p><strong>Equation:</strong></p>

\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

<p><strong>Explanation:</strong> MSE calculates the average squared difference between true $y_i$ and predicted $\hat{y}_i$ values, penalizing larger errors more heavily.</p>

<p><strong>Practical Use:</strong> Common in regression tasks.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>y_true</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
<span>y_pred</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.1</span><span>,</span> <span>1.9</span><span>,</span> <span>3.2</span><span>])</span>
<span>mse</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>((</span><span>y_true</span> <span>-</span> <span>y_pred</span><span>)</span><span>**</span><span>2</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;MSE: </span><span>{</span><span>mse</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: 0.01
</span></code></pre></div></div>

<h3 id="cross-entropy-loss">Cross-Entropy Loss</h3>

<p>(See <a href="#cross-entropy">Cross-Entropy</a> above for details.)</p>

<hr/>

<h2 id="advanced-ml-concepts">Advanced ML Concepts</h2>
<p>These equations power cutting-edge ML techniques.</p>

<h3 id="diffusion-process">Diffusion Process</h3>

<p><strong>Equation:</strong></p>

\[x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, I)\]

<p><strong>Explanation:</strong> This describes a forward diffusion process where data $x_0$ is gradually noised over time $t$, a key idea in diffusion models.</p>

<p><strong>Practical Use:</strong> Used in generative AI like image synthesis.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>torch</span>

<span>x_0</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>1.0</span><span>])</span>
<span>alpha_t</span> <span>=</span> <span>0.9</span>
<span>noise</span> <span>=</span> <span>torch</span><span>.</span><span>randn_like</span><span>(</span><span>x_0</span><span>)</span>
<span>x_t</span> <span>=</span> <span>torch</span><span>.</span><span>sqrt</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>alpha_t</span><span>))</span> <span>*</span> <span>x_0</span> <span>+</span> <span>torch</span><span>.</span><span>sqrt</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>1</span> <span>-</span> <span>alpha_t</span><span>))</span> <span>*</span> <span>noise</span>
<span>print</span><span>(</span><span>f</span><span>&#34;x_t: </span><span>{</span><span>x_t</span><span>}</span><span>&#34;</span><span>)</span>
</code></pre></div></div>

<hr/>

<h3 id="convolution-operation">Convolution Operation</h3>

<p><strong>Equation:</strong></p>

\[(f * g)(t) = \int f(\tau) g(t - \tau) \, d\tau\]

<p><strong>Explanation:</strong> Convolution combines two functions by sliding one over the other, extracting features in data like images.</p>

<p><strong>Practical Use:</strong> Core to convolutional neural networks (CNNs).</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>

<span>conv</span> <span>=</span> <span>nn</span><span>.</span><span>Conv2d</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>kernel_size</span><span>=</span><span>3</span><span>)</span>
<span>image</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>)</span>
<span>output</span> <span>=</span> <span>conv</span><span>(</span><span>image</span><span>)</span>
<span>print</span><span>(</span><span>output</span><span>.</span><span>shape</span><span>)</span>  <span># Output: torch.Size([1, 1, 26, 26])
</span></code></pre></div></div>

<hr/>

<h3 id="softmax-function">Softmax Function</h3>

<p><strong>Equation:</strong></p>

\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

<p><strong>Explanation:</strong> Softmax converts raw scores $z_i$ into probabilities, summing to 1, ideal for multi-class classification.</p>

<p><strong>Practical Use:</strong> Used in neural network outputs.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>z</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.0</span><span>,</span> <span>2.0</span><span>,</span> <span>3.0</span><span>])</span>
<span>softmax</span> <span>=</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>z</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>np</span><span>.</span><span>exp</span><span>(</span><span>z</span><span>))</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Softmax: </span><span>{</span><span>softmax</span><span>}</span><span>&#34;</span><span>)</span>  <span># Output: [0.09003057 0.24472847 0.66524096]
</span></code></pre></div></div>

<hr/>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p><strong>Equation:</strong></p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V\]

<p><strong>Explanation:</strong> Attention computes a weighted sum of values $V$ based on the similarity between queries $Q$ and keys $K$, scaled by $\sqrt{d_k}$.</p>

<p><strong>Practical Use:</strong> Powers transformers in NLP and beyond.</p>

<p><strong>Implementation:</strong></p>

<div><div><pre><code><span>import</span> <span>torch</span>

<span>def</span> <span>attention</span><span>(</span><span>Q</span><span>,</span> <span>K</span><span>,</span> <span>V</span><span>):</span>
    <span>d_k</span> <span>=</span> <span>Q</span><span>.</span><span>size</span><span>(</span><span>-</span><span>1</span><span>)</span>
    <span>scores</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>Q</span><span>,</span> <span>K</span><span>.</span><span>transpose</span><span>(</span><span>-</span><span>2</span><span>,</span> <span>-</span><span>1</span><span>))</span> <span>/</span> <span>torch</span><span>.</span><span>sqrt</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>d_k</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>float32</span><span>))</span>
    <span>attn</span> <span>=</span> <span>torch</span><span>.</span><span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
    <span>return</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>attn</span><span>,</span> <span>V</span><span>)</span>

<span>Q</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>1.</span><span>,</span> <span>0.</span><span>],</span> <span>[</span><span>0.</span><span>,</span> <span>1.</span><span>]])</span>
<span>K</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>1.</span><span>,</span> <span>1.</span><span>],</span> <span>[</span><span>1.</span><span>,</span> <span>0.</span><span>]])</span>
<span>V</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>0.</span><span>,</span> <span>1.</span><span>],</span> <span>[</span><span>1.</span><span>,</span> <span>0.</span><span>]])</span>
<span>output</span> <span>=</span> <span>attention</span><span>(</span><span>Q</span><span>,</span> <span>K</span><span>,</span> <span>V</span><span>)</span>
<span>print</span><span>(</span><span>output</span><span>)</span>
</code></pre></div></div>

<hr/>

<h2 id="conclusion">Conclusion</h2>

<p>This blog post has explored the most critical equations in machine learning, from foundational probability and linear algebra to advanced concepts like diffusion and attention. With theoretical explanations, practical implementations, and visualizations, you now have a comprehensive resource to understand and apply ML math. Point anyone asking about core ML math here—they’ll learn 95% of what they need in one place!</p>

<hr/>

<h2 id="further-reading">Further Reading</h2>
<ul>
  <li><em>Pattern Recognition and Machine Learning</em> by Christopher Bishop</li>
  <li><em>Deep Learning</em> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
  <li><a href="https://cs229.stanford.edu/">Stanford CS229: Machine Learning</a></li>
  <li><a href="https://pytorch.org/tutorials/">PyTorch Tutorials</a></li>
</ul>

  </article></div>
  </body>
</html>
