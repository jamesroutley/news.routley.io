<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NVIDIA/nvmath-python">Original</a>
    <h1>Nvmath-Python: Nvidia Math Libraries for the Python Ecosystem</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">nvmath-python brings the power of the NVIDIA math libraries to the Python ecosystem. The
package aims to provide intuitive pythonic APIs that provide users full access to all the
features offered by NVIDIA&#39;s libraries in a variety of execution spaces. nvmath-python works
seamlessly with existing Python array/tensor frameworks and focuses on providing
functionality that is missing from those frameworks.</p>

<p dir="auto">Using the nvmath-python API allows access to all parameters of the underlying NVIDIA
cuBLASLt library. Some of these parameters are unavailable in other wrappings of NVIDIA&#39;s
C-API libraries.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import cupy as cp
import nvmath

# Prepare sample input data. nvmath-python accepts input tensors from pytorch, cupy, and
# numpy.
m, n, k = 123, 456, 789
a = cp.random.rand(m, k).astype(cp.float32)
b = cp.random.rand(k, n).astype(cp.float32)
bias = cp.random.rand(m, 1).astype(cp.float32)

# Use the stateful Matmul object in order to perform multiple matrix multiplications
# without replanning. The nvmath API allows us to fine-tune our operations by, for
# example, selecting a mixed-precision compute type.
mm = nvmath.linalg.advanced.Matmul(
    a,
    b,
    options={
        &#34;compute_type&#34;: nvmath.linalg.advanced.MatmulComputeType.COMPUTE_32F_FAST_16F
    },
)

# Plan the matrix multiplication. Planning returns a sequence of algorithms that can be
# configured. We can also select epilog operations which are applied to the result of
# the multiplication without a separate function call.
mm.plan(
    epilog=nvmath.linalg.advanced.MatmulEpilog.BIAS,
    epilog_inputs={&#34;bias&#34;: bias},
)

# Execute the matrix multiplication.
result = mm.execute()

# Remember to free the Matmul object when finished or use it as a context manager
mm.free()

# Synchronize the default stream, since by default the execution is non-blocking for
# GPU operands.
cp.cuda.get_current_stream().synchronize()
print(f&#34;Input types = {type(a), type(b)}, device = {a.device, b.device}&#34;)
print(f&#34;Result type = {type(result)}, device = {result.device}&#34;)"><pre><span>import</span> <span>cupy</span> <span>as</span> <span>cp</span>
<span>import</span> <span>nvmath</span>

<span># Prepare sample input data. nvmath-python accepts input tensors from pytorch, cupy, and</span>
<span># numpy.</span>
<span>m</span>, <span>n</span>, <span>k</span> <span>=</span> <span>123</span>, <span>456</span>, <span>789</span>
<span>a</span> <span>=</span> <span>cp</span>.<span>random</span>.<span>rand</span>(<span>m</span>, <span>k</span>).<span>astype</span>(<span>cp</span>.<span>float32</span>)
<span>b</span> <span>=</span> <span>cp</span>.<span>random</span>.<span>rand</span>(<span>k</span>, <span>n</span>).<span>astype</span>(<span>cp</span>.<span>float32</span>)
<span>bias</span> <span>=</span> <span>cp</span>.<span>random</span>.<span>rand</span>(<span>m</span>, <span>1</span>).<span>astype</span>(<span>cp</span>.<span>float32</span>)

<span># Use the stateful Matmul object in order to perform multiple matrix multiplications</span>
<span># without replanning. The nvmath API allows us to fine-tune our operations by, for</span>
<span># example, selecting a mixed-precision compute type.</span>
<span>mm</span> <span>=</span> <span>nvmath</span>.<span>linalg</span>.<span>advanced</span>.<span>Matmul</span>(
    <span>a</span>,
    <span>b</span>,
    <span>options</span><span>=</span>{
        <span>&#34;compute_type&#34;</span>: <span>nvmath</span>.<span>linalg</span>.<span>advanced</span>.<span>MatmulComputeType</span>.<span>COMPUTE_32F_FAST_16F</span>
    },
)

<span># Plan the matrix multiplication. Planning returns a sequence of algorithms that can be</span>
<span># configured. We can also select epilog operations which are applied to the result of</span>
<span># the multiplication without a separate function call.</span>
<span>mm</span>.<span>plan</span>(
    <span>epilog</span><span>=</span><span>nvmath</span>.<span>linalg</span>.<span>advanced</span>.<span>MatmulEpilog</span>.<span>BIAS</span>,
    <span>epilog_inputs</span><span>=</span>{<span>&#34;bias&#34;</span>: <span>bias</span>},
)

<span># Execute the matrix multiplication.</span>
<span>result</span> <span>=</span> <span>mm</span>.<span>execute</span>()

<span># Remember to free the Matmul object when finished or use it as a context manager</span>
<span>mm</span>.<span>free</span>()

<span># Synchronize the default stream, since by default the execution is non-blocking for</span>
<span># GPU operands.</span>
<span>cp</span>.<span>cuda</span>.<span>get_current_stream</span>().<span>synchronize</span>()
<span>print</span>(<span>f&#34;Input types = <span><span>{</span><span>type</span>(<span>a</span>), <span>type</span>(<span>b</span>)<span>}</span></span>, device = <span><span>{</span><span>a</span>.<span>device</span>, <span>b</span>.<span>device</span><span>}</span></span>&#34;</span>)
<span>print</span>(<span>f&#34;Result type = <span><span>{</span><span>type</span>(<span>result</span>)<span>}</span></span>, device = <span><span>{</span><span>result</span>.<span>device</span><span>}</span></span>&#34;</span>)</pre></div>
<p dir="auto">nvmath-python exposes NVIDIA&#39;s device-side (Dx) APIs. This allows developers to call NVIDIA
library functions inside their custom device kernels. For example, a numba jit function can
call cuFFT in order to implement FFT-based convolution.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import numpy as np
from numba import cuda
from nvmath.device import fft

def random_complex(shape, real_dtype):
    return (
        np.random.randn(*shape).astype(real_dtype)
        + 1.j * np.random.randn(*shape).astype(real_dtype)
    )

def main():

    size = 128
    ffts_per_block = 1
    batch_size = 1

    # Instantiate device-side functions from cuFFTDx.
    FFT_fwd = fft(
        fft_type=&#34;c2c&#34;,
        size=size,
        precision=np.float32,
        direction=&#34;forward&#34;,
        ffts_per_block=ffts_per_block,
        elements_per_thread=2,
        execution=&#34;Block&#34;,
        compiler=&#34;numba&#34;,
    )
    FFT_inv = fft(
        fft_type=&#34;c2c&#34;,
        size=size,
        precision=np.float32,
        direction=&#34;inverse&#34;,
        ffts_per_block=ffts_per_block,
        elements_per_thread=2,
        execution=&#34;Block&#34;,
        compiler=&#34;numba&#34;,
    )

    value_type          = FFT_fwd.value_type
    storage_size        = FFT_fwd.storage_size
    shared_memory_size  = FFT_fwd.shared_memory_size
    fft_stride          = FFT_fwd.stride
    ept                 = FFT_fwd.elements_per_thread
    block_dim           = FFT_fwd.block_dim

    # Define a numba jit function targeting CUDA devices
    @cuda.jit(link=FFT_fwd.files + FFT_inv.files)
    def f(signal, filter):

        thread_data = cuda.local.array(shape=(storage_size,), dtype=value_type)
        shared_mem = cuda.shared.array(shape=(0,), dtype=value_type)

        fft_id = (cuda.blockIdx.x * ffts_per_block) + cuda.threadIdx.y
        if(fft_id &gt;= batch_size):
            return
        offset = cuda.threadIdx.x

        for i in range(ept):
            thread_data[i] = signal[fft_id, offset + i * fft_stride]

        # Call the cuFFTDx FFT function from *inside* your custom function
        FFT_fwd(thread_data, shared_mem)

        for i in range(ept):
            thread_data[i] = thread_data[i] * filter[fft_id, offset + i * fft_stride]

        FFT_inv(thread_data, shared_mem)

        for i in range(ept):
            signal[fft_id, offset + i * fft_stride] = thread_data[i]


    data = random_complex((ffts_per_block, size), np.float32)
    filter = random_complex((ffts_per_block, size), np.float32)

    data_d = cuda.to_device(data)
    filter_d = cuda.to_device(filter)

    f[1, block_dim, 0, shared_memory_size](data_d, filter_d)
    cuda.synchronize()

    data_test = data_d.copy_to_host()
    data_ref = np.fft.ifft(np.fft.fft(data, axis=-1) * filter, axis=-1) * size

    error = np.linalg.norm(data_test - data_ref) / np.linalg.norm(data_ref)
    print(f&#34;L2 error {error}&#34;)

    assert error &lt; 1e-5

if __name__ == &#34;__main__&#34;:
    main()"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>numba</span> <span>import</span> <span>cuda</span>
<span>from</span> <span>nvmath</span>.<span>device</span> <span>import</span> <span>fft</span>

<span>def</span> <span>random_complex</span>(<span>shape</span>, <span>real_dtype</span>):
    <span>return</span> (
        <span>np</span>.<span>random</span>.<span>randn</span>(<span>*</span><span>shape</span>).<span>astype</span>(<span>real_dtype</span>)
        <span>+</span> <span>1.j</span> <span>*</span> <span>np</span>.<span>random</span>.<span>randn</span>(<span>*</span><span>shape</span>).<span>astype</span>(<span>real_dtype</span>)
    )

<span>def</span> <span>main</span>():

    <span>size</span> <span>=</span> <span>128</span>
    <span>ffts_per_block</span> <span>=</span> <span>1</span>
    <span>batch_size</span> <span>=</span> <span>1</span>

    <span># Instantiate device-side functions from cuFFTDx.</span>
    <span>FFT_fwd</span> <span>=</span> <span>fft</span>(
        <span>fft_type</span><span>=</span><span>&#34;c2c&#34;</span>,
        <span>size</span><span>=</span><span>size</span>,
        <span>precision</span><span>=</span><span>np</span>.<span>float32</span>,
        <span>direction</span><span>=</span><span>&#34;forward&#34;</span>,
        <span>ffts_per_block</span><span>=</span><span>ffts_per_block</span>,
        <span>elements_per_thread</span><span>=</span><span>2</span>,
        <span>execution</span><span>=</span><span>&#34;Block&#34;</span>,
        <span>compiler</span><span>=</span><span>&#34;numba&#34;</span>,
    )
    <span>FFT_inv</span> <span>=</span> <span>fft</span>(
        <span>fft_type</span><span>=</span><span>&#34;c2c&#34;</span>,
        <span>size</span><span>=</span><span>size</span>,
        <span>precision</span><span>=</span><span>np</span>.<span>float32</span>,
        <span>direction</span><span>=</span><span>&#34;inverse&#34;</span>,
        <span>ffts_per_block</span><span>=</span><span>ffts_per_block</span>,
        <span>elements_per_thread</span><span>=</span><span>2</span>,
        <span>execution</span><span>=</span><span>&#34;Block&#34;</span>,
        <span>compiler</span><span>=</span><span>&#34;numba&#34;</span>,
    )

    <span>value_type</span>          <span>=</span> <span>FFT_fwd</span>.<span>value_type</span>
    <span>storage_size</span>        <span>=</span> <span>FFT_fwd</span>.<span>storage_size</span>
    <span>shared_memory_size</span>  <span>=</span> <span>FFT_fwd</span>.<span>shared_memory_size</span>
    <span>fft_stride</span>          <span>=</span> <span>FFT_fwd</span>.<span>stride</span>
    <span>ept</span>                 <span>=</span> <span>FFT_fwd</span>.<span>elements_per_thread</span>
    <span>block_dim</span>           <span>=</span> <span>FFT_fwd</span>.<span>block_dim</span>

    <span># Define a numba jit function targeting CUDA devices</span>
    <span>@<span>cuda</span>.<span>jit</span>(<span>link</span><span>=</span><span>FFT_fwd</span>.<span>files</span> <span>+</span> <span>FFT_inv</span>.<span>files</span>)</span>
    <span>def</span> <span>f</span>(<span>signal</span>, <span>filter</span>):

        <span>thread_data</span> <span>=</span> <span>cuda</span>.<span>local</span>.<span>array</span>(<span>shape</span><span>=</span>(<span>storage_size</span>,), <span>dtype</span><span>=</span><span>value_type</span>)
        <span>shared_mem</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>shape</span><span>=</span>(<span>0</span>,), <span>dtype</span><span>=</span><span>value_type</span>)

        <span>fft_id</span> <span>=</span> (<span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>ffts_per_block</span>) <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>y</span>
        <span>if</span>(<span>fft_id</span> <span>&gt;=</span> <span>batch_size</span>):
            <span>return</span>
        <span>offset</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>

        <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>ept</span>):
            <span>thread_data</span>[<span>i</span>] <span>=</span> <span>signal</span>[<span>fft_id</span>, <span>offset</span> <span>+</span> <span>i</span> <span>*</span> <span>fft_stride</span>]

        <span># Call the cuFFTDx FFT function from *inside* your custom function</span>
        <span>FFT_fwd</span>(<span>thread_data</span>, <span>shared_mem</span>)

        <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>ept</span>):
            <span>thread_data</span>[<span>i</span>] <span>=</span> <span>thread_data</span>[<span>i</span>] <span>*</span> <span>filter</span>[<span>fft_id</span>, <span>offset</span> <span>+</span> <span>i</span> <span>*</span> <span>fft_stride</span>]

        <span>FFT_inv</span>(<span>thread_data</span>, <span>shared_mem</span>)

        <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>ept</span>):
            <span>signal</span>[<span>fft_id</span>, <span>offset</span> <span>+</span> <span>i</span> <span>*</span> <span>fft_stride</span>] <span>=</span> <span>thread_data</span>[<span>i</span>]


    <span>data</span> <span>=</span> <span>random_complex</span>((<span>ffts_per_block</span>, <span>size</span>), <span>np</span>.<span>float32</span>)
    <span>filter</span> <span>=</span> <span>random_complex</span>((<span>ffts_per_block</span>, <span>size</span>), <span>np</span>.<span>float32</span>)

    <span>data_d</span> <span>=</span> <span>cuda</span>.<span>to_device</span>(<span>data</span>)
    <span>filter_d</span> <span>=</span> <span>cuda</span>.<span>to_device</span>(<span>filter</span>)

    <span>f</span>[<span>1</span>, <span>block_dim</span>, <span>0</span>, <span>shared_memory_size</span>](<span>data_d</span>, <span>filter_d</span>)
    <span>cuda</span>.<span>synchronize</span>()

    <span>data_test</span> <span>=</span> <span>data_d</span>.<span>copy_to_host</span>()
    <span>data_ref</span> <span>=</span> <span>np</span>.<span>fft</span>.<span>ifft</span>(<span>np</span>.<span>fft</span>.<span>fft</span>(<span>data</span>, <span>axis</span><span>=</span><span>-</span><span>1</span>) <span>*</span> <span>filter</span>, <span>axis</span><span>=</span><span>-</span><span>1</span>) <span>*</span> <span>size</span>

    <span>error</span> <span>=</span> <span>np</span>.<span>linalg</span>.<span>norm</span>(<span>data_test</span> <span>-</span> <span>data_ref</span>) <span>/</span> <span>np</span>.<span>linalg</span>.<span>norm</span>(<span>data_ref</span>)
    <span>print</span>(<span>f&#34;L2 error <span><span>{</span><span>error</span><span>}</span></span>&#34;</span>)

    <span>assert</span> <span>error</span> <span>&lt;</span> <span>1e-5</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>main</span>()</pre></div>
<p dir="auto">nvmath-python provides the ability to write custom prologs and epilogs for FFT functions as
a Python functions and compiled them LTO-IR. For example, to have unitary scaling for an
FFT, we can define an epilog which rescales the output by 1/sqrt(N).</p>
<div dir="auto" data-snippet-clipboard-copy-content="import cupy as cp
import nvmath
import math

# Create the data for the batched 1-D FFT.
B, N = 256, 1024
a = cp.random.rand(B, N, dtype=cp.float64) + 1j * cp.random.rand(B, N, dtype=cp.float64)

# Compute the normalization factor for unitary transforms
norm_factor = 1.0 / math.sqrt(N)

# Define the epilog function for the FFT.
def rescale(data_out, offset, data, user_info, unused):
    data_out[offset] = data * norm_factor

# Compile the epilog to LTO-IR.
with cp.cuda.Device():
    epilog = nvmath.fft.compile_epilog(rescale, &#34;complex128&#34;, &#34;complex128&#34;)

# Perform the forward FFT, applying the filter as a epilog...
r = nvmath.fft.fft(a, axes=[-1], epilog={&#34;ltoir&#34;: epilog})

# Finally, we can test that the fused FFT run result matches the result of separate
# calls
s = cp.fft.fftn(a, axes=[-1], norm=&#34;ortho&#34;)

assert cp.allclose(r, s)"><pre><span>import</span> <span>cupy</span> <span>as</span> <span>cp</span>
<span>import</span> <span>nvmath</span>
<span>import</span> <span>math</span>

<span># Create the data for the batched 1-D FFT.</span>
<span>B</span>, <span>N</span> <span>=</span> <span>256</span>, <span>1024</span>
<span>a</span> <span>=</span> <span>cp</span>.<span>random</span>.<span>rand</span>(<span>B</span>, <span>N</span>, <span>dtype</span><span>=</span><span>cp</span>.<span>float64</span>) <span>+</span> <span>1j</span> <span>*</span> <span>cp</span>.<span>random</span>.<span>rand</span>(<span>B</span>, <span>N</span>, <span>dtype</span><span>=</span><span>cp</span>.<span>float64</span>)

<span># Compute the normalization factor for unitary transforms</span>
<span>norm_factor</span> <span>=</span> <span>1.0</span> <span>/</span> <span>math</span>.<span>sqrt</span>(<span>N</span>)

<span># Define the epilog function for the FFT.</span>
<span>def</span> <span>rescale</span>(<span>data_out</span>, <span>offset</span>, <span>data</span>, <span>user_info</span>, <span>unused</span>):
    <span>data_out</span>[<span>offset</span>] <span>=</span> <span>data</span> <span>*</span> <span>norm_factor</span>

<span># Compile the epilog to LTO-IR.</span>
<span>with</span> <span>cp</span>.<span>cuda</span>.<span>Device</span>():
    <span>epilog</span> <span>=</span> <span>nvmath</span>.<span>fft</span>.<span>compile_epilog</span>(<span>rescale</span>, <span>&#34;complex128&#34;</span>, <span>&#34;complex128&#34;</span>)

<span># Perform the forward FFT, applying the filter as a epilog...</span>
<span>r</span> <span>=</span> <span>nvmath</span>.<span>fft</span>.<span>fft</span>(<span>a</span>, <span>axes</span><span>=</span>[<span>-</span><span>1</span>], <span>epilog</span><span>=</span>{<span>&#34;ltoir&#34;</span>: <span>epilog</span>})

<span># Finally, we can test that the fused FFT run result matches the result of separate</span>
<span># calls</span>
<span>s</span> <span>=</span> <span>cp</span>.<span>fft</span>.<span>fftn</span>(<span>a</span>, <span>axes</span><span>=</span>[<span>-</span><span>1</span>], <span>norm</span><span>=</span><span>&#34;ortho&#34;</span>)

<span>assert</span> <span>cp</span>.<span>allclose</span>(<span>r</span>, <span>s</span>)</pre></div>

<p dir="auto">All files hosted in this repository are subject to the <a href="https://github.com/NVIDIA/nvmath-python/blob/main/LICENSE">Apache 2.0</a> license.</p>

<p dir="auto">nvmath-python is in a Beta state. Beta products may not be fully functional, may contain
errors or design flaws, and may be changed at any time without notice. We appreciate your
feedback to improve and iterate on our Beta products.</p>
</article></div></div>
  </body>
</html>
