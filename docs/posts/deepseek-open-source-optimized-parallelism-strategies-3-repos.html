<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/profile-data">Original</a>
    <h1>DeepSeek Open Source Optimized Parallelism Strategies, 3 repos</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.</p>

<p dir="auto"><a href="https://github.com/deepseek-ai/profile-data/blob/main/train.json">[profile_data]</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/profile-data/blob/main/assets/train.jpg"><img src="https://github.com/deepseek-ai/profile-data/raw/main/assets/train.jpg" alt="train"/></a></p>
<p dir="auto">The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in <a href="https://github.com/deepseek-ai/dualpipe">DualPipe</a>. Each chunk contains 4 MoE (Mixture of Experts) layers.
The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.</p>


<p dir="auto"><a href="https://github.com/deepseek-ai/profile-data/blob/main/prefill.json">[profile_data]</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/profile-data/blob/main/assets/prefill.jpg"><img src="https://github.com/deepseek-ai/profile-data/raw/main/assets/prefill.jpg" alt="prefill"/></a></p>
<p dir="auto">For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.</p>

<p dir="auto"><a href="https://github.com/deepseek-ai/profile-data/blob/main/decode.json">[profile_data]</a> (Not ready, release soon)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/profile-data/blob/main/assets/decode.jpg"><img src="https://github.com/deepseek-ai/profile-data/raw/main/assets/decode.jpg" alt="decode"/></a></p>
<p dir="auto">For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to <a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a>.</p>
</article></div></div>
  </body>
</html>
