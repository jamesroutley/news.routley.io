<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/slashml/amd_inference">Original</a>
    <h1>AMD GPU Inference</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://github.com/bentoml/OpenLLM/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/57be334b2d2a3757c225808c0d6c45a33eae0710590ef1b662efcde77a1da63e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322d677265656e2e737667" alt="License: Apache-2.0" data-canonical-src="https://img.shields.io/badge/License-Apache%202-green.svg"/></a>
<a href="https://twitter.com/slash_ml" rel="nofollow"><img src="https://camo.githubusercontent.com/2c432bee872891c8591000ef640a44e5f231d233d56cfbbe8a226a5c4c228f2c/68747470733a2f2f62616467656e2e6e65742f62616467652f69636f6e2f40736c6173685f6d6c2f3030303030303f69636f6e3d74776974746572266c6162656c3d466f6c6c6f77" alt="X" data-canonical-src="https://badgen.net/badge/icon/@slash_ml/000000?icon=twitter&amp;label=Follow"/></a>
<a href="https://discord.gg/KyUYq8uX" rel="nofollow"><img src="https://camo.githubusercontent.com/664c15d4fa2ab074a49f42c215d53b98ac607c63fd9b69c1f3d78f9fb2fc8c3a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3132333435363738393031323334353637383f6c6f676f3d646973636f7264266c6162656c3d4a6f696e253230446973636f7264" alt="Community" data-canonical-src="https://img.shields.io/discord/123456789012345678?logo=discord&amp;label=Join%20Discord"/></a></p>
<p dir="auto">This project provides a Docker-based inference engine for running Large Language Models (LLMs) on AMD GPUs. It&#39;s designed to work with models from Hugging Face, with a focus on the LLaMA model family.</p>

<ul dir="auto">
<li>AMD GPU with ROCm support</li>
<li>Docker installed on your system</li>
<li>ROCm drivers installed on your host system (version 5.4.2 or compatible)</li>
</ul>

<div data-snippet-clipboard-copy-content="amd-gpu-inference/
├── src/
│   ├── __init__.py
│   ├── engine.py
│   ├── model.py
│   ├── utils.py
│   └── amd_setup.py
├── Dockerfile
├── requirements.txt
├── run_inference.py
├── run-docker-amd.sh
└── README.md"><pre><code>amd-gpu-inference/
├── src/
│   ├── __init__.py
│   ├── engine.py
│   ├── model.py
│   ├── utils.py
│   └── amd_setup.py
├── Dockerfile
├── requirements.txt
├── run_inference.py
├── run-docker-amd.sh
└── README.md
</code></pre></div>

<ol dir="auto">
<li>
<p dir="auto">Clone this repository:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/amd-gpu-inference.git
cd amd-gpu-inference"><pre><code>git clone https://github.com/yourusername/amd-gpu-inference.git
cd amd-gpu-inference
</code></pre></div>
</li>
<li>
<p dir="auto">Make the run script executable:</p>
<div data-snippet-clipboard-copy-content="chmod +x run-docker-amd.sh"><pre><code>chmod +x run-docker-amd.sh
</code></pre></div>
</li>
<li>
<p dir="auto">Run the inference engine with a specified model and prompt:</p>
<div data-snippet-clipboard-copy-content="./run-docker-amd.sh &#34;meta-llama/Llama-2-7b-chat-hf&#34; &#34;Translate the following English text to French: &#39;Hello, how are you?&#39;&#34;"><pre><code>./run-docker-amd.sh &#34;meta-llama/Llama-2-7b-chat-hf&#34; &#34;Translate the following English text to French: &#39;Hello, how are you?&#39;&#34;
</code></pre></div>
<p dir="auto">Replace <code>&#34;meta-llama/Llama-2-7b-chat-hf&#34;</code> with the Hugging Face model you want to use, and provide your own prompt.</p>
</li>
</ol>


<p dir="auto">The project includes an <code>Aptfile</code> that lists the necessary ROCm packages to be installed in the Docker container. This ensures that all required ROCm drivers and libraries are available for the inference engine to utilize the AMD GPU effectively.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Building the Docker Image</h3><a id="user-content-building-the-docker-image" aria-label="Permalink: Building the Docker Image" href="#building-the-docker-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>run-docker-amd.sh</code> script builds the Docker image automatically. If you want to build it manually, use:</p>
<div data-snippet-clipboard-copy-content="docker build -t amd-gpu-inference ."><pre><code>docker build -t amd-gpu-inference .
</code></pre></div>

<p dir="auto">The <code>run-docker-amd.sh</code> script handles running the container with the necessary AMD GPU flags. If you want to run it manually:</p>
<div data-snippet-clipboard-copy-content="docker run --rm -it \
    --device=/dev/kfd \
    --device=/dev/dri \
    --group-add=video \
    --cap-add=SYS_PTRACE \
    --security-opt seccomp=unconfined \
    amd-gpu-inference &#34;model_name&#34; &#34;your prompt here&#34;"><pre><code>docker run --rm -it \
    --device=/dev/kfd \
    --device=/dev/dri \
    --group-add=video \
    --cap-add=SYS_PTRACE \
    --security-opt seccomp=unconfined \
    amd-gpu-inference &#34;model_name&#34; &#34;your prompt here&#34;
</code></pre></div>
<p dir="auto">Replace <code>&#34;model_name&#34;</code> with the Hugging Face model you want to use, and <code>&#34;your prompt here&#34;</code> with your input text.</p>


<p dir="auto">You can use any model available on Hugging Face by specifying its repository name when running the container. For example:</p>
<div data-snippet-clipboard-copy-content="./run-docker-amd.sh &#34;facebook/opt-1.3b&#34; &#34;Your prompt here&#34;"><pre><code>./run-docker-amd.sh &#34;facebook/opt-1.3b&#34; &#34;Your prompt here&#34;
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Modifying the Inference Logic</h3><a id="user-content-modifying-the-inference-logic" aria-label="Permalink: Modifying the Inference Logic" href="#modifying-the-inference-logic"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you need to change how the inference is performed, modify the <code>run_inference.py</code> file. Remember to rebuild the Docker image after making changes.</p>

<ul dir="auto">
<li>Ensure that your AMD GPU drivers and ROCm are correctly installed and configured on your host system.</li>
<li>If you encounter &#34;out of memory&#34; errors, try using a smaller model or reducing the input/output length.</li>
<li>For model-specific issues, refer to the model&#39;s documentation on Hugging Face.</li>
</ul>

<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>

<ul dir="auto">
<li>This project uses the Hugging Face Transformers library.</li>
<li>ROCm is developed by AMD. Licensed under MIT License
See <a href="https://rocm.docs.amd.com/en/latest/about/license.html" rel="nofollow">https://rocm.docs.amd.com/en/latest/about/license.html</a> for details.</li>
</ul>
<p dir="auto">For any questions or issues, please open an issue in the GitHub repository.</p>
</article></div></div>
  </body>
</html>
