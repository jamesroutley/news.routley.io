<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matt.might.net/articles/hello-perceptron/">Original</a>
    <h1>Hello, Perceptron: An introduction to artificial neural networks</h1>
    
    <div id="readability-page-1" class="page"><div id="main-content">
  <h2 id="outline">Outline</h2>

<p>History makes for good pedagogy with neural networks.</p>

<p>The simplest possible artificial neural network contains just one very simple artificial
neuron – Frank Rosenblatt’s original perceptron.</p>

<p>(Rosenblatt’s perceptron is in turn based on McCulloch and Pitt’s even-more-simplified artificial neuron, but we’ll skip over that, since the perceptron permits a simple training algorithm.)</p>

<p>We’ll create an artifical neural network that consists of a single perceptron.</p>

<p>We’ll demonstrate that a single perceptron can “learn” basic logical functions such as AND, OR and NOT.</p>

<p>As a result, neural networks inherit the computational power of digital logic circuits:
suddenly, anything you can do with a logical circuit, you could also do with a neural network.</p>

<p>Once we’ve defined the perceptron, we’ll recreate the algorithm used to train it, a sort of “Hello World” exercise for machine learning.</p>

<p>This algorithm will consume examples of inputs and ouptuts to the perceptron, and it will figure out how to reconfigure the perceptron to mimic those examples.</p>

<p>The limits of this single-perceptron approach show up when trying to learn the Boolean function XOR.</p>

<p>This limitation in turn motivates the development of full-fledged artificial neural networks.</p>

<p>And, that development has three key conceptual parts:</p>

<ol>
<li> arranging multiple perceptrons in layers to improve expressiveness;</li>
<li> realizing that the simple perceptron learning algorithm is now problematic; and then</li>
<li> graduating to full artificial neurons to “simplify” learning.</li>
</ol>

<p>The full technical treatment of these developments is reserved for future
articles, but you will leave this article with a technical understanding of the
fundamental computational abstraction driving generative AI.</p>

<h2 id="moreresources">More resources</h2>

<p>If, after reading this article, you’re looking for a more comprehensive treatment, I recommend
<a href="https://amzn.to/41NQmo0">Artificial Intelligence: A Modern Approach</a>:</p>

<center>
<a href="https://www.amazon.com/Artificial-Intelligence-Approach-2-downloads-Artifical-ebook/dp/B092J75GML?keywords=artificial+intelligence+a+modern+approach+4th+edition&amp;qid=1682811034&amp;sprefix=artificial+intelligence%2Caps%2C100&amp;sr=8-1&amp;linkCode=li3&amp;tag=mmamzn06-20&amp;linkId=17751eae4299936d0bd51e110114a408&amp;language=en_US&amp;ref_=as_li_ss_il" target="_blank"><img src="https://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=B092J75GML&amp;Format=_SL250_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mmamzn06-20&amp;language=en_US"/></a><img src="https://ir-na.amazon-adsystem.com/e/ir?t=mmamzn06-20&amp;language=en_US&amp;l=li3&amp;o=1&amp;a=B092J75GML" width="1" height="1" alt=""/>
</center>

<p>This has been the default text since I was an undergraduate, yet it’s received continuous updates throughout the years, which means it covers the full breadth of classical and modern approaches to AI.</p>

<h2 id="whatsabiologicalneuron">What’s a (biological) neuron?</h2>

<p>Before we get to perceptrons and artificial neurons, it’s worth acknowledging biological neurons as their inspiration.</p>

<p>Biological neurons are cells that serve as the basic building block of information processing in the brain and in the nervous system more broadly.</p>

<p>From a computational perspective, a neuron is a transducer: a neuron transforms input signals from upstream neurons into an output signal for downstream neurons.</p>

<p>More specifically, a neuron tries to determine whether or not to activate its output signal (to “fire”) based on the upstream signals.</p>

<p>Depending on where incoming signals meet up with the neuron, some are pro-activation (excitatory) and some are anti-activation (inhibitory).</p>

<p>So, without going into too much detail:</p>

<ol>
<li> A neuron receives input signals from upstream neurons.</li>
<li> The neuron combines input pro- and anti-activation signals together.</li>
<li> When net “pro-activation” signal exceeds a threshold, the neuron “fires.”</li>
<li> Downstream neurons receive the signal, and they repeat this process.</li>
</ol>

<h2 id="whatsaperceptron">What’s a perceptron?</h2>

<p>The perceptron, first introduced by Frank Rosenblatt in 1958, is the simplest form of an artificial neuron.</p>

<p>Much like a biological neuron, a perceptron acts like a <em>computational transducer</em> combining multiple inputs to produce a single output.</p>

<p>In the context of modern machine larning, a perceptron is a <em>classifier</em>.</p>

<h3 id="whatsaclassifier">What’s a classifier?</h3>

<p>A classifier categorizes an input data point into one of several predefined classes.</p>

<p>For example, a classifier could categorize an email as <code>spam</code> or <code>not_spam</code>.</p>

<p>Or, a classifier might categorize an image as <code>dog</code>, <code>cat</code>, <code>bear</code> or <code>other</code>.</p>

<p>If there are only two categories, it’s a <em>binary</em> classifier.</p>

<p>If there are more than two categories, it’s a <em>multi-class</em> classifier.</p>

<p>A single perceptron by itself is a binary classifier, and the raw output of a perceptron is 0 or 1.</p>

<p>Of course, you could write a classifier by hand.</p>

<p>Here’s a hand-written classifier that takes a single number and “classifies” it as nonnegative (returning 1) or negative (returning 0):</p>

<pre><code>def is_nonnegative(n):
   if n &gt;= 0:
      return 1
   else:
      return 0
</code></pre>

<p>Machine learning often boils down to using lots of example input-output pairs to “train” these classifiers, so that they don’t have to be programmed by hand.</p>

<p>For this very simple classifier, here’s a table of inputs and ouptuts:</p>

<pre><code>Input:     5,    Classification: 1
Input:    10,    Classification: 1
Input:   2.5,    Classification: 1
Input:  0.01,    Classification: 1
Input:     0,    Classification: 1
Input:    -3,    Classification: 0
Input:  -7.8,    Classification: 0
</code></pre>

<p>Whether a given training algorithm can turn this into the “right” classifier – a close enough approximation of <code>is_nonnegative</code> – is a topic for a longer discussion.</p>

<p>But, that’s the idea – don’t code; train on data.</p>

<h3 id="whatsabinarylinearclassifier">What’s a binary linear classifier?</h3>

<p>More specifically, a perceptron can be thought of as a “binary linear classifier.”</p>

<p>The term <em>linear</em> has several related meanings in this context:</p>

<ul>
<li><p> A linear classifier is a type of classifier that makes its predictions
based on a <em>linear</em> combination of the input features.</p></li>
<li><p> And, for a linear classifier, the boundary separating the classes must be
“linear” – it must be representable by a point (in one dimension), a straight line (in two
dimensions), a plane (in three dimensions), or a hyperplane (in higher
dimensions).</p></li>
</ul>

<p>(All of this will make more sense once actual inputs are used.)</p>

<p>So, operationally, a perceptron treats an input as a vector of features (each
represented by a number) and computes a weighted sum, before applying a step
function to determine the output.</p>

<p>Because a perceptron classifies based on linear boundaries, classes that are
not “linearly separable” can’t be modeled using just one perceptron.</p>

<p>Overcoming this limitation later motivates the development of full artificial
neural networks.</p>

<p>The perceptron’s simplicity makes it an excellent starting point for
understanding the mechanics of artificial neural networks.</p>

<h2 id="theanatomyofaperceptron">The anatomy of a perceptron</h2>

<p>An individual perceptron is defined by three elements:</p>

<ol>
<li> the number of inputs it takes, <em>n</em>;</li>
<li> a list of of <em>n</em> weights, one for each input; and</li>
<li> a threshold to determine whether it should fire based on the input.</li>
</ol>

<p>The operation of a perceptron has two phases:</p>

<ol>
<li> multiplying the inputs by the weights and summing the results; and</li>
<li> checking for activation:</li>
</ol>

<ul>
<li> If the sum is greater than or equal to a threshold, the perceptron outputs 1.</li>
<li> If the sum is less than the threshold, the perceptron outputs 0.</li>
</ul>

<p>It’s straightforward to implement this in Python:</p>

<pre><code>def perceptron(inputs, weights, threshold):
    weighted_sum = sum(x * w for x, w in zip(inputs, weights))
    return 1 if weighted_sum &gt;= threshold else 0
</code></pre>

<p>And, then, we could re-implement <code>is_nonnegative</code> as a binary linear classifier:</p>

<pre><code>def is_nonnegative(x): 
    return perceptron([x], [1], 0)
</code></pre>

<p>Using this definition, we can also get a perceptron to simulate logical NOT:</p>

<pre><code>def not_function(x):
    weight = -1
    threshold = -0.5
    return perceptron([x], [weight], threshold)

print(&#34;NOT(0):&#34;, not_function(0))  # Outputs: 1
print(&#34;NOT(1):&#34;, not_function(1))  # Outputs: 0
</code></pre>

<h2 id="learning:fromexamplestocode">Learning: From examples to code</h2>

<p>Tweaking weights by hand is an inefficient way to program perceptrons.</p>

<p>So, suppose now that instead of picking weights and thresholds by hand, we want
to find the weights and threshold that correctly classify some example
input-output data automatically.</p>

<p>That is, suppose we want to “train” a perceptron based on examples of inputs
and desired outputs.</p>

<p>In particular, let’s take a look at the truth table for AND encoded as a list of input-output pairs:</p>

<pre><code>and_data = [
 ((0, 0),  0),
 ((0, 1),  0),
 ((1, 0),  0), 
 ((1, 1),  1)
]
</code></pre>

<p>Can we “train” a perceptron to act like this function?</p>

<p>Because the input points that output <code>0</code> are linearly separable from the input
points that output <code>1</code>, yes, we can!</p>

<p>Graphically, we can draw a line that separates (0,0), (0,1) and (1,0) from (1,1):</p>

<p><img src="https://matt.might.net/images/blog/linear-separability-AND.png"/></p><p>To find such a line automatically, we’ll implement the perceptron learning algorithm.</p>

<h3 id="theperceptronlearningalgorithm">The perceptron learning algorithm</h3>

<p>The perceptron learning algorithm is an iterative process that adjusts the weights and threshold of the perceptron based on how close it’s getting to the training data.</p>

<p>Here’s a high-level overview of the perceptron learning algorithm:</p>

<ol>
<li> Initialize the weights and threshold with random values.</li>
<li> For each input-output pair in the training data:

<ul>
<li>Compute the perceptron’s output using the current weights and threshold.</li>
<li>Update the weights and threshold based on the difference between the desired output and the perceptron’s output – the error.</li>
</ul></li>
<li> Repeat steps 2 and 3 until the perceptron classifies all
 input-output pairs correctly, or a specified number of iterations have been
 completed.</li>
</ol>

<p>The update rule for the weights and threshold is simple:</p>

<ul>
<li> If the perceptron’s output is correct, do not change the weights or threshold.</li>
<li> If the perceptron’s output is too low, increase the weights and decrease the threshold.</li>
<li> If the perceptron’s output is too high, decrease the weights and increase the threshold.</li>
</ul>

<p>To update the weights and threshold, we use a <em>learning rate</em>, which is a small
positive constant that determines the step size of the updates.</p>

<p>A smaller learning rate results in smaller updates and slower convergence, while a larger
learning rate results in larger updates and potentially faster convergence, but
also the risk of overshooting the optimal values.</p>

<p>For the sake of this implementation, let’s assume that the training data comes
as a list of pairs: each pair is the input (a tuple of numbers) paired with its
desired output (0 or 1).</p>

<p>Now, let’s implement the perceptron learning algorithm in Python:</p>

<pre><code>import random

def train_perceptron(data, learning_rate=0.1, max_iter=1000):

    # max_iter is the maximum number of training cycles to attempt
    # until stopping, in case training never converges.

    # Find the number of inputs to the perceptron by looking at
    # the size of the first input tuple in the training data:
    first_pair = data[0]
    num_inputs = len(first_pair[0])

    # Initialize the vector of weights and the threshold:
    weights = [random.random() for _ in range(num_inputs)]
    threshold = random.random()
   
    # Try at most max_iter cycles of training:
    for _ in range(max_iter):

        # Track how many inputs were wrong this time:
        num_errors = 0
        
        # Loop over all the training examples:
        for inputs, desired_output in data:
            output = perceptron(inputs, weights, threshold)
            error = desired_output - output
            
            if error != 0:
                num_errors += 1
                for i in range(num_inputs):
                    weights[i] += learning_rate * error * inputs[i]
                threshold -= learning_rate * error
        
        if num_errors == 0:
            break
    
    return weights, threshold
</code></pre>

<p>Now, let’s train the perceptron on the <code>and_data</code>:</p>

<pre><code>and_weights, and_threshold = train_perceptron(and_data)

print(&#34;Weights:&#34;, and_weights)
print(&#34;Threshold:&#34;, and_threshold)
</code></pre>

<p>This will output weights and threshold values that allow the perceptron to
behave like the AND function.</p>

<p>The values may not be unique, as there could be multiple sets of
weights and threshold values that result in the same classification.</p>

<p>So, if you train the perceptron twice, you may get different results.</p>

<p>To verify that the trained perceptron works as expected, we can test it on all
possible inputs:</p>

<pre><code>print(perceptron((0,0),and_weights,and_threshold)) # prints 0
print(perceptron((0,1),and_weights,and_threshold)) # prints 0
print(perceptron((1,0),and_weights,and_threshold)) # prints 0
print(perceptron((1,1),and_weights,and_threshold)) # prints 1
</code></pre>

<h3 id="learningtheorfunction">Learning the OR Function</h3>

<p>Now that we’ve successfully trained the perceptron for the AND function, let’s do the same for the OR function. We’ll start by encoding the truth table for OR as input-output pairs:</p>

<pre><code>or_data = [
    ((0, 0), 0),
    ((0, 1), 1),
    ((1, 0), 1),
    ((1, 1), 1)
]
</code></pre>

<p>Just like with the AND function, the data points for the OR function are also linearly separable, which means that a single perceptron should be able to learn this function.</p>

<p>Let’s train the perceptron on the <code>or_data</code>:</p>

<pre><code>or_weights, or_threshold = train_perceptron(or_data)

print(&#34;Weights:&#34;, or_weights)
print(&#34;Threshold:&#34;, or_threshold)
</code></pre>

<p>This will output weights and threshold values that allow the perceptron to
behave like the OR function. As before, the values may not be unique, and there
could be multiple sets of weights and threshold values that result in the same
classification.</p>

<p>Once again, we can test it on all possible inputs:</p>

<pre><code>print(perceptron((0,0),or_weights,or_threshold)) # prints 0
print(perceptron((0,1),or_weights,or_threshold)) # prints 1
print(perceptron((1,0),or_weights,or_threshold)) # prints 1
print(perceptron((1,1),or_weights,or_threshold)) # prints 1
</code></pre>

<h2 id="limitsofasingleperceptron:xor">Limits of a single perceptron: XOR</h2>

<p>Having trained the perceptron for the AND and OR functions, let’s attempt to train it for the XOR function.</p>

<p>The XOR function returns true if exactly one of its inputs is true, and false otherwise. First, we’ll encode the truth table for XOR as input-output pairs:</p>

<pre><code>xor_data = [
    ((0, 0), 0),
    ((0, 1), 1),
    ((1, 0), 1),
    ((1, 1), 0)
]
</code></pre>

<p>Now let’s try to train the perceptron on the <code>xor_data</code>:</p>

<pre><code>xor_weights, xor_threshold = train_perceptron(xor_data, max_iter=10000)

print(&#34;Weights:&#34;, xor_weights)
print(&#34;Threshold:&#34;, xor_threshold)
</code></pre>

<p>For my run, I got:</p>

<pre><code>Weights: [-0.19425288088361953, -0.07246046028471387]
Threshold: -0.09448636811679267
</code></pre>

<p>Despite increasing the maximum number of iterations to 10,000, we will find that the perceptron is unable to learn the XOR function:</p>

<pre><code>print(perceptron((0,0),xor_weights,xor_threshold)) # prints 0
print(perceptron((0,1),xor_weights,xor_threshold)) # prints 1
print(perceptron((1,0),xor_weights,xor_threshold)) # prints 1
print(perceptron((1,1),xor_weights,xor_threshold)) # prints 1!!
</code></pre>

<p>The reason for this failure is that the XOR function is not linearly separable.</p>

<p>Visually, this means that there is no straight line that can separate the points (0,1)
and (1,0) from (0,0) and (1,1).</p>

<p>Try it yourself: draw a square, and then see if you can draw a single line that separates the upper left and lower right corners away from the other two.</p>

<p>In other words, because perceptrons are binary linear classifiers, a single
perceptron is incapable of learning the XOR function.</p>

<h2 id="fromaperceptrontofullartificialneuralnets">From a perceptron to full artificial neural nets</h2>

<p>In the previous sections, we demonstrated how a single perceptron could learn
basic Boolean functions like AND, OR and NOT.</p>

<p>However, we also showed that a single perceptron is limited when it comes to
non-linearly separable functions, like the XOR function.</p>

<p>To overcome these limitations and tackle more complex problems, researchers
developed modern artificial neural networks (ANNs).</p>

<p>In this section, we will briefly discuss the key changes to the perceptron
model and the learning algorithm that enable the transition to ANNs.</p>

<h2 id="multilayerperceptronnetworks">Multilayer Perceptron Networks</h2>

<p>The first major change is the introduction of multiple layers of perceptrons,
also known as Multilayer Perceptron (MLP) networks. MLP networks consist of an
input layer, one or more hidden layers, and an output layer.</p>

<p>Each layer contains multiple perceptrons (also referred to as
neurons or nodes). The input layer receives the input data, and the output
layer produces the final result or classification.</p>

<p>In an MLP network, the output of a neuron in one layer becomes the input for
the neurons in the next layer. The layers between the input and output layers
are called hidden layers, as they do not directly interact with the input data
or the final output.</p>

<p>By adding hidden layers, MLP networks can model more complex, non-linear
relationships between inputs and outputs, effectively overcoming the
limitations of single perceptrons.</p>

<h2 id="activationfunctions">Activation Functions</h2>

<p>While the original perceptron model used a simple step function as the
activation function, modern ANNs use different activation functions that allow
for better learning capabilities and improved modeling of complex
relationships.</p>

<p>Some popular activation functions include the sigmoid function, hyperbolic
tangent (tanh) function, and Rectified Linear Unit (ReLU) function.</p>

<p>These activation functions introduce non-linearity to the neural network, which
enables the network to learn and approximate non-linear functions.</p>

<p>In addition, they provide “differentiability” (in the sense of calculus), a
critical property for training neural networks using gradient-based
optimization algorithms.</p>

<h2 id="backpropagationandgradientdescent">Backpropagation and gradient descent</h2>

<p>The perceptron learning algorithm is insufficient for training MLP networks, as
it is a simple update rule designed for single-layer networks.</p>

<p>Instead, modern ANNs use the backpropagation algorithm in conjunction with
gradient descent or its variants for training.</p>

<p>Backpropagation is an efficient method for computing the gradient of the error
with respect to each weight in the network. The gradient indicates the
direction and magnitude of the change in the weights needed to minimize the
error.</p>

<p>Backpropagation works by calculating the error at the output layer and then
propagating the error backward through the network, updating the weights in
each layer along the way.</p>

<p>Gradient descent is an optimization algorithm that uses the computed gradients
to update the weights and biases of the network. It adjusts the weights and
biases iteratively, taking small steps in the direction of the negative
gradient, aiming to minimize the error function.</p>

<p>Variants of gradient descent, like stochastic gradient descent (SGD) and
mini-batch gradient descent, improve the convergence speed and stability of the
learning process.</p>

<h2 id="onward">Onward</h2>

<p>In short, the transition from single perceptrons to full artificial neural
networks involves three key changes:</p>

<ol>
<li> Arranging multiple perceptrons in layers to improve expressiveness and model
non-linear relationships.</li>
<li> Introducing different activation functions that
provide non-linearity and differentiability.</li>
<li> Implementing the backpropagation algorithm and gradient descent for efficient and effective learning in
multilayer networks.</li>
</ol>

<p>With these changes, ANNs become capable of learning complex, non-linear
functions and solving a wide range of problems, ultimately leading to the
development of the powerful generative AI models we see today.</p>

<p>Future articles will delve deeper into each of these topics, exploring their
theoretical foundations and practical implementations.</p>

<h2 id="furtherreading">Further reading</h2>

<p>Michael Nielsen has an <a href="http://neuralnetworksanddeeplearning.com/">excellent free online textbook on machine learning</a> that also begins with perceptrons.</p>

<p>Stephen Wolfram wrote a <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">long and highly detailed explanation of machine learning all the way up through the technical development of ChatGPT</a>.</p>

<p>The textbook <a href="https://amzn.to/41NQmo0">Artificial Intelligence: A Modern Approach</a> by Russell and Norvig cover the full breadth of AI from classical to modern approaches in great detail:</p>

<center>
<a href="https://www.amazon.com/Artificial-Intelligence-Approach-2-downloads-Artifical-ebook/dp/B092J75GML?keywords=artificial+intelligence+a+modern+approach+4th+edition&amp;qid=1682811034&amp;sprefix=artificial+intelligence%2Caps%2C100&amp;sr=8-1&amp;linkCode=li3&amp;tag=mmamzn06-20&amp;linkId=17751eae4299936d0bd51e110114a408&amp;language=en_US&amp;ref_=as_li_ss_il" target="_blank"><img src="https://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=B092J75GML&amp;Format=_SL250_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mmamzn06-20&amp;language=en_US"/></a><img src="https://ir-na.amazon-adsystem.com/e/ir?t=mmamzn06-20&amp;language=en_US&amp;l=li3&amp;o=1&amp;a=B092J75GML" width="1" height="1" alt=""/>
</center>

  </div></div>
  </body>
</html>
