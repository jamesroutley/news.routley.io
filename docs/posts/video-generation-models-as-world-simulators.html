<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.com/research/video-generation-models-as-world-simulators">Original</a>
    <h1>Video generation models as world simulators</h1>
    
    <div id="readability-page-1" class="page"><div><!----><div id="content"><!--[--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.</p><p>Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,<span><sup><span>[^1]</span></sup><!----></span><span><sup><span>[^2]</span></sup><!----></span><span><sup><span>[^3]</span></sup><!----></span> generative adversarial networks,<span><sup><span>[^4]</span></sup><!----></span><span><sup><span>[^5]</span></sup><!----></span><span><sup><span>[^6]</span></sup><!----></span><span><sup><span>[^7]</span></sup><!----></span> autoregressive transformers,<span><sup><span>[^8]</span></sup><!----></span><span><sup><span>[^9]</span></sup><!----></span> and diffusion models.<span><sup><span>[^10]</span></sup><!----></span><span><sup><span>[^11]</span></sup><!----></span><span><sup><span>[^12]</span></sup><!----></span> These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="turning-visual-data-into-patches" data-heading=""><div><div><p><h2>Turning visual data into patches</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.<span><sup><span>[^13]</span></sup><!----></span><span><sup><span>[^14]</span></sup><!----></span> The success of the LLM paradigm is enabled in part by the use of tokens<em> </em>that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual <em>patches</em>. Patches have previously been shown to be an effective representation for models of visual data.<span><sup><span>[^15]</span></sup><!----></span><span><sup><span>[^16]</span></sup><!----></span><span><sup><span>[^17]</span></sup><!----></span><span><sup><span>[^18]</span></sup><!----></span> We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><figure><p><img src="https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="2031" height="378" alt="Figure Patches" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"/></p><figcaption><!--[--><!--]--></figcaption></figure></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,<span><sup><span>[^19]</span></sup><!----></span> and subsequently decomposing the representation into spacetime patches.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="video-compression-network" data-heading=""><div><div><p><h2>Video compression network</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We train a network that reduces the dimensionality of visual data.<span><sup><span>[^20]</span></sup><!----></span> This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.</p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Given a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="scaling-transformers-for-video-generation" data-heading=""><div><div><p><h2>Scaling transformers for video generation</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora is a diffusion model<span><sup><span>[^21]</span></sup><!----></span><span><sup><span>[^22]</span></sup><!----></span><span><sup><span>[^23]</span></sup><!----></span><span><sup><span>[^24]</span></sup><!----></span><span><sup><span>[^25]</span></sup><!----></span>; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion <em>transformer</em>.<span><sup><span>[^26]</span></sup><!----></span> Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,<span><sup><span>[^13]</span></sup><!----></span><span><sup><span>[^14]</span></sup><!----></span> computer vision,<span><sup><span>[^15]</span></sup><!----></span><span><sup><span>[^16]</span></sup><!----></span><span><sup><span>[^17]</span></sup><!----></span><span><sup><span>[^18]</span></sup><!----></span> and image generation.<span><sup><span>[^27]</span></sup><!----></span><span><sup><span>[^28]</span></sup><!----></span><span><sup><span>[^29]</span></sup><!----></span><br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><figure><p><img src="https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="1261" height="312" alt="Figure Diffusion" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"/></p><figcaption><!--[--><!--]--></figcaption></figure></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="variable-durations-resolutions-aspect-ratios" data-heading=""><div><div><p><h2>Variable durations, resolutions, aspect ratios</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Past approaches to image and video generation typically resize, crop or trim videos to a standard size – e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="improved-framing-and-composition" data-heading=""><div><div><p><h3>Improved framing and composition</h3></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model  trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right)s have improved framing.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>Training text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3<span><sup><span>[^30]</span></sup><!----></span> to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.</p><p>Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraMadlib-25"><!----><!----><!----><!----><div><div><div><div layout="full-bleed"><section><div><div><div><div><div><div><div><!--[--><div><p> taking a pleasant stroll in </p></div><!--]--></div></div></div></div><!----></div></div></div></section></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="prompting-with-images-and-videos" data-heading=""><div><div><p><h2>Prompting with images and videos</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>All of the results above and in our <a href="https://sashalaundy.com/sora" rel="noopener noreferrer">landing page</a> show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2<span><sup><span>[^31]</span></sup><!----></span> and DALL·E 3<span><sup><span>[^30]</span></sup><!----></span> images.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraVideoGrid-30"><!----><!----><!----><!----><div><div><div><div layout="auto"><div><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_0.png"/></p></div><!--]--><p>A Shiba Inu dog wearing a beret and black turtleneck.</p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraVideoGrid-31"><!----><!----><!----><!----><div><div><div><div layout="auto"><div><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_2.png"/></p></div><!--]--><p>Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.</p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraVideoGrid-32"><!----><!----><!----><!----><div><div><div><div layout="auto"><div><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_4.png"/></p></div><!--]--><p>An image of a realistic cloud that spells “SORA”.</p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraVideoGrid-33"><!----><!----><!----><!----><div><div><div><div layout="auto"><div><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_6.png"/></p></div><!--]--><p>In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.</p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="extending-generated-videos" data-heading=""><div><div><p><h3>Extending generated videos</h3></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora is also capable of extending videos, either forward or backward in time. Below are four videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the four videos starts different from the others, yet all four videos lead to the same ending.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We can use this method to extend a video both forward and backward to produce a seamless infinite loop.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Diffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,<span><sup><span>[^32]</span></sup><!----></span> to Sora. This technique enables Sora to transform  the styles and environments of input videos zero-shot.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="image-generation-capabilities" data-heading=""><div><div><p><h2>Image generation capabilities</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora is also capable of generating images. We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div id="SoraVideoGrid-47"><!----><!----><!----><!----><div><div><div><div layout="auto"><div><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_0.png"/><span>Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of field</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_1.png"/><span>Vibrant coral reef teeming with colorful fish and sea creatures</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_2.png"/><span>Digital art of a young tiger under an apple tree in a matte painting style with gorgeous details</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_3.png"/><span>A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2</span></p></div><!--]--><!----></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div id="emerging-simulation-capabilities" data-heading=""><div><div><p><h2>Emerging simulation capabilities</h2></p></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>We find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.</p><p><strong>3D consistency.</strong> Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p><strong>Long-range coherence and object permanence. </strong>A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p><strong>Interacting with the world.</strong> Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p><strong>Simulating digital worlds.</strong> Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>Sora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering. Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our <a href="https://sashalaundy.com/sora" rel="noopener noreferrer">landing page</a>.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--[--><!--]--><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><p>We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.<br/></p></div></div></div></div></div><!----><!----><!----><!----><!----><!----><!----></div><!--]--><!--]--></div></div></div>
  </body>
</html>
