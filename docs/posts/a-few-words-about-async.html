<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yoric.github.io/post/quite-a-few-words-about-async/">Original</a>
    <h1>A Few Words About Async</h1>
    
    <div id="readability-page-1" class="page"><div>
            
            
            
            
            
            
            
            
            <p>I’ve had a few conversations about async code recently (and not so
recently) and seen some code that seems to make wrong assumptions
about async, so I figured out it was time to have a serious chat
about async, what it’s for, what it guarantees and what it doesn’t.</p>
<p>Most of the code in this entry will be written with Python syntax
(and often Python libraries), but with a few minor exceptions, we’ll be
discussing concepts that are valid across languages.</p>

<p>We all know about performance, right? I mean, there is some task we want
our code to do (display something on screen, or request some data from a
database, or download a file, or solve a mathematical problem, …) and
that task should be finished as fast as possible. For bonus points, it
should use as little memory as possible.</p>
<p>Right?</p>
<p>Well… not quite.</p>
<p>This kind of performance is called <em>throughput</em>. And having a high
throughput is almost always a good thing. But in the 21st century,
focusing on throughput is generally the wrong goal.</p>
<p>You see, these days, most applications<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> look something like this:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>while</span> <span>True</span>:
</span></span><span><span>    <span>while</span> (event <span>:=</span> get_next_event()):
</span></span><span><span>        on_event(event)
</span></span><span><span>    wait_until_there_is_an_event()</span></span></code></pre></div></div>
  
</div><p>Maybe you’re writing that loop yourself, but usually, not. You might
be writing <code>asyncio.run</code> or <code>#[tokio::main]</code> or <code>Eio_main.run</code>, or
just running your code in the browser or Node.js or BEAM,
or in plenty of other ways, but you’re not escaping the <em>event loop</em>.</p>
<p>If you’re writing a video game, you receive an event whenever the user presses
a key, and also whenever it’s time to repaint the screen. If you’re writing
a GUI tool, you receive an event whenever the user clicks on a button. If you’re
writing a web server, you get an event whenever you receive a connection or
some data.</p>
<p>And yes, you want to finish the task quickly. In fact, if you’re writing any
kind of user-facing application, whether it’s a text processor or a video game,
you have about 16ms to finish the task. You can do many things in 16ms. But
there are also quite a few things that you can’t do, including opening a file
<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> or getting a response from your web server <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>If you’re writing a web server, you have more leeway. In most cases, you can
afford to wait 1 second, possibly even 2. But there are plenty of tasks that
your web server may need to complete and that will take more than 2 seconds.
For instance, extracting lots of data from a busy database, or getting anything
remotely coherent from a LLM.</p>
<p>So… now what?</p>
<p>Now we need to redefine performance. And in fact, there are plenty of definitions
of performance. The one we’re going to focus on in this discussion <em>latency</em>:
how long until <em>something</em> happens. You may not have finished opening your file,
getting a response from your web server, or received anything that looks remotely
coherent from Claude, but you need to respond <em>something</em> quickly.</p>
<p>Also, if you’re interested in the notion of Response Time Limits, it dates
back to 1993: <a href="https://www.nngroup.com/articles/response-times-3-important-limits/">https://www.nngroup.com/articles/response-times-3-important-limits/</a>.
I seem to recall that Microsoft lead some further research when they were developing
the original Surface tables (before the tablets). DoubleClick/Google also published
some additional refinements in the specific case of web applications and mobile web
applications. Sadly, I haven’t found the links.</p>

<p>Let’s start with a simple example:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>class</span> <span>ComputeFibonacciEvent</span>:
</span></span><span><span>    arg: int
</span></span><span><span>
</span></span><span><span><span>def</span> <span>fibonacci</span>(n: int) <span>-&gt;</span> int:
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    <span>return</span> fibonacci(n <span>-</span> <span>1</span>) <span>+</span> fibonacci(n <span>-</span> <span>2</span>)
</span></span><span><span>
</span></span><span><span><span>def</span> <span>on_event</span>(event):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        result <span>=</span> fibonacci(event<span>.</span>arg)
</span></span><span><span>        print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>)=</span><span>{</span>result<span>}</span><span>&#34;</span>)
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span>...</span></span></span></code></pre></div></div>
  
</div><p>Yes, I’m very aware that you can rewrite Fibonacci in a more efficient manner,
buy let’s keep this awfully inefficient implementation.
Feel free to replace this with any other task that is slow to execute.</p>
<p>Now, does our event loop fit within our 16ms budget? For a sufficiently large
value of <code>arg</code>, it might not. But if we exceed our 16ms budget, we are blocking
our application from repainting the screen, or taking new HTTP requests, etc.
and that’s bad.</p>
<p>So how do we make our computation fit?</p>
<p>Well, there are many solutions, but all of them are variants around the following
idea:</p>
<blockquote>
<p>Make <code>fibonacci</code> non-blocking.</p>
</blockquote>

<p>Non-blocking is not the most common word you’ll find around the web. You’ll often
read about asynchronous, concurrent, parallel. These are four distinct concepts
that people tend to confuse hopelessly.</p>
<blockquote>
<p>Code is <strong>non-blocking</strong> if it never <em>blocks</em> any critical thread.</p>
</blockquote>
<p>In this conversation, we’re interested in the thread containing the event loop.
Non-blocking is an objective. It’s also a guarantee provided by some functions
in your libraries or your operating system.</p>
<p>How do you achieve this? Well, that’s what this entire post is all about.</p>
<blockquote>
<p>Code is <strong>asynchronous</strong> if it <em>structured</em> to present explicit dependencies
between executions.</p>
</blockquote>
<p>Asynchronous is about code structure. Typically, this involves callbacks, or events,
or some abstraction on top of them.</p>
<p>Asynchronous does NOT guarantee that your code is non-blocking. In fact, the only
guarantee is that if you refactor your code to become non-blocking, you won’t break
everything.</p>
<blockquote>
<p>Code is <strong>concurrent</strong> if you can schedule independent tasks to run.</p>
</blockquote>
<p>Concurrency is also a programming style. Concurrency does not guarantee <em>when</em> the
tasks run. There are concurrency toolkits that simply wait until a task is complete
before running the next one. There are also concurrency primitives that will interleave
the execution of two concurrent tasks, attempting to ensure that each of them progresses
regularly. If this is done automatically, this is called <strong>preemptive multitasking</strong>. If
the code requires specific annotations, this is called <strong>cooperative multitasking</strong>. Most
developers use the word “concurrent” only if it involves some kind of multitasking.</p>
<p>Concurrency also does not guarantee that an operation is non-blocking.</p>
<p>Concurrent and asynchronous are often confused, but they’re different things:</p>
<div><ul>
<li>code can be concurrent without being asynchronous, e.g. if you launch multiple tasks and synchronize them implicitly
either by assuming run-to-completion or by using locks;</li>
<li>code can be asynchronous without being concurrent, e.g. Haskell is all about letting you specify dependencies, but without <code>par</code>, you can’t launch tasks.</li>
</ul></div>
<blockquote>
<p>Code is <strong>parallel</strong> if two tasks can run at the same physical instant.</p>
</blockquote>
<p>Parallelism is a property of the language, operating system, hardware and system load.
Code that is executed in parallel in one run could be executed sequentially in another.</p>
<p>If parallelism is guaranteed, then it can be used to guarantee that an operation is
non-blocking.</p>
<p>Concurrent and parallel are also often confused, but they’re different things:</p>
<div><ul>
<li>code can be concurrent without being parallel, e.g. threads in pure Python
will let you launch tasks, and only one of them will ever be executed at a time;</li>
<li>it’s a bit of a stretch, but code could be parallel without being concurrent, e.g.
if the garbage-collector or some tasks runs in parallel but the developer does not
have access to primitives to launch additional parallel tasks.</li>
</ul></div>

<p>We live in the 21st century, so (on most platforms) we have access to threads. Threads are
always a mean to achieve concurrency and, depending on resource constraints and your
programming language, may be a mean to achieve parallelism.</p>
<p>Let’s try to use them.</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>import</span> threading
</span></span><span><span>
</span></span><span><span><span>def</span> <span>on_event</span>(event):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        <span>def</span> <span>background</span>():
</span></span><span><span>            result <span>=</span> fibonacci(event<span>.</span>arg)
</span></span><span><span>            print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>)=</span><span>{</span>result<span>}</span><span>&#34;</span>)
</span></span><span><span>        thread <span>=</span> threading<span>.</span>Thread(target<span>=</span>background)
</span></span><span><span>        thread<span>.</span>start()
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span>...</span></span></span></code></pre></div></div>
  
</div><p>Based on a quick benchmark, creating and launching each thread in Python takes
about 6µs on my machine, so we’re well within budget. Yes, running fibonacci
in the background can still take an arbitrary amount of time, but that’s expected.
So… mission accomplished?</p>
<p>Well… yes and no.</p>
<p>If you look at your favorite web backend or video game or desktop application,
you’ll see that this is not the solution that the developers have picked.</p>

<p>
  <h2 id="threads-are-tricky">
    Threads are tricky
  </h2>
</p><p>Part of it is the difficulty. Programming with threads has long been considered too difficult
for mere mortals, with the need to use (and understand!) thread-safety, mutexes, atomic
operations and, sometimes, thread-local storage. While the specific case in the example
is trivially thread-safe, it is really easy to write code that appears thread-safe but
relies on some well-hidden global state (as is very common in Python libraries, for
instance). I have also, quite a few times, seen code misusing mutexes (typically by
protecting the wrong variable or protecting it at the wrong moment, or sometimes by
blocking the main thread with a mutex, hence making the entire exercise pointless) or atomicity
(by mis-understanding the memory model), so yes, being wary of threads makes all
sorts of sense.</p>
<p>In fact, to this day, I am not aware of any multi-threading safe GUI toolkit, and even
languages that make it simple to write multi-threaded code, such as Go, do not make it
simple to write <em>correct</em> multi-threaded code <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>. <a href="https://users.rust-lang.org/t/unsafe-std-set-var-change/112704">Even the Rust stdlib got one function
wrong for a long time</a>.</p>
<p>Or, in the words of David Baron:</p>
<figure>  <div>
    
    <div>
      <p>
        You must be this tall to write multi-threaded code (about 2.5m)
      </p>
    </div>
  </div>
</figure>

<p>By the way, I write that the snippet is trivially safe, but that’s actually not certain.
What happens if <code>print</code> is called by several threads at the same time? In such cases,
the original implementations of <code>printf</code> in C would cause all sorts of memory breakages.
Nowadays, it’s probably safe… but how do you check that? Rust explicitly uses locks
around stdout and stderr, to avoid any threading problems, but most other languages and
frameworks don’t.</p>

<p>
  <h2 id="threads-cost-resources">
    Threads cost resources
  </h2>
</p><p>Another reason is resource limitations. Each process can launch a finite number of threads.
Each user can launch a finite number of threads. Each kernel can launch a finite number of
threads. And when you’re writing and deploying your application, you often don’t know that
number, which means that you can’t rely upon it: for all you know, your application will
run without thread support (I have noticed this once with a Docker deployment).</p>
<p>Oh, and your threads typically eat some memory (8kb physical and 8Mb virtual on Linux,
last time I checked), which contributes (a bit) to making threads a complicated proposition:
if you launch a thread to make an operation non-blocking, and if launching threads may fail
(sometimes in hard-to-catch fashion), how should you handle the case in which your thread
launch fail? Can you even do it?</p>
<p>These resource limitations are the reason for which web backends cannot just rely on threads.
Because it’s seldom a good idea to let your users (who could be malicious, or using buggy
clients, or stampeding after an article on Hacker News) control how many resources
you use.</p>
<p>Now, these resource limitations have been known and worked around for decades, through
the use of thread pools:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>from</span> concurrent.futures <span>import</span> ThreadPoolExecutor
</span></span><span><span>thread_pool <span>=</span> ThreadPoolExecutor() <span># Place a limit on the number of threads used.</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>on_event</span>(event):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        <span>def</span> <span>background</span>():
</span></span><span><span>            result <span>=</span> fibonacci(event<span>.</span>arg)
</span></span><span><span>            print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>)=</span><span>{</span>result<span>}</span><span>&#34;</span>)
</span></span><span><span>        thread_pool<span>.</span>submit(background)
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span>...</span></span></span></code></pre></div></div>
  
</div><p>And in many cases (again, assuming that your code is thread-safe), this works.</p>
<p>When doesn’t it work?</p>
<p>First, it generally doesn’t work if you’re writing a library. You don’t know the thread
policy of your callers, so you could accidentally be introducing thread-unsafety in the
caller’s code by using a thread pool, or multiplying the number of thread pools, hence
breaking the constraint limits. Please don’t do that. If you wish to work with a thread
pool, ask your client code to provide it.</p>
<p>The second problem appears if your threads are, for some reason, blocked. This can happen,
for instance, if any of your threads needs to access a database, or a remote server, if
you have any kind of call to <code>sleep</code> or if, for some reason, your <code>print</code> has been rerouted
to a file that is, for some reason slow. In such cases, can quickly saturate the thread
pool with threads doing nothing (well, waiting for completion of some activity that is not
controlled by your code). Any further task will just have to wait until one of the waiting
threads has finished its work.</p>
<p>In other words, you have completely lost throughput. If you’re writing a webserver, this
suddenly means that you need more webservers to be able to serve all your users, which
increases your cloud bills and the energy bill paid by the planet. If you’re writing a
video game, sure, the framerate remains good, but the actions of the PCs and NPCs feel
sluggish. If you’re writing a desktop app, sure, the UI remains responsive, but your
users wait forever.</p>

<p>
  <h2 id="threads-may-be-giled">
    Threads may be GILed
  </h2>
</p><p>Let’s get one nasty thing out of the way: if you’re writing Python or Ruby (or pretty old
versions of OCaml), your threads are never, ever, going to run in parallel. That’s because
these languages rely on a Global Interpreter Lock, designed specifically to make sure that
only one thread is ever running at a given instant.</p>
<p>Why? Well, this makes the rest of the implementation of the language much, much simpler, in
particular refcounting/garbage-collection. This also avoids lots of pitfalls that would make
your life miserable. This also allows a number of optimizations (both in the VM/interpreter
and by at user-level) that would become very much unsafe if the code truly ran in parallel.</p>
<p>Note that (at least in Python), native code (e.g. PyTorch, NumPy, any code written with PyO3,
etc.) can <em>release the GIL</em>, which means that its content can run in background threads
without blocking other threads. Most of the time, it’s a good thing, but if the developer
of the native code doesn’t know what they’re doing, this can quickly lead to memory corruption.</p>
<p>What does this mean for performance? It means that if your code is not calling into code that
releases the GIL, it’s always going to be quite slower in multi-threaded mode than in
single-threaded mode. How do you know if code releases the GIL? Sadly, it’s pretty much
never documented, so you have to experiment to find out.</p>
<p>Also, the case of OCaml demonstrates that you can bring an ecosystem from GIL-ed
to fully multicore (OCaml ≥ 5), but suggests that it may take a pretty long time. Python
seems to be slowly heading in this direction, but I don’t think we’ll see anything usable
before 2030<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p>

<p>
  <h2 id="threads-are-kinda-slow">
    Threads are (kinda) slow
  </h2>
</p><p>Operating System threads need to <em>context-switch</em> to provide preemptive multitasking, i.e.
some thread runs on a CPU/core, then the thread is put on hold while some other code runs on
that CPU/core. This is transparent to the user, but there is a cost.</p>
<p>Roughly speaking, to context-switch between two tasks, the OS scheduler will need to:</p>
<div><ol>
<li>interrupt the user-level code on that CPU and place the CPU into kernel mode;</li>
<li>backup the registers, including the program counter;</li>
<li>backup the pointers to the thread stack, the thread-local storage, etc;</li>
<li>deactivate interrupt and signal-handling and backup their state;</li>
<li>replace registers, pointers, interrupt handlers, signal handlers with those of the new thread;</li>
<li>return to user land and resume execution of the code onto the CPU.</li>
</ol></div>
<p>There is a cost to all this.</p>
<p>I’ve been too lazy to benchmark, but I’ve seen benchmarks on a machine more recent than
mine that indicate a cost of 2-5µs during each context-switch. That’s time spent executing
code that’s not serving your needs. If your code needs to context-switch 5000 times per
core per second (that’s an arbitrary number), you have eaten 10-25ms of your budget just
on context-switching, so that’s roughly equivalent to one frame per second which you may
need to somehow compensate <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>.</p>
<p>There are, of course, other costs to threads. Every lock you need to acquire/release has
a raw synchronization cost, plus a contention cost. In particular, you very much want
to avoid grabbing a lock on the main thread, as this makes thread synchronization
a blocking operation. Even atomic operation you need to perform may have a performance
cost, in particular on your cache. Etc.</p>
<p>In most cases, you probably don’t care. In particular, if you’re writing Python, you
have bigger performance issues. However, if you’re writing performance-sensitive
code (e.g. a video game, a video player, a browser), threads are not just part of the
solution but also sometimes part of the performance problems you need to deal with.</p>

<p>
  <h2 id="what-about-green-threads">
    What about green threads?
  </h2>
</p><p>Green threads are threads implemented purely in user-land, i.e. they behave as threads
but they don’t go through OS-level scheduling.</p>
<p>Scheduling between green threads is very similar to scheduling OS threads, but two things
make it faster:</p>
<div><ol>
<li>scheduling doesn’t need to cross between user land and kernel;</li>
<li>there are fewer things to backup and restore (in particular, generally no interrupt
and signal handlers).</li>
</ol></div>
<p>Similarly, lock synchronization may be faster.</p>
<p>On the other hand, pure green threads do not benefit from multiple cores or CPUs,
which strongly decreases the usefulness of these threads.</p>
<p>Using pure green threads is rather uncommon these days. However, a few languages
have so-called M:N schedulers, which combine green threads and OS threads. We’ll
speak more of this in the section dedicated to Go.</p>

<p>
  <h2 id="so-threads">
    So, threads?
  </h2>
</p><p>In other words, while threads are a necessary component of any solution, they are not the
magic bullet that we can hope.</p>

<p>For a long time, Linux did not support threads. This has never prevented developers from writing
concurrent/parallel code. One of the workarounds for this lack of threads was to use multiple
processes. Similarly, the solution to running code across multiple CPUs or cores in GIL-based
languages has traditionally been to use multiple processes. In the 90s, OCaml even had a dialect
called JoCaml, which featured a rather excellent paradigm for parallelism and distribution.</p>
<p>With a high-level API, spawning execution on a process is fairly simple:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>from</span> concurrent.futures <span>import</span> ProcessPoolExecutor
</span></span><span><span>process_pool <span>=</span> ProcessPoolExecutor()
</span></span><span><span>
</span></span><span><span><span>def</span> <span>on_event</span>(event):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        future <span>=</span> process_pool<span>.</span>submit(fibonacci, event<span>.</span>arg)
</span></span><span><span>        future<span>.</span>add_done_callback(<span>lambda</span> result: print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>)=</span><span>{</span>result<span>}</span><span>&#34;</span>))
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span>...</span></span></span></code></pre></div></div>
  
</div><p>This has immediate benefits: processes are not limited by a GIL, so code will typically
run in parallel. Also, garbage-collectors are indendent across processes, so a slow
garbage-collection on one process will generally not block another process.</p>
<p>There are a few drawbacks to the approach, though.</p>

<p>
  <h2 id="processes-are-expensive">
    Processes are expensive
  </h2>
</p><p>Each process runs its own copy of Python (or Ruby, or JavaScript, etc.), including an in-memory copy of
not only the standard library, but every single dependency, a garbage-collector, etc. Also, if your
language is JITed, each process runs its own JIT, which means its own copy of all the profiling
data, and the optimized native code. The memory costs quickly add up.</p>
<p>In fact, one could argue that running multiple processes for a non-system language only makes sense if RAM is
free and infinite.</p>
<p>Also, just as there are limits to the number of threads, there are limits to the number of processes.</p>

<p>
  <h2 id="communications-are-expensive">
    Communications are expensive
  </h2>
</p><p>Communications between threads is simple: you just send the data by passing references.</p>
<p>Communication between processes (aka Inter Process Communication or IPC), though? That’s another
story. There are a few ways to do things.</p>
<p>You can use shared memory:</p>
<div><ol>
<li>process A <em>serializes</em> all the data it wishes to send to some local buffer;</li>
<li>now that process A knows how much memory is needed to represent the data, it
locks (internally) a segment of shared memory for this specific communication
with process B;</li>
<li>process A <em>copies</em> all this data to the shared memory segment;</li>
<li>process A sends a signal to process B to inform process B that it should read the data (it’s a system call, so this needs to go through the kernel);</li>
<li>process B receives a signal from process A (it’s an interrupt, so this may happen during garbage-collection for instance, or during a file access, etc. so the following steps may need to be delayed until garbage-collection is complete);</li>
<li>process B finds the data in the shared memory and copies it to some local buffer;</li>
<li>process B sends a signal to process A to inform process A that the data can now be deallocated (again, system call);</li>
<li>process A receives the signal from process B;</li>
<li>process A releases (internally) the locked segment;</li>
<li>in parallel with 8-9, process B deserializes and checks for corruption the data receives from process A;</li>
<li>process B has finally received the message.</li>
</ol></div>
<p>Yeah, that’s not just complicated (that part is hidden from the user by a nice IPC library), it’s expensive.</p>
<p>You can simplify things quite a bit by going through sockets or pipes instead of shared memory, but at the expense of making more system calls, plus you’ll need to somehow make your pipe I/O non-blocking, which brings us back to our original problem.</p>

<p>
  <h2 id="processes-are-kinda-slow">
    Processes are (kinda) slow
  </h2>
</p><p>Everything I wrote about threads being (kinda) slow? Well, all of this is true of processes,
except that processes have way more data to save/restore in their Process Control Block: memory
mappings, file descriptors, etc.</p>
<p>Also, locks between processes (which are fortunately needed much less often than locks within
a process) typically go through the file system, so they’re a bit more expensive than locks between
threads.</p>

<p>
  <h2 id="so-processes">
    So, processes?
  </h2>
</p><p>Processes are sometimes the right tool for the task, but their costs are steep, so you’ll need to be
very careful about picking processes to solve your problem. Unless you only have access to processes,
in which case… well, you don’t have a choice, do you?</p>

<p>Threads and processes are a fairly high-level and expensive constructs. Perhaps we got off on the wrong
foot, and the right way to solve our problem is to approach it from the other end. What if we rewrote
our function <code>fibonacci</code> to have it compute things concurrently, manually making sure that
we never block the event loop?</p>
<p>This would, of course, be easier if our implementation wasn’t (non-tail) recursive, but this
can be done.</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>@dataclass</span>
</span></span><span><span><span>class</span> <span>ComputeFibonacciEvent</span>(BaseEvent):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Event: We&#39;d like to compute `fibonacci(arg)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    arg: int
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    The value for which we wish to compute fibonacci.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    id: UUID
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    A unique id for this event.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    parent_id: UUID <span>|</span> <span>None</span>
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    If `None`, this is a toplevel request. Otherwise, the `id` of another
</span></span></span><span><span><span>    `ComputeFibonacciEvent` on behalf of which we&#39;re performing this
</span></span></span><span><span><span>    computation.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span><span>@dataclass</span>
</span></span><span><span><span>class</span> <span>CompletedFibonacciEvent</span>(BaseEvent):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Event: We have finished computing `fibonacci(arg)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    arg: int
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    The value for which we requested to compute fibonacci.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    parent_id: UUID <span>|</span> <span>None</span>
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    If `None`, this was a toplevel request. Otherwise, the `id` of another
</span></span></span><span><span><span>    `ComputeFibonacciEvent` on behalf of which we&#39;re performing this
</span></span></span><span><span><span>    computation.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    result: int
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    The value of `fibonacci(arg)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span><span>@dataclass</span>
</span></span><span><span><span>class</span> <span>PendingFibonacci</span>:
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Rendez-vous mechanism, holding the pending or partial state
</span></span></span><span><span><span>    of computing `fibonacci(arg)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    arg: int
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    The value for which we requested to compute fibonacci.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    parent_id: UUID <span>|</span> <span>None</span>
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    If `None`, this was a toplevel request. Otherwise, the `id` of another
</span></span></span><span><span><span>    `ComputeFibonacciEvent` on behalf of which we&#39;re performing this
</span></span></span><span><span><span>    computation.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    first: int <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    If `None`, we haven&#39;t computed `fibonacci(arg - 1)` yet. Otherwise,
</span></span></span><span><span><span>    the value of `fibonacci(arg - 1)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>pending_fibonaccis: dict[UUID, PendingFibonacci] <span>=</span> dict()
</span></span><span><span><span>&#34;&#34;&#34;
</span></span></span><span><span><span>A mapping of event id =&gt; PendingFibonacci.
</span></span></span><span><span><span>&#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>handle_event</span>(event: BaseEvent):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        <span>if</span> event<span>.</span>arg <span>&lt;=</span> <span>1</span>:
</span></span><span><span>            event_queue<span>.</span>put(CompletedFibonacciEvent(
</span></span><span><span>                parent_id<span>=</span>event<span>.</span>parent_id,
</span></span><span><span>                result<span>=</span><span>1</span>,
</span></span><span><span>                arg<span>=</span>event<span>.</span>arg,
</span></span><span><span>            ))
</span></span><span><span>        <span>else</span>:
</span></span><span><span>            <span># Enqueue the left and right computations.</span>
</span></span><span><span>            event_queue<span>.</span>put(ComputeFibonacciEvent(
</span></span><span><span>                id <span>=</span> uuid4(),
</span></span><span><span>                parent_id<span>=</span>event<span>.</span>id,
</span></span><span><span>                arg <span>=</span> event<span>.</span>arg <span>-</span> <span>1</span>,
</span></span><span><span>            ))
</span></span><span><span>            event_queue<span>.</span>put(ComputeFibonacciEvent(
</span></span><span><span>                id <span>=</span> uuid4(),
</span></span><span><span>                parent_id<span>=</span>event<span>.</span>id,
</span></span><span><span>                arg <span>=</span> event<span>.</span>arg <span>-</span> <span>2</span>,
</span></span><span><span>            ))
</span></span><span><span>            <span># Store what we need to propagate the result.</span>
</span></span><span><span>            pending_fibonaccis[event<span>.</span>id] <span>=</span> PendingFibonacci(
</span></span><span><span>                parent_id<span>=</span>event<span>.</span>parent_id,
</span></span><span><span>                arg<span>=</span>event<span>.</span>arg,
</span></span><span><span>            )
</span></span><span><span>    <span>elif</span> isinstance(event, CompletedFibonacciEvent):
</span></span><span><span>        pending <span>=</span> pending_fibonaccis[event<span>.</span>parent_id]
</span></span><span><span>        <span>if</span> pending<span>.</span>first <span>is</span> <span>None</span>:
</span></span><span><span>            pending<span>.</span>first <span>=</span> event<span>.</span>result
</span></span><span><span>            <span># We still need to wait for the second computation.</span>
</span></span><span><span>        <span>else</span>:
</span></span><span><span>            <span># We have obtained both computations.</span>
</span></span><span><span>            result <span>=</span> pending<span>.</span>first <span>=</span> event<span>.</span>result
</span></span><span><span>            <span>if</span> pending<span>.</span>parent_id <span>is</span> <span>None</span>:
</span></span><span><span>                <span>#... and we&#39;re done!</span>
</span></span><span><span>                print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>) = </span><span>{</span>result<span>}</span><span>&#34;</span>)
</span></span><span><span>            <span>else</span>:
</span></span><span><span>                <span>#...continue popping!</span>
</span></span><span><span>                event_queue<span>.</span>put(CompletedFibonacciEvent(
</span></span><span><span>                    parent_id<span>=</span>pending<span>.</span>parent,
</span></span><span><span>                    result<span>=</span>result,
</span></span><span><span>                    arg<span>=</span>event<span>.</span>arg
</span></span><span><span>                ))</span></span></code></pre></div></div>
  
</div><p>Ouch. That’s… quite a rewrite. We just turned a trivial four-lines
function into a 180 lines, impossible-to-debug, monster.</p>
<p>But this, as a mechanism, works. At each step of the event loop, the computation
is trivial and non-blocking. Alright, I’m cheating a bit: the call to
<code>print</code> remains blocking, but you could also rewrite it into a sequence
of non-blocking calls. In fact, if you look at Firefox or nginx, for
instance, you’ll find plenty of code written like this, usually
placing requests for long external operations (for instance, writing to
the database or to the network), then waking up once the request has
progressed to the next step, enqueuing the next step of the work as
a new event, etc.</p>
<p>Of course, the code I’ve written above is not nearly thread-safe. It could
be made thread-safe, and hopefully take advantage of parallelism, at some
cost in terms of throughput.</p>
<p>But before we start thinking about parallelism, let’s see how we can
improve that monster.</p>

<p>Continuation-passing style is a programming style in which functions
never return a result. Instead, each function receives as (usually last) argument
a closure (the “continuation”) with instructions on what to do with the
result.</p>
<p>If you have ever programmed with old-style Node, that’s exactly how it
used to work. If you have ever programmed with monads, this involves the
same idea.</p>
<p>So, let’s rewrite our code to use CPS:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>fibonacci_cps</span>(n: int, then: Callable[[int]]):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Compute `fibonacci(n)`, then call `then`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> then(<span>1</span>)
</span></span><span><span>    <span># Once we have the result of `fibonacci(n - 1)`, compute</span>
</span></span><span><span>    <span># `fibonacci(n - 2)`, then sum both.</span>
</span></span><span><span>    <span>def</span> <span>with_left</span>(left: int):
</span></span><span><span>        fibonacci_cps(
</span></span><span><span>            n<span>=</span>n <span>-</span> <span>2</span>,
</span></span><span><span>            then<span>=</span><span>lambda</span> right: then(left <span>+</span> right)
</span></span><span><span>        )
</span></span><span><span>    <span># Compute `fibonacci(n- 1)`.</span>
</span></span><span><span>    fibonacci_cps(
</span></span><span><span>        n<span>=</span>n <span>-</span> <span>1</span>,
</span></span><span><span>        then<span>=</span>with_left)
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>handle_event</span>(event: BaseEvent):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        fibonacci_cps(event<span>.</span>arg, <span>lambda</span> result: print(<span>f</span><span>&#34;fibonacci(</span><span>{</span>event<span>.</span>arg<span>}</span><span>)=</span><span>{</span>result<span>}</span><span>&#34;</span>))
</span></span><span><span>    <span>elif</span> isinstance(event, SleepEvent):
</span></span><span><span>        event<span>.</span>thunk()</span></span></code></pre></div></div>
  
</div><p>That’s nicer. So far, it’s blocking, but it’s nicer.</p>
<p>Now, by moving to CPS, we have removed the need to return, which means that
we can delay computation. For instance, without breaking the rest of the code,
we can add <code>wait</code> instructions in the middle of <code>fibonacci_cps</code>.</p>
<p>First, let’s add some general-purpose support code:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>@dataclass</span>
</span></span><span><span><span>class</span> <span>SleepEvent</span>(BaseEvent):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Wait until the next tick of the event loop before running `thunk`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    thunk: Callable[[]]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>wait</span>[T](continuation: Callable[[T]]) <span>-&gt;</span> Callable[[T]]:
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Wait until the next tick of the event loop before running `continuation`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>def</span> <span>result</span>(arg: T):
</span></span><span><span>        <span>def</span> <span>thunk</span>() <span>-&gt;</span> <span>None</span>:
</span></span><span><span>            <span>return</span> continuation(arg)
</span></span><span><span>        event_queue<span>.</span>put(SleepEvent(
</span></span><span><span>            thunk<span>=</span>thunk
</span></span><span><span>        ))
</span></span><span><span>    <span>return</span> result
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>handle_event</span>(event: BaseEvent):
</span></span><span><span>    <span>if</span> isinstance(event, SleepEvent):
</span></span><span><span>        event<span>.</span>thunk()
</span></span><span><span>    <span>else</span>
</span></span><span><span>        <span># ... As previously</span></span></span></code></pre></div></div>
  
</div><p>Once we have this, we may rewrite <code>fibonacci_cps</code> as follows:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>fibonacci_cps</span>(n: int, then: Callable[[int]]):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Compute `fibonacci(n)`, then call `then`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> wait(then)(<span>1</span>)
</span></span><span><span>    <span># Once we have the result of `fibonacci(n - 1)`, compute</span>
</span></span><span><span>    <span># `fibonacci(n - 2)`, then sum both.</span>
</span></span><span><span>    <span>def</span> <span>with_left</span>(left: int):
</span></span><span><span>        fibonacci_cps(
</span></span><span><span>            n<span>=</span>n <span>-</span> <span>2</span>,
</span></span><span><span>            then<span>=</span><span>lambda</span> right: wait(then)(left <span>+</span> right)
</span></span><span><span>        )
</span></span><span><span>    <span># Compute `fibonacci(n- 1)`.</span>
</span></span><span><span>    <span>def</span> <span>compute_left</span>():
</span></span><span><span>        fibonacci_cps(
</span></span><span><span>            n<span>=</span>n <span>-</span> <span>1</span>,
</span></span><span><span>            then<span>=</span>with_left)
</span></span><span><span>    wait(compute_left)</span></span></code></pre></div></div>
  
</div><p>…and with this, we have made our code non-blocking. And of course, <code>wait</code> and <code>SleepEvent</code>
can be reused for any other CPS function. We could go further and customize just how many
ticks we wait, or dispatch tasks to various CPUs if we wanted.</p>
<p>If you recall our earlier definitions, writing our code as CPS makes it <em>asynchronous</em>.
And we just demonstrated how to refactor our asynchronous code to also be non-blocking.</p>
<p>CPS calls are the real reason for which Node.js was initially lauded as “fast”. It wasn’t
about CPU speed, but about the fact that, thanks to CPS, you can run (literally) millions
of concurrent tasks, especially tasks that spend most of their time waiting for network
or database read/writes, without taxing the CPU too much.</p>
<p>So far, so good. Of course, we have still increased the code size for <code>fibonacci</code> from 4
lines to 18 and made it harder to read. Also, if you’re interested in performance, we
have allocated a lot of closures, which we’re going to pay in garbage-collection time.</p>
<p>Can we do better?</p>

<p>Generators are function-like objects that can be called repeatedly to return a succession
of values. From the point of view of languages that support CPS natively (more precisely,
languages that support <code>call/cc</code> or <code>delimcc</code>), generators are simple abstractions upon
simple use cases for continuations.</p>
<p>As it turns out, generators have been used in several languages and frameworks to achieve
concurrency.</p>
<p>Let’s start with some support code:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>@dataclass</span>
</span></span><span><span><span>class</span> <span>ContinueEvent</span>(BaseEvent):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Schedule an execution for the next tick of the event loop.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    generator: Generator[<span>None</span>, <span>None</span>, <span>None</span>]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>handle_event</span>(event: BaseEvent):
</span></span><span><span>    <span>if</span> isinstance(event, ComputeFibonacciEvent):
</span></span><span><span>        <span># Start computation.</span>
</span></span><span><span>        <span>def</span> <span>generator</span>() <span>-&gt;</span> Generator[<span>None</span>, <span>None</span>, <span>None</span>]:
</span></span><span><span>            fibo <span>=</span> fibonacci(event<span>.</span>n)
</span></span><span><span>            <span>try</span>:
</span></span><span><span>                <span>while</span> <span>True</span>:
</span></span><span><span>                    next(fibo)
</span></span><span><span>                    <span># Not ready yet.</span>
</span></span><span><span>                    <span>yield</span> <span>None</span>
</span></span><span><span>            <span>except</span> <span>StopIteration</span> <span>as</span> e:
</span></span><span><span>                result: int <span>=</span> e<span>.</span>value
</span></span><span><span>                print(<span>f</span><span>&#34;fibonacci</span><span>{</span>event<span>.</span>n<span>}</span><span>=</span><span>{</span>result<span>}</span><span>&#34;</span>)
</span></span><span><span>
</span></span><span><span>        event_queue<span>.</span>put(ContinueEvent(generator()))
</span></span><span><span>    <span>elif</span> isinstance(event, ContinueEvent):
</span></span><span><span>        <span># Continue computation</span>
</span></span><span><span>        <span>try</span>:
</span></span><span><span>            <span># Are we done yet?</span>
</span></span><span><span>            next(event<span>.</span>generator)
</span></span><span><span>
</span></span><span><span>            <span># Continue next tick.</span>
</span></span><span><span>            event_queue<span>.</span>put(event)
</span></span><span><span>        <span>except</span> <span>StopIteration</span>:
</span></span><span><span>            <span># Done</span>
</span></span><span><span>            <span>pass</span>
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span>raise</span> <span>NotImplementedError</span></span></span></code></pre></div></div>
  
</div><p>The general idea is that <code>handle_event</code> receives instances of <code>ContinueEvent</code> and keeps
calling <code>next(event.generator)</code>. If <code>next(event.generator)</code> returns (without raising), it
means that the code requested a pause. For our implementation of Fibonacci’s function,
this will happen because we decided to break the function call into several non-blocking
segments, but for anything involving, say, network calls, it will mean that the network
call isn’t finished yet.</p>
<p>In practice, here’s our new version of <code>fibonacci</code>:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>fibonacci</span>(n: int) <span>-&gt;</span> Generator[<span>None</span>, <span>None</span>, int]:
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    An implementation of fibonacci.
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Yields None to reschedule the computation to the next tick of the event loop.
</span></span></span><span><span><span>    After that, returns `int` with the result of `fibonacci(n)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    <span>yield</span> <span>None</span> <span># Take a break.</span>
</span></span><span><span>
</span></span><span><span>    waiting_left <span>=</span> fibonacci(n <span>-</span> <span>1</span>)
</span></span><span><span>    <span>try</span>:
</span></span><span><span>        <span>while</span> <span>True</span>:
</span></span><span><span>            next(waiting_left)
</span></span><span><span>            <span># Not `StopIteration` raised, which means we need to take a break.</span>
</span></span><span><span>            <span>yield</span> <span>None</span>
</span></span><span><span>    <span>except</span> <span>StopIteration</span> <span>as</span> e:
</span></span><span><span>        left: int <span>=</span> e<span>.</span>value
</span></span><span><span>
</span></span><span><span>    waiting_right <span>=</span> fibonacci(n <span>-</span> <span>2</span>)
</span></span><span><span>    <span>try</span>:
</span></span><span><span>        <span>while</span> <span>True</span>:
</span></span><span><span>            next(waiting_right)
</span></span><span><span>            <span># Not `StopIteration` raised, which means we need to take a break.</span>
</span></span><span><span>            <span>yield</span> <span>None</span>
</span></span><span><span>    <span>except</span> <span>StopIteration</span> <span>as</span> e:
</span></span><span><span>        right:int <span>=</span> e<span>.</span>value
</span></span><span><span>
</span></span><span><span>    <span>return</span> left <span>+</span> right</span></span></code></pre></div></div>
  
</div><p>Alright, it <em>is</em> still pretty long, but it is quite readable, in a Go style of
things, with lots of easy-to-skip copy &amp; paste code. In fact, if we had some
syntactic support, we could imagine rewriting this entire function as:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>fibonacci</span>(n: int) <span>-&gt;</span> Generator[<span>None</span>, <span>None</span>, int]:
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    An implementation of fibonacci.
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Yields None to reschedule the computation to the next tick of the event loop.
</span></span></span><span><span><span>    After that, returns `int` with the result of `fibonacci(n)`.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    <span>yield</span> <span>None</span> <span># Take a break.</span>
</span></span><span><span>
</span></span><span><span>    left  <span>=</span> <span>await</span> fibonacci(n <span>-</span> <span>1</span>) <span># Pseudo-syntax.</span>
</span></span><span><span>    right <span>=</span> <span>await</span> fibonacci(n <span>-</span> <span>2</span>) <span># Pseudo-syntax.</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> left <span>+</span> right</span></span></code></pre></div></div>
  
</div><p>…but let’s not get too much ahead of ourselves.</p>
<p>What are the benefits?</p>
<div><ol>
<li>The transformation from our original <code>fibonacci</code> is trivial and, in fact, mostly
automatizable.</li>
<li>In many cases, this transformation can be optimized by a compiler, to have small
to no overhead, by opposition to CPS.</li>
<li>It is easy to add or remove <code>yield None</code>, which in turn means that you can
control context-switches, good both for performance and to avoid multi-threading
pitfalls. Sadly, there is a non-trivial cognitive cost in most languages.
From the languages I know, only Rust manages to offload (most of) the cognitive
cost of paying attention to race conditions onto the compiler.</li>
<li>Writing an implementation of <code>ContinueEvent</code> that dispatches tasks to multiple
CPUs is quite simple.</li>
</ol></div>
<p>What are the downsides?</p>
<div><ol>
<li>We’re still allocating memory dynamically, which isn’t great wrt performance.</li>
<li>There are still no guarantees about being non-blocking. In particular, if we
forget to call <code>yield None</code>, our entire transformation will be pointless.</li>
</ol></div>
<p>Note that this rewrite as generators is, once again, a concurrent rewrite, which makes
no guarantee about non-blocking.</p>
<p>Can we do better?</p>

<p>In JavaScript/TypeScript, these days, instead of CPS or generators to achieve concurrency,
developers tend to use <code>Promise</code>. While JavaScript supports generators, the need to make
JavaScript code non-blocking (and in particular, historically, the JavaScript code that
powers the user interface of Firefox) predates the implementation of generators in the
language.</p>
<p>Without syntactic support, the implementation of Fibonacci would look more like</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="js"><span><span><span>/**
</span></span></span><span><span><span> * Sleep a few milliseconds. Non-blocking.
</span></span></span><span><span><span> */</span>
</span></span><span><span><span>function</span> <span>sleep</span>(<span>ms</span><span>:</span> <span>number</span>)<span>:</span> Promise<span>&lt;</span><span>void</span><span>&gt;</span> {
</span></span><span><span>    <span>return</span> <span>new</span> Promise((<span>then</span>) =&gt; <span>setTimeout</span>(<span>then</span>, <span>ms</span>));
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>function</span> <span>fibonacci</span>(<span>n</span><span>:</span> <span>number</span>)<span>:</span> Promise<span>&lt;</span><span>number</span><span>&gt;</span> {
</span></span><span><span>    <span>if</span> (<span>n</span> <span>&lt;=</span> <span>1</span>) {
</span></span><span><span>        <span>return</span> Promise.<span>resolve</span>(<span>1</span>);
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>sleep</span>(<span>0</span>).<span>then</span>(() =&gt; {
</span></span><span><span>        <span>return</span> <span>fibonacci</span>(<span>n</span> <span>-</span> <span>1</span>).<span>then</span>((<span>left</span>) =&gt; {
</span></span><span><span>            <span>return</span> <span>fibonacci</span>(<span>n</span> <span>-</span> <span>2</span>).<span>then</span>((<span>right</span>) =&gt; {
</span></span><span><span>                <span>return</span> <span>left</span> <span>+</span> <span>right</span>
</span></span><span><span>            })
</span></span><span><span>        })
</span></span><span><span>    });
</span></span><span><span>}</span></span></code></pre></div></div>
  
</div><p>which is essentially a higher-level API on top of CPS.</p>
<p>In practice, JavaScript has a double event loop, with <code>Promise.then()</code> being resolved
in the inner event loop (micro-ticks) and events (including <code>setTimeout</code>) being resolved
in the outer event loop (ticks). This simplifies considerably some user-facing APIs,
but we don’t need to enter the details here.</p>
<p>This formulation is a bit easier to read than CPS, plus provides a better path for
error-handling (not displayed here), but still a bit hard on the eyes and
also requires plenty of allocations. Also, in terms of performance, this
would not be very friendly to multi-threading. Since JavaScript does not support
what we usually call multi-threading <sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>, that’s ok for JavaScript, but explains
why the same solution isn’t pushed forward in other languages.</p>
<p>So, the question remains: can we do better?</p>

<p>Well, yes, we can. In fact, in Python or Rust, I don’t think I’ve ever seen application
code written by human beings in the above style (I have seen much in JavaScript
applications and some as part of Python or Rust libraries and frameworks, though).
What we do, instead, is introduce some syntactic sugar that lets us write</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>async</span> <span>def</span> <span>fibonacci</span>(n: int) <span>-&gt;</span> int:
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    An implementation of fibonacci.
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Yields back time to the scheduler at each non-trivial recursive call.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span>:
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    <span>await</span> asyncio<span>.</span>sleep(<span>0</span>) <span># Take a break.</span>
</span></span><span><span>
</span></span><span><span>    left  <span>=</span> <span>await</span> fibonacci(n <span>-</span> <span>1</span>)
</span></span><span><span>    right <span>=</span> <span>await</span> fibonacci(n <span>-</span> <span>2</span>)
</span></span><span><span>    <span>return</span> left <span>+</span> right</span></span></code></pre></div></div>
  
</div><p>This, to a very close approximation, is syntactic sugar for the code we wrote above.
You can run it with <code>asyncio.run</code>, the de facto standard executor for <code>async</code>/<code>await</code>.</p>

<p>
  <h2 id="asyncawait-in-rust">
    async/await in Rust
  </h2>
</p><p>The code is quite similar in Rust, and will compile essentially to the same
loop with <code>yield</code> as in Python:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="rust"><span><span><span>async</span> <span>fn</span> <span>fibonacci</span>(n: <span>u32</span>) -&gt; <span>u64</span> {
</span></span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span> {
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    }
</span></span><span><span>    tokio::time::sleep(Duration::new(<span>0</span>, <span>0</span>)).<span>await</span>; <span>// Take a break.
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>let</span> left  <span>=</span> Box::pin(fibonacci(n <span>-</span> <span>1</span>)).<span>await</span>;  <span>// We can&#39;t store recursive calls on the fixed-size pseudo-stack, so we need to allocate memory.
</span></span></span><span><span><span></span>    <span>let</span> right <span>=</span> Box::pin(fibonacci(n <span>-</span> <span>2</span>)).<span>await</span>;  <span>// Since we&#39;ll rewrite it, we also want it to remain in place (hence the `pin`).
</span></span></span><span><span><span></span>    left <span>+</span> right
</span></span><span><span>}</span></span></code></pre></div></div>
  
</div><p>Interestingly, as of this writing, <code>yield</code> is not part of Rust’s surface level language.
However, it is used internally for this purpose. The generator will, in turn, compile
to a much more complicated finite state machine which we won’t detail here (if you want to look at it, see <a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=aedf6dccc1a53459e39075e53b5b69c0">this playground</a> and click “MIR” instead of “Build”). This Rust code
is, of course, thread-safe, and will in fact be dispatched to available CPUs if you execute it with
tokio, the de facto standard executor for <code>async</code>/<code>await</code> on non-embedded platforms.</p>
<p>Note that while both Rust and Python compile to similar-looking generators,
there are quite a few differences in the underlying machinery, besides threads and
in-memory representation. In particular, where the Python executor needs to poll
by calling the generator (<code>Awaitable</code>) repeatedly until it eventually advances, the Rust executor
expects the be informed by the generator (<code>Future</code>) once it is ready to be polled once again.</p>
<p>Rust also supports canceling futures.</p>

<p>
  <h2 id="asyncawait-in-javascript">
    async/await in JavaScript
  </h2>
</p><p>The surface-level JavaScript/TypeScript code is, again, quite similar, and will compile to
the <code>Promise</code>-based code above:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="ts"><span><span><span>/**
</span></span></span><span><span><span> * Sleep a few milliseconds. Non-blocking.
</span></span></span><span><span><span> */</span>
</span></span><span><span><span>function</span> <span>sleep</span>(<span>ms</span>: <span>number</span>)<span>:</span> <span>Promise</span>&lt;<span>void</span>&gt; {
</span></span><span><span>    <span>return</span> <span>new</span> <span>Promise</span>((<span>then</span>) <span>=&gt;</span> <span>setTimeout</span>(<span>then</span>, <span>ms</span>));
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>async</span> <span>function</span> <span>fibonacci</span>(<span>n</span>: <span>number</span>)<span>:</span> <span>Promise</span>&lt;<span>number</span>&gt; {
</span></span><span><span>    <span>if</span> (<span>n</span> <span>&lt;=</span> <span>1</span>) {
</span></span><span><span>        <span>return</span> <span>1</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>await</span> <span>sleep</span>(<span>0</span>);
</span></span><span><span>    <span>let</span> <span>left</span>  <span>=</span> <span>await</span> <span>fibonacci</span>(<span>n</span> <span>-</span> <span>1</span>);
</span></span><span><span>    <span>let</span> <span>right</span> <span>=</span> <span>await</span> <span>fibonacci</span>(<span>n</span> <span>-</span> <span>2</span>);
</span></span><span><span>    <span>return</span> <span>left</span> <span>+</span> <span>right</span>;
</span></span><span><span>}</span></span></code></pre></div></div>
  
</div><p>This will execute with your browser or Node’s built-in executor.</p>
<p>Note that, despite similar surface-level syntax, the underlying behavior is quite
different from either Rust or Python: Promises do not rely on the executor to be
polled, put to sleep or awakened. Rather, Promises essentially schedule themselves.</p>

<p>
  <h2 id="executors">
    Executors?
  </h2>
</p><p>If you wonder what an executor is, well, it’s an event loop we
have been discussing throughout this piece, along with the basic events
required to handle <code>async</code>/<code>await</code> and generally whatever you need to
build your own event loop on top of it if you need one.</p>

<p>
  <h2 id="the-case-for-and-against">
    The case for and against
  </h2>
</p><p>Now, is there any drawback to <code>async</code>/<code>await</code>? Yes, there are a few, and they
differ across implementations.</p>
<p>The first drawback, in some languages, is stacks. If you’ve ever attempted to
debug async code in Python or read through stack traces, you may have suffered.
This used to be the case in JavaScript, too, although there have been improvements.
This problem is not universal, as Rust, C# or F#, for instance, have proper
async backtraces.</p>
<p>A second drawback is that <code>async</code>/<code>await</code> has a performance cost. CPU-bound code
written with <code>async</code>/<code>await</code> will simply never be as fast or as memory-efficient
as the equivalent synchronous code. The cost is higher in Python (which requires
polling and numerous allocations) or JavaScript (which supports wakeups but
requires even more allocations) than in Rust (which supports wakeups and can
often avoid allocations), but it exists regardless. Of course, if you’re writing
I/O-bound code, the cost of I/O is typically several orders of magnitude larger
than any <code>async</code>/<code>await</code> overhead, so you will probably not observe any difference.</p>
<p>But the biggest problem I’ve seen, by far, is that
<em>most developers don’t seem to understand <code>async</code>/<code>await</code></em>.</p>
<p>So let me insist: <code>async</code>/<code>await</code> is a mechanism to easily make your code
<em>asynchronous</em>. It will make your code concurrent. It’s also a tool that
you can use to make your code non-blocking, but by itself,
<em>it doesn’t make your code magically non-blocking</em>. In particular, if you
call blocking code from async code, <em>you will block</em>.</p>
<p>Also, since every call to <code>await</code> <em>might</em> hide a context-switch to another
scheduled concurrent task, you will encounter most of the same problems as
with multi-threaded code: you will encounter data races, you will need locks
and possibly task-local storage and, to make things worse, your usual locks
won’t work – for instance, Rust’s tokio offers its own async implementation
of <code>Mutex</code>, <code>RwLock</code>, etc. In fact, these implementations are typically
<em>slower</em> than their thread counterparts.</p>
<p>And finally, <code>async</code>/<code>await</code> is a form of function coloring. You can’t wait
for an <code>async</code> function to wait without <code>await</code> and you can’t use <code>await</code> in
anything other than an <code>async</code> function. This means that you can’t pass an
<code>async</code> function as callback to a function that expects a sync function,
including your <code>map</code>, <code>filter</code>, <code>fold</code> or list comprehensions. This limits
code reuse and means that, once in a while, it will entirely prevent you
from using an API – I recently encountered the problem in Python, with a
scikit optimization function that required a sync callback, but the function
could only implemented as <code>async</code>, since it relied on further <code>async</code> functions.
It’s seldom a problem, but when it is, you are without a solution.</p>
<p>As I’ve briefly alluded to, <code>async</code>/<code>await</code> is also available in a bunch of other languages, including
F#, C#<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>, Haskell, Swift, Kotlin and (to some extent) C++. All languages that I’ve checked out, with the exception of JavaScript, compile <code>async</code>/<code>await</code> in the same manner as Python or Rust.</p>

<p>
  <h2 id="asyncawait-and-non-blocking">
    Async/await and non-blocking
  </h2>
</p><p>Are <code>async</code>/<code>await</code> sufficient to make the code non-blocking? Well, I’ve already answered
that question above, but let me illustrate and explain.</p>
<p>If you look again at the desugared Python code, full of <code>while True</code> and <code>yield None</code>, you
will notice that <code>await</code> doesn’t yield control to the executor. In fact, you can run
code full of <code>await</code> and never context-switch to another task:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>import</span> asyncio
</span></span><span><span>
</span></span><span><span><span>async</span> <span>def</span> <span>noop</span>():
</span></span><span><span>    <span># Do nothing.</span>
</span></span><span><span>    <span>return</span>
</span></span><span><span>
</span></span><span><span><span>async</span> <span>def</span> <span>foo</span>():
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>100</span>):
</span></span><span><span>        <span>await</span> noop() <span># This await will not make your foo() and bar() interleave.</span>
</span></span><span><span>        print(<span>&#34;foo&#34;</span>)
</span></span><span><span>
</span></span><span><span><span>async</span> <span>def</span> <span>bar</span>():
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>100</span>):
</span></span><span><span>        <span>await</span> noop() <span># This await will not make your foo() and bar() interleave.</span>
</span></span><span><span>        print(<span>&#34;bar&#34;</span>)
</span></span><span><span>
</span></span><span><span><span>class</span> <span>Main</span>:
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        self<span>.</span>task_1 <span>=</span> <span>None</span>
</span></span><span><span>        self<span>.</span>task_2 <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>    <span>async</span> <span>def</span> <span>run</span>(self):
</span></span><span><span>        <span># Note: We need to prevent tasks from being garbage-collected.</span>
</span></span><span><span>        self<span>.</span>task_1 <span>=</span> asyncio<span>.</span>create_task(foo())
</span></span><span><span>        self<span>.</span>task_2 <span>=</span> asyncio<span>.</span>create_task(bar())
</span></span><span><span>
</span></span><span><span>main <span>=</span> Main()
</span></span><span><span>
</span></span><span><span>asyncio<span>.</span>run(main<span>.</span>run())
</span></span><span><span><span># Prints 100x foo() then 100x bar()</span></span></span></code></pre></div></div>
  
</div><p>In other words, <code>async</code>/<code>await</code>, by themselves, will <em>not</em> make your execution
non-blocking.</p>
<p>Does Rust <code>async</code>/<code>await</code> make code magically non-blocking?</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="rust"><span><span><span>async</span> <span>fn</span> <span>noop</span>() {
</span></span><span><span>    <span>// Do nothing
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>async</span> <span>fn</span> <span>foo</span>() {
</span></span><span><span>    <span>for</span> _ <span>in</span> <span>0</span><span>..</span><span>100</span> {
</span></span><span><span>        noop().<span>await</span>;
</span></span><span><span>        println!(<span>&#34;foo&#34;</span>)
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>async</span> <span>fn</span> <span>bar</span>() {
</span></span><span><span>    <span>for</span> _ <span>in</span> <span>0</span><span>..</span><span>100</span> {
</span></span><span><span>        noop().<span>await</span>;
</span></span><span><span>        println!(<span>&#34;bar&#34;</span>)
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>#[tokio::main]</span>
</span></span><span><span><span>async</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>    <span>let</span> foo <span>=</span> tokio::task::spawn(foo());
</span></span><span><span>    <span>let</span> bar <span>=</span> tokio::task::spawn(bar());
</span></span><span><span>    <span>let</span> _ <span>=</span> foo.<span>await</span>;
</span></span><span><span>    <span>let</span> _ <span>=</span> bar.<span>await</span>;
</span></span><span><span>}
</span></span><span><span><span>// Prints `foo` and `bar` randomly interleaved.
</span></span></span></code></pre></div></div>
  
</div><p>Victory? Well, not quite. Tokio has detected that the computer has several cores
and uses multi-threading. But what happens if we remove support for multi-threading?
Let’s rewrite <code>main</code>:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>main</span>() {
</span></span><span><span>    <span>// Force the configuration to use a single thread.
</span></span></span><span><span><span></span>    <span>let</span> runtime <span>=</span> tokio::runtime::Builder::new_current_thread()
</span></span><span><span>        .build()
</span></span><span><span>        .unwrap();
</span></span><span><span>    runtime.block_on(<span>async</span> {
</span></span><span><span>        <span>let</span> foo <span>=</span> tokio::task::spawn(foo());
</span></span><span><span>        <span>let</span> bar <span>=</span> tokio::task::spawn(bar());
</span></span><span><span>        <span>let</span> _ <span>=</span> foo.<span>await</span>;
</span></span><span><span>        <span>let</span> _ <span>=</span> bar.<span>await</span>;
</span></span><span><span>    })
</span></span><span><span>}
</span></span><span><span><span>// Prints 100x `foo` then 100x `bar`.
</span></span></span></code></pre></div></div>
  
</div><p>Yup, if we remove support for multi-threading, execution is sequential once
again. So no, in Rust either, <code>async</code>/<code>await</code> won’t make your code magically
non-blocking, although <code>tokio::task::spawn</code> might.</p>
<p>What about JavaScript?</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="js"><span><span><span>async</span> <span>function</span> <span>noop</span>() {
</span></span><span><span>    <span>// Do nothing
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>async</span> <span>function</span> <span>foo</span>() {
</span></span><span><span>    <span>for</span>(<span>let</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>100</span>; <span>++</span><span>i</span>) {
</span></span><span><span>        <span>await</span> <span>noop</span>();
</span></span><span><span>        <span>console</span>.<span>debug</span>(<span>&#34;foo&#34;</span>);
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>async</span> <span>function</span> <span>bar</span>() {
</span></span><span><span>    <span>for</span>(<span>let</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>100</span>; <span>++</span><span>i</span>) {
</span></span><span><span>        <span>await</span> <span>noop</span>();
</span></span><span><span>        <span>console</span>.<span>debug</span>(<span>&#34;bar&#34;</span>);
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>function</span> <span>main</span>() {
</span></span><span><span>    Promise.<span>race</span>([<span>foo</span>(), <span>bar</span>()])
</span></span><span><span>}
</span></span><span><span><span>// Prints `foo` `bar` `foo` `bar` `foo` `bar` ... 100x each.
</span></span></span></code></pre></div></div>
  
</div><p>Wait, what? In JavaScript, <code>async</code>/<code>await</code> makes code execute as concurrently
as one can hope?</p>
<p>Unfortunately, no.</p>
<p>If you recall, I mentioned that JavaScript has a double event loop, with micro-ticks
and ticks. <code>foo</code> and <code>bar</code> return instances of <code>Promise</code> and each call to <code>await</code> is
desugared to a <code>Promise.then()</code>. Some of the early prototypes of <code>Promise</code> had <code>Promise.then</code>
execute the code immediately, but developers were surprised, because they expected
<code>await</code> to, well, sleep. Other of the early prototypes called <code>setTimeout</code>, but this
meant that <code>Promise</code> and <code>async</code>/<code>await</code> could not be used naturally alongside some APIs such
as IndexedDB or Fetch, which committed their operations at the end of the current event, and
this proved also quite surprising for developers. So in the end, the standardized version of
<code>Promise</code> introduce the <em>micro-ticks</em> and <code>Promise.then()</code> automatically enqueues the closure
to be executed at the next micro-tick, but still in the same event. This meant that (unless
developers call <code>setTimeout</code> or wait for I/O), Promises cannot be used to automatically
chunkify work, but also made <code>Promise.then()</code> much faster to execute (and presumably
easier on the garbage-collector, I didn’t benchmark that).</p>
<p>So, in JavaScript <code>async</code>/<code>await</code> will encourage you to think about code as if it
were non-blocking, but it’s also not sufficient to make your code non-blocking.</p>

<p>
  <h2 id="async-and-non-blocking-io">
    Async and non-blocking I/O
  </h2>
</p><p>As mentioned earlier, I/O can be very slow. HTTP calls or database calls can take
unbounded amounts of time, and even disk I/O can slow things down considerably,
especially on network shares.</p>
<p>So far, we have cheated by focusing on a purely mathematical function. But in
real applications, you will need to deal with I/O and other blocking calls.
After all, as mentioned previously, if you call blocking code from asynchronous
code, well, you’re still blocking.</p>
<p>Luckily for you, your favorite async framework will usually provide non-blocking
and ready-to-use async operations for these tasks. Let’s take a look at how these
operations are made non-blocking.</p>
<p>There are typically two cases. In the first case, the operating system or
lower-layer libraries may already provide non-blocking calls for such operations,
e.g. <code>epoll</code>, <code>io_uring</code>, <code>kqueue</code> or I/O Completion Ports. Generally speaking,
these primitives will let applications or libraries:</p>
<div><ol>
<li>(un)register to be informed when an operation is possible;</li>
<li>(un)register to be informed when an operation is complete;</li>
<li>schedule an operation.</li>
</ol></div>
<p>While the specifics are different across these primitives, the
general idea is not dissimilar to what we have shown, for instance,
<a href="#chunks">earlier, when dividing fibonacci in chunks</a>. In fact, the
implementation of <code>async</code>/<code>await</code> in Rust is optimized for such a
<a href="https://doc.rust-lang.org/std/task/struct.Waker.html">wakeup mechanism</a>.</p>
<p>In practice, things are a bit more complicated. In fact, I don’t know of any <code>async</code>/<code>await</code> embedding
on top of <code>io_uring</code> in any language yet, because it doesn’t quite match this model. But
generally, that’s the idea.</p>
<p>In the second case, there is no non-blocking call for such operations. So we have to resort
to threads. In such cases, the framework will:</p>
<div><ol>
<li>request a thread from a thread pool;</li>
<li>dispatch the blocking call to the thread;</li>
<li>once the blocking call is complete, wake the executor with the result.</li>
</ol></div>
<p>In Python, this may look like:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="py"><span><span><span>class</span> <span>NonBlockingIOManager</span>:
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        self<span>.</span>pool <span>=</span> ThreadPoolExecutor()
</span></span><span><span>
</span></span><span><span>    <span>async</span> <span>def</span> <span>file_read</span>(self, path: str) <span>-&gt;</span> str:
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Non-blocking file read.
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>def</span> <span>task</span>():
</span></span><span><span>            <span>with</span> open(path) <span>as</span> file:
</span></span><span><span>                <span>return</span> file<span>.</span>read()
</span></span><span><span>        <span>return</span> <span>await</span> asyncio<span>.</span>get_event_loop()<span>.</span>run_in_executor(
</span></span><span><span>            self<span>.</span>pool,
</span></span><span><span>            task)</span></span></code></pre></div></div>
  
</div><p>This is not ideal, but in the absence of a better solution, it works.</p>
<p>And in fact, in JavaScript, <code>Promise</code> was designed to help integrate results
coming from multiple threads/processes in such a manner (but without dealing
with threads itself).</p>

<p>Go is a bit of an outlier, and one of the few programming languages that do not
provide any kind of support for <code>async</code>/<code>await</code>, or any of the methods explored
so far. Nevertheless, this language is considered top-of-the-class (by some criteria)
for concurrent programming and web servers.</p>
<p>Where Python, C#, Rust, JavaScript or (so far) Java have decided to make user-level
concurrency <em>explicit</em> by relying on <code>async</code>/<code>await</code> or lower-level constructions,
Go’s designers have decided to make it <em>implicit</em> by provided a transparent M:N
scheduler.</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>fibonacci</span>(<span>n</span> <span>uint32</span>) <span>uint64</span> {
</span></span><span><span>    <span>if</span> <span>n</span> <span>&lt;=</span> <span>1</span> {
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    }
</span></span><span><span>    <span>// (probably) no need to sleep manually
</span></span></span><span><span><span></span>    <span>return</span> <span>fibonacci</span>(<span>n</span><span>-</span><span>1</span>) <span>+</span> <span>fibonacci</span>(<span>n</span><span>-</span><span>2</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>type</span> <span>FibonacciEvent</span> <span>struct</span> {
</span></span><span><span>    <span>Arg</span> <span>uint32</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> <span>main</span>() {
</span></span><span><span>    <span>for</span> {
</span></span><span><span>        <span>e</span> <span>&lt;-</span> <span>nextEvent</span>;
</span></span><span><span>        <span>switch</span> <span>event</span> <span>:=</span> <span>e</span>.(<span>type</span>) {
</span></span><span><span>            <span>case</span> <span>FibonacciEvent</span>:
</span></span><span><span>                <span>go</span> <span>fibonacci</span>(<span>event</span>.<span>Arg</span>)
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}</span></span></code></pre></div></div>
  
</div><p>How does it work?</p>
<div><ol>
<li>The Go environment contains a pool of threads and pre-emptive scheduler.</li>
<li>Launching a goroutine with <code>go</code> allocates dynamically a new stack for this goroutine.</li>
<li>A goroutine runs on a thread from the thread pool.</li>
<li>The Go scheduler <em>may</em> stop a goroutine at any moment, saving its program counter, task-local storage, etc. and replacing them with those of another goroutine.</li>
<li>During compilation of C calls, the CGo compiler instruments the (non-blessed) C code
with a little assembly that calls into the runtime environment.</li>
<li>At runtime, when calling a C function, Go’s executor expects that the call is blocking,
monitors how long the thread is blocked by the C function, and past some delay, removes
the thread from the thread pool and replaces it with a new thread. Once execution of the
C function has completed, the thread is not returned to the thread pool.</li>
</ol></div>
<p>The immediate benefit comes in terms of resource usage/cognitive load. You can launch as many
tasks (“goroutines”) as you want, without having to deal with <code>async</code>/<code>await</code> or care about
memory usage, and these tasks will be executed concurrently, hopefully in parallel. Moreover,
Go functions are not colored by synchronicity. This means that interfaces or callbacks don’t
need to care whether they accept sync or async implementations, as there is no difference.</p>
<p>The immediate drawback is that, since concurrency is, to a large extent, implicit, it makes the
code harder to reason about. I don’t think that this is a real problem, but I’ve seen it lead to developers
writing concurrency-unsafe code and running it concurrently without paying attention. I think it’s
more a problem of education than language.</p>
<p>One could argue that implicit concurrency makes it harder to optimize code, but since very
few developers actually care about such level of code optimization, and since Go is one of
the fastest languages around, I consider this a minor impediment. Of course, if you need
more performance, use Rust.</p>
<p>There are real issues with goroutines, such as the implicit capture by reference or the ease of
accidentally copying a lock instead of sharing it, but these are not due to the concurrency model
or implementation.</p>

<p>
  <h2 id="other-languages-with-mn-scheduling">
    Other languages with M:N scheduling
  </h2>
</p><p>Now, I mention that Go is an outlier, but it’s by no mean the only language with M:N scheduling.
Erlang has been the poster child for M:N scheduling since the 90s, Haskell has supported it since
the early 2000s, Rust used to support it but removed the feature before 1.0, and Java is moving
towards supporting it. I believe that OCaml supports it, too, but I’m still investigating that.</p>
<p>Rust <em>removing</em> the feature is an interesting case for why M:N scheduling is not always the solution.</p>
<p>This happened for a variety of reasons. It took some time for Rust to decide what kind of language it was. Initial versions of Rust offered garbage-collection, a M:N scheduler, built-in support for real-time programming.
Each of these features was convenient, but made Rust harder to maintain or port to new
architectures.</p>
<p>In particular, as it became clear that Rust was a really good language to write very low-level code,
including firmware, memory allocators, embedded code and OS code, these features became blockers, as
they relied on having an operating system and an allocator in the first place. To make Rust a true
system language, these features had to leave (and be turned into libraries).</p>
<p>In addition, the Rust ethos prefers explicit costs to implicit ones. Many languages claim that (including
Go and Python), but few languages actually follow up on this design principle
(the ones I can think of are Rust, Zig, and
of course C and C++). Having a M:N scheduler requires allocating and growing stacks implicitly, which
goes against this ethos: allocating memory has a cost, imposes a choice of allocator on the user
(which would make Rust much less usable for e.g. browser engines or game engines), and may fail.</p>
<p>Not only that, but Rust is designed to both call into C and be callable from C
transparently and at zero implicit cost and zero implicit risk. However, to be able to call into C
without blocking, you need a machinery comparable to what CGo provides. This machinery requires,
once again, implicit allocations, hence hidden costs and hidden points of failure.</p>
<p>Also, M:N scheduling simply made no sense on some architectures. For instance, making M:N work
doesn’t make sense on architectures that do not support dynamic memory allocation or (embedded)
operating systems that do not support native threads.</p>
<p>And finally, M:N scheduling is complicated and was holding the language back. So the decision was made
to move M:N scheduling into a crate called <code>libgreen</code>. Then, as work on <code>Future</code>, then <code>async</code>/<code>await</code>
advanced, the Rust community decided that <code>async</code>/<code>await</code> served much better the Rust ethos of explicit
costs than M:N scheduling, and decision was taken to drop the feature.</p>
<p>None of this means that M:N scheduling is bad, incidentally. But it does illustrate that it is not
always a desired feature.</p>

<p>OCaml is a very interesting case. It has supported the equivalent of Go’s channels since the
late 1990s and, with version 5.0, it finally gained support for multicore. Recent benchmarks
suggest that it can be actually much faster than Go on some tasks (I didn’t take the time to
check these benchmarks, so don’t trust me on this).</p>
<p>In OCaml, using the Eio executor, our Fibonacci would look like:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>let</span> <span>rec</span> fibonacci <span>(</span>n<span>:</span><span>int</span><span>)</span> <span>=</span>
</span></span><span><span>  <span>if</span> n <span>&lt;=</span> 1 <span>then</span> <span>(</span>
</span></span><span><span>    1
</span></span><span><span>  <span>)</span> <span>else</span> <span>(</span>
</span></span><span><span>    Fiber.yield()<span>;</span> <span>(* take a break *)</span>
</span></span><span><span>    fibonacci <span>(</span>n <span>-</span> 1<span>)</span> <span>+</span> fibonacci <span>(</span>n <span>-</span> 2<span>)</span>
</span></span><span><span>  <span>)</span></span></span></code></pre></div></div>
  
</div><p>this implementation sits somewhere Go’s and Python/Rust/JavaScript. As Go, it doesn’t need <code>async</code>
or <code>await</code>. As Python/Rust/JavaScript/C++, it expects an explicit operation (here <code>Fiber.yield()</code>)
to allow cooperative context-switching.</p>
<p>But in fact, OCaml is its own beast. For one thing, as far as I understand, the OCaml compiler
doesn’t go through any compilation step specific to multi-tasking, either to compile <code>async</code>/<code>await</code>
into <code>yield</code> or <code>Promise</code> or anything similar, or to introduce implicit cooperative context-switching.
In fact, the above code can run, without change or recompilation, either as a backgrounded, non-blocking
task, or as a blocking task, depending on the context.</p>
<p>How does this work?</p>
<p>Well, the implementation of <code>Fiber.yield()</code> actually raises an <em>effect</em>. Effects are almost identical
to exceptions, with one key difference: where exception handlers can either handle the exception or
let it propagate, effect handlers can also decide to <em>resume</em> the work at the site the effect was
raised. In a way, effects are a generalization of both exceptions (they add the ability to resume)
and generators (they add the ability to cross an arbitrary stack depth).</p>
<p>This is an extremely powerful mechanism that can be used to provide context-customizable retries,
logging, mockable I/O, etc. and of course concurrency. For instance, we could write:</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>try</span> some_function() <span>with</span>
</span></span><span><span><span>|</span> effect <span>Yield</span> continuation <span>-&gt;</span> <span>(</span>
</span></span><span><span>    <span>(* Proceed in the next tick *)</span>
</span></span><span><span>    enqueue_event <span>(</span><span>Continue</span> continuation<span>)</span>
</span></span><span><span><span>)</span></span></span></code></pre></div></div>
  
</div><p>or, if we don’t want concurrency</p>
<div>
  
  
  
  <div><div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>try</span> some_function() <span>with</span>
</span></span><span><span><span>|</span> effect <span>Yield</span> continuation <span>-&gt;</span> <span>(</span>
</span></span><span><span>    <span>(* Proceed immediately *)</span>
</span></span><span><span>    continuation ()
</span></span><span><span><span>)</span></span></span></code></pre></div></div>
  
</div><p>… and we could even have both on the same stack to force a function to run without concurrency
locally (e.g. for performance reasons) despite running in a concurrent or even parallel executor.</p>
<p>It’s a very interesting mechanism that I’m still test-driving. I’m not sure about the performance
implications of the continuation. As far as I understand, context-switching between two fibers
is (or can be) as fast as in Go, but stack management is more complicated, as the code executed
in the <code>try</code> needs its stack, the code executed in the <code>effect</code> expression needs its stack, both
of these stacks stack on top of the code that calls the entire <code>try ... with ...</code> expression, and
<code>continuation()</code> require its own stack.</p>
<p>There’s also the issue that, much like exceptions, effects don’t show up in the type
of a function, which can lead to accidentally uncaught effects. Maybe an updated <code>ocamlexn</code> could
solve that?</p>
<p>Note: As pointed out by /u/phischu on Reddit, there are of course several more experimental
languages that expand this feature, including Lexa and Effekt.</p>


<p>
  <h2 id="threads">
    Threads
  </h2>
</p><div><ul>
<li>Threads are good, but tricky to use. By all means, please learn how to use them correctly and safely.</li>
<li>On most platforms, threads are needed at some level to handle CPU-heavy tasks.</li>
<li>On most platforms, threads are needed at some level to make some I/O non-blocking.</li>
<li>If you are in a position where millisecond matters, you will need to profile your thread switches.</li>
<li>Don’t forget to check whether you are constrained by a GIL.</li>
</ul></div>

<p>
  <h2 id="processes">
    Processes
  </h2>
</p><div><ul>
<li>Processes are good, but heavy.</li>
<li>You may need to use them for security/sandboxing/crash isolation.</li>
<li>They can be useful for performance, typically in GIL-constrained languages,
but the cost of communications can hurt a lot, so you’ll have to benchmark
carefully.</li>
</ul></div>

<p>
  <h2 id="chunkification">
    Chunkification
  </h2>
</p><div><ul>
<li>Chunkification makes your code really hard to read.</li>
<li>Chunkification always makes your code slower.</li>
<li>You can often avoid doing it manually, e.g. by using <code>async</code>/<code>await</code><sup id="fnref1:6"><a href="#fn:6" role="doc-noteref">6</a></sup>.</li>
</ul></div>

<p>
  <h2 id="asyncawait-1">
    Async/await
  </h2>
</p><div><ul>
<li><code>async</code>/<code>await</code> is useless if you perform blocking calls, whether they’re blocking on CPU or on I/O.</li>
<li>if you own the CPU-bound blocking code, you can use <code>async</code>/<code>await</code> to chunkify it cleanly into non-blocking code.</li>
<li>otherwise, the only generic way to turn blocking calls into non-blocking calls is to use threads (or sometimes, processes).</li>
<li>once you have non-blocking I/O, <code>async</code>/<code>await</code> shines at using your CPU efficiently in tasks that are constrained by I/O.</li>
<li><code>async</code>/<code>await</code> is function coloring, which may prevent you from using it.</li>
</ul></div>

<p>
  <h2 id="mn-scheduler-and-variants">
    M:N scheduler and variants
  </h2>
</p><div><ul>
<li>I can’t think of any good reason to not take advantage of a M:N scheduler if it’s available.</li>
<li>The risks are almost identical to multi-threading, so please take time to learn how to write thread-safe code.</li>
</ul></div>

<p>I hope that these few pages of code have helped clarify why <code>async</code>/<code>await</code> was designed, what
it is good for and what it won’t do for you, as well as a few alternatives.</p>
<p>If I find the opportunity, I’ll try and write a followup with benchmarks.</p>

          </div></div>
  </body>
</html>
