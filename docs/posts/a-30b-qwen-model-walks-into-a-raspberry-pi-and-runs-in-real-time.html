<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/">Original</a>
    <h1>A 30B Qwen model walks into a Raspberry Pi and runs in real time</h1>
    
    <div id="readability-page-1" class="page"><article id="blog-content">
      

      <div>
        <section>
          <p>
            For this release, we optimize for what people actually experience when they run a model: 
            <strong>fast, high-quality responses on a specific target device</strong>.
          </p>

          <p>
            We use Shapelearn, our bitlength learning method to choose weight datatypes for 
            <strong>Qwen3-30B-A3B-Instruct-2507</strong> that maximize performance in terms of 
            tokens per second (TPS) and output quality, with one practical constraint: the model 
            must fit comfortably in the available memory. Once it fits, making the file smaller 
            isn&#39;t a goal by itself. We only shrink further when it also improves the real tradeoff 
            people care about: <strong>speed vs. quality</strong>.
          </p>

          <p>
            Approaching bitlength learning this way matters because in llama.cpp, &#34;fewer bits&#34; doesn&#39;t 
            automatically mean &#34;more speed.&#34; Different quantization formats can trigger different kernels 
            and overheads, and on some GPUs, <strong>going lower-bit can even get slower</strong>, despite 
            using less memory.
          </p>

          <p>
            <strong>Bottom line:</strong> treat memory as a <strong>budget</strong> to meet, then optimize what 
            matters most: <strong>TPS and quality</strong>.
          </p>
        </section>

        

        <section>
          <h2>TL;DR</h2>
          <p>
            Yes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB),
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                Q3_K_S-2.70bpw [KQ-2]
              </a>
            </code>
            hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels
            real-time. More broadly, the same pattern shows up everywhere else: ByteShape models
            give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth
            and MagicQuant).
          </p>
        </section>

        <section>
          <h2>CPUs</h2>
          <p>
            On CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy
            tradeoff as one would expect: once the model fits, reducing footprint tends to increase
            TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit
            of quality for speed predictably, which makes it much easier to pick a point on the
            curve that matches your constraints.
          </p>

          <p>
            We&#39;ll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where
            &#34;fits in RAM&#34; is the limiting factor, then move to an Intel i7 with 64GB, where
            everything fits.
          </p>

          <h3>Raspberry Pi 5</h3>
          <p>
            The figure below shows TPS vs. normalized accuracy for the models that fit in RAM on
            the Raspberry Pi 5 16GB.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_pi.png" alt="Raspberry Pi 5 performance: tokens per second vs quality"/>
            <figcaption>
              Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Notably, sustaining <strong>8.5 TPS at 92%+ baseline accuracy</strong> with a 30B model on a 
            Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn 
            consistently produces better models, with ByteShape trending up and to the right of Unsloth, 
            achieving higher tokens per second at the same quality, or higher quality at the same throughput.
          </p>

          <p>We highlight choices for two primary objectives: accuracy or response time.</p>

          <ul>
            <li>
              <strong>Optimizing for response time while maintaining accuracy:</strong> For interactive, 
              on-device use, perceived responsiveness is driven by how quickly text appears, not peak 
              throughput. In practice, generation feels real-time once it reaches roughly <strong>8 TPS</strong>, 
              comfortably above typical reading speed. In this Raspberry Pi real-time regime, 
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                  Q3_K_S-2.70bpw [KQ-2]
                </a>
              </code>
              (2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the
              real-time threshold while maintaining high accuracy. Compared 
              to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW 
              and higher TPS, making it the more efficient choice for interactive edge deployment.
            </li>
            <li>
              <strong>Accuracy above all:</strong> The table below lists the models that achieve the highest 
              accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models 
              make the best use of the available resources to maximize accuracy, occupying the 
              <strong>lowest-error rows</strong> (~1.1–1.3% relative error, ~98.8% accuracy), while the 
              strongest Unsloth entries remain around 2.1–2.2% error (~97.9% accuracy). Compared to Unsloth&#39;s 
              <code>UD-Q3_K_XL [8]</code>, ByteShape achieves up to a <strong>1.87× lower error rate</strong> 
              while still operating at <strong>~5–6 TPS</strong>, comfortably within TPS-norms on Raspberry PI 
              making it the better choice when accuracy is the priority. </li>
          </ul>

          <table>
            <thead>
              <tr>
                <th>Model</th>
                <th>Relative Error</th>
                <th>BPW</th>
                <th>TPS</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.92bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q4_K_S-3.92bpw [KQ-7]
                    </a>
                  </code>
                </td>
                <td>1.14%</td>
                <td>3.92</td>
                <td>5.30</td>
              </tr>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.61bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q4_K_S-3.61bpw [KQ-6]
                    </a>
                  </code>
                </td>
                <td>1.25%</td>
                <td>3.61</td>
                <td>5.94</td>
              </tr>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q3_K_S-3.25bpw [KQ-5]
                    </a>
                  </code>
                </td>
                <td>2.03%</td>
                <td>3.25</td>
                <td>6.68</td>
              </tr>
              <tr>
                <td><code>UD-IQ3_XXS [6]</code></td>
                <td>2.22%</td>
                <td>3.38</td>
                <td>5.03</td>
              </tr>
              <tr>
                <td><code>UD-Q3_K_XL [8]</code></td>
                <td>2.13%</td>
                <td>3.62</td>
                <td>6.28</td>
              </tr>
            </tbody>
          </table>

          <p>
            Many other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare 
            them in other sections, but they&#39;re not applicable in the Raspberry Pi case. They simply don&#39;t fit!
          </p>

          <h3>Intel i7</h3>
          <p>
            Next, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs 
            normalized accuracy for all models.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_i7.png" alt="Intel i7 performance: tokens per second vs quality"/>
            <figcaption>
              Intel i7: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              Intel i7: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Overall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at 
            comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in 
            the 26+ TPS range, extending performance well beyond the other methods.
          </p>

          <p><strong>Highlights:</strong></p>

          <ul>
            <li>
              <strong>Quality-first:</strong> At the high-accuracy end of the table,
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf" target="_blank" rel="noopener noreferrer">
                  IQ4_XS-4.67bpw [KQ-9]
                </a>
              </code>
              achieves the lowest relative error (0.25%), outperforming the best-running Unsloth
              models (<code>Q6_K [20]</code> and <code>Q5_K_M [18]</code> whose 
              relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44× lower 
              error rate with higher throughput than <code>Q6_K [20]</code>, and a 1.76× lower error rate at 
              essentially the same speed as <code>Q5_K_M [18]</code>. MagicQuant <code>mxfp4 [3]</code> trails 
              in this regime, with both higher error and lower TPS.
            </li>
            <li>
              <strong>Balanced point:</strong> In the mid-accuracy, high-throughput region,
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf" target="_blank" rel="noopener noreferrer">
                  Q3_K_S-3.25bpw [KQ-5]
                </a>
              </code>
              combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall
              balance in the table. Matching or exceeding this accuracy with Unsloth 
              (<code>IQ4_XS [10]</code>) requires higher BPW and lower TPS, while choosing an Unsloth model 
              closer in speed (<code>Q3_K_S [7]</code>) incurs a 1.73× higher error rate. MagicQuant does not 
              offer a competitive model in this range; its fastest entry (<code>IQ4_NL [2]</code>) is behind both 
              ByteShape and Unsloth in accuracy and throughput.
            </li>
          </ul>

          <p>
            <strong>Takeaway:</strong> Across both quality-first and balanced settings, ByteShape consistently 
            converts the available bit budget into either higher accuracy or higher TPS, and is the only approach 
            that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.
          </p>
        </section>

        <section>
          <h2>GPUs: RTX5090/32GB and RTX4080/16GB</h2>
          <p>
            On GPUs, performance depends as much on <strong>kernel choice</strong> as on raw memory footprint. 
            For matmul/matvec, llama.cpp&#39;s quantization-specific GPU decode paths incur very different overheads, 
            so fewer bits per weight do <strong>not</strong> reliably translate to higher TPS. Instead, TPS often 
            peaks at quantization-specific sweet spots. Pushing BPW lower can even <strong>increase VRAM traffic 
            and instruction count</strong>, hurting performance rather than improving it. We dig into this behavior in 
            more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.
          </p>

          <p>
            We evaluate on two GPUs: an <strong>RTX 5090 (32 GB)</strong>, which can run models 
            <strong>above 4 BPW</strong> and typically reach the fastest sweet spots, and an 
            <strong>RTX 4080 (16 GB)</strong>, where <strong>&gt;4 BPW models do not fit</strong>, forcing 
            different trade-offs and making the device-optimized curve easier to see.
          </p>

          <h3>RTX 5090 (32GB of VRAM)</h3>
          <p>
            Let&#39;s start with the 5090, which has enough VRAM to support all of the quantized models. The figure 
            below shows TPS vs normalized accuracy.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_5090.png" alt="RTX 5090 performance: tokens per second vs quality"/>
            <figcaption>
              RTX 5090: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              RTX 5090: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Two things stand out immediately:</p>

          <p>
            Second, outside of that sweet spot, the tradeoff becomes much more uneven:
          </p>

          <ul>
            <li>
              Many other Unsloth and Magic Quant models show <strong>significantly lower TPS</strong>, regardless 
              of whether they are quantized more or less aggressively.
            </li>
            <li>
              Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality.
            </li>
          </ul>

          <p>
            <strong>Accuracy-critical workloads:</strong> when output quality is paramount,
            ByteShape delivers the most accurate model on the 5090:
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf" target="_blank" rel="noopener noreferrer">
                IQ4_XS-4.67bpw [IQ-8]
              </a>
            </code>
            (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses <code>Unsloth Q6_K [20]</code> (6.57 BPW, 264.88 TPS, 99.64% accuracy) 
            while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant 
            <code>mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]</code> (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both 
            accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.
          </p>

          

          <h3>RTX 4080 (16GB of VRAM)</h3>
          <p>
            Next, let&#39;s move to a more accessible GPU, especially in these memory-challenged times. The biggest 
            stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the &#34;magical&#34; 
            ~4b quantizations for a 30B model. How convenient!<strong> </strong>This &#34;avoids&#34; the 5090&#39;s ~4b sweet 
            spot and forces a more &#34;real-world&#34; comparison under a hard VRAM budget. The figure below shows TPS versus 
            normalized accuracy for all models that fit on the 4080.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_4080.png" alt="RTX 4080 performance: tokens per second vs quality"/>
            <figcaption>
              RTX 4080: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              RTX 4080: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            On the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, 
            delivering a better TPS–quality tradeoff.
          </p>

          <p>
            In particular, ByteShape&#39;s highest-quality model that fits,
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.87bpw.gguf" target="_blank" rel="noopener noreferrer">
                IQ4_XS-3.87bpw [IQ-6]
              </a>
            </code>
            (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:
          </p>

          <ul>
            <li>
              a 1.59× lower error rate and 9.4% higher TPS vs. <code>Unsloth Q3_K_XL [8]</code> (3.62 BPW, 196.42 TPS, 
              97.87% accuracy).
            </li>
            <li>
              a 2.54× lower error rate at the same TPS vs. <code>Unsloth IQ2_M [2]</code> (2.84 BPW, 214.79 TPS, 
              96.59% accuracy).
            </li>
          </ul>

          <p>
            As we move to higher throughput, ByteShape&#39;s maintains accuracy, while Unsloth&#39;s error rate experiences a cliff.
          </p>

          <h3>The Elephant in the Room: When 3-bits is not just 3-bits</h3>
          <p>
            There is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already 
            flying, and pushing quantization harder does not make things faster. It just manages to be smaller 
            and slower at the same time.
          </p>

          <p>
            Reducing the size of data doesn&#39;t automatically speed things up. While using fewer bits to store each 
            number seems like it should reduce memory traffic and speed up computation, GPUs don&#39;t work that way. 
            NVIDIA GPUs process work in fixed groups of 32 threads called &#34;warps,&#34; which move through instructions 
            together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, 
            and operations that the chip&#39;s circuits are physically designed to handle efficiently. When your workload 
            matches these &#34;golden paths&#34;, you get peak performance. Step outside them, and you hit slowdowns. This isn&#39;t 
            a design flaw, it&#39;s a deliberate tradeoff. Supporting more flexibility would require additional circuitry: 
            more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to 
            every operation, whether a program needs that flexibility or not.
          </p>

          <p>
            Here a few examples of relevant hardware &#34;quirks&#34;: VRAM is read in aligned 32-byte blocks, so reading one 
            or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention 
            depending on how data is laid out, meaning that a warp&#39;s accesses may complete in a single step or, in the 
            worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can 
            require extra instructions, with the cost depending on the quantization scheme.
          </p>

          <p>
            This explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit 
            kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword 
            parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than 
            shared memory to take advantage of data reuse when possible.
          </p>

          <p>
            So why llama.cpp hasn&#39;t been optimized to deliver peak speed for <strong>every</strong> bit-length? Our understanding 
            is that llama.cpp prioritizes <strong>portable, space-efficient quantization</strong> that can run across a 
            wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder 
            computation in ways that might help one GPU or one bit-width.
          </p>

          <p>
            A key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is 
            self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the 
            tensor, which makes the format easy to implement and fast to locate.
          </p>

          <p>
            The tradeoff is that GPUs often need to <strong>decode many blocks in parallel</strong> to keep their wide 
            compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more 
            scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for 
            some lower-bit formats.
          </p>

          <p>
            <strong>Point for example on RTX 5090:</strong> a matrix multiply [256, 768] × [768, 2048] takes 
            <strong>~54µs with </strong><code>iq4_xs</code><strong> </strong>datatype, but <strong>~62µs with </strong>
            <code>iq3_xxs</code><strong> </strong>(mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, 
            <strong>cutting nearly 1.2 bits per weight </strong>(a reduction of more than 25% in weight footprint) leads 
            to a <strong>~13% slowdown</strong>, directly hurting user experience.
          </p>

          <p>
            An excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all 
            the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.
          </p>
        </section>

        <section>
          <h2>Methodology (brief recap)</h2>
          <p>
            If you&#39;re wondering how we are scoring these points, the full methodology is discussed in our previous 
            <a href="https://byteshape.com/blogs/Qwen3-4B-I-2507/">blog post</a>. This post is intentionally focused on the curves and 
            device tradeoffs, so here is the quick version.
          </p>

          <p>
            For each quantized variant, we measure <strong>throughput (TPS)</strong> on the target device and 
            compute a single <strong>normalized quality</strong> score relative to the <strong>BF16 baseline</strong>, 
            using the same evaluation harness and prompts as the methodology post. The quality score aggregates 
            standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points 
            directly. In other words, every dot in the plots answers two questions: how fast does it run on this 
            device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.
          </p>

          <p>
            We also want to thank all for the many, excellent suggestions on our recent Reddit post for improving 
            and extending this evaluation strategy, and we’re actively working through them. Right now, evaluation 
            is the main bottleneck and not bitlength learning/quantization.
            Careful evaluation is essential to clearly communicate the strengths of each model.
          </p>
        </section>

        <section>
          <h2>Wrapping up</h2>
          <p>
            <strong>First, thank you for your tenacity.</strong> You made it through all of this without giving up. 
            We are sincerely flattered!
          </p>

          <p>
            <strong>The takeaway is simple:</strong> treat <strong>memory as a constraint, not a goal</strong>. Once 
            a model fits on your device, what matters is the tradeoff curve, <strong>TPS versus quality</strong>. 
            Across CPUs and GPUs, <strong>ByteShape</strong> consistently lands on the better side of that curve, 
            delivering either <strong>more speed at the same quality</strong> or <strong>higher quality at the same 
            speed</strong>.
          </p>

          <p>
            If you&#39;re deploying on a <strong>Raspberry Pi 5 (16 GB)</strong> and want a genuinely
            interactive experience, start with
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                Q3_K_S-2.70bpw [KQ-2]
              </a>
            </code>
            . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with
            little loss in throughput, the same rule applies:
            <strong>fit first, then optimize the tradeoff</strong>.
          </p>
          

          <p>
            We&#39;ll keep releasing more device-targeted variants (and more plots). If your system can&#39;t run a 30B 
            model smoothly, don&#39;t blame the model or the silicon. <strong>Blame the datatypes.</strong>
          </p>
        </section>
      </div>
    </article></div>
  </body>
</html>
