<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html">Original</a>
    <h1>AI Decodes the Calls of the Wild</h1>
    
    <div id="readability-page-1" class="page"><article id="article" role="main">
      <header id="section-Lead-rAPgV4vOO7" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsInRleHRGYWRlIjoibm9uZSIsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div data-scrollymoly-basis="true" data-testid="ENG-5457">
            <div>
              <div>
                <p>
                  
                  <h3>Artificial intelligence could reveal how animals of the land, sea and sky talk to others of their species. <strong>By Neil Savage</strong></h3>
                </p>
              </div>
            </div>
          </div>
          
        </div>
        
      </header>
      <div id="section-udq87PSGAo" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>Listening to sperm whales has taught Shane Gero the importance of seeing the animals he studies as individuals, each with a unique history.</p>
                    <p>He and his fellow scientists give the whales names — Pinchy, Quasimodo, Scar, Mysterio and Mysterio’s son Enigma. Often these names are based on some identifying physical feature. Fingers, for instance, is named after a pair of marks on her right fluke that look like she’s flashing a peace sign.</p>
                    <p>The scientists name the animals to remind themselves that the whales are not interchangeable. Gero’s children are learning this, too. “My kids know all of the animals by name,” he says. “I jokingly call them my ‘human family’, as opposed to the time I spend with my whale families.”</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-0gWG8iZDHU" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ=="><sh-background-transition data-lazyload-container="true" data-lazyload-trigger="true" data-transitions="1 fade;1 fade" classname="Layer--one Theme-Layer-BackgroundCanvas">
          
          
          
          <div slot="foreground">
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>
                        <div>
                          <p>Gero, a whale biologist at Carleton University in Ottawa, Canada, has spent 20 years trying to understand how whales communicate. In that time, he has learnt that whales make specific sounds that identify them as members of a family group, and that sperm whales (<em>Physeter macrocephalus</em>) in different regions of the ocean have dialects, just as people from various parts of the world might speak English differently.</p>
                          <p><span>Image credit: Project CETI</span></p>
                        </div>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>
                        <div>
                          <p>The chirps and whistles of dolphins, the rumblings of elephants and the trills and tweets of birdsong all have patterns and structure that convey information to other members of the animal’s species. For a person, the subtleties of these patterns can be difficult to identify and understand, but finding patterns is a task at which artificial intelligence (AI) excels. The hope of a growing number of biologists and computer scientists is that applying AI to animal sounds might reveal what these creatures are saying to each other.</p>
                          <p>Over the past year, AI-assisted studies have found that both African savannah elephants (<em>Loxodonta africana</em>)<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>1</sup></a> and common marmoset monkeys (<em>Callithrix jacchus</em>)<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>2</sup></a> bestow names on their companions. Researchers are also using machine-learning tools to map the vocalizations of crows. As the capability of these computer models improves, they might be able to shed light on how animals communicate, and enable scientists to investigate animals’ self-awareness — and perhaps spur people to make greater efforts to protect threatened species.</p>
                          <p><span>Image credit: Amanda Cotton/Project CETI</span></p>
                        </div>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>
                        <div>
                          <p>That’s not to say that there will be an animal version of Google Translate any time soon. The great progress seen in AI systems’ understanding, translation and generation of human language is mainly due to the vast quantity of examples available, the meanings of which are already known, says David Gruber, a marine microbiologist who founded the scientific and conservation project the <a href="https://www.projectceti.org/"><span>Cetacean Translation Initiative (CETI)</span></a>. “I think it’s a big assumption to assume that we could take all that technology and just turn it towards another species and have it somehow learn and start translating,” he says.</p>
                          <p><span>Image credit: Michael Lees/National Geographic/Via Project CETI</span></p>
                        </div>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
        </sh-background-transition></div>
      
      <div id="section-ayYLvx7ahN" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>At the ocean surface, sperm whales (<em>Physeter macrocephalus</em>) use a sequence of clicks to communicate with each other. Credit: Project CETI</span></p>
                        <p><span>At the ocean surface, sperm whales (<em>Physeter macrocephalus</em>) use a sequence of clicks to communicate with each other. Credit: Project CETI</span></p>
                      </figcaption>
                    </figure>
                    
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>The CETI project focuses on sperm whales and has become a sponsor of Gero’s research. But even before CETI, Gero had spent thousands of hours in the waters of the Caribbean leading the Dominica Sperm Whale Project, in which he and his colleagues collected data on more than 30 whale families that live near the island.</p>
                    <p>The whales spend most of their time seeking food deep in the ocean, as far as 2 kilometres below the surface. Sunlight doesn’t penetrate to those depths, so the whales make clicking sounds to find their prey by echolocation. They also use sequences of clicks called codas, which are each 3–40 clicks long, to stay in touch with other whales. At the surface, where they don’t need echolocation, whales use the codas during socialization.</p>
                    <p>Gero and other researchers have learnt that whales group together in what they have dubbed clans, each with a distinctive diet, social behaviour and use of their habitat. These clans, which can contain thousands of individuals in families led by female whales, communicate in their own dialects, which are distinguishable from those of others by the tempos of their codas. For instance, two clans will use the same pattern of five clicks in succession, but with different tempos and pauses. Those dialects, Gero says, mark “cultural boundaries” between clans.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-ycUNoyQie1" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>The CETI project focuses on sperm whales and has become a sponsor of Gero’s research. But even before CETI, Gero had spent thousands of hours in the waters of the Caribbean leading the Dominica Sperm Whale Project, in which he and his colleagues collected data on more than 30 whale families that live near the island.</p>
                    <p>The whales spend most of their time seeking food deep in the ocean, as far as 2 kilometres below the surface. Sunlight doesn’t penetrate to those depths, so the whales make clicking sounds to find their prey by echolocation. They also use sequences of clicks called codas, which are each 3–40 clicks long, to stay in touch with other whales. At the surface, where they don’t need echolocation, whales use the codas during socialization.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>At the ocean surface, sperm whales (<em>Physeter macrocephalus</em>) use a sequence of clicks to communicate with each other. Credit: Project CETI</span></p>
                        <p><span>At the ocean surface, sperm whales (<em>Physeter macrocephalus</em>) use a sequence of clicks to communicate with each other. Credit: Project CETI</span></p>
                      </figcaption>
                    </figure>
                    <p>Gero and other researchers have learnt that whales group together in what they have dubbed clans, each with a distinctive diet, social behaviour and use of their habitat. These clans, which can contain thousands of individuals in families led by female whales, communicate in their own dialects, which are distinguishable from those of others by the tempos of their codas. For instance, two clans will use the same pattern of five clicks in succession, but with different tempos and pauses. Those dialects, Gero says, mark “cultural boundaries” between clans.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div id="section-zmVXgJh0AV" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>The CETI team used hydrophones to record sound beneath the surface. Credit: Marissa Velez/Project CETI</span></p>
                        <p><span>The CETI team used hydrophones to record sound beneath the surface. Credit: Marissa Velez/Project CETI</span></p>
                      </figcaption>
                    </figure>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        
                        
                      </figcaption>
                    </figure>
                    <p><span><strong>“We think that that’s like saying, </strong>‘<strong>I am from Dominica, are you?</strong>’<strong>”</strong></span></p>
                    
                    <figure>
                      <div>
                        <div>
                          
                        </div>
                      </div>
                      <figcaption></figcaption>
                    </figure>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>To understand the rhythm and tempo of the codas, the team manually created graphic representations of whale sound recordings, known as spectrograms. These provide a way to visualize sound, depicting characteristics such as volume and frequency. For people, they can be used to identify individual units of speech known as phonemes.</p>
                    <p>The process was time-consuming; every minute of recording took a team member roughly 10 minutes to separate out all the individual clicks. Turning that task over to a machine-learning algorithm sped up the work vastly, Gero says, and also helped to separate which sound came from which animal.</p>
                    <p>But AI also allowed the researchers to go further. Manually, they had essentially been cataloguing individual words, but AI allowed them to look at the codas in the equivalent of whale sentences and even entire conversations. New structures began to emerge. “Machine learning is really good at seeing patterns that are hard to pick up through standard statistical approaches,” Gero says.</p>
                    <p>The work<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>3</sup></a> uncovered fine modulations of the intervals between clicks, which the scientists labelled ‘rubato’, borrowing a musical term for slight changes in tempo that make a piece more expressive. They also discovered the occasional addition of a click, which they named ‘ornamentation’ after the musical practice of adding notes atop the melody.</p>
                    <p>The significance of these features is not yet clear. But by using rhythm, tempo, rubato and ornamentation in different combinations, the whales can produce a huge set of different codas. The researchers collected a data set of 8,719 codas and uncovered what they call a sperm whale phonetic alphabet, which they think the whales might use as building blocks for sharing complex information.</p>
                    <p>As AI uncovers these features of whale vocalizations, researchers are asking what meaning they might carry. Does rubato increase before a dive, or decrease when a mother communicates with her calf, for instance? “If you don’t know rubato exists, then you can’t start asking ‘when is rubato important?’” Gero says. His team is analysing these features.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-z2lMzdJIol" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        
                        
                      </figcaption>
                    </figure>
                    <p><span><strong>“We think that that’s like saying, </strong>‘<strong>I am from Dominica, are you?</strong>’<strong>”</strong></span></p>
                    <p>To understand the rhythm and tempo of the codas, the team manually created graphic representations of whale sound recordings, known as spectrograms. These provide a way to visualize sound, depicting characteristics such as volume and frequency. For people, they can be used to identify individual units of speech known as phonemes.</p>
                    <p>The process was time-consuming; every minute of recording took a team member roughly 10 minutes to separate out all the individual clicks. Turning that task over to a machine-learning algorithm sped up the work vastly, Gero says, and also helped to separate which sound came from which animal.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>The CETI team used hydrophones to record sound beneath the surface. Credit: Marissa Velez/Project CETI</span></p>
                        <p><span>The CETI team used hydrophones to record sound beneath the surface. Credit: Marissa Velez/Project CETI</span></p>
                      </figcaption>
                    </figure>
                    <p>But AI also allowed the researchers to go further. Manually, they had essentially been cataloguing individual words, but AI allowed them to look at the codas in the equivalent of whale sentences and even entire conversations. New structures began to emerge. “Machine learning is really good at seeing patterns that are hard to pick up through standard statistical approaches,” Gero says.</p>
                    <p>The work<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>3</sup></a> uncovered fine modulations of the intervals between clicks, which the scientists labelled ‘rubato’, borrowing a musical term for slight changes in tempo that make a piece more expressive. They also discovered the occasional addition of a click, which they named ‘ornamentation’ after the musical practice of adding notes atop the melody.</p>
                    <figure>
                      <div>
                        <div>
                          
                        </div>
                      </div>
                      <figcaption></figcaption>
                    </figure>
                    <p>The significance of these features is not yet clear. But by using rhythm, tempo, rubato and ornamentation in different combinations, the whales can produce a huge set of different codas. The researchers collected a data set of 8,719 codas and uncovered what they call a sperm whale phonetic alphabet, which they think the whales might use as building blocks for sharing complex information.</p>
                    <p>As AI uncovers these features of whale vocalizations, researchers are asking what meaning they might carry. Does rubato increase before a dive, or decrease when a mother communicates with her calf, for instance? “If you don’t know rubato exists, then you can’t start asking ‘when is rubato important?’” Gero says. His team is analysing these features.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div id="section-uiLREna1FK" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Mickey Pardo listens to wild African elephants communicate. Credit: Mickey Pardo</span></p>
                        <p><span>Mickey Pardo listens to wild African elephants communicate. Credit: Mickey Pardo</span></p>
                      </figcaption>
                    </figure>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Common marmoset monkeys (<em>Callithrix jacchus</em>) have names for other marmosets. Credit: Maria Gaellman/Alamy</span></p>
                        <p><span>Common marmoset monkeys (<em>Callithrix jacchus</em>) have names for other marmosets. Credit: Maria Gaellman/Alamy</span></p>
                      </figcaption>
                    </figure>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>Sperm whales aren’t the only creatures that use specific vocalizations to identify themselves. Behavioural ecologist Mickey Pardo, then at Colorado State University in Fort Collins, and his colleagues used machine learning to discover that wild African elephants have what seem to be names. That is, they address other elephants with vocalizations specific to the individual<a href="https://oscar-sites-nature-cm10.public.springernature.app/platform/rh/preview/page/nature/ai-decodes-the-calls-of-the-wild/50305852?view=fragmentPreview#ref-CR1"><sup><u>1</u></sup></a>. (Pardo now works from Colorado for Cornell University in New York.)</p>
                    <p>Researchers already knew that the animals make low rumbling sounds that differ depending on whether they’re out of sight of one another or in close contact, as well as whether a mother is interacting with her calf. Pardo and his colleagues saw that elephants would react to some calls while ignoring others. </p>
                    <p>To see whether those calls that received a response were unique, they trained a machine-learning model with vocalizations that researchers had labelled as evoking a reaction. The algorithm learnt the acoustic features of those calls, and was then tasked with spotting those features in new calls and predicting the intended recipient. </p>
                    <p>The computer correctly matched the calls to the recipient 27.5% of the time, “which might not seem like that much, but you have to remember that we wouldn’t expect elephants to use names in every call,” Pardo says. By contrast, a model trained with random features was right only 8% of the time. The team verified that these calls were meaningful to the elephants by playing back recordings of them and checking which animal responded.</p>
                    <p>David Omer, a neurologist at Hebrew University of Jerusalem, Israel, did something similar with marmoset monkeys. He and his team trained a computer on the calls of the marmosets, and found that members of the same family used calls with similar acoustic features to label other marmosets<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>2</sup></a>. </p>
                    <p>Pardo would like to use the same techniques to see whether he can decode any other elephant vocabulary, such as terms for locations. Elephants make certain calls when they’re trying to get their cohort moving. If some of those sounds identify movement towards a particular place, researchers might be able to identify those with AI. To verify their findings, the researchers could then play back that call and watch where the elephants go.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-V4NhqfDDfz" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>Sperm whales aren’t the only creatures that use specific vocalizations to identify themselves. Behavioural ecologist Mickey Pardo, then at Colorado State University in Fort Collins, and his colleagues used machine learning to discover that wild African elephants have what seem to be names. That is, they address other elephants with vocalizations specific to the individual<a href="https://oscar-sites-nature-cm10.public.springernature.app/platform/rh/preview/page/nature/ai-decodes-the-calls-of-the-wild/50305852?view=fragmentPreview#ref-CR1"><sup><u>1</u></sup></a>. (Pardo now works from Colorado for Cornell University in New York.)</p>
                    <p>Researchers already knew that the animals make low rumbling sounds that differ depending on whether they’re out of sight of one another or in close contact, as well as whether a mother is interacting with her calf. Pardo and his colleagues saw that elephants would react to some calls while ignoring others.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Mickey Pardo listens to wild African elephants communicate. Credit: Mickey Pardo</span></p>
                        <p><span>Mickey Pardo listens to wild African elephants communicate. Credit: Mickey Pardo</span></p>
                      </figcaption>
                    </figure>
                    <p>To see whether those calls that received a response were unique, they trained a machine-learning model with vocalizations that researchers had labelled as evoking a reaction. The algorithm learnt the acoustic features of those calls, and was then tasked with spotting those features in new calls and predicting the intended recipient. </p>
                    <p>The computer correctly matched the calls to the recipient 27.5% of the time, “which might not seem like that much, but you have to remember that we wouldn’t expect elephants to use names in every call,” Pardo says. By contrast, a model trained with random features was right only 8% of the time. The team verified that these calls were meaningful to the elephants by playing back recordings of them and checking which animal responded.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Common marmoset monkeys (<em>Callithrix jacchus</em>) have names for other marmosets. Credit: Maria Gaellman/Alamy</span></p>
                        <p><span>Common marmoset monkeys (<em>Callithrix jacchus</em>) have names for other marmosets. Credit: Maria Gaellman/Alamy</span></p>
                      </figcaption>
                    </figure>
                    <p>David Omer, a neurologist at Hebrew University of Jerusalem, Israel, did something similar with marmoset monkeys. He and his team trained a computer on the calls of the marmosets, and found that members of the same family used calls with similar acoustic features to label other marmosets<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>2</sup></a>. </p>
                    <p>Pardo would like to use the same techniques to see whether he can decode any other elephant vocabulary, such as terms for locations. Elephants make certain calls when they’re trying to get their cohort moving. If some of those sounds identify movement towards a particular place, researchers might be able to identify those with AI. To verify their findings, the researchers could then play back that call and watch where the elephants go.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-yI5O2Sz2rU" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ=="><sh-background-transition data-lazyload-container="true" data-lazyload-trigger="true" data-transitions="" classname="Layer--one Theme-Layer-BackgroundCanvas">
          
          <div slot="foreground">
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>
                        <div>
                          <p>In a separate study, Pardo and his co-authors recorded calls from elephants in two populations in Kenya<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>4</sup></a>. The researchers then used machine learning to show that there were clear vocal differences between the two populations, as well as subtle differences between elephants in different social groups within the two populations.</p>
                          <p>That could be important information for conservationists who might be trying to keep endangered populations healthy by introducing elephants into an existing group, Pardo says. If the animals can’t understand each other, that could cause problems for the newcomer. “Understanding whether different populations of elephants could communicate with each other actually has some important practical implications,” he says.</p>
                          <p>Elephant specialists know that calls contain information about the individual making the sound, including their sex, age and physiological condition. If scientists could learn to tease out that information, they could then use passive acoustic monitoring — microphones placed around an area — to learn about a particular set of elephants. </p>
                          <p>“We could potentially use that to figure out the sex ratio and age structure of an elephant population in an environment where it’s really hard to observe the elephants directly, like in a forested environment,” Pardo says. And that, in turn, would help conservationists to work out how a particular group is doing, such as whether its numbers are growing or declining.</p>
                          <p><span>Image credit: George Wittemyer</span></p>
                        </div>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
        </sh-background-transition></div>
      <div id="section-cSsmplqK4X" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Male elephant seals (<em>Mirounga</em> spp.) use vocalizations to deter would-be attackers. Credit: George Rose/Getty</span></p>
                        <p><span>Male elephant seals (<em>Mirounga</em> spp.) use vocalizations to deter would-be attackers. Credit: George Rose/Getty</span></p>
                      </figcaption>
                    </figure>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>AI can be an important tool for this type of research, says Caroline Casey, an animal behavioural ecologist at the University of California, Santa Cruz. She worries, however, that both the public and some scientists might put too much faith in the technology’s abilities. “I just don’t think it’s the magic wand that’s going to be able to illuminate the mysteries of animal communication in the way that maybe the media has projected,” she says.</p>
                    <p>Casey spent five years on her PhD thesis, in which she demonstrated that elephant seals (<em>Mirounga</em> spp.) give themselves names<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>5</sup></a>. During periods of fasting of around one month, the seals conserve energy and males avoid fighting each other for dominance. Instead, they make vocalizations to identify themselves as the winners of previous fights to discourage other seals from attacking. Casey learnt this by studying spectrograms, observing the animals’ behaviour and playing back calls to the animals. She is clear that AI will not remove the need for this kind of high-quality fieldwork. However, she worries that this might be missed by funders drawn in by the allure of AI.</p>
                    <p>Casey also thinks that the value of human intuition might be being overlooked. Using an AI-based classifier to interpret animals’ calls could diminish human bias in the research — a good thing, Casey acknowledges. But at the same time, she thinks that machines’ lack of understanding of the world might hinder their ability to make sense of the patterns that they uncover. “The human mind is able to integrate our understanding of our own world and the way that we operate, and use that to actually aid in the interpretation of animal behaviour,” she says. “I think it’s an advantage.”</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-e3xDyskfYd" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>AI can be an important tool for this type of research, says Caroline Casey, an animal behavioural ecologist at the University of California, Santa Cruz. She worries, however, that both the public and some scientists might put too much faith in the technology’s abilities. “I just don’t think it’s the magic wand that’s going to be able to illuminate the mysteries of animal communication in the way that maybe the media has projected,” she says.</p>
                    <p>Casey spent five years on her PhD thesis, in which she demonstrated that elephant seals (<em>Mirounga</em> spp.) give themselves names<a href="https://www.nature.com/immersive/d41586-024-04050-5/index.html#section-References-jCEkMC06G6"><sup>5</sup></a>. During periods of fasting of around one month, the seals conserve energy and males avoid fighting each other for dominance. Instead, they make vocalizations to identify themselves as the winners of previous fights to discourage other seals from attacking. Casey learnt this by studying spectrograms, observing the animals’ behaviour and playing back calls to the animals. She is clear that AI will not remove the need for this kind of high-quality fieldwork. However, she worries that this might be missed by funders drawn in by the allure of AI.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Male elephant seals (<em>Mirounga</em> spp.) use vocalizations to deter would-be attackers. Credit: George Rose/Getty</span></p>
                        <p><span>Male elephant seals (<em>Mirounga</em> spp.) use vocalizations to deter would-be attackers. Credit: George Rose/Getty</span></p>
                      </figcaption>
                    </figure>
                    <p>Casey also thinks that the value of human intuition might be being overlooked. Using an AI-based classifier to interpret animals’ calls could diminish human bias in the research — a good thing, Casey acknowledges. But at the same time, she thinks that machines’ lack of understanding of the world might hinder their ability to make sense of the patterns that they uncover. “The human mind is able to integrate our understanding of our own world and the way that we operate, and use that to actually aid in the interpretation of animal behaviour,” she says. “I think it’s an advantage.”</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div id="section-UNuHWplcRV" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <figure data-videoplayer="true" data-lazyload-video="true">
                      <div><video data-id="4N0yJy5LMw" preload="none" controls="" controlslist="nodownload" playsinline="" data-landscape-poster="./assets/SMq0Xspkiv/donatella_test_playback_rxn_clip-frame-0ms-1920x1080.jpg" data-portrait-poster="./assets/SMq0Xspkiv/donatella_test_playback_rxn_clip-frame-0ms-1920x1080.jpg" data-videoplayer-media="true">
                          <source type="video/webm" data-landscape="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.webm" data-portrait="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.webm"/>
                          <source type="video/mp4" data-landscape="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.mp4" data-portrait="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.mp4"/>
                          <p>Your browser does not support this video</p>
                        </video></div>
                      <figcaption>
                        <p><span>An elephant reacts to the playback of a call that was originally addressed to her. Credit: Mickey Pardo</span></p>
                        <p><span>An elephant reacts to the playback of a call that was originally addressed to her. Credit: Mickey Pardo</span></p>
                      </figcaption>
                    </figure>
                    
                    <p><span><strong>“Deep-learning models make it possible or even easy to get all kinds of results that we can’t really get any other way.”</strong></span></p>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Olivier Pietquin (back row, second left) and other members of the Earth Species Project are attempting to decode animal communication. Credit: Earth Species Project</span></p>
                        <p><span>Olivier Pietquin (back row, second left) and other members of the Earth Species Project are attempting to decode animal communication. Credit: Earth Species Project</span></p>
                      </figcaption>
                    </figure>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>Much of the excitement about AI over the past decade has come from the achievements of neural networks — systems built on an analogy of how the human brain processes information through collections of neurons. Deep learning, in which data pass through many layers of a neural network, was what led to the creation of the chatbot ChatGPT. The sperm whale, elephant and marmoset studies, however, used earlier forms of AI known as decision trees and random forests.</p>
                    <p>A decision tree is a classification algorithm that looks like a flow chart. It might ask, for example, whether the sound it has been given has a frequency above a certain value. If yes, it might then ask whether the call lasts for a certain length of time, and so on, until it has decided whether the call matches the acoustic variables it was trained to look for using human-labelled data sets. A random forest is a collection of many decision trees, each constructed from a randomly chosen subset of the data.</p>
                    <p>Kurt Fristrup, an evolutionary biologist at Colorado State University who wrote the random-forest algorithm for the elephant project, says that tree-based algorithms have several advantages for this kind of work. For one, they can work with less information than is required to train a neural network, and even thousands of hours’ of recordings of animal calls is still a relatively small data set. Furthermore, because of the way that tree-based algorithms break down the variables, they’re not likely to be thrown off by mislabelled or unlabelled data.</p>
                    <p>The random forest also provides a way to verify that similar calls match: different calls that show the same features should each end up in the same ‘leaf’ of an individual tree. “Since there were on the order of a thousand of these trees, you get a fairly fine-grained measure of how similar two calls are by how often they landed in the same leaf,” Fristrup says.</p>
                    <p>It is also easier to see how a random-forest algorithm came to a particular conclusion than it is with deep learning, which can produce answers that leave scientists scratching their heads about how the model reached its decision. “Deep-learning models make it possible or even easy to get all kinds of results that we can’t really get any other way,” Fristrup says. But if scientists don’t understand the reasoning behind it, they might not learn “what we would have learnt had we got into it by the older, less efficient, and less computationally intense path” of a random forest, he says.</p>
                    <p>Despite this, the ability of a neural network to generalize from a relatively small, labelled data set and discover patterns by examining large amounts of unlabelled data is appealing to many researchers.</p>
                    <p>Machine-learning specialist Olivier Pietquin is the AI research director at the <a href="https://www.earthspecies.org/"><span>Earth Species Project</span></a>, an international team headquartered in Berkeley, California, that is using AI to decode the communications of animal species. He wants to take advantage of neural networks’ ability to generalize from one data set to another by training models using not only a large range of sounds from different animals, but also other acoustic data, including human speech and music.</p>
                    <p>The hope is that the computer might derive some basic underlying features of sound before building on that understanding to recognize features in animal vocalizations specifically. This is the same way in which an image-recognition algorithm trained on pictures of human faces learns some basic characteristics of pixels that describe first an oval and then an eye. The algorithm can then take those basics and recognize the face of a cat, even if human faces make up most of its training data.</p>
                    <p>“We could imagine using speech data and hope that it will transfer to any other animal that has a vocal tract and vocal cords,” Pietquin says. The whistle made by a flute, for example, might be similar enough to a bird whistle that the computer could make inferences from it.</p>
                    <p>A model trained in this way could be useful for identifying what sounds convey information and which ones are just noise. To work out what the calls might mean, however, still requires a person to observe the animal’s behaviour and add labels to what the computer has identified. Identifying speech, which is what researchers are currently trying to achieve, is just a first step towards comprehending it. “Understanding is really a tough step,” Pietquin says.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-4vkpPCzCyi" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>Much of the excitement about AI over the past decade has come from the achievements of neural networks — systems built on an analogy of how the human brain processes information through collections of neurons. Deep learning, in which data pass through many layers of a neural network, was what led to the creation of the chatbot ChatGPT. The sperm whale, elephant and marmoset studies, however, used earlier forms of AI known as decision trees and random forests.</p>
                    <p>A decision tree is a classification algorithm that looks like a flow chart. It might ask, for example, whether the sound it has been given has a frequency above a certain value. If yes, it might then ask whether the call lasts for a certain length of time, and so on, until it has decided whether the call matches the acoustic variables it was trained to look for using human-labelled data sets. A random forest is a collection of many decision trees, each constructed from a randomly chosen subset of the data.</p>
                    <p>Kurt Fristrup, an evolutionary biologist at Colorado State University who wrote the random-forest algorithm for the elephant project, says that tree-based algorithms have several advantages for this kind of work. For one, they can work with less information than is required to train a neural network, and even thousands of hours’ of recordings of animal calls is still a relatively small data set. Furthermore, because of the way that tree-based algorithms break down the variables, they’re not likely to be thrown off by mislabelled or unlabelled data.</p>
                    <p>The random forest also provides a way to verify that similar calls match: different calls that show the same features should each end up in the same ‘leaf’ of an individual tree. “Since there were on the order of a thousand of these trees, you get a fairly fine-grained measure of how similar two calls are by how often they landed in the same leaf,” Fristrup says.</p>
                    <figure data-videoplayer="true" data-lazyload-video="true">
                      <div><video data-id="4N0yJy5LMw" preload="none" controls="" controlslist="nodownload" playsinline="" data-landscape-poster="./assets/SMq0Xspkiv/donatella_test_playback_rxn_clip-frame-0ms-1920x1080.jpg" data-portrait-poster="./assets/SMq0Xspkiv/donatella_test_playback_rxn_clip-frame-0ms-1920x1080.jpg" data-videoplayer-media="true">
                          <source type="video/webm" data-landscape="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.webm" data-portrait="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.webm"/>
                          <source type="video/mp4" data-landscape="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.mp4" data-portrait="./assets/4N0yJy5LMw/donatella_test_playback_rxn_clip-1920x1080.mp4"/>
                          <p>Your browser does not support this video</p>
                        </video></div>
                      <figcaption>
                        <p><span>An elephant reacts to the playback of a call that was originally addressed to her. Credit: Mickey Pardo</span></p>
                        <p><span>An elephant reacts to the playback of a call that was originally addressed to her. Credit: Mickey Pardo</span></p>
                      </figcaption>
                    </figure>
                    <p>It is also easier to see how a random-forest algorithm came to a particular conclusion than it is with deep learning, which can produce answers that leave scientists scratching their heads about how the model reached its decision. “Deep-learning models make it possible or even easy to get all kinds of results that we can’t really get any other way,” Fristrup says. But if scientists don’t understand the reasoning behind it, they might not learn “what we would have learnt had we got into it by the older, less efficient, and less computationally intense path” of a random forest, he says.</p>
                    <p>Despite this, the ability of a neural network to generalize from a relatively small, labelled data set and discover patterns by examining large amounts of unlabelled data is appealing to many researchers.</p>
                    <p>Machine-learning specialist Olivier Pietquin is the AI research director at the <a href="https://www.earthspecies.org/"><span>Earth Species Project</span></a>, an international team headquartered in Berkeley, California, that is using AI to decode the communications of animal species. He wants to take advantage of neural networks’ ability to generalize from one data set to another by training models using not only a large range of sounds from different animals, but also other acoustic data, including human speech and music.</p>
                    <p>The hope is that the computer might derive some basic underlying features of sound before building on that understanding to recognize features in animal vocalizations specifically. This is the same way in which an image-recognition algorithm trained on pictures of human faces learns some basic characteristics of pixels that describe first an oval and then an eye. The algorithm can then take those basics and recognize the face of a cat, even if human faces make up most of its training data.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Olivier Pietquin (back row, second left) and other members of the Earth Species Project are attempting to decode animal communication. Credit: Earth Species Project</span></p>
                        <p><span>Olivier Pietquin (back row, second left) and other members of the Earth Species Project are attempting to decode animal communication. Credit: Earth Species Project</span></p>
                      </figcaption>
                    </figure>
                    <p>“We could imagine using speech data and hope that it will transfer to any other animal that has a vocal tract and vocal cords,” Pietquin says. The whistle made by a flute, for example, might be similar enough to a bird whistle that the computer could make inferences from it.</p>
                    <p>A model trained in this way could be useful for identifying what sounds convey information and which ones are just noise. To work out what the calls might mean, however, still requires a person to observe the animal’s behaviour and add labels to what the computer has identified. Identifying speech, which is what researchers are currently trying to achieve, is just a first step towards comprehending it. “Understanding is really a tough step,” Pietquin says.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-EAC5TqPPud" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ=="><sh-background-transition data-lazyload-container="true" data-lazyload-trigger="true" data-transitions="" classname="Layer--one Theme-Layer-BackgroundCanvas">
          
          <div slot="foreground">
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>
                        <div>
                          <p>Researchers at the Earth Species Project have already created one neural network, called Voxaboxen, that they are applying to the study of crow communication.</p>
                          <p>A population of carrion crows (<em>Corvus corone</em>) in northern Spain, unlike their counterparts elsewhere in Europe, share the responsibility of caring for their young. A group of crows will take turns defending a nest, cleaning it and caring for the chicks. “We think they must coordinate with vocal communication to do these tasks. And this was the reason why we started studying communication in crows,” says Daniela Canestrari, a behavioural ecologist at the University of León in Spain.</p>
                          <p><span>Image credit: Claudia Wascher</span></p>
                        </div>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
        </sh-background-transition></div>
      <div id="section-uE9K7UrSYI" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>She and fellow behavioural ecologist Vittorio Baglione attach tags to the crows’ tail feathers. The device has a tiny microphone, along with an accelerometer and magnetometer to measure a bird’s movement along with its calls. The tag collects about six days’ worth of data, then eventually falls to the ground and emits a signal that allows researchers to retrieve it.</p>
                    <p>One of the first things the researchers discovered with their tags was that, as well as the loud ‘caws’ that can be picked up by microphones at a distance, the crows also make softer sounds that are only audible up close. That could prove to be a rich source of information, but it also increases the volume of data that the researchers have to deal with. “We couldn’t really process all this information without artificial intelligence,” Canestrari says.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div id="section-IqZN9kP8YM" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <p><span><strong>“The differences are sometimes very subtle.”</strong></span></p>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p><span>Voxaboxen was designed to organize animal sounds after being primed with recordings from a particular species, in this case 188 hours’ worth of human-labelled carrion crow calls.</span> The neural network’s task is to tease apart all the sounds in the recordings, to find which sounds over the six days came from crows, and whether they came from the crow that was tagged or from a different one. Once the calls have been detected, the next step is to try to classify them into categories; this process is roughly equivalent to making lists of words. “It’s a very difficult task because the differences are sometimes very subtle,” Baglione says.</p>
                    <p>To work out what the calls mean, the researchers use AI to match up accelerometer data with video recordings of the same birds, and look for correlations between the observed behaviour and the sounds.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-fnf8A4UH9P" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p><span><strong>“The differences are sometimes very subtle.”</strong></span></p>
                    <p><span>Voxaboxen was designed to organize animal sounds after being primed with recordings from a particular species, in this case 188 hours’ worth of human-labelled carrion crow calls.</span> The neural network’s task is to tease apart all the sounds in the recordings, to find which sounds over the six days came from crows, and whether they came from the crow that was tagged or from a different one. Once the calls have been detected, the next step is to try to classify them into categories; this process is roughly equivalent to making lists of words. “It’s a very difficult task because the differences are sometimes very subtle,” Baglione says.</p>
                    <p>To work out what the calls mean, the researchers use AI to match up accelerometer data with video recordings of the same birds, and look for correlations between the observed behaviour and the sounds.</p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div id="section-YGPyT0yV3S" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <p><span><strong>“Understanding is really a tough step.”</strong></span></p>
                    
                    <figure>
                      <div>
                        <div>
                          
                        </div>
                      </div>
                      <figcaption></figcaption>
                    </figure>
                    
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Sperm whales (<em>Physeter macrocephalus) </em>can become tangled in fishing nets. Credit: Dario Romeo/Getty</span></p>
                        <p><span>Sperm whales (<em>Physeter macrocephalus) </em>can become tangled in fishing nets. Credit: Dario Romeo/Getty</span></p>
                      </figcaption>
                    </figure>
                  </div>
                  
                </div>
              </div>
              <div>
                <div>
                  <div>
                    <p>Researchers are cautious about suggesting that AI models will eventually give us the ability to talk to the animals. Pardo says that his main goal is not so much to be able to talk to wildlife and pets, but rather to learn something about their minds and how they perceive themselves and the world. The fact that some animals seem to have names, for instance, implies that they’re capable of conceiving of other individuals as entities and coming up with labels, which he says suggests that they have a sophisticated level of abstract thinking.</p>
                    <p>It’s still an open question whether animals are even capable of more than a rudimentary level of communication, and Pardo says that there’s no agreed definition of what constitutes a language. “To call something language, I would want it to be a system that can basically be used to communicate about almost any thought, including abstract concepts,” Pardo says. “I don’t think we have evidence for that in any non-human species.” If scientists could show that animals do have that language, then they could try to work out how to communicate with them.</p>
                    <p>Even if direct communication remains out of reach, many scientists involved in this research see the improvement of conservation efforts as a major goal. Showing that animals have minds of their own can increase empathy for them. Gruber points to the work of US biologist Roger Payne in the 1960s, whose description of the complexity of humpback whale (<em>Megaptera novaeangliae</em>) song galvanized the movement to ban whale hunting.</p>
                    <p>Saving whales is an important goal for Gero. During his research off the coast of Dominica, he watched as a social unit of sperm whales he’d named ‘the group of seven’ shrank to only three as their companions were struck by ships or tangled in nets. If humans could communicate with whales, perhaps they could inform the giant mammals that they were in a boat lane and at risk of a collision. Or, as Gero would prefer, he could ask the whales why they need to be in a particular area, and route the boats away from it.</p>
                    <p>“I don’t want to make a whale alarm for all the bad things that humans are doing because we’re bad neighbours,” he says. “I would rather learn from the animals about how to be a good neighbour, and then do that.”</p>
                    <p>Infant mortality among sperm whales around Dominica has become so bad — around one-third of calves die, sometimes because they lose their parents — that researchers there have delayed naming the babies until they’re at least two or three years old. But they do continue to name the whales, as a reminder of the roles of each individual in its group, and the part each group plays in the overall survival of the species.</p>
                    <p>Conservationists cannot, for instance, relocate whales 4,000 kilometres from the Azores to Dominica to replenish the Caribbean population and expect them to handle their new environment, Gero says. “Our science addresses these animals under the assumption that they’re uniquely important to the network of life that they are in,” he says. “So calling one ‘Fingers’ is a short form of saying we recognize that she’s different from Pinchy, and if Fingers dies, we can’t just replace her with Pinchy.”</p>
                    <p>Like Gero, Pardo is more interested in what animals have to say to us than what we might say to them. If he could talk to the elephants, he says, he would want to ask them how they feel about the way that humans treat them. “If it were possible for humans to hear from other animals in their own words, ‘Hey, stop fucking killing us’, maybe people would actually do that.”</p>
                    
                    <p><em>doi: https://doi.org/10.1038/d41586-024-04050-5</em></p>
                    
                    <p><span>An earlier version of this story misstated how Voxaboxen was trained to categorize carrion crow calls.</span></p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-xzA9RluOF3" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <p>Researchers are cautious about suggesting that AI models will eventually give us the ability to talk to the animals. Pardo says that his main goal is not so much to be able to talk to wildlife and pets, but rather to learn something about their minds and how they perceive themselves and the world. The fact that some animals seem to have names, for instance, implies that they’re capable of conceiving of other individuals as entities and coming up with labels, which he says suggests that they have a sophisticated level of abstract thinking.</p>
                    <p>It’s still an open question whether animals are even capable of more than a rudimentary level of communication, and Pardo says that there’s no agreed definition of what constitutes a language. “To call something language, I would want it to be a system that can basically be used to communicate about almost any thought, including abstract concepts,” Pardo says. “I don’t think we have evidence for that in any non-human species.” If scientists could show that animals do have that language, then they could try to work out how to communicate with them.</p>
                    <p>Even if direct communication remains out of reach, many scientists involved in this research see the improvement of conservation efforts as a major goal. Showing that animals have minds of their own can increase empathy for them. Gruber points to the work of US biologist Roger Payne in the 1960s, whose description of the complexity of humpback whale (<em>Megaptera novaeangliae</em>) song galvanized the movement to ban whale hunting.</p>
                    <figure>
                      <div>
                        <div>
                          
                        </div>
                      </div>
                      <figcaption></figcaption>
                    </figure>
                    <p>Saving whales is an important goal for Gero. During his research off the coast of Dominica, he watched as a social unit of sperm whales he’d named ‘the group of seven’ shrank to only three as their companions were struck by ships or tangled in nets. If humans could communicate with whales, perhaps they could inform the giant mammals that they were in a boat lane and at risk of a collision. Or, as Gero would prefer, he could ask the whales why they need to be in a particular area, and route the boats away from it.</p>
                    <p>“I don’t want to make a whale alarm for all the bad things that humans are doing because we’re bad neighbours,” he says. “I would rather learn from the animals about how to be a good neighbour, and then do that.”</p>
                    <p>Infant mortality among sperm whales around Dominica has become so bad — around one-third of calves die, sometimes because they lose their parents — that researchers there have delayed naming the babies until they’re at least two or three years old. But they do continue to name the whales, as a reminder of the roles of each individual in its group, and the part each group plays in the overall survival of the species.</p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        <p><span>Sperm whales (<em>Physeter macrocephalus) </em>can become tangled in fishing nets. Credit: Dario Romeo/Getty</span></p>
                        <p><span>Sperm whales (<em>Physeter macrocephalus) </em>can become tangled in fishing nets. Credit: Dario Romeo/Getty</span></p>
                      </figcaption>
                    </figure>
                    <p>Conservationists cannot, for instance, relocate whales 4,000 kilometres from the Azores to Dominica to replenish the Caribbean population and expect them to handle their new environment, Gero says. “Our science addresses these animals under the assumption that they’re uniquely important to the network of life that they are in,” he says. “So calling one ‘Fingers’ is a short form of saying we recognize that she’s different from Pinchy, and if Fingers dies, we can’t just replace her with Pinchy.”</p>
                    <p>Like Gero, Pardo is more interested in what animals have to say to us than what we might say to them. If he could talk to the elephants, he says, he would want to ask them how they feel about the way that humans treat them. “If it were possible for humans to hear from other animals in their own words, ‘Hey, stop fucking killing us’, maybe people would actually do that.”</p>
                    
                    <p><em>doi: https://doi.org/10.1038/d41586-024-04050-5</em></p>
                    
                    <p><span>An earlier version of this story misstated how Voxaboxen was trained to categorize carrion crow calls.</span></p>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-References-jCEkMC06G6" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <h2><strong>References</strong></h2>
                    <ol><li><span>Pardo, M. A. </span><em>et al.</em><span> </span><em>Nature Ecol. Evol.</em><span> </span><strong>8</strong><span>, 1353–1364 (2024). </span><a href="https://doi.org/10.1038/s41559-024-02420-w">Article</a></li><li><span>Oren, G. </span><em>et al.</em><span> </span><em>Science</em><span> </span><strong>385</strong><span>, 996–1003 (2024).</span> <a href="https://doi.org/10.1126/science.adp3757">Article</a></li><li>Sharma, P. <em>et al.</em> <em>Nature Commun.</em> <strong>15</strong>, 3617 (2024). <a href="https://doi.org/10.1038/s41467-024-47221-8">Article</a></li><li><span>Pardo, M. A. </span><em>et al.</em><span> </span><em>R. Soc. Open Sci.</em><span> </span><strong>11</strong><span>, 241264 (2024). </span><a href="https://doi.org/10.1098/rsos.241264">Article</a></li><li>Mathevon, N., Casey, C., Reichmuth, C. &amp; Charrier, I. <em>Curr Biol.</em> <strong>27</strong>, 2352–2356 (2017). <a href="https://doi.org/10.1016/j.cub.2017.06.035">Article</a></li>
                    </ol>
                    
                    <p><strong>Author: </strong>Neil Savage</p>
                    <p><strong>Video production:</strong> Emily Bates</p>
                    <p><strong>Additional filming:</strong> Peaking Zebra Productions</p>
                    <p><strong>Art director: </strong>Mohamed Ashour</p>
                    <p><strong>Picture editor: </strong>Kezia Levitas</p>
                    <p><strong>Subeditor: </strong>Jenny McCarthy</p>
                    <p><strong>Project manager: </strong>Beth MacNamara</p>
                    <p><strong>Editors: </strong>Richard Hodson, Esme Hedley</p>
                    
                    <p><span>This article is part of </span><a href="https://www.nature.com/immersive/robotics-ai/index.html"><span>Nature Outlook: Robotics and artificial intelligence</span></a><span>, a supplement produced with financial support from FII Institute. <em>Nature</em> maintains full independence in all editorial decisions related to the content. </span><a href="https://partnerships.nature.com/commercial-content-at-nature-research/"><span>About this content</span></a><span>.</span></p>
                    <p><span><strong>The supporting organization retains sole responsibility for the following message:</strong></span></p>
                    <figure data-lazyload-container="true" data-lazyload-trigger="true">
                      
                      <figcaption>
                        
                        
                      </figcaption>
                    </figure>
                    <p>FII Institute is a global non-profit foundation with an investment arm and one agenda: Impact on Humanity. Committed to ESG principles, we foster the brightest minds and transform ideas into real-world solutions in five focus areas: AI and Robotics, Education, Healthcare, and Sustainability.</p>
                    <p>We are in the right place at the right time – when decision makers, investors, and an engaged generation of youth come together in aspiration, energized and ready for change. We harness that energy into three pillars – THINK, XCHANGE, ACT – and invest in the innovations that make a difference globally.</p>
                    <p>Join us to own, co-create and actualize a brighter, more sustainable future for humanity.</p>
                    
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div id="section-wh20IECvhr" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    
                    <div><p><img src="https://www.nature.com/e29sgq9v/article/d41586-024-02792-w" width="1" height="1" alt=""/></p><svg height="1em" viewBox="0 0 140 14" role="img" focusable="false" aria-hidden="true" style="display:block;">
                        <path d="M74.9,5.7c0-2.8-2.4-3.8-4.6-3.8c-1.1,0-2.8,0-3.8,0v11.6h2.9V10h0.9c0.2,0,0.3,0,0.5,0l0,0l0,0c0.5,0.8,1.1,2,1.3,3.5 c0.6,0,1.1,0,1.5,0h0.8c0.3,0,0.5,0,0.8,0c-0.3-1.8-1.2-3.5-1.8-4.4l0,0l0,0C74.3,8.2,74.9,7.1,74.9,5.7z M71.8,6.1 c0,1.2-0.5,1.7-1.6,1.7c-0.4,0-0.6,0-0.8,0l0,0V4.1l0,0c0.3,0,0.6,0,0.9,0C71.6,4.1,71.8,4.9,71.8,6.1z M64.3,13.5v-2.3h-4.1V8.7 h3.7V6.5h-3.7V4.2h4.1V1.9h-6.8v9.5c0,0,0,0.9,0.6,1.5c0.4,0.4,0.9,0.6,1.6,0.6C61.7,13.6,64,13.5,64.3,13.5z M51.4,13.9 c1.1,0,2.8-0.3,4.1-0.9V7.2h-3.8v2h1.2v2.4l0,0c-0.3,0.1-1,0.2-1.3,0.2c-1.5,0-2.1-1.1-2.1-4c0-2.5,0.9-3.8,2.7-3.8 c0.6,0,1.5,0.3,2.4,0.7l0.8-1.9c-1.1-0.7-2.4-1-3.6-1c-1.7,0-3,0.5-3.8,1.5c-0.8,1-1.2,2.5-1.2,4.5C46.6,12.2,47.9,13.9,51.4,13.9z M41.7,13.5h3V1.9h-2.5v7.6l-3.4-7.6h-3.1v11.6h2.5V6.2L41.7,13.5L41.7,13.5z M30.2,13.5h2.9V2h-2.9V13.5z M26.6,9.1L26.6,9.1 L26.6,9.1c1-0.8,1.5-2,1.5-3.4c0-2.8-2.4-3.8-4.6-3.8c-1.1,0-2.8,0-3.8,0v11.6h2.9V10h0.9c0.2,0,0.3,0,0.5,0l0,0l0,0 c0.5,0.8,1.1,2,1.3,3.5c0.6,0,1.1,0,1.5,0h0.8c0.3,0,0.5,0,0.8,0C28.1,11.7,27.2,10,26.6,9.1z M25,6.1c0,1.2-0.5,1.7-1.6,1.7 c-0.4,0-0.6,0-0.8,0l0,0V4.1l0,0c0.3,0,0.6,0,0.9,0C24.8,4.1,25,4.9,25,6.1z M18.2,5.6c0-2.4-1.5-3.7-4.3-3.7c-1.1,0-2.8,0-3.8,0 v11.6H13V10h0.9C15.2,10,18.2,9.6,18.2,5.6z M15.5,6.1c0,1.2-0.5,1.7-1.6,1.7c-0.4,0-0.6,0-0.8,0l0,0V4.1l0,0c0.3,0,0.6,0,0.9,0 C15.2,4.1,15.5,4.9,15.5,6.1z M3.3,4.1c0-0.6,0.4-1.3,1.4-1.3c0.7,0,1.5,0.2,2.5,0.7l1-2.1C7,0.8,5.8,0.4,4.5,0.4 c-2.6,0-4.2,1.4-4.2,3.8s1.8,3.3,3.3,4c1.1,0.5,2,1,2,1.8c0,0.7-0.7,1.2-1.6,1.2c-0.8,0-1.7-0.3-2.9-0.9l-1,2.2 c1.4,0.8,2.7,1.1,4.1,1.1c2.7,0,4.3-1.5,4.3-4c0-2.4-1.8-3.2-3.3-3.9C4.2,5.3,3.3,4.9,3.3,4.1L3.3,4.1z M134.8,11.2V8.7h3.7V6.5 h-3.7V4.2h4.1V1.9h-6.8v9.5c0,0,0,0.9,0.6,1.5c0.4,0.4,0.9,0.6,1.6,0.6c2.1,0,4.4-0.1,4.7-0.1v-2.3H134.8L134.8,11.2z M128.6,9.1 L128.6,9.1L128.6,9.1c1-0.8,1.5-2,1.5-3.4c0-2.8-2.4-3.8-4.6-3.8c-1.1,0-2.8,0-3.8,0v11.6h2.9V10h0.9c0.2,0,0.3,0,0.5,0l0,0l0,0 c0.5,0.8,1.1,2,1.3,3.5c0.6,0,1.1,0,1.5,0h0.8c0.3,0,0.5,0,0.8,0C130.1,11.7,129.2,10,128.6,9.1L128.6,9.1z M127,6.1 c0,1.2-0.5,1.7-1.6,1.7c-0.4,0-0.6,0-0.8,0l0,0V4.1l0,0c0.3,0,0.6,0,0.9,0C126.8,4.1,127,4.9,127,6.1z M119.5,9.4V1.9h-3v7.7 c0,1.2-0.1,1.9-1.4,1.9c-1.3,0-1.5-0.6-1.5-1.9V1.9h-3v7.8c0,2.8,1.3,4,4.3,4C118.1,13.8,119.5,12.4,119.5,9.4L119.5,9.4z M106.8,4.3h2.2V1.9h-7.8v2.4h2.6v9.2h3C106.8,13.5,106.8,4.3,106.8,4.3z M98,13.5h3.1L98.3,1.9h-4.5L91,13.5h2.9l0.4-1.8h3.2 L98,13.5L98,13.5z M97.2,9.5h-2.5l1.2-5H96l0,0L97.2,9.5L97.2,9.5z M85.9,13.5h3.4v-13h-2.8l0.1,9.1l-4.2-9.1h-3.4v13h2.7l0-9 l0.1,0.2L85.9,13.5L85.9,13.5z"></path>
                      </svg>
                      <p><span>Springer Nature</span></p>

                      <p>

                      <img src="https://www.nature.com/a3byum8d/article/d41586-024-04050-5" width="1" height="1" alt=""/>
                    </p></div>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
    </article></div>
  </body>
</html>
