<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://klafyvel.me/blog/articles/fft-arduino/">Original</a>
    <h1>I over-engineered a Fast Fourier Transform for Arduino</h1>
    
    <div id="readability-page-1" class="page"><p>The lengthy, excruciating, details.</p><div> <div><p>Everything began with me wanting to implement the Fast Fourier Transform (FFT) on my Arduino Uno for a side project. The first thing you do in such case is asked your favorite search engine for existing solutions. If <a href="https://www.google.com/search?client=firefox-b-d&amp;q=arduino+FFT">you google &#34;arduino FFT&#34;</a> one of the first result will be related to this instructable: <a href="https://www.instructables.com/ApproxFFT-Fastest-FFT-Function-for-Arduino/"><em>ApproxFFT: The Fastest FFT Function for Arduino</em></a>. As you can imagine, this could only tickle my interest: there was an existing solution to my problem, and the title suggested that it was the fastest available! And thus, on April 18ᵗʰ 2021,<sup id="fnref:date"><a href="#fndef:date">[1]</a></sup> I started a journey that would bring me to write my own tutorial on implementing the FFT in Julia, learn AVR Assembly and write a blog post about it, about one year and a half later.</p> <p>There is a <a href="https://github.com/Klafyvel/AVR-FFT">companion GitHub repository</a> where you can retrieve all the codes presented in this article.</p> <div>  <p> This is the long version of the story. If you are only interested in nice plots showing the speed and the accuracy of my proposed solution, please head to the dedicated instructable : <a href="https://www.instructables.com/Faster-Than-the-Fastest-FFT-for-Arduino/">Faster than the Fastest FFT for Arduino</a> ! </p> </div> <table id="fndef:date"> <tbody><tr> <td><a href="#fnref:date">[1]</a> </td><td>Yes, <a href="https://support.mozilla.org/en-US/questions/937585#answer-369869">I went through my Firefox history database</a> to find this date. </td></tr></tbody></table> <hr/>  <div><ol><li><a href="#table_of_contents">Table of contents</a></li><li><a href="#why_reinvent_the_wheel">Why reinvent the wheel?</a><ol><li><a href="#because_i_did_not_know_how_to_implement_the_fft">Because I did not know how to implement the FFT.</a></li><li><a href="#because_i_thought_it_was_possible_to_do_better">Because I thought it was possible to do better.</a><ol><li><a href="#in-place_or_out-of-place_algorithm">In-place or out-of-place algorithm?</a></li><li><a href="#trigonometry_can_be_blazingly_fast">Trigonometry can be <em>blazingly fast</em>. 🚀🚀🚀 🔥🔥</a></li></ol></li></ol></li><li><a href="#interlude_some_tooling_for_debugging">Interlude: some tooling for debugging.</a><ol><li><a href="#using_arduino-cli_to_upload_your_code">Using <code>arduino-cli</code> to upload your code.</a></li><li><a href="#dont_bother_with_communication_protocols_over_serial">Don&#39;t bother with communication protocols over Serial.</a></li></ol></li><li><a href="#fast_accurate_fft_and_other_floating-point_trickeries">Fast, accurate FFT, and other floating-point trickeries.</a><ol><li><a href="#a_first_dummy_implementation_of_the_fft">A first dummy implementation of the FFT.</a></li><li><a href="#forbidden_occult_arts_are_fun">Forbidden occult arts are fun. 😈</a></li><li><a href="#approximate_floating-point_fft">Approximate floating-point FFT.</a></li></ol></li><li><a href="#how_fixed-point_arithmetic_came_to_the_rescue">How fixed-point arithmetic came to the rescue.</a><ol><li><a href="#fixed-point_multiplication">Fixed-point multiplication.</a></li><li><a href="#controlled_result_growth">Controlled result growth.</a></li><li><a href="#trigonometry_is_demanding">Trigonometry is demanding.</a></li><li><a href="#saturating_additions_aka_trigonometry_is_demanding_returns">Saturating additions. (a.k.a. &#34;Trigonometry is demanding&#34; returns.)</a></li><li><a href="#calculating_modules_with_a_chainsaw">Calculating modules with a chainsaw.</a></li><li><a href="#16_bits_fixed-point_fft">16 bits fixed-point FFT.</a></li><li><a href="#8_bits_fixed-point_fft">8 bits fixed-point FFT.</a></li><li><a href="#implementing_fixed-point_fft_for_longer_inputs">Implementing fixed-point FFT for longer inputs</a></li></ol></li><li><a href="#benchmarking_all_these_solutions">Benchmarking all these solutions.</a></li><li><a href="#closing_thoughts">Closing thoughts.</a></li></ol></div> <hr/>  <p>As I said in the introduction, I explicitly researched an implementation of the FFT because I did not want to implement my own. So what changed my mind ?</p> <h2 id="because_i_did_not_know_how_to_implement_the_fft"><a href="#because_i_did_not_know_how_to_implement_the_fft">Because I did not know how to implement the FFT.</a></h2> <p>Let&#39;s start with the obvious: abhilash_patel&#39;s instructable is a <strong>Great</strong> instructable. It is part of a series of instructables on implementing the FFT on Arduino, and this is his fastest accurate implementation. The instructable does a great job at explaining the big ideas behind it, with not only appropriate, but also good-looking illustrations. That is why I decided to read his code, to be certain of my good understanding of it.</p> <p>And that is the exact moment I entered an infinite spiral. Not because the code was bad, even though it could use some indenting, but because I did not understand how it achieves its purpose. To my own disappointment, I realized that maybe I did not know how to implement an FFT. Sure, I had my share of lectures on the Fourier Transform, and on the Fast Fourier Transform, but the lecturers only showed us how the FFT was an algorithm with a very nice complexity through its recursive definition. But what I was looking at did not even remotely look like what I expected to see.</p> <p>So I did what seemed the most sensible thing to me at the time: I spent nights reading Wikipedia pages and obscure articles on 2000s looking website to understand how the FFT was <em>actually</em> implemented. </p> <p>About one month later, on May 23ʳᵈ, I started writing a tutorial on zestedesavoir.com : <a href="https://zestedesavoir.com/tutoriels/3939/jouons-a-implementer-une-transformee-de-fourier-rapide/">&#34;Jouons à implémenter une transformée de Fourier rapide !&#34;</a>, a sloppy translation of which is also available on <a href="https://mytimeatrecurse.substack.com/blog/articles/fft-julia/">my blog</a>. My goal here was to write down what I had learned throughout the month, and it helped me clarify the math behind the implementation. Today, I use it as a reference when I have doubts on the implementation. </p> <p>With this newly acquired knowledge on FFT implementations, I was ready to have another look at @abhilash_patel&#39;s code.</p> <h2 id="because_i_thought_it_was_possible_to_do_better"><a href="#because_i_thought_it_was_possible_to_do_better">Because I thought it was possible to do better.</a></h2> <p>As I said, I was now capable of understanding the code provided by @abhilash_patel. And there I found two low-hanging fruits:</p> <ul> <li><p>The program was weirdly mixing in-place and out-of-place algorithm,</p> </li><li><p>The trigonometry computation was inefficient.</p> </li></ul> <p>Let me state more clearly what I mean here.</p> <h3 id="in-place_or_out-of-place_algorithm"><a href="#in-place_or_out-of-place_algorithm">In-place or out-of-place algorithm?</a></h3> <p>The FFT can either be implemented <em>in-place</em> or <em>out-of-place</em>. Implementing <em>out-of-place</em> of course allows you to keep the input data unchanged by the computation. However, the <em>in-place</em> algorithm offers several key advantages, the first, obvious, one being that it only requires the amount of space needed to store the input array.</p> <p>This might not be obvious, but it also works for real-valued signals. Indeed, one might think that if you have an array of, say, <code>float</code> representing such a signal, its FFT would require twice the amount of space since the Fourier transform is complex-valued. The trick here is to use a key property of the Fourier transform : the Fourier transform of a real-valued signal, knowing the positive-frequencies part is enough. You can see the full explanation in my <a href="https://mytimeatrecurse.substack.com/blog/articles/fft-julia/#the_special_case_of_a_real_signal">blog post on implementing the FFT in Julia</a>.</p> <p>This would help me get an FFT implementation that can run on more than 256 data points on my Arduino Uno, which the original instructable implementation cannot.<sup id="fnref:sizerequirement"><a href="#fndef:sizerequirement">[2]</a></sup></p> <table id="fndef:sizerequirement"> <tbody><tr> <td><a href="#fnref:sizerequirement">[2]</a> </td><td>Even though the code used for the benchmark cannot. This is not due to a memory size issue, but to the variable types I used for my buffers (<code>uint8_t</code>). I think you can understand this would be easily fixed to run the FFT on bigger samples, and since I was especially interested in benchmarks in time, I allowed myself that. </td></tr></tbody></table> <h3 id="trigonometry_can_be_blazingly_fast"><a href="#trigonometry_can_be_blazingly_fast">Trigonometry can be <em>blazingly fast</em>. 🚀🚀🚀 🔥🔥</a></h3> <p>I believe this is where the biggest improvement in benchmark-time originates from. <a href="https://www.instructables.com/ApproxFFT-Fastest-FFT-Function-for-Arduino/#step2">Step 2 of the original instructable</a> details how to use a kind of look-up table to compute very quickly the trigonometry functions. This is an efficient method if you have to implement a fast cosine or a fast sine function. However, using such a method for the FFT means forgetting a very interesting property of the algorithm : the angles for which trigonometry calculations is required do not appear at random <strong>at all</strong>. In fact for each recursion step of the algorithm, they increase by a constant amount, and always start from the same angle : 0.</p> <p>This arithmetical progression of the angle allows using a simple, yet efficient formula for calculating the next sine and cosine :</p> <p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.2500em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo>+</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>−</mo><mo stretchy="false">[</mo><mi>α</mi><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>+</mo><mi>β</mi><mi>sin</mi><mo>⁡</mo><mi>θ</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo>+</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>sin</mi><mo>⁡</mo><mi>θ</mi><mo>−</mo><mo stretchy="false">[</mo><mi>α</mi><mi>sin</mi><mo>⁡</mo><mi>θ</mi><mo>−</mo><mi>β</mi><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}\cos(\theta + \delta) &amp;= \cos\theta - [\alpha \cos\theta + \beta\sin\theta]\\\sin(\theta + \delta) &amp;= \sin\theta - [\alpha\sin\theta - \beta\cos\theta]\end{aligned}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span>cos</span><span>(</span><span>θ</span><span></span><span>+</span><span></span><span>δ</span><span>)</span></span></span><span><span></span><span><span>sin</span><span>(</span><span>θ</span><span></span><span>+</span><span></span><span>δ</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>=</span><span></span><span>cos</span><span></span><span>θ</span><span></span><span>−</span><span></span><span>[</span><span>α</span><span></span><span>cos</span><span></span><span>θ</span><span></span><span>+</span><span></span><span>β</span><span></span><span>sin</span><span></span><span>θ</span><span>]</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span>sin</span><span></span><span>θ</span><span></span><span>−</span><span></span><span>[</span><span>α</span><span></span><span>sin</span><span></span><span>θ</span><span></span><span>−</span><span></span><span>β</span><span></span><span>cos</span><span></span><span>θ</span><span>]</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></p><p>With <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>2</mn><msup><mrow><mi>sin</mi><mo>⁡</mo></mrow><mn>2</mn></msup><mrow><mo fence="true">(</mo><mfrac><mi>δ</mi><mn>2</mn></mfrac><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mtext> </mtext><mi>β</mi><mo>=</mo><mi>sin</mi><mo>⁡</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">\alpha = 2\sin^2\left(\frac{\delta}{2}\right),\;\beta=\sin\delta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>α</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span><span></span><span><span>sin</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>δ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span>)</span></span></span><span></span><span>,</span><span></span><span></span><span>β</span><span></span><span>=</span><span></span></span><span><span></span><span>sin</span><span></span><span>δ</span></span></span></span>.</p> <p>I have included the derivation of these formulas in <a href="https://mytimeatrecurse.substack.com/blog/articles/fft-julia/#optimization_of_trigonometric_functions">the relevant section of my tutorial</a>.</p> <p>As I said, this is most likely the biggest source of improvement in execution time, as trigonometry computation-time instantaneously becomes negligible using this trick.</p>  <p>I am a big fan of the <a href="https://julialang.org/">Julia programming language</a>. It is my main programming tool at work, and I also use it for my hobbies. However, I believe the tips given in this section are easily transportable to other programming languages.</p> <p>The main idea here is that when you start working with arrays of data, good old <code>Serial.println</code> is not usable anymore. Because you cannot simply evaluate the correctness of your results at a simple glance, you want to use higher level tools, such as statistical analysis or plotting libraries. And since you are also likely to want to upload your code to the Arduino often, it is convenient to be able to upload it programmatically.</p> <p>This machinery allows testing all the different implementations in a reproducible way. All the examples given in this article are calculated on the following input signal.</p> <figure> <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/test_signal.png" alt="Input signal used in the tests below."/> <figcaption> Input signal used in the tests below.</figcaption> </figure> <h2 id="using_arduino-cli_to_upload_your_code"><a href="#using_arduino-cli_to_upload_your_code">Using <code>arduino-cli</code> to upload your code.</a></h2> <p>At the time I started this project, the <a href="https://docs.arduino.cc/software/ide-v2">new Arduino IDE</a> wasn&#39;t available yet. If you have ever used the <code>1.x</code> versions of the IDE, then you know why one would like to avoid the old IDE. Thankfully, there is a command-line utility that allows uploading code from your terminal: <a href="https://arduino.github.io/arduino-cli/0.28/"><code>arduino-cli</code></a>. If you take a look at the GitHub <a href="https://github.com/Klafyvel/AVR-FFT">repository</a>, you&#39;ll notice a Julia script, which purpose is to upload code to the Arduino and retrieve the results of computations and benchmarks. The upload part is simply a system call to <code>arduino-cli</code>.</p> <pre><code><span>function</span> upload_code(directory)
    build = joinpath(workdir, directory, <span>&#34;build&#34;</span>)
    ino = joinpath(workdir, directory, directory * <span>&#34;.ino&#34;</span>)

    build_command = <span>`arduino-cli compile -b arduino:avr:uno -p <span>$portname</span> --build-path &#34;<span>$build</span>&#34; -u -v &#34;<span>$ino</span>&#34;`</span>
    run(pipeline(build_command, <span>stdout</span>=<span>&#34;log_arduino-cli.txt&#34;</span>, <span>stderr</span>=<span>&#34;log_arduino-cli.txt&#34;</span>))
<span>end</span></code></pre> <h2 id="dont_bother_with_communication_protocols_over_serial"><a href="#dont_bother_with_communication_protocols_over_serial">Don&#39;t bother with communication protocols over Serial.</a></h2> <p>At first, I was tempted to use some fancy communication protocols for the serial link. This is not useful in our case, because you can simply reset the Arduino programmatically to ensure the synchronization of the computer and the development board, and then exchange raw binary data.</p> <p>Resetting is done <a href="https://stackoverflow.com/a/21082531">using the DTR pin of the port</a>. In Julia, you can do this like this using the <a href="https://github.com/JuliaIO/LibSerialPort.jl"><code>LibSerialPort.jl</code></a> library:</p> <pre><code><span>function</span> reset_arduino()
    LibSerialPort.open(portname, baudrate) <span>do</span> sp
        <span>@info</span> <span>&#34;Resetting Arduino&#34;</span>
        
        set_flow_control(sp, dtr=SP_DTR_ON)
        sleep(<span>0.1</span>)
        set_flow_control(sp, dtr=SP_DTR_OFF)
        sp_flush(sp, SP_BUF_INPUT)
        sp_flush(sp, SP_BUF_OUTPUT)
    <span>end</span>
<span>end</span></code></pre> <p>Because your computer can now reset the Arduino at will, you can easily ensure the synchronization of your board. That means the benchmark script knows when to read data from the Arduino. </p> <p>Then, the Arduino would send data to the computer like this:</p> <pre><code>Serial.<span>write</span>((byte*)data, <span>sizeof</span>(<span>fixed_t</span>)*N);</code></pre>
<p>This way, the array <code>data</code> is sent directly through the serial link as a stream of raw bytes. We don&#39;t bother with any form of encoding.</p>
<p>On the computer side, you can easily read the incoming data:</p>
<pre><code>data = zeros(retrieve_datatype, n_read)
read!(sp, data)</code></pre>
<p>Where <code>sp</code> is an object created by <code>LibSerialPort.jl</code> when opening a port.</p>
<p>You can then happily analyze your data, it&#39;s <a href="https://dataframes.juliadata.org/stable/"><code>DataFrames.jl</code></a> and <a href="https://docs.makie.org/stable/"><code>Makie.jl</code></a> time !</p>

<p>My first approach was to re-use as much as I could the code I wrote for my FFT tutorial in Julia. That&#39;s why I started working with floating-point arithmetic. This also was convenient because it kept away some issues like overflowing numbers, that I had to address once I started working with fixed-point arithmetic.</p>
<h2 id="a_first_dummy_implementation_of_the_fft"><a href="#a_first_dummy_implementation_of_the_fft">A first dummy implementation of the FFT.</a></h2>
<p>As I said, my first implementation was a simple, stupid translation of one of the codes presented in my Julia tutorial. I did not even bother with writing optimized trigonometry functions, I just wanted something that worked as a basis for other implementations. The code is fairly simple and can be viewed <a href="https://github.com/Klafyvel/AVR-FFT/blob/main/ExactFFT/ExactFFT.ino">here</a>.</p>
<p>As expected, this gives almost error-free results.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/results_ExactFFT.svg" alt="Module of approximate floating-point FFT on Arduino. Comparison with reference implementation."/> 
<figcaption> Module of approximate floating-point FFT on Arduino. Comparison with reference implementation.</figcaption>
</figure>

<h2 id="forbidden_occult_arts_are_fun"><a href="#forbidden_occult_arts_are_fun">Forbidden occult arts are fun. 😈</a></h2>
<p>Now let&#39;s move on to more interesting stuffs. The first obvious improvement you can make on the base implementation is <a href="#trigonometry_can_be_blazingly_fast">fast trigonometry</a>, and that&#39;s what yields the biggest improvement in terms of speed. Then, I decided to mess around with IEEE-754 to write my own approximate routines for float multiplication, halving and modulus calculation. The idea is always the same: treat IEEE-754 representation of a floating-point number as its logarithm. This does give <a href="https://github.com/Klafyvel/AVR-FFT/blob/72410901891639147376c9a900ef97132eb6e807/FloatFFT/FloatFFT.ino#L346-L376">weird-looking implementations</a> though. I have written several posts on Zeste-de-Savoir explaining how all these work. It is in French, but I trust you can make DeepL run!</p>
<ul>
<li><p><a href="https://zestedesavoir.com/billets/4153/approximer-rapidement-le-carre-dun-nombre-flottant/">&#34;Approximer rapidement le carré d&#39;un nombre flottant&#34;</a> explains how to square a number using its floating-point representation.</p>

</li><li><p><a href="https://zestedesavoir.com/billets/4199/ieee-754-quand-votre-code-prend-la-float/">&#34;IEEE 754: Quand votre code prend la float&#34;</a> explains how the IEEE-754 representation of a number looks alike it&#39;s logarithm.</p>

</li><li><p><a href="https://zestedesavoir.com/billets/4226/multiplications-avec-arduino-jetons-nous-a-la-float/">&#34;Multiplications avec Arduino: jetons-nous à la float&#34;</a> explains how the approximate multiplication of two floating-point numbers can be efficiently calculated.</p>

</li></ul>
<h2 id="approximate_floating-point_fft"><a href="#approximate_floating-point_fft">Approximate floating-point FFT.</a></h2>
<p>Without further delay, here is a sneak preview of the result I got with the approximate floating-point FFT. For a full benchmark, you will have to wait for the end of this article! The code is available <a href="https://github.com/Klafyvel/AVR-FFT/blob/main/FloatFFT/FloatFFT.ino">here</a>.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/results_FloatFFT.svg" alt="Module of approximate floating-point FFT on Arduino. Comparison with reference implementation."/> 
<figcaption> Module of approximate floating-point FFT on Arduino. Comparison with reference implementation.</figcaption>
</figure>


<p>Rather than endlessly optimizing the floating-point implementation, I decided to change my approach. The main motivation being: <strong>Floats are actually overkill for our purpose</strong>. Indeed, they have the ability to represent numbers with a good relative precision over enormous ranges. However, when calculating FFTs the range output variables may cover can indeed vary, but not that much. And most importantly, it varies <strong>predictably</strong>. This means a <strong>fixed-point</strong> representation can be used. Also, because of their amazing properties Floats actually take a lot of space in the limited RAM available on a microcontroller. And finally, I want to be able to run FFTs on signal read from Arduino&#39;s ADC. If my program can deal with <code>int</code>-like data types, then it&#39;ll spare me the trouble of converting from integers to floating-points.</p>
<h2 id="fixed-point_multiplication"><a href="#fixed-point_multiplication">Fixed-point multiplication.</a></h2>
<p>I first played with the idea of implementing a fixed-point FFT because I realized the <a href="http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf#_OPENTOPIC_TOC_PROCESSING_d94e3581">AVR instruction set</a> gives us the <code>fmul</code> instruction, dedicated to multiplying fixed-point numbers. This means we can use it to have a speed-efficient implementation of the multiplication, that should even beat the custom <code>float</code> one.</p>
<p>I wrote a <a href="https://zestedesavoir.com/contenus/4258/en-periode-de-canicule-une-idee-fixe-economiser-la-float/">blog-post</a> on Zeste-de-Savoir (in French) on implementing the fixed-point multiplication. It is based on the proposed implementation in the AVR instruction set manual.</p>
<pre><code>
<span><span>fixed_t</span> <span>fixed_mul</span><span>(<span>fixed_t</span> a, <span>fixed_t</span> b)</span> </span>{
  <span>fixed_t</span> result;
  <span>asm</span> (
      
      <span>&#34;clr r2&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fmuls %B[a],%B[b]&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;movw %A[result],__tmp_reg__&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;mov __tmp_reg__,%B[a]&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;eor __tmp_reg__,%B[b]&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;eor __tmp_reg__,%B[result]&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fmul %A[a],%A[b]&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;adc %A[result],r2&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;movw r18,__tmp_reg__&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;fmulsu %B[a],%A[b]&#34;</span> <span>&#34;\n\t&#34;</span> 
                                  
                                  
      <span>&#34;sbc %B[result],r2&#34;</span> <span>&#34;\n\t&#34;</span>
      
      <span>&#34;add r19,__tmp_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;adc %A[result],__zero_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;adc %B[result],r2&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fmulsu %B[b],%A[a]&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;sbc %B[result],r2&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;add r19,__tmp_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;adc %A[result],__zero_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;adc %B[result],r2&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;clr __zero_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
      :
      [result]<span>&#34;+r&#34;</span>(result):
      [a]<span>&#34;a&#34;</span>(a),[b]<span>&#34;a&#34;</span>(b):
      <span>&#34;r2&#34;</span>,<span>&#34;r18&#34;</span>,<span>&#34;r19&#34;</span>
  );
  <span>return</span> result;
}</code></pre>
<p>Obviously, you can also create the same function for 8-bits fixed-point arithmetic.</p>
<pre><code><span><span>fixed8_t</span> <span>fixed_mul_8_8</span><span>(<span>fixed8_t</span> a, <span>fixed8_t</span> b)</span> </span>{
  <span>fixed8_t</span> result;

  <span>asm</span> (
    <span>&#34;fmuls %[a],%[b]&#34;</span> <span>&#34;\n\t&#34;</span>
    <span>&#34;mov %[result],__zero_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
    <span>&#34;clr __zero_reg__&#34;</span> <span>&#34;\n\t&#34;</span>
    :
    [result]<span>&#34;+r&#34;</span>(result):
    [a]<span>&#34;a&#34;</span>(a),[b]<span>&#34;a&#34;</span>(b)
  );
  <span>return</span> result;
}</code></pre>
<p>As you can see, this requires writing some assembly code because the <code>fmul</code> instruction is not directly accessible from C. However, even though it is fairly simple, this limits the implementation to AVR platforms. You might still get some reasonably efficient code by implementing everything in pure C, and extend the implementation to other platforms.</p>
<h2 id="controlled_result_growth"><a href="#controlled_result_growth">Controlled result growth.</a></h2>
<p>As I said before, the FFT grows predictably. First, we can see that the final Fourier transform is bounded. Recall that the FFT is an algorithm to compute the Discrete Fourier Transform (DFT), which is written:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.2500em" columnalign="right left right" columnspacing="0em 1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>X</mi><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>i</mi><mi>π</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
X[k] &amp;=&amp; \sum_{n=0}^{N-1}x[n]e^{-2i\pi nk/N}
\end{aligned}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span>X</span><span>[</span><span>k</span><span>]</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>=</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>n</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>N</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>x</span><span>[</span><span>n</span><span>]</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>−</span><span>2</span><span>iπnk</span><span>/</span><span>N</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span> is the discrete Fourier transform of the input signal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span></span></span></span> of size <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>. From that we have:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.2500em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mi mathvariant="normal">∣</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≤</mo><mrow><mo fence="true">∣</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>i</mi><mi>π</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo fence="true">∣</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≤</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mo fence="true">∣</mo><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>i</mi><mi>π</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow></msup><mo fence="true">∣</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≤</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mo fence="true">∣</mo><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo fence="true">∣</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≤</mo><mi>N</mi><mo>×</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>n</mi></munder><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mi mathvariant="normal">∣</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
|X[k]| &amp;\leq \left|\sum_{n=0}^{N-1}x[n]e^{-2i\pi nk/N}\right|\\
&amp;\leq \sum_{n=0}^{N-1}\left|x[n]e^{-2i\pi nk/N}\right| \\
&amp;\leq \sum_{n=0}^{N-1}\left|x[n]\right|\\
&amp;\leq N\times\max_n|x[n]|
\end{aligned}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span>∣</span><span>X</span><span>[</span><span>k</span><span>]</span><span>∣</span></span></span><span><span></span><span></span></span><span><span></span><span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>≤</span><span></span><span><span><span><span><span><span><span><span></span><span><span>∣</span></span></span><span><span></span><span><svg width="0.333em" height="1.8160299999999998em" style="width:0.333em" viewBox="0 0 333 1816" preserveAspectRatio="xMinYMin"><path d="M145 0 H188 V1816 H145z M145 0 H188 V1816 H145z"></path></svg></span></span><span><span></span><span><span>∣</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>n</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>N</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>x</span><span>[</span><span>n</span><span>]</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>−</span><span>2</span><span>iπnk</span><span>/</span><span>N</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><span>∣</span></span></span><span><span></span><span><svg width="0.333em" height="1.8160299999999998em" style="width:0.333em" viewBox="0 0 333 1816" preserveAspectRatio="xMinYMin"><path d="M145 0 H188 V1816 H145z M145 0 H188 V1816 H145z"></path></svg></span></span><span><span></span><span><span>∣</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span><span></span><span></span><span>≤</span><span></span><span><span><span><span><span><span></span><span><span><span>n</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>N</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span><span><span></span><span><span>∣</span></span></span><span><span></span><span><svg width="0.333em" height="0.61601em" style="width:0.333em" viewBox="0 0 333 616" preserveAspectRatio="xMinYMin"><path d="M145 0 H188 V616 H145z M145 0 H188 V616 H145z"></path></svg></span></span><span><span></span><span><span>∣</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>x</span><span>[</span><span>n</span><span>]</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>−</span><span>2</span><span>iπnk</span><span>/</span><span>N</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><span>∣</span></span></span><span><span></span><span><svg width="0.333em" height="0.61601em" style="width:0.333em" viewBox="0 0 333 616" preserveAspectRatio="xMinYMin"><path d="M145 0 H188 V616 H145z M145 0 H188 V616 H145z"></path></svg></span></span><span><span></span><span><span>∣</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span><span></span><span></span><span>≤</span><span></span><span><span><span><span><span><span></span><span><span><span>n</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>N</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>∣</span><span>x</span><span>[</span><span>n</span><span>]</span><span>∣</span></span></span></span><span><span></span><span><span></span><span></span><span>≤</span><span></span><span>N</span><span></span><span>×</span><span></span><span><span><span><span><span><span></span><span><span>n</span></span></span><span><span></span><span><span>max</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>∣</span><span>x</span><span>[</span><span>n</span><span>]</span><span>∣</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></p><p>In our case, because we use the <code>Q0f7</code> fixed point format, the input signal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span></span></span></span> is in the range <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>[</span><span>−</span><span>1</span><span>,</span><span></span><span>1</span><span>]</span></span></span></span>. That means the components of the DFT are within range <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>N</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-N,N]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>[</span><span>−</span><span>N</span><span>,</span><span></span><span>N</span><span>]</span></span></span></span>. Note that these bounds are attained for some signals, <em>e.g.</em> a constant input.</p>
<p>With that, we know how to scale the result of the FFT so that it can be stored. But what about the intermediary steps ? How do we ensure that the intermediary values stay within range? You may recall from <a href="https://mytimeatrecurse.substack.com/blog/articles/fft-julia/#analysis_of_the_first_implementation">the blog post explaining  FFT</a> this kind of &#34;butterfly&#34; diagrams:</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-julia/radix2_inv.png" alt="Butterfly diagram of an FFT on 8 points input signal. Each column represents a 
  step in the algorithm, and each line is a case of the array. The various polygons identify cases that are part of the same subdivision of the array,
  and the arrows show how we combine them to go the next step of the algorithm."/> 
<figcaption> Butterfly diagram of an FFT on 8 points input signal. Each column represents a 
  step in the algorithm, and each line is a case of the array. The various
  polygons identify cases that are part of the same subdivision of the array,
  and the arrows show how we combine them to go the next step of the
  algorithm.</figcaption>
</figure>

<p>This diagram also shows you that each step of the algorithm actually performs some FFTs on input signals of smaller sizes. That means our bounding rule applies for intermediary signals, given that we plug the right size of input signal in the formula! Notice how at each step, corresponding sub-FFTs have a size of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">2^{i}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span></span></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span> is the number of the step, starting at 0. That basically means that if we scale down the signal between each step by dividing it by a factor of two, we will keep the signal bounded in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>[</span><span>−</span><span>1</span><span>,</span><span></span><span>1</span><span>]</span></span></span></span> at each step!</p>
<p>Note that this does not mean we get the optimal scale for every input signal. For example, signals which are poorly periodic would have a lot of low module Fourier coefficients, and would not fully take advantage of the scale offered by our representation. I did some tests scaling the array only when it was needed, and did not notice many changes in terms of execution times, so that&#39;s something you might want to explore if your project requires it.</p>
<h2 id="trigonometry_is_demanding"><a href="#trigonometry_is_demanding">Trigonometry is demanding.</a></h2>
<blockquote>
<p>If all you have is a hammer, everything looks like a nail.</p>
<p>~ <a href="https://en.wikipedia.org/wiki/Law_of_the_instrument#Abraham_Maslow">Abraham Maslow</a></p>
</blockquote>
<p>Once I had fixed-point arithmetic working, I started wanting to use it everywhere. But I quickly encountered an issue: trigonometry stopped working.</p>
<p>The reason is simple, 8-bits precision is not enough for trigonometry calculations when we approach the small angles. The key point here, is that the precision needed for fixed-point calculation of trigonometry functions depends on the size of the input array. Recall from section <a href="#trigonometry_can_be_blazingly_fast">Trigonometry can be blazingly fast. 🚀🚀🚀 🔥🔥</a> that we need to precompute values for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>α</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>β</span></span></span></span>, where</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>α</mi><mo>=</mo><mn>2</mn><msup><mrow><mi>sin</mi><mo>⁡</mo></mrow><mn>2</mn></msup><mrow><mo fence="true">(</mo><mfrac><mi>δ</mi><mn>2</mn></mfrac><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi>β</mi><mo>=</mo><mi>sin</mi><mo>⁡</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">\alpha = 2\sin^2\left(\frac{\delta}{2}\right),\quad\beta=\sin\delta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>α</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span><span></span><span><span>sin</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>δ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span>)</span></span></span><span></span><span>,</span><span></span><span></span><span>β</span><span></span><span>=</span><span></span></span><span><span></span><span>sin</span><span></span><span>δ</span></span></span></span></span></p><p>And <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>δ</span></span></span></span> is the angle increment by which we want to increase the angle of the complex number we are summing with in the FFT. This angle depends on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>, the total length of the input array, and is equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{2\pi}{N}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>2</span><span>π</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span>. That means we need to be able to represent at least <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mrow><mi>sin</mi><mo>⁡</mo></mrow><mn>2</mn></msup><mfrac><mi>π</mi><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">2\sin^2\frac{\pi}{N}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span></span><span><span>sin</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>N</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>π</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span> for trigonometry to work. For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">N=256</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>256</span></span></span></span>, this is approximately equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.000301</mn></mrow><annotation encoding="application/x-tex">0.000301</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>0.000301</span></span></span></span>. Unfortunately, the lowest number one can represent using <code>Q0f7</code> fixed point representation, that is with 7 bits in the fractional part, is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup><mo>=</mo><mn>0.0078125</mn></mrow><annotation encoding="application/x-tex">2^{-7}=0.0078125</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span><span>−</span><span>7</span></span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>0.0078125</span></span></span></span>. That is why even for the 8 bit fixed point FFT, trigonometry calculations are performed using 16 bits fixed point arithmetic.</p>
<p>This limit on trigonometry also explains why the code presented here is not usable &#34;as is&#34; for very long arrays. Indeed, while 512 cases-long arrays could be handled using 16-bits trigonometry, the theoretical limit for an Arduino Uno would be 1024 cases-long arrays (because RAM is 2048 bytes, and we need some space for temporary variables), and that would require 32-bits trigonometry, which I did not implement.</p>
<h2 id="saturating_additions_aka_trigonometry_is_demanding_returns"><a href="#saturating_additions_aka_trigonometry_is_demanding_returns">Saturating additions. (a.k.a. &#34;Trigonometry is demanding&#34; returns.)</a></h2>
<p>One other issue with trigonometry I did not see coming is its sensitivity to overflow. Since there is basically no protection against it, overflowing a fixed-point representation of a number flips the sign. In the case of trigonometry this is especially annoying, because that means we add a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>π</span></span></span></span> phase error for even the slightest error when values are close to one. And trust me, it took me some time to understand where the error was coming from. </p>
<p>To mitigate this, I had to implement my own addition, that saturates to one instead of flipping the sign when overflow happens. The trick here is to use the status register (<code>SREG</code>) of the microcontroller to detect overflow. Again this requires doing the addition in assembly, as the check needs to happen right after the addition was performed, and there is no way to tell what the compiler might do between the addition and the actual check. </p>
<p>Checking overflow is done using the <a href="http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf#_OPENTOPIC_TOC_PROCESSING_d94e3581"><code>brvc</code></a> instruction (<em>Branch if Overflow Cleared</em>), and the function for 16-bits saturating addition goes like this:</p>
<pre><code>
<span><span>fixed_t</span> <span>fixed_add_saturate</span><span>(<span>fixed_t</span> a, <span>fixed_t</span> b)</span> </span>{
  <span>fixed_t</span> result;
  <span>asm</span> (
      <span>&#34;movw %A[result], %A[a]&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;add %A[result],%A[b]&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;adc %B[result],%B[b]&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;brvc fixed_add_saturate_goodbye&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;subi %B[result], 0&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;brmi fixed_add_saturate_plus_one&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fixed_add_saturate_minus_one:&#34;</span> <span>&#34;\n\t&#34;</span> 
      <span>&#34;ldi %B[result],0x80&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;ldi %A[result],0x00&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;jmp fixed_add_saturate_goodbye&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fixed_add_saturate_plus_one:&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;ldi %B[result],0x7f&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;ldi %A[result],0xff&#34;</span> <span>&#34;\n\t&#34;</span>
      <span>&#34;fixed_add_saturate_goodbye:&#34;</span> <span>&#34;\n\t&#34;</span>
      :
      [result]<span>&#34;+d&#34;</span>(result):
      [a]<span>&#34;r&#34;</span>(a),[b]<span>&#34;r&#34;</span>(b)
  );

  <span>return</span> result;
}</code></pre>
<p>One might be tempted to use this routine for every single addition performed in the program. This is actually useless, since additions in the actual FFT algorithm will not overflow thanks to scaling, if they are done in a sensible order (check the code if you want to see how!).</p>
<h2 id="calculating_modules_with_a_chainsaw"><a href="#calculating_modules_with_a_chainsaw">Calculating modules with a chainsaw.</a></h2>
<p>After a lot of wandering on the Internets, I ended up using <a href="http://www.azillionmonkeys.com/qed/sqroot.html#distance">Paul Hsieh&#39;s technique for computing approximate modules of vectors</a>. However, while writing this article I discovered some mistakes and things that could be improved in his article, so I ended up writing <a href="https://mytimeatrecurse.substack.com/blog/articles/approximate-euclidian-norm/">a dedicated article on this</a>, showing how you can minimize the mean square error, and get at most a 5.3% error.</p>
<p>The main idea is that you can approximate the unit circle using a set of well-chosen octagons. That reminds me of what a rough cylinder carved using a chainsaw might look like, hence the name of this section.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/approximate-euclidian-norm/code/output/illustration5.svg" alt="One of the figures of the article on approximating the norm. Look at how this look like something carved using a chainsaw!"/> 
<figcaption> One of the figures of the article on approximating the norm. Look at how this look like something carved using a chainsaw!</figcaption>
</figure>

<h2 id="16_bits_fixed-point_fft"><a href="#16_bits_fixed-point_fft">16 bits fixed-point FFT.</a></h2>
<p>Enough small talk, time for some action! You can find <a href="https://github.com/Klafyvel/AVR-FFT/blob/main/Fixed16FFT/Fixed16FFT.ino">here</a> the code for 16-bits fixed-point FFT. The benchmark is available at the end of this article, but in the meantime here is the error comparison against reference implementation.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/results_Fixed16FFT.svg" alt="Calculated module of the Fourier transform of the input signal using
16-bits fixed-points arithmetic for various input signal lengths. Comparison with reference implementation."/> 
<figcaption> Calculated module of the Fourier transform of the input signal using
16-bits fixed-points arithmetic for various input signal lengths. Comparison with reference implementation.</figcaption>
</figure>

<h2 id="8_bits_fixed-point_fft"><a href="#8_bits_fixed-point_fft">8 bits fixed-point FFT.</a></h2>
<p>And now the fastest FFT on Arduino that I implemented, the 8-bits fixed-point FFT! As for previous implementations, you can find the code <a href="https://github.com/Klafyvel/AVR-FFT/blob/main/Fixed8FFT/Fixed8FFT.ino">here</a>. Below is a comparison of the calculated module of the FFT against a reference implementation.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/results_Fixed8FFT.svg" alt="Calculated module of the Fourier transform of the input signal using
8-bits fixed-points arithmetic for various input signal lengths. Comparison with reference implementation."/> 
<figcaption> Calculated module of the Fourier transform of the input signal using
8-bits fixed-points arithmetic for various input signal lengths. Comparison with reference implementation.</figcaption>
</figure>

<h2 id="implementing_fixed-point_fft_for_longer_inputs"><a href="#implementing_fixed-point_fft_for_longer_inputs">Implementing fixed-point FFT for longer inputs</a></h2>
<p>The Arduino Uno has 2048 bytes of RAM. But because this implementation of the FFT needs an input array whose length is a power of two, and because you need some space for variables,<sup id="fnref:determined"><a href="#fndef:determined">[3]</a></sup> the limit would be a 1024 bytes long FFT. But the code presented here would have to be modified a bit (not that much). From where I am standing I see two major issues:</p>
<ol>
<li><p>As discussed previously, trigonometry would need 32-bits arithmetic. That means you would need to implement the multiplication and saturating addition for those numbers.</p>

</li><li><p>The buffers are single bytes right now, so you would need to upgrade them to 16-bits buffers.</p>

</li></ol>
<p>Once those two issues, and the inevitable hundreds of other issues I did not think of are addressed, I don&#39;t see why one could not perform FFT on 1024 bytes-long input arrays.</p>
<table id="fndef:determined">
    <tbody><tr>
        <td><a href="#fnref:determined">[3]</a>
        </td><td>Although I am sure a <em>very</em> determined person would be able to fit all the temporary variables in registers and calculate a 2048 bytes-long FFT. <strong>Do it, I vouch for you, you beautiful nerd!</strong>
    
</td></tr></tbody></table>


<p>I won&#39;t go into the details of how I do the benchmarks here, it&#39;s basically just using the Arduino <code>micros()</code> function. I present here only two benchmarks: how much time is required to run the FFT, and how &#34;bad&#34; the result is, measured with the <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>. Now, this is not the perfect way to measure the error made by the algorithm, so I do encourage you to have a look at the different comparison plots above. You will also notice that <code>ApproxFFT</code> seems to perform poorly in terms of error for small-sized input arrays. This is because it does not compute the result for frequency 0, so the error is probably over-estimated. Overall, I think it is safe to say that <code>ApproxFFT</code> and <code>Fixed16FFT</code> introduce the same amount of errors in the calculation. Notice how <code>ExactFFT</code> is <em>literally</em> billions times more precise than the other FFT algorithms. For 8-bits algorithms, the <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)#Noise_and_error_characteristics">quantization</a> mean squared error is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup><mi mathvariant="normal">/</mi><msub><mrow></mrow><mn>3</mn></msub><mi>L</mi><mi>S</mi><msup><mi>B</mi><mn>2</mn></msup><mo>≈</mo><mn>2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">{}^1/{}_3 LSB^2\approx2\times10^{-5}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>1</span></span></span></span></span></span></span></span><span>/</span><span><span></span><span><span><span><span><span><span></span><span><span>3</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>L</span><span>S</span><span><span>B</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>≈</span><span></span></span><span><span></span><span>2</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>−</span><span>5</span></span></span></span></span></span></span></span></span></span></span></span>, which means there are still sources of error introduced in the algorithm other than simple quantization. The same goes for <code>ApproxFFT</code> and <code>Fixed16FFT</code>, where the quantization error is approximately <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>10</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3\times10^{-10}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>−</span><span>10</span></span></span></span></span></span></span></span></span></span></span></span>.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/error_comparison.svg" alt="Mean-square error benchmark. The y-axis has a logarithmic scale, so you can see how much better `ExactFFT` performs!"/> 
<figcaption> Mean-square error benchmark. The y-axis has a logarithmic scale, so you can see how much better `ExactFFT` performs!</figcaption>
</figure>

<p>Execution time is where my implementations truly shine. Indeed, you can see that for 256 bytes-long input array, <code>Fixed8FFT</code> only needs about 12 ms to compute the FFT, when it takes 52ms for <code>ApproxFFT</code> to do the same. And if you need the same level of precision as what <code>ApproxFFT</code> offers, you can use <code>Fixed16FFT</code>, which only needs about 30ms to perform the computation. It&#39;s worth noticing that <code>FloatFFT</code> is not far behind, with only 67ms needed to compute the 256 bytes FFT. Of course Exact FFT takes much longer.</p>

<figure>
 <img src="https://mytimeatrecurse.substack.com/assets/blog/articles/fft-arduino/execution_time_comparison.svg" alt="Execution time benchmark. `Fixed8FFT` is truly fast!"/> 
<figcaption> Execution time benchmark. `Fixed8FFT` is truly fast!</figcaption>
</figure>


<p>It has been a fun journey! I had a lot of fun and &#34;ha-ha!&#34; moments when debugging all these implementations. As I wrote before, there are ways to improve them, either by making <code>Fixed8FFT</code> able to handle longer input arrays, or writing a custom-made addition for floating-point number to speed-up <code>FloatFFT</code>. I don&#39;t know if I will do it in the near future, as this whole project was just intended to be a small side-project, which ended-up bigger than expected. </p>
<p>As always, feel free to contact me if you need any further detail on this. You can join me on <a href="https://mastodon.social/@klafyvel">mastodon</a>, or on <a href="https://github.com/Klafyvel">GitHub</a>, or even through the comment section below! In the meantime, have fun with your projects. :)</p>
</div>
  
      </div></div>
  </body>
</html>
