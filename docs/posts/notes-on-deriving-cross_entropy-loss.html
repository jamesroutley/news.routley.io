<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stpn.bearblog.dev/notes-on-deriving-cross-entropy-loss/">Original</a>
    <h1>notes on deriving `cross_entropy` loss</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-13T21:26Z">
                    13 Jun, 2025
                </time>
            </i>
        </p>
    

    <p>At <a href="https://www.recurse.com/scout/click?t=5523c0b9c525bf244fc04081e71a485a">recurse center</a>, I&#39;ve been implementing a language model from scratch by <a href="https://github.com/stephen/stanford-cs336/">following along</a> the <a href="https://cs336.stanford.edu">Stanford CS336 class</a> online.</p>
<p>Part of the first assignment implements cross-entropy loss from scratch. Even though I&#39;ve seen cross-entropy quite a few different times in other material, I keep finding myself confused about its relation to other concepts like KL divergence and softmax, &#34;log probs&#34; and &#34;nll loss&#34; and how it&#39;s actually computed numerically.</p>
<p>I thought it&#39;d be helpful for myself to write a bit about this to sharpen my intuition for the situation.</p>
<p>A basic understanding of loss functions, model training and softmax is probably prerequisite here, as I don&#39;t want to get too into the weeds coming from first principles. I found <a href="https://www.youtube.com/watch?v=KHVR587oW8I">this video</a> and <a href="https://www.youtube.com/watch?v=q0AkK8aYbLY">this video</a> do a good job of explaining how cross-entropy loss is defined.</p>
<p>Cross-entropy loss is most commonly used as the loss function for training models (<em>training loss</em>) where we want the model to learn a distribution. For instance, in classification tasks (&#34;what&#39;s in this image?&#34;), cross-entropy is used to train the predictions the model is making (&#34;35% likely this is a cat, 65% likely this is a bus&#34;) to more closely match what&#39;s actually in the image (&#34;100% bus&#34;).</p>
<p>The training output label is usually formed as a <a href="https://en.wikipedia.org/wiki/One-hot">one-hot vector</a>. For instance, the one-hot for our image classification task might have a probability for each output label we might classify the image. For <code>[cat_prob, bus_prob, house_prob, scarecrow, ...]</code> we might have:</p>
<ul>
<li>training data one-hot: <code>[0, 1, 0, ...]</code></li>
<li>predicted probabilities before training: <code>[.01, .04, .02, ...]</code></li>
<li>predicted probabilities after training: <code>[.01, .95, .001, ...]</code></li>
</ul>
<p>LLMs aren&#39;t classification tasks, but similarly use one-hots during training, e.g. if the token sequence is &#34;the quick brown fox&#34; the training data one-hot might represent the token for &#34;jumped&#34;.</p>
<p>Our goal here is to go from a somewhat impenetrable definition of cross-entropy and understand it enough to get to a working <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html">pytorch function</a> we could actually run.</p>
<h3 id="definitions">definitions</h3><p>To warm us up, we first need a few definitions.</p>
<p>First, we use <strong><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> as the true distribution</strong> and <strong><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> for the predicted distribution</strong>. For our ML purposes, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> corresponds to the training data and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> corresponds to the predictions from our model. The subscript <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> tells you we&#39;re looking at the probability <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> at index <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>.</p>
<p>We also have <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow></math> that is the <strong>entropy</strong> of the distribution. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></math> is the cross-entropy of those two distributions and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> is the entropy of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> itself. The technical definition for entropy is something about <a href="https://youtu.be/tXE23653JrU?t=117">&#34;how much information&#34; is in the distribution</a>, but I found that somewhat unhelpful to think about. We will soon realize that we don&#39;t need to worry about the definition for this one too much.</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></math> is the <a href="https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence"><strong>KL divergence</strong></a> between the two distributions. The KL divergence is a distance measure that tells us how different P and Q is. You could read this term as &#34;the KL divergence from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math>&#34; or better yet, &#34;how surprised (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mi>L</mi></mrow></math>) you are by the prediction (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math>), given the training data (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math>)&#34;</p>
<h3 id="cross-entropy">cross entropy</h3><p>With those definitions in mind, we can now reveal what we&#39;re working with. Cross-entropy is defined as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>cross entropy</mtext></mrow></munder><mo>=</mo><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>+</mo><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math><p>We&#39;re going to take this formula and break it down piece by piece, taking advantages of a few simplifications that are possible for our one-hot vector case. (Note that many of the assumptions we&#39;ll make <strong>do not apply</strong> if the target distribution is not one-hot!)</p>
<h2 id="measuring-cross-entropy-loss-as-a-proxy-for-kl-divergence">measuring cross-entropy loss as a proxy for KL divergence</h2><p>To start out, let&#39;s label the terms:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>cross entropy</mtext></mrow></munder><mo>=</mo><munder><munder><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder><mo>+</mo><munder><munder><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>inherent entropy</mtext></mrow></munder></mrow></math><p>Using the definitions we have above, the cross-entropy can be read as the &#34;amount of difference between the two distributions&#34; (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></math>) plus &#34;the inherent variation that is part of the training data&#34; (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math>).</p>
<h3 id="eliminating-the-math-xmlnshttpwwww3org1998mathmathml-displayinlinemrowmihmimo-stretchyfalsex00028momipmimo-stretchyfalsex00029momrowmath-entropy-term">eliminating the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> entropy term</h3><p>You might be wondering how to calculate this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> term, but luckily we don&#39;t have to for long.</p>
<p>For training loss, we can drop the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> term with a couple justifications:</p>
<ul>
<li><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> as a term never accounts for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> (literally, the parameters of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi></mrow></math> don&#39;t take <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math>), so varying <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> by training our model will never impact <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math>. The term is just inherent to the training data itself. You could think of this as a floor for the amount of loss we&#39;ll see for training.</li>
<li>For one-hot vectors specifically, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math> is going to always be <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0</mn></mrow></math> anyways, i.e. an image will always be labeled &#34;this picture is 100% a cat&#34;, not &#34;this picture is 90% a cat, 10% a bus&#34; (assuming that <a href="https://en.wikipedia.org/wiki/Catbus">catbus</a> is not one of our output classifications).</li>
</ul>
<p>Having convinced ourselves of dropping <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></math>, we now have:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>cross entropy</mtext></mrow></munder><mo>=</mo><munder><munder><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder></mrow></math><p>We can now see that when we use cross-entropy loss, we&#39;re <a href="https://bear-images.sfo2.cdn.digitaloceanspaces.com/stpn/51am.webp">really just using KL divergence</a>.</p>
<p>I like this understanding of cross-entropy loss much better. If we talk about KL divergence, a measure of <em>distance</em>, it makes some intuitive sense why you&#39;d want to minimize the distance between the model predictions and training data. Cross-entropy in my mind is a bit more hand-wavy with something about information theory and bits that makes the picture less fuzzy.</p>
<h2 id="calculating-kl-divergence">calculating kl divergence</h2><p>Now we know that cross-entropy loss simplifies down to KL divergence. Let&#39;s look at its definition:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>log</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math><p>In other words, we sum over the distribution, looking at the difference (the log ratio term) between the true and predicted probabilities for each, weighted by the true probability (the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> term).</p>
<p>For our one-hot case, we can simplify this equation by dropping the summation and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> terms. This is because the training data&#39;s <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi></mrow></math> is always going to be a one-hot vector, so <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> is going to be 0 everywhere except the one true label where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn></mrow></math>, e.g. for the label where the image is a bus:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><munder><munder><mrow><mn>0</mn><mo>*</mo><mi>log</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>⏟</mo></munder><mrow><mtext>cat prob</mtext></mrow></munder><mo>+</mo><munder><munder><mrow><mn>1</mn><mo>*</mo><mi>log</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>⏟</mo></munder><mrow><mtext>bus prob</mtext></mrow></munder><mo>+</mo><munder><munder><mrow><mn>0</mn><mo>*</mo><mi>log</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>⏟</mo></munder><mrow><mtext>scarecrow prob</mtext></mrow></munder><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo></mrow></math><p>We can remove all of the terms where P(i) is 0, simplifying down to:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder><mo>=</mo><munder><munder><mrow><mi>log</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>⏟</mo></munder><mrow><mtext>term for true label only</mtext></mrow></munder></mrow></math><h3 id="manipulating-logarithms">manipulating logarithms</h3><p>From here, if we remember our <a href="https://mathsathome.com/logarithm-laws/">quotient rule</a> (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mo stretchy="false">(</mo><mi>a</mi><mo>/</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></math>), we can continue to break our formula down:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder><mo>=</mo><mi>log</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math><p>Since <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> for a one-hot vector is going to be <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn></mrow></math>, we can replace the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math> term with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math>, i.e. just <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0</mn></mrow></math>:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><munder><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>KL divergence</mtext></mrow></munder><mo>=</mo><mtext>CE</mtext><mo>=</mo><mn>0</mn><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math><h2 id="negative-log-probability-nll">negative log probability (NLL)</h2><p>We can see that we&#39;ve now arrived at <em>negative log likelihood</em>! As in literally, the cross-entropy function boils down to take the negative of the log of the likelihood (probability).</p>
<p>Calculating cross-entropy is the same as calculating KL divergence is the same thing as calculating NLL:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>NLL</mtext><mo>=</mo><mtext>CE</mtext><mo>=</mo><mtext>KL Divergence</mtext><mo>=</mo><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mtext>true class</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math><p>Though we see that these are the equivalent, I&#39;ll continue to use the term cross-entropy to be consistent.</p>
<p>Finally, we can take a stab at a pytorch function. Since we know that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> is the predictions from our model, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi><mo stretchy="false">(</mo><mtext>true class</mtext><mo stretchy="false">)</mo></mrow></math> here can be read as &#34;the model output for the true class of training example&#34;.</p>
<div><pre><span></span><span>import</span><span> </span><span>torch</span><span> </span><span>as</span><span> </span><span>t</span>
<span>from</span><span> </span><span>jaxtyping</span><span> </span><span>import</span> <span>Float</span>

<span>def</span><span> </span><span>cross_entropy</span><span>(</span><span>q</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;d&#34;</span><span>],</span> <span>p_index</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>float</span><span>:</span>
    <span>return</span> <span>-</span><span>t</span><span>.</span><span>log</span><span>(</span><span>q</span><span>[</span><span>p_index</span><span>])</span><span>.</span><span>item</span><span>()</span>
</pre></div>
<p>where <code>q</code> corresponds to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math>, the output probabilities from our model.</p>
<h2 id="numerical-stability-and-floating-point-error">numerical stability and floating point error</h2><p>At this point, we could be pretty much done if it weren&#39;t for the reality of how floating point numbers are computed. If we were to open up the hood on our model, we&#39;d see that output probabilities come from a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow></math> operation. Softmax takes raw outputs from the model and shapes them into a probability distribution that adds up to 100%. It&#39;s defined as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><mo>∑</mo><msup><mi>e</mi><mrow><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math><p>If we wrap this up in our cross entropy function, we get</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>CE</mtext><mo>=</mo><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>log</mi><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><mo>∑</mo><msup><mi>e</mi><mrow><mi>z</mi></mrow></msup></mrow></mfrac><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math><p>which is a bit weird since we&#39;re taking the log of an exponent. Computers calculating <code>log(exp())</code> might end up with large or small numbers that are difficult to represent in <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic#Accuracy_problems">floating point</a>.</p>
<p>We&#39;ll do some &#34;numerical stability tricks&#34; to make it less likely we accidentally overflow or underflow the value this composed calculation. Note that this is somewhat similar to what we might do symbolically by hand, i.e. first cancel terms that can be simplified before computing the value.</p>
<p>Again using our quotient rule, we can first break this up:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>CE</mtext><mo>=</mo><mo>−</mo><mi>log</mi><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mi>log</mi><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mo>∑</mo><msup><mi>e</mi><mrow><mi>z</mi></mrow></msup><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math><p>then rearrange and simplify the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mo stretchy="false">(</mo><msup><mi>e</mi><mi>z</mi></msup><mo stretchy="false">)</mo></mrow></math> term:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>CE</mtext><mo>=</mo><mi>log</mi><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mo>∑</mo><msup><mi>e</mi><mrow><mi>z</mi></mrow></msup><mo stretchy="true" fence="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></math><p>or in pseudocode:</p>
<div><pre><span></span><span>log</span><span>(</span><span>sum</span><span>(</span><span>exp</span><span>(</span><span>z</span><span>))</span> <span>-</span> <span>z</span><span>[</span><span>true_class_index</span><span>]</span>
</pre></div>
<p>Also note that we still have a <code>log(sum(exp(z)))</code> which can&#39;t be reduced symbolically but goes in and out of the exponential range. Here, we can do a trick where we subtract the max from <code>z</code> first to keep all of the values stably around <code>0</code> in the domain.</p>
<p>Putting everything together now, we can write:</p>
<div><pre><span></span><span>def</span><span> </span><span>cross_entropy</span><span>(</span><span>logits</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;d&#34;</span><span>],</span> <span>p_index</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>float</span><span>:</span>
   <span>log_probs</span> <span>=</span> <span>log_softmax</span><span>(</span><span>logits</span><span>)</span>
   <span>return</span> <span>-</span><span>log_probs</span><span>[</span><span>p_index</span><span>]</span><span>.</span><span>item</span><span>()</span>
</pre></div>
<p>where <code>log_softmax</code> is:</p>
<div><pre><span></span><span>def</span><span> </span><span>log_softmax</span><span>(</span><span>logits</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;d&#34;</span><span>])</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span><span>:</span>
   <span>m</span> <span>=</span> <span>logits</span><span>.</span><span>max</span><span>()</span>
   <span>v</span> <span>=</span> <span>logits</span> <span>-</span> <span>m</span>
   <span>return</span> <span>v</span> <span>-</span> <span>t</span><span>.</span><span>log</span><span>(</span><span>v</span><span>.</span><span>exp</span><span>()</span><span>.</span><span>sum</span><span>())</span>
</pre></div>
<p>Note that we now have to take the pre-softmax&#39;d output, usually called a <em>logit</em>, to let us simplify the expression. This mismatch (i.e. why doesn&#39;t the <code>cross_entropy</code> function take the raw probabilities instead of the logits?) always felt a bit confusing to me. We can now see that it&#39;s necessary to allow for these stability tricks.</p>
<p>Finally, we still have <code>t.log(v.exp().sum())</code> which is still not ideal. We can do the <a href="https://en.wikipedia.org/wiki/LogSumExp">logsumexp</a> trick here which again involves subtracting and re-adding the max to keep the values in a good range:</p>
<div><pre><span></span><span>def</span><span> </span><span>log_softmax</span><span>(</span><span>logits</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;d&#34;</span><span>])</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span><span>:</span>
   <span>m</span> <span>=</span> <span>logits</span><span>.</span><span>max</span><span>()</span>
   <span>v</span> <span>=</span> <span>logits</span> <span>-</span> <span>m</span>
   <span>return</span> <span>v</span> <span>-</span> <span>logsumexp</span><span>(</span><span>v</span><span>)</span>

<span>def</span><span> </span><span>logsumexp</span><span>(</span><span>x</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;d&#34;</span><span>])</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span><span>:</span>
   <span>m</span> <span>=</span> <span>logits</span><span>.</span><span>max</span><span>()</span>
   <span>return</span> <span>m</span> <span>+</span> <span>t</span><span>.</span><span>log</span><span>((</span><span>x</span> <span>-</span> <span>m</span><span>)</span><span>.</span><span>exp</span><span>()</span><span>.</span><span>sum</span><span>())</span>
</pre></div>
<h2 id="bringing-in-the-batch-dimension">bringing in the batch dimension</h2><p>If we want to run this function across multiple batches at once, we lastly need to do some dimensional wrangling:</p>
<div><pre><span></span><span>def</span><span> </span><span>cross_entropy</span><span>(</span><span>logits</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;b d&#34;</span><span>],</span> <span>p_index</span><span>:</span> <span>Int</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;b&#34;</span><span>])</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span><span>:</span>
    <span>log_probs</span> <span>=</span> <span>log_softmax</span><span>(</span><span>logits</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
    <span>return</span> <span>-</span><span>log_probs</span><span>.</span><span>gather</span><span>(</span><span>dim</span><span>=-</span><span>1</span><span>,</span> <span>index</span><span>=</span> <span>p_index</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>))</span><span>.</span><span>squeeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span>.</span><span>mean</span><span>()</span>

<span>def</span><span> </span><span>log_softmax</span><span>(</span><span>logits</span><span>:</span> <span>Float</span><span>[</span><span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>&#34;b d&#34;</span><span>],</span> <span>dim</span><span>:</span> <span>int</span><span>):</span>
    <span>m</span> <span>=</span> <span>logits</span><span>.</span><span>max</span><span>(</span><span>dim</span><span>=</span><span>dim</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span><span>.</span><span>values</span>
    <span>v</span> <span>=</span> <span>logits</span> <span>-</span> <span>m</span>
    <span>return</span> <span>v</span> <span>-</span> <span>logsumexp</span><span>(</span><span>v</span><span>,</span> <span>dim</span><span>=</span><span>dim</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span>

<span>def</span><span> </span><span>logsumexp</span><span>(</span><span>x</span><span>:</span> <span>t</span><span>.</span><span>Tensor</span><span>,</span> <span>dim</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>t</span><span>.</span><span>Tensor</span><span>:</span>
    <span>m</span> <span>=</span> <span>x</span><span>.</span><span>max</span><span>(</span><span>dim</span><span>=</span><span>dim</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span><span>.</span><span>values</span>
    <span>return</span> <span>m</span> <span>+</span> <span>t</span><span>.</span><span>log</span><span>((</span><span>x</span> <span>-</span> <span>m</span><span>)</span><span>.</span><span>exp</span><span>()</span><span>.</span><span>sum</span><span>(</span><span>dim</span><span>=</span><span>dim</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>))</span>
</pre></div>
<p>Note that we take the mean loss over the batches, since we&#39;re trying to get a single loss number across all batches.</p>
<p>And here we are! As a quick summary, we&#39;ve shown:</p>
<ul>
<li>how cross-entropy loss and KL divergence are related</li>
<li>where &#34;negative log likelihood&#34; comes from</li>
<li>why the <code>cross_entropy</code> function takes the pre-softmax&#39;d logits instead of the output probabilities directly</li>
</ul>
<p>The true implementation for <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html"><code>cross_entropy</code></a> in pytorch is a bit more complicated because it also deals with the case where the training distribution isn&#39;t a one-hot vector.</p>



    

    
        
            <p>
                
                    <a href="https://stpn.bearblog.dev/blog/?q=worknotes">#worknotes</a>
                
            </p>
        

        
            


        
    


  </div></div>
  </body>
</html>
