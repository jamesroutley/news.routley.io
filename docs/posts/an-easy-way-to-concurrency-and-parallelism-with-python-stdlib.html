<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.bitecode.dev/p/the-easy-way-to-concurrency-and-parallelism">Original</a>
    <h1>An easy way to concurrency and parallelism with Python stdlib</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><em>Concurrency and parallelism are hard, plus Python could do better in that regard.</em></p><p><em>Yet there are quite a few problems that are not falling in that category: scraping the web, avoiding to block a UI, zipping many files…</em></p><p><em>For those, Python actually comes with pretty decent tools: the pool executors.</em></p><p><em>You can distribute work to a bunch of process workers or thread workers with a few lines of code:</em></p><pre><code><code>from concurrent.futures import ThreadPoolExecutor, as_completed

with ThreadPoolExecutor(max_workers=5) as executor:
    executor.submit(do_something_blockint)</code></code></pre><p><em>This simple approach only covers a limited use cases, but for those, it works surprisingly well.</em></p><p><em>What&#39;s more, those use cases are the ones you are most like to have to solve by yourselves, as the other ones often have infra or libs solutions already.</em></p><p>The material about concurrency and parallelism is generally written about two types of people: beginners that have no idea what they are talking about and hyper specialized people.</p><p>The second camp will tell you about the very hard problem they solved with it, like processing a 64 GB vector matrix with 21 workers in 2us, or creating a server that handles millions of connections so that people can share their cat picture 300ms faster.</p><p>The first one will just repeat what they read without thinking, usually applying what the second one said but bringing their conclusion to their level of problems.</p><p>This can give you the impression that concurrency and parallelism need to be complicated with statements like:</p><ul><li><p>sharing resources is hard;</p></li><li><p>Python is slow;</p></li><li><p>the GIL neuters threads;</p></li><li><p>process intercommunication is costly;</p></li><li><p>etc.</p></li></ul><p>While all this is true, it doesn&#39;t paint the whole picture:</p><ul><li><p>Python is really good at delegating tasks to the infrastructure like your DB, your cache store, your reverse proxy, etc. They will do the concurrency and paraelelism.</p></li><li><p>There are a lot of solved problems in many spaces, at least at the scale you are likely going to encounter. E.G: WSGI servers will pop several processes to deal with concurrent requests while long tasks will be sent to a queue. This works very decently for the vast majority of web projects.</p></li><li><p>While we can&#39;t solve P = NP in theory, IRL we are usually happy to just give a partial solution to it and call it a day. Or a product. It&#39;s the same for concurrency and parallelism: simplify the problem or the requirements, and you may very well make your client happy with a basic solution.</p></li></ul><p>But above all, many day-to-day problems are just... not that complicated.</p><p>And it&#39;s likely that you can solve them efficiently with what Python provides for threading or multiprocessing.</p><p>Of course, you can argue that dealing with threads and processes in itself is hard, and again, you would be right.</p><p>But the Python standard library comes with a beautiful abstraction for them I see too few people use: the pool executors.</p><p>So this article is about them, and how adding them to your toolbox will make a whole class of tasks solvable with little efforts.</p><p><span>If you read </span><a href="https://www.bitecode.dev/p/asyncio-twisted-tornado-gevent-walk" rel="">Asyncio, twisted, tornado, gevent walk into a bar</a><span>, you may remember this little script that makes a GET request to a bunch of URLs, loan the page and print the title:</span></p><pre><code><code>import re
import time
from urllib.request import Request, urlopen

URLs = [
    &#34;https://www.bitecode.dev/p/relieving-your-python-packaging-pain&#34;,
    &#34;https://www.bitecode.dev/p/hype-cycles&#34;,
    &#34;https://www.bitecode.dev/p/why-not-tell-people-to-simply-use&#34;,
    &#34;https://www.bitecode.dev/p/nobody-ever-paid-me-for-code&#34;,
    &#34;https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager&#34;,
    &#34;https://www.bitecode.dev/p/the-costly-mistake-so-many-makes&#34;,
    &#34;https://www.bitecode.dev/p/the-weirdest-python-keyword&#34;,
]

title_pattern = re.compile(r&#34;&lt;title[^&gt;]*&gt;(.*?)&lt;/title&gt;&#34;, re.IGNORECASE)

user_agent = (
    &#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/116.0&#34;
)


def fetch_url(url):
    start_time = time.time()

    headers = {&#34;User-Agent&#34;: user_agent}
    with urlopen(Request(url, headers=headers)) as response:
        html_content = response.read().decode(&#34;utf-8&#34;)
        match = title_pattern.search(html_content)
        title = match.group(1) if match else &#34;Unknown&#34;

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f&#34;Time taken ({url}): {elapsed_time:.4f} seconds\n&#34;, end=&#34;&#34;)
    return title


def main():
    for url in URLs:
        title = fetch_url(url)
        print(f&#34;URL: {url}\nTitle: {title}&#34;, flush=True)


if __name__ == &#34;__main__&#34;:
    # Let&#39;s time how long all this takes
    global_start_time = time.time()
    main()
    global_elapsed_time = time.time() - global_start_time
    print(f&#34;Total time taken for all URLs: {global_elapsed_time:.4f} seconds&#34;)</code></code></pre><p>Because it&#39;s blocking on every request, waiting for the network to answer before starting the next requests, the global execution time is the sum of the time each request takes:</p><pre><code><code>Time taken (https://www.bitecode.dev/p/relieving-your-python-packaging-pain): 0.1869 seconds
URL: https://www.bitecode.dev/p/relieving-your-python-packaging-pain
Title: Relieving your Python packaging pain - Bite code!
Time taken (https://www.bitecode.dev/p/hype-cycles): 0.1888 seconds
URL: https://www.bitecode.dev/p/hype-cycles
Title: XML is the future - Bite code!
Time taken (https://www.bitecode.dev/p/why-not-tell-people-to-simply-use): 0.1875 seconds
URL: https://www.bitecode.dev/p/why-not-tell-people-to-simply-use
Title: Why not tell people to &amp;quot;simply&amp;quot; use pyenv, poetry or anaconda
Time taken (https://www.bitecode.dev/p/nobody-ever-paid-me-for-code): 0.2070 seconds
URL: https://www.bitecode.dev/p/nobody-ever-paid-me-for-code
Title: Nobody ever paid me for code - Bite code!
Time taken (https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager): 0.6739 seconds
URL: https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager
Title: Python cocktail: mix a context manager and an iterator in equal parts
Time taken (https://www.bitecode.dev/p/the-costly-mistake-so-many-makes): 0.3111 seconds
URL: https://www.bitecode.dev/p/the-costly-mistake-so-many-makes
Title: The costly mistake so many make with numpy and pandas
Time taken (https://www.bitecode.dev/p/the-weirdest-python-keyword): 0.3057 seconds
URL: https://www.bitecode.dev/p/the-weirdest-python-keyword
Title: The weirdest Python keyword - Bite code!
Total time taken for all URLs: 2.0623 seconds
</code></code></pre><p><span>While we have shown you can get good perfs on this problem using asyncio or gevent, the simplest tool for the job with decent performances would be a </span><a href="https://docs.python.org/3/library/concurrent.futures.html" rel="">ThreadPoolExecutor</a><span>.</span></p><p>The code is mostly the same. One import is added at the top:</p><pre><code><code>from concurrent.futures import ThreadPoolExecutor, as_completed</code></code></pre><p><span>Then only the </span><code>main()</code><span> function changes:</span></p><pre><code><code>def main():
    with ThreadPoolExecutor(max_workers=len(URLs)) as executor:
        tasks = {}
        for url in URLs:
            future = executor.submit(fetch_url, url)
            tasks[future] = url

        for future in as_completed(tasks):
            title = future.result()
            url = tasks[future]
            print(f&#34;URL: {url}\nTitle: {title}&#34;)</code></code></pre><p>Let&#39;s get into the details on how it works:</p><pre><code><code>with ThreadPoolExecutor(max_workers=len(URLs)) as executor:</code></code></pre><p>This creates the pool executors with queues workers:</p><ul><li><p>The pool is an object that will start, hold and manage x threads transparently for you. Here, the number of threads is equal to the number of URLs because that&#39;s how many requests we want to be in parallel.</p></li><li><p>It will create, manage and hold queues for each thread to send work to them, and get results from them.</p></li><li><p>When the context manager exits, it waits for the threads to finish all tasks, and close them.</p></li></ul><p>Then:</p><pre><code><code>tasks = {}
for url in URLs:
    future = executor.submit(fetch_url, url)
    tasks[future] = url</code></code></pre><p><span>This submits the function </span><code>fetch_url</code><span> to the executor so that it will put it in a worker queue of tasks to executes. Note we don&#39;t do </span><code>fetch_url(url)</code><span>, as we don&#39;t want to call the function ourselves. We pass independently the function and the argument.</span></p><p><span>This submission returns a </span><code>Future</code><span> object to us. Futures are holding a reference to the task you just submitted, and will let you get the result later on when it&#39;s ready.</span></p><p>Because the task as asynchronous, we don&#39;t know in which order they will finish, so we make a dict of the futures and the URL they are working on for printing the result later on.</p><p>This loops blocks very little.</p><p>Finally:</p><pre><code><code>for future in as_completed(tasks):
    title = future.result()
    url = tasks[future]
    print(f&#34;URL: {url}\nTitle: {title}&#34;)</code></code></pre><p><span>We use </span><code>as_complete()</code><span> to loop on the futures (remember looping on a dict gives you the keys) as the task they refer to is finished. This loop is blocking until all tasks are completed.</span></p><p><code>future.result()</code><span> will get us the result that </span><code>fetch_url</code><span> returned or raise the exception that </span><code>fetch_url</code><span> encountered during execution.</span></p><p>We use our dictionary to find out which future maps to which URL, since they arrive in the order they finish, not the order we put them in.</p><p>The result is much faster:</p><pre><code><code>Time taken (https://www.bitecode.dev/p/nobody-ever-paid-me-for-code): 0.2688 seconds
URL: https://www.bitecode.dev/p/nobody-ever-paid-me-for-code
Title: Nobody ever paid me for code - Bite code!
Time taken (https://www.bitecode.dev/p/relieving-your-python-packaging-pain): 0.2901 seconds
Time taken (https://www.bitecode.dev/p/the-weirdest-python-keyword): 0.2881 seconds
URL: https://www.bitecode.dev/p/relieving-your-python-packaging-pain
Title: Relieving your Python packaging pain - Bite code!
URL: https://www.bitecode.dev/p/the-weirdest-python-keyword
Title: The weirdest Python keyword - Bite code!
Time taken (https://www.bitecode.dev/p/the-costly-mistake-so-many-makes): 0.3892 seconds
URL: https://www.bitecode.dev/p/the-costly-mistake-so-many-makes
Title: The costly mistake so many make with numpy and pandas
Time taken (https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager): 0.4014 seconds
URL: https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager
Title: Python cocktail: mix a context manager and an iterator in equal parts
Time taken (https://www.bitecode.dev/p/why-not-tell-people-to-simply-use): 0.4482 seconds
URL: https://www.bitecode.dev/p/why-not-tell-people-to-simply-use
Title: Why not tell people to &amp;quot;simply&amp;quot; use pyenv, poetry or anaconda
Time taken (https://www.bitecode.dev/p/hype-cycles): 0.4617 seconds
URL: https://www.bitecode.dev/p/hype-cycles
Title: XML is the future - Bite code!
Total time taken for all URLs: 0.4644 seconds</code></code></pre><p>When a request is waiting on the network, another thread is executing.</p><p>The whole script:</p><pre><code><code>import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.request import Request, urlopen

URLs = [
    &#34;https://www.bitecode.dev/p/relieving-your-python-packaging-pain&#34;,
    &#34;https://www.bitecode.dev/p/hype-cycles&#34;,
    &#34;https://www.bitecode.dev/p/why-not-tell-people-to-simply-use&#34;,
    &#34;https://www.bitecode.dev/p/nobody-ever-paid-me-for-code&#34;,
    &#34;https://www.bitecode.dev/p/python-cocktail-mix-a-context-manager&#34;,
    &#34;https://www.bitecode.dev/p/the-costly-mistake-so-many-makes&#34;,
    &#34;https://www.bitecode.dev/p/the-weirdest-python-keyword&#34;,
]

title_pattern = re.compile(r&#34;&lt;title[^&gt;]*&gt;(.*?)&lt;/title&gt;&#34;, re.IGNORECASE)

# We&#39;ll pretend to be Firefox or substack is going to kick us
user_agent = (
    &#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/116.0&#34;
)


def fetch_url(url):
    start_time = time.time()

    headers = {&#34;User-Agent&#34;: user_agent}
    with urlopen(Request(url, headers=headers)) as response:
        html_content = response.read().decode(&#34;utf-8&#34;)
        match = title_pattern.search(html_content)
        title = match.group(1) if match else &#34;Unknown&#34;

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f&#34;Time taken ({url}): {elapsed_time:.4f} seconds\n&#34;, end=&#34;&#34;)
    return title


def main():
    with ThreadPoolExecutor(max_workers=len(URLs)) as executor:
        tasks = {}
        for url in URLs:
            future = executor.submit(fetch_url, url)
            tasks[future] = url

        for future in as_completed(tasks):
            title = future.result()
            url = tasks[future]
            print(f&#34;URL: {url}\nTitle: {title}&#34;)


if __name__ == &#34;__main__&#34;:
    global_start_time = time.time()
    main()
    global_elapsed_time = time.time() - global_start_time
    print(f&#34;Total time taken for all URLs: {global_elapsed_time:.4f} seconds&#34;)
</code></code></pre><p><span>What would a version with multiprocessing look like? Pretty much the same, but, we use </span><code>ProcessPoolExecutor</code><span> instead.</span></p><p>So we change the import:</p><pre><code><code>from concurrent.futures import ProcessPoolExecutor, as_completed</code></code></pre><p>And the call:</p><pre><code><code>with ProcessPoolExecutor(max_workers=5) as executor:</code></code></pre><p>Note that here the number of workers maps to the number of CPU cores I want to dedicate to the program. Processes are way more expensive than threads, as each starts a new Python instance.</p><p>Using separate processes or GIL locked threads have a limited set of good use cases, and using a pool executor on top of that shrinks this group to an even smaller niche.</p><p>Thread pools are good for:</p><ul><li><p>Tasks (network, file, etc.) that needs less than 10_000 I/O interactions per second. The number is higher than you would expect, because threads are surprisingly cheap nowadays, and you can spawn a lot of them without bloating memory too much. The limit is more the price of context switching. This is not a scientific number, it&#39;s a general direction that you should challenge by measuring your own particular case.</p></li><li><p>When you need to share data between the tasks.</p></li><li><p>When you are not CPU bound.</p></li><li><p>When you are OK to execute tasks a bit slower to you ensure you are not blocking any of them (E.G: user UI and a long calculation).</p></li><li><p>When you are CPU bound, but the CPU calculations are delegating to a C extension that releases the GIL, such as numpy. Free parallelism on the cheap, yeah!</p></li></ul><p>E.G: a web scraper, a GUI to zip files, a development server, sending emails without blocking web page rendering, etc.</p><p>Process pools are good for:</p><ul><li><p>When you don&#39;t need to share data between tasks.</p></li><li><p>When you are CPU bound.</p></li><li><p>When you don&#39;t have too many tasks to run at the same time.</p></li><li><p>When you need true parallelism and want to exercise your juicy cores.</p></li></ul><p>Both are bad if you need to cancel tasks, collaborate a lot between tasks, deal precisely with the task lifecycle, needs a huge number of workers or want to milk out every single bit of perfs. You won’t get nowhere near Rust level of speed.</p><p>It’s still very useful, though.</p><ul><li><p><code>if __name__ == &#34;__main__&#34;</code><span> is important for multiprocessing because it will spawn a new Python, that will import the module. You don&#39;t want this module to spawn a new Python that imports the module that will spawn a new Python...</span></p></li><li><p><span>If the function to submit to the executor has complicated arguments to be passed to it, use a </span><code>lambda</code><span> or </span><code>functools.partial</code><span>.</span></p></li><li><p><code>max_worker = 1</code><span> is a very nice way to get a poor man’s task queue.</span></p></li></ul></div></div></div></article></div></div></div>
  </body>
</html>
