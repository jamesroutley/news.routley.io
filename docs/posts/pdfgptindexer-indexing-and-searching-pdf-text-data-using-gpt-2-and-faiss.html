<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/raghavan/PdfGptIndexer">Original</a>
    <h1>PdfGptIndexer: Indexing and searching PDF text data using GPT-2 and FAISS</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" dir="auto"><a id="user-content-description" aria-hidden="true" href="#description"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Description</h2>
<p dir="auto">PdfGptIndexer is an efficient tool for indexing and searching PDF text data using OpenAI&#39;s GPT-2 model and FAISS (Facebook AI Similarity Search). This software is designed for rapid information retrieval and superior search accuracy.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-libraries-used" aria-hidden="true" href="#libraries-used"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Libraries Used</h2>
<ol dir="auto">
<li><a href="https://github.com/deanmalmgren/textract">Textract</a> - A Python library for extracting text from any document.</li>
<li><a href="https://github.com/huggingface/transformers">Transformers</a> - A library by Hugging Face providing state-of-the-art general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG).</li>
<li><a href="https://python.langchain.com/" rel="nofollow">Langchain</a> - A text processing and embeddings library.</li>
<li><a href="https://github.com/facebookresearch/faiss">FAISS (Facebook AI Similarity Search)</a> - A library for efficient similarity search and clustering of dense vectors.</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-installing-dependencies" aria-hidden="true" href="#installing-dependencies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installing Dependencies</h2>
<p dir="auto">You can install all dependencies by running the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install langchain openai textract transformers langchain faiss-cpu"><pre>pip install langchain openai textract transformers langchain faiss-cpu</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How It Works</h2>
<p dir="auto">The PdfGptIndexer operates in several stages:</p>
<ol dir="auto">
<li>It first processes a specified folder of PDF documents, extracting the text and splitting it into manageable chunks using a GPT-2 tokenizer from the Transformers library.</li>
<li>Each text chunk is then embedded using the OpenAI GPT-2 model through the LangChain library.</li>
<li>These embeddings are stored in a FAISS index, providing a compact and efficient storage method.</li>
<li>Finally, a query interface allows you to retrieve relevant information from the indexed data by asking questions. The application fetches and displays the most relevant text chunk.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png"><img src="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png" alt="Untitled-2023-06-16-1537"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-advantages-of-storing-embeddings-locally" aria-hidden="true" href="#advantages-of-storing-embeddings-locally"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Advantages of Storing Embeddings Locally</h2>
<p dir="auto">Storing embeddings locally provides several advantages:</p>
<ol dir="auto">
<li>Speed: Once the embeddings are stored, retrieval of data is significantly faster as there&#39;s no need to compute embeddings in real-time.</li>
<li>Offline access: After the initial embedding creation, the data can be accessed offline.</li>
<li>Compute Savings: You only need to compute the embeddings once and reuse them, saving computational resources.</li>
<li>Scalability: This makes it feasible to work with large datasets that would be otherwise difficult to process in real-time.</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-running-the-program" aria-hidden="true" href="#running-the-program"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Running the Program</h2>
<p dir="auto">To run the program, you should:</p>
<ol dir="auto">
<li>Make sure you have installed all dependencies.</li>
<li>Clone the repository to your local machine.</li>
<li>Navigate to the directory containing the Python script.</li>
<li>Replace &#34;&lt;OPENAI_API_KEY&gt;&#34; with your actual OpenAI API key in the script.</li>
<li>Finally, run the script with Python.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python3 pdf_gpt_indexer.py"><pre><span>python3</span> <span>pdf_gpt_indexer</span>.<span>py</span></pre></div>
<p dir="auto">Please ensure that the folders specified in the script for PDF documents and the output text files exist and are accessible. The query interface will start after the embeddings are computed and stored. You can exit the query interface by typing &#39;exit&#39;.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-exploring-custom-data-with-chatgpt" aria-hidden="true" href="#exploring-custom-data-with-chatgpt"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Exploring Custom Data with ChatGPT</h2>
<p dir="auto">Check out the post <a href="https://devden.raghavan.studio/p/chatgpt-using-your-own-data" rel="nofollow">here</a> for a comprehensive guide on how to utilize ChatGPT with your own custom data.</p>
</article>
          </div></div>
  </body>
</html>
