<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://carper.ai/instruct-gpt-announcement/">Original</a>
    <h1>CarperAI announces plans for the first open-source “instruction-tuned” LM</h1>
    
    <div id="readability-page-1" class="page"><div>
				
				
				
				
				
				
				<div>
				<div>
				
				
				
				
				<div>
				
				
				
				
				<div>
<p><em>If you are interested in joining, check out our <strong><a href="https://discord.gg/canadagoose" data-type="URL" data-id="https://discord.gg/canadagoose">Discord</a></strong> or </em><strong><a href="https://twitter.com/carper.ai" data-type="URL" data-id="https://twitter.com/carper.ai"><em>Twitter</em></a></strong><em>.</em></p>

<p>CarperAI is the newest lab within the EleutherAI research collective and focuses on improving the performance and safety of large language models (LLMs) with reinforcement learning.</p>

<ul>
<ul>
<li>CarperAI will release a chinchilla-optimal large language model explicitly trained to follow human instructions, partnering with Scale AI, Multi, Humanloop, and Hugging Face.</li>
</ul>
</ul>
<ul>
<ul>
<li>The open-source LLM was trained using Reinforcement Learning from Human Feedback, a technique to improve LLMs&#39; safety and ease of use. The open-source release is crucial for enabling academics, independent researchers, and startups to conduct science and build upon state-of-the-art models.</li>
</ul>
</ul>

<p>CarperAI, a new research lab within the EleutherAI research collective, aims to democratize the &#34;LLMs&#34; &#34;instruction-tuning&#34; of large language models, the same way Stable Diffusion democratized image generation. Industry leader OpenAI pioneered the technique of teaching LLMs to follow instructions with their InstructGPT-3 model last year. Still, such models are either locked behind APIs or not released, limiting their value to most academics, hobbyists, and smaller companies. Last week, CarperAI released trlX, the first public implementation of the technique that can be used to train models with billions of parameters, to widespread acclaim.</p>

<p><strong>Today, they&#39;re</strong> <strong>going a step further and announcing a broad coalition aimed at training and publicly releasing instruction-tuned models</strong> with EleutherAI and Multi, experts in training large language models, and Scale, Humanloop, and HuggingFace, experts in labelling and human annotation.</p>


<h3><strong>Large language models have demonstrated extraordinary capabilities and pushed the frontier of AI.</strong></h3>


<p>They enable better search, writing assistants, code generation and even generalist assistants that automate tasks. Notably, compared to traditional supervised machine learning, they do not need large labelled datasets to be adapted for new tasks. Instead, most large language models are trained on the simple task of next-word prediction on massive unlabelled datasets. </p>


<h3><strong>Unfortunately, LLMs trained by next-word prediction are difficult to use, often produce factually inaccurate or offensive output, and can be used in harmful applications.</strong></h3>


<p>A partial solution is to take a language model trained in the usual way and adjust it afterwards to produce more socially acceptable and honest content by repeatedly prompting a language model with instruction, gathering feedback from humans on its outputs and adjusting the models&#39; parameters in the direction of better predicted human feedback. For example, OpenAI and DeepMind have used Reinforcement Learning from Human Feedback (RHLF) <strong>OpenAI</strong>, <strong>DeepMind</strong>, and <strong>Anthropic</strong> to produce LLMs that can follow instructions and are considerably more truthful and easier to use. In <strong>prior work</strong>, OpenAI found that the outputs from models trained with RLHF were preferred to those from <em>100x larger</em> models trained without human feedback.</p>


<h3><strong>Few organizations have the resources and technical expertise to build a Large Language Model of this scale and complexity.</strong></h3>


<p>Instruction-tuning requires expertise in training large language models, which few outside major tech companies possess. CarperAI&#39;s models will be trained by <strong>EleutherAI</strong>, their parent org and a pioneer in training open-source LLMs, and Multi, a new AI startup working on applying bleeding-edge LLM technology for enterprise automation. In addition, CarperAI is partnering with Scale, Humanloop, and Hugging Face to fine-tune the model. Scale accelerates the development of AI by providing AI data &amp; model infrastructure and full-service operational AI solutions, and Humanloop specializes in adapting LLMs from human feedback. Together they will be helping collect the human feedback data that will be used to improve the underlying language model. Hugging Face will provide the hosting mechanisms to share and load the models in an accessible way. In addition, they will also collaborate on developing demos of its spaces and evaluation tools.</p>


<h3><strong>There have been open-source releases of large language models before</strong>, <strong>but this is the first attempt to create an open model trained with RLHF.</strong></h3>


<p>We view RLHF training as an essential step in making LLMs useful and safe to be deployed in a public setting. The risks of LLMs have been well documented and range from spreading misinformation to reinforcing social biases. Compared to standard language models, training with RLHF dramatically reduces these risks and, at the same time, <em>increases</em> the model&#39;s usefulness.</p>


<p>It is expected that the release of this model will spur both research and innovation. In addition, it will enable many new applications and companies and allow us to deepen our understanding of state-of-the-art AI systems.</p>


<p><a href="https://twitter.com/carperai">CarperAI</a> | <a href="https://twitter.com/scale_ai">Scale</a> | <a href="https://twitter.com/humanloop">Humanloop</a> | <a href="https://twitter.com/huggingface">Hugging Face</a> | <a href="https://twitter.com/multi_agi">Multi</a> | <a href="https://twitter.com/AiEleuther">EleutherAI</a> | <a href="https://twitter.com/StabilityAI">StabilityAI</a></p>


<figure><img data-attachment-id="1497" data-permalink="https://carper.ai/instruct-gpt-announcement/option-5_highres/" data-orig-file="https://i0.wp.com/carper.ai/wp-content/uploads/2022/10/option-5_highres.png?fit=3600%2C1476&amp;ssl=1" data-orig-size="3600,1476" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="option 5_highres" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/carper.ai/wp-content/uploads/2022/10/option-5_highres.png?fit=300%2C123&amp;ssl=1" data-large-file="https://i0.wp.com/carper.ai/wp-content/uploads/2022/10/option-5_highres.png?fit=1024%2C420&amp;ssl=1" width="1024" height="420" src="https://i0.wp.com/carper.ai/wp-content/uploads/2022/10/option-5_highres.png?resize=1024%2C420&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-srcset="https://carper.ai/wp-content/uploads/2022/10/option-5_highres-980x402.png 980w, https://carper.ai/wp-content/uploads/2022/10/option-5_highres-480x197.png 480w" data-lazy-sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw" data-lazy-src="https://i0.wp.com/carper.ai/wp-content/uploads/2022/10/option-5_highres.png?resize=1024%2C420&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></figure>
</div>
			</div>
			</div>
				
				
				
				
			</div>
				
				
			</div><p>CarperAI is the newest lab within the EleutherAI research collective and focuses on improving the performance and safety of large language models (LLMs) with reinforcement learning.<!-- wp:list --></p><p>CarperAI, a new research lab within the EleutherAI research collective, aims to democratize the &#34;LLMs&#34; &#34;instruction-tuning&#34; of large language models, the same way Stable Diffusion democratized image generation. Industry leader OpenAI pioneered the technique of teaching LLMs to follow instructions with their InstructGPT-3 model last year. Still, such models are either locked behind APIs or not released, limiting their value to most academics, hobbyists, and smaller companies. Last week, CarperAI released trlX, the first public implementation of the technique that can be used to train models with billions of parameters, to widespread acclaim.<!-- /wp:paragraph --></p><p><strong>Today, they&#39;re</strong> <strong>going a step further and announcing a broad coalition aimed at training and publicly releasing instruction-tuned models</strong> with EleutherAI and Multi, experts in training large language models, and Scale, Humanloop, and HuggingFace, experts in labelling and human annotation.</p><p>They enable better search, writing assistants, code generation and even generalist assistants that automate tasks. Notably, compared to traditional supervised machine learning, they do not need large labelled datasets to be adapted for new tasks. Instead, most large language models are trained on the simple task of next-word prediction on massive unlabelled datasets. </p><p>A partial solution is to take a language model trained in the usual way and adjust it afterwards to produce more socially acceptable and honest content by repeatedly prompting a language model with instruction, gathering feedback from humans on its outputs and adjusting the models&#39; parameters in the direction of better predicted human feedback. For example, OpenAI and DeepMind have used Reinforcement Learning from Human Feedback (RHLF) <strong>OpenAI</strong>, <strong>DeepMind</strong>, and <strong>Anthropic</strong> to produce LLMs that can follow instructions and are considerably more truthful and easier to use. In <strong>prior work</strong>, OpenAI found that the outputs from models trained with RLHF were preferred to those from <em>100x larger</em> models trained without human feedback.</p><p>Instruction-tuning requires expertise in training large language models, which few outside major tech companies possess. CarperAI&#39;s models will be trained by <strong>EleutherAI</strong>, their parent org and a pioneer in training open-source LLMs, and Multi, a new AI startup working on applying bleeding-edge LLM technology for enterprise automation. In addition, CarperAI is partnering with Scale, Humanloop, and Hugging Face to fine-tune the model. Scale accelerates the development of AI by providing AI data &amp; model infrastructure and full-service operational AI solutions, and Humanloop specializes in adapting LLMs from human feedback. Together they will be helping collect the human feedback data that will be used to improve the underlying language model. Hugging Face will provide the hosting mechanisms to share and load the models in an accessible way. In addition, they will also collaborate on developing demos of its spaces and evaluation tools.</p><p>We view RLHF training as an essential step in making LLMs useful and safe to be deployed in a public setting. The risks of LLMs have been well documented and range from spreading misinformation to reinforcing social biases. Compared to standard language models, training with RLHF dramatically reduces these risks and, at the same time, <em>increases</em> the model&#39;s usefulness.</p><p>It is expected that the release of this model will spur both research and innovation. In addition, it will enable many new applications and companies and allow us to deepen our understanding of state-of-the-art AI systems.</p></div>
  </body>
</html>
