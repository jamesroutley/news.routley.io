<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Original</a>
    <h1>Neural networks in the 1990s</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-120891" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670558589746905090" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="79" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:26:19.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:26:19.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="mainEntityOfPage"/><article aria-labelledby="id__qgb9je6kcrn id__9iqw8oaga8r id__xh4ytdjrka id__derxu3x3azi id__sn9ai3mmo0n id__ewltvzl6698 id__r19gqhq8na id__p3g1kp8zce id__tkg9tj23dul id__nxofenc6xrg id__10kkh0h6t0p id__t7wwr0sbzpq id__ak2gvtq8aqn id__4yt68z9rgxo id__leebs21l03p id__dpi6yzifdn id__oojz4wd44i id__wk0h8k5da4 id__am6pm23yae" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>It is interesting how many old papers used neural networks with only a dozen or so units. Computers weren’t THAT slow in the 90s — BLAS (basic linear algebra subprograms) was already a thing that vendors hyper-optimized for. Not much overlap between HPC and NN people? </span><a dir="ltr" href="https://t.co/gpUNHLFkP3" rel="noopener noreferrer nofollow" target="_blank" role="link"><span aria-hidden="true">https://</span>cs.utoronto.ca/~hinton/absps/<span aria-hidden="true">sunspots.pdf</span><span aria-hidden="true">…</span></a></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670640732493824000" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="8" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T03:52:44.000Z" itemprop="dateCreated"/><meta content="2023-06-19T03:52:44.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670640732493824000" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__xhmu86qy5pl id__7y6b8a72t9 id__znqqqk2ic7o id__i19r9210bxl id__0ngvcpojquge id__z6i680vuue id__hsu4rdsamna id__9nb3tdz8dx id__4cehuhn1xmh id__ji1agh3zol8 id__htheh6zimgq id__xwvvqb3amye id__zkzq4hospfm id__165fcu7qs4z id__oms73v2rzd id__a3rtrdin8mk id__9b45wmc5oo id__zms5y6y5oo id__y2uiu4hc08e" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Even with sigmoids and a single layer, using 4096 neurons instead of 12 makes a big difference, and is no harder to train. A BLAS based 4k network would probably train faster on hardware of the day than a matlab implementation for 12 neurons, but you have to consider performance.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670562414830534658" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="5" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:41:31.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:41:31.000Z" itemprop="datePublished"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__dfc3i4yajom id__bdppva5ngxp id__mi0q6o6nfzc id__60dl2ex66d4 id__bs52rbbkb0l id__mcyu79dvi5j id__10f5pikq917j id__nchb5qjdvw id__xmsses6vaw8 id__ws3s68h7i id__jav67nctc0g id__x7b44kulxna id__60ajh5fhmtl id__t43p5r4gdf id__6muf6czu3av id__p5vcqf7c0i id__yxlmqr9jc1 id__b0kzfkvb1mp id__8fhx9zf3n0i" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-TimSweeneyEpic"><div><div><div><div><a href="https://twitter.com/TimSweeneyEpic" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/795819168629198849/SBY3ARvZ_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Also perhaps there wasn’t an obviously fruitful middle ground in between the small NN models studied by the earlier papers and today’s large image and language models. Paraphrasing Jensen, games were fortunate to have a continuum of ever-increasing computing utility.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670563624019009536" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:46:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:46:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670563624019009536" itemprop="url"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="isPartOf"/><article aria-labelledby="id__u9y73ebv29 id__pltrts1r69 id__3xxrxl197qv id__06vf6i3q435i id__0ei4mybctcxa id__oa4rpbwz01l id__kqiuq6f3e6 id__8f470iit8yo id__mwcds28l8me id__qjhs8uy0r7l id__j9ttb3n3b4 id__pcpuvwsikta id__2iw6hepdnj id__16llgde4cv3 id__24v1rubovo2 id__qqbbvfsuzi id__z25rvlfj7xe id__14y8vdmwgnu id__vjfy6ygc9na" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Most of the tasks that they measure performance on would steadily improve with model size, even at those tiny starting points, although the power law scaling might have looked very unappealing relative to hoped-for algorithmic breakthroughs.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560161591709696" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:32:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:32:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/Sayter/status/1670560161591709696" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__5vlanbhljr5 id__pnj9iz3444 id__0uatytrxaii id__0nxin0t4vg9 id__rc2yqayipt id__h5zukl79wvq id__nzyskgnu7kq id__j34agrc6e3a id__xvqvgnaj3ya id__2z2umn1oxlj id__woklyz9ms id__9xjjx5o6ub9 id__crubcxlz66a id__1fo0gj55sok id__ka4aczkkzx id__6zwuhdhfkt2 id__qhsdbkytffl id__nbh74za3uoi id__fvhrb6cmd8f" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-Sayter"><div><div><div><div><a href="https://twitter.com/Sayter" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1565566877501427718/16F_ybGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>neural networks go back to like 1795 depending on your lvl of technicality</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560391296843776" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:33:29.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:33:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/invisioner/status/1670560391296843776" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__pw9enz7f6cs id__zl92a4l4qt id__xmhid6vo7v id__kcoc926xlgb id__qy7cm2fb6 id__5sjjtoz4ia3 id__hhvijvz9yu id__p8w3px2hal id__pim0qxv8ab id__0sgwv5w38d id__ypzvo9ej0t id__gqme2rnspkw id__xxbamx0z5ld id__gg0vhoen3jk id__girjdd350s4 id__y8d2z09zqcs id__2u5o3fwsubs id__1kp1a540x7mj id__w9b8kx8giak" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-invisioner"><div><div><div><div><a href="https://twitter.com/invisioner" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664156292556939264/wtRdeMj9_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670564129923375105" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:48:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:48:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__aml5er0ddvn id__0bvrpmhox49g id__3ijklmzf0fd id__av9tsktgut id__x6w1ipzide id__qff43kv3dj id__90fwns263dm id__2jwjgho1g2z id__fdnc87kybv id__3iqhbdnpzjb id__rwk7b89p8v id__zv1j8zxjeis id__4gvl5uvcran id__ck6jx73dvb id__wxfdb0uo41a id__8xok297oz37 id__j23wq27xec id__fhbjf9uaxxq id__59eoxpt5pws" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-klausraizer"><div><div><div><div><a href="https://twitter.com/klausraizer" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/511875923138723841/1mITJOnu_400x400.jpeg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>If I recall correctly, what we had back then was still not mature enough. Vanishing gradients when adding more layers were still a serious issue and we didn&#39;t know how to deal with it properly. And adding more neurons per layer was mostly another parameter you tried to optimize.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670568986336174080" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:07:38.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:07:38.000Z" itemprop="datePublished"/><meta content="https://twitter.com/vikhyatk/status/1670568986336174080" itemprop="url"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="isPartOf"/><article aria-labelledby="id__ku1rmsvexx id__1e4o6riaian id__1c5izbo2o67 id__z40iic7d94 id__r2uyde44bjs id__bt98vgi0k6s id__jn60jl3kgin id__dlv7a9d2au7 id__hz09hew9q0b id__2bod0ttmawk id__p8hok1dyznb id__2nidf8sb5tc id__pyf571p4oc id__8uiri87hpxc id__u9uf8dims3 id__qryuzhvckp id__nags7ywjcb id__u0e23234l1 id__zvr21yv2kbf" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-vikhyatk"><div><div><div><div><a href="https://twitter.com/vikhyatk" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Agree - scaling depth was really hard before ResNet-style architectures.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670576267618316288" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:36:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:36:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/masegraye/status/1670576267618316288" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__dfdufvl2kj id__v54bp85acl id__pva8sap78ep id__l82xbct92ia id__4foqh6s0815 id__0lrur1vbhvwg id__59c5su8iai6 id__rqax9onuxwf id__dnlbcye18l6 id__cqillghnibm id__ack5fg21cs id__4oxw58lw48w id__vt69m2hgnv id__adoqdzwt1fe id__xrz91iumyrc id__ftcizxgj46k id__73nuf5rde9 id__6qb5wpzty1d id__q2da1ufwgfi" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-masegraye"><div><div><div><div><a href="https://twitter.com/masegraye" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1639806071244988418/Co0AoGGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>In the academic world, you can go “theoretical” or “applied”, but few go “production”.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670587368745648129" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:20:41.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:20:41.000Z" itemprop="datePublished"/><meta content="https://twitter.com/voxelbased/status/1670587368745648129" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__uk68zrzc34a id__qxvg8vgx48l id__t33j3e2bvco id__7ozd3t82c5q id__8gr7rifz4jo id__y832f7lu7ha id__f4ncdd8kram id__roy8hxd2s8 id__mrizvaw5apq id__2q3sp1bvl0j id__tr73kzchkli id__my55vvlavcl id__28zybpu175u id__pxgnmqeby4 id__le6qsh97woc id__4jrzkpc0aid id__nfum5phu4c id__ans6o4vinqt id__6yyicz2kvrj" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-voxelbased"><div><div><div><div><a href="https://twitter.com/voxelbased" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1561708024632328193/kanMG_si_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Large neural networks weren&#39;t tried due to performance, and performance wasn&#39;t optimized because large networks weren&#39;t used

Until it was so easy someone had to go for it</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670590997489762304" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:35:06.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:35:06.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jocerfranquiz/status/1670590997489762304" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__4oer7cn1q4v id__t79ohjau7um id__umihidynrgg id__tzbh06rt4pl id__tvrwz8kptd id__pvnp9hblrxe id__3ylrasebemt id__m7ach6aakom id__z9a8g7bjad id__y2i18etokhb id__wgqgx1cbn7o id__yy24h4lzluh id__jls301t25r id__q6b3jaawkaf id__mip1rpyqfdm id__nkba5k34g2o id__ss3vnjwb6s id__yjr27pwlv1 id__t76l9jmjblq" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jocerfranquiz"><div><div><div><div><a href="https://twitter.com/jocerfranquiz" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1666580896764375041/ifrC6-iN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Consider that Parallelism was an absolute sadistic exercise in the 90s</span></p></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
