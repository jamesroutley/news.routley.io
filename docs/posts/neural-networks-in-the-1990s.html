<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Original</a>
    <h1>Neural networks in the 1990s</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-142873" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670558589746905090" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="52" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:26:19.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:26:19.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="mainEntityOfPage"/><article aria-labelledby="id__b98rhbnk9f id__in33easuyh9 id__vnaxk7wsrri id__3ujvw0jhzbr id__vvkauslc3k id__st9owmx9fh8 id__yp9qaaydjmr id__nhshfukbxi id__6t9ax35bim3 id__9u4b5212z7l id__i7p6uxfu5g id__l7klu5okin id__k7mm20w1oos id__pit9qsq44gg id__g3amlo9ehj5 id__6vv6n6ogwar id__5dsfsf0cxk id__ee4ga5lmxq5 id__fstejxl288p" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://nagle.town/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>It is interesting how many old papers used neural networks with only a dozen or so units. Computers weren’t THAT slow in the 90s — BLAS (basic linear algebra subprograms) was already a thing that vendors hyper-optimized for. Not much overlap between HPC and NN people? </span><a dir="ltr" href="https://t.co/gpUNHLFkP3" rel="noopener noreferrer nofollow" target="_blank" role="link"><span aria-hidden="true">https://</span>cs.utoronto.ca/~hinton/absps/<span aria-hidden="true">sunspots.pdf</span><span aria-hidden="true">…</span></a></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670640732493824000" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="5" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T03:52:44.000Z" itemprop="dateCreated"/><meta content="2023-06-19T03:52:44.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670640732493824000" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__qklwaocbws id__kjvap680o1m id__r13zzmnios id__gywa149rp3s id__nd4t0d8bb4e id__q74a0qy8myp id__8uuaktrcv0f id__7yt03gdeuzp id__qzo1150s2wb id__zx3dikdsphd id__2j0f6h4ej6x id__4kjhrm3c3f id__tq6xvo7oe8 id__lqdp0mvp2ec id__13wieew9pnnq id__3jlr1o9qd5k id__nuoe9t2kvk id__ebcbhfzoa9p id__jon1ejmzkkb" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://nagle.town/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Even with sigmoids and a single layer, using 4096 neurons instead of 12 makes a big difference, and is no harder to train. A BLAS based 4k network would probably train faster on hardware of the day than a matlab implementation for 12 neurons, but you have to consider performance.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670562414830534658" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:41:31.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:41:31.000Z" itemprop="datePublished"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__i6wqcbtx92l id__wt8agjtmn9k id__nzdx9tccj8j id__7bwxerg8ur9 id__xwjvwcdzffh id__8jy41fu1bhg id__1yhwjugsbbg id__cp7oq79xiyu id__lyqwzu0dj7 id__4lxt0j26cp id__errz09ztd4c id__gbf9t56l1xs id__5arvn7udlih id__gsqfmxj6fo id__fsefl06no5c id__31gzl0ly3my id__qfmxirbssps id__rcge5rzxrdc id__wtdqm03o9ve" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-TimSweeneyEpic"><div><div><div><div><a href="https://nagle.town/TimSweeneyEpic" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/795819168629198849/SBY3ARvZ_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Also perhaps there wasn’t an obviously fruitful middle ground in between the small NN models studied by the earlier papers and today’s large image and language models. Paraphrasing Jensen, games were fortunate to have a continuum of ever-increasing computing utility.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670563624019009536" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="6" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:46:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:46:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670563624019009536" itemprop="url"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="isPartOf"/><article aria-labelledby="id__gt6mjem1i2 id__vn12scc1te id__os27u8lkrqg id__s4fx0ayr0gk id__6vmxkz8hbc id__9xftybts35q id__77ex06j3po id__p3w26fdrgg9 id__8fdiilfg3in id__oa6vvkj5uwn id__uuskvo0k9fe id__qf0k77eujp id__3z8l8l4p42d id__aj1lr9u7mz id__wo2c3o7pg0i id__0vbiajglkvki id__ivwo6b466gc id__2af8tjb00ag id__f7phnc69c1a" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://nagle.town/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Most of the tasks that they measure performance on would steadily improve with model size, even at those tiny starting points, although the power law scaling might have looked very unappealing relative to hoped-for algorithmic breakthroughs.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560161591709696" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:32:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:32:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/Sayter/status/1670560161591709696" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__gzmmrszxd2 id__4shno0jkhmc id__4qhhs302bzo id__ud03hz6wnjp id__7dvq9dwvdje id__7utfedn9q1 id__3gbhdlr7yof id__bc7z25h36ng id__fondnpz2tyi id__o6oxpd9iegd id__9vuo49c3s2 id__xuzdoqm1g7e id__d6jalwpiyeq id__e4egongw1iq id__io4ptno61a id__8ruoah2kaim id__yrv21zol1wp id__5lg4f1hlwe id__413y3paw2q6" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-Sayter"><div><div><div><div><a href="https://nagle.town/Sayter" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1565566877501427718/16F_ybGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>neural networks go back to like 1795 depending on your lvl of technicality</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560391296843776" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:33:29.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:33:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/invisioner/status/1670560391296843776" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__1pq0s5emjr id__d7bxsuboty id__npuw70av4ye id__5fjccb9wzvd id__3c3lycr44v4 id__x3a8tgujqi id__s0vmk7jud2 id__65324zc4jpu id__bhtdtjn5lzj id__ckihuxbv6a9 id__tfstjy0chl id__o6g790ou61 id__wgw1krfhpv id__wr9eg0wsl8i id__oxzj01sdza id__tbw0ctvoha id__6gqs8t6cqxk id__f42hx66uxfa id__wri3biil7c" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-invisioner"><div><div><div><div><a href="https://nagle.town/invisioner" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664156292556939264/wtRdeMj9_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670564129923375105" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:48:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:48:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__1zogq1z7k7o id__g8clqv2tmiv id__rvuhtq4alci id__b86rq62z5bh id__325gba4wuyu id__qbteefjmga id__z32v9ccpzcm id__iugcapeisnj id__fwfsptryakm id__24x0x6kcxwt id__4npoatt06pu id__kh2v9454vbc id__tgkg747weo id__27zhncjc4bz id__itcryv70hz id__fle74jt03uu id__fhqvs4h5h1u id__4vsba7y8cmf id__8z7k3h5gzct" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-klausraizer"><div><div><div><div><a href="https://nagle.town/klausraizer" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/511875923138723841/1mITJOnu_400x400.jpeg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>If I recall correctly, what we had back then was still not mature enough. Vanishing gradients when adding more layers were still a serious issue and we didn&#39;t know how to deal with it properly. And adding more neurons per layer was mostly another parameter you tried to optimize.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670568986336174080" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:07:38.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:07:38.000Z" itemprop="datePublished"/><meta content="https://twitter.com/vikhyatk/status/1670568986336174080" itemprop="url"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="isPartOf"/><article aria-labelledby="id__zmp7iiqlfdh id__bu8xdpuaqqk id__v3r0o814bx id__yxbd9gh4pw id__ky6d2s1ra9c id__jzwtgh8i2z id__h5vyfzr3bvs id__t9v6fw215nc id__q0znjv76i4k id__vkzmtt8yw9q id__s9h33jnkfxg id__7l0pkft44q7 id__2ju4yyio8zj id__r56njejkm6f id__bwmel3mpthl id__k03tkvsjbem id__tergyzeo40s id__1zxv2upegzo id__vsq4tms6f8o" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-vikhyatk"><div><div><div><div><a href="https://nagle.town/vikhyatk" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Agree - scaling depth was really hard before ResNet-style architectures.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670576267618316288" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:36:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:36:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/masegraye/status/1670576267618316288" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__4mfs5i7vf5 id__z0honq37d3h id__pv5w9rn1x5 id__ggsx1gc8fdh id__ppt7o2lqt6a id__yycirr8mgo8 id__nuth73dp9k id__4u3yhypr98x id__g2osa5d473t id__dylv0s41b2e id__o4v6ww1ed8 id__18fq3vw55nw id__jkjpo4ilzun id__e6menfchhxh id__pug9jojoppf id__ydhr099ehf id__mdnst5n1u9c id__hcgzjfig1ee id__n9rvw3bdv5i" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-masegraye"><div><div><div><div><a href="https://nagle.town/masegraye" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1639806071244988418/Co0AoGGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>In the academic world, you can go “theoretical” or “applied”, but few go “production”.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670587368745648129" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:20:41.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:20:41.000Z" itemprop="datePublished"/><meta content="https://twitter.com/voxelbased/status/1670587368745648129" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__1q4qtc2lzgci id__v3o631ux6oa id__uy6nw2yvg8p id__dyrttyzd2f9 id__symr5n2lp2 id__atzqbn2vam id__abf2uu5iwf id__mpiqcx7p2jd id__ykd7z8hp5o id__ama13wpi4sp id__38euboe126l id__yfp9ney2b6g id__qfgoezk951 id__rs2tejcabs id__9fi84x6gjz4 id__1nb3ytnn0s1 id__uc733onrqqi id__05c9wv8hnvyr id__kt27urcete" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-voxelbased"><div><div><div><div><a href="https://nagle.town/voxelbased" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1561708024632328193/kanMG_si_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Large neural networks weren&#39;t tried due to performance, and performance wasn&#39;t optimized because large networks weren&#39;t used

Until it was so easy someone had to go for it</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670590997489762304" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:35:06.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:35:06.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jocerfranquiz/status/1670590997489762304" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__6e9srgwzeoe id__9sym01593im id__5ck7yg82bgl id__y2g4qu9xw0r id__c028uq3n2c7 id__imy2sdtxy2 id__0n8u4c7ebm2 id__bd6y8rawquf id__lq6c4htlpgk id__8pmdp2e4w5c id__6tjd41zflbc id__8lrgqhuk2dh id__r7jck8wwdl id__5abtrltr32 id__917gm8d8fui id__654fsr6hsmd id__sa3vdxsn7hd id__ryphntb60n id__qrqs5oyjxt" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jocerfranquiz"><div><div><div><div><a href="https://nagle.town/jocerfranquiz" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1666580896764375041/ifrC6-iN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Consider that Parallelism was an absolute sadistic exercise in the 90s</span></p></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
