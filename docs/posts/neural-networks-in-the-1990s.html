<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Original</a>
    <h1>Neural networks in the 1990s</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-42951" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670558589746905090" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="57" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:26:19.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:26:19.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="mainEntityOfPage"/><article aria-labelledby="id__4tzu32qyelf id__1u8xa05totg id__6cr0znbfz8 id__eofq1bd2ti9 id__ncs0c6pzus id__5gyvyrykz8i id__r149h0x6a6j id__76giqaoxzdn id__5w3sto19zpe id__13rtkh3t3rcb id__xste8r97js id__ygtpqjsxcca id__epz45ngvm8k id__9r0h6d8ki3f id__i7u18fr5g6k id__uj43k4hrjw id__3z0d2b85729 id__io267wubu4e id__5dqy8qhna8m" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>It is interesting how many old papers used neural networks with only a dozen or so units. Computers weren’t THAT slow in the 90s — BLAS (basic linear algebra subprograms) was already a thing that vendors hyper-optimized for. Not much overlap between HPC and NN people? </span><a dir="ltr" href="https://t.co/gpUNHLFkP3" rel="noopener noreferrer nofollow" target="_blank" role="link"><span aria-hidden="true">https://</span>cs.utoronto.ca/~hinton/absps/<span aria-hidden="true">sunspots.pdf</span><span aria-hidden="true">…</span></a></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670640732493824000" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="6" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T03:52:44.000Z" itemprop="dateCreated"/><meta content="2023-06-19T03:52:44.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670640732493824000" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__kffvktkh00b id__ebawgt2fu9 id__g1nm3w3jfbw id__wo7kr0wer3q id__h4ibgegvdrn id__7shpdpigvn5 id__yend6r2p7d id__aymyplriacc id__1zjdi5dixu9 id__stclkg3a2ar id__xp40pu7k91a id__vdqdmjw4ad id__e2ytlwth1u9 id__1cdhfecn57v id__2hhmhq7i61p id__vwshjoqa8lq id__xeio8m93tn id__mff44u1fbpc id__p10f6wa63hj" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Even with sigmoids and a single layer, using 4096 neurons instead of 12 makes a big difference, and is no harder to train. A BLAS based 4k network would probably train faster on hardware of the day than a matlab implementation for 12 neurons, but you have to consider performance.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670562414830534658" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="5" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:41:31.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:41:31.000Z" itemprop="datePublished"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__0ux0p5euywx id__uqnzirqm5 id__ntoqna17nta id__oxdpr7r1eyl id__a2ww2t8fpka id__7w11fa8wcp7 id__hj41mlk4cum id__5lsdzdk307m id__65mag9v6zj6 id__vopv9onilog id__jgvjjwr95h id__qnrnzwphcw9 id__gfde34rr8ba id__8o641o8tg0l id__w1h6kad1tzh id__rzrmua5zbi id__58gnln0ityb id__8h9lldfmq7 id__ilv11msjflr" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-TimSweeneyEpic"><div><div><div><div><a href="https://twitter.com/TimSweeneyEpic" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/795819168629198849/SBY3ARvZ_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Also perhaps there wasn’t an obviously fruitful middle ground in between the small NN models studied by the earlier papers and today’s large image and language models. Paraphrasing Jensen, games were fortunate to have a continuum of ever-increasing computing utility.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670563624019009536" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="6" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:46:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:46:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670563624019009536" itemprop="url"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="isPartOf"/><article aria-labelledby="id__4elh7c0aj3r id__i0rxvbcmeh id__htfrbfotll id__z8ghzaatvrm id__jbg79m5fttm id__y79mhsqeeas id__ev05wek6ycn id__3k9chv83q5n id__cureyy61w2a id__ogwxao9gx6p id__xrzvjkj7g7c id__724deqzddb5 id__ecb8kfd80o8 id__jnfml6i33kq id__8z2ee6vdrc2 id__f0qffs7dq6m id__h69aqmtcu75 id__se5p0kd8sfm id__48neaahxm02" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Most of the tasks that they measure performance on would steadily improve with model size, even at those tiny starting points, although the power law scaling might have looked very unappealing relative to hoped-for algorithmic breakthroughs.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560161591709696" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:32:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:32:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/Sayter/status/1670560161591709696" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__pzx9yw8aa3 id__f4er7rivgsv id__vn5p17jf7ol id__z3nnwrhbql id__wr5ok4mcwsq id__3j3h1ado1x1 id__bg0e2lkakp id__qaq8sy1isy id__7r7j0tb3aje id__ir6olseq8yc id__ygwmsi6w5xj id__zljvsjiwz2 id__kmnf1tglan9 id__8k7edkhd2 id__pf95xorymr id__h1wnygz7l0k id__k8otcjv11tp id__s0wi7chs49i id__l0tcjqhrv1" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-Sayter"><div><div><div><div><a href="https://twitter.com/Sayter" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1565566877501427718/16F_ybGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>neural networks go back to like 1795 depending on your lvl of technicality</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560391296843776" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:33:29.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:33:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/invisioner/status/1670560391296843776" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__moz3rpswtr id__el8m27nrbr7 id__jbz5nyrwbg id__6d1x4248lj5 id__z4cyep0b769 id__v9go4lnpexq id__z38kxqr29zm id__fwfp0n3826 id__kfz060d81z id__hsuda700l id__00qe2krt98x5 id__9g586ia3x4 id__6ce2aalkx7b id__2n3wp8b6por id__ldoya9miz7l id__adcyolzbtf id__yxvw5pkplbn id__8jhwmbtxd7m id__hk26sohw2ho" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-invisioner"><div><div><div><div><a href="https://twitter.com/invisioner" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664156292556939264/wtRdeMj9_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670564129923375105" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:48:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:48:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__b9w90yqefm id__252btcachx7 id__vjyeunwdlwe id__3df5n91e8h id__bu0m0x2dx3 id__h7zvqrrch9e id__vf7ic5aj5j9 id__abieczc38y id__rzxs7fwobgo id__evg3bjwy46t id__teq3v0bgn4s id__jbfsmddnvhl id__osa4n2fc09 id__jv8ch56en2 id__mx20yxx6wq id__22xw1lp00qb id__fayofk462mm id__b5mixypmg5t id__a7zv5yavq9a" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-klausraizer"><div><div><div><div><a href="https://twitter.com/klausraizer" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/511875923138723841/1mITJOnu_400x400.jpeg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>If I recall correctly, what we had back then was still not mature enough. Vanishing gradients when adding more layers were still a serious issue and we didn&#39;t know how to deal with it properly. And adding more neurons per layer was mostly another parameter you tried to optimize.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670568986336174080" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:07:38.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:07:38.000Z" itemprop="datePublished"/><meta content="https://twitter.com/vikhyatk/status/1670568986336174080" itemprop="url"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="isPartOf"/><article aria-labelledby="id__deqviyakt6i id__ep7a9u7pzot id__5ocdx59z8 id__ay5v8o588mb id__jffy27wtu5 id__iwa03wn9o3 id__ng1v8qhtj6l id__pi4m807bp9p id__jdakgvkrbz id__e71291oq4ap id__6q6qftc7ge9 id__nz08rekygf id__96vux9sunov id__xje5c4zcyj id__batkdo7j55t id__gvzkkfax75o id__k1e25ald69g id__lyyp78jd9rk id__bgly7mjtnb4" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-vikhyatk"><div><div><div><div><a href="https://twitter.com/vikhyatk" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Agree - scaling depth was really hard before ResNet-style architectures.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670576267618316288" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:36:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:36:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/masegraye/status/1670576267618316288" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__m5pfwr503p id__36l09n803nr id__82g9f2b5lp4 id__mredk8vmoq id__bv1sr272lna id__6et0y3301xb id__66pa7tzazcq id__j5qrrx84nnj id__mplnjqju0km id__cclj95nmh8v id__vi71f2ue4z id__ep6lgk4f9t7 id__x4rlu1rv4sf id__7no0tczcjj2 id__a7avcnrzjh id__wbcnxnvdwzq id__cyjwec1sz5q id__kawfkc9j3m id__fh9c1wcgy2o" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-masegraye"><div><div><div><div><a href="https://twitter.com/masegraye" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1639806071244988418/Co0AoGGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>In the academic world, you can go “theoretical” or “applied”, but few go “production”.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670587368745648129" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:20:41.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:20:41.000Z" itemprop="datePublished"/><meta content="https://twitter.com/voxelbased/status/1670587368745648129" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__p90vddvrqop id__94vk5zi75ga id__ixg25gu719 id__osn61mxns7 id__1w4wnv3eg9u id__awap6qyiksg id__sbebkgekx4 id__3i1te3sr26s id__7dw8ws3rvj8 id__kyicd8zxson id__3tf27o3552 id__rjyqqtv2r6 id__2kgkia5yjqv id__gd4b7r9ggik id__d2802n0lkap id__kck3qyp4jno id__lw8to8c26f id__b9f4b3efwe id__jomohe1a27a" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-voxelbased"><div><div><div><div><a href="https://twitter.com/voxelbased" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1561708024632328193/kanMG_si_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Large neural networks weren&#39;t tried due to performance, and performance wasn&#39;t optimized because large networks weren&#39;t used

Until it was so easy someone had to go for it</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670590997489762304" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:35:06.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:35:06.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jocerfranquiz/status/1670590997489762304" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__6jqbpowvicc id__wx6dtkyk7s id__mw46aq9ast id__b9e5m7duvhi id__0oyyie0uaoom id__0vutvg1snzpn id__dqkjbfeqt04 id__3brjnek8vlc id__z8st3f1pkl id__l1zf12rbc5 id__nuymdjtzvlq id__u83wett5r7t id__xsx1prsqrg id__b5tzsir9min id__wr2f2vzty5g id__ewfa0dth1ij id__ypmfhlhjp1a id__aa2vqqv4y8p id__l2rltyb7spg" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jocerfranquiz"><div><div><div><div><a href="https://twitter.com/jocerfranquiz" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1666580896764375041/ifrC6-iN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Consider that Parallelism was an absolute sadistic exercise in the 90s</span></p></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
