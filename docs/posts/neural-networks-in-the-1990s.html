<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Original</a>
    <h1>Neural networks in the 1990s</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-153043" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670558589746905090" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="61" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:26:19.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:26:19.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="mainEntityOfPage"/><article aria-labelledby="id__awbcarxtmdr id__dgpedp8z66e id__xmguiz24zs id__edij0f8co4c id__mwc1855bufb id__9g50swc50u9 id__ldb583qu00j id__mxcmwrx6ss id__hcee5oajzec id__vut1phf9hek id__m9qd2iljbao id__b8advti62pk id__8q3e0tlmthm id__a8qfy3x38iu id__wion2n82o2j id__eqgwjgbb1lh id__fhlm30manhw id__7ns2u2nhgjj id__76fgmzx640p" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>It is interesting how many old papers used neural networks with only a dozen or so units. Computers weren’t THAT slow in the 90s — BLAS (basic linear algebra subprograms) was already a thing that vendors hyper-optimized for. Not much overlap between HPC and NN people? </span><a dir="ltr" href="https://t.co/gpUNHLFkP3" rel="noopener noreferrer nofollow" target="_blank" role="link"><span aria-hidden="true">https://</span>cs.utoronto.ca/~hinton/absps/<span aria-hidden="true">sunspots.pdf</span><span aria-hidden="true">…</span></a></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670640732493824000" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T03:52:44.000Z" itemprop="dateCreated"/><meta content="2023-06-19T03:52:44.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670640732493824000" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__3og41wtop18 id__wd7qdzrgc9 id__higgg2ys637 id__pzobmuwemj id__4h61bajm9kq id__9i9hmymgf5f id__34def2r90oc id__ntha0y7b0er id__grc4ofje0n4 id__jlcoiyjw37q id__26irt8gpdqk id__72fvlovt9zi id__lvdn4kqsil id__1au9pnkmkhb id__8qjw8ztrfs9 id__omrfhruuish id__ax9zgj9wjh7 id__ep5dci4ew1s id__yqvd1xwszb" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Even with sigmoids and a single layer, using 4096 neurons instead of 12 makes a big difference, and is no harder to train. A BLAS based 4k network would probably train faster on hardware of the day than a matlab implementation for 12 neurons, but you have to consider performance.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670562414830534658" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="5" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:41:31.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:41:31.000Z" itemprop="datePublished"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__ami8sv5wl78 id__twdpch8739 id__m3kscx8nj8b id__rvvzt6fnzt id__qqlmv2gh7u id__44s9qo03p87 id__l1sye3s2fo id__erbrs8y835 id__aookxlyjutf id__msi8t08z8um id__1ngzsqax8xo id__9qgavuq6t6 id__lkj5gscdsnp id__9loze9f0tag id__85xzuegz5lw id__5vfpm5yc6kf id__pbjtc2yp6dp id__evp0npy9a7t id__tnq8fo2aw" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-TimSweeneyEpic"><div><div><div><div><a href="https://twitter.com/TimSweeneyEpic" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/795819168629198849/SBY3ARvZ_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Also perhaps there wasn’t an obviously fruitful middle ground in between the small NN models studied by the earlier papers and today’s large image and language models. Paraphrasing Jensen, games were fortunate to have a continuum of ever-increasing computing utility.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670563624019009536" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:46:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:46:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670563624019009536" itemprop="url"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="isPartOf"/><article aria-labelledby="id__dk782q3opd6 id__ou9lzwe9gw id__6mjqqft8q1l id__qq8juelpe3d id__vyfvigoyca id__6o0ttfc6jbl id__70u3dafd4m3 id__k1cju3i0kd id__fxrdne43wlm id__e82puv8lluc id__5u72mglaxlj id__skm5biy12gn id__aek60zpj2gl id__ije99sa2jeq id__z0npenxh4ad id__pl9npaei3ug id__7iri2bpvj44 id__visevho4vcg id__g1ha8kzycph" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://twitter.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Most of the tasks that they measure performance on would steadily improve with model size, even at those tiny starting points, although the power law scaling might have looked very unappealing relative to hoped-for algorithmic breakthroughs.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560161591709696" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:32:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:32:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/Sayter/status/1670560161591709696" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__saskbmwnmo id__euu8komhevw id__hv8nxu7qmur id__subxjk65ez id__ufugwfotlcd id__5gd5fmbf502 id__unwq24fc0uo id__bxhyvd458ik id__qc1ifcguxvd id__mic035qtblq id__txaff9elz58 id__anvh5s1qfdd id__i7nm1p934da id__pnaw794wms id__e4kpj88ohj9 id__ck8tloih7y5 id__4stt8m4u1sm id__402tyqszas7 id__1fg7x63buvu" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-Sayter"><div><div><div><div><a href="https://twitter.com/Sayter" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1565566877501427718/16F_ybGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>neural networks go back to like 1795 depending on your lvl of technicality</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560391296843776" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:33:29.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:33:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/invisioner/status/1670560391296843776" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__iig9pcketi9 id__al47im06da6 id__f6g8tp94m59 id__advs7rztigc id__ctvubbfxlzf id__8v102cn3kqn id__66bipzjk17e id__nasyvpc0ga id__abvby727ej id__4py2liyjob id__1uk68qr7ocn id__x4y5xzdbee id__zcr99zl5ca id__9fkbshefonj id__sniwl4sze6 id__mslorcazn1 id__3xkgwctn98b id__r7lfrcqjlha id__yh45p3akqfe" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-invisioner"><div><div><div><div><a href="https://twitter.com/invisioner" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664156292556939264/wtRdeMj9_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670564129923375105" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:48:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:48:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__tw0nsyaui id__6qedstqf8cb id__867ydxs8bib id__zd4iiat6yam id__mg49rysg9n id__yigiyr6k7jh id__pmk1u51uv5n id__7v1z7c1fhga id__p0pfeezm3b id__eg0d1bp2ssa id__jeate52h0y id__md8oihq3fl8 id__557dszo41it id__fn9zdbd8kuv id__9l0kj6wxbd5 id__jon0ndeqmkm id__cxhrpnzy7kv id__ibmrc2is35 id__vc0lxg4pqp" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-klausraizer"><div><div><div><div><a href="https://twitter.com/klausraizer" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/511875923138723841/1mITJOnu_400x400.jpeg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>If I recall correctly, what we had back then was still not mature enough. Vanishing gradients when adding more layers were still a serious issue and we didn&#39;t know how to deal with it properly. And adding more neurons per layer was mostly another parameter you tried to optimize.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670568986336174080" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:07:38.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:07:38.000Z" itemprop="datePublished"/><meta content="https://twitter.com/vikhyatk/status/1670568986336174080" itemprop="url"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="isPartOf"/><article aria-labelledby="id__yaqr480n2kj id__e27xq3kb5re id__7hmziz36fdl id__h21dl2kya7b id__y43sa7su5f id__e39966k0kn id__6g5uui1md2u id__syjdwq191p id__4muwrrk37q7 id__vrcaqry33l id__pjwbxfxgf1 id__a02pgu3fthg id__qiwvvflxdmq id__a1t78fsfmh id__2xvonc7x0fj id__hg59oxr2c6d id__mm76b2793 id__ztn8g2sop9 id__41b6m9axbnh" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-vikhyatk"><div><div><div><div><a href="https://twitter.com/vikhyatk" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Agree - scaling depth was really hard before ResNet-style architectures.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670576267618316288" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:36:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:36:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/masegraye/status/1670576267618316288" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__sl776xlj2xe id__cnzo31jwm3c id__nugfyuqb14 id__bmbanu4dohc id__qjrshmmx1z id__wpq8vyim19h id__wuo7dzix6p7 id__45vh1v308k4 id__j3w4ybwtst id__5ygn14ywkhu id__ojzugrwr0y id__6jy9l108fie id__h49jtj20a6d id__5csnbb138ow id__spx7d72z2cg id__chh9khp4kli id__xx770b9sxfh id__1v97m6alte7 id__mvtoboxgfyg" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-masegraye"><div><div><div><div><a href="https://twitter.com/masegraye" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1639806071244988418/Co0AoGGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>In the academic world, you can go “theoretical” or “applied”, but few go “production”.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670587368745648129" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:20:41.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:20:41.000Z" itemprop="datePublished"/><meta content="https://twitter.com/voxelbased/status/1670587368745648129" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__4tjlgqxx4us id__pqb7nzggiy id__a8wqbf8f48l id__xukfx4fo2d id__s95ycbqxtci id__wcbgf8j9dy id__wd4ekkaqq8 id__chuglbwidiw id__6yff8y5fcl5 id__7aei3bv977p id__lrqlneu3xe id__tu137e1ge5b id__9r560yrjk9m id__j1drai1l5d id__iglkium117l id__p8y70aol0w id__xxw8qvuyam id__0ryh9n8ud5rr id__9dr6e5jn2jr" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-voxelbased"><div><div><div><div><a href="https://twitter.com/voxelbased" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1561708024632328193/kanMG_si_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Large neural networks weren&#39;t tried due to performance, and performance wasn&#39;t optimized because large networks weren&#39;t used

Until it was so easy someone had to go for it</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670590997489762304" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:35:06.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:35:06.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jocerfranquiz/status/1670590997489762304" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__1hq9k2zods9 id__jbcxc6ex33 id__kmbrssjnm4 id__qju9ze0ke1 id__9n2hgwow22e id__wygr4n399y id__k7u6feyuvns id__ohzwg4lcrv id__cvrp3ansuv5 id__zu2c0snz11 id__3uijg8x2p09 id__1qi9ez7zklr id__99wo4hjhc3u id__yvlmge812ug id__6oee1xkkzao id__gl3aop0i6ca id__vj7p2upph0f id__4fxgj3c92us id__joyvpjptjtc" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jocerfranquiz"><div><div><div><div><a href="https://twitter.com/jocerfranquiz" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1666580896764375041/ifrC6-iN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Consider that Parallelism was an absolute sadistic exercise in the 90s</span></p></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
