<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Original</a>
    <h1>Neural networks in the 1990s</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-67132" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670558589746905090" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="73" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:26:19.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:26:19.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="mainEntityOfPage"/><article aria-labelledby="id__3vbbxxe3lyw id__bk693bagemw id__zk7yz7if03 id__y7o8ke7shsa id__1acb942el4l id__n7a4e3pm5x id__a9ak4f24ov id__lhdfpszkq8p id__1qvy9p7pk2x id__shs6pv9hibh id__6comhx5ii9g id__iabjd8dx9ha id__r4msn3b6q9g id__0po7fwn0kf0l id__xz0pxgsz8n id__taui5jtatjf id__fg6v30737j id__3tf92a0hk07 id__mu8b415zwqm" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://www.datadoodad.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>It is interesting how many old papers used neural networks with only a dozen or so units. Computers weren’t THAT slow in the 90s — BLAS (basic linear algebra subprograms) was already a thing that vendors hyper-optimized for. Not much overlap between HPC and NN people? </span><a dir="ltr" href="https://t.co/gpUNHLFkP3" rel="noopener noreferrer nofollow" target="_blank" role="link"><span aria-hidden="true">https://</span>cs.utoronto.ca/~hinton/absps/<span aria-hidden="true">sunspots.pdf</span><span aria-hidden="true">…</span></a></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670640732493824000" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="8" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T03:52:44.000Z" itemprop="dateCreated"/><meta content="2023-06-19T03:52:44.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670640732493824000" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__n14agu3v97 id__cv84ux6w0x9 id__70apzzjjary id__pidjv54slya id__1uxaz27uybs id__69tsprqbt9v id__7xjbm6usif5 id__dgwcoct66le id__ijovg9boy1b id__sju8v2puhbl id__ilbp26olsfh id__9sno1rku6o9 id__9j8xat6rhv id__bs8zmmzwrjg id__yl95jionhzp id__jdaqq75hhj id__35ac05v0jff id__hg014xt6goa id__7osyhilxuk7" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://www.datadoodad.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Even with sigmoids and a single layer, using 4096 neurons instead of 12 makes a big difference, and is no harder to train. A BLAS based 4k network would probably train faster on hardware of the day than a matlab implementation for 12 neurons, but you have to consider performance.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670562414830534658" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="5" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:41:31.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:41:31.000Z" itemprop="datePublished"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__ckmv5vdd5h6 id__t8d3ns4tc7 id__ho57mitpoj id__ggzq6vslkl id__65ovg6mnu1f id__jzj5v61ykge id__6ii53f6gh5 id__1caneu5von2 id__9yksz1ssiws id__5y2kq94woe6 id__evwavvm4c3q id__ixgylswhwps id__n23gfh71kv id__w9qka3orekn id__l2z2mhelcus id__z2r7cj7h1c id__7jg4jswxc8k id__ghkgu97p4kj id__fhg4mpca34b" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-TimSweeneyEpic"><div><div><div><div><a href="https://www.datadoodad.com/TimSweeneyEpic" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/795819168629198849/SBY3ARvZ_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Also perhaps there wasn’t an obviously fruitful middle ground in between the small NN models studied by the earlier papers and today’s large image and language models. Paraphrasing Jensen, games were fortunate to have a continuum of ever-increasing computing utility.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670563624019009536" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:46:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:46:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670563624019009536" itemprop="url"/><meta content="https://twitter.com/TimSweeneyEpic/status/1670562414830534658" itemprop="isPartOf"/><article aria-labelledby="id__cahb59is82o id__mykir1chheb id__cnjmlfq1jw id__5vx43ynbbr id__qj6mzw3mjp9 id__cfnrtoqml0b id__4394hnt1tsm id__gx1d4kczfhh id__e2piavzefbo id__wmrfls5c3p id__7ikis5nw87q id__bmwekxjmlfc id__gj6u1zarkd8 id__e44nabnhaal id__smnm4qc5tup id__9jsymg4odvm id__k3jkde7mr4j id__sdd7fv2qx7n id__f6ftwz6qaph" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-ID_AA_Carmack"><div><div><div><div><a href="https://www.datadoodad.com/ID_AA_Carmack" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1560764938083352577/B1X3m4NN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Most of the tasks that they measure performance on would steadily improve with model size, even at those tiny starting points, although the power law scaling might have looked very unappealing relative to hoped-for algorithmic breakthroughs.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560161591709696" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:32:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:32:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/Sayter/status/1670560161591709696" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__8kdcblbfhys id__cecl95zuaeu id__wi1ldzznpqt id__qfnkqyqlgjl id__xspy4po88za id__3m28z9ag0vk id__eiipbadknqt id__8z2tqm6v66g id__alwik9lckza id__c71vm4cds26 id__wyaztdhw84g id__gfaec0hyncb id__m7lnprcqsa id__r8i2wckae5p id__sr3c7hezkob id__inbq307cfh id__hjzshrq1lvr id__mbwm7gh2drt id__3c4dofl7rda" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-Sayter"><div><div><div><div><a href="https://www.datadoodad.com/Sayter" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1565566877501427718/16F_ybGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>neural networks go back to like 1795 depending on your lvl of technicality</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670560391296843776" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:33:29.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:33:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/invisioner/status/1670560391296843776" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__b4jsdn5a5x id__0j3m2nv3noxg id__83lz9718lj9 id__60z4fdz21iy id__hkxqtfm2kzn id__6c1ttmtbk65 id__s54w4n367uj id__gnii8upppwl id__ol28xyedhdm id__pu11uh3gl9n id__rldohag8vg id__jbtlvu1pu3p id__sgji3lx6tic id__ke20hb6d1ab id__rg0idkiev5g id__lmmesad95sc id__2wdxp5kl1eg id__og7omqdb6y7 id__nsvo0ywzzt" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-invisioner"><div><div><div><div><a href="https://www.datadoodad.com/invisioner" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664156292556939264/wtRdeMj9_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670564129923375105" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T22:48:20.000Z" itemprop="dateCreated"/><meta content="2023-06-18T22:48:20.000Z" itemprop="datePublished"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__wn37n4k3pfo id__npox1h871w id__l8itnbxcf09 id__gh6ow3npi0l id__7ot9u682gba id__2zgobqndie id__81blgk5w16f id__5ebksn00rig id__iyqf0fewm5p id__x7lzlcn3a9 id__kq7f8gs3g6e id__mea1iahwbh id__jg3xxj7gcl id__9rz696wth9d id__p7phhyk9od id__02ana4nikixd id__4hnl1adau7b id__kbs6y4qdje id__q4hotw74zmp" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-klausraizer"><div><div><div><div><a href="https://www.datadoodad.com/klausraizer" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/511875923138723841/1mITJOnu_400x400.jpeg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>If I recall correctly, what we had back then was still not mature enough. Vanishing gradients when adding more layers were still a serious issue and we didn&#39;t know how to deal with it properly. And adding more neurons per layer was mostly another parameter you tried to optimize.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670568986336174080" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:07:38.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:07:38.000Z" itemprop="datePublished"/><meta content="https://twitter.com/vikhyatk/status/1670568986336174080" itemprop="url"/><meta content="https://twitter.com/klausraizer/status/1670564129923375105" itemprop="isPartOf"/><article aria-labelledby="id__xkjc4wcpwcf id__b66w4k8hgy8 id__nw6erx5bqym id__p4x42sbpfzq id__lycbv0sfv2p id__5kqndyfrqqv id__ctcnxio92fi id__ihxmknmd1ie id__o06tdif023 id__nr0wf6a1zum id__z9edial3xk id__j8lr90111m id__qj4m7jcb0lk id__gzpfn8aqybn id__9lke8omk7mh id__8m8t3xuhx5o id__o0b4pv8e558 id__jptfwcxjuwk id__5kc0skwib7k" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-vikhyatk"><div><div><div><div><a href="https://www.datadoodad.com/vikhyatk" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Agree - scaling depth was really hard before ResNet-style architectures.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670576267618316288" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-18T23:36:34.000Z" itemprop="dateCreated"/><meta content="2023-06-18T23:36:34.000Z" itemprop="datePublished"/><meta content="https://twitter.com/masegraye/status/1670576267618316288" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__zyait3cy02s id__67qi0jb86a5 id__k8eecl7l39t id__24g87guxnxv id__jxibf4eszwf id__xyck6o19fnp id__dis88jxfjbf id__9bqpyycmnqk id__xdr7mgoxx4 id__vigo47t9ykl id__9sr80o0c6l id__1wy6a337j9c id__ndmt4evtj9 id__vvcv5w1oqn id__0i7o3xfmke5s id__ywvdo8mqw3g id__d221o9fpjiw id__gtop2doh9je id__adtf5hkjrtf" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-masegraye"><div><div><div><div><a href="https://www.datadoodad.com/masegraye" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1639806071244988418/Co0AoGGf_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>In the academic world, you can go “theoretical” or “applied”, but few go “production”.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670587368745648129" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:20:41.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:20:41.000Z" itemprop="datePublished"/><meta content="https://twitter.com/voxelbased/status/1670587368745648129" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__x03o36dee3b id__qyzs7n72jk id__mdzsr4j9lsh id__yz5zrq9xj4 id__1qwhyow94nz id__saasvubn5cb id__1h0lgr6wnr1 id__4kppjqr03zm id__r460v6nvxt id__ug0pskkzzkn id__8zoyxo4y0g8 id__8x4ytisewj9 id__6cheu7rzkya id__duspfkdx2x5 id__wvbgazsgzfs id__2rqql573h5j id__d2fr76wra29 id__tkw7gfpuzye id__5qf3s6ixypm" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-voxelbased"><div><div><div><div><a href="https://www.datadoodad.com/voxelbased" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1561708024632328193/kanMG_si_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Large neural networks weren&#39;t tried due to performance, and performance wasn&#39;t optimized because large networks weren&#39;t used

Until it was so easy someone had to go for it</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1670590997489762304" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="0" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-06-19T00:35:06.000Z" itemprop="dateCreated"/><meta content="2023-06-19T00:35:06.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jocerfranquiz/status/1670590997489762304" itemprop="url"/><meta content="https://twitter.com/ID_AA_Carmack/status/1670558589746905090" itemprop="isPartOf"/><article aria-labelledby="id__2dcjqrlzugl id__uph87yz9inh id__xf3tksdue6c id__pw8upedrpmp id__bwikmv53vll id__6tjml5w88h id__rwvc2ocl15r id__ezphak1bz8 id__ei3deahf12 id__twovgbfywmd id__8p2gez6fkka id__9ts90gmi37u id__hzppvclccy id__rq4itfh6zph id__mfanzp5v0kq id__ikp8keo5etn id__1wk6yno6br9 id__q6yycbuy5j id__iotg9kx8ky" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jocerfranquiz"><div><div><div><div><a href="https://www.datadoodad.com/jocerfranquiz" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1666580896764375041/ifrC6-iN_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Consider that Parallelism was an absolute sadistic exercise in the 90s</span></p></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
