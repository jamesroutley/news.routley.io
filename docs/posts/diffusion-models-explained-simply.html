<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.seangoedecke.com/diffusion-models-explained/">Original</a>
    <h1>Diffusion Models Explained Simply</h1>
    
    <div id="readability-page-1" class="page"><article><header></header><section><p>Transformer-based large language models are relatively easy to understand. You break language down into a finite set of “tokens” (words or sub-word components), then train a neural network on millions of token sequences so it can predict the next token based on all the previous ones. Despite some clever tricks (mainly about how the model processes the previous tokens in the sequence), the core mechanism is relatively simple.</p>
<p>It’s harder to build the same kind of intuition about diffusion models (in part because the papers are much harder to read). But diffusion models are almost as big a part of the AI revolution as transformers. High-quality image generation has driven a lot of user interest in AI, particularly ChatGPT’s recent upgraded image generation.</p>
<p>Even if you don’t care much about images, there are also some fairly capable text-based diffusion models - not yet competitive with frontier transformer models, but it’s certainly possible that we’d someday see a diffusion language model that’s state-of-the-art in its niche.</p>
<h3>The core intuition</h3>
<p>So what are diffusion models? How are they different from transformers? What is the animating intuition that makes sense of how diffusion models work?</p>
<p>Imagine a picture of a dog. You could slowly add randomly-colored pixels to that picture - the visual equivalent of “white noise” - until it just looks like noise. You could do the same for any possible image. All those possible images look very different, but the eventual noise looks the same. That means that <strong>for any possible image, there is a gradient of steps between that image and “pure noise”</strong>.</p>
<p><span>
      <a href="https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/a19d2/gaussian-noise.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="gaussian noise" title="gaussian noise" src="https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/1c72d/gaussian-noise.jpg" srcset="/static/b626e5866ab86470fd5e640f73ad085b/a80bd/gaussian-noise.jpg 148w,
/static/b626e5866ab86470fd5e640f73ad085b/1c91a/gaussian-noise.jpg 295w,
/static/b626e5866ab86470fd5e640f73ad085b/1c72d/gaussian-noise.jpg 590w,
/static/b626e5866ab86470fd5e640f73ad085b/a19d2/gaussian-noise.jpg 812w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy"/>
  </a>
    </span></p>
<p>What if you could train a model to understand that gradient?</p>
<h3>Training and inference</h3>
<p>To train a diffusion model, you take a large set of images, each expressed as a big tensor, and a caption for each image, each expressed as a normal text-model embedding. At each step in the training, for the current image, you add a little bit of random noise. Then you pass that noisy image and caption to the model, and ask it to predict exactly what noise was added to the image (e.g. which pixels changed from what color to what color). Unlike a language model, there’s no “tokens” - every model step takes a full image as input and produces a “noise report” as output.  Finally, you reward the model<sup id="fnref-1"><a href="#fn-1">1</a></sup> based on how close the model’s prediction was. </p>
<p>It’s important to train on noisy images, all the way from a little bit of noise to images that are indistinguishable from static. Typically that’s done by adding increasing amounts of noise to images in the training set during training (on a fixed schedule). Eventually your model gets really good at identifying the last layer of noise, even from images that just look like the “pure noise” image above.</p>
<p>At inference time, that’s exactly what you do: start with pure noise and a user-provided caption, then run the model to identify the “top” layer of noise. Remove that layer, then keep running the model and removing layers until you’re left with the “original” image. In reality, that image was entirely generated by the model. <strong>This process of identifying a layer of noise and reversing it is called “denoising”.</strong></p>
<p>There are lots of tricks that get used in this process, but the two most important ones are variational auto-encoders and classifier-free guidance.</p>
<h4>Variational auto-encoders</h4>
<p>Expressing an image (or a video) as a big tensor is very expensive. Images have a lot of pixels! In practice, diffusion models operate on a <em>compressed</em> version of each image, kind of like how text models operate on strings of tokens rather than individual letters of bytes. How is that compressed version generated?</p>
<p>Typically with a variational autoencoder (VAE) model that is trained first. That model learns to turn a big image tensor into a smaller random-looking tensor, while still being able to convert it back into the original image. Why use a VAE rather than an existing well-known compression like JPEG?</p>
<ul>
<li>It’s important that the compressed representation be random-looking (i.e. Gaussian-shaped) so the denoising process works properly. JPEG compression is highly structured</li>
<li>The compressed representation must always be the same size, which current compression algorithms don’t do</li>
<li>It’s OK for the VAE to discard some details (e.g. camera noise) which JPEG compression will retain</li>
</ul>
<p>So the usual strategy for training and inference is to run a VAE over your image tensor, add noise, denoise on that, and then decode it back to an original full-size image. Note that there are some models that don’t use VAE, like DALLE-3, but it’s much slower and more expensive.</p>
<h4>Classifier-free guidance</h4>
<p>There’s a common trick to make sure the model is actually learning to generate images based on the caption, instead of just any possible image. During training, you zero out the caption for some images, so the model learns two functions: not just how to remove the noise for a caption, but how to remove the noise for any possible image. During inference, you run once with a caption and once without, and blend the predictions (magnifying the difference between those two vectors). That makes sure the model is paying a lot of attention to the caption.</p>
<h3>Key differences from transformers</h3>
<p>The fundamental operation here is totally different from transformer-based language models, so many of your intuitions about transformers won’t apply. For instance:</p>
<ul>
<li>At each inference step, transformers keep generating new tokens, while diffusion models go from a (e.g.) 256x256 pixel image to a different 256x256 pixel image.</li>
<li>Transformers start with nothing but the prompt, but diffusion models need a “blank canvas” of pure noise to work from.</li>
<li>Transformers don’t “edit” previously generated tokens - once they’re outputted, they’re locked in - but diffusion models can and do change previous output as they go. </li>
<li>If you stop a transformer early, you probably don’t get the answer you were looking for. If you stop a diffusion model early, you get a noisy version of the image you wanted.</li>
</ul>
<p>That last point indicates an interesting capability that diffusion models have: you get a kind of built-in quality knob. If you want fast inference at the cost of quality, you can just run the model for less time and end up with more noise in the final output<sup id="fnref-2"><a href="#fn-2">2</a></sup>. If you want high quality and you’re happy to take your time getting there, you can keep running the model until it’s finished removing noise.</p>
<h3>Why does it work?</h3>
<p>Transformers work because (as it turns out) the structure of human language contains a functional model of the world. If you train a system to predict the next word in a sentence, you therefore get a system that “understands” how the world works at a surprisingly high level. All kinds of exciting capabilities fall out of that - long-term planning, human-like conversation, tool use, programming, and so on.</p>
<p>What is the equivalent animating intuition for diffusion models? I don’t really know, but it’s probably something about the relationship between noise and data - if you can train a system to tell the difference between them, you’re necessarily encoding a model of the world into that system? I bet there’s a much nicer way of articulating this, or a better intuition that could be teased out here.</p>
<p>The same principles that work for images work for other kinds of data: video, audiom, and even text.</p>
<h3>Diffusion video models</h3>
<p>So far this has all been about image diffusion models. What about diffusion models that generate video? As far as I can tell, there are lots of different approaches, but the simplest one is to treat the entire video as a single noisy input. Instead of having your input be a tensor that represents a single picture, your input is a (much larger) tensor that represents all the frames in a video clip. As the model learns to identify noise, it’s also learning each frame relates to the other frames in the clip (object permanence, cause and effect, and so on).</p>
<p>I find it very cool that you can run effectively the same approach for video that you do for single images. It suggests that the fundamental mechanism here is very powerful. It also sheds some light on why the current video diffusion models (like OpenAI’s Sora or Google’s VEO) only generate clips and can’t just “keep going” like a text-based transformer model can.</p>
<p>Incidentally, audio generation works the same way, just with a big audio tensor instead of a big video tensor.</p>
<h3>Diffusion text models</h3>
<p>What about diffusion models that generate text? Text-based diffusion models are really strange, because you can’t just add noisy pixels to text in the same way that you can to images or video. The <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201/#sec12">main strategy</a> seems to be adding noise to the text <em>embeddings</em><sup id="fnref-3"><a href="#fn-3">3</a></sup>. At inference time, you start with a big block of pure-noise embeddings (presumably just random numbers) then denoise until it becomes actual decodable text.</p>
<p>How do you turn embeddings back into text? There’s no obvious way. If you just try and look up the “closest” token to each embedding, you often end up with gibberish. If you use a separate decoder model to translate the embeddings, that works but feels a bit like cheating - at that point your diffusion model is really just generating a plan for your real text-generation model.</p>
<h3>Summary</h3>
<ul>
<li>Diffusion models are trained to identify small amounts of noise in images, based on a caption embedding</li>
<li>That means you can start with pure noise and a user-provided caption and just keep chipping away layers of noise until you get to what the model thinks the original image should look like</li>
<li>The operating model is very different from transformers: not sequence-based, operates on previous outputs, and can in principle be sped up or stopped early</li>
<li>Video diffusion works the same way as image diffusion, but it’s harder for the model to learn because it requires tracking consistency over time</li>
<li>Text diffusion is weird because you can’t easily add noise to language, and if you convert to embeddings before adding noise it’s hard to reliably convert back</li>
</ul>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>May 19, 2025<!-- --> │ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a>, <a href="https://www.seangoedecke.com/tags/explainers/">explainers</a></p><hr/></article></div>
  </body>
</html>
