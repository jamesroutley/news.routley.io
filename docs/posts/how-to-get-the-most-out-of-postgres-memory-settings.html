<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tembo.io/blog/optimizing-memory-usage">Original</a>
    <h1>How to Get the Most Out of Postgres Memory Settings</h1>
    
    <div id="readability-page-1" class="page"><div>  <p>It’s no secret that databases use <em>a lot</em> of RAM. When Postgres needs to build a result set, a very common pattern is to match against an index, retrieve associated rows from one or more tables, and finally merge, filter, aggregate, and sort tuples into usable output. Every one of these steps relies on memory, and Postgres may handle thousands of such queries simultaneously.</p>
<p>Given the memory-hungry nature of databases like Postgres, how can we manage that voracious appetite to best utilize what’s actually available? How do we tackle the arduous task of wrangling the different types of memory allocation Postgres needs to operate efficiently? How can we prevent the operating system from defensively terminating a Postgres process that is using <em>too much</em>?</p>
<p>Measuring Postgres RAM usage is important, and less trivial than we might expect. <a href="https://www.cybertec-postgresql.com/en/memory-context-for-postgresql-memory-management/">Advice for tuning memory</a> is <a href="https://stormatics.tech/blogs/postgresql-memory-management">numerous</a> and <a href="https://aws.amazon.com/blogs/database/determining-the-optimal-value-for-shared_buffers-using-the-pg_buffercache-extension-in-postgresql/">varied</a>, so it can be difficult to proceed with confidence. So allow us to distill the wisdom of trusted experts on Postgres memory usage into steps you can take to get the best bang for your RAM buck.</p>
<p>It’s extremely tempting to simply reach for that “upgrade instance” button, but there may be <em>another</em> way.</p>
<h2 id="sharing-is-caring">Sharing is Caring</h2>
<p>The first and by far the largest segment of RAM associated with a Postgres service is called <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-SHARED-BUFFERS">shared buffers</a>. It represents the most frequently retrieved rows from all tables and indexes, backed by a scoring heuristic based on use frequency. Essentially the more often a page is referenced, the more likely it is already located in shared buffers and does not need to be requested from the operating system. Kirk Roybal has a good explanation on <a href="https://www.youtube.com/watch?v=u-r8VuzXeBE">shared buffers internals</a> for those interested in the finer points.</p>
<p>The structure of Postgres shared buffers looks something like this:</p>
<p><img src="https://tembo.io/_astro/shared-buffers.T5-RBj2D_ZvMSxO.webp" alt="Segment of Shared Buffers" width="542" height="312" loading="lazy" decoding="async"/></p>
<p>It’s a static value that must be allocated when Postgres starts, and it won’t get any bigger, so we don’t have to worry about this contributing to <em>unexpected</em> memory problems. It’s important to size this properly, because it can only be changed when Postgres starts or restarts.</p>
<div> <p><img src="https://tembo.io/_astro/warning.OCtihNIB_18QjNr.svg" alt="warning" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>While Postgres considers this memory allocated up-front, the operating
system might not. As a result, it’s possible to specify extremely high
values up to the amount of RAM in the instance. This kind of dangerous
over-allocation implicitly over-commits RAM since it does not account for
other usage described in this article. Doing so will eventually lead to an
out-of-memory scenario and a subsequent crash.</p> </div>
<p>The default Postgres <a href="https://en.wikipedia.org/wiki/Page_(computer_memory)">page size</a> is 8kb, and each page represents several index records or table rows. Before Postgres can use an index or row, it must first retrieve the corresponding page into RAM. If those pages are in shared buffers, their “score” increases and they’re less likely to be evicted later. Given that we want as much data as possible to be cached by Postgres, it may seem that higher settings are <em>always</em> better.</p>
<p>The default value Postgres uses for <code>shared_buffers</code> is 128MB. The most frequently cited recommendation for production systems is to use 25% of available RAM, which also happens to be what we use here at Tembo. This rule of thumb is an excellent starting point and works for <em>most</em> systems because it scales with hardware. A large database is likely exceptionally busy and requires many CPU threads, and these are usually accompanied by lots of RAM, especially in cloud environments. It’s most important that the busiest portion of the database fits in RAM when possible, and 25% is a good starting point when you don’t know the exact value.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>I performed a series of benchmarks for EDB in 2022 to determine how shared
buffers influence Postgres performance. This analysis focused on <a href="https://www.enterprisedb.com/blog/harnessing-shared-buffers-and-reaping-performance-benefits-part-1">legacy
hardware outfitted with hard
drives</a>
and more modern <a href="https://www.enterprisedb.com/blog/harnessing-shared-buffers-part-2">SSD-based
systems</a>.
Ultimately we found that the 25% advice is <em>generally</em> sufficient, but
ultimately depends on how the database is used. Reporting systems, for
instance, actually do slightly better with <em>lower</em> settings due to reduced
cache hit rates caused by complex ad-hoc queries.</p> </div>
<p>What if we want a more precise value to avoid reserving too much, however? Constantly sweeping large extents of RAM unnecessarily adds system overhead and can result in <a href="https://www.enterprisedb.com/blog/steady-storage-stampede">disk flushing complications</a> after all. How do we find the amount of RAM used by the most active tables and indexes? This is where the <a href="https://www.postgresql.org/docs/current/pgbuffercache.html"><code>pg_buffercache</code></a> extension comes in handy. It provides a view into the shared buffers so we can figure out exactly which tables and indexes are allocated, including the relative score of every page. For the purposes of tuning <code>shared_buffers</code>, we only really need to know how much of the buffer is full using a query like this:</p>
<pre tabindex="0"><code><span><span>WITH</span><span> state</span><span> AS</span><span> (</span></span>
<span><span>  SELECT</span><span> count</span><span>(</span><span>*</span><span>) </span><span>FILTER</span><span> (</span><span>WHERE</span><span> relfilenode </span><span>IS NOT NULL</span><span>) </span><span>AS</span><span> used,</span></span>
<span><span>         count</span><span>(</span><span>*</span><span>) </span><span>FILTER</span><span> (</span><span>WHERE</span><span> relfilenode </span><span>IS</span><span> NULL</span><span>) </span><span>AS</span><span> empty</span><span>,</span></span>
<span><span>         count</span><span>(</span><span>*</span><span>) </span><span>AS</span><span> total</span></span>
<span><span>    FROM</span><span> pg_buffercache</span></span>
<span><span>)</span></span>
<span><span>SELECT</span><span> *</span><span>, </span><span>round</span><span>(used </span><span>*</span><span> 1</span><span>.</span><span>0</span><span> /</span><span> total </span><span>*</span><span> 100</span><span>, </span><span>1</span><span>) </span><span>AS</span><span> percent</span></span>
<span><span>  FROM</span><span> state</span><span>;</span></span></code></pre>
<p>Such a query shows how many pages of the buffer are used out of the total. If our database has been running for a while and the buffer cache isn’t 100% utilized, our setting may be too high and we can reduce the size of the instance or the value of <code>shared_buffers</code> to compensate. If it’s at 100% and only small fractions of many tables are cached, it may be beneficial to assign sequentially higher values until we see diminishing returns. It’s possible to perform this tuning without the aid of <code>pg_buffercache</code>, but it certainly eases the process!</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <div> <p>Postgres 16 has added a new <a href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-IO-VIEW"><code>pg_stat_io</code></a> view which can also provide some insight for tuning <code>shared_buffers</code>. This query can show the hit ratio as well as client backend reads and writes.</p><pre tabindex="0"><code><span><span>SELECT</span><span> (hits </span><span>/</span><span> (reads </span><span>+</span><span> hits)::</span><span>float</span><span>) </span><span>AS</span><span> hit_ratio,</span></span>
<span><span>       reads, writes</span></span>
<span><span>  FROM</span><span> pg_stat_io</span></span>
<span><span> WHERE</span><span> backend_type </span><span>=</span><span> &#39;client backend&#39;</span></span>
<span><span>   AND</span><span> context </span><span>=</span><span> &#39;normal&#39;</span></span>
<span><span>   AND</span><span> object</span><span> =</span><span> &#39;relation&#39;</span><span>;</span></span></code></pre><p>As explained by <a href="https://www.youtube.com/watch?v=rCzSNdUOEdg&amp;t=1094s">Melanie Plageman</a>, a read to write ratio close to 1 may indicate Postgres is constantly cycling the same pages in and out of <code>shared_buffers</code>. We’d want to increase <code>shared_buffers</code> to reduce this kind of thrashing if possible.</p> </div> </div>
<p>If we start crossing past 50% of system RAM, we should consider increasing the size of the instance instead. Postgres still needs memory for user sessions and associated queries after all. Speaking of…</p>
<h2 id="working-memory">Working Memory</h2>
<p>The other half of the memory balancing act falls under the memory Postgres uses to actually get things done, known as work memory. The parameter that controls this is—appropriately enough—named <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM"><code>work_mem</code></a>. The default is 4MB and this is one of the first values a user might modify in an attempt to make a query run faster.</p>
<div> <p><img src="https://tembo.io/_astro/warning.OCtihNIB_18QjNr.svg" alt="warning" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>Be wary here! It might be tempting to increase <code>work_mem</code> if the operating
system terminates Postgres due to an “out of memory” message, but that will
just exacerbate the problem. This will <em>increase</em> the amount of RAM Postgres
uses and make it <em>more</em> likely to face such terminations. Pay special
attention to this section to avoid that fate!</p> </div>
<p>However, this name is actually something of a misnomer. Many interpret “work memory” as a single allocation assigned to all work Postgres might perform while working on a query, but it can actually be <em>far more than that</em>. Consider the simplified execution diagram for a fairly basic query which is retrieving all products ever ordered by a certain user, and then sorting the results by order line item:</p>
<p><img src="https://tembo.io/_astro/query-nodes.1Hn8kYo3_te8w6.webp" alt="Example of query nodes" width="2122" height="782" loading="lazy" decoding="async"/></p>
<p>Each step requires combining only two items, and the product of that combination is then sent through further steps. Most databases refer to each of these steps as a “node”, and queries are commonly comprised of multiple nodes. This example in particular requires four nodes to produce the end result. When considering memory usage in a Postgres server, it’s <strong>critically important</strong> to understand that each node is allocated a <strong>separate</strong> instance of <code>work_mem</code>. So if we were using the <code>work_mem</code> default of 4MB, this query could consume up to 16MB of RAM.</p>
<p>If we have a moderately busy server with 100 of these queries running simultaneously, we could use up to 1.6GB of RAM simply to calculate results. More complex queries would require even more RAM depending on how many nodes are necessary to execute the query. The <a href="https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-MAX-CONNECTIONS"><code>max_connections</code></a> parameter also plays a part here, as it defines how many concurrent sessions may be executing a query.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>Use the
<a href="https://www.postgresql.org/docs/current/sql-explain.html"><code>EXPLAIN</code></a>
command to see the execution plan of a query. It will show how Postgres will
execute the query and all of the nodes necessary to produce output. Used in
conjunction with the <code>pg_stat_statements</code> extension, it becomes possible to
isolate the most active queries and estimate overall memory use due to
<code>work_mem</code>.</p> </div>
<p>This is one of the major reasons behind the 25% recommendation for <code>shared_buffers</code>. Until a database is closely profiled for query concurrency and complexity, it may not be safe to allocate a higher value. To do so could risk the operating system rejecting memory requests or terminating Postgres itself.</p>
<p>Conversely, if <code>work_mem</code> is set too low, any rows or intermediate results that won’t fit in RAM will spill to disk, which is orders of magnitude slower. Thankfully this is easy to detect by checking the <code>pg_stat_database</code> view with a query like this:</p>
<pre tabindex="0"><code><span><span>SELECT</span><span> datname, pg_size_pretty(temp_bytes </span><span>/</span><span> temp_files) </span><span>AS</span><span> overflow</span></span>
<span><span>  FROM</span><span> pg_stat_database</span></span>
<span><span> WHERE</span><span> temp_files </span><span>&gt;</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> datname  | overflow</span></span>
<span><span>----------+----------</span></span>
<span><span> mytestdb | </span><span>6017</span><span> kB</span></span></code></pre>
<p>Postgres keeps track of the cumulative size and count of all temporary files written to disk. We can use that to find a coarse average and if the size is reasonable, we can increase <code>work_mem</code> by this amount. Afterwards, the majority of queries will no longer require disk storage as temporary working space. It’s not uncommon for <code>work_mem</code> to be 2-4MB short of pure memory execution, resulting in queries that are much slower than they could be.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>How do we know “about how much” RAM is available per session? Try this: (80%
of total RAM - shared_buffers) / (max_connections). So if we have 16GB of
RAM, 4GB of shared buffers, and 100 max connections, we’d end up with about
88MB available per session. We would divide this value by the average number
of query plan nodes to obtain a good setting for <code>work_mem</code>. There is <a href="https://thebuild.com/blog/2023/03/13/everything-you-know-about-setting-work_mem-is-wrong/">room
for
interpretation</a>
for those interested in the finer minutiae.</p> </div>
<h2 id="ongoing-maintenance">Ongoing Maintenance</h2>
<p>The last of the tunables for Postgres RAM usage is similar to work memory, but associated specifically with maintenance. Consequently it has a similar parameter name in <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM"><code>maintenance_work_mem</code></a>. This specifies the amount of RAM dedicated to perform operations such as <code>VACUUM</code>, <code>CREATE INDEX</code>, and <code>ALTER TABLE ADD FOREIGN KEY</code>, with a default of 64MB.</p>
<p>Because this is limited to one operation per session, and it’s unlikely many concurrent such actions will occur, it’s considered safe enough to use higher values. It’s very common to set this as high as 1GB or even 2GB, as these maintenance operations can be very memory intensive and complete much faster if they can operate entirely in RAM.</p>
<p>One important caveat here is the Postgres autovacuum process which marks dead tuples for later reuse. Autovacuum launches background tasks until the limit of <a href="https://www.postgresql.org/docs/current/runtime-config-autovacuum.html#GUC-AUTOVACUUM-MAX-WORKERS"><code>autovacuum_max_workers</code></a>, and <em>each of these</em> may use a full instance of <code>maintenance_work_mem</code>. Most servers with RAM to spare will be safe with 1GB of maintenance work memory, but if RAM is scarce, we should be more judicious. There’s a specific <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-AUTOVACUUM-WORK-MEM"><code>autovacuum_work_mem</code></a> parameter if we want to restrict the autovacuum workers in particular.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>Note that the Postgres autovacuum workers cannot use more than 1GB, so
configuring <code>autovacuum_work_mem</code> over this value will have no effect.</p> </div>
<h2 id="session-pooling">Session Pooling</h2>
<p>The easiest way to reduce memory consumption is to put a logical cap on potential allocations. Postgres is currently a process-based engine, meaning every user session is assigned a physical process rather than a thread. This means every connection comes with a certain amount of RAM overhead, and contributes to context switching. As a result, one common recommendation is to set <code>max_connections</code> no higher than 4x the amount of available CPU threads. This minimizes the amount of time spent switching active sessions between CPUs and naturally limits the amount of RAM sessions can collectively consume.</p>
<p>Remember, if every session is executing a query, and each node represents one allocation of <code>work_mem</code>, our theoretical maximum work memory usage is: <code>connections * nodes * work_mem</code>. It’s not always possible to reduce query complexity, but we can <em>usually</em> reduce connection count. In cases where applications always open a certain inflated amount of sessions, or a multiple separate microservices rely on Postgres, this may be easier said than done.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <div> <p>Remember this formula: <code>work_mem * max_connections * 5</code></p><p>It’s a rough estimate of the maximum amount RAM a Postgres instance might allocate to user sessions simply for processing basic queries, assuming all connections are active. If the server doesn’t have enough RAM for this value, consider decreasing one of the factors, or increasing RAM.</p><p>Our estimate of five nodes per average query may not be accurate for your application, so adjust it as necessary once you have a better grasp of your query execution plans.</p> </div> </div>
<p>That means the next step is to introduce a <a href="https://tembo.io/docs/product/cloud/apps/connection-pooler/">connection pooler</a> like PgBouncer. This decouples client connections from the database and reuses the expensive Postgres sessions between them. When properly configured, several hundred clients can share a few dozen Postgres connections with no impact to the application.</p>
<p>We’re left with something like this:</p>
<p><img src="https://tembo.io/_astro/pgbouncer-stack.4nQUXY85_2iuV9W.webp" alt="Pgbouncer connection multiplexing" width="642" height="422" loading="lazy" decoding="async"/></p>
<p>We’ve seen PgBouncer multiplex over 1000 connections to 40-50 this way, dramatically reducing the overall amount of memory consumption caused by process overhead.</p>
<h2 id="reducing-bloat">Reducing Bloat</h2>
<p>Quite possibly the most difficult aspect of memory usage to track is that of table bloat. Postgres uses Multi-Version Concurrency Control (<a href="https://www.postgresql.org/docs/current/mvcc.html">MVCC</a>) to represent data in its storage system. This means any time a table row is modified, Postgres creates another copy of the row somewhere in the table, marked with a new version number. The Postgres <a href="https://www.postgresql.org/docs/current/routine-vacuuming.html"><code>VACUUM</code></a> process marks old row versions as “unused” space so that new row versions can be placed there.</p>
<p>Postgres has an automatic vacuum background process that continuously finds these reusable allocations and tries to make sure tables don’t grow without bound. Sometimes the default configuration for this is not sufficient for especially high volume systems however, and such maintenance can fall behind. As a result, tables may slowly fill with more <em>dead</em> rows than <em>live</em> ones, resulting in a table that is “bloated” full of old data.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>Our CTO Samay Sharma has <a href="https://www.youtube.com/watch?v=D832gi8Qrv4">a great
talk</a> on optimizing autovacuum,
and we have an <a href="https://tembo.io/blog/optimizing-postgres-auto-vacuum/">in-depth article</a>
from Adam Hendel as well. I’ve even <a href="https://www.enterprisedb.com/blog/pg-phriday-tidying-vacuum">covered this topic in the
past</a>. Properly
tuning autovacuum is an <em>incredibly</em> subtle topic that deserves an equally
thorough explanation. Even experienced Postgres DBAs could do with an
occasional refresher.</p> </div>
<p>Consider what this means for our shared buffers if a table is extremely bloated. Remember our earlier depiction of shared buffers? What happens if each page only contains one live row and several dead rows? We’d end up in a situation like this:</p>
<p><img src="https://tembo.io/_astro/shared-buffers-bloat.1emG5Mg3_Z1yJ8Wh.webp" alt="Bloated Tables in Shared Buffers" width="542" height="312" loading="lazy" decoding="async"/></p>
<p>If a particular query requires 10 rows in this scenario, we’d have to fetch 10 pages into shared buffers and end up wasting a lot of memory that could have been used for something else. If these rows are in particularly high demand, the usage count would keep them in shared buffers and make our cache much less efficient.</p>
<p>There are a lot of queries floating around the internet which can estimate table bloat, but the only way to get a concrete view of how pages look in a table is to use the <a href="https://www.postgresql.org/docs/current/pgstattuple.html"><code>pgstattuple</code></a> extension. Once that extension is installed, we can see <em>exactly</em> how bloated a particular table may be using a query like this:</p>
<pre tabindex="0"><code><span><span>SELECT</span><span> tuple_percent, free_percent</span></span>
<span><span>  FROM</span><span> pgstattuple(</span><span>&#39;my_table&#39;</span><span>);</span></span></code></pre>
<p>If the <code>free_percent</code> is greater than 30%, it may be necessary to modify autovacuum to be more aggressive. If it’s <em>significantly</em> greater than 30%, it’s probably a good idea to remove the bloat entirely. Unfortunately there’s currently no easy way to do this. Currently the only supported method is to essentially rebuild the table using a command like this:</p>
<pre tabindex="0"><code><span><span>VACUUM FULL my_table;</span></span></code></pre>
<p>A <code>VACUUM FULL</code> rewrites the entire table by relocating all live rows to a new location and discards the old bloated copy. This process allocates an access exclusive lock for its duration, so requires some kind of downtime in almost all cases.</p>
<p>An alternative to this is the <a href="https://reorg.github.io/pg_repack/"><code>pg_repack</code></a> extension supported by Tembo. This command-line tool can also reorganize a table to remove bloat, but does so without an exclusive lock and in an entirely online manner. Because this tool exists outside the Postgres core and modifies table and index storage, it’s often considered advanced usage. We recommend copious tests in a non-production environment before using it.</p>
<div> <p><img src="https://tembo.io/_astro/information.LEP00Rqx_ZVPnm9.svg" alt="info" width="24" height="24" loading="lazy" decoding="async"/>  </p> <p>It’s possible to go even further than this by playing <a href="https://www.2ndquadrant.com/en/blog/on-rocks-and-sand/">column
Tetris</a> and
reorganizing column order to maximize rows per page. This is probably an
extreme level of optimization, but is a viable strategy for environments
with the freedom to rebuild tables this way.</p> </div>
<h2 id="the-balancing-act">The Balancing Act</h2>
<p>Getting all of these parameters and resources configured properly is both an art and a science. We’ve seen how we can measure the actual usage of our shared buffers, and we can determine if work memory is too low. But what if—as in most cases—we’re constrained by our available hardware or budget? As a good example of this, what happens if we encounter a memory usage alert or the operating system terminated our service in the past? This is where the “art” comes in.</p>
<p>In some situations where memory is scarce, we may need to reduce <code>shared_buffers</code> slightly to make room for more <code>work_mem</code>. Or perhaps we must reduce both. If our application requires high session counts, it may make more sense to reduce <code>work_mem</code> or introduce a connection pool to prevent concurrent sessions from racking up extensive RAM allocations. It may make more sense to reduce <code>maintenance_work_mem</code> if we’d increased it in the past under the assumption we had sufficient RAM for everything. It’s a lot to consider!</p>
<p>If you followed along with our recommendations through the article, your <code>shared_buffers</code>, <code>work_mem</code>, <code>maintenance_work_mem</code>, and <code>autovacuum_work_mem</code> are likely more precisely tuned for your workload. However, it’s still possible—especially in low-memory instances—the above recommendations aren’t quite enough. In situations like that, we recommend following this order of operations to both maximize memory use and <em>avoid resource exhaustion</em>:</p>
<ol>
<li>If you’re not already using one, add a connection pooler and reduce <code>max_connections</code>. It’s often the quickest and easiest way to cut maximum resource consumption.</li>
<li>Use <code>EXPLAIN</code> on your most frequent queries reported by <code>pg_stat_statements</code> to find the <em>maximum</em> amount of nodes in queries rather than the average. Then set <code>work_mem</code> no higher than <code>(80% of total RAM - shared_buffers) / (max_connections * max plan nodes)</code>. The cumulative effect of <code>work_mem</code> is multiplicative, so controlling this is one of the easiest ways to prevent over-consumption.</li>
<li>Revert <code>maintenance_work_mem</code> and <code>autovacuum_work_mem</code> back to the default of 64MB. Consider increasing by 8MB increments if maintenance tasks are too slow and more RAM is available. These parameters don’t require restart, so are a bit more flexible for experimentation.</li>
<li>Use the <code>pg_buffercache</code> extension to see how much of which tables are stored in <code>shared_buffers</code>. Examine each table and index closely and see if there’s a way to reduce this by archiving data, revising queries to use less information, and so on. This may also include <code>VACUUM FULL</code> or <code>pg_repack</code> to compact pages used by active bloated tables. This may allow you to reduce <code>shared_buffers</code> accordingly, since the active data set is now smaller.</li>
<li>If <code>pg_buffercache</code> shows <code>shared_buffers</code> is full and can’t be reduced any further without evicting active pages, use the <code>usagecount</code> column to prioritize the most active pages. The value for this column goes from 1-5, so if we focus on pages used 3-5 times, we can reduce <code>shared_buffers</code> without significantly impacting performance. Remember, shared buffers work alongside operating system and filesystem caches but can’t be purged in the same way when memory pressure is high. Rather than Postgres having direct knowledge of cached pages, smaller shared buffers rely on the operating system to cache frequently used data. As such, it can be a good option when RAM is especially scarce.</li>
<li>Last but not least, provision more capable hardware. This could be purchasing a larger server or moving up to the next larger cloud instance size. If the database truly needs more RAM for its current workload and reducing the above parameters would adversely affect system performance too dramatically, it usually makes more sense to upgrade.</li>
</ol>
<p>As you can see, upgrading doesn’t have to the only, or even the <em>first</em> option to consider when configuring a Postgres cluster for optimal memory use. A busier and more popular application will require more database resources in general, but in many cases there is plenty of runway to leverage between hardware upgrades.</p>
<p>In the end, we hope you learned a bit about how Postgres uses memory, and the best ways to ensure a well-balanced database instance on <em>any</em> hardware allocation. Postgres performance is great out of the box, but it’s even better with a bit of informed TLC!</p>  </div></div>
  </body>
</html>
