<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/InternLM/InternLM">Original</a>
    <h1>InternLM â€“ new open source 7B LLM</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">

<h2 tabindex="-1" dir="auto"><a id="user-content-introduction" aria-hidden="true" href="#introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:</p>
<ul dir="auto">
<li>It leverages trillions of high-quality tokens for training to establish a powerful knowledge base.</li>
<li>It supports an 8k context window length, enabling longer input sequences and stronger reasoning capabilities.</li>
<li>It provides a versatile toolset for users to flexibly build their own workflows.</li>
</ul>
<p dir="auto">Additionally, a lightweight training framework is offered to support model pre-training without the need for extensive dependencies. With a single codebase, it supports pre-training on large-scale clusters with thousands of GPUs, and fine-tuning on a single GPU while achieving remarkable performance optimizations. InternLM achieves nearly 90% acceleration efficiency during training on 1024 GPUs.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-internlm-7b" aria-hidden="true" href="#internlm-7b"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>InternLM-7B</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-performance-evaluation" aria-hidden="true" href="#performance-evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance Evaluation</h3>
<p dir="auto">We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool <a href="https://github.com/internLM/OpenCompass/">OpenCompass</a>. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the <a href="https://opencompass.org.cn/rank" rel="nofollow">OpenCompass leaderboard</a> for more evaluation results.</p>
<table>
<thead>
<tr>
<th>Datasets\Models</th>
<th><strong>InternLM-Chat-7B</strong></th>
<th><strong>InternLM-7B</strong></th>
<th>LLaMA-7B</th>
<th>Baichuan-7B</th>
<th>ChatGLM2-6B</th>
<th>Alpaca-7B</th>
<th>Vicuna-7B</th>
</tr>
</thead>
<tbody>
<tr>
<td>C-Eval(Val)</td>
<td>53.2</td>
<td>53.4</td>
<td>24.2</td>
<td>42.7</td>
<td>50.9</td>
<td>28.9</td>
<td>31.2</td>
</tr>
<tr>
<td>MMLU</td>
<td>50.8</td>
<td>51.0</td>
<td>35.2*</td>
<td>41.5</td>
<td>46.0</td>
<td>39.7</td>
<td>47.3</td>
</tr>
<tr>
<td>AGIEval</td>
<td>42.5</td>
<td>37.6</td>
<td>20.8</td>
<td>24.6</td>
<td>39.0</td>
<td>24.1</td>
<td>26.4</td>
</tr>
<tr>
<td>CommonSenseQA</td>
<td>75.2</td>
<td>59.5</td>
<td>65.0</td>
<td>58.8</td>
<td>60.0</td>
<td>68.7</td>
<td>66.7</td>
</tr>
<tr>
<td>BUSTM</td>
<td>74.3</td>
<td>50.6</td>
<td>48.5</td>
<td>51.3</td>
<td>55.0</td>
<td>48.8</td>
<td>62.5</td>
</tr>
<tr>
<td>CLUEWSC</td>
<td>78.6</td>
<td>59.1</td>
<td>50.3</td>
<td>52.8</td>
<td>59.8</td>
<td>50.3</td>
<td>52.2</td>
</tr>
<tr>
<td>MATH</td>
<td>6.4</td>
<td>7.1</td>
<td>2.8</td>
<td>3.0</td>
<td>6.6</td>
<td>2.2</td>
<td>2.8</td>
</tr>
<tr>
<td>GSM8K</td>
<td>34.5</td>
<td>31.2</td>
<td>10.1</td>
<td>9.7</td>
<td>29.2</td>
<td>6.0</td>
<td>15.3</td>
</tr>
<tr>
<td>HumanEval</td>
<td>14.0</td>
<td>10.4</td>
<td>14.0</td>
<td>9.2</td>
<td>9.2</td>
<td>9.2</td>
<td>11.0</td>
</tr>
<tr>
<td>RACE(High)</td>
<td>76.3</td>
<td>57.4</td>
<td>46.9*</td>
<td>28.1</td>
<td>66.3</td>
<td>40.7</td>
<td>54.0</td>
</tr>
</tbody>
</table>
<ul dir="auto">
<li>The evaluation results were obtained from<a href="https://github.com/internLM/OpenCompass/">OpenCompass 20230706</a>  (some data marked with *, which menas come from the original papers), and evaluation configuration can be found in the configuration files provided by <a href="https://github.com/internLM/OpenCompass/">OpenCompass</a>.</li>
<li>The evaluation data may have numerical differences due to the version iteration of <a href="https://github.com/internLM/OpenCompass/">OpenCompass</a>, so please refer to the latest evaluation results of <a href="https://github.com/internLM/OpenCompass/">OpenCompass</a>.</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-model-zoo" aria-hidden="true" href="#model-zoo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model Zoo</h3>
<p dir="auto">InternLM 7B and InternLM 7B Chat, trained using InternLM, have been open-sourced. We provide two formats of model weights for use. In addition to loading the models using the Transformers format, you can also load the weights directly using InternLM for further pre-training or human preference alignment training.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>InternLM Format Weight Download Link</th>
<th>Transformers Format Weight Download Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>InternLM 7B</strong></td>
<td><a href="https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/2ddac1d69172383658f1a7111d271a96c6af8100619aa6ec0036db71302eca61/68747470733a2f2f63646e2d7374617469632e6f70656e786c61622e6f72672e636e2f6865616465722f6f70656e786c61625f6d6f64656c732e737667" alt="Open in OpenXLab" data-canonical-src="https://cdn-static.openxlab.org.cn/header/openxlab_models.svg"/></a></td>
<td><a href="https://huggingface.co/internlm/internlm-7b" rel="nofollow"><g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ðŸ¤—</g-emoji>internlm/intern-7b</a></td>
</tr>
<tr>
<td><strong>InternLM Chat 7B</strong></td>
<td><a href="https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/2ddac1d69172383658f1a7111d271a96c6af8100619aa6ec0036db71302eca61/68747470733a2f2f63646e2d7374617469632e6f70656e786c61622e6f72672e636e2f6865616465722f6f70656e786c61625f6d6f64656c732e737667" alt="Open in OpenXLab" data-canonical-src="https://cdn-static.openxlab.org.cn/header/openxlab_models.svg"/></a></td>
<td><a href="https://huggingface.co/internlm/internlm-chat-7b" rel="nofollow"><g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ðŸ¤—</g-emoji>internlm/intern-chat-7b</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><strong>Limitations:</strong> Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-import-from-transformers" aria-hidden="true" href="#import-from-transformers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Import from Transformers</h3>
<p dir="auto">To load the InternLM 7B Chat model using Transformers, use the following code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&#34;internlm/internlm-chat-7b&#34;, trust_remote_code=True)
&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&#34;internlm/internlm-chat-7b&#34;, trust_remote_code=True).cuda()
&gt;&gt;&gt; model = model.eval()
&gt;&gt;&gt; response, history = model.chat(tokenizer, &#34;hello&#34;, history=[])
&gt;&gt;&gt; print(response)
Hello! How can I help you todayï¼Ÿ
&gt;&gt;&gt; response, history = model.chat(tokenizer, &#34;please provide three suggestions about time management&#34;, history=history)
&gt;&gt;&gt; print(response)
Sure, here are three tips for effective time management:

1. Prioritize tasks based on importance and urgency: Make a list of all your tasks and categorize them into &#34;important and urgent,&#34; &#34;important but not urgent,&#34; and &#34;not important but urgent.&#34; Focus on completing the tasks in the first category before moving on to the others.
2. Use a calendar or planner: Write down deadlines and appointments in a calendar or planner so you don&#39;t forget them. This will also help you schedule your time more effectively and avoid overbooking yourself.
3. Minimize distractions: Try to eliminate any potential distractions when working on important tasks. Turn off notifications on your phone, close unnecessary tabs on your computer, and find a quiet place to work if possible.

Remember, good time management skills take practice and patience. Start with small steps and gradually incorporate these habits into your daily routine."><pre><span>&gt;&gt;</span><span>&gt;</span> <span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>, <span>AutoModelForCausalLM</span>
<span>&gt;&gt;</span><span>&gt;</span> <span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>&#34;internlm/internlm-chat-7b&#34;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>&gt;&gt;</span><span>&gt;</span> <span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>&#34;internlm/internlm-chat-7b&#34;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>).<span>cuda</span>()
<span>&gt;&gt;</span><span>&gt;</span> <span>model</span> <span>=</span> <span>model</span>.<span>eval</span>()
<span>&gt;&gt;</span><span>&gt;</span> <span>response</span>, <span>history</span> <span>=</span> <span>model</span>.<span>chat</span>(<span>tokenizer</span>, <span>&#34;hello&#34;</span>, <span>history</span><span>=</span>[])
<span>&gt;&gt;</span><span>&gt;</span> <span>print</span>(<span>response</span>)
<span>Hello</span>! <span>How</span> <span>can</span> <span>I</span> <span>help</span> <span>you</span> <span>today</span>ï¼Ÿ
<span>&gt;&gt;</span><span>&gt;</span> <span>response</span>, <span>history</span> <span>=</span> <span>model</span>.<span>chat</span>(<span>tokenizer</span>, <span>&#34;please provide three suggestions about time management&#34;</span>, <span>history</span><span>=</span><span>history</span>)
<span>&gt;&gt;</span><span>&gt;</span> <span>print</span>(<span>response</span>)
<span>Sure</span>, <span>here</span> <span>are</span> <span>three</span> <span>tips</span> <span>for</span> <span>effective</span> <span>time</span> <span>management</span>:

<span>1.</span> <span>Prioritize</span> <span>tasks</span> <span>based</span> <span>on</span> <span>importance</span> <span>and</span> <span>urgency</span>: <span>Make</span> <span>a</span> <span>list</span> <span>of</span> <span>all</span> <span>your</span> <span>tasks</span> <span>and</span> <span>categorize</span> <span>them</span> <span>into</span> <span>&#34;important and urgent,&#34;</span> <span>&#34;important but not urgent,&#34;</span> <span>and</span> <span>&#34;not important but urgent.&#34;</span> <span>Focus</span> <span>on</span> <span>completing</span> <span>the</span> <span>tasks</span> <span>in</span> <span>the</span> <span>first</span> <span>category</span> <span>before</span> <span>moving</span> <span>on</span> <span>to</span> <span>the</span> <span>others</span>.
<span>2.</span> <span>Use</span> <span>a</span> <span>calendar</span> <span>or</span> <span>planner</span>: <span>Write</span> <span>down</span> <span>deadlines</span> <span>and</span> <span>appointments</span> <span>in</span> <span>a</span> <span>calendar</span> <span>or</span> <span>planner</span> <span>so</span> <span>you</span> <span>don</span>&#39;<span>t</span> <span>forget</span> <span>them</span>. <span>This</span> <span>will</span> <span>also</span> <span>help</span> <span>you</span> <span>schedule</span> <span>your</span> <span>time</span> <span>more</span> <span>effectively</span> <span>and</span> <span>avoid</span> <span>overbooking</span> <span>yourself</span>.
<span>3.</span> <span>Minimize</span> <span>distractions</span>: <span>Try</span> <span>to</span> <span>eliminate</span> <span>any</span> <span>potential</span> <span>distractions</span> <span>when</span> <span>working</span> <span>on</span> <span>important</span> <span>tasks</span>. <span>Turn</span> <span>off</span> <span>notifications</span> <span>on</span> <span>your</span> <span>phone</span>, <span>close</span> <span>unnecessary</span> <span>tabs</span> <span>on</span> <span>your</span> <span>computer</span>, <span>and</span> <span>find</span> <span>a</span> <span>quiet</span> <span>place</span> <span>to</span> <span>work</span> <span>if</span> <span>possible</span>.

<span>Remember</span>, <span>good</span> <span>time</span> <span>management</span> <span>skills</span> <span>take</span> <span>practice</span> <span>and</span> <span>patience</span>. <span>Start</span> <span>with</span> <span>small</span> <span>steps</span> <span>and</span> <span>gradually</span> <span>incorporate</span> <span>these</span> <span>habits</span> <span>into</span> <span>your</span> <span>daily</span> <span>routine</span>.</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-dialogue" aria-hidden="true" href="#dialogue"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dialogue</h3>
<p dir="auto">You can interact with the InternLM Chat 7B model through a frontend interface by running the following code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install streamlit==1.24.0
pip install transformers==4.30.2
streamlit run web_demo.py"><pre>pip install streamlit==1.24.0
pip install transformers==4.30.2
streamlit run web_demo.py</pre></div>
<p dir="auto">The effect is as follows</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/9102141/251349183-11b60ee0-47e4-42c0-8278-3051b2f17fe4.jpg"><img src="https://user-images.githubusercontent.com/9102141/251349183-11b60ee0-47e4-42c0-8278-3051b2f17fe4.jpg" alt="demo"/></a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-deployment" aria-hidden="true" href="#deployment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Deployment</h3>
<p dir="auto">We use <a href="https://github.com/InternLM/LMDeploy">LMDeploy</a> to complete the one-click deployment of InternLM.</p>
<ol dir="auto">
<li>First, install LMDeploy:</li>
</ol>
<div data-snippet-clipboard-copy-content="python3 -m pip install lmdeploy"><pre><code>python3 -m pip install lmdeploy
</code></pre></div>
<ol start="2" dir="auto">
<li>Use the following command for quick deployment:</li>
</ol>
<div data-snippet-clipboard-copy-content="python3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf"><pre><code>python3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf
</code></pre></div>
<ol start="3" dir="auto">
<li>After exporting the model, you can start a server and have a conversation with the deployed model using the following command:</li>
</ol>
<div data-snippet-clipboard-copy-content="python3 -m lmdeploy.serve.client {server_ip_addresss}:33337"><pre><code>python3 -m lmdeploy.serve.client {server_ip_addresss}:33337
</code></pre></div>
<p dir="auto"><a href="https://github.com/InternLM/LMDeploy">LMDeploy</a> provides a complete workflow for deploying InternLM. Please refer to the <a href="https://github.com/InternLM/LMDeploy">deployment tutorial</a> for more details on deploying InternLM.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-fine-tuning--training" aria-hidden="true" href="#fine-tuning--training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fine-tuning &amp; Training</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-pre-training-and-fine-tuning-tutorial" aria-hidden="true" href="#pre-training-and-fine-tuning-tutorial"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pre-training and Fine-tuning Tutorial</h3>
<p dir="auto">Please refer to <a href="https://github.com/InternLM/InternLM/blob/main/doc/en/usage.md">Usage Tutorial</a> to start InternLM installation, data processing, pre-training and fine-tuning.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-convert-to-transformers-format" aria-hidden="true" href="#convert-to-transformers-format"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Convert to Transformers Format</h3>
<p dir="auto">The model trained by InternLM can be easily converted to HuggingFace Transformers format, which is convenient for seamless docking with various open source projects in the community. With the help of <code>tools/convert2hf.py</code>, the weights saved during training can be converted into transformers format with one command</p>
<div dir="auto" data-snippet-clipboard-copy-content="python convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer tokenizes/tokenizer.model"><pre>python convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer tokenizes/tokenizer.model</pre></div>
<p dir="auto">After conversion, it can be loaded as transformers by the following code</p>
<div dir="auto" data-snippet-clipboard-copy-content="&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
&gt;&gt;&gt; model = AutoModel.from_pretrained(&#34;hf_ckpt/&#34;, trust_remote_code=True).cuda()"><pre><span>&gt;&gt;</span><span>&gt;</span> <span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>, <span>AutoModel</span>
<span>&gt;&gt;</span><span>&gt;</span> <span>model</span> <span>=</span> <span>AutoModel</span>.<span>from_pretrained</span>(<span>&#34;hf_ckpt/&#34;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>).<span>cuda</span>()</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-training-system" aria-hidden="true" href="#training-system"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training System</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-system-architecture" aria-hidden="true" href="#system-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>System Architecture</h3>
<p dir="auto">Please refer to the <a href="https://github.com/InternLM/InternLM/blob/main/doc/en/structure.md">System Architecture document</a> for further details.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-training-performance" aria-hidden="true" href="#training-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training Performance</h3>
<p dir="auto">InternLM deeply integrates Flash-Attention, Apex and other high-performance model operators to improve training efficiency. By building the Hybrid Zero technique, it achieves efficient overlap of computation and communication, significantly reducing cross-node communication traffic during training. InternLM supports expanding the 7B model from 8 GPUs to 1024 GPUs, with an acceleration efficiency of up to 90% at the thousand-GPU scale, a training throughput of over 180 TFLOPS, and an average of over 3600 tokens per GPU per second. The following table shows InternLM&#39;s scalability test data at different configurations:</p>
<table>
<thead>
<tr>
<th>Number of GPUs</th>
<th>8</th>
<th>16</th>
<th>32</th>
<th>64</th>
<th>128</th>
<th>256</th>
<th>512</th>
<th>1024</th>
</tr>
</thead>
<tbody>
<tr>
<td>TGS</td>
<td>4078</td>
<td>3939</td>
<td>3919</td>
<td>3944</td>
<td>3928</td>
<td>3920</td>
<td>3835</td>
<td>3625</td>
</tr>
<tr>
<td>TFLOPS</td>
<td>192</td>
<td>192</td>
<td>186</td>
<td>186</td>
<td>185</td>
<td>185</td>
<td>186</td>
<td>182</td>
</tr>
</tbody>
</table>
<p dir="auto">TGS represents the average number of tokens processed per GPU per second. For more performance test data, please refer to the <a href="https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md">Training Performance document</a> for further details.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contribution" aria-hidden="true" href="#contribution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contribution</h2>
<p dir="auto">We appreciate all the contributors for their efforts to improve and enhance InternLM. Community users are highly encouraged to participate in the project. Please refer to the contribution guidelines for instructions on how to contribute to the project.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgements" aria-hidden="true" href="#acknowledgements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">InternLM codebase is an open-source project contributed by Shanghai AI Laboratory and researchers from different universities and companies. We would like to thank all the contributors for their support in adding new features to the project and the users for providing valuable feedback. We hope that this toolkit and benchmark can provide the community with flexible and efficient code tools for fine-tuning InternLM and developing their own models, thus continuously contributing to the open-source community. Special thanks to the two open-source projects, flash-attention (<a href="https://github.com/HazyResearch/flash-attention">https://github.com/HazyResearch/flash-attention</a>) and ColossalAI (<a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a>).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-open-source-license" aria-hidden="true" href="#open-source-license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Open Source License</h2>
<p dir="auto">The code in this repository is open-source under the Apache-2.0 license. The InternLM weights are fully open for academic research and also allow commercial use with written permission from the official team. For inquiries about commercial licenses and collaborations, please contact <a href="mailto:internlm@pjlab.org.cn">internlm@pjlab.org.cn</a>.</p>
</article>
          </div></div>
  </body>
</html>
