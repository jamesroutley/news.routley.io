<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/">Original</a>
    <h1>How DALL-E 2 Works</h1>
    
    <div id="readability-page-1" class="page"><div>
    <section>
                </section>
    <section>
      <p>OpenAI&#39;s groundbreaking model <a href="https://openai.com/dall-e-2/">DALL-E 2</a> hit the scene at the beginning of the month, setting a new bar for image generation and manipulation. With only short text prompt, DALL-E 2 can <strong>generate completely new images</strong> that combine distinct and unrelated objects in semantically plausible ways, like the images below which were generated by entering the prompt <strong>&#34;a bowl of soup that is a portal to another dimension as digital art&#34;</strong>.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/soup.png" alt="" loading="lazy" width="2000" height="1995" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/soup.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/04/soup.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/04/soup.png 1600w, https://www.assemblyai.com/blog/content/images/2022/04/soup.png 2073w" sizes="(min-width: 720px) 720px"/><figcaption>Various images generated by DALL-E 2 given the above prompt (<a href="https://openai.com/dall-e-2/">source</a>).</figcaption></figure><p>DALL-E 2 can even modify existing images, create variations of images that maintain their salient features, and interpolate between two input images. DALL-E 2&#39;s impressive results have many wondering exactly how such a powerful model works under the hood. </p><p>In this article, <strong>we will take an in-depth look at how DALL-E 2 manages to create such astounding images</strong> like those above. Plenty of background information will be given and the explanation levels will run the gamut, so this article is suitable for readers at several levels of Machine Learning experience. Let&#39;s dive in!</p><h2 id="how-dall-e-2-works-a-birds-eye-view">How DALL-E 2 Works: A Bird&#39;s-Eye View</h2><p>Before diving into the details of how DALL-E 2 works, let&#39;s orient ourselves with a high-level overview of how DALL-E 2 generates images. While DALL-E 2 can perform a variety of tasks, including image manipulation and interpolation as mentioned above, <strong>we will focus on the task of image generation</strong> in this article.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/diffusion.png" alt="" loading="lazy" width="2000" height="618" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/diffusion.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/04/diffusion.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/04/diffusion.png 1600w, https://www.assemblyai.com/blog/content/images/2022/04/diffusion.png 2150w" sizes="(min-width: 720px) 720px"/><figcaption>A birds-eye view of the DALL-E 2 image generation process (modified from <a href="https://arxiv.org/abs/2204.06125">source</a>).</figcaption></figure><p>At the highest level, DALL-E 2&#39;s works very simply:</p><ol><li>First, a text prompt is input into a <strong>text encoder</strong> that is trained to map the prompt to a representation space.</li><li>Next, a model called the <strong>prior</strong> maps the text encoding to a corresponding <strong>image</strong> <strong>encoding</strong> that captures the semantic information of the prompt contained in the text encoding.</li><li>Finally, an <strong>image decoding </strong>model stochastically generates an image which is a visual manifestation of this semantic information.</li></ol><p>From a bird&#39;s eye-view, that&#39;s all there is to it! Of course, there are plenty of interesting implementation specifics to discuss, which we will get into below. If you want a bit more detail without getting into the nitty-gritty, or you prefer to watch your content rather than read it, feel free to check out our video breakdown of DALL-E 2 here:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/F1X4fHzF4mQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><h2 id="how-dall-e-2-works-a-detailed-look">How DALL-E 2 Works: A Detailed Look</h2><p>Now it&#39;s time to dive into each of the above steps separately. Let&#39;s get started by looking at how DALL-E 2 learns to link related textual and visual abstractions.</p><h3 id="step-1linking-textual-and-visual-semantics">Step 1 - Linking Textual and Visual Semantics</h3><p>After inputting<strong> &#34;a teddy bear riding a skateboard in Times Square&#34;</strong>, DALL-E 2 outputs the following image:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/image-1.png" alt="" loading="lazy" width="683" height="672" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/image-1.png 600w, https://www.assemblyai.com/blog/content/images/2022/04/image-1.png 683w"/><figcaption><a href="https://openai.com/dall-e-2/">source</a></figcaption></figure><p>How does DALL-E 2 know how a textual concept like &#34;teddy bear&#34; is manifested in the visual space? The <strong>link between textual semantics and their visual representations</strong> in DALL-E 2 is learned by another OpenAI model called <strong>CLIP </strong>(<strong>C</strong>ontrastive <strong>L</strong>anguage-<strong>I</strong>mage <strong>P</strong>re-training).</p><p>CLIP is trained on hundreds of millions of images and their associated captions, learning<em> how much</em><strong> </strong>a given text snippet relates to an image. That is, rather than trying to <em>predict</em> a caption given an image, CLIP instead just learns how <em>related</em> any given caption is to an image. This <strong>contrastive</strong> rather than <strong>predictive</strong> objective allows CLIP to learn the link between textual and visual representations of the same abstract object. The entire DALL-E 2 model hinges on CLIP&#39;s ability to learn semantics from natural language, so let&#39;s take a look at how CLIP is trained to understand its inner workings.</p><!--kg-card-begin: html--><p><h4>CLIP Training</h4></p><!--kg-card-end: html--><p>The fundamental principles of training CLIP are quite simple:</p><ol><li>First, all images and their associated captions are passed through their respective encoders, mapping all objects into an <em>m-</em>dimensional space.</li><li>Then, the cosine similarity of each <em>(image, text)</em> pair is computed.</li><li>The training objective is to simultaneously <strong>maximize the cosine similarity </strong>between N<strong> correct </strong>encoded image/caption<strong> </strong>pairs and <strong>minimize the cosine similarity </strong>between N<sup>2</sup> - N<strong> incorrect </strong>encoded image/caption pairs.</li></ol><p>This training process is visualized below:</p><figure><div><video src="https://www.assemblyai.com/blog/content/media/2022/04/CLIP_training-1.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video><div><div><p><span>0:00</span></p><p>/<span></span></p></div></div></div><figcaption>Overview of the CLIP training process</figcaption></figure><!--kg-card-begin: html--><div data-template="accordion">
    <header>
        <p>Additional Training Details</p>
    </header>
    <div>
        <p>More information about the CLIP training process can be found below.</p>
        <ul>
            <li> <b>Cosine Similarity</b>
                <ul>
                    <li>The Cosine Similarity of two vectors is simply the <strong>dot product of two vectors scaled by the product of their magnitudes</strong>. It measures the angle between two vectors in a vector space; and, in the context of Machine Learning, determines how &#34;similar&#34; two vectors are to each other. If we consider each &#34;direction&#34; in the vector space as having a meaning, then the cosine similarity between two encoded vectors measures how &#34;alike&#34; the concepts represented by the vectors. </li>
                </ul>
            </li>
            <li> <b>Training Data</b>
                <ul>
                    <li>CLIP is trained on the WebImageText dataset, which is composed of 400 million pairs of images and their corresponding natural language captions (not to be confused with <a href="https://github.com/google-research-datasets/wit">Wikipedia-based Image Text</a>)</li>
                </ul>
            </li>
            <li> <b>Parallelizability</b>
                <ul>
                    <li>The parallelizability of CLIP&#39;s training process is immediately evident - all of the encodings and cosine similarities can be computing in parallel.</li>
                </ul>
            </li>
            <li> <b>Text Encoder Architecture</b>
                <ul>
                    <li>The text encoder is a <a href="https://arxiv.org/abs/1706.03762">Transformer</a></li>
                </ul>
            </li>
            <li> <b>Image Encoder Architecture</b>
                <ul>
                    <li>The image encoder is a <a href="https://arxiv.org/abs/2010.11929">Vision Transformer</a></li>
                </ul>
            </li>            
        </ul>
    </div>
</div><!--kg-card-end: html--><!--kg-card-begin: html--><p><h4>Significance of CLIP to DALL-E 2</h4></p><!--kg-card-end: html--><p>CLIP is important to DALL-E 2 because <strong>it is what ultimately determines how semantically-related </strong>a natural language snippet is to a visual concept, which is critical for <em>text-conditional</em> image generation.</p><!--kg-card-begin: html--><div data-template="accordion">
    <header>
        <p>Additional Information</p>
    </header>
    <div>
        <p>CLIP&#39;s contrastive objective allows it to understand semantic information in a way that convolution models that learn only feature maps cannot. This disparity can easily be observed by contrasting how CLIP, used in a zero-shot manner, performs <i>across datasets</i> relative to an ImageNet-trained ResNet-101. In particular, contrasting how these models compare on <a href="https://image-net.org/">ImageNet</a> vs <a href="https://github.com/HaohanWang/ImageNet-Sketch">ImageNet Sketch</a> reveals this disparity well.<br/></p>
        <figure><img src="https://github.com/AssemblyAI-Examples/How-DALL-E-2-Works/raw/main/imn.png"/>
            
            <img src="https://github.com/AssemblyAI-Examples/How-DALL-E-2-Works/raw/main/CLIP%20vs%20ResNet-101.png"/>
            
            </figure></div>
    </div><!--kg-card-end: html--><h3 id="step-2generating-images-from-visual-semantics">Step 2 - Generating Images from Visual Semantics</h3><p>After training, the CLIP model is frozen and DALL-E 2 moves onto its next task - learning to <em>reverse </em>the image encoding mapping that CLIP just learned. CLIP learns a representation space in which it is easy to determine the relatedness of textual and visual encodings, but our interest is in image <strong>generation</strong>. We must therefore learn how to exploit the representation space to accomplish this task. </p><p>In particular, OpenAI employs a modified version of another one of its previous models, <a href="https://arxiv.org/abs/2112.10741">GLIDE</a>, to perform this image generation. The GLIDE model learns to <em>invert </em>the image encoding process in order to stochastically decode CLIP image embeddings.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/CLIP_to_GLIDE-1.png" alt="" loading="lazy" width="1584" height="465" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/CLIP_to_GLIDE-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/04/CLIP_to_GLIDE-1.png 1000w, https://www.assemblyai.com/blog/content/images/2022/04/CLIP_to_GLIDE-1.png 1584w" sizes="(min-width: 720px) 720px"/><figcaption>An image of a Corgi playing a flamethrowing trumpet passed through CLIP&#39;s image encoder. GLIDE then uses this encoding to generate a new image that maintains the salient features of the original. (modified from <a href="https://arxiv.org/abs/2204.06125">source</a>)</figcaption></figure><p>As depicted in the image above, it should be noted that the goal is <strong>not</strong> to build an autoencoder and <em>exactly</em> reconstruct an image given its embedding, but to instead generate an image which <strong>maintains the salient features of the original image </strong>given its embedding. In order perform this image generation, GLIDE uses a <strong>Diffusion Model</strong>.</p><h4 id="what-is-a-diffusion-model">What is a Diffusion Model?</h4><p>Diffusion Models are a thermodynamics-inspired invention that have significantly grown in popularity in recent years<sup>[<a href="https://arxiv.org/abs/1503.03585">1</a>][<a href="https://arxiv.org/abs/1907.05600">2</a>]</sup>, which learn to generate data by <em>reversing a gradual noising process</em>. Depicted in the figure below, the noising process is viewed as a parameterized Markov chain that gradually adds noise to an image to corrupt it, eventually (asymptotically) resulting in pure Gaussian noise. The Diffusion Model learns to navigate backwards along this chain, gradually removing the noise over a series of timesteps to reverse this process. </p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/image-3.png" alt="" loading="lazy" width="1762" height="282" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/image-3.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/04/image-3.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/04/image-3.png 1600w, https://www.assemblyai.com/blog/content/images/2022/04/image-3.png 1762w" sizes="(min-width: 720px) 720px"/><figcaption>Diffusion Model schematic (<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>).</figcaption></figure><p>If the Diffusion Model is then &#34;cut in half&#34; after training, it can be used to <em>generate</em> an image by randomly sampling Gaussian noise and then de-noising it to generate a photorealistic image. Some may recognize that this technique is highly reminiscent of generating data with <a href="https://www.assemblyai.com/blog/introduction-to-variational-autoencoders-using-keras/">Autoencoders</a>, and Diffusion Models and Autoencoders are, in fact, <a href="https://benanne.github.io/2022/01/31/diffusion.html">related</a>.</p><!--kg-card-begin: html--><p><h4>GLIDE Training</h4></p><!--kg-card-end: html--><p>While GLIDE was not the first Diffusion Model, its important contribution was in modifying them to allow for <strong>text-conditional image generation</strong>. In particular, one will notice that Diffusion Models <em>start</em> from randomly sampled Gaussian noise. It at first unclear how to tailor this process to generate <em>specific</em> images. If a Diffusion Model is trained on a human face dataset, it will reliably generate photorealistic images of human faces; but what if someone wants to generate a face with a <em>specific</em> feature, like brown eyes or blonde hair?</p><p>GLIDE extends the core concept of Diffusion Models by <strong>augmenting the training process with additional textual information</strong>, ultimately resulting in text-conditional image generation. Let&#39;s take a look at the training process for GLIDE:</p><figure><div><video src="https://www.assemblyai.com/blog/content/media/2022/04/openshot_project.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video><div><div><p><span>0:00</span></p><p>/<span></span></p></div></div></div><figcaption>GLIDE training process.</figcaption></figure><!--kg-card-begin: html--><div data-template="accordion">
    <header>
        <p>Additional Training Details</p>
    </header>
    <div>
        <p>More information about the GLIDE training process can be found below.</p><ul>
            <li> <strong>Upsampling</strong>
                <ul>
                    <li>The images generated by the reverse-Diffusion process are 64 x 64, and so the authors also train upsampling models textually-conditioned in a similar way in order to bring the generated data up to 1,024 x 1,024.</li>
                </ul>
            </li>
            <li> <strong>Ablated Diffusion Model</strong>
                <ul>
                    <li>OpenAI first addressed this problem with its <a href="https://arxiv.org/abs/2105.05233">Ablated Diffusion Model</a> (ADM), which originally just included class-conditioning. OpenAI extended this concept with GLIDE to <strong>generalize the Diffusion Model conditioning to include general natural language</strong>.</li>
                    <li>ADM was originally created in order to combine the ability of Diffusion Models to generate photorealistic images with the ability of text conditional models to amalgamate unrelated objects in semantically plausible ways.</li>
                    <li>Also included in the ADM paper was an ablation study to explore the topic of optimizing Diffusion Model architectures (hence <strong>Ablated</strong> Diffusion Model). The details of this exploration are outside of the purview of this article, but interested readers should reference the linked paper for more details.</li>
                </ul>
            </li>
        </ul>
    </div>
</div><!--kg-card-end: html--><p>Here are some examples of images generated with GLIDE. The authors note that GLIDE performs better than DALL-E (1) for photorealism and caption similarity.</p><figure><img src="https://lh6.googleusercontent.com/cXN1oajLqcKulckAtNBQocL-SCAs_7fXwze-45Jce2g3CIBeY3n429vTAP50ldgXNwuPAc9jDwb_tRSlDYKtSZUSANgv3JoCFbDjoeakEkN10UR7J9QayCE_9JRexjoblXVrarV9" alt="" loading="lazy"/><figcaption>Examples of images generated by GLIDE (<a href="https://arxiv.org/pdf/2112.10741.pdf">source</a>).</figcaption></figure><p>DALL-E 2 uses a modified GLIDE model uses projected CLIP text embeddings in two ways. The first is by adding them to GLIDE&#39;s existing timestep embedding, and the second is by creating four extra tokens of context, which are concatenated to the output sequence of the GLIDE text encoder. </p><!--kg-card-begin: html--><p><h4>Significance of GLIDE to DALL-E 2</h4></p><!--kg-card-end: html--><p>GLIDE is important to DALL-E 2 because it allowed the authors to easily port over GLIDE&#39;s text-conditional photorealistic image generation capabilities to DALL-E 2 by instead conditioning on <strong>image encodings </strong>in the representation space.  Therefore, DALL-E 2&#39;s modified GLIDE learns to <strong>generate semantically consistent images conditioned on CLIP image encodings</strong>. It is also important to note that the reverse-Diffusion process is stochastic, and therefore variations can easily be generated by inputting the <em>same</em> image encoding vectors through the modified GLIDE model multiple times. </p><h3 id="step-3mapping-from-textual-semantics-to-corresponding-visual-semantics">Step 3 - Mapping from Textual Semantics to Corresponding Visual Semantics</h3><p>While the modified-GLIDE model successfully generates images that reflect the semantics captured by image encodings, how do we go about actually go about <em>finding </em>these encoded representations? In other words, how do we go about injecting the text conditioning information from our prompt into the image generation process?</p><p>Recall that, in addition to our <em>image</em> encoder, CLIP also learns a <em>text</em> encoder. DALL-E 2 uses another model, which the authors call the <strong>prior</strong>, in order to map <strong>from the text encodings </strong>of image captions<strong> to the</strong> <strong>image encodings </strong>of their corresponding images. The DALL-E 2 authors experiment with both Autoregressive Models and Diffusion Models for the prior, but ultimately find that they yield comparable performance. Given that the Diffusion Model is much more computationally efficient, and it is therefore selected as the prior for DALL-E 2.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/text_to_image_encoding_2.png" alt="" loading="lazy" width="856" height="478" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/text_to_image_encoding_2.png 600w, https://www.assemblyai.com/blog/content/images/2022/04/text_to_image_encoding_2.png 856w" sizes="(min-width: 720px) 720px"/><figcaption>Prior mapping from a text encoding to its corresponding image encoding (modified from <a href="https://arxiv.org/abs/2204.06125">source</a>).</figcaption></figure><!--kg-card-begin: html--><p><h4>Prior Training</h4></p><!--kg-card-end: html--><p>The Diffusion Prior in DALL-E 2 consists of a decoder-only Transformer. It operates, with a causal attention mask, on an ordered sequence of</p><ol><li>The tokenized text/caption.</li><li>The CLIP text encodings of these tokens.</li><li>An encoding for the diffusion timestep.</li><li>The noised image passed through the CLIP image encoder.</li><li>Final encoding whose output from Transformer is used to predict the unnoised CLIP image encoding. </li></ol><!--kg-card-begin: html--><div data-template="accordion">
    <header>
        <p>Additional Training Details</p>
    </header>
    <div>
        <p>More information about the Prior training process can be found below.</p><ul>
            <li> <strong>Conditioning on the Caption</strong>
                <ul>
                    <li>The Diffusion Prior is conditioned not only on the CLIP text embedding of the caption, but also the caption itself. The former is a deterministic function of the latter and this dual-conditioning is therefore fully permissible.</li>
                </ul>
            </li>
            <li> <strong>Classifier-Free Guidance</strong>
                <ul>
                    <li>To improve sample quality, sampling is randomly conducted using classifier-free guidance 10% of the time by dropping the text-conditioning information.</li>
                </ul>
            </li>
            <li> <strong>Double Sample Generation</strong>
                <ul>
                    <li>To improve quality during sampling time, two image embeddings are generated with the prior and the one with the higher dot product with the text embedding is selected. It is unclear why the authors use the dot product here as opposed to the cosine similarity.</li>
                </ul>
            </li>
            <li> <strong>Why do we need the prior?</strong>
                <ul>
                    <li>The authors note that training such a prior is not strictly necessary for a caption-to-image model. One option would be to condition only on the caption itself. This would simply yield the model GLIDE, and the authors perform a thorough analysis comparing the two in the paper. Another option would be to feed into the decoder the CLIP text embedding, rather than using the prior to generate a CLIP image embedding from it and then use that. The authors found experimentally that the former produces reasonable results, although results not as good as those of the latter. Ultimately, using the prior <strong>improves image diversity</strong>.</li>
                </ul>
            </li>
        </ul>
    </div>
</div><!--kg-card-end: html--><h3 id="step-4putting-it-all-together">Step 4 - Putting It All Together</h3><p>At this point, we have all of DALL-E 2&#39;s functional components and need only to chain them together for text-conditional image generation:</p><ol><li>First the CLIP text encoder maps the image description into the <strong>representation space</strong>.</li><li>Then the diffusion prior maps from the CLIP text encoding to a <strong>corresponding CLIP image encoding</strong>. </li><li>Finally, the modified-GLIDE generation model maps from the representation space into the image space via reverse-Diffusion, <strong>generating one of many possible images that conveys the semantic information</strong> within the input caption.</li></ol><figure><img src="https://www.assemblyai.com/blog/content/images/2022/04/diffusion.png" alt="" loading="lazy" width="2000" height="618" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/04/diffusion.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/04/diffusion.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/04/diffusion.png 1600w, https://www.assemblyai.com/blog/content/images/2022/04/diffusion.png 2150w" sizes="(min-width: 720px) 720px"/><figcaption>High-level overview of the DALL-E 2 image-generation process (modified from <a href="https://arxiv.org/abs/2204.06125">source</a>).</figcaption></figure><h2 id="summary">Summary</h2><p>In this article we covered how the world&#39;s premier textually-conditioned image generation model works under the hood. DALL-E 2 can generate semantically plausible photorealistic images given a text prompt, can produce images with specific artistic styles, can produce variations of the same salient features represented in different ways, and can modify existing images. </p><p>While there is a lot of discussion to be had about DALL-E 2 and its importance to both Deep Learning and the world at large, we draw your attention to 3<strong> key takeaways</strong> from the development of DALL-E 2</p><ol><li>First, DALL-E 2 demonstrates the <strong>power of Diffusion Models</strong> in Deep Learning, with both the prior <em>and </em>image generation sub-models in DALL-E 2 being Diffusion-based. While only rising to popular use in the past few years, Diffusion Models have already proven their worth, and those tuned-in to Deep Learning research should expect to see more of them in the future.</li><li>The second point is to highlight both the need and <strong>power of using natural language as a means to train</strong> <strong>State-of-the-Art Deep Learning models</strong>. This point does not originate with DALL-E 2 (in particular, CLIP demonstrated it previously), but nevertheless it is important to appreciate that the power of DALL-E 2 stems ultimately from the absolutely <em>massive</em> amount of paired natural language/image data that is available on the internet. Using such data not only removes the developmental bottleneck associated with the laborious and painstaking process of manually labelling datasets; but the noisy, uncurated nature of such data better reflects real-world data that Deep Learning models must be robust to.</li><li>Finally, DALL-E 2 <strong>reaffirms the position of Transformers</strong> as supreme for models trained on web-scale datasets given their impressive parallelizability.</li></ol><h2 id="references">References</h2><ol><li><a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a></li><li><a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a></li><li><a href="https://arxiv.org/pdf/2204.06125.pdf">Hierarchical Text-Conditional Image Generation with CLIP Latents</a></li><li><a href="https://arxiv.org/abs/2105.05233">Diffusion Models Beat GANs on Image Synthesis</a></li><li><a href="https://arxiv.org/pdf/2006.11239.pdf">Denoising Diffusion Probabilistic Models</a></li><li><a href="https://arxiv.org/pdf/2103.00020.pdf">Learning Transferable Visual Models From Natural Language Supervision</a></li><li><a href="https://arxiv.org/pdf/2112.10741.pdf">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></li></ol>
    </section>
  </div></div>
  </body>
</html>
