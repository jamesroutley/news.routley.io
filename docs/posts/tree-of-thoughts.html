<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/kyegomez/tree-of-thoughts">Original</a>
    <h1>Tree of Thoughts</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kyegomez/tree-of-thoughts/blob/main/tree-of-thoughts.jpeg"><img src="https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg" alt="tree of thoughts banner"/></a></p>
<p dir="auto"><a href="https://arxiv.org/pdf/2305.10601.pdf" rel="nofollow">Paper link</a></p>
<p dir="auto">Tree of Thoughts (ToT) is an all-new powerful and flexible algorithm that advances model reasoning by a whopping 70%. This is an plug in and play verision, connect your own models and enjoy superintelligence!</p>
<p dir="auto">Share this repository by clicking on the following buttons <g-emoji alias="blush" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png">😊</g-emoji></p>
<p dir="auto"><a href="https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts" rel="nofollow"><img src="https://camo.githubusercontent.com/a474432345fd9fec72e2d5d55c5e836d0c6b765f39cefcc0379fb398f285ec24/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c3f7374796c653d736f6369616c2675726c3d68747470732533412532462532466769746875622e636f6d2532466b7965676f6d657a253246747265652d6f662d74686f7567687473" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url?style=social&amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts"/></a>
<a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts" rel="nofollow"><img src="https://camo.githubusercontent.com/d81b0b7b3b5d0527c1fdb39af296815321fb18d90e3c3cf7bb11a6f779468437/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53686172652d4c696e6b6564496e2d626c75653f7374796c653d736f6369616c266c6f676f3d6c696e6b6564696e" alt="LinkedIn" data-canonical-src="https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;logo=linkedin"/></a></p>

<p dir="auto">This implementation of Tree of Thoughts is brought to you by Agora, Agora advances Humanity with open source SOTA Multi-Modality AI research! We plan on combating Humanity&#39;s grandest root problems like food insecurity, planetary insecurity, and disease, and hopefully death itself.</p>
<p dir="auto"><a href="https://discord.gg/qUtxnK2NMf" rel="nofollow">Join our Discord and contribute to this project</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting started</h2>
<p dir="auto">Clone this repository with <code>git clone https://github.com/kyegomez/tree-of-thoughts</code></p>
<p dir="auto">or:</p>
<p dir="auto"><code>pip install tree-of-thoughts </code></p>
<p dir="auto">Navigate to the repository folder: <code>cd tree-of-thoughts</code></p>
<p dir="auto"><code>pip install openai</code></p>
<p dir="auto">Create a Python script (e.g., example.py) and import the necessary classes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from tree_of_thoughts.treeofthoughts import OpenAILanguageModel, CustomLanguageModel, TreeofThoughts, OptimizedOpenAILanguageModel, OptimizedTreeofThoughts, HuggingLanguageModel

use_v2 = False

api_key=&#34;&#34;

api_base= &#34;&#34; # leave it blank if you simply use default openai api url

if not use_v2:
    #v1
    model = OpenAILanguageModel(api_key=api_key, api_base=api_base # api_model=&#34;gpt4&#34; # for higher performance base model is not smart
    ) 
else:
    #v2 parallel execution, caching, adaptive temperature
    model = OptimizedOpenAILanguageModel(api_key=api_key, api_base=api_base, # api_model=&#34;gpt4&#34; # for higher performance base model is not smart
    )

#huggingface
# model_name=&#34;gpt2&#34;
# model = HuggingLanguageModel(model_name)


#choose search algorithm(&#39;BFS&#39; or &#39;DFS&#39;)
search_algorithm = &#34;BFS&#34;

#cot or propose
strategy=&#34;cot&#34;

# value or vote
evaluation_strategy = &#34;value&#34;

# if not use_v2:
#     #create an instance of the tree of thoughts class v1


tree_of_thoughts = TreeofThoughts(model, search_algorithm)


# else:
#     #or v2 -&gt; dynamic beam width -&lt; adjust the beam width [b] dynamically based on the search depth quality of the generated thoughts # does not work now use regular tree of thoughts
#     tree_of_thoughts= OptimizedTreeofThoughts(model, search_algorithm)

input_problem = &#34;use 4 numbers and basic arithmetic operations (+-*/) to obtain 24&#34; #note for super intelligent responses you&#39;ll have to be more explicit in your prompt and select a better model
    


input_problem = &#34;What are the best reasoning methods to advance Large Language Models&#34;
k = 5 #number of thoughts to generate
T = 3 # maximum depth of the search tree
b = 5 # branching factor -&lt; number of child nodes for each branch
vth = 0.5 # pruning state -&gt; any evaluated thought below this is eliminated
timeout = 10 #10 seconds timeout before stop
confidence = 0.8 #cmodel is confident on performance
max_iterations = 40 #tree branh nodes 
convergence_threshold = 0.01 #determining when the search process has converged
convergence_count = 5 # number of searchers to be considered converged
#read documentation for more

#call the solve emthod with the input problem and other params
solution = tree_of_thoughts.solve(input_problem, k, T, b, vth, timeout, confidence, max_iterations, convergence_threshold, convergence_count)

    


# # Save the tree and metrics to a JSON file
# file_name = &#34;logs/tree_of_thoughts_output.json&#34;
# tree_of_thoughts.save_tree_to_json(file_name)
"><pre><span>from</span> <span>tree_of_thoughts</span>.<span>treeofthoughts</span> <span>import</span> <span>OpenAILanguageModel</span>, <span>CustomLanguageModel</span>, <span>TreeofThoughts</span>, <span>OptimizedOpenAILanguageModel</span>, <span>OptimizedTreeofThoughts</span>, <span>HuggingLanguageModel</span>

<span>use_v2</span> <span>=</span> <span>False</span>

<span>api_key</span><span>=</span><span>&#34;&#34;</span>

<span>api_base</span><span>=</span> <span>&#34;&#34;</span> <span># leave it blank if you simply use default openai api url</span>

<span>if</span> <span>not</span> <span>use_v2</span>:
    <span>#v1</span>
    <span>model</span> <span>=</span> <span>OpenAILanguageModel</span>(<span>api_key</span><span>=</span><span>api_key</span>, <span>api_base</span><span>=</span><span>api_base</span> <span># api_model=&#34;gpt4&#34; # for higher performance base model is not smart</span>
    ) 
<span>else</span>:
    <span>#v2 parallel execution, caching, adaptive temperature</span>
    <span>model</span> <span>=</span> <span>OptimizedOpenAILanguageModel</span>(<span>api_key</span><span>=</span><span>api_key</span>, <span>api_base</span><span>=</span><span>api_base</span>, <span># api_model=&#34;gpt4&#34; # for higher performance base model is not smart</span>
    )

<span>#huggingface</span>
<span># model_name=&#34;gpt2&#34;</span>
<span># model = HuggingLanguageModel(model_name)</span>


<span>#choose search algorithm(&#39;BFS&#39; or &#39;DFS&#39;)</span>
<span>search_algorithm</span> <span>=</span> <span>&#34;BFS&#34;</span>

<span>#cot or propose</span>
<span>strategy</span><span>=</span><span>&#34;cot&#34;</span>

<span># value or vote</span>
<span>evaluation_strategy</span> <span>=</span> <span>&#34;value&#34;</span>

<span># if not use_v2:</span>
<span>#     #create an instance of the tree of thoughts class v1</span>


<span>tree_of_thoughts</span> <span>=</span> <span>TreeofThoughts</span>(<span>model</span>, <span>search_algorithm</span>)


<span># else:</span>
<span>#     #or v2 -&gt; dynamic beam width -&lt; adjust the beam width [b] dynamically based on the search depth quality of the generated thoughts # does not work now use regular tree of thoughts</span>
<span>#     tree_of_thoughts= OptimizedTreeofThoughts(model, search_algorithm)</span>

<span>input_problem</span> <span>=</span> <span>&#34;use 4 numbers and basic arithmetic operations (+-*/) to obtain 24&#34;</span> <span>#note for super intelligent responses you&#39;ll have to be more explicit in your prompt and select a better model</span>
    


<span>input_problem</span> <span>=</span> <span>&#34;What are the best reasoning methods to advance Large Language Models&#34;</span>
<span>k</span> <span>=</span> <span>5</span> <span>#number of thoughts to generate</span>
<span>T</span> <span>=</span> <span>3</span> <span># maximum depth of the search tree</span>
<span>b</span> <span>=</span> <span>5</span> <span># branching factor -&lt; number of child nodes for each branch</span>
<span>vth</span> <span>=</span> <span>0.5</span> <span># pruning state -&gt; any evaluated thought below this is eliminated</span>
<span>timeout</span> <span>=</span> <span>10</span> <span>#10 seconds timeout before stop</span>
<span>confidence</span> <span>=</span> <span>0.8</span> <span>#cmodel is confident on performance</span>
<span>max_iterations</span> <span>=</span> <span>40</span> <span>#tree branh nodes </span>
<span>convergence_threshold</span> <span>=</span> <span>0.01</span> <span>#determining when the search process has converged</span>
<span>convergence_count</span> <span>=</span> <span>5</span> <span># number of searchers to be considered converged</span>
<span>#read documentation for more</span>

<span>#call the solve emthod with the input problem and other params</span>
<span>solution</span> <span>=</span> <span>tree_of_thoughts</span>.<span>solve</span>(<span>input_problem</span>, <span>k</span>, <span>T</span>, <span>b</span>, <span>vth</span>, <span>timeout</span>, <span>confidence</span>, <span>max_iterations</span>, <span>convergence_threshold</span>, <span>convergence_count</span>)

    


<span># # Save the tree and metrics to a JSON file</span>
<span># file_name = &#34;logs/tree_of_thoughts_output.json&#34;</span>
<span># tree_of_thoughts.save_tree_to_json(file_name)</span></pre></div>
<p dir="auto">Or Integrate your own custom language model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="
class CustomLanguageModel(AbstractLanguageModel):
    def __init__(self, model):
        self.model = model

    def generate_thoughts(self, state, k):
        #implement the thought generation logic using self.model
        pass

    def evaluate_states(self, states):
        #implement state evaluation logic using self.model
        pass
"><pre><span>class</span> <span>CustomLanguageModel</span>(<span>AbstractLanguageModel</span>):
    <span>def</span> <span>__init__</span>(<span>self</span>, <span>model</span>):
        <span>self</span>.<span>model</span> <span>=</span> <span>model</span>

    <span>def</span> <span>generate_thoughts</span>(<span>self</span>, <span>state</span>, <span>k</span>):
        <span>#implement the thought generation logic using self.model</span>
        <span>pass</span>

    <span>def</span> <span>evaluate_states</span>(<span>self</span>, <span>states</span>):
        <span>#implement state evaluation logic using self.model</span>
        <span>pass</span></pre></div>
<p dir="auto">Run the example script</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--features" aria-hidden="true" href="#-features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png">🌟</g-emoji> Features:</h2>
<ul dir="auto">
<li>General problem-solving framework for language models</li>
<li>Supports both breadth-first search (BFS) and depth-first search (DFS) algorithms</li>
<li>Easy integration with popular language models like OpenAI and Hugging Face</li>
<li>Extensible and adaptable to different problem properties and resource constraints</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-algorithmic-pseudocode" aria-hidden="true" href="#algorithmic-pseudocode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Algorithmic Pseudocode</h2>
<ol dir="auto">
<li>Define the thought decomposition based on the problem properties.</li>
<li>Create a thought generator function G(pθ, s, k) with two strategies:
a. Sample i.i.d. thoughts from a CoT prompt.
b. Propose thoughts sequentially using a &#34;propose prompt&#34;.</li>
<li>Create a state evaluator function V(pθ, S) with two strategies:
a. Value each state independently.
b. Vote across states.</li>
<li>Choose a search algorithm (BFS or DFS) based on the tree structure.</li>
<li>Implement the chosen search algorithm.</li>
<li>Execute the chosen search algorithm with the input problem, thought generator, state evaluator, and other required parameters.</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-tree-of-thoughts-class" aria-hidden="true" href="#tree-of-thoughts-class"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tree of Thoughts Class</h2>
<div dir="auto" data-snippet-clipboard-copy-content="class TreeofThoughts:
    
    def __init__(self, model, search_algorithm):
        self.model = model
        self.search_algorithm = search_algorithm

    def solve(self, x, k, T, b, vth):
        if self.search_algorithm == &#39;BFS&#39;:
            return self.tot_bfs(x, k, T, b)
        elif self.search_algorithm == &#39;DFS&#39;:
            return self.tot_dfs(x, k, T, vth)
        else:
            raise ValueError(&#34;Invalid search algorithm. Choose &#39;BFS&#39; or &#39;DFS&#39;.&#34;)

    def tot_bfs(self, x, k, T, b):
        S0 = {x}
        for t in range(1, T + 1):
            S0_t = {(*s, z) for s in S0 for z in self.model.generate_thoughts(s, k)}
            Vt = self.model.evaluate_states(S0_t)
            St = sorted(S0_t, key=lambda s: Vt[s], reverse=True)[:b]
            S0 = set(St)
        return self.model.generate_thoughts(max(St, key=lambda s: Vt[s]), 1)

    def tot_dfs(self, x, k, T, vth):
        output = []

        def dfs(s, t):
            if t &gt; T:
                output.append(self.model.generate_thoughts(s, 1))
                return
            for s_prime in sorted(self.model.generate_thoughts(s, k)):
                if self.model.evaluate_states({s_prime})[s_prime] &gt; vth:
                    dfs((*s, s_prime), t + 1)

        dfs(x, 1)
        return output
    "><pre><span>class</span> <span>TreeofThoughts</span>:
    
    <span>def</span> <span>__init__</span>(<span>self</span>, <span>model</span>, <span>search_algorithm</span>):
        <span>self</span>.<span>model</span> <span>=</span> <span>model</span>
        <span>self</span>.<span>search_algorithm</span> <span>=</span> <span>search_algorithm</span>

    <span>def</span> <span>solve</span>(<span>self</span>, <span>x</span>, <span>k</span>, <span>T</span>, <span>b</span>, <span>vth</span>):
        <span>if</span> <span>self</span>.<span>search_algorithm</span> <span>==</span> <span>&#39;BFS&#39;</span>:
            <span>return</span> <span>self</span>.<span>tot_bfs</span>(<span>x</span>, <span>k</span>, <span>T</span>, <span>b</span>)
        <span>elif</span> <span>self</span>.<span>search_algorithm</span> <span>==</span> <span>&#39;DFS&#39;</span>:
            <span>return</span> <span>self</span>.<span>tot_dfs</span>(<span>x</span>, <span>k</span>, <span>T</span>, <span>vth</span>)
        <span>else</span>:
            <span>raise</span> <span>ValueError</span>(<span>&#34;Invalid search algorithm. Choose &#39;BFS&#39; or &#39;DFS&#39;.&#34;</span>)

    <span>def</span> <span>tot_bfs</span>(<span>self</span>, <span>x</span>, <span>k</span>, <span>T</span>, <span>b</span>):
        <span>S0</span> <span>=</span> {<span>x</span>}
        <span>for</span> <span>t</span> <span>in</span> <span>range</span>(<span>1</span>, <span>T</span> <span>+</span> <span>1</span>):
            <span>S0_t</span> <span>=</span> {(<span>*</span><span>s</span>, <span>z</span>) <span>for</span> <span>s</span> <span>in</span> <span>S0</span> <span>for</span> <span>z</span> <span>in</span> <span>self</span>.<span>model</span>.<span>generate_thoughts</span>(<span>s</span>, <span>k</span>)}
            <span>Vt</span> <span>=</span> <span>self</span>.<span>model</span>.<span>evaluate_states</span>(<span>S0_t</span>)
            <span>St</span> <span>=</span> <span>sorted</span>(<span>S0_t</span>, <span>key</span><span>=</span><span>lambda</span> <span>s</span>: <span>Vt</span>[<span>s</span>], <span>reverse</span><span>=</span><span>True</span>)[:<span>b</span>]
            <span>S0</span> <span>=</span> <span>set</span>(<span>St</span>)
        <span>return</span> <span>self</span>.<span>model</span>.<span>generate_thoughts</span>(<span>max</span>(<span>St</span>, <span>key</span><span>=</span><span>lambda</span> <span>s</span>: <span>Vt</span>[<span>s</span>]), <span>1</span>)

    <span>def</span> <span>tot_dfs</span>(<span>self</span>, <span>x</span>, <span>k</span>, <span>T</span>, <span>vth</span>):
        <span>output</span> <span>=</span> []

        <span>def</span> <span>dfs</span>(<span>s</span>, <span>t</span>):
            <span>if</span> <span>t</span> <span>&gt;</span> <span>T</span>:
                <span>output</span>.<span>append</span>(<span>self</span>.<span>model</span>.<span>generate_thoughts</span>(<span>s</span>, <span>1</span>))
                <span>return</span>
            <span>for</span> <span>s_prime</span> <span>in</span> <span>sorted</span>(<span>self</span>.<span>model</span>.<span>generate_thoughts</span>(<span>s</span>, <span>k</span>)):
                <span>if</span> <span>self</span>.<span>model</span>.<span>evaluate_states</span>({<span>s_prime</span>})[<span>s_prime</span>] <span>&gt;</span> <span>vth</span>:
                    <span>dfs</span>((<span>*</span><span>s</span>, <span>s_prime</span>), <span>t</span> <span>+</span> <span>1</span>)

        <span>dfs</span>(<span>x</span>, <span>1</span>)
        <span>return</span> <span>output</span>
    </pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage-examples" aria-hidden="true" href="#usage-examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage Examples</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-openai-api" aria-hidden="true" href="#openai-api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>OpenAI API</h3>
<p dir="auto">To use Tree of Thoughts with OpenAI&#39;s API, create a custom model class that inherits from <code>AbstractLanguageModel</code> and implements the required methods using OpenAI&#39;s API. Then, create an instance of the <code>TreeOfThoughts</code> class with the custom model and the desired search algorithm (&#39;BFS&#39; or &#39;DFS&#39;).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-hugging-face-transformers" aria-hidden="true" href="#hugging-face-transformers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hugging Face Transformers</h3>
<p dir="auto">To run huggingface transformers</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/kyegomez/tree-of-thoughts
cd tree-of-thoughts
python3 huggingfaceExample.py"><pre><code>git clone https://github.com/kyegomez/tree-of-thoughts
cd tree-of-thoughts
python3 huggingfaceExample.py
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="from tree_of_thoughts import HuggingLanguageModel

model_name=&#34;gpt2&#34;
model_tokenizer=&#34;your tokenizer&#34;

huggingface_model = HuggingLanguageModel(model_name, model_tokenizer)"><pre><span>from</span> <span>tree_of_thoughts</span> <span>import</span> <span>HuggingLanguageModel</span>

<span>model_name</span><span>=</span><span>&#34;gpt2&#34;</span>
<span>model_tokenizer</span><span>=</span><span>&#34;your tokenizer&#34;</span>

<span>huggingface_model</span> <span>=</span> <span>HuggingLanguageModel</span>(<span>model_name</span>, <span>model_tokenizer</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="class HuggingLanguageModel(AbstractLanguageModel):
    def __init__(self, model_name):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def generate_thoughts(self, state, k):
        state_text = &#39; &#39;.join(state)
        prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, generate {k} coherent thoughts to achieve the reasoning process:&#34;

        inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)
        outputs = self.model.generate(**inputs, num_return_sequences=k)
        thoughts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

        return thoughts

    def evaluate_states(self, states, inital_prompt):
        state_values = {}
        for state in states:
            state_text = &#39; &#39;.join(state)
            prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, pessimitically evaluate its value as a float between 0 and 1 based on it&#39;s potential to achieve {inital_prompt}&#34;

            inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)
            outputs = self.model.generate(**inputs, num_return_sequences=1)
            value_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            try:
                value = float(value_text)
            except ValueError:
                value = 0  # Assign a default value if the conversion fails

            state_values[state] = value

        return state_values

"><pre><span>class</span> <span>HuggingLanguageModel</span>(<span>AbstractLanguageModel</span>):
    <span>def</span> <span>__init__</span>(<span>self</span>, <span>model_name</span>):
        <span>self</span>.<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_name</span>)
        <span>self</span>.<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>)

    <span>def</span> <span>generate_thoughts</span>(<span>self</span>, <span>state</span>, <span>k</span>):
        <span>state_text</span> <span>=</span> <span>&#39; &#39;</span>.<span>join</span>(<span>state</span>)
        <span>prompt</span> <span>=</span> <span>f&#34;Given the current state of reasoning: &#39;<span><span>{</span><span>state_text</span><span>}</span></span>&#39;, generate <span><span>{</span><span>k</span><span>}</span></span> coherent thoughts to achieve the reasoning process:&#34;</span>

        <span>inputs</span> <span>=</span> <span>self</span>.<span>tokenizer</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)
        <span>outputs</span> <span>=</span> <span>self</span>.<span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, <span>num_return_sequences</span><span>=</span><span>k</span>)
        <span>thoughts</span> <span>=</span> [<span>self</span>.<span>tokenizer</span>.<span>decode</span>(<span>output</span>, <span>skip_special_tokens</span><span>=</span><span>True</span>) <span>for</span> <span>output</span> <span>in</span> <span>outputs</span>]

        <span>return</span> <span>thoughts</span>

    <span>def</span> <span>evaluate_states</span>(<span>self</span>, <span>states</span>, <span>inital_prompt</span>):
        <span>state_values</span> <span>=</span> {}
        <span>for</span> <span>state</span> <span>in</span> <span>states</span>:
            <span>state_text</span> <span>=</span> <span>&#39; &#39;</span>.<span>join</span>(<span>state</span>)
            <span>prompt</span> <span>=</span> <span>f&#34;Given the current state of reasoning: &#39;<span><span>{</span><span>state_text</span><span>}</span></span>&#39;, pessimitically evaluate its value as a float between 0 and 1 based on it&#39;s potential to achieve <span><span>{</span><span>inital_prompt</span><span>}</span></span>&#34;</span>

            <span>inputs</span> <span>=</span> <span>self</span>.<span>tokenizer</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)
            <span>outputs</span> <span>=</span> <span>self</span>.<span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, <span>num_return_sequences</span><span>=</span><span>1</span>)
            <span>value_text</span> <span>=</span> <span>self</span>.<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>], <span>skip_special_tokens</span><span>=</span><span>True</span>)

            <span>try</span>:
                <span>value</span> <span>=</span> <span>float</span>(<span>value_text</span>)
            <span>except</span> <span>ValueError</span>:
                <span>value</span> <span>=</span> <span>0</span>  <span># Assign a default value if the conversion fails</span>

            <span>state_values</span>[<span>state</span>] <span>=</span> <span>value</span>

        <span>return</span> <span>state_values</span>
</pre></div>

<p dir="auto">This algorithm is still infant yet it&#39;s potential remains unimaginable, let&#39;s advance the reasoning of AI&#39;s together under this banner.</p>

<p dir="auto">You can easily share this repository by clicking on the following buttons:</p>
<p dir="auto"><a href="https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts" rel="nofollow"><img src="https://camo.githubusercontent.com/a474432345fd9fec72e2d5d55c5e836d0c6b765f39cefcc0379fb398f285ec24/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c3f7374796c653d736f6369616c2675726c3d68747470732533412532462532466769746875622e636f6d2532466b7965676f6d657a253246747265652d6f662d74686f7567687473" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url?style=social&amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts"/></a>
<a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts" rel="nofollow"><img src="https://camo.githubusercontent.com/d81b0b7b3b5d0527c1fdb39af296815321fb18d90e3c3cf7bb11a6f779468437/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53686172652d4c696e6b6564496e2d626c75653f7374796c653d736f6369616c266c6f676f3d6c696e6b6564696e" alt="LinkedIn" data-canonical-src="https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;logo=linkedin"/></a></p>
<p dir="auto">For Instagram, while it doesn&#39;t directly support sharing of web links, you can share the screenshot of our project and the link in your caption or bio. You can download the project screenshot by clicking the image below:</p>
<p dir="auto"><a href="https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg"><img src="https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg" alt="Tree of Thoughts"/></a></p>
<p dir="auto">We greatly appreciate any help in spreading the word about our project. Thank you for your support!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-roadmap" aria-hidden="true" href="#roadmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Roadmap</h2>
<p dir="auto">now:
Generate suite of evaluations used in the paper testing AI agents with other reasoning methods like COT and self consistency and run them in parallel to conduct evaluation experiments.</p>
<p dir="auto">Implement a more sophisticated prompt engineering strategy to guide the model&#39;s reasoning process more effectively.</p>
<p dir="auto">Make TreeofThoughts class completely customizable with a config yml file with params like
chatbot:
type: &#34;openai&#34;
max_context_length: 8000
include_chat_history_in_query: false
openai:
model: &lt;model_name&gt;
api_key: &lt;your_open_ai_api_key&gt;</p>
<p dir="auto">Script that generates an dataset based on a topic input, -&gt; set of questions are asked, then multiple trees of thoughts are run concurrently to generate the decision making rich dataset</p>
<p dir="auto">Introduce a reinforcement learning, distillment, and finetuning scripts to finely tune the model based on feedback from the Tree of Thoughts algorithm.</p>
<p dir="auto">Integrate heuristics that autonomously determine the search algorithm based on indicators</p>
<p dir="auto">Integrate heuristics that autonomously determine the strategy cos or propose</p>
<p dir="auto">Integrate heuristics that autonomously set the input params:</p>
<p dir="auto">k =
T =
b =
vth =</p>

<p dir="auto">multi-modality tree of thoughts</p>
<p dir="auto">multi-modality forest of thoughts</p>
<p dir="auto">multi-modality world of thoughts</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-multi-modality-tree-of-thoughts-" aria-hidden="true" href="#multi-modality-tree-of-thoughts-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Multi-Modality Tree of Thoughts <g-emoji alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png">🌐</g-emoji><g-emoji alias="deciduous_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f333.png">🌳</g-emoji></h3>
<p dir="auto">The next big advancement for the Tree of Thoughts algorithm is to extend it to multi-modality, enabling it to handle not only text but also images, audio, and other data types. This will bring us closer to multi-modal superintelligence.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-actionable-steps" aria-hidden="true" href="#actionable-steps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Actionable Steps</h4>
<ol dir="auto">
<li>Research and identify suitable multi-modal pre-trained models that can handle various data types (e.g., text, images, audio).</li>
<li>Adapt the thought decomposition, thought generator, and state evaluator functions to handle multi-modal data.</li>
<li>Develop a method for combining different modalities in the search tree, allowing the algorithm to reason across different data types.</li>
<li>Implement and test the multi-modal Tree of Thoughts algorithm with various problems and datasets.</li>
<li>Optimize the algorithm for performance and resource usage, ensuring it scales well with large multi-modal datasets.</li>
<li>Publish the results and gather feedback from the community to further improve the multi-modal Tree of Thoughts algorithm.</li>
</ol>
<p dir="auto">Join us on this exciting journey to advance the Tree of Thoughts algorithm to multi-modality superintelligence! <g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji></p>

<h2 tabindex="-1" dir="auto"><a id="user-content-x-str" aria-hidden="true" href="#x-str"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>x (str):</h2>
<p dir="auto">The initial problem statement or prompt for which the Tree of Thoughts algorithm will generate a solution.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-k-int-default5" aria-hidden="true" href="#k-int-default5"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>k (int, default=5):</h2>
<p dir="auto">The number of thoughts to generate at each state.
A higher value of k will result in more thoughts being generated, potentially leading to a more diverse set of solutions. However, increasing k may also increase the computational complexity and time required to find a solution.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-t-int-default3" aria-hidden="true" href="#t-int-default3"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>T (int, default=3):</h2>
<p dir="auto">The maximum depth of the search tree.
A higher value of T allows the algorithm to explore deeper states, potentially leading to better solutions. However, increasing T may also increase the computational complexity and time required to find a solution.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-b-int-default5" aria-hidden="true" href="#b-int-default5"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>b (int, default=5):</h2>
<p dir="auto">The branching factor of the search tree, which determines the maximum number of child nodes for each parent node.
A higher value of b allows the algorithm to explore more states, potentially leading to better solutions. However, increasing b may also increase the computational complexity and time required to find a solution.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-vth-float-default05" aria-hidden="true" href="#vth-float-default05"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>vth (float, default=0.5):</h2>
<p dir="auto">The value threshold for pruning states.
States with a value below this threshold will be discarded, reducing the search space. A higher value of vth will result in a more aggressive pruning strategy, potentially speeding up the search process. However, setting vth too high may cause the algorithm to discard promising states, leading to suboptimal solutions.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-timeout-int-default10" aria-hidden="true" href="#timeout-int-default10"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>timeout (int, default=10):</h2>
<p dir="auto">The maximum time (in seconds) allowed for the search process. If the search process exceeds this time limit, the algorithm will return the best solution found so far.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-confidence_threshold-float-default08" aria-hidden="true" href="#confidence_threshold-float-default08"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>confidence_threshold (float, default=0.8):</h2>
<p dir="auto">The confidence threshold for determining when a solution is satisfactory. If the algorithm finds a solution with a confidence value above this threshold, it will return the solution immediately.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-max_iterations-int-default40" aria-hidden="true" href="#max_iterations-int-default40"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>max_iterations (int, default=40):</h2>
<p dir="auto">The maximum number of iterations allowed for the search process. If the search process exceeds this number of iterations, the algorithm will return the best solution found so far.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-convergence_threshold-float-default001" aria-hidden="true" href="#convergence_threshold-float-default001"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>convergence_threshold (float, default=0.01):</h2>
<p dir="auto">The convergence threshold for determining when the search process has converged. If the difference in confidence values between consecutive iterations is below this threshold for a specified number of iterations (convergence_count), the algorithm will return the best solution found so far.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-convergence_count-int-default5" aria-hidden="true" href="#convergence_count-int-default5"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>convergence_count (int, default=5):</h2>
<p dir="auto">The number of consecutive iterations required for the search process to be considered converged. If the difference in confidence values between consecutive iterations is below the convergence_threshold for this number of iterations, the algorithm will return the best solution found so far.</p>

<p dir="auto">Thanks to: Shunyu Yao Princeton University, Dian Yu Google DeepMind, Jeffrey Zhao, Google DeepMind, Izhak Shafran Google DeepMind, Thomas L. Griffiths, Princeton University, Yuan Cao Google DeepMind, Karthik Narasimha, Princeton University for sharing this amazing work with the world!</p>
<p dir="auto">And, thanks to Phil Wang or Lucidrains for inspiring me to devote myself to open source AI Research</p>
</article>
          </div></div>
  </body>
</html>
