<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">Original</a>
    <h1>How to implement Q&amp;A against your docs with GPT3 embeddings and Datasette</h1>
    
    <div id="readability-page-1" class="page"><div id="primary">

<div>




<p>If you’ve spent any time with GPT-3 or ChatGPT, you’ve likely thought about how useful it would be if you could point them at a specific, current collection of text or documentation and have it use that as part of its input for answering questions.</p>
<p>It turns out there is a neat trick for doing exactly that. I’ve been experimenting with it using my <a href="https://datasette.io/">Datasette</a> web application as a rapid prototyping platform for trying out new AI techniques using custom SQL functions.</p>
<p>Here’s how to do this:</p>
<ul>
<li>Run a text search (or a semantic search, described later) against your documentation to find content that looks like it could be relevant to the user’s question</li>
<li>Grab extracts of that content and glue them all together into a blob of text</li>
<li>Construct a prompt consisting of that text followed by “Given the above content, answer the following question: ” and the user’s question</li>
<li>Send the whole thing through <a href="https://beta.openai.com/docs/api-reference/completions">the GPT-3 API</a> and see what comes back</li>
</ul>
<p>I’ve been calling this the <strong>semantic search answers</strong> pattern.</p>
<p>This is a form of <a href="https://simonwillison.net/tags/promptengineering/">prompt engineering</a> (and hence is prone to <a href="https://simonwillison.net/series/prompt-injection/">prompt injection attacks</a>, so bear that in mind). You need to fit your overall prompt into the token limit for GPT-3, which is currently 4,000 tokens. A token is more-or-less the same thing as a word.</p>
<p>Since publishing this post I’ve been pointed to the <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb">Question Answering using Embeddings</a> notebook in the OpenAI cookbook which describes this exact same technique.</p>
<p>In this post I’ll show how I implemented this pattern using SQLite, Datasette and some custom SQL functions and tools written in Python.</p>
<h4>First, some demos</h4>
<p>I’ve implemented this pattern against my blog—3,000 posts going back over twenty years. I built it using some custom SQLite SQL functions that call the OpenAI APIs.</p>
<p>First, some examples of questions and answers I ran against my blog:</p>
<p><strong>What does shot scraper do?</strong></p>
<blockquote>
<p>Shot-scraper is a command-line tool that helps automate the process of taking screenshots of web pages and scraping data from them using JavaScript. It can be used to take one-off screenshots or take multiple screenshots in a repeatable way by defining them in a YAML file. It can also be used to execute JavaScript on a page and return the resulting value.</p>
</blockquote>
<p><a href="https://shot-scraper.datasette.io/">shot-scraper</a> is a tool I released in 2022—after the 2021 GPT-3 training cut-off date. This is a very good summary—better I think than anything I’ve written about <code>shot-scraper</code> myself.</p>
<p><strong>What museums has Simon been to?</strong></p>
<blockquote>
<p>Simon has been to the Pitt Rivers museum, the British Museum, the Science Museum, the Natural History Museum, the V&amp;A, the Victoria and Albert Museum, and the Pioneer Museum in Paso Robles. He has also seen the Giant Squid at the Natural History Museum and Charles Darwin’s collection at the spirit building.</p>
</blockquote>
<p>This is entirely correct, though I’ve talked about a lot more museums than that on my blog.</p>
<p><strong>What are some unconventional things you can do with GitHub Actions?</strong></p>
<blockquote>
<ol>
<li>Create dynamic content for GitHub repository templates using cookiecutter and GitHub Actions</li>
<li>Automate repetitive tasks like code linting, testing, and deployment using GitHub Actions</li>
<li>Run scheduled scrapers in GitHub Actions</li>
<li>Use GitHub Actions to trigger a webhook call when a repository is forked</li>
<li>Create an automated issue-tracking system with GitHub Issues and GitHub Actions</li>
<li>Automatically extract captions and transcripts from online videos using Whisper and GitHub Actions.</li>
</ol>
</blockquote>
<p>Some of my relevant posts:</p>
<ul>
<li><a href="https://simonwillison.net/2021/Aug/28/dynamic-github-repository-templates/">Dynamic content for GitHub repository templates using cookiecutter and GitHub Actions</a></li>
<li><a href="https://simonwillison.net/2020/Oct/9/git-scraping/">Git scraping: track changes over time by scraping to a Git repository</a></li>
<li><a href="https://simonwillison.net/2022/Sep/30/action-transcription/">A tool to run caption extraction against online videos using Whisper and GitHub Issues/Actions</a></li>
</ul>
<p>I’ve never written anything about triggering webhook calls or issue-tracking systems though, so 4 and 5 in that list are a little suspicious.</p>
<p><strong>What is the big grey box at the top of the utility pole? What does it do?</strong></p>
<blockquote>
<p>The big grey box at the top of the utility pole is likely a transformer. Transformers are used to step down the voltage of electricity from the high-voltage lines that run along the pole to the lower-voltage lines that are used to provide electricity to homes and businesses.</p>
</blockquote>
<p>I’m including this example because I’ve never written anything even remotely related to transformers and utility poles on my blog. This demonstrates that in the lack of useful context GPT-3 will answer the question entirely by itself—which may or may not be what you want from this system.</p>
<h4>Try this out yourself</h4>
<p>If you want to try this out yourself you’ll need to get your own API key from OpenAI. I don’t want to foot the bill for people using my blog as a free source of GPT-3 prompt answers!</p>
<p>You can <a href="https://beta.openai.com/signup">sign up for one here</a>. I believe they are still running a free trial period.</p>
<p>Now head over to this page:</p>
<p><a href="https://datasette.simonwillison.net/simonwillisonblog/answer_question?_hide_sql=1">https://datasette.simonwillison.net/simonwillisonblog/answer_question?_hide_sql=1</a></p>
<p>You’ll need to paste in your OpenAI key. I am not logging these anywhere, and the form stores these in a cookie in order to avoid transmitting it over a GET query string where it could be accidentally logged somewhere.</p>
<p>Then type in your question and see what comes back!</p>
<p>Let’s talk about how this all works—in a whole lot of detail.</p>
<h4>Semantic search using embeddings</h4>
<p>You can implement the first step of this sequence using any search engine you like—but there’s a catch: we are encouraging users here to ask questions, which increases the chance that they might include text in their prompt which doesn’t exactly match documents in our index.</p>
<p>“What are the key features of Datasette?” for example might miss blog entries that don’t include the word “feature” even though they describe functionality of the software in detail.</p>
<p>What we want here is <strong>semantic search</strong>—we want to find documents that match the meaning of the user’s search term, even if the matching keywords are not present.</p>
<p>OpenAI have a less well-known API that can help here, which had a big upgrade (and major price reduction) <a href="https://openai.com/blog/new-and-improved-embedding-model/">back in December</a>: their embedding model.</p>
<p>An <strong>embedding</strong> is a list of floating point numbers.</p>
<p>As an example, consider a latitude/longitude location: it’s a list of two floating point numbers. You can use those numbers to find other nearby points by calculating distances between them.</p>
<p>Add a third number and now you can plot locations in three dimensional space—and still calculate distances between them to find the closest points.</p>
<p>This idea keeps on working even as we go beyond three dimensions: you can calculate distances between vectors of any length, no matter how many dimensions they have.</p>
<p>So if we can represent some text in a many-multi-dimensional vector space, we can calculate distances between those vectors to find the closest matches.</p>
<p>The OpenAI embedding model lets you take any string of text (up to a ~8,000 word length limit) and turn that into a list of 1,536 floating point numbers. We’ll call this list the “embedding” for the text.</p>
<p>These numbers are derived from a sophisticated language model. They take a vast amount of knowledge of human language and flatten that down to a list of floating point numbers—at 4 bytes per floating point number that’s 4*1,536 = 6,144 bytes per embedding—6KiB.</p>
<p>The distance between two embeddings represents how semantically similar the text is to each other.</p>
<p>The two most obvious applications of this are search and similarity scores.</p>
<p>Take a user’s search term. Calculate its embedding. Now find the distance between that embedding and every pre-calculated embedding in a corpus and return the 10 closest results.</p>
<p>Or for document similarity: calculate embeddings for every document in a collection, then look at each one in turn and find the closest other embeddings: those are the documents that are most similar to it.</p>
<p>For my semantic search answers implementation, I use an embedding-based semantic search as the first step to find the best matches for the question. I then assemble these top 5 matches into the prompt to pass to GPT-3.</p>
<h4>Calculating embeddings</h4>
<p>Embeddings can be calculated from text using <a href="https://beta.openai.com/docs/guides/embeddings">the OpenAI embeddings API</a>. It’s really easy to use:</p>
<div><pre>curl https://api.openai.com/v1/embeddings \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -H <span><span>&#34;</span>Authorization: Bearer <span>$OPENAI_API_KEY</span><span>&#34;</span></span> \
  -d <span><span>&#39;</span>{&#34;input&#34;: &#34;Your text string goes here&#34;,</span>
<span>       &#34;model&#34;:&#34;text-embedding-ada-002&#34;}<span>&#39;</span></span></pre></div>
<p>The documentation doesn’t mention this, but you can pass a list of strings (<a href="https://github.com/openai/openai-python/blob/777c1c3de1a9cfc0a33e6376cc09b9badbb9cdf9/openai/embeddings_utils.py#L43">up to 2048</a> according to the official Python library source code) as <code>&#34;input&#34;</code> to run embeddings in bulk:</p>
<div><pre>curl https://api.openai.com/v1/embeddings \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -H <span><span>&#34;</span>Authorization: Bearer <span>$OPENAI_API_KEY</span><span>&#34;</span></span> \
  -d <span><span>&#39;</span>{&#34;input&#34;: [&#34;First string&#34;, &#34;Second string&#34;, &#34;Third string&#34;],</span>
<span>       &#34;model&#34;:&#34;text-embedding-ada-002&#34;}<span>&#39;</span></span></pre></div>
<p>The returned data from this API looks like this:</p>
<div><pre>{
  <span>&#34;data&#34;</span>: [
    {
      <span>&#34;embedding&#34;</span>: [
        <span>-0.006929283495992422</span>,
        <span>-0.005336422007530928</span>,
        <span>...</span>
        <span>-4.547132266452536e-05</span>,
        <span>-0.024047505110502243</span>
      ],
      <span>&#34;index&#34;</span>: <span>0</span>,
      <span>&#34;object&#34;</span>: <span><span>&#34;</span>embedding<span>&#34;</span></span>
    }
  ]</pre></div>
<p>As expected, it’s a list of 1,536 floating point numbers.</p>
<p>I’ve been storing embeddings as a binary string that appends all of the floating point numbers together, using their 4-byte representation.</p>
<p>Here are the tiny Python functions I’ve been using for doing that:</p>
<pre><span>import</span> <span>struct</span>

<span>def</span> <span>decode</span>(<span>blob</span>):
    <span>return</span> <span>struct</span>.<span>unpack</span>(<span>&#34;f&#34;</span> <span>*</span> <span>1536</span>, <span>blob</span>)

<span>def</span> <span>encode</span>(<span>values</span>):
    <span>return</span> <span>struct</span>.<span>pack</span>(<span>&#34;f&#34;</span> <span>*</span> <span>1536</span>, <span>*</span><span>values</span>)</pre>
<p>I then store them in SQLite <code>blob</code> columns in my database.</p>
<p>I wrote a custom tool for doing this, called <a href="https://datasette.io/tools/openai-to-sqlite">openai-to-sqlite</a>. I can run it like this:</p>
<div><pre>openai-to-sqlite embeddings simonwillisonblog.db \
  --sql <span><span>&#39;</span>select id, title, body from blog_entry<span>&#39;</span></span> \
  --table blog_entry_embeddings</pre></div>
<p>This concatenates together the <code>title</code> and <code>body</code> columns from that table, runs them through the OpenAI embeddings API and stores the results in a new table called <code>blog_entry_embeddings</code> with the following schema:</p>
<div><pre>CREATE TABLE [blog_entry_embeddings] (
   [id] <span>INTEGER</span> <span>PRIMARY KEY</span>,
   [embedding] BLOB
)</pre></div>
<p>I can join this against the <code>blog_entry</code> table by ID later on.</p>
<h4>Finding the closest matches</h4>
<p>The easiest way to calculate similarity between two embedding arrays is to use cosine similarity. A simple Python function for that looks like this:</p>
<pre><span>def</span> <span>cosine_similarity</span>(<span>a</span>, <span>b</span>):
    <span>dot_product</span> <span>=</span> <span>sum</span>(<span>x</span> <span>*</span> <span>y</span> <span>for</span> <span>x</span>, <span>y</span> <span>in</span> <span>zip</span>(<span>a</span>, <span>b</span>))
    <span>magnitude_a</span> <span>=</span> <span>sum</span>(<span>x</span> <span>*</span> <span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>a</span>) <span>**</span> <span>0.5</span>
    <span>magnitude_b</span> <span>=</span> <span>sum</span>(<span>x</span> <span>*</span> <span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>b</span>) <span>**</span> <span>0.5</span>
    <span>return</span> <span>dot_product</span> <span>/</span> (<span>magnitude_a</span> <span>*</span> <span>magnitude_b</span>)</pre>
<p>You can brute-force find the top matches for a table by executing that comparison for every row and returning the ones with the highest score.</p>
<p>I added this to my <a href="https://datasette.io/plugins/datasette-openai">datasette-openai</a> Datasette plugin as a custom SQL function called <code>openai_embedding_similarity()</code>. Here’s a query that uses it:</p>
<div><pre>with input <span>as</span> (
  <span>select</span>
    embedding
  <span>from</span>
    blog_entry_embeddings
  <span>where</span>
    id <span>=</span> :entry_id
),
top_n <span>as</span> (
  <span>select</span>
    id,
    openai_embedding_similarity(
      <span>blog_entry_embeddings</span>.<span>embedding</span>,
      <span>input</span>.<span>embedding</span>
    ) <span>as</span> score
  <span>from</span>
    blog_entry_embeddings,
    input
  <span>order by</span>
    score <span>desc</span>
  <span>limit</span>
    <span>20</span>
)
<span>select</span>
  score,
  <span>blog_entry</span>.<span>id</span>,
  <span>blog_entry</span>.<span>title</span>
<span>from</span>
  blog_entry
  <span>join</span> top_n <span>on</span> <span>blog_entry</span>.<span>id</span> <span>=</span> <span>top_n</span>.<span>id</span></pre></div>
<p><a href="https://datasette.simonwillison.net/simonwillisonblog?sql=with+input+as+%28%0D%0A++select%0D%0A++++embedding%0D%0A++from%0D%0A++++blog_entry_embeddings%0D%0A++where%0D%0A++++id+%3D+%3Aentry_id%0D%0A%29%2C%0D%0Atop_n+as+%28%0D%0A++select%0D%0A++++id%2C%0D%0A++++openai_embedding_similarity%28%0D%0A++++++blog_entry_embeddings.embedding%2C%0D%0A++++++input.embedding%0D%0A++++%29+as+score%0D%0A++from%0D%0A++++blog_entry_embeddings%2C%0D%0A++++input%0D%0A++order+by%0D%0A++++score+desc%0D%0A++limit%0D%0A++++20%0D%0A%29%0D%0Aselect%0D%0A++score%2C%0D%0A++blog_entry.id%2C%0D%0A++blog_entry.title%0D%0Afrom%0D%0A++blog_entry%0D%0A++join+top_n+on+blog_entry.id+%3D+top_n.id&amp;entry_id=7977">Try that out here</a>.</p>
<p>This takes as input the ID of one of my blog entries and returns a list of the other entries, ordered by their similarity score.</p>
<p>Unfortunately this is pretty slow! It takes over 1.3s to run against all 3,000 embeddings in my blog.</p>
<p>I did some research and found that a highly regarded solutions for fast vector similarity calculations is <a href="https://github.com/facebookresearch/faiss">FAISS</a>, by Facebook AI research. It has neat Python bindings and can be installed using <code>pip install faiss-cpu</code> (the <code>-gpu</code> version requires a GPU).</p>
<p>FAISS works against an in-memory index. My blog’s Datasette instance uses the <a href="https://simonwillison.net/2021/Jul/28/baked-data/">baked data</a> pattern which means the entire thing is re-deployed any time the data changes—as such, I can spin up an in-memory index once on startup without needing to worry about updating the index continually as rows in the database change.</p>
<p>So I built another plugin to do that: <a href="https://datasette.io/plugins/datasette-faiss">datasette-faiss</a>—which can be configured to build an in-memory FAISS index against a configured table on startup, and can then be queried using another custom SQL function.</p>
<p>Here’s the related entries query from above rewritten to use the FAISS index:</p>
<div><pre>with input <span>as</span> (
  <span>select</span>
    embedding
  <span>from</span>
    blog_entry_embeddings
  <span>where</span>
    id <span>=</span> :entry_id
),
top_n <span>as</span> (
  <span>select</span> value <span>as</span> id <span>from</span> json_each(
    faiss_search(
      <span><span>&#39;</span>simonwillisonblog<span>&#39;</span></span>,
      <span><span>&#39;</span>blog_entry_embeddings<span>&#39;</span></span>,
      <span>input</span>.<span>embedding</span>,
      <span>20</span>
    )
  ), input
)
<span>select</span>
  <span>blog_entry</span>.<span>id</span>,
  <span>blog_entry</span>.<span>title</span>
<span>from</span>
  blog_entry
  <span>join</span> top_n <span>on</span> <span>blog_entry</span>.<span>id</span> <span>=</span> <span>top_n</span>.<span>id</span></pre></div>
<p><a href="https://datasette.simonwillison.net/simonwillisonblog?sql=with+input+as+%28%0D%0A++select%0D%0A++++embedding%0D%0A++from%0D%0A++++blog_entry_embeddings%0D%0A++where%0D%0A++++id+%3D+%3Aentry_id%0D%0A%29%2C%0D%0Atop_n+as+%28%0D%0A++select+value+as+id+from+json_each%28%0D%0A++++faiss_search%28%0D%0A++++++%27simonwillisonblog%27%2C%0D%0A++++++%27blog_entry_embeddings%27%2C%0D%0A++++++input.embedding%2C%0D%0A++++++20%0D%0A++++%29%0D%0A++%29%2C+input%0D%0A%29%0D%0Aselect%0D%0A++blog_entry.id%2C%0D%0A++blog_entry.title%0D%0Afrom%0D%0A++blog_entry%0D%0A++join+top_n+on+blog_entry.id+%3D+top_n.id&amp;entry_id=7977">This one runs</a> in 4.8ms!</p>
<p><code>faiss_search(database_name, table_name, embedding, n)</code> returns a JSON array of the top <code>n</code> IDs from the specified embeddings table, based on distance scores from the provided <code>embedding</code>.</p>
<p>The <code>json_each()</code> trick here is a workaround for the fact that Python’s SQLite driver doesn’t yet provide an easy way to write table-valued functions—SQL functions that return something in the shape of a table.</p>
<p>Instead, I use <code>json_each()</code> to turn the string JSON array of IDs from <code>datasette_faiss()</code> into a table that I can run further joins against.</p>
<h4>Implementing semantic search with embeddings</h4>
<p>So far we’ve just seen embeddings used for finding similar items. Let’s implement semantic search, using a user-provided query.</p>
<p>This is going to need an API key again, because it involves a call to OpenAI to run embeddings against the user’s search query.</p>
<p>Here’s the SQL query:</p>
<div><pre><span>select</span>
  value,
  <span>blog_entry</span>.<span>title</span>,
  substr(<span>blog_entry</span>.<span>body</span>, <span>0</span>, <span>500</span>)
<span>from</span>
  json_each(
    faiss_search(
      <span><span>&#39;</span>simonwillisonblog<span>&#39;</span></span>,
      <span><span>&#39;</span>blog_entry_embeddings<span>&#39;</span></span>,
      (
        <span>select</span>
          openai_embedding(:query, :_cookie_openai_api_key)
      ),
      <span>10</span>
    )
  )
  <span>join</span> blog_entry <span>on</span> value <span>=</span> <span>blog_entry</span>.<span>id</span>
  <span>where</span> length(coalesce(:query, <span><span>&#39;</span><span>&#39;</span></span>)) <span>&gt;</span> <span>0</span></pre></div>
<p><a href="https://datasette.simonwillison.net/simonwillisonblog/embedding_search">Try that here</a> (with extra some cosmetic tricks.)</p>
<p>We’re using a new function here: <code>openai_embedding()</code>—which takes some text and an API key and returns an embedding for that text.</p>
<p>The API key comes from <code>:_cookie_openai_api_key</code>—this is a special Datasette mechanism called <a href="https://docs.datasette.io/en/stable/sql_queries.html#magic-parameters">magic parameters</a> which can read variables from cookies.</p>
<p>The <a href="https://datasette.io/plugins/datasette-cookies-for-magic-parameters">datasette-cookies-for-magic-parameters</a> plugin notices these and turns them into an interface for the user to populate the cookies with, decsribed earlier.</p>
<p>One last trick: adding <code>where length(coalesce(:query, &#39;&#39;)) &gt; 0</code> to the query means that the query won’t run if the user hasn’t entered any text into the search box.</p>
<h4>Constructing a prompt from semantic search query results</h4>
<p>Getting back to our semantic search answers pattern.</p>
<p>We need a way to construct a prompt for GPT-3 using the results of our semantic search query.</p>
<p>There’s one big catch: GPT-3 has a length limit, and it’s strictly enforced. If you pass even one token over that limit you’ll get an error.</p>
<p>We want to use as much material from the top five search results as possible, leaving enough space for the rest of the prompt (the user’s question and our own text) and the prompt response.</p>
<p>I ended up solving this with another custom SQL function:</p>
<div><pre><span>select</span> openai_build_prompt(content, <span><span>&#39;</span>Context:</span>
<span>------------</span>
<span><span>&#39;</span></span>, <span><span>&#39;</span></span>
<span>------------</span>
<span>Given the above context, answer the following question: <span>&#39;</span></span> <span>||</span> :question,
  <span>500</span>
  ) <span>from</span> search_results</pre></div>
<p>This function works as an <em>aggregate</em> function—it takes a table of results and returns a single string.</p>
<p>It takes the column to aggregate—in this case <code>content</code>—as the first argument. Then it takes a prefix and a suffix, which are concatenated together with the aggregated content in the middle.</p>
<p>The third argument is the number of tokens to allow for the response.</p>
<p>The function then attempts to truncate each of the input values to the maximum length that will still allow them all to be concatenated together while staying inside that 4,000 token limit.</p>
<h4>Adding it all together</h4>
<p>With all of the above in place, the following query is my full implementation of semantic search answers against my blog:</p>
<div><pre>with query <span>as</span> (
  <span>select</span>
    openai_embedding(:question, :_cookie_openai_api_key) <span>as</span> q
),
top_n <span>as</span> (
  <span>select</span>
    value
  <span>from</span> json_each(
    faiss_search(
      <span><span>&#39;</span>simonwillisonblog<span>&#39;</span></span>,
      <span><span>&#39;</span>blog_entry_embeddings<span>&#39;</span></span>,
      (<span>select</span> q <span>from</span> query),
      <span>5</span>
    )
  )
  <span>where</span> length(coalesce(:question, <span><span>&#39;</span><span>&#39;</span></span>)) <span>&gt;</span> <span>0</span>
),
texts <span>as</span> (
  <span>select</span> <span><span>&#39;</span>Created: <span>&#39;</span></span> <span>||</span> created <span>||</span> <span><span>&#39;</span>, Title: <span>&#39;</span></span> <span>||</span> title <span>||</span> 
  <span><span>&#39;</span>, Body: <span>&#39;</span></span> <span>||</span> openai_strip_tags(body) <span>as</span> <span>text</span>
  <span>from</span> blog_entry <span>where</span> id <span>in</span> (<span>select</span> value <span>from</span> top_n)
),
prompt <span>as</span> (
  <span>select</span> openai_build_prompt(<span>text</span>, <span><span>&#39;</span>Context:</span>
<span>------------</span>
<span><span>&#39;</span></span>, <span><span>&#39;</span></span>
<span>------------</span>
<span>Given the above context, answer the following question: <span>&#39;</span></span> <span>||</span> :question,
  <span>500</span>
  ) <span>as</span> prompt <span>from</span> texts
)
<span>select</span>
  <span><span>&#39;</span>Response<span>&#39;</span></span> <span>as</span> title,
  openai_davinci(
    prompt,
    <span>500</span>,
    <span>0</span>.<span>7</span>,
    :_cookie_openai_api_key
  ) <span>as</span> value
  <span>from</span> prompt
  <span>where</span> length(coalesce(:question, <span><span>&#39;</span><span>&#39;</span></span>)) <span>&gt;</span> <span>0</span>
<span>union all</span>
<span>select</span>
  <span><span>&#39;</span>Prompt<span>&#39;</span></span> <span>as</span> title,
  prompt <span>from</span> prompt</pre></div>
<p>As you can see, I really like using CTEs (the <code>with name as (...)</code> pattern) to assemble complex queries like this.</p>
<p>The <code>texts as ...</code> CTE is where I strip HTML tags from my content (using another custom function from the <code>datasete-openai</code> plugin called <code>openai_strip_tags()</code>) and assemble it along with the <code>Created</code> and <code>Title</code> metadata. Adding these gave the system a better chance of answering questions like “When did Natalie and Simon get married?” with the correct year.</p>
<p>The last part of this query uses a handy debugging trick: it returns two rows via a <code>union all</code>—the first has a <code>Response</code> label and shows the response from GPT-3, while the second has a <code>Prompt</code> label and shows the prompt that I passed to the model.</p>
<p><img src="https://static.simonwillison.net/static/2023/semantic-search-answer.jpg" alt="A Datasette form page. Question is When did Natalie and Simon get married?. Answer is Natalie and Simon got married on Saturday the 5th of June in 2010. The prompt is then displayed, which is a whole bunch of text from relevant blog entries."/></p>
<h4>Next steps</h4>
<p>There are <em>so many ways</em> to improve this system.</p>
<ul>
<li>Smarter prompt design. My prompt here is the first thing that I got to work—I’m certain there are all kinds of tricks that could be used to make this more effective.</li>
<li>Better selection of the content to include in the prompt. I’m using embedding search but then truncating to the first portion: a smarter implementation would attempt to crop out the most relevant parts of each entry, maybe by using embeddings against smaller chunks of text.
<ul>
  <li>Yoz <a href="https://mastodon.social/@yoz/109684742557484622">tipped me off</a> to <a href="https://gpt-index.readthedocs.io/en/latest/">GPT Index</a>, a project which aims to solve this exact problem by using a pre-trained LLM to help summarize text to better fit in a prompt used for these kinds of queries.</li>
<li>Spotted <a href="https://twitter.com/theseamouse/status/1614453236349693953">this idea</a> from Hassan Hayat: “don’t embed the question when searching. Ask GPT-3 to generate a fake answer, embed this answer, and use this to search”. See also this paper about <a href="https://arxiv.org/abs/2212.10496">Hypothetical Document
Embeddings</a>, via <a href="https://twitter.com/mathemagic1an/status/1615378778863157248">Jay Hack</a>.</li>
</ul>
</li>
<li>Hold out for GPT-4: I’ve heard rumours that the next version of the model will have a significantly larger token limit, which should result in much better results from this mechanism.</li>
</ul>
<h4>Want my help implementing this?</h4>
<p>I plan to use this pattern to add semantic search and semantic search answers to my <a href="https://datasette.cloud/">Datasette Cloud</a> SaaS platform. <a href="https://forms.gle/mbMZFKpK2kEY7t8C6">Please get in touch</a> if this sounds like a feature that could be relevant to your organization.</p>




</div>

</div></div>
  </body>
</html>
