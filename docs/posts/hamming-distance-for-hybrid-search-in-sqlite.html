<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://notnotp.com/notes/hamming-distance-for-hybrid-search-in-sqlite/">Original</a>
    <h1>Hamming Distance for Hybrid Search in SQLite</h1>
    
    <div id="readability-page-1" class="page"><div>
  <article itemscope="" itemtype="https://schema.org/BlogPosting">
    
    <time itemprop="datePublished" datetime="2026-02-05T00:00:00+00:00">Published on 5 February 2026</time>

    <p>
      This article shows how I implemented semantic search in SQLite using
      binary embeddings and Hamming distance, enabling hybrid search without
      external vector databases.
    </p>

    <p>
      SQLite&#39;s FTS5 extension provides excellent text search with BM25 ranking.
      However, it doesn&#39;t support semantic search, which means you can&#39;t combine
      keyword matching with meaning-based retrieval, a technique known as hybrid
      search.
    </p>

    

    <h2>Binary embeddings and Hamming distance</h2>
    <p>
      Semantic search works by converting text into numerical vectors
      (embeddings) that capture meaning (in my case, I do that via an API).
      Similar texts (i.e., talking about the same things) produce similar
      vectors. Embeddings generally use float32 values, that is, a
      1024-dimensional embedding requires 4KiB per document.
    </p>
    <p>
      Binary embeddings quantize each dimension to a single bit (0 or 1). This
      reduces storage dramatically, because 1024 dimensions become only 128
      bytes. The similarity metric changes from cosine distance to Hamming
      distance, so it also allows using fast simple bit operations vs more
      expensive floating-point arithmetic.
    </p>

    <p>
      The tradeoff is accuracy: binary quantization loses information. For many
      applications (including mine), this is acceptable given the storage and
      speed benefits, especially if it&#39;s combined with another classic text
      search algorithm like BM25.
    </p>

    <h2>Hamming Distance</h2>
    <p>
      Hamming distance counts the number of bit positions where two binary
      vectors differ.
    </p>

    <p>
      Example with 8-bit vectors:
    </p>
    <pre><code>Position:  0 1 2 3 4 5 6 7
Vector A:  1 0 1 1 0 1 1 0
Vector B:  1 0 0 1 1 0 1 0
               ^   ^ ^</code></pre>

    <p>
      Bits differ at positions 2, 4, and 5. Hamming distance = 3. For semantic
      search, <strong>lower Hamming distance means higher similarity</strong>.
    </p>

    <p>
      To compute Hamming distance efficiently, we XOR the two vectors (1 where
      bits differ, 0 where same), then count the 1s (population count or
      &#34;popcount&#34;). Modern CPUs have dedicated popcount instructions, making this
      very fast.
    </p>
    <p>Example:</p>

    <pre><code>Vector A:  1 0 1 1 0 1 1 0
Vector B:  1 0 0 1 1 0 1 0
A XOR B:   0 0 1 0 1 1 0 0

popcount(A XOR B) = 3 ones = distance of 3</code></pre>

    <h2>Implementation as a SQLite extension</h2>
    <p>
      SQLite extensions are dynamically loaded shared libraries (<code>.so</code> on Linux,
      <code>.dylib</code> on macOS) that can register custom SQL functions. This
      is simpler than forking SQLite or using external tools. The extension
      needs to: Register the function name with SQLite, implement the
      computation logic, and finally handle BLOB inputs and return an integer
      result.
    </p>

    <p>
      Registration code:
    </p>

    <pre><code>#include &lt;stdint.h&gt;
#include &lt;sqlite3ext.h&gt;
SQLITE_EXTENSION_INIT1

int sqlite3_extension_init(
  sqlite3 *db, 
  char **pzErrMsg, 
  const sqlite3_api_routines *pApi
){
  SQLITE_EXTENSION_INIT2(pApi);
  return sqlite3_create_function(
    db, &#34;hamming_distance&#34;, 2,     /* Name when called from SQL */
    SQLITE_UTF8 | SQLITE_DETERMINISTIC,
    0,
    hamming_distance,
    0, 0
  );
}</code></pre>

    <p>
      The core function receives two BLOB arguments (the binary vectors) and
      returns their Hamming distance:
    </p>

    <pre><code>static void hamming_distance(sqlite3_context *context, int argc, sqlite3_value **argv) {
  if (argc != 2) {
      sqlite3_result_error(context, &#34;hamming_distance() requires 2 arguments&#34;, -1);
      return;
  }

    const unsigned char *blob1 = sqlite3_value_blob(argv[0]);
    const unsigned char *blob2 = sqlite3_value_blob(argv[1]);
    int len1 = sqlite3_value_bytes(argv[0]);
    int len2 = sqlite3_value_bytes(argv[1]);

     if (len1 != len2) {
        sqlite3_result_error(context, &#34;vectors must be same length&#34;, -1);
        return;
    }

    /*
       Hamming Distance

       This cast assumes the architecture supports unaligned 64-bit access,
       which is true for x86_64 and ARMv8-A+, including Apple Silicon and
       Raspberry Pi 4 I tested against.
    */
    const uint64_t *v1 = (const uint64_t *)blob1;
    const uint64_t *v2 = (const uint64_t *)blob2;

    int chunks = len1 / 8;
    uint64_t distance = 0;

    // Process 8-byte chunks
    for (int i = 0; i &lt; chunks; i++) {
        distance += __builtin_popcountll(v1[i] ^ v2[i]);
    }

    // Remaining bytes
    for (int i = chunks * 8; i &lt; len1; i++) {
        distance += __builtin_popcount(blob1[i] ^ blob2[i]);
    }

    sqlite3_result_int64(context, distance);
}

</code></pre>

    <p>
      This implementation does the following:
    </p>
    <ul>
      <li>XORs corresponding 64-bit chunks from both vectors</li>
      <li>
        Uses <code>__builtin_popcount[ll]()</code> for bit counting (compiles to
        the adequate CPU instruction)
      </li>
      <li>Accumulates the total distance</li>
      <li>Handles any remaining bytes that don&#39;t fit in 64-bit chunks</li>
    </ul>

    <p>
      Here are the commands I used to compile the file for macOS (<code>hamming.dylib</code>) and for Linux (<code>hamming.so</code>):
    </p>

    <pre><code>hamming.dylib: hamming.c
        clang -g -fPIC -dynamiclib hamming.c -o hamming.dylib \
              -undefined dynamic_lookup -march=native -O3

hamming.so: hamming.c
        gcc -g -fPIC -shared hamming.c -o hamming.so \
        -march=native -O3
</code></pre>

    <h2>Performance on large datasets</h2>

    <p>
      Testing with 1 million rows of 128-byte binary vectors (1024 dimensions):
    </p>

    <pre><code>.load hamming -- load hamming.so or hamming.dylib

CREATE TABLE documents (
    rowid INTEGER PRIMARY KEY,
    embedding BLOB NOT NULL  -- 128 bytes for 1024-bit binary
);

-- Populate with 1M random embeddings
WITH RECURSIVE 
  cnt(x) AS (
    SELECT 1
    UNION ALL
    SELECT x+1 FROM cnt
    LIMIT 1000000
  )
INSERT INTO documents (rowid, embedding)
SELECT x, randomblob(128) FROM cnt;

.timer on

-- Searching
SELECT rowid, hamming_distance(:vec, embedding) as dist
FROM documents
ORDER BY dist
LIMIT 10
</code></pre>

    <p>
      Note: Instead of <code>:vec</code>, I hardcoded a raw 128-byte blob by
      hand. I omitted it to avoid visual clutter.
    </p>

    <p>
      Having <code>.timer on</code> tells SQLite to report the time taken by the
      following queries. To avoid disk latency and get consistent results, I ran
      tests on in-RAM (`:memory:`) databases.
    </p>

    <pre><code>% sqlite3 :memory: &lt; bench.sql
Run Time: real 0.035056 user 0.035006 sys 0.000053
</code></pre>

    <p>
      Not bad: <strong>35ms for 1 million rows</strong> on my Apple M4 chip,
      including the sort to get top-10 results. To measure just the Hamming
      distance computation without sorting overhead, we can put an impossible
      <code>WHERE</code>-clause that is always false, thus forcing SQLite to
      scan all rows (but returning none):
    </p>

    <pre><code>SELECT rowid, hamming_distance(:vec, embedding) as dist
FROM documents
WHERE dist &lt; 0; -- Impossible condition, no sorting needed</code></pre>

    <pre><code>% sqlite3 :memory: &lt; bench-no-sort.sql
Run Time: real 0.028491 user 0.028477 sys 0.000017
</code></pre>

    <p>
      <strong>28ms without sorting</strong>. The <code>ORDER BY</code> adds
      approximately 7ms to sort 1M rows.
    </p>

    <h3>Is O(<var>n</var>) acceptable?</h3>

    <p>
      Whether we use an <code>ORDER BY + LIMIT</code> or not, this scans <i>every</i> row, there is no indexing and no pruning. But, it&#39;s still
      pretty fast.
    </p>

    <p>
      Implementing HNSW or IVF indexing would add complexity that didn&#39;t seem
      justified for my project, along with memory overhead (maybe 2-5x the data
      size) and build-time cost. The boring O(<var>n</var>) simplicity is often
      worth it at this scale.
    </p>

    <h2>Limitations</h2>
    <p>
      This implementation has a significant limitation: it&#39;s just a function
      callable from SQL, not a full-fledged virtual table. SQLite computes <code>hamming_distance()</code>
      for every row, then sorts all results, then takes the top 10.
    </p>
    <p>
      For a true top-<i>k</i> selection, even without any indexing, I believe
      the extension would need to be a virtual table that can: recognize the
      <code>ORDER BY + LIMIT k</code>
      pattern, maintain a heap of <var>k</var> best candidates during scanning,
      and avoid sorting the full result set
    </p>

    <h2>Hybrid search with RRF</h2>
    <p>
      Combining FTS5 BM25 ranking with semantic search requires merging two
      different relevance signals. The standard approach is Reciprocal Rank
      Fusion (RRF). I went with that.
    </p>

    <p>
      Run both searches independently: BM25 search on tokenized text (keyword
      matching), and then Hamming distance on binary embeddings (semantic
      similarity).
      </p>

    <p>
      The query text needs to be transformed into a binary vector first. I use
      an external API for this, and run the BM25 query while awaiting the
      response to hide the API latency.
    </p>

    <p>
      RRF merges ranked lists without needing to normalize scores across
      different retrieval systems. It works by converting ranks into scores. For
      each document, calculate:
      <code>score(doc) = 1/(k + rank_bm25 + 1) + 1/(k + rank_semantic +
        1)</code>

      Where: <code>rank</code> is the position in that retrieval method&#39;s
      results (0-indexed) and <code>k</code> is a constant (typically 60) that
      controls how quickly scores decay. Documents appearing in both lists
      receive additive boosts. Documents in only one list still contribute. The
      final ranking sorts by combined score.
    </p>

    <pre><code>
function mergeRRF(
  semanticIds: number[], // Sorted by relevance, assume no dup
  bm25Ids: number[],
  k: number = 60, // RRF constant
): number[] {
  const scores = new Map&lt;number, number&gt;();

  semanticIds.forEach((id, rank) =&gt; {
    scores.set(id, (scores.get(id) || 0) + 1 / (k + rank + 1));
  });

  bm25Ids.forEach((id, rank) =&gt; {
    scores.set(id, (scores.get(id) || 0) + 1 / (k + rank + 1));
  });

  return Array.from(scores.entries())
    .sort((a, b) =&gt; b[1] - a[1])
    .map(([id]) =&gt; id);
}</code></pre>

    <h3>Example real queries demonstrating hybrid benefits</h3>

    <ul>
      <li>
        &#34;Apple founder&#34;: Semantic search disambiguates (Steve Jobs, not fruit)
      </li>
      <li>
        &#34;Python snake habitat&#34;: BM25 keyword &#34;snake&#34; filters out programming
        content
      </li>
      <li>
        &#34;Python programming tutorial&#34;: Semantic understanding recognizes
        programming context
      </li>
    </ul>

    <p>
      The combination handles both precise keyword requirements and fuzzy
      semantic understanding, all within a single SQLite database.
    </p>

    <h2>When to use this approach</h2>

    <p>
      This works well when the dataset size is small (let&#39;s say &lt;10M rows on
      beefy hardware), or even more if queries are infrequent enough that O(n) +
      sort time latency is acceptable, and if you want to avoid external vector
      database dependencies.
    </p>
  </article>
</div></div>
  </body>
</html>
