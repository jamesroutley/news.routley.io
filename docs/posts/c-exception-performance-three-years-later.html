<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://databasearchitects.blogspot.com/2024/12/c-exception-performance-three-years.html">Original</a>
    <h1>C&#43;&#43; exception performance three years later</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-3837995605364899281" itemprop="description articleBody">
<p>About three years ago we noticed serious <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2544r0.html">performance problems in C++ exception unwinding</a>. Due to contention on the unwinding path these became more and more severe the more cores a system had, and unwinding could slow down by orders of magnitude. Due to the constraints of backwards compatibility this contention was not easy to eliminate, and <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2544r0.html">P2544</a> discussed ways to fix this problem via language changes in C++.</p><p>But fortunately people found less invasive solutions. First, Florian Weimer <a href="https://sourceware.org/git/?p=glibc.git;a=commit;h=5d28a8962dcb6ec056b81d730e3c6fb57185a210">changed the glibc</a> to provide a lock-free mechanism to find the (static) unwind tables for a given shared object. Which eliminates the most serious contention for &#34;simple&#34; C++ programs. For example in a <a href="https://github.com/neumannt/exceptionperformance/blob/master/main.cpp">micro-benchmark</a> that calls a function with some computations (100 calls to sqrt per function invocation), and which throws with a certain probability, we previously had very poor scalability with increasing core count. With his patch we now see with gcc 14.2 on a dual-socket EPYC 7713 the following performance development (runtime in ms):</p>
<table><thead>
  <tr>
    <th></th>
    <th>1</th>
    <th>2</th>
    <th>4</th>
    <th>8</th>
    <th>16</th>
    <th>32</th>
    <th>64</th>
    <th>128</th>
    <th>threads</th>
  </tr></thead>
<tbody>
  <tr>
    <td>0% failure<br/></td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>42</td>
    <td></td>
  </tr>
  <tr>
    <td>0.1% failure</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>29</td>
    <td>32</td>
    <td></td>
  </tr>
  <tr>
    <td>1% failure</td>
    <td>29<br/></td>
    <td>30<br/></td>
    <td>30<br/></td>
    <td>30</td>
    <td>30</td>
    <td>30</td>
    <td>32</td>
    <td>34</td>
    <td></td>
  </tr>
  <tr>
    <td>10% failure</td>
    <td>36</td>
    <td>36</td>
    <td>37</td>
    <td>37</td>
    <td>37</td>
    <td>37<br/></td>
    <td>47<br/></td>
    <td>65</td>
    <td></td>
  </tr>
</tbody></table>
<p>Which is more or less perfect. 128 threads are a bit slower, but that is to be expected as one EPYC only has 64 cores. With higher failure rates unwinding itself becomes slower but that is still acceptable here. Thus most C++ programs are just fine.</p><p>For <a href="http://umbra-db.com">our use case</a> that is not enough, though. We dynamically generate machine code at runtime, and we want to be able to pass exceptions through generated code. The _dl_find_object mechanism of glibc is not used for JITed code, instead libgcc maintains its own lookup structure. Historically this was a simple list with a global lock, which of course had terrible performance. But through <a href="https://databasearchitects.blogspot.com/2022/06/making-unwinding-through-jit-ed-code.html">a series of patches</a> we managed to change libgcc into using a <a href="https://github.com/gcc-mirror/gcc/blob/master/libgcc/unwind-dw2-btree.h">lock-free b-tree</a> for maintaining the dynamic unwinding frames. Using a similar experiment to the one above, but now with JIT-generated code (using LLVM 19), we get the following:</p>

<table><thead>
  <tr>
    <th></th>
    <th>1</th>
    <th>2</th>
    <th>4</th>
    <th>8</th>
    <th>16</th>
    <th>32</th>
    <th>64</th>
    <th>128</th>
    <th>threads</th>
  </tr></thead>
<tbody>
  <tr>
    <td>0% failure<br/></td>
    <td>32</td>
    <td>38</td>
    <td>48</td>
    <td>64</td>
    <td>48</td>
    <td>36</td>
    <td>59</td>
    <td>62</td>
    <td></td>
  </tr>
  <tr>
    <td>0.1% failure</td>
    <td>32</td>
    <td>32</td>
    <td>32</td>
    <td>32<br/></td>
    <td>32<br/></td>
    <td>48<br/></td>
    <td>62</td>
    <td>68<br/></td>
    <td></td>
  </tr>
  <tr>
    <td>1% failure</td>
    <td>41<br/></td>
    <td>40<br/></td>
    <td>40<br/></td>
    <td>40</td>
    <td>53<br/></td>
    <td>69<br/></td>
    <td>80<br/></td>
    <td>83<br/></td>
    <td></td>
  </tr>
  <tr>
    <td>10% failure</td>
    <td>123</td>
    <td>113</td>
    <td>103<br/></td>
    <td>116<br/></td>
    <td>128</td>
    <td>127<br/></td>
    <td>131<br/></td>
    <td>214<br/></td>
    <td></td>
  </tr>
</tbody></table>
<p>The numbers have more noise than for statically generated code, but overall observation is the same: Unwinding now scales with the number of cores, and we can safely use C++ exceptions even on machines with large core counts.</p><p>So is everything perfect now? No. First, only gcc has a fully scalable frame lookup mechanism. clang has its own implementation, and as far as I know it still does not scale properly due to a global lock in DwarfFDECache. Note that at least on many Linux distributions clang uses libgcc by default, thus the problem is not immediately obvious there, but a pure llvm/clang build will have scalability problems. And  second unwinding through JIT-ed code is a quite a bit slower, which is unfortunate. But admittedly the problem is less severe than shown here, the benchmark with JITed code simply has a stack frame more to unwind due to the way static code and JITed code interact. And it makes sense to prioritize static unwinding over dynamic unwinding frames, as most people never JIT-generate code.</p><p>Overall we are now quite happy with the unwinding mechanism. The bottlenecks are gone, and performance is fine even with high core counts. It is still not appropriate for high failure rates, something like <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0709r4.pdf">P709</a> would be better for that, but we can live with the status quo.</p>

</div></div>
  </body>
</html>
