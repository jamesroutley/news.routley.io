<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/usefulsensors/qc_npu_benchmark">Original</a>
    <h1>AI PCs Aren&#39;t Good at AI: The CPU Beats the NPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><a id="user-content-benchmarking-qualcomms-npu-on-the-microsoft-surface-tablet" aria-label="Permalink: Benchmarking Qualcomm&#39;s NPU on the Microsoft Surface Tablet" href="#benchmarking-qualcomms-npu-on-the-microsoft-surface-tablet"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">TL;DR - We see 1.3% of Qualcomm&#39;s NPU 45 Teraops/s claim when benchmarking Windows AI PCs</p>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#installation">Installation</a>
<ul dir="auto">
<li><a href="#python">Python</a></li>
<li><a href="#cmake">Cmake</a></li>
<li><a href="#visual-studio">Visual Studio</a></li>
<li><a href="#pip-packages">Pip Packages</a></li>
</ul>
</li>
<li><a href="#benchmark">Benchmark</a>
<ul dir="auto">
<li><a href="#running">Running</a></li>
<li><a href="#understanding-the-output">Understanding the Output</a></li>
<li><a href="#what-the-benchmark-measures">What the Benchmark Measures</a></li>
<li><a href="#possible-confounding-factors">Possible Confounding Factors</a>
<ul dir="auto">
<li><a href="#compute-bound">Compute Bound</a></li>
<li><a href="#power-settings">Power Settings</a></li>
<li><a href="#model-topology">Model Topology</a></li>
<li><a href="#configuration-errors">Configuration Errors</a></li>
<li><a href="#onnx-framework">Onnx Framework</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#interpreting-the-results">Interpreting the Results</a></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Microsoft now offers Surface tablets that run Windows on a Qualcomm Arm-based
SoC. These are marketed as AI PCs, due to their ability to run machine learning
models faster and more efficiently than other systems. We are fans of
Qualcomm&#39;s hardware, and its NPU in particular, so we&#39;ve invested a lot of time
and resources into porting our third-party app to this plaform.</p>
<p dir="auto">Unfortunately there  aren&#39;t many code examples or benchmarks available to
demonstrate how to achieve fast results as an external developer, so we&#39;ve put
together a small standalone project to show the performance we&#39;re seeing. It&#39;s
significantly below what we&#39;d hoped for, so we&#39;re publishing this benchmark to
see if we can get ideas on how to achieve lower latency. I&#39;m hopeful there will
be software changes, either at the application, framework, or driver level,
that will improve these results in the future, since I&#39;ve seen the underlying
hardware perform very effectively on other platforms like Android.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Python</h3><a id="user-content-python" aria-label="Permalink: Python" href="#python"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We&#39;re using Python to run our test scripts, and on Windows <a href="https://docs.python.org/3/using/windows.html" rel="nofollow">there are several ways to install the language</a>.
As of October 2nd, 2024, the Python available on the Microsoft Store doesn&#39;t
support the Arm architecture, and so it&#39;s not suitable for running the packages
we need to access Qualcomm&#39;s NPU. Instead, you should use <a href="https://www.python.org/downloads/" rel="nofollow">the official Python dot org installer</a>.
For the results reported here I used <a href="https://www.python.org/ftp/python/3.11.9/python-3.11.9-arm64.exe" rel="nofollow">version 3.11.9</a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Cmake</h3><a id="user-content-cmake" aria-label="Permalink: Cmake" href="#cmake"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We&#39;ll also need the cmake build tool to compile Onnx (since prebuilt packages
aren&#39;t yet available for Windows on Arm). To do this I ran the following
command from a Powershell:</p>
<div data-snippet-clipboard-copy-content="winget install cmake"><pre><code>winget install cmake
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Visual Studio</h3><a id="user-content-visual-studio" aria-label="Permalink: Visual Studio" href="#visual-studio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The build process also requires Visual Studio for the compiler. Download Visual
Studio Community Edition (not Code!) from <a href="https://visualstudio.microsoft.com/downloads/" rel="nofollow">visualstudio.microsoft.com/downloads/</a>.</p>
<p dir="auto">During the installation you will be prompted to select <code>Workload</code> from several options: select <code>Desktop C++ Development</code> checkbox then press install.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Pip Packages</h3><a id="user-content-pip-packages" aria-label="Permalink: Pip Packages" href="#pip-packages"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can install all the required Python packages by running the following
from within this folder:</p>
<div data-snippet-clipboard-copy-content="py -m pip install -r requirements.txt"><pre><code>py -m pip install -r requirements.txt
</code></pre></div>
<p dir="auto">This includes a couple of custom packages. The first is <a href="https://github.com/petewarden/onnx/tree/rel-1.16.2">my branch of Onnx</a>,
which has <a href="https://github.com/onnx/onnx/pull/6407" data-hovercard-type="pull_request" data-hovercard-url="/onnx/onnx/pull/6407/hovercard">a fix for compiling using the official <code>py</code> launcher</a>
backported to Onnx version 1.16, since the Qualcomm Onnx Runtime doesn&#39;t work
with newer Onnx versions (giving an <code>Unsupported model IR version</code> error).</p>
<p dir="auto">I also grab <a href="https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/7982ae20-ed19-4a35-a362-a96ac99897b7/pypi/download/ort-nightly-qnn/1.20.dev20240928001/ort_nightly_qnn-1.20.0.dev20240928001-cp311-cp311-win_arm64.whl#sha256=3b12e3882d1afadf66c2349b2a167dfcbb9ae7a332dc98e0fd51c101d34ddf6e" rel="nofollow">a nightly build</a>
of <a href="https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html" rel="nofollow">Qualcomm&#39;s Onnx Runtime package</a>.
If you want to install a more recent version, there&#39;s <a href="https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ort-nightly-qnn/" rel="nofollow">a list here</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Benchmark</h2><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Running</h3><a id="user-content-running" aria-label="Permalink: Running" href="#running"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To execute the benchmark, run:</p>
<div data-snippet-clipboard-copy-content="py benchmark_matmul.py"><pre><code>py benchmark_matmul.py
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Understanding the Output</h3><a id="user-content-understanding-the-output" aria-label="Permalink: Understanding the Output" href="#understanding-the-output"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The Onnx runtime initially generates a lot of log spam, including:</p>
<div data-snippet-clipboard-copy-content="Error in cpuinfo: Unknown chip model name &#39;Snapdragon(R) X 12-core X1E80100 @ 3.40 GHz&#39;.
Please add new Windows on Arm SoC/chip support to arm/windows/init.c!
unknown Qualcomm CPU part 0x1 ignored"><pre><code>Error in cpuinfo: Unknown chip model name &#39;Snapdragon(R) X 12-core X1E80100 @ 3.40 GHz&#39;.
Please add new Windows on Arm SoC/chip support to arm/windows/init.c!
unknown Qualcomm CPU part 0x1 ignored
</code></pre></div>
<p dir="auto">and</p>
<div data-snippet-clipboard-copy-content="Starting stage: Finalizing Graph Sequence
Completed stage: Finalizing Graph Sequence (115919 us)
Starting stage: Completion
Completed stage: Completion (1025 us)"><pre><code>Starting stage: Finalizing Graph Sequence
Completed stage: Finalizing Graph Sequence (115919 us)
Starting stage: Completion
Completed stage: Completion (1025 us)
</code></pre></div>
<p dir="auto">After all those messages, you should see the actual benchmark
results at the end, something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="************ Benchmark Results ************
NPU quantized compute, float I/O accuracy difference is 0.0100
NPU quantized compute and I/O accuracy difference is 0.0060
CPU took 8.42ms, 821,141,860,688 ops per second
NPU (quantized compute, float I/O) took 30.63ms, 225,667,671,183 ops per second
NPU (quantized compute and I/O) took 12.05ms, 573,475,650,364 ops per second"><pre><span>************</span> Benchmark Results <span>************</span>
NPU quantized compute, float I/O accuracy difference is 0.0100
NPU quantized compute and I/O accuracy difference is 0.0060
CPU took 8.42ms, 821,141,860,688 ops per second
NPU (quantized compute, float I/O) took 30.63ms, 225,667,671,183 ops per second
NPU (quantized compute and I/O) took 12.05ms, 573,475,650,364 ops per second</pre></div>
<p dir="auto">The first two lines confirm that the numerical results of the operations match
between the CPU and the NPU. The final three show the latency of the three
approaches to running a simple model. The latency is the wall time it took to
execute the model from start to finish, and the ops per second is calculated
from that latency to indicate the equivalent computational throughput.</p>
<p dir="auto">In this example, we see the CPU is capable of running 821 billion ops/second
(821 Gigaops), the first NPU approach gives us 225 Gigaops, and the second 573
Gigaops.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">What the Benchmark Measures</h3><a id="user-content-what-the-benchmark-measures" aria-label="Permalink: What the Benchmark Measures" href="#what-the-benchmark-measures"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This benchmark is designed to resemble some real world models we depend on,
running 6 large matrix multiplications that are similar to the most
time-consuming layers in transformer models like OpenAI&#39;s Whisper. The shapes
are (6, 1500, 256) X (6, 256, 1500), producing a (6, 1500, 1500) result. The
model we running consists of a single MatMul node with two inputs and one
output.</p>
<p dir="auto">The models are created on the fly using the Onnx model framework, and then fed
into the Onnx runtime. The control model is a pure float version that runs
entirely on the CPU.</p>
<p dir="auto">The NPU mostly requires quantized models to run effectively (though it has
limited support for float16). The first approach we took to quantization used
<a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#static-quantization" rel="nofollow">the official ORT <code>quantize_static()</code> method</a>.
For convenience this leaves the input and output tensors in 32-bit float and
performs runtime conversions at the start and end of the graph so that the rest
of the computation happens in eight-bit.</p>
<p dir="auto">Unfortunately we discovered that the conversion operations as implemented on
the NPU were extremely slow, much slower than the main matrix multiplication
in fact. You can see the results in the <code>npu_quant_profile.csv</code> file in this
repository, with conversions taking over 75% of the time.</p>
<p dir="auto">To work around this, we constructed an equivalent model graph programmatically
with eight-bit inputs and outputs This is the second &#34;quantized compute and
I/O&#34; approach mentioned in the results. This is usually around three times
faster than the float I/O version, and profiling shows most of the time is
going on the matrix multiplication, as we&#39;d hope.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Possible Confounding Factors</h3><a id="user-content-possible-confounding-factors" aria-label="Permalink: Possible Confounding Factors" href="#possible-confounding-factors"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">There are a lot of variables involved in measuring performance. Here are some
of the assumptions we&#39;ve made:</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Compute Bound</h4><a id="user-content-compute-bound" aria-label="Permalink: Compute Bound" href="#compute-bound"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Modern transformer models are based around large matrix multiplications, unlike
older convolutional models. One potential issue is that accelerators could
become memory bound if the layers start to resemble matrix times vectors, since
that doesn&#39;t allow reuse of many of the weights, and performance becomes bottle
necked on fetching values from DRAM. We&#39;ve tried to avoid that by making both
the input matrices more square, so that tiling and reuse should be possible.</p>
<p dir="auto">The original matrices from the tiny Whisper model had a k dimension of only 64,
so in case that was too small we bumped it up to 256 in this benchmark to give
as much room for SIMD optimizations as possible.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Power Settings</h4><a id="user-content-power-settings" aria-label="Permalink: Power Settings" href="#power-settings"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Windows has a lot of different configuration options around energy usage, so we
tried to ensure that all of the settings were on &#34;Best Performance&#34; and that we
ran the benchmark with the tablet connected to mains power. There&#39;s also a
session option on the Qualcomm Onnx Runtime, <code>htp_performance_mode</code>, that we
set to <code>sustained_high_performance</code>, since that seemed to give the lowest
overall latency in our experiments.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Model Topology</h4><a id="user-content-model-topology" aria-label="Permalink: Model Topology" href="#model-topology"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We wanted to create a graph of operations that reflected modern AI models, but
was simple enough to easily interpret. We could have added multiple layers, or
used convolutions, or static weights, but settled for a single matrix
multiplication operation with dynamic inputs, since that reflected the
transformer architectures that are widely used for LLMs and other modern
models.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Configuration Errors</h4><a id="user-content-configuration-errors" aria-label="Permalink: Configuration Errors" href="#configuration-errors"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">It&#39;s possible that the way we build and run our models causes them to fall off
the fast path of the drivers or accelerator implementation. For example, we&#39;re
using unsigned eight-bit quantization, with qdq elements in the graph. We&#39;ve
attempted to follow best practice from the documentation, but we&#39;d welcome ways
to improve performance, especially since these would improve the performance of
our actual applications.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Onnx Framework</h4><a id="user-content-onnx-framework" aria-label="Permalink: Onnx Framework" href="#onnx-framework"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">There are multiple different ways to access AI acceleration on Windows. We
looked at DirectML, but it only seems to support GPU access. OpenVino doesn&#39;t
run on our Arm hardware, as far as we can tell. We&#39;ve seen similar performance
results to those shown here using the <a href="https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai" rel="nofollow">Qualcomm QNN SDK</a>
directly. TensorFlow Lite isn&#39;t supported on Windows for Arm. From this
research and our experiments, Onnx is supported by both Microsoft and Qualcomm,
and seems to be the best framework to use to get accelerated performance from
the NPU, but we&#39;re interested in learning if other APIs would be more
appropriate.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Interpreting the Results</h2><a id="user-content-interpreting-the-results" aria-label="Permalink: Interpreting the Results" href="#interpreting-the-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The results shown here are current as of October 2nd, 2024, when running on a
Microsoft Surface Pro 11th Edition, with a Snapdragon(R) X 12-core X1E80100
clocked at 3.40 GHz. The first obvious thing is that the NPU results, even
without float conversion, are slower than the CPU. This is not ideal for an
accelerator, even though it could still potentially offer energy or sustained
performance advantages that make it worth using.</p>
<p dir="auto">The second conclusion is that the measured performance of 573 billion
operations per second is only 1.3% of the 45 trillion ops/s that <a href="https://www.microsoft.com/en-us/surface/devices/surface-pro-11th-edition" rel="nofollow">the marketing material</a>
promises.</p>
<p dir="auto">By contrast, running the same model on an Nvidia Geforce RTX 4080 Laptop GPU
runs in 3.2ms, an equivalent of 2,160 billion operations per second, almost
four times the throughput.</p>
</article></div></div>
  </body>
</html>
