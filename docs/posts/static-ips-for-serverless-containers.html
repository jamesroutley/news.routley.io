<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://modal.com/blog/vprox">Original</a>
    <h1>Static IPs for Serverless Containers</h1>
    
    <div id="readability-page-1" class="page"><div>  <div><p data-svelte-h="svelte-3sbs4s">At Modal, we built a high-availability, Go-based VPN proxy called <em>vprox</em>.</p> <p>This is a deployment of <a rel="nofollow" href="https://www.wireguard.com/">WireGuard</a>, so it operates
on Layer 3 (IP) of the network stack and allows us to funnel outbound traffic
from containers around the world through static IPv4 addresses. In the event of
a single-node failure, its static IPs are associated with other proxy nodes, and
containers reconnect within seconds.</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-1.svg" alt="Map of Modal containers in different places funneling through a static IP proxy with WireGuard tunnels"/> </p> <p>This blog post is about the guts of our network infrastructure, which powers
<a href="https://modal.com/docs/guide/proxy-ips">Static IP Proxies</a>.</p> <h2 id="scenario" data-svelte-h="svelte-15axy86">Scenario</h2> <p data-svelte-h="svelte-15b9ky8">The year is 2024, and you are deciding on a serverless cloud platform. You
stumble upon Modal. Run <code>pip install modal</code>, write a short Python function, and
<code>modal deploy</code> it. Amazing, now you‚Äôve got a cron job and API endpoint in the
cloud, within seconds.</p>  <p>Modal functions run on hardware around the world, in
<a href="https://modal.com/blog/region-selection-launch">dozens of regions</a> across multiple cloud
providers. This is how we optimize the prices on your compute and scale
dynamically to meet demand. It‚Äôs all to make developers happy, since now you
don‚Äôt have to think about this stuff. (We get it, we‚Äôre infrastructure
engineers.)</p> <p data-svelte-h="svelte-13b1bl8">But now let‚Äôs say you want to connect your serverless function to your MongoDB
cloud database, and it requires a specific IP access list. Uh oh‚Ä¶</p> <center data-svelte-h="svelte-jd7vgl"><img src="https://modal-cdn.com/cdnbot/tmpr5_wsi7u_2639b00c.webp" alt="Edit IP Access List Entry" width="600"/></center> <p data-svelte-h="svelte-aezq97">Usually, with a traditional provider you‚Äôd deploy some VMs and assign them a
static IP address or two, then distribute them across your machines and add
those to your access list. So now your application runs on cloud hosts at some
particular IPs, like <code>20.21.20.21</code>. Only these machines can access your MongoDB
database, and no one else can around the world.</p> <p data-svelte-h="svelte-1vqftv1">But if you‚Äôre running a serverless computing workload, which can not only run in
any data center around the world, but also scale up and down‚Ä¶ you won‚Äôt know
what IP address your code is running on! So that access list would have
thousands of entries and will be constantly changing, which really isn‚Äôt going
to cut it.</p> <p data-svelte-h="svelte-1lizj1k">Plus, Modal has an isolated container runtime that lets us share each host‚Äôs CPU
and memory between workloads. If a host has one IP, your container and
another customer‚Äôs container on that host would have the same IP, so that
bypasses the security of your access list.</p> <h2 id="okay-but-i-want-to-access-my-database-from-modal" data-svelte-h="svelte-3g7uaz">Okay but I want to access my database from Modal</h2> <p data-svelte-h="svelte-118b262">So you need a static outbound IP address. Is that possible? Let‚Äôs break it down.
IP addresses act as identifiers for sending and receiving internet data. When
two different containers communicate with a web service like Google, each has a
unique source IP. This allows Google‚Äôs server to reply directly to the correct
container.</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-2.svg" alt="Two containers, both sending data to a server, with different IPs"/> </p> <p data-svelte-h="svelte-n4o15x">In this standard setup, the outbound IP address is tightly coupled to the
container. How can you decouple static IP addresses from the actual compute
resources? There‚Äôs a solution for this: <strong>You use a proxy.</strong></p> <p>We started by adding <a rel="nofollow" href="https://en.wikipedia.org/wiki/SOCKS">SOCKS5</a> proxies to
Modal. SOCKS5 is an unsung hero of the Internet proxy world; it‚Äôs a technology
from 1996 (<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc1928">RFC 1928</a>) that lets
you send a request <em data-svelte-h="svelte-q4pvpt">through</em> another computer. SOCKS5 is secretly built into a
lot of software like OpenSSH, if you pass in the obscure <code data-svelte-h="svelte-hyyf4p">-D</code> flag that enables
the feature. But SOCKS5 doesn‚Äôt work out-of-the-box. You need to edit your
application to use an esoteric network shim.</p> <p data-svelte-h="svelte-1dp56pp">If you wanted containers running your function to use a SOCKS5-based Modal
Proxy, we would define one for your workspace. Once the Proxy object is created,
you can use it like so:</p>  <p data-svelte-h="svelte-coa8qv">Now other functions can call <code>get_user_count()</code> remotely as a serverless
invocation. The proxy is only spun up within the <code>get_user_count()</code> function.</p>  <p data-svelte-h="svelte-12yw1j5">Slick! But this is brittle. Replacing the standard library‚Äôs <code>socket.socket</code>
object doesn‚Äôt work for libraries that don‚Äôt use <code>socket</code> directly. And many
common libraries don‚Äôt, such as asyncpg, datadog, aiohttp, httpx, or grpcio.</p> <p data-svelte-h="svelte-1hwzmna">The broader issue is that the API is not obvious. It passes on the complexity to
the user, who needs to figure out how to wire up the SOCKS5 proxy to their
libraries. Modal is a cloud provider, and our philosophy on developer experience
is that things ‚Äújust work‚Äù ‚Äî we should implement features that are correct and
efficient by design.</p> <p>So we stepped down a layer. Since we own the entire runtime, how can we
configure networking so that <em data-svelte-h="svelte-9ary3k">all outbound Internet</em> <em data-svelte-h="svelte-1mjeabc">access</em> goes through that
IP? You should be able to just send a simple request to
<a rel="nofollow" href="https://ifconfig.me">ifconfig.me</a> and get back your proxy‚Äôs IP, for instance.</p> <h2 id="enter-wireguard" data-svelte-h="svelte-1nd9c0t">Enter WireGuard</h2> <p data-svelte-h="svelte-qzj7qn">At Layer 3, the Internet Protocol, all traffic looks the same‚Äîwhether it‚Äôs
Wikipedia, YouTube, or MongoDB. It‚Äôs just MTU-sized packets, usually a few
kilobytes each, traveling through routers to their destinations. To ensure a
consistent source IP address for outbound internet traffic across multiple
containers, we can route all container traffic through a VPN. A VPN not only
encrypts traffic but also can mask its source IP address, achieving the desired
consistency.</p> <p data-svelte-h="svelte-1s32srq">WireGuard is a really simple VPN, and it‚Äôs included in the mainline Linux
kernel. So we just have to bootstrap a WireGuard network between the proxy
server and Modal workers (machines that run containers), configure traffic
routing, and we‚Äôre all set!</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-3.svg" alt="Two containers sending data to a server, with a proxy in the middle"/> </p> <p data-svelte-h="svelte-1r3j2pr">To set up the actual WireGuard network, we start an HTTPS listener on port 443
of the WireGuard server. This binds to the public IP of the proxy node, and it
takes ‚Äúconnect‚Äù POST requests to set up connections to the WireGuard network. We
can rely on the security of TLS to handle VPN key distribution.</p> <p>When a POST request is received with the client‚Äôs public key, the server
validates the credentials of the client, then allocates an IP in the subnet and
adds it as a peer at that IP. The server also has a loop that removes idle peers
after a few minutes of not receiving WireGuard handshakes. We use the
<a rel="nofollow" href="https://github.com/WireGuard/wgctrl-go">wgctrl</a> library for this, which
provides Go bindings for the WireGuard API, including access to the
<code data-svelte-h="svelte-1encs16">LastHandshakeTime</code> property of each peer.</p>  <p data-svelte-h="svelte-b0r7kw">On the client side, we periodically probe the WireGuard VPN connection every few
seconds. This is implemented by the <code>CheckConnection</code> function below. If the
pings fail, the client assumes that the connection is dead, and it then begins
trying to reconnect to the server and recover by sending a new ‚Äúconnect‚Äù POST
request.</p>  <p>This behavior is implemented in our open-source Go package
<a rel="nofollow" href="https://github.com/modal-labs/vprox">vprox</a>, which we‚Äôll talk about more at the
end of this blog post.</p> <h2 id="policy-based-routing-on-container-traffic" data-svelte-h="svelte-i7tp1m">Policy-based routing on container traffic</h2> <p>There‚Äôs still a missing piece to the puzzle: Modal workers are multi-tenant. We
run gVisor sandboxes from multiple functions on the same host, which is what
lets us provide a serverless compute product with <a href="https://modal.com/pricing">flexible pricing</a>.</p> <p data-svelte-h="svelte-dnrj5b">How does container networking work? Well, very briefly:</p> <ul><li>Each worker machine runs multiple containers, and each container gets its own
<em><a rel="nofollow" href="https://man7.org/linux/man-pages/man7/network_namespaces.7.html">network namespace</a></em>.</li> <li data-svelte-h="svelte-adf55u">Inside the network namespace, the container has a <em>veth (virtual Ethernet)
interface</em>. This acts as a virtual network card, similar to the WiFi card on
your laptop.</li> <li>Veth‚Äôs come in pairs. The other half of the veth lives on a
<a rel="nofollow" href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bridge">bridge device</a>
(like a <a rel="nofollow" href="https://en.wikipedia.org/wiki/Network_switch">switch</a>).</li> <li>When containers send outbound packets to the Internet, they pass through the
veth and arrive at the bridge, where the host Linux kernel is configured to
<strong><a rel="nofollow" href="https://en.wikipedia.org/wiki/Network_address_translation">masquerade</a>
each packet‚Äôs source IP address</strong> before exiting the machine.</li></ul> <p>If you didn‚Äôt get all that, it‚Äôs fine! The important part is the last step, IP
masquerade. This is a form of network address translation, or
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Network_address_translation">SNAT</a>. It‚Äôs just
like how your home router uses SNAT to make all devices in your house have the
same public IP address. Each cloud host at Modal uses SNAT so containers running
on that host appear to the outside world to have the host‚Äôs public IP.</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-4.svg" alt="Multiple containers on a host behind a network bridge and masquerade rule"/> </p> <p>This is the classic container networking setup. To introduce WireGuard, we need
to tell traffic from one container to go to a designated WireGuard interface
without affecting its neighbors. This requires an update to the kernel‚Äôs
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Routing_table">routing table</a>. When a container
sends a packet to the outside world, we should inspect the packet‚Äôs source IP
and redirect it to the proper VPN interface based on the container‚Äôs metadata.</p> <p>But people familiar with Linux might see a problem here: the
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Iproute2">iproute2</a> system in Linux doesn‚Äôt
actually let you put down routing table entries by source IP! In Linux, routing
tables are based on
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR blocks</a> of
destination IPs. So you can route packets <em data-svelte-h="svelte-1wkmrfv">to</em> <code data-svelte-h="svelte-5wv3mt">142.251.40.174</code> (google.com)
from all containers, but you can‚Äôt tell packets <em data-svelte-h="svelte-yvfinu">from</em> a specific container to
go through a VPN.</p> <p data-svelte-h="svelte-k4co4f">There‚Äôs a solution, but it means we need to sin a bit. We‚Äôre going to write some
<em>policy-based routing</em> rules. ü´¢</p>  <p>Policy-based routing works by switching between multiple routing tables. The
routing tables in Linux are numbered from 1 to 2^31. So we can assign a routing
table to each container that requests a proxy, allocating indices to avoid
repeats, then
<a rel="nofollow" href="https://www.man7.org/linux/man-pages/man8/ip-rule.8.html">edit the policy database</a>
so that traffic from that container goes through the routing table.</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-5.svg" alt="Diagram of container traffic through a policy database and route table"/> </p> <p data-svelte-h="svelte-1xhepix">We really <em>didn‚Äôt</em> want to do this, it‚Äôs tricky! For example, what happens if
two containers start up at the same time? They need to synchronize and decide
which one gets the next numbered routing table. And if a container crashes
early, it adds another kernel resource to clean up. Dynamically configuring the
policy database was not our first choice of solution.</p> <p>(Technical detail: As an alternative approach, we tried doing it in eBPF first
with <a rel="nofollow" href="https://docs.kernel.org/bpf/redirect.html"><code data-svelte-h="svelte-vmildm">xdp_redirect()</code></a> in our packet
filter attached to the container bridge device. But this didn‚Äôt work. It was
incompatible with SNAT because eBPF skips the Linux netfilter stack.)</p> <p data-svelte-h="svelte-bpoc2p">Luckily, our runtime is pretty resilient to unexpected crashes (it‚Äôs written in
safe Rust, with testing, monitoring and conscious async-oriented design), and
overall we haven‚Äôt run into any reliability issues with our implementation so
far.</p> <p data-svelte-h="svelte-dt4uos">That concludes our worker-side implementation of the proxy! Now, back to the
server‚Ä¶</p> <h2 id="so-you-have-a-proxy-server-running-for-every-ip" data-svelte-h="svelte-eppjj4">So you have a proxy server running for every IP?</h2> <p data-svelte-h="svelte-gb5fyl">We did initially! But starting one cloud VM for every proxy server is pretty
expensive, and it‚Äôs not very efficient on resource utilization. The entire point
of serverless is shared tenancy, after all. To make this faster and more
reliable, we started assigning multiple IPs to each proxy server, so that one
unit of shared hardware could manage all of these associations.</p> <p data-svelte-h="svelte-1ovxg7s">Each <code>vprox server</code> node has one or more IPs living on one or more network
interfaces. For example, on AWS, the latest <code>c7gn.8xlarge</code> instance type ($2.00
/ hr) with 100 Gigabit networking can have up to 8 network interfaces, with 30
IPv4 addresses per interface. This is a pretty good deal ‚Äî at full packing of
240 IPs, each costs less than $0.01 / hr while also allowing for individual
proxies to burst up to 100 Gbps of shared bandwidth.</p> <p>To avoid contention and control the bandwidth used by different IP proxies on
the same server, we can use the
<a rel="nofollow" href="https://man7.org/linux/man-pages/man8/tc.8.html">tc traffic shaping system</a> in
Linux.</p> <h3 id="juggling-ips-between-servers" data-svelte-h="svelte-6rhytx">Juggling IPs between servers</h3> <p data-svelte-h="svelte-n2rfhb">We didn‚Äôt just stop there though. We wrote some code that hits the cloud
instance metadata endpoint and detects within a couple seconds if you made any
changes to the IP addresses associated with the instance. If you did, <code>vprox</code>
automatically reconfigures itself to reallocate blocks of WireGuard IPs, move
around connections, bootstrap WireGuard interfaces, and start accepting
connections from clients to the new IP address.</p> <p>This may seem like overengineering, but it reduces the amount of configuration
for <code data-svelte-h="svelte-133xquj">vprox</code> and makes the server significantly more flexible. Plus, it‚Äôs
fault-tolerant by design!
<a rel="nofollow" href="https://queue.acm.org/detail.cfm?id=2898444">Reconciliation loops</a> are the
hidden heroes of distributed systems, as we all know.</p> <p data-svelte-h="svelte-v6gysk">When enabled, the network proxy is on the hot path of every request from a
serverless function, so high availability is crucial. You wouldn‚Äôt want your API
to start failing because you can‚Äôt connect to MongoDB anymore due to the <em>one</em>
proxy instance going down! So we implemented another reconciliation loop,
globally, that creates many servers and juggles the IPs around in event of a
termination.</p> <p> <img src="https://modal-cdn.com/vprox-blog-post/vprox-6.svg" alt="Failover of one proxy node causing IP reassignment"/> </p> <p data-svelte-h="svelte-120w2ex">This can happen if the compute instance becomes unhealthy or needs to be taken
down for maintenance for any reason. The <code>vprox</code> client is also designed to
detect network partitions by periodically sending pings, and when it detects
that it has disconnected, we can automatically recover the connection to the new
server in under 10 seconds.</p> <p data-svelte-h="svelte-e7pm6o">Since IP is an inherently unreliable and unordered protocol, you‚Äôll probably
never even notice if your proxy goes down! Even if you‚Äôre running an HTTP
request at that exact moment, it will just result in a few dropped packets,
which are automatically retried at the TCP layer. No database errors for you ‚Äî a
perfect recovery.</p> <h3 id="what-is-rp_filter-anyway" data-svelte-h="svelte-1kqfb28">What is rp_filter anyway?</h3> <p>(We‚Äôre going to get into <a rel="nofollow" href="https://en.wikipedia.org/wiki/Sysctl">sysctl</a> here.
Sysctl is a way to configure attributes of the Linux kernel. Think of it like an
OS-wide configuration file.)</p> <p data-svelte-h="svelte-kuqel1">When testing <code>vprox</code> on different distributions of Linux, we ran into a problem
that we had to debug. Specifically, it was tested to be working on Ubuntu 24.04,
but it didn‚Äôt seem to be working on Oracle Linux 9. What happened? WireGuard is
part of the kernel, and iptables / iproute2 are supported by both distributions,
so this should be cross-platform.</p> <p>The issue turned out to be caused by a feature called <em data-svelte-h="svelte-h1xfux">reverse path filtering</em>.
Basically, the sysctl
<a rel="nofollow" href="https://sysctl-explorer.net/net/ipv4/rp_filter/"><code data-svelte-h="svelte-bvzrnk">net.ipv4.conf.all.rp_filter</code></a>
controls whether IP packets received on an interface are dropped. If strict
filtering is set, Linux will drop packets whose source address doesn‚Äôt appear to
match the path in the routing table that would otherwise be used to send packets
to that destination.</p> <p data-svelte-h="svelte-1mynh39">Since we‚Äôre sending packets to all kinds of public Internet sources through
these WireGuard interfaces, when they return on the interface, the kernel isn‚Äôt
happy about their source IP and drops them. It detects that a more ‚Äúdirect‚Äù path
would be to go through the default interface on the host instead. We need to
relax rp_filter.</p> <p data-svelte-h="svelte-gmy1dn">Curiously, when we disabled the rp_filter setting by setting it to 0, vprox
didn‚Äôt work. We had to explicitly set it to 2, which is ‚Äúloose mode‚Äù that checks
the incoming packet against the kernel‚Äôs FIB (forwarding information bus).</p>  <p data-svelte-h="svelte-6q1f9f">Honestly, I don‚Äôt know why vprox only works when reverse path filtering is in
loose mode and not when it is disabled. But we switched the value of the sysctl,
and now it works reliably across Linux distributions.</p> <h2 id="using-vprox" data-svelte-h="svelte-1uk36qr">Using vprox</h2> <p>If you‚Äôre a developer on Modal, you can get access to our
<a href="https://modal.com/docs/guide/proxy-ips">static IP proxies</a> feature on the Team plan. Just create
a proxy and voil√†, you‚Äôve got an outbound IP. No SOCKS5 required!</p>  <p data-svelte-h="svelte-isox7e">Right now each Proxy corresponds to a single static IP address, but we‚Äôre
planning to extend this to region-specific proxies where your container may
automatically select an IP from the nearest geographic location to minimize
latency.</p> <p>But this blog post is about the internals, and as mentioned before, we
open-sourced our control plane ‚Äî how we run WireGuard in production and
integrate it into the Modal serverless function runtime. You can find this in
the <a rel="nofollow" href="https://github.com/modal-labs/vprox">modal-labs/vprox</a> repository on
GitHub. With just a couple commands, you can run a VPN server and any number of
clients. All aspects of the networking are configurable.</p> <p data-svelte-h="svelte-1s8dxqa">A nice thing about this implementation is IP discovery. On AWS, we periodically
poll the instance metadata endpoint to find the IPs attached, so you don‚Äôt have
to update this manually. Just run <code>vprox server --cloud aws</code> and watch the magic
happen. (It should be easy to port this code to other cloud providers, but we‚Äôve
only tried deploying on AWS ourselves.)</p> <hr/> <p data-svelte-h="svelte-19au1nm">We‚Äôre excited to see how you use static IPs at Modal! This project has been fun
for many of us. I‚Äôm grateful to my coworker Luis Capelo for deploying vprox
in production, and to our intern Jeffrey Meng for implementing IP discovery and
client reconnection.</p> <p>If you‚Äôre interested in crafting reliable, secure systems at scale for the next
generation of cloud infrastructure, <a href="https://modal.com/careers">we‚Äôre hiring</a> at Modal.</p> </div></div></div>
  </body>
</html>
