<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/2noise/ChatTTS">Original</a>
    <h1>ChatTTS-Best open source TTS Model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://github.com/2noise/ChatTTS/blob/main/README.md"><strong>English</strong></a> | <a href="https://github.com/2noise/ChatTTS/blob/main/README_CN.md"><strong>中文简体</strong></a></p>
<p dir="auto">ChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on HuggingFace is a 40,000 hours pre trained model without SFT.</p>
<p dir="auto">For formal inquiries about model and roadmap, please contact us at <a href="mailto:open-source@2noise.com">open-source@2noise.com</a>. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.</p>
<hr/>

<ol dir="auto">
<li><strong>Conversational TTS</strong>: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.</li>
<li><strong>Fine-grained Control</strong>: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections.</li>
<li><strong>Better Prosody</strong>: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.</li>
</ol>
<p dir="auto">For the detailed description of the model, you can refer to <a href="https://www.bilibili.com/video/BV1zn4y1o7iV" rel="nofollow">video on Bilibili</a></p>
<hr/>

<p dir="auto">This repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.</p>
<p dir="auto">ChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.</p>
<hr/>


<div dir="auto" data-snippet-clipboard-copy-content="import ChatTTS
from IPython.display import Audio

chat = ChatTTS.Chat()
chat.load_models()

texts = [&#34;&lt;PUT YOUR TEXT HERE&gt;&#34;,]

wavs = chat.infer(texts, use_decoder=True)
Audio(wavs[0], rate=24_000, autoplay=True)"><pre><span>import</span> <span>ChatTTS</span>
<span>from</span> <span>IPython</span>.<span>display</span> <span>import</span> <span>Audio</span>

<span>chat</span> <span>=</span> <span>ChatTTS</span>.<span>Chat</span>()
<span>chat</span>.<span>load_models</span>()

<span>texts</span> <span>=</span> [<span>&#34;&lt;PUT YOUR TEXT HERE&gt;&#34;</span>,]

<span>wavs</span> <span>=</span> <span>chat</span>.<span>infer</span>(<span>texts</span>, <span>use_decoder</span><span>=</span><span>True</span>)
<span>Audio</span>(<span>wavs</span>[<span>0</span>], <span>rate</span><span>=</span><span>24_000</span>, <span>autoplay</span><span>=</span><span>True</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="###################################
# Sample a speaker from Gaussian.
import torch
std, mean = torch.load(&#39;ChatTTS/asset/spk_stat.pt&#39;).chunk(2)
rand_spk = torch.randn(768) * std + mean

params_infer_code = {
  &#39;spk_emb&#39;: rand_spk, # add sampled speaker 
  &#39;temperature&#39;: .3, # using custom temperature
  &#39;top_P&#39;: 0.7, # top P decode
  &#39;top_K&#39;: 20, # top K decode
}

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = {
  &#39;prompt&#39;: &#39;[oral_2][laugh_0][break_6]&#39;
} 

wav = chat.infer(&#34;&lt;PUT YOUR TEXT HERE&gt;&#34;, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
# For word level manual control.
text = &#39;What is [uv_break]your favorite english food?[laugh][lbreak]&#39;
wav = chat.infer(text, skip_refine_text=True, params_infer_code=params_infer_code)
"><pre><span>###################################</span>
<span># Sample a speaker from Gaussian.</span>
<span>import</span> <span>torch</span>
<span>std</span>, <span>mean</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>&#39;ChatTTS/asset/spk_stat.pt&#39;</span>).<span>chunk</span>(<span>2</span>)
<span>rand_spk</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>768</span>) <span>*</span> <span>std</span> <span>+</span> <span>mean</span>

<span>params_infer_code</span> <span>=</span> {
  <span>&#39;spk_emb&#39;</span>: <span>rand_spk</span>, <span># add sampled speaker </span>
  <span>&#39;temperature&#39;</span>: <span>.3</span>, <span># using custom temperature</span>
  <span>&#39;top_P&#39;</span>: <span>0.7</span>, <span># top P decode</span>
  <span>&#39;top_K&#39;</span>: <span>20</span>, <span># top K decode</span>
}

<span>###################################</span>
<span># For sentence level manual control.</span>

<span># use oral_(0-9), laugh_(0-2), break_(0-7) </span>
<span># to generate special token in text to synthesize.</span>
<span>params_refine_text</span> <span>=</span> {
  <span>&#39;prompt&#39;</span>: <span>&#39;[oral_2][laugh_0][break_6]&#39;</span>
} 

<span>wav</span> <span>=</span> <span>chat</span>.<span>infer</span>(<span>&#34;&lt;PUT YOUR TEXT HERE&gt;&#34;</span>, <span>params_refine_text</span><span>=</span><span>params_refine_text</span>, <span>params_infer_code</span><span>=</span><span>params_infer_code</span>)

<span>###################################</span>
<span># For word level manual control.</span>
<span>text</span> <span>=</span> <span>&#39;What is [uv_break]your favorite english food?[laugh][lbreak]&#39;</span>
<span>wav</span> <span>=</span> <span>chat</span>.<span>infer</span>(<span>text</span>, <span>skip_refine_text</span><span>=</span><span>True</span>, <span>params_infer_code</span><span>=</span><span>params_infer_code</span>)</pre></div>
<details open="">
  <summary><div dir="auto"><h4 tabindex="-1" dir="auto">Example: self introduction</h4><a id="user-content-example-self-introduction" aria-label="Permalink: Example: self introduction" href="#example-self-introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary>
<div dir="auto" data-snippet-clipboard-copy-content="inputs_en = &#34;&#34;&#34;
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements [laugh]like like 
[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
&#34;&#34;&#34;.replace(&#39;\n&#39;, &#39;&#39;) # English is still experimental.

params_refine_text = {
  &#39;prompt&#39;: &#39;[oral_2][laugh_0][break_4]&#39;
} 
audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)
audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)"><pre><span>inputs_en</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>chat T T S is a text to speech model designed for dialogue applications. </span>
<span>[uv_break]it supports mixed language input [uv_break]and offers multi speaker </span>
<span>capabilities with precise control over prosodic elements [laugh]like like </span>
<span>[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. </span>
<span>[uv_break]it delivers natural and expressive speech,[uv_break]so please</span>
<span>[uv_break] use the project responsibly at your own risk.[uv_break]</span>
<span>&#34;&#34;&#34;</span>.<span>replace</span>(<span>&#39;<span>\n</span>&#39;</span>, <span>&#39;&#39;</span>) <span># English is still experimental.</span>

<span>params_refine_text</span> <span>=</span> {
  <span>&#39;prompt&#39;</span>: <span>&#39;[oral_2][laugh_0][break_4]&#39;</span>
} 
<span>audio_array_cn</span> <span>=</span> <span>chat</span>.<span>infer</span>(<span>inputs_cn</span>, <span>params_refine_text</span><span>=</span><span>params_refine_text</span>)
<span>audio_array_en</span> <span>=</span> <span>chat</span>.<span>infer</span>(<span>inputs_en</span>, <span>params_refine_text</span><span>=</span><span>params_refine_text</span>)</pre></div>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description intro_en_m.webm">intro_en_m.webm</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/130631963/334550151-e0f51251-db7f-4d39-a0e9-3e095bb65de1.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY5NjcxMTIsIm5iZiI6MTcxNjk2NjgxMiwicGF0aCI6Ii8xMzA2MzE5NjMvMzM0NTUwMTUxLWUwZjUxMjUxLWRiN2YtNGQzOS1hMGU5LTNlMDk1YmI2NWRlMS53ZWJtP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MjlUMDcxMzMyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDkxN2M3ZTM1Mzc4MTE2M2QzYmQ1OGI1M2NmZDY3NzdhMTI2OTU4MjYzMjk3MjhkZTFiOWQ0ODYwYmM2YmYzZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.qyT3tVgYJyqNP2fAeoh80JYk03CVwba2Vxh9BH8o1_A" data-canonical-src="https://private-user-images.githubusercontent.com/130631963/334550151-e0f51251-db7f-4d39-a0e9-3e095bb65de1.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY5NjcxMTIsIm5iZiI6MTcxNjk2NjgxMiwicGF0aCI6Ii8xMzA2MzE5NjMvMzM0NTUwMTUxLWUwZjUxMjUxLWRiN2YtNGQzOS1hMGU5LTNlMDk1YmI2NWRlMS53ZWJtP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MjlUMDcxMzMyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDkxN2M3ZTM1Mzc4MTE2M2QzYmQ1OGI1M2NmZDY3NzdhMTI2OTU4MjYzMjk3MjhkZTFiOWQ0ODYwYmM2YmYzZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.qyT3tVgYJyqNP2fAeoh80JYk03CVwba2Vxh9BH8o1_A" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description intro_en_f.webm">intro_en_f.webm</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/130631963/334550275-f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY5NjcxMTIsIm5iZiI6MTcxNjk2NjgxMiwicGF0aCI6Ii8xMzA2MzE5NjMvMzM0NTUwMjc1LWY1ZGNkZDAxLTEwOTEtNDdjNS04MjQxLWM0ZjZhYWFhOGJiZC53ZWJtP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MjlUMDcxMzMyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTQ2NGUxOGQ4MDQ2ZDkyYzczNGRjMWJiZTIyYzAzMzgzYWVlYWE1YmFlZjM5ZGUxN2Q5N2E5Y2I5MWZkZTllMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.yDse5kPerkrjirofAGF6ojFkF-g8j89cm14KSxQgVCU" data-canonical-src="https://private-user-images.githubusercontent.com/130631963/334550275-f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY5NjcxMTIsIm5iZiI6MTcxNjk2NjgxMiwicGF0aCI6Ii8xMzA2MzE5NjMvMzM0NTUwMjc1LWY1ZGNkZDAxLTEwOTEtNDdjNS04MjQxLWM0ZjZhYWFhOGJiZC53ZWJtP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MjlUMDcxMzMyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTQ2NGUxOGQ4MDQ2ZDkyYzczNGRjMWJiZTIyYzAzMzgzYWVlYWE1YmFlZjM5ZGUxN2Q5N2E5Y2I5MWZkZTllMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.yDse5kPerkrjirofAGF6ojFkF-g8j89cm14KSxQgVCU" controls="controls" muted="muted">

  </video>
</details>

</details>
<hr/>

<ul>
<li> Open-source the 40k hour base model and spk_stats file</li>
<li> Open-source VQ encoder and Lora training code</li>
<li> Streaming audio generation without refining the text*</li>
<li> Open-source the 40k hour version with multi-emotion control</li>
<li> ChatTTS.cpp maybe? (PR or new repo are welcomed.)</li>
</ul>
<hr/>

<div dir="auto"><h5 tabindex="-1" dir="auto">How much VRAM do I need? How about infer speed?</h5><a id="user-content-how-much-vram-do-i-need-how-about-infer-speed" aria-label="Permalink: How much VRAM do I need? How about infer speed?" href="#how-much-vram-do-i-need-how-about-infer-speed"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090D GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.65.</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">model stability is not good enough, with issues such as multi speakers or poor audio quality.</h5><a id="user-content-model-stability-is-not-good-enough-with-issues-such-as-multi-speakers-or-poor-audio-quality" aria-label="Permalink: model stability is not good enough, with issues such as multi speakers or poor audio quality." href="#model-stability-is-not-good-enough-with-issues-such-as-multi-speakers-or-poor-audio-quality"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This is a problem that typically occurs with autoregressive models(for bark and valle). It&#39;s generally difficult to avoid. One can try multiple samples to find a suitable result.</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">Besides laughter, can we control anything else? Can we control other emotions?</h5><a id="user-content-besides-laughter-can-we-control-anything-else-can-we-control-other-emotions" aria-label="Permalink: Besides laughter, can we control anything else? Can we control other emotions?" href="#besides-laughter-can-we-control-anything-else-can-we-control-other-emotions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.</p>
<hr/>

<ul dir="auto">
<li><a href="https://github.com/suno-ai/bark">bark</a>, <a href="https://github.com/coqui-ai/TTS">XTTSv2</a> and <a href="https://arxiv.org/abs/2301.02111" rel="nofollow">valle</a> demostrate a remarkable TTS result by a autoregressive-style system.</li>
<li><a href="https://github.com/fishaudio/fish-speech">fish-speech</a> reveals capability of GVQ as audio tokenizer for LLM modeling.</li>
<li><a href="https://github.com/gemelo-ai/vocos">vocos</a> which is used as a pretrained vocoder.</li>
</ul>
<hr/>

<ul dir="auto">
<li><a href="https://audio.westlake.edu.cn/" rel="nofollow">wlu-audio lab</a> for early algorithm experiments.</li>
</ul>
</article></div></div>
  </body>
</html>
