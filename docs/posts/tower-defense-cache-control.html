<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jasonthorsness.com/26">Original</a>
    <h1>Tower Defense: Cache Control</h1>
    
    <div id="readability-page-1" class="page"><div><p>For those of you who landed here directly and missed my <a href="https://www.jasonthorsness.com/">implausible intro</a>,
this isn‚Äôt a game. It‚Äôs something even better: an article about caching!</p>
<p>I‚Äôll describe the techniques used on my sites <a href="https://www.jasonthorsness.com/">jasonthorsness.com</a> and
<a href="https://hn.unlurker.com">hn.unlurker.com</a>. The latter is open-source so you can
<a href="https://github.com/jasonthorsness/unlurker-web">peruse the implementation</a>.</p>
<p>If you are looking to defend a meager budget with web-exposed side projects from an onslaught of web
traffic you might find these approaches helpful.</p>
<p>And if you are part of the problem, a user among countless others in your horde, driven en masse in
wave after wave against my defenses ‚Äî read on to scout my setup and find out whether or not it‚Äôs up
to the challenge.</p>
<p>The article progresses through three difficulty levels:</p>
<table><thead><tr><th>Difficulty Level</th><th>Description</th></tr></thead><tbody><tr><td><a href="#easy"><strong>Easy</strong></a></td><td>Mostly-static sites</td></tr><tr><td><a href="#medium"><strong>Medium</strong></a></td><td>Data-driven dynamic sites</td></tr><tr><td><a href="#hard"><strong>Hard</strong></a></td><td>Authenticated per-user sites</td></tr></tbody></table>
<h2>Easy: Mostly-Static Sites</h2>
<p>Static means the content is the same for every user and doesn‚Äôt change as a function of time. For
example, due to my sluggish pace of writing, <a href="https://www.jasonthorsness.com/">jasonthorsness.com</a> remains unchanged for weeks in
between updates. Even then, my <a href="https://www.jasonthorsness.com/16">older articles like this one</a> don‚Äôt change unless I adjust a
common layout. For this kind of site, applying content-hashed resources, using a CDN, and keeping
dynamic bits client-side is the dominant industry combination.</p>
<h3>Content-Hashed Resources</h3>
<p>To improve the handling of supporting resources (CSS, JS, images, etc.) a universal practice in
modern web frameworks is to add content hashes to the resource names. By deriving the name of the
file from its contents, you can consider a file with a given name unchanging. For example, on this
site, one of the fonts is at the time of writing served as
<code>/_next/static/media/bd734242e06bd6ad-s.p.woff2</code> with the cache-control header of
<code>public,max-age=31536000,immutable</code>. The <code>bd734242e06bd6ad</code> part of the name is a hash of the file
contents. The CDN and the user‚Äôs browser will cache this file for as long as they want without
worrying about it becoming stale. If I ever change the font, the file will have a different name so
all caches will miss and fetch the new font from the origin.</p>
<p>If you open the network tab in your browser‚Äôs dev tools and refresh this page, you‚Äôll see most
resources are served in this manner and arrive in 0-1 ms from the memory or disk cache. This is
simultaneously the least costly and lowest-latency sort of caching ‚Äî nothing does any work except
the user‚Äôs browser.</p>
<figure><img title="resources served from disk or memory" alt="network" loading="lazy" width="1129" height="489" decoding="async" data-nimg="1" src="https://www.jasonthorsness.com/_next/static/media/content_hash.7350ac34.png"/><p>resources served from disk or memory</p></figure>
<p>üè∞ In Tower Defense lingo: content-hashed static files in the browser cache let you defeat waves of
requests right at the spawn point.</p>
<h3>CDNs</h3>
<p>Resources referenced from outside the site itself, like the path you see in the URL bar, can‚Äôt have
hashes appended because the links would be broken whenever the content changes. The server instead
delivers the response with an ‚ÄúEtag‚Äù header that identifies the current version of the content. When
the browser requests the resource again, it includes the last Etag value in an ‚ÄúIf-None-Match‚Äù
header. Whenever the current ETag on the server matches the If-None-Match value from the browser,
the server is allowed to respond with <code>304 Not Modified</code> rather than the actual content.</p>
<p>If you open the network tab in your browser‚Äôs dev tools once more and refresh this page, you‚Äôll see
<code>/26</code> is served with a <code>304</code> response. You should also see fewer than 2 kiB transferred for the
entire page! If you see more than that, it‚Äôs likely browser extensions you have injecting stuff. Try
it again in Incognito Mode or with a guest profile.</p>
<p>A problem with <code>304 Not Modified</code> is that there‚Äôs still a round-trip to the server. But is it really
a problem? If you look carefully at that network tab, you might see <code>/26</code> served to you in fewer
than ~60 milliseconds. The single origin server for this page is somewhere in the eastern US, but
the reported latency will stay low worldwide. This is because rather than serving from the origin,
the resources are cached and served from a network of delivery points around the globe, commonly
described as a CDN (content-delivery network). This reduces load on the origin and ensures snappy
performance for users all over the world. Low-latency world-wide is important if you respect your
users ‚Äî even this irrelevant blog regularly gets traffic from USA, Europe, and Asia.</p>
<p>This site uses Vercel‚Äôs CDN, which has at the time of this writing
<a href="https://vercel.com/docs/edge-network/regions">119 locations</a> to serve content. Beyond static
resources, you can also run code in these locations to implement
<a href="https://www.jasonthorsness.com/6">custom low-latency functionality</a>.</p>
<figure><img title="low-latency, low-transfer" alt="network" loading="lazy" width="1507" height="271" decoding="async" data-nimg="1" src="https://www.jasonthorsness.com/_next/static/media/304.c3f9f396.png"/><p>low-latency, low-transfer</p></figure>
<p>üè∞ CDN locations are the towers in-between the spawn point and your base. When things are working
correctly they take care of most of the waves.</p>
<h3>What About The Dynamic Parts?</h3>
<p>Even a mostly-static site might have dynamic components. For example, beyond the
<a href="https://www.jasonthorsness.com/6">city search</a> edge function mentioned above, I have a
<a href="https://www.jasonthorsness.com/19">VPS monitoring chart</a>, some
<a href="https://www.jasonthorsness.com/18">DIY analytics</a>, some
<a href="https://www.jasonthorsness.com/20">LLM silliness</a>, and a page that requires users to
<a href="https://www.jasonthorsness.com/24">sign in to compile code changes</a>. All of these require
additional dynamic content that varies by time or user input.</p>
<p>To ensure most content can remain optimized for static delivery, the dynamic parts are all handled
via client-side JavaScript that makes separate API calls to dedicated dynamic API endpoints. This
enables a clean separation of the static and dynamic parts of the site, and also helps prevent
unnecessary triggering of dynamic functionality from crawlers and other bots.</p>
<p>üè∞ Requests for dynamic resources typically must reach the origin. Fortunately in the Tower Defense
metaphor the origin itself is not the end-goal of the creeps. They are after precious resources
inside: CPU cycles and upstream APIs. Continue to the next difficulty level for some ‚Äúinside the
server‚Äù strategies for protection.</p>
<h2>Medium: A Data-Driven Dynamic Site</h2>
<p>A data-driven site has most of its content change automatically over time. This blog is not such a
site, so for this section we‚Äôll look at my recent project
<a href="https://hn.unlurker.com">hn.unlurker.com</a>. Unlurker falls fully into the dynamic category: it
always shows only the latest activity from <a href="https://www.jasonthorsness.com/news.ycombinator.com">Hacker News</a>. The content becomes
stale quickly so caching is a challenge. The Unlurker site uses two layers of caching:</p>
<ul>
<li>short-term cache-control headers</li>
<li>backend memory cache, single-instancing, and disk cache</li>
</ul>
<h3>Short-Term Cache-Control Headers</h3>
<p>Even if you have a data-driven dynamic site, often the content can be treated as static for a short
time. For Unlurker, new comments and stories only appear a few times a minute, so I can apply the
following cache-control header:</p>
<pre><code>public, max-age=15, s-maxage=15, stale-while-revalidate=15
</code></pre>
<ul>
<li>For the first 15 seconds, the content is considered fresh.</li>
<li>For the next 15 seconds, the content is stale but will still be used. The CDN will fetch a fresh
copy in the background.</li>
<li>After 30 seconds, the content is fully stale and will not be used.</li>
</ul>
<p>If you look at the headers from <a href="https://hn.unlurker.com">hn.unlurker.com</a> you‚Äôll only see
<code>public, max-age=15</code> because the CDN strips the rest and handles them internally. To see the effect,
toggle options in the drop-downs back and forth every few seconds. You will see the latency stay at</p>
<p>&lt; ~60 ms forever. The CDN does the expensive refresh from the origin in the background
asynchronously due to the <code>stale-while-revalidate=15</code>. You can inspect the <code>X-Vercel-Cache</code> header
to see whether the content was <code>HIT</code> (fresh) <code>STALE</code> (still used but triggered an asynchronous
refresh) or <code>MISS</code> (fully stale and fetched synchronously from the origin). For me this corresponds
to a latency of 1-2ms for local cache, ~60 ms for hit or stale from the CDN, and likely ~800 ms for
a miss as it goes all the way to my poor VPS and likely the HN API.</p>
<figure><img title="three Vercel cache states" alt="stale-while-revalidate" loading="lazy" width="443" height="142" decoding="async" data-nimg="1" src="https://www.jasonthorsness.com/_next/static/media/stale_while_revalidate.faae036d.png"/><p>three Vercel cache states</p></figure>
<p><code>stale-while-revalidate</code> is a relatively recent cache-control option. It keeps latency low for all
users; nobody ‚Äúpays the price‚Äù for being the first to request after the cache expires. Control over
this header is why I couldn‚Äôt use NextJS for Unlurker ‚Äî NextJS doesn‚Äôt seem to support it for
dynamic pages, and NextJS ISR has a major limitation compared to <code>stale-while-revalidate</code> in that it
doesn‚Äôt support a maximum staleness. For low-traffic sites, users can see hours-old content, which
is unacceptable. I switched to <a href="https://reactrouter.com/">react-router</a> on Vercel which doesn‚Äôt mess
with the headers.</p>
<p>With the options available on hn.unlurker.com, there are only 10 * 12 * 8 * 2 or 1920 possible
combinations, refreshed at most once every 15 seconds, so this technique caps the front-end request
rate to 128 requests per second, regardless of the incoming request rate from user‚Äôs browsers.</p>
<p>The front-end function in this case applies no further caching and each request initiates a single
fetch to the backend API for data.</p>
<p>üè∞ Is the Tower Defense metaphor breaking down yet? Short-term cache control headers are like the
browser-cache and CDN towers we‚Äôve discussed so far, clearing nearly all the creeps, but they
periodically spawn a creep of their own that heads for the origin. Could this be a novel gameplay
mechanic? You read it here first.</p>
<h3>Backend Caches</h3>
<p>The Unlurker backend runs on a shared 2 vCPU VPS. The program running there is the last chance to
protect the CPU cycles and the upstream HN APIs. For dynamic sites like this one, caching and
efficient request handling within the web server is just as important as leveraging browser caching
and CDNs.</p>
<p>The Unlurker backend uses memory caching to protect the CPU cycles, then single instancing and disk
caching to improve performance and protect the HN API.</p>
<h4>Memory Caching</h4>
<p>Anything expensive to compute that might be requested multiple times is useful to keep in a memory
cache. Unlurker maintains a
<a href="https://github.com/jasonthorsness/unlurker-web-backend/blob/b031ee9e6c8bbe09b1d67a88d32deaa64e838ce8/main.go#L36">cache of normalized comment text</a>
for each item. The comments and stories themselves are stored in a memory cache as well for 60
seconds. This makes the cost of a request with no cache misses just a few hash lookups plus the
response serialization.</p>
<h3>Single-Instancing</h3>
<p>Upon a memory-cache miss, the backend needs to fetch the story or comment from the disk cache or
maybe even the HN API. These are relatively expensive operations. To reduce the cost, requests for
the same resource are combined into a single request. This is easy in Go, typically using the
<a href="https://pkg.go.dev/golang.org/x/sync/singleflight">singleflight</a> package but in this case (for good
integration with the memory cache) using a
<a href="https://github.com/jasonthorsness/unlurker/blob/main/hn/core/bulk_single_flight_getter.go">custom implementation</a>.
No matter how many requests come in for the same item concurrently, only one check will be made
against the disk cache and only one request made to the HN API. Unlurker‚Äôs overall load on the HN
API is likely lost in the noise (especially if I‚Äôve created load by convincing enough people to try
<a href="https://www.jasonthorsness.com/25">downloading the whole thing</a>)</p>
<h4>Disk Caching</h4>
<p>Memory caching alone has a couple of issues: space in RAM is limited and process restarts clear the
entire cache. To address these, Unlurker also keeps stories and comments in a
<a href="https://github.com/jasonthorsness/unlurker/blob/main/hn/core/item_file_cache.go">disk cache in the form of a SQLite database</a>.
This is a bit slower than memory, but it effectively has no size limit and survives process
restarts.</p>
<p>Rather than expire items after a fixed 60 seconds, the disk cache uses a ‚Äústaleness‚Äù function based
on the age of the story or comment. It slowly climbs from ~60 seconds for new items to ~30 minutes
for items a few days old, then more rapidly increases until items more than a couple weeks old are
considered immutable.</p>
<p>Requests for items are made in batches, so deriving the expiration from the creation time of items
also helps eliminate clusters of expirations and spreads the requests made to the HN API out over
time.</p>
<p>üè∞ The memory cache, single-instancing, and disk caches are clusters of powerful towers right around
the base taking care of almost all of the remaining creeps. If they are well-chosen, they can handle
an incredible load, and the eventual number that pass to consume significant CPU and initiate
requests to the HN API will be less than your health points. You‚Äôve won!</p>
<h4>Why Not Redis?</h4>
<p>I have a single VPS, so I can get by with a simple SQLite database. If I had many instances of my
API on separate servers, I might at some point want to replace the disk cache with a Redis instance
and <em>might</em> consider using Redis for cross-server single-instancing. But for my site (and probably
most other sites) it‚Äôs way beyond what the situation requires.</p>
<h2>Hard: Authenticated Per-User Sites</h2>
<p>Unfortunately for this article, I‚Äôve only recently begun to add some authenticated features to my
side projects. The article <a href="https://www.jasonthorsness.com/24">LLM not LLVM</a> requires users to authenticate
before they can use an LLM to ‚Äúrecompile‚Äù the examples on the page, but it‚Äôs solely a client-side
function.</p>
<p>For per-user sites, the first step is always to identify and isolate the non-per-user pieces and
serve them with the same techniques as for static and dynamic sites.</p>
<p>Beyond that, for the truly per-user pieces, caching at the edge becomes much more challenging ‚Äî
data is often too sensitive to cache in the CDN, and even if you could, it‚Äôs per-user anyway so
cache hits are rare. The caching solution becomes a partnership between the user‚Äôs browser and the
origin server, which understands the authentication scheme and can cache per-user responses using
the same memory and disk and single-instancing schemes already mentioned.</p>
<p>Strategies that download data to the user‚Äôs browser and handle requests locally can help. This way
many functions over slowly-changing data can be computed on the client, and only deltas need to be
synced from the server.</p>
<p>There are many more approaches ‚Äî maybe my next project should require authentication so I can
explore this difficulty level a bit more.</p>
<h2>Conclusion</h2>
<p>Caching has always been critically important for site performance, and as more-and-more sites depend
on metered APIs (like OpenAI‚Äôs LLMs) and serverless hosting providers (like Vercel) it‚Äôs become just
as important for cost management. Get the cache architecture right and you‚Äôll be surprised at how
far you can stretch a tiny budget with only a few vCPU and a few gigabytes of RAM. Keep in mind, the
sites of the past ran on servers with a miniscule fraction of the resources given to sites today,
and in many cases they probably handled the load far better thanks for more careful caching and
planning.</p>
<p>Thanks for reading! If you have any questions or comments, please reach out to me on
<a href="https://x.com/JasonThorsness">X</a>. If anyone cares to develop Tower Defense: Cache Control, go right
ahead!</p><!--$--><!--/$--><!--$--><!--/$--></div></div>
  </body>
</html>
