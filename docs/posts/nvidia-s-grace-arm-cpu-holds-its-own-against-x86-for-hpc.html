<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nextplatform.com/2024/02/06/nvidias-grace-arm-cpu-holds-its-own-against-x86-for-hpc/">Original</a>
    <h1>Nvidia&#39;s &#34;Grace&#34; Arm CPU holds its own against x86 for HPC</h1>
    
    <div id="readability-page-1" class="page"><article id="post-143603">
	
		<div>
<figure>
<img src="https://www.nextplatform.com/wp-content/uploads/2022/03/nvidia-grace-die-shots.jpg" alt="" title="nvidia-grace-die-shots"/>
</figure>

<p>In many ways, the “Grace” CG100 server processor created by Nvidia – its first true server CPU and a very useful adjunct for extending the memory space of its “Hopper” GH100 GPU accelerators – was designed perfectly for HPC simulation and modeling workloads. And several major supercomputing labs are putting the Grace CPU through the HPC paces and we are seeing some interesting early results.</p>
<p>The Grace CPU has a relatively high core count and a relatively low thermal footprint, and it has banks of low-power DDR5 (LPDDR5) memory – the kind used in laptops but gussied up with error correction to be server class – of sufficient capacity to be useful for HPC systems, which typically have 256 GB or 512 GB per node these days and sometimes less.</p>
<p>Put two Grace CPUs together into a Grace-Grace superchip, a tightly coupled package using NVLink chip-to-chip interconnects that provide memory coherence across the LPDDR5 memory banks and that consumes only around 500 watts, and it gets plenty interesting for the HPC crowd. That yields a total of 144 Arm Neoverse “Demeter” V2 cores with the Armv9 architecture, and 1 TB of physical memory with 1.1 TB/sec of peak theoretical bandwidth. For some reason, probably relating to yield on the LPDDR5 memory, only 960 GB of that memory capacity and only 1 TB/sec of that memory bandwidth is actually available. If Nvidia wanted to do it, it could create a four-way Grace compute module that would be coherent across 288 cores and 1.9 TB of memory with 2 TB/sec of aggregate bandwidth. Such a quad might give an <em>N-1</em> or <em>N-2</em> generation GPU a run for the money. . . .</p>
<p>For reference, we did <a href="https://www.nextplatform.com/2022/03/25/nvidias-grace-arm-server-chip-is-a-game-changer/">our initial analysis on the Grace chip</a> at launch back in March 2022, <a href="https://www.nextplatform.com/2022/08/29/details-emerge-on-nvidias-grace-arm-cpu/">drilled down into the architecture of the Grace chip</a> in August 2022 (when no one was sure what Arm core Nvidia was using as yet), and <a href="https://www.nextplatform.com/2023/09/13/other-than-nvidia-who-will-use-arms-neoverse-v2-core/">went deep into the Demeter V2 core</a> in September 2023 when Arm released details on the architecture. We are not going to get into the architecture all over again but we will remind you that the Arm V2 core that Nvidia adopted for Grace (rather than design its own core) has four 128-bit SVE2 vector engines, making it comparable to the pair of AVX-512 vector engines in an Intel Xeon SP architecture and therefore able to run classic HPC workloads as well as certain AI inference workloads (those that aren’t too fat) and maybe even the retraining of modestly sized AI models.</p>
<p>The data recently published out of the Barcelona Supercomputing Center and the State University of New York campuses in Stony Brook and Buffalo certainly bear this out. Both groups published some benchmark results pitting Grace-Hopper and Grace-Grace superchips on a wide variety of HPC and AI benchmarks, and it shows what we already surmised: if you look at thermals and probably cost, the Grace CPU is going to be able to pull its weight in HPC.</p>
<p>Both organizations published papers out of the <a href="https://sighpc.ipsj.or.jp/HPCAsia2024/">HPC Asia 2024 conference</a> held in Nagoya, Japan last week. The one that came out of BSC is called <em>Nvidia Grace Superchip Early Evaluation for HPC Applications</em>, <a href="https://dl.acm.org/doi/pdf/10.1145/3636480.3637284">which you can read here</a>, and the one from the Stony Brook and Buffalo researchers is called <em>First Impressions of the Nvidia Grace CPU Superchip and Nvidia Grace Hopper Superchip for Scientific Workloads</em>, <a href="https://dl.acm.org/doi/pdf/10.1145/3636480.3637097">which you can read here</a>. Together, the papers present a realistic view of how key HPC applications perform on Grace-Grace and Grace-Hopper superchips. The paper from the SUNY researchers is more useful perhaps because it brings together performance figures from multiple HPC centers and one cloud builder. To be specific, the data from the second paper draws on performance data from Stony Brook, AWS, Pittsburgh Supercomputing Center, Texas Advanced Computing Center, and Purdue University.</p>
<p>BSC compared the performance of the Nvidia Grace-Grace and Grace-Hopper superchips, which are part of the experimental cluster portion of its <a href="https://www.nextplatform.com/2022/06/16/atos-wins-marenostrum-5-deal-at-barcelona-supercomputing-center/">MareNostrum 5 system</a>, against the X86 CPU nodes of the prior MareNostrum 4 supercomputer, which was based on nodes comprised of a pair of 24-core “Skylake” Xeon SP-8160 Platinum processors running at 2.1 GHz. Here is a handy dandy block diagram of the MareNostrum 4 nodes compared to the Grace-Hopper and Grace-Grace nodes:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper.jpg" rel="attachment wp-att-143604"><img fetchpriority="high" decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper.jpg" alt="" width="1130" height="240" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper.jpg 1130w, https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-768x163.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-600x127.jpg 600w" sizes="(max-width: 1130px) 100vw, 1130px"/></a></p>
<p>On the Grace-Hopper nodes, BSC only tested various HPC applications on the CPU portion of the superchip. (The Stony Brook team tested the CPU-CPU pair and the CPU-GPU pair in its evaluation of the early adopter Nvidia systems.)</p>
<p>Here is another handy dandy table that BSC put together comparing the architectures of the three systems tested:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-salients.jpg" rel="attachment wp-att-143605"><img decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-salients.jpg" alt="" width="541" height="414" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-salients.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2024/02/bsc-marenostrum-vs-nvidia-grace-hopper-salients-80x60.jpg 80w" sizes="(max-width: 541px) 100vw, 541px"/></a></p>
<p>BSC says that the early access versions of the Grace processor had CPUs that were geared down to 3.2 GHz and that the memory bandwidth was also geared down from what Nvidia expected that full production units would have. The exact amount was not quantified, but the unit tested had a clock speed of around 3.2 GHz on the Grace CPU.</p>
<p>As for applications, BSC ran its homegrown Alya computational mechanics code as well as the OpenFOAM computational fluid dynamics, the NEMO oceanic climate model, the LAMMPS molecular dynamics model, and the PhysiCell multicellular simulation framework on the three types of nodes. Here is the rundown on how the Grace-Grace nodes compared to the MareNostrum 4 nodes. We are ignoring the Grace-Hopper nodes since the GPUs were not used and since it should be roughly half the performance of the Grace-Grace nodes. Take a look at these speedups when the same number of cores are used:</p>
<ul>
<li>On the Alya application, Grace-Grace was 1.67X faster to 1.81X.</li>
<li>On OpenFOAM, the speedup with Grace-Grace was 4.49X.</li>
<li>On NEMO, the speedup was 2.78X.</li>
<li>On LAMMPS, the speedup was 2.1X to 2.9X for the same number of cores, varying from 1 to 288.</li>
<li>On PhysiCell, the speedup was 3.24X for the same 48 cores on each node.</li>
</ul>
<p>Obviously, the Grace-Grace unit has three times as many cores, so the node-to-node performance should be in proportion to this.</p>
<p>The Stony Brook paper also did a bunch of benchmarks and collected results from other machines, as we pointed out above. Here is the table showing the relative performance of the various nodes running the HPC Challenge (HPCC) benchmark, with the Matrix, LINPACK and FFT elements pulled out separately:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcc.jpg" rel="attachment wp-att-143607"><img decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcc.jpg" alt="" width="1121" height="380" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcc.jpg 1121w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcc-768x260.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcc-600x203.jpg 600w" sizes="(max-width: 1121px) 100vw, 1121px"/></a></p>
<p>It has been a long time since we have seen benchmark data with error bars, which are obviously always present because of the difficulty of making readings and which most tests do not include. Anyway, at the socket level, the Grace-Grace superchip performance somewhere between an Intel “Ice Lake” and “Skylake” Xeon SP and somewhere higher than a “Milan” and “Rome” AMD Epyc. (Beautiful tables, but the way. <em>Thank you</em>.)</p>
<p>On the much tougher High Performance Conjugate Gradients (HPCG) test, which stresses the balance between compute and memory bandwidth and which often makes supercomputers look pathetic, here is how the Grace-Grace superchip stacked up:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcg.jpg" rel="attachment wp-att-143608"><img loading="lazy" decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcg.jpg" alt="" width="750" height="206" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcg.jpg 1014w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcg-768x211.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-hpcg-600x165.jpg 600w" sizes="(max-width: 750px) 100vw, 750px"/></a></p>
<p>Here is how Grace-Grace stacked up on OpenFOAM, using the MotoBikeQ simulation with 11 million cells across all machines:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-openfoam.jpg" rel="attachment wp-att-143609"><img loading="lazy" decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-openfoam.jpg" alt="" width="750" height="158" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-openfoam.jpg 1014w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-openfoam-768x162.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-openfoam-600x127.jpg 600w" sizes="(max-width: 750px) 100vw, 750px"/></a></p>
<p>We would have expected for the Grace-Grace unit to do better here. <em>Hmmm.</em></p>
<p>And finally, here is how the Gromacs molecular dynamics benchmark lined up on the various nodes, with both CPU-GPU and CPU-only variations:</p>
<p><a href="http://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-gromacs.jpg" rel="attachment wp-att-143606"><img loading="lazy" decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-gromacs.jpg" alt="" width="650" height="397" srcset="https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-gromacs.jpg 897w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-gromacs-768x469.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/02/stony-brook-x86-versus-nvidia-grace-hopper-gromacs-600x367.jpg 600w" sizes="(max-width: 650px) 100vw, 650px"/></a></p>
<p>We have a winner! Look at how well that Grace-Hopper combination does. But any CPU paired with the same Hopper GPU would probably do as well. On the CPU-only Grace-Grace unit, the Gromacs performance is almost as potent as a pair of “Sapphire Rapids” Xeon Max Series CPUs. It is noteworthy that the HBM memory on this chip doesn’t help that much for Gromacs. <em>Hmmmm</em>.</p>
<p>Anyway, that is some food for thought about the Grace CPU and HPC workloads. There are other benchmarks in the Stony Brook paper, so be sure to check them out.</p>
		<!-- QUIZ HERE -->
<div id="block-9">
<figure><a href="https://go.theregister.com/k/hpe_solutions_AI"><img decoding="async" src="https://www.nextplatform.com/wp-content/uploads/2023/05/HPE_button_19959_V2.png" alt=""/></a></figure>
</div><div id="text-26"><h4><span>Sign up to our Newsletter</span></h4>			<p>Featuring highlights, analysis, and stories from the week directly from us to your inbox with nothing in between.</p>
		</div></div></article></div>
  </body>
</html>
