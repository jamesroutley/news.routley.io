<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://catern.com/compdist.html">Original</a>
    <h1>Your computer is a distributed system</h1>
    
    <div id="readability-page-1" class="page">

Most computers are essentially a distributed system internally,
and provide their more familiar programming interface as an abstraction on top of that.
Piercing through these abstractions can yield greater performance,
but most programmers do not need to do so.
This is something unique:
an abstraction that hides the distributed nature of a system
and <em>actually succeeds</em>.
<p>
Many have observed that today&#39;s computers are distributed systems
implementing the abstraction of the shared-memory multiprocessor.
The wording in the title is from the 2009 paper
&#34;<a href="https://barrelfish.org/publications/barrelfish_hotos09.pdf">Your computer is already a distributed system. Why isn&#39;t your OS?</a>&#34;.
Some points from that and other sources:
</p><ul>
<li>In a typical computer
  there&#39;s many components running concurrently,
  e.g. graphics cards, storage devices, network cards,
  all running their own programs (&#34;firmware&#34;) on their own cores,
  independent of the main CPU
  and communicating over the internal network (e.g. a PCI bus).
</li><li>Components can appear and disappear and
  <!-- ram failure, disk failure, graphics card failure, OSs are resilient to these things -->
  <a href="https://www.cyberciti.biz/tips/linux-find-out-if-harddisk-failing.html">fail</a>
  independently,
  and other components have to
  <a href="https://linux.die.net/man/8/hotplug">deal with this</a>.
  Much like on larger-scale unreliable networks,
  <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/usb.h#L1843">timeouts</a>
  are used to detect communication failures.
</li><li>Components can be malicious and send adversarial inputs;
  e.g., a malicious USB or Thunderbolt device
  can try to exploit vulnerabilities in other hardware components in the system,
  <a href="https://en.wikipedia.org/wiki/PlayStation_3_Jailbreak">sometimes</a>
  <a href="https://en.wikipedia.org/wiki/Thunderspy">successfully</a>.
  The system should be robust to such attacks,
  just as with larger-scale distributed systems.
</li><li>These components are all updated on
  <a href="https://fwupd.org/">their own schedule</a>.
  Often the computer&#39;s owner is
  <a href="https://en.wikipedia.org/wiki/Proprietary_firmware">not allowed</a>
  to push their own updates to these components.
</li><li>The
  <a href="http://norvig.com/21-days.html#answers">latency</a>
  between different components of a computer is highly relevant;
  e.g., the latency of communication between the CPU and main memory or storage devices
  is huge compared to communication within the CPU.
</li><li>To reduce these latencies, we put caches in the middle,
  just as we use caches and CDNs in larger distributed systems.
</li><li>Those caches themselves are sophisticated distributed systems,
  implemented with
  <a href="https://en.wikipedia.org/wiki/Cache_coherence#Coherence_mechanisms">message passing</a>
  between individual cores.
  Providing this consistency has a latency
  <a href="https://en.wikipedia.org/wiki/PACELC_theorem">cost</a>
  just as it does in larger distributed systems.
</li></ul>
As the shared-memory multiprocessor is actually a distributed system underneath,
we can, if necessary, reason explicitly about that underlying system
and use the same techniques that larger-scale distributed systems use:
<ul>
<li>Performing operations closer to the data,
  e.g. through compute elements embedded into
  <a href="https://www.usenix.org/conference/osdi14/technical-sessions/presentation/seshadri">storage devices</a>
  and
  <a href="https://www.netronome.com/products/agilio-software/agilio-ebpf-software/">network interfaces</a>,
  is faster.
</li><li>Offloading heavy computations to a &#34;remote service&#34; can be faster,
  both because the service can run with
  <a href="https://en.wikipedia.org/wiki/Computation_offloading">better resources</a>,
  and because the service will have
  <a href="https://catern.com/flexsc.html">better locality</a>.
</li><li><a href="https://www.eideticom.com/media-news/blog/33-p2pdma-in-linux-kernel-4-20-rc1-is-here.html">Communicating directly between devices</a>
  rather than going through the centralized main memory
  reduces load on the CPU and improves latency.
</li><li>Using message passing instead of shared memory is
  <a href="https://www.seltzer.com/margo/teaching/CS508-generic/papers-a1/roghanchi17.pdf">faster</a>,
  for the same reasons larger distributed systems don&#39;t use distributed shared memory.
</li></ul>
But most programs do not do such things;
the abstraction of the shared-memory multiprocessor
is sufficient for most programs.
<p>
That this abstraction is successful is surprising.
In distributed systems,
it is often supposed that you cannot abstract away
the issues of distributed programming.
One classic and representative quote:
</p><blockquote cite="https://www.win.tue.nl/~johanl/educ/2II45/2010/Lit/Tanenbaum%20RPC%2088.pdf">
  It is our contention that
  a large number of things may now go wrong
  due to the fact that
  RPC tries to make remote procedure calls look exactly like local ones,
  but is unable to do it perfectly.
  
</blockquote>
Following this,
most approaches to large-scale distributed programming today
make the &#34;distributed&#34; part explicit:
They expose (and require programmers to deal with)
network failures, latency, insecure networks, partial failure, etc.
<p>
So it is surprising to find that essentially all programs are written
in an environment that abstracts away its own distributed nature:
the shared-memory multiprocessor, as discussed above.
Such programs are filled with RPCs which appear as if they&#39;re local operations.
Accesses to memory and disk are synchronous RPCs to relatively distant hardware,
which block the execution of the program while waiting for a response.
<!--
 !-- Indeed, it's not possible on current mainstream architectures to write a program
 !-- which explicitly <em>asynchronously</em>
 !-- sends and receives data from memory.
  -->
<!-- this could be used to implement arbitrary software hyperthreading;
     where a task could be blocked while it's mid-load from memory
  -->
<!-- fair point from Corbin: DMA transfers are asynchronous memory reads/writes.
     but it's just that the CPU isn't allowed to do them (between registers and RAM).
  -->
But for most programs, this is completely fine.
</p><p>
This offers hope that it is possible to some day
abstract away the distributed nature
of larger-scale systems.
Perhaps eventually we can even use the same abstractions for distributed systems of any size,
from individual computers to globe-spanning networks.
<!--
 !-- <p>
 !-- One
 !-- <a href="http://www.cs.cmu.edu/~tom7/papers/modal-types-for-mobile-code.pdf">possibility</a>
 !-- is to build a notion of "location"
 !-- explicitly into our programming languages,
 !-- and statically track where each expression "is located".
 !-- Computation in different locations, and communication of data between those locations,
 !-- could be explicitly described with a single program.
 !-- This would be a powerful abstraction both for the lowest-level hardware implementation
 !-- and the highest-level globe-spanning distributed system.
  -->


</p></div>
  </body>
</html>
