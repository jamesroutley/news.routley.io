<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/intel-pci-history">Original</a>
    <h1>Intel undercut a standards body to give us the PCI connector</h1>
    
    <div id="readability-page-1" class="page"><div data-headline="The Sneaky Standard" data-elid="2668256641" data-post-url="https://spectrum.ieee.org/intel-pci-history" data-authors="Ernie Smith" data-page-title="Intel’s PCI History: the Sneaky Standard - IEEE Spectrum"><div><p><em><em>A version of this post originally </em></em><a href="https://tedium.co/2024/02/09/intel-pci-standardization-history/" rel="noopener noreferrer" target="_blank"><u><em><em>appeared</em></em></u></a><em><em> on </em></em><a href="https://tedium.co/" rel="noopener noreferrer" target="_blank"><u><em><em>Tedium</em></em></u></a><em><em>, Ernie Smith’s newsletter, which hunts for the end of the long tail.</em></em></p><p>Personal computing has changed a lot in the past four decades, and one of the biggest changes, perhaps the most unheralded, comes down to compatibility. These days, you generally can’t fry a computer by plugging in a joystick that the computer doesn’t support. Simply put, standardization slowly fixed this. One of the best examples of a bedrock standard is the peripheral component interconnect, or PCI, which came about in the early 1990s and appeared in some of the decade’s earliest consumer machines three decades ago this year. To this day, PCI slots are used to connect network cards, sound cards, disc controllers, and other peripherals to computer motherboards via a bus that carries data and control signals. PCI’s lessons gradually shaped other standards, like USB, and ultimately made computers less frustrating. So how did we get it? Through a moment of canny deception.</p><p><span data-rm-shortcode-id="210b942d57e11b15d4d29ea81ca02f08"><iframe frameborder="0" height="auto" type="lazy-iframe" scrolling="no" data-runner-src="https://www.youtube.com/embed/7IaHcFXz0qg?rel=0&amp;start=5" width="100%"></iframe></span><small placeholder="Add Photo Caption...">Commercial - <a href="https://spectrum.ieee.org/tag/intel">Intel</a> Inside Pentium Processor (1994)</small><small placeholder="Add Photo Credit..."><a href="https://www.youtube.com/watch?v=7IaHcFXz0qg&amp;t=5s" target="_blank">www.youtube.com</a></small></p><h2>Embracing standards: the computing industry’s gift to itself<br/></h2><p>In the 1980s, when you used the likes of an Apple II or a Commodore 64 or an MS-DOS machine, you were essentially locked into an ecosystem. Floppy disks often weren’t compatible. The peripherals didn’t work across platforms. If you wanted to sell hardware in the 1980s, you were stuck building multiple versions of the same device.</p><p>For example, the <a href="https://en.wikipedia.org/wiki/KoalaPad" rel="noopener noreferrer" target="_blank"><u>KoalaPad</u></a> was a common drawing tool sold in the early 1980s for numerous platforms, including the <a href="https://en.wikipedia.org/wiki/Atari_8-bit_family" rel="noopener noreferrer" target="_blank"><u>Atari 800</u></a>, the <a href="https://en.wikipedia.org/wiki/Apple_II" rel="noopener noreferrer" target="_blank"><u>Apple II</u></a>, the <a href="https://en.wikipedia.org/wiki/TRS-80" rel="noopener noreferrer" target="_blank"><u>TRS-80</u></a>, the <a href="https://en.wikipedia.org/wiki/Commodore_64" rel="noopener noreferrer" target="_blank"><u>Commodore 64</u></a>, and the <a href="https://en.wikipedia.org/wiki/IBM_Personal_Computer" rel="noopener noreferrer" target="_blank"><u>IBM PC</u></a>. It was essentially the same device on every platform, and yet, KoalaPad’s manufacturer, Koala Technologies, had to make five different versions of this device, with five different manufacturing processes, five different connectors, five different software packages, and a lot of overhead. It was wasteful, made being a hardware manufacturer more costly, and added to consumer confusion.</p><p><span data-rm-shortcode-id="2fbd1609bde4383da99fba22453a4947"><iframe frameborder="0" height="auto" type="lazy-iframe" scrolling="no" data-runner-src="https://www.youtube.com/embed/XK1_sp9pRlM?rel=0&amp;start=1" width="100%"></iframe></span><small placeholder="Add Photo Caption...">Drawing on a 1983 KoalaPad (Apple IIe)</small><small placeholder="Add Photo Credit..."><a href="https://www.youtube.com/watch?v=XK1_sp9pRlM&amp;t=1s" target="_blank">www.youtube.com</a></small></p><p>This slowly began to change in around 1982, when the <a href="https://www.forbes.com/sites/timbajarin/2021/08/25/attack-of-the-clones-how-ibm-lost-control-of-the-pc-market/?sh=6b58d8515b81" target="_blank"><u>market of IBM PC clones</u></a> started taking off. It was a happy accident—IBM’s decision to use a bunch of off-the-shelf components for its PC accidentally turned them into a de facto standard. Gradually, it became harder for computing platforms to become islands unto themselves. Even when IBM itself tried and failed to sell the computing world on a bunch of proprietary standards in its <a href="https://www.pcworld.com/article/465931/the_ibm_ps_2_25_years_of_pc_history.html" target="_blank"><u>PS/2 line</u></a>, it didn’t work. The cat was already out of the bag. It was too late.<br/></p><p>So how did we end up with the standards that we have today, and the PCI expansion card standard specifically? PCI wasn’t the only game in town—you could argue, for example, that if things played out differently, <a href="https://tedium.co/2022/10/26/isa-expansion-slot-history/" target="_blank"><u>we’d all be</u></a> using <a href="https://en.wikipedia.org/wiki/NuBus" rel="noopener noreferrer" target="_blank"><u>NuBus</u></a> or <a href="https://en.wikipedia.org/wiki/Micro_Channel_architecture" rel="noopener noreferrer" target="_blank"><u>Micro Channel </u></a>architecture. But it was a standard seemingly for the long haul, far beyond other competing standards of its era.</p><p>Who’s responsible for spearheading this standard? Intel. While PCI was a cross-platform technology, it proved to be an important strategy for the chipmaker to consolidate its power over the PC market at a time when IBM had taken its foot off the gas, choosing to focus on <a href="https://www.nytimes.com/1991/10/03/business/ibm-now-apple-s-main-ally.html" rel="noopener noreferrer" target="_blank">its own PowerPC architecture</a> and narrower plays like the <a href="https://www.fastcompany.com/90145427/how-ibms-thinkpad-became-a-design-icon" rel="noopener noreferrer" target="_blank"><u>ThinkPad</u></a> instead, and was no longer shaping the architecture of the PC.</p><p>The vision of PCI was simple: an interconnect standard that was not intended to be limited to one line of processors or one bus. But don’t mistake standardization for cooperation. PCI was a chess piece—a part of a different game than the one PC manufacturers were playing.</p><p><img alt="Close up of a board showing several black raised PCIe interconnects." data-rm-shortcode-id="514aa5acccbc336c53038e2f0c87e28b" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/close-up-of-a-board-showing-several-black-raised-pcie-interconnects.jpg?id=52237741&amp;width=980" height="1250" id="f64c9" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%202000%201250&#39;%3E%3C/svg%3E" width="2000"/><small placeholder="Add Photo Caption...">The PCI standard and its derivatives have endured for over three decades. Modern computers with a GPU often use a PCIe interconnect. </small><small placeholder="Add Photo Credit...">Alamy</small></p><h2>In the early 1990s, Intel needed a win</h2><p>In the years before <a href="https://en.wikipedia.org/wiki/Pentium" rel="noopener noreferrer" target="_blank"><u>Intel’s Pentium chipset</u></a> came out in 1993, there seemed to be some skepticism about whether Intel could maintain its status at the forefront of the desktop-computing field.</p><p>In lower-end consumer machines, players like <a href="https://tedium.co/2017/05/18/intel-386-486-trademark-battles/" rel="noopener noreferrer" target="_blank">Advanced Micro Devices (<u>AMD</u></a>) and<a href="https://www.techspot.com/article/2120-cyrix/" rel="noopener noreferrer" target="_blank"><u>Cyrix</u></a> were starting to shake their weight around. At the high end of the professional market, workstation-level computing from the likes of <a href="https://thenewstack.io/sun-microsystems-a-look-back-at-a-tech-company-ahead-of-its-time/" rel="noopener noreferrer" target="_blank"><u>Sun Microsystems</u></a>, <a href="https://tedium.co/2018/10/04/sgi-collector-history/" rel="noopener noreferrer" target="_blank"><u>Silicon Graphics</u></a>, and <a href="https://tedium.co/2020/12/15/altavista-history-digital-dot-com-domain-name/" rel="noopener noreferrer" target="_blank"><u>Digital Equipment Corporation</u></a> suggested there wasn’t room for Intel in the long run. And laterally, the company suddenly found itself competing with a triple threat of IBM, Motorola, and Apple, whose <a href="https://tedium.co/2020/06/16/apple-powerpc-intel-transition-history/" rel="noopener noreferrer" target="_blank"><u>PowerPC</u></a> chip was about to hit the market.</p><p>A <a href="https://archive.is/zU9aX" rel="noopener noreferrer" target="_blank"><u>Bloomberg piece</u></a> from the period painted Intel as being boxed in between these various extremes:</p><blockquote>If its rivals keep gaining, Intel could eventually lose ground all around.</blockquote><p>This was a deep underestimation of Intel’s market position, it turned out. The company was actually well-positioned to shape the direction of the industry through standardization. They had a direct say on what appeared on the motherboards of millions of computers, and that gave them impressive power to wield. If Intel didn’t want to support a given standard, that standard would likely be dead in the water.</p><h2>How Intel crushed a standards body on the way to giving us an essential technology</h2><p>The <a href="https://vesa.org/" rel="noopener noreferrer" target="_blank"><u>Video Electronics Standards Association</u></a>, or VESA, is perhaps best known today for its <a href="https://www.ergotron.com/en-us/support/vesa-standard" rel="noopener noreferrer" target="_blank"><u>mounting system for computer monitors</u></a> and its<a href="https://vesa.org/displayport-developer/about-displayport/" rel="noopener noreferrer" target="_blank"><u>DisplayPort technology</u></a>. But in the early 1990s, it was working on a video-focused successor to the <a href="https://en.wikipedia.org/wiki/Industry_Standard_Architecture" rel="noopener noreferrer" target="_blank"><u>Industry Standard Architecture</u></a> (ISA) internal bus, widely used in IBM PC clones.</p><p>A bus, the physical wiring that lets a CPU talk to internal and external peripheral devices, is something of a bedrock of computing—and in the wrong setting, a bottleneck. The ISA expansion card slot, which had become a de facto standard in the 1980s, had given the IBM PC clone market something to build against during its first decade. But by the early 1990s, for high-bandwidth applications, particularly video, it was holding back innovation. It just wasn’t fast enough to keep up, even after it had been upgraded from being able to handle 8 bits of data at once to 16.</p><p>That’s where the VESA Local Bus (VL-Bus) came into play. Built to work only with video cards, the standard offered a faster connection, and could handle 32 bits of data. It was <a href="https://books.google.com/books?id=YfkUKcyI7KIC&amp;pg=PT139" rel="noopener noreferrer" target="_blank">targeted at the Super VGA standard</a>, which offered higher resolution (up to 1280 x 1024 pixels) and richer colors at a time when Windows was finally starting to take hold in the market. To overcome the limitations of the ISA bus, graphics card and motherboard manufacturers started collaborating on proprietary interfaces, creating an array of incompatible graphics buses. The lack of a consistent experience around Super VGA led to VESA’s formation. The new VESA slot, which extended the existing 16-bit ISA bus with an additional 32-bit video-specific connector, was an attempt to fix that.</p><p>It wasn’t a massive leap—more like a stopgap improvement on the way to better graphics.</p><p>And it looked like Intel was going to go for the VL-BUS. But there was one problem—Intel actually wasn’t feeling it, and Intel didn’t exactly make that point clear to the companies supporting the VESA standards body until it was too late for them to react.</p><p>Intel revealed its hand in an interesting way, according to <a href="https://www.newspapers.com/image/462123935/?clipping_id=32785690&amp;fcfToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmcmVlLXZpZXctaWQiOjQ2MjEyMzkzNSwiaWF0IjoxNzE0NDM4NTM0LCJleHAiOjE3MTQ1MjQ5MzR9.giL2GKz1-XQApU9SKJXoHMfNxIODT4te3lSOvqzVlWo" rel="noopener noreferrer" target="_blank"><u><em><em>The</em></em></u><u></u><u><em><em>San Francisco Examiner</em></em></u><u></u></a><a href="https://www.newspapers.com/article/the-san-francisco-examiner-pci-vs-vesa/32785690/" rel="noopener noreferrer" target="_blank"><u>tech reporter Gina Smith</u></a>:</p><blockquote>Until now, virtually everyone expected VESA’s so-called VL-Bus technology to be the standard for building local bus products. But just two weeks before VESA was planning to announce what it came up with, Intel floored the VESA local bus committee by saying it won’t support the technology after all. In a letter sent to VESA local bus committee officials, Intel stated that supporting VESA’s local bus technology “was no longer in Intel’s best interest.” And sources say it went on to suggest that VESA and Intel should work together to minimize the negative press impact that might arise from the decision.</blockquote><p>But Intel had seen an opportunity to put its imprint on the computing industry. That opportunity came in the form of PCI, a technology that the firm’s <a href="https://en.wikipedia.org/wiki/Intel_Architecture_Labs#:~:text=Intel%20Architecture%20Labs%20(IAL)%20was,of%20Intel%20during%20the%201990s." rel="noopener noreferrer" target="_blank"><u>Intel Architecture Labs</u></a> started developing around 1990, two years before the fateful rejection of VESA. Essentially, Intel had been playing both sides on the standards front.</p><h2>Why PCI</h2><p>Why make such a hard shift, screwing over a trusted industry standards body out of nowhere? Beyond wanting to put its mark on the standard, Intel also saw an opportunity to build something more future-proof; something that could benefit not just graphic cards but every expansion card in the machine.</p><p><a href="https://books.google.com/books?id=IuaYd-eFaFoC&amp;pg=PA142" rel="noopener noreferrer" target="_blank"><u>As John R. Quinn wrote in </u><u><em><em>PC Magazine</em></em></u><u> in 1992</u></a>:</p><blockquote>Intel’s PCI bus specification requires more work on the part of peripheral chip-makers, but offers several theoretical advantages over the VL-Bus. In the first place, the specification allows up to ten peripherals to work on the PCI bus (including the PCI controller and an optional expansion-bus controller for ISA, EISA, or MCA). It, too, is limited to 33 MHz, but it allows the PCI controller to use a 32-bit or a 64-bit data connection to the CPU.</blockquote><p>To put that all another way, VESA came up with a slightly faster bus standard for the next generation of graphics cards, one just fast enough to meet the needs of Intel’s recent <a href="https://en.wikipedia.org/wiki/I486" rel="noopener noreferrer" target="_blank"><u>i486</u></a> microprocessor users. Intel came up with an interface designed to reshape the next decade of computing, one that it would let its competitors use. This bus would allow people to upgrade their processor across generations without needing to upgrade their motherboard. Intel brought a gun to a knife fight, and it made the whole debate about VL-Bus seem insignificant in short order.</p><p>The result was that, no matter how miffed the VESA folks were, Intel had consolidated power for itself by creating an open standard that would eventually win the next generation of computers. Sure, Intel let other companies use the PCI standard, even companies like Apple that weren’t directly doing business with Intel on the CPU side. But Intel, by pushing forth PCI, suddenly made itself relevant to the entire next generation of the computing industry in a way that ensured it would have a second foothold in hardware. The “Intel Inside” marketing label was not limited to the processors, as it turned out.</p><p>The influence of Intel’s introduction of PCI is still felt: Thirty-two years later, and three decades after PCI became a major consumer standard, we’re still using PCI derivatives in modern computing devices.</p><h2>PCI and other standards</h2><p>Looking at PCI, and its successor <a href="https://en.wikipedia.org/wiki/PCI_Express" rel="noopener noreferrer" target="_blank"><u>PCI express</u></a>, less as ways that we connect the peripherals we use with our computers, and more as a way for Intel to maintain its dominance over the PC industry, highlights something fascinating about standardization.</p><p>It turns out that perhaps Intel’s greatest investment in computing in the 1990s was not the <a href="https://tedium.co/2020/09/04/intel-floating-point-glitch-history/" rel="noopener noreferrer" target="_blank"><u>Pentium chipset</u></a>, but its investment in Intel Architecture Labs, which quietly made the entire computing industry better by working on the things that frustrated consumers and manufacturers alike.</p><p>Essentially, as IBM had begun to take its eye off the massive clone market it unwittingly built during this period, Intel used standardization to fill the power void. It worked pretty well, and made the company integral to computer hardware beyond the CPU. In fact, devices you use daily—that Intel played zero part in creating—have benefited greatly from the company’s standards work. If you’ve ever used a device with a <a href="https://www.intel.com/content/www/us/en/standards/usb-two-decades-of-plug-and-play-article.html" rel="noopener noreferrer" target="_blank">USB</a> or <a href="https://www.bluetooth.com/about-us/bluetooth-origin/" rel="noopener noreferrer" target="_blank">Bluetooth</a> connection, you can thank Intel for that.</p><p>Craig Kinnie, the director of Intel Architecture Labs in the 1990s, <a href="https://www.newspapers.com/article/the-news-tribune-microsoftintel-accord/140619683/" rel="noopener noreferrer" target="_blank"><u>said it best in 1995</u></a>, upon coming to an agreement with <a href="https://spectrum.ieee.org/tag/microsoft">Microsoft</a> on a 3D graphics architecture for the PC platform. “What’s important to us is we move in the same direction,” he said. “We are working on convergent paths now.”</p><p>That was about collaborating with Microsoft. But really, it has been Intel’s modus operandi for decades—what’s good for the technology field is good for Intel. Innovations developed or invented by Intel—like Thunderbolt, <a href="https://www.digitaltrends.com/computing/how-ultrabooks-challenged-macbook-air-and-won/" rel="noopener noreferrer" target="_blank"><u>Ultrabooks</u></a>, and Next Unit Computers (<a href="https://en.wikipedia.org/wiki/Next_Unit_of_Computing" rel="noopener noreferrer" target="_blank"><u>NUCs</u></a>)—have done much to shape the way we buy and use computers. </p><p>For all the talk of Moore’s Law as a driving factor behind Intel’s success, the true story might be its sheer cat-herding capabilities. The company that builds the standards builds the industry. Even as Intel faces increasing competition from alliterative processing players like ARM, Apple, and AMD, as long as it doesn’t lose sight of the roles standards played in its success, it might just hold on a few years longer.</p><p>Ironically, Intel’s standards-driving winning streak, now more than three decades old, might have all started the day it decided to walk out on a standards body.</p></div></div></div>
  </body>
</html>
