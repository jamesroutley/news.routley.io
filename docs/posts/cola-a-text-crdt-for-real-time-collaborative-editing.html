<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nomad.foo/blog/cola">Original</a>
    <h1>Cola: A text CRDT for real-time collaborative editing</h1>
    
    <div id="readability-page-1" class="page"><div id="_0-85"><p>In this blog post I’ll talk about the theoretical background and the technical
implementation of <a href="https://github.com/nomad/cola">cola</a>, a text CRDT for real-time collaborative editing
written in Rust. Reading this is not at all necessary to be able to <em>use</em> cola,
so if that’s all you’re interested in you can safely skip this post and go
straight to the <a href="https://docs.rs/cola-crdt/latest">documentation</a>.</p>
<p>The post is divided into three parts:</p>
<ul>
<li>
<p>in the <a href="#first-part-intro-to-text-crdts">first part</a> we’ll go over how to
represent the state of the document and the edits that transform it in a
way that guarantees convergence across all replicas. The ideas presented in
this part are widely covered <a href="https://dl.acm.org/doi/10.1145/1180875.1180916">in</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0743731510002716?via%3Dihub">the</a> <a href="https://dl.acm.org/doi/10.1145/2957276.2957300">literature</a>,
so if you’re already familiar with text CRDTs you can probably skip it;</p>
</li>
<li>
<p>in the <a href="#second-part-cola">second part</a> we’ll see how to efficiently
implement the framework discussed in the first part in code;</p>
</li>
<li>
<p>finally, in the <a href="#third-part-benchmarks">third part</a> we’ll look at some
benchmarks comparing cola to other Rust-based CRDT libraries.</p>
</li>
</ul>
<h2 id="first-part-intro-to-text-crdts">First part: Intro to text CRDTs<a href="#first-part-intro-to-text-crdts"><span></span></a></h2>
<p>A Conflict-free Replicated Data Type, or CRDT, is an umbrella term for any data
structure that can be replicated and modified concurrently across multiple
sites, and is guaranteed to converge without the need for a central authority
to coordinate the changes.</p>
<p>There are many different kinds of CRDTs: counters, sets, maps, lists, trees,
etc. Here we’ll focus on a sequence CRDT that can be used to represent plain
text documents.</p>
<p>In the setting of our problem we have a number of peers on a distributed
network all editing the same document. Each peer can insert or delete text and
immediately see the changes applied to their local replica. These edits are
then sent to the other peers which somehow integrate them into their own
replicas.</p>
<p>The only assumption we make on the network layer is that every edit is received
by all the peers in the network. Edits can be sent multiple times and in any
order, but they must eventually reach all the peers.</p>
<p>Our goal is to design a data structure that can, at the very minimum, converge
to the same state at every peer once all the edits have been received.
The final document we end up with should also “make sense” from the user’s
point of view. For example, we could have a CRDT that sorts the inserted
characters in alphabetical order. This would technically solve all concurrency
problems, but I doubt anyone would use it.</p>
<h3 id="anchors-in-a-sea-of-text">Anchors in a sea of text<a href="#anchors-in-a-sea-of-text"><span></span></a></h3>
<p>Let’s start by looking at the simplest approach: doing nothing. When someone
inserts or deletes some text we just broadcast the edit “as is”: <code>insert &#34;abc&#34; at offset 8</code>, <code>delete between offset 10 and offset 13</code>, etc.</p>
<p>This is clearly wrong because all we need to diverge is for another peer to
concurrently modify the document in the <code>0..offset</code> region.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/9c0d89db28e9f9c0c6b83df6caf35aee361c20f34949f969629915b1b614dfae.mov" type="video/mp4"/></video>
</figure>

<p>The problem is that offsets depend on the state of the document at the time an
edit was made. We can’t just exchange offsets without also exchanging the
context they depend on.</p>
<p>What if we used characters to refer to positions in the document instead of
offsets? We could then send <code>insert &#34;abc&#34; after the &#39;d&#39;</code> or <code>delete between the &#39;f&#39; and the &#39;o&#39;</code>.</p>
<p>When a peer receives that edit it doesn’t matter where the <code>&#39;d&#39;</code> ended up,
they’ll know they have to insert <code>&#34;abc&#34;</code> after it.</p>
<p>But what if there are multiple <code>&#39;d&#39;</code>s in the document? What if the <code>&#39;d&#39;</code> was
deleted? We still have to think some things through, but the idea of using
content to refer to positions is a step in the right direction.</p>
<p>Let’s solve the first problem: how do we tell our <code>&#39;d&#39;</code>s apart?</p>
<p>In our universe time only flows in <a href="https://en.wikipedia.org/wiki/Arrow_of_time">one direction</a>, so we could
assign an increasing number to each character as it’s inserted into the
document. The first character a peer inserts gets the number 0, the second 1,
and so on. Timestamping in this way works because for every n, a peer can only
insert its n-th character once, so we can use this number as a stable
identifier for a position in the document.</p>
<p>Or can we? After all there could many n-th characters in the same document, one
for every peer that ever contributed to it. To distiguish them we also assign
each peer a unique identifier, called a <a href="https://github.com/nomad/cola/blob/1c8ecb4cb2b4ed2bf3e8ba62e8762601153f2d8a/src/replica_id.rs#L29"><code>ReplicaId</code></a> in cola, and
use the pair <code>ReplicaId.n</code> to uniquely identify a character.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/59b62d87eeb75b2103baa25c72f3465b17c03d00050fa23af48923102617301e.mov" type="video/mp4"/></video>
</figure>

<p>Guaranteeing uniqueness of the n half is both possible and easy: we just
increment a local counter. Doing the same for the <code>ReplicaId</code> half without a
central server is not. The best we can do is to use random integers big enough
to make the probability of a collision <a href="https://en.wikipedia.org/wiki/Birthday_problem#Probability_table">negligible</a>, like a
<a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">UUID</a>.</p>
<p>This n number is also called a <em>temporal offset</em> in cola, and the <code>ReplicaId.n</code>
pair is called an <a href="https://github.com/nomad/cola/blob/1c8ecb4cb2b4ed2bf3e8ba62e8762601153f2d8a/src/run_tree.rs#L980-L986"><code>Anchor</code></a>. <code>Anchor</code>s are incredibly important
because they allow us to specify both insertions and deletions in a
concurrency-enabled way. Insertions are identified by a single <code>Anchor</code> which
tells us where to insert the text, while deletions are identified by two
<code>Anchor</code>s, one for the start and one for the end of the deleted region.</p>
<h3 id="breaking-ties">Breaking ties<a href="#breaking-ties"><span></span></a></h3>
<p>When a peer integrates an insertion it may find other insertions in its replica
that have the same <code>Anchor</code>. How do we decide which one comes first?</p>
<p>Intuitively we’d want the last insertion to come first.</p>
<p>But the concept of a “last” operation is not well defined in a distributed
system. Using wall clock timestamps is problematic because wall clocks lack
monotonicity and also suffer from clock drift. What we really want is a way to
determine if an insertion was made in an environment in which another insertion
was already present.</p>
<p>This is exactly what <a href="https://en.wikipedia.org/wiki/Lamport_timestamp">Lamport timestamps</a> are used for. A Lamport
clock is a logical clock that is updated by a very simple algorithm: every time
a peer inserts some text it increases its clock by one, and when it receives a
remote insertion it sets the clock to <code>max(current, remote_timestamp) + 1</code>.</p>
<p>This guarantees that if insertion <code>A</code> is made by a peer that has already
integrated insertion <code>B</code>, then <code>A</code>’s Lamport timestamp will be greater than
<code>B</code>’s.</p>
<p>We can now handle conflicting insertions by sorting them in descending order by
their Lamport timestamp.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/f1bb8c4a9b8e2d5c7b89a1cf5ef47d3df71258f74ff955d3298b5f939190f418.mov" type="video/mp4"/></video>
<figcaption>When we integrate a remote insertion we
start at the insertion&#39;s Anchor. We then skip blocks until we find the first
one whose Lamport timestamp is less than or equal to the one of the insertion
that&#39;s being integrated.</figcaption>
</figure>

<p>There’s one last case we need to handle: conflicting and <em>concurrent</em>
insertions, i.e. insertions with the same <code>Anchor</code> and Lamport timestamp.</p>
<p>From the user’s point of view there’s not an order that’s more “correct” than
the others, so all we care about is consistency between peers. cola breaks the
tie by sorting in ascending order on the <code>ReplicaId</code> of the peer that made the
insertion.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/3790d2bba7d29e21ec788c9ff6697456fce3c3f66206673908187273ba0a7d44.mov" type="video/mp4"/></video>
<figcaption>Insertions with the same Lamport timestamps are
sorted by their ReplicaId in ascending order. Here the &#39;R&#39; is inserted by
Peer 1 and the &#39;D&#39; by Peer 2, so the &#39;R&#39; goes before the &#39;D&#39;.</figcaption>
</figure>

<h3 id="deletions">Deletions<a href="#deletions"><span></span></a></h3>
<p>With insertions out of the way let’s now focus on deletions. As I’ve briefly
mentioned, every time a peer deletes some text we transform the start and end
offsets representing the deleted range into <code>Anchor</code>s, which we can broadcast
to the other peers.</p>
<p>Deletions are a bit easier to reason about because, unlike with insertions, we
don’t have to worry as much about concurrency or causality issues. Two peers
concurrently deleting the same region of text produces the same result as if
only one of them had done it: the text is gone.</p>
<p>There are however three problems we need to tackle before we can consider
deletions solved:</p>
<ol>
<li>
<p>what do we do with the deleted regions of text? We can’t entirely remove
them from the document because there may be edits we haven’t yet seen
whose <code>Anchor</code>s lie within them;<!-- DIAG: show what happens if we remove
     a region of text--></p>
</li>
<li>
<p>how do we know when a remote deletion is ready to be integrated? We’ll
diverge if a peer integrates a deletion before it has received all the
content that the peer who created the deletion had when they made it;<!--
     DIAG: show how this can cause divergence--></p>
</li>
<li>
<p>within the region of text included between the start and end <code>Anchor</code>s of
a deletion, how do we determine which characters should be deleted and
which should be kept? We’ll diverge if the peer who integrates a deletion
deletes text that wasn’t yet received by the peer who created the
deletion.<!-- DIAG: show how this can cause divergence--></p>
</li>
</ol>
<p>The solution to the first problem is to use <em>tombstones</em>. A tombstoned
character is simply marked as deleted, but it’s still kept in the document.
This clearly increases the memory footprint of our data structure as more and
more text is deleted. We’ll see in the performance section how we can mitigate
this problem.</p>
<!-- DIAG: show how characters are tombstoned-->
<p>A solution to other two problems is to include in the deletion message a map
where the keys are <code>ReplicaId</code>s and the values are the timestamps of the last
character that the peer who created the deletion had seen when they made it.</p>
<p>This is called a <a href="https://en.wikipedia.org/wiki/Version_vector">version
vector</a>. Version
vectors are useful because we can implement a <a href="https://en.wikipedia.org/wiki/Partially_ordered_set">partial
order</a> on them that lets
us compare what content different peers have seen. If peer A’s vv is &gt;= than
peer B’s vv then we know that A has seen at least as much content as B, and
possibly more.</p>
<p>Using version vectors we can solve the second problem by making a peer wait to
integrate a deletion until its vv is &gt;= than the vv included in the deletion
message.</p>
<!-- DIAG: show how this solves the issue-->
<p>We can also solve the third problem by skipping over characters whose timestamp
is strictly greater than the timestamp included in the deletion’s vv.</p>
<!-- DIAG: show how this solves the issue-->
<h2 id="second-part-cola">Second part: cola’s implementation<a href="#second-part-cola"><span></span></a></h2>
<p>In the previous part we’ve walked through the theoretical framework cola uses
to solve collaborative editing. It’s now time for the fun part: implementing it
in code in a performant way.</p>
<h3 id="linkin-blocks">Linkin Blocks<a href="#linkin-blocks"><span></span></a></h3>
<p>Let’s start by designing the data structure each peer will use to represent the
state of its local replica of the document. This is called a
<a href="https://github.com/nomad/cola/blob/1c8ecb4cb2b4ed2bf3e8ba62e8762601153f2d8a/src/replica.rs#L49-L75"><code>Replica</code></a> in cola.</p>
<p>As we’ve seen, every time a peer inserts some text we assign each character its
own block, which includes a temporal offset and a Lamport timestamp.
It’s very natural to represent this sequence of blocks as a linked list, so
that’s where we’ll start.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/9f5cf94f4d66543edc1f88c5e6d5e92a056d97b9bb1c76fa88547a7d8fab2756.mov" type="video/mp4"/></video>
</figure>

<p>Before we dive further into the implementation I’d like to point out something.
Up until now I’ve been including the textual content associated with each block
in every illustration. This was mostly done for clarity, but none of the
algorithms we’ve discussed so far actually need to know what the content of an
insertion is.</p>
<p>This also means we can decouple all the CRDT machinery we’re building from the
implementation of the text buffer itself. In fact, there’s not a single
function in cola’s API that takes a string as an argument. cola has no idea
what the content of the document you’re editing is, all it deals with are
blocks of numbers.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/58fc4a767e1ab6d6b1c1d47ad95e9aab8d7850cad7d36ada8b06c49fdb742e3e.mov" type="video/mp4"/></video>
</figure>

<h3 id="rle-all-the-things">RLE all the things<a href="#rle-all-the-things"><span></span></a></h3>
<p>The first big optimization we can do is to reduce the number of blocks we need
to represent the document. We can’t have any hope of landing on a performant
implementation if we have a bunch of metadata associated with every single
character that’s ever been inserted.</p>
<p>Luckily, we don’t have to. Every time there’s a run of blocks whose character
timestamps increase sequentially we can <a href="https://en.wikipedia.org/wiki/Run-length_encoding">run-length encode</a> them into a
single block. This means pasting the entire <a href="https://en.wikipedia.org/wiki/Manhattan_Project">Wikipedia page on the
Manhattan Project</a> will only take up a single block instead of
107,000.</p>
<p>RLEing in this way also allows us to represent a sentence typed
character-by-character without moving the cursor or deleting anything with a
single block, instead of one per key press.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/ffc9cbdcd185d5ccd085dd493f9fda934ced56f2db9b430aea9baa56cb87857f.mov" type="video/mp4"/></video>
</figure>

<p>These runs of consecutive blocks are called <a href="https://github.com/nomad/cola/blob/1c8ecb4cb2b4ed2bf3e8ba62e8762601153f2d8a/src/run_tree.rs#L761-L773"><code>EditRun</code></a>s in cola.
Once an <code>EditRun</code> is broken it remains fixed for the remaning lifetime of the
document and can’t be extended anymore. A run that’s not yet broken is called
<em>active</em>.</p>
<p>When text is inserted in the midle of an existing <code>EditRun</code> we simply split it
into two shorter runs and insert the new text between them.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/b677c41c05469c13e0a69343d8dd3224250b11554b3253f6f863cc5ecabcb9b6.mov" type="video/mp4"/></video>
</figure>

<p>When text is removed from the document we split it off from the run it’s part
of and mark it as deleted (tombstones, remember?). Deleted runs can also be
RLEd in the same way as active runs, which mitigates the memory footprint
problem we mentioned in the previous section.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/8bbd20e161d63712ee0e5ef33157ad8ccbcecad144837ab2552d517c619dc3de.mov" type="video/mp4"/></video>
</figure>

<h3 id="a-better-alternative-b-trees">A better alternative: B-trees<a href="#a-better-alternative-b-trees"><span></span></a></h3>
<p>There’s two distinct paths we need to think about while designing our data
structure:</p>
<ul>
<li>
<p>the <em>upstream</em> path, which deals with transforming a local edit specified
in terms of offsets into the document into an edit that can be sent to
other peers, and</p>
</li>
<li>
<p>the <em>downstream</em> path, which deals with transforming a remote edit received
from another peer into a local edit that can be applied to the document.</p>
</li>
</ul>
<p>Let’s start by improving the upstream path. For both insertions and deletions
this boils down to:</p>
<ol>
<li>
<p>finding the run that contains an offset to create an <code>Anchor</code>;</p>
</li>
<li>
<p>if necessary, splitting it into a max of two (for insertions) or three
(for deletions) smaller runs.</p>
</li>
</ol>
<p>Since we’re using a linked list the second step is <code>O(1)</code>. The first one
however requires us to scan the entire list from the beginning of the document
until we land on the right run, which is a linear time complexity. Not good.</p>
<p>We could improve this by caching a pointer to the currently active run together
with the run’s offset in the document. This helps because when we edit a
document we don’t jump around randomly. We usually insert or delete entire runs
of characters before moving the cursor. There are editing patterns that don’t
follow this assumption like when using multiple cursors, but it’s still a good
heuristic.</p>
<p>While this makes the best-case performance constant, the worst case would still
be linear. If the user inserts a character at the start of the document and
then another one at the end we still have to walk the entire list.</p>
<p>It’s clear that a linear data structure won’t cut it if we want to be
performant with all kinds of editing patterns. We need a logarithmic worst-case
complexity.</p>
<p>This is where B-trees come in. A <a href="https://en.wikipedia.org/wiki/B-tree">B-tree</a> is a balanced tree where
every node can have a variable number of children between a minimum and a
maximum. “Balanced” simply means that for every internal node, all its children
have the same height.</p>
<p>The runs that where previously the nodes of the linked list are now represented
as leaves in the B-tree. Leaves are grouped into internal nodes, or <em>inodes</em>
for short. Each inode stores not only its children but also the sum of their
lengths (with tombstoned runs contributing a length of 0). Inodes are in turn
grouped into other inodes all the way up to the root, whose length coincides
with the length of the document.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/de05e6e1f00da8d81c9e9aa516514d2c7c3eefebe48603fab6fa6fa480fd78ed.mov" type="video/mp4"/></video>
</figure>

<p>B-trees are a great fit for our problem because they can be traversed and
inserted into in logarithmic time.</p>
<p>To reach a given run we start from the root and loop through its children until
we find the one containing the offset we’re looking for. We then simply repeat
the same process with the child until we reach a leaf node.</p>
<p>Inserting a new run is in general a bit more involved because we need to make
sure that the tree remains balanced and that all its other invariants are
preserved. I won’t go into the details here, but the important thing is that
this can be done in <code>O(log n)</code>.</p>
<p>By using a B-tree we can do local edits in logarithmic time. Great! But what
about integrating remote edits? If we receive an edit like <code>insert 2.3..7 at 1.2</code> we currently have no way to efficiently find the run that contains the
anchor <code>1.2</code>. We’d have to check every single run from the beginning of the
document until we find the right one.</p>
<h3 id="an-even-better-alternative-g-trees">An even better alternative: G-trees<a href="#an-even-better-alternative-g-trees"><span></span></a></h3>
<p>The problem with our B-tree is that we don’t have a way to <em>identify</em> its
leaves, and if we can’t identify them we can’t efficiently perform the
<code>anchor -&gt; run</code> conversion.</p>
<p>We could store pointers to the leaves somewhere, but what do we use them for?
Having a pointer to the leaf of a B-tree doesn’t really help because insertions
are performed top-down, starting from the root, and a pointer doesn’t tell us
how to “navigate” down the tree to reach the leaf it points to.</p>
<p>Well, then “just do them bottom-up!” you might say. This would require to store
a parent pointer in every node of the tree (except the root), effectively
creating a sort of “doubly-linked” B-tree. Also remember that cola is written
in Rust, and the Rust compiler really doesn’t like data structures that don’t
have a clear ownership hierarchy. If you tried to implement this in safe Rust
you’d quickly end up with with a bunch of <code>Rc&lt;RefCell&lt;_&gt;&gt;</code>s everywhere, which
is not only slow but also ugly.</p>
<p>By wondering into the <code>unsafe</code> lands of Rust we could probably do it with raw
pointers and <code>Pin</code>ned nodes, but to prove that that kind of code is safe I’d
have to reason a lot more about how computers actually work than I’d like to. I
like Rust because it allows me not to do that.</p>
<p>The ownership model of a B-tree is very simple: every inode owns its children.
Unfortunately this simplicity leads to the kinds of problems I’ve described if
we want to do bottom-up operations. What we need is a way to express which
children belong to which parent, without having the latter actually own them
(in the Rust sense of the term).</p>
<p>The solution is surprisingly simple: we can store every node in a dynamic
array, (a <code>Vec</code> in Rust) and use indices into this vector to refer to other
nodes. This solves the ownership problem because the vector now owns all the
nodes. And by simply storing the index of the parent in every node we can
navigate the tree in both directions without having to use any <code>unsafe</code> code.</p>
<p>This all hinges on the assumption that once we’ve inserted a node into the
vector its index never changes. Inserting new nodes doesn’t cause any issues:
we just append them to the end. Removing a node however would be doubly bad:
not only is it a linear operation (because we’re now using a vector instead of
a tree), but it would also invalidate all the indices of the nodes that come
after it.</p>
<p>But as we’ve seen we never remove runs from the document anyway. Deleting some
text only marks the corresponding run as tombstoned, but it’s still there, so
we don’t have to worry about this.</p>
<p>I couldn’t find any established name for this kind of tree-in-a-vector data
structure, so I’ve been calling it a <em>G-tree</em>, short for grow-only tree.</p>
<p>The <a href="https://github.com/nomad/cola/blob/1c8ecb4cb2b4ed2bf3e8ba62e8762601153f2d8a/src/gtree.rs">Rust code</a> for it is slightly more efficient than what I’m presenting here,
but it’s basically:</p>
<pre><code><span>trait</span> <span>Leaf</span> (
    <span>fn</span> <span>len</span>(<span>&amp;</span><span>self</span>) -&gt; <span>u64</span>;
)

<span>struct</span> <span>Gtree</span><span>&lt;</span><span>L</span>: <span>Leaf</span><span>&gt;</span> (
    <span>inodes</span>: <span>Vec</span><span>&lt;</span><span>Inode</span><span>&gt;</span>,
    <span>leaves</span>: <span>Vec</span><span>&lt;</span><span>Leaf</span><span>&lt;</span><span>L</span><span>&gt;</span><span>&gt;</span>,
    <span>root_idx</span>: <span>InodeIdx</span>,
)

<span>struct</span> <span>Inode</span> {
    <span>children</span>: <span>Vec</span><span>&lt;</span><span>NodeIdx</span><span>&gt;</span>,
    <span>parent</span>: <span>Option</span><span>&lt;</span><span>InodeIdx</span><span>&gt;</span>,
    <span>total_len</span>: <span>u64</span>,
}

<span>struct</span> <span>Leaf</span><span>&lt;</span><span>L</span><span>&gt;</span> {
    <span>value</span>: <span>L</span>,
    <span>parent</span>: <span>InodeIdx</span>,
}

<span>enum</span> <span>NodeIdx</span> {
    <span>Inode</span>(<span>InodeIdx</span>),
    <span>Leaf</span>(<span>LeafIdx</span>),
}

<span>struct</span> <span>InodeIdx</span>(<span>usize</span>);

<span>struct</span> <span>LeafIdx</span>(<span>usize</span>);
</code></pre>

<p>The nice properties of the B-tree that we care about, namely top-down search
and insertion in logarithmic time, are still here. This is because we still
have the <em>structure</em> of a B-tree, i.e. the parent-child relationships between
the nodes, we’ve just changed its representation in memory.</p>
<figure>
<video controls="" loop="" muted="" playsinline=""><source src="/media/f658a51896c4266d73ba3f310e3f89c70ff08bfcfabb4f2638f8e6c2ff132e00.mov" type="video/mp4"/></video>
</figure>

<p>But this is not all. By using a G-tree we get a few other nice properties for
free:</p>
<ul>
<li>
<p>we can use the index of a leaf in the vector – a.k.a. the <code>LeafIdx</code> – as
a stable identifier for that leaf. No pointers, yay!</p>
</li>
<li>
<p>remember when we considered caching the pointer to the currently active
run when we were trying to make the linked-list-based implementation
faster? We can do that now, except instead of storing a pointer we store
the <code>LeafIdx</code> of the run.</p>
<p>This is a big deal because it makes repeated edits at the same cursor
location extremely fast. We just extend the active run and update the
length of every ancestor all the way up to the root.</p>
<p>Let’s do a bit of math to estimate just how (in)expensive cursor-cache hits
are. cola’s G-tree has a branching factor of 32, with an average occupancy
of around 20 children per inode. With just 4 levels we usually
store around 160k distinct <code>EditRun</code>s, and even documents with very long
editing histories barely reach a fraction of that.</p>
<p>This means a cache hit costs us a few integer comparisons and 2-4 integer
additions. No tree traversals and no new allocations. It doesn’t get much
faster than that;</p>
</li>
<li>
<p>serializing and deserializing becomes trivial. (De)Serializing things
that are represented as trees in memory is always a bit annoying because it
involves “flattening” the tree into a linear sequence of bytes in a way
that preserves its structure. But in a G-tree everything is already stored
linearly in memory, so we can just (de)serialize the vector.</p>
</li>
</ul>
<h3 id="anchors-to-leaves">Anchors to Leaves<a href="#anchors-to-leaves"><span></span></a></h3>
<p>Let’s recap our performance journey so far:</p>
<ul>
<li>
<p>we started with a linked list of runs, which was slow (linear) for both
upstream and downstream operations;</p>
</li>
<li>
<p>we then switched to a B-tree, which was logarithmic for upstream operations
but still still linear for downstream ones;</p>
</li>
<li>
<p>we’ve finally landed on the G-tree which, like the B-tree, is logarithmic
for upstream operations, but can also be logarithmic for downstream ones.</p>
</li>
</ul>
<p>Remember that downstream operations involve integrating remote edits of the
form <code>insert 2.3..7 at 1.2</code> or <code>delete between 3.4 and 2.2</code>. All the operations
are specified in terms of anchors. To turn them into concrete edits the user
can apply we need to convert the anchors into offsets into the document, i.e.
turning them into something of the form <code>insert 2.3..7 at 18</code> or <code>delete between 17 and 25</code>.</p>
<p>We’ve also seen how the G-tree can be traversed in both directions, so if we
knew which run contains a given anchor we could easily get the corresponding
offset by traversing the tree upwards. And “knowing which run” really means
“knowing which <code>LeafIdx</code>”, because as we’ve seen the <code>LeafIdx</code> acts as a stable
identifier for a run.</p>
<p>We’ve reduced the problem of integrating remote edits into performing the
<code>Anchor -&gt; LeafIdx</code> conversion. But this is relatively easy compared to what
we’ve done so far.</p>
<p>A simple solution would be to keep a secondary G-tree (or even a B-tree) where
the leaves contain the <code>ReplicaId</code>, the temporal range and the <code>LeafIdx</code> of the
run containing the same <code>ReplicaId</code> and range in the main G-tree, like so:</p>
<figure>
<img src="https://miles.land/media/2fdfc65e5c6c3aa407729ed9d4b736b5524a53b7a08b980a2b53fe4f70e6f7d6.png"/>
</figure>

<p>Note how the leaves of this tree are totally ordered, first by <code>ReplicaId</code> and
then by temporal range. A total order allows us to bubble the greatest
<code>Anchor</code> within the children of an inode up to the root, and use <code>Anchor</code>s
as offsets into this tree. When we need to convert an <code>Anchor</code> into a <code>LeafIdx</code>
we just navigate the tree downwards until we find the right leaf, which will
contain the <code>LeafIdx</code> we’re looking for.</p>
<p>This is a perfectly valid design since search and insertion both run in <code>O(log n)</code>, where <code>n</code> is the total number of runs in the document, which is the same
complexity as the main G-tree.</p>
<p>With that said, you won’t find this secondary G-tree in cola’s source code. I
won’t go over the details of the actual implementation cola uses because it’s a
bit involved and there’s not a ton to learn from it. The important thing to
know is that both search and insertion are <code>O(log f)</code>, where <code>f</code> is the number
of “fragments” that the <code>EditRun</code> containing the anchor has been split into
over time. Of course <code>f</code> is always <code>&lt;=</code> than <code>n</code>, and usually much smaller than
it.</p>
<p>Aaand.. we’re done! We’ve finally reached a design that guarantees convergence,
intent preservation and good performance (we’ll see just how good in the
following (and final) section).</p>
<p>There’s actually still work to do to make cola production-ready like supporting
undo/redo and a few other things, but the foundation is there.</p>
<h2 id="third-part-benchmarks">Third part: Benchmarks<a href="#third-part-benchmarks"><span></span></a></h2>
<p>I’ve benchmarked cola against 3 other CRDTs implemented in Rust:
<a href="https://github.com/josephg/diamond-types">diamond-types</a>, <a href="https://github.com/automerge/automerge">automerge</a> and <a href="https://github.com/y-crdt/y-crdt/">yrs</a>. I’ve measured the time each CRDT
takes to process the editing history of a real-world character-by-character
editing trace taken from <a href="https://miles.land/posts/classmates-legal-threat-fizz-defcon/link">this</a> dataset using <a href="https://github.com/bheisler/criterion.rs">criterion</a>, a popular
benchmarking library for Rust. Each trace was measured in both the upstream
and downstream directions. <a href="https://github.com/noib3/crdt-benches">This</a> is the code that’s being benchmarked.</p>
<p>In the following graphs I’m using cola’s performance times 100 as the deadline,
so if a CRDT is more than 100x slower than cola its measurement is not shown.</p>
<p>The benchmarks were run on a 2018 MacBook Pro with a 2.2 GHz 6-Core Intel Core
i7. If you run them on your machine you’ll probably get different results, but
the relative performance of the CRDTs should be similar.</p>
<p>In the upstream direction both yrs and automerge go over the deadline, with
cola being 1.4-2x faster than diamond-types.</p>
<figure>
<img src="https://miles.land/media/30617a50a7b4db09a6af753160a77e35d3693feeb38b45733129654fae71e1d4.png"/>
<figcaption>Upstream results.</figcaption>
</figure>

<p>The results in the downstream direction are similar, except now diamond-types
crashes with every trace so I couldn’t get any measurements (if it was my fault
for not using the library correctly please let me know and I’ll update the
post).</p>
<figure>
<img src="https://miles.land/media/2578d3345676d298152a544ab305226830c8b4558945320d59c6bb2438397b15.png"/>
<figcaption>Downstream results.</figcaption>
</figure>

<p>cola is ~2x slower than before, which is expected since integrating remote
edits is in general more expensive than creating local ones, but it still
performs amazingly well. It’s about as fast as (if not faster than) the
<a href="https://github.com/nomad/crop">fastest</a> <a href="https://github.com/josephg/jumprope">rope</a> <a href="https://github.com/cessen/ropey">libraries</a> in both directions, and
it’s currently the fastest text CRDT implementation I know of.</p>
<p>✌🏻</p>

</div></div>
  </body>
</html>
