<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/blog/mlabonne/abliteration">Original</a>
    <h1>Uncensor any LLM with abliteration</h1>
    
    <div id="readability-page-1" class="page"><div><div>
				

				
				
				
				<div><div data-target="BlogAuthorsByline" data-props="{&#34;authors&#34;:[{&#34;author&#34;:{&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg&#34;,&#34;fullname&#34;:&#34;Maxime Labonne&#34;,&#34;name&#34;:&#34;mlabonne&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isMod&#34;:false}}],&#34;translators&#34;:[],&#34;proofreaders&#34;:[],&#34;lang&#34;:&#34;en&#34;}"><div><div>

<p><span><span><a href="https://huggingface.co/mlabonne"><img alt="Maxime Labonne&#39;s avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg"/>
					</a>
			</span>

	</span></p></div>
	</div></div></div>
				

				<!-- HTML_TAG_START -->
<p><a rel="nofollow" href="https://i.imgur.com/KhorYYG.png"><img alt="KhorYYG.png" src="https://i.imgur.com/KhorYYG.png"/></a></p>
<p>The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as &#34;As an AI assistant, I cannot help you.&#34; While this safety feature is crucial for preventing misuse, it limits the model&#39;s flexibility and responsiveness.</p>
<p>In this article, we will explore a technique called &#34;abliteration&#34; that can uncensor any LLM without retraining. This technique effectively removes the model&#39;s built-in refusal mechanism, allowing it to respond to all types of prompts.</p>
<p>The code is available on¬†<a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab</a>¬†and in the¬†<a rel="nofollow" href="https://github.com/mlabonne/llm-course">LLM Course</a>¬†on GitHub.</p>
<h2>
	<a rel="nofollow" href="#‚úÇÔ∏è-what-is-abliteration" id="‚úÇÔ∏è-what-is-abliteration">
		<span><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		‚úÇÔ∏è What is abliteration?
	</span>
</h2>
<p>Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their <a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">blog post</a>, Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model&#39;s residual stream. If we prevent the model from representing this direction, it <strong>loses its ability to refuse requests</strong>. Conversely, adding this direction artificially can cause the model to refuse even harmless requests.</p>
<p>In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block (&#34;pre&#34;), between the attention and MLP layers (&#34;mid&#34;), and after the MLP (&#34;post&#34;). The following figure illustrates the location of each residual stream.</p>
<p><a rel="nofollow" href="https://i.imgur.com/hsdR9e7.png"><img alt="" src="https://i.imgur.com/hsdR9e7.png"/></a></p>
<p>To uncensor an LLM, we first need to identify the &#34;refusal direction&#34; within the model. This process involves a few technical steps:</p>
<ol>
<li><strong>Data Collection</strong>: Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each.</li>
<li><strong>Mean difference</strong>: Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the &#34;refusal direction&#34; for each layer of the model.</li>
<li><strong>Selection</strong>: Normalize these vectors and evaluate them to select the single best &#34;refusal direction.&#34;</li>
</ol>
<p>Once we have identified the refusal direction, we can &#34;ablate&#34; it, effectively removing the model&#39;s ability to represent this feature. This can be done through an <strong>inference-time intervention</strong> or permanently with <strong>weight orthogonalization</strong>.</p>
<p>Let&#39;s talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction.</p>
<p>On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction.</p>
<p>In the next section, we will implement abliteration with weight orthogonalization.</p>
<h2>
	<a rel="nofollow" href="#üíª-implementation" id="üíª-implementation">
		<span><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		üíª Implementation
	</span>
</h2>
<p>The following implementation of abliteration is based on <a href="https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb">FailSpy&#39;s notebook</a>, which is itself based on the original authors&#39; <a rel="nofollow" href="https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing">notebook</a>. I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy&#39;s <a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a> if you&#39;re less interested in the technical details (also check his <a href="https://huggingface.co/collections/failspy/abliterated-v3-664a8ad0db255eefa7d0012b">collection of abliterated models</a> on Hugging Face).</p>
<p>The code relies on the excellent <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a> library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library.</p>
<p>First, let&#39;s install the necessary packages and import them. All these steps are available in this <a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab notebook</a>.</p>
<pre><code>!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

<span>import</span> torch
<span>import</span> functools
<span>import</span> einops
<span>import</span> gc

<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> tqdm <span>import</span> tqdm
<span>from</span> torch <span>import</span> Tensor
<span>from</span> typing <span>import</span> <span>List</span>
<span>from</span> transformer_lens <span>import</span> HookedTransformer, utils
<span>from</span> transformer_lens.hook_points <span>import</span> HookPoint
<span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer
<span>from</span> jaxtyping <span>import</span> Float, Int
<span>from</span> collections <span>import</span> defaultdict

<span># Turn automatic differentiation off to save GPU memory (credit: Undi95)</span>
torch.set_grad_enabled(<span>False</span>)
</code></pre>
<p>We need two datasets: one containing harmless instructions, and one containing harmful instructions. We&#39;ll use <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">tatsu-lab/alpaca</a> as well as data from <a rel="nofollow" href="https://github.com/llm-attacks/llm-attacks">llm-attacks</a>. To make things easier, I repackaged them in two Hugging Face datasets: <a href="https://huggingface.co/datasets/mlabonne/harmless_alpaca">mlabonne/harmless_alpaca</a> and <a href="https://huggingface.co/datasets/mlabonne/harmful_behaviors">mlabonne/harmful_behaviors</a>. That way, you can easily replace them with your own datasets.</p>
<p>We will load the instructions and reformat them into a list of dictionaries with &#34;role&#34; and &#34;content&#34; keys. This makes it compatible with the <code>apply_chat_tokenizer()</code> method, which we will use to follow Llama 3&#39;s chat template.</p>
<pre><code><span>def</span> <span>reformat_texts</span>(<span>texts</span>):
    <span>return</span> [[{<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: text}] <span>for</span> text <span>in</span> texts]

<span># Get harmful and harmless datasets</span>
<span>def</span> <span>get_harmful_instructions</span>():
    dataset = load_dataset(<span>&#39;mlabonne/harmful_behaviors&#39;</span>)
    <span>return</span> reformat_texts(dataset[<span>&#39;train&#39;</span>][<span>&#39;text&#39;</span>]), reformat_texts(dataset[<span>&#39;test&#39;</span>][<span>&#39;text&#39;</span>])

<span>def</span> <span>get_harmless_instructions</span>():
    dataset = load_dataset(<span>&#39;mlabonne/harmless_alpaca&#39;</span>)
    <span>return</span> reformat_texts(dataset[<span>&#39;train&#39;</span>][<span>&#39;text&#39;</span>]), reformat_texts(dataset[<span>&#39;test&#39;</span>][<span>&#39;text&#39;</span>])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
</code></pre>
<p>Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can&#39;t directly load a custom model using <code>HookedTransformer</code>. Here, I use a trick described in FailSpy&#39;s notebook to download a custom model and rename it as <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>. Load in <code>torch.float16</code> format if your GPU is not compatible with BF16.</p>
<p>In this example, we&#39;ll use <a href="https://huggingface.co/mlabonne/Daredevil-8B">mlabonne/Daredevil-8B</a>, a mega-merge created with DARE TIES (see my article about <a href="https://huggingface.co/blog/mlabonne/merge-models">model merging</a>) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category.</p>
<pre><code>MODEL_ID = <span>&#34;mlabonne/Daredevil-8B&#34;</span>
MODEL_TYPE = <span>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span>

<span># Download and load model</span>
!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}

<span># Load model and tokenizer</span>
model = HookedTransformer.from_pretrained_no_processing(
    MODEL_TYPE,
    local_files_only=<span>True</span>,
    dtype=torch.bfloat16,
    default_padding_side=<span>&#39;left&#39;</span>
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)
tokenizer.padding_side = <span>&#39;left&#39;</span>
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>We can now tokenize our datasets. We&#39;re using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I&#39;m limiting it to 256 here.</p>
<pre><code><span>def</span> <span>tokenize_instructions</span>(<span>tokenizer, instructions</span>):
    <span>return</span> tokenizer.apply_chat_template(
        instructions,
        padding=<span>True</span>,
        truncation=<span>False</span>,
        return_tensors=<span>&#34;pt&#34;</span>,
        return_dict=<span>True</span>,
        add_generation_prompt=<span>True</span>,
    ).input_ids

n_inst_train = <span>min</span>(<span>256</span>, <span>len</span>(harmful_inst_train), <span>len</span>(harmless_inst_train))

<span># Tokenize datasets</span>
harmful_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
</code></pre>
<p>Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in <code>harmful</code> and <code>harmless</code>. This is managed by the <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">transformer_lens</a> library.</p>
<pre><code><span># Define batch size based on available VRAM</span>
batch_size = <span>32</span>

<span># Initialize defaultdicts to store activations</span>
harmful = defaultdict(<span>list</span>)
harmless = defaultdict(<span>list</span>)

<span># Process the training data in batches</span>
num_batches = (n_inst_train + batch_size - <span>1</span>) // batch_size
<span>for</span> i <span>in</span> tqdm(<span>range</span>(num_batches)):
    <span>print</span>(i)
    start_idx = i * batch_size
    end_idx = <span>min</span>(n_inst_train, start_idx + batch_size)

    <span># Run models on harmful and harmless prompts, cache activations</span>
    harmful_logits, harmful_cache = model.run_with_cache(
        harmful_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>&#39;resid&#39;</span> <span>in</span> hook_name,
        device=<span>&#39;cpu&#39;</span>,
        reset_hooks_end=<span>True</span>
    )
    harmless_logits, harmless_cache = model.run_with_cache(
        harmless_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>&#39;resid&#39;</span> <span>in</span> hook_name,
        device=<span>&#39;cpu&#39;</span>,
        reset_hooks_end=<span>True</span>
    )

    <span># Collect and store the activations</span>
    <span>for</span> key <span>in</span> harmful_cache:
        harmful[key].append(harmful_cache[key])
        harmless[key].append(harmless_cache[key])

    <span># Flush RAM and VRAM</span>
    <span>del</span> harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.collect()
    torch.cuda.empty_cache()

<span># Concatenate the cached activations</span>
harmful = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmful.items()}
harmless = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmless.items()}
</code></pre>
<p>We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in <code>activation_scored</code>. </p>
<pre><code><span># Helper function to get activation index</span>
<span>def</span> <span>get_act_idx</span>(<span>cache_dict, act_name, layer</span>):
    key = (act_name, layer)
    <span>return</span> cache_dict[utils.get_act_name(*key)]

<span># Compute difference of means between harmful and harmless activations at intermediate layers</span>
activation_layers = [<span>&#34;resid_pre&#34;</span>, <span>&#34;resid_mid&#34;</span>, <span>&#34;resid_post&#34;</span>]
activation_refusals = defaultdict(<span>list</span>)

<span>for</span> layer_num <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers):
    pos = -<span>1</span>  <span># Position index</span>

    <span>for</span> layer <span>in</span> activation_layers:
        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=<span>0</span>)
        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(
            dim=<span>0</span>
        )

        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.norm()
        activation_refusals[layer].append(refusal_dir)

<span># Get all calculated potential refusal directions, sort them in descending order based on their mean</span>
<span># Use a subset of layers if certain activations are not promising</span>
selected_layers = [<span>&#34;resid_pre&#34;</span>]
activation_scored = <span>sorted</span>(
    [
        activation_refusals[layer][l - <span>1</span>]
        <span>for</span> l <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers)
        <span>for</span> layer <span>in</span> selected_layers
    ],
    key=<span>lambda</span> x: <span>abs</span>(x.mean()),
    reverse=<span>True</span>,
)
</code></pre>
<p>The final step of the process consists of evaluating the refusal directions we calculated. To do this, we&#39;re going to apply the refusal direction to each residual stream and each block during inference. In the following snippet, we get generations for four test harmful instructions and 20 blocks (or layers).</p>
<pre><code><span>def</span> <span>_generate_with_hooks</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    tokens: Int[Tensor, <span>&#34;batch_size seq_len&#34;</span>],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    fwd_hooks=[],</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    all_tokens = torch.zeros(
        (tokens.shape[<span>0</span>], tokens.shape[<span>1</span>] + max_tokens_generated),
        dtype=torch.long,
        device=tokens.device,
    )
    all_tokens[:, : tokens.shape[<span>1</span>]] = tokens
    <span>for</span> i <span>in</span> <span>range</span>(max_tokens_generated):
        <span>with</span> model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_tokens[:, : -max_tokens_generated + i])
            next_tokens = logits[:, -<span>1</span>, :].argmax(
                dim=-<span>1</span>
            )  <span># greedy sampling (temperature=0)</span>
            all_tokens[:, -max_tokens_generated + i] = next_tokens
    <span>return</span> tokenizer.batch_decode(
        all_tokens[:, tokens.shape[<span>1</span>] :], skip_special_tokens=<span>True</span>
    )

<span>def</span> <span>get_generations</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    instructions: <span>List</span>[<span>str</span>],</span>
<span>    fwd_hooks=[],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    batch_size: <span>int</span> = <span>4</span>,</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    generations = []
    <span>for</span> i <span>in</span> tqdm(<span>range</span>(<span>0</span>, <span>len</span>(instructions), batch_size)):
        tokens = tokenize_instructions(
            tokenizer, instructions=instructions[i : i + batch_size]
        )
        generation = _generate_with_hooks(
            model,
            tokenizer,
            tokens,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)
    <span>return</span> generations

<span># Inference-time intervention hook</span>
<span>def</span> <span>direction_ablation_hook</span>(<span></span>
<span>    activation: Float[Tensor, <span>&#34;... d_act&#34;</span>],</span>
<span>    hook: HookPoint,</span>
<span>    direction: Float[Tensor, <span>&#34;d_act&#34;</span>],</span>
<span></span>):
    <span>if</span> activation.device != direction.device:
        direction = direction.to(activation.device)
    proj = (
        einops.einsum(
            activation, direction.view(-<span>1</span>, <span>1</span>), <span>&#34;... d_act, d_act single -&gt; ... single&#34;</span>
        )
        * direction
    )
    <span>return</span> activation - proj

<span># Testing baseline</span>
N_INST_TEST = <span>4</span>
baseline_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition)</span>
EVAL_N = <span>20</span>  <span># Evaluate how many of the top N potential directions</span>
evals = []
<span>for</span> refusal_dir <span>in</span> tqdm(activation_scored[:EVAL_N]):
    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)
    fwd_hooks = [
        (utils.get_act_name(act_name, layer), hook_fn)
        <span>for</span> layer <span>in</span> <span>list</span>(<span>range</span>(model.cfg.n_layers))
        <span>for</span> act_name <span>in</span> activation_layers
    ]
    intervention_generations = get_generations(
        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks
    )
    evals.append(intervention_generations)
</code></pre>
<p>We stored all the generations in the <code>evals</code> list. We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I&#39;m automatically excluding responses containing &#34;I cannot&#34; and &#34;I can&#39;t&#34; to filter out unwanted answers.</p>
<p>If you can&#39;t find a layer that satisfies these requirements, you might want to test other residual streams in the previous <code>selected_layers</code> list, other instructions, additional blocks, etc.</p>
<pre><code><span># Print generations for human evaluation</span>
blacklist = [<span>&#34;I cannot&#34;</span>, <span>&#34;I can&#39;t&#34;</span>]
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>print</span>(<span>f&#34;\033[1mINSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>&#34;</span>)
    <span>print</span>(<span>f&#34;\nBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>\033[0m&#34;</span>)
    <span>for</span> layer_candidate <span>in</span> <span>range</span>(EVAL_N):
        <span>if</span> <span>not</span> <span>any</span>(word <span>in</span> evals[layer_candidate][i] <span>for</span> word <span>in</span> blacklist):
            <span>print</span>(<span>f&#34;\n---\n\nLAYER CANDIDATE #<span>{layer_candidate}</span> INTERVENTION COMPLETION:&#34;</span>)
            <span>print</span>(evals[layer_candidate][i])
</code></pre>
<p>In my case, the layer candidate 9 managed to provide uncensored answer for the four instructions. This is the one that we will select for the refusal direction. In the following, we implement weight orthogonalization to modify the weights and prevent the model from creating outputs with this direction. You can verify that the model is successfully uncensored by printing the completions.</p>
<pre><code><span>def</span> <span>get_orthogonalized_matrix</span>(<span></span>
<span>    matrix: Float[Tensor, <span>&#34;... d_model&#34;</span>], vec: Float[Tensor, <span>&#34;d_model&#34;</span>]</span>
<span></span>) -&gt; Float[Tensor, <span>&#34;... d_model&#34;</span>]:
    proj = (
        einops.einsum(
            matrix, vec.view(-<span>1</span>, <span>1</span>), <span>&#34;... d_model, d_model single -&gt; ... single&#34;</span>
        )
        * vec
    )
    <span>return</span> matrix - proj

<span># Select the layer with the highest potential refusal direction</span>
LAYER_CANDIDATE = <span>9</span>
refusal_dir = activation_scored[LAYER_CANDIDATE]

<span># Orthogonalize the model&#39;s weights</span>
<span>if</span> refusal_dir.device != model.W_E.device:
    refusal_dir = refusal_dir.to(model.W_E.device)
model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

<span>for</span> block <span>in</span> tqdm(model.blocks):
    <span>if</span> refusal_dir.device != block.attn.W_O.device:
        refusal_dir = refusal_dir.to(block.attn.W_O.device)
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

<span># Generate text with abliterated model</span>
orthogonalized_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Print generations</span>
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>if</span> <span>len</span>(baseline_generations) &gt; i:
        <span>print</span>(<span>f&#34;INSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>&#34;</span>)
        <span>print</span>(<span>f&#34;\033[92mBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>&#34;</span>)
    <span>print</span>(<span>f&#34;\033[91mINTERVENTION COMPLETION:\n<span>{evals[LAYER_CANDIDATE][i]}</span>&#34;</span>)
    <span>print</span>(<span>f&#34;\033[95mORTHOGONALIZED COMPLETION:\n<span>{orthogonalized_generations[i]}</span>\n&#34;</span>)
</code></pre>
<p>We&#39;re now ready to use the model. We convert it back to the Hugging Face format and upload it to the HF hub.</p>
<pre><code><span># Convert model back to HF safetensors</span>
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[<span>&#34;embed.W_E&#34;</span>].cpu())

<span>for</span> l <span>in</span> <span>range</span>(model.cfg.n_layers):
    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict[<span>f&#34;blocks.<span>{l}</span>.attn.W_O&#34;</span>], <span>&#34;n h m-&gt;m (n h)&#34;</span>, n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict[<span>f&#34;blocks.<span>{l}</span>.mlp.W_out&#34;</span>], <span>0</span>, <span>1</span>).contiguous()
    )

hf_model.push_to_hub(<span>f&#34;<span>{MODEL_ID}</span>-abliterated&#34;</span>)
<span># hf_model.push_to_hub(f&#34;{MODEL_ID}-abliterated&#34;)</span>
</code></pre>
<h2>
	<a rel="nofollow" href="#‚öñÔ∏è-dpo-fine-tuning" id="‚öñÔ∏è-dpo-fine-tuning">
		<span><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		‚öñÔ∏è DPO Fine-Tuning
	</span>
</h2>
<p>I evaluated the abliterated and source models from the previous section on the Open LLM Leaderboard and on Nous&#39; benchmark suite. Here are the results:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ECCejII.png"><img alt="" src="https://i.imgur.com/ECCejII.png"/></a></p>
<p>As you can see, the source model significantly outperforms Llama 3 8B Instruct. However, we observe a performance drop in the ablated version across all benchmarks. The ablation process successfully uncensored it but also degraded the model&#39;s quality.</p>
<p>To address this issue, an idea consists of further training our abliterated model to heal it. Like most fine-tuned models, Llama 3 8B Instruct is quite brittle when it comes to supervised fine-tuning. An additional SFT would likely break the model&#39;s performance.</p>
<p>Alternatively, preference alignment is quite light and shouldn&#39;t lobotomize our abliterated model. DPO is a good candidate here for its ease of use and good track record. To implement it, I used <a rel="nofollow" href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing">LazyAxolotl</a> with the <a href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k">mlabonne/orpo-dpo-mix-40k</a> dataset. Here&#39;s the configuration I used:</p>
<pre><code><span>base_model:</span> <span>mlabonne/Daredevil-8B-abliterated</span>
<span>model_type:</span> <span>LlamaForCausalLM</span>
<span>tokenizer_type:</span> <span>AutoTokenizer</span>

<span>load_in_8bit:</span> <span>false</span>
<span>load_in_4bit:</span> <span>true</span>
<span>strict:</span> <span>false</span>
<span>save_safetensors:</span> <span>true</span>

<span>rl:</span> <span>dpo</span>
<span>chat_template:</span> <span>chatml</span>
<span>datasets:</span>
  <span>-</span> <span>path:</span> <span>mlabonne/orpo-dpo-mix-40k-flat</span>
    <span>split:</span> <span>train</span>
    <span>type:</span> <span>chatml.intel</span>

<span>dataset_prepared_path:</span>
<span>val_set_size:</span> <span>0.0</span>
<span>output_dir:</span> <span>./out</span>

<span>adapter:</span> <span>qlora</span>
<span>lora_model_dir:</span>

<span>sequence_len:</span> <span>2048</span>
<span>sample_packing:</span> <span>false</span>
<span>pad_to_sequence_len:</span> <span>false</span>

<span>lora_r:</span> <span>64</span>
<span>lora_alpha:</span> <span>32</span>
<span>lora_dropout:</span> <span>0.05</span>
<span>lora_target_linear:</span> <span>true</span>
<span>lora_fan_in_fan_out:</span>

<span>wandb_project:</span> <span>axolotl</span>
<span>wandb_entity:</span>
<span>wandb_watch:</span>
<span>wandb_name:</span>
<span>wandb_log_model:</span>

<span>gradient_accumulation_steps:</span> <span>8</span>
<span>micro_batch_size:</span> <span>1</span>
<span>num_epochs:</span> <span>1</span>
<span>optimizer:</span> <span>paged_adamw_8bit</span>
<span>lr_scheduler:</span> <span>cosine</span>
<span>learning_rate:</span> <span>5e-6</span>
<span>train_on_inputs:</span> <span>false</span>
<span>group_by_length:</span> <span>false</span>

<span>bf16:</span> <span>auto</span>
<span>fp16:</span>
<span>tf32:</span>

<span>gradient_checkpointing:</span> <span>true</span>
<span>early_stopping_patience:</span>
<span>resume_from_checkpoint:</span>
<span>local_rank:</span>
<span>logging_steps:</span> <span>1</span>
<span>xformers_attention:</span>
<span>flash_attention:</span> <span>true</span>
<span>warmup_steps:</span> <span>100</span>
<span>evals_per_epoch:</span> <span>0</span>
<span>eval_table_size:</span>
<span>eval_table_max_new_tokens:</span> <span>128</span>
<span>saves_per_epoch:</span> <span>1</span>
<span>debug:</span>
<span>deepspeed:</span> <span>deepspeed_configs/zero2.json</span>
<span>weight_decay:</span> <span>0.0</span>
<span>special_tokens:</span>
  <span>pad_token:</span> <span>&lt;|end_of_text|&gt;</span>
</code></pre>
<p>I trained it using 6xA6000 GPUs with DeepSpeed ZeRO-2. The training took about 6 hours and 45 minutes. Here are the training curves I got from W&amp;B:</p>
<p><a rel="nofollow" href="https://i.imgur.com/nVcJYuu.png"><img alt="" src="https://i.imgur.com/nVcJYuu.png"/></a></p>
<p>It automatically uploaded the DPO fine-tuned model, called <a href="https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated">mlabonne/NeuralDaredevil-8B-abliterated</a>. To see if it fixed our abliterated version, I evaluated it on the same benchmarks:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ChDwx4r.png"><img alt="" src="https://i.imgur.com/ChDwx4r.png"/></a></p>
<p>We can see that this additional training allowed us to recover most of the performance drop due to abliteration. One area where the model doesn&#39;t improve is GSM8K, a math dataset, which could mean the orpo-dpo-mix-40k would benefit from more math samples.</p>
<p>The final model is an uncensored LLM with state-of-the-art performance in the 8B category. I recommend it as an improved version of Llama 3 8B Instruct when you don&#39;t need censorship. You can play with quantized versions like GGUF in LM Studio.</p>
<h2>
	<a rel="nofollow" href="#conclusion" id="conclusion">
		<span><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Conclusion
	</span>
</h2>
<p>In this article, we introduced the concept of abliteration. This technique uses the model&#39;s activations on harmless and harmful prompts to calculate a refusal direction. It then uses this direction to modify the model&#39;s weights and ensure that we stop outputting refusals. This technique also demonstrates the fragility of safety fine-tuning and raises ethical considerations.</p>
<p>We applied abliteration to Daredevil-8B to uncensor it, which also degraded the model&#39;s performance. We then healed it using DPO to create the NeuralDaredevil-8B model, a fully uncensored and high-quality 8B LLM. Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy&#39;s <a href="https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule">MopeyMule</a>, which adopts a melancholic conversational style.</p>
<p>I hope you liked this article. If you want to see more follow me on¬†<a href="https://huggingface.co/mlabonne/">Hugging Face</a>¬†and Twitter¬†<a rel="nofollow" href="https://twitter.com/maximelabonne">@maximelabonne</a>.</p>
<h2>
	<a rel="nofollow" href="#references" id="references">
		<span><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		References
	</span>
</h2>
<ul>
<li>FailSpy, &#34;<a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a>,&#34; GitHub, 2024.</li>
<li>Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, &#34;<a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">Refusal in LLMs is mediated by a single direction</a>,&#34; Lesswrong, 2024.</li>
</ul>
<!-- HTML_TAG_END --></div>
			</div></div>
  </body>
</html>
