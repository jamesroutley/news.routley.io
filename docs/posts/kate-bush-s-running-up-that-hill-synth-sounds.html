<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reverbmachine.com/blog/kate-bush-running-up-that-hill-synth-sounds/">Original</a>
    <h1>Kate Bush’s “Running Up That Hill” synth sounds</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Hello! I’ve started to run a few more servers recently
(<a href="https://nginx-playground.wizardzines.com">nginx playground</a>,
<a href="https://messwithdns.net">mess with dns</a>,
<a href="https://dns-lookup.jvns.ca">dns lookup</a>), so I’ve been
thinking about monitoring.</p>
<p>It wasn’t initially totally obvious to me how to monitor these websites, so I
wanted to quickly write up what how I did it.</p>
<p>I’m not going to talk about how to monitor Big Serious Mission Critical
websites at all, only tiny unimportant websites.</p>
<h3 id="goal-spend-approximately-0-time-on-operations">goal: spend approximately 0 time on operations</h3>
<p>I want the sites to mostly work, but I also want to spend approximately 0% of
my time on the ongoing operations.</p>
<p>I was initially very wary of running servers at all because at my last job I
was on a <sup>24</sup>⁄<sub>7</sub> oncall rotation for some critical services, and in my mind “being
responsible for servers” meant “get woken up at 2am to fix the servers” and
“have lots of complicated dashboards”.</p>
<p>So for a while I only made static websites so that I wouldn’t have to think
about servers.</p>
<p>But eventually I realized that any server I was going to write was going to be
very low stakes, if they occasionally go down for 2 hours it’s no big deal, and
I could just set up some very simple monitoring to help keep them running.</p>
<h3 id="not-having-monitoring-sucks">not having monitoring sucks</h3>
<p>At first I didn’t set up any monitoring for my servers at all. This had the
extremely predictable outcome of – sometimes the site broke, and I didn’t find
out about it until somebody told me!</p>
<h3 id="step-1-an-uptime-checker">step 1: an uptime checker</h3>
<p>The first step was to set up an uptime checker. There are tons of these out
there, the ones I’m using right now are <a href="https://updown.io/">updown.io</a> and
<a href="https://uptimerobot.com/">uptime robot</a>. I like updown’s user interface and
<a href="https://updown.io/#pricing">pricing</a> structure more (it’s per request instead of a monthly fee), but uptime
robot has a more generous free tier.</p>
<p>These</p>
<ol>
<li>check that the site is up</li>
<li>if it goes down, it emails me</li>
</ol>
<p>I find that email notifications are a good level for me, I’ll find out pretty
quickly if the site goes down but it doesn’t wake me up or anything.</p>
<h3 id="step-2-an-end-to-end-healthcheck">step 2: an end-to-end healthcheck</h3>
<p>Next, let’s talk about what “check that the site is up” actually means.</p>
<p>At first I just made one of my healthcheck endpoints a function that returned
<code>200 OK</code> no matter what.</p>
<p>This is kind of useful – it told me that the server was on!</p>
<p>But unsurprisingly I ran into problems because it wasn’t checking that the API
was actually <em>working</em> – sometimes the healthcheck succeeded even though the
rest of the service had actually gotten into a bad state.</p>
<p>So I updated it to actually make a real API request and make sure it
succeeded.</p>
<p>All of my services do very few things (the nginx playground has just 1
endpoint), so it’s pretty easy to set up a healthcheck that actually runs
through most of the actions the service is supposed to do.</p>
<p>Here’s what the end-to-end healthcheck handler for the nginx playground looks
like. It’s very basic: it just makes another POST request (to itself) and
checks if that request succeeds or fails.</p>
<pre><code>func healthHandler(w http.ResponseWriter, r *http.Request) {
	// make a request to localhost:8080 with `healthcheckJSON` as the body
	// if it works, return 200
	// if it doesn&#39;t, return 500
	client := http.Client{}
	resp, err := client.Post(&#34;http://localhost:8080/&#34;, &#34;application/json&#34;, strings.NewReader(healthcheckJSON))
	if err != nil {
		log.Println(err)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	if resp.StatusCode != http.StatusOK {
		log.Println(resp.StatusCode)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	w.WriteHeader(http.StatusOK)
}
</code></pre>
<h3 id="healthcheck-frequency-hourly">healthcheck frequency: hourly</h3>
<p>Right now I’m running most of my healthchecks every hour, and some every 30
minutes.</p>
<p>I run them hourly because updown.io’s pricing is per healthcheck, I’m
monitoring 18 different URLs, and I wanted to keep my healthcheck budget pretty
minimal at $5/year.</p>
<p>Taking an hour to find out that one of these websites has gone down seems ok to
me – if there is a problem there’s no guarantee I’ll get to fixing it all that
quickly anyway.</p>
<p>If it were free to run them more often I’d probably run them every 5-10 minutes instead.</p>
<h3 id="step-3-automatically-restart-if-the-healthcheck-fails">step 3: automatically restart if the healthcheck fails</h3>
<p>Some of my websites are on fly.io, and fly has a pretty standard feature where
I can configure a HTTP healthcheck for a service and restart the service if the
healthcheck starts failing.</p>
<p>“Restart a lot” is a very useful strategy to paper over bugs that I haven’t
gotten around to fixing yet – for a while the nginx playground had a process
leak where <code>nginx</code> processes weren’t getting terminated, so the server kept
running out of RAM.</p>
<p>With the healthcheck, the result of this was that every day or so, this would happen:</p>
<ul>
<li>the server ran out of RAM</li>
<li>the healthcheck started failing</li>
<li>it get restarted</li>
<li>everything was fine again</li>
<li>repeat the whole saga again some number of hours later</li>
</ul>
<p>Eventually I got around to actually fixing the process leak, but it was nice to
have a workaround in place that could keep things running while I was
procrastinating fixing the bug.</p>
<p>These healthchecks to decide whether to restart the service run more often: every 5 minutes or so.</p>
<h3 id="this-is-not-the-best-way-to-monitor-big-services">this is not the best way to monitor Big Services</h3>
<p>This is probably obvious and I said this already at the beginning, but “write
one HTTP healthcheck” is not the best approach for monitoring a large complex
service. But I won’t go into that because that’s not what this post is about.</p>
<h3 id="it-s-been-working-well-so-far">it’s been working well so far!</h3>
<p>I originally wrote this post 3 months ago in April, but I waited until now to
publish it to make sure that the whole setup was working.</p>
<p>It’s made a pretty big difference – before I was having some very silly
downtime problems, and now for the last few months the sites have been up
99.95% of the time!</p>
</div></div>
  </body>
</html>
