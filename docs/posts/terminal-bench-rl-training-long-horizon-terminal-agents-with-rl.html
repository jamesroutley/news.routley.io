<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Danau5tin/terminal-bench-rl">Original</a>
    <h1>Show HN: Terminal-Bench-RL: Training Long-Horizon Terminal Agents with RL</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>TL;DR:</strong></p>
<ul dir="auto">
<li>I successfully built stable RL training infrastructure that scales to 32x H100 GPUs across 4 bare metal nodes for training long-horizon terminal-based coding agents.</li>
<li>In doing so, I developed Terminal-Agent-Qwen3-32b to become the <strong>highest scoring Qwen3 agent on <a href="https://github.com/laude-institute/terminal-bench">terminal-bench</a></strong>. WITHOUT training! (currently under submission):
<ul dir="auto">
<li>Unfortunately I am too GPU poor to train a SOTA coding agent 😅 (estimated £30k-£50k in compute required), but if anyone has the GPUs, this project should get you there!</li>
</ul>
</li>
</ul>
<p dir="auto">This project builds upon the <a href="https://github.com/rllm-org/rllm">rLLM framework</a> developed by UC Berkeley Sky Lab, extending it with custom environments and infrastructure specifically designed for terminal-based agent training.</p>

<ul dir="auto">
<li><a href="#-training-on-1m-worth-of-compute">💻💰 Training on $1M worth of compute</a>
<ul dir="auto">
<li><a href="#other-training-runs">Other training runs</a></li>
</ul>
</li>
<li><a href="#-placing-a-spot-on-the-terminal-bench-leaderboard">🏆 Placing a spot on the Terminal Bench Leaderboard</a>
<ul dir="auto">
<li><a href="#%EF%B8%8F-action-based-architecture">🏗️ Action-Based Architecture</a></li>
</ul>
</li>
<li><a href="#training-details">Training details</a>
<ul dir="auto">
<li><a href="#%EF%B8%8F-reward-design">⚖️ Reward Design</a></li>
<li><a href="#-answer-verification-65-weight">✅ Answer Verification (65% weight)</a></li>
<li><a href="#-llm-as-a-judge-35-weight">🤖 LLM-as-a-Judge (35% weight)</a>
<ul dir="auto">
<li><a href="#-judge-evaluation-system">🧪 Judge Evaluation System</a></li>
<li><a href="#-dynamic-llm-judge-switching">🔄 Dynamic LLM Judge Switching</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%EF%B8%8F-rllm-integration-architecture">🏗️ rLLM Integration Architecture</a>
<ul dir="auto">
<li><a href="#terminal-agent-terminalbenchagent">Terminal Agent (<code>TerminalBenchAgent</code>)</a></li>
<li><a href="#docker-environment-dockerisolatedenv">Docker Environment (<code>DockerIsolatedEnv</code>)</a></li>
</ul>
</li>
<li><a href="#-training--rollout-details">🔄 Training &amp; Rollout Details</a>
<ul dir="auto">
<li><a href="#-rollout-strategy">🔁 Rollout Strategy</a></li>
<li><a href="#%EF%B8%8F-training-configuration-presets">⚙️ Training Configuration Presets</a></li>
<li><a href="#-key-hyperparameters-production-config">📊 Key Hyperparameters (Production Config)</a></li>
</ul>
</li>
<li><a href="#%EF%B8%8F-dataset-details">🗂️ Dataset Details</a>
<ul dir="auto">
<li><a href="#-dataset-structure">📊 Dataset Structure</a></li>
<li><a href="#-training-environment-creation">🐳 Training Environment Creation</a></li>
<li><a href="#-docker-resource-management">🧹 Docker Resource Management</a></li>
<li><a href="#-dataset-preparation-pipeline">📂 Dataset Preparation Pipeline</a></li>
</ul>
</li>
<li><a href="#-getting-started">🚀 Getting Started</a>
<ul dir="auto">
<li><a href="#development-setup">Development Setup</a></li>
<li><a href="#terminal-bench-evaluation-reproduction">Terminal Bench Evaluation Reproduction</a></li>
<li><a href="#training-deployment">Training Deployment</a>
<ul dir="auto">
<li><a href="#single-node-training">Single Node Training</a></li>
<li><a href="#multi-node-training">Multi-Node Training</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#-future-improvements">🔮 Future Improvements</a>
<ul dir="auto">
<li><a href="#-full-training-run">🚀 Full Training Run</a></li>
<li><a href="#-curriculum-learning">🤓 Curriculum learning</a></li>
<li><a href="#-dataset-expansion">📊 Dataset Expansion</a></li>
<li><a href="#-smart-data-filtering">🎯 Smart Data Filtering</a></li>
</ul>
</li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">💻💰 Training on $1M worth of compute</h2><a id="user-content--training-on-1m-worth-of-compute" aria-label="Permalink: 💻💰 Training on $1M worth of compute" href="#-training-on-1m-worth-of-compute"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This image shows my training code running at full throttle on 32x H100&#39;s, distributed across a 4x bare metal node cluster, training Qwen3-32B. Thank you <a href="https://hyperbolic.ai/" rel="nofollow">Hyperbolic</a> for such a streamlined experience! This was fun!
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/32xh100_hyperbolic.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/32xh100_hyperbolic.png" alt="32_h100_gpus_hyperbolic_screenshot"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/32xh100s.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/32xh100s.png" alt="32_h100_gpus_nvtop_screenshot"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/32xh100s_ray.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/32xh100s_ray.png" alt="32_h100_gpus_ray_screenshot"/></a></p>
<p dir="auto">Due to the extreme cost of this level of compute, I was not able to run it forever! So I made sure it worked and also ran the code on less extravagent hardware setups too.</p>

<p dir="auto">I also ran Qwen3-32B training for longer on a 2x bare metal node cluster with 16x H100s:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/16xh100s_ray_3_100_pc.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/16xh100s_ray_3_100_pc.png" alt="16_h100_gpus_ray_screenshot"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/16xh100_hyperbolic.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/16xh100_hyperbolic.png" alt="16_h100_gpus_hyperbolic_screenshot"/></a></p>
<p dir="auto">Also 1 VM instance with 8x H100s:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/8xh100s_at_76percent_uasge.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/8xh100s_at_76percent_uasge.png" alt="8_h100_gpus_nvtop_screenshot"/></a></p>
<p dir="auto">My longest training run was using 2xA100s on a single VM instance, where I trained Qwen3-8B for over 60 steps:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/readme_images/8b_run_reward.png"><img src="https://github.com/Danau5tin/terminal-bench-rl/raw/main/readme_images/8b_run_reward.png" alt="8b_reward_screenshot"/></a>
Note: I did not expect the 8B to begin learning the complex behaviours required to solve the tasks in the dataset. However it was great to run the training through the dataset and ensure the code is stable.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">🏆 Placing a spot on the Terminal Bench Leaderboard</h2><a id="user-content--placing-a-spot-on-the-terminal-bench-leaderboard" aria-label="Permalink: 🏆 Placing a spot on the Terminal Bench Leaderboard" href="#-placing-a-spot-on-the-terminal-bench-leaderboard"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://www.tbench.ai/" rel="nofollow">Terminal bench</a> is a brilliant benchmark created by Stanford and <a href="https://www.laude.org/" rel="nofollow">Laude Institute</a> to quantify agents&#39; ability to complete complex tasks in the terminal.</p>
<p dir="auto">Through prompt engineering &amp; custom tool design, my Qwen3-32B agent outperformed Stanford&#39;s Terminus-Qwen3-235B-30A MoE agent, as well as Deepseek R1 &amp; OpenAI&#39;s GPT-4.1 with Codex agent, to become the highest scoring Qwen3 agent on the leaderboard.</p>
<p dir="auto">The results.json for the eval run can be found <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/tbench_eval_run_results.json">here</a>.</p>
<p dir="auto">I am sure that with the compute budget for training, my agent would climb the leaderboard significantly.</p>

<p dir="auto">My motivation behind this entire project was to place on the leaderboard of terminal bench by using RL to train a sophisticated LLM agent. In order to do so, I developed the tools (inspired by Claude Code) which a capable AI agent would use to help complete complex terminal/coding tasks, as well as a <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/src/agent_core/system_prompt.md">system message</a> which encouraged the agent to use those tools and approach the task in a specific way.</p>
<p dir="auto">These tools can be found <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/src/agent_core/env_components">here</a> and include:</p>
<ul dir="auto">
<li><strong>📝 Todo Management</strong>: Planning and tracking task progress</li>
<li><strong>📁 File Operations</strong>: Read, write, and edit files</li>
<li><strong>🔍 Search Tools</strong>: Grep, glob, and ls for file exploration</li>
<li><strong>⚡ Bash Execution</strong>: Run terminal commands with output capture</li>
<li><strong>🗒️ Scratchpad</strong>: Space for note-taking</li>
<li><strong>👍 Task Completion</strong>: Signal when the agent believes the task is complete</li>
</ul>
<p dir="auto">Note: Technically the agent could have access to only the bash tool and would still have the same capabilites as all these tools above. Saving the development time and maintenance. However by providing clear APIs to specific tools, it enables the agent to understand and leverage tools much more effectively.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">🏗️ Action-Based Architecture</h3><a id="user-content-️-action-based-architecture" aria-label="Permalink: 🏗️ Action-Based Architecture" href="#️-action-based-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The agent communicates through a structured XML/YAML format that ensures reliable parsing and execution:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&lt;todo&gt;
operations:
  - action: add
    content: &#34;Find and analyze all Python test files&#34;
  - action: add
    content: &#34;Run pytest and fix any failing tests&#34;
view_all: true
&lt;/todo&gt;"><pre>&lt;<span>todo</span>&gt;
operations:
  - action: add
    content: &#34;Find and analyze all Python test files&#34;
  - action: add
    content: &#34;Run pytest and fix any failing tests&#34;
view_all: true
&lt;/<span>todo</span>&gt;</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="&lt;bash&gt;
cmd: &#39;find . -name &#34;*.py&#34; -path &#34;*/test*&#34; | head -10&#39;
timeout_secs: 30
&lt;/bash&gt;"><pre>&lt;<span>bash</span>&gt;
cmd: &#39;find . -name &#34;*.py&#34; -path &#34;*/test*&#34; | head -10&#39;
timeout_secs: 30
&lt;/<span>bash</span>&gt;</pre></div>
<p dir="auto">This architecture provides:</p>
<ul dir="auto">
<li><strong>Type Safety</strong>: Each action (bash, file, search, todo) has a dedicated handler with validation</li>
<li><strong>Error Recovery</strong>: Malformed YAML triggers helpful error messages guiding the agent to correct syntax</li>
<li><strong>Sequential Execution</strong>: Actions are processed one at a time with mandatory stop-and-wait behavior</li>
<li><strong>Consistent Feedback</strong>: Every action returns structured results the agent can learn from and adjust its plan</li>
</ul>
<p dir="auto">As well as developing these tools, I also wrote out a <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/src/agent_core/system_prompt.md">system prompt</a> which encourages best practices such as:</p>
<ul dir="auto">
<li><strong>Structured Task Execution</strong>: Clear problem approach phases (Planning → Exploration → Execution → Verification)</li>
<li><strong>Multi-Turn Interaction</strong>: Action-environment cycle with proper stop-and-wait behavior</li>
<li><strong>Mandatory Todo Management</strong>: Required initial planning and continuous task tracking</li>
<li><strong>Read-Only Exploration</strong>: Gather information before making any changes</li>
</ul>
<p dir="auto">With this system message &amp; tool combination + a capable LLM (I chose Qwen3-32B), I was able to place 19th on the terminal bench leaderboard (currently under submission) with a score of 13.75%. This outperformed:</p>
<ul dir="auto">
<li>Terminus agent with Qwen3-235B by Stanford</li>
<li>Terminus agent with Deepseek-R1 by Stanford</li>
<li>Codex agent with GPT-4.1 by OpenAI</li>
<li>Codex agent with codex-mini by OpenAI</li>
</ul>
<p dir="auto">The agent can be seen <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/evaluation/terminal_bench_eval">here</a>.</p>
<p dir="auto">I would be extremely excited to see where Qwen3-32B would be on the leaderboard if I could afford to pay for the compute cost of a proper RL run!</p>
<hr/>

<p dir="auto"><strong>As mentioned above, the compute costs of a full training run on a 32B LLM for long horizon terminal/coding tasks are not accessible for me</strong>, however the training code and dataset is ready to go and has been tested to train stably on hardware setups from 2x A100s all the way to 32x H100s.</p>

<p dir="auto">To provide meaningful supervision during RL, rewards were computed using <strong>two complementary methods</strong>:</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">✅ Answer Verification (65% weight)</h3><a id="user-content--answer-verification-65-weight" aria-label="Permalink: ✅ Answer Verification (65% weight)" href="#-answer-verification-65-weight"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Each training datapoint included Python unit tests to verify task completion</li>
<li>Tests were assigned individual weights to provide granular partial credit</li>
<li>Test execution ran in the isolated Docker container in which the agent completed its work</li>
<li>Weighted scoring: passed tests contributed their weight to the final test score</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">🤖 LLM-as-a-Judge (35% weight)</h3><a id="user-content--llm-as-a-judge-35-weight" aria-label="Permalink: 🤖 LLM-as-a-Judge (35% weight)" href="#-llm-as-a-judge-35-weight"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Used <strong>Claude-4-Sonnet</strong> as an external judge to evaluate agent behavior</li>
<li>Evaluated four primary components:
<ul dir="auto">
<li><strong>Action Output Success (35%)</strong>: Valid XML actions, successful parsing, error recovery</li>
<li><strong>Todo Usage &amp; Planning (25%)</strong>: Initial planning, task tracking, continuous updates</li>
<li><strong>Phase Adherence (25%)</strong>: Following the 5-phase workflow (Planning → Exploration → Refinement → Execution → Verification)</li>
<li><strong>Tool Usage Effectiveness (15%)</strong>: Appropriate tool selection, purposeful actions</li>
</ul>
</li>
<li>Applied quality modifiers for error recovery, discovery quality, and efficiency</li>
<li>Penalized overthinking without action, gaming behaviors, and phase violations</li>
<li>Scored on HOW the agent worked, not WHETHER the task was completed</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">🧪 Judge Evaluation System</h4><a id="user-content--judge-evaluation-system" aria-label="Permalink: 🧪 Judge Evaluation System" href="#-judge-evaluation-system"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To ensure the LLM judge provided accurate and consistent scoring during RL training, I developed a simple <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/evaluation/llm_as_a_judge_evals">evaluation system</a>:</p>
<ul dir="auto">
<li>Created test cases showing different agent trajectories</li>
<li>Tested multiple LLM models as judges including: Kimi K2, Qwen-3-Coder, Claude Sonnet 4, Claude Haiku 3.5, to compare scoring accuracy.</li>
<li>Found Claude Sonnet 4 provided the most consistent and accurate scoring, correctly identifying issues like lack of exploration and overthinking
<ul dir="auto">
<li>Unfortunately Sonnet-4 is extremely expensive, so it is not very affordable for a 32 rollout, 1650 step run! But it was the only model which understood a good from bad trajectory well enough.</li>
<li>Many other models (including Haiku 3.5) gave inflated scores to problematic agent behaviors, with some scoring 0.85-0.95 for agents that skipped critical phases</li>
</ul>
</li>
</ul>
<p dir="auto">To analyze judge model performance:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run evaluation on a specific model
uv run python evaluation/llm_as_a_judge_evals/judge_eval.py --model openrouter/openai/gpt-4.1 --attempts 3

# Generate performance report showing best models
uv run python evaluation/llm_as_a_judge_evals/report.py"><pre><span><span>#</span> Run evaluation on a specific model</span>
uv run python evaluation/llm_as_a_judge_evals/judge_eval.py --model openrouter/openai/gpt-4.1 --attempts 3

<span><span>#</span> Generate performance report showing best models</span>
uv run python evaluation/llm_as_a_judge_evals/report.py</pre></div>
<p dir="auto"><strong>Top 5 Judge Models Performance:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Pass Rate</th>
<th>Avg Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Claude Sonnet 4</td>
<td>46.67%</td>
<td><strong>0.26</strong></td>
</tr>
<tr>
<td>2</td>
<td>Claude 3.5 Haiku</td>
<td>46.67%</td>
<td>0.70</td>
</tr>
<tr>
<td>3</td>
<td>Qwen3 Coder</td>
<td>26.67%</td>
<td>0.76</td>
</tr>
<tr>
<td>4</td>
<td>Devstral Medium</td>
<td>23.33%</td>
<td>0.50</td>
</tr>
<tr>
<td>5</td>
<td>Kimi K2</td>
<td>23.33%</td>
<td>0.53</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Claude Sonnet 4 ranks #1 despite having the same pass rate as Haiku because its significantly lower average score (0.26 vs 0.70) indicates stricter, more accurate judging (on the <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/evaluation/llm_as_a_judge_evals/config.yaml">eval dataset</a>). Lower scores mean the model better identifies problematic agent behaviors that other judges miss.</p>
<p dir="auto">Other models tested include: GPT-4.1, Gemma-3-27B-IT, Qwen3-32B, and Qwen3-235B-A22B.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">🔄 Dynamic LLM Judge Switching</h4><a id="user-content--dynamic-llm-judge-switching" aria-label="Permalink: 🔄 Dynamic LLM Judge Switching" href="#-dynamic-llm-judge-switching"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To handle overloaded models, token limits or performance requirements during long training runs, the infrastructure supports hot-swapping between different LLM judge backends:</p>
<ul dir="auto">
<li><strong>Runtime switching</strong> without interrupting training process</li>
<li>Switch between Claude Code CLI and LiteLLM backends as needed</li>
<li>Useful when hitting API token limits or budget constraints</li>
<li>See <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/training_scripts/switch_judge_backend.py"><code>switch_judge_backend.py</code></a> and <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/docs/llm_judge_switching_readme.md">switching documentation</a></li>
</ul>
<p dir="auto">Example workflow:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start with Claude Code CLI
python training_scripts/launch_training.py prod_32b_8_gpus

# Need to change? Switch to LiteLLM API (Can also proivde env vars to populate into training)
python training_scripts/switch_judge_backend.py litellm anthropic/claude-3-opus-20240229

# Later, switch back
python training_scripts/switch_judge_backend.py ccode"><pre><span><span>#</span> Start with Claude Code CLI</span>
python training_scripts/launch_training.py prod_32b_8_gpus

<span><span>#</span> Need to change? Switch to LiteLLM API (Can also proivde env vars to populate into training)</span>
python training_scripts/switch_judge_backend.py litellm anthropic/claude-3-opus-20240229

<span><span>#</span> Later, switch back</span>
python training_scripts/switch_judge_backend.py ccode</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">🏗️ rLLM Integration Architecture</h2><a id="user-content-️-rllm-integration-architecture" aria-label="Permalink: 🏗️ rLLM Integration Architecture" href="#️-rllm-integration-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project extends <a href="https://github.com/rllm-org/rllm">rLLM</a>&#39;s <code>BaseAgent</code> and <code>BaseEnv</code> interfaces to create a complete RL training loop:</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Terminal Agent (<code>TerminalBenchAgent</code>)</h3><a id="user-content-terminal-agent-terminalbenchagent" aria-label="Permalink: Terminal Agent (TerminalBenchAgent)" href="#terminal-agent-terminalbenchagent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Extends rLLM&#39;s <code>BaseAgent</code> to manage multi-turn conversations between the environment and LLM</li>
<li>Maintains conversation history with system prompt, user instructions, and agent responses</li>
<li>Tracks complete trajectories with observations, actions, and rewards for GRPO training</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Docker Environment (<code>DockerIsolatedEnv</code>)</h3><a id="user-content-docker-environment-dockerisolatedenv" aria-label="Permalink: Docker Environment (DockerIsolatedEnv)" href="#docker-environment-dockerisolatedenv"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Extends rLLM&#39;s <code>BaseEnv</code> to provide isolated Docker containers for each training rollout</li>
<li>Each rollout spawns a fresh container from the task&#39;s Dockerfile specification</li>
<li>Executes agent actions through Docker, returning real terminal output as observations</li>
<li>Computes rewards via software tests (65%) and LLM judge evaluation (35%)</li>
<li>Ensures complete isolation between parallel rollouts for diverse solution exploration</li>
</ul>
<p dir="auto">The training loop follows rLLM&#39;s standard flow: reset → observation → LLM inference → action → environment step → reward → repeat. For some more details on this, see <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/docs/rllm_specific/understanding_of_agents_and_envs.md"><code>docs/rllm_specific/understanding_of_agents_and_envs.md</code></a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">🔄 Training &amp; Rollout Details</h2><a id="user-content--training--rollout-details" aria-label="Permalink: 🔄 Training &amp; Rollout Details" href="#-training--rollout-details"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project leveraged <strong>Group Relative Policy Optimization (GRPO)</strong>, which encourages the model to learn from relative advantages within a group of sampled responses, making it particularly well-suited for structured reasoning tasks.</p>

<ul dir="auto">
<li><strong>16 samples per training prompt</strong> (configurable), each generated with a <strong>temperature of 1.2</strong> to encourage diversity while maintaining coherence</li>
<li>Complete trajectory isolation via Docker containers for each rollout</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">⚙️ Training Configuration Presets</h3><a id="user-content-️-training-configuration-presets" aria-label="Permalink: ⚙️ Training Configuration Presets" href="#️-training-configuration-presets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The training infrastructure supports multiple hardware configurations through simple preset selection:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Quick test run on 2x A100s
python training_scripts/launch_training.py test_8b_2_gpus

# Production run on 8x H100s
python training_scripts/launch_training.py prod_32b_8_gpus

# Scale to 32x H100s across 4 nodes
python training_scripts/launch_training.py prod_32b_4x8_h100"><pre><span><span>#</span> Quick test run on 2x A100s</span>
python training_scripts/launch_training.py test_8b_2_gpus

<span><span>#</span> Production run on 8x H100s</span>
python training_scripts/launch_training.py prod_32b_8_gpus

<span><span>#</span> Scale to 32x H100s across 4 nodes</span>
python training_scripts/launch_training.py prod_32b_4x8_h100</pre></div>
<p dir="auto">Available presets scale from development to production:</p>
<ul dir="auto">
<li><strong><code>test_8b_2_gpus</code></strong>: Quick validation with Qwen3-8B on 2x 80GB GPUs</li>
<li><strong><code>runway_32b_4_gpus</code></strong>: Standard training with Qwen3-32B on 4x GPUs</li>
<li><strong><code>prod_32b_8_gpus</code></strong>: Production setup on single 8x GPU node</li>
<li><strong><code>prod_32b_2x8_h100</code></strong>: Multi-node training on 16x H100s (2 nodes)</li>
<li><strong><code>prod_32b_4x8_h100</code></strong>: Full scale on 32x H100s (4 nodes)</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">📊 Key Hyperparameters (Production Config)</h3><a id="user-content--key-hyperparameters-production-config" aria-label="Permalink: 📊 Key Hyperparameters (Production Config)" href="#-key-hyperparameters-production-config"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Algorithm</strong>: GRPO with rejection sampling</li>
<li><strong>Learning Rate</strong>: 1e-6 with gradient clipping (max norm = 0.1)</li>
<li><strong>Batch Configuration</strong>: Adaptive based on GPU count</li>
<li><strong>Sequence Length</strong>: 32,768 tokens max</li>
<li><strong>Max single response Length</strong>: 4,000 tokens per response</li>
<li><strong>Training Duration</strong>: 10 epochs through dataset</li>
<li><strong>Parallelization</strong>: Automatic tensor/sequence parallel sizing</li>
<li><strong>Precision</strong>: bfloat16 for efficiency</li>
<li><strong>Monitoring</strong>: WandB integration + detailed trajectory logging</li>
</ul>
<p dir="auto">The infrastructure automatically handles:</p>
<ul dir="auto">
<li>Model distribution across GPUs with optimal tensor/sequence parallelism</li>
<li>Memory optimization based on hardware (0.7-0.85 GPU utilization)</li>
<li>Docker container lifecycle management for isolated rollouts</li>
<li>Checkpoint saving and optional HuggingFace upload</li>
</ul>

<p dir="auto">As part of this repo are 331 training tasks, ranging from easy to extremely hard complexity.</p>
<blockquote>
<p dir="auto">🤓🤖 I developed a comprehensive multi-agent synthetic data pipeline powered by Claude Code + Opus-4 to generate and <strong>(importantly)</strong> validate each datapoint. The repo for this framework can be found <a href="https://github.com/Danau5tin/tbench-agentic-data-pipeline">here</a>!</p>
</blockquote>

<p dir="auto">Each training datapoint in <code>dataset/latest_verified.csv</code> contains:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;task_id&#34;: &#34;git-deployment-workflow-setup&#34;,    # Unique task identifier
    &#34;difficulty&#34;: &#34;hard&#34;,                          # easy|medium|hard|extremely_hard
    &#34;category&#34;: &#34;system-administration&#34;,           # Task category
    &#34;prompt&#34;: &#34;I need help setting up a simple CI/CD system...&#34;,  # The actual task instruction
    &#34;dockerfile&#34;: &#34;FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest\n...&#34;,  # Docker environment setup
    &#34;test_functions&#34;: &#34;def test_hook_script_executable():\n    ...&#34;,  # Pytest verification code
    &#34;test_weights&#34;: {                              # Weight for each test (for partial credit)
        &#34;test_hook_script_executable&#34;: 0.35,
        &#34;test_nginx_service_running&#34;: 0.15,
        &#34;test_deployment_works_correctly&#34;: 0.50
    },
    &#34;additional_files&#34;: {                          # Optional files to include in container
        &#34;backup_config.json&#34;: &#34;{\n  \&#34;schedules\&#34;: [...&#34;,
        &#34;collision_detector.py&#34;: &#34;#!/usr/bin/env python3\n...&#34;
    }
}"><pre>{
    <span>&#34;task_id&#34;</span>: <span>&#34;git-deployment-workflow-setup&#34;</span>,    <span># Unique task identifier</span>
    <span>&#34;difficulty&#34;</span>: <span>&#34;hard&#34;</span>,                          <span># easy|medium|hard|extremely_hard</span>
    <span>&#34;category&#34;</span>: <span>&#34;system-administration&#34;</span>,           <span># Task category</span>
    <span>&#34;prompt&#34;</span>: <span>&#34;I need help setting up a simple CI/CD system...&#34;</span>,  <span># The actual task instruction</span>
    <span>&#34;dockerfile&#34;</span>: <span>&#34;FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest<span>\n</span>...&#34;</span>,  <span># Docker environment setup</span>
    <span>&#34;test_functions&#34;</span>: <span>&#34;def test_hook_script_executable():<span>\n</span>    ...&#34;</span>,  <span># Pytest verification code</span>
    <span>&#34;test_weights&#34;</span>: {                              <span># Weight for each test (for partial credit)</span>
        <span>&#34;test_hook_script_executable&#34;</span>: <span>0.35</span>,
        <span>&#34;test_nginx_service_running&#34;</span>: <span>0.15</span>,
        <span>&#34;test_deployment_works_correctly&#34;</span>: <span>0.50</span>
    },
    <span>&#34;additional_files&#34;</span>: {                          <span># Optional files to include in container</span>
        <span>&#34;backup_config.json&#34;</span>: <span>&#34;{<span>\n</span>  <span>\&#34;</span>schedules<span>\&#34;</span>: [...&#34;</span>,
        <span>&#34;collision_detector.py&#34;</span>: <span>&#34;#!/usr/bin/env python3<span>\n</span>...&#34;</span>
    }
}</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">🐳 Training Environment Creation</h3><a id="user-content--training-environment-creation" aria-label="Permalink: 🐳 Training Environment Creation" href="#-training-environment-creation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">During training, each task generates multiple parallel rollouts (trajectories), with each rollout executed in complete isolation:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Parallel Rollout Generation</strong>:</p>
<ul dir="auto">
<li><strong>N_ROLLOUTS</strong>: Configurable per training preset (e.g., 4 for test runs, 16 for production)</li>
<li>Each rollout runs simultaneously in its own Docker container</li>
<li>Complete independence between rollouts allows diverse solution exploration</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Per-Rollout Environment Setup</strong>:</p>
<ul dir="auto">
<li>New Docker container created using the task&#39;s <code>dockerfile</code></li>
<li>Container starts with a clean filesystem based on the Dockerfile</li>
<li>Any <code>additional_files</code> are written to the container before the agent begins</li>
<li>Agent receives the task <code>prompt</code> as its initial instruction</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Agent Execution</strong>: The agent interacts with its isolated environment using tools (bash, file operations, etc.)</p>
</li>
<li>
<p dir="auto"><strong>Verification</strong>: After completion, <code>test_functions</code> are executed to compute the test score</p>
</li>
<li>
<p dir="auto"><strong>Cleanup</strong>: Container is destroyed after trajectory completion</p>
</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">🧹 Docker Resource Management</h3><a id="user-content--docker-resource-management" aria-label="Permalink: 🧹 Docker Resource Management" href="#-docker-resource-management"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Due to the high volume of Docker containers being created (up to 24 containers running in parallel during training), the infrastructure includes automatic resource cleanup:</p>
<ul dir="auto">
<li><strong>Automatic cleanup daemon</strong> periodically removes stopped containers and unused networks</li>
<li>Runs every 2 minutes to prevent resource exhaustion</li>
<li>Immediate cleanup on training startup to ensure clean environment</li>
<li>See <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/src/tbench_rllm/docker_cleanup.py"><code>docker_cleanup.py</code></a> and <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/src/tbench_rllm/docker_env.py"><code>docker_env.py</code></a> for cleanup implementation details</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">📂 Dataset Preparation Pipeline</h3><a id="user-content--dataset-preparation-pipeline" aria-label="Permalink: 📂 Dataset Preparation Pipeline" href="#-dataset-preparation-pipeline"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The training data flows through a multi-stage preparation pipeline before being used by rLLM:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>CSV Dataset</strong> (<code>dataset/latest_verified.csv</code>): Contains task definitions with prompts, Dockerfiles, test functions, and weights</p>
</li>
<li>
<p dir="auto"><strong>Terminal Bench Tasks</strong> (via <code>convert_dataset_to_tasks.py</code>):</p>
<ul dir="auto">
<li>Converts each CSV row into a Terminal Bench task directory structure in order to leverage the TerminalBench Docker harness and unit test runner + parser for reward calculation during RL run.</li>
<li>Runs in parallel to speed up conversion of tasks</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Parquet Format</strong> (via <code>tasks_to_parquet_converter.py</code>):</p>
<ul dir="auto">
<li>Creates <code>data/terminal_bench/*.parquet</code> files for rLLM&#39;s data loader</li>
<li>Each row contains <code>extra_info</code> dict with: task_name, task_path, instruction, test_weights, dockerfile_contents, etc.</li>
<li>This <code>extra_info</code> is passed to <code>DockerIsolatedEnv.from_dict()</code> during training to create environments</li>
</ul>
</li>
</ol>


<p dir="auto">Clone the repository and install dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recurse-submodules https://github.com/Danau5tin/terminal-bench-rl.git
cd terminal-bench-rl
uv sync"><pre>git clone --recurse-submodules https://github.com/Danau5tin/terminal-bench-rl.git
<span>cd</span> terminal-bench-rl
uv sync</pre></div>
<p dir="auto">That&#39;s it! UV will handle all dependencies automatically.</p>
<blockquote>
<p dir="auto"><strong>Note</strong>: This project includes a forked version of the terminal-bench repository with the Python version requirement reduced from 3.13 to 3.12 for compatibility.</p>
</blockquote>
<div dir="auto"><h3 tabindex="-1" dir="auto">Terminal Bench Evaluation Reproduction</h3><a id="user-content-terminal-bench-evaluation-reproduction" aria-label="Permalink: Terminal Bench Evaluation Reproduction" href="#terminal-bench-evaluation-reproduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">After setup, you can reproduce my Terminal Bench evaluation [result](./tbench_eval_run_results.jsons:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Set environment variables, example:
export LITE_LLM_API_KEY=&#34;your_huggingface_token&#34;
export LITE_LLM_API_BASE=&#34;https://router.huggingface.co/v1&#34;
export LITELLM_MODEL=&#34;openai/Qwen/Qwen3-32B:nebius&#34;

# Run the evaluation
./evaluation/terminal_bench_eval/run_eval.sh"><pre><span><span>#</span> Set environment variables, example:</span>
<span>export</span> LITE_LLM_API_KEY=<span><span>&#34;</span>your_huggingface_token<span>&#34;</span></span>
<span>export</span> LITE_LLM_API_BASE=<span><span>&#34;</span>https://router.huggingface.co/v1<span>&#34;</span></span>
<span>export</span> LITELLM_MODEL=<span><span>&#34;</span>openai/Qwen/Qwen3-32B:nebius<span>&#34;</span></span>

<span><span>#</span> Run the evaluation</span>
./evaluation/terminal_bench_eval/run_eval.sh</pre></div>
<p dir="auto">This will run the agent with the same configuration that achieved 13.75% on the leaderboard.</p>


<p dir="auto">For detailed single-node training setup:</p>
<ul dir="auto">
<li>See <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/docs/single_node_training_walkthrough.md">Single Node Training Guide</a></li>
</ul>

<p dir="auto">For distributed training across multiple nodes:</p>
<ul dir="auto">
<li>See <a href="https://github.com/Danau5tin/terminal-bench-rl/blob/main/docs/multi_node_training_walkthrough.md">Multi-Node Training Guide</a></li>
</ul>
<hr/>

<p dir="auto">Given more time and resources, several enhancements would further improve training effectiveness:</p>

<p dir="auto">At this point, I feel as if I have created a great cake recipe, put everything in place to make it, and yet I can&#39;t afford to pay for the oven!!! 😂</p>
<ul dir="auto">
<li>With sufficient compute budget, I&#39;d run full training and then evaluate the trained model. I am confident that it would outperform the untrained Qwen3-32B.</li>
</ul>

<ul dir="auto">
<li>I&#39;d also implement curriculum learning to progressively increase task difficulty, starting with easy and medium tasks, with a high weight on the judge reward to encourage behaviour such as using todo list, etc.</li>
<li>Then when these easier tasks are being completed with the correct behaviour, I would remove the judge completely, and move to 100% software verification of the tasks. Allowing the model to lose the strict constraints of what it learnt at first, and explore optimised routes to success from a principled base.</li>
</ul>

<ul dir="auto">
<li><strong>Generate more datapoints</strong>: Currently I have only ~331 tasks due to time constraints</li>
<li>A larger dataset (1000+ tasks) would provide more diverse scenarios and tech stacks for robust training</li>
<li>I&#39;d also take the time to give each datapoint careful validation, which takes time but ensures quality</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">🎯 Smart Data Filtering</h3><a id="user-content--smart-data-filtering" aria-label="Permalink: 🎯 Smart Data Filtering" href="#-smart-data-filtering"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Pre-filter trivial datapoints</strong>: Before training, I&#39;d run the untrained model on all tasks</li>
<li>I&#39;d remove datapoints where the model achieves zero or perfect scores (0.0 or 1.0 reward)</li>
<li>This would save GPU time by focusing training on tasks where the model can actually learn</li>
</ul>
<hr/>

<ul dir="auto">
<li>I want to thank everyone who contributed to Terminal Bench, it is a great benchmark and just what I&#39;ve been looking for!</li>
<li>A big thank you to the minds behind rLLM too! I did try this with other training frameworks, and they contained critical bugs which are not solved today, so it was a breath of fresh air to use a framework that performed so well!</li>
<li>A thank you for the Claude Code team for inspiring the tool use and agent behavioural approach used in this project!</li>
<li>A big thank you to the research team at Anthropic, for creating such great models. Opus-4 assisted me heavily in this project, especially when debugging problems on deployed clusters of distributed GPUs.</li>
</ul>
<p dir="auto">This project was a lot of fun! Thanks for reading!
Dan</p>
</article></div></div>
  </body>
</html>
