<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://centralflows.github.io/part1/">Original</a>
    <h1>How does gradient descent work?</h1>
    
    <div id="readability-page-1" class="page">
    <nav>
        
        
        
    </nav>

    <p>
        This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
     </p>

    

    <!-- <div class="figure">
        <img src="training/animate/results/quadratic2.gif" alt="Quadratic optimization animation">
    </div> -->

    <p>
        The simplest optimization algorithm is deterministic gradient descent:
    </p>

    <p>
        \[
            w_{t+1} = w_t - \eta \, \nabla L(w_t)
        \]
    </p>

    <p>
        Perhaps surprisingly, traditional analyses of gradient descent cannot capture the typical dynamics of gradient descent in deep learning.  We&#39;ll first explain why, and then we&#39;ll present a new analysis of gradient descent that <em>does</em> apply in deep learning.
    </p>

    <h2>The dynamics of gradient descent</h2>

    <p>
        Let&#39;s start with the picture that everyone has likely seen before.
        Suppose that we run gradient descent on a quadratic function \( \frac{1}{2} S x^2\), i.e. a smiley-face parabola.
        The parameter \(S\) controls the second derivative (&#34;curvature&#34;) of the parabola: when \(S\) is larger, the parabola is steeper. 
    </p>

    <p>
        If we run gradient descent on this function with learning rate \(\eta\), there are two possible outcomes.  On the one hand, if \(S &lt; 2/\eta\), then the parabola is &#34;flat enough&#34; for the learning rate \(\eta\), and gradient descent will converge.
        On the other hand, if \(S &gt;<!--2--> 2/\eta\), then the parabola is &#34;too sharp&#34; for the learning rate \(\eta\), and gradient descent will oscillate back and forth with increasing magnitude.
    </p>

    <div>
        <div>
            <video width="100%">
                <source src="../media/quadratic.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Consider gradient descent with learning rate \(\eta\) on a 1d quadratic function with curvature \(S\). If \(S &lt; 2/\eta\), as on the left, the optimizer will converge; if \(S &gt; 2/\eta\), as on the right, the optimizer will diverge.</p>
    </div>

    <p>
        The same is true for a quadratic function in multiple dimensions.  On a multi-dimensional quadratic, the eigenvalues of the Hessian matrix quantify the curvature along the corresponding eigenvectors.
        If any Hessian eigenvalue exceeds the threshold \(2/\eta\), then this means that the quadratic is &#34;too sharp&#34; in the corresponding eigenvector direction, and gradient descent will oscillate along that direction with increasing magnitude.
    </p>

    <div>
        <div>
            <video width="100%">
                <source src="../media/multi2.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>We run gradient descent with learning rate \(\eta\) on a 2d quadratic function where the curvature along the eigenvector \(q_1\) is &#34;too large&#34; (i.e. \( &gt; 2/\eta\)), 
            but the curvature along \(q_2\) is sufficiently small (i.e. \( &lt; 2/\eta\)).  Observe that the optimizer diverges along direction \(q_1\).</p>
    </div>


    <p>
        Ok, that&#39;s quadratics, but what about deep learning objectives?  Well, on an arbitrary deep learning objective \(L(w)\), we can always take a quadratic Taylor approximation around our current location in weight space.
        It&#39;s reasonable to think that the dynamics of gradient descent on this quadratic function might resemble the short-term dynamics of gradient descent on the real neural objective.
        As we&#39;ve seen, the dynamics of gradient descent on this quadratic are controlled by the largest eigenvalue of the Hessian matrix \(H(w)\), which we will call the <em>sharpness</em> \(S(w)\):
        \[S(w) := \lambda_1(H(w)).\]
        Namely, if \(S(w) &gt; 2/\eta\), we know that gradient descent on the quadratic Taylor approximation would oscillate and blow up along the top Hessian eigenvector(s).
        This argument suggests that gradient descent cannot function properly in regions of weight space where \(S(w) &gt; 2/\eta\).
    </p>

    <p>
        In light of this discussion, why does gradient descent work in deep learning?
        Perhaps the most natural explanation is that gradient descent stays in regions of weight space where the sharpness \(S(w)\) is less than \(2/\eta\).
        In other words, if we define the &#34;stable region&#34; as the subset of weight space where the sharpness is less than \(2/\eta\), then 
        perhaps gradient descent stays inside the stable region throughout training, as in the following cartoon:
    </p>

    <div id="expectation-figure">
        <p><img src="https://centralflows.github.io/media/expectation-small.png" alt="Traditional optimization theory suggests gradient descent stays in the stable region"/></p><p>The traditional perspective is that gradient descent remains throughout training inside the &#34;stable region&#34;, i.e. the subset of weight space where \(S(w) \le 2/\eta\),  visualized here as a gray blob.</p>
    </div>

    <p> 
        Indeed, this is the picture suggested by traditional optimization theory (this is &#34;local L-smoothness&#34;). 
    </p>

    <p> 
        Yet, as we will now see, the reality in deep learning is quite different.
        Let&#39;s train a neural network using gradient descent with \(\eta = 0.02\).
        This network happens to be a Vision Transformer trained on a subset of CIFAR-10, but you&#39;d see a similar picture with basically any neural network on any dataset.
    </p>

    <p>
        As we train, we&#39;ll plot the evolution of the sharpness \(S(w)\).  Watch what happens:
    </p>

    <div>
        <div>
            <video width="100%">
                <source src="../media/progressive-sharpening.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>While training a network using gradient descent with learning rate \(\eta = 0.02\)  (a ViT on CIFAR-10), we plot the evolution of the sharpness \(S(w)\).  Observe that the sharpness rises until it reaches the threshold \(2/\eta\), at which point we pause the animation.</p>
    </div>

    <p>
        You can see that the sharpness \(S(w)\) rises until it reaches the threshold \(2/\eta\).
        This means that gradient descent has left the stable region.
        At this point, we know that gradient descent would diverge if it were run on the local quadratic Taylor approximation to the training objective, as there is now a direction that is &#34;too sharp&#34; for the learning rate \(\eta\).
        But, what will happen on the real objective?  Let&#39;s see.
    </p>
    <p>
        For the next few iterations of training, we&#39;ll plot the train loss and sharpness (as before), but also the displacement of the iterate along the top Hessian eigenvector, i.e. the quantity that is predicted to oscillate.
    </p>

    <div>
        <div>
            <video width="100%">
                <source src="../media/blowup.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>For the next few iterations, we plot the train loss, the sharpness, and the displacement of the iterate along the top Hessian eigenvector.  Observe that gradient descent oscillates with growing magnitude along the top eigenvector direction.</p>
    </div>

    <p>
        As you can see, gradient descent does indeed oscillate with growing magnitude along the top Hessian eigenvector, just as a quadratic Taylor approximation would predict.
        These oscillations eventually grow large enough that the train loss starts to go <em>up</em> instead of <em>down</em>.
    </p>
    <p>
        Things seem to be going poorly.  Will gradient descent succeed?  It&#39;s not clear how it can: so long as the sharpness exceeds \(2/\eta\), a quadratic Taylor approximation predicts that gradient descent will continue to oscillate with ever-increasing magnitude along the top eigenvector direction.
    </p>
    <p>
        Let&#39;s see what happens over the next few steps of training:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/stabilization.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Over the next few iterations, the sharpness mysteriously drops below \(2/\eta\).  Subsequently, the oscillations shrink and the loss comes back down.</p>
    </div>
    <p>
        As if by magic, the sharpness <em>drops</em>.  And it drops below \(2/\eta\), which is just what we needed to happen.
        Once the sharpness falls below \(2/\eta\), the oscillations shrink, as we&#39;d expect from taking a new quadratic Taylor approximation.
        Similarly, the loss comes back down, which is also to be expected.  But a key question remains: why did the sharpness conveniently drop, just when we needed it to?
    </p>
    <p>
        Before we answer this question, let&#39;s take a look at the complete training run:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>We show the full training run.  The sharpness rises to \(2/\eta\) and then equilibrates around that value.</p>
    </div>
    <p>
        You can see that after rising to \(2/\eta\), the sharpness ceases to grow further, and instead equilibrates around that value.  Meanwhile, the training loss behaves non-monotonically over the short-term, while decreasing consistently over the long term.
    </p>
    <p>
        If we try another learning rate, say \(\eta = 0.01\), then the same thing happens:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/smaller-lr.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>For the smaller learning rate \(\eta = 0.01\), we see the same behavior as before, except at the new value of \(2/\eta\).</p>
    </div>
    <p>
        These dynamics are quite surprising. If the traditional picture is that gradient descent remains inside the stable region throughout training, then the reality is that gradient descent is frequently exiting the stable region, but is somehow <em>steering itself back inside</em> each time.
    </p>
    <div>
        <p><img src="https://centralflows.github.io/media/reality-small.png" alt="Traditional optimization theory suggests gradient descent stays in the stable region"/></p><p>Whereas the <a href="#expectation-figure">traditional picture</a> is that gradient descent stays entirely within the stable region, in reality gradient descent is tending to leave the stable region, but is <em>dynamically steering</em> itself back inside.</p>
    </div>
    <p>
        We call these dynamics training at the <em>edge of stability</em> (EOS).
        This behavior is not specific to this network; rather, as far as anyone can tell, it is a universal phenomenon in deep learning.
        For example, here are several vision architectures trained using gradient descent on a subset of CIFAR-10:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos-loss-curves-vision.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <!-- <div class="caption">.</div> -->
    </div>
    <p>
        And here are several sequence architectures trained using gradient descent on a sequence prediction task:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos-loss-curves-sequence.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <!-- <div class="caption">.</div> -->
    </div>
    <p>
        Now, practical loss curves don&#39;t look quite like this — for example, they don&#39;t usually have such exaggerated loss spikes.
        That&#39;s because practical training usually uses stochastic optimization, whereas this is deterministic gradient descent.
        We study gradient descent because, as the simplest optimizer, understanding gradient descent would seem to be a necessary stepping stone to understanding SGD.
        <a href="https://arxiv.org/abs/2412.20553">Similar phenomena</a> do occur during SGD and in fact, the study of related phenomena was <a href="https://arxiv.org/abs/1807.05031">first</a> <a href="https://arxiv.org/abs/2002.09572">pioneered</a> by <a href="https://sjastrzebski.com/">Stanisław Jastrzębski</a> in the context of SGD.
    </p>
    <p>
        Several years ago, one of us (Jeremy) wrote a <a href="https://arxiv.org/abs/2103.00065">paper</a> which showed that gradient descent exhibits these EOS dynamics, and posed the question:
    </p>
    <p>
        How does gradient descent converge in deep learning, if not for any reason given in the optimization literature?
    </p>
    <p>
        In response, the other of us (Alex) co-wrote a <a href="https://arxiv.org/abs/2209.15594">paper</a> that gave an explanation.
        It turns out that the key to understanding these gradient descent dynamics is to consider a <em>third-order</em> Taylor expansion of the objective, which is one order higher than is normally used when analyzing gradient descent.
        A third-order Taylor expansion reveals the key ingredient missing from traditional optimization theory:
    </p>
    <p>
        Oscillations along the top Hessian eigenvector automatically trigger reduction of the top Hessian eigenvalue.
    </p>
    <div>
        <p><img src="https://centralflows.github.io/media/divergence-illustration.png"/></p><p>The iterate \(w\) is displaced from \(\overline{w}\) along the direction \(u\) with magnitude \(x\).</p>
    </div>
    <p>
        Let&#39;s sketch this argument informally.
        Suppose that gradient descent is oscillating along the top Hessian eigenvector, \(u\).
        Let \(\overline{w}\) denote the point where we&#39;d be if we were <em>not</em> oscillating.
        But because we <em>are</em> oscillating, the current iterate \(w\) is displaced from \(\overline{w}\) along the direction \(u\) by some magnitude, call it \(x\).
        Thus the iterate is at:
        \[ w = \overline{w} + x u\]
    </p>
    <p>
        How does the gradient  at \(w\), where we are, compare to the gradient at \(\overline{w}\), where we&#39;d be if we weren&#39;t oscillating?
        Let&#39;s do a Taylor expansion of \(\nabla L\) around \(\overline{w}\).
        The first two terms of this Taylor expansion are:
        \[
        \nabla L(\overline{w} + xu)
        = \overset{\textcolor{red}{\text{first term}\strut\strut\strut}}{\nabla L(\overline{w})}
        + \overset{\textcolor{red}{\text{second term}\strut\strut\strut}}{
          \underbrace{x H(\overline{w}) u}_{\textcolor{red}{=\, x S(\overline{w}) u}}
        }
        + \mathcal{O}(x^2)
         \]
    </p>
    <p>
        Since \(u\) is an eigenvector of the Hessian \( H(\overline{w}) \) with eigenvalue equal to the sharpness \(S(\overline{w})\), the second term can be simplified as \(x H(\overline{w}) u = x S(\overline{w}) u\), which is a vector pointing in the \(u\) direction.
        This term causes a negative gradient step computed at \(\overline{w} + xu\) to move in the \(-u\) direction.
        That is, this term is causing us to oscillate back and forth along the top Hessian eigenvector, as predicted by the classical theory.
    </p>
    <p>
        The &#34;magic&#34; comes from the <em>next</em> term in the Taylor expansion:
        \[
        \nabla L(\overline{w} + xu)
        = \textcolor{gray}{
            \overset{\textcolor{gray}{\text{first term}\strut\strut\strut}}{\nabla L(\overline{w})}
        + \overset{\textcolor{gray}{\text{second term}\strut\strut\strut}}{
        x S(\overline{w}) u
        }}
        +\overset{\textcolor{red}{\text{third term}\strut\strut\strut}}{
            \underbrace{\frac{1}{2} x^2 \nabla_{\overline{w}} \left[ u^T H(\overline{w}) u \right]}_{\textcolor{red}{= \frac{1}{2} x^2 \nabla S(\overline{w})}}
            }
        + \mathcal{O}(x^3)
        \]
    </p>
    <p>
        This term looks a little intimidating at first, but let&#39;s unpack it.
        The quantity \( u^T H(\overline{w}) u \) is the curvature in the \(u\) direction.
        The gradient of this quantity, \(\nabla_{\overline{w}} [u^T H(\overline{w}) u ]\), is the <em>gradient</em> of the curvature in the \(u\) direction.
        Since \(u\) is the top Hessian eigenvector, the curvature in the \(u\) direction is the top Hessian eigenvalue, i.e. the sharpness.
        Similarly, it can be shown that the <em>gradient</em> of this curvature is the <em>gradient</em> of the sharpness.
    </p>
    <p>
        Thus, when gradient descent is oscillating along the top Hessian eigenvector with magnitude \(x\), the gradient automatically picks up a term \( \frac{1}{2} x^2 \nabla S(\overline{w}) \) which points in the direction of gradient of the sharpness, \(\nabla S(\overline{w})\).
        As a result, each negative gradient step on the loss implicitly takes a negative gradient step on the <em>sharpness</em> of the loss, with the &#34;step size&#34; \(\frac{1}{2} \eta x^2 \).
        Thus, <em>oscillations automatically reduce sharpness</em>, and the strength of this effect is proportional to the squared magnitude of the oscillation \( x^2 \).
    </p>
    <p>
        Equipped with this new insight, we can finally understand the behavior of gradient descent that we previously observed:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/onecycle.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>When the oscillations grow large enough, they exert a non-negligible downward force on the sharpness, pushing the iterate back into the stable region.</p>
    </div>
    <p>
        When gradient descent leaves the stable region, it starts to oscillate with growing magnitude along the top Hessian eigenvector.  
        At first, these oscillations are small, so their effect on the sharpness is negligible.  But the oscillations soon grow large enough to exert a non-negligible sharpness-reduction effect.
        This acts to decrease the sharpness, pushing gradient descent back into the stable region, after which point the oscillations shrink.
    </p>
    <p>
        In effect, a third-order Taylor approximation reveals that gradient descent has in-built <a href="https://en.wikipedia.org/wiki/Negative_feedback">negative feedback mechanism</a> for regulating sharpness: 
        when the sharpness \(S(w)\) exceeds \(2/\eta\), gradient descent oscillates, but the precise effect of such oscillations is to reduce ... the sharpness \(S(w)\)!
        <!-- This negative feedback mechanism is responsible for the successful convergence of gradient descent in deep learning at practical learning rates. -->
    </p>
    <p>
        In the special case where there is exactly one eigenvalue at the edge of stability, the EOS dynamics consist of consecutive cycles of the kind we just saw, where the sharpness first rises above, and then is pushed below, the value \(2/\eta\):
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos-one-unstable.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>With exactly one eigenvalue at the edge of stability, the dynamics consist of consecutive cycles.</p>
    </div>
    <p>
        However, it is common for more than one eigenvalue to eventually reach the edge of stability.
        The dynamics with multiple eigenvalues at EOS are more complex, and may be <a href="https://en.wikipedia.org/wiki/Chaos_theory">chaotic</a> in the technical sense.
        When there are \(k\) eigenvalues at the edge of stability, gradient descent oscillates within the span of the corresponding \(k\) eigenvectors, and these oscilations cause all \(k\) eigenvalues to stay 
        dynamically regulated around the value \(2/\eta\):
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos-multiple.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p> We plot the top four Hessian eigenvalues. 
            The first eigenvalue (blue) enters EOS around step 1750, the second (orange) just after step 2000, and the third (green) shortly before step 3000.
            Thereafter, there are three eigenvalues at the edge of stability.
        </p>
    </div>
    <p>
        While these dynamics are challenging to analyze, the key insight of our <a href="https://arxiv.org/abs/2410.24206"></a>new paper is that understanding the EOS dynamics in fine-grained detail may not be necessary.
        Rather, we argue that the more important question is: what <em>macroscopic</em> path does gradient descent take through weight space?
        The goal of our analysis will be to derive a differential equation called a <em>central flow</em> that characterizes this path.
    </p>
    <h2>Deriving the central flow</h2>
    <p>
        The usual continuous-time approximation to gradient descent is the <em>gradient flow</em>:
    \[
        \begin{align}
            \frac{dw}{dt} = - \eta \, \nabla L(w), \label{eq:gradient-flow}
        \end{align}
    \]
    Gradient descent roughly follows the gradient flow <em>before</em> reaching the edge of stability. 
    But <em>after</em> reaching EOS, gradient descent splits off from gradient flow, and takes a different path through weight space, as shown in the following animation:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/gradientflow.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Gradient descent (blue) approximately follows gradient flow (red) before reaching the edge of stability, but splits off afterwards.</p>
    </div>
    <p>
        The reason is that gradient flow never oscillates, and hence allows the sharpness to keep rising beyond \(2/\eta\), whereas gradient descent undergoes oscillations which keep the sharpness regulated around \(2/\eta\).
    </p>
    <p>
        Our <em>central flow</em> is a differential equation that characterizes the trajectory of gradient descent even at the edge of stability.
        The central flow is depicted by a black dashed line in the following animation:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/centralflow.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The central flow (black) approximates gradient descent (blue) even at the edge of stability.</p>
    </div>
    <p>
        The central flow models the <em>time-averaged</em> (i.e. locally smoothed) trajectory of gradient descent, as illustrated in the following cartoon of the weight-space dynamics:
    </p>
    <div>
            <p><img src="https://centralflows.github.io/media/fig1.png" ,="" alt="Central flow cartoon"/></p><p>The central flow (black) models the time-averaged trajectory of gradient descent (blue).</p>
        </div>
    
    <p>
        We&#39;ll derive the central flow using informal mathematical reasoning, and we will then validate its accuracy experimentally.
        Essentially, our logic proceeds as follows: (1) We will assume that the time-averaged trajectory of gradient descent can be captured by a differential equation.
        (2) We will argue that only one differential equation makes sense (the central flow).
        (3) Finally, we will show empirically that this central flow does indeed match the long-term trajectory of gradient descent in a variety of deep learning settings.
    </p>
    <p>
        Rigorously establishing that gradient descent follows the central flow is an important open problem that is beyond the scope of our paper.
        Such an analysis would require making assumptions about the objective that we would not know how to justify, and would demand analytical tools that go beyond the usual toolkit of optimization theory.
        We hope that our compelling experimental results will inspire future efforts aimed at establishing a rigorous basis for similar analyses.
        <!-- Nevertheless, while our derivation is informal, we stress that it will yield highly accurate, nontrivial numerical predictions about the optimization of real neural networks. -->
        <!-- We believe that informal analyses such as the present one may be a necessary stepping stone to rigorous theory. -->
    </p>
    <p>
        Let&#39;s start our derivation by considering the special case where only one eigenvalue is at the edge of stability.
        The central flow \(w(t)\) will model the time-averaged iterates of gradient descent.
        Since gradient descent oscillates along the top Hessian eigenvector \(u(t)\), we model each iterate as:
        \[
        \underset{\color{red} \text{iterate}}{w_t} = \underset{\color{red} \begin{array}{c}  \text{time-averaged} \\[-4pt] \text{iterate} \end{array} }{w(t)} + \underset{\color{red} \begin{array}{c} \text{perturbation along} \\[-4pt] \text{top eigenvector} \end{array} }{x_t \, u(t)}.
    \]
    </p>
    <p>
        Therefore, by Taylor expansion, the gradient at the iterate is:
        \[
            \underset{\color{red} \begin{array}{c} \text{gradient at} \\[-4pt] \text{iterate} \end{array}}{\nabla L(w_t)} \approx \underset{ \color{red} \begin{array}{c} \text{gradient at time-} \\[-4pt] \text{averaged iterate} \end{array}}{\nabla L(w(t))} + \underset{\color{red} \text{oscillation}}{x_t S(w(t)) u(t)} + \underset{\color{red} \text{sharpness reduction}}{\tfrac{1}{2} x_t^2 \nabla S(w(t))}.
        \]
    </p>
    <p>
        Therefore, if we abuse notation and use \(\mathbb{E}\) to denote &#34;averages over time&#34;, then the &#34;time-averaged&#34; gradient is:
            \[
            \underset{\color{red} \begin{array}{c} \text{time-averaged} \\[-4pt] \text{gradient} \end{array}}{\mathbb{E}[\nabla L(w_t)]} \approx
                 \underset{ \color{red} \begin{array}{c} \text{gradient at time-} \\[-4pt] \text{averaged iterate} \end{array} }{\nabla L(w(t))}
                + \underset{\color{red} \text{0 because } \mathbb{E}[x_t] = 0 \text{ }}{\color{red}{\cancel{\color{black} \mathbb{E}[x_t] \, S(w(t)) u(t)}}}
                + \underset{\color{red} \text{implicit sharpness penalty}}{\frac{1}{2} \mathbb{E} [x_t^2] \, \nabla S(w(t))}.
        \]
    </p>
    <p>
        That is, the time-averaged gradient equals the gradient at the time-averaged iterate, plus a term proportional to the gradient of the sharpness.
        The latter scales with \(\mathbb{E} [x_t^2]\), the time-average of the squared magnitude of the oscillations, i.e.
        the variance of the oscillations.
        The larger the oscillations, the stronger the induced sharpness penalty.
    </p>
    <p>
        Based on this calculation, we make the ansatz that the time-averaged dynamics of gradient descent
        can be captured by a sharpness-penalized gradient flow that follows this time-averaged gradient:
        \[\begin{align}
            \frac{dw}{dt} = - \eta \, \left[ \nabla L(w) + \underbrace{\frac{1}{2} \sigma^2(t) \nabla S(w)}_{\color{red} \text{sharpness penalty}} \right], \label{eq:central-flow-ansatz-one-unstable}
        \end{align}\]
    </p>
    <p>
        where \(\sigma^2(t)\) is a still-unknown quantity that models the &#34;instantaneous variance&#34; of the oscillations at step \(t\).
        Intuitively, this flow averages out the oscillations themselves, while retaining their lasting effect on the trajectory,
        which takes the form of the sharpness penalty.
    </p>
    <p>
        To set \(\sigma^2(t)\), we will argue that there is only one possible value that it can take.
        At EOS, the sharpness \(S(w)\) is equilibrating at \(2/\eta\), and is consequently <em>invariant over time</em>.
        Therefore, we will require that the time derivative of the sharpness along the central flow is zero.
        It turns out that there is a unique value of \(\sigma^2(t)\) that is compatible with this equilibrium condition.
    </p>
    <p>
        To see why, note that the time derivative of the sharpness under a flow of the form \eqref{eq:central-flow-ansatz-one-unstable} is given by:
        \[
        \begin{align}
        \frac{dS}{dt} &amp;= \left \langle \nabla S(w), \frac{dw}{dt} \right \rangle \tag{chain rule} \\[0.4em]
                    &amp;= \left \langle \nabla S(w), - \eta \, \left[ \nabla L(w) + \frac{1}{2} \sigma^2(t) \nabla S(w) \right] \right \rangle \tag{ansatz for $\tfrac{dw}{dt}$} \\[0.4em]
                    &amp;= \underset{\color{red} \begin{array}{c} \text{time derivative of sharpness} \\[-4pt] \text{under gradient flow} \end{array}}{\left \langle \nabla S(w), - \eta \nabla L(w) \right \rangle} -
                      \underset{\color{red} \begin{array}{c} \text{sharpness reduction} \\[-4pt] \text{from oscillations} \end{array} }{\tfrac{1}{2} \eta \, \sigma^2(t) \| \nabla S(w) \|^2}.  \tag{simplify} 
        \end{align}
        \]
    </p>
    <p>
        The first term is the time derivative of the sharpness under gradient flow, which will be positive (as otherwise we would have left the edge of stability).
        The second term is the sharpness reduction that is induced by the oscillations.
        Basic algebra shows that there is a unique value of \(\sigma^2(t)\) for which the second term cancels the first term to yield \(\frac{dS}{dt} = 0\), and this is given by:
    </p>
    <p>
        \[
            \begin{align}
                \sigma^2(t) = \frac{2 \langle -\nabla L(w), \nabla S(w) \rangle }{\| \nabla S(w)\|^2}. \label{eq:sigma-squared-one-unstable}
            \end{align}
        \]
        </p>
    <p>
        The central flow is defined as equation \eqref{eq:central-flow-ansatz-one-unstable}, with this particular value of \(\sigma^2(t)\) plugged in:
        \[  
        \begin{align}
            \frac{dw}{dt} = - \eta \, \left[ \nabla L(w) + \frac{1}{2} \sigma^2(t) \nabla S(w) \right] \quad \text{where}\quad
            \sigma^2(t) = \frac{2 \langle -\nabla L(w), \nabla S(w) \rangle }{\| \nabla S(w)\|^2}. \label{eq:central-flow-one-unstable}
        \end{align}
        \]
    </p>
    <p>
        Let&#39;s check out this central flow in action.  Starting at the point in training where the sharpness first reaches \(2/\eta\), we&#39;ll run both gradient descent and the central flow \eqref{eq:central-flow-one-unstable} side by side:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/central-flow-one-unstable.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The central flow&#39;s \(\sigma^2(t)\) accurately predicts the variance of the oscillations (middle), and the central flow&#39;s \(w(t)\) accurately tracks the long-term trajectory of gradient descent (right).</p>
    </div>
    <p>
        The left pane shows that the sharpness cycles around \(2/\eta\) under gradient descent, and stays fixed at exactly \(2/\eta\) under the central flow.
        This is not particularly interesting, since we constructed the central flow specifically to have this property.
        What is more interesting are the other two panes.
    </p>
    <p>
        The middle pane shows that our formula \eqref{eq:sigma-squared-one-unstable} for \(\sigma^2(t)\) can accurately predict the instantaneous variance of the oscillations.
        In light blue, we show the squared magnitude of the displacement between gradient descent
        and the central flow along the top Hessian eigenvector, and in thick blue we compute the empirical time-average of this quantity, i.e. the empirical variance of the oscillations (we use Gaussian smoothing).
        Observe that the central flow&#39;s \(\sigma^2(t)\), in black, accurately predicts this quantity.
    </p>
    <p>
        The right pane shows that the gradient descent stays close to the central flow over time.
        In particular, it shows that the Euclidean distance between these two processes remains small.
        In contrast, the figure below shows that the distance between gradient descent and the <em>gradient flow</em> starts to grow large once training enters EOS:
    </p>
    <div>
        <p><img src="https://centralflows.github.io/media/central-flow-single-action-gf-short.png"/></p><p>The gradient flow (red) takes a very different trajectory than gradient descent.</p>
    </div>
    <p>
        Thus, although our analysis utilizes informal mathematical reasoning, we can see that this analysis yields precise, nontrivial numerical predictions about the optimization dynamics.
    </p>
    <p>
        So far, we have focused on the special case where only one eigenvalue is at the edge of stability.
        But the central flow formalism extends more generally to the setting where an arbitrary number of eigenvalues can be at the edge of stability (including zero).
        In general, we model gradient descent as displaced from its time-averaged trajectory \(w(t)\) by some perturbation \(\delta_t\) that lies
        within the span of the unstable eigenvectors:
        \[
            \underset{\color{red} \text{iterate}}{w_t} = \underset{\color{red} \begin{array}{c}  \text{time-averaged} \\[-4pt] \text{iterate} \end{array} }{w(t)} + \underset{\color{red} \begin{array}{c} \text{perturbation} \end{array} }{\delta_t}.
        \]
    </p>
    <p>
        Going through a similar argument as before, we arrive at the ansatz that the time-averaged iterates of gradient descent follow a flow of the form:
        \[
            \begin{align}
                \frac{dw}{dt} = - \eta \left[ \nabla L(w) \, + \,\underbrace{\tfrac{1}{2} \nabla_w \langle H(w), \Sigma(t) \rangle}_{\color{red} \text{penalize \(\Sigma\)-weighted Hessian}} \right], \label{eq:central-flow-ansatz-multi-unstable}
            \end{align}
        \]
        where \(\Sigma(t)\ := \mathbb{E}[\delta_t \delta_t^T]\) models the instantaneous covariance matrix of the oscillations.
        Here, \(\langle \cdot, \cdot \rangle \) denotes the <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">trace inner product</a> between two matrices, so the quantity \(\langle H(w), \Sigma(t) \rangle\) is a linear combination
        of the entries of the Hessian, where each entry is weighted by the corresponding entry of \(\Sigma(t)\). 
        The flow implicitly penalizes this quantity, to capture the effect on the time-averaged trajectory of oscillations with covariance \(\Sigma(t)\).
    </p>
    <p>
        As before, to determine \(\Sigma(t)\), we argue that only one value &#34;makes sense.&#34; Namely, we impose three conditions on the flow:
        </p><ol>
            <li>Since gradient descent prevents any Hessian eigenvalue from rising above \(2/\eta\), we require that the central flow should prevent any Hessian eigenvalue from rising beyond \(2/\eta\).
            </li><li>Since gradient descent oscillates within the span of the unstable eigenvectors, we require that the matrix \(\Sigma(t)\), which models the covariance of these oscillations, should be supported within the Hessian&#39;s \(2/\eta \) eigenspace.</li>
            <li>Since \(\Sigma(t)\) models a covariance matrix, we require that it should be positive semidefinite.</li>
        </ol>
    
    <p>
        It turns out that there is a <em>unique</em> value of \(\Sigma(t)\) satisfying all three conditions, and it can be found by solving a convex program called a <em>semidefinite complementarity problem</em> (SDCP).
        The central flow is defined as equation \eqref{eq:central-flow-ansatz-multi-unstable} with this particular value of \(\Sigma(t)\) plugged in.
        Note that when the sharpness is below  \(2/\eta\), the SDCP will always return \(\Sigma(t) = 0\), and so the central flow will reduce to the gradient flow.
    </p>
    <p>
        Let&#39;s now watch the central flow in action:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/centralflow-multi-unstable.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Under the central flow, each eigenvalue rises to \(2/\eta\) and then stays locked there.</p>
    </div>
    <p>
        Initially, all Hessian eigenvalues are below \(2/\eta\), so \(\Sigma(t) = 0\), and the central flow reduces to the gradient flow.
        After the top Hessian eigenvalue reaches \(2/\eta\), \(\Sigma(t)\) becomes rank-one, and the central flow keeps the top eigenvalue locked at \(2/\eta\), as it mimics the effects of oscillating along the the top eigenvector direction. 
        After the second Hessian eigenvalue also reaches \(2/\eta\), \(\Sigma(t)\) becomes rank-two, and the central flow keeps the top two eigenvalues both locked at \(2/\eta\), as it mimics the effects of oscillating simultaneously along the top two eigenvector directions. 
    </p>
    <p>
        On the right, observe that the distance between the gradient descent and the central flow stays small over time, whereas the distance between gradient descent and <em>gradient flow</em>
            starts to grow large once the dynamics enter EOS.  (The red line ends early because we stop discretizing gradient flow once the sharpness gets high enough that discretization becomes too challenging.)
    </p>
    <p>
        Now for the coolest part, in our subjective opinion.
        As before, we can verify that our prediction \(\Sigma(t)\) does indeed accurately model the instantaneous covariance matrix of the oscillations.
        In particular, we can see that each eigenvalue of \(\Sigma(t)\) accurately predicts the instantaneous variance of oscillations along the corresponding eigenvector of \(\Sigma(t)\):
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/predict-sigma.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Each eigenvalue of \(\Sigma(t)\) accurately predicts the instantaneous variance of oscillations along the corresponding eigenvector.</p>
    </div>
    <p>
        We think this is really cool.  While the oscillations appear to be chaotic, they obey a hidden underlying order — namely, their <em>covariance</em> is predictable.
        Moreover, the long-term trajectory of gradient descent only depends on the oscillations via this covariance.
        Thus, we don&#39;t <em>need</em> to understand the exact oscillations to understand the long-term path taken by gradient descent — we just need to understand their covariance, which is much easier.
    </p>
    <h2>Interpreting the central flow</h2>
    <p>
        OK, so we&#39;ve shown that the central flow takes the same long-term path as gradient descent.
        But, one might ask, if they&#39;re the same, then why is one better?
        Well, as a smooth curve, the central flow is a fundamentally simpler object than the oscillatory gradient descent trajectory.
        For example, under the gradient descent trajectory, the network&#39;s predictions evolve erratically, due to the oscillations:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/eos-network-predictions.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>We show the network&#39;s predictions on three test examples (columns), plotting four output units each (colors). 
            These predictions evolve erratically, due to the oscillations.
        </p>
    </div>
    <p>
        By contrast, since the central flow is a smooth curve, everything (including the network&#39;s predictions) evolves smoothly under the central flow:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/central-flow-network-predictions.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The network&#39;s predictions evolve smoothly under the central flow (black).</p>
    </div>
    <p>
        Philosophically, we think of the central flow as the &#34;true&#34; training process, and we regard the actual gradient descent trajectory as merely a &#34;noisy&#34; approximation to this true underlying process.
    </p>
    <p>
        In particular, let&#39;s use the central flow perspective to understand something that every deep learning practitioner is familiar with: the train loss curve.
        Here&#39;s what a typical gradient descent loss curve looks like:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/loss-raw.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>Gradient descent&#39;s train loss curve \(L(w_t)\) is non-monotonic.</p>
    </div>
    <p>
        As you can see, the loss behaves non-monotonically over short-timescales, while only decreasing over long-timescales.
        This makes it challenging to reason about the progress of optimization.
    </p>
    <p>
        In contrast, the central flow&#39;s loss curve \(L(w(t))\) is much nicer:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/loss-central-flow.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The central flow loss curve \(L(w(t))\) decreases monotonically.</p>
    </div>
    <p>
        In fact, it is possible to <em>prove</em> that the central flow decreases the loss monotonically: \( \frac{d}{dt} L(w(t)) \leq 0 \).
        Thus, the loss along the central flow \(L(w(t))\) is a <b>hidden progress metric</b>, or potential function, for the optimization process.
    </p>
    <p>
        As you can see above, the loss along the central flow is consistently <em>lower</em> than the loss along the gradient descent trajectory.
        The intuitive explanation is that at EOS, gradient descent can be viewed as <a href="https://arxiv.org/abs/1802.08770">&#34;bouncing between valley walls&#34;</a>, whereas
        the central flow moves nearly along the &#34;valley floor.&#34;  The loss is higher on the valley walls, when gradient descent is located, than on the valley floor, where the central flow is located.
    </p>
    <!-- <div class="figure"  style="width:25%; float: right; margin: 0 0 16px 32px;"> -->
    <div>
        <p><img src="https://centralflows.github.io/media/valley-annotation.png"/></p><p>Gradient descent can be visualized as bouncing between the &#34;walls of a valley&#34;, whereas the central flow moves nearly along the &#34;valley floor.&#34;</p>
    </div>
    <p>
        Fortunately, because the central flow models the covariance \(\Sigma(t)\) with which gradient descent oscillates around the central flow, it can render predictions for the <em>time-averaged</em> train loss along the gradient descent trajectory:
        \[
            \underset{\color{red} \begin{array}{c}  \text{time-averaged} \\[-4pt] \text{train loss} \end{array}}{\mathbb{E}[L(w_t)]} =
            \underset{\color{red} \begin{array}{c}  \text{loss at} \\[-4pt] \text{central flow} \end{array}}{L(w(t))} +
            \underset{\color{red} \begin{array}{c}  \text{contribution from} \\[-4pt] \text{oscillations} \end{array}}{\frac{1}{\eta} \text{tr}(\Sigma(t))}
        \]
    </p>


    <p>
        The following animation shows that this prediction is quite accurate:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/loss-predicted.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>We can accurately predict (dashed black) the time-averaged train loss of gradient descent.</p>
    </div>

    <p>
        The loss along the central flow \(L(w(t))\) is roughly analogous to the loss evaluated at an exponential moving average of the recent weights, whereas the prediction for the time-averaged train loss  \(L(w(t)) + \tfrac{1}{\eta} \text{tr}(\Sigma(t))\) is roughly analogous to the smoothed training loss curve.
    </p>

    <p>
        The same principle applies to the gradient norm.
        The gradient norm along the central flow is much smaller than the gradient norm along the gradient descent trajectory, since most of the latter is
        dominated by the back-and-forth oscillations &#34;across the valley&#34; that cancel out over the long run.
        Nevertheless, by leveraging \(\Sigma(t)\), the central flow can predict the time-averaged gradient norm of gradient descent:
    </p>

    <div>
        <div>
            <video width="100%">
                <source src="../media/gradient-norm.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The central flow can predict the time-averaged gradient norm along the gradient descent trajectory.
            Most of this gradient norm originates from the oscillations.
        </p>
    </div>
    <h2>Empirical validation</h2>
    <p>
        Although this post has focused on a particular ViT as a running example, we stress that our analysis is fully generic, and applies across a variety of architectures and tasks.
        For example, here we show real vs. predicted loss curves for a variety of deep learning settings:
    </p>
    <div>
        <div>
            <video width="100%">
                <source src="../media/central-flow-loss-curves.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
            <p>Click to play</p>
        </div>
        <p>The central flow (black) can accurately predict the time-averaged loss curve across a variety of deep learning settings.</p>
    </div>
    <p>
        Please see the paper for a discussion of the circumstances under which the central flow is a good approximation to gradient descent.
    </p>
    <!-- <h2>Conclusion</h2>
    <p class="body-text">
        We are now in a position to answer the titular question: "how does gradient descent work?"
        Gradient descent takes an oscillatory path through weight space, following some macroscopic path while oscillating back-and-forth along the highest-curvature directions.
        Visually, this resembles moving through a "valley" by way of bouncing between the "valley walls".
    </p>
    <p class="body-text">
        The oscillations arise due to high curvature, and they in turn trigger reduction of curvature, an automatic negative feedback mechanism which is responsible for gradient descent's successful convergence.
    </p>
    <p class="body-text">
        Because of the oscillations, the exact gradient descent trajectory is hard to understand.
        For example, the network predictions behave erratically, and the train loss curve behaves non-monotonically.
        In contrast, the time-averaged trajectory is much easier to understand: along this time-averaged trajectory, the network predictions evolve smoothly, and the train loss curve decreases monotonically.
        The oscillations are generated when the curvature exceeds \(2/\eta\), and they do not trigger divergence because they automatically steer gradient descent into a less-sharp region.
    </p>
    <p class="body-text">
        This time-averaged trajectory can be captured by a differential equation called the <em>central flow</em>.
        The central flow averages out the oscillations themselves, while retaining their lasting effect on the trajectory, which is distilled into an implicit curvature penalty.
    </p>
    <p class="body-text">
        Key to this is that the oscillations only seem to affect this time-averaged trajectory via their covariance, and there is only one covariance that is compatible with the observed EOS dynamics. 
    </p> -->
    <p>
        Interested in this line of work?  Consider pursuing a PhD with <a href="https://alex-damian.github.io/">Alex Damian</a>, who will join the MIT Math and EECS departments in Fall 2026.
     </p>
    <!-- <p class="body-text">
        We are now in a position to answer the titular question: "how does gradient descent work?"
        Gradient descent takes an oscillatory path through weight space — it follows some macroscopic trajectory while oscillating back-and-forth along the top Hessian eigenvectors.
        (Visually, this resembles moving through a "valley" while bouncing between the "valley walls".)
        The oscillations dominate the train loss and gradient norm over the short-term, but mostly cancel out over the long run.
        Their only lasting effect on the macroscopic trajectory is to induce implicit reduction of curvature, which prevents any Hessian eigenvalues from rising above \(2/\eta\).        
    </p>
    <p class="body-text">
        To analyze the system, we derive a differential equation called a <em>central flow</em> that characterizes the macroscopic trajectory \(w(t)\), as well as the covariance of the oscillations \(\Sigma(t)\).
        <strong>TODO finish this</strong>
         While the exact oscillations may be chaotic, their covariance is predictable, and the macroscopic gradient descent trajectory only depends on the oscillations via this covariance. -->
        <!-- To characterize this macroscopic trajectory, we can jointly solve for the macroscopic trajectory and oscillation covariance, as there is only one combination that satisfies several natural desiderata. -->
        <!-- The central flow is a <b>hidden progress metric</b>, or potential function, for the optimization process. -->
    <!-- </p> -->

 </div>
  </body>
</html>
