<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ntietz.com/blog/can-i-ethically-use-llms/?utm_source=atom&amp;utm_medium=feed">Original</a>
    <h1>Can I ethically use LLMs?</h1>
    
    <div id="readability-page-1" class="page"><div><p>The title is not a rhetorical question, and I&#39;m not going to bury an answer.
I don&#39;t <em>have</em> an answer.
This post is my exploration of the question, and why I think it <em>is</em> a question.</p>
<p>Important things up front: what&#39;s my relationship with LLMs today?</p>
<ul>
<li><em>I don&#39;t use any LLMs regularly.</em> I do have access to GitHub Copilot through my employer. I have it available on a hotkey, I think, and I cannot remember how to trigger it since I do not use it.</li>
<li><em>I&#39;ve explored using LLMs in the past.</em> I used to be a regular Copilot user, and I explored ChatGPT, Claude, etc. to see what their capabilities were. I have done trainings for my coworkers on how to use them effectively, though I would not feel comfortable doing so now.</li>
<li><em>My employer&#39;s product uses LLMs.</em> I don&#39;t want to link to my employer, but yeah, I guess my paycheck depends on being okay with integrating them in? It&#39;s complicated (a refrain). (This post is, obviously, <em>my</em> opinions and does not reflect my employer.)</li>
<li><em>I don&#39;t think using or not using them is a moral failing.</em> There is a lot of moralizing around LLM usage. I&#39;m not doing any of that here. I have my own beliefs (or my own questions), but I don&#39;t think people using LLMs are immoral (or vice versa).</li>
</ul>
<p>So, you can see I have used them and I&#39;m not absolutist, but I don&#39;t use them today.
Why not?
Why did I stop using them in spite of the advances, where they&#39;re more capable than ever?</p>
<p>It&#39;s because of these questions and issues.
Where I have undecided ethical questions, I lean toward the more conservative choice of not using them until I have clarity on the ethics.
(Note: I am <em>not</em> inviting folks to email me with answers to this question.)</p>

<p>Another technology that uses a lot of energy is blockchains.
I think using public blockchains is almost universally unethical since there are other, better, less harmful options.
Part of the harm from blockchains is an absurd amount of energy usage.</p>
<p>LLMs also use a lot of energy.
This can be split into <em>training</em> and <em>inference</em> energy usage.
These vary based on the model.</p>
<p>Some models can run locally on Apple silicon, and those are lower energy usage—their upper bound is running your computer full tilt, and an M4 Mac mini&#39;s max power consumption is 65 watts.
This is roughly equivalent to one incandescent light bulb, or 8 LED light bulbs.
It&#39;s good to turn off unnecessary lights, but doing so isn&#39;t going to solve the climate crisis; we need bigger, more sweeping reforms.
I don&#39;t think that local models are going to significantly alter the climate crisis in either direction.</p>
<p>Other models are massive and run in data centers on lots of power-hungry GPUs.
These data centers also require construction, and that comes with its own environmental impact.
An article from <a href="https://www.tomsguide.com/ai/chatgpt-energy-emergency-heres-how-much-electricity-openai-and-others-are-sucking-up-per-week">Tom&#39;s Guide</a> last year showed that &#34;a single query on ChatGPT-4 can use up to 3 bottles of water, that a year of queries uses enough electricity to power over nine houses&#34;.
A lot of the cost comes from <em>new data centers</em> being built.</p>
<p>The demand for LLMs has led to more demand for power generation and more demand for data centers.
And this new power generation is coming from <em>gas-fired plants</em> instead of sustainable, clean energy sources, because that&#39;s all we can build fast enough.</p>
<p>A lot of attention is given to the training side.
The numbers for training are large and shocking: Llama 3 used <a href="https://cacm.acm.org/blogcacm/the-energy-footprint-of-humans-and-large-language-models/">500 MWh</a> and GPT-3 training used <a href="https://cacm.acm.org/blogcacm/the-energy-footprint-of-humans-and-large-language-models/">1,287 MWh</a>—even more if you include the cost of training failed models which preceded these, the experiments that made the models possible.
The listed figures are high, and 500 MWh is about the cost of a large jet flying for 7 hours.
But we do it <em>once</em> per foundational model, and then the cost is spread across <em>all</em> the remaining usage.</p>
<p>I don&#39;t think that the training side is significantly shifting the equation on climate change.
We&#39;d have a much larger impact on improving the climate crisis by advocating for remote work—reducing vehicles on the road, making many flights unnecessary—than by not training models.</p>
<p>Overall it feels to me like <em>local models</em> have a clearly acceptable impact, and <em>data center models</em> have higher energy usage but still probably do not change the situation very much.</p>

<p>The training data for LLMs has largely been lifted without the consent of the people who generated that data.
This is a lot of writing, music, videos, visual art, all of it.
There are <em>some</em> attempts at there at using licensed data only, but the majority of models, and the most popular ones, are unlicensed data.</p>
<p>Now the question is: is this an ethical problem?</p>
<p>I know there are opinions on both sides of this.
Some say that this data is publicly visible on the internet (though some of the data was <em>not</em> on the internet), and so it&#39;s fair game.
Others say that this use isn&#39;t one that people consented to, and it should require that consent.</p>
<p>My thought experiment is this:
If we made search engines today, would people have this same objection to a search engine using their data without their express consent?
I think most people would ultimately support search engines.
They <em>are</em> different than LLMs, because they (mostly) serve results that point you to the original source, rather than create <em>new</em> content for you to replace the original sources with.</p>
<p>But maybe people would reject search engines.
And maybe consistency between these two isn&#39;t necessary, or maybe it&#39;s a false consistency—there could be other differentiating details that lead to different answers.</p>
<p>Where I come down is that I think we need a robust mechanism to opt out of use of data for training, but that it&#39;s probably fine to train with publicly available data on the internet.
What you <em>do</em> with the trained model is another question entirely.
When you try to replace people making original works instead of creating an entirely new function, that&#39;s where you get questions.</p>

<p>There&#39;s a lot of LLM usage that is just trying to replace people in entire jobs.
I mean, it feels like all of it.</p>
<p>We see LLMs that are meant to replace writers and editors, artists and illustrators, musicians and songwriters.
It doesn&#39;t <em>say</em> that directly—it says you&#39;re empowered to create things yourself.
But what it means is that people should be able to press a few buttons and, with no artist involved, get out a beautiful artwork.
Sounds like replacing to me.</p>
<p>This is something we&#39;ve long done with technology.
We make technology that puts people out of jobs, and that was the whole industrial revolution.
That&#39;s what happened with shipping from the containerization of ports, putting many dockworkers out of jobs.</p>
<p>Replacing people in the abstract is not unethical.
The problem is if we fail to deal with the harm created from replacing people.
And people <em>will</em> be harmed, because losing your job or the value of it going down has a serious impact on quality of life.</p>
<p>When we put people out of work, we—both society and technologists—have an ethical responsibility to ensure there&#39;s a plan to mitigate the harm from that.
Maybe that means grants for living expenses while people switch fields, if put out of work in an LLM-heavy industry.
Maybe that means a universal basic income.
But it certainly doesn&#39;t mean doing <em>nothing</em>.</p>

<p>One of the major well-known problems of LLMs is a tendency to &#34;hallucinate,&#34; or to confidently state facts that are made up from whole cloth.
They also have an unknown amount of bias, with unknown mitigations in place, due to being closed systems.</p>
<p>This is a big problem!
We&#39;re not good at seeing what information is incorrect in something that&#39;s generated to look like it&#39;s the <em>most likely string of tokens</em>.
If there is incorrect information in there, we&#39;ll just miss it.
This means that people can make poor decisions on the basis of what an LLM tells them.
They can have lost income due to its mistakes.</p>
<p>And the bias?
That&#39;s a <em>huge</em> problem, because it means that we don&#39;t know if this system will reinforce existing problematic norms.
We don&#39;t know what it <em>will</em> reinforce, because the training data is closed and there&#39;s not a lot of public evaluation on bias.
So ultimately, we&#39;re left with an unknown harm of unknown magnitude to an unknown population.</p>

<p>A big ethical concern for me is also what this will do to our entire society.
Many technologies are heralded as &#34;democratizing&#34; things.
Spotify &#34;democratized&#34; music by making it so that anyone can get listens—but, y&#39;all, it ended up flattening the tail and making the popular artists <em>more</em> money while making small artists <em>less</em> money.
Will LLMs do the same?</p>
<p>We know that the big models need large data centers to run their training and inference.
And even small models need beefy hardware to run inference, let alone training!</p>
<p>We have some access to models which can run locally, which is a good step.
But the problem is that we can only run <em>other people&#39;s</em> models.
They&#39;ll have those people&#39;s decisions baked in, decisions on which data to include or to exclude, decisions on how to approach questions of bias and abuse.</p>
<p>And when the hardware to run the biggest models is only accessible to a few companies, that means that those companies really get a lot more powerful.
OpenAI and Anthropic and Google and Meta all have the ability to run really large models.
I certainly don&#39;t, though.</p>
<p>This means that a technology that many are heralding as making things more accessible to everyone is controlled by a small handful of people.
A small handful of people can decide how the models are trained, and set policies on how they&#39;re used.
In a time when the US government is trying to get any paper retracted that mentioned queer people, and erasing trans people from the Stonewall monument, it feels <em>self-evident</em> that letting a small group of people control this technology imperils the future of many people.</p>
<p>* * *</p>
<p>Ultimately, I want robots to do the things I don&#39;t want to do.
I want them to do my dishes and my laundry.
I don&#39;t want them to play music instead of me, write code instead of me, write words instead of me.</p>
<p>I am not sure whether or not using LLMs is unethical.
There are certainly <em>ways</em> of using them which are unquestionably unethical—as is true with every technology.
And there are ways of <em>developing</em> LLMs which are unethical—as is true with every technology.</p>
<p>But the problems with them are large.
I think it <em>is</em> unethical to use them without addressing the ethical questions above.
If you&#39;re <em>not</em> working on mitigating the harms from LLMs (which do exist), then you might be doing something unethical.</p>
<hr/>





</div><p>
    If this post was enjoyable or useful for you, <strong>please share it!</strong>
    If you have comments, questions, or feedback, you can email <a href="mailto:me@ntietz.com">my personal email</a>.
    To get new posts and support my work, subscribe to the <a href="http://muratbuffalo.blogspot.com/newsletter/">newsletter</a>. There is also an <a href="http://muratbuffalo.blogspot.com/atom.xml">RSS feed</a>.
  </p><p>
  If you&#39;re looking to grow more effective as a software engineer, please consider my <a href="http://muratbuffalo.blogspot.com/coaching/">coaching services</a>.
  And if you are looking to solve problems that invovle software, you may want to consider my <a href="http://muratbuffalo.blogspot.com/services/">consulting services</a>.
  </p></div>
  </body>
</html>
