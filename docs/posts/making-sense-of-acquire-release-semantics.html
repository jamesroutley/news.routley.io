<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://davekilian.com/acquire-release.html">Original</a>
    <h1>Making Sense of Acquire-Release Semantics</h1>
    
    <div id="readability-page-1" class="page"><div>
        
        <p>February 2023</p>
        
        <p><em>Multiprocessor Synchronization</em> was one of my favorite classes during my undergrad ‚Äî it had a clear progression from theory to practice, starting at the theory of consensus numbers and moving onto atomic operations and then synchronization primitives and lock-free data structures, all from first principles. With careful thought and a little intuition-bending, every problem could be stated clearly and solved in a neat, orderly way ‚Ä¶ in Java.</p>

<p>Later on, when I tried to translate what I learned into C, I learned why the course was taught in Java. üôÇ</p>

<p>Java does a lot more to help you write correct multithreaded code than I realized back then. I knew having a garbage collector was helpful, and I guess I sort of knew <code>volatile</code> did <em>something</em>, but I didn‚Äôt appreciate the extent of what that <em>something</em> is. Without it, the algorithms and techniques I learned in my multiprocessor course still worked, but coding them correctly required a much deeper understanding of how computers run code at the most basic level.</p>

<p>In this post, I want to introduce, or maybe re-introduce you to <em>acquire-release semantics</em>. Plenty of developers who can competently write lock-free code in native languages like C, C++ and Rust still have a fuzzy idea of what these are or when to use them. They‚Äôre actually not so hard in and of themselves ‚Äî what makes them difficult is the misconceptions you may harbor about what goes on at the hardware level when your code is running. So, in a way, that‚Äôs really what I want to explain in this post.</p>

<p>When we‚Äôre done, you should be able to answer the following question confidently:</p>

<center>When I use acquire or release semantics, <i>what</i> is being acquired and released?</center>

<p>The short answer is <em>ownership of other memory</em>, but that probably leaves you with more questions than answers. So let‚Äôs start from the beginning! It‚Äôll be a long climb, but I think you‚Äôll like the view from the top!</p>

<p>To start us out, read this in your best <a href="https://www.hbo.com/silicon-valley/cast-and-crew/bertram-gilfoyle">Gilfoyle</a> impression:</p>

<p><em>‚ÄúIf memory worked the way you thought it did, you‚Äôd never need to acquire or release anything. Unfortunately, memory doesn‚Äôt work the way you think it does.‚Äù</em></p>

<h2 id="how-you-think-memory-works">How you think memory works</h2>

<p>For the sake of example, let‚Äôs say you‚Äôre building a lock-free FIFO queue.</p>

<p>There are many strategies for building a queue; we‚Äôll use a circular array. Let‚Äôs say there‚Äôs a single ‚Äòproducer‚Äô thread solely responsible for enqueuing items, as well as another ‚Äòconsumer‚Äô thread solely responsible for dequeuing those items. So, we‚Äôre building a <em>lock-free single-producer single-consumer ring queue</em>. If this seems contrived, pretend we‚Äôre working with Linux‚Äôs <a href="https://en.wikipedia.org/wiki/Io_uring">io_uring</a> interface, where a ring is two of these queues: a <em>send queue</em> that submits I/O from userland to the kernel, and a <em>completion queue</em> for the kernel to deliver results back to the user thread.</p>

<p>Here‚Äôs a ‚Äòtoy‚Äô implementation of this queue design:</p>

<div><div><pre><code><span>class</span> <span>toy_queue</span> <span>{</span>
  
  <span>unsigned</span> <span>head</span><span>;</span>
  <span>unsigned</span> <span>tail</span><span>;</span>
  <span>unsigned</span> <span>size</span><span>;</span>
  <span>void</span> <span>**</span><span>entries</span><span>;</span>
  
<span>public:</span>
  
  <span>void</span> <span>init</span><span>()</span> <span>{</span>
    <span>head</span> <span>=</span> <span>0</span><span>;</span> <span>tail</span> <span>=</span> <span>0</span><span>;</span>
    <span>memset</span><span>(</span><span>entries</span><span>,</span> <span>0</span><span>,</span> <span>size</span> <span>*</span> <span>sizeof</span><span>(</span><span>void</span> <span>*</span><span>));</span>
  <span>}</span>
  
  <span>void</span> <span>put</span><span>(</span><span>void</span> <span>*</span><span>entry</span><span>)</span> <span>{</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
    <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
    <span>tail</span><span>++</span><span>;</span>
  <span>}</span>
  
  <span>bool</span> <span>poll</span><span>(</span><span>void</span> <span>**</span><span>entry</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span>
      <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
      <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>
      <span>head</span><span>++</span><span>;</span>
      <span>return</span> <span>true</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>return</span> <span>false</span><span>;</span> <span>// empty</span>
    <span>}</span>
  <span>}</span>
  
<span>};</span>
</code></pre></div></div>

<p>For brevity, I skipped heap allocating the entry array, and we‚Äôll just have to hope the caller never overflows the queue. We‚Äôre going to revisit this code several times throughout the post, so make sure you get it before moving on. All good? Then here‚Äôs a puzzle for ya:</p>

<p>There‚Äôs a bug in this queue, and I‚Äôm wondering if you can find it. When you run this on a real computer, sometimes <code>poll()</code> returns success without giving you an entry. That is, <code>poll()</code> returns true, but <code>entry</code> is still null ‚Äî even though the producer enqueued something non-null. Here‚Äôs a repro program that sometimes demonstrates the issue:</p>

<div><div><pre><code><span>static</span> <span>toy_queue</span> <span>queue</span><span>;</span>
<span>static</span> <span>foo</span> <span>an_entry</span><span>;</span>

<span>void</span> <span>producer</span><span>()</span> <span>{</span>
  <span>queue</span><span>.</span><span>put</span><span>(</span><span>&amp;</span><span>an_entry</span><span>);</span>
<span>}</span>

<span>void</span> <span>consumer</span><span>()</span> <span>{</span>
  <span>void</span> <span>*</span><span>entry</span> <span>=</span> <span>nullptr</span><span>;</span>
  <span>bool</span> <span>exists</span> <span>=</span> <span>queue</span><span>.</span><span>poll</span><span>(</span><span>&amp;</span><span>entry</span><span>);</span>
  <span>if</span> <span>(</span><span>exists</span><span>)</span> <span>{</span>
    <span>// oops, entry is null!</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Since we‚Äôre letting the producer and consumer threads race, there are two valid outcomes for the consumer:</p>

<ol>
  <li><code>exists</code> is false and <code>entry</code> is null (poll serialized before put)</li>
  <li><code>exists</code> is true and <code>entry</code> points to <code>an_entry</code> (put serialized before poll)</li>
</ol>

<p>Each of these things happens some of the time. But sometimes, <code>exists</code> is true, but <code>entry</code> is still null. How could <em>that</em> happen? Think it over!</p>

<p>If you think it‚Äôs obvious this can happen, try working backwards:</p>

<ul>
  <li>We know <code>poll()</code> returns true</li>
  <li>That means <code>poll()</code> must have seen <code>tail &gt; head</code></li>
  <li>Which means <code>put()</code> must have already incremented <code>tail</code></li>
  <li>But incrementing <code>tail</code> is the <em>last</em> step of a <code>put()</code></li>
  <li>And checking the <code>tail</code> is the <em>first</em> step of a <code>poll()</code></li>
  <li>So <code>put()</code> finished before <code>poll()</code> even started!</li>
  <li>‚Ä¶ but then, where did the entry go?</li>
</ul>

<p>Just to show you there‚Äôs nothing up my sleeve, let‚Äôs try combining the logic of both threads into a single log of the whole program. If we do that, the following order should be the only one possible: notice how moving the first line of <code>poll()</code> up so it happens even one line earlier, would change the return value of <code>poll()</code> to <code>false</code>:</p>

<div><div><pre><code><span>put:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span>
<span>put</span><span>:</span> <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span>
<span>put</span><span>:</span> <span>tail</span><span>++</span>
<span>poll</span><span>:</span> <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span>
<span>poll</span><span>:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span>
<span>poll</span><span>:</span> <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>]</span>
<span>poll</span><span>:</span> <span>head</span><span>++</span>
<span>poll</span><span>:</span> <span>return</span> <span>true</span>
</code></pre></div></div>

<p>But this clearly has <code>put()</code> writing to <code>entries[i]</code> before <code>poll()</code> reads it. So why does <code>poll()</code> read null?</p>

<p>See, not so obvious! If I‚Äôve managed to stump you now, then allow me to reveal what‚Äôs wrong. We fell victim to one of the classic blunders, the most famous of which is ‚Äúnever get involved in a land war in Asia,‚Äù but only slightly less well-known is this:</p>

<h2 id="memory-isnt-linearizable">Memory isn‚Äôt linearizable!</h2>

<p>Have you used a debugger before? If yes, I suspect it affected your intuition of computer memory. If you‚Äôve ever stepped through code line by line, watching as your variables change each time you step, it‚Äôs tempting to think that‚Äôs how code really works: running one line at a time (or at least, one assembly instruction at a time), with the contents of your variables changing in memory instantly during each step.</p>

<p>We can say this more rigorously than ‚Äúlike debugging.‚Äù In fact, there‚Äôs a theoretical framework that captures this intuition with formal mathematics, called <strong>linearizability</strong>. The basic idea is that, if your computer guarantees linearizability, then you should always be able to identify a single point in time during each instruction when the memory changes for that instruction instantly became visible to every thread simultaneously. That point in time is called a <em>linearization point</em>.</p>

<p>If you can collapse each thread‚Äôs logic into a sequence of linearization points, it‚Äôs been proved formally that you can do something that intuitively seemed right all along: combining the linear histories of two threads into a single linear history that explains everything that happened in each thread. Sound familiar? Because, that‚Äôs exactly what we just <em>failed</em> to do for our repro program! The reason is simple: there‚Äôs no such thing as a single global order of memory operations on a real computer, memory isn‚Äôt linearizable, and what you see in a debugger isn‚Äôt what happens at runtime!</p>

<p><img src="https://davekilian.com/assets/debuggers-lie.jpeg" alt=""/></p>

<p>Take a closer look at the body of <code>toy_queue::put()</code>:</p>

<div><div><pre><code><span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
<span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
<span>tail</span><span>++</span><span>;</span>
</code></pre></div></div>

<p>Remember, your CPU doesn‚Äôt directly read and write memory; memory is many times slower than a CPU, so the CPU reads and writes memory through a cache. Maybe when running this code, it so happens that we get a cache hit for <code>tail</code> and <code>size</code>, but we miss on <code>entries[i]</code>. Now we‚Äôre blocked: we can‚Äôt execute the second line of this function until the memory contents come into the cache, and memory is slow, so that might take a while. Wouldn‚Äôt it be nice if the CPU could work ahead while it waits?</p>

<p>How about we execute the third line of code while we‚Äôre blocked on the second? In other words, why not run the program <em>as if</em> it were written like this?</p>

<div><div><pre><code><span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
<span>tail</span><span>++</span><span>;</span> <span>// &lt;-- do this early</span>
<span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
</code></pre></div></div>

<p>Sure, it‚Äôs not what the code says, but if we made this change, how would anyone know the difference? Either way, <code>tail</code> gets incremented and <code>entries[i]</code> gets initialized; neither value gets read back until both have been written. If your code can‚Äôt even tell I changed the order, then changing the order can‚Äôt possibly break the code, right? Right! So some CPUs are designed to make changes like this to your code, speeding it up without impacting correctness.</p>

<p>Hm, but we <em>did</em> impact correctness, didn‚Äôt we? Looking at <code>put()</code> narrowly through a single-threaded lens, it doesn‚Äôt seem like changing the order of these two memory writes matters, but when we zoom out and look at the whole program, it‚Äôs definitely broken now. This two-line swap introduced a race: if you bump <code>tail</code> before writing <code>entries[i]</code>, you‚Äôre reporting there‚Äôs an item in the queue <em>before</em> the item is actually in the queue! So now the consumer might go and read <code>entries[i]</code> before you write it!</p>

<div><div><pre><code><span>put:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span>
<span>put</span><span>:</span> <span>tail</span><span>++</span>               <span>&lt;--</span> <span>oops</span>
<span>poll</span><span>:</span> <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span>    <span>&lt;--</span> <span>oh</span> <span>no</span>
<span>poll</span><span>:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span>
<span>poll</span><span>:</span> <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>]</span> <span>&lt;--</span> <span>stop</span>
<span>poll</span><span>:</span> <span>head</span><span>++</span>
<span>poll</span><span>:</span> <span>return</span> <span>true</span>
<span>put</span><span>:</span> <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span>   <span>&lt;--</span> <span>no</span>
</code></pre></div></div>

<p>Well, that explains it: <code>poll()</code> sometimes returns true without an entry, because that‚Äôs really the state of the queue: <code>tail</code> really says there‚Äôs an entry available, and <code>entries[i]</code> really hasn‚Äôt been set yet. But hey, our code doesn‚Äôt do that ‚Äî the CPU did. This is a highly unusual situation, wouldn‚Äôt you say? I mean, how many times have we all made fun of this guy?</p>

<p><img src="https://davekilian.com/assets/cpu-is-wrong.jpeg" alt=""/></p>

<p>Well, this time, Principal [Software Engineer] Skinner might have a point. We wrote correct code, and in trying to dynamically optimize it, the CPU transformed our correct code into broken code. But if we were to lodge this complaint with someone who designs CPUs, they might turn this around on us: <em>our</em> code is wrong because <em>we</em> were wrong to assume linearizable memory.</p>

<p><img src="https://davekilian.com/assets/code-is-wrong.jpeg" alt=""/></p>

<p>This is an old argument hashed out many times on remote corners of the Internet. Regardless of your position, ultimately what matters is CPUs aren‚Äôt going to change, which means our code has to.</p>

<h2 id="so-what-do-we-do-now">So what do we do now?</h2>

<p>If you‚Äôre meta-gaming, you might think this is the point we break out the atomics ‚Äî add some fetch-and-adds or compare-and-swaps and our problems will disappear, right? Not quite, I‚Äôm afraid.</p>

<p>Atomics are useful for solving a different problem: they resolve conflicts between parallel updates to the same variable. For example, if we had multiple threads trying to increment <code>tail</code> at the same time, we could use an atomic fetch-and-add to serialize the updates so each thread does something meaningful. But there are no such conflicts in our queue. We have exactly one producer thread managing all updates to <code>entries</code> and <code>tail</code>, and one consumer thread managing <code>head</code>; those three are our only variables. And if all updates to a variable always come from the same thread, then updates are always serial, so there are no conflicts to resolve. There are never conflicting updates to any variable in our queue, so we have no need for atomics.</p>

<p>Our problem is <strong>memory ordering</strong>.</p>

<p>When we put an entry in the queue, we need to update <em>two</em> variables: the queue entry and the tail pointer. Order matters: we want to write the entry first and <em>then</em> increment the tail, because updating the tail is what tells the consumer an entry is ready to be polled. If the consumer tries to poll while we‚Äôre updating, as long as we haven‚Äôt incremented the tail yet, the consumer will pass by without ‚Äòseeing‚Äô our in-progress update yet. It‚Äôs a good plan! The only reason this didn‚Äôt work is we don‚Äôt have the <em>memory ordering guarantees</em> we thought we did. We were completely reasonable, but ultimately wrong in thinking writing two lines of code in order would make the CPU execute them in that order. Now that we know a CPU might shuffle our code around, we need some way to tell the CPU no, you can‚Äôt do that, order matters here.</p>

<p>We can say that using <strong>fences</strong>. (Fences are also known as <em>barriers</em> ‚Äî but for this post, I‚Äôll use the term <em>fence</em>).</p>

<p>Different CPUs express the idea of a fence in different ways. As a first-order approximation, you can think of a fence as a kind of CPU instruction, but an odd one at that: these ‚Äòfence‚Äô instructions don‚Äôt do anything, they just sit there, marking points in your code like mile marker signposts on a freeway. We call them ‚Äúfences‚Äù because they enact boundaries. If a CPU decides it‚Äôs a good idea to push an instruction down so it happens later, or pull one up so it happens sooner, but the instruction runs into a fence along the way, then <em>bonk!</em> the instruction stops moving. Instructions can‚Äôt cross a fence.</p>

<p>To see how this works in practice, let‚Äôs imagine we have a method called <code>fence()</code> that gets compiled to a CPU fence instruction like I just described. Then here‚Äôs how we might use <code>fence()</code> to fix our queue:</p>

<div><div><pre><code><span>class</span> <span>toy_queue</span> <span>{</span>
  
  <span>unsigned</span> <span>head</span><span>;</span>
  <span>unsigned</span> <span>tail</span><span>;</span>
  <span>unsigned</span> <span>size</span><span>;</span>
  <span>void</span> <span>**</span><span>entries</span><span>;</span>
  
<span>public:</span>
  
  <span>void</span> <span>init</span><span>()</span> <span>{</span>
    <span>head</span> <span>=</span> <span>0</span><span>;</span> <span>tail</span> <span>=</span> <span>0</span><span>;</span>
    <span>memset</span><span>(</span><span>entries</span><span>,</span> <span>0</span><span>,</span> <span>size</span> <span>*</span> <span>sizeof</span><span>(</span><span>void</span> <span>*</span><span>));</span>
  <span>}</span>
  
  <span>void</span> <span>put</span><span>(</span><span>void</span> <span>*</span><span>entry</span><span>)</span> <span>{</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
    <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
    <span>fence</span><span>();</span> <span>// &lt;-- new!</span>
    <span>tail</span><span>++</span><span>;</span>
  <span>}</span>
  
  <span>bool</span> <span>poll</span><span>(</span><span>void</span> <span>**</span><span>entry</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span>
      <span>fence</span><span>();</span>
      <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
      <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>
      <span>head</span><span>++</span><span>;</span>
      <span>return</span> <span>true</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>return</span> <span>false</span><span>;</span> <span>// empty</span>
    <span>}</span>
  <span>}</span>  
  
<span>};</span>
</code></pre></div></div>

<p>See how we added a new <code>fence()</code> call to <code>put()</code>? That fixes the reordering problem we‚Äôve described at length. Now if the CPU gets bored waiting for <code>entries[i]</code> to be read into the cache, and tries to pull up the <code>tail++</code> line so it happens sooner, <em>bonk!</em>, the <code>tail++</code> line hits that fence and stops moving. We‚Äôve forced the entry to be written before the tail is bumped. Problem solved!</p>

<p>But hang on ‚Ä¶</p>

<p><img src="https://davekilian.com/assets/fence-in-poll.jpeg" alt=""/></p>

<p>There was a second bug all along! üòÄ Just as the producer must make sure the entry is written before making it visible (by incrementing the tail), so too must the consumer ensure the entry is visible (tail was incremented) before reading the entry itself. Without that <code>fence()</code> call in <code>poll()</code>, a CPU would be allowed in principle to change <code>poll()</code> so the entry is read <em>before</em> the tail is checked, kind of like this:</p>

<div><div><pre><code><span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
<span>void</span> <span>*</span><span>temp</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>

<span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span>
  <span>*</span><span>entry</span> <span>=</span> <span>temp</span><span>;</span>
  <span>head</span><span>++</span><span>;</span>
  <span>return</span> <span>true</span><span>;</span>
<span>}</span>

<span>return</span> <span>false</span><span>;</span> <span>// empty</span>
</code></pre></div></div>

<p>Once again, this creates a race condition where the polling thread can return true, but still read null:</p>

<div><div><pre><code><span>poll:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
<span>poll</span><span>:</span> <span>void</span> <span>*</span><span>temp</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>
<span>put</span><span>:</span> <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span>
<span>put</span><span>:</span> <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span>
<span>put</span><span>:</span> <span>tail</span><span>++</span>
<span>poll</span><span>:</span> <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span>
<span>poll</span><span>:</span> <span>*</span><span>entry</span> <span>=</span> <span>temp</span><span>;</span>
<span>poll</span><span>:</span> <span>head</span><span>++</span><span>;</span>
<span>poll</span><span>:</span> <span>return</span> <span>true</span><span>;</span>
</code></pre></div></div>

<p>Notice how <code>temp</code> will be null, because <code>put()</code> really hadn‚Äôt started yet; but the return value will be <code>true</code> because, by the time we checked the tail, <code>put()</code> had already completed. This is a new bug with the same symptoms: the return value is true but the entry isn‚Äôt initialized. That‚Äôs what the new fence in <code>poll()</code> fixes.</p>

<p>Now I don‚Äôt know about you, but I find this second bug concerning. So far the CPU has come up with two different ways to break our code, and both were the kinds of subtle, impossible-to-repro problems that can slip through review and testing and then run roughshod through production. Even worse, the second bug had the exact same symptoms as the first one; imagine finally finding and fixing the first bug, only to find the same thing is still happening in prod days after the rollout! When is this going to end? How do we know when the CPU is out of tricks? Can we ever declare our queue correct?</p>

<p>Maybe we should try to <em>prove</em> our queue works now.</p>

<h2 id="reasoning-about-fences">Reasoning about Fences</h2>

<p>No more shooting from the hip and reasoning based on what we expect. To show the bug is fixed, we need to understand what guarantees the CPU gives us, and use those to justify what we did. We need to channel the energy of that rules-lawyering friend who you keep inviting to your DnD sessions for some reason.</p>

<p>There are four steps needed to hand off an entry through the queue:</p>

<ol>
  <li>The entry is written to <code>entries[i]</code></li>
  <li>The entry is made ‚Äòvisible‚Äô by incrementing <code>tail</code></li>
  <li>On poll, a consumer sees <code>tail</code> was incremented</li>
  <li>The entry is read from <code>entries[i]</code></li>
</ol>

<p>For reference, here are <code>put()</code> and <code>poll()</code> again, with those four steps marked:</p>

<div><div><pre><code><span>void</span> <span>put</span><span>(</span><span>void</span> <span>*</span><span>entry</span><span>)</span> <span>{</span>
  <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
  <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span> <span>// (1)</span>
  <span>fence</span><span>();</span>
  <span>tail</span><span>++</span><span>;</span> <span>// (2)</span>
<span>}</span>

<span>bool</span> <span>poll</span><span>(</span><span>void</span> <span>**</span><span>entry</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span> <span>// (3)</span>
    <span>fence</span><span>();</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
    <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span> <span>// (4)</span>
    <span>head</span><span>++</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span>
<span>}</span>  
</code></pre></div></div>

<p>If all four of these steps run in order, there is no ‚Äòmissing entry‚Äô bug. There can‚Äôt be: <code>poll()</code> only thinks an entry is available if it checks the tail (3) after it was incremented (2), and if everything happens in order, then the entry is written (1) before the handshake in (2-3), and is later read (4) after the handshake. So if we see an entry is available, then the entry‚Äôs read always comes after the write.</p>

<p>The question is, do we really know the order always stays 1-2-3-4 now? Let‚Äôs try checking each pairwise relationship between any two neighboring steps:</p>

<p><strong>The entry is written (1) before the tail is updated (2)</strong>: This order is guaranteed by the fence that appears between these two steps. The fence works because both steps are in the same thread.</p>

<p><strong>The producer increments the tail (2) before the consumer sees it (3)</strong>: This order is guaranteed by the ‚Äòobvious‚Äô rule that memory only changes if someone writes it; in this program, only the producer writes to tail. So if the consumer sees the tail was incremented in (3), then the producer must have previously incremented it in (2).</p>

<p>Note we have no guarantee how long it takes after the producer writes the tail (2) for the value to become available for the consumer to read (3); however, this doesn‚Äôt affect the relative order of (2) and (3), so for this proof, the delay is not important.</p>

<p><strong>The consumer sees the new tail (3) before it reads the entry (4)</strong>: Once again, we have two operations in the same thread that appear on opposite sides of a fence, so they happen in program order.</p>

<p>And there you go. Even though the CPU might start step 1 way earlier than you might expect, and even though step 4 might start way late, and even if plenty of time could pass between steps 2 and 3, none of the matters; we‚Äôve established that the relative order of all four steps will always be 1-2-3-4 using rules backed by guarantees from the CPU‚Äôs designers. We have finally rid ourselves of that bug! üéâ</p>

<p>Our queue now works, so we <em>could</em> declare ourselves done ‚Ä¶ but you want to go the extra step, right? Because, the <code>fence()</code> method we came up with here works, but it isn‚Äôt particularly efficient. And besides, we don‚Äôt even know how to implement the <code>fence()</code> method yet. But for now, let‚Äôs focus on the performance aspect:</p>

<p>Consider this: when you have a bunch of bugs assigned to you, and you have a fix in code review, what do you do? I mean, if the answer is ‚Äúsit around and wait until my code is merged,‚Äù yo, I don‚Äôt judge; but I think many people would start working on the next bug in the meantime. If that‚Äôs you, then you understand the value of being able to take work items, break them down into smaller pieces, and then interleave those smaller pieces of work to stay busy so you get things done sooner. That‚Äôs pretty much what a CPU is doing when it ‚Äúworks ahead‚Äù in the way that was breaking our multithreaded code. The more you limit a CPU‚Äôs ability to break down the work and shuffle the pieces around, the more time it has to spend waiting instead of getting things done.</p>

<p>In that light, <code>fence()</code> <em>might</em> be overkill. Can we do something else, something with weaker (meaning more flexible) memory ordering guarantees?</p>

<p>Well, it‚Äôs tricky. There are a lot of things you can do in hardware <em>in principle</em>, but you only have space for so many transistors, which means anything you‚Äôre going to put in hardware had better be for something a lot of software can use. Whatever we invent here has to work for lots of multithreaded programs, not just our queue. Luckily, our queue happens to use a <em>very</em> common pattern we can certainly optimize for:</p>

<h2 id="single-ownership">Single Ownership</h2>

<p>In most multithreaded code, memory is owned by a single thread.</p>

<p>That‚Äôs obvious when you have threads working independently, each with its own private memory. But even when threads share memory, you almost always set it up so the threads <em>take turns</em> with it. In this setup, you could claim the memory has a single owner, because at any given point in time, you can identify a single thread that owns the memory: namely, whichever thread is taking its turn right now. (By the way, this also happens to be the principle underlying Rust‚Äôs <code>RefCell&lt;&gt;</code> type.)</p>

<p>One-at-a-time ownership ‚Ä¶ that‚Äôs how locks work, isn‚Äôt it?</p>

<div><div><pre><code><span>static</span> <span>foo</span> <span>an_entry</span><span>;</span>
<span>static</span> <span>std</span><span>::</span><span>mutex</span> <span>lock</span><span>;</span>

<span>void</span> <span>producer</span><span>()</span> <span>{</span>
  <span>lock</span><span>.</span><span>lock</span><span>();</span>
  <span>// (stuff)</span>
  <span>an_entry</span><span>.</span><span>ready</span> <span>=</span> <span>true</span><span>;</span>
  <span>lock</span><span>.</span><span>unlock</span><span>();</span>
<span>}</span>

<span>void</span> <span>consumer</span><span>()</span> <span>{</span>
  <span>lock</span><span>.</span><span>lock</span><span>();</span>
  <span>if</span> <span>(</span><span>an_entry</span><span>.</span><span>ready</span><span>)</span> <span>{</span>
    <span>// (stuff)</span>
  <span>}</span>
  <span>lock</span><span>.</span><span>unlock</span><span>();</span>
<span>}</span>
</code></pre></div></div>

<p>Locks are a perfectly valid way for threads to hand off ownership of shared memory. A thread tries to obtain ownership of the memory by grabbing a lock; once it has the lock, it can be certain nobody else is using the memory; this thread is the sole owner. When the thread is done, it hands off ownership by releasing the lock. At any given point in time, the memory either has one owner (the thread holding the lock) or no owner (when nobody has the lock).</p>

<p>But hey, it‚Äôs not just locks that can broker ownership of memory. What about our queue?</p>

<div><div><pre><code><span>static</span> <span>toy_queue</span> <span>queue</span><span>;</span>
<span>static</span> <span>foo</span> <span>an_entry</span><span>;</span>

<span>void</span> <span>producer</span><span>()</span> <span>{</span>
  <span>// (stuff)</span>
  <span>queue</span><span>.</span><span>put</span><span>(</span><span>&amp;</span><span>an_entry</span><span>);</span>
<span>}</span>

<span>void</span> <span>consumer</span><span>()</span> <span>{</span>
  <span>void</span> <span>*</span><span>entry</span> <span>=</span> <span>nullptr</span><span>;</span>
  <span>bool</span> <span>exists</span> <span>=</span> <span>queue</span><span>.</span><span>poll</span><span>(</span><span>&amp;</span><span>entry</span><span>);</span>
  <span>if</span> <span>(</span><span>exists</span><span>)</span> <span>{</span>
    <span>// (stuff)</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Look how the queue accomplishes the same basic ownership handoff as the lock. The producer starts off with implicit ownership of <code>an_entry</code>, and releases it by putting the entry in the queue. Later, the consumer dequeues the entry, thus obtaining ownership of the memory. In between, while the item is still in the queue, nobody owns the entry‚Äôs memory.</p>

<p>Looking at both of these examples and abstracting, it seems like we have stumbled upon some kind of ‚Äútwo-phase handoff‚Äù pattern. A thread which owns the memory does something to release it, notifying everybody it is no longer the current owner; later, a thread comes by and acquires ownership of the memory. When you put it in such a generic way, it seems likely this handoff scheme is useful in a wide variety of situations, not just the few we‚Äôve already talked about, don‚Äôt you think? Let‚Äôs try to optimize for this situation.</p>

<p>By the way, look how we‚Äôve started to use words like ‚Äúacquire‚Äù and ‚Äúrelease‚Äù and ‚Äúownership.‚Äù We‚Äôre getting close to the summit of our long climb! How‚Äôs the view looking from up here?</p>

<h2 id="acquire-and-release-mechanics">Acquire and Release Mechanics</h2>

<p>Okay, since we‚Äôre already thinking about this asynchronous handoff process at a very abstract level, let‚Äôs keep going. Abstractly, what can we say about <em>all</em> algorithms where one thread releases ownership of shared memory, and another thread asynchronously acquires ownership of said memory later?</p>

<p>First of all, it‚Äôs always possible for one thread to start handoff in parallel with another thread checking if it can complete handoff; these two steps have to be serialized. That means there should be exactly one memory operation to acquire ownership, and exactly one to release it. Let‚Äôs call these the <strong>acquire</strong> and <strong>release</strong> operations, respectively. Since the acquire and release operations must be serialized, they need to operate on the same variable, and they need to both be atomic; that‚Äôs what allows us to serialize them relative to one another. Note that a plain memory read or write counts as ‚Äúatomic‚Äù for our purposes here, as do the read-modify-write operations like fetch-and-add or compare-exchange we normally think of when we say ‚Äúatomic.‚Äù</p>

<p>If this sounds confusing, it might help to see these two operations in more concrete terms.</p>

<p>For the queue, the variable that both threads synchronize on is <code>tail</code>. The release is when the producer bumps the tail pointer; after that, the entry is no longer producer-owned since it can be picked up by the consumer at any time. The acquire is when the consumer later verifies the tail has been updated; that‚Äôs how the consumer knows it now owns the memory:</p>

<div><div><pre><code><span>void</span> <span>put</span><span>(</span><span>void</span> <span>*</span><span>entry</span><span>)</span> <span>{</span>
  <span>unsigned</span> <span>i</span> <span>=</span> <span>tail</span> <span>%</span> <span>size</span><span>;</span>
  <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
  <span>fence</span><span>();</span>
  <span>tail</span><span>++</span><span>;</span> <span>// &lt;-- release</span>
<span>}</span>

<span>bool</span> <span>poll</span><span>(</span><span>void</span> <span>**</span><span>entry</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>tail</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span> <span>// &lt;-- acquire</span>
    <span>fence</span><span>();</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
    <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>
    <span>head</span><span>++</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span>
<span>}</span>  
</code></pre></div></div>

<p>For a lock, we typically use a <code>state</code> variable that has a single bit indicating whether someone currently has the lock. That‚Äôs the variable threads synchronize on; the release might be a plain memory write that clears the locked bit, and the acquire might be a compare-and-swap that sets the lock bit if it‚Äôs not already set:</p>

<div><div><pre><code><span>class</span> <span>toy_lock</span> <span>{</span>
  <span>int</span> <span>state</span><span>;</span>
  
<span>public:</span>
  <span>toy_lock</span><span>()</span> <span>:</span> <span>state</span><span>(</span><span>0</span><span>)</span> <span>{</span> <span>}</span>
  
  <span>void</span> <span>unlock</span><span>()</span> <span>{</span>
    <span>state</span> <span>=</span> <span>0</span><span>;</span> <span>// release</span>
  <span>}</span>

  <span>bool</span> <span>try_lock</span><span>()</span> <span>{</span>
    <span>return</span> <span>cas</span><span>(</span><span>&amp;</span><span>state</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span> <span>// acquire</span>
  <span>}</span>
<span>};</span>
</code></pre></div></div>

<p>Does that kind of make sense? If so, then let‚Äôs go back to thinking abstractly. What else can we say about the acquire and release operations in <em>all</em> possible algorithms that hand off ownership like this?</p>

<p>Here‚Äôs an interesting observation: there‚Äôs no bound on how long any one thread will need to own the shared memory, but we want to be able to hand off the memory as soon as a thread is done with it. Since there‚Äôs no timing guarantee, the only way to make this work is for the releasing thread to <em>actively</em> notify other threads that it‚Äôs done now, which means it has to write something to memory. So a release must involve some kind of memory write. Similarly, other threads must observe that write, so an acquire must involve some sort of read.</p>

<p>In practice, a release is often just a plain memory write. There are two common patterns for the acquire: it can be a plain memory read, if it‚Äôs clear which thread owns the memory next, like with the queue. Or, the acquire can be an atomic read-modify-write by which the thread attempts to claim ownership for itself, like with the lock.</p>

<p>Putting in all together, the basic scheme is, abstractly:</p>

<ol>
  <li>Thread $A$ owns the shared memory and does stuff with it</li>
  <li>Thread $A$ does something involving a <strong>write</strong> to <strong>release</strong> ownership</li>
  <li>Thread $B$ does something involving a <strong>read</strong> to <strong>acquire</strong> ownership</li>
  <li>Thread $B$ owns the shared memory and does stuff with it</li>
</ol>

<p>Just like we saw with the queue earlier, though, we can‚Äôt implement sequential protocols like this without establishing ordering guarantees! So what guarantees do we need here? Remember, we‚Äôre trying to come up with the weakest (most flexible) guarantees we can. Let‚Äôs think about that next:</p>

<h2 id="acquire-and-release-semantics">Acquire and Release Semantics</h2>

<p>We‚Äôre back to thinking about fence-like things and ordering guarantees, but this time, our goal is subtly different from before. Back then, we were laser-focused on the internal workings of our queue; this time, we‚Äôre also concerned about what the calling code is doing to the shared memory before and after we hand off ownership. To see what I mean, consider this queue program again:</p>

<div><div><pre><code><span>static</span> <span>toy_queue</span> <span>queue</span><span>;</span>
<span>static</span> <span>foo</span> <span>an_entry</span><span>;</span>

<span>void</span> <span>producer</span><span>()</span> <span>{</span>
  <span>// (stuff)</span>
  <span>queue</span><span>.</span><span>put</span><span>(</span><span>&amp;</span><span>an_entry</span><span>);</span>
<span>}</span>

<span>void</span> <span>consumer</span><span>()</span> <span>{</span>
  <span>void</span> <span>*</span><span>entry</span> <span>=</span> <span>nullptr</span><span>;</span>
  <span>bool</span> <span>exists</span> <span>=</span> <span>queue</span><span>.</span><span>poll</span><span>(</span><span>&amp;</span><span>entry</span><span>);</span>
  <span>if</span> <span>(</span><span>exists</span><span>)</span> <span>{</span>
    <span>// (stuff)</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>We want to make sure all the producer‚Äôs (stuff) is done by the time the consumer starts doing its (stuff). We don‚Äôt want bog down callers handling fences and acquire-release and all that junk; we want to provide ordering guarantees for the <em>caller‚Äôs code</em> that surrounds ours, in addition to making sure our code also works internally.</p>

<p>Interestingly, in our current queue implementation, we don‚Äôt need to do anything special to support this. The <code>fence()</code> method has a global effect on order: the one in <code>put()</code> separates everything that came before (including the calling code and our write to the entry array)  from the point we update the tail and everything thereafter. Similarly, the one in <code>poll()</code> separates everything that came before (including when we check the tail index) from everything that comes afterward, including when we read the entry from the entry array <em>and</em> everything the caller does thereafter).</p>

<p>But we don‚Äôt want a full <code>fence()</code> here; can we come up with something more flexible? Remember, the more freedom a CPU has to reorder the code, the faster it runs!</p>

<p>Well, here‚Äôs an idea: <code>fence()</code> is bidirectional. It means ‚Äúfinish everything that came before this before starting anything that comes after this.‚Äù But isn‚Äôt bidirectionality more than we need? For a release, we only care that stuff <em>before</em> the release has completed; it‚Äôs totally fine if the CPU works ahead on something else! For an acquire, we only need the acquire to block stuff that comes <em>afterward</em>; it‚Äôs fine if stuff <em>before</em> gets delayed!</p>

<p>So, instead of two full fences, how about two half fences? Let‚Äôs try these two rules on for size:</p>

<ul>
  <li>A <strong>write-release</strong> guarantees that all <strong>preceding</strong> code completes <strong>before</strong> the releasing write</li>
  <li>A <strong>read-acquire</strong> guarantees that all <strong>following</strong> code starts <strong>after</strong> the acquiring read</li>
</ul>

<p>Together, these two rules are called <strong>acquire and release semantics</strong>. They work a little differently than fences we‚Äôve been describing so far. Instead of standalone CPU instructions, we think of acquire and release semantics as requirements you can attach to a memory operation. When you‚Äôre releasing ownership of memory by writing to some variable, you can tag that write as needing <em>write-release semantics</em> to ensure all preceding code finishes before the write finally happens in memory. Similarly, if you‚Äôre trying to acquire ownership of memory by doing something that involves a read, you can tag that read as needing <em>read-acquire semantics</em> to ensure the code that follows the read does not start until you have actually read the value from memory.</p>

<p>That‚Äôs ‚Ä¶ really about all there is to it! Now you know what acquire and release semantics are! Not so bad once you get over your old ideas about memory, eh?</p>

<p>Here‚Äôs a final example implementation of our old friend, the lock-free single-producer single-consumer ring queue, now using acquire and release semantics. I‚Äôve also switched over to the full C++ <a href="https://en.cppreference.com/w/c/atomic">stdatomic</a> API to show what ‚Äòtagging‚Äô memory operations with ordering semantics looks like in practice:</p>

<div><div><pre><code><span>class</span> <span>toy_queue</span> <span>{</span>
  
  <span>unsigned</span> <span>head</span><span>;</span>
  <span>atomic_uint</span> <span>tail</span><span>;</span>
  <span>unsigned</span> <span>size</span><span>;</span>
  <span>void</span> <span>**</span><span>entries</span><span>;</span>
  
<span>public:</span>
  
  <span>void</span> <span>init</span><span>()</span> <span>{</span>
    <span>head</span> <span>=</span> <span>0</span><span>;</span>
    <span>tail</span><span>.</span><span>store</span><span>(</span><span>0</span><span>);</span>
    <span>memset</span><span>(</span><span>entries</span><span>,</span> <span>0</span><span>,</span> <span>size</span> <span>*</span> <span>sizeof</span><span>(</span><span>void</span> <span>*</span><span>));</span>
  <span>}</span>
  
  <span>void</span> <span>put</span><span>(</span><span>void</span> <span>*</span><span>entry</span><span>)</span> <span>{</span>    
    <span>unsigned</span> <span>tl</span> <span>=</span> <span>tail</span><span>.</span><span>load</span><span>(</span><span>memory_order</span><span>::</span><span>relaxed</span><span>);</span>
    <span>unsigned</span> <span>i</span> <span>=</span> <span>tl</span> <span>%</span> <span>size</span><span>;</span>
    <span>entries</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
    <span>tail</span><span>.</span><span>store</span><span>(</span><span>tl</span> <span>+</span> <span>1</span><span>,</span> <span>memory_order</span><span>::</span><span>release</span><span>);</span>
  <span>}</span>
  
  <span>bool</span> <span>poll</span><span>(</span><span>void</span> <span>**</span><span>entry</span><span>)</span> <span>{</span>
    <span>unsigned</span> <span>tl</span> <span>=</span> <span>tail</span><span>.</span><span>load</span><span>(</span><span>memory_order</span><span>::</span><span>acquire</span><span>);</span>
    <span>if</span> <span>(</span><span>tl</span> <span>&gt;</span> <span>head</span><span>)</span> <span>{</span>
      <span>unsigned</span> <span>i</span> <span>=</span> <span>head</span> <span>%</span> <span>size</span><span>;</span>
      <span>*</span><span>entry</span> <span>=</span> <span>entries</span><span>[</span><span>i</span><span>];</span>
      <span>head</span><span>++</span><span>;</span>
      <span>return</span> <span>true</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>return</span> <span>false</span><span>;</span> <span>// empty</span>
    <span>}</span>
  <span>}</span>
  
<span>};</span>
</code></pre></div></div>

<p>I‚Äôve made <code>tail</code> an <code>atomic&lt;&gt;</code> type, and tagged each access with one of three semantics:</p>

<ul>
  <li><code>acquire</code>: This is a read-acquire; block upcoming operations until this is done (but allow preceding operations to be delayed if it‚Äôs convenient).</li>
  <li><code>release</code>: This is a write-release: wait for all preceding operations to complete before doing this (but work ahead on stuff following this if it‚Äôs convenient).</li>
  <li><code>relaxed</code>: No ordering guarantees, do this any time it‚Äôs convenient.</li>
</ul>

<p>We use the <code>release</code> semantic when bumping the tail, since that‚Äôs how the current thread releases its ownership of the item it just enqueued. Past this point, the consumer can pick up the item off the queue at any time, so we need to be certain this thread isn‚Äôt still running code that might be using the shared memory, in this method or in the calling context.</p>

<p>Similarly, we use the <code>acquire</code> semantic when reading the tail in <code>poll()</code>, since that‚Äôs how we know ownership of an entry has passed to us. We need to be certain this thread isn‚Äôt already running code on the shared memory until we‚Äôve observed the producer‚Äôs releasing write.</p>

<p>Finally, I should note the <code>relaxed</code> read in <code>put()</code> cannot cross the write-release at the end of that method. Acquire-release semantics impose order on relaxed memory operations the same as they do for regular variable reads and writes. In fact, you can think of ‚Äònormal‚Äô variable reads and writes as ‚Äòhaving relaxed semantics.‚Äô</p>

<p>And there you have it ‚Ä¶ memory order, ownership, and acquire and release semantics!</p>

<h2 id="parting-notes">Parting Notes</h2>

<p>Here are a few more things you might want to know. You can consider these areas you might want to do some further research to broaden and deepen your knowledge of what we talked about today:</p>

<p><strong>It isn‚Äôt just the CPU that messes with your code; you‚Äôre compiler‚Äôs in on it too.</strong> When you turn on optimizations, e.g. by passing one of the <code>-O</code> flags to gcc, the compiler will do all sorts of <em>fun</em> stuff to your code in the name of performance, using magic as black as what CPUs do at runtime. Luckily, you usually don‚Äôt have to worry about this, because compilers also understand memory ordering semantics like acquire and release: if you tag your code with a memory ordering guarantee, both your compiler and your CPU will honor it. You usually don‚Äôt do anything special about compile-time reordering.</p>

<p><strong>The <code>fence()</code> method is kind of an imaginary construction.</strong> Using the stdatomic library, the closest you can get to the fence semantics we described in this post is to use an ‚Äúacquire-release‚Äù fence, meaning it has both the semantics of an acquire (preventing upcoming operations from being started before the fence) and a release (preventing preceding operations from being delayed past the fence). You might implement <code>fence()</code> like this:</p>

<div><div><pre><code><span>using</span> <span>namespace</span> <span>std</span><span>;</span>
<span>void</span> <span>fence</span><span>()</span> <span>{</span>
  <span>atomic_thread_fence</span><span>(</span><span>memory_order</span><span>::</span><span>acq_rel</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>However, this is only an approximation of what we were talking about; it actually does something <a href="https://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/">more subtle</a> than what I described. In general, use the <code>atomic_thread_fence</code> method with caution, as it‚Äôs unintuitive.</p>

<p><strong>There are even stronger ordering guarantees than anything we‚Äôve talked about so far.</strong> The acquire and release semantics we‚Äôve discussed in this post <em>only</em> affect the relative order instructions take effect in memory. They <em>don‚Äôt</em> require the instructions to take effect right away! So for example, if you tag a write as a write-release, then you guarantee all pending memory writes in your program <em>will eventually</em> complete before the releasing write <em>eventually</em> completes, but there‚Äôs no guarantee that any of this happens before the CPU moves on to the next instruction. All of your writes could still be <a href="https://en.wikipedia.org/wiki/Write_buffer">buffered inside your CPU</a>!</p>

<p>Occasionally you need a guarantee that your writes really are in memory before the next line of code executes. A famous example of this involves the two ‚Äòinterested‚Äô flags in  <a href="https://en.wikipedia.org/wiki/Peterson%27s_algorithm">Peterson‚Äôs lock algorithm</a>. To support code that needs these kinds of immediacy guarantees, the stdatomic API offers <em>sequentially consistent</em> semantics, which can be tagged using <code>seq_cst</code>. As you might have guessed, <code>seq_cst</code> is even slower than <code>acq_rel</code>.</p>

<p>Sequential consistency is about the closest you‚Äôll ever get to linearizable memory in practice. Remember how, at the start of this post, I mentioned underappreciating the heavy lifting Java‚Äôs <code>volatile</code> keyword had been doing for me? <a href="https://bartoszmilewski.com/2008/11/11/who-ordered-sequential-consistency/">Java‚Äôs <code>volatile</code> keyword offers sequential consistency</a> for all reads and writes to all variables you mark volatile. You will have a hard time finding any of this acquire or release stuff in a Java program!</p>

<p><strong>Acquire and release semantics don‚Äôt have any meaning on Intel- and AMD-brand processors.</strong> On x86 CPUs, which is to say, on just about any Intel- or AMD-brand CPU you can buy right now, memory operations on a single CPU core happen in program order. These CPUs <em>don‚Äôt</em> play the reordering tricks we‚Äôve been worried about all along, and our original ‚Äúbroken‚Äù queue actually would have worked on these CPUs without fences or acquire-release semantics or anything (so long as the compiler didn‚Äôt play any tricks of its own). However, these CPUs do buffer stores, so they‚Äôre not sequentially consistent unless you use fence instructions such as <code>MFENCE</code>.</p>

<p>This puts us in a kind of funny situation: according to the stdatomic docs, and the C++ spec, our original queue is technically wrong, but on some pretty popular CPUs, it‚Äôll work just fine anyways. But the first time you try to run your code on a different kind of CPU, well, you should try putting your ear against their computer‚Äôs case so you can hear all the looney tunes ‚Äúbang boom pow‚Äù sounds coming from their CPU as it executes your code. It‚Äôs a good idea not to assume code that uses acquire/release semantics is correct unless you‚Äôve tried running it on a CPU architecture that does more aggressive reordering, such as ARM or PowerPC.</p>

<p>With that, in terms of acquire and release semantics ‚Ä¶</p>

<p><img src="https://davekilian.com/assets/thats-about-it.jpeg" alt=""/></p>

<p>Hopefully that was helpful and I wasn‚Äôt too boring. See ya next time!</p>

    </div></div>
  </body>
</html>
