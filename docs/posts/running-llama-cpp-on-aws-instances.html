<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp/discussions/4225">Original</a>
    <h1>Running Llama.cpp on AWS Instances</h1>
    
    <div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="5892773" data-target-translation-type="discussion">
        <tr>
    <td>
        
<p dir="auto">The <code>llama.cpp</code> project offers unique ways of utilizing cloud computing resources. Here we will demonstrate how to deploy a <code>llama.cpp</code> server on a AWS instance for serving quantum and full-precision F16 models to multiple clients efficiently.</p>
<h2 dir="auto">Select an instance</h2>
<ul dir="auto">
<li>
<p dir="auto">Go to AWS instance listings: <a href="https://aws.amazon.com/ec2/pricing/on-demand/" rel="nofollow">https://aws.amazon.com/ec2/pricing/on-demand/</a></p>
</li>
<li>
<p dir="auto">Sort by price and find the cheapest one with NVIDIA GPU:</p>
</li>
<li>
<p dir="auto">Check the specs:</p>
</li>
<li>
<p dir="auto">The <code>g4dn.xlarge</code> instance has 1x T4 Tensor Core GPU with 16GB VRAM. Here are the NVIDIA specs for convenience:</p>
<details>
  <summary>Click me</summary>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1991296/285655937-2e4e9ba5-9c46-427f-9cd3-80c5ed4069c7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDExNzAyMzAsIm5iZiI6MTcwMTE2OTkzMCwicGF0aCI6Ii8xOTkxMjk2LzI4NTY1NTkzNy0yZTRlOWJhNS05YzQ2LTQyN2YtOWNkMy04MGM1ZWQ0MDY5YzcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTEyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzExMjhUMTExMjEwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzYwOWRkMjJjOTYyOTIzMzQ0NGUyYzM3ZWZmOGQxZmI0OTY5NDIxM2YyNjA2MzM4MGVlMmFlMGIzN2NlMDViOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.X_FDN_ZRmnwOOghEYSODvi1uySec_g-9ma8Ck225-LY"><img src="https://private-user-images.githubusercontent.com/1991296/285655937-2e4e9ba5-9c46-427f-9cd3-80c5ed4069c7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDExNzAyMzAsIm5iZiI6MTcwMTE2OTkzMCwicGF0aCI6Ii8xOTkxMjk2LzI4NTY1NTkzNy0yZTRlOWJhNS05YzQ2LTQyN2YtOWNkMy04MGM1ZWQ0MDY5YzcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTEyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzExMjhUMTExMjEwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzYwOWRkMjJjOTYyOTIzMzQ0NGUyYzM3ZWZmOGQxZmI0OTY5NDIxM2YyNjA2MzM4MGVlMmFlMGIzN2NlMDViOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.X_FDN_ZRmnwOOghEYSODvi1uySec_g-9ma8Ck225-LY" alt="image"/></a></p>
</details>
</li>
<li>
<p dir="auto">Start the instance and login over SSH</p>
</li>
<li>
<p dir="auto">Also, make sure to enable inbound connections to port 8888 - we will need it later for the HTTP server</p>
</li>
</ul>
<h3 dir="auto">Select a model and prepare <code>llama.cpp</code></h3>
<p dir="auto">We have just 16GB VRAM to work with, so we likely want to choose a 7B model. Lately, the <a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="nofollow">OpenHermes-2.5-Mistral-7B</a> model is getting some traction so let&#39;s go with it.</p>
<p dir="auto">We will clone the latest <code>llama.cpp</code> repo, download the model and convert it to GGUF format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# get the model data
git lfs install
git clone https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B

# clone llama.cpp and setup python conversion stuff
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
pip install -r requirements.txt
ln -sfn ../OpenHermes-2.5-Mistral-7B ./models/openhermes-7b-v2.5

# convert to F16 GGUF
python3 convert.py ./models/openhermes-7b-v2.5 --outfile ./models/openhermes-7b-v2.5/ggml-model-f16.gguf --outtype f16

# quantize to Q8_0 and Q4_K
./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf q8_0
./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf q4_k"><pre><span><span>#</span> get the model data</span>
git lfs install
git clone https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B

<span><span>#</span> clone llama.cpp and setup python conversion stuff</span>
git clone https://github.com/ggerganov/llama.cpp
<span>cd</span> llama.cpp
pip install -r requirements.txt
ln -sfn ../OpenHermes-2.5-Mistral-7B ./models/openhermes-7b-v2.5

<span><span>#</span> convert to F16 GGUF</span>
python3 convert.py ./models/openhermes-7b-v2.5 --outfile ./models/openhermes-7b-v2.5/ggml-model-f16.gguf --outtype f16

<span><span>#</span> quantize to Q8_0 and Q4_K</span>
./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf q8_0
./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf q4_k</pre></div>
<h2 dir="auto">Do some performance benchmarks</h2>
<p dir="auto">The T4 GPUs have just 320GB/s memory bandwidth, so we cannot expect huge tok/s numbers, but let&#39;s work with what we have.</p>
<table role="table">
<thead>
<tr>
<th>Max clients</th>
<th>Max prompt</th>
<th>Max Len</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2048</td>
<td>512</td>
</tr>
</tbody>
</table>
<p dir="auto">We assume that at any moment in time, there will be a maximum of 4 queries being processed in parallel. Each query can have a maximum individual prompt of 2048 tokens and each query can generate a maximum of 512 tokens. So in order to support this scenario, we need to have a KV cache of size <code>4*(2048 + 512) = 10240 tokens (1280 MiB, F16)</code>.</p>
<p dir="auto">Let&#39;s benchmark stock <code>llama.cpp</code> using the F16 model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# build the benchmark tool
LLAMA_CUBLAS=1 make -j batched-bench

# bench the F16 model
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 14363.04 MiB (model: 13563.03 MiB, context: 800.00 MiB)

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|  2048 |    128 |    1 |   2176 |    1.914 |  1070.20 |    8.108 |    15.79 |   10.022 |   217.13 |
|  2048 |    512 |    1 |   2560 |    1.828 |  1120.61 |   32.709 |    15.65 |   34.537 |    74.12 |"><pre><span><span>#</span> build the benchmark tool</span>
LLAMA_CUBLAS=1 make -j batched-bench

<span><span>#</span> bench the F16 model</span>
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 14363.04 MiB (model: 13563.03 MiB, context: 800.00 MiB)

<span>|</span>    PP <span>|</span>     TG <span>|</span>    B <span>|</span>   N_KV <span>|</span>   T_PP s <span>|</span> S_PP t/s <span>|</span>   T_TG s <span>|</span> S_TG t/s <span>|</span>      T s <span>|</span>    S t/s <span>|</span>
<span>|</span>-------<span>|</span>--------<span>|</span>------<span>|</span>--------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    1 <span>|</span>   2176 <span>|</span>    1.914 <span>|</span>  1070.20 <span>|</span>    8.108 <span>|</span>    15.79 <span>|</span>   10.022 <span>|</span>   217.13 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    1 <span>|</span>   2560 <span>|</span>    1.828 <span>|</span>  1120.61 <span>|</span>   32.709 <span>|</span>    15.65 <span>|</span>   34.537 <span>|</span>    74.12 <span>|</span></pre></div>
<details>
  <summary>Legend</summary>
<ul dir="auto">
<li>PP: prompt size in tokens</li>
<li>TG: text to generate in tokens</li>
<li>B: number of batches (i.e. parallel requests)</li>
<li>N_KV: required size of the KV cache in tokens</li>
<li>T_PP: time to process the prompts</li>
<li>S_PP: speed of processing the prompts in tok/s</li>
<li>T_TG: time to generate the response</li>
<li>S_TG: speed of the generation in tok/s</li>
<li>T: total time to process the batches (i.e. requests)</li>
<li>S: total speed (including prompt and text) in tok/s</li>
</ul>
</details>
<p dir="auto">We immediately notice that there is not enough VRAM to load both the F16 model and the 10240 tokens KV cache. This means the maximum clients we can serve in this case is just 1. The TG speed is also not great as we expected: ~16 t/s.</p>
<p dir="auto">Let&#39;s for a moment relax the requirements and say that the max prompt size would be <code>512</code> instead of <code>2048</code>. This scenario now fits in the available VRAM and here are the results:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# bench the F16 model using small prompt size of 512
LLAMA_CUBLAS=1 make -j batched-bench &amp;&amp; ./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 512 128,512 1,2,3,4

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|   512 |    128 |    1 |    640 |    0.398 |  1288.00 |    7.589 |    16.87 |    7.987 |    80.13 |
|   512 |    128 |    2 |   1280 |    0.798 |  1283.52 |    8.554 |    29.93 |    9.352 |   136.87 |
|   512 |    128 |    3 |   1920 |    1.298 |  1183.32 |    9.053 |    42.42 |   10.351 |   185.49 |
|   512 |    128 |    4 |   2560 |    1.838 |  1114.15 |    9.277 |    55.19 |   11.115 |   230.33 |
|   512 |    512 |    1 |   1024 |    0.373 |  1372.17 |   30.693 |    16.68 |   31.066 |    32.96 |
|   512 |    512 |    2 |   2048 |    0.784 |  1306.16 |   34.753 |    29.47 |   35.537 |    57.63 |
|   512 |    512 |    3 |   3072 |    1.274 |  1205.64 |   37.243 |    41.24 |   38.517 |    79.76 |
|   512 |    512 |    4 |   4096 |    1.846 |  1109.59 |   38.492 |    53.21 |   40.338 |   101.54 |"><pre><span><span>#</span> bench the F16 model using small prompt size of 512</span>
LLAMA_CUBLAS=1 make -j batched-bench <span>&amp;&amp;</span> ./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 512 128,512 1,2,3,4

<span>|</span>    PP <span>|</span>     TG <span>|</span>    B <span>|</span>   N_KV <span>|</span>   T_PP s <span>|</span> S_PP t/s <span>|</span>   T_TG s <span>|</span> S_TG t/s <span>|</span>      T s <span>|</span>    S t/s <span>|</span>
<span>|</span>-------<span>|</span>--------<span>|</span>------<span>|</span>--------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>
<span>|</span>   512 <span>|</span>    128 <span>|</span>    1 <span>|</span>    640 <span>|</span>    0.398 <span>|</span>  1288.00 <span>|</span>    7.589 <span>|</span>    16.87 <span>|</span>    7.987 <span>|</span>    80.13 <span>|</span>
<span>|</span>   512 <span>|</span>    128 <span>|</span>    2 <span>|</span>   1280 <span>|</span>    0.798 <span>|</span>  1283.52 <span>|</span>    8.554 <span>|</span>    29.93 <span>|</span>    9.352 <span>|</span>   136.87 <span>|</span>
<span>|</span>   512 <span>|</span>    128 <span>|</span>    3 <span>|</span>   1920 <span>|</span>    1.298 <span>|</span>  1183.32 <span>|</span>    9.053 <span>|</span>    42.42 <span>|</span>   10.351 <span>|</span>   185.49 <span>|</span>
<span>|</span>   512 <span>|</span>    128 <span>|</span>    4 <span>|</span>   2560 <span>|</span>    1.838 <span>|</span>  1114.15 <span>|</span>    9.277 <span>|</span>    55.19 <span>|</span>   11.115 <span>|</span>   230.33 <span>|</span>
<span>|</span>   512 <span>|</span>    512 <span>|</span>    1 <span>|</span>   1024 <span>|</span>    0.373 <span>|</span>  1372.17 <span>|</span>   30.693 <span>|</span>    16.68 <span>|</span>   31.066 <span>|</span>    32.96 <span>|</span>
<span>|</span>   512 <span>|</span>    512 <span>|</span>    2 <span>|</span>   2048 <span>|</span>    0.784 <span>|</span>  1306.16 <span>|</span>   34.753 <span>|</span>    29.47 <span>|</span>   35.537 <span>|</span>    57.63 <span>|</span>
<span>|</span>   512 <span>|</span>    512 <span>|</span>    3 <span>|</span>   3072 <span>|</span>    1.274 <span>|</span>  1205.64 <span>|</span>   37.243 <span>|</span>    41.24 <span>|</span>   38.517 <span>|</span>    79.76 <span>|</span>
<span>|</span>   512 <span>|</span>    512 <span>|</span>    4 <span>|</span>   4096 <span>|</span>    1.846 <span>|</span>  1109.59 <span>|</span>   38.492 <span>|</span>    53.21 <span>|</span>   40.338 <span>|</span>   101.54 <span>|</span></pre></div>
<p dir="auto"><code>llama.cpp</code> supports efficient quantization formats. By using a quantum model, we can reduce the base VRAM required to store the model in memory and thus free some VRAM for a bigger KV cache. This will allow us to serve more clients with the original prompt size of <code>2048</code> tokens. Let&#39;s repeat the same benchmark using <code>Q8_0</code> and <code>Q4_K</code> quantum models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# bench the Q8_0 model
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf 10240 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 9169.84 MiB (model: 7205.84 MiB, context: 1964.00 MiB)

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|  2048 |    128 |    1 |   2176 |    2.596 |   788.77 |    4.974 |    25.73 |    7.570 |   287.44 |
|  2048 |    128 |    2 |   4352 |    6.247 |   655.65 |    7.614 |    33.62 |   13.861 |   313.97 |
|  2048 |    128 |    3 |   6528 |   11.200 |   548.56 |    9.499 |    40.43 |   20.699 |   315.38 |
|  2048 |    128 |    4 |   8704 |   17.268 |   474.42 |   10.880 |    47.06 |   28.148 |   309.23 |
|  2048 |    512 |    1 |   2560 |    2.514 |   814.65 |   20.158 |    25.40 |   22.672 |   112.92 |
|  2048 |    512 |    2 |   5120 |    6.221 |   658.39 |   31.274 |    32.74 |   37.495 |   136.55 |
|  2048 |    512 |    3 |   7680 |   11.136 |   551.71 |   39.606 |    38.78 |   50.743 |   151.35 |
|  2048 |    512 |    4 |  10240 |   17.295 |   473.65 |   46.026 |    44.50 |   63.321 |   161.72 |

# bench the Q4_K model
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf 10240 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 6059.07 MiB (model: 4095.06 MiB, context: 1964.00 MiB)

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|  2048 |    128 |    1 |   2176 |    2.409 |   849.99 |    3.938 |    32.50 |    6.348 |   342.79 |
|  2048 |    128 |    2 |   4352 |    5.864 |   698.52 |    6.554 |    39.06 |   12.417 |   350.48 |
|  2048 |    128 |    3 |   6528 |   10.635 |   577.70 |    8.433 |    45.53 |   19.068 |   342.35 |
|  2048 |    128 |    4 |   8704 |   16.522 |   495.82 |    9.795 |    52.27 |   26.317 |   330.74 |
|  2048 |    512 |    1 |   2560 |    2.301 |   890.02 |   16.083 |    31.83 |   18.384 |   139.25 |
|  2048 |    512 |    2 |   5120 |    5.804 |   705.69 |   27.014 |    37.91 |   32.818 |   156.01 |
|  2048 |    512 |    3 |   7680 |   10.540 |   582.93 |   35.224 |    43.61 |   45.764 |   167.82 |
|  2048 |    512 |    4 |  10240 |   16.542 |   495.22 |   41.611 |    49.22 |   58.153 |   176.09 |"><pre><span><span>#</span> bench the Q8_0 model</span>
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf 10240 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 9169.84 MiB (model: 7205.84 MiB, context: 1964.00 MiB)

<span>|</span>    PP <span>|</span>     TG <span>|</span>    B <span>|</span>   N_KV <span>|</span>   T_PP s <span>|</span> S_PP t/s <span>|</span>   T_TG s <span>|</span> S_TG t/s <span>|</span>      T s <span>|</span>    S t/s <span>|</span>
<span>|</span>-------<span>|</span>--------<span>|</span>------<span>|</span>--------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    1 <span>|</span>   2176 <span>|</span>    2.596 <span>|</span>   788.77 <span>|</span>    4.974 <span>|</span>    25.73 <span>|</span>    7.570 <span>|</span>   287.44 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    2 <span>|</span>   4352 <span>|</span>    6.247 <span>|</span>   655.65 <span>|</span>    7.614 <span>|</span>    33.62 <span>|</span>   13.861 <span>|</span>   313.97 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    3 <span>|</span>   6528 <span>|</span>   11.200 <span>|</span>   548.56 <span>|</span>    9.499 <span>|</span>    40.43 <span>|</span>   20.699 <span>|</span>   315.38 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    4 <span>|</span>   8704 <span>|</span>   17.268 <span>|</span>   474.42 <span>|</span>   10.880 <span>|</span>    47.06 <span>|</span>   28.148 <span>|</span>   309.23 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    1 <span>|</span>   2560 <span>|</span>    2.514 <span>|</span>   814.65 <span>|</span>   20.158 <span>|</span>    25.40 <span>|</span>   22.672 <span>|</span>   112.92 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    2 <span>|</span>   5120 <span>|</span>    6.221 <span>|</span>   658.39 <span>|</span>   31.274 <span>|</span>    32.74 <span>|</span>   37.495 <span>|</span>   136.55 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    3 <span>|</span>   7680 <span>|</span>   11.136 <span>|</span>   551.71 <span>|</span>   39.606 <span>|</span>    38.78 <span>|</span>   50.743 <span>|</span>   151.35 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    4 <span>|</span>  10240 <span>|</span>   17.295 <span>|</span>   473.65 <span>|</span>   46.026 <span>|</span>    44.50 <span>|</span>   63.321 <span>|</span>   161.72 <span>|</span>

<span><span>#</span> bench the Q4_K model</span>
./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf 10240 0 99 0 2048 128,512 1,2,3,4

llama_new_context_with_model: total VRAM used: 6059.07 MiB (model: 4095.06 MiB, context: 1964.00 MiB)

<span>|</span>    PP <span>|</span>     TG <span>|</span>    B <span>|</span>   N_KV <span>|</span>   T_PP s <span>|</span> S_PP t/s <span>|</span>   T_TG s <span>|</span> S_TG t/s <span>|</span>      T s <span>|</span>    S t/s <span>|</span>
<span>|</span>-------<span>|</span>--------<span>|</span>------<span>|</span>--------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>----------<span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    1 <span>|</span>   2176 <span>|</span>    2.409 <span>|</span>   849.99 <span>|</span>    3.938 <span>|</span>    32.50 <span>|</span>    6.348 <span>|</span>   342.79 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    2 <span>|</span>   4352 <span>|</span>    5.864 <span>|</span>   698.52 <span>|</span>    6.554 <span>|</span>    39.06 <span>|</span>   12.417 <span>|</span>   350.48 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    3 <span>|</span>   6528 <span>|</span>   10.635 <span>|</span>   577.70 <span>|</span>    8.433 <span>|</span>    45.53 <span>|</span>   19.068 <span>|</span>   342.35 <span>|</span>
<span>|</span>  2048 <span>|</span>    128 <span>|</span>    4 <span>|</span>   8704 <span>|</span>   16.522 <span>|</span>   495.82 <span>|</span>    9.795 <span>|</span>    52.27 <span>|</span>   26.317 <span>|</span>   330.74 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    1 <span>|</span>   2560 <span>|</span>    2.301 <span>|</span>   890.02 <span>|</span>   16.083 <span>|</span>    31.83 <span>|</span>   18.384 <span>|</span>   139.25 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    2 <span>|</span>   5120 <span>|</span>    5.804 <span>|</span>   705.69 <span>|</span>   27.014 <span>|</span>    37.91 <span>|</span>   32.818 <span>|</span>   156.01 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    3 <span>|</span>   7680 <span>|</span>   10.540 <span>|</span>   582.93 <span>|</span>   35.224 <span>|</span>    43.61 <span>|</span>   45.764 <span>|</span>   167.82 <span>|</span>
<span>|</span>  2048 <span>|</span>    512 <span>|</span>    4 <span>|</span>  10240 <span>|</span>   16.542 <span>|</span>   495.22 <span>|</span>   41.611 <span>|</span>    49.22 <span>|</span>   58.153 <span>|</span>   176.09 <span>|</span></pre></div>
<p dir="auto">Using the quantum models and a KV cache of size <code>4*(2048 + 512) == 10240</code> we can now successfully serve 4 clients in parallel and have plenty of VRAM left. The prompt processing speed is not as good as F16, but the text generation is better or similar.</p>
<p dir="auto">Note that <code>llama.cpp</code> supports continuous batching and sharing a common prompt. A sample implementation is demonstrated in the <a href="https://github.com/ggerganov/llama.cpp/pull/3228" data-hovercard-type="pull_request" data-hovercard-url="/ggerganov/llama.cpp/pull/3228/hovercard">parallel.cpp</a> example. Here is a sample run with the <code>Q4_K</code> quantum model, simulating 4 clients in parallel, asking short questions with a shared assistant prompt of 300 tokens, for a total of 64 requests:</p>
<div dir="auto" data-snippet-clipboard-copy-content="LLAMA_CUBLAS=1 make -j parallel &amp;&amp; ./parallel -m ./models/openhermes-7b-v2.5/ggml-model-f16.gguf -n -1 -c 4096 --cont_batching --parallel 4 --sequences 64 --n-gpu-layers 99 -s 1"><pre>LLAMA_CUBLAS=1 make -j parallel <span>&amp;&amp;</span> ./parallel -m ./models/openhermes-7b-v2.5/ggml-model-f16.gguf -n -1 -c 4096 --cont_batching --parallel 4 --sequences 64 --n-gpu-layers 99 -s 1</pre></div>
<details>
  <summary>Results from `parallel`</summary>
<div data-snippet-clipboard-copy-content="llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = mostly Q4_K - Medium
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) 
llm_load_print_meta: general.name   = models
llm_load_print_meta: BOS token = 1 &#39;&lt;s&gt;&#39;
llm_load_print_meta: EOS token = 32000 &#39;&lt;|im_end|&gt;&#39;
llm_load_print_meta: UNK token = 0 &#39;&lt;unk&gt;&#39;
llm_load_print_meta: LF token  = 13 &#39;&lt;0x0A&gt;&#39;
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =   70.42 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 35/35 layers to GPU
llm_load_tensors: VRAM used: 4095.06 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: offloading v cache to GPU
llama_kv_cache_init: offloading k cache to GPU
llama_kv_cache_init: VRAM kv self = 512.00 MiB
llama_new_context_with_model: kv self size  =  512.00 MiB
llama_build_graph: non-view tensors processed: 740/740
llama_new_context_with_model: compute buffer total size = 291.07 MiB
llama_new_context_with_model: VRAM scratch buffer: 288.00 MiB
llama_new_context_with_model: total VRAM used: 4895.07 MiB (model: 4095.06 MiB, context: 800.00 MiB)

No new questions so proceed with build-in defaults.


main: Simulating parallel requests from clients:
main: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299

main: Evaluating the system prompt ...

Processing requests ...

main: clearing the KV cache
Client   0, seq    0, started decoding ...
Client   1, seq    1, started decoding ...
Client   2, seq    2, started decoding ...
Client   3, seq    3, started decoding ...
Client   1, seq   1/ 64, prompt   15 t, response   22 t, time  1.20 s, speed 30.83 t/s, cache miss 0  
Input:    What is the best way to cook a steak?
Response: The best way to cook a steak depends on your personal preference, but here is a general guideline:

Client   1, seq    4, started decoding ...
Client   1, seq   4/ 64, prompt   13 t, response   12 t, time  0.69 s, speed 36.18 t/s, cache miss 0  
Input:    Recommend some interesting books to read.
Response: Here are some books that I recommend for your reading pleasure:

Client   1, seq    5, started decoding ...
Client   3, seq   3/ 64, prompt   22 t, response   56 t, time  3.06 s, speed 25.48 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: I am familiar with the Special Theory of Relativity and I would be happy to explain it to you. The Special Theory of Relativity is a theory proposed by Albert Einstein in 1905 that explains the relationship between space and time. It has two postulates:

...

Client   2, seq  63/ 64, prompt   22 t, response   46 t, time  2.36 s, speed 28.80 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it describes the physical laws that govern objects in motion. The theory is based on two postulates:

Client   3, seq  61/ 64, prompt   22 t, response   96 t, time  4.52 s, speed 26.13 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it is based on two postulates or principles. The first postulate is that the laws of physics are the same for all observers who are moving at a constant velocity relative to each other. The second postulate is that the speed of light in a vacuum is always the same, regardless of the motion of the source of light or the observer.

main: clearing the KV cache

run parameters as at 2023-11-26 14:56:55

main: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299
External prompt file: used built-in defaults
Model and path used:  ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf

Total prompt tokens:    956, speed: 17.48 t/s
Total gen tokens:      3794, speed: 69.39 t/s
Total speed (AVG):           speed: 86.87 t/s
Cache misses:             0


llama_print_timings:        load time =   12698.38 ms
llama_print_timings:      sample time =    1845.35 ms /  3858 runs   (    0.48 ms per token,  2090.66 tokens per second)
llama_print_timings: prompt eval time =   51162.52 ms /  5021 tokens (   10.19 ms per token,    98.14 tokens per second)
llama_print_timings:        eval time =     750.60 ms /    28 runs   (   26.81 ms per token,    37.30 tokens per second)
llama_print_timings:       total time =   54680.08 ms"><pre><code>llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = mostly Q4_K - Medium
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) 
llm_load_print_meta: general.name   = models
llm_load_print_meta: BOS token = 1 &#39;&lt;s&gt;&#39;
llm_load_print_meta: EOS token = 32000 &#39;&lt;|im_end|&gt;&#39;
llm_load_print_meta: UNK token = 0 &#39;&lt;unk&gt;&#39;
llm_load_print_meta: LF token  = 13 &#39;&lt;0x0A&gt;&#39;
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =   70.42 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 35/35 layers to GPU
llm_load_tensors: VRAM used: 4095.06 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: offloading v cache to GPU
llama_kv_cache_init: offloading k cache to GPU
llama_kv_cache_init: VRAM kv self = 512.00 MiB
llama_new_context_with_model: kv self size  =  512.00 MiB
llama_build_graph: non-view tensors processed: 740/740
llama_new_context_with_model: compute buffer total size = 291.07 MiB
llama_new_context_with_model: VRAM scratch buffer: 288.00 MiB
llama_new_context_with_model: total VRAM used: 4895.07 MiB (model: 4095.06 MiB, context: 800.00 MiB)

No new questions so proceed with build-in defaults.


main: Simulating parallel requests from clients:
main: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299

main: Evaluating the system prompt ...

Processing requests ...

main: clearing the KV cache
Client   0, seq    0, started decoding ...
Client   1, seq    1, started decoding ...
Client   2, seq    2, started decoding ...
Client   3, seq    3, started decoding ...
Client   1, seq   1/ 64, prompt   15 t, response   22 t, time  1.20 s, speed 30.83 t/s, cache miss 0  
Input:    What is the best way to cook a steak?
Response: The best way to cook a steak depends on your personal preference, but here is a general guideline:

Client   1, seq    4, started decoding ...
Client   1, seq   4/ 64, prompt   13 t, response   12 t, time  0.69 s, speed 36.18 t/s, cache miss 0  
Input:    Recommend some interesting books to read.
Response: Here are some books that I recommend for your reading pleasure:

Client   1, seq    5, started decoding ...
Client   3, seq   3/ 64, prompt   22 t, response   56 t, time  3.06 s, speed 25.48 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: I am familiar with the Special Theory of Relativity and I would be happy to explain it to you. The Special Theory of Relativity is a theory proposed by Albert Einstein in 1905 that explains the relationship between space and time. It has two postulates:

...

Client   2, seq  63/ 64, prompt   22 t, response   46 t, time  2.36 s, speed 28.80 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it describes the physical laws that govern objects in motion. The theory is based on two postulates:

Client   3, seq  61/ 64, prompt   22 t, response   96 t, time  4.52 s, speed 26.13 t/s, cache miss 0  
Input:    Are you familiar with the Special Theory of Relativity and can you explain it to me?
Response: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it is based on two postulates or principles. The first postulate is that the laws of physics are the same for all observers who are moving at a constant velocity relative to each other. The second postulate is that the speed of light in a vacuum is always the same, regardless of the motion of the source of light or the observer.

main: clearing the KV cache

run parameters as at 2023-11-26 14:56:55

main: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299
External prompt file: used built-in defaults
Model and path used:  ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf

Total prompt tokens:    956, speed: 17.48 t/s
Total gen tokens:      3794, speed: 69.39 t/s
Total speed (AVG):           speed: 86.87 t/s
Cache misses:             0


llama_print_timings:        load time =   12698.38 ms
llama_print_timings:      sample time =    1845.35 ms /  3858 runs   (    0.48 ms per token,  2090.66 tokens per second)
llama_print_timings: prompt eval time =   51162.52 ms /  5021 tokens (   10.19 ms per token,    98.14 tokens per second)
llama_print_timings:        eval time =     750.60 ms /    28 runs   (   26.81 ms per token,    37.30 tokens per second)
llama_print_timings:       total time =   54680.08 ms
</code></pre></div>
</details>
<h2 dir="auto">Running a demo HTTP server</h2>
<p dir="auto">The <code>llama.cpp</code> <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server">server</a> example can be build and started like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# start llama.cpp server, max 4 clients in parallel, prompt size 2048, max seq 512, listen on port 8888
LLAMA_CUBLAS=1 make -j server &amp;&amp; ./server -m models/openhermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512

# send a completion request via curl
curl -s http://XXX.XXX.XXX.XXX:8888/v1/chat/completions \
    -H &#34;Content-Type: application/json&#34; \
    -H &#34;Authorization: Bearer no-key&#34; \
    -d &#39;{
        &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,
        &#34;messages&#34;: [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: &#34;You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.&#34;
            },
            {
                &#34;role&#34;: &#34;user&#34;,
                &#34;content&#34;: &#34;Write a limerick about python exceptions&#34;
            }
        ]
    }&#39; | jq

# ... result:

{
  &#34;choices&#34;: [
    {
      &#34;finish_reason&#34;: &#34;stop&#34;,
      &#34;index&#34;: 0,
      &#34;message&#34;: {
        &#34;content&#34;: &#34;There once was a coder named Sue,\nWho worked with Python, quite true.\nShe&#39;d write her code,\nBut exceptions would load,\nAnd leave her with feelings of askew.&#34;,
        &#34;role&#34;: &#34;assistant&#34;
      }
    }
  ],
  &#34;created&#34;: 1701012712,
  &#34;id&#34;: &#34;chatcmpl-sHBoOZIbYDI3M6vfzWREuNJxRJ0WuBqN&#34;,
  &#34;model&#34;: &#34;gpt-3.5-turbo-0613&#34;,
  &#34;object&#34;: &#34;chat.completion&#34;,
  &#34;usage&#34;: {
    &#34;completion_tokens&#34;: 43,
    &#34;prompt_tokens&#34;: 48,
    &#34;total_tokens&#34;: 91
  }
}"><pre><span><span>#</span> start llama.cpp server, max 4 clients in parallel, prompt size 2048, max seq 512, listen on port 8888</span>
LLAMA_CUBLAS=1 make -j server <span>&amp;&amp;</span> ./server -m models/openhermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512

<span><span>#</span> send a completion request via curl</span>
curl -s http://XXX.XXX.XXX.XXX:8888/v1/chat/completions \
    -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
    -H <span><span>&#34;</span>Authorization: Bearer no-key<span>&#34;</span></span> \
    -d <span><span>&#39;</span>{</span>
<span>        &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,</span>
<span>        &#34;messages&#34;: [</span>
<span>            {</span>
<span>                &#34;role&#34;: &#34;system&#34;,</span>
<span>                &#34;content&#34;: &#34;You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.&#34;</span>
<span>            },</span>
<span>            {</span>
<span>                &#34;role&#34;: &#34;user&#34;,</span>
<span>                &#34;content&#34;: &#34;Write a limerick about python exceptions&#34;</span>
<span>            }</span>
<span>        ]</span>
<span>    }<span>&#39;</span></span> <span>|</span> jq

<span><span>#</span> ... result:</span>

{
  <span><span>&#34;</span>choices<span>&#34;</span></span>: [
    {
      <span><span>&#34;</span>finish_reason<span>&#34;</span></span>: <span><span>&#34;</span>stop<span>&#34;</span></span>,
      <span><span>&#34;</span>index<span>&#34;</span></span>: 0,
      <span><span>&#34;</span>message<span>&#34;</span></span>: {
        <span><span>&#34;</span>content<span>&#34;</span></span>: <span><span>&#34;</span>There once was a coder named Sue,\nWho worked with Python, quite true.\nShe&#39;d write her code,\nBut exceptions would load,\nAnd leave her with feelings of askew.<span>&#34;</span></span>,
        <span><span>&#34;</span>role<span>&#34;</span></span>: <span><span>&#34;</span>assistant<span>&#34;</span></span>
      }
    }
  ],
  <span><span>&#34;</span>created<span>&#34;</span></span>: 1701012712,
  <span><span>&#34;</span>id<span>&#34;</span></span>: <span><span>&#34;</span>chatcmpl-sHBoOZIbYDI3M6vfzWREuNJxRJ0WuBqN<span>&#34;</span></span>,
  <span><span>&#34;</span>model<span>&#34;</span></span>: <span><span>&#34;</span>gpt-3.5-turbo-0613<span>&#34;</span></span>,
  <span><span>&#34;</span>object<span>&#34;</span></span>: <span><span>&#34;</span>chat.completion<span>&#34;</span></span>,
  <span><span>&#34;</span>usage<span>&#34;</span></span>: {
    <span><span>&#34;</span>completion_tokens<span>&#34;</span></span>: 43,
    <span><span>&#34;</span>prompt_tokens<span>&#34;</span></span>: 48,
    <span><span>&#34;</span>total_tokens<span>&#34;</span></span>: 91
  }
}</pre></div>
<p dir="auto">An alternative way for really quick deployment of <code>llama.cpp</code> for demo purposes is to use the <a href="https://github.com/ggerganov/llama.cpp/blob/master/scripts/server-llm.sh">server-llm.sh</a> helper script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash -c &#34;$(curl -s https://ggml.ai/server-llm.sh)&#34;"><pre>bash -c <span><span>&#34;</span><span><span>$(</span>curl -s https://ggml.ai/server-llm.sh<span>)</span></span><span>&#34;</span></span></pre></div>
<p dir="auto">For more info, see: <a data-error-text="Failed to load title" data-id="1970017102" data-permission-text="Title is private" data-url="https://github.com/ggerganov/llama.cpp/issues/3868" data-hovercard-type="pull_request" data-hovercard-url="/ggerganov/llama.cpp/pull/3868/hovercard" href="https://github.com/ggerganov/llama.cpp/pull/3868">#3868</a></p>
<h2 dir="auto">Final notes</h2>
<p dir="auto">This was a short walkthrough of how to setup and bench <code>llama.cpp</code> in the cloud that I hope would be useful for people looking for a simple and efficient LLM solution. There are many details not covered here and one needs to understand some of the intricate details of the <code>llama.cpp</code> and <code>ggml</code> implementations in order to take full advantage of the available compute resources. Knowing when to use a quantum model vs F16 model for example requires understanding of the existing CUDA kernels and their limitations. The code base is still relatively simple and allows to easily customize the implementation according to the specific needs of a project. Such customizations can yield significant performance gains compared to the stock <code>llama.cpp</code> implementation that is available out-of-the-box from <code>master</code>.</p>
    </td>
  </tr>

    </tbody>
  </div></div>
  </body>
</html>
