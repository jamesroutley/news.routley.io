<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ollama.com/blog/multimodal-models">Original</a>
    <h1>Ollama&#39;s new engine for multimodal models</h1>
    
    <div id="readability-page-1" class="page"><div>
      
  <article>
    
    <h2>May 15, 2025</h2>
    <section>
      <p><img src="https://ollama.com/public/blog/multimodal.png" alt="multimodality Ollama"/></p>

<p>Ollama now supports multimodal models via Ollama’s new engine, starting with new vision multimodal models:</p>

<ul>
<li><a href="https://ollama.com/library/llama4">Meta Llama 4</a></li>
<li><a href="https://ollama.com/library/gemma3">Google Gemma 3</a></li>
<li><a href="https://ollama.com/library/qwen2.5vl">Qwen 2.5 VL</a></li>
<li><a href="https://ollama.com/library/mistral-small3.1">Mistral Small 3.1</a></li>
<li>and more <a href="https://ollama.com/search?c=vision">vision models</a>.</li>
</ul>

<h2>General Multimodal Understanding &amp; Reasoning</h2>

<h3>Llama 4 Scout</h3>

<pre><code>ollama run llama4:scout
</code></pre>

<p>(Note: this is a 109 billion parameter, mixture-of-experts model.)</p>

<p>Example: asking location-based questions about a video frame:</p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-min.png" alt="video frame analysis example"/></p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-answer-min.png" alt="video frame analysis example answer"/></p>

<p>You can then ask follow-up questions:</p>

<pre><code>ollama@ollamas-computer ~ % ollama run llama4:scout
&gt;&gt;&gt; what do you see in this image?  /Users/ollama/Downloads/multimodal-example1.png 
Added image &#39;/Users/ollama/Downloads/multimodal-example1.png&#39;
The image depicts a scenic waterfront area with a prominent clock tower at its center. The clock tower, likely the Ferry Building Clock Tower in San Francisco, features a tall structure with a clock face on each side and an American flag flying atop. 

In the foreground, there are several buildings, including what appears to be a marketplace or shopping center, with a parking lot and a street lined with cars. The background showcases a large body of water, possibly a bay or harbor, with a bridge visible in the distance. The sky above is blue with white clouds, suggesting a daytime setting with pleasant weather conditions.

Overall, the image presents a picturesque view of a waterfront area, highlighting the iconic clock tower and the surrounding architecture, as well as the natural beauty of the bay and sky.

&gt;&gt;&gt; how far is this building from the stanford campus in palo alto? 
The distance between the Ferry Building in San Francisco and the Stanford University campus in Palo Alto is approximately 35 miles (56 kilometers). The drive typically takes around 45-60 minutes, depending on traffic conditions.

&gt;&gt;&gt; what&#39;s the best way to get there? 
The best way to get to Stanford University from the Ferry Building in San Francisco depends on your preferences and budget. Here are a few options:

1. **By Car**: Take US-101 South to CA-85 South, then continue on CA-101 South. The journey takes around 45-60 minutes, depending on traffic.

2. **By Train and Shuttle**: Take Caltrain from the San Francisco Station (4th and King Streets) to the Palo Alto Station. From there, you can take the Stanford University shuttle (Red and White) to campus. The total travel time is around 1 hour and 15 minutes.

3. **By Ride-hailing Services**: Uber or Lyft from San Francisco to Stanford University takes around 45-60 minutes, depending on traffic.

Please check for any service disruptions or schedule changes before your trip.
</code></pre>

<h3>Gemma 3</h3>

<pre><code>ollama run gemma3
</code></pre>

<p>Example of giving multiple images and asking about their relations.</p>

<p>This demonstrates how a user can input multiple images at once, or do so via follow up prompts and ask about the images.</p>

<p><img src="https://ollama.com/public/blog/multi-images.png" alt="multiple images"/></p>

<p><img src="https://ollama.com/public/blog/gemma3-answer.png" alt="gemma 3 answers"/></p>

<pre><code>ollama@ollamas-computer ~ % ollama run gemma3
&gt;&gt;&gt; /Users/ollama/Downloads/img1.png /Users/ollama/Downloads/img2.png /Users/ollama/Downloads/img3.png /Users/
... ollama/Downloads/img4.png tell me what animal appears in all 4 images? 

Added image &#39;/Users/ollama/Downloads/img1.png&#39;
Added image &#39;/Users/ollama/Downloads/img2.png&#39;
Added image &#39;/Users/ollama/Downloads/img3.png&#39;
Added image &#39;/Users/ollama/Downloads/img4.png&#39;

The animal featured in all four images is a **llama**.

&gt;&gt;&gt; is there a marine mammal in one of the images? 
Yes! You are absolutely right to spot that. 

In the first image, there’s a **dolphin** (whale) boxing with the llama.

&gt;&gt;&gt; in the boxing image, is the llama more likely to win or the whale? 

That’s a fantastic question! Given the image, the llama is *much* more likely to win. 

Here’s why:

*   **Punching Power:** The llama is clearly throwing a punch with a closed fist, demonstrating intent and force.

*   **Whale’s Reaction:** The whale is clearly recoiling in pain and fear.

It&#39;s a comical mismatch, but the llama has the upper hand in this boxing match!
</code></pre>

<h2>Document scanning</h2>

<h3>Qwen 2.5 VL</h3>

<pre><code>ollama run qwen2.5vl
</code></pre>

<p>Example of using Qwen 2.5 VL for character recognition.</p>

<p><img src="https://ollama.com/public/blog/check-example.jpg" alt="check example"/></p>

<p><img src="https://ollama.com/public/blog/check-example-answer-min.png" alt="check answer"/></p>

<p>Example understanding and translating vertical Chinese spring couplets to English.</p>

<p><img src="https://ollama.com/public/blog/spring-couplets.jpg" alt="spring couplets example"/></p>

<p><img src="https://ollama.com/public/blog/spring-couplets-answer.png" alt="spring couplets answer"/></p>

<h2>Ollama’s new multimodal engine</h2>

<p><img src="https://ollama.com/public/blog/ollama-car.png" alt="Ollama’s new car"/></p>

<p>Ollama has so far relied on the <a href="https://github.com/ggml-org/llama.cpp">ggml-org/llama.cpp</a> project for model support and has instead focused on ease of use and model portability.</p>

<p>As more multimodal models are released by major research labs, the task of supporting these models the way Ollama intends became more and more challenging.</p>

<p>We set out to support a new engine that makes multimodal models first-class citizens, and getting Ollama’s partners to contribute more directly the community - the GGML tensor library.</p>

<p><strong>What does this mean?</strong></p>

<p>To sum it up, this work is to improve the reliability and accuracy of Ollama’s local inference, and to set the foundations for supporting future modalities with more capabilities - i.e. speech, image generation, video generation, longer context sizes, improved tool support for models.</p>

<p>Let’s break down a couple specific areas:</p>

<h4>Model modularity</h4>

<p>Our goal is to confine each model’s “blast radius” to itself—improving reliability and making it easier for creators and developers to integrate new models.</p>

<p>Today, <em>ggml/llama.cpp</em> offers first-class support for text-only models. For multimodal systems, however, the <strong>text decoder</strong> and <strong>vision encoder</strong> are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.</p>

<p>Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained. This isolation lets model creators implement and ship their code without patching multiple files or adding cascading <code>if</code> statements. They no longer need to understand a shared multimodal projection function or worry about breaking other models—they can focus solely on their own model and its training.</p>

<p>Examples of how some models are implemented are available on <a href="https://github.com/ollama/ollama/tree/main/model/models">Ollama’s GitHub repository</a>.</p>

<h4>Accuracy</h4>

<p>Large images produce large number of tokens which may exceed the batch size. Processing this correctly with the right positional information is challenging specifically when a single image crosses boundaries.</p>

<p>Ollama adds metadata as it processes images to help improve accuracy.</p>

<p>Some examples:</p>

<ul>
<li><p>Should causal attention be on / off?</p></li>

<li><p>Is it possible to split the image embeddings into batches for processing, and if possible, what are the boundaries when accounting for quality of output, and the computer being used for inference? If an image is split in the wrong place, the quality of output goes down. This is usually defined by the model, and can be checked in its paper?</p></li>
</ul>

<p>Many other local inference tools implement this differently; while a similar result may be achieved, it does not follow how the models were designed and trained.</p>

<h4>Memory management</h4>

<p><strong>Image caching</strong></p>

<p>Once an image is processed, Ollama caches it so later prompts are faster; the image remains in cache while it is still being used and is not discarded for memory-cleanup limits.</p>

<p><strong>Memory estimation &amp; KV cache optimizations</strong></p>

<p>Ollama collaborates with hardware manufacturers and an operating system partner to make sure the correct hardware metadata is detected for Ollama to better estimate and optimize for memory usage. For many firmware releases, partners will validate/test it against Ollama to minimize regression and to benchmark against new features.</p>

<p>Ollama has some KV cache optimizations to improve how memory can be efficiently used. Ollama configures causal attention at the individual model level instead of configuring as a group.</p>

<p>Examples:</p>

<ul>
<li><p>Google DeepMind’s Gemma 3 leverages sliding window attention, and Ollama can leverage that to allocate a subset or a portion of the model’s context length to improve performance, and because of the memory efficiency, this means we can increase the context length of the model on the same system or use the remaining memory for higher concurrency.</p></li>

<li><p>To uniquely support <strong>Meta’s Llama 4 Scout and Maverick models</strong>, Ollama has implemented chunked attention, attention tuning to support longer context size, specific 2D rotary embedding, and in the mixture-of-experts type of model.</p></li>
</ul>

<p>If a model’s attention layer isn’t fully implemented, such as sliding window attention or chunked attention, it may still <em>‘work’</em>. However, because this isn’t how the model was trained, the end user may begin to see erratic or degraded output by the model itself over time. This becomes especially prominent the longer the context / sequence due to cascading effects.</p>

<h3>What’s next</h3>

<ul>
<li>Support longer context sizes</li>
<li>Support thinking / reasoning</li>
<li>Tool calling with streaming responses</li>
<li>Enabling computer use</li>
</ul>

<h3>Acknowledgements</h3>

<p><strong>Models</strong></p>

<p>Thank you to the teams working to release openly available and powerful vision models - Google DeepMind, Meta Llama, Alibaba Qwen, Mistral, IBM Granite, and the many countless people &amp; labs in the community.</p>

<p><strong>GGML</strong></p>

<p>Thank you to the GGML team for the tensor library that powers Ollama’s inference – accessing GGML directly from Go has given a portable way to design custom inference graphs and tackle harder model architectures not available before in Ollama.</p>

<p><strong>Hardware partners</strong></p>

<p>Thank you to hardware partners for collaborating and helping to improve inference on a large number of devices - NVIDIA, AMD, Qualcomm, Intel, and Microsoft.</p>

    </section>
  </article>

    </div></div>
  </body>
</html>
