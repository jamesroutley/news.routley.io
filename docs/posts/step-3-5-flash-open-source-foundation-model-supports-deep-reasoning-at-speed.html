<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://static.stepfun.com/blog/step-3.5-flash/">Original</a>
    <h1>Step 3.5 Flash – Open-source foundation model, supports deep reasoning at speed</h1>
    
    <div id="readability-page-1" class="page"><div>
        <section id="hero">
            <div>
                
                
                
                <div>
                    <div>
                        <div id="teaserChartArea">
                            
                            <div id="teaserPlotArea">
                                
                                
                                
                                <div id="teaserDataPoints">
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/stepfun-color-logo.svg" alt="Step 3.5 Flash" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;S&lt;/span&gt;&#39;;"/>
    </p>
    <p>81.0</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/glm-logo.svg" alt="GLM-4.7" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;G&lt;/span&gt;&#39;;"/>
    </p>
    <p>78.5</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/deepseek-color-logo.svg" alt="DeepSeek V3.2" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;D&lt;/span&gt;&#39;;"/>
    </p>
    <p>77.3</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/k-only-light.svg" alt="Kimi K2.5" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;K&lt;/span&gt;&#39;;"/>
    </p>
    <p>80.5</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/gemini-color-logo.svg" alt="Gemini 3.0 Pro" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;G&lt;/span&gt;&#39;;"/>
    </p>
    <p>80.7</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/claude-color-logo.svg" alt="Claude Opus 4.5" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;C&lt;/span&gt;&#39;;"/>
    </p>
    <p>80.6</p>
    
</div>
<div>
    <p><img src="https://hamy.xyz/blog/assets/logos/openai-logo.svg" alt="GPT-5.2 xhigh" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;teaser-point-initial\&#39;&gt;G&lt;/span&gt;&#39;;"/>
    </p>
    <p>82.2</p>
    
</div>
</div>
                            </div>
                            <div>
                                <div id="teaserXTicks"><p>200</p><p>400</p><p>600</p><p>800</p><p>1000</p><p>Likely &gt;1000</p></div>
                                <p>Total Model Parameters (B)</p>
                            </div>
                        </div>
                    </div>
                </div>
                <p>Scores represent the mean of the following eight benchmarks listed below, excluding xbench-DeepSearch. The Step 3.5 Flash score is derived under standard settings (i.e., $w/o$ Parallel Thinking).</p>
                <div>
                    <p data-content="hero.abstract"><strong>Step 3.5 Flash</strong> is our most capable open-source foundation model, engineered to deliver frontier reasoning and agentic capabilities with exceptional efficiency. Built on a sparse Mixture of Experts (MoE) architecture, it selectively activates only <strong>11B of its 196B parameters</strong> per token. This &#34;intelligence density&#34; allows it to rival the reasoning depth of top-tier proprietary models, while maintaining the agility required for real-time interaction.</p>
                    <div>
                        <ul data-content="hero.highlights">
                            <li>
                                <strong>Deep Reasoning at Speed:</strong>
                                <span>While chatbots are built for reading, agents must reason fast. Powered by 3-way Multi-Token Prediction (MTP-3), Step 3.5 Flash achieves a generation throughput of 100–300 tok/s in typical usage (peaking at 350 tok/s for single-stream coding tasks). This allows for complex, multi-step reasoning chains with immediate responsiveness.</span>
                            </li>
                            <li>
                                <strong>A Robust Engine for Coding &amp; Agents:</strong>
                                <span>Step 3.5 Flash is purpose-built for agentic tasks, integrating a scalable RL framework that drives consistent self-improvement. It achieves 74.4% on SWE-bench Verified and 51.0% on Terminal-Bench 2.0, proving its ability to handle sophisticated, long-horizon tasks with unwavering stability.</span>
                            </li>
                            <li>
                                <strong>Efficient Long Context:</strong>
                                <span>The model supports a cost-efficient 256K context window by employing a 3:1 Sliding Window Attention (SWA) ratio—integrating three SWA layers for every one full-attention layer. This hybrid approach ensures consistent performance across massive datasets or long codebases while significantly reducing the computational overhead typical of standard long-context models.</span>
                            </li>
                            <li>
                                <strong>Accessible Local Deployment:</strong>
                                <span>Optimized for accessibility, Step 3.5 Flash brings elite-level intelligence to local environments. It runs securely on high-end consumer hardware (e.g., Mac Studio M4 Max, NVIDIA DGX Spark), ensuring data privacy without sacrificing performance.</span>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section id="benchmark">
            <div>
                <div>

                    
                    

                    
                    

                    
                    <div id="agentCategory">
                        <h4>Agent</h4>
                        <div>
                            
                            
                            <div data-benchmark="browsecompZHContext">
    
    <div>
        
        <div><div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/stepfun-dark-logo.png" alt="Step 3.5 Flash" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;S&lt;/span&gt;&#39;;"/></p><p>56.3</p>
                        
                    </div>
                </div>
            </div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/stepfun-dark-logo.png" alt="StepFun Research" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;S&lt;/span&gt;&#39;;"/></p><p>35.0</p>
                        
                    </div>
                </div>
            </div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/k-only-light.svg" alt="Kimi K2.5 (Thinking)" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;K&lt;/span&gt;&#39;;"/></p><p>40.0</p>
                        
                    </div>
                </div>
            </div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/manus.svg" alt="Manus Agent (Quality Mode)" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;M&lt;/span&gt;&#39;;"/></p><p>40.0</p>
                        
                    </div>
                </div>
            </div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/grok-logo.svg" alt="SuperGrok Expert" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;G&lt;/span&gt;&#39;;"/></p><p>40.0</p>
                        
                    </div>
                </div>
            </div>
            <div>
                <div>
                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/logos/openai-logo.svg" alt="ChatGPT-5-Pro" onerror="this.style.display=&#39;none&#39;;this.parentElement.innerHTML+=&#39;&lt;span class=\&#39;bar-initial\&#39;&gt;G&lt;/span&gt;&#39;;"/></p><p>75.0</p>
                        
                    </div>
                </div>
            </div></div>
        </div>
    </div>
                        </div>
                        </div>
                    </div>

                </div>

                <p>Performance of Step 3.5 Flash measured across <strong>Reasoning, Coding, and Agentic Tasks</strong>. Open-source models (left) are sorted by their total parameter count, while top-tier proprietary models are shown on the right. xbench-DeepSearch scores are sourced from <a href="https://xbench.org/agi/aisearch" target="_blank" rel="noopener">official publications</a> for consistency. The shadowed bars represent the enhanced performance of Step 3.5 Flash using <a href="https://arxiv.org/pdf/2601.05593" target="_blank" rel="noopener">Parallel Thinking</a>.</p>

            </div>
        </section>

        <section id="showcase">
            <div>
                <h2>Step 3.5 Flash: Intelligence in Practice</h2>
                <p>True intelligence density is not just about peak performance on conventional benchmarks, but about robustness in dynamic, real-world scenarios. While we value strong results on standard metrics as a foundation, our primary goal is to validate that the model functions as a resilient and effective partner when facing the unpredictability of actual execution.</p>
                <p>In the following part, we consolidate a range of performance feedback from real-world showcases, rigorous internal benchmarks, and supplemental public leaderboards. Covering everything from advanced reasoning in math and coding to everyday interaction capabilities, these results demonstrate that Step 3.5 Flash is not just fast enough to think—it is <strong>Reliable Enough to Act</strong>.</p>

                
                

                
                <div id="showcase-agentic-coding">
                    

                    <div>
                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/Earth.mp4" type="video/mp4"/>
                            </video>
                            <p>Tactical Weather Intelligence Dashboard — A flight-cockpit inspired 3D globe visualizer engineered for high-density data environments. Featuring a custom WebGL 2.0 engine, it manages 15,000+ active nodes with real-time WebSocket telemetry. This case demonstrates our model&#39;s ability to build low-latency data pipelines and high-performance geospatial visualizations with a focus on system stability and professional-grade UI/UX.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/Ocean_v2_4k.mp4" type="video/mp4"/>
                            </video>
                            <p>Three.js Procedural Ocean Engine — A high-performance rendering system featuring fractal-based wave geometry and ray-traced surfaces. It leverages Fresnel reflectance and PBR materials for photorealistic lighting. This showcase highlights our model&#39;s expertise in Computer Graphics (CG), complex rendering pipeline design, and seamless integration of Three.js/GLSL/Shadertoy workflows.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/Agentic_Coding_with_text.mp4" type="video/mp4"/>
                            </video>
                            <p>Agentic Workflow Take In — A case demonstrates how Step assists in executing daily data processes, achieving end-to-end data production. It aligns upstream data formats, accurately calls data generation models, verifies and transforms the results, and generates workflow reports, embodying the core concept of Agent-in-the-loop. Step can effectively take over our daily workflows, undertaking complex and repetitive processes.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/SolarDisplay.mp4" type="video/mp4"/>
                            </video>
                            <p>Epic Solar System Simulation — A 3D interactive model of the solar system with cinematic lighting and atmosphere, presenting a shocking visual narrative from nothingness to a complete galaxy through an epic opening performance of dynamically generated and orbiting planets one by one. This demonstrates Step comprehensive creative ability in 3D scene orchestration, lighting and atmosphere creation, and control of interactive narrative rhythm.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/DataAnaly_en_4k_v2.mp4" type="video/mp4"/>
                            </video>
                            <p>Autonomous Business Intelligence Engine — End-to-end data processing—from CSV ingestion to Cubic Spline interpolation and multi-scenario forecasting. Demonstrates high-order reasoning in multi-step tool use, automated error correction during code execution, and complex data visualization. Successfully modeled a 60% DNU drop scenario, identifying a 1.6x quality gap between acquisition channels. It reflects the model&#39;s agentic strength in systematic problem solving and its ability to act as a self-directed Data Scientist.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/coding_case/Deepresearch_explore.mp4" type="video/mp4"/>
                            </video>
                            <p>Autonomous Large-Scale Repository Architect — A specialized agentic workflow for navigating and deciphering high-complexity codebases. Beyond simple file scanning, the model performs deep-trace logic mapping and cross-module dependency analysis to synthesize the &#34;mental model&#34; of an entire ecosystem. This showcase demonstrates the model&#39;s superior cognitive capacity for large-scale software architecture, enabling it to autonomously generate professional Wikis that connect high-level design patterns to low-level implementation details across thousands of lines of code.</p>
                            
                        </div>
                    </div>

                    
                    <div>
                        <p><strong>Beyond Vibe Coding - Driving Professional Data Agent in Claude Code.</strong> Within advanced agent frameworks like Claude Code, LLMs have evolved beyond &#34;vibe coding&#34; to becoming active problem-solvers capable of driving complex workflows to accomplish sophisticated objectives. To evaluate this in a real-world context, we task Step 3.5 Flash to act as a professional data analyst within the Claude Code environment.</p>
                        <p>We curate a benchmark of 50 end-to-end tasks that reflect the intricate nature of Internet backend data analysis. As shown in the table below, Step 3.5 Flash demonstrates exceptional proficiency in managing these multi-stage processes—independently handling data ingestion, cleaning, feature construction, and results interpretation. With a score of 39.58%, it proves to be a robust engine for sophisticated agentic systems, outperforming several frontier models in analytical accuracy.</p>

                        <div>
                            <div>
                                <h3>Professional Data Analysis Benchmark</h3>
                                
                            </div>
                            <p>We notice that frontier models like Gemini 3.0 Pro didn&#39;t perform as expected in this specific test. This could be due to framework compatibility issues within Claude Code, or simply a difference in analytical capability. Either way, the takeaway here is how well Step 3.5 Flash syncs with the Claude Code, enabling it to handle professional data tasks with solid reliability.</p>
                        </div>
                    </div>
                </div>

                
                <div id="showcase-deep-research">
                    
                    <p>While Step 3.5 Flash is compact, its utility is no longer limited by its internal parametric knowledge. In the agentic era, the ability to leverage the internet as a dynamic knowledge base is more critical than static memory—a strength proven by Step 3.5 Flash&#39;s performance on benchmarks like xbench-DeepSearch and BrowserComp.</p>
                    <p>Deep Research extends basic information retrieval by delegating the entire research workflow to an agentic loop of planning, searching, reflecting, and writing. To evaluate Step 3.5 Flash on this complex process, we use the Scale AI <a href="https://scale.com/research/researchrubrics" target="_blank" rel="noopener">Research Rubrics</a>, a benchmark designed to assess the factual grounding and reasoning depth of long-form research. Our implementation facilitates this through a single-agent loop based on a ReAct architecture, natively integrating specialized tools such as <em>batch_web_surfer</em> and <em>shell</em> for iterative investigations. This approach allows Step 3.5 Flash to achieve a score of 65.27%, delivering research quality that competes with OpenAI and Gemini Deep Research while maintaining significantly higher inference efficiency.</p>

                    
                    <div>
                        <div>
                            <h3>Performance on <span>ResearchRubrics</span></h3>
                            <div>
                                <div>
                                    <p><span>Step 3.5 Flash</span></p>
                                    <p><span>65.3</span>
                                    <span>ReAct Agent</span>
                                </p></div>
                                <div>
                                    <p><span>Gemini DeepResearch</span></p>
                                    <p><span>63.7</span>
                                    <span>Agent System</span>
                                </p></div>
                                
                                <div>
                                    <p><span>OpenAI DeepResearch</span></p>
                                    <p><span>60.7</span>
                                    <span>Agent System</span>
                                </p></div>
                                <div>
                                    <p><span>Qwen DeepResearch</span></p>
                                    <p><span>49.2</span>
                                    <span>Agent System</span>
                                </p></div>
                            </div>
                        </div>
                        <p>We evaluated commercial agents by collecting reports from their official web interfaces (captured Dec 2–15, 2025) under default configurations, while our internal models utilized the ReAct framework for report generation. All outputs were subsequently appraised by an LLM judge using a ternary grading for each criterion.</p>
                    </div>

                    
                    <div>
                        <p>We demonstrate Step 3.5 Flash&#39;s exceptional Deep Research capabilities through a case study on early childhood science education. In this instance, Step 3.5 Flash synthesized a comprehensive research report of approximately 10,000 words, distilling complex neuroplasticity theories into an actionable, expert-grade guide for ages 0–3. The output bridges theoretical milestones with practical &#34;Parental Scripts,&#34; reframing sensory play as structured inquiry while maintaining a rigorous focus on both cognitive depth and safety guidance.</p>
                        

                        <p><strong>Multi-Agent Orchestration Framework.</strong> Step 3.5 Flash also natively supports a multi-agent architecture where a Master Agent orchestrates complex tasks through autonomous planning and dynamic routing. This hierarchical framework dispatches specialized Search and Verify agents to handle retrieval and factual grounding via parallel tool-invocation loops. To ensure precision, a Summary Agent consolidates each sub-agent&#39;s trajectory into structured feedback, enabling the Master Agent to synthesize a final, coherent response.</p>
                        <div data-case-file="raw_case/dr_bmk_multi_agent/case1.json" data-multi-agent="true">
                            <div>
                                
                                <p><span>Multi-Agent Deep Research</span>
                            </p></div>
                            
                        </div>
                    </div>
                </div>

                
                <div id="showcase-edge-cloud">
                    

                    <div>
                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/Edge_cloud_Collaboration/gui/gui-case1.mp4" type="video/mp4"/>
                            </video>
                            <p>In this case, the user asks to search for the latest Arxiv papers on GUI Agents, summarize them, and immediately share the result via WeChat. Step 3.5 Flash, acting as the &#39;Cloud Brain,&#39; first executes the search and summarization in the cloud for maximum speed. Once the content is ready, it triggers the &#39;Hand&#39;—our on-device <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a>—to wake up the phone, open WeChat, and deliver the message to the specific contact. This is Cloud-Device Synergy in action.</p>
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="raw_case/Edge_cloud_Collaboration/gui/gui-case2.mp4" type="video/mp4"/>
                            </video>
                            <p>In this case, the user asks to compare Mac Mini M4 prices across platforms. Step 3.5 Flash, acting as the &#39;Cloud Brain,&#39; decomposes this complex request into specific sub-tasks for Taobao, JD.com, and Pinduoduo. This cloud-side planning significantly lowers the difficulty for the on-device <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a>, ensuring higher success rates as it retrieves real-time data from each app. Step 3.5 Flash then synthesizes the results to identify Pinduoduo as the cheapest option and offers a buying guide. This demonstrates <strong>Cloud-Device Synergy</strong>: cloud intelligence simplifies local execution for reliable results.</p>
                        </div>
                    </div>

                    <p>Furthermore, we conduct a comparative evaluation on the <a href="https://arxiv.org/pdf/2512.15431" target="_blank">AndroidDaily Hard</a> subset, a benchmark tailored for Chinese mobile application scenarios encompassing e-commerce, entertainment, and other daily tasks.</p>

                    <div>
                        <div>
                            <h3>Performance on AndroidDaily Hard</h3>
                            <div>
                                
                                <div>
                                    <p><span>Step 3.5 Flash + Step-GUI</span></p>
                                    <p><span>57</span>
                                    <span>Edge–Cloud</span>
                                </p></div>
                            </div>
                        </div>
                    </div>

                    <p>We compare two paradigms: (1) single-agent <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> executing tasks independently on-device, and (2) an edge-cloud collaborative framework integrating Step 3.5 Flash with <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> via GUI-MCP. The results demonstrate that utilizing Step 3.5 Flash as the cloud-based host agent to orchestrate <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> significantly enhances the system&#39;s performance in complex scenarios.</p>
                </div>

                
                <div id="showcase-math">
                    
                    <p>Step 3.5 Flash demonstrates exceptional logical rigor in competition-level math. Through the deep analysis of IMO Shortlisted problems, the model proves its core strength in complex symbolic reasoning and abstract structural synthesis.</p>

                    <div>
                        
                        <div id="mathCase1">
                            
                            
                            <p>The problem seeks to characterize all real numbers \(\alpha\) such that the sum of the floor functions \(S_n = \sum_{k=1}^n \lfloor k\alpha\rfloor\) is always divisible by \(n\). The primary difficulty lies in the fact that \(\alpha\) is a real number, requiring one to separate its integer part \(m\) and fractional part \(\theta\) to analyze how the summation interacts with the modularity of \(n\). The core insight of the proof is reducing the problem to the behavior of the fractional sum \(T_n = \sum_{k=1}^n \lfloor k\theta\rfloor\) and employing induction to show that the divisibility constraints force extreme values for the floor functions.</p>
                        </div>

                        
                        <div id="mathCase2">
                            
                            
                            <p>The problem asks whether a specific inequality involving the sums of exponential terms \(3^{a_n}\) and \(2^{a_n}\) must hold for at least one \(n\) in any sequence of positive real numbers. The primary difficulty lies in the potentially divergent behavior of the numerator and denominator, which makes it non-obvious whether the ratio ever drops below a fixed constant like \(1/2024\). The core insight of the proof is to perform a change of variables \(x_i = 2^{a_i}\) and identify the power \(\alpha = \log_2 3\), transforming the expression into a ratio of power sums \(\frac{\sum x_i^\alpha}{(\sum x_i)^2}\).</p>
                        </div>
                    </div>
                </div>

                
                <div id="showcase-reliability">
                    

                    <p><strong>We also care about interaction reliability</strong>—the model&#39;s ability to not just solve problems, but to engage users with precision and professional judgment. To test this, we evaluated Step 3.5 Flash across two critical dimensions:</p>

                    <ul>
                        <li><strong>Proactive Intent Clarification</strong>: In our internal benchmark of 74 ambiguous real-world requests (primarily localized queries), Step 3.5 Flash consistently identified missing information and asked targeted questions to clarify user intent rather than making assumptions.</li>
                    </ul>

                    <div>
                        <div>
                            <h3>Proactive Intent Clarification</h3>
                            
                        </div>
                    </div>

                    <ul>
                        <li><strong>Advisory &amp; Consultation</strong>: Across 500 prompts in a balanced bilingual setting spanning life, learning, and workplace contexts, the model demonstrated solid domain knowledge and a professional style, maintaining high instruction-following standards in both English and Chinese.</li>
                    </ul>

                    <div>
                        <table>
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>Average</th>
                                    <th>Usefulness</th>
                                    <th>Logic</th>
                                    <th>Tone</th>
                                    <th>Instruction-following</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>GPT-5.2</td><td>77.8%</td><td>77.2%</td><td>81.9%</td><td>73.0%</td><td>79.6%</td></tr>
                                <tr><td>Gemini 3.0 Pro</td><td>70.6%</td><td>73.9%</td><td>61.7%</td><td>72.3%</td><td>74.4%</td></tr>
                                <tr><td>Step 3.5 Flash</td><td>70.5%</td><td>73.3%</td><td>62.1%</td><td>72.4%</td><td>74.2%</td></tr>
                                <tr><td>Deepseek V3.2</td><td>70.3%</td><td>72.5%</td><td>64.4%</td><td>71.2%</td><td>72.9%</td></tr>
                                <tr><td>Claude Opus 4.5</td><td>68.5%</td><td>69.7%</td><td>66.5%</td><td>65.9%</td><td>72.1%</td></tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </section>

        

        <section id="techniques">
            <div>
                <h2>The Engine Behind</h2>

                <div>
                    <div>
                        <h3 data-content="method.subsection_3_1_title">Architecture Optimized for Flash-Speed Decoding and Inference</h3>

                        <p>The architecture of <strong>Step 3.5 Flash</strong> is defined by a model-system co-design that prioritizes <strong>inference cost and speed</strong> as the core architectural constraint. We employ a <strong>Sparse Mixture-of-Experts (MoE)</strong> backbone to decouple global model capacity from per-token computation. While the total knowledge base spans <strong>196B parameters</strong>, the system only activates <strong>11B parameters per token</strong> during inference. To further reduce <strong>memory overhead</strong>, we strategically utilize dense layers for the first few layers of the network for high intelligence density.</p>

                        <p>To navigate the quadratic bottleneck of long-context processing, we leverage a hybrid attention layout that interleaves <strong>Sliding-Window Attention (SWA)</strong> with <strong>Full Attention</strong> at a 3:1 ratio. We specifically opted for SWA over linear alternatives to maintain the architectural flexibility required for <strong>speculative decoding</strong>. SWA is inherently compatible with <strong>Multi-Token Prediction (MTP)</strong> heads. These heads predict additional future tokens in parallel with the primary output, enabling <strong>parallel verification</strong>. This allows the model to validate multiple token hypotheses in a single pass, effectively breaking the serial constraints of standard autoregressive decoding.</p>

                        <p>To ensure this lightweight hybrid structure retains peak performance, we implemented two critical enhancements. We utilized an <strong>augmented query-head count</strong> in the SWA layers—increasing from 64 to 96—to strengthen representational power without expanding the \(KV\) cache footprint. This modification is highly efficient: since the attention window is fixed, the computational cost of these additional heads remains constant regardless of total sequence length. This allows us to scale up model expressiveness without the &#34;long-context penalty&#34; where attention costs usually explode as the conversation grows. Complementing this is our <strong><a href="https://arxiv.org/abs/2505.06708" target="_blank" rel="noopener">Head-wise Gated Attention</a></strong>, which functions as an input-dependent attention sink. By dynamically modulating information flow, this mechanism preserves numerical stability while incurring negligible overhead.</p>

                        <p>These strategic architectural refinements demonstrate that frontier-level reasoning can be decoupled from prohibitive latency. By integrating <strong>sparse-active execution</strong> with <strong>concurrent token verification</strong>, the model achieves a decoding throughput up to <strong>350 tokens per second (TPS)</strong> on NVIDIA Hopper GPUs while running SWE-bench Verified.</p>

                        <p><strong>Last but not least</strong>, the <strong>optimized total parameter scale</strong> of Step 3.5 Flash facilitates highly accessible, local inference. By consolidating its total capacity to a scale compatible with high-end personal hardware, the model supports high-fidelity private deployment on workstations such as the <strong>Apple M4 Max</strong>, <strong>NVIDIA DGX Spark</strong>, or <strong>AMD AI Max+ 395</strong>, providing a 100% trusted execution environment.</p>

                        <div>
                            <p><img src="https://hamy.xyz/blog/assets/step3.5.svg" alt="Architecture"/></p><p>The overall architecture of Step 3.5 Flash.</p>
                        </div>

                        <p>As the local deployment of large language models (LLMs) becomes increasingly prevalent, we have successfully adapted the Step 3.5 Flash to NVIDIA DGX Spark 128GB device based on the edge-side inference engine llama.cpp, and simultaneously released the INT4 quantized model weights in GGUF format. On NVIDIA DGX Spark, the Step 3.5 Flash achieves a generation speed of 20 tokens per second; by integrating the INT8 quantization technology for KVCache, it supports an extended context window of up to 256K tokens, thus delivering long text processing capabilities on par with cloud-based inference. The new model can be tested by developers on NVIDIA accelerated infrastructure via <a href="https://build.nvidia.com" target="_blank">build.nvidia.com</a>.</p>
                    </div>

                    <div>
                        <h3 data-content="method.subsection_3_2_title">Scalable RL Unleashes the Reasoning Potential</h3>

                        <p>We introduce a scalable reinforcement learning framework designed to reliably train reasoning and agentic language models at scale.</p>

                        <p>Modern RL pipelines for LLMs rely on high-throughput inference engines to generate rollouts, while optimization happens asynchronously in a separate training system. At scale, this setup introduces two compounding challenges:</p>

                        <ol>
                            <li>Training–inference mismatch, caused by numerical and architectural differences between systems</li>
                            <li>Off-policy drift, as policies evolve while rollouts lag behind</li>
                        </ol>

                        <p>For long reasoning sequences, even minor token-level discrepancies can explode into extreme importance weights—leading to unstable updates, early convergence, or complete training collapse.</p>

                        <p>To address this, we propose <strong>Metropolis Independence Sampling Filtered Policy Optimization (MIS-PO)</strong>, which replaces fragile importance weighting with <strong>strict sample filtering</strong>. Instead of scaling gradients with continuous importance-sampling ratios as in PPO, MIS-PO uses these ratios solely as a <strong>binary acceptance criterion</strong>. Trajectories whose likelihood deviates too far between the inference and training policies are simply excluded from optimization, while accepted samples are treated as effectively on-policy. Concretely, the policy update is driven by</p>

                        <p>
                            \[\mathcal{L}_{actor} = - \mathbb{E}_{\tau \sim \pi_{\theta_\text{vllm}}} \left[ \mathbb{I}(\tau) \cdot \log \pi_\theta(a_t|s_t) \cdot \hat{A}_t \right],\]
                        </p>

                        <p>where the binary indicator \(\mathbb{I}(\tau)\) filters out off-distribution samples. This design dramatically reduces gradient variance and enables stable, long-horizon optimization without aggressive clipping.</p>

                        <p>Our framework also includes <strong>truncation-aware value bootstrapping</strong>, which prevents long reasoning trajectories from being incorrectly penalized when hitting context limits, and <strong>routing confidence monitoring</strong> for Mixture-of-Experts models, providing a practical signal for RL stability at scale.</p>

                        <p>Together, these components turn reinforcement learning into a <strong>reliable engine for continuous self-improvement</strong>, enabling consistent gains across mathematics, coding, and tool use, while remaining stable under large-scale, off-policy training.</p>
                    </div>

                    <div>
                        <p><img src="https://hamy.xyz/blog/assets/tb_plots_grad_norm_eb.svg" alt="RL Algorithm Ablation"/></p><p>Training dynamics of different RL algorithms. Ablations are conducted on the Qwen model.</p>
                    </div>
                </div>
            </div>
        </section>

        

        <section id="benchmarks">
            <div>
                <h2>Benchmarks</h2>
                <p>In our benchmark table, we provide a detailed, side-by-side comparison of today&#39;s top-performing open-source models. Across a wide range of metrics, Step 3.5 Flash stands out with consistently strong results. Our evaluation focuses on three core dimensions—Reasoning, Coding and Agentic Capability—and visualizes score differences across peer models in a horizontal, at-a-glance format.</p>

                <div>
                    <table>
                        <thead>
                            <tr>
                                <th>Benchmark</th>
                                <th>Step 3.5 Flash</th>
                                <th>DeepSeek V3.2</th>
				<th>Kimi </th>
                                <th>GLM-4.7</th>
                                <th>MiniMax M2.1</th>
                                <th>MiMo-V2 Flash</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td># Activated Params</td>
                                <td>11B</td>
                                <td>37B</td><td>32B</td><td>32B</td><td>10B</td><td>15B</td>
                            </tr>
                            <tr>
                                <td># Total Params (MoE)</td>
                                <td>196B</td>
                                <td>671B</td><td>1T</td><td>355B</td><td>230B</td><td>309B</td>
                            </tr>
                            <tr>
                                <td>Est. decoding cost</td>
                                <td>1.0x</td>
                                <td>6.0x</td>
                                <td>18.9x</td>
                                <td>18.9x</td>
                                <td>3.9x</td>
                                <td><span>1.2x</span></td>
                            </tr>
                            <tr><td></td><td></td><td></td><td>Agent</td><td></td><td></td><td></td></tr>
                            <tr><td>τ²-Bench</td><td>88.2</td><td>80.3 (85.2*)</td><td><span>74.3*</span><span>/</span><span>85.4*</span></td><td>87.4</td><td>86.6*</td><td>80.3 (84.1*)</td></tr>
			    <tr><td>BrowseComp</td><td>51.6</td><td>51.4</td><td>41.5* / <span>60.6</span></td><td>52.0</td><td>47.4</td><td>45.4</td></tr>

			    <tr><td>BrowseComp</td><td>69.0</td><td>67.6</td><td><span>60.2</span><span>/</span><span>74.9</span></td><td>67.5</td><td>62.0</td><td>58.3</td></tr>

			    <tr><td>BrowseComp-ZH</td><td>66.9</td><td>65.0</td><td>62.3 / 62.3*</td><td>66.6</td><td>47.8*</td><td>51.2*</td></tr>

			    <tr><td>BrowseComp-ZH</td><td>73.7</td><td>—</td><td><span>—</span><span>/</span><span>—</span></td><td>—</td><td>—</td><td>—</td></tr>

			    <tr><td>GAIA</td><td>84.5</td><td>75.1*</td><td><span>75.6*</span><span>/</span><span>75.9*</span></td><td>61.9*</td><td>64.3*</td><td>78.2*</td></tr>

			    <tr><td>xbench-DeepSearch</td><td>83.7</td><td>78.0*</td><td><span>76.0*</span><span>/</span><span>76.7*</span></td><td>72.0*</td><td>68.7*</td><td>69.3*</td></tr>
                            <tr><td>xbench-DeepSearch</td><td>56.3</td><td>55.7*</td><td><span>—</span><span>/</span><span>40+</span></td><td>52.3*</td><td>43.0*</td><td>44.0*</td></tr>
                            <tr><td><span>ResearchRubrics</span></td><td>65.3</td><td>55.8*</td><td><span>56.2*</span><span>/</span><span>59.5*</span></td><td>62.0*</td><td>60.2*</td><td>54.3*</td></tr>
                            <tr><td></td><td></td><td></td><td>Reasoning</td><td></td><td></td><td></td></tr>
                            <tr><td>AIME 2025</td><td>97.3</td><td>93.1</td><td><span>94.5</span><span>/</span><span>96.1</span></td><td>95.7</td><td>83.0</td><td>94.1 (95.1*)</td></tr>
                            <tr><td>HMMT 2025 (Feb.)</td><td>98.4</td><td>92.5</td><td><span>89.4</span><span>/</span><span>95.4</span></td><td>97.1</td><td>71.0*</td><td>84.4 (95.4*)</td></tr>
                            <tr><td>HMMT 2025 (Nov.)</td><td>94.0</td><td>90.2</td><td><span>89.2*</span><span>/</span><span>—</span></td><td>93.5</td><td>74.3*</td><td>91.0*</td></tr>
                            <tr><td>IMOAnswerBench</td><td>85.4</td><td>78.3</td><td><span>78.6</span><span>/</span><span>81.8</span></td><td>82.0</td><td>60.4*</td><td>80.9*</td></tr>
                            <tr><td></td><td></td><td></td><td>Coding</td><td></td><td></td><td></td></tr>
                            <tr><td>LiveCodeBench-V6</td><td>86.4</td><td>83.3</td><td><span>83.1</span><span>/</span><span>85.0</span></td><td>84.9</td><td>—</td><td>80.6 (81.6*)</td></tr>
                            <tr><td>SWE-bench Verified</td><td>74.4</td><td>73.1</td><td><span>71.3</span><span>/</span><span>76.8</span></td><td>73.8</td><td>74.0</td><td>73.4</td></tr>
                            <tr><td>Terminal-Bench 2.0</td><td>51.0</td><td>46.4</td><td><span>35.7*</span><span>/</span><span>50.8</span></td><td>41.0</td><td>47.9</td><td>38.5</td></tr>
                        </tbody>
                    </table>
                </div>
                <ul>
                    <li>&#34;—&#34; indicates the score is not publicly available or not tested.</li>
                    <li>&#34;*&#34; indicates the original score was inaccessible or lower than our reproduced, so we report the evaluation under the same test conditions as Step 3.5 Flash to ensure fair comparability.</li>
                    <li>BrowseComp (with Context Manager): when the effective context length exceeds a predefined threshold, the agent resets the context and restarts the agent loop. (By contrast, Kimi K2.5 and DeepSeek-V3.2 used a discard-all strategy.)</li>
                    <li>In decoding cost section, decoding **Estimated using a similar but more accurate approach than <a href="https://arxiv.org/abs/2507.19427" target="_blank" rel="noopener">arxiv.org/abs/2507.19427</a></li>
                </ul>
            </div>
        </section>

        

        <section id="limitations">
            <div>
                <h2>Known Issues and Future Directions</h2>
                <ol>
                    <li><strong>Token Efficiency.</strong> Step 3.5 Flash achieves frontier-level agentic intelligence but currently relies on longer generation trajectories than Gemini 3.0 Pro to reach comparable quality.</li>
                    <li><strong>Efficient Universal Mastery.</strong> We aim to unify generalist versatility with deep domain expertise. To achieve this efficiently, we are advancing variants of on-policy distillation, allowing the model to internalize expert behaviors with higher sample efficiency.</li>
                    <li><strong>RL for More Agentic Tasks.</strong> While Step 3.5 Flash demonstrates competitive performance on academic agentic benchmarks, the next frontier of agentic AI necessitates the application of RL to intricate, expert-level tasks found in professional work, engineering, and research.</li>
                    <li><strong>Operational Scope and Constraints.</strong> Step 3.5 Flash is tailored for coding and work-centric tasks, but may experience reduced stability during distribution shifts. This typically occurs in highly specialized domains or long-horizon, multi-turn dialogues, where the model may exhibit repetitive reasoning, mixed-language outputs, or inconsistencies in time and identity awareness.</li>
                </ol>
            </div>
        </section>

        <section id="meet-stepfun">
            <div>
                <h2>Meet StepFun</h2>
                <ul>
                    <li>
                        <strong>OpenClaw</strong> is a powerful agentic platform that works seamlessly with Step 3.5 Flash.
                        <details>
                            <summary>Quick Setup</summary>
                            <div>
                                <p><strong>Install:</strong> <code>curl -fsSL https://openclaw.ai/install.sh | bash</code></p>
                                <p><strong>Onboard:</strong> Run <code>openclaw onboard</code>.</p>
                                <p><strong>Configure:</strong> In WebUI (Config → Models), add a new provider:</p>
                                <ul>
                                    <li>Type: <code>openai-completions</code> → Base URL: <code>https://api.stepfun.ai/v1</code></li>
                                    <li>Model ID: <code>step-3.5-flash</code> (Context: 256000)</li>
                                </ul>
                            </div>
                        </details>
                        For a full walkthrough, see our <strong><a href="https://github.com/stepfun-ai/Step-3.5-Flash/tree/main/cookbooks/openclaw" target="_blank" rel="noopener">OpenClaw Cookbook</a></strong>.
                    </li>
                    <li>Step 3.5 Flash is available via our <strong>API platform (<a href="https://platform.stepfun.com/docs/zh/llm/reasoning" target="_blank" rel="noopener">中文</a><span>/</span><a href="https://platform.stepfun.ai/docs/en/llm/reasoning" target="_blank" rel="noopener">EN</a>)</strong>, and you can chat with it on the <strong>Web (<a href="https://www.stepfun.com/" target="_blank" rel="noopener">中文</a><span>/</span><a href="https://stepfun.ai/" target="_blank" rel="noopener">EN</a>)</strong> or in our <strong>App (<a href="https://apps.apple.com/cn/app/%E9%98%B6%E8%B7%83ai-%E9%98%B6%E8%B7%83%E6%98%9F%E8%BE%B0ai%E5%8A%A9%E6%89%8B/id6502382318" target="_blank" rel="noopener">iOS</a><span>/</span><a href="https://play.google.com/store/apps/details?id=cn.yuewen.ywapp&amp;hl=zh" target="_blank" rel="noopener">Android</a>)</strong>.</li>
                    <li>Join our <strong><a href="https://discord.gg/RcMJhNVAQc" target="_blank" rel="noopener">Discord community</a></strong> for updates, support, and early access.</li>
                </ul>
            </div>
        </section>

    </div></div>
  </body>
</html>
