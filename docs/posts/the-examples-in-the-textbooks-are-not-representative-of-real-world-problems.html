<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://statmodeling.stat.columbia.edu/2022/03/27/the-examples-you-see-in-the-textbooks-are-not-representative-of-the-sorts-of-problems-you-see-in-the-real-world/">Original</a>
    <h1>The examples in the textbooks are not representative of real world problems</h1>
    
    <div id="readability-page-1" class="page"><article id="post-46801">
	<!-- .entry-header -->

	<div>
		<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/11/image-5.jpg" alt="" width="350"/></p>
<p>Bill Harris points to <a href="https://xkcd.com/2545/">this cartoon</a> from Randall Munroe.  I think he (Munroe) is really on to something here.  Not about Bayes’s theorem but about the sorts of examples that are in textbooks.</p>
<p>Consider the following sorts of examples that I’d say are overrepresented in textbooks (including my own; sorry!):</p>
<p>– Comparisons and estimates where the result is “statistically significant” and the 95% interval for the effect of interest excludes zero.</p>
<p>– Simpson’s paradox situation where adjusting for a factor changes the sign of an effect.</p>
<p>– Regression discontinuity analyses where there’s a pleasantly smooth and monotonic relationship between the predicted outcome and the running variable.</p>
<p>– Instrumental variables analyses where the instrument is really strong.</p>
<p>– Probability examples that are counterintuitive.</p>
<p>In short:  in a textbook, most of the examples work.  And when they don’t work, the failure illustrates some important point.</p>
<p>What’s going on here?  Textbook examples are there to illustrate a method.  The recommended approach is to first give two examples where the method works, then one where it fails.  But we, the textbook writers, are busy, so we’ll often just give the one that works and stop.  As I wrote <a href="https://statmodeling.stat.columbia.edu/2021/05/25/causal-inference-the-mixtape/">once</a>, “the book has a chapter on regression discontinuity designs. That’s fine. But all we see in the chapter are successes.”</p>
<p>Then again, on this blog, sometimes all you see are the failures.  That’s not representative either</p>
<p>And then there are some examples such as aggregation and conditional probability where it’s just standard to demonstrate the principle with a surprising example.</p>
<p>As you can see, I have no overarching theory here; it just seems like a topic worth thinking about.  Maybe in my next textbook I’ll try to address the issue in some way, maybe just to bring it up to remind the readers about this inevitable selection problem.  I’m also reminded of the famous book, “What They Don’t Teach You at Harvard Business School.”  Selection creates opportunities!</p>
			</div><!-- .entry-content -->

	<!-- .entry-meta -->
</article></div>
  </body>
</html>
