<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://masteringbackend.com/posts/docker-tutorial">Original</a>
    <h1>A definitive guide to Docker in 2023</h1>
    
    <div id="readability-page-1" class="page"><div data-v-5f285bec=""><section data-v-0291d782="" data-v-5f285bec=""><section data-v-0291d782=""><div id="chapter" data-v-0291d782=""><div id="chapter" data-v-0291d782="">  <div data-v-0291d782=""><div data-v-0291d782=""><article data-v-0291d782=""><p>This is the most comprehensive guide on Docker online.</p><p>In this docker tutorial, you will learn docker from scratch to an advanced level starting from the concept of containerization to Docker Compose in Docker. </p><p>You will learn how to dockerize a JavaScript application and how to use dockerize a full-stack application using Docker Compose.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1688781237/masteringbackend/posts/vectors/4140038.png" alt="The Definitive Guide To Docker in 2023" data-srcset="" data-v-0291d782=""/></p></div></div></div></section> <section data-v-0291d782=""></section> <span data-v-9c43a50a="" data-v-0291d782=""><section id="docker-overview" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><p data-v-9c43a50a=""><h2 data-v-9c43a50a=""><span data-v-9c43a50a="">Chapter 1:</span>
Complete Docker Overview
</h2></p>  <div data-v-9c43a50a=""><div data-v-9c43a50a=""><article data-v-9c43a50a=""><p>In this chapter, you will learn the complete overview of Docker and containerization to help you understand the concept of Docker and how to develop containerized applications.</p><p>This chapter will teach you some Docker terminologies such as Docker Image, Docker Containers, and Docker Registry.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1684533784/masteringbackend/posts/vectors/2842680.png" alt="" data-srcset="" data-v-9c43a50a=""/></p></div></div></div></section> <div id="article" data-v-9c43a50a=""><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>Prerequisites</h2><p>Before continuing with this tutorial, you will need to have an understanding of:</p><ul><li><p>Familiarity with Terminal</p></li><li><p>Familiarity with JavaScript.</p></li></ul><p>This will aid your understanding of the Docker. Nevertheless, before I go down the Docker route, let’s understand the history of containerization and the problems Docker is trying to solve.</p><h2>History of Containerization</h2></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><p>The concept of Containerization is not new, in fact, according to <a target="_blank" rel="" href="https://www.techtarget.com/searchitoperations/feature/Dive-into-the-decades-long-history-of-container-technology">TechTarget</a>, it dates back to the 1960s with VM partitioning which enables multiple users to access a computer concurrently.</p><p>However, the emergence of the Docker Engine in 2013 placed a significant impact on popularizing the concept of Containerization because it made it much easier to containerize your applications.</p><p>As of the time of writing, if you look at the <a target="_blank" rel="" href="https://survey.stackoverflow.co/2023/#other-tools">2023 Stack Overflow Developer Survey</a>, it reveals that Docker is the top-used and most popular tool among professional developers in other tools categories. That’s a 53% rise from its second-place spot last year.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_stackoverflow_stats_8f240c33b7.png" alt="docker stackoverflow stats" width="1024" height="768"/></p><p>You can clearly spot the demand for Docker in software engineering. However, learning Docker can seem intimidating at first that’s why I decided to make it simple to learn Docker. I broke down some of the complex things about Docker for complete beginners.</p><h2>What is Containerization?</h2><p>Containerization is a process of packaging an application&#39;s code with all the files and libraries it needs to run on any infrastructure.</p><p>You may have heard these popular words from your team members before:</p><p>“It was working on my system”</p><p>These kinds of problems are what containerization is trying to solve, the ability to package your application with all the required dependencies it needs to run the application on any infrastructure.</p><p>Let’s say you built your application with Node.js, installed different versions of dependencies, and use the Linux operating system.</p><p>Let’s say the entirety of your application uses the following dependencies for instance:</p><ul><li><p>Node.js v15</p></li><li><p>Express.js v4</p></li><li><p>Postgres v13</p></li><li><p>Nginx v1.23</p></li><li><p>TypeORM v0.3</p></li><li><p>Linux</p></li><li><p>etc</p></li></ul><p>Then you want to deploy or share this project with a teammate who uses a different operating system or has already installed different versions of these dependencies, It might cause errors as you run this application in a different environment.</p><p>Because if you look deeper, Node.js depends on a build tool called <code>node-gyp</code> for building native add-ons. And according to the <a target="_blank" rel="" href="https://github.com/nodejs/node-gyp#installation">installation guide</a> in the <a target="_blank" rel="" href="https://github.com/nodejs/node-gyp">official repository</a>, this build tool requires Python 2 or 3 and a proper C/C++ compiler tool-chain which becomes a pain to install and set up properly in different operating systems.</p><p><strong>Here’s my personal experience, I moved from using Windows to Linux on June 13, 2018, after trying to install and set up Memcached in Windows without success.</strong></p><p>Let’s assume one of your teammates uses Windows while you use Linux. Now you have to consider the inconsistencies in handling paths between these two operating systems. Additionally, if you’re using Nginx and Redis in your application, Nginx is not well-optimized to run on Windows, Redis doesn’t even come pre-built for Windows.</p><p>Even if you passed through the entire development phase, what about the deployment phase, how do you build these different codebases from the different operating systems together to become one?</p><p>There will always be questions, issues, and problems when working with isolated teams and different codebases.</p><p>That’s where packaging the application in containers comes in handy because it will include all the dependencies exact version needed, the exact operating system, and the database system included in the exact arrangement, etc.</p><p>When this packaged application is shipped to your teammate or deployed, the application has the same dependencies it needs to run and therefore experiences no errors or failures.</p><p>Therefore, all these issues can be solved if only you could:</p><ul><li><p>Develop and run the application inside an isolated environment called containers that contains the exact dependencies as your final deployment environment.</p></li><li><p>Put your application inside a single file (known as an image) with all the dependencies and necessary configurations.</p></li><li><p>And upload the image on a central server (known as a registry) that is accessible by everyone with proper authorization for your teammates.</p></li></ul><p>In this scenario, your teammates will only download the image from the registry and run the application as it is within an isolated environment free from platform-specific inconsistencies. Your operations team can even deploy it directly on a server since the image comes with all the proper production configurations.</p><p>That’s the idea behind containerization — putting your applications inside a self-contained package while making it portable and reproducible across various environments.</p><p>Now what is Docker and what role does it play in making the concept of containerization popular? That’s exactly what I will explore in the next section.</p></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>What is Docker?</h2><p>Docker is an open-source containerization platform that allows you to containerize or bundle your application in a container and share them using public or private registries. It enables the development team to develop, ship, and run application containers on a common OS and separate applications from infrastructures so that software is delivered quickly.</p><p>While explaining the concept of containerization, you should have noticed that it was just an idea that solves a myriad of problems in software engineering by bundling things into containers.</p><p>It was a concept and not the actual implementation. This very idea has a number of implementations.</p><p>However, Docker is one of the actual implementations of the containerization idea and a very popular one according to the Stack Overflow survey.</p><p>As stated earlier, the idea of containerization is old and there have been many technologies that have helped in the major turning points for the technology. I will save you the history for this tutorial but you can read the <a target="_blank" rel="" href="https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016">brief history of containers from the 1970s till date</a> which covers everything you need to know.</p><p>Furthermore, let’s explore the features of Docker and what makes it very popular among professional software engineers.</p><h2>Features of Docker</h2><p>Every technology has unique problems they are trying to solve. In this section, I will explore the key features and other features of Docker.</p><h3><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/features_of_docker_05cb0566b7.png" alt="features of docker" width="1024" height="768"/></h3><h3>Containerization</h3><p>Docker allows applications to be packaged into containers, providing a lightweight and portable environment that includes all the necessary dependencies. Containers offer consistency across different systems, making it easier to deploy and run applications consistently across various environments.</p><h3>Image-based Deployment</h3><p>Docker employs a layered file system known as Docker images. Images contain everything needed to run an application, including the code, runtime, libraries, and system tools. These images can be version-controlled, shared, and reused, ensuring consistent deployment across development, testing, and production environments.</p><h3>Efficient Resource Utilization</h3><p>Docker enables efficient resource utilization by running multiple containers on a single host, utilizing shared resources while maintaining isolation. This allows for better utilization of hardware resources, resulting in higher efficiency and cost savings.</p><h3>Rapid Application Deployment</h3><p>Docker simplifies the application deployment process. With Docker, applications can be packaged into containers and deployed quickly, ensuring consistency and reducing the chances of deployment-related issues.</p><h3>Scalability</h3><p>Docker provides built-in scalability features, allowing applications to scale horizontally by adding or removing containers based on demand. Docker&#39;s orchestration tools, such as Docker Swarm or Kubernetes, enable efficient container management and automated scaling.</p><h3>Isolation and Security</h3><p>Docker containers provide process isolation, ensuring that applications and their dependencies are encapsulated and separated from the underlying host system. This isolation enhances security by reducing the attack surface and minimizing the impact of potential vulnerabilities.</p><h3>Swarm</h3><p>Docker Swarm is an orchestration tool designed specifically for Docker containers, providing clustering and scheduling capabilities. It leverages the Docker API at its forefront, facilitating seamless integration with diverse tools for efficient management. Swarm functions as a self-organizing ensemble of container engines, offering the flexibility of employing pluggable backends.</p><h3>Services</h3><p>Services refer to a collection of tasks that define the desired state of a container within a cluster. Each task listed within Services represents an individual instance of a container that should be active. Swarm, the orchestration tool, takes care of intelligently distributing and scheduling these tasks across the available nodes within the cluster.</p><h3>Routing Mesh</h3><p>Docker&#39;s routing functionality directs incoming requests for published ports to active containers on available nodes. This capability allows connections to be established even if there are no running tasks on a specific node.</p><h2><strong>What can I use Docker for?</strong></h2><h3>Consistent Delivery of Applications</h3><p>With Docker, you can deliver your application faster and more consistently by bundling your application with all its dependencies in a container and sharing the container with your teammates or deploying it to a production environment easily.</p><p>Containers provide an exceptional solution for facilitating seamless integration and swift delivery within a continuous integration and continuous delivery (CI/CD) workflows.</p><h3>Responsive Deployment and Scaling</h3><p>The Docker containers are very portable and can run easily on the developer’s local machine, a physical or virtual server, a cloud server, etc. The portability and lightweight nature of Docker containers make it painless to deploy and scale applications.</p><h3>Running more workloads on the same hardware</h3><p>With Docker, you can optimize resource utilization and enable the efficient execution of multiple applications or services on a single physical or virtual machine.</p><p>Traditionally, running multiple applications on the same hardware would involve installing them directly on the operating system, which can lead to compatibility issues, dependency conflicts, and resource wastage. However, Docker&#39;s containerization technology addresses these challenges.</p><h2>Why you should learn Docker</h2><p>We have seen a tremendous increase in developer-focused tools in recent years and it’s gradually becoming difficult to keep up with learning them. Also, identifying good developer tools that will stand the test of time is difficult.</p><p>So if you ask:</p><p>“Why should you learn Docker” In your mind then it&#39;s a valid wrong question.</p><p>Personally, before I embark on learning any technology I make sure to Do My Own Research (DYOR) and discover why I should learn a specific technology.</p><p>In this section, I’m going to share my personal reasons plus that of my friend <a target="_blank" rel="" href="https://twitter.com/dannybster">Dan Bruce</a> when we jumped on a call to discuss the same topic.</p><p>So why did I or Dan Bruce learn Docker?</p><p>Here is why?</p><p><em>First, different reasons propel individuals to choose a particular tool for their project and the points listed below are opinionated to give you the reasons to learn Docker today.</em></p><p>In the previous sections, we have explored the benefits of Docker to the development teams and also stated some of the features of Docker that make it stand out from other Containerization tools.</p><p>However, If the above benefits are not enough, here are our top reasons to learn Docker:</p><p><strong>Career Opportunities</strong></p><p>According to the <a target="_blank" rel="" href="https://survey.stackoverflow.co/2023/#other-tools">Stack Overflow Survey 2023</a>, Docker is the top tool for professional software engineers and companies are building more containerized applications thereby highly searching for engineers who can dockerize their applications.</p><p><strong>Containerizing your applications</strong></p><p>Aside from working for a company with your Docker skills, you can take advantage of building containerized applications for your projects and use your Docker skills to containerize your applications.</p><p><strong>Onboarding team</strong></p><p>In engineering teams, onboarding new teammates use to be very challenging because different team members use different operating systems or different versions of applications and dependencies. Therefore setting up a team member can be a nightmare. From my experience joining a Fintech company years ago, it took me more than 4 weeks to fully set up the code base on my Mac since the codebase was developed in Linux.</p><p>Docker will have solved this problem by allowing me to install all the dependencies with one command without manually installing all the dependencies.</p><p>Therefore, eliminating the popular “It worked on my machine” complaint.</p><p><strong>Building your Infrastructure</strong></p><p>Following the containerization approach using Docker forces you to think about your infrastructure from the development point. When you build your application using Docker and a containerization mindset, you’re already setting the infrastructure for the production environment, staging environment as well as development environment.</p><h2>Docker vs Other Containerization Implementations</h2><p>As we have discovered, Docker is not the only containerization implementation. There are other containerization engines such as <a target="_blank" rel="" href="https://podman.io/">Podman</a> developed by Red Hat, <a target="_blank" rel="" href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> by Google, and <a target="_blank" rel="" href="https://coreos.com/rkt/">RKT</a> by CoreOS that are amazing alternatives to Docker.</p><p>According to <a target="_blank" rel="" href="https://www.statista.com/statistics/1256245/containerization-technologies-software-market-share/">Statista</a>, In 2022, Docker commands a 27 percent market share of the containerization technologies market. LXC and Kubernetes held the second and third places, respectively.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_market_share_453dd46c72.png" alt="docker market share" width="1024" height="768"/>Next, take a look at the summary of some of the features of Docker alternatives and compare them with the features and benefits of using Docker.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_alternatives_a109b3844b.png" alt="docker alternatives" width="1024" height="768"/></p></article></div></div></span><span data-v-9c43a50a="" data-v-0291d782=""><section id="docker-deep-dive" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><p data-v-9c43a50a=""><h2 data-v-9c43a50a=""><span data-v-9c43a50a="">Chapter 2:</span>
Docker Deep Dive
</h2></p>  <div data-v-9c43a50a=""><div data-v-9c43a50a=""><article data-v-9c43a50a=""><p>In this chapter, I will take you deeper into the Docker architecture.</p><p>I will explore different Docker concepts and terminologies such as Docker images, and Docker containers and lastly, you learn the architecture of Docker enough to build your first containerized application.</p><p>This chapter is split into 3 sections to discuss the important components of Docker and how they work together in Docker, Docker Architecture, Docker images, and Docker Containers.</p><p>If you’re excited as I am, let’s dive right in.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1688776189/masteringbackend/posts/vectors/8058227.png" alt="" data-srcset="" data-v-9c43a50a=""/></p></div></div></div></section> <div id="article" data-v-9c43a50a=""><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>Docker: The Architecture</h2><p>Building a containerized application with Docker is easy and that’s why it’s popular. However, there underlining architecture that makes Docker super easy, and understanding these architectures can accelerate the way you build containerized applications.</p><p>Docker is designed using the client-server architecture where a client sends a request to the server and the server sends back a response to the client. The Docker client talks to the Docker daemon (server) using a REST API, over UNIX sockets or a network interface.</p><p>The engine consists of these three components:</p><ol><li><p><strong>Docker Daemon:</strong> The Docker Daemon (<code>dockerd</code>) which is the server that does the heavy lifting of building, running, and distributing your Docker containers and many other Docker objects. It is a process that keeps running in the background and waits for commands from the client.</p></li><li><p><strong>Docker Client:</strong> The client (<code>docker</code>) is a command-line interface and the primary way users interact with Docker. When you type a Docker command like <code>docker pull</code> the docker client sends the command to the Docker Daemon using the Docker API for the command to be carried out.</p></li><li><p><strong>Docker API (REST API):</strong> The REST API acts as a bridge between the daemon and the client. When you enter any command, the Docker Client calls out to the appropriate endpoint to communicate with the Daemon to fulfill your request.</p></li></ol><p>The user executes commands using the Docker client command line and the client uses the Docker Rest API to communicate with the Docker Daemon to get the job done.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_architecture_48609cb3d6.png" alt="docker architecture" width="1024" height="768"/>Next, let me work you through what happens when you type a Docker command in your terminal let’s say the <code>docker run &lt;image&gt;</code> command. This will help us understand the full picture of Docker.</p><ol><li><p>When you type the command <code>docker pull &lt;image&gt;</code> command where <code>&lt;image&gt;</code> the image you want to pull.</p></li><li><p>The Docker client reaches out to the Docker Daemon for the image using Docker REST API.</p></li><li><p>The Docker Daemon reaches out for the image in your local machine repository and if not found, it will display an error like <code>Unable to find image &lt;image&gt; locally.</code></p></li><li><p>Next, the Docker Daemon will look for the image in Docker Hub either a private or public repository depending on your configuration.</p></li><li><p>Next, the Docker Daemon will save the image in your local machine repository for subsequent requests.</p></li><li><p>Lastly, It will return a response containing the image while outputting the wall of text on your terminal.</p></li></ol><p>It&#39;s the default behavior of the Docker daemon to look for images in the hub that are not present locally. But once an image has been fetched, it&#39;ll stay in the local cache. So if you execute the command again, you won&#39;t see the following lines in the output:</p><pre><code>Unable to find image &#39;&lt;image&gt;&#39; locally
latest: Pulling from &lt;image&gt;
0e03bdcc26d7: Pull complete
Digest: sha256:d58e752213a51785838f9eed2b7a498ffa1cb3aa7f946dda11af39286c3db9a9
Status: Downloaded newer image for &lt;image&gt;
</code></pre><h2>Docker Objects</h2><p>When working with Docker, you will surely create different objects such as images, containers, networks, volumes, etc in fact, you have already seen how to pull Docker images from the Docker repository from the illustration above using the command <code>docker pull &lt;image&gt;</code>.</p><p>The Image that you pulled, created, or deleted is part of Docker Objects and in this lesson, we will learn 2 of the most used Docker objects viz:</p><ol><li><p>Docker Images</p></li><li><p>Docker Containers</p></li></ol><p>To learn more about other Docker Objects, explore our Docker Hub content here.</p></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>What is Docker Image?</h2><p>A Docker image is a read-only multi-layered self-contained template with instructions to create a container. An image is a template used to create a container. Images can be created based on another image with some personalized customization for specific use cases.</p><p>The container image must contain everything needed to run an application - all dependencies, configurations, scripts, binaries, etc. The image also contains other configurations for the container, such as environment variables, a default command to run, and other metadata.</p><p>The <a target="_blank" rel="" href="https://opencontainers.org/">Open Container Initiative (OCI)</a> defined a standard specification for creating container images which means that images created for Docker will also work with Podman. Unlike in the early days when container engines had different image formats.</p><p>There are many images already published on the Docker registry or other registries that are publicly available for use based on the OCI standards.</p><p>You can choose from the list or create your own image from scratch as we will demonstrate further in this lesson.</p><p><strong>Creating a Docker Image</strong></p><p>Docker images are created using a Dockerfile, to build your own image, you create a `<code>Dockerfile`</code> with a simple syntax for defining the steps needed to create the image and run it. Here is an example of a simple `<code>Dockerfile`</code> to create an image:</p><pre><code>FROM node:16.17.0-alpine

# Set working directory
WORKDIR /usr/src/app

# Install dependencies
COPY package*.json ./
RUN npm ci --production

# Copy app source code
COPY . .

# Build app
RUN npm run build --production

COPY ./.env ./build

# Expose port, you can expose any port your app uses here
EXPOSE 3333

# Start app
CMD [&#34;node&#34;, &#34;./build/server.js&#34;]
</code></pre><p>The Dockerfile is self-explanatory due to the comments included in the file. Let’s go deeper into Docker commands used in the `<code>Dockerfile`</code>.</p><ol><li><p><strong>FROM</strong>: specifies which image is being used to build this new Docker image</p></li><li><p><strong>RUN</strong>: used to run a command while building the Docker image</p></li><li><p><strong>WORKDIR</strong>: It creates a new folder inside the Docker image</p></li><li><p><strong>COPY</strong>: used to copy the source codes and other files into a folder specified inside the Docker image.</p></li><li><p><strong>EXPOSE</strong>: used to expose the port number of the Docker image to the outside (client) machine.</p></li><li><p><strong>CMD</strong>: used to set the default command to be executed when a container is run. It is usually used in conjunction with the ENTRYPOINT command to provide a default application to be run when the container is started.</p></li></ol><p>This simple `<code>Dockerfile`</code> creates an image from a base image in this case `<code>node:16.17.0-alpine`</code> and set the working directory inside the container to `<code>/usr/src/app`</code> using the `<code>WORKDIR`</code> command, inside the `<code>app`</code> directory is where all the files the is copied using the command `<code>COPY package*.json ./` </code>will be stored.</p><p>Next, the image executes the `<code>npm ci --production`</code> using the `<code>RUN`</code> command inside the container, Next, we copy all the files from our local project directory to the container working directory.</p><p>Lastly, we execute the `<code>run build`</code> command, copy other files, expose the container port to the outside world, and start our server.</p><p>This was only a demonstration to show you how to use `<code>Dockerfile`</code> to create an image. Every command in the Dockerfile above has specific functionality and there are many more commands you can use to create your container images. You can explore all the commands in the <a target="_blank" rel="" href="https://docs.docker.com/engine/reference/builder/">official documentation</a>.</p><p>Now that you have created a Dockerfile, it&#39;s of no use if you don’t build it to an image. Therefore, let’s look at some of the Docker Client commands for images:</p><p><strong>Image Commands</strong></p><p>Here are some of the popular commands you can use to manage images:</p><p><strong>Build the Docker Image</strong></p><p>Once you&#39;ve created a Dockerfile, you can build the image using the `<code>docker build`</code> command where `<code>image-name`</code> is the user-defined name of the image.</p><pre><code>docker build -t image-name .
</code></pre><h3><strong>Run the Docker Image</strong></h3><p>There are several ways to run your image using the Docker Run command. Below, we&#39;re going to explore a few ways to run your Docker image:</p><p>Run the Docker Image:</p><pre><code>docker run image-name
</code></pre><p>You can run the Docker Image with specific parameters. For example, to run the Docker Image with parameters such as port with `<code>-p`</code>, the name with `<code>--name`</code>, and interactive mode with `<code>-it`</code>, use the following command:</p><pre><code>docker run --name container-name -it -p 3333:3333 image-name
</code></pre><p>Below is a table showing all the commands you can use to explore and manage Docker container images.</p><table><tbody><tr><th colspan="1" rowspan="1"><p>docker image build</p></th><th colspan="1" rowspan="1"><p>Build an image from a Dockerfile. It’s the same with `<code>docker build`</code> command</p></th></tr><tr><td colspan="1" rowspan="1"><p>docker image history</p></td><td colspan="1" rowspan="1"><p>Show the history of an image</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image import</p></td><td colspan="1" rowspan="1"><p>Import the contents from a tarball to create a filesystem image</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image inspect</p></td><td colspan="1" rowspan="1"><p>Display detailed information on one or more images</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image load</p></td><td colspan="1" rowspan="1"><p>Load an image from a tar archive or STDIN</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image ls</p></td><td colspan="1" rowspan="1"><p>List images</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image prune</p></td><td colspan="1" rowspan="1"><p>Remove unused images</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image pull</p></td><td colspan="1" rowspan="1"><p>Download an image from a registry</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image push</p></td><td colspan="1" rowspan="1"><p>Upload an image to a registry</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image rm</p></td><td colspan="1" rowspan="1"><p>Remove one or more images</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image save</p></td><td colspan="1" rowspan="1"><p>Save one or more images to a tar archive (streamed to STDOUT by default)</p></td></tr><tr><td colspan="1" rowspan="1"><p>docker image tag</p></td><td colspan="1" rowspan="1"><p>Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</p></td></tr></tbody></table><p>Now that you have learned how to create your own image for your project, you can also create an image the same way and upload it to the Docker registry for others or your team members to use. Let’s explore what the Docker registry entails.</p></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>What is a Docker Registries?</h2><p>An image registry serves as a centralized repository where you can upload your own images and download images created by others. <a target="_blank" rel="" href="https://hub.docker.com/">Docker Hub</a>, the default public registry for Docker, and <a target="_blank" rel="" href="https://quay.io/">Quay</a> by Red Hat, another highly popular image registry.</p><p>You can go to <a target="_blank" rel="" href="https://hub.docker.com/">Docker Hub</a> and scan through to see the most popular image or some of the images for your favorite tools. When you are at the home page, click on the Explore menu to see all the images as shown below:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_registry_400c2e60b7.png" alt="Docker registry" width="1427" height="940"/></p></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>What are Docker Containers?</h2><p>The concept of containerization gave birth to containers. Therefore, the concept of containers is very fundamental to the world of containerization.</p><p>A container is a runnable instance of an image. A docker container can be created, started, stopped, moved, or deleted using the Docker CLI Client or any other accepted clients. You can combine multiple containers to become one or attach one or more networks, storage, or create a new image based on the current state of the container.</p><p>If you know virtual machines, then you may consider containers to be equivalent to modern virtual machines.</p><p>Docker containers are completely isolated environments from the host machine as well as other containers. They are built to be a more lightweight, standalone, executable package of software that includes everything needed to run an application which is the codes, runtimes, system tools, system libraries, or settings.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_container_bc45e76851.png" alt="Docker container.png" width="1024" height="768"/>Let me work you through the diagram a little deeper to understand Docker containers from the ground up.</p><p>Docker containers are lightweight because they share the machine’s OS system kernel and therefore do not require an OS per application, driving higher server efficiencies and reducing server and licensing costs.</p><p>As you can see in the diagram above, each container relies on the Docker layer which in turn uses the resources from the machine’s OS.</p><p>This is one of the differences between virtual machines and Docker containers, and also a great benefit of using Docker containers because a virtual machine relies on individual Guest OS for each application as shown in the image below:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/virtual_machines_fe1d97d8dc.png" alt="virtual machines" width="1024" height="768"/>Now that we understand Docker containers, let’s look at how to manipulate and manage a Docker container in the next lesson.</p><p><strong>How to Run a Container</strong></p><p>When we learned how to run Docker images, we used the <code>docker run</code> command to achieve it. The command is also used to create and start a container by specifying the image name and some optional options as shown below:</p><pre><code>docker run image-name
</code></pre><p>This command will create and start a container if the image name is specified correctly and it exists anywhere in your local machine or Docker registry.</p><p>However, Docker has updated this command and made it easy to understand thereby improving developer experiences. The structure of the new command syntax looks like this:</p><pre><code>docker &lt;object&gt; &lt;command&gt; &lt;options&gt;
</code></pre><p>Where:</p><ul><li><p><code>&lt;object&gt;</code> is the type of Docker object you&#39;ll be manipulating. This can be a <code>container</code>, <code>image</code>, <code>network</code>, or <code>volume</code> object.</p></li><li><p><code>&lt;command&gt;</code> indicates the task to be carried out by the daemon, that is the <code>run</code> command.</p></li><li><p><code>&lt;options&gt;</code> can be any valid parameter that can override the default behavior of the command, like the <code>-publish</code> option for port mapping.</p></li></ul><p>Now the complete syntax to start and run a container from an image will look like this:</p><pre><code>docker container run image-name
</code></pre><p>Let’s replace the <code>image-name</code> with something more practical, let’s run the <code>nginx</code> container as an example. To run a container using this image, execute the following command on your terminal:</p><pre><code>docker container run --publish 8080:80 nginx
</code></pre><p>This will result in an output such as:</p><pre><code>Unable to find image &#39;nginx:latest&#39; locally
latest: Pulling from library/nginx
3ae0c06b4d3a: Pull complete 
efe5035ea617: Pull complete 
a9b1bd25c37b: Pull complete 
f853dda6947e: Pull complete 
38f44e054f7b: Pull complete 
ed88a19ddb46: Pull complete 
495e6abbed48: Pull complete 
Digest: sha256:08bc36ad52474e528cc1ea3426b5e3f4bad8a130318e3140d6cfe29c8892c7ef
Status: Downloaded newer image for nginx:latest

/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up

2023/07/06 12:51:31 [notice] 1#1: using the &#34;epoll&#34; event method
2023/07/06 12:51:31 [notice] 1#1: nginx/1.25.1
2023/07/06 12:51:31 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) 
2023/07/06 12:51:31 [notice] 1#1: OS: Linux 5.15.49-linuxkit
2023/07/06 12:51:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2023/07/06 12:51:31 [notice] 1#1: start worker processes
2023/07/06 12:51:31 [notice] 1#1: start worker process 29
2023/07/06 12:51:31 [notice] 1#1: start worker process 30
2023/07/06 12:51:31 [notice] 1#1: start worker process 31
2023/07/06 12:51:31 [notice] 1#1: start worker process 32
2023/07/06 12:51:31 [notice] 1#1: start worker process 33
</code></pre><p>Also, notice that we have a new option added to the command, the <code>--publish</code> option. Let’s talk about that in the next section and other popular options you can use with your Docker container command.</p><p><strong>How to Publish a Port</strong></p><p>As you already know, Docker containers are isolated environments. Your host system (local machine) does not know anything that’s going on inside the container environment unless you expose it.</p><p>Therefore, one way to expose the running application inside your container is by exposing the port to your host system.</p><p>Let’s say, for example, you started an Express application inside your container on port <code>3000</code>, your host machine will not know that port <code>3000</code> is running your Express application unless you explicitly expose it when you want to create the container using the command below:</p><pre><code>docker container run --publish 3000:3000 my-express-app
</code></pre><p>When you write <code>--publish 3000:3000</code>, it meant any request sent to port <code>3000</code> of your host system will be forwarded to port <code>3000</code> inside the container‌.</p><p>The process is called port mapping, you can map your container port to a different and available port on your local machine as shown in this example:</p><pre><code>docker container run --publish 4000:3000 my-express-app
</code></pre><p>In this case, if you visit <code>localhost:3000</code>, it will not work because your container port is mapped to <code>4000</code> on your local machine and it’s expecting traffic from <code>localhost:4000</code>.</p><p>Also, you can use a shorthand version of the command option as shown below which means the same thing:</p><pre><code>docker container run -p 4000:3000 my-express-app
</code></pre><p><strong>How to Use Detached Mode</strong></p><p>Next, you can run your container commands in detached mode meaning that your command will run in the background and your terminal will be wide open for new commands.</p><p>Here’s a command and the option to do so:</p><pre><code>docker container run  --detach  --publish 4000:3000 my-express-app
</code></pre><p>or the shorthand version:</p><pre><code>docker container run -d -p 4000:3000 my-express-app
</code></pre><p><strong>How to List Containers</strong></p><p>The next command is the <code>container ls</code> command, which allows you to list all available containers in your local machine that are currently running.</p><p>If you have any container in your local machine running, it should show you a list of them as shown below:</p><pre><code>docker container ls

# CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                  NAMES
# 9f21cb888058        my-express-app  &#34;/docker-entrypoint.…&#34;   5 seconds ago       Up 5 seconds        0.0.0.0:8080-&gt;80/tcp   upbeat_burn
</code></pre><p>To list all your containers including all states such as running, stopped, etc. Use the command shown in the example below:</p><pre><code>docker container ls --all

 // or the short-hand version

docker container ls -a

CONTAINER ID   IMAGE            COMMAND                  CREATED            STATUS                     PORTS                  NAMES
9f21cb888058   my-express-app  &#34;/docker-entrypoint.…&#34;   5 seconds ago       Up 5 seconds               0.0.0.0:8080-&gt;80/tcp   upbeat_burn
ae9192b8e32d   node            &#34;docker-entrypoint.s…&#34;   9 minutes ago       Exited (0) 6 minutes ago                          upbeat_blackburn
</code></pre><p>A container named <code>upbeat_burn</code> is running. It was created <code>5 seconds ago</code> and the status is <code>Up 5 seconds,</code> which indicates that the container has been running fine since its creation.</p><p>The <code>CONTAINER ID</code> is <code>9f21cb888058</code> which is the first 12 characters of the full container ID. The full container ID is <code>9f21cb88805810797c4b847dbd330d9c732ffddba14fb435470567a7a3f46cdc</code> which is 64 characters long. This full container ID was printed as the output of the <code>docker container run</code> command in the previous section.</p><p><strong>How to Name or Rename a Container</strong></p><p>Every container has two identifiers by default which are:</p><ul><li><p><code>CONTAINER ID</code> - a random 64-character-long string.</p></li><li><p><code>NAME</code> - a combination of two random words, joined with an underscore.</p></li></ul><p>You can rename a container to your user-defined name which will be easy to refer to than using the two randomly generated identifiers.</p><p>You can rename a container using the <code>--name</code> option or <code>-n</code> for short. To run another container using the <code>my-express-app</code> image with the name <code>my-express-app-container</code> you can execute the following command:</p><pre><code>docker container run --detach --publish 4000:3000 --name my-express-app-container my-express-app
</code></pre><p>A new container with the name <code>my-express-app-container</code> will be started.</p><p>You can even rename old containers using the <code>container rename</code> command. The syntax for the command is as follows:</p><pre><code>docker container rename &lt;container identifier&gt; &lt;new name&gt;
</code></pre><p>Here&#39;s an example:</p><pre><code>docker container rename my-express-app my-express-app-container-2
</code></pre><p>The command doesn&#39;t yield any output but you can verify that the changes have taken place using the <code>container ls</code> command. The <code>rename</code> command works for containers both in the running state and the stopped state.</p><p><strong>How to Stop or Kill a Running Container</strong></p><p>To stop a running container is easy. You can use the <code>ctrl + c</code> command to stop a running container on your terminal.</p><p>However, if you’re container is running in a detached mode, you need to use the following command to stop it.</p><p>Below is a generic syntax for the command is as follows:</p><pre><code>docker container stop &lt;container identifier&gt;
</code></pre><p>Where the <code>container identifier</code> can either be the id or the name of the container. You can get the identifier with the <code>docker container ls</code> command. Next, here’s an example to stop a running container.</p><pre><code>docker container stop my-express-app-container

# my-express-app-container
</code></pre><p>If you use the name as an identifier, you&#39;ll get the name thrown back to you as output. The <code>stop</code> command shuts down a container gracefully by sending a <code>SIGTERM</code> signal. If the container doesn&#39;t stop within a certain period, a <code>SIGKILL</code> signal is sent which shuts down the container immediately.</p><p>In cases where you want to send a <code>SIGKILL</code> signal instead of a <code>SIGTERM</code> signal, you may use the <code>container kill</code> command instead. The <code>container kill</code> command follows the same syntax as the <code>stop</code> command.</p><pre><code>docker container kill my-express-app-container
</code></pre><p><strong>How to Restart a Container</strong></p><p>Restarting a container can happen in two ways:</p><ul><li><p>Restarting the container from a failed, stopped, or killed state.</p></li><li><p>Rebooting a container from a running state.</p></li></ul><p>You can start a container from a failed, stopped, or killed state using the command below:</p><pre><code>docker container start my-express-app-container
</code></pre><p>Next, you can reboot a running container using the following command:</p><pre><code>docker container restart my-express-app-container
</code></pre><p>The main difference between the two commands is that the <code>container restart</code> command attempts to stop the target container and then starts it back up again, whereas the <code>start</code> command just starts an already stopped container.</p><p>In the case of a stopped container, both commands are exactly the same. But in the case of a running container, you must use the <code>container restart</code> command.</p><p><strong>How to Create a Container Without Running</strong></p><p>Sometimes, you just want to create a container without running it. You can achieve this using the following command:</p><pre><code>docker container create --publish 4000:3000 my-express-app-container
</code></pre><p>The <code>STATUS</code> of the container is <code>Created</code> at the moment, and, given that it&#39;s not running, it won&#39;t be listed without the use of the <code>--all</code> option in the <code>docker container ls</code> command.</p><p>The <code>create</code> command will create a new container and store it inside your local machine without running it. So that you can use the <code>docker container start</code> command to start the container next time without creating it again.</p><p><strong>How to Remove Containers</strong></p><p>Sometimes, you want to remain unused containers since all containers including stopped and killed containers still remain in your local system. Removing unused containers can save your system memory.</p><p>The following command is used to remove any container by passing in the identifier of the container you want to delete.</p><p>Here’s the syntax:</p><pre><code>docker container rm &lt;container identifier&gt;
</code></pre><p>You can use the <code>docker container ls -all</code> command to get the identifier.</p><pre><code>docker container rm 6cf52771dde1

# 6cf52771dde1
</code></pre><p>This command will delete the container whose identifier is specified as seen above.</p><p>In addition, you can use the <code>--rm</code> option to indicate a one-time container meaning that you want the container to be deleted immediately after they are stopped. You can use the <code>--rm</code> option in both <code>start</code> and <code>run</code> commands. Let’s look at this example with the <code>run</code> command.</p><pre><code>docker container run --rm --detach --publish 5000:3000 --name my-express-app-container-one-time my-express-app-container
</code></pre><p>The <code>my-express-app-container-one-time</code> container will be deleted immediately after it is stopped or killed.</p><p><strong>How to Run a Container in Interactive Mode</strong></p><p>Some images come with different lightweight versions of operating systems such as Linux and different distributions such as Ubuntu, Fedora, Kali, etc. If you use any of these images that come with an operating system. You can interact with the inside of the operating system when running the container.</p><p>For instance, Programming languages such as <a target="_blank" rel="" href="https://hub.docker.com/_/python">python</a>, <a target="_blank" rel="" href="https://hub.docker.com/_/php">php</a>, <a target="_blank" rel="" href="https://hub.docker.com/_/golang">go</a>, or run-times like <a target="_blank" rel="" href="https://hub.docker.com/_/node">node</a> and <a target="_blank" rel="" href="https://hub.docker.com/r/hayd/deno">deno</a> all have their official images.</p><p>These images do not just run some pre-configured program. They are instead configured to run a shell by default. In the case of the operating system images, it can be something like <code>sh</code> or <code>bash</code> and in the case of the programming languages or runtimes, it is usually their default language shell.</p><p>For example, if you are using the Ubuntu image, you can interact with the Ubuntu shell to install programs, navigate to folders or create a new file. In the same way, if you’re using a Node.js image, you might want to interact with the default language shell.</p><p>To achieve this, Docker provides the interactive <code>--interactive</code> or <code>-it</code> option which allows you to interact with the shell of your image when starting or running a container.</p><p>Here’s an example with an Ubuntu image:</p><pre><code>docker container run --rm -it ubuntu

Unable to find image &#39;ubuntu:latest&#39; locally
latest: Pulling from library/ubuntu
5af00eab9784: Pull complete 
Digest: sha256:0bced47fffa3361afa981854fcabcd4577cd43cebbb808cea2b1f33a3dd7f508
Status: Downloaded newer image for ubuntu:latest

<a href="https://masteringbackend.com/cdn-cgi/l/email-protection" data-cfemail="0c7e6363784c3c35683f6f383f3f3a3f3568">[email protected]</a>:/# cat /etc/os-release 
PRETTY_NAME=&#34;Ubuntu 22.04.2 LTS&#34;
NAME=&#34;Ubuntu&#34;
VERSION_ID=&#34;22.04&#34;
VERSION=&#34;22.04.2 LTS (Jammy Jellyfish)&#34;
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL=&#34;&lt;https://www.ubuntu.com/&gt;&#34;
SUPPORT_URL=&#34;&lt;https://help.ubuntu.com/&gt;&#34;
BUG_REPORT_URL=&#34;&lt;https://bugs.launchpad.net/ubuntu/&gt;&#34;
PRIVACY_POLICY_URL=&#34;&lt;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&gt;&#34;
UBUNTU_CODENAME=jammy
<a href="https://masteringbackend.com/cdn-cgi/l/email-protection" data-cfemail="186a77776c5828217c2b7b2c2b2b2e2b217c">[email protected]</a>:/#
</code></pre><p>The <code>-it</code> option sets the stage for you to interact with any interactive program inside a container. This option is actually two separate options mashed together.</p><ul><li><p>The <code>i</code> or <code>-interactive</code> option connects you to the input stream of the container so that you can send inputs to bash.</p></li><li><p>The <code>t</code> or <code>-tty</code> option makes sure that you get some good formatting and a native terminal-like experience by allocating a pseudo-tty.</p></li></ul><p>You need to use the <code>-it</code> option whenever you want to run a container in interactive mode. Another example can be running the <code>node</code> image as follows:</p><pre><code>docker container run -it node

Unable to find image &#39;node:latest&#39; locally
latest: Pulling from library/node
42cbebf8bc11: Pull complete 
9a0518ec5756: Pull complete 
356172c718ac: Pull complete 
dddcd3ceb998: Pull complete 
abe47058ac42: Pull complete 
08ff2ee7b183: Pull complete 
70b6353bf75e: Pull complete 
2742b73156b1: Pull complete 
Digest: sha256:57391181388cd89ede79f371e09373824051eb0f708165dcd0965f18b3682f35
Status: Downloaded newer image for node:latest

Welcome to Node.js v20.3.1.
Type &#34;.help&#34; for more information.
&gt; 5 + 5
10
&gt;
</code></pre><p><strong>How to Execute Commands Inside a Container</strong></p><p>You can execute a command inside your container, assuming you want to execute a command that is not available in Windows operating while you’re using Windows at the moment.</p><p>You can easily spin up a container that uses a Linux operating system and execute the command immediately. You can even make it a one-time container as discussed above just to help you achieve a specific task in Linux.</p><p>For example, assume that you want to encode a string using the <code>base64</code> program. This is something that&#39;s available in almost any Linux or Unix-based operating system (but not on Windows).</p><p>In this situation, you can quickly spin up a container using images like <a target="_blank" rel="" href="https://hub.docker.com/_/busybox">busybox</a> and let it do the job.</p><p>The generic syntax for encoding a string using <code>base64</code> is as follows:</p><pre><code>echo -n solomon-secret | base64

# c29sb21vbi1zZWNyZXQ=
</code></pre><p>And the generic syntax for passing a command to a container that is not running is as follows:</p><pre><code>docker container run &lt;image name&gt; &lt;command&gt;
</code></pre><p>To perform the base64 encoding using the busybox image, you can execute the following command:</p><pre><code>docker container run --rm busybox sh -c &#34;echo -n solomon-secret | base64

# c29sb21vbi1zZWNyZXQ=
</code></pre><p>What happens here is that, in a <code>container run</code> command, whatever you pass after the image name gets passed to the default entry point of the image to be executed.</p></article></div></div></span><span data-v-9c43a50a="" data-v-0291d782=""><section id="building-with-docker" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><p data-v-9c43a50a=""><h2 data-v-9c43a50a=""><span data-v-9c43a50a="">Chapter 3:</span>
Building with Docker
</h2></p>  <div data-v-9c43a50a=""><div data-v-9c43a50a=""><article data-v-9c43a50a=""><p>In this chapter, I will work you through building dockerized applications with Docker.</p><p>You will learn how to install Docker in different operating systems, and most importantly, you will build a containerized JavaScript application by putting into practice all we have learned in the previous chapters. </p><p>Lastly, you will how to use Docker Compose to define and run multi-container <strong><em>Docker</em></strong> applications.</p><p>If you’re excited as I am, let’s dive right in.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1688778123/masteringbackend/posts/vectors/3190343.png" alt="" data-srcset="" data-v-9c43a50a=""/></p></div></div></div></section> <div id="article" data-v-9c43a50a=""><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>How to install Docker</h2><p>Docker is one of the easiest applications to install on your local machine.</p><p>However, it depends greatly on the operating system your local machine is running on and I hope its Mac 😎</p><p>Nevertheless, Docker runs flawlessly on all major operating systems such as Mac, Windows, and Linux with Mac being the easiest. We will learn to install Docker on these three major operating systems.</p><h3>Install Docker on Mac</h3><p>To install Docker on Mac, simply go to the <a target="_blank" rel="" href="https://www.docker.com/products/docker-desktop/">Docker Download page</a> and download the latest version of Docker. Click on the <em>Download for Mac (stable)</em> button for Mac.</p><p>After downloading successfully, you will get an <em>Apple Disk Image</em> file which you can open and click on the application file to start the installation. All you have to do is drag the file and drop it in your Applications directory.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/drag_docker_in_applications_directory_ea7a28dae8.png" alt="drag and drop docker directory" width="1000" height="543"/>You can start Docker by simply double-clicking the application icon. Once the application starts, you&#39;ll see the Docker icon appear on your menu bar.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_Menu_Icon_8d9461cdbd.png" alt="Docker Menu Icon" width="2240" height="1260"/>Now, open up the terminal and execute <code>docker --version</code> and <code>docker-compose --version</code> to ensure the success of the installation.</p><h3>Install Docker on Windows</h3><p>Installing Docker in Windows is almost the same with only a few changes and extra steps that you’ll need to go through. The installation steps are as follows:</p><ol><li><p>Visit <a target="_blank" rel="" href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">this site</a> and follow the instructions for installing WSL2 on Windows 10 or later.</p></li><li><p>Then go to the official <a target="_blank" rel="" href="https://www.docker.com/products/docker-desktop">download page</a> and click the <em>Download for Windows (stable)</em> button.</p></li><li><p>Double-click the downloaded installer and go through the installation with the defaults.</p></li></ol><p>Once the installation is done, click <em>Docker Desktop</em> either from the start menu or your desktop to start the Docker engine. The docker icon should show up on your taskbar.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_running_status_3d896fe70b.png" alt="Docker running status" width="345" height="142"/>Lastly, open up Ubuntu or whatever distribution you&#39;ve installed from Microsoft Store. Execute the <code>docker --version</code> and <code>docker-compose --version</code> commands to make sure that the installation was successful.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_version_check_8a515005ff.png" alt="docker version check" width="979" height="512"/>You can access Docker from your regular Command Prompt or PowerShell as well.</p><h3>Install Docker on Linux</h3><p>Installing Docker in Linux is a bit different because Linux has different distributions and each comes with different installation files and processes.</p><p>Also, when you install the Docker Desktop package on Windows or Mac, it comes with a collection of tools like <code>Docker Engine</code>, <code>Docker Compose</code>, <code>Docker Dashboard</code>, <code>Kubernetes</code>, and a few other goodies.</p><p>However, on Linux, you don’t get such a bundle. Instead, you install all the necessary tools you need manually. Installation procedures for different distributions are as follows:</p><ul><li><p>If you’re on Ubuntu, you may follow the <a target="_blank" rel="" href="https://docs.docker.com/engine/install/ubuntu/">Install Docker Engine on Ubuntu</a> section from the official docs.</p></li><li><p>For other distributions, <em>installation per distribution</em> guides are available on the official docs.</p><ul><li><p><a target="_blank" rel="" href="https://docs.docker.com/engine/install/debian/">Install Docker Engine on Debian</a></p></li><li><p><a target="_blank" rel="" href="https://docs.docker.com/engine/install/fedora/">Install Docker Engine on Fedora</a></p></li><li><p><a target="_blank" rel="" href="https://docs.docker.com/engine/install/centos/">Install Docker Engine on CentOS</a></p></li></ul></li><li><p>If you’re on a distribution that is not listed in the docs, you may follow the <a target="_blank" rel="" href="https://docs.docker.com/engine/install/binaries/">Install Docker Engine from the binaries</a> guide instead.</p></li><li><p>Regardless of the procedure you follow, you’ll have to go through some <a target="_blank" rel="" href="https://docs.docker.com/engine/install/linux-postinstall/">Post-installation steps for Linux</a> which are very important.</p></li><li><p>Once you’re done with the Docker installation, you’ll have to install another tool named Docker Compose. You may follow the <a target="_blank" rel="" href="https://docs.docker.com/compose/install/">Install Docker Compose</a> guide from the official docs.</p></li></ul><p>If you follow the instruction for Ubuntu, once the installation is done, open up the terminal and execute <code>docker --version</code> and <code>docker-compose --version</code> to ensure the success of the installation.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_Linux_version_check_d72fae3d57.png" alt="Docker Linux version check" width="786" height="527"/>Now that we have installed Docker, let’s look at how to dockerize a JavaScript application with Docker.</p></article></div><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>How to Dockerize a JavaScript Application</h2><p>In this lesson, you will learn how to Dockerize a simple JavaScript project. The project is a simple Express 5 Todo API that I have already built.</p><p>Here’s the Postman result of the API:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_Project_Result_e20e4855f3.png" alt="Docker Project Result" width="2508" height="1732"/>If you want to follow along with creating the project, you can read through <a target="_blank" rel="" href="https://masteringbackend.com/posts/expressjs-5-tutorial-the-ultimate-guide">Express 5: The Ultimate Guide</a> for the step-by-step guide or you can clone the <a target="_blank" rel="" href="https://github.com/Kaperskyguru/express-todo">repository from GitHub</a>.</p><h3>Create a Dockerfile</h3><p>Create a <code>Dockerfile</code> in the root directory of the Express 5 Todo API project and add the following script:</p><pre><code># Create from a base image
FROM node:16.17.0-alpine

# Set working directory
WORKDIR /usr/src/app

# Copy Package.json files
COPY package*.json ./

# Install production-only dependencies
RUN npm ci --production

# Copy all files now
COPY . .

# Expose port 3000 to your host machine
EXPOSE 3000

# Run Npm Start command
CMD [ &#34;npm&#34;, &#39;start&#39;]
</code></pre><p>Look at the comments in the script for the explanation.</p><p>After creating your <code>Dockerfile</code> and adding the necessary instructions to create a container, the next step is to build the image.</p><h3>Build the image</h3><p>We will use the <code>docker build</code> or <code>docker image build</code> command we explored above to build the image. Open your terminal and type in the following command in the directory where the <code>Dockerfile</code> is located:</p><pre><code>docker build -t my-express-app .</code></pre><p>When you enter the command, Docker will start pulling the base image and setting up your container for you. You will see a result similar to the one below if everything is successful.</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_build_command_preview_de8312e087.png" alt="Docker build command preview" width="836" height="465"/>The next step is to run the container that we’ve created.</p><h3>Running the image</h3><p>As we have already explored, you can use the <code>Run</code> command to run any image. Type the following command into your terminal to run your image:</p><pre><code>docker run --name my-express-app-container -it -p 4000:3000 my-express-app
</code></pre><p>If this works, you should be greeted with a screenshot as shown below:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/running_container_preview_4cc6333477.png" alt="running container preview" width="933" height="353"/>As you can see, inside the container the post is <code>3000</code> but have mapped that point to <code>4000</code> in our host system. So to preview the API in the browser or access it with Postman, we will use port <code>localhost:4000</code>.</p><p>If everything works properly, if you visit <code>localhost:4000</code>you should see the result similar to the one below:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/Docker_project_result_preview_069cff0c7c.png" alt="Docker project result preview" width="930" height="540"/>We have practically Dockerised a simple JavaScript application. We have created a Docker image using Dockerfile, build and run it into a container</p><p>Furthermore, you can practice all the commands we have listed above to master each of them because they will become handy as you build more complex applications.</p><p>Next, let’s look at how to Dockerize a full-stack application that includes a front end, a backend, and a database. This will help us understand how to Dockerize complex applications using Docker Compose.</p></article></div></div></span><span data-v-9c43a50a="" data-v-0291d782=""><section id="advanced-docker" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><p data-v-9c43a50a=""><h2 data-v-9c43a50a=""><span data-v-9c43a50a="">Chapter 4:</span>
Advanced Docker
</h2></p>  <div data-v-9c43a50a=""><div data-v-9c43a50a=""><article data-v-9c43a50a=""><p>In the advanced Docker Guide, what you will learn includes:</p><ul><li><p>Docker Compose</p></li><li><p>Docker Volumes/Storage</p></li><li><p>Docker Build Pattern</p></li><li><p>Caching and Layers in Docker</p></li><li><p>Docker Tags and Versioning</p></li><li><p>Docker Registering</p></li><li><p>Docker Networks</p></li><li><p>Docker Swarm</p></li><li><p>Dockerize FullStack App</p></li></ul><p>Let me know in the comments section. I will share it with you.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1688780469/masteringbackend/posts/vectors/advanced-build.svg" alt="" data-srcset="" data-v-9c43a50a=""/></p></div></div></div></section> <div id="article" data-v-9c43a50a=""><div id="article" data-v-5fdb459a="" data-v-9c43a50a=""><article data-clarity-region="article" data-v-306b98bf="" data-v-5fdb459a=""><h2>Introduction to Docker Compose?</h2><p>In the examples so far, we have only dockerized a single application. Now let’s imagine we want to Dockerize a complete full-stack application that includes the backend, Frontend, and maybe a database for now.</p><p>How do we achieve this, traditionally, you will say. Why not create a different Dockerfile for each or use Docker multi-stage build?</p><p>Well, that could work depending on your strategy and if you’re ready for the stress.</p><p>However, Docker has a solution for Dockerizing complex applications like this called Docker Compose.</p><h3>What is Docker Compose?</h3><p>Docker compose is a tool used to define and run multi-container Docker applications. You can define all the services that your application depends on and Docker Compose will install and configure each of the services and run it as one application.</p><p>Therefore, if you have the Backend, Frontend, and Database for a full-stack application, you can define all these services based on how you want your application to work using Docker Compose YAML file and Docker Compose will carry out your instructions in the file.</p><p>Docker Compose works in all environments including development, staging, production, testing, and also CI workflows. It has simple commands to manage the whole lifecycle of your application infrastructure such as:</p><ul><li><p>Stopping, starting, and rebuilding all services with a single command.</p></li><li><p>Viewing the status of your running services.</p></li><li><p>Streaming the log output of running services.</p></li><li><p>Running a one-off command on a service.</p></li></ul><h3>Key Features of Docker Compose</h3><p>Here are some of the key features when using Docker Compose to manage your application services.</p><ul><li><p>Docker compose allows you to have multiple isolated environments as one single host.</p></li><li><p>It allows you to preserve volume storage when containers are created.</p></li><li><p>If you have multiple services, and you made changes to one, Docker Compose will only recreate the container that has changed.</p></li><li><p>Docker Compose supports variables and you can move a composition between environments such as dev, prod, staging, etc.</p></li></ul><p>Let’s look at an example. I have a full stack project I have developed that includes a Backend API service, a frontend service with React, and a Postgre database service.</p><h3>Installing Docker Compose</h3><p>If you follow the instructions above to install Docker on your Windows and Mac, then Docker Compose is already installed for you. However, if you’re Linux and Docker Compose is not installed, then you need to follow the instruction in the <a target="_blank" rel="" href="https://docs.docker.com/compose/install/">official documentation to install Docker Compose</a>.</p><p>If you’re unsure. Type the following command in your terminal to check if you already have Docker Compose installed.</p><pre><code>docker compose version
</code></pre><p>You should be greeted with the version of Docker Compose currently installed on your local machine. Below is the version of Docker Compose I will be using on a Mac machine.</p><pre><code>Docker Compose version v2.17.3
</code></pre><p>Now that we have installed Docker Compose, we are ready to dockerize our full-stack JavaScript application.</p><h3>Dockerizing a Full Stack App with Docker Compose</h3><p>To dockerize our full-stack application using Docker Compose, we will create a <code>docker-compose.yaml</code> file in the root directory of our application and define all the services that make up our full-stack application.</p><p>In our case, we have a full stack as shown in the demo video below:</p><p>[video here]</p><p>The application contains three services, the backend, the frontend, and a PostgreSQL database. Therefore, you can <a target="_blank" rel="" href="https://github.com/Kaperskyguru/strapi-int-test"><span>clone the project from this repository</span></a> or structure your project as shown in the screenshot below:</p><p><img src="https://strapi-images-aws-s3.s3.us-west-2.amazonaws.com/docker_compose_project_structure_7f5bb523c0.png" alt="docker compose project structure" width="1528" height="550"/>Create a generic Docker Image</p><p>Now, let’s explore the content of the <code>Dockerfile</code> first before we look at the content of the <code>docker-compose.yaml</code> file.</p><pre><code>FROM node:14.15.0

ARG PACKAGE_PATH=
ARG WORKING_DIR=

WORKDIR ${WORKING_DIR}

COPY ${PACKAGE_PATH}/package*.json ${WORKING_DIR}

RUN npm install --silent

COPY ${PACKAGE_PATH} ${WORKING_DIR}

VOLUME $WORKING_DIR/node_modules

CMD [ &#34;npm&#34;, &#34;start&#34; ]
</code></pre><p>This <code>Dockerfile</code> is used for both the frontend and the backend since both are JavaScript application and depends on NodeJS as the base image.</p><p>Next, I declared some variables to hold different <code>PACKAGE_PATH</code> and <code>WORKING_DIR</code> based on what is supplied by each service.</p><p>Next, I set the <code>WORKING_DIR</code> based on what is supplied and <code>COPY</code> the files located in the location supplied to the <code>PACKAGE_PATH</code> argument.</p><p>Next, I run the <code>npm install --silent</code> and copy the remaining files. Lastly, I created a volume to mount our <code>node_modules</code> based on the <code>WORKING_DIR</code> supplied and start the application with the <code>npm start</code> command.</p><p>The <code>Dockerfile</code> is the same as the one above we used to create the Docker images excerpt we are making it generic to accept two different Node applications using Docker <code>ARG</code> or arguments.</p><p>We haven’t discussed Docker Volumes in this guide but you can visit the Docker Content Hub for more advanced topics like Volumes.</p><h3>Create a Docker Compose file</h3><p>Now, let’s explore the content of the <code>docker-compose.yaml</code> file. Open the file or create a new one and add the following code.</p><pre><code>version: &#34;3.5&#34;

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PACKAGE_PATH: api
        WORKING_DIR: /usr/src/
    expose:
      - 8000
    ports:
      - 8000:8000
    environment:
      - NODE_ENV=development
      - TOKEN_SECRET=mYsEcReT
      - PORT=8000
      - TOKEN=eyJhbGciOiJIUzI1NiJ9.c29sb21vbg.WsTPoPpKhR7fFW0KsfN0A3II5M_iKLPpbnV2OOJOKcc
      - BASE_URL=http://api:8000

    env_file:
      - ./common.env
    volumes:
      - ./api:/usr/src
    depends_on:
      - postgres

    command: &gt;
      sh -c &#34;npm install knex -g &amp;&amp;
             npx knex migrate:latest &amp;&amp;
             npx knex seed:run &amp;&amp;
             npm start &amp;&amp;
             npm test&#34;

  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PACKAGE_PATH: frontend
        WORKING_DIR: /usr/src/
    expose:
      - 3000
    ports:
      - 3000:3000
    environment:
      - REACT_APP_ENV=production
      - REACT_APP_BACKEND=http://0.0.0.0:8000/graphql
      - NODE_PATH=/usr/src/
      - REACT_APP_TOKEN=eyJhbGciOiJIUzI1NiJ9.c29sb21vbg.WsTPoPpKhR7fFW0KsfN0A3II5M_iKLPpbnV2OOJOKcc

    env_file:
      - ./common.env
    volumes:
      - ./frontend:/usr/src
    depends_on:
      - api
    command: [&#34;npm&#34;, &#34;start&#34;]

  postgres:
    image: postgres
    restart: always
    env_file: ./common.env
</code></pre><p>Let me walk you through the nitty-gritty of this code snippet.</p><p>Before we continue with the code walkthrough, here’s the content of the <code>common.env</code> which all our services use. So add the following code to your <code>common.env</code> file which will be used to set up your Postgres instance.</p><pre><code>DB_CLIENT=postgresql
POSTGRES_DB=strapi_test_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_PORT=5432
POSTGRES_HOST=postgres

BASE_URL=http://{DOCKER_IP}:8000
</code></pre><p><strong>Code Walkthrough</strong></p><p>The Compose file is a <a target="_blank" rel="" href="https://yaml.org/">YAML</a> file defining services, networks, and volumes for a Docker application. The file is split into services, their properties, and the instructions to successfully create a container out of the service.</p><p><strong><em>Docker Compose Version</em></strong></p><p>The first line declares the version of Docker Compose we want to use to execute the <code>docker-compose</code> file.</p><pre><code>version: &#34;3.5&#34;
</code></pre><p>As of the time of writing, we are using version <code>3.5</code>. You can read more about <a target="_blank" rel="" href="https://docs.docker.com/compose/compose-file/compose-versioning/">Docker Compose versioning</a> but we will move on for now.</p><p><strong><em>Services</em></strong></p><p>The next section of the file is the part of the services. Here we define all the services that make up our application. In our case, the backend API, the frontend, and a PostgreSQL database as defined respectively.</p><ol><li><p><strong>First, we have the API service:</strong></p></li></ol><pre><code>  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PACKAGE_PATH: api
        WORKING_DIR: /usr/src/
    expose:
      - 8000
    ports:
      - 8000:8000
    environment:
      - NODE_ENV=development
      - TOKEN_SECRET=mYsEcReT
      - PORT=8000
      - TOKEN=eyJhbGciOiJIUzI1NiJ9.c29sb21vbg.WsTPoPpKhR7fFW0KsfN0A3II5M_iKLPpbnV2OOJOKcc
      - BASE_URL=http://api:8000

    env_file:
      - ./common.env
    volumes:
      - ./api:/usr/src
    depends_on:
      - postgres

    command: &gt;
      sh -c &#34;npm install knex -g &amp;&amp;
             npx knex migrate:latest &amp;&amp;
             npx knex seed:run &amp;&amp;
             npm start &amp;&amp;
             npm test&#34;

</code></pre><p>To create a container, we need an image. Therefore, in the <code>build</code> section of the <code>API</code> service, we pass <code>.</code> to the <code>context</code> property telling Docker Compose to use the current directory as the root directory so that we can find the <code>Dockerfile</code> we created earlier and pass the arguments to it as discussed earlier.</p><p>Here’s the snippet for the <code>build</code> section. This section will build an image out of the <code>Dockerfile</code> with the right values passed to the <code>ARG</code> arguments.</p><pre><code> build:
      context: .
      dockerfile: Dockerfile
      args:
        PACKAGE_PATH: api
        WORKING_DIR: /usr/src/
</code></pre><p>Next, we expose the container port which is <code>8000</code> using the <code>expose</code> property, after that, we also map the port to <code>8000</code> using the <code>port</code> property. This means that the container exposes port <code>8000</code> and we also map it to <code>8000</code> in our host machine. So <code>[localhost:8000](&lt;http://localhost:8000&gt;)</code> will show our API endpoint.</p><p>Next, we define some environment variables that are not sensitive and use the <code>env_file</code> property to define sensitive environment variables or common environment variables that will be used in all our services.</p><p>Next, we mount a volume storage from <code>./api</code> to the folder inside our container <code>/usr/src</code> using the <code>volumes</code> property. Volumes are just a mechanism for persisting data generated by and used by Docker containers.</p><p>Next, we use the <code>depends_on</code> property to tell Docker Compose that the <code>API</code> service is depending on the <code>postgres</code> service which means that the <code>API</code> service will not start until the <code>postgres</code> is completely started successfully.</p><p>Lastly, we use the <code>command</code> property to run all the commands we need to set up our backend. First, we install <code>knex</code> globally because need it to run our database migration and database seeders. Lastly, we start our backend server and run tests with <code>npm start</code> and <code>npm test</code> commands respectively.</p><ol><li><p><strong>Secondly, we have the Frontend service:</strong></p></li></ol><p>The next service is the <code>frontend</code> service, which is the same thing as the <code>api</code> service except we only change the <code>PACKAGE_PATH</code> and <code>WORKING_DIR</code> locations, map <code>3000</code> as the port, create individual environment variables, and make sure it depends on the <code>api</code> service.</p><ol><li><p><strong>Third, we have the Postgres service:</strong></p></li></ol><p>The last service is the <code>postgres</code> service. Here we don’t need the <code>build</code> step because we using the official <code>postgres</code> image from Docker Hub. We set it to always restart if it fails at some point because our backend service depends on it. Lastly, we pass the <code>common.env</code> file to the <code>env_file</code> property which contains the properties needed to set up a new Postgresql instance.</p><pre><code>postgres:
    image: postgres
    restart: always
    env_file: ./common.env</code></pre></article></div></div></span><span data-v-9c43a50a="" data-v-0291d782=""><section id="conclusion-docker" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><div id="chapter" data-v-9c43a50a=""><p data-v-9c43a50a=""><h2 data-v-9c43a50a=""><span data-v-9c43a50a="">Conclusion:</span>
Docker
</h2></p>  <div data-v-9c43a50a=""><div data-v-9c43a50a=""><article data-v-9c43a50a=""><p>Don’t just learn Docker. Learn the concept of containerization also.</p></article></div> <p><img data-src="https://res.cloudinary.com/kaperskydisk/image/upload/v1684535805/masteringbackend/posts/vectors/1905.i126.001.P.m005.c30.programmer_set-11.png" alt="" data-srcset="" data-v-9c43a50a=""/></p></div></div></div></section> </span> </section>  </div></div>
  </body>
</html>
