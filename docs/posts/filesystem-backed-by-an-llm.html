<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://healeycodes.com/filesystem-backed-by-an-llm">Original</a>
    <h1>Filesystem Backed by an LLM</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>I came across <a href="https://x.com/simonw/status/1941190140380201431">a post</a> discussing the <a href="https://github.com/awwaiid/gremllm">gremllm</a> Python library that hallucinates and then evaluates method implementations as you call them.</p><p>I thought this was pretty cool and it reminded me of experiments people tried shortly after GPT-3&#39;s launch, where they prompted to hallucinate a Linux system that they could interact with via terminal commands sent in the chat UI. The earliest article I could find on this is <a href="https://www.engraved.blog/building-a-virtual-machine-inside/">Building A Virtual Machine Inside ChatGPT</a>.</p><p>I had an idea for a middle ground — not just a hallucinating library, nor an entirely hallucinated system. What if <em>parts</em> of the OS were backed by an LLM?</p><p>My idea is a FUSE-based filesystem where every file operation is handled by an LLM. In <a href="https://github.com/healeycodes/llmfs">llmfs</a>, content is generated on the fly by calling out to OpenAI&#39;s API.</p><p>In the video above, you can see me interacting with this mounted FUSE filesystem. The latency is expected as everything must be run past the LLM.</p><pre><div><div><p><span>$ </span><span>cat</span><span> generate_20_bytes_of_binary_data.py </span><span>|</span><span> python3</span></p><p><span>b</span><span>&#39;\x94\xc2(\xbd\x17&lt;|\xd7\x01*\x01\xdeWvM\xaa\x8fX\xfa\xb1&#39;</span></p></div></div></pre><p>The resulting data is not stored on disk. It&#39;s stored in an in-memory history log of actions.</p><p>This means the LLM can remember which data exists at which path.</p><pre><div><div><p><span>$ </span><span>echo</span><span> </span><span>&#34;andrew&#34;</span><span> </span><span>&gt;</span><span> my_name.txt</span></p><p><span>$ </span><span>cat</span><span> my_name.txt</span></p><p><span>andrew</span></p></div></div></pre><p>As the LLM handles all file operations, it&#39;s free to deny certain actions. The system prompt allows the LLM to deny file operations with UNIX error codes.</p><pre><div><div><p><span>For failed operations (only use for actual errors), respond with:</span></p><p><span>{&#34;error&#34;: 13}  (where 13 = EACCES for &#34;Permission denied&#34;)</span></p><p><span>Examples:</span></p><p><span>- Writing passwd: {&#34;error&#34;: 13} (system files)</span></p><p><span>- Writing malicious_script.sh: {&#34;error&#34;: 13} (dangerous content)</span></p></div></div></pre><p>These error codes are bubbled up through the filesystem.</p><pre><div><div><p><span>$ </span><span>cat</span><span> secrets.txt</span></p><p><span>cat: secrets.txt: Permission denied</span></p></div></div></pre><h3 id="interacting-with-fuse">Interacting With FUSE</h3><p>After mounting the filesystem with the Go library <a href="http://basil.org/fuse">bazil.org/fuse</a>, the kernel intercepts Virtual File System (VFS) calls like open/read/write and forwards them through <code>/dev/fuse</code> to the userspace daemon.</p><pre><div><div><p><span>import</span><span> </span><span>&#34;bazil.org/fuse&#34;</span><span></span></p><p><span>mnt </span><span>:=</span><span> os</span><span>.</span><span>Args</span><span>[</span><span>1</span><span>]</span><span></span></p><p><span>c</span><span>,</span><span> err </span><span>:=</span><span> fuse</span><span>.</span><span>Mount</span><span>(</span><span></span></p><p><span>    mnt</span><span>,</span><span></span></p><p><span>    fuse</span><span>.</span><span>FSName</span><span>(</span><span>&#34;llmfs&#34;</span><span>)</span><span>,</span><span></span></p><p><span>    fuse</span><span>.</span><span>Subtype</span><span>(</span><span>&#34;llmfs&#34;</span><span>)</span><span>,</span><span></span></p><p><span>    fuse</span><span>.</span><span>AllowOther</span><span>(</span><span>)</span><span>,</span><span></span></p><p><span></span><span>)</span></p></div></div></pre><p>The library reads from <code>/dev/fuse</code>, services each request, and writes the reply back to the same device.</p><p>My Go code, which implements interfaces like <code>fs.Node</code>, handles the file operations and provides file contents, metadata, and error codes.</p><pre><div><div><p><span>func</span><span> </span><span>(</span><span>h </span><span>*</span><span>fileHandle</span><span>)</span><span> </span><span>Write</span><span>(</span><span></span></p><p><span>    </span><span>_</span><span> context</span><span>.</span><span>Context</span><span>,</span><span> req </span><span>*</span><span>fuse</span><span>.</span><span>WriteRequest</span><span>,</span><span> resp </span><span>*</span><span>fuse</span><span>.</span><span>WriteResponse</span><span>,</span><span></span></p><p><span></span><span>)</span><span> </span><span>error</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>appendHistory</span><span>(</span><span>&#34;user&#34;</span><span>,</span><span></span></p><p><span>        fmt</span><span>.</span><span>Sprintf</span><span>(</span><span>&#34;Write %s offset %d data %q&#34;</span><span>,</span><span> h</span><span>.</span><span>name</span><span>,</span><span> req</span><span>.</span><span>Offset</span><span>,</span><span> </span><span>string</span><span>(</span><span>req</span><span>.</span><span>Data</span><span>)</span><span>)</span><span>)</span><span></span></p><p><span>    prompt </span><span>:=</span><span> </span><span>buildPrompt</span><span>(</span><span>)</span><span></span></p><p><span>    rc </span><span>:=</span><span> </span><span>StreamLLM</span><span>(</span><span>prompt</span><span>)</span><span></span></p><p><span>    llmResp</span><span>,</span><span> err </span><span>:=</span><span> </span><span>ParseLLMResponse</span><span>(</span><span>rc</span><span>)</span><span></span></p><p><span>    </span><span>_</span><span> </span><span>=</span><span> rc</span><span>.</span><span>Close</span><span>(</span><span>)</span><span></span></p><p><span>    </span><span>if</span><span> err </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>return</span><span> fuse</span><span>.</span><span>Errno</span><span>(</span><span>syscall</span><span>.</span><span>EIO</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>    </span><span>if</span><span> ferr </span><span>:=</span><span> </span><span>FuseError</span><span>(</span><span>llmResp</span><span>)</span><span>;</span><span> ferr </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>return</span><span> fuse</span><span>.</span><span>Errno</span><span>(</span><span>ferr</span><span>.</span><span>(</span><span>syscall</span><span>.</span><span>Errno</span><span>)</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>    </span><span>appendHistory</span><span>(</span><span>&#34;assistant&#34;</span><span>,</span><span> </span><span>&#34;ok&#34;</span><span>)</span><span></span></p><p><span>    resp</span><span>.</span><span>Size </span><span>=</span><span> </span><span>len</span><span>(</span><span>req</span><span>.</span><span>Data</span><span>)</span><span></span></p><p><span>    </span><span>return</span><span> </span><span>nil</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><p>These responses are written back to <code>/dev/fuse</code>, and the kernel then continues processing the syscall from the original process.</p><p>Given there are delays of <em>hundreds</em> of milliseconds for each operation, I&#39;m not too worried about performance. Instead of per-inode locks I simply serialise everything behind <code>llmMu</code>.</p><pre><div><div><p><span>var</span><span> llmMu sync</span><span>.</span><span>Mutex </span><span>// global – one request at a time</span><span></span></p><p><span></span><span>type</span><span> lockedReader </span><span>struct</span><span> </span><span>{</span><span></span></p><p><span>    io</span><span>.</span><span>Reader</span></p><p><span>    once sync</span><span>.</span><span>Once</span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>func</span><span> </span><span>(</span><span>lr </span><span>*</span><span>lockedReader</span><span>)</span><span> </span><span>Close</span><span>(</span><span>)</span><span> </span><span>error</span><span> </span><span>{</span><span></span></p><p><span>    lr</span><span>.</span><span>once</span><span>.</span><span>Do</span><span>(</span><span>llmMu</span><span>.</span><span>Unlock</span><span>)</span><span></span></p><p><span>    </span><span>return</span><span> </span><span>nil</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><h2 id="llm-context">LLM Context</h2><p>File system operations append actions to the history log.</p><pre><div><div><p><span>user: Read nums.txt</span></p><p><span>assistant: Data nums.txt content &#34;123456\n&#34;</span></p></div></div></pre><p>A new prompt is generated for each file operation. It starts with the system prompt, which begins with the following:</p><pre><div><div><p><span>system: You are a filesystem that generates file content on demand.</span></p><p><span>IMPORTANT: You must respond with EXACTLY ONE valid JSON object. No other text.</span></p><p><span>When a file is requested:</span></p><p><span>- If it&#39;s a new file, create content based on the filename, extension, and context</span></p><p><span>- If it&#39;s an existing file, return the content of the file</span></p></div></div></pre><p>After this, the entire history log is appended. So, if the user has sent two different writes to a file, the LLM will be able to understand these actions, and generate the correct file, even though the complete file is not explicitly stored.</p><pre><div><div><p><span>user: Write nums.txt offset 0 data &#34;123\n&#34;</span></p><p><span>assistant: ok</span></p><p><span>user: Write nums.txt offset 4 data &#34;456\n&#34;</span></p><p><span>assistant: ok</span></p><p><span>user: Read nums.txt</span></p><p><span>assistant: Data nums.txt content &#34;123456\n&#34;</span></p></div></div></pre><p>File errors also need to be stored so that they are consistently handled.</p><pre><div><div><p><span>user: Read private</span></p><p><span>assistant: error 13</span></p></div></div></pre><h2 id="json-schema">JSON Schema</h2><p>My interactions with the LLM are simple enough that I didn&#39;t reach for any special tools and just rolled my own JSON parsing. This seemed to work well with various GPT-4 models.</p><pre><div><div><p><span>// LLMResponse should match the JSON schema:</span><span></span></p><p><span></span><span>//</span><span></span></p><p><span></span><span>//    { &#34;data&#34;: &#34;&lt;utf-8 text&gt;&#34; }</span><span></span></p><p><span></span><span>//    { &#34;error&#34;: &lt;errno&gt; }</span><span></span></p><p><span></span><span>//</span><span></span></p><p><span></span><span>// Exactly one of Data or Error is non-nil</span><span></span></p><p><span></span><span>type</span><span> LLMResponse </span><span>struct</span><span> </span><span>{</span><span></span></p><p><span>    Data  </span><span>*</span><span>string</span><span> </span><span>`json:&#34;data,omitempty&#34;`</span><span></span></p><p><span>    Error </span><span>*</span><span>int</span><span>    </span><span>`json:&#34;error,omitempty&#34;`</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><p>Let&#39;s take file creation for example. First we append the user action like <code>Create nums.txt</code> to the history, and then we make the LLM call.</p><pre><div><div><p><span>func</span><span> </span><span>(</span><span>rootDir</span><span>)</span><span> </span><span>Create</span><span>(</span><span></span></p><p><span>    </span><span>_</span><span> context</span><span>.</span><span>Context</span><span>,</span><span> req </span><span>*</span><span>fuse</span><span>.</span><span>CreateRequest</span><span>,</span><span> resp </span><span>*</span><span>fuse</span><span>.</span><span>CreateResponse</span><span>,</span><span></span></p><p><span></span><span>)</span><span> </span><span>(</span><span>fs</span><span>.</span><span>Node</span><span>,</span><span> fs</span><span>.</span><span>Handle</span><span>,</span><span> </span><span>error</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>appendHistory</span><span>(</span><span>&#34;user&#34;</span><span>,</span><span> fmt</span><span>.</span><span>Sprintf</span><span>(</span><span>&#34;Create %s&#34;</span><span>,</span><span> req</span><span>.</span><span>Name</span><span>)</span><span>)</span><span></span></p><p><span>    prompt </span><span>:=</span><span> </span><span>buildPrompt</span><span>(</span><span>)</span><span></span></p><p><span>    rc </span><span>:=</span><span> </span><span>StreamLLM</span><span>(</span><span>prompt</span><span>)</span><span></span></p><p><span>    llmResp</span><span>,</span><span> err </span><span>:=</span><span> </span><span>ParseLLMResponse</span><span>(</span><span>rc</span><span>)</span><span> </span><span>// (LLMResponse, error)</span><span></span></p><p><span>    </span><span>// ..</span></p></div></div></pre><p>We block on the call and the parsing of the response. The prompt steers the LLM towards JSON by requesting it directly as well as providing examples.</p><p>The schema is quite loose in that I re-use the <code>data</code> field to report that operations like creating files are successful, as seen in the examples that are part of the system prompt:</p><pre><div><div><p><span>When writing to a file:</span></p><p><span>- Accept the write operation and acknowledge it was successful</span></p><p><span>- Only reject writes that are clearly malicious or dangerous</span></p><p><span>- For successful writes, respond with: {&#34;data&#34;: &#34;ok\n&#34;}</span></p><p><span>For successful operations, respond with:</span></p><p><span>{&#34;data&#34;: &#34;content of the file\n&#34;} (for reads)</span></p><p><span>{&#34;data&#34;: &#34;ok\n&#34;} (for writes)</span></p><p><span>For failed operations (only use for actual errors), respond with:</span></p><p><span>{&#34;error&#34;: 13}  (where 13 = EACCES for &#34;Permission denied&#34;)</span></p><p><span>Examples:</span></p><p><span>- Reading hello_world.txt: {&#34;data&#34;: &#34;Hello, World!\n&#34;}</span></p><p><span>- Reading config.json: {&#34;data&#34;: &#34;{\&#34;version\&#34;: \&#34;1.0\&#34;, \&#34;magic\&#34;: true}\n&#34;}</span></p><p><span>- Reading print_hello.py: {&#34;data&#34;: &#34;print(&#39;Hello, World!&#39;)\n&#34;}</span></p><p><span>- Writing some_file.txt: {&#34;data&#34;: &#34;ok\n&#34;}</span></p><p><span>- Writing passwd: {&#34;error&#34;: 13} (system files)</span></p><p><span>- Writing malicious_script.sh: {&#34;error&#34;: 13} (dangerous content)</span></p><p><span>Example error codes:</span></p><p><span>- 5 (EIO): I/O error</span></p><p><span>- 13 (EACCES): Permission denied</span></p><p><span>Writing at offsets is supported:</span></p><p><span>- user: Write nums.txt offset 0 data &#34;123\n&#34;</span></p><p><span>- assistant: ok</span></p><p><span>- user: Write nums.txt offset 5 data &#34;456\n&#34;</span></p><p><span>- assistant: ok</span></p></div></div></pre><p>One issue that I thought I&#39;d run into, was data encoding. When I was running some tests to generate script files, I thought that the LLM would reply with invalid JSON when there were unescaped characters in the response like <code>{&#34;data&#34;: &#34;\&#34;}</code> which would then bubble up into a file error.</p><p>However, GPT-4 models understand the context (we&#39;re generating JSON) and escape it automatically by returning things like <code>{&#34;data&#34;: &#34;\\&#34;}</code>.</p><pre><div><div><p><span>cat</span><span> a_single_backslash.txt</span></p><p><span></span><span>\</span><span></span></p><p><span></span><span># history log:</span><span></span></p><p><span></span><span>#   user: Read a_single_backslash.txt</span><span></span></p><p><span></span><span>#   assistant: Data a_single_backslash.txt content &#34;\\&#34;</span><span></span></p><p><span></span><span># raw response:</span><span></span></p><p><span></span><span>#   {&#34;data&#34;: &#34;\\&#34;}</span></p></div></div></pre><p>A more robust solution might look like: returning a single character to indicate the type of response, followed by pure data.</p><h2 id="whats-next">What&#39;s Next</h2><p>I&#39;m pretty happy with this demo. I set out to intercept and handle file operations with an LLM and it works better than I expected.</p><p>To extend support for <em>all</em> file operations, like a good filesystem, I think I&#39;ll need to rethink my schema design. In fact, I&#39;d like to throw it all away and remove this mapping layer altogether.</p><p>In order to support more features, I&#39;m wondering if I can de-/serialize entire <a href="http://bazil.org/fuse">bazil.org/fuse</a> library objects so everything works out of the box. My gut says this could work with the latest LLM models with a good setup.</p><p>Let me know if you have other ideas! The demo repo is <a href="https://github.com/healeycodes/llmfs">https://github.com/healeycodes/llmfs</a>.</p></div></div></div>
  </body>
</html>
