<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/announcing-mistral-7b/">Original</a>
    <h1>Mistral 7B, the most powerful language model for its size to date, Apache 2.0</h1>
    
    <div id="readability-page-1" class="page"><div><p>Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date.</p><h2 id="mistral-7b-in-short">Mistral 7B in short</h2><p>Mistral 7B is a 7.3B parameter model that:</p><ul><li>Outperforms Llama 2 13B on all benchmarks</li><li>Outperforms Llama 1 34B on many benchmarks</li><li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks</li><li>Uses Grouped-query attention (GQA) for faster inference</li><li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost</li></ul><p>We’re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.</p><ul><li><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar">Download it</a> and use it anywhere (including locally) with <a href="https://github.com/mistralai/mistral-src">our reference implementation</a></li><li>Deploy it on any cloud (AWS/GCP/Azure), using vLLM <a href="https://docs.mistral.ai/cloud-deployment/skypilot">inference server and skypilot</a></li><li>Use it on <a href="https://huggingface.co/mistralai">HuggingFace</a></li></ul><p>Mistral 7B is easy to fine-tune on any task. As a demonstration, we’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.</p><h3 id="performance-in-details">Performance in details</h3><p>We compared Mistral 7B to the Llama 2 family, and re-run all model evaluations ourselves for fair comparison.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_bars.png" alt="histograms"/>
<em>Performance of Mistral 7B and different Llama models on a wide range of benchmarks. For all metrics, all models were re-evaluated with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 13B on all metrics, and is on par with Llama 34B (since Llama 2 34B was not released, we report results on Llama 34B). It is also vastly superior in code and reasoning benchmarks.</em></p><p>The benchmarks are categorized by their themes:</p><ul><li>Commonsense Reasoning: 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.</li><li>World Knowledge: 5-shot average of NaturalQuestions and TriviaQA.</li><li>Reading Comprehension: 0-shot average of BoolQ and QuAC.</li><li>Math: Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4</li><li>Code: Average of 0-shot Humaneval and 3-shot MBPP</li><li>Popular aggregated results: 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)</li></ul><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_table.png" alt="table"/></p><p>An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput.
<img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_effective_sizes.png" alt="effective_sizes"/>
<em>Results on MMLU, Commonsense Reasoning, World Knowledge and Reading comprehension for Mistral 7B and Llama 2 (7B/13/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress).</em></p><p><strong>Note</strong>: Important differences between our evaluation and the LLaMA2 paper’s:</p><ul><li>For MBPP, we use the hand-verified subset</li><li>For TriviaQA, we do not provide Wikipedia contexts</li></ul><h3 id="flash-and-furious-attention-drift">Flash and Furious: Attention drift</h3><p>Mistral 7B uses a sliding window attention (SWA) mechanism (<a href="https://arxiv.org/pdf/1904.10509.pdf">Child et al.</a>, <a href="https://arxiv.org/pdf/2004.05150v2.pdf">Beltagy et al.</a>), in which each layer attends to the previous <code>4,096</code> hidden states.
The main improvement, and reason for which this was initially investigated, is a linear compute cost of O(sliding_window.seq_len). In practice, changes made to <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> and <a href="https://facebookresearch.github.io/xformers">xFormers</a> yield a 2x speed improvement for sequence length of 16k with a window of 4k. A huge thanks to Tri Dao and Daniel Haziza for helping include these changes on a tight schedule.</p><p>Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token <code>i</code> at layer <code>k</code> attends to tokens <code>[i-sliding_window, i]</code> at layer <code>k-1</code>. These tokens attended to tokens <code>[i-2*sliding_window, i]</code>. Higher layers have access to informations further in the past than what the attention patterns seems to entail.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/attention_local.png" alt="Local attention"/></p><p>Finally, a fixed attention span means we can limit our cache to a size of <code>sliding_window</code> tokens, using rotating buffers (read more in our <a href="https://github.com/mistralai/mistral-src">reference implementation repo</a>). This saves half of the cache memory for inference on sequence length of <code>8192</code>, without impacting model quality.</p><h2 id="acknowledgements">Acknowledgements</h2><p>We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the <a href="https://www.cineca.it">CINECA/EuroHPC</a> team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/facebookresearch/xformers">xFormers</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a> for their precious assistance in implementing new features and integrating their solutions into ours. We thank the teams of HuggingFace, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.</p></div></div>
  </body>
</html>
