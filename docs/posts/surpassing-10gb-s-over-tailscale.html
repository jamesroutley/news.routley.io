<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tailscale.com/blog/more-throughput/">Original</a>
    <h1>Surpassing 10Gb/S over Tailscale</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>Hi, it’s us again. You might remember us from <a href="https://tailscale.com/blog/throughput-improvements/">when we made</a> significant performance-related changes to <a href="https://git.zx2c4.com/wireguard-go/about/">wireguard-go</a>, the userspace <a href="https://www.wireguard.com/">WireGuard</a>® implementation that Tailscale uses. We’re releasing a set of changes that further improves client throughput on Linux. We <a href="https://github.com/WireGuard/wireguard-go/pull/75">intend to upstream</a> these changes to WireGuard as we did with the previous set of changes, which have <a href="https://github.com/WireGuard/wireguard-go/compare/2163620...f26efb6">since landed upstream</a>.</p>
<p><strong>With this new set of changes, Tailscale joins the 10Gb/s club on bare metal Linux, and wireguard-go pushes past (for now) the in-kernel WireGuard implementation on that hardware.</strong> How did we do it? Through UDP segmentation offload and checksum optimizations. You can experience these improvements in the current unstable Tailscale client release, and also in Tailscale v1.40, available in the coming days. Continue reading to learn more, or jump down to the <a href="#results">Results</a> section if you just want numbers.</p>
<h2 id="background">Background</h2>
<p>The data plane in Tailscale is built atop <a href="https://git.zx2c4.com/wireguard-go/about/">wireguard-go</a>, a userspace WireGuard implementation written in Go. wireguard-go acts as a pipeline, receiving packets from the operating system via a <a href="https://en.wikipedia.org/wiki/TUN/TAP">TUN</a> interface. It encrypts them, assuming a valid peer exists for their addressed destination, and sends them to a remote peer via a UDP socket. The flow in the opposite direction is similar. Packets from valid peers are decrypted after being read from a UDP socket, then are written back to the kernel’s TUN interface driver.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/wireguard-go-data-flow.svg"/>
    
</figure>

<p>The changes we made in v1.36 modified this pipeline, enabling packet vectors to flow end-to-end, rather than single packets. The techniques applied on both ends of the pipeline reduced the number of system calls per packet, and on the TUN side they reduced the cost of moving a packet through the kernel networking stack.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/improved-throughput.svg"/>
    
</figure>

<p><a href="https://tailscale.com/blog/throughput-improvements/#results">This greatly improved throughput</a>, and we have continued to build upon it with the changes we describe in this post.</p>
<h2 id="baseline">Baseline</h2>
<p><strong>Disclaimer about benchmarks</strong>: This post contains benchmarks! These benchmarks are reproducible at the time of writing, and we provide details about the environments we ran them in. But benchmark results tend to vary across environments, and they also tend to go stale as time progresses. Your mileage may vary.</p>
<p>Before getting into the details of what we changed, we need to record some baselines for later comparison. These benchmarks are conducted using <a href="https://github.com/esnet/iperf">iperf3</a>, as single stream TCP tests, with cubic congestion control. All hosts are running Ubuntu 22.04 with the latest available Linux kernel for that distribution.</p>
<p>We baselined throughput for <a href="https://git.zx2c4.com/wireguard-go/tree/?id=052af4a8072bbbd3bfe7edf46fe3c1b350f71f08">wireguard-go@052af4a</a> and in-kernel WireGuard. These tests were conducted between two pairs of hosts:</p>
<ul>
<li>2 x AWS c6i.8xlarge instance types</li>
<li>2 x “bare metal” servers powered by <a href="https://ark.intel.com/content/www/us/en/ark/products/134586/intel-core-i512400-processor-18m-cache-up-to-4-40-ghz.html">i5-12400</a> CPUs &amp; Mellanox MCX512A-ACAT NICs</li>
</ul>
<p>For consistency, the c6i.8xlarge instance type is the same we used in the <a href="https://tailscale.com/blog/throughput-improvements/">precursory blog post</a>. The instances are in the same region and availability zone:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@c6i-8xlarge-1:~$ ec2metadata <span>|</span> grep -E <span>&#39;instance-type:|availability-zone:&#39;</span>
</span></span><span><span>availability-zone: us-east-2b
</span></span><span><span>instance-type: c6i.8xlarge
</span></span><span><span>
</span></span><span><span>ubuntu@c6i-8xlarge-2:~$ ec2metadata <span>|</span> grep -E <span>&#39;instance-type:|availability-zone:&#39;</span>
</span></span><span><span>availability-zone: us-east-2b
</span></span><span><span>instance-type: c6i.8xlarge
</span></span><span><span>
</span></span><span><span>ubuntu@c6i-8xlarge-1:~$ ping 172.31.23.111 -c <span>5</span> -q
</span></span><span><span>PING 172.31.23.111 <span>(</span>172.31.23.111<span>)</span> 56<span>(</span>84<span>)</span> bytes of data.
</span></span><span><span>
</span></span><span><span>--- 172.31.23.111 ping statistics ---
</span></span><span><span><span>5</span> packets transmitted, <span>5</span> received, 0% packet loss, <span>time</span> 4094ms
</span></span><span><span>rtt min/avg/max/mdev <span>=</span> 0.109/0.126/0.168/0.022 ms
</span></span></code></pre></div><p>We’ve added the i5-12400 systems for a bare metal comparison with interfaces operating above 10Gb/s. The i5-12400 CPU is a modern (released Q1 2022) desktop-class chip, available for $183 USD at the time of writing. The Mellanox NICs are connected at 25Gb/s via a direct attach copper (DAC) cable:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@i5-12400-1:~$ lscpu <span>|</span> grep Model.name <span>&amp;&amp;</span> cpupower frequency-info -d <span>&amp;&amp;</span> cpupower frequency-info -p
</span></span><span><span>Model name:                  	12th Gen Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i5-12400
</span></span><span><span>analyzing CPU 0:
</span></span><span><span>  driver: intel_pstate
</span></span><span><span>analyzing CPU 0:
</span></span><span><span>  current policy: frequency should be within <span>800</span> MHz and 5.60 GHz.
</span></span><span><span>              	The governor <span>&#34;performance&#34;</span> may decide which speed to use
</span></span><span><span>              	within this range.
</span></span><span><span>jwhited@i5-12400-1:~$ sudo ethtool enp1s0f0np0 <span>|</span> grep Speed <span>&amp;&amp;</span> sudo ethtool -i enp1s0f0np0 <span>|</span> egrep <span>&#39;driver|^version&#39;</span>
</span></span><span><span>	Speed: 25000Mb/s
</span></span><span><span>driver: mlx5_core
</span></span><span><span>version: 5.15.0-69-generic
</span></span><span><span>
</span></span><span><span>jwhited@i5-12400-2:~$ lscpu <span>|</span> grep Model.name <span>&amp;&amp;</span> cpupower frequency-info -d <span>&amp;&amp;</span> cpupower frequency-info -p
</span></span><span><span>Model name:                  	12th Gen Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i5-12400
</span></span><span><span>analyzing CPU 0:
</span></span><span><span>  driver: intel_pstate
</span></span><span><span>analyzing CPU 0:
</span></span><span><span>  current policy: frequency should be within <span>800</span> MHz and 5.60 GHz.
</span></span><span><span>              	The governor <span>&#34;performance&#34;</span> may decide which speed to use
</span></span><span><span>              	within this range.
</span></span><span><span>jwhited@i5-12400-2:~$ sudo ethtool enp1s0f0np0 <span>|</span> grep Speed <span>&amp;&amp;</span> sudo ethtool -i enp1s0f0np0 <span>|</span> egrep <span>&#39;driver|^version&#39;</span>
</span></span><span><span>	Speed: 25000Mb/s
</span></span><span><span>driver: mlx5_core
</span></span><span><span>version: 5.15.0-69-generic
</span></span><span><span>
</span></span><span><span>jwhited@i5-12400-1:~$ ping 10.0.0.20 -c <span>5</span> -q
</span></span><span><span>PING 10.0.0.20 <span>(</span>10.0.0.20<span>)</span> 56<span>(</span>84<span>)</span> bytes of data.
</span></span><span><span>
</span></span><span><span>--- 10.0.0.20 ping statistics ---
</span></span><span><span><span>5</span> packets transmitted, <span>5</span> received, 0% packet loss, <span>time</span> 4078ms
</span></span><span><span>rtt min/avg/max/mdev <span>=</span> 0.008/0.035/0.142/0.053 ms
</span></span></code></pre></div><p>Now for the iperf3 baseline tests.</p>
<p>c6i.8xlarge over in-kernel WireGuard:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@c6i-8xlarge-1:~$ iperf3 -i <span>0</span> -c c6i-8xlarge-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux c6i-8xlarge-1 5.19.0-1022-aws <span>#23~22.04.1-Ubuntu SMP Fri Mar 17 15:38:24 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:56:53 GMT
</span></span><span><span>Connecting to host c6i-8xlarge-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: 3jzl3sa34hkbpwbmg4dbfh6aovbknnw7x5hn
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>51194</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.11 GBytes  2.67 Gbits/sec   <span>51</span>   1.00 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.11 GBytes  2.67 Gbits/sec   <span>51</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  3.11 GBytes  2.66 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 5.1% <span>(</span>0.3%u/4.8%s<span>)</span>, remote/receiver 11.2% <span>(</span>0.2%u/11.0%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>c6i.8xlarge over <a href="https://git.zx2c4.com/wireguard-go/tree/?id=052af4a8072bbbd3bfe7edf46fe3c1b350f71f08">wireguard-go@052af4a</a>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@c6i-8xlarge-1:~$ iperf3 -i <span>0</span> -c c6i-8xlarge-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux c6i-8xlarge-1 5.19.0-1022-aws <span>#23~22.04.1-Ubuntu SMP Fri Mar 17 15:38:24 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:55:42 GMT
</span></span><span><span>Connecting to host c6i-8xlarge-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: zlcrq3xqyr6cfmrtysrm42xcg3bbjzir3qob
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>54410</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  6.21 GBytes  5.34 Gbits/sec    <span>0</span>   3.15 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  6.21 GBytes  5.34 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  6.21 GBytes  5.31 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 8.6% <span>(</span>0.2%u/8.4%s<span>)</span>, remote/receiver 11.8% <span>(</span>0.6%u/11.2%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>i5-12400 over in-kernel WireGuard:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@i5-12400-1:~$ iperf3 -i <span>0</span> -c i5-12400-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux i5-12400-1 5.15.0-69-generic <span>#76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:41:44 GMT
</span></span><span><span>Connecting to host i5-12400-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: hqkn7s3scipxku5rzpcgqt4rakutkpwybtvx
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>48564</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  13.7 GBytes  11.8 Gbits/sec  <span>8725</span>    <span>753</span> KBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  13.7 GBytes  11.8 Gbits/sec  <span>8725</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  13.7 GBytes  11.7 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 26.3% <span>(</span>0.1%u/26.2%s<span>)</span>, remote/receiver 17.4% <span>(</span>0.5%u/16.9%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>i5-12400 over <a href="https://git.zx2c4.com/wireguard-go/tree/?id=052af4a8072bbbd3bfe7edf46fe3c1b350f71f08">wireguard-go@052af4a</a>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@i5-12400-1:~$ iperf3 -i <span>0</span> -c i5-12400-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux i5-12400-1 5.15.0-69-generic <span>#76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:39:22 GMT
</span></span><span><span>Connecting to host i5-12400-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: ohzzlzkcvnk45ya32vm75ezir6njydqwipkl
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>52486</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  9.74 GBytes  8.36 Gbits/sec  <span>507</span>   3.01 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  9.74 GBytes  8.36 Gbits/sec  <span>507</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  9.74 GBytes  8.32 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 11.7% <span>(</span>0.1%u/11.6%s<span>)</span>, remote/receiver 6.5% <span>(</span>0.2%u/6.3%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>With the baselines captured, let’s look at some profiling data to understand where we may be bottlenecked.</p>
<h2 id="linux-perf-and-flame-graphs">Linux perf and flame graphs</h2>
<p>The <a href="https://www.brendangregg.com/flamegraphs.html">flame graphs</a> below were rendered from <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf data</a>. They represent the amount of CPU time spent for a given function/stack. The wider the function, the more expensive it (and/or its children) are. These are interactive; you can click to zoom and hover to see percentages.</p>
<p>This first graph is from the iperf3 sender:</p>
<figure data-flamegraph="" data-flamegraph-json="sender.json" data-flamegraph-title="Sender" data-flamegraph-min-frame-size="12"></figure>

<p>Notably, more time is being spent sending UDP packets than encrypting their payloads. Let’s take a look at the receiver:</p>
<figure data-flamegraph="" data-flamegraph-json="receiver.json" data-flamegraph-title="Receiver" data-flamegraph-min-frame-size="12"></figure>

<p>The receiver looks fairly similar, with UDP reception being nearly equal in time spent relative to decryption.</p>
<p>We are using the <code>{send,recv}mmsg()</code> (two m’s) system calls, which help to amortize the cost of making a syscall. However, on the kernel side of the system call, we see <code>{send,recv}mmsg()</code> calls into <code>{send,recv}msg()</code> (single m). This means that we still pay the cost of traversing the kernel networking stack for every single packet, because the kernel side simply iterates through the batch.</p>
<p>On the TUN side of wireguard-go, we make use of <a href="https://tailscale.com/blog/throughput-improvements/#tcp-segmentation-offload-tso">TCP segmentation offload (TSO)</a> and generic receive offload (GRO), which enable multiple TCP segments to pass through the kernel stack as a single segment:</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/transmit-tuntso-udpsendmmsg.svg"/>
    
</figure>


    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/receive-udprecvmmsg-tungro.svg"/>
    
</figure>

<p>What we need is something similar, but for UDP. Enter UDP generic segmentation offload.</p>
<h2 id="udp-generic-segmentation-offload-gso">UDP generic segmentation offload (GSO)</h2>
<p>UDP GSO enables the kernel to delay segmentation of a batch of UDP datagrams in a similar fashion to the TCP variant, reducing the CPU cycles per byte cost of traversing the networking stack. Linux support was authored by Willem de Bruijn and <a href="https://github.com/torvalds/linux/commit/cb586c63e3fc5b227c51fd8c4cb40b34d3750645">introduced into the kernel in v4.18</a>. UDP GSO was <a href="https://www.youtube.com/watch?v=ccUeG1dAhbw">propelled by the adoption of QUIC in the datacenter</a>, but its benefits are not limited to QUIC. It is best described by part of its summary commit message:</p>
<blockquote>
<p>Segmentation offload reduces cycles/byte for large packets by
amortizing the cost of protocol stack traversal.</p>
<p>This patchset implements GSO for UDP. A process can concatenate and
submit multiple datagrams to the same destination in one send call
by setting socket option SOL_UDP/UDP_SEGMENT with the segment size,
or passing an analogous cmsg at send time.</p>
<p>The stack will send the entire large (up to network layer max size)
datagram through the protocol layer. At the GSO layer, it is broken
up in individual segments. All receive the same network layer header
and UDP src and dst port. All but the last segment have the same UDP
header, but the last may differ in length and checksum.”</p>
</blockquote>
<p>After implementing UDP GSO on the UDP socket side of wireguard-go, the transmit direction now looks like this:</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/transmit-tuntso-udpsendmmsg-udpgso.svg"/>
    
</figure>

<p>But what about the receive path? It would be ideal to optimize both directions. Paolo Abeni authored UDP generic receive offload (GRO) support, and it was <a href="https://github.com/torvalds/linux/commit/cab6949bf70a68ee5aada5f1973c0bb906d354cf">introduced into the Linux kernel in v5.0</a>. With UDP GRO the receive direction now looks like this:</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/receive-udprecvmmsg-udpgro-tungro.svg"/>
    
</figure>

<p><a href="https://git.kernel.org/pub/scm/docs/man-pages/man-pages.git/commit/man7/udp.7?id=806eabd74910447f21005160e90957bde4db0183">Updates to the UDP man page</a> for these new features eventually arrived, in which an important requirement for UDP GSO is described:</p>
<blockquote>
<p>Segmentation offload depends on checksum offload, as datagram checksums are computed after segmentation.</p>
</blockquote>
<p>Checksum offload is widely supported across ethernet devices today. It also reduces the cost of the kernel networking stack, as ethernet devices tend to have specialized hardware that is very efficient at computing <a href="https://www.rfc-editor.org/rfc/rfc1071.html">RFC1071 checksums</a>. It’s often paired with segmentation offload, which as the man page describes, may need to be performed by the layer performing segmentation.</p>
<p>In fact, we already have to offload checksumming inside of the TCP segmentation offload implementation in wireguard-go. The kernel hands us a “monster segment,” which we are responsible for segmenting. This includes calculating checksums for the individual segments.</p>
<h2 id="tun-checksum-offload">TUN checksum offload</h2>
<p>If we look back at the flame graphs we’ll find the function responsible for computing the internet checksum as part of the existing TCP segmentation offloading (<code>tun.checksum()</code>, inlined with <code>tun.checksumNoFold()</code>). It contributes to a modest percentage of perf samples (6.6% on the sender) before making any changes. After reducing the cost of the kernel’s UDP stack, the relative cost of TUN checksum offload increases with throughput, and it becomes our next candidate to optimize.</p>
<p>The existing <code>tun.checksumNoFold()</code> function was this:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>// TODO: Explore SIMD and/or other assembly optimizations.
</span></span></span><span><span><span></span><span>func</span> <span>checksumNoFold</span><span>(</span><span>b</span> <span>[]</span><span>byte</span><span>,</span> <span>initial</span> <span>uint64</span><span>)</span> <span>uint64</span> <span>{</span>
</span></span><span><span>	<span>ac</span> <span>:=</span> <span>initial</span>
</span></span><span><span>	<span>i</span> <span>:=</span> <span>0</span>
</span></span><span><span>	<span>n</span> <span>:=</span> <span>len</span><span>(</span><span>b</span><span>)</span>
</span></span><span><span>	<span>for</span> <span>n</span> <span>&gt;=</span> <span>4</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>i</span> <span>:</span> <span>i</span><span>+</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>n</span> <span>-=</span> <span>4</span>
</span></span><span><span>		<span>i</span> <span>+=</span> <span>4</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>for</span> <span>n</span> <span>&gt;=</span> <span>2</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint16</span><span>(</span><span>b</span><span>[</span><span>i</span> <span>:</span> <span>i</span><span>+</span><span>2</span><span>]))</span>
</span></span><span><span>		<span>n</span> <span>-=</span> <span>2</span>
</span></span><span><span>		<span>i</span> <span>+=</span> <span>2</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>n</span> <span>==</span> <span>1</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>b</span><span>[</span><span>i</span><span>])</span> <span>&lt;&lt;</span> <span>8</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>return</span> <span>ac</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>It is responsible for summing the bytes in <code>b</code> with <code>initial</code>, and returning the sum as a uint64. Internet checksums are uint16 values, which the return from this function gets folded into. But why are we returning a uint64 to begin with? Because there is already one existing optimization present here. We sum 4 bytes at a time, instead of 2. This can cut checksum cost in half. RFC 1071 describes the mathematical properties that enable this optimization along with the concept of folding:</p>
<blockquote>
<p>On machines that have word-sizes that are multiples of 16 bits,
it is possible to develop even more efficient implementations.
Because addition is associative, we do not have to sum the
integers in the order they appear in the message.  Instead we
can add them in “parallel” by exploiting the larger word size.</p>
<p>To compute the checksum in parallel, simply do a 1’s complement
addition of the message using the native word size of the
machine.  For example, on a 32-bit machine we can add 4 bytes at
a time: [A,B,C,D]+’… When the sum has been computed, we “fold”
the long sum into 16 bits by adding the 16-bit segments.  Each
16-bit addition may produce new end-around carries that must be
added.</p>
</blockquote>
<p>There is one more low-hanging optimization available to us — unwinding the loops! Checking the length of <code>b</code> after every summation is expensive overhead, especially for larger packets. RFC 1071 also describes this optimization:</p>
<blockquote>
<p>To reduce the loop overhead, it is often useful to “unwind” the
inner sum loop, replicating a series of addition commands within
one loop traversal.  This technique often provides significant
savings, although it may complicate the logic of the program
considerably.</p>
</blockquote>
<p>After applying some unwinding we end up with this function, which has some repetitive bits omitted for the sake of brevity:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>// TODO: Explore SIMD and/or other assembly optimizations.
</span></span></span><span><span><span>// TODO: Test native endian loads. See RFC 1071 section 2 part B.
</span></span></span><span><span><span></span><span>func</span> <span>checksumNoFold</span><span>(</span><span>b</span> <span>[]</span><span>byte</span><span>,</span> <span>initial</span> <span>uint64</span><span>)</span> <span>uint64</span> <span>{</span>
</span></span><span><span>    <span>ac</span> <span>:=</span> <span>initial</span>
</span></span><span><span>	
</span></span><span><span>    <span>for</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>128</span> <span>{</span>
</span></span><span><span>        <span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[:</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>4</span><span>:</span><span>8</span><span>]))</span> 
</span></span><span><span>		<span>// (omitted) continues to 128 
</span></span></span><span><span><span></span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>128</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>64</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[:</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>4</span><span>:</span><span>8</span><span>]))</span> 
</span></span><span><span>		<span>// (omitted) continues to 64 
</span></span></span><span><span><span></span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>64</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>32</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[:</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>4</span><span>:</span><span>8</span><span>]))</span> 
</span></span><span><span>		<span>// (omitted) continues to 32 
</span></span></span><span><span><span></span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>32</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>16</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[:</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>4</span><span>:</span><span>8</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>8</span><span>:</span><span>12</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>12</span><span>:</span><span>16</span><span>]))</span>
</span></span><span><span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>16</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>8</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[:</span><span>4</span><span>]))</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>[</span><span>4</span><span>:</span><span>8</span><span>]))</span>
</span></span><span><span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>8</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>4</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>b</span><span>))</span>
</span></span><span><span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>4</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>&gt;=</span> <span>2</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint16</span><span>(</span><span>b</span><span>))</span>
</span></span><span><span>		<span>b</span> <span>=</span> <span>b</span><span>[</span><span>2</span><span>:]</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>if</span> <span>len</span><span>(</span><span>b</span><span>)</span> <span>==</span> <span>1</span> <span>{</span>
</span></span><span><span>		<span>ac</span> <span>+=</span> <span>uint64</span><span>(</span><span>b</span><span>[</span><span>0</span><span>])</span> <span>&lt;&lt;</span> <span>8</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	
</span></span><span><span>    <span>return</span> <span>ac</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>This optimization reduced the run time of the function by ~57%, as evidenced by the output of <a href="https://pkg.go.dev/golang.org/x/perf/cmd/benchstat">benchstat</a>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>$ benchstat old.txt new.txt
</span></span><span><span>goos: linux
</span></span><span><span>goarch: amd64
</span></span><span><span>pkg: golang.zx2c4.com/wireguard/tun
</span></span><span><span>cpu: 12th Gen Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i5-12400
</span></span><span><span>                 │   old.txt    │               new.txt               │
</span></span><span><span>                 │    sec/op    │   sec/op     vs base                │
</span></span><span><span>Checksum/64-12     10.670n ± 2%   4.769n ± 0%  -55.30% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/128-12    19.665n ± 2%   8.032n ± 0%  -59.16% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/256-12     37.68n ± 1%   16.06n ± 0%  -57.37% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/512-12     76.61n ± 3%   32.13n ± 0%  -58.06% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/1024-12   160.55n ± 4%   64.25n ± 0%  -59.98% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/1500-12   231.05n ± 7%   94.12n ± 0%  -59.26% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/2048-12    309.5n ± 3%   128.5n ± 0%  -58.48% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/4096-12    603.8n ± 4%   257.2n ± 0%  -57.41% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/8192-12   1185.0n ± 3%   515.5n ± 0%  -56.50% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/9000-12   1328.5n ± 5%   564.8n ± 0%  -57.49% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>Checksum/9001-12   1340.5n ± 3%   564.8n ± 0%  -57.87% <span>(</span><span>p</span><span>=</span>0.000 <span>n</span><span>=</span>10<span>)</span>
</span></span><span><span>geomean             185.3n        77.99n       -57.92%
</span></span></code></pre></div><p>This optimization also translated to a 10% throughput improvement for some of the environments we tested. Now, on to the overall results.</p>
<h2 id="results">Results</h2>
<p>Applying UDP segmentation offload, UDP receive coalescing, and checksum unwinding resulted in significant throughput improvements for wireguard-go, and so also in the Tailscale client.</p>
<p>wireguard-go (c6i.8xlarge) with UDP GSO, GRO, and checksum unwinding:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@c6i-8xlarge-1:~$ iperf3 -i <span>0</span> -c c6i-8xlarge-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux c6i-8xlarge-1 5.19.0-1022-aws <span>#23~22.04.1-Ubuntu SMP Fri Mar 17 15:38:24 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:58:19 GMT
</span></span><span><span>Connecting to host c6i-8xlarge-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: efpxfeszrxxsjdo643josagi2akj3f2lcmdh
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>35218</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  8.53 GBytes  7.32 Gbits/sec    <span>0</span>   3.14 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  8.53 GBytes  7.32 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  8.53 GBytes  7.29 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 10.4% <span>(</span>0.2%u/10.2%s<span>)</span>, remote/receiver 20.8% <span>(</span>0.8%u/20.0%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>wireguard-go (i5-12400) with UDP GSO, GRO, and checksum unwinding:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@i5-12400-1:~$ iperf3 -i <span>0</span> -c i5-12400-2-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux i5-12400-1 5.15.0-69-generic <span>#76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Wed, <span>12</span> Apr <span>2023</span> 23:42:52 GMT
</span></span><span><span>Connecting to host i5-12400-2-wg, port <span>5201</span>
</span></span><span><span>      Cookie: q6hm54yvcbxdrsnh2foexkunzdsdudwy5wfj
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.1 port <span>43006</span> connected to 10.9.9.2 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  15.2 GBytes  13.0 Gbits/sec  <span>1212</span>   3.01 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  15.2 GBytes  13.0 Gbits/sec  <span>1212</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  15.2 GBytes  13.0 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 18.9% <span>(</span>0.3%u/18.6%s<span>)</span>, remote/receiver 4.0% <span>(</span>0.2%u/3.8%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div>
    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/results.svg" alt="Single TCP stream iperf3 test, OS: Ubuntu 22.04, Kernel: 5.15"/>
    
</figure>

<p><strong>With these performance improvements, Tailscale joins the 10Gb/s club on bare metal Linux, and wireguard-go pushes past (for now) the in-kernel WireGuard implementation on that hardware.</strong> The AWS c6i.8xlarge instances hit a wall at 7.3Gb/s that appears to be an artificial limit of the underlay network. We were unable to exceed a similar bitrate for UDP packets with no WireGuard involved.</p>
<h2 id="note-about-udp-gso-in-hardware">Note about UDP GSO in-hardware</h2>
<p>Just like TSO is the “in-hardware” cousin of GSO, UDP GSO has a similar variant, listed as <code>tx-udp-segmentation</code> by ethtool:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@i5-12400-1:~$ ethtool -k enp1s0f0np0 <span>|</span> grep udp-seg
</span></span><span><span>tx-udp-segmentation: on
</span></span></code></pre></div><p>It extends the delayed segmentation of datagrams through to the device, and our transmit direction flow now looks like this:</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/more-throughput/transmit-tuntso-udpsendmmsg-hardwareudpgso.svg"/>
    
</figure>

<p>This hardware support exists in the 25G NICs we used on the i5-12400 systems. It did improve throughput slightly (5%) for that hardware, but it really shined for some of the older generation CPUs. For one example, here’s an <a href="https://www.intel.com/content/www/us/en/products/sku/65732/intel-xeon-processor-e31230-v2-8m-cache-3-30-ghz/specifications.html">E3-1230-V2</a> (released Q2 2012) system with the same NIC.</p>
<p>E3-1230-V2 over <a href="https://git.zx2c4.com/wireguard-go/tree/?id=052af4a8072bbbd3bfe7edf46fe3c1b350f71f08">wireguard-go@052af4a</a>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@e3-1230-v2:~$ iperf3 -i <span>0</span> -c i5-12400-1-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux e3-1230-v2 5.15.0-67-generic <span>#74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>13</span> Apr <span>2023</span> 02:27:23 GMT
</span></span><span><span>Connecting to host i5-12400-1-wg, port <span>5201</span>
</span></span><span><span>      Cookie: pcfb7wqlh653l3r6r4oxxjenfxh4hdlqowho
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.3 port <span>35310</span> connected to 10.9.9.1 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.91 GBytes  3.36 Gbits/sec    <span>0</span>   3.09 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.91 GBytes  3.36 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  3.91 GBytes  3.34 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 10.6% <span>(</span>0.4%u/10.2%s<span>)</span>, remote/receiver 2.0% <span>(</span>0.0%u/2.0%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>E3-1230-V2 over wireguard-go with UDP GSO, GRO, checksum unwinding, and tx-udp-segmentation off:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@e3-1230-v2:~$ sudo ethtool -K enp1s0f0np0 tx-udp-segmentation off
</span></span><span><span>jwhited@e3-1230-v2:~$ iperf3 -i <span>0</span> -c i5-12400-1-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux e3-1230-v2 5.15.0-67-generic <span>#74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>13</span> Apr <span>2023</span> 02:28:12 GMT
</span></span><span><span>Connecting to host i5-12400-1-wg, port <span>5201</span>
</span></span><span><span>      Cookie: 6rtbzadj2on7igc7bt2hfhphdg2ebfgwxzim
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.3 port <span>58036</span> connected to 10.9.9.1 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  5.65 GBytes  4.86 Gbits/sec    <span>0</span>   3.14 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  5.65 GBytes  4.86 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  5.65 GBytes  4.84 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 19.1% <span>(</span>0.6%u/18.5%s<span>)</span>, remote/receiver 1.9% <span>(</span>0.1%u/1.8%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>E3-1230-V2 over wireguard-go with UDP GSO, GRO, checksum unwinding, and tx-udp-segmentation on:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>jwhited@e3-1230-v2:~$ sudo ethtool -K enp1s0f0np0 tx-udp-segmentation on
</span></span><span><span>jwhited@e3-1230-v2:~$ iperf3 -i <span>0</span> -c i5-12400-1-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux e3-1230-v2 5.15.0-67-generic <span>#74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>13</span> Apr <span>2023</span> 02:28:58 GMT
</span></span><span><span>Connecting to host i5-12400-1-wg, port <span>5201</span>
</span></span><span><span>      Cookie: lod6fulhls3wvtqy7uoakmldifdtcc3nbvfv
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.3 port <span>46724</span> connected to 10.9.9.1 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  7.68 GBytes  6.59 Gbits/sec    <span>2</span>   3.12 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  7.68 GBytes  6.59 Gbits/sec    <span>2</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  7.68 GBytes  6.56 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 25.6% <span>(</span>1.0%u/24.6%s<span>)</span>, remote/receiver 8.0% <span>(</span>0.3%u/7.7%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p><strong>That’s a 35% increase in throughput with hardware UDP segmentation offload enabled, and nearly a 2x increase over the baseline.</strong></p>
<h2 id="conclusions">Conclusions</h2>
<p>Continuing on our journey to improve packet processing overhead led us to discover and use relatively young Linux kernel features. We made use of UDP generic segmentation offload, UDP generic receive offload, and checksum loop unwinding, enabling us to reach a new milestone — surpassing 10Gb/s over Tailscale.</p>
<p>Thanks to <a href="https://github.com/sailorfrag">Adrian Dewhurst</a> for his detailed review and feedback, to <a href="https://www.zx2c4.com/">Jason A. Donenfeld</a> for his ongoing review of our patches, and to our designer <a href="https://dannypagano.com/">Danny Pagano</a> for the illustrations.</p>
    </div></div>
  </body>
</html>
