<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.philschmid.de/mini-deepseek-r1">Original</a>
    <h1>Mini-R1: Reproduce DeepSeek R1 &#34;Aha Moment&#34;</h1>
    
    <div id="readability-page-1" class="page"><div><p>The release of Deepseek R1 shocked the industry. Why? Well, DeepSeek-R1 is an open model that rivals OpenAI&#39;s o1 in complex reasoning tasks, introduced using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach. They not only released the model, but also a research paper on how they did it.</p>
<p>In the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2501.12948">paper</a> they described an &#34;aha moment&#34; when using pure RL to train the model. During this phase, DeepSeek-R1-Zero (the first test of DeepSeek-R1) learns to allocate more thinking time to a problem by reevaluating its initial approach without any human feedback or data describing how to do it.  They describe this as an &#34;aha moment&#34; as:</p>
<blockquote>
<p>This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.</p>
</blockquote>
<p>In this blog post we want to recreate the small &#34;aha moment&#34; of DeepSeek-R1 using Group Relative Policy Optimization (GRPO) and the Countdown Game. We will train an open model using reinforcement learning trying to teach it self-verification and search abilities all on its own to solve the Countdown Game.
The Countdown game is a numbers puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, ×, ÷) to reach or get as close as possible to a target number.</p>
<div><pre><code>Target Number: 952
Available Numbers: 25, 50, 75, 100, 3, 6

(100 × (3 × 3)) + (50 + 6 / 3) = 952
</code></pre></div>
<p>The blog post includes an interactive code which you can run in a Jupyter Notebook on how to train a model using GRPO and Q-Lora. This is a great way to learn how to use TRL and GRPO, but it is very slow and requires a lot of compute. Additionally, I added a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py">script</a> and instructions to run the training on Node with multiple GPUs or a SLURM cluster.</p>
<ol>
<li><a href="#1-setup-the-development-environment">Setup the development environment</a></li>
<li><a href="#2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game">Generate training samples with reasoning prefix from the Countdown Game</a></li>
<li><a href="#3-train-the-model-using-grpo-educational-part">Train the model using GRPO (Educational part)</a></li>
<li><a href="#4-distributed-training-example-for-grpo-using-deepspeed-and-vllm">Distributed Training example for GRPO using Deepspeed and vLLM</a></li>
<li><a href="#5-results-and-training-observations">Results and Training Observations</a></li>
</ol>
<p><em>Note: This blog is inspired by <a target="_blank" rel="noopener noreferrer" href="https://x.com/jiayi_pirate/status/1882839370505621655">Jiayi Pan</a> who initially explored the idea and proofed it with a small model.</em></p>
<p>But Before we start, let&#39;s take a look at the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">Group Relative Policy Optimization (GRPO)</a> and understand how it works.</p>
<p><strong>Group Relative Policy Optimization (GRPO)</strong></p>
<p>Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs. It was introduced in the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> paper in the context of mathematical reasoning. GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model. Instead, it estimates baselines from group scores, reducing memory usage and computational overhead. GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness.</p>
<ol>
<li><strong>Sampling</strong>: Generate multiple outputs for each prompt using the current policy</li>
<li><strong>Reward Scoring</strong>: Each generation is scored using a reward function, could be (rule-based or outcome-based)</li>
<li><strong>Advantage Calculation</strong>: The average reward of the generated outputs is used as a baseline. The advantage of each solution within the group is then computed relative to this baseline. The reward is normalized within a group.</li>
<li><strong>Policy Optimization</strong>: The policy tries to maximize the GRPO objective, which includes the calculated advantages and a KL divergence term. This is different from how PPO implements the KL term within the reward.</li>
</ol>
<p><img src="https://www.philschmid.de/static/blog/mini-deepseek-r1/grpo.png" alt="grpo.png"/></p>
<h2 id="1-setup-the-development-environment"><a href="#1-setup-the-development-environment" aria-hidden="true" tabindex="-1"><span></span></a>1. Setup the development environment</h2>
<p>Our first step is to install Hugging Face Libraries and Pytorch, vllm, and trl, transformers and datasets. If you haven&#39;t heard of trl yet, don&#39;t worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span># Install Pytorch &amp; other libraries, make sure to match your GPU driver version</span></span>
<span data-line=""><span>%</span><span>pip install </span><span>&#34;torch==2.5.1&#34;</span><span> tensorboard </span><span>&#34;setuptools&lt;71.0.0&#34;</span><span>  --</span><span>index</span><span>-</span><span>url https:</span><span>//</span><span>download.pytorch.org</span><span>/</span><span>whl</span><span>/</span><span>cu121</span></span>
<span data-line=""> </span>
<span data-line=""><span># Install flash-attn</span></span>
<span data-line=""><span>%</span><span>pip install flash</span><span>-</span><span>attn </span></span>
<span data-line=""> </span>
<span data-line=""><span># Install Hugging Face libraries</span></span>
<span data-line=""><span>%</span><span>pip install  </span><span>--</span><span>upgrade \</span></span>
<span data-line=""><span>  &#34;transformers==4.48.1&#34;</span><span> \</span></span>
<span data-line=""><span>  &#34;datasets==3.1.0&#34;</span><span> \</span></span>
<span data-line=""><span>  &#34;accelerate==1.3.0&#34;</span><span> \</span></span>
<span data-line=""><span>  &#34;hf-transfer==0.1.9&#34;</span><span> \</span></span>
<span data-line=""><span>  &#34;deepspeed==0.15.4&#34;</span><span> \</span></span>
<span data-line=""><span>  &#34;trl==0.14.0&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span># install vLLM </span></span>
<span data-line=""><span>%</span><span>pip install </span><span>&#34;vllm==0.7.0&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>## IMPORTANT: If you want to run the notebook and the interactive cells you also need to install the following libraries:</span></span>
<span data-line=""><span># But first read it the blog post and then decide as they might conflict with the libraries for distributed training. </span></span>
<span data-line=""><span># %pip install &#34;peft==0.14.0&#34; &#34;bitsandbytes==0.45.0&#34;</span></span>
<span data-line=""> </span></code></pre></div></figure>
<p><em>Note: you may need to restart the kernel to use updated packages.</em></p>
<p>We will use the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/models">Hugging Face Hub</a> as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/join">Hugging Face</a> for this. After you have an account, we will use the <code>login</code> util from the <code>huggingface_hub</code> package to log into our account and store our token (access key) on the disk.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> huggingface_hub </span><span>import</span><span> login</span></span>
<span data-line=""> </span>
<span data-line=""><span>login(</span><span>token</span><span>=</span><span>&#34;&#34;</span><span>, </span><span>add_to_git_credential</span><span>=</span><span>True</span><span>) </span><span># ADD YOUR TOKEN HERE</span></span></code></pre></div></figure>
<h2 id="2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game"><a href="#2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game" aria-hidden="true" tabindex="-1"><span></span></a>2. Generate training samples with reasoning prefix from the Countdown Game</h2>
<p>We are going to use the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4">Jiayi-Pan/Countdown-Tasks-3to4</a> dataset, which contains samples with 3 to 4 numbers and solutions.</p>
<p>As Model we are going to use <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">Qwen/Qwen2.5-3B-Instruct</a> which is a 3B parameter instruction tuned model. This makes it easier to showcase the &#34;aha moment&#34; as it already follows the prompt format. But you can use the base version of Qwen or other models as well. <a target="_blank" rel="noopener noreferrer" href="https://x.com/jiayi_pirate/status/1882839487417561307">Jiayi-Pan</a> explored that the model needs to have a certain quality to be able to learn the reasoning process, starting with &gt; 1.5B parameters.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> transformers </span><span>import</span><span> AutoTokenizer</span></span>
<span data-line=""><span>from</span><span> datasets </span><span>import</span><span> load_dataset</span></span>
<span data-line=""> </span>
<span data-line=""><span># Load dataset from Hugging Face Hub</span></span>
<span data-line=""><span>dataset_id </span><span>=</span><span> &#34;Jiayi-Pan/Countdown-Tasks-3to4&#34;</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> load_dataset(dataset_id, </span><span>split</span><span>=</span><span>&#34;train&#34;</span><span>)</span></span>
<span data-line=""><span># select a random subset of 50k samples</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> dataset.shuffle(</span><span>seed</span><span>=</span><span>42</span><span>).select(</span><span>range</span><span>(</span><span>50000</span><span>))</span></span>
<span data-line=""> </span>
<span data-line=""><span># Load tokenizer from Hugging Face Hub to format the dataset to our &#34;r1&#34; prompt </span></span>
<span data-line=""><span>tokenizer </span><span>=</span><span> AutoTokenizer.from_pretrained(</span><span>&#34;Qwen/Qwen2.5-3B-Instruct&#34;</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span># gemerate r1 prompt with a prefix for the model to already start with the thinking process</span></span>
<span data-line=""><span>def</span><span> generate_r1_prompt</span><span>(numbers, target):</span></span>
<span data-line=""><span>    r1_prefix </span><span>=</span><span> [{</span></span>
<span data-line=""><span>        &#34;role&#34;</span><span>: </span><span>&#34;system&#34;</span><span>,</span></span>
<span data-line=""><span>        &#34;content&#34;</span><span>: </span><span>&#34;You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.&#34;</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>      { </span></span>
<span data-line=""><span>        &#34;role&#34;</span><span>: </span><span>&#34;user&#34;</span><span>,</span></span>
<span data-line=""><span>        &#34;content&#34;</span><span>: </span><span>f</span><span>&#34;Using the numbers </span><span>{</span><span>numbers</span><span>}</span><span>, create an equation that equals </span><span>{</span><span>target</span><span>}</span><span>. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in &lt;think&gt; &lt;/think&gt; tags. And return the final equation and answer in &lt;answer&gt; &lt;/answer&gt; tags, for example &lt;answer&gt; (1 + 2) / 3 = 1 &lt;/answer&gt;.&#34;</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>      {</span></span>
<span data-line=""><span>        &#34;role&#34;</span><span>: </span><span>&#34;assistant&#34;</span><span>,</span></span>
<span data-line=""><span>        &#34;content&#34;</span><span>: </span><span>&#34;Let me solve this step by step.</span><span>\n</span><span>&lt;think&gt;&#34;</span></span>
<span data-line=""><span>      }]</span></span>
<span data-line=""><span>    return</span><span> {</span><span>&#34;prompt&#34;</span><span>: tokenizer.apply_chat_template(r1_prefix, </span><span>tokenize</span><span>=</span><span>False</span><span>, </span><span>continue_final_message</span><span>=</span><span>True</span><span>), </span><span>&#34;target&#34;</span><span>: target}</span></span>
<span data-line=""> </span>
<span data-line=""><span># convert our dataset to the r1 prompt</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> dataset.map(</span><span>lambda</span><span> x: generate_r1_prompt(x[</span><span>&#34;nums&#34;</span><span>], x[</span><span>&#34;target&#34;</span><span>]))</span></span>
<span data-line=""> </span>
<span data-line=""><span># split the dataset into train and test</span></span>
<span data-line=""><span>train_test_split </span><span>=</span><span> dataset.train_test_split(</span><span>test_size</span><span>=</span><span>0.1</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>train_dataset </span><span>=</span><span> train_test_split[</span><span>&#34;train&#34;</span><span>]</span></span>
<span data-line=""><span>test_dataset </span><span>=</span><span> train_test_split[</span><span>&#34;test&#34;</span><span>]</span></span></code></pre></div></figure>
<h2 id="3-train-the-model-using-grpo-educational-part"><a href="#3-train-the-model-using-grpo-educational-part" aria-hidden="true" tabindex="-1"><span></span></a>3. Train the model using GRPO (Educational part)</h2>
<p><em>Note: Section 3 is shows the basic on how to use TRL and GRPO. If you want to run the interactive cells you need to install <code>bitsandbytes</code> and <code>peft</code> as they are required for the <code>Trainer</code> class. This section is mostly for educational purposes.</em></p>
<p>TRL supports Group Relative Policy Optimization (GRPO) through a dedicated <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/trl/main/en/grpo_trainer">GRPOTrainer</a> for aligning LLMs from preference data, as described in <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>. The <code>GRPOTrainer</code> is a subclass of the <code>Trainer</code> from the <code>transformers</code> library and supports all the same features, including logging, checkpointing, distributed training, and parameter efficient fine-tuning (PEFT).</p>
<p>The <code>GRPOTrainer</code> supports generic Outcome Reward Models (ORM) and custom reward functions, that can be used to implement Rule-Based Reward Models. In the Deepseek R1 paper they implemented Rule-Based Reward Models to verify the correctness of the generated solutions. In our exmaple we are going to do a similar approach, where we will create 2 reward functions that:</p>
<ol>
<li><strong>Format Reward</strong>: Checks if the generated format is correct <code>&lt;think&gt; [thinking] &lt;/think&gt;&lt;answer&gt; [answer] &lt;/answer&gt;</code></li>
<li><strong>Accuracy Reward</strong>: Extracts the equation from the <code>&lt;answer&gt;</code> tag and evaluates it against the target and if every number is used once.</li>
</ol>
<p><em>Note: Correct <code>&lt;answer&gt;</code> in our example includes the equation, for example <code>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;</code></em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>import</span><span> re</span></span>
<span data-line=""> </span>
<span data-line=""><span>def</span><span> format_reward_func</span><span>(completions, target, </span><span>**</span><span>kwargs):</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    Format: &lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;</span></span>
<span data-line=""><span>    Args:</span></span>
<span data-line=""><span>        completions (list[str]): Generated outputs</span></span>
<span data-line=""><span>        target (list[str]): Expected answers</span></span>
<span data-line=""><span>      </span></span>
<span data-line=""><span>      Returns:</span></span>
<span data-line=""><span>          list[float]: Reward scores</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    rewards </span><span>=</span><span> []</span></span>
<span data-line=""> </span>
<span data-line=""><span>    for</span><span> completion, gt </span><span>in</span><span> zip</span><span>(completions, target):</span></span>
<span data-line=""> </span>
<span data-line=""><span>      try</span><span>:</span></span>
<span data-line=""><span>        # add synthetic &lt;think&gt; as its already part of the prompt and prefilled for the assistant to more easily match the regex</span></span>
<span data-line=""><span>        completion </span><span>=</span><span> &#34;&lt;think&gt;&#34;</span><span> +</span><span> completion        </span></span>
<span data-line=""><span>        # Check if the format is correct</span></span>
<span data-line=""><span>        regex </span><span>=</span><span> r</span><span>&#34;</span><span>^</span><span>&lt;think&gt;</span><span>([</span><span>^</span><span>&lt;]</span><span>*</span><span>(?:</span><span>&lt;</span><span>(?!</span><span>/</span><span>?</span><span>think&gt;</span><span>)</span><span>[</span><span>^</span><span>&lt;]</span><span>*</span><span>)</span><span>*</span><span>)</span><span>&lt;</span><span>\/</span><span>think&gt;</span><span>\n</span><span>&lt;answer&gt;</span><span>([\s\S]</span><span>*?</span><span>)</span><span>&lt;</span><span>\/</span><span>answer&gt;</span><span>$</span><span>&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>        match </span><span>=</span><span> re.search(regex, completion, re.</span><span>DOTALL</span><span>) </span></span>
<span data-line=""><span>        # if the format is not correct, reward is 0</span></span>
<span data-line=""><span>        if</span><span> match </span><span>is</span><span> None</span><span> or</span><span> len</span><span>(match.groups()) </span><span>!=</span><span> 2</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>        else</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>1.0</span><span>)</span></span>
<span data-line=""><span>      except</span><span> Exception</span><span>:</span></span>
<span data-line=""><span>        rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>    return</span><span> rewards</span></span>
<span data-line=""> </span>
<span data-line=""><span>def</span><span> equation_reward_func</span><span>(completions, target, nums, </span><span>**</span><span>kwargs):</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    Evaluates completions based on:</span></span>
<span data-line=""><span>    2. Mathematical correctness of the answer</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Args:</span></span>
<span data-line=""><span>        completions (list[str]): Generated outputs</span></span>
<span data-line=""><span>        target (list[str]): Expected answers</span></span>
<span data-line=""><span>        nums (list[str]): Available numbers</span></span>
<span data-line=""><span>    </span></span>
<span data-line=""><span>    Returns:</span></span>
<span data-line=""><span>        list[float]: Reward scores</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    rewards </span><span>=</span><span> []</span></span>
<span data-line=""><span>    for</span><span> completion, gt, numbers </span><span>in</span><span> zip</span><span>(completions, target, nums):</span></span>
<span data-line=""><span>      try</span><span>:</span></span>
<span data-line=""><span>        # add synthetic &lt;think&gt; as its already part of the prompt and prefilled for the assistant to more easily match the regex</span></span>
<span data-line=""><span>        completion </span><span>=</span><span> &#34;&lt;think&gt;&#34;</span><span> +</span><span> completion</span></span>
<span data-line=""><span>        # Check if the format is correct</span></span>
<span data-line=""><span>        match </span><span>=</span><span> re.search(</span><span>r</span><span>&#34;</span><span>&lt;answer&gt;</span><span>(.</span><span>*?</span><span>)</span><span>&lt;</span><span>\/</span><span>answer&gt;</span><span>&#34;</span><span>, completion)</span></span>
<span data-line=""><span>        if</span><span> match </span><span>is</span><span> None</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>            continue</span></span>
<span data-line=""><span>        # Extract the &#34;answer&#34; part from the completion</span></span>
<span data-line=""><span>        equation </span><span>=</span><span> match.group(</span><span>1</span><span>).strip()</span></span>
<span data-line=""><span>        # Extract all numbers from the equation</span></span>
<span data-line=""><span>        used_numbers </span><span>=</span><span> [</span><span>int</span><span>(n) </span><span>for</span><span> n </span><span>in</span><span> re.findall(</span><span>r</span><span>&#39;</span><span>\d</span><span>+</span><span>&#39;</span><span>, equation)]</span></span>
<span data-line=""><span>        </span></span>
<span data-line=""><span>        # Check if all numbers are used exactly once</span></span>
<span data-line=""><span>        if</span><span> sorted</span><span>(used_numbers) </span><span>!=</span><span> sorted</span><span>(numbers):</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>            continue</span></span>
<span data-line=""><span>        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace</span></span>
<span data-line=""><span>        allowed_pattern </span><span>=</span><span> r</span><span>&#39;</span><span>^[\d+</span><span>\-</span><span>*/().\s]</span><span>+</span><span>$</span><span>&#39;</span></span>
<span data-line=""><span>        if</span><span> not</span><span> re.match(allowed_pattern, equation):</span></span>
<span data-line=""><span>           rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>           continue</span></span>
<span data-line=""><span>        </span></span>
<span data-line=""><span>        # Evaluate the equation with restricted globals and locals</span></span>
<span data-line=""><span>        result </span><span>=</span><span> eval</span><span>(equation, {</span><span>&#34;__builtins__&#34;</span><span>: </span><span>None</span><span>}, {})</span></span>
<span data-line=""><span>        # Check if the equation is correct and matches the ground truth</span></span>
<span data-line=""><span>        if</span><span> abs</span><span>(</span><span>float</span><span>(result) </span><span>-</span><span> float</span><span>(gt)) </span><span>&lt;</span><span> 1e-5</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>1.0</span><span>)</span></span>
<span data-line=""><span>        else</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>      except</span><span> Exception</span><span>:</span></span>
<span data-line=""><span>            # If evaluation fails, reward is 0</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>) </span></span>
<span data-line=""><span>    return</span><span> rewards</span></span></code></pre></div></figure>
<p>Lets try our reward function with a sample.</p>
<p><em>Note: None of the example starts with <code>&lt;think&gt;</code> as we added it synthetically to the prompt.</em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>correct_sample_1 </span><span>=</span><span> &#34;&#34;&#34;We need to find an equation using the numbers 19, 36, 55, and 7</span></span>
<span data-line=""><span>exactly once, with basic arithmetic operations, that equals 65. One possible</span></span>
<span data-line=""><span>combination is 55 + 36 - 19 + 7... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>correct_sample_2 </span><span>=</span><span> &#34;&#34;&#34; ... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_format </span><span>=</span><span> &#34;&#34;&#34;User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_format_2 </span><span>=</span><span> &#34;&#34;&#34;To find the equation that equals 79 using the numbers 95, 78, 6, 88, I&#39;ll start by adding 88 and 95:                      </span></span>
<span data-line=""><span>95 + 88 = 183                                                                                                              </span></span>
<span data-line=""><span>Now, let&#39;s subtract 104 from 183 to get 79:</span></span>
<span data-line=""><span>183 - 104 = 79</span></span>
<span data-line=""><span>&lt;think&gt; 183 - 104 = 79 &lt;/think&gt;&lt;think&gt; 183 - 104 = 79 &lt;/think&gt;&lt;answer&gt; 183 - 104 = 79 &lt;/answer&gt;&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_result </span><span>=</span><span> &#34;&#34;&#34; ... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 18 &lt;/answer&gt;&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""> </span>
<span data-line=""><span>test_rewards </span><span>=</span><span> format_reward_func(</span><span>completions</span><span>=</span><span>[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], </span><span>target</span><span>=</span><span>[</span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>], </span><span>nums</span><span>=</span><span>[[</span><span>19</span><span>, </span><span>36</span><span>, </span><span>55</span><span>, </span><span>7</span><span>]] </span><span>*</span><span> 5</span><span>)</span></span>
<span data-line=""><span>assert</span><span> test_rewards </span><span>==</span><span> [</span><span>1.0</span><span>, </span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>, </span><span>1.0</span><span>], </span><span>&#34;Reward function is not working&#34;</span></span>
<span data-line=""><span>test_rewards </span><span>=</span><span> equation_reward_func(</span><span>completions</span><span>=</span><span>[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], </span><span>target</span><span>=</span><span>[</span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>, </span><span>&#34;65&#34;</span><span>], </span><span>nums</span><span>=</span><span>[[</span><span>19</span><span>, </span><span>36</span><span>, </span><span>55</span><span>, </span><span>7</span><span>]] </span><span>*</span><span> 5</span><span>)</span></span>
<span data-line=""><span>assert</span><span> test_rewards </span><span>==</span><span> [</span><span>1.0</span><span>, </span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>], </span><span>&#34;Reward function is not working&#34;</span></span></code></pre></div></figure>
<p>This looks good, now lets define our remaining training parameters, create a trainer and start training.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> trl </span><span>import</span><span> GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig</span></span>
<span data-line=""> </span>
<span data-line=""><span># our model we are going to use as policy </span></span>
<span data-line=""><span>model_config </span><span>=</span><span> ModelConfig(</span></span>
<span data-line=""><span>    model_name_or_path</span><span>=</span><span>&#34;Qwen/Qwen2.5-3B-Instruct&#34;</span><span>,</span></span>
<span data-line=""><span>    torch_dtype</span><span>=</span><span>&#34;bfloat16&#34;</span><span>,</span></span>
<span data-line=""><span>    attn_implementation</span><span>=</span><span>&#34;flash_attention_2&#34;</span><span>,</span></span>
<span data-line=""><span>    use_peft</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    load_in_4bit</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span># Hyperparameters</span></span>
<span data-line=""><span>training_args </span><span>=</span><span> GRPOConfig(</span></span>
<span data-line=""><span>    output_dir</span><span>=</span><span>&#34;qwen-r1-aha-moment&#34;</span><span>,</span></span>
<span data-line=""><span>    learning_rate</span><span>=</span><span>5e-7</span><span>,</span></span>
<span data-line=""><span>    lr_scheduler_type</span><span>=</span><span>&#34;cosine&#34;</span><span>,</span></span>
<span data-line=""><span>    logging_steps</span><span>=</span><span>10</span><span>,</span></span>
<span data-line=""><span>    max_steps</span><span>=</span><span>100</span><span>,</span></span>
<span data-line=""><span>    per_device_train_batch_size</span><span>=</span><span>1</span><span>,</span></span>
<span data-line=""><span>    gradient_accumulation_steps</span><span>=</span><span>1</span><span>,</span></span>
<span data-line=""><span>    gradient_checkpointing</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    gradient_checkpointing_kwargs</span><span>=</span><span>{</span><span>&#34;use_reentrant&#34;</span><span>: </span><span>False</span><span>},</span></span>
<span data-line=""><span>    bf16</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    # GRPO specific parameters</span></span>
<span data-line=""><span>    max_prompt_length</span><span>=</span><span>256</span><span>,</span></span>
<span data-line=""><span>    max_completion_length</span><span>=</span><span>1024</span><span>, </span><span># max length of the generated output for our solution</span></span>
<span data-line=""><span>    num_generations</span><span>=</span><span>2</span><span>,</span></span>
<span data-line=""><span>    beta</span><span>=</span><span>0.001</span><span>,</span></span>
<span data-line=""><span>    </span></span>
<span data-line=""><span>)</span></span>
<span data-line=""><span>trainer </span><span>=</span><span> GRPOTrainer(</span></span>
<span data-line=""><span>    model</span><span>=</span><span>model_config.model_name_or_path,</span></span>
<span data-line=""><span>    reward_funcs</span><span>=</span><span>[format_reward_func, equation_reward_func],</span></span>
<span data-line=""><span>    args</span><span>=</span><span>training_args,</span></span>
<span data-line=""><span>    train_dataset</span><span>=</span><span>train_dataset,</span></span>
<span data-line=""><span>    eval_dataset</span><span>=</span><span>test_dataset,</span></span>
<span data-line=""><span>    peft_config</span><span>=</span><span>get_peft_config(model_config),</span></span>
<span data-line=""><span>)</span></span></code></pre></div></figure>
<p>We can start our training by calling the <code>train</code> method on the trainer instance.</p>
<p><em>Note: Reinforcement Training is very slow and compute intensive. Running a single step on 1x L4 with Q-LoRA, Batch size of 1 and only 2 generations per samples takes &gt;20 minutes.</em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span># Train and push the model to the Hub</span></span>
<span data-line=""><span>trainer.train()</span></span>
<span data-line=""><span># Save model</span></span>
<span data-line=""><span>trainer.save_model(training_args.output_dir)</span></span></code></pre></div></figure>
<h2 id="4-distributed-training-example-for-grpo-using-deepspeed-and-vllm"><a href="#4-distributed-training-example-for-grpo-using-deepspeed-and-vllm" aria-hidden="true" tabindex="-1"><span></span></a>4. Distributed Training example for GRPO using Deepspeed and vLLM</h2>
<p>More than 20 minutes per step with only 2 generations per sample is not feasible. We need to scale up our training. Hugging Face TRL added support for distributed training with Deepspeed and using vLLM for faster generation. I preprared a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py">run_r1_grpo.py</a> script and a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml">receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</a> config file to run the training.</p>
<p>This configuration is tested and validated on a Node with 4x H100 80GBs, where a single step takes around 45-60s, as we can leverage vLLM for generation and DeepSpeed for distributed training. Therefore we need to make sure we correctly set the <code>num_processes</code> to the number of GPUs you have - 1 as the last one will be used with vLLM for Generation. If you are using more GPUS you need to change the <code>vllm_device</code> in the config file to last index GPU, e.g. if you have 8 GPUs you need to set <code>vllm_device=7</code> and your <code>num_processes</code> to 7.</p>
<p>command to run the training:</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="bash" data-theme="github-dark github-light"><span data-line=""><span>accelerate</span><span> launch</span><span> --num_processes</span><span> 3</span><span> --config_file</span><span> configs/accelerate_configs/deepspeed_zero3.yaml</span><span> scripts/run_r1_grpo.py</span><span> --config</span><span> receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</span></span></code></pre></div></figure>
<p>With the optimized distributed training a single step with 8 generations per sample on 4x H100 80GBs takes around 45-60s. The full training for 450 steps takes around 6 hours.</p>
<h2 id="5-results-and-training-observations"><a href="#5-results-and-training-observations" aria-hidden="true" tabindex="-1"><span></span></a>5. Results and Training Observations</h2>
<p>The script saves random completions to the <code>completion_samples</code> folder, which you can use to inspect the model&#39;s progress. It includes <code>completion_samples.txt</code> and <code>success_completion_samples.txt</code>. The <code>completion_samples.txt</code> includes all completions, while the <code>success_completion_samples.txt</code> which correctly solves the equation. Below you can find the interesating training obeserations on how the performance changes over time, as well as the Tensornoard logs and successfull reasoning samples.</p>
<p>The model with checkpoints for every 25th step can be found at <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/philschmid/qwen-2.5-3b-r1-countdown">philschmid/qwen-2.5-3b-r1-countdown</a>.</p>
<h3 id="hyperparameters"><a href="#hyperparameters" aria-hidden="true" tabindex="-1"><span></span></a>Hyperparameters</h3>
<p>I started the experiment using the hyperparameters from the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> paper with a learning rate of 1e-6 and a beta (KL coefficient) of 0.04, which led to unstable training runs after around 150 steps. I ran some small ablations and decreased both the learning rate to 5e-7 and the beta to 0.001, based on a test from <a target="_blank" rel="noopener noreferrer" href="https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05">OpenRLHF</a>. I coulnd&#39;t test how increasing the <code>num_generations</code> from 8 to 64 would affect the training. 64 is the generation value, which was used in the DeepSeekMath paper. All other parameters can be found in the <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml">grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</a> config file.</p>
<h3 id="training-observations"><a href="#training-observations" aria-hidden="true" tabindex="-1"><span></span></a>Training Observations:</h3>
<ul>
<li>At ~50 steps the model has learned the correct format <code>&lt;think&gt;...&lt;/think&gt;\n&lt;answer&gt;...&lt;/answer&gt;</code>.</li>
<li>At 100 steps the success rate for solving the equation is around 25%. The model starts to &#34;reason&#34; with words see examples below.</li>
<li>At 200 steps the performance seems to converge much slower and we are at ~40% success rate. The model starts to learn a new &#34;format&#34; where it solves the equation similar to how you would do it programmatically, by trying different combinations and reviewing the results, see &#34;Successfull Reasoning Samples between step 200 and 450&#34;.</li>
<li>At 450 steps we have 50% success rate for solving the equation. The performance still improves slowly and the model kept its new format form from step 200.</li>
</ul>
<p>I have 3 potential assumptions why the model shifts from &#34;word reasoning&#34; to &#34;programmatic execution&#34;:</p>
<ol>
<li>Qwen 2.5 3B is not strong enough or to small, Deepseek mentions that you need a very strong base model.</li>
<li>The reward functions are not good enough defined and it reward hacks the model to solve the equation. We could try to force it to use words, e.g. having a number to word frequency condition. (We don&#39;t know much about the reward functions from Deepseek)</li>
<li>Only training on the Countdown Game tasks might force the model naturally to learn the most effective way to solve the equation as no other formats are required.</li>
<li>We didn&#39;t train the model long enough, in the R1 paper they showed visuals of training for over 8000 steps.</li>
</ol>
<h4 id="tensorboard-results"><a href="#tensorboard-results" aria-hidden="true" tabindex="-1"><span></span></a>Tensorboard Results</h4>
<p><img src="https://www.philschmid.de/static/blog/mini-deepseek-r1/tensorboard-r1.png" alt="Qwen R1 Tensorboard results"/></p>
<h4 id="successfull-reasoning-samples-up-to-step-200"><a href="#successfull-reasoning-samples-up-to-step-200" aria-hidden="true" tabindex="-1"><span></span></a>Successfull Reasoning Samples up to step 200</h4>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to think about the numbers we have: 14, 70, and 80. To get 85, I think 80 might be a good starting point because it&#39;s right in between 85. If I subtract or add 80, it won&#39;t lead to 85 easily. Next, 70 seems to be a good intermediate number, especially to use in conjunction with 14. </span></span>
<span data-line=""><span>Now, I can think about different combinations using addition, subtraction and multiplication/division. If I take 80 - 70 = 10, which is not needed. So, I need to find a way to use 14. If I subtract 14 from 80, I get 66, which still isn&#39;t 85. If I subtract 70 from 80, I get 10 again, this isn&#39;t constructive yet. </span></span>
<span data-line=""><span>Multiplication could create numbers larger than the target 85. Let&#39;s consider division: If I divide 70 by 14, I get 5. Then if I add this result to 80, I get 85. This combination seems promising: 80 - (70 / 14) = 80 - 5 = 75, this still isn&#39;t quite right. Let me correct my approach: Let&#39;s use division and then addition. If I take 80 / (70 / 14) → 80 / 5 = 16, and then 80 - 16 = 64, still incorrect. Let me simplify: 80 / (70 / 14) → 80 / 5 = 16, and then 80 - 16 gives 64 again, let me correct: 80 - (70 / 14) → 80 - 5 = 75, but I need 85. Thus I will use the combinations in a slightly different order to insert numbers: I get 80 + (70 / 14) = 80 + 5 = 85&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 80 + (70 / 14) &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to consider how I can use the numbers 1, 21, and 90 to reach 68. I can try different combinations of operations. Let&#39;s try to use subtraction and addition. If I subtract 90 from 21, I get -69, which is too far from 68. Now, if I add 1 to 90, I get 91, which is also not right. Another way is to see what happens if I subtract 21 from 90, which gives me 69. This is closer but still not 68. Let me use multiplication instead of subtraction. 90 * 1 = 90. Now, if I try to reduce this slightly by subtracting 21, I am not getting close to 68.</span></span>
<span data-line=""><span>Now, let&#39;s try another multiplication. I&#39;ll think of 1 * (larger value) + (smaller value). Let&#39;s try 1 * 90 + 21. This evaluates to 90 + 21 = 111, which is too much. Instead, I will try: 90 / 1 + 21 - 7. But this gives a division by 7, which seems complex. Let&#39;s re-evaluate: I&#39;ll use 1 to divide 21: 21 / 1 = 21. Now, subtract 21 from 90, and add 1, we have 90 - 21 = 69, using one...</span></span>
<span data-line=""><span>Hey, wait on re-evaluating I will subtract 21 from 90, and include 1 to complete the solution: 90 - (21 + 1) which would be 68.&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 90 - (21 + 1) &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to consider the numbers we have: 59, 80, and 66. I need to use these numbers to get 45. Let&#39;s try to use subtraction first since it can quickly reduce large numbers.</span></span>
<span data-line=""><span>Let&#39;s see if we can get close by subtracting or adding these numbers. If I subtract 66 from 80, I get 14. Then I need to make this 14 equal 45 when combined with 59. That gives me 14 + 59 = 73, which is too high.</span></span>
<span data-line=""><span>Now let&#39;s try adding numbers. If I add 59 and 66, I get 125. Then I would need to subtract 80 from 125 to get 45, but 80 is too large to fit well with this.</span></span>
<span data-line=""><span>Let me try a different approach. What if I subtract 66 from 80 first and then add 59? That would be 14 + 59 = 73, which is still not 45.</span></span>
<span data-line=""><span>Hmm, I need to think of another combination. What if I start with 59 and then add 80 and subtract 66? That would be (59 + 80) - 66, let me calculate it. That equals 139 - 66, which is 73.</span></span>
<span data-line=""><span>That&#39;s still not 45, so let me try rearranging it slightly: maybe 80 - 66 + 59 would be closer to 45. Calculating again: 80 - 66 is 14, then adding 59 gives 73.</span></span>
<span data-line=""><span>This way doesn&#39;t work either. Let&#39;s flip this: maybe (59 + 66) - 80. Let me check: that&#39;s 125 - 80, which equals 45.&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt;(59 + 66) - 80&lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to find a combination of the numbers 61, 4, 94, and 21 that can be manipulated using basic arithmetic operations to equal 50. One way to approach this is to look for numbers that can be used to reach a target number close to 50 and then adjust with the remaining numbers.</span></span>
<span data-line=""><span>Let&#39;s try combining 94 and 21 first:</span></span>
<span data-line=""><span>94 + 21 = 115</span></span>
<span data-line=""><span>Now, we need to get from 115 to 50. We can try subtracting 65:</span></span>
<span data-line=""><span>115 - 65 = 50</span></span>
<span data-line=""><span>Now we need to find a way to get 65 using the remaining numbers 61 and 4. We can try:</span></span>
<span data-line=""><span>61 - 4 = 57</span></span>
<span data-line=""><span>Now we need to get from 57 to 65. Adding 8 would give us that:</span></span>
<span data-line=""><span>57 + 8 = 65</span></span>
<span data-line=""><span>So, we need to find a way to get 8 using 61 and 4. We can try:</span></span>
<span data-line=""><span>61 - 94 = -36</span></span>
<span data-line=""><span>This is negative, so let&#39;s try dividing:</span></span>
<span data-line=""><span>61 / 94 = 0.648987...</span></span>
<span data-line=""><span>This is not helping, so let&#39;s try multiplication and division in a different way:</span></span>
<span data-line=""><span>(61 - 4) * (94 / 21) = 57 * 4.476... = 254.4</span></span>
<span data-line=""><span>This is not working either. Let&#39;s try a different approach:</span></span>
<span data-line=""><span>(94 - 61) = 33</span></span>
<span data-line=""><span>Now we have:</span></span>
<span data-line=""><span>33 + 21 = 54</span></span>
<span data-line=""><span>We need to adjust from 54 to 50, so we can:</span></span>
<span data-line=""><span>54 - 4 = 50</span></span>
<span data-line=""><span>So, the equation becomes:</span></span>
<span data-line=""><span>(94 - 61) + (21 - 4) = 50</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt;(94 - 61) + (21 - 4)&lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I&#39;ll consider the possibility of directly subtracting two numbers to get close to 31. If I subtract 57 from 63, I get 7. Now, if I subtract this 7 from 25, I get 25 - 7 = 18. Far from 31, so this doesn&#39;t work.</span></span>
<span data-line=""><span>Next, let&#39;s try adding and subtracting. Dividing 63 by 25 will give a decimal, so that&#39;s unlikely to work. Now, let&#39;s see if adding and subtracting in another order could work. For example, let&#39;s try 63 - 25, and then we see this gets us 38. Now, we need a way to get to 31 from here. Subtracting 7 from 38 would work, but we can&#39;t figure out how to create that 7 from the given numbers.</span></span>
<span data-line=""><span>Hmm, let me try to incorporate all numbers by adding and subtracting. If I try 63 + 25, I get 88. This is quite far from 31, but if 88 - 57, I get 31 exactly!</span></span>
<span data-line=""><span>This seems to be the equation: 63 + 25 - 57 equals 31.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 63 + 25 - 57 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<h4 id="successfull-reasoning-samples-between-step-200-and-450"><a href="#successfull-reasoning-samples-between-step-200-and-450" aria-hidden="true" tabindex="-1"><span></span></a>Successfull Reasoning Samples between step 200 and 450</h4>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 39, 16, and 55 with basic arithmetic operations to get 78. Let&#39;s try different combinations:</span></span>
<span data-line=""><span>- 55 + 39 - 16 = 84 - 16 = 68 (too low)</span></span>
<span data-line=""><span>- 55 + 16 - 39 = 71 - 39 = 32 (too low)</span></span>
<span data-line=""><span>- 55 - 16 + 39 = 39 + 39 = 78 (this works)</span></span>
<span data-line=""><span>So, the equation is 55 - 16 + 39 = 78. &lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 55 - 16 + 39 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 21, 5, and 40 with basic arithmetic operations to get 24. Let&#39;s try different combinations:</span></span>
<span data-line=""><span>- 21 - 5 - 40 = -15</span></span>
<span data-line=""><span>- 21 - 5 + 40 = 56</span></span>
<span data-line=""><span>- 21 + 5 - 40 = -14</span></span>
<span data-line=""><span>- 21 * 5 / 40 = 2.1 (not an integer, let&#39;s try another)</span></span>
<span data-line=""><span>- 40 * 5 / 21 = 9.52 (not 24)</span></span>
<span data-line=""><span>- 40 / 5 - 21 = 9 - 21 = -12</span></span>
<span data-line=""><span>- 5 * 40 / 21 = 100 / 21 ~= 4.76 (not 24)</span></span>
<span data-line=""><span>- 5 + 40 - 21 = 24 ! This works.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 5 + 40 - 21 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 75, 99, 62, and 34 with basic arithmetic operations to get 72. Let&#39;s try different combinations:</span></span>
<span data-line=""><span>- 99 - 75 + 62 - 34 = 92 - 34 = 58 (too high)</span></span>
<span data-line=""><span>- 99 - 75 - 62 + 34 = 24 - 62 + 34 = -38 + 34 = -4 (too low)</span></span>
<span data-line=""><span>- 99 + 75 - 62 - 34 = 174 - 96 = 78 (too high)</span></span>
<span data-line=""><span>- 99 + 75 - 62 + 34 = 174 + 34 - 62 = 208 - 62 = 146 (too high)</span></span>
<span data-line=""><span>- 99 - 75 + 62 + 34 = 24 + 96 = 120 (too high)</span></span>
<span data-line=""><span>- 75 + 99 - 62 - 34 = 174 - 96 = 78 (too high)</span></span>
<span data-line=""><span>- 75 + 99 - 62 + 34 = 174 + 34 - 62 = 208 - 62 = 146 (too high)</span></span>
<span data-line=""><span>- 75 + 62 - 99 + 34 = 137 - 99 + 34 = 38 + 34 = 72</span></span>
<span data-line=""><span>So, 75 + 62 - 99 + 34 equals 72.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 75 + 62 - 99 + 34 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<h2 id="conclusion"><a href="#conclusion" aria-hidden="true" tabindex="-1"><span></span></a>Conclusion</h2>
<p>The release of DeepSeek R1 and its research paper might be breakpoint for the open-science and open-source development. Just a week after DeepSeek release, we&#39;ve been able to reproduce a simple version of R1 learned &#34;reasoning&#34; using GRPO and the Countdown Game. While our implementation focuses on a specific task rather than general reasoning and convergence into a very specific &#34;reasoning&#34; format, it shows that the method is working.</p>
<p>In our mini R1 experiment we used GRPO, with two rule-based reward but already required significant compute: 4 H100 GPUs running for 6 hours to complete just 450 training steps on a 3B parameter model. This gives us an idea of the compute needs that you will need to scale Reinforcement Learning. Deepseek ran a 671B model for over 8000 steps and they probably did many ablations.</p>
<p>Looking in to 2025, it&#39;s clear that we are on the cusp of even more significant progress. RL will become more accessible and user-friendly, more researchers and developers will explore its potential, but also require amount of more compute as before and compared to supervised fine-tuning.</p>
<p>I am excited for 2025. If you are have any question or ideas feel free to reach out to me.</p></div></div>
  </body>
</html>
