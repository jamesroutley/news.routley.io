<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://madebyoll.in/posts/world_emulation_via_dnn/">Original</a>
    <h1>World Emulation via Neural Network</h1>
    
    <div id="readability-page-1" class="page"><div>
    

<p>I turned a forest trail near my apartment into a playable neural world.</p>
<a href="https://madebyoll.in/posts/world_emulation_via_dnn/demo" id="demo"><video playsinline="" autoplay="" loop="" muted="">
    <source src="./neural_world_demo.mp4" type="video/mp4"/></video>
</a>

<p>By &#34;neural world&#34;, I mean that the entire thing is a neural network generating new images based on previous images + controls. There is no level geometry, no code for lighting or shadows, no scripted animation. Just a neural net in a loop.</p>

<p><img src="https://madebyoll.in/posts/world_emulation_via_dnn/inference_diagram.jpg" alt="A diagram illustrating a neural network that consumes noise, controls, and memory, and produces video frames and memory."/></p><p>By &#34;in your web browser&#34; I mean this world runs locally, in <em>your</em> web browser. Once the world has loaded, you can continue exploring even in Airplane Mode.</p>

<video playsinline="" autoplay="" loop="" muted=""><source src="./neural_world_demo_iphone.mp4" type="video/mp4"/></video>

<p>So, why bother creating a world this way? There are some interesting conceptual reasons (I&#39;ll get to them later), but my main goal was just to outdo a prior post.</p>

<p>See, <a href="https://x.com/madebyollin/status/1566838643771457536">three years ago</a>, I got a simple two-dimensional video game world to run in-browser by training a neural network to mimic gameplay videos from YouTube.
</p>

<a href="https://madebyoll.in/posts/game_emulation_via_dnn/demo">
<video playsinline="" autoplay="" loop="" muted=""><source src="./pokemon_world.mp4" type="video/mp4"/></video>
</a>

<p>
Mimicking a 2D video game world was cute, but ultimately kind of pointless;</p>

<p>
The wonderful, unique, exciting property of neural worlds is that they can be constructed from any video file,
not just screen recordings of old video games. </p>

<p>So for this post, to demonstrate what makes neural networks truly special,</p>

<h2 id="recording-data">Recording data</h2>

<p>To begin this project, I walked through a forest trail, recording videos with my phone,
using a customized camera app which also recorded my phone&#39;s motion.</p>

<video playsinline="" autoplay="" loop="" muted=""><source src="./capture_app.mp4" type="video/mp4"/></video>

<p>I collected ~15 minutes of video and motion recordings. I&#39;ve visualized motion as a &#34;walking&#34; control stick on the left and a &#34;looking&#34; control stick on the right.</p>

<video playsinline="" autoplay="" loop="" muted=""><source src="./sample_videos.mp4" type="video/mp4"/></video>

<p>
Back at home, I transferred the recordings to my laptop, and shuffled them into a list of (<code>previous frame, control</code> → <code>next frame</code>) pairs just like my previous game-emulation dataset.</p>


<p><img src="https://madebyoll.in/posts/world_emulation_via_dnn/sample_pairs.jpg" alt="A screenshot of two examples from the dataset, showing the memory and control inputs as well as the corresponding video frame outputs."/></p><p>Now, all I needed to do was train a neural network to mimic the behavior of these input→output pairs. I already had working code from my previous game-emulation project,</p>

<h2 id="training-baselines">Training baselines</h2>

<p>
Applying my previous game-emulation-via-neural-network recipe to this new dataset produced, regrettably, a sort of interactive forest-flavored soup.
</p>

<video autoplay="" playsinline="" loop="" muted=""><source src="./forest_soup_gameplay.mp4" type="video/mp4"/></video>

<p>
My neural network couldn&#39;t predict the actual next frame accurately,
and it couldn&#39;t make up new details fast enough to compensate,
so the resulting world collapsed even if I gave it a running start by initializing from real video frames:
</p>

<video autoplay="" playsinline="" loop="" muted=""><source src="./forest_soup.mp4" type="video/mp4"/></video>

<p>Undaunted, I started work on a new version of the neural world training code.<br/></p>

<h2 id="upgrading-the-training-recipe">Upgrading the training recipe</h2>

<p>To help my network understand real-world video, I made the following upgrades:</p>

<ol>
    <li><strong>Adding more control information.</strong> I upgraded the &#34;control&#34; network input from simple 2D controls to more-informative 3D (<a href="https://en.wikipedia.org/wiki/Six_degrees_of_freedom">6DoF</a>) controls.</li>
    <li><strong>Adding more memory.</strong> I upgraded the &#34;memory&#34; network input from a single frame to 32 frames (using lower resolution for the older frames).</li>
    <li><strong>Adding multiple scales.</strong> I restructured the network to process all inputs across multiple resolutions, instead of a fixed 1/8 resolution.</li>
</ol>

<p><img src="https://madebyoll.in/posts/world_emulation_via_dnn/network_upgrades.jpg" alt="A before/after diagram of the neural network architecture."/></p><p>These upgrades let me stave off soupification enough to get a half-baked demo:</p>

<video autoplay="" playsinline="" loop="" muted=""><source src="./first_stable_rollouts.mp4" type="video/mp4"/></video>

<p>This was  significant progress. Unfortunately, the world was still pretty melty,</p>

<h2 id="upgrading-the-training-recipe-more">Upgrading the training recipe more</h2>

<p>This time, I left the inputs/outputs as-is and focused on finding incremental improvements to the training procedure. Here&#39;s a mercifully-abbreviated montage:</p>

<video autoplay="" playsinline="" loop="" muted=""><source src="./improvement_montage.mp4" type="video/mp4"/></video>

<p>The biggest jumps in quality came from:</p>

<ol>
    <li><strong>Making the network bigger</strong>: I added even more layers of neural network processing, while striving to maintain a somewhat-playable FPS.</li>
    <li><strong>Picking a better training objective</strong>: I adjusted training to put less emphasis on <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">detail prediction</a> and more emphasis on <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">detail generation</a>.</li>
    <li><strong>Training longer</strong>: I trained the network longer on a selected subset of video frames to try and eke out the highest-quality results.</li>
</ol>

<p>
Here&#39;s a summary of the final forest world recipe:
</p>

<ul>
    <li><strong>Dataset</strong>: 22,814 frames (30FPS SDR video, timestamped ARKit poses) captured at Marymoor Park Audobon Bird Loop with iPhone 13 Pro.</li>
    <li><strong>Inputs</strong>:
        3x4-element relative camera pose, 2-element gravity-relative roll/pitch, relative time delta, valid/augmented bit,</li>
    
    <li><strong>Model</strong>: Asymmetric (decoder-heavy) 4-scale UNet with reduced-size full-resolution decoder block. </li>
    <li><strong>Training</strong>: AdamW constant LR + SWA, L1 + adversarial loss, stability fixes from the game-emulation recipe, around ~100 GPU-hours (~$100 USD).</li>
    <li><strong>Inference</strong>: Control-conditioned sequential autoregression with 60FPS cap, preprocessing in JS, network in ONNX Runtime Web&#39;s WebGL backend.</li>
</ul>


<p>
Whew. So, let&#39;s return to the original question: </p>

<h2 id="two-ways-to-create-worlds">Two ways to create worlds</h2>

<p>
Traditional game worlds are made like paintings. You sit in front of an empty canvas and layer <a href="https://www.youtube.com/watch?v=BFld4EBO2RE">keystroke upon keystroke</a> until you get something beautiful. Every lifelike detail in a traditional game is only there because <a href="https://www.youtube.com/watch?v=9XWxsJKpYYI">some artist painted it in</a>.
</p>

<p>
Neural worlds are made rather differently. </p>

<p>
So, if traditional game worlds are paintings, neural worlds are photographs.</p>

<p><img src="https://madebyoll.in/posts/world_emulation_via_dnn/information_flow.jpg" alt="A doodle showing how information flows in painting-style worlds vs. photo-style worlds."/></p><p>
Admittedly, as of this post, neural worlds resemble <em>very early</em> photographs.</p>

<p><a href="https://collectionscaptured.ncl.ac.uk/digital/collection/p21051coll22/id/4/">
<img src="https://madebyoll.in/posts/world_emulation_via_dnn/early_photo.jpg" alt="An early daguerrotype."/>
</a></p><p>
The exciting part was that cameras reduced realistic-image-creation from an artistic problem to a technological one.</p>

<p><img src="https://madebyoll.in/posts/world_emulation_via_dnn/current_photo.jpg" alt="A modern iPhone photograph."/></p><p>
I think that neural worlds will improve in fidelity just like photographs did.
In time, neural worlds will have trees that bend in the wind, lilypads that bob in the rain, birds that sing to each other. </p>

<p>
I think the tools for creating neural worlds can also, eventually, be just as convenient as today&#39;s cameras.
In the same way that a modern digital camera creates images or videos at the press of a button,
we could have a tool to create worlds.
</p>

<p>If neural worlds become as lifelike, cheap, and composable as photos are today,</p>

<p>
I think that would be very exciting indeed!
</p>

<hr/>

<p>Neural networks which model the world are often called &#34;world models&#34; and many smart people have worked on them; a classic example is Comma&#39;s <a href="https://arxiv.org/pdf/1608.01230">&#34;Learning a Driving Simulator&#34;</a>, and some more recent examples are OpenDriveLabs&#39; <a href="https://github.com/OpenDriveLab/Vista">Vista</a> or Wayve&#39;s <a href="https://wayve.ai/thinking/gaia-2/">GAIA-2</a>. If you&#39;re a programmer interested in training your own world models, I recommend looking at <a href="https://diamond-wm.github.io">DIAMOND</a> or <a href="https://github.com/buoyancy99/diffusion-forcing">Diffusion Forcing</a>.</p>

<p>Compared to serious &#34;Foundation World Models&#34; with billions of parameters,</p>
</div></div>
  </body>
</html>
