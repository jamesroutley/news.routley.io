<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scirate.com/">Original</a>
    <h1>SciRate: An open source website to browse, save, and comment ArXiv articles</h1>
    
    <div id="readability-page-1" class="page"><div><div><ul><li><p>We describe and analyze algorithms for classically simulating measurement of an $n$-qubit quantum state $\psi$ in the standard basis, that is, sampling a bit string $x$ from the probability distribution $|\langle x|\psi\rangle|^2$. Our algorithms reduce the sampling task to computing poly$(n)$ amplitudes of $n$-qubit states; unlike previously known techniques they do not require computation of marginal probabilities. First we consider the case where $|\psi\rangle=U|0^n\rangle$ is the output state of an $m$-gate quantum circuit $U$. We propose an exact sampling algorithm which involves computing $O(m)$ amplitudes of $n$-qubit states generated by subcircuits of $U$ spanned by the first $t=1,2,\ldots,m$ gates. We show that our algorithm can significantly accelerate quantum circuit simulations based on tensor network contraction methods or low-rank stabilizer decompositions. As another striking consequence we obtain an efficient classical simulation algorithm for measurement-based quantum computation with the surface code resource state on any planar graph, generalizing a previous algorithm which was known to be efficient only under restrictive topological constraints on the ordering of single-qubit measurements. Second, we consider the case in which $\psi$ is the unique ground state of a local Hamiltonian with a spectral gap that is lower bounded by an inverse polynomial function of $n$. We prove that a simple Metropolis-Hastings Markov Chain mixes rapidly to the desired probability distribution provided that $\psi$ obeys a certain technical condition, which we show is satisfied for all sign-problem free Hamiltonians. This gives a sampling algorithm which involves computing $\mathrm{poly}(n)$ amplitudes of $\psi$.</p></li><li><p>A semidefinite program (SDP) is a particular kind of convex optimization problem with applications in operations research, combinatorial optimization, quantum information science, and beyond. In this work, we propose variational quantum algorithms for approximately solving SDPs. For one class of SDPs, we provide a rigorous analysis of their convergence to approximate locally optimal solutions, under the assumption that they are weakly constrained (i.e., $N\gg M$, where $N$ is the dimension of the input matrices and $M$ is the number of constraints). We also provide algorithms for a more general class of SDPs that requires fewer assumptions. Finally, we numerically simulate our quantum algorithms for applications such as MaxCut, and the results of these simulations provide evidence that convergence still occurs in noisy settings.</p></li><li><p>We extend the entanglement bootstrap approach to (3+1)-dimensions. We study knotted excitations of (3+1)-dimensional liquid topological orders and exotic fusion processes of loops. As in previous work in (2+1)-dimensions, we define a variety of superselection sectors and fusion spaces from two axioms on the ground state entanglement entropy. In particular, we identify fusion spaces associated with knots. We generalize the information convex set to a new class of regions called immersed regions, promoting various theorems to this new context. Examples from solvable models are provided; for instance, a concrete calculation of knot multiplicity shows that the knot complement of a trefoil knot can store quantum information. We define spiral maps that allow us to understand consistency relations for torus knots as well as spiral fusions of fluxes.</p></li><li><p>We present a systematic approach for generating duality transformations in quantum lattice models. Within our formalism, dualities are completely characterized by equivalent but distinct realizations of a given (possibly non-abelian and non-invertible) symmetry. These different realizations are encoded into fusion categories, and dualities are methodically generated by considering all Morita equivalent categories. The full set of symmetric operators can then be constructed from the categorical data. We construct explicit intertwiners, in the form of matrix product operators, that convert local symmetric operators of one realization into local symmetric operators of its dual. Concurrently, it maps local operators that transform non-trivially into non-local ones. This guarantees that the structure constants of the algebra of all symmetric operators are equal in both dual realizations. Families of dual Hamiltonians, possibly with long range interactions, are then designed by taking linear combinations of the corresponding symmetric operators. We illustrate this approach by establishing matrix product operator intertwiners for well-known dualities such as Kramers-Wannier and Jordan-Wigner, consider theories with two copies of the Ising category symmetry, and present an example with quantum group symmetries. Finally, we comment on generalizations to higher dimensions of this categorical approach to dualities.</p></li><li><p>Reinforcement learning studies how an agent should interact with an environment to maximize its cumulative reward. A standard way to study this question abstractly is to ask how many samples an agent needs from the environment to learn an optimal policy for a $\gamma$-discounted Markov decision process (MDP). For such an MDP, we design quantum algorithms that approximate an optimal policy ($\pi^*$), the optimal value function ($v^*$), and the optimal $Q$-function ($q^*$), assuming the algorithms can access samples from the environment in quantum superposition. This assumption is justified whenever there exists a simulator for the environment; for example, if the environment is a video game or some other program. Our quantum algorithms, inspired by value iteration, achieve quadratic speedups over the best-possible classical sample complexities in the approximation accuracy ($\epsilon$) and two main parameters of the MDP: the effective time horizon ($\frac{1}{1-\gamma}$) and the size of the action space ($A$). Moreover, we show that our quantum algorithm for computing $q^*$ is optimal by proving a matching quantum lower bound.</p></li><li><p>We investigate the computational power of the recently introduced class of isometric tensor network states (isoTNSs), which generalizes the isometric conditions of the canonical form of one-dimensional matrix-product states to tensor networks in higher dimensions. We discuss several technical details regarding the implementation of isoTNSs-based algorithms and compare different disentanglers -- which are essential for an efficient handling of isoTNSs. We then revisit the time evolving block decimation for isoTNSs ($\text{TEBD}^2$) and explore its power for real time evolution of two-dimensional (2D) lattice systems. Moreover, we introduce a density matrix renormalization group algorithm for isoTNSs ($\text{DMRG}^2$) that allows to variationally find ground states of 2D lattice systems. As a demonstration and benchmark, we compute the dynamical spin structure factor of 2D quantum spin systems for two paradigmatic models: First, we compare our results for the transverse field Ising model on a square lattice with the prediction of the spin-wave theory. Second, we consider the Kitaev model on the honeycomb lattice and compare it to the result from the exact solution.</p></li><li><p>In holographic theories, the reflected entropy has been shown to be dual to the area of the entanglement wedge cross section. We study the same problem in random tensor networks demonstrating an equivalent duality. For a single random tensor we analyze the important non-perturbative effects that smooth out the discontinuity in the reflected entropy across the Page phase transition. By summing over all such effects, we obtain the reflected entanglement spectrum analytically, which agrees well with numerical studies. This motivates a prescription for the analytic continuation required in computing the reflected entropy and its Rényi generalization which resolves an order of limits issue previously identified in the literature. We apply this prescription to hyperbolic tensor networks and find answers consistent with holographic expectations. In particular, the random tensor network has the same non-trivial tripartite entanglement structure expected from holographic states. We furthermore show that the reflected Rényi spectrum is not flat, in sharp contrast to the usual Rényi spectrum of these networks. We argue that the various distinct contributions to the reflected entanglement spectrum can be organized into approximate superselection sectors. We interpret this as resulting from an effective description of the canonically purified state as a superposition of distinct tensor network states. Each network is constructed by doubling and gluing various candidate entanglement wedges of the original network. The superselection sectors are labelled by the different cross-sectional areas of these candidate entanglement wedges.</p></li><li><p>This chapter addresses the question of quantum entanglement in disordered chains, focusing on the von-Neumann and Rényi entropies for three important classes of random systems: Anderson localized, infinite randomness criticality, and many-body localization (MBL). We review previous works, and also present new results for the entanglement entropy of random spin chains at low and high energy.</p></li><li><p>The spin degree of freedom of an electron or a nucleus is one of the most basic properties of nature and functions as an excellent qubit, as it provides a natural two-level system that is insensitive to electric fields, leading to long quantum coherence times. We review the physics of semiconductor spin qubits, focusing not only on the early achievements of spin initialization, control, and readout in GaAs quantum dots, but also on recent advances in Si and Ge spin qubits, including improved charge control and readout, coupling to other quantum degrees of freedom, and scaling to larger system sizes. We begin by introducing the four major types of spin qubits: single spin qubits, donor spin qubits, singlet-triplet spin qubits, and exchange-only spin qubits. We then review the mesoscopic physics of quantum dots, including single-electron charging, valleys, and spin-orbit coupling. We next give a comprehensive overview of the physics of exchange interactions, a crucial resource for single- and two-qubit control in spin qubits. The bulk of this review is centered on the presentation of results from each major spin qubit type, the present limits of fidelity, and a brief overview of alternative spin qubit platforms. We then give a physical description of the impact of noise on semiconductor spin qubits, aided in large part by an introduction to the filter function formalism. Lastly, we review recent efforts to hybridize spin qubits with superconducting systems, including charge-photon coupling, spin-photon coupling, and long-range cavity-mediated spin-spin interactions. Cavity-based readout approaches are also discussed. This review is intended to give an appreciation for the future prospects of semiconductor spin qubits, while highlighting the key advances in mesoscopic physics over the past two decades that underlie the operation of modern quantum-dot and donor spin qubits.</p></li><li><p>Enhancing classical machine learning (ML) algorithms through quantum kernels is a rapidly growing research topic in quantum machine learning (QML). A key challenge in using kernels -- both classical and quantum -- is that ML workflows involve acquiring new observations, for which new kernel values need to be calculated. Transferring data back-and-forth between where the new observations are generated &amp; a quantum computer incurs a time delay; this delay may exceed the timescales relevant for using the QML algorithm in the first place. In this work, we show quantum kernel matrices can be extended to incorporate new data using a classical (chordal-graph-based) matrix completion algorithm. The minimal sample complexity needed for perfect completion is dependent on matrix rank. We empirically show that (a) quantum kernel matrices can be completed using this algorithm when the minimal sample complexity is met, (b) the error of the completion degrades gracefully in the presence of finite-sampling noise, and (c) the rank of quantum kernel matrices depends weakly on the expressibility of the quantum feature map generating the kernel. Further, on a real-world, industrially-relevant data set, the completion error behaves gracefully even when the minimal sample complexity is not reached.</p></li><li><p>We have repurposed Google Tensor Processing Units (TPUs), application-specific chips developed for machine learning, into large-scale dense linear algebra supercomputers. The TPUs&#39; fast inter-core interconnects (ICI)s, physically two-dimensional network topology, and high-bandwidth memory (HBM) permit distributed matrix multiplication algorithms to rapidly become computationally bound. In this regime, the matrix-multiply units (MXU)s dominate the runtime, yielding impressive scaling, performance, and raw size: operating in float32 precision, a full 2048-core pod of third generation TPUs can multiply two matrices with linear size $N= 220= 1 048 576$ in about 2 minutes. Via curated algorithms emphasizing large, single-core matrix multiplications, other tasks in dense linear algebra can similarly scale. As examples, we present (i) QR decomposition; (ii) resolution of linear systems; and (iii) the computation of matrix functions by polynomial iteration, demonstrated by the matrix polar factorization.</p></li><li><p>Fewer-qubit quantum logic gate, serving as a basic unit for constructing universal multiqubit gates, has been widely applied in quantum computing and quantum information. However, traditional constructions for fewer-qubit gates often utilize a multi-pulse protocol which inevitably suffers from serious intrinsic errors during the gate execution. In this article, we report an optimal model about universal two- and three-qubit CNOT gates mediated by excitation to Rydberg states with easily-accessible van der Waals interactions. This gate depends on a global optimization to implement amplitude and phase modulated pulses via genetic algorithm, which can facilitate the gate operation with fewer optical pulses. Compared to conventional multi-pulse piecewise schemes, our gate can be realized by simultaneous excitation of atoms to the Rydberg states, saving the time for multi-pulse switching at different spatial locations. Our numerical simulations show that a single-pulse two(three)-qubit CNOT gate is possibly achieved with a fidelity of 99.23$\%$(90.39$\%$) for two qubits separated by 7.10 $\mu$m when the fluctuation of Rydberg interactions is excluded. Our work is promising for achieving fast and convenient multiqubit quantum computing in the study of neutral-atom quantum technology.</p></li><li><p>Accurate models of real quantum systems are important for investigating their behaviour, yet are difficult to distill empirically. Here, we report an algorithm -- the Quantum Model Learning Agent (QMLA) -- to reverse engineer Hamiltonian descriptions of a target system. We test the performance of QMLA on a number of simulated experiments, demonstrating several mechanisms for the design of candidate Hamiltonian models and simultaneously entertaining numerous hypotheses about the nature of the physical interactions governing the system under study. QMLA is shown to identify the true model in the majority of instances, when provided with limited a priori information, and control of the experimental setup. Our protocol can explore Ising, Heisenberg and Hubbard families of models in parallel, reliably identifying the family which best describes the system dynamics. We demonstrate QMLA operating on large model spaces by incorporating a genetic algorithm to formulate new hypothetical models. The selection of models whose features propagate to the next generation is based upon an objective function inspired by the Elo rating scheme, typically used to rate competitors in games such as chess and football. In all instances, our protocol finds models that exhibit $F_1$-score $\geq 0.88$ when compared with the true model, and it precisely identifies the true model in 72% of cases, whilst exploring a space of over $250,000$ potential models. By testing which interactions actually occur in the target system, QMLA is a viable tool for both the exploration of fundamental physics and the characterisation and calibration of quantum devices.</p></li><li><p>Non-Gaussian Component Analysis (NGCA) is the following distribution learning problem: Given i.i.d. samples from a distribution on $\mathbb{R}^d$ that is non-gaussian in a hidden direction $v$ and an independent standard Gaussian in the orthogonal directions, the goal is to approximate the hidden direction $v$. Prior work \citeDKS17-sq provided formal evidence for the existence of an information-computation tradeoff for NGCA under appropriate moment-matching conditions on the univariate non-gaussian distribution $A$. The latter result does not apply when the distribution $A$ is discrete. A natural question is whether information-computation tradeoffs persist in this setting. In this paper, we answer this question in the negative by obtaining a sample and computationally efficient algorithm for NGCA in the regime that $A$ is discrete or nearly discrete, in a well-defined technical sense. The key tool leveraged in our algorithm is the LLL method \citeLLL82 for lattice basis reduction.</p></li><li><p>We introduce and study generalized Rényi entropies defined through the traces of products of ${\rm Tr}_B (|\Psi_i\rangle\langle \Psi_j|)$ where $|\Psi_i\rangle$ are eigenstates of a two-dimensional conformal field theory (CFT). When $|\Psi_i\rangle=|\Psi_j\rangle$ these objects reduce to the standard Rényi entropies of the eigenstates of the CFT. Exploiting the path integral formalism, we show that the second generalized Rényi entropies are equivalent to four-point correlators. We then focus on a free bosonic theory for which the mode expansion of the fields allows us to develop an efficient strategy to compute the second generalized Rényi entropy for all eigenstates. As a byproduct, our approach also leads to new results for the standard Rényi and relative entropies involving arbitrary descendent states of the bosonic CFT.</p></li><li><p>Geometric quantum computation offers a practical strategy toward robust quantum computation due to its inherently error tolerance. However, the rigorous geometric conditions lead to complex and/or error-disturbed quantum controls, especially for logical qubits that involve more physical qubits, whose error tolerance is effective in principle though, their experimental demonstration is still demanding. Thus, how to best simplify the needed control and manifest its full advantage has become the key to widespread applications of geometric quantum computation. Here we propose a new fast and robust geometric scheme, with the decoherence-free-subspace encoding, and present its physical implementation on superconducting quantum circuits, where we only utilize the experimentally demonstrated parametrically tunable coupling to achieve high-fidelity geometric control over logical qubits. Numerical simulation verifies that it can efficiently combine the error tolerance from both the geometric phase and logical-qubit encoding, displaying our gate-performance superiority over the conventional dynamical one without encoding, in terms of both gate fidelity and robustness. Therefore, our scheme can consolidate both error suppression methods for logical-qubit control, which sheds light on the future large-scale quantum computation.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/cs.DS">cs.DS</a> arXiv:2112.09124v1 </p></div></div><p>In STOC&#39;95 [ADMSS95] Arya et al. showed that any set of $n$ points in $\mathbb R^d$ admits a $(1+\epsilon)$-spanner with hop-diameter at most 2 (respectively, 3) and $O(n \log n)$ edges (resp., $O(n \log \log n)$ edges). They also gave a general upper bound tradeoff of hop-diameter at most $k$ and $O(n \alpha_k(n))$ edges, for any $k \ge 2$. The function $\alpha_k$ is the inverse of a certain Ackermann-style function at the $\lfloor k/2 \rfloor$th level of the primitive recursive hierarchy, where $\alpha_0(n) = \lceil n/2 \rceil, \alpha_1(n) = \left\lceil \sqrt{n} \right\rceil, \alpha_2(n) = \lceil \log{n} \rceil, \alpha_3(n) = \lceil \log\log{n} \rceil, \alpha_4(n) = \log^* n, \alpha_5(n) = \lfloor \frac{1}{2} \log^*n \rfloor$, \ldots. Roughly speaking, for $k \ge 2$ the function $\alpha_{k}$ is close to $\lfloor \frac{k-2}{2} \rfloor$-iterated log-star function, i.e., $\log$ with $\lfloor \frac{k-2}{2} \rfloor$ stars. Whether or not this tradeoff is tight has remained open, even for the cases $k = 2$ and $k = 3$. Two lower bounds are known: The first applies only to spanners with stretch 1 and the second is sub-optimal and applies only to sufficiently large (constant) values of $k$. In this paper we prove a tight lower bound for any constant $k$: For any fixed $\epsilon &gt; 0$, any $(1+\epsilon)$-spanner for the uniform line metric with hop-diameter at most $k$ must have at least $\Omega(n \alpha_k(n))$ edges.</p></li><li><p>Let $T_{\epsilon}$, $0 \le \epsilon \le 1/2$, be the noise operator acting on functions on the boolean cube $\{0,1\}^n$. Let $f$ be a distribution on $\{0,1\}^n$ and let $q &gt; 1$. We prove tight Mrs. Gerber-type results for the second Renyi entropy of $T_{\epsilon} f$ which take into account the value of the $q^{th}$ Renyi entropy of $f$. For a general function $f$ on $\{0,1\}^n$ we prove tight hypercontractive inequalities for the $\ell_2$ norm of $T_{\epsilon} f$ which take into account the ratio between $\ell_q$ and $\ell_1$ norms of $f$.</p></li><li><p>Quantum process tomography conventionally uses a multitude of initial quantum states and then performs state tomography on the process output. Here we propose and study an alternative approach which requires only a single (or few) known initial states together with time-delayed measurements for reconstructing the unitary map and corresponding Hamiltonian of the time dynamics. The overarching mathematical framework and feasibility guarantee of our method is provided by the Takens embedding theorem. We explain in detail how the reconstruction of a single qubit Hamiltonian works in this setting, and provide numerical methods and experiments for general few-qubit and lattice systems with local interactions. In particular, the method allows to find the Hamiltonian of a two qubit system by observing only one of the qubits.</p></li><li><p>We show that determining if an $n$-vertex graph has twin-width at most 4 is NP-complete, and requires time $2^{\Omega(n/\log n)}$ unless the Exponential-Time Hypothesis fails. Along the way, we give an elementary proof that $n$-vertex graphs subdivided at least $2 \log n$ times have twin-width at most 4. We also show how to encode trigraphs $H$ (2-edge colored graphs involved in the definition of twin-width) into graphs $G$, in the sense that every $d$-sequence (sequence of vertex contractions witnessing that the twin-width is at most $d$) of $G$ inevitably creates $H$ as an induced subtrigraph, whereas there exists a partial $d$-sequence that actually goes from $G$ to $H$. We believe that these facts and their proofs can be of independent interest.</p></li><li><p>The prototypical system constituted by a two-level atom interacting with a quantized single-mode electromagnetic field is described by the quantum Rabi model (QRM). The QRM is potentially valid at any light-matter interaction regime, ranging from the weak (where the decay rates exceeds the coupling rate) to the deep strong coupling (where the interaction rate exceeds the bare transition frequencies of the subsystems). However, when reaching the ultrastrong coupling regime, several theoretical issues may prevent the correct description of the observable dynamics of such a system: (i) the standard quantum optics master equation fails to correctly describe the interaction of this system with the reservoirs; (ii) the correct output photon rate is no longer proportional to the intracavity photon number; and (iii) the appears to violate gauge invariance. Here, we study the photon flux emission rate of this system under the incoherent excitation of the two-level atom for any light-matter interaction strength, and consider different effective temperatures. The dependence of the emission spectra on the coupling strength is the result of the interplay between energy levels, matrix elements of the observables, and the density of states of the reservoirs. Within this approach, we also study the occurence of light-matter decoupling in the deep strong coupling regime, and show how all of the obtained results are gauge invariant.</p></li><li><p>We present a comparative study between classical probability and quantum probability from the Bayesian viewpoint, where probability is construed as our rational degree of belief on whether a given statement is true. From this viewpoint, including conditional probability, three issues are discussed: i) Given a measure of the rational degree of belief, does it satisfy the axioms of the probability? ii) Given the probability satisfying these axioms, is it seen as the measure of the rational degree of belief? iii) Can the measure of the rational degree of belief be evaluated in terms of the relative frequency of events occurring? Here we show that as with the classical probability, all these issues can be resolved affirmatively in the quantum probability, provided that the relation to the relative frequency is slightly modified in case of a small number of observations. This implies that the relation between the Bayesian probability and the relative frequency in quantum mechanics is the same as that in the classical probability theory, including conditional probability.</p></li><li><p>Optomechanical systems open new possibilities in fundamental research at the interface between quantum information and gravity. Recently, an ambitious experimental proposal was suggested by Bose et al. to measure the entanglement between two optomechanical systems generated by their gravitational interaction. The scheme relies on witnessing entanglement between the two systems. Here we develop a general framework to study the quality of bipartite entanglement witnesses using fidelity witnesses. We then apply this framework to the gravitational entanglement proposal, optimizing for the detection of entanglement. We construct a witness consisting of only 5 non-trivial spin measurements, which we compare with other proposed witnesses. With post-processing our witness can detect entanglement for any choice of phases in the setup, up to a set of measure zero.</p></li><li><p>The dynamics of a quantum particle is governed by its wavefunction, which in turn is determined by the classical potential to which it is subjected. However the wavefunction itself induces a quantum potential, the particle `sees&#39; the sum of the classical and quantum potentials, and there is no way to separate the two. Therefore in principle, part or whole of an observed potential may be attributable to a quantum potential. We examine this possibility and discuss implications.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/cs.DS">cs.DS</a> arXiv:2112.08454v1 </p></div></div><p>The Longest Common Subsequence (LCS) of two strings is a fundamental string similarity measure with a classical dynamic programming solution taking quadratic time. Despite significant efforts, little progress was made in improving the runtime. Even in the realm of approximation, not much was known for linear time algorithms beyond the trivial $\sqrt{n}$-approximation. Recent breakthrough result provided a $n^{0.497}$-factor approximation algorithm [HSSS19], which was more recently improved to a $n^{0.4}$-factor one [BCD21]. The latter paper also showed a $n^{2-2.5\alpha}$ time algorithm which outputs a $n^{\alpha}$ approximation to the LCS, but so far no sub-polynomial approximation is known in truly subquadratic time. In this work, we show an algorithm which runs in $O(n)$ time, and outputs a $n^{o(1)}$-factor approximation to LCS$(x,y)$, with high probability, for any pair of length $n$ input strings. Our entire algorithm is merely an efficient black-box reduction to the Block-LIS problem, introduced very recently in [ANSS21], and solving the Block-LIS problem directly.</p></li><li><p>An invited Commentary in: Nature Reviews Physics 3, 7 (2021).</p></li><li><p>We describe and realize an experimental procedure for assessing the incompatibility of two qubit measurements. The experiment consists in a state discrimination task where either measurement is used according to some partial intermediate information. The success statistics of the task provides an upper bound for the amount of incompatibility of the two measurements, as it is quantified by means of their incompatibility robustness. For a broad class of unbiased and possibly noisy qubit measurements, one can make this upper bound coincide with the true value of the robustness by suitably tuning the preparation of the experiment. We demonstrate this fact in an optical setup, where the qubit states are encoded into the photons&#39; polarization degrees of freedom, and incompatibility is directly accessed by virtue of a refined control on the amplitude, phase and purity of the final projection stage of the measurements. Our work thus establishes the practical feasibility of a recently proposed method for the detection of quantum incompatibility.</p></li><li><p>We consider the dynamics of continuously measured many-body chaotic quantum systems. Focusing on the observable of state purification, we analytically describe the limits of strong and weak measurement rate, where in the latter case monitoring up to time scales exponentially long in the numbers of particles is required. We complement the analysis of the limiting regimes with the construction of an effective replica theory providing information on the stability and the symmetries of the respective phases. The analytical results are tested by comparison to exact numerical simulations for a measured SYK model.</p></li><li><p>Polarization is one of light&#39;s most versatile degrees of freedom for both classical and quantum applications. The ability to measure light&#39;s state of polarization and changes therein is thus essential; this is the science of polarimetry. It has become ever more apparent in recent years that the quantum nature of light&#39;s polarization properties is crucial, from explaining experiments with single or few photons to understanding the implications of quantum theory on classical polarization properties. We present a self-contained overview of quantum polarimetry, including discussions of classical and quantum polarization, their transformations, and measurements thereof. We use this platform to elucidate key concepts that are neglected when polarization and polarimetry are considered only from classical perspectives.</p></li><li><p>Recently, generalization bounds of the non-convex empirical risk minimization paradigm using Stochastic Gradient Langevin Dynamics (SGLD) have been extensively studied. Several theoretical frameworks have been presented to study this problem from different perspectives, such as information theory and stability. In this paper, we present a unified view from privacy leakage analysis to investigate the generalization bounds of SGLD, along with a theoretical framework for re-deriving previous results in a succinct manner. Aside from theoretical findings, we conduct various numerical studies to empirically assess the information leakage issue of SGLD. Additionally, our theoretical and empirical results provide explanations for prior works that study the membership privacy of SGLD.</p></li><li><p>We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7% with MViT-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/hep-th">hep-th</a> arXiv:2112.09132v1 </p></div></div><p>We compute holographic entanglement entropy for subregions of a BCFT thermal state living on a nongravitating black hole background. The system we consider is doubly holographic and dual to an eternal black string with an embedded Karch-Randall brane that is parameterized by its angle. Entanglement islands are conventionally expected to emerge at late times to preserve unitarity at finite temperature, but recent calculations at zero temperature have shown such islands do not exist when the brane lies below a critical angle. When working at finite temperature in the context of a black string, we find that islands exist even when the brane lies below the critical angle. We note that although these islands exist when they are needed to preserve unitarity, they are restricted to a finite connected region on the brane which we call the atoll. Depending on two parameters -- the size of the subregion and the brane angle -- the entanglement entropy either remains constant in time or follows a Page curve. We discuss this rich phase structure in the context of bulk reconstruction.</p></li><li><p>Existing state-of-the-art methods for Video Object Segmentation (VOS) learn low-level pixel-to-pixel correspondences between frames to propagate object masks across video. This requires a large amount of densely annotated video data, which is costly to annotate, and largely redundant since frames within a video are highly correlated. In light of this, we propose HODOR: a novel method that tackles VOS by effectively leveraging annotated static images for understanding object appearance and scene context. We encode object instances and scene information from an image frame into robust high-level descriptors which can then be used to re-segment those objects in different frames. As a result, HODOR achieves state-of-the-art performance on the DAVIS and YouTube-VOS benchmarks compared to existing methods trained without video annotations. Without any architectural modification, HODOR can also learn from video context around single annotated video frames by utilizing cyclic consistency, whereas other methods rely on dense, temporally consistent annotations.</p></li><li><p>The advent of large-scale training has produced a cornucopia of powerful visual recognition models. However, generative models, such as GANs, have traditionally been trained from scratch in an unsupervised manner. Can the collective &#34;knowledge&#34; from a large bank of pretrained vision models be leveraged to improve GAN training? If so, with so many models to choose from, which one(s) should be selected, and in what manner are they most effective? We find that pretrained computer vision models can significantly improve performance when used in an ensemble of discriminators. Notably, the particular subset of selected models greatly affects performance. We propose an effective selection mechanism, by probing the linear separability between real and fake samples in pretrained model embeddings, choosing the most accurate model, and progressively adding it to the discriminator ensemble. Interestingly, our method can improve GAN training in both limited data and large-scale settings. Given only 10k training samples, our FID on LSUN Cat matches the StyleGAN2 trained on 1.6M images. On the full dataset, our method improves FID by 1.5x to 2x on cat, church, and horse categories of LSUN.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/cs.CV">cs.CV</a> arXiv:2112.09129v1 </p></div></div><p>Decoupling spatiotemporal representation refers to decomposing the spatial and temporal features into dimension-independent factors. Although previous RGB-D-based motion recognition methods have achieved promising performance through the tightly coupled multi-modal spatiotemporal representation, they still suffer from (i) optimization difficulty under small data setting due to the tightly spatiotemporal-entangled modeling;(ii) information redundancy as it usually contains lots of marginal information that is weakly relevant to classification; and (iii) low interaction between multi-modal spatiotemporal information caused by insufficient late fusion. To alleviate these drawbacks, we propose to decouple and recouple spatiotemporal representation for RGB-D-based motion recognition. Specifically, we disentangle the task of learning spatiotemporal representation into 3 sub-tasks: (1) Learning high-quality and dimension independent features through a decoupled spatial and temporal modeling network. (2) Recoupling the decoupled representation to establish stronger space-time dependency. (3) Introducing a Cross-modal Adaptive Posterior Fusion (CAPF) mechanism to capture cross-modal spatiotemporal information from RGB-D data. Seamless combination of these novel designs forms a robust spatialtemporal representation and achieves better performance than state-of-the-art methods on four public motion datasets. Our code is available at https://github.com/damo-cv/MotionRGBD.</p></li><li><p>We consider the interplay of the Early Dark Energy (EDE) model, the Swampland Distance Conjecture (SDC), and cosmological parameter tensions. EDE is a proposed resolution of the Hubble tension relying upon a near-Planckian scalar field excursion, while the SDC predicts an exponential sensitivity of masses of other fields to such an excursion, $m\propto e^{-c|\Delta \phi|/M_{\rm pl}}$ with $c\sim{\cal O}(1)$. Meanwhile, EDE is in tension with large-scale structure (LSS) data, due to shifts in the standard $\Lambda$CDM parameters necessary to fit the cosmic microwave background (CMB). One might hope that a proper treatment of the model, e.g., accounting for the SDC, may ameliorate the tension with LSS. Motivated by these considerations, we introduce the Early Dark Sector (EDS) model, wherein the mass of dark matter is exponentially sensitive to super-Planckian field excursions of the EDE scalar. The EDS model exhibits new phenomenology in both the early and late universe, the latter due to an EDE-mediated dark matter self-interaction. This dark matter-philic &#34;fifth force&#34;, while constrained to be small, remains active in the late universe and is not screened in virialized halos. We find that the new interaction with dark matter partially resolves the LSS tension. However, the marginalized posteriors are nonetheless consistent with $f_{\rm EDE}=0$ at 95$\%$ CL once the Dark Energy Survey Year 3 measurement of $S_8$ is included. We study constraints on the model from Atacama Cosmology Telescope data, and find a factor of two improvement on the error bar on the SDC parameter $c$, along with an increased preference for the EDE component. We discuss the implications of these constraints for the SDC, and find the tightest observational constraints to date on a swampland parameter, suggesting that an EDE description of cosmological data is in tension with the SDC.</p></li><li><p>Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&#34;Implicit Clothed humans Obtained from Normals&#34;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.</p></li><li><p>Object detection in high-resolution satellite imagery is emerging as a scalable alternative to on-the-ground survey data collection in many environmental and socioeconomic monitoring applications. However, performing object detection over large geographies can still be prohibitively expensive due to the high cost of purchasing imagery and compute. Inspired by traditional survey data collection strategies, we propose an approach to estimate object count statistics over large geographies through sampling. Given a cost budget, our method selects a small number of representative areas by sampling from a learnable proposal distribution. Using importance sampling, we are able to accurately estimate object counts after processing only a small fraction of the images compared to an exhaustive approach. We show empirically that the proposed framework achieves strong performance on estimating the number of buildings in the United States and Africa, cars in Kenya, brick kilns in Bangladesh, and swimming pools in the U.S., while requiring as few as 0.01% of satellite images compared to an exhaustive approach.</p></li><li><p>The complexity of atmospheric retrieval models is largely data-driven and one-dimensional models have generally been considered adequate with current data quality. However, recent studies have suggested that using 1D models in retrievals can result in anomalously cool terminator temperatures and biased abundance estimates even with existing transmission spectra of hot Jupiters. Motivated by these claims and upcoming high-quality transmission spectra we systematically explore the limitations of 1D models using synthetic and current observations. We use 1D models of varying complexity, both analytic and numerical, to revisit claims of biases when interpreting transmission spectra of hot Jupiters with inhomogeneous terminator compositions. Overall, we find the reported biases to be resulting from specific model assumptions rather than intrinsic limitations of 1D atmospheric models in retrieving current observations of asymmetric terminators. Additionally, we revise atmospheric retrievals of the hot Jupiter WASP-43b ($T_{\rm eq}=1440$ K) and the ultra-hot Jupiter WASP-103b ($T_{\rm eq}=2484$ K ) for which previous studies inferred abnormally cool atmospheric temperatures. We retrieve temperatures consistent with expectations. We note, however, that in the limit of extreme terminator inhomogeneities and high data quality some atmospheric inferences may conceivably be biased, although to a lesser extent than previously claimed. To address such cases, we implement a 2D retrieval framework for transmission spectra which allows accurate constraints on average atmospheric properties and provides insights into the spectral ranges where the imprints of atmospheric inhomogeneities are strongest. Our study highlights the need for careful considerations of model assumptions and data quality before attributing biases in retrieved estimates to unaccounted atmospheric inhomogeneities.</p></li><li><p>We obtain the full-color four-loop three-point form factor of the stress-tensor supermultiplet in N=4 SYM, based on the color-kinematics (CK) duality and generalized unitarity method. The CK-dual solution, while manifesting all dual Jacobi relations and satisfying the minimal power-counting of loop momenta, lies in a 133-dimensional solution space. We also show that the planar form factor integrand satisfies precisely a directional dual conformal symmetry in the lightlike limit of the operator momentum, which is supported by explicit four-loop calculations.</p></li><li><p>We demonstrate the existence of Q-balls in non-minimally coupled inflation models with a complex inflaton in the Palatini formulation of gravity. We show that there exist Q-ball solutions which are compatible with inflation and we derive a window in the inflaton mass squared for which this is the case. In particular, we confirm the existence of Q-ball solutions with $\phi \sim 10^{17}-10^{18} \GeV $, consistent with the range of field values following the end of slow-roll Palatini inflation. We study the Q-balls and their properties both numerically and in an analytical approximation. The existence of such Q-balls suggests that the complex inflaton condensate can fragment into Q-balls, and that there may be an analogous process for the case of a real inflaton with fragmentation to neutral oscillons. We discuss the possible post-inflationary cosmology following the formation of Q-balls, including an early Q-ball matter domination (eMD) period and the effects of this on the reheating dynamics of the model, gravitational wave signatures which may be detectable in future experiments, and the possibility that Q-balls could lead to the formation of primordial black holes (PBHs). In particular, we show that Palatini Q-balls with field strengths typical of inflaton condensate fragmentation can directly form black holes with masses around 500 kg or more when the self-coupling is $\lambda = 0.1$, resulting in very low (less than 100 GeV) reheating temperatures from black hole decay, with smaller black hole masses and larger reheating temperatures possible for smaller values of $\lambda$. Q-ball dark matter from non-minimally coupled Palatini inflation may also be a direction for future work.</p></li><li><p>Interactive object understanding, or what we can do to objects and how is a long-standing goal of computer vision. In this paper, we tackle this problem through observation of human hands in in-the-wild egocentric videos. We demonstrate that observation of what human hands interact with and how can provide both the relevant data and the necessary supervision. Attending to hands, readily localizes and stabilizes active objects for learning and reveals places where interactions with objects occur. Analyzing the hands shows what we can do to objects and how. We apply these basic principles on the EPIC-KITCHENS dataset, and successfully learn state-sensitive features, and object affordances (regions of interaction and afforded grasps), purely by observing hands in egocentric videos.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/hep-th">hep-th</a> arXiv:2112.09119v1 </p></div></div><p>The standard way to perform calculations for quantum field theories involves the S-matrix and the assumption that the theory is free at past and future infinity. However, this assumption may not hold for field theories in non-trivial backgrounds such as curved spacetimes or finite temperature. In this work we examine the situation at early times for Minkowski spacetime at finite temperature via the use of the Schwinger-Keldysh formalism. We find that there are additional cross terms between the real and imaginary time fields making our propagator matrix $3\times 3$ rather than the more familiar $2\times 2$. This suggests the theory is indeed not free at past infinity even for Minkowski spacetime at finite temperature.</p></li><li><p>Information retrieval is an important component in natural language processing, for knowledge intensive tasks such as question answering and fact checking. Recently, information retrieval has seen the emergence of dense retrievers, based on neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new domains or applications with no training data, and are often outperformed by term-frequency methods such as BM25 which are not supervised. Thus, a natural question is whether it is possible to train dense retrievers without supervision. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers, and show that it leads to strong retrieval performance. More precisely, we show on the BEIR benchmark that our model outperforms BM25 on 11 out of 15 datasets. Furthermore, when a few thousands examples are available, we show that fine-tuning our model on these leads to strong improvements compared to BM25. Finally, when used as pre-training before fine-tuning on the MS-MARCO dataset, our technique obtains state-of-the-art results on the BEIR benchmark.</p></li><li><p>Using a fully connected feedforward neural network we study topological invariants of a class of Calabi--Yau manifolds constructed as hypersurfaces in toric varieties associated with reflexive polytopes from the Kreuzer--Skarke database. In particular, we find the existence of a simple expression for the Euler number that can be learned in terms of limited data extracted from the polytope and its dual.</p></li><li><p>We consider level-set percolation for the Gaussian membrane model on $\mathbb{Z}^d$, with $d \geq 5$, and establish that as $h \in \mathbb{R}$ varies, a non-trivial percolation phase transition for the level-set above level $h$ occurs at some finite critical level $h_\ast$, which we show to be positive in high dimensions. Along $h_\ast$, two further natural critical levels $h_{\ast\ast}$ and $\overline{h}$ are introduced, and we establish that $-\infty &lt;\overline{h} \leq h_\ast \leq h_{\ast\ast} &lt; \infty$, in all dimensions. For $h &gt; h_{\ast\ast}$, we find that the connectivity function of the level-set above $h$ admits stretched exponential decay, whereas for $h &lt; \overline{h}$, chemical distances in the (unique) infinite cluster of the level-set are shown to be comparable to the Euclidean distance, by verifying conditions identified by Drewitz, Ráth and Sapozhnikov, see arXiv:1212.2885, for general correlated percolation models. As a pivotal tool to study its level-set, we prove novel decoupling inequalities for the membrane model.</p></li><li><p>In a recent experiment [Lin et al., arXiv:2112.07841], the superconducting phase hosted by a heterostructure of mirror-symmetric twisted trilayer graphene and WSe$_2$ was shown to exhibit significantly different critical currents in opposite directions in the absence of external magnetic fields. We here develop a microscopic theory and analyze necessary conditions for this zero-field superconducting diode effect. Taking into account the spin-orbit coupling induced in trilayer graphene via the proximity effect, we classify the pairing instabilities and normal-state orders and derive which combinations are consistent with the observed diode effect, in particular, its field trainability. We perform explicit calculations of the diode effect in several different models, including the full continuum model for the system, and illuminate the relation between the diode effect and finite-momentum pairing. Our theory also provides a natural explanation of the observed sign change of the current asymmetry with doping, which can be related to an approximate chiral symmetry of the system, and of the enhanced transverse resistance above the superconducting transition. Our findings not only elucidate the rich physics of trilayer graphene on WSe$_2$, but also establish a means to distinguish between various candidate interaction-induced orders in spin-orbit-coupled graphene moiré systems, and could therefore serve as a guide for future experiments as well.</p></li><li><div><div><p>Dec 17 2021 <a href="http://harihareswara.net/arxiv/hep-ex">hep-ex</a> arXiv:2112.09114v1 </p></div></div><p>The top quark pair production cross section is measured in proton-proton collisions at a center-of-mass energy of 5.02 TeV. The data were collected in a special LHC low-energy and low-intensity run in 2017, and correspond to an integrated luminosity of 302 pb$^{-1}$. The measurement is performed using events with one electron and one muon of opposite charge, and at least two jets. The measured cross section is 60.7 $\pm$ 5.0 (stat) $\pm$ 2.8 (syst) $\pm$ 1.1 (lumi) pb. To reduce the statistical uncertainty, a combination with the result in the single lepton + jets channel, based on data collected in 2015 at the same center-of-mass energy and corresponding to an integrated luminosity of 27.4 pb$^{-1}$, is then performed. The resulting measured value is 63.0 $\pm$ 4.1 (stat) $\pm$ 3.0 (syst+lumi) pb, in agreement with the standard model prediction of 66.8$^{+2.9}_{-3.1}$ pb.</p></li><li><p>Eikonal exponentiation in QFT describes the emergence of classical physics at long distances in terms of a non-trivial resummation of infinitely many diagrams. Long ago, &#39;t Hooft proposed a beautiful correspondence between ultra-relativistic scalar eikonal scattering and one-to-one scattering in a background shockwave space-time, bypassing the need to resum. In this spirit, we propose here a covariant method for computing one-to-one amplitudes in curved background space-times which gives rise what we conjecture to be a general expression for the eikonal amplitude. We show how the one-to-one scattering amplitude for scalars on any stationary space-time reduces to a boundary term that captures the long-distance behavior of the background and has the structure of an exponentiated eikonal amplitude. In the case of scalar scattering on Schwarzschild, we recover the known results for gravitational scattering of massive scalars in the eikonal regime. For Kerr, we find a remarkable exponentiation of the tree-level amplitude for gravitational scattering between a massive scalar and a massive particle of infinite spin. This amplitude exhibits a Kawai-Lewellen-Tye-like factorization, which we use to evaluate the eikonal amplitude in momentum space, and study its analytic properties.</p></li><li><p>We analyse the dynamics of the pullback of the map $z \longmapsto z^m$ on the $n$-dimensional complex torus and toric varieties. We will observe that tropical objects naturally appear in the weak limit, and review several theorems in tropical geometry.</p></li></ul></div></div></div>
  </body>
</html>
