<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/alxyng/flappybird">Original</a>
    <h1>Flappy Bird in 1000 lines of C</h1>
    
    <div id="readability-page-1" class="page"><p>AWS <a href="https://aws.amazon.com/blogs/aws/new-amazon-s3-tables-storage-optimized-for-analytics-workloads/">announced S3 Tables yesterday</a>, which brings native support
for <a href="https://iceberg.apache.org/">Apache Iceberg</a> to S3. It‚Äôs hard to overstate how exciting this is for the
data analytics ecosystem. Here‚Äôs a quick rundown of my thoughts so far:</p><div>
  <li>
    <p><strong>The integration is deep</strong>.</p>

    <p>This isn‚Äôt a separate service that sits on top of S3. Rather AWS has added a
new type of bucket to the S3 service itself: a <em><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-buckets.html">table bucket</a></em>.</p>

    <p>This design seems to be S3‚Äôs new standard practice for these major,
paradigm-shifting features. Express One Zone works analogously: you need to
create a <em>directory bucket</em> to use the Express One Zone storage class. I‚Äôm
not sure whether this stems from an underlying technical constraint or
whether this is simply an API design choice.</p>

    <p>Table buckets come with a host of <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_Amazon_S3_Tables.html">new APIs</a>. It‚Äôs the stuff you‚Äôd expect for
working with Iceberg tables. To name a few: <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_s3TableBuckets_CreateNamespace.html"><code>CreateNamespace</code></a>,
<a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_s3TableBuckets_CreateTable.html"><code>CreateTable</code></a>, <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_s3TableBuckets_ListTables.html"><code>ListTables</code></a>, <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_s3TableBuckets_RenameTable.html"><code>RenameTable</code></a>,
<a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_s3TableBuckets_PutTableMaintenanceConfiguration.html"><code>PutTableMaintenanceConfiguration</code></a>.</p>

    <p>Table maintenance (i.e., data file compaction, snapshot management, leaked
file cleanup) is <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-maintenance-overview.html">handled automatically</a> by S3. You‚Äôre billed
for the maintenance operations, though‚Äîmore on this later‚Äîand so you can
disable automatic maintenance on a per-table basis if you‚Äôd prefer to handle
maintenance yourself.</p>

    <p>Integration with AWS analytics services (Athena, Redshift, EMR, QuickSight,
Data Firehouse) is possible but goofy. It requires enabling an AWS Glue Data
Catalog feature<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> that automatically mirrors the catalogs from the S3 table
buckets in your account like so:</p>

    <p><img src="https://docs.aws.amazon.com/images/AmazonS3/latest/userguide/images/S3Tables-glue-catalog.png" alt="Glue Data Catalog"/></p>
  </li>
  <li>
    <p><strong>The price seems right.</strong></p>

    <p>Here‚Äôs the quick cost comparison against S3 standard buckets:</p>

    <table>
      <thead>
        <tr>
          <th>Resource</th>
          <th>S3 standard bucket</th>
          <th>S3 table bucket</th>
          <th>Œî</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Storage</td>
          <td>$0.023 per GB-month</td>
          <td>$0.0265 per GB-month</td>
          <td>+15%</td>
        </tr>
        <tr>
          <td>PUTs</td>
          <td>$0.005 per 1k reqs</td>
          <td>$0.005 per 1k reqs</td>
          <td>0%</td>
        </tr>
        <tr>
          <td>GETs</td>
          <td>$0.0004 per 1k reqs</td>
          <td>$0.0004 per 1k reqs</td>
          <td>0%</td>
        </tr>
        <tr>
          <td>Monitoring</td>
          <td>‚Äî</td>
          <td>$0.025 per 1k objects</td>
          <td>‚àû</td>
        </tr>
        <tr>
          <td>Compaction</td>
          <td>‚Äî</td>
          <td>$0.004 per 1k objects</td>
          <td>‚àû</td>
        </tr>
        <tr>
          <td>Compaction</td>
          <td>‚Äî</td>
          <td>$0.05 per GB processed</td>
          <td>‚àû</td>
        </tr>
      </tbody>
    </table>

    <p>The price of PUT and GET requests are unchanged. Storage costs are 15%
higher, which seems tolerable. A 15% increase on ‚Äúvery cheap‚Äù is still
‚Äúvery cheap.‚Äù</p>

    <p>My quick back of the envelope calculation is that monitoring costs will be
immaterial, assuming a 100MB+ average file size. A 1TB table will cost
$27.14/mo in storage costs, but only $0.26/mo in monitoring costs.</p>

    <p>Compaction costs are more of a mixed bag. For analytic workloads that write
infrequently, they also look to be immaterial. But for streaming workloads
that write frequently (say, once per second, or once per every ten seconds),
compaction costs may be prohibitive. The cost per object processed looks
tolerable (writing an object per second results in only 2.5MM objects per
month that need to be compacted), but write amplification will be severe,
and the cost per GB processed is likely to add up.</p>

    <p>To really get a sense for compaction costs, someone will need to run some
experiments. A lot depends on how often S3 <em>chooses</em> to compact data files
for a given workload, which is not something that‚Äôs directly under the user‚Äôs
control.</p>
  </li>
  <li>
    <p><strong>The effort required for a data tool to write Iceberg tables has dropped
dramatically.</strong></p>

    <p>While most OLAP tools today have good support for reading Iceberg tables,
only a lucky few tools (Databricks, Snowflake, Spark, Hive, Flink, to
name the big players) have good support for <em>writing</em> Iceberg tables.</p>

    <p>Notably, <a href="https://github.com/duckdb/duckdb_iceberg/issues/37">DuckDB can‚Äôt write Iceberg tables</a>,
<a href="https://github.com/ClickHouse/ClickHouse/issues/49973">ClickHouse can‚Äôt write Iceberg tables</a>,
and Amazon Redshift can‚Äôt write Iceberg tables. The system I work on, <a href="https://materialize.com/">Materialize</a>,
also can‚Äôt write Iceberg tables.</p>

    <p>Why? To read an Iceberg table, all you need is to do is parse a bit of
metadata on S3 will tell you which Parquet data files to download and decode.
But to <em>write</em> an Iceberg table requires much more care. Each write to an
Iceberg table creates new data files in S3. Over time, the table‚Äôs data gets
split across many small files, and reading the data from the table becomes
prohibitively slow. The solution is to periodically <a href="https://iceberg.apache.org/docs/1.5.1/maintenance/#compact-data-files">compact the
table</a>, which combines the data files into fewer larger files and
drops any data that has been overwritten/deleted. See <a href="https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/">this article from
Dremio</a> for a better explanation.</p>

    <p>The only way I know of today to compact an Iceberg table is via running a
Spark or Flink job that looks like this:</p>

    <div><div><pre><code><span>Table</span> <span>table</span> <span>=</span> <span>...</span>
<span>SparkActions</span>
 <span>.</span><span>get</span><span>()</span>
 <span>.</span><span>rewriteDataFiles</span><span>(</span><span>table</span><span>)</span>
 <span>.</span><span>execute</span><span>();</span>
</code></pre></div>    </div>

    <p>Neither the Python Iceberg library nor the Rust Iceberg
library today support compaction
(<a href="https://github.com/apache/iceberg-python/issues/1092">apache/iceberg-python#1092</a>,
<a href="https://github.com/apache/iceberg-rust/issues/624">apache/iceberg-rust#624</a>).<sup id="fnref:3"><a href="#fn:3" rel="footnote" role="doc-noteref">2</a></sup>
So unless your OLAP system is written in Java or another JVM-based language,
you‚Äôre basically out of luck if you wanted to write Iceberg tables.</p>

    <p>That changes with S3 table buckets, which handle all the required maintenance
operations for Iceberg automatically, in the background, for a small fee.
Compaction no longer needs to be reinvented in each system that wants to
write Iceberg tables.</p>

    <p>As a developer of a system that wants to write Iceberg tables without
managing Iceberg compaction, I see this as a huge win.<sup id="fnref:4"><a href="#fn:4" rel="footnote" role="doc-noteref">3</a></sup> It‚Äôs early days,
but when we build an Iceberg sink for Materialize, we‚Äôre planning to lean on
S3 table buckets to handle compaction. This is essentially a bet that other
object storage technologies (Google Cloud Storage, Azure Blob Storage,
Cloudflare R2, MinIO, Ceph), will eventually follow suit and provide their
own equivalent of S3 table buckets that transparently handle Iceberg table
maintenance.</p>
  </li>
  <li>
    <p><strong>Perhaps Iceberg REST catalogs can go quietly into the night?</strong> üå∂Ô∏è<sup id="fnref:5"><a href="#fn:5" rel="footnote" role="doc-noteref">4</a></sup></p>

    <p><a href="https://iceberg.apache.org/concepts/catalog/">Iceberg catalogs</a> keep track of which Iceberg tables live at which path in
the S3 bucket and, crucially, which files within that path comprise the live
version of the table. This catalog is key to the consistency guarantees that
have made Iceberg successful. It‚Äôs what ensures that two systems attempting
to write to the table at the same time don‚Äôt corrupt each other‚Äôs writes.
Reading from an Iceberg table doesn‚Äôt require interacting with the
catalog<sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">5</a></sup>, but writing to an Iceberg table always does.</p>

    <p>Dealing with the catalog is, unfortunately, quite annoying. There is not one
canonical Iceberg catalog, but rather several catalog implementations. Some
catalog implementations exist only as Java libraries. Some catalog
implementations are REST services that you can interact with over HTTP from
any language.</p>

    <p>If you‚Äôre lucky, someone in your organization has already set up an Iceberg
catalog for you to use. If you‚Äôre unlucky, you‚Äôll need to evaluate the
several possible options (Hive metastore? JDBC? AWS Glue? Polaris?), figure
out how to get it deployed into your production environment, and then figure
out how to set up authentication and distribute credentials to all your 
Iceberg-using applications.</p>

    <p>Now, with S3 table buckets, AWS has introduced yet another catalog
implementation. It‚Äôs called, unsurprisingly, the <code>S3TablesCatalog</code>. The
source code for the <code>S3TablesCatalog</code> is available on GitHub:
<a href="https://github.com/awslabs/s3-tables-catalog">https://github.com/awslabs/s3-tables-catalog</a>. It‚Äôs a pretty straightforward
wrapper over the aforementioned <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_Amazon_S3_Tables.html">S3 tables API</a>. In fact most
Iceberg catalog operations are one-to-one with S3 tables API operations.<sup id="fnref:6"><a href="#fn:6" rel="footnote" role="doc-noteref">6</a></sup></p>

    <p>For me personally, as an end user of Iceberg, this actually seems <em>great</em>. I
don‚Äôt want to think about which catalog implementation to use. I don‚Äôt want
to have to run a separate catalog service. I don‚Äôt want to have to set up
authentication for my catalog that‚Äôs separate from my S3 bucket.</p>

    <p>S3 tables take all these questions off the table. I just use the catalog
implementation that comes built-in to where I‚Äôm storing my data. The slickest
part here might be the authentication story: the same AWS IAM policy that
grants an application access to the data files can also grant access to the
necessary catalog APIs.</p>

    <p>In my ideal world, all the major object store providers would eventually
provide table buckets with integrated Iceberg catalogs, and we could do away
with separate catalog services entirely. üå∂Ô∏è<sup id="fnref:5:1"><a href="#fn:5" rel="footnote" role="doc-noteref">4</a></sup></p>

    <p>Are there niche uses for custom catalog implementations that I‚Äôm missing? The
catalog API is so straightforward that I just can‚Äôt imagine what
differentiated value a separate catalog implementation (e.g., Polaris) could
provide over the S3-native catalog.</p>
  </li>
  <li>
    <p><strong>The S3 feature drought is over.</strong></p>

    <p>S3 has now launched three major features in 12 months. Last November we got
<a href="https://aws.amazon.com/blogs/aws/new-amazon-s3-express-one-zone-high-performance-storage-class/">S3 Express One Zone</a> and this summer we got <a href="https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/">conditional writes</a>.</p>

    <p>Maybe it‚Äôs just my perception, but it feels like something material has
shifted in S3‚Äôs development culture. During S3‚Äôs first decade of life we
regularly saw major feature releases (Glacier, object versioning, object
lifecycle rules, multi-region buckets), but those dried up around 2015‚Äîuntil
now.</p>
  </li>
  <li>
    <p><strong>The name implies more table formats could be added in the future</strong>.</p>

    <p>It‚Äôs telling that the name of the new feature is ‚ÄúS3 Tables‚Äù and not ‚ÄúS3
Iceberg.‚Äù It seems like AWS wanted to leave the door open to supporting other
table formats in the future.</p>

    <p>Perhaps we‚Äôll see support for <a href="https://hudi.apache.org/">Apache Hudi</a> or <a href="https://paimon.apache.org/">Apache Paimon</a> tables in the
future, or an as-yet-to-be-developed open table format. AWS seems to be
<a href="https://aws.amazon.com/blogs/big-data/choosing-an-open-table-format-for-your-transactional-data-lake-on-aws/">tracking the open table format ecosystem</a> quite closely.</p>
  </li>
</div></div>
  </body>
</html>
