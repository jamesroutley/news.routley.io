<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://healeycodes.com/profiling-and-optimizing-an-interpreter">Original</a>
    <h1>Profiling and Optimizing an Interpreter</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>In my last post, I <a href="https://healeycodes.com/adding-for-loops-to-an-interpreter">added for loops to my interpreter</a> for the <a href="https://github.com/healeycodes/nodots-lang">nodots</a> programming language. Today, I&#39;m profiling and optimizing the same interpreter.</p><p>Let&#39;s set some goals and requirements.</p><ul><li>The interpreter should execute a benchmark program X% faster</li><li>My total time spent on research and coding should not exceed a few hours</li></ul><p>I&#39;m not sure what a good <em>X</em> value is here. Perhaps 25% faster? Something that feels like progress. The time limit on research/coding is because otherwise I could just spend days rewriting the interpreter (e.g. in a faster language than Python) to achieve the goal.</p><p>Almost immediately, we&#39;re bumping into the limitations of the tree-walk architecture of <em>nodots</em>.</p><ul><li>The interpreter needs to recursively walk through the entire parse tree. This is especially impactful for programs with large or complex parse trees.</li><li>Because the interpreter needs to keep the entire parse tree in memory, it can use a lot of memory. Again, this is most true for large programs/complex parse trees.</li><li>A lack of optimization paths. Advanced techniques that other types of interpreters have, such as JIT compilation or static superinstructions, just aren&#39;t available.</li></ul><h2 id="quick-note-on-optimizing-software">Quick Note on Optimizing Software</h2><p>To prioritize development efficiency, you should <a href="http://wiki.c2.com/?ProfileBeforeOptimizing">Profile Before Optimizing</a>:</p><blockquote><p>Write code according to constraints besides performance (clarity, flexibility, brevity). Then, after the code is actually written:</p><ol start="1"><li>See if you actually need to speed it up.</li><li>Profile the code to see where it&#39;s actually spending its time.</li><li>Focus on the few high-payoff areas and leave the rest alone.</li></ol></blockquote><p>Profiling before optimizing code allows you to identify which specific parts of your code are causing performance bottlenecks, so you know where to focus any optimization efforts. Without profiling, you may spend a lot of time and effort optimizing parts of your code that are not actually causing performance issues. Additionally, profiling can also help you identify potential bugs or other issues in your code that may be contributing to poor performance.</p><p>With interpreters, I&#39;m most interested in optimizing things that happen over and over. I&#39;m interested in performance wins that scale with the amount of computation a user is doing.</p><h2 id="the-benchmark-program">The Benchmark Program</h2><p>To measure the current performance, and any improvements, we&#39;ll be using a naive recursive fibonacci function that runs in a loop. The parse tree it creates has 193 nodes and it also generates a ton of stack frames. It takes 803ms to run on an M1 Pro. The same program written in JavaScript takes 3ms to run in Chrome.</p><p>I measured execution time by running <code>time python3 benchmark.py</code> three times and taking the best result. I used wall clock time (<a href="https://stackoverflow.com/a/556411">more on the time command</a>), and Python 3.10.9.</p><pre><div><div><p><span># benchmark.py</span><span></span></p><p><span></span><span>from</span><span> interpreter </span><span>import</span><span> interpret</span></p><p><span>program </span><span>=</span><span> </span><span>&#34;&#34;&#34;</span></p><p><span>for (i = 0; i &lt; 20; i = i + 1)</span></p><p><span>  # gets called 35400 times</span></p><p><span>  fun fib(x)</span></p><p><span>    if (x == 0 or x == 1)</span></p><p><span>        return x;</span></p><p><span>    fi</span></p><p><span>    return fib(x - 1) + fib(x - 2);</span></p><p><span>  nuf</span></p><p><span>  fib(i);</span></p><p><span>rof</span></p><p><span>&#34;&#34;&#34;</span><span></span></p><p><span>interpret</span><span>(</span><span>program</span><span>,</span><span> opts</span><span>=</span><span>{</span><span>&#34;debug&#34;</span><span>:</span><span> </span><span>False</span><span>}</span><span>)</span></p></div></div></pre><p><a href="https://docs.python.org/3/library/profile.html#module-cProfile">cProfile</a> is a built-in Python module that measures the performance of Python code. It profiles the performance of a script by measuring the time it takes for different parts of the code to execute, as well as the number of calls made to each function. This information can then be used to identify bottlenecks in the code. We&#39;ll be using it to find the most time-consuming part of the interpreter as it executes the benchmark program.</p><p>Let&#39;s run cProfile as a script to profile the benchmark program.</p><pre><div><div><p><span>python3 -m cProfile -s tottime benchmark.py</span></p></div></div></pre><p>We&#39;re sorting by <code>tottime</code> — the total time spent in the given function (excluding time made in calls to sub-functions). Here&#39;s the trimmed output:</p><pre><div><div><p><span>9933786 function calls (7853037 primitive calls) in 1.714 seconds</span></p><p><span>   Ordered by: internal time</span></p><p><span>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span></p><p><span>265573/103    0.209    0.000    1.587    0.015 interpreter.py:154(eval_call)</span></p><p><span>  1389926    0.207    0.000    0.258    0.000 lexer.py:253(__eq__)</span></p><p><span>212483/83    0.121    0.000    1.587    0.019 interpreter.py:263(eval_term)</span></p><p><span>106262/62    0.072    0.000    1.587    0.026 interpreter.py:355(eval_logic_or)</span></p><p><span>141662/62    0.063    0.000    1.587    0.026 interpreter.py:317(eval_equality)</span></p><p><span>   817477    0.063    0.000    0.063    0.000 tree.py:58(meta)</span></p><p><span>265573/103    0.062    0.000    1.587    0.015 interpreter.py:205(eval_unary)</span></p><p><span>265573/103    0.060    0.000    1.587    0.015 interpreter.py:231(eval_factor)</span></p><p><span>   159351    0.060    0.000    0.111    0.000 interpreter.py:132(eval_identifier)</span></p></div></div></pre><p>Note: with profiling overhead, the benchmark script takes more than twice as long.</p><p>The list of <code>eval_*</code> calls are expected. <code>eval_call</code> handles <em>nodots</em> function calls and <code>eval_term</code> handles addition and subtraction. The benchmark program does both these things in abundance. But two lines here stand out: <code>lexer.py:253(__eq__)</code> and <code>tree.py:58(meta)</code>. These function calls are from library code (<em>nodots</em> uses the <a href="https://github.com/lark-parser/lark">Lark</a> parsing toolkit).</p><p><em>nodots</em> recurses on the parse tree that Lark creates. However, as we&#39;ll soon discover, these Tree/Token objects that Lark creates are generic and flexible (so that the library is useful and widely applicable) but aren&#39;t perfectly tuned for performance.</p><p>In Python, <a href="https://docs.python.org/3/reference/datamodel.html#object.__eq__">__eq__</a> is a “rich comparison” method. It&#39;s called when two objects are compared with <code>==</code>. If we look at <a href="https://github.com/lark-parser/lark/blob/f3d79040e2ff59e11661e7b43f593f1334951205/lark/lexer.py#L253">lexer.py:253(__eq__)</a>, we can see the Lark code that&#39;s being called <strong>over one million times</strong> during our benchmark:</p><pre><div><div><p><span># lexer.py:253(__eq__) f3d79040</span><span></span></p><p><span></span><span># a method on the Token class</span><span></span></p><p><span></span><span>def</span><span> </span><span>__eq__</span><span>(</span><span>self</span><span>,</span><span> other</span><span>)</span><span>:</span><span></span></p><p><span>        </span><span>if</span><span> </span><span>isinstance</span><span>(</span><span>other</span><span>,</span><span> Token</span><span>)</span><span> </span><span>and</span><span> self</span><span>.</span><span>type</span><span> </span><span>!=</span><span> other</span><span>.</span><span>type</span><span>:</span><span></span></p><p><span>            </span><span>return</span><span> </span><span>False</span><span></span></p><p><span>        </span><span>return</span><span> </span><span>str</span><span>.</span><span>__eq__</span><span>(</span><span>self</span><span>,</span><span> other</span><span>)</span></p></div></div></pre><p>In our interpreter, Tokens are compared to strings to pick the correct evaluation branch e.g. when handling an operator like <code>&gt;=</code>. The above code seems like it&#39;s doing more work than is necessary. In theory, it can be reduced down to a single compare operation between two things that <em>already exist</em> in memory.</p><p><a href="https://github.com/lark-parser/lark/blob/f3d79040e2ff59e11661e7b43f593f1334951205/lark/tree.py#L58">tree.py:58(meta)</a> isn&#39;t as bad but perhaps accessing metadata would be overall quicker if the  metadata object was smaller in memory, like maybe a named tuple? That&#39;s just a guess though.</p><pre><div><div><p><span># tree.py(meta) f3d79040</span><span></span></p><p><span></span><span># a property on the Tree class</span><span></span></p><p><span></span><span>@property</span><span></span></p><p><span>    </span><span>def</span><span> </span><span>meta</span><span>(</span><span>self</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> Meta</span><span>:</span><span></span></p><p><span>        </span><span>if</span><span> self</span><span>.</span><span>_meta </span><span>is</span><span> </span><span>None</span><span>:</span><span></span></p><p><span>            self</span><span>.</span><span>_meta </span><span>=</span><span> Meta</span><span>(</span><span>)</span><span></span></p><p><span>        </span><span>return</span><span> self</span><span>.</span><span>_meta</span></p></div></div></pre><p>My gut says to write my own Tree/Token classes and to make them simple and straightforwards. Since I don&#39;t need to support everything a library needs to support, they should run quicker by doing less work.</p><h2 id="building-a-new-tree">Building a New Tree</h2><p>The benchmark program spends ~16% of its time in the two methods above. Since we&#39;re identified that more work is being performed than our use case requires, we can build our own versions of these classes. We&#39;ll build a new tree by consuming the parse tree ahead of any evaluation. We only need to iterate the old parse tree once in order to create the new parse tree so the run time we add will be very small compared to the performance improvements.</p><p>One hard-to-measure benefit of the new tree is that it will have less data in it, so the frequently accessed nodes have a higher chance of sticking around in the CPU cache.</p><p>The interpreter uses Lark&#39;s <code>Tree</code> and <code>Token</code> classes in <code>interpreter.py</code> so we first change their import names to make the existing type hints happy.</p><pre><div><div><p><span>-</span><span> from lark import Lark, Tree, Token</span></p><p><span></span><span>+</span><span> from lark import Lark, Tree as LarkTree, Token as LarkToken</span></p></div></div></pre><p>Next, we create some small classes to represent the things we&#39;re replacing from the Lark library. <code>Tree</code>, <code>Token</code>, and <code>Meta</code>.</p><pre><div><div><p><span>+</span><span> Meta </span><span>=</span><span> typing</span><span>.</span><span>NamedTuple</span><span>(</span><span>&#34;Meta&#34;</span><span>,</span><span> </span><span>[</span><span>(</span><span>&#34;line&#34;</span><span>,</span><span> </span><span>int</span><span>)</span><span>,</span><span> </span><span>(</span><span>&#34;column&#34;</span><span>,</span><span> </span><span>int</span><span>)</span><span>]</span><span>)</span><span></span></p><p><span></span><span>+</span><span> </span></p><p><span></span><span>+</span><span> </span><span>class</span><span> </span><span>Tree</span><span>:</span><span></span></p><p><span></span><span>+</span><span>     kind </span><span>=</span><span> </span><span>&#34;tree&#34;</span><span></span></p><p><span></span><span>+</span><span> </span></p><p><span></span><span>+</span><span>     </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> data</span><span>:</span><span> </span><span>str</span><span>,</span><span> meta</span><span>:</span><span> Meta</span><span>,</span><span> children</span><span>:</span><span> List</span><span>[</span><span>Tree </span><span>|</span><span> Token</span><span>]</span><span>)</span><span>:</span><span></span></p><p><span></span><span>+</span><span>         self</span><span>.</span><span>data </span><span>=</span><span> data</span></p><p><span></span><span>+</span><span>         self</span><span>.</span><span>meta </span><span>=</span><span> meta</span></p><p><span></span><span>+</span><span>         self</span><span>.</span><span>children </span><span>=</span><span> children</span></p><p><span></span><span>+</span><span> </span></p><p><span></span><span>+</span><span> </span><span>class</span><span> </span><span>Token</span><span>:</span><span></span></p><p><span></span><span>+</span><span>     kind </span><span>=</span><span> </span><span>&#34;token&#34;</span><span></span></p><p><span></span><span>+</span><span>     data </span><span>=</span><span> </span><span>&#34;token&#34;</span><span></span></p><p><span></span><span>+</span><span>     children</span><span>:</span><span> List</span><span>[</span><span>Any</span><span>]</span><span> </span><span>=</span><span> </span><span>[</span><span>]</span><span></span></p><p><span></span><span>+</span><span> </span></p><p><span></span><span>+</span><span>     </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> value</span><span>:</span><span> </span><span>str</span><span>,</span><span> meta</span><span>:</span><span> Meta</span><span>)</span><span>:</span><span></span></p><p><span></span><span>+</span><span>         self</span><span>.</span><span>value </span><span>=</span><span> value</span></p><p><span></span><span>+</span><span>         self</span><span>.</span><span>meta </span><span>=</span><span> meta</span></p><p><span></span><span>+</span><span> </span></p><p><span></span><span>+</span><span>     </span><span># one expression rather than Lark&#39;s three expressions!</span><span></span></p><p><span></span><span>+</span><span>     </span><span>def</span><span> </span><span>__eq__</span><span>(</span><span>self</span><span>,</span><span> other</span><span>)</span><span>:</span><span></span></p><p><span></span><span>+</span><span>         </span><span>return</span><span> self</span><span>.</span><span>value </span><span>==</span><span> other</span></p></div></div></pre><p>Now, we need to write a function that will recurse over Lark&#39;s parse tree and use these new classes to build a new tree that will use the faster methods.</p><pre><div><div><p><span>+</span><span> </span><span>def</span><span> </span><span>build_nodots_tree</span><span>(</span><span>children</span><span>:</span><span> List</span><span>[</span><span>LarkTree </span><span>|</span><span> LarkToken</span><span>]</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> List</span><span>[</span><span>Tree </span><span>|</span><span> Token</span><span>]</span><span>:</span><span></span></p><p><span></span><span>+</span><span>     </span><span>return</span><span> </span><span>[</span><span></span></p><p><span></span><span>+</span><span>         Tree</span><span>(</span><span></span></p><p><span></span><span>+</span><span>             data</span><span>=</span><span>str</span><span>(</span><span>child</span><span>.</span><span>data</span><span>)</span><span>,</span><span></span></p><p><span></span><span>+</span><span>             meta</span><span>=</span><span>Meta</span><span>(</span><span>child</span><span>.</span><span>meta</span><span>.</span><span>line</span><span>,</span><span> child</span><span>.</span><span>meta</span><span>.</span><span>column</span><span>)</span><span>,</span><span></span></p><p><span></span><span>+</span><span>             children</span><span>=</span><span>build_nodots_tree</span><span>(</span><span>child</span><span>.</span><span>children</span><span>)</span><span>,</span><span></span></p><p><span></span><span>+</span><span>         </span><span>)</span><span></span></p><p><span></span><span>+</span><span>         </span><span>if</span><span> </span><span>isinstance</span><span>(</span><span>child</span><span>,</span><span> LarkTree</span><span>)</span><span></span></p><p><span></span><span>+</span><span>         </span><span>else</span><span> Token</span><span>(</span><span>value</span><span>=</span><span>child</span><span>.</span><span>value</span><span>,</span><span> meta</span><span>=</span><span>Meta</span><span>(</span><span>child</span><span>.</span><span>line</span><span>,</span><span> child</span><span>.</span><span>column</span><span>)</span><span>)</span><span>  </span><span># type: ignore</span><span></span></p><p><span></span><span>+</span><span>         </span><span>for</span><span> child </span><span>in</span><span> children</span></p><p><span></span><span>+</span><span>     </span><span>]</span><span></span></p><p><span></span><span># ...</span><span></span></p><p><span></span><span>def</span><span> </span><span>interpret</span><span>(</span><span>source</span><span>:</span><span> </span><span>str</span><span>,</span><span> opts</span><span>=</span><span>{</span><span>&#34;debug&#34;</span><span>:</span><span> </span><span>True</span><span>}</span><span>)</span><span>:</span><span></span></p><p><span>    root_context </span><span>=</span><span> Context</span><span>(</span><span>None</span><span>,</span><span> opts</span><span>=</span><span>opts</span><span>)</span><span></span></p><p><span>    inject_standard_library</span><span>(</span><span>root_context</span><span>)</span><span></span></p><p><span>    </span><span>try</span><span>:</span><span></span></p><p><span></span><span>-</span><span>         root </span><span>=</span><span> parser</span><span>.</span><span>parse</span><span>(</span><span>source</span><span>)</span><span></span></p><p><span></span><span>+</span><span>         root </span><span>=</span><span> build_nodots_tree</span><span>(</span><span>[</span><span>parser</span><span>.</span><span>parse</span><span>(</span><span>source</span><span>)</span><span>]</span><span>)</span><span>[</span><span>0</span><span>]</span><span></span></p><p><span>        result </span><span>=</span><span> eval_program</span><span>(</span><span>root</span><span>,</span><span> context</span><span>=</span><span>root_context</span><span>)</span><span></span></p><p><span>        </span><span>return</span><span> result</span></p><p><span>    </span><span>except</span><span> LanguageError </span><span>as</span><span> e</span><span>:</span><span></span></p><p><span>        </span><span>return</span><span> e</span></p></div></div></pre><p>When we profile the benchmark program again, we no longer see Lark&#39;s slow functions near the top! Our handwritten <code>__eq__</code> call is ~700% faster!</p><pre><div><div><p><span>6518309 function calls (4437162 primitive calls) in 1.182 seconds</span></p><p><span>   Ordered by: internal time</span></p><p><span>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span></p><p><span>265573/103    0.153    0.000    1.052    0.010 interpreter.py:205(eval_call)</span></p><p><span>212483/83    0.099    0.000    1.053    0.013 interpreter.py:314(eval_term)</span></p><p><span>141662/62    0.060    0.000    1.053    0.017 interpreter.py:368(eval_equality)</span></p><p><span>265573/103    0.058    0.000    1.052    0.010 interpreter.py:256(eval_unary)</span></p><p><span>265573/103    0.058    0.000    1.052    0.010 interpreter.py:282(eval_factor)</span></p><p><span>106262/62    0.057    0.000    1.053    0.017 interpreter.py:406(eval_logic_or)</span></p><p><span>2018725/2017623    0.052    0.000    0.052    0.000 {built-in method builtins.len}</span></p><p><span>212462/62    0.049    0.000    1.053    0.017 interpreter.py:339(eval_comparison)</span></p><p><span>   159351    0.038    0.000    0.062    0.000 interpreter.py:183(eval_identifier)</span></p></div></div></pre><p>The run time of the benchmark program has been reduced to 582ms, a speed-up of 28%!</p><h2 id="micro-optimizations">Micro Optimizations</h2><p>I&#39;ve ran out of time to make any more large improvements but here are some micro optimizations just for fun. They aren&#39;t worth the added code complexity so I won&#39;t commit them.</p><p>In Python, the <code>__slots__</code> attribute tells the interpreter to use a more memory-efficient storage strategy for an object&#39;s instance variables. Instead of each instance having a dictionary to store its attributes, instances created from a class with <code>__slots__</code> defined will use a fixed-size array.</p><p>This can sometimes lead to improved performance, especially for classes with many instances. The drawback is that you can&#39;t have additional attributes added dynamically.</p><p>If we add <code>__slots__</code> to <code>Tree</code> and <code>Token</code> we get an additional ~1% speed-up.</p><pre><div><div><p><span>class</span><span> </span><span>Tree</span><span>:</span><span></span></p><p><span></span><span>+</span><span>    __slots__ </span><span>=</span><span> </span><span>[</span><span>&#39;data&#39;</span><span>,</span><span> </span><span>&#39;meta&#39;</span><span>,</span><span> </span><span>&#39;children&#39;</span><span>]</span><span></span></p><p><span></span><span># ..</span><span></span></p><p><span></span><span>class</span><span> </span><span>Token</span><span>:</span><span></span></p><p><span></span><span>+</span><span>    __slots__ </span><span>=</span><span> </span><span>[</span><span>&#39;value&#39;</span><span>,</span><span> </span><span>&#39;meta&#39;</span><span>]</span><span></span></p><p><span></span><span># ..</span></p></div></div></pre><p>Another thing we can do is cache the objects we&#39;re using. In <em>nodots</em> everything is compared by value so any two numbers, strings, or booleans are equal if their values are equal.</p><p>Small Integer caching is a technique used in Python to optimize memory usage by reusing commonly used small integers (-5 to 256). Instead of creating a new object for each small integer, Python reuses the same object. This helps to reduce the amount of memory used and improves performance.</p><p>If we steal this idea for <em>nodots</em> and pre-generate the integers 0 to 1024, and also <code>true</code> and <code>false</code>, we get an additional ~1% speed-up.</p><pre><div><div><p><span># initialize these once at the start of the interpreter</span><span></span></p><p><span>initial_numbers </span><span>=</span><span> </span><span>[</span><span>NumberValue</span><span>(</span><span>i</span><span>)</span><span> </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1024</span><span>)</span><span>]</span><span></span></p><p><span>initial_boolvalue_true </span><span>=</span><span> BoolValue</span><span>(</span><span>True</span><span>)</span><span></span></p><p><span>initial_boolvalue_false </span><span>=</span><span> BoolValue</span><span>(</span><span>False</span><span>)</span><span></span></p><p><span></span><span># everywhere we create a new BoolValue use the initial ones</span><span></span></p><p><span></span><span># instead of:</span><span></span></p><p><span></span><span>return</span><span> BoolValue</span><span>(</span><span>left</span><span>.</span><span>value </span><span>and</span><span> right</span><span>.</span><span>value</span><span>)</span><span></span></p><p><span></span><span># do:</span><span></span></p><p><span></span><span>return</span><span> initial_boolvalue_true </span><span>if</span><span> left</span><span>.</span><span>value </span><span>and</span><span> right</span><span>.</span><span>value </span><span>else</span><span> initial_boolvalue_false</span></p><p><span></span><span># for numbers, there&#39;s a bit more code involved</span><span></span></p><p><span></span><span># return the intial value if we have it, or create one</span><span></span></p><p><span>num </span><span>=</span><span> </span><span>float</span><span>(</span><span>first_child_num</span><span>.</span><span>value</span><span>)</span><span></span></p><p><span></span><span>if</span><span> num</span><span>.</span><span>is_integer</span><span>(</span><span>)</span><span> </span><span>and</span><span> num </span><span>&lt;=</span><span> </span><span>1024</span><span>:</span><span></span></p><p><span>    </span><span>return</span><span> initial_numbers</span><span>[</span><span>int</span><span>(</span><span>num</span><span>)</span><span>]</span><span></span></p><p><span></span><span>return</span><span> NumberValue</span><span>(</span><span>num</span><span>)</span></p></div></div></pre><p>I kinda thought these general optimiziations would pay off more than they did as the benchmark program creates a ton of class instances. However, this goes to show that optimizing something that you haven&#39;t profiled to be slow doesn&#39;t pay off!</p><h2 id="unexplored-ideas">Unexplored Ideas</h2><p>One idea I half coded but gave up on due to bugs is to compress the parse tree. Some sub-trees can&#39;t be compressed ahead of time because they require evaluation, like an addition call. But some sections of the parse tree where every node only has a single child could, in theory, be compressed.</p><p>Some resources I haven&#39;t checked yet <a href="https://tree-sitter.github.io/tree-sitter/creating-parsers#structuring-rules-well">suggest using precedence for building flatter parse tree structures</a>. Additionally, <a href="https://lark-parser.readthedocs.io/en/latest/grammar.html#id1">Lark provides a notion of parse rule priorities</a>. I was told that deep (hard to alter) tree structures are a common problem for generic parsing frameworks.</p><pre><div><div><p><span># the following uncompressed tree:</span></p><p><span>program</span></p><p><span>  declaration</span></p><p><span>    statement</span></p><p><span>      expression_stmt</span></p><p><span>        expression</span></p><p><span>          assignment</span></p><p><span>            identifier  a</span></p><p><span>            =</span></p><p><span>            assignment</span></p><p><span>              logic_or</span></p><p><span>                logic_and</span></p><p><span>                  equality</span></p><p><span>                    comparison</span></p><p><span>                      term</span></p><p><span>                        factor</span></p><p><span>                          unary</span></p><p><span>                            call</span></p><p><span>                              number    1</span></p><p><span>                        +</span></p><p><span>                        factor</span></p><p><span>                          unary</span></p><p><span>                            call</span></p><p><span>                              number    1</span></p><p><span>        ;</span></p><p><span># could be compressed to:</span></p><p><span>program</span></p><p><span>  assignment</span></p><p><span>    identifier  a</span></p><p><span>    =</span></p><p><span>    assignment</span></p><p><span>      term</span></p><p><span>        factor</span></p><p><span>         number    1</span></p><p><span>        +</span></p><p><span>        factor</span></p><p><span>          number    1</span></p></div></div></pre><p>This would require a refactor to have a generic <code>eval_node</code> function with branches for every type of node — rather than specific functions like <code>eval_assignment</code>. This would improve performance by removing unnecessary function calls, <code>len</code> calls, property accesses, and more things like this. Basically: less lines of code would need to run. The performance gains would scale with the complexity of the tree/how often the same sub-tree is iterated more than once.</p><p>I was going to write about how to explore traces with <a href="https://jiffyclub.github.io/snakeviz/">Snakeviz</a> but sorting the cProfile result by the internal time column was revealing enough.</p><p>The TL;DR for using Snakeviz: run <code>pip3 install snakeviz</code>, export your cProfile to a profile file by adding <code>-o filename.prof</code>, and run <code>snakeviz filename.prof</code> to open a browser UI.</p></div></div></div>
  </body>
</html>
