<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Ostorlab/KEV">Original</a>
    <h1>Open-Source Detector of CISA&#39;s Known Exploitable Vulnerabilities</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <!-- <nav class="sidebar"> -->
    <!-- Table of Contents -->
    <!-- <h4>Table of Contents</h4> -->
    <!--  -->
    <!-- </nav> -->
    <!-- <br /> -->
    <!-- <br /> -->
    <h3 id="can-you-feel-the-moe">Can You Feel The MoE?</h3>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/mixtral/mistral_logo.png" width="800" alt="Mistral Logo"/>
    <figcaption>Love a little WordArt throwback</figcaption>
    </figure>
</div>



<p>Since the infamous
<a href="https://x.com/MistralAI/status/1706877320844509405?s=20">BitTorrent link launch</a>
of Mixtral, Mistral‚Äôs Mixture of Expert (MoE) model, there‚Äôs been renewed
attention<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> paid to MoE models.</p>

<p>This week, Mistral released the <a href="https://arxiv.org/pdf/2401.04088.pdf">paper</a>
accompanying the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">model</a>.
This feels like a great time to dig into the details of the Mixtral model and
the impact that it‚Äôs having on the MoE and LLM communities so far.</p>

<h2 id="mixtral-and-the-moe-paradigm">Mixtral and the MoE paradigm</h2>

<p>We discussed the intuition behind MoE models in
<a href="https://www.kolaayonrinde.com/blog/2023/10/22/moe-analogy.html">An Analogy for Understanding Mixture of Expert Models</a>:</p>

<blockquote>
  <p>In <a href="https://arxiv.org/pdf/2101.03961.pdf">Sparse Mixture of Experts</a> (MoEs),
we swap out the <code>MLP layers</code> of the vanilla transformer for an <code>Expert Layer</code>.
The Expert Layer is made up of multiple MLPs called ‚ÄúExperts‚Äù. For each input
we select one expert to send that input to. In this way, each token has
different parameters applied to it. A dynamic routing mechanism decides how to
map tokens to Experts.</p>
</blockquote>





<p>This approach gives models more parameters <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> without requiring more compute
or latency for each forward pass. MoE models also typically have better sample
efficiency - that is, their performance improves much faster than dense
transformers in training, when given the same amount of compute. This isn‚Äôt
quite a free lunch because it requires more memory to store the model for
inference, but, if you have enough memory, it‚Äôs pretty great.</p>

<p>Mixtral 8x7B has the backbone of Mistral-7B (their previous model). As with
Mistral-7B, Mixtral uses Group Query Attention and Sliding Window Attention. The
main changes are a 32k context window out of the box and replacing the Feed
Forward Networks (FFNs) with Mixture of Expert (MoE) layers.</p>

<p>Mixtral opts for an MoE layer with <code>8 FFN experts</code> which are sparsely activated
by choosing the <code>top 2</code> at each layer. <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Having <code>8 experts</code> means that where the original Mistral had a single FFN per
transformer block, Mixtral has 8 separate FFNs. <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>Rather than each token rather than being processed by all the parameters, a
routing network dynamically selects the <code>top 2</code> experts for each token depending
on the content of the token itself. Hence, though the total parameter count is
47B, the ‚Äúactive‚Äù parameter count (i.e. the number of parameters used for each
forward pass) comes in at 13B. <sup id="fnref:params" role="doc-noteref"><a href="#fn:params" rel="footnote">5</a></sup></p>

<p>Succinctly an MoE layer is given as:</p><p>

\[\displaystyle \sum_{i=0}^{n-1}G(x)_i \cdot E_i(x),\]

</p><p>where G is a gating function which is 0 everywhere except at 2 indices and where
each $E_i$ is a single expert FFN. Note that in the above formula since most of
the entries of the sum are zeros (as G(x) is zero for most i), we only have to
compute some of the $E_i$s rather than all of them. This is where MoEs have
computational efficiency advantages over using bigger models or using an
ensemble of models.</p>

<p>There is an MoE layer in each of the transformer blocks (32 in this case) and
hence we do this routing procedure 32 times for each forward pass. In a
traditional ensemble model, N (8 in this case) models have their predictions
averaged, so there are 8 token paths. We can compare the number of possible
paths that each token could take in an MoE to these ensemble methods:</p>

<blockquote>
  <p>At each layer we choose 2 of the 8 experts to process our token. There are
$\binom{8}{2}$ = 28 ways to do this. And this happens at each of the 32 layers
giving $28^32$ possible paths overall, which is huge<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">6</a></sup>! ü§Ø The variety of
possible paths here points towards increasingly Adaptive Computation in
models. In Adaptive Computation, we consider models which handle different
tokens with different parameters and different amounts of compute.</p>
</blockquote>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/mixtral/moe_layers_sketch.png" width="800" alt="MoE layers"/>
    <figcaption>A diagram showing an example path that a token could take for the first two layers</figcaption>
    </figure>
</div>



<p>Up until now there have been a two barriers to truly performant and stably
trainable MoEs:</p>

<h4 id="problem-1-training-moes-properly-from-a-mathematical-perspective">Problem 1: Training MoEs properly from a mathematical perspective</h4>

<p>MoE models have an inherently discrete step, the hard routing, and this
typically harms the gradient flow. Typically we want fully differentiable
functions for backprop and MoEs aren‚Äôt even continuous! Considering
mathematically plausible approximations to the true gradient can hugely improve
MoE training. Recent approaches like
<a href="https://arxiv.org/pdf/2310.00811.pdf">Sparse Backpropagation</a> and
<a href="https://arxiv.org/abs/2308.00951">Soft MoE for encoders</a> provide better
gradient flow and hence more performant models.</p>

<h4 id="problem-2-training-moes-efficiently">Problem 2: Training MoEs efficiently</h4>

<p>Compared to their FLOP-class, MoEs are larger models. Their size means that
there are real benefits to effective parallelisation and minimising
communication costs. Many frameworks such as
<a href="https://arxiv.org/pdf/2201.05596.pdf">DeepSpeed MoE</a> now support MoE training
in a fairly hardware efficient way. <br/></p>

<p>Having overcome both of these issues, we‚Äôre now ready to use MoEs more in
practise.</p>



<h2 id="notes-on-mixtrals-paper">Notes on Mixtral‚Äôs paper</h2>

<h3 id="evals">Evals</h3>

<p>The Mixtral base model outperforms popular (and larger) models like Llama 2 70B,
Gemini Pro and GPT-3.5 on most benchmarks. Note that these models are not only
larger in total parameter count but are also larger in active parameter count
too!</p>

<p>At the time of writing, Mixtral is the best open-source model and the 3rd best
Chat model, only beaten by GPT-4 and Claude 2.0 variants.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/mixtral/llm_leaderboard.png" width="800" alt="LLM Leaderboard"/>
    <figcaption></figcaption>
    </figure>
</div>

<h3 id="context-window">Context Window</h3>

<p>Mixtral shows impressive use of its whole 32k context window. The model has
relatively good recall even for mid-context tokens.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/mixtral/context_window.png" width="800" alt="Graph of Long Context Performance"/>
    <!-- <figcaption>Mixtral maintains good performance across its context window and seems to effectively use all of the context</figcaption> -->
    </figure>
</div>

<h3 id="instruction-fine-tuning">Instruction Fine-Tuning</h3>

<p>Along with the base model, Mistral also released Instruction Fine-Tuned Chat and
Assistant models. For alignment, they opted for Direct Preference Optimisation
(DPO) which is proving to be a powerful and less finicky alternative to the
traditional RLHF. <sup id="fnref:lambert" role="doc-noteref"><a href="#fn:lambert" rel="footnote">7</a></sup></p>

<h3 id="interpretability">Interpretability</h3>

<p>One hypothesis about MoEs is that some experts might specialise in a particular
domain (e.g. mathematics, biology, code, poetry etc.). This hypothesis is an old
one which has consistently been shown to be mistaken in the literature. Here,
the authors confirm, as in previous MoE papers, there is little difference in
the distribution of experts used for different domains (although they report
being surprised by this finding!). Often experts seem to specialise
syntactically (e.g. an expert for punctuation or whitespace), rather than
semantically (an expert for neuroscience). <sup id="fnref:specialisation" role="doc-noteref"><a href="#fn:specialisation" rel="footnote">8</a></sup></p>

<p>Although the distribution of experts is fairly uniform overall, interestingly
two adjacent tokens are much more likely to be processed by the same expert,
than we might naively predict. In other words, once an expert sees one token,
it‚Äôs quite likely to also see the next one - experts like to alley-oop
themselves! üèÄ</p>

<p>This recent <a href="https://arxiv.org/pdf/2312.17238.pdf">paper</a> details ways to
exploit this alley-oop property by caching the recently used expert weights in
fast memory.</p>

<p><em>As I‚Äôve noted
<a href="https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html#whatsnext:~:text=%F0%9F%94%B3-,Modularity,-As%20mentioned%20above">previously</a>,
I‚Äôm excited about the explicit modularity in MoE models for increased
interpretability</em>.</p>

<h3 id="notable-omissions">Notable omissions</h3>

<p>There‚Äôs little information in the paper about expert balancing techniques. Many
different auxiliary losses have been proposed for expert balancing and it would
be cool to see which loss function Mistral found to work well at this scale.</p>

<p>The authors are also quite hush about the pretrain, instruction or feedback
datasets used to train the model. Given the impressive performance, it‚Äôs quite
likely that there‚Äôs some secret sauce in the dataset compilation and filtering.
It seems increasingly likely that data will be a moat for Foundation Model
providers <sup id="fnref:moat" role="doc-noteref"><a href="#fn:moat" rel="footnote">9</a></sup>.</p>



<h2 id="mixtral-in-the-wild">Mixtral in the wild</h2>

<h3 id="impact-for-on-device-llms">Impact for On Device LLMs</h3>

<p>MoEs win by having increased performance with faster inference. Founder Sharif
Shameem <a href="https://x.com/sharifshameem/status/1734470299314459108?s=20">writes</a>,
‚ÄúThe Mixtral MoE model genuinely feels like an inflection point ‚Äî a true GPT-3.5
level model that can run at 30 tokens/sec on an M1 MacBook Pro. Imagine all the
products now possible when inference is 100% free and your data stays on your
device!‚Äù</p>

<p>Indeed since the launch of Mixtral, it‚Äôs been used in many applications from
<em>the enterprise</em> to <em>local chatbots</em> to <em>DIY Home Assistants √† la Siri</em>.</p>

<hr/>



<p>As many people use MoE models on-device for the first time, I expect that we
will start to see more methods which speed up MoE inference. The
<a href="https://arxiv.org/pdf/2312.17238.pdf">Fast MoE Inference paper</a> and MoE
specific quantisation like <a href="https://arxiv.org/pdf/2310.16795.pdf">QMoE</a> are all
great steps in this direction.</p>

<p>In particular, Quantization can be thought of as storing a model compressed like
we do for audio in MP3s. We degrade the quality model slightly and get massive
decreases in the memory that it requires. We can typically quantise MoEs even
more aggressively than dense models and retain strong performance.</p>

<h3 id="impact-for-foundation-model-companies">Impact for Foundation Model Companies</h3>

<p>Mistral was only started a matter of months ago with a super lean team and is
already SOTA for Open Source models. This is impressive from their team but it
may also suggest that Foundation Models are being commodified real quick.</p>

<p>Originally Mistral were offering Mixtral behind their API for \$1.96 per
million tokens. Considering GPT-4 is $10-30 at the time of writing, this seemed
fair for a hosted API. Within days different inference providers undercut
Mistral
<a href="https://twitter.com/JosephJacks_/status/1735756308496667101">significantly</a>:</p>

<blockquote><p lang="en" dir="ltr">Last week <a href="https://twitter.com/MistralAI?ref_src=twsrc%5Etfw">@MistralAI</a> launched pricing for the Mixtral MoE: $2.00~ / 1M tokens.</p>‚Äî JJ ‚Äî oss/acc (@JosephJacks_) <a href="https://twitter.com/JosephJacks_/status/1735756308496667101?ref_src=twsrc%5Etfw">December 15, 2023</a></blockquote>


<p>There was even one provider who was giving away tokens <em>for free</em>. I know a race
to the bottom when I see one‚Ä¶</p>

<p>The consumer/developer is truly winning here but it reiterates the point that
Foundation Model companies should expect the value of tokens to fall
<em>dramatically</em>.
<a href="https://www.amazon.co.uk/Zero-One-Notes-Start-Future/dp/0753555204">Competition is for Losers</a>,
as Peter Thiel might say; it‚Äôs very possible to compete away all the profits to
zero. <sup id="fnref:altman" role="doc-noteref"><a href="#fn:altman" rel="footnote">10</a></sup>. It increasingly looks like most of the value captured from an
LLM business perspective will likely be in the application layer (e.g.
Perplexity, Copilot) and the infrastructure layer (e.g. AWS/Azure).</p>

<h3 id="impact-for-the-scientific-community">Impact for the Scientific Community</h3>

<p>Mixtral is a huge win for the scientific and interpretability communities. We
now finally have a model which is comfortably better than GPT3.5 and whose
weights are freely available to researchers.</p>

<p>In addition, given Mixtral shares the same backbone as the previous Mistral 7B,
it seems plausible some weights were re-used as initialisations for Mixtral.
This approach is known in the literature as
<a href="https://arxiv.org/pdf/2212.05055.pdf">Sparse Upcycling</a>. If Sparse Upcycling
works, this suggests that the compute required to make great MoE models might be
much less than previous thought. Researchers can take advantage of existing
models like Llama 2 etc. rather than having to pretrain entirely from scratch,
which completely changes which projects are feasible for academics and the
GPU-poor.</p>

<h4 id="open-source-ml-in-the-age-of-adaptive-computation">Open Source ML in the Age of Adaptive Computation</h4>

<p>‚ÄúIn 2012 we were detecting cats and dogs and in 2022 we were writing human-like
pottery, generating beautiful and novel imagery, solving the protein folding
problem and writing code. Why is that?‚Äù</p>

<p>Arthur Mensch, Mistral co-founder, suggests most of the reason is ‚Äúthe free flow
of information. You had academic labs [and] very big industry labs communicating
all the time about their results and building on top of others‚Äô results. That‚Äôs
the way we [significantly improved] the architecture and training techniques. We
made everything work as a community‚Äù.</p>

<p>We‚Äôre not at the end of the ML story just yet. There‚Äôs still science to be done
and inventions to be discovered so we still need the free flow of information.</p>

<blockquote>
  <p>In this house we love Open Source models and papers. ü§ó</p>
</blockquote>

<p>Expect MoEs to become even more important for 2024. The age of
<a href="https://github.com/koayon/awesome-adaptive-computation">Adaptive Computation</a>
is here.</p>




  </div></div>
  </body>
</html>
