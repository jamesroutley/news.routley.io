<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html">Original</a>
    <h1>Beyond Automatic Differentiation</h1>
    
    <div id="readability-page-1" class="page"><div>
<div id="post-body-5107802027865733485">
<p><span>Posted by Matthew Streeter, Software Engineer, Google Research</span>

</p><p>
<a href="https://en.wikipedia.org/wiki/Derivative">Derivatives</a> play a central role in optimization and machine learning. By locally approximating a <a href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss">training loss</a>, derivatives guide an optimizer toward lower values of the loss. Automatic differentiation frameworks such as <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://github.com/google/jax">JAX</a> are an essential part of modern machine learning, making it feasible to use gradient-based optimizers to train very complex models.
</p>

<p>
But are derivatives all we need? By themselves, derivatives only tell us how a function behaves on an <a href="https://en.wikipedia.org/wiki/Infinitesimal">infinitesimal</a> scale. To use derivatives effectively, we often need to know more than that. For example, to choose a <a href="https://en.wikipedia.org/wiki/Learning_rate">learning rate</a> for <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, we need to know something about how the loss function behaves over a small but <em>finite</em> window. A finite-scale analogue of automatic differentiation, if it existed, could help us make such choices more effectively and thereby speed up training.
</p>
<p>
In our new paper &#34;<a href="https://arxiv.org/pdf/2212.11429.pdf">Automatically Bounding The Taylor Remainder Series: Tighter Bounds and New Applications</a>&#34;, we present an algorithm called AutoBound that computes polynomial upper and lower bounds on a given function, which are valid over a user-specified <a href="https://en.wikipedia.org/wiki/Interval_(mathematics)">interval</a>. We then begin to explore AutoBound&#39;s applications. Notably, we present a meta-optimizer called SafeRate that uses the upper bounds computed by AutoBound to derive learning rates that are guaranteed to monotonically reduce a given loss function, without the need for time-consuming hyperparameter tuning.Â We are also making AutoBound available as an <a href="https://github.com/google/autobound">open-source library</a>.</p>

</div>
</div></div>
  </body>
</html>
