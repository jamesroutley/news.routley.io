<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://preshing.com/20150316/semaphores-are-surprisingly-versatile/">Original</a>
    <h1>Semaphores are surprisingly versatile (2015)</h1>
    
    <div id="readability-page-1" class="page"><div><p>In multithreaded programming, it’s important to make threads wait. They must wait for exclusive access to a resource. They must wait when there’s no work available. One way to make threads wait – and put them to sleep inside the kernel, so that they no longer take any CPU time – is with a <strong>semaphore</strong>.</p>

<p>I used to think semaphores were strange and old-fashioned. They were invented by Edsger Dijkstra <a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">back in the early 1960s</a>, before anyone had done much multithreaded programming, or much programming at all, for that matter. I knew that a semaphore could keep track of available units of a resource, or function as a clunky kind of <a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29#Semaphores_vs._mutexes">mutex</a>, but that seemed to be about it.</p>

<p>My opinion changed once I realized that, using only semaphores and atomic operations, it’s possible to implement all of the following primitives:</p>

<ol>
  <li>A Lightweight Mutex</li>
  <li>A Lightweight Auto-Reset Event Object</li>
  <li>A Lightweight Read-Write Lock</li>
  <li>Another Solution to the Dining Philosophers Problem</li>
  <li>A Lightweight Semaphore With Partial Spinning</li>
</ol>

<!-- more -->
<p>Not only that, but these implementations share some desirable properties. They’re <em>lightweight</em>, in the sense that some operations happen entirely in userspace, and they can (optionally) spin for a short period before sleeping in the kernel. You’ll find all of the C++11 source code <a href="https://github.com/preshing/cpp11-on-multicore">on GitHub</a>. Since the standard C++11 library does not include semaphores, I’ve also provided a portable <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/sema.h"><code>Semaphore</code></a> class that maps directly to native semaphores on Windows, MacOS, iOS, Linux and other POSIX environments. You should be able to drop any of these primitives into almost any existing C++11 project.</p>

<p><a href="https://github.com/preshing/cpp11-on-multicore"><img src="https://preshing.com/images/cpp11om-repo.png"/></a></p>

<h2 id="a-semaphore-is-like-a-bouncer">A Semaphore Is Like a Bouncer</h2>

<p>Imagine a set of waiting threads, lined up in a queue – much like a lineup in front of a busy nightclub or theatre. A semaphore is like a bouncer at the front of the lineup. He only allows threads to proceed when instructed to do so.</p>

<p><img src="https://preshing.com/images/sema-intro.png"/></p>

<p>Each thread decides for itself when to join the queue. Dijkstra called this the <code>P</code> operation. <code>P</code> originally stood for some funny-sounding Dutch word, but in a modern semaphore implementation, you’re more likely to see this operation called <code>wait</code>. Basically, when a thread calls the semaphore’s <code>wait</code> operation, it enters the lineup.</p>

<p>The bouncer, himself, only needs to understand a single instruction. Originally, Dijkstra called this the <code>V</code> operation. Nowadays, the operation goes by various names, such as <code>post</code>, <code>release</code> or <code>signal</code>. I prefer <code>signal</code>. Any running thread can call <code>signal</code> at any time, and when it does, the bouncer releases exactly one waiting thread from the queue. (Not necessarily in the same order they arrived.)</p>

<p>Now, what happens if some thread calls <code>signal</code> <em>before</em> there are any threads waiting in line? No problem: As soon as the next thread arrives in the lineup, the bouncer will let it pass directly through. And if <code>signal</code> is called, say, 3 times on an empty lineup, the bouncer will let the next 3 threads to arrive pass directly through.</p>

<p><img src="https://preshing.com/images/sema-count.png"/></p>

<p>Of course, the bouncer needs to keep track of this number, which is why all semaphores maintain an <a href="http://linux.die.net/man/7/sem_overview">integer counter</a>. <code>signal</code> increments the counter, and <code>wait</code> decrements it.</p>

<p>The beauty of this strategy is that if <code>wait</code> is called some number of times, and <code>signal</code> is called some number of times, the outcome is always the same: The bouncer will always release the same number of threads, and there will always be the same number of threads left waiting in line, regardless of the order in which those <code>wait</code> and <code>signal</code> calls occurred.</p>

<p><img src="https://preshing.com/images/sema-order.png"/></p>

<h2 id="a-lightweight-mutex">1. A Lightweight Mutex</h2>

<p>I’ve already shown how to implement a lightweight mutex in an <a href="http://preshing.com/20120226/roll-your-own-lightweight-mutex">earlier post</a>. I didn’t know it at the time, but that post was just one example of a reusable pattern. The trick is to build another mechanism in front of the semaphore, which I like to call the <strong>box office</strong>.</p>

<p><img src="https://preshing.com/images/sema-box-office.png"/></p>

<p>The box office is where the real decisions are made. Should the current thread wait in line? Should it bypass the queue entirely? Should another thread be released from the queue? The box office cannot directly check how many threads are waiting on the semaphore, nor can it check the semaphore’s current signal count. Instead, the box office must somehow keep track of its own previous decisions. In the case of a lightweight mutex, all it needs is an atomic counter. I’ll call this counter <code>m_contention</code>, since it keeps track of how many threads are simultaneously contending for the mutex.</p>

<div><div>
  <div><pre><span>class</span> <span>LightweightMutex</span>
{
<span>private</span>:
    std::atomic&lt;<span>int</span>&gt; <span>m_contention</span>;         
    Semaphore m_semaphore;                 
</pre></div>
</div>
</div>

<p>When a thread decides to lock the mutex, it first visits the box office to increment <code>m_contention</code>.</p>

<div><div>
  <div><pre><span>public</span>:
    <span>void</span> lock()
    {
        <span>if</span> (<span>m_contention.fetch_add(<span>1</span>, std::memory_order_acquire)</span> &gt; <span>0</span>)  
        {
            m_semaphore.wait();     
        }
    }
</pre></div>
</div>
</div>

<p>If the previous value was 0, that means no other thread has contended for the mutex yet. As such, the current thread immediately considers itself the new owner, bypasses the semaphore, returns from <code>lock</code> and proceeds into whatever code the mutex is intended to protect.</p>

<p>Otherwise, if the previous value was greater than 0, that means another thread is already considered to own the mutex. In that case, the current thread must wait in line for its turn.</p>

<p><img src="https://preshing.com/images/sema-mutex-1.png"/></p>

<p>When the previous thread unlocks the mutex, it visits the box office to decrement the counter:</p>

<div><div>
  <div><pre>    <span>void</span> unlock()
    {
        <span>if</span> (<span>m_contention.fetch_sub(<span>1</span>, std::memory_order_release)</span> &gt; <span>1</span>)  
        {
            m_semaphore.signal();   
        }
    }
</pre></div>
</div>
</div>

<p>If the previous counter value was 1, that means no other threads arrived in the meantime, so there’s nothing else to do. <code>m_contention</code> is simply left at 0.</p>

<p>Otherwise, if the previous counter value was greater than 1, another thread has attempted to lock the mutex, and is therefore waiting in the queue. As such, we alert the bouncer that it’s now safe to release the next thread. That thread will be considered the new owner.</p>

<p><img src="https://preshing.com/images/sema-mutex-2.png"/></p>

<p>Every visit to the box office is an indivisible, atomic operation. Therefore, even if multiple threads call <code>lock</code> and <code>unlock</code> concurrently, they will always visit the box office one at a time. Furthermore, the behavior of the mutex is <em>completely determined</em> by the decisions made at the box office. After they visit the box office, they may operate on the semaphore in an unpredictable order, but that’s OK. As I’ve already explained, the outcome will remain valid regardless of the order in which those semaphore operations occur. (In the worst case, some threads may trade places in line.)</p>

<p>This class is considered “lightweight” because it bypasses the semaphore when there’s no contention, thereby avoiding system calls. I’ve published it to GitHub as <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/benaphore.h"><code>NonRecursiveBenaphore</code></a> along with a <a href="http://preshing.com/20120305/implementing-a-recursive-mutex">recursive version</a>. However, there’s no need to use these classes in practice. Most available mutex implementations are <a href="http://preshing.com/20111124/always-use-a-lightweight-mutex">already lightweight</a>. Nonetheless, they’re noteworthy for serving as inspiration for the rest of the primitives described here.</p>

<h2 id="a-lightweight-auto-reset-event-object">2. A Lightweight Auto-Reset Event Object</h2>

<p>You don’t hear autoreset event objects discussed very often, but as I mentioned in my <a href="http://preshing.com/20141024/my-multicore-talk-at-cppcon-2014">CppCon 2014 talk</a>, they’re widely used in game engines. Most often, they’re used to notify a single other thread (possibly sleeping) of available work.</p>

<p><a href="http://preshing.com/20141024/my-multicore-talk-at-cppcon-2014"><img src="https://preshing.com/images/cppcon-event-slide.png"/></a></p>

<p>An autoreset event object is basically a semaphore that ignores redundant signals. In other words, when <code>signal</code> is called multiple times, the event object’s signal count will never exceed 1. That means you can go ahead and publish work units somewhere, blindly calling <code>signal</code> after each one. It’s a flexible technique that works even when you publish work units to some data structure other than a queue.</p>

<p>Windows has native support for event objects, but its <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ms686211.aspx"><code>SetEvent</code></a> function – the equivalent of <code>signal</code> – can be expensive. One one machine, I timed it at <strong>700 ns</strong> per call, even when the event was already signaled. If you’re publishing thousands of work units between threads, the overhead for each <code>SetEvent</code> can quickly add up.</p>

<p>Luckily, the box office/bouncer pattern reduces this overhead significantly. All of the autoreset event logic can be implemented at the box office using atomic operations, and the box office will invoke the semaphore only when it’s absolutely necessary for threads to wait.</p>

<p><img src="https://preshing.com/images/sema-event.png"/></p>

<p>I’ve published the implementation as <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/autoresetevent.h"><code>AutoResetEvent</code></a>. This time, the box office has a different way to keep track of how many threads have been sent to wait in the queue. When <code>m_status</code> is negative, its magnitude indicates how many threads are waiting:</p>

<div><div>
  <div><pre><span>class</span> <span>AutoResetEvent</span>
{
<span>private</span>:
    
    
    
    std::atomic&lt;<span>int</span>&gt; <span>m_status</span>;
    Semaphore m_sema;
</pre></div>
</div>
</div>

<p>In the event object’s <code>signal</code> operation, we increment <code>m_status</code> atomically, up to the limit of 1:</p>

<div><div>
  <div><pre><span>public</span>:
    <span>void</span> signal()
    {
        <span>int</span> oldStatus = m_status.load(std::memory_order_relaxed);
        <span>for</span> (;;)    
        {
            assert(oldStatus &lt;= <span>1</span>);
            <span><span>int</span> newStatus = oldStatus &lt; <span>1</span> ? oldStatus + <span>1</span> : <span>1</span>;</span>
            <span>if</span> (m_status.compare_exchange_weak(oldStatus, newStatus, std::memory_order_release, std::memory_order_relaxed))
                <span>break</span>;
            
            
        }
        <span>if</span> (oldStatus &lt; <span>0</span>)
            m_sema.signal();    
    }
</pre></div>
</div>
</div>

<p>Note that because the initial load from <code>m_status</code> is relaxed, it’s important for the above code to call <code>compare_exchange_weak</code> even if <code>m_status</code> already equals 1. Thanks to commenter Tobias Brüll for pointing that out. See <a href="https://github.com/preshing/cpp11-on-multicore/tree/master/tests/lostwakeup">this README file</a> for more information.</p>

<h2 id="a-lightweight-read-write-lock">3. A Lightweight Read-Write Lock</h2>

<p>Using the same box office/bouncer pattern, it’s possible to implement a pretty good <a href="http://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock">read-write lock</a>. This read-write lock is completely lock-free in the absence of writers, it’s starvation-free for both readers and writers, and just like the other primitives, it can spin before putting threads to sleep. It requires two semaphores: one for waiting readers, and another for waiting writers. The code is available as <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/rwlock.h"><code>NonRecursiveRWLock</code></a>.</p>

<p><img src="https://preshing.com/images/sema-rwlock.png"/></p>

<h2 id="another-solution-to-the-dining-philosophers-problem">4. Another Solution to the Dining Philosophers Problem</h2>

<p>The box office/bouncer pattern can also solve Dijkstra’s <a href="http://en.wikipedia.org/wiki/Dining_philosophers_problem">dining philosophers problem</a> in a way that I haven’t seen described elsewhere. If you’re not familiar with this problem, it involves philosophers that share dinner forks with each other. Each philosopher must obtain two specific forks before he or she can eat. I don’t believe this solution will prove useful to anybody, so I won’t go into great detail. I’m just including it as further demonstration of semaphores’ versatility.</p>

<p>In this solution, we assign each philosopher (thread) its own dedicated semaphore. The box office keeps track of which philosophers are eating, which ones have requested to eat, and the order in which those requests arrived. With that information, the box office is able to shepherd all philosophers through their bouncers in an optimal way.</p>

<p><img src="https://preshing.com/images/sema-philosophers.png"/></p>

<p>I’ve posted two implementations. One is <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/diningphilosophers.h"><code>DiningPhilosophers</code></a>, which implements the box office using a mutex. The other is <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/diningphilosophers.h"><code>LockReducedDiningPhilosophers</code></a>, in which every visit to the box office is lock-free.</p>

<h2 id="a-lightweight-semaphore-with-partial-spinning">5. A Lightweight Semaphore with Partial Spinning</h2>

<p>You read that right: It’s possible to combine a semaphore with a box office to implement… another semaphore.</p>

<p>Why would you do such a thing? Because you end up with a <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/sema.h"><code>LightweightSemaphore</code></a>. It becomes extremely cheap when the lineup is empty and the signal count climbs above zero, regardless of how the underlying semaphore is implemented. In such cases, the box office will rely entirely on atomic operations, leaving the underlying semaphore untouched.</p>

<p><img src="https://preshing.com/images/sema-lwsema2.png"/></p>

<p>Not only that, but you can make threads wait in a <a href="http://en.wikipedia.org/wiki/Spinlock">spin loop</a> for a short period of time before invoking the underlying semaphore. This trick helps avoid expensive system calls when the wait time ends up being short.</p>

<p>In the <a href="https://github.com/preshing/cpp11-on-multicore/tree/master/common">GitHub repository</a>, all of the other primitives are implemented on top of <code>LightweightSemaphore</code>, rather than using <code>Semaphore</code> directly. That’s how they all inherit the ability to partially spin. <code>LightweightSemaphore</code> sits on top of <code>Semaphore</code>, which in turn encapsulates a platform-specific semaphore.</p>

<p><img src="https://preshing.com/images/semaphore-class-diagram.png"/></p>

<p>The repository comes with a simple test suite, with each test case exercising a different primitive. It’s possible to remove <code>LightweightSemaphore</code> and force all primitives to use <code>Semaphore</code> directly. Here are the resulting timings on my Windows PC:</p>

<table id="shake">
<thead><tr><th></th><th>LightweightSemaphore</th><th>Semaphore</th></tr></thead>
<tbody>
<tr><td>testBenaphore</td><td>375 ms</td><td>5503 ms</td></tr>
<tr><td>testRecursiveBenaphore</td><td>393 ms</td><td>404 ms</td></tr>
<tr><td>testAutoResetEvent</td><td>593 ms</td><td>4665 ms</td></tr>
<tr><td>testRWLock</td><td>598 ms</td><td>7126 ms</td></tr>
<tr><td>testDiningPhilosophers</td><td>309 ms</td><td>580 ms</td></tr>
</tbody></table>

<p>As you can see, the test suite benefits significantly from <code>LightweightSemaphore</code> in this environment. Having said that, I’m pretty sure the current spinning strategy is not optimal for every environment. It simply spins a fixed number of 10000 times before falling back on <code>Semaphore</code>. I looked briefly into adaptive spinning, but the best approach wasn’t obvious. Any suggestions?</p>

<h2 id="comparison-with-condition-variables">Comparison With Condition Variables</h2>

<p>With all of these applications, semaphores are more general-purpose than I originally thought – and this wasn’t even a complete list. So why are semaphores absent from the standard C++11 library? For the same reason they’re absent from Boost: a preference for <a href="http://en.wikipedia.org/wiki/Monitor_%28synchronization%29"><strong>mutexes and condition variables</strong></a>. From the library maintainers’ point of view, conventional semaphore techniques are just <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2043.html#SemaphoreTypes">too error prone</a>.</p>

<p>When you think about it, though, the box office/bouncer pattern shown here is really just an optimization for condition variables in a specific case – the case where all condition variable operations are performed at the end of the critical section.</p>

<p>Consider the <code>AutoResetEvent</code> class described above. I’ve implemented <a href="https://github.com/preshing/cpp11-on-multicore/blob/master/common/autoreseteventcondvar.h"><code>AutoResetEventCondVar</code></a>, an equivalent class based on a condition variable, in the same repository. Its condition variable is always manipulated at the end of the critical section.</p>

<div><div>
  <div><pre><span>void</span> AutoResetEventCondVar::signal()
{
    
    std::unique_lock&lt;std::mutex&gt; lock(m_mutex);
    <span>int</span> oldStatus = m_status;
    <span>if</span> (oldStatus == <span>1</span>)
        <span>return</span>;     
    m_status++;
    <span>if</span> (oldStatus &lt; <span>0</span>)
        <span>m_condition.notify_one()</span>;   
}
</pre></div>
</div>
</div>

<p>We can optimize <code>AutoResetEventCondVar</code> in two steps:</p>

<ol>
  <li>
    <p>Pull each condition variable outside of its critical section and convert it to a semaphore. The order-independence of semaphore operations makes this safe. After this step, we’ve already implemented the box office/bouncer pattern. (In general, this step also lets us avoid a <a href="http://javaagile.blogspot.ca/2012/12/the-thundering-herd.html">thundering herd</a> when multiple threads are signaled at once.)</p>
  </li>
  <li>
    <p>Make the box office lock-free by <a href="http://preshing.com/20150402/you-can-do-any-kind-of-atomic-read-modify-write-operation">converting all operations to CAS loops</a>, greatly improving its scalability. This step results in <code>AutoResetEvent</code>.</p>
  </li>
</ol>

<p><img src="https://preshing.com/images/sema-condvar.png"/></p>

<p>On my Windows PC, using <code>AutoResetEvent</code> in place of <code>AutoResetEventCondVar</code> makes the associated test case run <strong>10x</strong> faster.</p>
</div></div>
  </body>
</html>
