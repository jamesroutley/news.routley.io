<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/BICLab/SpikingBrain-7B">Original</a>
    <h1>SpikingBrain 7B â€“ More efficient than classic LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">ðŸ“„ Technical Report: <a href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/SpikingBrain_Report_Chi.pdf">Chinese</a> | <a href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/SpikingBrain_Report_Eng.pdf">English</a></p>
<hr/>

<p dir="auto">Inspired by brain mechanisms, <strong>SpikingBrain</strong> integrates <strong>hybrid efficient attention</strong>, <strong>MoE modules</strong>, and <strong>spike encoding</strong> into its architecture, supported by a universal conversion pipeline compatible with the open-source model ecosystem. This enables continual pre-training with less than 2% of the data while achieving performance comparable to mainstream open-source models. We further adapt frameworks, operators, parallel strategies, and communication primitives for <strong>non-NVIDIA (MetaX) clusters</strong>, ensuring stable large-scale training and inference. SpikingBrain achieves over 100Ã— speedup in TTFT for 4M-token sequences, while spiking delivers over 69% sparsity at the micro level. Combined with macro-level MoE sparsity, these advances provide valuable guidance for the design of next-generation neuromorphic chips.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/assets/fig1.png"><img src="https://avi.im/BICLab/SpikingBrain-7B/raw/main/assets/fig1.png" alt=""/></a></p>
<hr/>

<p dir="auto">This repository provides the full implementation and weights of <strong>SpikingBrain-7B</strong>, including the <strong>HuggingFace version</strong>, <strong>vLLM inference version</strong>, and <strong>quantized version</strong>, enabling flexible deployment and research across different scenarios.</p>
<div data-snippet-clipboard-copy-content="SpikingBrain-7B/
â”œâ”€â”€ hf_7B_model/ # HuggingFace version
â”œâ”€â”€ run_model/   # Model run examples
â”œâ”€â”€ vllm_hymeta/ # vLLM plugins and inference support
â”œâ”€â”€ W8ASpike/    # Quantized inference version
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt 
â””â”€â”€ README.md "><pre><code>SpikingBrain-7B/
â”œâ”€â”€ hf_7B_model/ # HuggingFace version
â”œâ”€â”€ run_model/   # Model run examples
â”œâ”€â”€ vllm_hymeta/ # vLLM plugins and inference support
â”œâ”€â”€ W8ASpike/    # Quantized inference version
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt 
â””â”€â”€ README.md 
</code></pre></div>
<hr/>

<p dir="auto"><strong>vllm-hymeta</strong> is the plugin adaptation of HyMeta (Hybrid Models built on MetaX GPUs) for the <a href="https://github.com/vllm-project/vllm/tree/main">vLLM inference framework</a>, providing efficient inference support on NVIDIA GPUs.</p>
<p dir="auto">By leveraging the <a href="https://blog.vllm.ai/2025/05/12/hardware-plugin.html" rel="nofollow">plugins mechanism</a> in vLLM, hardware backends can be integrated in a modular fashion, bringing the following benefits:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Decoupled codebase</strong>: Backend-specific code remains independent, keeping the vLLM core cleaner.</p>
</li>
<li>
<p dir="auto"><strong>Reduced maintenance cost</strong>: vLLM developers can focus on general functionality without being affected by backend-specific implementations.</p>
</li>
<li>
<p dir="auto"><strong>Faster integration</strong>: New backends can be integrated quickly and evolve independently with less engineering effort.</p>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Container Deployment (NVIDIA)</h3><a id="user-content-container-deployment-nvidia" aria-label="Permalink: Container Deployment (NVIDIA)" href="#container-deployment-nvidia"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="sudo docker run -itd \
    --entrypoint /bin/bash \
    --network host \
    --name hymeta-bench \
    --shm-size 160g \
    --gpus all \
    --privileged \
    -v /host_path:/container_path \
    docker.1ms.run/vllm/vllm-openai:v0.10.0"><pre>sudo docker run -itd \
    --entrypoint /bin/bash \
    --network host \
    --name hymeta-bench \
    --shm-size 160g \
    --gpus all \
    --privileged \
    -v /host_path:/container_path \
    docker.1ms.run/vllm/vllm-openai:v0.10.0</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/BICLab/SpikingBrain-7B.git
cd SpikingBrain-7B
pip install ."><pre>git clone https://github.com/BICLab/SpikingBrain-7B.git
<span>cd</span> SpikingBrain-7B
pip install <span>.</span></pre></div>
<p dir="auto">Recommended environment for installing <strong>vllm-hymeta</strong> on NVIDIA GPUs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="decorator
pyyaml
scipy
setuptools
setuptools-scm
flash_attn==2.7.3
flash-linear-attention==0.1
vllm==0.10.0
torch==2.7.1"><pre>decorator
pyyaml
scipy
setuptools
setuptools-scm
<span>flash_attn=</span>=2.7.3
<span>flash-linear-attention=</span>=0.1
<span>vllm=</span>=0.10.0
<span>torch=</span>=2.7.1</pre></div>

<p dir="auto">You can serve a model with vLLM in the simplest way using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve &lt;your_model_path&gt; \
  --served-model-name &lt;model_name&gt; \
  --gpu-memory-utilization &lt;ratio&gt; \
  --block-size &lt;size&gt; \
  --dtype bfloat16 \
  --port &lt;port_number&gt;"><pre>vllm serve <span>&lt;</span>your_model_path<span>&gt;</span> \
  --served-model-name <span>&lt;</span>model_name<span>&gt;</span> \
  --gpu-memory-utilization <span>&lt;</span>ratio<span>&gt;</span> \
  --block-size <span>&lt;</span>size<span>&gt;</span> \
  --dtype bfloat16 \
  --port <span>&lt;</span>port_number<span>&gt;</span></pre></div>
<p dir="auto">You may also set <code>--tensor-parallel-size</code> and <code>--pipeline-parallel-size</code> when launching if you want to run with multiple GPUs.</p>
<hr/>

<p dir="auto"><strong>W8ASpike</strong> is the quantized inference version of SpikingBrain-7B, aiming to reduce inference cost under low-precision settings and explore the potential of Spiking Neural Networks (SNNs).</p>
<p dir="auto">The current implementation adopts <strong>pseudo-spiking</strong>, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Pseudo-spiking</strong>: Efficient approximation at the tensor level, suitable for prototyping and research.</p>
</li>
<li>
<p dir="auto"><strong>True-spiking</strong>: Requires asynchronous hardware and event-driven operator support, which is beyond the scope of this repository.</p>
</li>
</ul>
<p dir="auto">The activation spike encoding process here is inspired by the pseudo-spiking interfaces from <a href="https://github.com/BICLab/Int2Spike">BICLab/Int2Spike</a>. For additional PyTorch-based spiking interfaces, please refer to the Int2Spike library.</p>
<hr/>

<p dir="auto">The model weights are hosted on <strong>ModelScope</strong>. Please select the appropriate version based on your needs:</p>
<ul dir="auto">
<li><strong>Pre-trained model (7B):</strong> <a href="https://www.modelscope.cn/models/Panyuqi/V1-7B-base" rel="nofollow">https://www.modelscope.cn/models/Panyuqi/V1-7B-base</a></li>
<li><strong>Chat model (7B-SFT):</strong> <a href="https://www.modelscope.cn/models/Panyuqi/V1-7B-sft-s3-reasoning" rel="nofollow">https://www.modelscope.cn/models/Panyuqi/V1-7B-sft-s3-reasoning</a></li>
<li><strong>Quantized weights (7B-W8ASpike):</strong> <a href="https://www.modelscope.cn/models/Abel2076/SpikingBrain-7B-W8ASpike" rel="nofollow">https://www.modelscope.cn/models/Abel2076/SpikingBrain-7B-W8ASpike</a></li>
</ul>

<p dir="auto">Example scripts are provided in <a href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/run_model"><code>run_model/</code></a> for running the model with the released checkpoints.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Hugging Face</strong></p>
</li>
<li>
<p dir="auto"><strong>vLLM</strong></p>
</li>
</ul>

<p dir="auto">Table 1: <strong>Performance evaluation of the SpikingBrain-7B pre-trained model.</strong> All models are tested with the HuggingFace framework and evaluated using a perplexity-based method. Except for Qwen2.5, the other baselines are trained on limited Chinese data, resulting in clear disadvantages on CMMLU and C-Eval.
<a target="_blank" rel="noopener noreferrer" href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/assets/table1.png"><img src="https://avi.im/BICLab/SpikingBrain-7B/raw/main/assets/table1.png" alt=""/></a></p>
<p dir="auto">Table 2: <strong>Performance evaluation of the SpikingBrain-76B pre-trained model.</strong> All models are tested with the vLLM framework and evaluated using a perplexity-based method. Except for Qwen2.5, the other baselines are trained on limited Chinese data, resulting in clear disadvantages on CMMLU and C-Eval.
<a target="_blank" rel="noopener noreferrer" href="https://avi.im/BICLab/SpikingBrain-7B/blob/main/assets/table2.png"><img src="https://avi.im/BICLab/SpikingBrain-7B/raw/main/assets/table2.png" alt=""/></a></p>
<hr/>

<p dir="auto">If you find our work useful, please consider citing <strong>SpikingBrain</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{pan2025spikingbrain,
  title={SpikingBrain Technical Report: Spiking Brain-inspired Large Models},
  author={Pan, Yuqi and Feng, Yupeng and Zhuang, Jinghao and Ding, Siyu and Liu, Zehao and Sun, Bohan and Chou, Yuhong and Xu, Han and Qiu, Xuerui and Deng, Anlin and others},
  journal={arXiv preprint arXiv:2509.05276},
  year={2025}
}"><pre><span>@article</span>{<span>pan2025spikingbrain</span>,
  <span>title</span>=<span><span>{</span>SpikingBrain Technical Report: Spiking Brain-inspired Large Models<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Pan, Yuqi and Feng, Yupeng and Zhuang, Jinghao and Ding, Siyu and Liu, Zehao and Sun, Bohan and Chou, Yuhong and Xu, Han and Qiu, Xuerui and Deng, Anlin and others<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2509.05276<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
