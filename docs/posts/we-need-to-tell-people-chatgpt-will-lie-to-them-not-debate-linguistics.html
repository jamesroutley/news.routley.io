<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Apr/7/chatgpt-lies/">Original</a>
    <h1>We need to tell people ChatGPT will lie to them, not debate linguistics</h1>
    
    <div id="readability-page-1" class="page"><div>




<p><strong>ChatGPT lies to people</strong>. This is a serious bug that has so far resisted all attempts at a fix. We need to prioritize helping people understand this, not debating the most precise terminology to use to describe it.</p>
<h4>We accidentally invented computers that can lie to us</h4>
<p>I <a href="https://twitter.com/simonw/status/1643469011127259136">tweeted</a> (and <a href="https://fedi.simonwillison.net/@simon/110144293948444462">tooted</a>) this:</p>
<blockquote><p lang="en" dir="ltr">We accidentally invented computers that can lie to us and we can’t figure out how to make them stop</p>- Simon Willison (@simonw) <a href="https://twitter.com/simonw/status/1643469011127259136">April 5, 2023</a></blockquote>
<p>Mainly I was trying to be pithy and amusing, but this thought was inspired by reading Sam Bowman’s excellent review of the field, <a href="https://cims.nyu.edu/~sbowman/eightthings.pdf">Eight Things to Know about Large Language Models</a>. In particular this:</p>
<blockquote>
<p>More capable models can better recognize the specific circumstances under which they are trained. Because of this, they are more likely to learn to act as expected in precisely those circumstances while behaving competently but unexpectedly in others. This can surface in the form of problems that Perez et al. (2022) call sycophancy, where a model answers subjective questions in a way that flatters their user’s stated beliefs, and sandbagging, where models are more likely to endorse common misconceptions when their user appears to be less educated.</p>
</blockquote>
<p>Sycophancy and sandbagging are my two favourite new pieces of AI terminology!</p>
<p>What I find fascinating about this is that these extremely problematic behaviours are not the system working as intended: they are bugs! And we haven’t yet found a reliable way to fix them.</p>
<p>(Here’s the paper that snippet references: <a href="https://arxiv.org/abs/2212.09251">Discovering Language Model Behaviors with Model-Written Evaluations</a> from December 2022.)</p>
<h4>“But a machine can’t deliberately tell a lie”</h4>
<p>I got quite a few replies complaining that it’s inappropriate to refer to LLMs as “lying”, because to do so anthropomorphizes them and implies a level of intent which isn’t possible.</p>
<p>I completely agree that anthropomorphism is bad: these models are fancy matrix arithmetic, not entities with intent and opinions.</p>
<p>But in this case, I think the visceral clarity of being able to say “ChatGPT will lie to you” is a worthwhile trade.</p>
<p>Science fiction has been presenting us with a model of “artificial intelligence” for decades. It’s firmly baked into our culture that an “AI” is an all-knowing computer, incapable of lying and able to answer any question with pin-point accuracy.</p>
<p>Large language models like ChatGPT, on first encounter, seem to fit that bill. They appear astonishingly capable, and their command of human language can make them seem like a genuine intelligence, at least at first glance.</p>
<p>But the more time you spend with them, the more that illusion starts to fall apart.</p>
<p>They fail spectacularly when prompted with logic puzzles, or basic arithmetic, or when asked to produce citations or link to sources for the information they present.</p>
<p>Most concerningly, they hallucinate or confabulate: they make things up! My favourite example of this remains <a href="https://simonwillison.net/2023/Mar/10/chatgpt-internet-access/#i-dont-believe-it">their ability to entirely imagine the content of a URL</a>. I still see this catching people out every day. It’s remarkably convincing.</p>
<p><a href="https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/">Why ChatGPT and Bing Chat are so good at making things up</a> is an excellent in-depth exploration of this issue from Benj Edwards at Ars Technica.</p>
<h4>We need to explain this in straight-forward terms</h4>
<p>We’re trying to solve two problems here:</p>
<ol>
  <li>ChatGPT cannot be trusted to provide factual information. It has a very real risk of making things up, and if people don’t understand it they are guaranteed to be mislead.</li>
  <li>Systems like ChatGPT are not sentient, or even intelligent systems. They do not have opinions, or feelings, or a sense of self. We must resist the temptation to anthropomorphize them.</li>
</ol>
<p>I believe that <strong>the most direct form of harm caused by LLMs today is the way they mislead their users</strong>. The first problem needs to take precedence.</p>
<p>It is vitally important that new users understand that these tools cannot be trusted to provide factual answers. We need to help people get there as quickly as possible.</p>
<p>Which of these two messages do you think is more effective?</p>
<p><strong>ChatGPT will lie to you</strong></p>
<p>Or</p>
<p><strong>ChatGPT doesn’t lie, lying is too human and implies intent. It hallucinates. Actually no, hallucination still implies human-like thought. It confabulates. That’s a term used in psychiatry to describe when someone replaces a gap in one’s memory by a falsification that one believes to be true—though of course these things don’t have human minds so even confabulation is unnecessarily anthropomorphic. I hope you’ve enjoyed this linguistic detour!</strong></p>
<p>Let’s go with the first one. We should be shouting this message from the rooftops: <strong>ChatGPT will lie to you</strong>.</p>
<p>That doesn’t mean it’s not useful—it can be astonishingly useful, for all kinds of purposes... but seeking truthful, factual answers is very much not one of them. And everyone needs to understand that.</p>
<p>Convincing people that these aren’t a sentient AI out of a science fiction story can come later. Once people understand their flaws this should be an easier argument to make!</p>
<h4 id="warn-off-or-help-on">Should we warn people off or help them on?</h4>
<p>This situation raises an ethical conundrum: if these tools can’t be trusted, and people are demonstrably falling for their traps, should we encourage people not to use them at all, or even campaign to have them banned?</p>
<p>Every day I personally find new problems that I can solve more effectively with the help of large language models. Some recent examples from just the last few weeks:</p>
<ul>
<li>
<a href="https://til.simonwillison.net/gpt3/gpt4-api-design">GPT-4 for API design research</a>—<a href="https://gist.github.com/simonw/fa2379b97420404a81b0fcdb4db79657">ChatGPT transcript</a>
</li>
<li>
<a href="https://til.simonwillison.net/googlecloud/video-frame-ocr">Reading thermometer temperatures over time from a video</a>—<a href="https://gist.github.com/simonw/365ca7e4fde3ae8221ca1da219ce3fc9">transcript</a>
</li>
<li>
<a href="https://til.simonwillison.net/datasette/row-selection-prototype">Interactive row selection prototype with Datasette</a>—<a href="https://gist.github.com/simonw/d1c1c4ec33914b0f68bf3e55a5104d65">transcript</a>
</li>
<li>
<a href="https://til.simonwillison.net/jq/git-log-json">Convert git log output to JSON using jq</a>—<a href="https://gist.github.com/simonw/c3b486fa90d7c32a0e8dfb47e151090a">transcript</a>
</li>
</ul>
<p>Each of these represents a problem I could have solved without ChatGPT... but at a time cost that would have been prohibitively expensive, to the point that I wouldn’t have bothered.</p>
<p>I wrote more about this in <a href="https://simonwillison.net/2023/Mar/27/ai-enhanced-development/">AI-enhanced development makes me more ambitious with my projects</a>.</p>
<p>Honestly, at this point using ChatGPT in the way that I do feels like a massively unfair competitive advantage. I’m not worried about AI taking people’s jobs: I’m worried about the impact of AI-enhanced developers like myself.</p>
<p>It genuinely feels unethical for me <em>not</em> to help other people learn to use these tools as effectively as possible. I want everyone to be able to do what I can do with them, as safely and responsibly as possible.</p>
<p>I think the message we should be emphasizing is this:</p>
<p><strong>These are incredibly powerful tools. They are far harder to use effectively than they first appear. Invest the effort, but approach with caution: we accidentally invented computers that can lie to us and we can’t figure out how to make them stop.</strong></p>
<p>There’s a time for linguistics, and there’s a time for grabbing the general public by the shoulders and shouting “It lies! The computer lies to you! Don’t trust anything it says!”</p>




</div></div>
  </body>
</html>
