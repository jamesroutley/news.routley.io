<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.blacksmith.sh/blog/faster-docker-builds-using-a-remote-buildkit-instance">Original</a>
    <h1>Faster Docker builds using a remote BuildKit instance</h1>
    
    <div id="readability-page-1" class="page"><div><p>Docker has fundamentally changed how developers build and deploy applications, with most companies leveraging containers in some capacity. At Blacksmith, we regularly see our customer’s Docker builds taking 30 minutes or more, which can significantly hinder developer productivity and delay the deployment of hot-fixes.</p><p>In this post, we’ll give you the exact steps needed to setup a remote BuildKit instance on AWS. The 30 minutes it’ll take to follow this blog can dramatically speed up your Docker builds for your entire org. But before diving deeper, let&#39;s review some Docker fundamentals. There are three primary levers one can pull to optimize Docker build times:</p><ol role="list"><li><strong>Hardware</strong>: Since Docker builds are often compute-intensive, running them on a more powerful machine with a high core-count can often make them substantially faster.</li><li><strong>Layer Caching</strong>: BuildKit has powerful support for incrementally caching layers for a given Dockerfile, and this can be leveraged in various ways to improve Docker build performance by an order of magnitude for a lot of common workflows. We&#39;ve previously written about <a href="https://www.blacksmith.sh/blog/push-cache-repeat-amazon-ecr-as-a-remote-docker-cache-for-github-actions" target="_blank">using AWS ECR as a Docker cache</a>.</li><li><strong>Dockerfile Optimization</strong>: Crafting an efficient Dockerfile is crucial for build performance. While an in-depth discussion of Dockerfile best practices is beyond the scope of this post, Docker provides some excellent <a href="https://docs.docker.com/guides/workshop/09_image_best/" target="_blank">guidelines on their website</a>.</li></ol><h2>What is BuildKit?</h2><p>BuildKit is a modern backend that replaces the legacy Docker builder, offering improved performance and new features.</p><p>However, the most relevant feature for our use case is BuildKit&#39;s ability to execute builds on a remote instance. This lets us offload the build process from the local machine to a more powerful remote server.</p><p>BuildKit achieves this by using a client-server architecture. The BuildKit client, which runs on your local machine or CI/CD runner, communicates with the remote BuildKit daemon over a secure connection. When you initiate a build, the client sends the build context (Dockerfile, source code, etc.) to the remote daemon, which executes the build and streams logs back to the client.</p><h2>Running a Remote BuildKit Instance on AWS</h2><p>Let&#39;s circle back to the first two levers we mentioned for accelerating Docker builds: using a powerful machine and having a persistent build cache. BuildKit allows you to run a remote Docker builder instance on any cloud provider, and we&#39;ll walk through an example of setting this up on AWS.</p><p>By running BuildKit on AWS, we can:</p><ol role="list"><li><strong>Choose a compute-optimized machine</strong>: We can select a more powerful EC2 instance with better CPU performance than being limited to the often slower machines provided by GitHub runners.</li><li><strong>Share the Build Cache</strong>: We can store the build cache on an EBS volume attached to the EC2 instance. This allows us to reuse the cache across multiple builds, as the Docker layers are persisted on the EBS volume.</li></ol><h2>Running the remote BuildKit instance</h2><p>We&#39;ve created a Terraform configuration file in this repository <a href="https://github.com/useblacksmith/remote-buildkit-terraform" target="_blank">https://github.com/useblacksmith/remote-buildkit-terraform</a> to automate the provisioning and configuration of the necessary resources for running our remote BuildKit instance on AWS. At a high level, here&#39;s what it does:</p><ol role="list"><li><strong>Provisioning an EC2 Instance</strong>: The Terraform configuration launches a <code>c5a.4xlarge</code> EC2 instance to host the BuildKit daemon. This instance type provides:<ul role="list"><li>16 vCPUs</li><li>32 GB of memory</li><li>100 GB gp3 SSD-based EBS volume</li><li>This configuration is sufficient for a small team&#39;s Docker build requirements. We&#39;re provisioning this in <code>us-east-2</code>, but you can run it in whichever region you prefer. However, you should verify that the Amazon Machine Image (AMI) we&#39;re using is available in that region.</li></ul></li><li><strong>Establishing Trust with GitHub Actions</strong>: An OpenID Connect (OIDC) provider is set up to enable GitHub Actions to assume an IAM role with short-lived access tokens. This eliminates the need for long-lived access keys and ensures secure access to the BuildKit instance and other AWS resources.</li></ol><p>Note that our BuildKit instance is configured to run on port <code>9999</code> of the EC2 instance. The instance&#39;s public IP address and this port are essential for configuring the GitHub Actions workflow to connect to the remote BuildKit instance.</p><h2>Setting Up Your Local Environment</h2><p>To start, follow these steps:</p><ol role="list"><li>Clone the <a href="https://github.com/useblacksmith/remote-buildkit-terraform" target="_blank">repository</a> to your local machine</li><li>Install and configure your <a href="https://aws.amazon.com/cli/" target="_blank">AWS CLI</a> with the appropriate credentials</li><li>Update variables in <code>terraform.tfvars</code> to point to where you’re running your GitHub Actions:<ul role="list"><li><code>github_org</code>: your GitHub organization</li><li><code>github_repo</code>: your repository name</li></ul></li><li>Run the Terraform configuration and wait for the resources to be provisioned</li></ol><p><code>‍</code></p><pre contenteditable="false"><code><span>terraform init
</span>terraform plan
terraform apply</code></pre><p>‍</p><p>You will notice the following output, take note of this output.</p><p>‍</p><pre contenteditable="false"><code><span>buildkit_instance_public_ip = &lt;IP-name&gt;
</span><span>github_actions_role_arn = </span><span>&#34;arn:aws:iam::&lt;ACCOUNT-ID&gt;:role/GithubActionsBuildKitRole&#34;</span></code></pre><p>‍</p><h2>Configuring GitHub Actions Workflow</h2><p>Once the remote BuildKit instance is up and running, it&#39;s time to modify your secrets and the GitHub Action workflow files running docker builds.</p><ol role="list"><li>In your GitHub repository, navigate to the &#34;Settings&#34; tab and click on &#34;Secrets&#34;.</li><li>Add these two new secrets:<ul role="list"><li><code>AWS_ACCOUNT_ID</code>: Paste the AWS secret output from the Terraform configuration.</li><li><code>BUILDKIT_HOST</code>: Paste the public IP address of the provisioned EC2 instance.</li></ul></li><li>In your workflow file, make sure to reference the <code>BUILDKIT_HOST</code> secret along with port 9999 when specifying the remote BuildKit server endpoint (e.g., <code>tcp://${{ secrets.BUILDKIT_HOST }}:9999</code>).<code>‍</code></li></ol><p><code>‍</code></p><pre contenteditable="false"><code><span>steps:</span><span>
</span><span>    </span><span>-</span><span> </span><span>uses:</span><span> </span><span>actions/checkout@v3</span><span>
</span><span>    </span><span>-</span><span> </span><span>uses:</span><span> </span><span>aws-actions/configure-aws-credentials@v4</span><span>
</span><span>      </span><span>with:</span><span>
</span><span>        </span><span>role-to-assume:</span><span> </span><span>arn:aws:iam::${{</span><span> </span><span>secrets.AWS_ACCOUNT_ID</span><span> </span><span>}}:role/GithubActionsBuildKitRole</span><span>
</span><span>        </span><span>aws-region:</span><span> </span><span>us-east-2</span><span>
</span><span>    </span><span>-</span><span> </span><span>uses:</span><span> </span><span>docker/setup-buildx-action@v2</span><span>
</span><span>      </span><span>with:</span><span>
</span><span>        </span><span>driver:</span><span> </span><span>remote</span><span>
</span><span>        </span><span>endpoint:</span><span> </span><span>tcp://${{</span><span> </span><span>secrets.BUILDKIT_HOST</span><span> </span><span>}}:9999</span><span>
</span><span>    </span><span>-</span><span> </span><span>uses:</span><span> </span><span>docker/build-push-action@v2</span><span>
</span><span>      </span><span>with:</span><span>
</span><span>        </span><span>context:</span><span> </span><span>.</span><span>
</span><span>        </span><span>file:</span><span> </span><span>./Dockerfile</span><span>
</span><span>        </span><span>push:</span><span> </span><span>false</span><span>
</span><span>        </span><span>tags:</span><span> </span><span>test-image:latest</span><span>
</span><span>        </span><span>load:</span><span> </span><span>true</span></code></pre><p><code>‍<br/></code></p><p>When we triggered a build, our first uncached run took <strong>6:22 minutes</strong>.</p><figure><p><img src="https://cdn.prod.website-files.com/667db86cfee88934419c207a/66ba581829fada0d58276cfa_66ba57e774e14685f166bc64_uncached.png" loading="lazy" alt=""/></p></figure><p>‍</p><p>When we reran the job, the cached run only took <strong>1:34 minutes</strong>.</p><figure><p><img src="https://cdn.prod.website-files.com/667db86cfee88934419c207a/66ba581829fada0d58276cfe_66ba5806b6762415ae029027_cached.png" loading="lazy" alt=""/></p></figure><p>‍</p><p>As you can see from the logs, each layer had a cache hit, significantly improving the build time.</p><figure><p><img src="https://cdn.prod.website-files.com/667db86cfee88934419c207a/66ba7322fc0b6b72fc1031ac_66ba731ac69a94f3ae4bae98_Screenshot%25202024-08-12%2520at%25204.38.39%25E2%2580%25AFPM.png" loading="lazy" alt=""/></p></figure><p>Caching Docker layers significantly improves build times, reducing the build duration from 6:22 minutes to just 1:34 minutes in this example. Docker layer caching is particularly effective in situations where your “base” layers observe minimal changes, as Docker rebuilds only the layers starting from the modified one while reusing the cached layers that remain unaltered. This cache will be shared across your entire org and so all CI builds will benefit from it.</p><h2>Downsides</h2><p>Although using a single shared BuildKit instance hosted on an EC2 machine is a simple approach, we do want to callout some drawbacks that limit its scalability and effectiveness for larger engineering teams.</p><ol role="list"><li><strong>Lack of autoscaling</strong>: This setup does not support autoscaling to handle fluctuating demand. Many of our customers have GitHub Actions workflows that trigger dozens of concurrent builds. If the EC2 instance does not have enough resources to handle the load, it can lead to resource contention. During peak times, this may result in slow build times and potentially cause the BuildKit EC2 instance to run out of memory. The inability to automatically scale resources up and down based on need is a major limitation.</li><li><strong>Cost</strong>: Constantly running a dedicated EC2 instance is not cost-effective, especially for teams that are not geographically distributed. For example, keeping a reserved c5a.4xlarge instance running for an entire month costs over $250. If most of the team is in the same time zone, the instance will be idle much of the time, wasting money.</li><li><strong>Security</strong>: Using a single shared BuildKit instance introduces security risks by failing to properly isolate projects from each other. In this shared environment, there is an increased chance of accidental credential exposure, and cross-contamination between projects.</li></ol><p>You could explore dynamic suspension and resumption of EC2 instances, hot loading EBS volumes or using spot instances (to reduce costs). The main tradeoff is that suspending and provisioning new instances would increase CI wait times since provisioning a new instance for each build has a cold start time associated with it. <a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-initialize.html" target="_blank">AWS has pointers</a> on decreasing the boot-up time for instances using EBS volumes.</p><p>In conclusion, using a powerful remote BuildKit instance can significantly reduce Docker build times for small to medium-sized teams. While there are scalability concerns, we’ve seen many teams get very far with this solution as it offers a simple yet effective way to improve build performance, allowing you to focus on what matters.</p></div></div>
  </body>
</html>
