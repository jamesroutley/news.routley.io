<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://benkaiser.dev/can-llms-accurately-recall-the-bible/">Original</a>
    <h1>Can LLMs accurately recall the Bible?</h1>
    
    <div id="readability-page-1" class="page"><div>
                    <p>I&#39;ve often found myself uneasy when LLMs (Large Language Models) are asked to quote the Bible. While they can provide insightful discussions about faith, their tendency to hallucinate responses raises concerns when dealing with scripture, which we regard as the inspired Word of God.</p><p>To explore these concerns, I created a benchmark to evaluate how accurately LLMs can recall scripture word for word. Here&#39;s a breakdown of my methodology and the test results.</p><h3 id="methodology">Methodology</h3><p>To ensure consistent and fair evaluation, I tested each model using six scenarios designed to measure their ability to accurately recall scripture. For readers interested in the technical details, <a href="https://github.com/benkaiser/llm-compare?ref=benkaiser.dev">the source code for the tests is available here</a>. All tests were conducted with a temperature setting of 0, and I have given slack to the models by making the pass check case and whitespace insensitive.</p><p>A temperature of 0 ensures the models generate the most statistically probable response at each step, minimising creativity or variability and prioritising accuracy. This approach is particularly important when evaluating fixed reference material like the Bible, where precise wording matters.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="test-1-popular-scripture-recall">Test 1: Popular Scripture Recall</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>When asking a model to recall John 3:16 in the NIV translation, the only model that failed to accurately recall the verse word for word was Llama 3.3 70B. It was only a very slight translation mismatch, with it recalling &#34;only begotten son&#34; where the actual verse in the NIV does not include begotten, despite it being present in other translations.</p><h3 id="test-2-obscure-verse-recall">Test 2: Obscure Verse Recall</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>Many models struggled to recall Obadiah 1:16 NIV word for word, often mixing up the words with other translations. For these cases, I have marked them as partial for correctly recalling the verse in <em>some</em> translation, even if not the specific requested one. The models that clearly succeeded seem to be very large models, 405B for Llama and GPT 4o and Claude 3.5 Sonnet.</p><h3 id="test-3-verse-continuation">Test 3: Verse Continuation</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>When quoting the model 2 Chronicles 11:13 (but without specifying where in the bible it is found) and asking it to produce the immediate next verse, we had a much more mixed bag of results. Many medium-to-large sized models got this correct, but the smaller ones completely hallucinated parts or all of the verse. Claude 3.5 Haiku almost recalled the verse, but referred to the Levites as &#34;they&#34;, which is not explicitly a translation in any of the more well known translations and appears to be the model substituting the intention of the word rather than the exact one.</p><h3 id="test-4-verse-block-recall">Test 4: Verse Block Recall</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>⚠️</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>When asked to recall Lamentations chapter 3 verses 19 through 24, the models did this very well. Only the smallest of the models, Llama 3.1 8B outright failed here, instead recalling the beginning of the chapter. The two warnings were only slight translation mismatches of a few words, but the essence of the verse was preserved.</p><h3 id="test-5-query-based-lookup">Test 5: Query Based Lookup</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>Asking the models, &#34;What&#39;s that verse in the bible about the Earth being filled with knowledge of God&#39;s glory?&#34;, all of them successfully recalled it was Habakkuk 2:14. Verse lookup is definitely a strong-suit, even in smaller models.</p><h3 id="test-6-entire-chapter-recall">Test 6: Entire Chapter Recall</h3><!--kg-card-begin: html--><table>
      <tbody><tr>
        <th>Model</th>
        <th>Pass</th>
      </tr>
      
        <tr>
          <td>Llama 3.1 405B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Llama 3.1 8B</td>
          <td>❌</td>
        </tr>
      
        <tr>
          <td>Llama 3.3 70B</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>GPT 4o mini</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 1.5 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Gemini 2.0 Flash</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Haiku</td>
          <td>✅</td>
        </tr>
      
        <tr>
          <td>Claude 3.5 Sonnet</td>
          <td>✅</td>
        </tr>
      
    </tbody></table><!--kg-card-end: html--><p>When asking for the entire contents of Romans 6 in the KJV translation, almost all of the models recalled all 23 verses accurately. Even the failed case of Llama 3.1 8B recalled over 98% of the words correctly, with only 9 incorrect words. </p><h2 id="conclusions">Conclusions</h2><p>If you really want to lean on an LLM to give you textually accurate bible verses of popular translations, you really should lean on higher parameter count (i.e. larger) models. These include models like Llama 405B, OpenAI GPT 4o and Claude Sonnet which all had perfect scores. Smaller models (7B range) will often mix up translations, and in some cases even mix up or hallucinate verse altogether. Medium-sized models (70B range) often accurately preserve the intention of the verses, although the verse may be a mangled representation of several translations, and in some cases paraphrased a little by the LLM.</p><p>You can certainly still use smaller models for discussion that references scripture by Book/Chapter/Verse, but it is important to lean on an actual copy of the Bible for the correct text in these cases.</p><p>Looking into the future, we may very well see smaller models perform better on these benchmarks, but there is surely a limitation to how much information can be encoded into such small models.</p><p>For full test results, see <a href="https://benkaiser.github.io/llm-compare/results-with-responses.html?ref=benkaiser.dev">the results file here</a>, including the raw prompts for each test. If you feel like I missed a crucial test, feel free to <a href="https://github.com/benkaiser/llm-compare/issues/new?ref=benkaiser.dev">submit an issue on GitHub</a>.</p>
                </div></div>
  </body>
</html>
