<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/">Original</a>
    <h1>VaultGemma: The most capable differentially private LLM</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20250912">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="w2rqp">As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. <a href="https://en.wikipedia.org/wiki/Differential_privacy" target="_blank" rel="noopener noreferrer">Differential privacy</a> (DP) offers a mathematically robust solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional <a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener noreferrer">scaling laws</a> — rules describing performance dynamics — by reducing training stability (the model&#39;s ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of input prompts sent to the model simultaneously for processing) and computation costs.</p><p data-block-key="kvsp">Our new research, “<a href="https://arxiv.org/abs/2501.18914" target="_blank" rel="noopener noreferrer">Scaling Laws for Differentially Private Language Models</a>”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on <a href="https://huggingface.co/google/vaultgemma-1b" target="_blank" rel="noopener noreferrer">Hugging Face</a> and <a href="https://www.kaggle.com/models/google/vaultgemma" target="_blank" rel="noopener noreferrer">Kaggle</a>, alongside a <a href="https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf" target="_blank" rel="noopener noreferrer">technical report</a>, to advance the development of the next generation of private AI.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="w2rqp">Understanding the scaling laws</h2><p data-block-key="emls0">With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the &#34;noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.</p><p data-block-key="fbiiv">To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?”</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="sv3qw">Key findings: A powerful synergy</h2><p data-block-key="cr1ma">Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget (<a href="https://en.wikipedia.org/wiki/Floating_point_operations_per_second" target="_blank" rel="noopener noreferrer">FLOPs</a>) or data budget (tokens).</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <p data-block-key="sv3qw">To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.</p>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <p data-block-key="sv3qw">This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations — i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.</p>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="sv3qw">Applying the scaling laws to build VaultGemma</h2><p data-block-key="69vq3">The <a href="https://deepmind.google/models/gemma/" target="_blank" rel="noopener noreferrer">Gemma</a> models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.</p><h3 data-block-key="d6ih5">Algorithmic advancements: Training at scale</h3><p data-block-key="ac2lq">The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.</p><p data-block-key="2tili">One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of <a href="https://en.wikipedia.org/wiki/Poisson_sampling" target="_blank" rel="noopener noreferrer"><i>Poisson sampling</i></a>, which is a central component of <a href="https://arxiv.org/abs/1607.00133" target="_blank" rel="noopener noreferrer">DP-SGD</a>. We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on <a href="https://arxiv.org/abs/2411.04205" target="_blank" rel="noopener noreferrer">Scalable DP-SGD</a>, which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="sv3qw">Results</h2><p data-block-key="efh07">Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.</p><p data-block-key="25ku7">From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="sv3qw">We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., <a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener noreferrer">HellaSwag</a>, <a href="https://arxiv.org/abs/1905.10044" target="_blank" rel="noopener noreferrer">BoolQ</a>, <a href="https://arxiv.org/abs/1911.11641" target="_blank" rel="noopener noreferrer">PIQA</a>, <a href="https://arxiv.org/abs/1904.09728" target="_blank" rel="noopener noreferrer">SocialIQA</a>, <a href="https://arxiv.org/abs/1705.03551" target="_blank" rel="noopener noreferrer">TriviaQA</a>, <a href="https://arxiv.org/abs/1911.01547" target="_blank" rel="noopener noreferrer">ARC-</a>C, <a href="https://arxiv.org/abs/1911.01547" target="_blank" rel="noopener noreferrer">ARC-</a>E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.</p><p data-block-key="36cfk">Finally, the model comes with strong theoretical and empirical privacy protections.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h3 data-block-key="sv3qw">Formal privacy guarantee</h3><p data-block-key="2ivmr">In general, both the privacy parameters (ε, δ) and the privacy <i>unit</i> are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a <i>sequence</i>-level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the <a href="https://arxiv.org/abs/2408.00118" target="_blank" rel="noopener noreferrer">Gemma 2</a> model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, <a href="https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/">user-level differential privacy</a> would be a better choice.</p><p data-block-key="b4na6">What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h3 data-block-key="sv3qw">Empirical memorization</h3><p data-block-key="1td9o">Sequence-level DP provably bounds the influence of any single training sequence (example) on the final model. We prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="sv3qw">Conclusion</h2><p data-block-key="ej03m">VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.</p><p data-block-key="bv1kj">While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="sv3qw">Acknowledgements</h2><p data-block-key="doolt"><i>We&#39;d like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.</i></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
