<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.nvidia.com/labs/dir/magic3d/">Original</a>
    <h1>Magic3D: High-Resolution Text-to-3D Content Creation</h1>
    
    <div id="readability-page-1" class="page"><div>

        <!-- Abstract -->
        <h5>Abstract</h5>
        <p>
            DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results.
            However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time.
            In this paper, we address these limitations by utilizing a two-stage optimization framework.
            First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure.
            Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model.
            Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2× faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution.
            User studies show 61.7% raters to prefer our approach over DreamFusion.
            Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.
        </p>

        <!-- Summary video -->
        <hr/>
        <h5>Video</h5>
        <p>
            <!-- <iframe width=860 height=484 class="large_video" src="https://www.youtube.com/embed/dCmCZs2Hpi0" frameborder=0></iframe> -->
            <video width="100%" controls=""><source src="assets/video.mp4" type="video/mp4"/>X</video>
        </p>

        <!-- 3D meshes -->
        <hr/>
        <h5>High-Resolution 3D Meshes</h5>
        <div>
            <p>
                Magic3D can create high-quality 3D textured mesh models from input text prompts.
                It utilizes a coarse-to-fine strategy leveraging both low- and high-resolution diffusion priors for learning the 3D representation of the target content.
                Magic3D synthesizes 3D content with 8× higher-resolution supervision than <a href="http://dreamfusion3d.github.io/">DreamFusion</a> while also being 2× faster.
            </p>
            <p>
                <b>[...]</b> indicates helper captions added to improve quality, e.g. &#34;A DSLR photo of&#34;.<br/>
            </p>

            <div>
                <div>
                    
                    <model-viewer id="mesh1-model" src="assets/selected_glb_files/005.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> A beautiful dress made out of garbage bags, on a mannequin. Studio lighting, high quality, high resolution. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh2-model" src="assets/selected_glb_files/009.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> A blue poison-dart frog sitting on a water lily. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh3-model" src="assets/selected_glb_files/048.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] a car made out of sushi. </p>
                </div>
            </div>
            <div>
                <div>
                    
                    <model-viewer id="mesh4-model" src="assets/selected_glb_files/034.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] a bagel filled with cream cheese and lox. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh5-model" src="assets/selected_glb_files/115.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] an ice cream sundae. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh6-model" src="assets/selected_glb_files/130.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] a peacock on a surfboard. </p>
                </div>
            </div>
            <div>
                <div>
                    
                    <model-viewer id="mesh7-model" src="assets/selected_glb_files/137.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] a plate piled high with chocolate chip cookies. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh8-model" src="assets/selected_glb_files/210.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] Neuschwanstein Castle, aerial view. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh9-model" src="assets/selected_glb_files/211.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] the Imperial State Crown of England. </p>
                </div>
            </div>
            <div>
                <div>
                    
                    <model-viewer id="mesh10-model" src="assets/selected_glb_files/212.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] the leaning tower of Pisa, aerial view. </p>
                </div>
                
                <div>
                    
                    <model-viewer id="mesh12-model" src="assets/selected_glb_files/279.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> A silver platter piled high with fruits. </p>
                </div>
            </div>
            <div>
                <div>
                    
                    <model-viewer id="mesh13-model" src="assets/selected_glb_files/387.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] a silver candelabra sitting on a red velvet tablecloth, only one candle is lit. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh14-model" src="assets/selected_glb_files/400.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> [...] Sydney opera house, aerial view. </p>
                </div>
                <div>
                    
                    <model-viewer id="mesh15-model" src="assets/selected_glb_files/405.glb" ar="" shadow-intensity="1" camera-controls="" touch-action="pan-y"></model-viewer>
                    
                    
                    <p> Michelangelo style statue of an astronaut. </p>
                </div>
            </div>
        </div>

        <!-- Prompt-based Edit -->
        <hr/>
        <h5>Prompt-based Editing</h5>
        <div>
            <p>
                Given a coarse model generated with a base text prompt, we can modify parts of the text in the prompt, and then fine-tune the NeRF and 3D mesh models to obtain an edited high-resolution 3D mesh.
            </p>
            <div>
                
                <p><img src="https://research.nvidia.com/labs/dir/magic3d/images/arrow.png"/>
                </p>
                
                
                
            </div>
            <div>
                <p> A <span>squirrel wearing a leather jacket</span> riding a <span>motorcycle</span>. </p>
                
                <p> A <span>bunny</span> riding a <span>scooter</span>. </p>
                <p> A <span>fairy</span> riding a <span>bike</span>. </p>
                <p> A <span>steampunk squirrel</span> riding a <span>horse</span>. </p>
            </div>
            <div>
                
                <p><img src="https://research.nvidia.com/labs/dir/magic3d/images/arrow.png"/>
                </p>
                
                
                
            </div>
            <div>
                <p> A <span>baby bunny</span> sitting on top of a stack of <span>pancakes</span>. </p>
                
                <p> A <span>lego bunny</span> sitting on top of a stack of <span>books</span>. </p>
                <p> A <span>metal bunny</span> sitting on top of a stack of <span>broccoli</span>. </p>
                <p> A <span>metal bunny</span> sitting on top of a stack of <span>chocolate cookies</span>. </p>
            </div>
        </div>

        <!-- 3D Dreambooth -->
        <hr/>
        <h5>Other Editing Capabilities</h5>
        <div>
            <p>
                Given input images for a subject instance, we can fine-tune the diffusion models with <a href="https://dreambooth.github.io/">DreamBooth</a> and optimize the 3D models with the given prompts.
                The identity of the subject can be well-preserved in the 3D models.
            </p>
            <div>
                <p><img src="https://research.nvidia.com/labs/dir/magic3d/assets/dreambooth/static1.png"/>
                </p>
                <p><img src="https://research.nvidia.com/labs/dir/magic3d/assets/dreambooth/static2.png"/>
                </p>
            </div>
        </div>
        <div>
            <p>
                We can also condition the diffusion model (eDiff-I) on an input image to transfer its style to the output 3D model.
            </p>
            <p><img src="https://research.nvidia.com/labs/dir/magic3d/assets/style_transfer-a.png"/>
            </p>
        </div>

        <!-- Approach -->
        <hr/>
        <h5>Approach</h5>
        <div>
            <p>
                We utilize a two-stage coarse-to-fine optimization framework for fast and high-quality text-to-3D content creation.
                In the first stage, we obtain a coarse model using a low-resolution diffusion prior and accelerate this with a hash grid and sparse acceleration structure.
                In the second stage, we use a textured mesh model initialized from the coarse neural representation, allowing optimization with an efficient differentiable renderer interacting with a high-resolution latent diffusion model.
            </p>
            <p><img src="https://research.nvidia.com/labs/dir/magic3d/assets/diagram.jpg"/>
            </p>
        </div>

        <!-- Citation -->
        <hr/>
        <h5>Citation</h5>
        <div>
            <div id="citation"><p>
                @inproceedings{lin2023magic3d,</p></div>
        </div>

    </div></div>
  </body>
</html>
