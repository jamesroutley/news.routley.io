<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepimagination.cc/Magic3D/">Original</a>
    <h1>Magic3D: High-Resolution Text-to-3D Content Creation</h1>
    
    <div id="readability-page-1" class="page"><div>

        <!-- Summary video -->
        <h5>Video</h5>
        <p>
            <!-- <iframe width=860 height=484 class="large_video" src="https://www.youtube.com/embed/dCmCZs2Hpi0" frameborder=0></iframe> -->
            <video width="100%" controls=""><source src="assets/video.mp4" type="video/mp4"/>X</video>
        </p>

        <!-- 3D meshes -->
        <hr/>
        <h5>High-Resolution 3D Meshes</h5>
        <div>
            <p>
                Magic3D can create high-quality 3D textured mesh models from input text prompts.
                It utilizes a coarse-to-fine strategy that leverages both low- and highresolution diffusion priors for learning the 3D representation of the target content.
                Magic3D synthesizes 3D content with 8× higher-resolution supervision than DreamFusion while also being 2× faster.
            </p>
            <p>
                <b>[...]</b> indicates helper captions added to improve quality, e.g. &#34;A DSLR photo of&#34;.</p>

            
            <div>
                <p> A beautiful dress made out of garbage bags, on a mannequin. Studio lighting, high quality, high resolution. </p>
                <p> A blue poison-dart frog sitting on a water lily. </p>
                <p> [...] a car made out of sushi. </p>
            </div>
            
            <div>
                <p> [...] a bagel filled with cream cheese and lox. </p>
                <p> [...] an ice cream sundae. </p>
                <p> [...] a peacock on a surfboard. </p>
            </div>
            
            <div>
                <p> [...] a plate piled high with chocolate chip cookies. </p>
                <p> [...] Neuschwanstein Castle, aerial view. </p>
                <p> [...] the Imperial State Crown of England. </p>
            </div>
            
            <div>
                <p> [...] the leaning tower of Pisa, aerial view. </p>
                <p> A ripe strawberry. </p>
                <p> A silver platter piled high with fruits. </p>
            </div>
            
            <div>
                <p> [...] a silver candelabra sitting on a red velvet tablecloth, only one candle is lit. </p>
                <p> [...] Sydney opera house, aerial view. </p>
                <p> Michelangelo style statue of an astronaut. </p>
            </div>
        </div>

        <!-- Prompt-based Edit -->
        <hr/>
        <h5>Prompt-based Editing</h5>
        <div>
            <p>
                Given a coarse model generated with a base text prompt, we can modify parts of the text in the prompt, and then fine-tune the NeRF and 3D mesh models to obtain an edited high-resolution 3D mesh.
            </p>
            <div>
                
                <p><img src="https://deepimagination.cc/Magic3D/images/arrow.png"/>
                </p>
                
                
                
            </div>
            <div>
                <p> A <span>squirrel wearing a leather jacket</span> riding a <span>motorcycle</span>. </p>
                
                <p> A <span>bunny</span> riding a <span>scooter</span>. </p>
                <p> A <span>fairy</span> riding a <span>bike</span>. </p>
                <p> A <span>steampunk squirrel</span> riding a <span>horse</span>. </p>
            </div>
            <div>
                
                <p><img src="https://deepimagination.cc/Magic3D/images/arrow.png"/>
                </p>
                
                
                
            </div>
            <div>
                <p> A <span>baby bunny</span> sitting on top of a stack of <span>pancakes</span>. </p>
                
                <p> A <span>lego bunny</span> sitting on top of a stack of <span>books</span>. </p>
                <p> A <span>metal bunny</span> sitting on top of a stack of <span>broccoli</span>. </p>
                <p> A <span>metal bunny</span> sitting on top of a stack of <span>chocolate cookies</span>. </p>
            </div>
        </div>

        <!-- 3D Dreambooth -->
        <hr/>
        <h5>Other Editing Capabilities</h5>
        <div>
            <p>
                Given input images for a subject instance, we can fine-tune the diffusion models with <a href="https://dreambooth.github.io/">DreamBooth</a> and optimize the 3D models with the given prompts.
                The identity of the subject can be well-preserved in the 3D models.
            </p>
            <div>
                <p><img src="https://deepimagination.cc/Magic3D/assets/dreambooth/static1.png"/>
                </p>
                <p><img src="https://deepimagination.cc/Magic3D/assets/dreambooth/static2.png"/>
                </p>
            </div>
            <!-- <div class="row">
                <div class="column-4">
                    <img src="assets/dreambooth/input_cat.png" style="width: 100%;">
                </div>
                <div class="column-4">
                    <video id="dreambooth1" loop muted autoplay style="width: 100%;"> <source src="assets/dreambooth/cat_bike2_rgb-a.mp4" type="video/mp4"> </video>
                </div>
                <div class="column-4">
                    <img src="assets/dreambooth/input_dog.png" style="width: 100%">
                </div>
                <div class="column-4">
                    <video id="dreambooth1" loop muted autoplay style="width: 100%;"> <source src="assets/dreambooth/dog_run_rgb-a.mp4" type="video/mp4"> </video>
                </div>
            </div>
            <div class="row">
                <div class="vid-prompt-4"> (input images) </div>
                <div class="vid-prompt-4"> A <b>[V]</b> cat riding a bike. </div>
                <div class="vid-prompt-4"> (input images) </div>
                <div class="vid-prompt-4"> A <b>[V]</b> dog running down the track. </div>
            </div> -->
        </div>
        <div>
            <p>
                We can also condition the diffusion model (eDiff-I) on an input image to transfer its style to the output 3D model.
                <!-- The style of the reference image can be transferred to the 3D model by providing it to the diffusion model as conditional input. -->
                <!-- We apply different styles during 3D synthesis with two different text prompts. -->
            </p>
            <p><img src="https://deepimagination.cc/Magic3D/assets/style_transfer-a.png"/>
            </p>
        </div>

        <!-- Method -->
        <hr/>
        <h5>Method</h5>
        <div>
            <p>
                We utilize a two-stage coarse-to-fine optimization framework for fast and high-quality text-to-3D content creation.
                In the first stage, we obtain a coarse model using a low-resolution diffusion prior and accelerate this with a hash grid and sparse acceleration structure.
                In the second stage, we use a textured mesh model initialized from the coarse neural representation, allowing optimization with an efficient differentiable renderer interacting with a high-resolution latent diffusion model.
            </p>
            <p><img src="https://deepimagination.cc/Magic3D/assets/diagram.jpg"/>
            </p>
        </div>

        <!-- Citation. -->
        <hr/>
        <h5>Citation</h5>
        

    </div></div>
  </body>
</html>
