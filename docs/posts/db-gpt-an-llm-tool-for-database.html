<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/csunny/DB-GPT">Original</a>
    <h1>Show HN: DB-GPT, an LLM tool for database</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<hr/>
<p dir="auto"><a href="https://nayak.io/csunny/DB-GPT/blob/main/README.zh.md">简体中文</a></p>
<p dir="auto"><a href="https://star-history.com/#csunny/DB-GPT" rel="nofollow"><img src="https://camo.githubusercontent.com/d9eea66805e44e7a6cef3dcc5bff24f97794374bbebc3e275decd2f65792f1e4/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6373756e6e792f44422d475054" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=csunny/DB-GPT"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-what-is-db-gpt" aria-hidden="true" href="#what-is-db-gpt"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>What is DB-GPT?</h2>
<p dir="auto">As large models are released and iterated upon, they are becoming increasingly intelligent. However, in the process of using large models, we face significant challenges in data security and privacy. We need to ensure that our sensitive data and environments remain completely controlled and avoid any data privacy leaks or security risks. Based on this, we have launched the DB-GPT project to build a complete private large model solution for all database-based scenarios. This solution supports local deployment, allowing it to be applied not only in independent private environments but also to be independently deployed and isolated according to business modules, ensuring that the ability of large models is absolutely private, secure, and controllable.</p>
<p dir="auto">DB-GPT is an experimental open-source project that uses localized GPT large models to interact with your data and environment. With this solution, you can be assured that there is no risk of data leakage, and your data is 100% private and secure.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-features" aria-hidden="true" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<p dir="auto">Currently, we have released multiple key features, which are listed below to demonstrate our current capabilities:</p>
<ul dir="auto">
<li>
<p dir="auto">SQL language capabilities</p>
<ul dir="auto">
<li>SQL generation</li>
<li>SQL diagnosis</li>
</ul>
</li>
<li>
<p dir="auto">Private domain Q&amp;A and data processing</p>
</li>
<li>
<p dir="auto">Database knowledge Q&amp;A</p>
</li>
<li>
<p dir="auto">Data processing</p>
</li>
<li>
<p dir="auto">Plugins</p>
<ul dir="auto">
<li>Support custom plugin execution tasks and natively support the Auto-GPT plugin, such as:</li>
<li>Automatic execution of SQL and retrieval of query results</li>
<li>Automatic crawling and learning of knowledge</li>
</ul>
</li>
<li>
<p dir="auto">Unified vector storage/indexing of knowledge base</p>
<ul dir="auto">
<li>Support for unstructured data such as PDF, Markdown, CSV, and WebURL</li>
</ul>
</li>
<li>
<p dir="auto">Milti LLMs Support</p>
<ul dir="auto">
<li>Supports multiple large language models, currently supporting Vicuna (7b, 13b), ChatGLM-6b (int4, int8)</li>
<li>TODO: codegen2, codet5p</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-demo" aria-hidden="true" href="#demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demo</h2>
<p dir="auto">Run on an RTX 4090 GPU. <a href="https://www.youtube.com/watch?v=1PWI6F89LPo" rel="nofollow">YouTube</a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-run" aria-hidden="true" href="#run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run</h3>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/demo_en.gif"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/demo_en.gif" width="600px" data-animated-image=""/></a>
</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-sql-generation" aria-hidden="true" href="#sql-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SQL Generation</h3>
<ol dir="auto">
<li>Generate Create Table SQL</li>
</ol>
<p dir="auto">
   <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/SQL_Gen_CreateTable_en.png"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/SQL_Gen_CreateTable_en.png" width="600px"/></a>
</p>
<ol start="2" dir="auto">
<li>Generating executable SQL:To generate executable SQL, first select the corresponding database and then the model can generate SQL based on the corresponding database schema information. The successful result of running it would be demonstrated as follows:</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/exeable_en.png"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/exeable_en.png" width="600px"/></a>
</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-qa" aria-hidden="true" href="#qa"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Q&amp;A</h3>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/DB_QA_en.png"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/DB_QA_en.png" width="600px"/></a>
</p>
<ol dir="auto">
<li>Based on the default built-in knowledge base, question and answer.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/Knownledge_based_QA_en.png"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/Knownledge_based_QA_en.png" width="600px"/></a>
</p>
<ol start="2" dir="auto">
<li>Add your own knowledge base.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/new_knownledge_en.gif"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/new_knownledge_en.gif" width="600px" data-animated-image=""/></a>
</p>
<ol start="3" dir="auto">
<li>Learning from crawling data from the Internet</li>
</ol>
<ul dir="auto">
<li>TODO</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-introduction" aria-hidden="true" href="#introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">DB-GPT creates a vast model operating system using <a href="https://github.com/lm-sys/FastChat">FastChat</a> and offers a large language model powered by <a href="https://huggingface.co/Tribbiani/vicuna-7b" rel="nofollow">Vicuna</a>. In addition, we provide private domain knowledge base question-answering capability through LangChain. Furthermore, we also provide support for additional plugins, and our design natively supports the Auto-GPT plugin.</p>
<p dir="auto">Is the architecture of the entire DB-GPT shown in the following figure:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://nayak.io/csunny/DB-GPT/blob/main/assets/DB-GPT.png"><img src="https://nayak.io/csunny/DB-GPT/raw/main/assets/DB-GPT.png" width="600px"/></a>
</p>
<p dir="auto">The core capabilities mainly consist of the following parts:</p>
<ol dir="auto">
<li>Knowledge base capability: Supports private domain knowledge base question-answering capability.</li>
<li>Large-scale model management capability: Provides a large model operating environment based on FastChat.</li>
<li>Unified data vector storage and indexing: Provides a uniform way to store and index various data types.</li>
<li>Connection module: Used to connect different modules and data sources to achieve data flow and interaction.</li>
<li>Agent and plugins: Provides Agent and plugin mechanisms, allowing users to customize and enhance the system&#39;s behavior.</li>
<li>Prompt generation and optimization: Automatically generates high-quality prompts and optimizes them to improve system response efficiency.</li>
<li>Multi-platform product interface: Supports various client products, such as web, mobile applications, and desktop applications.</li>
</ol>
<p dir="auto">Below is a brief introduction to each module:</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-knowledge-base-capability" aria-hidden="true" href="#knowledge-base-capability"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Knowledge base capability</h3>
<p dir="auto">As the knowledge base is currently the most significant user demand scenario, we natively support the construction and processing of knowledge bases. At the same time, we also provide multiple knowledge base management strategies in this project, such as:</p>
<ol dir="auto">
<li>Default built-in knowledge base</li>
<li>Custom addition of knowledge bases</li>
<li>Various usage scenarios such as constructing knowledge bases through plugin capabilities and web crawling. Users only need to organize the knowledge documents, and they can use our existing capabilities to build the knowledge base required for the large model.</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-llms-management" aria-hidden="true" href="#llms-management"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LLMs Management</h3>
<p dir="auto">In the underlying large model integration, we have designed an open interface that supports integration with various large models. At the same time, we have a very strict control and evaluation mechanism for the effectiveness of the integrated models. In terms of accuracy, the integrated models need to align with the capability of ChatGPT at a level of 85% or higher. We use higher standards to select models, hoping to save users the cumbersome testing and evaluation process in the process of use.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-vector-storage-and-indexing" aria-hidden="true" href="#vector-storage-and-indexing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Vector storage and indexing</h3>
<p dir="auto">In order to facilitate the management of knowledge after vectorization, we have built-in multiple vector storage engines, from memory-based Chroma to distributed Milvus. Users can choose different storage engines according to their own scenario needs. The storage of knowledge vectors is the cornerstone of AI capability enhancement. As the intermediate language for interaction between humans and large language models, vectors play a very important role in this project.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-connections" aria-hidden="true" href="#connections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Connections</h3>
<p dir="auto">In order to interact more conveniently with users&#39; private environments, the project has designed a connection module, which can support connection to databases, Excel, knowledge bases, and other environments to achieve information and data exchange.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-agent-and-plugin" aria-hidden="true" href="#agent-and-plugin"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Agent and Plugin</h3>
<p dir="auto">The ability of Agent and Plugin is the core of whether large models can be automated. In this project, we natively support the plugin mode, and large models can automatically achieve their goals. At the same time, in order to give full play to the advantages of the community, the plugins used in this project natively support the Auto-GPT plugin ecology, that is, Auto-GPT plugins can directly run in our project.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-prompt-automatic-generation-and-optimization" aria-hidden="true" href="#prompt-automatic-generation-and-optimization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Prompt Automatic Generation and Optimization</h3>
<p dir="auto">Prompt is a very important part of the interaction between the large model and the user, and to a certain extent, it determines the quality and accuracy of the answer generated by the large model. In this project, we will automatically optimize the corresponding prompt according to user input and usage scenarios, making it easier and more efficient for users to use large language models.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-multi-platform-product-interface" aria-hidden="true" href="#multi-platform-product-interface"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Multi-Platform Product Interface</h3>
<p dir="auto">TODO: In terms of terminal display, we will provide a multi-platform product interface, including PC, mobile phone, command line, Slack and other platforms.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-deployment" aria-hidden="true" href="#deployment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Deployment</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-1-hardware-requirements" aria-hidden="true" href="#1-hardware-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>1. Hardware Requirements</h3>
<p dir="auto">As our project has the ability to achieve ChatGPT performance of over 85%, there are certain hardware requirements. However, overall, the project can be deployed and used on consumer-grade graphics cards. The specific hardware requirements for deployment are as follows:</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>VRAM Size</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 4090</td>
<td>24 GB</td>
<td>Smooth conversation inference</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>24 GB</td>
<td>Smooth conversation inference, better than V100</td>
</tr>
<tr>
<td>V100</td>
<td>16 GB</td>
<td>Conversation inference possible, noticeable stutter</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto"><a id="user-content-2-install" aria-hidden="true" href="#2-install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>2. Install</h3>
<p dir="auto">This project relies on a local MySQL database service, which you need to install locally. We recommend using Docker for installation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker run --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=aa12345678 -dit mysql:latest"><pre>$ docker run --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=aa12345678 -dit mysql:latest</pre></div>
<p dir="auto">We use <a href="https://github.com/chroma-core/chroma">Chroma embedding database</a> as the default for our vector database, so there is no need for special installation. If you choose to connect to other databases, you can follow our tutorial for installation and configuration.
For the entire installation process of DB-GPT, we use the miniconda3 virtual environment. Create a virtual environment and install the Python dependencies.</p>
<div data-snippet-clipboard-copy-content="python&gt;=3.10
conda create -n dbgpt_env python=3.10
conda activate dbgpt_env
pip install -r requirements.txt"><pre><code>python&gt;=3.10
conda create -n dbgpt_env python=3.10
conda activate dbgpt_env
pip install -r requirements.txt
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-3-run" aria-hidden="true" href="#3-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>3. Run</h3>
<p dir="auto">You can refer to this document to obtain the Vicuna weights: <a href="https://github.com/lm-sys/FastChat/blob/main/README.md#model-weights">Vicuna</a> .</p>
<p dir="auto">If you have difficulty with this step, you can also directly use the model from <a href="https://huggingface.co/Tribbiani/vicuna-7b" rel="nofollow">this link</a> as a replacement.</p>
<ol dir="auto">
<li>Run server</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ python pilot/server/llmserver.py"><pre>$ python pilot/server/llmserver.py</pre></div>
<p dir="auto">Run gradio webui</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ python pilot/server/webserver.py"><pre>$ python pilot/server/webserver.py</pre></div>
<p dir="auto">Notice:  the webserver need to connect llmserver,  so you need change the .env file. change the MODEL_SERVER = &#34;<a href="http://127.0.0.1:8000" rel="nofollow">http://127.0.0.1:8000</a>&#34; to your address.  It&#39;s very important.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage-instructions" aria-hidden="true" href="#usage-instructions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage Instructions</h2>
<p dir="auto">We provide a user interface for Gradio, which allows you to use DB-GPT through our user interface. Additionally, we have prepared several reference articles (written in Chinese) that introduce the code and principles related to our project.</p>
<ul dir="auto">
<li><a href="https://medium.com/@cfqcsunny/llm-practical-in-action-series-1-combined-langchain-vicuna-application-practical-701cd0413c9f" rel="nofollow">LLM Practical In Action Series (1) — Combined Langchain-Vicuna Application Practical</a></li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-multi-llms-usage" aria-hidden="true" href="#multi-llms-usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Multi LLMs Usage</h3>
<p dir="auto">To use multiple models, modify the LLM_MODEL parameter in the .env configuration file to switch between the models.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgement" aria-hidden="true" href="#acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgement</h2>
<p dir="auto">The achievements of this project are thanks to the technical community, especially the following projects:</p>
<ul dir="auto">
<li><a href="https://github.com/lm-sys/FastChat">FastChat</a> for providing chat services</li>
<li><a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="nofollow">vicuna-13b</a> as the base model</li>
<li><a href="https://langchain.readthedocs.io/" rel="nofollow">langchain</a> tool chain</li>
<li><a href="https://github.com/Significant-Gravitas/Auto-GPT">Auto-GPT</a> universal plugin template</li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face</a> for big model management</li>
<li><a href="https://github.com/chroma-core/chroma">Chroma</a> for vector storage</li>
<li><a href="https://milvus.io/" rel="nofollow">Milvus</a> for distributed vector storage</li>
<li><a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM</a> as the base model</li>
<li><a href="https://github.com/jerryjliu/llama_index">llama_index</a> for enhancing database-related knowledge using <a href="https://arxiv.org/abs/2301.00234" rel="nofollow">in-context learning</a> based on existing knowledge bases.</li>
</ul>

<h2 tabindex="-1" dir="auto"><a id="user-content-contributors" aria-hidden="true" href="#contributors"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributors</h2>
<table>
<thead>
<tr>
<th><a href="https://github.com/csunny"><img src="https://avatars.githubusercontent.com/u/17919400?v=4" width="100px;"/></a><br/></th>
<th><a href="https://github.com/xudafeng"><img src="https://avatars.githubusercontent.com/u/1011681?v=4" width="100px;"/></a><br/></th>
<th><a href="https://github.com/yhjun1026"><img src="https://avatars.githubusercontent.com/u/7636723?s=96&amp;v=4" width="100px;"/></a><br/></th>
<th><a href="https://github.com/Aries-ckt"><img src="https://avatars.githubusercontent.com/u/13723926?v=4" width="100px;"/></a><br/></th>
<th><a href="https://github.com/thebigbone"><img src="https://avatars.githubusercontent.com/u/95130644?v=4" width="100px;"/></a><br/></th>
</tr>
</thead>
</table>
<p dir="auto">This project follows the git-contributor <a href="https://github.com/xudafeng/git-contributor">spec</a>, auto updated at <code>Fri May 19 2023 00:24:18 GMT+0800</code>.</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-licence" aria-hidden="true" href="#licence"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Licence</h2>
<p dir="auto">The MIT License (MIT)</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contact-information" aria-hidden="true" href="#contact-information"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contact Information</h2>
<p dir="auto">We are working on building a community, if you have any ideas about building the community, feel free to contact us. <a href="https://discord.com/invite/twmZk3vv" rel="nofollow">Discord</a></p>
</article>
          </div></div>
  </body>
</html>
