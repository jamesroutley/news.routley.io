<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/">Original</a>
    <h1>Let&#39;s stop ascribing meaning to code points (2017)</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>Update: This post got a sequel, <a href="http://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/">Breaking our latin-1 assumptions</a>.</em></p>

<p>Iâ€™ve seen misconceptions about Unicode crop up regularly in posts discussing it. One very common
misconception Iâ€™ve seen is that <em>code points have cross-language intrinsic meaning</em>.</p>

<p>It usually comes up when people are comparing UTF8 and UTF32. Folks start implying that code points
<em>mean</em> something, and that O(1) indexing or slicing at code point boundaries is a useful operation.
Iâ€™ve also seen this assumption manifest itself in actual programs which make incorrect assumptions
about the nature of code points and mess things up when fed non-Latin text.</p>

<p>If you like reading about unicode, you might also want to go through <a href="https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/">Eeveeâ€™s article</a>
on the dark corners of unicode. Great read!</p>

<h2 id="encodings">Encodings</h2>

<p>So, anyway, we have some popular encodings for Unicode. UTF8 encodes 7-bit code points as a single
byte, 11-bit code points as two bytes, 16-bit code points as 3 bytes, and 21-bit code points as four
bytes. UTF-16 encodes the first three in two bytes, and the last one as four bytes (logically, a
pair of two-byte code units). UTF-32 encodes all code points as 4-byte code units. UTF-16 is mostly
a â€œworst of both worldsâ€ compromise at this point, and the main programming language I can think of
that uses it (and exposes it in this form) is Javascript, and that too in a broken way.</p>

<p>The nice thing about UTF8 is that it saves space. Of course, that is subjective and dependent on
the script you use most commonly, for example my first name is 12 bytes in UTF-8 but only 4
in ISCII (or a hypothetical unicode-based encoding that swapped the Devanagri Unicode block with
the ASCII block). It also uses more space over the very non-hypothetical UTF-16 encoding if you
tend to use code points in the U+0800 - U+FFFF range. It always uses less space than UTF-32 however.</p>

<p>A commonly touted disadvantage of UTF-8 is that string indexing is <code>O(n)</code>. Because code points take
up a variable number of bytes, you wonâ€™t know where the 5th codepoint is until you scan the string
and look for it. UTF-32 doesnâ€™t have this problem; itâ€™s always <code>4 * index</code> bytes away.</p>

<p>The problem here is that indexing by code point shouldnâ€™t be an operation you ever need!</p>

<h2 id="indexing-by-code-point">Indexing by code point</h2>

<p>The main time you want to be able to index by code point is if youâ€™re implementing algorithms
defined in the unicode spec that operate on unicode strings (casefolding, segmentation, NFD/NFC).
Most if not all of these algorithms operate on whole strings, so implementing them
as an iteration pass is usually necessary anyway, so you donâ€™t lose anything if you canâ€™t
do arbitrary code point indexing.</p>

<p>But for application logic, dealing with code points doesnâ€™t really make sense. This is because
code points have no intrinsic meaning. They are not â€œcharactersâ€. Iâ€™m using scare quotes here
because a â€œcharacterâ€ isnâ€™t a well-defined concept either, but weâ€™ll get to that later.</p>

<p>For example, â€œeÌâ€ is two code points (<code>e</code> +<code> Ì</code>), where one of them is a combining accent. My name,
â€œà¤®à¤¨à¥€à¤·â€, visually looks like three â€œcharactersâ€, but is four code points. The â€œà¤¨à¥€â€ is made up of <code>à¤¨</code> + <code>à¥€</code>. My last name contains a â€œcharacterâ€ made up of three code points (as well as multiple other two-code-point
â€œcharactersâ€). The flag emoji â€œğŸ‡ºğŸ‡¸â€ is also made of two code points, <code>ğŸ‡º</code> + <code>ğŸ‡¸</code>.</p>

<p>One false assumption thatâ€™s often made is that code points are a single column wide. Theyâ€™re not.
They sometimes bunch up to form characters that fit in single â€œcolumnsâ€. This is often dependent on
the font, and if your application relies on this, you should be querying the font. There are even
code points like U+FDFD (ï·½) which are often rendered multiple columns wide. In fact, in my
<em>monospace</em> font in my text editor, that character is rendered <em>almost</em> 12 columns wide. Yes,
â€œalmostâ€, subsequent characters get offset a tiny bit, presumably because the font selection
picking a non-monospace font for the character.</p>

<p>Another false assumption is that editing actions (selection, backspace, cut, paste) operate on code
points. In both Chrome and Firefox, selection will often include multiple code points. All the
multi-code-point examples I gave above fall into this category. An interesting testcase for this is
the string â€œá„€á„€á„€ê°á†¨á†¨â€, which will rarely if ever render as a single â€œcharacterâ€ but will be considered
as one for the purposes of selection, pretty much universally. Iâ€™ll get to why this is later.</p>

<p>Backspace can gobble multiple code points at once too, but the heuristics are different. The reason
behind this is that backspace needs to mirror the act of typing, and while typing sometimes
constructs multi-codepoint characters, backspace decomposes it piece by piece. In cases where a
multi-codepoint â€œcharacterâ€ <em>can</em> be logically decomposed (e.g. â€œletter + accentâ€), backspace will
decompose it, by removing the accent or whatever. But some multi-codepoint characters are not
â€œconstructionsâ€ of general concepts that should be exposed to the user. For example, a user should
never need to know that the â€œğŸ‡ºğŸ‡¸â€ flag emoji is made of <code>ğŸ‡º</code> + <code>ğŸ‡¸</code>, and hitting backspace on it should
delete both codepoints. Similarly, variation selectors and other such code points shouldnâ€™t
be treated as their own unit when backspacing.</p>

<p>On my Mac most builtin apps (which I presume use the OSX UI toolkits) seem to use the same
heuristics that Firefox/Chrome use for selection for both selection and backspace. While the
treatment of code points in editing contexts is not consistent, it seems like applications
consistently do not consider code points as â€œediting unitsâ€.</p>

<p>Now, it is true that you often need <em>some</em> way to index a string. For example, if you have a large
document and need to represent a slice of it. This could be a user-selection, or something delimeted
by markup. Basically, youâ€™ve already gone through the document and have a section you want to be
able to refer to later without copying it out.</p>

<p>However, you donâ€™t need code point indexing here, byte
indexing works fine! UTF8 is designed so that you can check if youâ€™re on a code point boundary even
if you just byte-index directly. It does this by restricting the kinds of bytes allowed. One-byte
code points never have the high bit set (ASCII). All other code points have the high bit set in each
byte. The first byte of multibyte codepoints always starts with a sequence that specifies the number
of bytes in the codepoint, and such sequences canâ€™t be found in the lower-order bytes of any
multibyte codepoint. You can see this visually in the table <a href="https://en.wikipedia.org/wiki/UTF-8#Description">here</a>. The upshot of all this
is that you just need to check the current byte if you want to be sure youâ€™re on a codepoint
boundary, and if you receive an arbitrarily byte-sliced string, you will not mistake it for
something else. Itâ€™s not possible to have a valid code point be a subslice of another, or form a
valid code point by subslicing a sequence of two different ones by cutting each in half.</p>

<p>So all you need to do is keep track of the byte indices, and use them for slicing it later.</p>

<p>All in all, itâ€™s important to always remember that â€œcode pointâ€ doesnâ€™t have intrinsic meaning. If
you need to do a segmentation operation on a string, find out what <em>exactly</em> youâ€™re looking for, and
what concept maps closest to that. Itâ€™s rare that â€œcode pointâ€ is the concept youâ€™re looking for.
In <em>most</em> cases, what youâ€™re looking for instead is â€œgrapheme clusterâ€.</p>

<h2 id="grapheme-clusters">Grapheme clusters</h2>

<p>The concept of a â€œcharacterâ€ is a nebulous one. Is â€œá„€á…¡á†¨â€ a single character, or
three? How about â€œà¤¨à¥€â€? Or â€œà®¨à®¿â€? Or the â€œğŸ‘¨â€â¤ï¸â€ğŸ‘¨â€ emoji<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>? Or the â€œğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§â€ family emoji<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">2</a></sup>?
Different scripts have different concepts which may not clearly map to the Latin notion of â€œletterâ€
or our programmery notion of â€œcharacterâ€.</p>

<p>Unicode itself gives the term <a href="http://unicode.org/glossary/#character">â€œcharacterâ€</a> multiple incompatible meanings, and as
far as I know doesnâ€™t use the term in any normative text.</p>

<p>Often, you need to deal with what is actually displayed to the user. A lot of terminal emulators do
this wrong, and end up messing up cursor placement. I used to use irssi-xmpp to keep my Facebook and
Gchat conversations in my IRC client, but I eventually stopped as I was increasingly chatting in
Marathi or Hindi and I prefer using the actual script over romanizing<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">3</a></sup>, and it would just break
my terminal<sup id="fnref:5" role="doc-noteref"><a href="#fn:5">4</a></sup>. Also, they got rid of the XMPP bridge but Iâ€™d already cut down on it by then.</p>

<p>So sometimes, you need an API querying what the font is doing. Generally, when talking about the
actual rendered image, the term â€œglyphâ€ or â€œglyph imageâ€ is used.</p>

<p>However, you canâ€™t always query the font. Text itself exists independent of rendering, and sometimes
you need a rendering-agnostic way of segmenting it into â€œcharactersâ€.</p>

<p>For this, Unicode has a concept of <a href="http://unicode.org/glossary/#grapheme_cluster">â€œgrapheme clusterâ€</a>. Thereâ€™s also â€œextended grapheme
clusterâ€ (EGC), which is basically an updated version of the concept. In this post, whenever
I use the term â€œgrapheme clusterâ€, I am talking about EGCs.</p>

<p>The term is defined and explored in <a href="http://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries">UAX #29</a>. It starts by pinning down the still-nebulous
concept of â€œuser-perceived characterâ€ (â€œa basic unit of a writing system for a languageâ€),
and then declares the concept of a â€œgrapheme clusterâ€ to be an approximation to this notion
that we can determine programmatically.</p>

<p>A rough definition of grapheme cluster is a â€œhorizontally segmentable unit of textâ€.</p>

<p>The spec goes into detail as to the exact algorithm that segments text at grapheme cluster
boundaries. All of the examples I gave in the first paragraph of this section are single grapheme
clusters. So is â€œá„€á„€á„€ê°á†¨á†¨â€ (or â€œá„€á„€á„€á„€á…¡á†¨á†¨á†¨â€), which apparently is considered a
single syllable block in Hangul even though it is not of the typical form of leading consonant +
vowel + optional tail consonant, but is not something youâ€™d see in modern Korean. The spec
explicitly talks of this case so it seems to be on purpose. I like this string because nothing I
know of renders it as a single glyph; so you can easily use it to tell if a particular segmentation-
aware operation uses grapheme clusters as segmentation. If you try and select it, in most browsers
you will be forced to select the whole thing, but backspace will delete the jamos one by one. For
the second string, backspace will decompose the core syllable block too (in the first string the
syllable block á„€á…¡á†¨ is â€œprecomposedâ€ as a single code point, in the second one I
built it using combining jamos).</p>

<p>Basically, unless you have very specific requirements or are able to query the font, use an API that
segments strings into grapheme clusters wherever you need to deal with the notion of â€œcharacterâ€.</p>

<h2 id="language-defaults">Language defaults</h2>

<p>Now, a lot of languages by default are now using Unicode-aware encodings. This is great. It gets rid
of the misconception that characters are one byte long.</p>

<p>But it doesnâ€™t get rid of the misconception that user-perceived characters are one code point long.</p>

<p>There are only two languages I know of which handle this well: Swift and Perl 6. I donâ€™t know much
about Perl 6â€™s thing so I canâ€™t really comment on it, but I am really happy with what Swift does:</p>

<p>In Swift, the <code>Character</code> type is an extended grapheme cluster. This does mean that a
character itself is basically a string, since EGCs can be arbitrarily many code points long.</p>

<p>All the APIs by default deal with EGCs. The length of a string is the number of EGCs in it. They
are indexed by EGC. Iteration yields EGCs. The default comparison algorithm uses unicode
canonical equivalence, which I think is kind of neat. Of course, APIs that work with code
points are exposed too, you can iterate over the code points using <code>.unicodeScalars</code>.</p>

<p>The internal encoding itself is â€¦ weird (and as far as I can tell not publicly exposed), but as a
higher level language I think itâ€™s fine to do things like that.</p>

<p>I strongly feel that languages should be moving in this direction, having defaults involving
grapheme clusters.</p>

<p>Rust, for example, gets a lot of things right â€“ it has UTF-8 strings. It internally uses byte
indices in slices. Explicit slicing usually uses byte indices too, and will panic if out of bounds.
The non-O(1) methods are all explicit, since you will use an iterator to perform the operation (E.g.
<code>.chars().nth(5)</code>). This encourages people to <em>think</em> about the cost, and it also  encourages people
to coalesce the cost with nearby iterations â€“ if you are going to do multiple <code>O(n)</code> things, do
them in a single iteration! Rust <code>char</code>s represent code points. <code>.char_indices()</code> is
a useful string iteration method that bridges the gap between byte indexing and code points.</p>

<p>However, while the documentation does mention grapheme clusters, the stdlib is not aware of the
concept of grapheme clusters at all. The default â€œfundamentalâ€ unit of the string in Rust is
a code point, and the operations revolve around that. If you want grapheme clusters, you
may use <a href="https://unicode-rs.github.io/unicode-segmentation/unicode_segmentation/trait.UnicodeSegmentation.html#tymethod.graphemes"><code>unicode-segmentation</code></a></p>

<p>Now, Rust is a systems programming language and it just wouldnâ€™t do to have expensive grapheme
segmentation operations all over your string defaults. Iâ€™m very happy that the expensive <code>O(n)</code>
operations are all only possible with explicit acknowledgement of the cost. So I do think that going
the Swift route would be counterproductive for Rust. Not that it <em>can</em> anyway, due to backwards
compatibility :)</p>

<p>But I would prefer if the grapheme segmentation methods were in the stdlib (they used to be).
This is probably not something that will happen, though I should probably push for the unicode
crates being move into the nursery at least.</p>


</div></div>
  </body>
</html>
