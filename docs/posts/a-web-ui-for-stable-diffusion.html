<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Original</a>
    <h1>A Web UI for Stable Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">A browser interface based on Gradio library for Stable Diffusion.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/screenshot.png"><img src="https://github.com/AUTOMATIC1111/stable-diffusion-webui/raw/master/screenshot.png" alt=""/></a></p>
<h2 dir="auto"><a id="user-content-feature-showcase" aria-hidden="true" href="#feature-showcase"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Feature showcase</h2>
<p dir="auto"><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase">Detailed feature showcase with images, art by Greg Rutkowski</a></p>
<ul dir="auto">
<li>Original txt2img and img2img modes</li>
<li>One click install and run script (but you still must install python and git)</li>
<li>Outpainting</li>
<li>Inpainting</li>
<li>Prompt matrix</li>
<li>Stable Diffusion upscale</li>
<li>Attention</li>
<li>Loopback</li>
<li>X/Y plot</li>
<li>Textual Inversion</li>
<li>Extras tab with:
<ul dir="auto">
<li>GFPGAN, neural network that fixes faces</li>
<li>RealESRGAN, neural network upscaler</li>
<li>ESRGAN, neural network with a lot of third party models</li>
</ul>
</li>
<li>Resizing aspect ratio options</li>
<li>Sampling method selection</li>
<li>Interrupt processing at any time</li>
<li>4GB videocard support</li>
<li>Correct seeds for batches</li>
<li>Prompt length validation</li>
<li>Generation parameters added as text to PNG</li>
<li>Tab to view an existing picture&#39;s generation parameters</li>
<li>Settings page</li>
<li>Running custom code from UI</li>
<li>Mouseover hints fo most UI elements</li>
<li>Possible to change defaults/mix/max/step values for UI elements via text config</li>
<li>Random artist button</li>
<li>Tiling support: UI checkbox to create images that can be tiled like textures</li>
<li>Progress bar and live image generation preview</li>
</ul>
<h2 dir="auto"><a id="user-content-installing-and-running" aria-hidden="true" href="#installing-and-running"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installing and running</h2>
<p dir="auto">You need <a href="https://www.python.org/downloads/windows/" rel="nofollow">python</a> and <a href="https://git-scm.com/download/win" rel="nofollow">git</a>
installed to run this, and an NVidia videocard.</p>
<p dir="auto">You need <code>model.ckpt</code>, Stable Diffusion model checkpoint, a big file containing the neural network weights. You
can obtain it from the following places:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original" rel="nofollow">official download</a></li>
<li><a href="https://drive.yerf.org/wl/?id=EBfTrmcCCUAGaQBXVIj5lJmEhjoP1tgl" rel="nofollow">file storage</a></li>
<li>magnet:?xt=urn:btih:3a4a612d75ed088ea542acac52f9f45987488d1c&amp;dn=sd-v1-4.ckpt&amp;tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&amp;tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337</li>
</ul>
<p dir="auto">You optionally can use GFPGAN to improve faces, then you&#39;ll need to download the model from <a href="https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth">here</a>.</p>
<p dir="auto">To use ESRGAN models, put them into ESRGAN directory in the same location as webui.py. A file will be loaded
as model if it has .pth extension. Grab models from the <a href="https://upscale.wiki/wiki/Model_Database" rel="nofollow">Model Database</a>.</p>
<h3 dir="auto"><a id="user-content-automatic-installationlaunch" aria-hidden="true" href="#automatic-installationlaunch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Automatic installation/launch</h3>
<ul dir="auto">
<li>install <a href="https://www.python.org/downloads/windows/" rel="nofollow">Python 3.10.6</a> and check &#34;Add Python to PATH&#34; during installation. You must install this exact version.</li>
<li>install <a href="https://git-scm.com/download/win" rel="nofollow">git</a></li>
<li>place <code>model.ckpt</code> into webui directory, next to <code>webui.bat</code>.</li>
<li><em><em>(optional)</em></em> place <code>GFPGANv1.3.pth</code> into webui directory, next to <code>webui.bat</code>.</li>
<li>run <code>webui-user.bat</code> from Windows Explorer. Run it as normal user, <em><strong>not</strong></em> as administrator.</li>
</ul>
<h4 dir="auto"><a id="user-content-troubleshooting" aria-hidden="true" href="#troubleshooting"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Troubleshooting</h4>
<ul dir="auto">
<li>if your version of Python is not in PATH (or if another version is), edit <code>webui-user.bat</code>, and modify the
line <code>set PYTHON=python</code> to say the full path to your python executable, for example: <code>set PYTHON=B:\soft\Python310\python.exe</code>.
You can do this for python, but not for git.</li>
<li>if you get out of memory errors and your video-card has a low amount of VRAM (4GB), use custom parameter <code>set COMMANDLINE_ARGS</code> (see section below)
to enable appropriate optimization according to low VRAM guide below (for example, <code>set COMMANDLINE_ARGS=--medvram --opt-split-attention</code>).</li>
<li>to prevent the creation of virtual environment and use your system python, use custom parameter replacing <code>set VENV_DIR=-</code> (see below).</li>
<li>webui.bat installs requirements from files <code>requirements_versions.txt</code>, which lists versions for modules specifically compatible with
Python 3.10.6. If you choose to install for a different version of python,  using custom parameter <code>set REQS_FILE=requirements.txt</code>
may help (but I still recommend you to just use the recommended version of python).</li>
<li>if you feel you broke something and want to reinstall from scratch, delete directories: <code>venv</code>, <code>repositories</code>.</li>
<li>if you get a green or black screen instead of generated pictures, you have a card that doesn&#39;t support half precision
floating point numbers (Known issue with 16xx cards). You must use <code>--precision full --no-half</code> in addition to command line
arguments (set them using <code>set COMMANDLINE_ARGS</code>, see below), and the model will take much more space in VRAM (you will likely
have to also use at least <code>--medvram</code>).</li>
<li>installer creates python virtual environment, so none of installed modules will affect your system installation of python if
you had one prior to installing this.</li>
<li>About <em>&#34;You must install this exact version&#34;</em> from the instructions above: you can use any version of python you like,
and it will likely work, but if you want to seek help about things not working, I will not offer help unless you this
exact version for my sanity.</li>
</ul>
<h4 dir="auto"><a id="user-content-how-to-run-with-custom-parameters" aria-hidden="true" href="#how-to-run-with-custom-parameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to run with custom parameters</h4>
<p dir="auto">It&#39;s possible to edit <code>set COMMANDLINE_ARGS=</code> line in <code>webui.bat</code> to run the program with different command line arguments, but that may lead
to inconveniences when the file is updated in the repository.</p>
<p dir="auto">The recommndended way is to use another .bat file named anything you like, set the parameters you want in it, and run webui.bat from it.
A <code>webui-user.bat</code> file included into the repository does exactly this.</p>
<p dir="auto">Here is an example that runs the prgoram with <code>--opt-split-attention</code> argument:</p>
<div data-snippet-clipboard-copy-content="@echo off

set COMMANDLINE_ARGS=--opt-split-attention

call webui.bat"><pre lang="commandline"><code>@echo off

set COMMANDLINE_ARGS=--opt-split-attention

call webui.bat
</code></pre></div>
<p dir="auto">Another example, this file will run the program with custom python path, a different model named <code>a.ckpt</code> and without virtual environment:</p>
<div data-snippet-clipboard-copy-content="@echo off

set PYTHON=b:/soft/Python310/Python.exe
set VENV_DIR=-
set COMMANDLINE_ARGS=--ckpt a.ckpt

call webui.bat"><pre lang="commandline"><code>@echo off

set PYTHON=b:/soft/Python310/Python.exe
set VENV_DIR=-
set COMMANDLINE_ARGS=--ckpt a.ckpt

call webui.bat
</code></pre></div>
<h3 dir="auto"><a id="user-content-what-options-to-use-for-low-vram-video-cards" aria-hidden="true" href="#what-options-to-use-for-low-vram-video-cards"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What options to use for low VRAM video-cards?</h3>
<p dir="auto">You can, through command line arguments, enable the various optimizations which sacrifice some/a lot of speed in favor of
using less VRAM. Those arguments are added to the <code>COMMANDLINE_ARGS</code> parameter, see section above.</p>
<p dir="auto">Here&#39;s a list of optimization arguments:</p>
<ul dir="auto">
<li>If you have 4GB VRAM and want to make 512x512 (or maybe up to 640x640) images, use <code>--medvram</code>.</li>
<li>If you have 4GB VRAM and want to make 512x512 images, but you get an out of memory error with <code>--medvram</code>, use <code>--medvram --opt-split-attention</code> instead.</li>
<li>If you have 4GB VRAM and want to make 512x512 images, and you still get an out of memory error, use <code>--lowvram --always-batch-cond-uncond --opt-split-attention</code> instead.</li>
<li>If you have 4GB VRAM and want to make images larger than you can with <code>--medvram</code>, use  <code>--lowvram --opt-split-attention</code>.</li>
<li>If you have more VRAM and want to make larger images than you can usually make (for example 1024x1024 instead of 512x512), use <code>--medvram --opt-split-attention</code>. You can use <code>--lowvram</code>
also but the effect will likely be barely noticeable.</li>
<li>Otherwise, do not use any of those.</li>
</ul>
<h3 dir="auto"><a id="user-content-running-online" aria-hidden="true" href="#running-online"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Running online</h3>
<p dir="auto">Use <code>--share</code> option to run online. You will get a xxx.app.gradio link. This is the intended way to use the
program in collabs.</p>
<p dir="auto">Use <code>--listen</code> to make the server listen to network connections. This will allow computers on local newtork
to access the UI, and if you configure port forwarding, also computers on the internet.</p>
<p dir="auto">Use <code>--port xxxx</code> to make the server listen on a specific port, xxxx being the wanted port. Remember that
all ports below 1024 needs root/admin rights, for this reason it is advised to use a port above 1024.
Defaults to port 7860 if available.</p>
<h3 dir="auto"><a id="user-content-google-collab" aria-hidden="true" href="#google-collab"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Google collab</h3>
<p dir="auto">If you don&#39;t want or can&#39;t run locally, here is google collab that allows you to run the webui:</p>
<p dir="auto"><a href="https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh" rel="nofollow">https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh</a></p>
<h3 dir="auto"><a id="user-content-textual-inversion" aria-hidden="true" href="#textual-inversion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Textual Inversion</h3>
<p dir="auto">To make use of pretrained embeddings, create <code>embeddings</code> directory (in the same palce as <code>webui.py</code>)
and put your embeddings into it. They must be .pt files, each with only one trained embedding,
and the filename (without .pt) will be the term you&#39;d use in prompt to get that embedding.</p>
<p dir="auto">As an example, I trained one for about 5000 steps: <a href="https://files.catbox.moe/e2ui6r.pt" rel="nofollow">https://files.catbox.moe/e2ui6r.pt</a>; it does not produce
very good results, but it does work. Download and rename it to Usada Pekora.pt, and put it into embeddings dir
and use Usada Pekora in prompt.</p>
<h3 dir="auto"><a id="user-content-how-to-change-ui-defaults" aria-hidden="true" href="#how-to-change-ui-defaults"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to change UI defaults?</h3>
<p dir="auto">After running once, a <code>ui-config.json</code> file appears in webui directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;txt2img/Sampling Steps/value&#34;: 20,
    &#34;txt2img/Sampling Steps/minimum&#34;: 1,
    &#34;txt2img/Sampling Steps/maximum&#34;: 150,
    &#34;txt2img/Sampling Steps/step&#34;: 1,
    &#34;txt2img/Batch count/value&#34;: 1,
    &#34;txt2img/Batch count/minimum&#34;: 1,
    &#34;txt2img/Batch count/maximum&#34;: 32,
    &#34;txt2img/Batch count/step&#34;: 1,
    &#34;txt2img/Batch size/value&#34;: 1,
    &#34;txt2img/Batch size/minimum&#34;: 1,"><pre>{
    <span>&#34;txt2img/Sampling Steps/value&#34;</span>: <span>20</span>,
    <span>&#34;txt2img/Sampling Steps/minimum&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Sampling Steps/maximum&#34;</span>: <span>150</span>,
    <span>&#34;txt2img/Sampling Steps/step&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Batch count/value&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Batch count/minimum&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Batch count/maximum&#34;</span>: <span>32</span>,
    <span>&#34;txt2img/Batch count/step&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Batch size/value&#34;</span>: <span>1</span>,
    <span>&#34;txt2img/Batch size/minimum&#34;</span>: <span>1</span>,</pre></div>
<p dir="auto">Edit values to your liking and the next time you launch the program they will be applied.</p>
<h3 dir="auto"><a id="user-content-manual-installation" aria-hidden="true" href="#manual-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Manual installation</h3>
<p dir="auto">Alternatively, if you don&#39;t want to run webui.bat, here are instructions for installing
everything by hand. This can run on both Windows and Linux (if you&#39;re on linux, use <code>ls</code>
instead of <code>dir</code>).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install torch with CUDA support. See https://pytorch.org/get-started/locally/ for more instructions if this fails.
pip install torch --extra-index-url https://download.pytorch.org/whl/cu113

# check if torch supports GPU; this must output &#34;True&#34;. You need CUDA 11. installed for this. You might be able to use
# a different version, but this is what I tested.
python -c &#34;import torch; print(torch.cuda.is_available())&#34;

# clone web ui and go into its directory
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui

# clone repositories for Stable Diffusion and (optionally) CodeFormer
mkdir repositories
git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion
git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers
git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer

# install requirements of Stable Diffusion
pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary

# install k-diffusion
pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary

# (optional) install GFPGAN (face resoration)
pip install git+https://github.com/TencentARC/GFPGAN.git --prefer-binary

# (optional) install requirements for CodeFormer (face resoration)
pip install -r repositories/CodeFormer/requirements.txt --prefer-binary

# install requirements of web ui
pip install -r requirements.txt  --prefer-binary

# update numpy to latest version
pip install -U numpy  --prefer-binary

# (outside of command line) put stable diffusion model into web ui directory
# the command below must output something like: 1 File(s) 4,265,380,512 bytes
dir model.ckpt

# (outside of command line) put the GFPGAN model into web ui directory
# the command below must output something like: 1 File(s) 348,632,874 bytes
dir GFPGANv1.3.pth"><pre><span><span>#</span> install torch with CUDA support. See https://pytorch.org/get-started/locally/ for more instructions if this fails.</span>
pip install torch --extra-index-url https://download.pytorch.org/whl/cu113

<span><span>#</span> check if torch supports GPU; this must output &#34;True&#34;. You need CUDA 11. installed for this. You might be able to use</span>
<span><span>#</span> a different version, but this is what I tested.</span>
python -c <span><span>&#34;</span>import torch; print(torch.cuda.is_available())<span>&#34;</span></span>

<span><span>#</span> clone web ui and go into its directory</span>
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
<span>cd</span> stable-diffusion-webui

<span><span>#</span> clone repositories for Stable Diffusion and (optionally) CodeFormer</span>
mkdir repositories
git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion
git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers
git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer

<span><span>#</span> install requirements of Stable Diffusion</span>
pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary

<span><span>#</span> install k-diffusion</span>
pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary

<span><span>#</span> (optional) install GFPGAN (face resoration)</span>
pip install git+https://github.com/TencentARC/GFPGAN.git --prefer-binary

<span><span>#</span> (optional) install requirements for CodeFormer (face resoration)</span>
pip install -r repositories/CodeFormer/requirements.txt --prefer-binary

<span><span>#</span> install requirements of web ui</span>
pip install -r requirements.txt  --prefer-binary

<span><span>#</span> update numpy to latest version</span>
pip install -U numpy  --prefer-binary

<span><span>#</span> (outside of command line) put stable diffusion model into web ui directory</span>
<span><span>#</span> the command below must output something like: 1 File(s) 4,265,380,512 bytes</span>
dir model.ckpt

<span><span>#</span> (outside of command line) put the GFPGAN model into web ui directory</span>
<span><span>#</span> the command below must output something like: 1 File(s) 348,632,874 bytes</span>
dir GFPGANv1.3.pth</pre></div>
<blockquote>
<p dir="auto">Note: the directory structure for manual instruction has been changed on 2022-09-09 to match automatic installation: previosuly
webui was in a subdirectory of stable diffusion, now it&#39;s the reverse. If you followed manual installation before the
chage, you can still use the program with you existing directory sctructure.</p>
</blockquote>
<p dir="auto">After that the installation is finished.</p>
<p dir="auto">Run the command to start web ui:</p>

<p dir="auto">If you have a 4GB video card, run the command with either <code>--lowvram</code> or <code>--medvram</code> argument:</p>
<div data-snippet-clipboard-copy-content="python webui.py --medvram"><pre><code>python webui.py --medvram
</code></pre></div>
<p dir="auto">After a while, you will get a message like this:</p>
<div data-snippet-clipboard-copy-content="Running on local URL:  http://127.0.0.1:7860/"><pre><code>Running on local URL:  http://127.0.0.1:7860/
</code></pre></div>
<p dir="auto">Open the URL in browser, and you are good to go.</p>
<h3 dir="auto"><a id="user-content-windows-11-wsl2-instructions" aria-hidden="true" href="#windows-11-wsl2-instructions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Windows 11 WSL2 instructions</h3>
<p dir="auto">Alternatively, here are instructions for installing under Windows 11 WSL2 Linux distro, everything by hand:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install conda (if not already done)
wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
chmod +x Anaconda3-2022.05-Linux-x86_64.sh 
./Anaconda3-2022.05-Linux-x86_64.sh

# Clone webui repo
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui

# Create and activate conda env
conda env create -f environment-wsl2.yaml
conda activate automatic

# (optional) install requirements for GFPGAN (upscaling)
wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth"><pre><span><span>#</span> install conda (if not already done)</span>
wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
chmod +x Anaconda3-2022.05-Linux-x86_64.sh 
./Anaconda3-2022.05-Linux-x86_64.sh

<span><span>#</span> Clone webui repo</span>
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
<span>cd</span> stable-diffusion-webui

<span><span>#</span> Create and activate conda env</span>
conda env create -f environment-wsl2.yaml
conda activate automatic

<span><span>#</span> (optional) install requirements for GFPGAN (upscaling)</span>
wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth</pre></div>
<p dir="auto">After that follow the instructions in the <code>Manual instructions</code> section starting at step <code>:: clone repositories for Stable Diffusion and (optionally) CodeFormer</code>.</p>
<h2 dir="auto"><a id="user-content-credits" aria-hidden="true" href="#credits"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Credits</h2>
<ul dir="auto">
<li>Stable Diffusion - <a href="https://github.com/CompVis/stable-diffusion">https://github.com/CompVis/stable-diffusion</a>, <a href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></li>
<li>k-diffusion - <a href="https://github.com/crowsonkb/k-diffusion.git">https://github.com/crowsonkb/k-diffusion.git</a></li>
<li>GFPGAN - <a href="https://github.com/TencentARC/GFPGAN.git">https://github.com/TencentARC/GFPGAN.git</a></li>
<li>ESRGAN - <a href="https://github.com/xinntao/ESRGAN">https://github.com/xinntao/ESRGAN</a></li>
<li>Ideas for optimizations and some code (from users) - <a href="https://github.com/basujindal/stable-diffusion">https://github.com/basujindal/stable-diffusion</a></li>
<li>Idea for SD upscale - <a href="https://github.com/jquesnelle/txt2imghd">https://github.com/jquesnelle/txt2imghd</a></li>
<li>Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.</li>
<li>(You)</li>
</ul>
</article>
          </div></div>
  </body>
</html>
