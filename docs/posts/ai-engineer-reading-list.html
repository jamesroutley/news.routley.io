<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/2025-papers">Original</a>
    <h1>AI Engineer Reading List</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em><span>Discussions on </span><a href="https://x.com/latentspacepod/status/1872719928618565646" rel="">X</a><span>, </span><a href="https://www.linkedin.com/posts/drjimfan_lets-start-the-new-year-with-a-buffet-of-activity-7280638043932270592-Ge2n/?utm_source=share&amp;utm_medium=member_desktop" rel="">LinkedIn</a><span>, </span><a href="https://www.youtube.com/watch?v=hnIMY9pLPdg" rel="">YouTube</a><span>. Also: Meet AI Engineers in person! Applications closing soon for attending and sponsoring </span><strong><a href="https://www.latent.space/p/2025-summit" rel="">AI Engineer Summit NYC</a><span>, Feb 20-21.</span></strong></em></p><p><span>The picks from all the speakers in our </span><a href="https://www.youtube.com/watch?v=wT636THdZZo&amp;list=PLWEAb1SXhjlfG63F03R52DZXpHzVB1_5j" rel="">Best of 2024 series</a><span> catches you up for 2024, but since we wrote about running</span><a href="https://www.latent.space/p/paperclub" rel=""> Paper Clubs</a><span>, we’ve been asked many times for a reading list to recommend for those starting from scratch at work or with friends. We started with </span><a href="https://a16z.com/ai-canon/" rel="">the 2023 a16z Canon</a><span>, but it needs a 2025 update and a practical focus. </span></p><p>Here we curate “required reads” for the AI engineer. Our design goals are: </p><ul><li><p><strong>pick ~50 papers</strong><span> (~one</span><span> a week for a year), optional extras. Arbitrary constraint.</span></p></li><li><p><span>tell you </span><em><strong>why</strong></em><strong> this paper matters</strong><span> instead of just name drop without helpful context</span></p></li><li><p><span>be </span><em><strong>very</strong></em><strong> practical</strong><span> for the AI Engineer; no time wasted on </span><em>Attention is All You Need</em><span>, bc 1) </span><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g" rel="">everyone</a><span> </span><a href="https://github.com/elicit/machine-learning-list#introduction-to-machine-learning" rel="">else</a><span> already starts there, 2) most won’t really need it at work</span></p></li></ul><p>We ended up picking 5 “papers” per section for:</p><ul><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-frontier-llms" rel="">Section 1: Frontier LLMs</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-benchmarks-and-evals" rel="">Section 2: Benchmarks and Evals</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-prompting-icl-and-chain-of-thought" rel="">Section 3: Prompting, ICL &amp; Chain of Thought</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-retrieval-augmented-generation" rel="">Section 4: Retrieval Augmented Generation</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-agents" rel="">Section 5: Agents</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-code-generation" rel="">Section 6: Code Generation</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-vision" rel="">Section 7: Vision</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-voice" rel="">Section 8: Voice</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-imagevideo-diffusion" rel="">Section 9: Image/Video Diffusion</a></p></li><li><p><a href="https://www.latent.space/p/2025-papers#%C2%A7section-finetuning" rel="">Section 10: Finetuning</a></p></li></ul><ol><li><p><strong><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="">GPT1</a><span>, </span><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="">GPT2</a><span>,  </span><a href="https://arxiv.org/abs/2005.14165" rel="">GPT3</a><span>, </span><a href="https://arxiv.org/abs/2107.03374" rel="">Codex</a><span>, </span><a href="https://arxiv.org/abs/2203.02155" rel="">InstructGPT</a><span>, </span><a href="https://arxiv.org/abs/2303.08774" rel="">GPT4</a><span> papers</span></strong><span>. Self explanatory. </span><strong><a href="https://openai.com/index/chatgpt/" rel="">GPT3.5</a></strong><span>, </span><strong><a href="https://openai.com/index/hello-gpt-4o/" rel="">4o</a></strong><span>, </span><strong><a href="https://openai.com/index/introducing-openai-o1-preview/" rel="">o1</a></strong><span>, and </span><strong><a href="https://openai.com/index/deliberative-alignment/" rel="">o3</a></strong><span> tended to have launch events and system cards</span><span> instead.</span></p></li><li><p><strong><a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" rel="">Claude 3</a><span> and </span><a href="https://arxiv.org/abs/2312.11805" rel="">Gemini 1</a><span> papers</span></strong><span> to understand the competition. Latest iterations are </span><a href="https://www.latent.space/p/claude-sonnet" rel="">Claude 3.5 Sonnet</a><span> and </span><a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash" rel="">Gemini 2.0 Flash</a><span>/</span><a href="https://ai.google.dev/gemini-api/docs/thinking-mode" rel="">Flash Thinking</a><span>. Also </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2302.13971" rel="">LLaMA 1</a><span>, </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2</a><span>, </span><a href="https://arxiv.org/abs/2407.21783" rel="">Llama 3</a></strong><span> </span><strong>papers </strong><span>to understand the leading open models. You can also view </span><a href="https://arxiv.org/abs/2310.06825" rel="">Mistral 7B</a><span>, </span><a href="https://arxiv.org/abs/2401.04088" rel="">Mixtral</a><span> and </span><a href="https://arxiv.org/abs/2410.07073" rel="">Pixtral</a><span> as a branch on the Llama family tree.</span></p></li><li><p><strong><span>DeepSeek </span><a href="https://arxiv.org/abs/2401.02954" rel="">V1</a><span>, </span><a href="https://arxiv.org/abs/2401.14196" rel="">Coder</a><span>, </span><a href="https://arxiv.org/abs/2401.06066" rel="">MoE</a><span>, </span><a href="https://arxiv.org/abs/2405.04434" rel="">V2</a><span>,</span><a href="https://github.com/deepseek-ai/DeepSeek-V3" rel=""> V3</a><span> papers</span></strong><span>. Leading (relatively) open model lab.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2407.21075" rel="">Apple Intelligence</a><span> paper</span></strong><span>. It’s on every Mac and iPhone.</span></p></li></ol><p>You can both use and learn a lot from other LLMs, this is a vast topic. </p><ul><li><p><span>In particular, BERTs are underrated as workhorse classification models - see </span><a href="https://buttondown.com/ainews/archive/ainews-modernbert-small-new-retrieverclassifier/" rel="">ModernBERT</a><span> for the state of the art, and </span><a href="https://www.answer.ai/posts/colbert-pooling.html" rel="">ColBERT for applications</a><span>. </span></p></li><li><p><span>Honorable mentions of LLMs to know: AI2 (</span><a href="https://arxiv.org/abs/2402.00838" rel="">Olmo</a><span>, </span><a href="https://arxiv.org/abs/2409.17146" rel="">Molmo</a><span>, </span><a href="https://arxiv.org/abs/2409.02060" rel="">OlmOE</a><span>, </span><a href="https://allenai.org/blog/tulu-3-technical" rel="">Tülu</a><span> 3, </span><a href="https://x.com/soldni/status/1875266934943649808?s=46" rel="">Olmo 2</a><span>), </span><a href="https://github.com/xai-org/grok-1" rel="">Grok</a><span>, </span><a href="https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/" rel="">Amazon Nova</a><span>, </span><a href="https://www.wired.com/story/chinese-startup-01-ai-is-winning-the-open-source-ai-race/" rel="">Yi</a><span>, </span><a href="https://www.latent.space/p/yitay" rel="">Reka</a><span>, </span><a href="https://buttondown.com/ainews/archive/ainews-jamba-mixture-of-architectures-dethrones/" rel="">Jamba</a><span>, </span><a href="https://cohere.com/command" rel="">Cohere</a><span>, </span><a href="https://buttondown.com/ainews/archive/ainews-to-be-named-2748/" rel="">Nemotron</a><span>, </span><a href="https://arxiv.org/abs/2412.08905" rel="">Microsoft Phi</a><span>, </span><a href="https://www.latent.space/p/2024-open-models" rel="">HuggingFace SmolLM</a><span> - mostly lower in ranking or lack papers. </span></p></li><li><p><span>Research to know: If time allows, we recommend the </span><strong>Scaling Laws</strong><span> literature: </span><a href="http://arxiv.org/abs/2001.08361" rel="">Kaplan</a><span>, </span><a href="https://arxiv.org/abs/2203.15556" rel="">Chinchilla</a><span>, </span><a href="https://arxiv.org/abs/2206.07682" rel="">Emergence</a><span> / </span><a href="https://arxiv.org/abs/2304.15004" rel="">Mirage</a><span>, </span><a href="https://arxiv.org/abs/2401.00448" rel="">Post-Chinchilla laws</a><span>.</span></p></li><li><p><span>In 2025, the frontier (o1, </span><a href="https://en.wikipedia.org/wiki/OpenAI_o3" rel="">o3</a><span>, </span><a href="https://api-docs.deepseek.com/news/news1120" rel="">R1</a><span>, </span><a href="https://qwenlm.github.io/blog/qwq-32b-preview/" rel="">QwQ</a><span>/</span><a href="https://qwenlm.github.io/blog/qvq-72b-preview/" rel="">QVQ</a><span>, </span><a href="https://fireworks.ai/blog/fireworks-compound-ai-system-f1" rel="">f1</a><span>) will be very much dominated by </span><strong>reasoning models</strong><span>, which have essentially no direct papers, but the basic knowledge is </span><a href="https://arxiv.org/abs/2305.20050" rel="">Let’s Verify Step By Step</a><span>, </span><a href="https://www.youtube.com/watch?v=Y5-FeaFOEFM" rel="">STaR</a><span>, and </span><a href="https://www.youtube.com/live/Gr_eYXdHFis" rel="">Noam Brown’s talks/podcasts</a><span>. Most practical knowledge is </span><a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=false&amp;query=o1&amp;sort=byPopularity&amp;type=story" rel="">accumulated by outsiders</a><span> (</span><a href="https://www.youtube.com/watch?v=skT89EvIjrc&amp;t=68s" rel="">LS talk</a><span>) and tweets.</span></p></li></ul><ol><li><p><strong><a href="https://arxiv.org/abs/2009.03300" rel="">MMLU</a><span> paper</span></strong><span> - the main </span><strong>knowledge</strong><span> benchmark, next to </span><strong><a href="https://arxiv.org/abs/2311.12022" rel="">GPQA</a></strong><span> and </span><a href="https://arxiv.org/abs/2206.04615" rel="">BIG-Bench</a><span>. In 2025 frontier labs use </span><strong><a href="https://arxiv.org/abs/2406.01574" rel="">MMLU Pro</a></strong><span>, </span><strong><a href="https://arxiv.org/abs/2311.12022" rel="">GPQA Diamond</a><span>, and </span><a href="https://arxiv.org/abs/2210.09261" rel="">BIG-Bench Hard</a><span>.</span></strong></p></li><li><p><strong><a href="https://arxiv.org/abs/2310.16049" rel="">MuSR</a><span> paper</span></strong><span> - evaluating </span><strong>long context</strong><span>, next to </span><a href="https://arxiv.org/abs/2412.15204" rel="">LongBench</a><span>, </span><a href="https://arxiv.org/abs/2406.10149" rel="">BABILong</a><span>, and </span><a href="https://www.latent.space/p/gradient" rel="">RULER</a><span>. Solving </span><a href="https://arxiv.org/abs/2307.03172" rel="">Lost in The Middle</a><span> and other issues with </span><a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" rel="">Needle in a Haystack</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2103.03874" rel="">MATH </a><span>paper</span></strong><span> - a compilation of </span><strong>math competition problems</strong><span>. Frontier labs focus on </span><a href="https://arxiv.org/abs/2411.04872" rel="">FrontierMath</a><span> and hard subsets of MATH: MATH level 5, </span><a href="https://www.kaggle.com/datasets/hemishveeraboina/aime-problem-set-1983-2024" rel="">AIME</a><span>, </span><a href="https://github.com/ryanrudes/amc" rel="">AMC10/AMC12</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2311.07911" rel="">IFEval</a><span> paper </span></strong><span>- the leading </span><strong>instruction following </strong><span>eval and only external benchmark </span><a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models" rel="">adopted by Apple</a><span>. You could also view </span><a href="https://arxiv.org/abs/2306.05685" rel="">MT-Bench</a><span> as a form of IF.</span></p></li><li><p><strong><a href="https://arcprize.org/arc" rel="">ARC AGI</a><span> challenge</span></strong><span> - a famous </span><strong>abstract reasoning</strong><span> “IQ test” benchmark that has lasted far longer than many quickly saturated benchmarks.</span></p></li></ol><p><span>We covered many of these in </span><a href="https://www.latent.space/p/benchmarks-101" rel="">Benchmarks 101</a><span> and </span><a href="https://www.latent.space/p/benchmarks-201" rel="">Benchmarks 201</a><span>, while our </span><a href="https://www.latent.space/p/carlini" rel="">Carlini</a><span>, </span><a href="https://www.latent.space/p/lmarena" rel="">LMArena</a><span>, and </span><a href="https://www.latent.space/p/braintrust" rel="">Braintrust</a><span> episodes covered private, arena, and product evals (read </span><a href="https://hamel.dev/blog/posts/llm-judge/" rel="">LLM-as-Judge</a><span> and </span><a href="https://applied-llms.org/#evaluation-monitoring" rel="">the Applied LLMs essay</a><span>). Benchmarks are linked to </span><a href="https://www.latent.space/p/datasets-101" rel="">Datasets</a><span>.</span></p><blockquote><p><em><strong>Note</strong><span>: The GPT3 paper (“Language Models are Few-Shot Learners”) should already have introduced In-Context Learning (ICL) - a close cousin of prompting. We also consider </span><a href="https://www.latent.space/i/93381455/what-is-prompt-injection" rel="">prompt injections</a><span> required knowledge — </span><a href="https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/" rel="">Lilian Weng</a><span>, </span><a href="https://simonwillison.net/series/prompt-injection/" rel="">Simon W</a><span>.</span></em></p></blockquote><ol><li><p><strong><a href="https://arxiv.org/abs/2406.06608" rel="">The Prompt Report</a></strong><span> </span><strong>paper</strong><span> - a survey of prompting papers (</span><a href="https://www.latent.space/p/learn-prompting" rel="">podcast</a><span>).</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2201.11903" rel="">Chain-of-Thought</a><span> paper </span></strong><span>- one of multiple claimants to popularizing </span><strong>Chain of Thought</strong><span>, along with </span><strong><a href="https://arxiv.org/abs/2112.00114" rel="">Scratchpads</a></strong><span> and </span><strong><a href="https://arxiv.org/abs/2205.11916" rel="">Let’s Think Step By Step</a></strong></p></li><li><p><strong><a href="https://arxiv.org/abs/2305.10601" rel="">Tree of Thought</a><span> paper</span></strong><span> -</span><strong> </strong><span>introducing </span><strong>lookaheads</strong><span> and </span><strong>backtracking</strong><span> (</span><a href="https://www.latent.space/p/shunyu" rel="">podcast</a><span>)</span></p></li><li><p><strong><a href="https://aclanthology.org/2021.emnlp-main.243/" rel="">Prompt Tuning</a><span> paper -</span></strong><span> you may not need prompts - if you can do </span><a href="https://arxiv.org/abs/2101.00190" rel="">Prefix-Tuning</a><span>, </span><a href="https://arxiv.org/abs/2402.10200" rel="">adjust decoding</a><span> (say </span><a href="https://github.com/xjdr-alt/entropix" rel="">via entropy</a><span>), or </span><a href="https://vgel.me/posts/representation-engineering/" rel="">representation engineering</a></p></li><li><p><strong><a href="https://arxiv.org/abs/2211.01910" rel="">Automatic Prompt Engineering</a><span> paper</span></strong><span> - it is increasingly obvious that </span><strong>humans are terrible zero-shot prompters</strong><span> and </span><em>prompting itself </em><span>can be enhanced by LLMs. The most notable implementation of this is in the </span><a href="https://arxiv.org/abs/2310.03714" rel="">DSPy paper</a><span>/framework.</span></p></li></ol><p><span>Section 3 is one area where reading disparate papers may not be as useful as having more practical guides - we recommend </span><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" rel="">Lilian Weng</a><span>, </span><a href="https://eugeneyan.com/writing/prompting/" rel="">Eugene Yan</a><span>, and </span><a href="https://github.com/anthropics/prompt-eng-interactive-tutorial" rel="">Anthropic’s Prompt Engineering Tutorial</a><span> and </span><a href="https://www.youtube.com/watch?v=hkhDdcM5V94" rel="">AI Engineer Workshop</a><span>.</span></p><ol><li><p><strong><a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html" rel="">Introduction to Information Retrieval</a><span> </span></strong><span>- a bit unfair to recommend a book, but we are trying to make the point that RAG is an IR problem and </span><a href="https://en.wikipedia.org/wiki/Information_retrieval#History" rel="">IR has a 60 year history</a><span> that includes </span><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="">TF-IDF</a><span>, </span><a href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="">BM25</a><span>, </span><a href="https://github.com/facebookresearch/faiss" rel="">FAISS</a><span>, </span><a href="https://arxiv.org/abs/1603.09320" rel="">HNSW</a><span> and other “boring” techniques.</span></p></li><li><p><strong><span>2020 </span><a href="https://arxiv.org/abs/2005.11401" rel="">Meta RAG</a><span> paper</span></strong><span> - which coined the term. The original authors have started Contextual and have </span><a href="https://contextual.ai/introducing-rag2/" rel="">coined RAG 2.0</a><span>. Modern “table stakes” for RAG — </span><a href="https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/query_transformations/" rel="">HyDE</a><span>, </span><a href="https://research.trychroma.com/evaluating-chunking" rel="">chunking</a><span>, </span><a href="https://cohere.com/blog/rerank-3pt5" rel="">rerankers</a><span>, </span><a href="https://www.youtube.com/watch?v=i2vBaFzCEJw" rel="">multimodal data</a><span> are </span><a href="https://www.youtube.com/watch?v=TRjq7t2Ms5I&amp;t=152s" rel="">better</a><span> </span><a href="https://www.youtube.com/watch?v=FDEmbYPgG-s" rel="">presented</a><span> </span><a href="https://www.youtube.com/watch?v=DId2KP8Ykz4" rel="">elsewhere</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2210.07316" rel="">MTEB</a><span> paper </span></strong><span>- </span><a href="https://news.ycombinator.com/item?id=42504379" rel="">known overfitting</a><span> that </span><a href="https://x.com/Nils_Reimers/status/1870812625505849849" rel="">its author considers it dead</a><span>, but still de-facto benchmark. Many embeddings have papers - pick your poison - </span><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" rel="">SentenceTransformers</a><span>, </span><strong><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo" rel="">OpenAI</a></strong><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo" rel="">, </a><strong><a href="https://www.youtube.com/watch?v=VIqXNRsRRQo" rel="">Nomic Embed, Jina v3, cde-small-v1</a><span>, </span></strong><a href="https://x.com/zach_nussbaum/status/1873813021786767699?s=46&amp;t=tMWvmS3OL3Ssg0b9lKvp4Q" rel="">ModernBERT Embed</a><span> - with </span><a href="https://huggingface.co/blog/matryoshka" rel="">Matryoshka embeddings</a><span> increasingly standard.</span></p></li><li><p><strong><a href="https://arxiv.org/pdf/2404.16130" rel="">GraphRAG</a><span> paper</span></strong><span> - </span><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-graphrag" rel="">Microsoft’s</a><span> take on adding knowledge graphs to RAG, </span><a href="https://buttondown.com/ainews/archive/ainews-graphrag/" rel="">now open sourced</a><span>. One of the </span><a href="https://www.youtube.com/watch?v=knDDGYHnnSI" rel="">most popular trends in RAG</a><span> in 2024, alongside of </span><a href="https://github.com/stanford-futuredata/ColBERT" rel="">ColBERT</a><span>/ColPali/ColQwen (more in the Vision section).</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2309.15217" rel="">RAGAS</a><span> paper - </span></strong><span>the simple RAG eval </span><a href="https://x.com/swyx/status/1724490887147978793" rel="">recommended by OpenAI</a><span>. See also</span><strong><span> </span><a href="https://arxiv.org/abs/2407.07858v1" rel="">Nvidia FACTS framework</a><span> </span></strong><span>and </span><strong><a href="https://lilianweng.github.io/posts/2024-07-07-hallucination/" rel="">Extrinsic Hallucinations in LLMs</a><span> </span></strong><span>- Lilian Weng’s survey of causes/evals for hallucinations (see also </span><a href="https://x.com/_jasonwei/status/1871285864690815053" rel="">Jason Wei on recall vs precision</a><span>).</span></p></li></ol><p><span>RAG is the bread and butter of AI Engineering at work in 2024, so there are a LOT of industry resources and practical experience you will be expected to have. </span><a href="https://docs.llamaindex.ai/en/stable/understanding/rag/" rel="">LlamaIndex</a><span> (</span><a href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/" rel="">course</a><span>) and </span><a href="https://python.langchain.com/docs/tutorials/rag/" rel="">LangChain</a><span> (</span><a href="https://www.youtube.com/watch?v=wd7TZ4w1mSw" rel="">video</a><span>) have perhaps invested the most in educational resources. You should also be familiar with the perennial </span><a href="https://arxiv.org/abs/2407.16833" rel="">RAG vs Long Context</a><span> debate.</span></p><ol><li><p><strong><a href="https://arxiv.org/abs/2310.06770" rel="">SWE-Bench</a><span> paper </span></strong><span>(</span><a href="https://www.latent.space/p/iclr-2024-benchmarks-agents?utm_source=publication-search#%C2%A7section-b-benchmarks" rel="">our podcast</a><span>) - after </span><a href="https://www.latent.space/p/claude-sonnet" rel="">adoption by Anthropic</a><span>, Devin and </span><a href="https://openai.com/index/introducing-swe-bench-verified/" rel="">OpenAI</a><span>, probably the highest profile agent benchmark today (vs </span><a href="https://github.com/web-arena-x/webarena" rel="">WebArena</a><span> or </span><a href="https://x.com/jiayi_pirate/status/1871249410128322856" rel="">SWE-Gym</a><span>). Technically a coding benchmark, but more a test of agents than raw LLMs. See also </span><a href="https://arxiv.org/abs/2405.15793" rel="">SWE-Agent</a><span>, </span><a href="https://arxiv.org/abs/2410.03859" rel="">SWE-Bench Multimodal</a><span> and the </span><a href="https://kprize.ai/" rel="">Konwinski Prize</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2210.03629" rel="">ReAct</a><span> paper </span></strong><span>(</span><a href="https://www.latent.space/p/shunyu" rel="">our podcast</a><span>) - ReAct started a long line of research on </span><strong>tool using and function calling LLMs, </strong><span>including </span><a href="https://gorilla.cs.berkeley.edu/" rel="">Gorilla</a><span> and the </span><a href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="">BFCL Leaderboard</a><span>. Of historical interest - </span><a href="https://arxiv.org/abs/2302.04761" rel="">Toolformer</a><span> and </span><a href="https://arxiv.org/abs/2303.17580" rel="">HuggingGPT</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2310.08560" rel="">MemGPT</a><span> paper</span></strong><span> - one of many notable approaches to emulating long running agent memory, adopted by </span><a href="https://openai.com/index/memory-and-new-controls-for-chatgpt/" rel="">ChatGPT</a><span> and </span><a href="https://langchain-ai.github.io/langgraph/concepts/memory/#episodic-memory" rel="">LangGraph</a><span>. Versions of these are reinvented in every agent system from </span><a href="https://arxiv.org/abs/2308.00352" rel="">MetaGPT</a><span> to </span><a href="https://arxiv.org/abs/2308.08155" rel="">AutoGen</a><span> to </span><a href="https://github.com/joonspk-research/generative_agents" rel="">Smallville</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2305.16291" rel="">Voyager</a><span> paper</span></strong><span> - Nvidia’s take on 3 </span><a href="https://arxiv.org/abs/2309.02427" rel="">cognitive architecture</a><span> components (</span><strong>curriculum, skill library, sandbox</strong><span>) to improve performance. More abstractly, skill library/curriculum can be abstracted as a form of </span><a href="https://arxiv.org/abs/2409.07429" rel="">Agent Workflow Memory</a><span>.</span></p></li><li><p><strong><span>Anthropic on </span><a href="https://www.anthropic.com/research/building-effective-agents" rel="">Building Effective Agents</a></strong><span> - just a great state-of-2024 recap that focuses on the importance of </span><strong>chaining</strong><span>, </span><strong>routing, parallelization, orchestration, evaluation, and optimization.</strong><span> See also </span><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" rel="">Lilian Weng’s Agents</a><span> (ex OpenAI), </span><a href="https://www.latent.space/p/shunyu" rel="">Shunyu Yao on LLM Agents</a><span> (now at OpenAI) and </span><a href="https://huyenchip.com//2025/01/07/agents.html" rel="">Chip Huyen’s Agents</a><span>.</span></p></li></ol><p><span>We covered many of </span><a href="https://www.latent.space/p/2024-agents" rel="">the 2024 SOTA agent designs at NeurIPS</a><span>, and you can find more readings in </span><a href="https://llmagents-learning.org/f24" rel="">the UC Berkeley LLM Agents MOOC</a><span>. Note that we skipped bikeshedding agent definitions, but if you really need one, you could </span><a href="https://www.youtube.com/watch?v=wnsZ7DuqYp0" rel="">use mine</a><span>.</span></p><ol><li><p><strong><a href="https://arxiv.org/abs/2211.15533" rel="">The Stack paper</a><span> </span></strong><span>- the original open dataset twin of The Pile focused on code, starting a great lineage of open codegen work from </span><a href="https://huggingface.co/datasets/bigcode/the-stack-v2" rel="">The Stack v2</a><span> to </span><a href="https://arxiv.org/abs/2402.19173" rel="">StarCoder</a><span>.</span></p></li><li><p><strong>Open Code Model papers</strong><span> - choose from </span><a href="https://arxiv.org/abs/2401.14196" rel="">DeepSeek-Coder</a><span>, </span><a href="https://arxiv.org/abs/2409.12186" rel="">Qwen2.5-Coder</a><span>, or </span><a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/" rel="">CodeLlama</a><span>. Many regard </span><a href="https://www.latent.space/p/claude-sonnet" rel="">3.5 Sonnet as the best code model</a><span> but it has no paper.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2107.03374" rel="">HumanEval/Codex paper</a></strong><span> - This is a saturated benchmark, but is required knowledge for the code domain. SWE-Bench is more famous for coding now, but is expensive/evals agents rather than models. Modern replacements include </span><a href="https://aider.chat/docs/leaderboards/" rel="">Aider</a><span>, </span><a href="https://arxiv.org/abs/2312.02143" rel="">Codeforces</a><span>, </span><a href="https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard" rel="">BigCodeBench</a><span>, </span><a href="https://livecodebench.github.io/" rel="">LiveCodeBench</a><span> and </span><a href="https://buttondown.com/ainews/archive/ainews-to-be-named-5745/" rel="">SciCode</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2401.08500" rel="">AlphaCodeium paper</a><span> </span></strong><span>- Google published </span><a href="https://news.ycombinator.com/item?id=34020025" rel="">AlphaCode</a><span> and </span><a href="https://x.com/RemiLeblond/status/1732419456272318614" rel="">AlphaCode2</a><span> which did very well on programming problems, but here is one way </span><strong>Flow Engineering </strong><span>can add a lot more performance to any given base model.</span></p></li><li><p><strong><a href="https://criticgpt.org/criticgpt-openai/" rel="">CriticGPT</a><span> paper</span></strong><span> - LLMs are </span><a href="https://arxiv.org/abs/2412.15004v1" rel="">known</a><span> to generate code that can have security issues. OpenAI trained CriticGPT to spot them, and Anthropic </span><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#safety-relevant-code" rel="">uses SAEs to identify LLM features</a><span> that cause this, but it is a problem you should be aware of.</span></p></li></ol><p><span>CodeGen is another field where much of the frontier has moved from research to industry and </span><a href="https://www.youtube.com/watch?v=Ve-akpov78Q" rel="">practical engineering advice on codegen</a><span> and </span><a href="https://www.youtube.com/watch?v=T7NWjoD_OuY&amp;t=8s" rel="">code agents like Devin</a><span> are only found in industry blogposts and talks rather than research papers.</span></p><ul><li><p><strong>Non-LLM Vision work</strong><span> is still important: e.g. the </span><strong><a href="https://arxiv.org/abs/1506.02640" rel="">YOLO</a><span> paper </span></strong><span>(now </span><a href="https://github.com/ultralytics/ultralytics" rel="">up to v11</a><span>, but </span><a href="https://news.ycombinator.com/item?id=42352342" rel="">mind the lineage</a><span>), but increasingly transformers like </span><a href="https://arxiv.org/abs/2304.08069" rel="">DETRs Beat YOLOs</a><span> too.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2103.00020" rel="">CLIP</a><span> paper </span></strong><span>- the first successful </span><a href="https://arxiv.org/abs/2010.11929" rel="">ViT</a><span> from Alec Radford. These days, superceded by </span><a href="https://arxiv.org/abs/2201.12086" rel="">BLIP</a><span>/</span><a href="https://arxiv.org/abs/2301.12597" rel="">BLIP2</a><span> or </span><a href="https://www.latent.space/i/152857207/part-vision" rel="">SigLIP/PaliGemma</a><span>, but still required to know.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2401.06209" rel="">MMVP benchmark</a><span> </span></strong><span>(</span><a href="https://www.latent.space/p/2024-vision" rel="">LS Live</a><span>)- quantifies important issues with CLIP. Multimodal versions of MMLU (</span><a href="https://arxiv.org/abs/2311.16502" rel="">MMMU</a><span>) and </span><a href="https://arxiv.org/abs/2410.03859" rel="">SWE-Bench</a><span> do exist.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2304.02643" rel="">Segment Anything Model </a><span>and </span><a href="https://arxiv.org/abs/2408.00714" rel="">SAM 2</a><span> paper</span></strong><span> (</span><a href="https://latent.space/p/sam2" rel="">our pod</a><span>) - the very successful image and video segmentation foundation model. Pair with </span><a href="https://github.com/IDEA-Research/GroundingDINO" rel="">GroundingDINO</a><span>.</span></p></li><li><p><strong>Early fusion research</strong><span>: Contra the cheap “late fusion” work like </span><a href="https://arxiv.org/abs/2304.08485" rel="">LLaVA</a><span> (</span><a href="https://www.latent.space/p/neurips-2023-papers" rel="">our pod</a><span>), early fusion covers Meta’s </span><a href="https://huyenchip.com/2023/10/10/multimodal.html" rel="">Flamingo</a><span>, </span><a href="https://arxiv.org/abs/2405.09818" rel="">Chameleon</a><span>, Apple’s </span><a href="https://arxiv.org/abs/2411.14402" rel="">AIMv2</a><span>, Reka </span><a href="https://arxiv.org/abs/2404.12387" rel="">Core</a><span>, et al. In reality there </span><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/" rel="">are at least 4 streams of visual LM work</a><span>.</span></p></li></ul><p><span>Much frontier VLM work these days is no longer published (the last we really got was </span><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf" rel="">GPT4V system card</a><span> and </span><a href="https://arxiv.org/abs/2309.17421" rel="">derivative papers</a><span>). We recommend having working experience with vision capabilities of 4o (including </span><a href="https://blog.roboflow.com/gpt-4o-object-detection/" rel="">finetuning 4o vision</a><span>), Claude 3.5 Sonnet/Haiku, Gemini 2.0 Flash, and o1. Others: </span><a href="https://mistral.ai/news/pixtral-large/" rel="">Pixtral</a><span>, </span><a href="https://buttondown.com/ainews/archive/ainews-llama-32-on-device-1b3b-and-multimodal/" rel="">Llama 3.2</a><span>, </span><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14" rel="">Moondream</a><span>, </span><a href="https://news.ycombinator.com/item?id=42505038" rel="">QVQ</a><span>.</span></p><ul><li><p><strong><a href="https://arxiv.org/abs/2212.04356" rel="">Whisper</a><span> paper </span></strong><span>- the successful </span><strong>ASR </strong><span>model from Alec Radford. Whisper </span><a href="https://news.ycombinator.com/item?id=33884716" rel="">v2</a><span>, </span><a href="https://news.ycombinator.com/item?id=38166965" rel="">v3</a><span> and </span><a href="https://github.com/huggingface/distil-whisper" rel="">distil-whisper</a><span> and </span><a href="https://amgadhasan.substack.com/p/demystifying-openais-new-whisper" rel="">v3 Turbo</a><span> are open weights but have no paper.</span></p></li><li><p><strong><a href="http://AudioPaLM" rel="">AudioPaLM</a><span> paper </span></strong><span>- our last look at Google’s voice thoughts before PaLM became Gemini. See also: Meta’s </span><a href="https://arxiv.org/abs/2407.21783" rel="">Llama 3 explorations into speech</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2205.04421?utm_source=chatgpt.com" rel="">NaturalSpeech</a><span> paper </span></strong><span>- one of a few leading </span><strong>TTS</strong><span> approaches. Recently </span><a href="https://arxiv.org/abs/2403.03100" rel="">v3</a><span>.</span></p></li><li><p><strong><a href="http://Moshi" rel="">Kyutai Moshi</a><span> paper </span></strong><span>- an impressive full-duplex speech-text open weights model with </span><a href="https://www.youtube.com/watch?v=hm2IJSKcYvo" rel="">high profile demo</a><span>. See also </span><a href="https://www.hume.ai/blog/introducing-octave" rel="">Hume OCTAVE</a><span>.</span></p></li><li><p><strong><a href="https://www.latent.space/p/realtime-api" rel="">OpenAI Realtime API: The Missing Manual</a></strong><span> - Again, frontier omnimodel work is not published, but we did our best to document the Realtime API.</span></p></li></ul><p><span>We do recommend diversifying from the big labs here for now - try Daily, Livekit, Vapi, Assembly, Deepgram, Fireworks, Cartesia, Elevenlabs etc. See </span><a href="https://www.cartesia.ai/blog/state-of-voice-ai-2024" rel="">the State of Voice 2024</a><span>. While NotebookLM’s voice model is not public, </span><a href="https://www.latent.space/p/notebooklm" rel="">we got the deepest description of the modeling process</a><span> that we know of.</span></p><p>With Gemini 2.0 also being natively voice and vision multimodal, the Voice and Vision modalities are on a clear path to merging in 2025 and beyond.</p><ul><li><p><strong><a href="https://arxiv.org/abs/2112.10752" rel="">Latent Diffusion</a><span> paper</span></strong><span> - effectively the Stable Diffusion paper. See also </span><a href="https://stability.ai/news/stable-diffusion-v2-release" rel="">SD2</a><span>, </span><a href="https://arxiv.org/abs/2307.01952" rel="">SDXL</a><span>, </span><a href="https://arxiv.org/abs/2403.03206" rel="">SD3</a><span> papers. These days the team is working on </span><a href="https://github.com/black-forest-labs/flux" rel="">BFL Flux</a><span> [schnell|dev|pro].</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2102.12092" rel="">DALL-E</a><span> / </span><a href="https://arxiv.org/abs/2204.06125" rel="">DALL-E-2</a><span> / </span><a href="https://cdn.openai.com/papers/dall-e-3.pdf" rel="">DALL-E-3</a><span> paper </span></strong><span>- OpenAI’s image generation.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2205.11487" rel="">Imagen</a><span> / </span><a href="https://deepmind.google/technologies/imagen-2/" rel="">Imagen 2</a><span> / </span><a href="https://arxiv.org/abs/2408.07009" rel="">Imagen 3</a><span> paper</span></strong><span> - Google’s image gen. See also </span><a href="https://www.reddit.com/r/singularity/comments/1exsq4d/introducing_ideogram_20_our_most_advanced/" rel="">Ideogram</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2303.01469" rel="">Consistency Models</a><span> paper</span></strong><span> - this distillation work with </span><a href="https://arxiv.org/abs/2310.04378" rel="">LCMs</a><span> spawned the </span><a href="https://www.latent.space/p/tldraw" rel="">quick draw viral moment of Dec 2023</a><span>. These days, updated with </span><a href="https://arxiv.org/abs/2410.11081" rel="">sCMs</a><span>.</span></p></li><li><p><strong><a href="https://openai.com/index/sora/" rel="">Sora</a><span> blogpost</span></strong><span> - text to video - no paper of course beyond </span><a href="https://arxiv.org/abs/2212.09748" rel="">the DiT paper</a><span> (same authors), but still the most significant launch of the year, with many </span><a href="https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard" rel="">open weights competitors</a><span> like </span><a href="https://arxiv.org/abs/2412.00131" rel="">OpenSora</a><span>. </span><a href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/" rel="">Lilian Weng survey here</a><span>.</span></p></li></ul><p><span>We also highly recommend familiarity with ComfyUI (upcoming episode). </span><a href="https://www.youtube.com/watch?v=1mG678f1ZYU&amp;pp=ygUOdGV4dCBkaWZmdXNpb24%3D" rel="">Text Diffusion</a><span>, </span><a href="https://arxiv.org/abs/2302.03917" rel="">Music Diffusion</a><span>, and </span><a href="https://arxiv.org/abs/2406.06525" rel="">autoregressive image generation</a><span> are niche but rising.</span></p><ul><li><p><strong><a href="https://arxiv.org/abs/2106.09685" rel="">LoRA</a><span>/</span><a href="http://arxiv.org/abs/2305.14314" rel="">QLoRA</a><span> paper</span></strong><span> - the de facto way to finetune models cheaply, whether on local models or with 4o (</span><a href="https://www.latent.space/p/cosine" rel="">confirmed on pod</a><span>). </span><a href="https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html" rel="">FSDP+QLoRA</a><span> is educational.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2305.18290" rel="">DPO</a><span> paper </span></strong><span>- the popular, if slightly inferior, alternative to </span><a href="https://arxiv.org/abs/1707.06347" rel="">PPO</a><span>, now supported by OpenAI as </span><a href="https://platform.openai.com/docs/guides/fine-tuning#preference" rel="">Preference Finetuning</a><span>.</span></p></li><li><p><strong><a href="https://arxiv.org/abs/2404.03592" rel="">ReFT</a><span> paper</span></strong><span> - instead of finetuning a few layers, focus on features instead.</span></p></li><li><p><strong><a href="https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/" rel="">Orca 3/AgentInstruct</a><span> paper</span></strong><span> - see the </span><a href="https://www.latent.space/p/2024-syndata-smolmodels" rel="">Synthetic Data picks at NeurIPS</a><span> but this is a great way to get finetue data.</span></p></li><li><p><strong>RL/Reasoning Tuning papers</strong><span> - </span><a href="https://www.interconnects.ai/p/openais-reinforcement-finetuning" rel="">RL Finetuning for o1</a><span> is debated, but </span><a href="https://arxiv.org/abs/2305.20050" rel="">Let’s Verify Step By Step</a><span> and </span><a href="https://x.com/swyx/status/1867990396762243324" rel="">Noam Brown’s many public talks</a><span> give hints for how it works.</span></p></li></ul><p><span>We recommend going thru </span><a href="https://github.com/unslothai/unsloth" rel="">the Unsloth notebooks</a><span> and HuggingFace’s </span><a href="https://www.philschmid.de/fine-tune-llms-in-2025" rel="">How to fine-tune open LLMs</a><span> for more on the full process. This is obviously an endlessly deep rabbit hole that, at the extreme, overlaps with the Research Scientist track.</span></p><p><span>This list will seem intimidating and you will fall off the wagon a few times. Just get back on it. We’ll update with more thru 2025 to keep it current. You can make up your own approach but you can use our </span><a href="https://www.latent.space/i/152108729/how-to-read-papers-in-an-hour" rel="">How To Read Papers In An Hour</a><span> as a guide if that helps. Many folks also chimed in with </span><a href="https://x.com/swyx/status/1875606586569453592" rel="">advice here</a><span>.</span></p><ul><li><p><strong><span>If you’re looking to go thru this with new friends, reader Krispin has started a discord here: </span><a href="https://app.discuna.com/invite/ai_engineer" rel="">https://app.discuna.com/invite/ai_engineer</a><span> and you’re also ofc welcome to join </span><a href="https://discord.gg/xJJMRaWCRt" rel="">the Latent Space discord</a><span>.</span></strong></p></li><li><p><span>Reader Niels has started a notes blog where he is pulling out highlights: </span><a href="https://niels-ole.com/2025/01/05/notes-on-the-2025-ai-engineer-reading-list" rel="">https://niels-ole.com/2025/01/05/notes-on-the-2025-ai-engineer-reading-list</a></p></li></ul><p>Did we miss anything obvious? It’s quite possible. Please comment below and we’ll update with credit to help the community.</p><p>Happy reading!</p><p><em><span>Thanks to </span><a href="https://eugeneyan.com/" rel="">Eugene Yan</a><span> and </span><a href="https://x.com/vibhuuuus" rel="">Vibhu Sapra</a><span> for great suggestions to this list.</span></em></p></div></div></div>
  </body>
</html>
