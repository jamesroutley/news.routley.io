<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://christopherkrapu.com/blog/2026/ocr-textbooks-modal-deepseek/">Original</a>
    <h1>Rolling your own serverless OCR in 40 lines of code</h1>
    
    <div id="readability-page-1" class="page"><article> <div id="markdown-content"> <p>A few months ago, I wanted to make my copy of Gelman’s <em>Bayesian Data Analysis</em> searchable for use in a statistics-focused agent.</p> <p>There are some pretty sophisticated OCR tools out there but they tend to have usage limits or get expensive when you’re processing thousands of pages. DeepSeek recently released an <a href="https://arxiv.org/abs/2510.18234" rel="external nofollow noopener" target="_blank">open OCR model</a> that handles mathematical notation well, and I figured I could run it myself if I had access to a GPU. Sadly, my daily driver is a decade-old Titan Xp which no longer supports the latest PyTorch versions and thus can’t run DeepSeek OCR.</p> <p>I ended up using Modal for this.</p> <h3 id="what-is-modal">What is Modal?</h3> <p>Modal is a serverless compute platform that lets you run Python code on cloud infrastructure without managing servers. The killer feature for machine learning work is that you can define a container image, attach a GPU, and pay only for the seconds your code is actually running.</p> <div><div><pre><code><span>import</span> <span>modal</span>

<span>image</span> <span>=</span> <span>modal</span><span>.</span><span>Image</span><span>.</span><span>from_registry</span><span>(</span>
    <span>&#34;</span><span>nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04</span><span>&#34;</span><span>,</span>
    <span>add_python</span><span>=</span><span>&#34;</span><span>3.11</span><span>&#34;</span><span>,</span>
<span>).</span><span>pip_install</span><span>(</span><span>&#34;</span><span>torch</span><span>&#34;</span><span>,</span> <span>&#34;</span><span>transformers</span><span>&#34;</span><span>,</span> <span>...)</span>

<span>app</span> <span>=</span> <span>modal</span><span>.</span><span>App</span><span>(</span><span>&#34;</span><span>my-gpu-app</span><span>&#34;</span><span>)</span>

<span>@app.function</span><span>(</span><span>image</span><span>=</span><span>image</span><span>,</span> <span>gpu</span><span>=</span><span>&#34;</span><span>A100</span><span>&#34;</span><span>)</span>
<span>def</span> <span>process_something</span><span>():</span>
    <span># This runs on an A100 with all your deps installed
</span>    <span>pass</span>
</code></pre></div></div> <p>The decorator pattern is what makes Modal pleasant to use. You write normal Python, sprinkle decorators on the functions that need special hardware, and Modal handles the rest: building the container, provisioning the GPU, routing your requests. For OCR, this is perfect.</p> <h3 id="the-ocr-script">The OCR script</h3> <p>The core idea is simple: deploy a FastAPI server on Modal that accepts images and returns markdown text. Let’s walk through the important pieces.</p> <h3 id="defining-the-container-image">Defining the container image</h3> <p>First, we build a container with all the dependencies. DeepSeek’s OCR model needs PyTorch, transformers, and a few image processing libraries:</p> <div><div><pre><code><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>
<span>import</span> <span>modal</span>

<span>APP_NAME</span> <span>=</span> <span>&#34;</span><span>deepseek-ocr-books-api-batch</span><span>&#34;</span>
<span>ROOT</span> <span>=</span> <span>Path</span><span>(</span><span>__file__</span><span>).</span><span>resolve</span><span>().</span><span>parents</span><span>[</span><span>1</span><span>]</span>
<span>BOOKS_DIR</span> <span>=</span> <span>ROOT</span> <span>/</span> <span>&#34;</span><span>references</span><span>&#34;</span> <span>/</span> <span>&#34;</span><span>books</span><span>&#34;</span>
<span>PARSED_DIR</span> <span>=</span> <span>BOOKS_DIR</span> <span>/</span> <span>&#34;</span><span>parsed</span><span>&#34;</span>

<span>image</span> <span>=</span> <span>(</span>
    <span>modal</span><span>.</span><span>Image</span><span>.</span><span>from_registry</span><span>(</span>
        <span>&#34;</span><span>nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04</span><span>&#34;</span><span>,</span>
        <span>add_python</span><span>=</span><span>&#34;</span><span>3.11</span><span>&#34;</span><span>,</span>
    <span>)</span>
    <span>.</span><span>apt_install</span><span>(</span><span>&#34;</span><span>git</span><span>&#34;</span><span>,</span> <span>&#34;</span><span>libgl1</span><span>&#34;</span><span>,</span> <span>&#34;</span><span>libglib2.0-0</span><span>&#34;</span><span>)</span>
    <span>.</span><span>pip_install</span><span>(</span>
        <span>&#34;</span><span>torch==2.6.0</span><span>&#34;</span><span>,</span>
        <span>&#34;</span><span>torchvision==0.21.0</span><span>&#34;</span><span>,</span>
        <span>&#34;</span><span>transformers==4.46.3</span><span>&#34;</span><span>,</span>
        <span>&#34;</span><span>PyMuPDF</span><span>&#34;</span><span>,</span>
        <span>&#34;</span><span>Pillow</span><span>&#34;</span><span>,</span>
        <span>&#34;</span><span>numpy</span><span>&#34;</span><span>,</span>
        <span>extra_index_url</span><span>=</span><span>&#34;</span><span>https://download.pytorch.org/whl/cu118</span><span>&#34;</span><span>,</span>
    <span>)</span>
<span>)</span>

<span>app</span> <span>=</span> <span>modal</span><span>.</span><span>App</span><span>(</span><span>APP_NAME</span><span>)</span>
</code></pre></div></div> <p>The paths at the top let us find PDFs relative to the script location, keeping configuration close to where it’s used.</p> <h3 id="the-fastapi-endpoint">The FastAPI endpoint</h3> <p>Here’s where the magic happens. We wrap a FastAPI server in Modal’s <code>@modal.asgi_app()</code> decorator, which means Modal will handle spinning up GPU instances and routing HTTP requests to them:</p> <div><div><pre><code><span>@app.function</span><span>(</span><span>image</span><span>=</span><span>image</span><span>,</span> <span>gpu</span><span>=</span><span>&#34;</span><span>A100</span><span>&#34;</span><span>,</span> <span>timeout</span><span>=</span><span>60</span> <span>*</span> <span>60</span> <span>*</span><span>2</span><span>)</span> <span># timeout of 2 hours
</span><span>@modal.asgi_app</span><span>()</span>
<span>def</span> <span>fastapi_app</span><span>():</span>
    <span>from</span> <span>fastapi</span> <span>import</span> <span>FastAPI</span><span>,</span> <span>File</span><span>,</span> <span>UploadFile</span>
    <span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
    <span>import</span> <span>torch</span>
    <span>from</span> <span>transformers</span> <span>import</span> <span>AutoModel</span><span>,</span> <span>AutoTokenizer</span>

    <span>api</span> <span>=</span> <span>FastAPI</span><span>()</span>
    
    <span>model_name</span> <span>=</span> <span>&#34;</span><span>deepseek-ai/DeepSeek-OCR</span><span>&#34;</span>
    <span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>,</span> <span>trust_remote_code</span><span>=</span><span>True</span><span>)</span>
    <span>model</span> <span>=</span> <span>AutoModel</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>,</span> <span>trust_remote_code</span><span>=</span><span>True</span><span>)</span>
    <span>model</span> <span>=</span> <span>model</span><span>.</span><span>cuda</span><span>().</span><span>to</span><span>(</span><span>torch</span><span>.</span><span>bfloat16</span><span>).</span><span>eval</span><span>()</span>
</code></pre></div></div> <p>The model loads once when the container starts. Subsequent requests reuse the same loaded model, which is crucial for throughput when processing hundreds of pages.</p> <p><i></i> <span>The `trust_remote_code=True` flag is necessary for DeepSeek&#39;s model because it includes custom code in the HuggingFace repository</span> </p> <h3 id="handling-batched-inference">Handling batched inference</h3> <p>OCR is embarrassingly parallel since each page is independent. We can process multiple pages in a single forward pass through the model, which is faster than processing them one at a time:</p> <div><div><pre><code><span>@api.post</span><span>(</span><span>&#34;</span><span>/ocr_batch</span><span>&#34;</span><span>)</span>
<span>async</span> <span>def</span> <span>ocr_batch</span><span>(</span><span>files</span><span>:</span> <span>list</span><span>[</span><span>UploadFile</span><span>]</span> <span>=</span> <span>File</span><span>(...))</span> <span>-&gt;</span> <span>dict</span><span>[</span><span>str</span><span>,</span> <span>list</span><span>[</span><span>str</span><span>]]:</span>
    <span>images</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
        <span>image_bytes</span> <span>=</span> <span>await</span> <span>file</span><span>.</span><span>read</span><span>()</span>
        <span>images</span><span>.</span><span>append</span><span>(</span><span>Image</span><span>.</span><span>open</span><span>(</span><span>io</span><span>.</span><span>BytesIO</span><span>(</span><span>image_bytes</span><span>)).</span><span>convert</span><span>(</span><span>&#34;</span><span>RGB</span><span>&#34;</span><span>))</span>
    <span>batch_items</span> <span>=</span> <span>[</span><span>prepare_inputs</span><span>(</span><span>image</span><span>)</span> <span>for</span> <span>image</span> <span>in</span> <span>images</span><span>]</span>
    <span>texts</span> <span>=</span> <span>run_batch</span><span>(</span><span>batch_items</span><span>)</span>
    <span>return</span> <span>{</span><span>&#34;</span><span>texts</span><span>&#34;</span><span>:</span> <span>texts</span><span>}</span>
</code></pre></div></div> <p>The <code>run_batch</code> function handles the actual model inference. It pads inputs to the same length, runs them through the model in one shot, and decodes the outputs:</p> <div><div><pre><code><span>def</span> <span>run_batch</span><span>(</span><span>batch_items</span><span>):</span>
    <span># Pad sequences to same length
</span>    <span>lengths</span> <span>=</span> <span>[</span><span>item</span><span>[</span><span>0</span><span>].</span><span>size</span><span>(</span><span>0</span><span>)</span> <span>for</span> <span>item</span> <span>in</span> <span>batch_items</span><span>]</span>
    <span>max_len</span> <span>=</span> <span>max</span><span>(</span><span>lengths</span><span>)</span>
    
    <span>input_ids</span> <span>=</span> <span>torch</span><span>.</span><span>full</span><span>((</span><span>len</span><span>(</span><span>batch_items</span><span>),</span> <span>max_len</span><span>),</span> <span>pad_id</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>
    <span># ... (padding logic)
</span>    
    <span>with</span> <span>torch</span><span>.</span><span>autocast</span><span>(</span><span>&#34;</span><span>cuda</span><span>&#34;</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span><span>):</span>
        <span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
            <span>output_ids</span> <span>=</span> <span>model</span><span>.</span><span>generate</span><span>(</span>
                <span>input_ids</span><span>.</span><span>cuda</span><span>(),</span>
                <span>images</span><span>=</span><span>images</span><span>,</span>
                <span>max_new_tokens</span><span>=</span><span>8192</span><span>,</span>
                <span>temperature</span><span>=</span><span>0.0</span><span>,</span>
            <span>)</span>
    
    <span># Decode outputs
</span>    <span>outputs</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>i</span><span>,</span> <span>out_ids</span> <span>in</span> <span>enumerate</span><span>(</span><span>output_ids</span><span>):</span>
        <span>token_ids</span> <span>=</span> <span>out_ids</span><span>[</span><span>lengths</span><span>[</span><span>i</span><span>]:].</span><span>tolist</span><span>()</span>
        <span>text</span> <span>=</span> <span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>token_ids</span><span>,</span> <span>skip_special_tokens</span><span>=</span><span>False</span><span>)</span>
        <span>outputs</span><span>.</span><span>append</span><span>(</span><span>text</span><span>.</span><span>strip</span><span>())</span>
    <span>return</span> <span>outputs</span>
</code></pre></div></div> <p>Setting <code>temperature=0.0</code> makes the output deterministic, which helps the model generate results which are more reproducible.</p> <h3 id="the-local-client">The local client</h3> <p>With the server deployed on Modal, we need a client to feed it pages. The <code>@app.local_entrypoint()</code> decorator marks a function that runs on your local machine but can communicate with the Modal-deployed server:</p> <div><div><pre><code><span>@app.local_entrypoint</span><span>()</span>
<span>def</span> <span>main</span><span>(</span><span>api_url</span><span>:</span> <span>str</span><span>,</span> <span>book</span><span>:</span> <span>str</span> <span>=</span> <span>&#34;&#34;</span><span>,</span> <span>max_pages</span><span>:</span> <span>int</span> <span>=</span> <span>None</span><span>,</span> <span>batch_size</span><span>:</span> <span>int</span> <span>=</span> <span>1</span><span>):</span>
    <span>import</span> <span>fitz</span>  <span># PyMuPDF
</span>    
    <span>if</span> <span>book</span><span>:</span>
        <span>pdf_paths</span> <span>=</span> <span>[</span><span>BOOKS_DIR</span> <span>/</span> <span>book</span><span>]</span>
    <span>else</span><span>:</span>
        <span>pdf_paths</span> <span>=</span> <span>sorted</span><span>(</span><span>BOOKS_DIR</span><span>.</span><span>glob</span><span>(</span><span>&#34;</span><span>*.pdf</span><span>&#34;</span><span>))</span>
    
    <span>for</span> <span>pdf_path</span> <span>in</span> <span>pdf_paths</span><span>:</span>
        <span>with</span> <span>fitz</span><span>.</span><span>open</span><span>(</span><span>pdf_path</span><span>)</span> <span>as</span> <span>doc</span><span>:</span>
            <span>batch_pages</span> <span>=</span> <span>[]</span>
            <span>for</span> <span>page_index</span> <span>in</span> <span>range</span><span>(</span><span>doc</span><span>.</span><span>page_count</span><span>):</span>
                <span>page</span> <span>=</span> <span>doc</span><span>[</span><span>page_index</span><span>]</span>
                <span>pix</span> <span>=</span> <span>page</span><span>.</span><span>get_pixmap</span><span>(</span><span>matrix</span><span>=</span><span>fitz</span><span>.</span><span>Matrix</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>))</span>  <span># 2x zoom
</span>                <span>batch_pages</span><span>.</span><span>append</span><span>(</span><span>pix</span><span>.</span><span>tobytes</span><span>(</span><span>&#34;</span><span>png</span><span>&#34;</span><span>))</span>
                
                <span>if</span> <span>len</span><span>(</span><span>batch_pages</span><span>)</span> <span>&gt;=</span> <span>batch_size</span><span>:</span>
                    <span># Send batch to server
</span>                    <span>response</span> <span>=</span> <span>requests</span><span>.</span><span>post</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>api_url</span><span>}</span><span>/ocr_batch</span><span>&#34;</span><span>,</span> <span>files</span><span>=</span><span>files</span><span>)</span>
                    <span>texts</span> <span>=</span> <span>response</span><span>.</span><span>json</span><span>()[</span><span>&#34;</span><span>texts</span><span>&#34;</span><span>]</span>
                    <span># Save results...
</span></code></pre></div></div> <p>The render-at-2x trick (<code>fitz.Matrix(2, 2)</code>) is important. Higher resolution input means the OCR model can read smaller text and mathematical subscripts more accurately.</p> <h3 id="cleaning-up-the-output">Cleaning up the output</h3> <p>DeepSeek’s OCR model includes grounding tags, which are coordinates indicating where each piece of text appeared on the page. These can be useful for some applications, but I didn’t need them for searchable text:</p> <div><div><pre><code><span>tag_pattern</span> <span>=</span> <span>re</span><span>.</span><span>compile</span><span>(</span>
    <span>r</span><span>&#34;</span><span>&lt;\|ref\|&gt;(.*?)&lt;\|/ref\|&gt;&lt;\|det\|&gt;.*?&lt;\|/det\|&gt;</span><span>&#34;</span><span>,</span>
    <span>flags</span><span>=</span><span>re</span><span>.</span><span>DOTALL</span><span>,</span>
<span>)</span>

<span>for</span> <span>page_idx</span><span>,</span> <span>text</span> <span>in</span> <span>zip</span><span>(</span><span>batch_page_indices</span><span>,</span> <span>texts</span><span>):</span>
    <span>text</span> <span>=</span> <span>tag_pattern</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>&#34;</span><span>\1</span><span>&#34;</span><span>,</span> <span>text</span><span>)</span>  <span># Keep text, drop coordinates
</span>    <span>page_path</span> <span>=</span> <span>pages_dir</span> <span>/</span> <span>f</span><span>&#34;</span><span>page_</span><span>{</span><span>page_idx</span> <span>+</span> <span>1</span><span>:</span><span>04</span><span>d</span><span>}</span><span>.mmd</span><span>&#34;</span>
    <span>page_path</span><span>.</span><span>write_text</span><span>(</span><span>text</span><span>,</span> <span>encoding</span><span>=</span><span>&#34;</span><span>utf-8</span><span>&#34;</span><span>)</span>
</code></pre></div></div> <p>The <code>.mmd</code> extension stands for “multimodal markdown,” a convention for markdown that came from an OCR source and might have some artifacts.</p> <h3 id="running-it">Running it</h3> <p>To use this script, you first deploy the server:</p> <div><div><pre><code>modal deploy deepseek_ocr_modal.py
</code></pre></div></div> <p>This gives you a URL like <code>https://your-workspace--deepseek-ocr-books-api-batch-fastapi-app.modal.run</code>. Then run the client:</p> <div><div><pre><code>modal run deepseek_ocr_modal.py <span>--api-url</span> <span>&#34;https://...&#34;</span> <span>--book</span> <span>&#34;Gelman - Bayesian Data Analysis.pdf&#34;</span>
</code></pre></div></div> <p>For BDA’s ~600 pages, with a batch size of 4, processing takes about 45 minutes on an A100. The output is a directory of markdown files, one per page, plus a concatenated <code>document.mmd</code> with page markers. The whole thing cost maybe $2 bucks. I think it’s a great deal. I also now have a setup I can reuse for any PDF, including course notes, papers, and other textbooks.</p> <p>The OCR quality on mathematical content is surprisingly good. Nearly all equations come through intact.</p> <p>The real payoff comes from downstream use. I can now <code>grep</code> through BDA, paste sections into Claude and ask it to explain the notation, or build a proper search index. All of this from a PDF that was previously just a collection of images.</p> <p>Here’s what the text looks like once it’s parsed and combined into a single file:</p> <div><div><pre><code>
## Exponential model

The exponential distribution is commonly used to model &#39;waiting times&#39; and other continuous, positive, real- valued random variables, often measured on a time scale. The sampling distribution of an outcome \(y\) , given parameter \(\theta\) , is

\[p(y|\theta) = \theta \exp (-y\theta),\mathrm{for} y  0,\]

and \(\theta = 1 / \mathrm{E}(y|\theta)\) is called the &#39;rate.&#39; Mathematically, the exponential is a special case of the gamma distribution with the parameters \((\alpha ,\beta) = (1,\theta)\) . In this case, however, it is being used as a sampling distribution for an outcome \(y\) , not a prior distribution for a parameter \(\theta\) , as in the Poisson example.

The exponential distribution has a &#39;memoryless&#39; property that makes it a natural model for survival or lifetime data; the probability that an object survives an additional length of time \(t\) is independent of the time elapsed to this point: \(\operatorname *{Pr}(y  t + s\mid y  s,\theta) = \operatorname *{Pr}(y  t\mid \theta)\) for any \(s,t\) . The conjugate prior distribution for the exponential parameter \(\theta\) , as for the Poisson mean, is \(\operatorname {Gamma}(\theta |\alpha ,\beta)\) with corresponding posterior distribution \(\operatorname {Gamma}(\theta |\alpha +1,\beta +y)\) . The sampling distribution of \(n\) independent exponential observations, \(y = (y_{1},\ldots ,y_{n})\) , with constant rate \(\theta\) is

\[p(y|\theta) = \theta^{n}\exp (-n\bar{y}\theta),\mathrm{for}\bar{y}\geq 0,\]

which when viewed as the likelihood of \(\theta\) , for fixed \(y\) , is proportional to a \(\operatorname {Gamma}(n + 1,n\bar{y})\) density. Thus the \(\operatorname {Gamma}(\alpha ,\beta)\) prior distribution for \(\theta\) can be viewed as \(\alpha - 1\) exponential observations with total waiting time \(\beta\) (see Exercise 2.19).
&lt;--- Page Split ---&gt;
image
image_caption
&lt;centerFigure 2.6 The counties of the United States with the highest \(10\%\) age-standardized death rates for cancer of kidney/ureter for U.S. white males, 1980-1989. Why are most of the shaded counties in the middle of the country? See Section 2.7 for discussion. &lt;/center

### 2.7 Example: informative prior distribution for cancer rates

At the end of Section 2.4, we considered the effect of the prior distribution on inference given a fixed quantity of data. Here, in contrast, we consider a large set of inferences, each based on different data but with a common prior distribution. In addition to illustrating the role of the prior distribution, this example introduces hierarchical modeling, to which we return in Chapter 5.
</code></pre></div></div> <p>Compare that to the original passage from the textbook:</p> <p> <img src="https://christopherkrapu.com/images/bda-passage.png" alt="Original passage from Bayesian Data Analysis" width="50%"/> </p> <p>I think it did a pretty good job!</p> <p>If you have a collection of scanned textbooks, this approach might be worth the 5 minutes it takes to set it up.</p> </div> </article></div>
  </body>
</html>
