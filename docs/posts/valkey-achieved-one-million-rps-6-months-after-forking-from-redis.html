<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://valkey.io/blog/unlock-one-million-rps-part2/">Original</a>
    <h1>Valkey achieved one million RPS 6 months after forking from Redis</h1>
    
    <div id="readability-page-1" class="page"><div>
        

<p>
  2024-09-13
  
    · 
    
      
      
      <em>Dan Touitou</em>,
    
      
      
      <em>Uri Yagelnik</em>
    
  
</p>

<p>In the <a href="https://valkey.io/blog/unlock-one-million-rps/">first part</a> of this blog, we described how we offloaded almost all I/O operations to I/O threads, thereby freeing more CPU cycles in the main thread to execute commands. When we profiled the execution of the main thread, we found that a considerable amount of time was spent waiting for external memory. This was not entirely surprising, as when accessing random keys, the probability of finding the key in one of the processor caches is relatively low.  Considering that external memory access latency is approximately 50 times higher than L1 cache, it became clear that despite showing 100% CPU utilization, the main process was mostly “waiting”. In this blog, we describe the technique we have been using to increase the number of parallel memory accesses, thereby reducing the impact that external memory latency has on performance.</p>
<h3 id="speculative-execution-and-linked-lists">Speculative execution and linked lists</h3>
<p>Speculative execution is a performance optimization technique used by modern processors, where the processor guesses the outcome of conditional operations and executes in parallel subsequent instructions ahead of time. Dynamic data structures, such as linked lists and search trees, have many advantages over static data structures: they are economical in memory consumption, provide fast insertion and deletion mechanisms, and can be resized efficiently. However, some dynamic data structures have a major drawback: they hinder the processor&#39;s ability to speculate on future memory load instructions that could be executed in parallel. This lack of concurrency is especially problematic in very large dynamic data structures, where most pointer accesses result in high-latency external memory access.</p>
<p>In this blog, Memory Access Amortization, a method that facilitates speculative execution to improve performance, is introduced along with how it is applied in Valkey. The basic idea behind the method is that by interleaving the execution of operations that access random memory locations, one can achieve significantly better performance than by executing them serially.</p>
<p>To depict the problem we are trying to solve consider the following <a href="https://valkey.io/assets/C/list_array.c">function</a> which gets an array of linked list and returns sum of all values in the lists:</p>
<pre data-lang="c"><code data-lang="c"><span>unsigned long </span><span>sequentialSum</span><span>(size_t </span><span>arr_size</span><span>, list **</span><span>la</span><span>) {
</span><span>    list *lp;
</span><span>    </span><span>unsigned long</span><span>  res = </span><span>0</span><span>; 
</span><span>
</span><span>    </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; arr_size; i++) { 
</span><span>        lp = la[i]; 
</span><span>        </span><span>while </span><span>(lp) { 
</span><span>            res += lp-&gt;val;
</span><span>            lp = lp-&gt;next;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>return</span><span> res; 
</span><span>}
</span></code></pre>
<p>Executing this function on an array of 16 lists containing 10 million elements each takes approximately 20.8 seconds on an ARM processor (Graviton 3). Now consider the following alternative implementation which instead of scanning the lists separately,  interleaves the executions of the lists scans:</p>
<pre data-lang="c"><code data-lang="c"><span>unsigned long </span><span>interleavedSum</span><span>(size_t </span><span>arr_size</span><span>, list **</span><span>la</span><span>) {
</span><span>    list **lthreads = </span><span>malloc</span><span>(arr_size * sizeof(list *)); 
</span><span>    </span><span>unsigned long</span><span> res = </span><span>0</span><span>; 
</span><span>    </span><span>int</span><span> n = arr_size; 
</span><span>
</span><span>    </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; arr_size; i++) {
</span><span>        lthreads[i] = la[i]; 
</span><span>        </span><span>if </span><span>(lthreads[i] == </span><span>NULL</span><span>) 
</span><span>            n--; 
</span><span>    } 
</span><span>
</span><span>    </span><span>while</span><span>(n) {
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; arr_size; i++) { 
</span><span>            </span><span>if </span><span>(lthreads[i] == </span><span>NULL</span><span>) 
</span><span>                </span><span>continue</span><span>; 
</span><span>            res += lthreads[i]-&gt;val;
</span><span>            lthreads[i] = lthreads[i]-&gt;next; 
</span><span>            </span><span>if </span><span>(lthreads[i] == </span><span>NULL</span><span>) 
</span><span>                n--;
</span><span>        }  
</span><span>    }
</span><span>
</span><span>    </span><span>free</span><span>(lthreads);
</span><span>    </span><span>return</span><span> res; 
</span><span>}
</span></code></pre>
<p>Running this new version with the same input as previously described takes less than 2 seconds, achieving a 10x speedup! The explanation for this significant improvement lies in the processor&#39;s speculative execution capabilities. In a standard sequential traversal of a linked list, as seen in the first version of the function, the processor cannot &#39;speculate&#39; on future memory access instructions. This limitation becomes particularly costly with large lists, where each pointer access likely results in a expensive external memory access. In contrast, the alternative implementation, which interleaves list traversals, allows the processor to issue more memory accesses in parallel. This leads to an overall reduction in memory access latency through amortization.</p>
<p>One way to maximize the amount of parallel memory access issued is to add prefetch instructions. Replacing</p>
<pre data-lang="c"><code data-lang="c"><span>             </span><span>if </span><span>(lthreads[i] == </span><span>NULL</span><span>) 
</span><span>                n--;
</span></code></pre>
<p>with</p>
<pre data-lang="c"><code data-lang="c"><span>            </span><span>if </span><span>(lthreads[i]) 
</span><span>                </span><span>__builtin_prefetch</span><span>(lthreads[i]);
</span><span>            </span><span>else 
</span><span>                n--;
</span></code></pre>
<p>reduces the execution time further to 1.8 sec.</p>
<h3 id="back-to-valkey">Back to Valkey</h3>
<p>In the first part, we described how we updated the existing I/O threads implementation to increase parallelism and reduce the amount of I/O operations executed by the main thread to a minimum. Indeed, we observed an increase in the number of requests per second, reaching up to 780K SET commands per second. Profiling the execution revealed that Valkey&#39;s main thread was spending more than 40% of its time in a single function: lookupKey, whose goal is to locate the command keys in Valkey&#39;s main dictionary. This dictionary is implemented as a straightforward chained hash, as shown in the picture below:
<img src="https://valkey.io/assets/media/pictures/lookupKey.jpg" alt="dict find"/>
On a large enough set of keys, almost every memory address accessed while searching the dictionary will not be found in any of the processor caches, resulting in costly external memory accesses. Also, similarly as with the linked list from above, since the addresses in the table→dictEntry→...dictEntry→robj sequence are serially dependent, it is not possible to determine the next address to be accessed before the previous address in the chain has been resolved.</p>
<h3 id="batching-and-interleaving">Batching and interleaving</h3>
<p>To overcome this inefficiency, we adopted the following approach. Every time a batch of incoming commands from the I/O threads is ready for execution, Valkey’s main thread efficiently prefetches the memory addresses needed for future lookupKey invocations for the keys involved in the commands  before executing the commands. This prefetch phase is achieved by dictPrefetch, which, similarly as with the linked list example from above, interleaves the table→dictEntry→...dictEntry→robj search sequences for all keys. This reduces the time spent on lookupKey by more than 80%. Another issue we had to address was that all the incoming parsed commands from the I/O threads were not present in the L1/L2 caches of the core running Valkey’s main thread. This was also resolved using the same method.  All the relevant code can be found in <a href="https://github.com/valkey-io/valkey/blob/unstable/src/memory_prefetch.c">memory_prefetch.c</a>. In total the impact of the memory access amortization on Valkey performance is almost 50% and it increased the requests per second to more than 1.19M rps.</p>
<h3 id="how-to-reproduce-valkey-8-0-performance-numbers">How to reproduce Valkey 8.0 performance numbers</h3>
<p>This section will walk you through the process of reproducing our performance results, where we achieved 1.19 million requests per second using Valkey 8.</p>
<h3 id="hardware-setup">Hardware Setup</h3>
<p>We conducted our tests on an AWS EC2 c7g.4xlarge instance, featuring 16 cores on an ARM-based (aarch64) architecture.</p>
<h3 id="system-configuration">System Configuration</h3>
<blockquote>
<p>Note: The core assignments used in this guide are examples. Optimal core selection may vary depending on your specific system configuration and workload.</p>
</blockquote>
<p>Interrupt affinity - locate the network interface with <code>ifconfig</code> (let&#39;s assume it is <code>eth0</code>) and its associated IRQs with</p>
<pre data-lang="bash"><code data-lang="bash"><span>grep</span><span> eth0 /proc/interrupts | </span><span>awk </span><span>&#39;</span><span>{print $1}</span><span>&#39; | </span><span>cut -d</span><span> :</span><span> -f</span><span> 1
</span></code></pre>
<p>In our setup, lines <code>48</code> to <code>55</code> are allocated for <code>eth0</code> interrupts. Allocate one core per 4 IRQ lines:</p>
<pre data-lang="bash"><code data-lang="bash"><span>for</span><span> i </span><span>in </span><span>{48..50}; </span><span>do </span><span>echo</span><span> 1000 &gt; /proc/irq/$</span><span>i</span><span>/smp_affinity; </span><span>done
</span><span>for</span><span> i </span><span>in </span><span>{51..55}; </span><span>do </span><span>echo</span><span> 2000 &gt; /proc/irq/$</span><span>i</span><span>/smp_affinity; </span><span>done  
</span></code></pre>
<p>Server configuration - launch the Valkey server with these minimal configurations:</p>
<pre data-lang="bash"><code data-lang="bash"><span>./valkey-server --io-threads</span><span> 9</span><span> --save --protected-mode</span><span> no
</span></code></pre>
<p><code>--save</code> disables dumping to RDB file and <code>--protected-mode no </code>  allows connections from external hosts. <code>--io-threads</code> number includes the main thread and the IO threads, meaning that in our case 8 I/O threads are launched in addition to the main thread.</p>
<p>Main thread affinity - pin the main thread to a specific CPU core, avoiding the cores handling IRQs. Here we use core #3:</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> taskset</span><span> -cp</span><span> 3 `</span><span>pidof</span><span> valkey-server`
</span></code></pre>
<blockquote>
<p>Important: We suggest experimenting with different core pinning strategies to find the optimal performance while avoiding conflicts with IRQ-handling cores.</p>
</blockquote>
<h3 id="benchmark-configuration">Benchmark Configuration</h3>
<p>Run the benchmark from a separate instance using the following parameters:</p>
<ul>
<li>Value size: 512 bytes</li>
<li>Number of keys: 3 million</li>
<li>Number of clients: 650</li>
<li>Number of threads: 50 (may vary for optimal results)</li>
</ul>
<pre data-lang="bash"><code data-lang="bash"><span>./valkey-benchmark -t</span><span> set</span><span> -d</span><span> 512</span><span> -r</span><span> 3000000</span><span> -c</span><span> 650</span><span> --threads</span><span> 50</span><span> -h </span><span>&#34;</span><span>host-name</span><span>&#34;</span><span> -n</span><span> 100000000000
</span></code></pre>
<blockquote>
<p>Important: When running the benchmark, it may take a few seconds for the database to get populated and for the performance to stabilize. You can adjust the <code>-n</code> parameter to ensure the benchmark runs long enough to reach optimal throughput.</p>
</blockquote>
<h3 id="testing-and-availability">Testing and Availability</h3>
<p><a href="https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc2">Valkey 8.0 RC2</a> is available now for evaluation with I/O threads and memory access amortization.</p>

<div>
  <h2>About the authors</h2>
  
    
      
      
      <div>
  <p><img src="https://valkey.io/assets/media/authors/dantouitou.jpeg" alt="photo of Dan Touitou"/>
    
  </p>
  <div>
    <h2><a href="https://valkey.io/authors/dantouitou/">Dan Touitou</a></h2>
    <p>Dan is a software engineer at AWS</p>
<p>In his spare time, Dan loves solving complex partial differential equations with his family and telling fairy tales about himself.</p>

    
  </div>
</div>
    
      
      
      <div>
  <p><img src="https://valkey.io/assets/media/authors/uriyagelnik.png" alt="photo of Uri Yagelnik"/>
    
  </p>
  <div>
    <h2><a href="https://valkey.io/authors/uriyagelnik/">Uri Yagelnik</a></h2>
    <p>Uri Yagelnik is a Software Engineer at AWS with over a decade of experience, Uri is passionate about distributed systems, databases, and performance optimization. In his spare time, he enjoys hiking with his family and reading.</p>

    
  </div>
</div>
    
  
</div>
<hr/>
<!-- This file was imported from jekyllcodex.org following this guide: https://jekyllcodex.org/without-plugin/share-buttons/. We want to thank jekyllcodex.org for providing this file and guide. -->





    </div></div>
  </body>
</html>
