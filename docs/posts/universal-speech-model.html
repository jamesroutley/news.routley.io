<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sites.research.google/usm/">Original</a>
    <h1>Universal Speech Model</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
          <p>
            <a href="https://arxiv.org/abs/2303.01037">Research paper</a>
             
            <!-- <a href="" class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-button--accent">Blog post</a>
              -->
            <a href="https://docs.google.com/forms/d/e/1FAIpQLSeGcbvPTiJJ13zaqCyR8yemwVOxgva2vKRC9QPWwJc_3PzI_w/viewform?resourcekey=0-VCza0C8UrsQtAJTkzdWDfw">Request API Access</a>
            <br/>
          </p>
          
          <p>Universal Speech Model (USM) is a family of state-of-the-art speech models with 
            2B parameters trained on 12 million hours of speech and 28 billion sentences of text, spanning 
            300+ languages. USM, which is for use in YouTube (e.g., for closed captions), can perform 
            automatic speech recognition (ASR) on widely-spoken languages like English and Mandarin, but 
            also languages like Punjabi, Assamese, Santhali, Balinese, Shona, Malagasy, 
            Luganda, Luo, Bambara, Soga, Maninka, Xhosa, Akan, Lingala, Chichewa, Nkore, Nzema to 
            name a few. Some of these languages are spoken by fewer than twenty million people, 
            making it very hard to find the necessary training data. </p>
          <p> We demonstrate that utilizing a large unlabeled multilingual dataset to pre-train the 
            encoder of our model and fine-tuning on a smaller set of labeled data enables us to 
            recognize these under-represented languages.  Moreover, our model training process is 
            effective for adapting to new languages and data. </p>
      </div>
    </div><div>
      <div>
          
          <p>
						We thank all the co-authors for contributing to the project and paper including 
            Andrew Rosenberg, Ankur Bapna, Bhuvana Ramabhadran, Bo Li, Chung-Cheng Chiu, Daniel Park,
            Françoise Beaufays, Gary Wang, Ginger Perng, James Qin, Jason Riesa, Johan Schalkwyk, 
            Ke Hu, Nanxin Chen, Parisa Haghani, Pedro Moreno Mengibar, Rohit Prabhavalkar, Tara Sainath, 
            Trevor Strohman, Vera Axelrod, Wei Han, Yonghui Wu, Yongqiang Wang, Yu Zhang, 
            Zhehuai Chen, and Zhong Meng.
            </p>
      </div>
    </div></div>
  </body>
</html>
