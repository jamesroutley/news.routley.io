<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2307.12306">Original</a>
    <h1>Tackling the curse of dimensionality with physics-informed neural networks</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2307.12306">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  The curse-of-dimensionality (CoD) taxes computational resources heavily with
exponentially increasing computational cost as the dimension increases. This
poses great challenges in solving high-dimensional PDEs as Richard Bellman
first pointed out over 60 years ago. While there has been some recent success
in solving numerically partial differential equations (PDEs) in high
dimensions, such computations are prohibitively expensive, and true scaling of
general nonlinear PDEs to high dimensions has never been achieved. In this
paper, we develop a new method of scaling up physics-informed neural networks
(PINNs) to solve arbitrary high-dimensional PDEs. The new method, called
Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs
into pieces corresponding to different dimensions and samples randomly a subset
of these dimensional pieces in each iteration of training PINNs. We
theoretically prove the convergence guarantee and other desired properties of
the proposed method. We experimentally demonstrate that the proposed method
allows us to solve many notoriously hard high-dimensional PDEs, including the
Hamilton-Jacobi-Bellman (HJB) and the Schr√∂dinger equations in thousands of
dimensions very fast on a single GPU using the PINNs mesh-free approach. For
instance, we solve nontrivial nonlinear PDEs (one HJB equation and one
Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using
SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD
can be applied to any current and future variants of PINNs to scale them up for
arbitrary high-dimensional PDEs.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Zheyuan Hu [<a href="https://arxiv.org/show-email/9d19e99f/2307.12306">view email</a>]
      </p></div></div>
  </body>
</html>
