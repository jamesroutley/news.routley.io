<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://modal.com/blog/notebooks-internals">Original</a>
    <h1>We built a cloud GPU notebook that boots in seconds</h1>
    
    <div id="readability-page-1" class="page"><div><a href="https://modal.com/blog"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><!--[--><!----><path d="m15 18-6-6 6-6"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----> <span>Back</span></a> <p>Engineering</p> <p><span>September 16, 2025</span><span>‚Ä¢</span><span>15 minute read</span></p></div><div><!----><article><!--[--><p>üëã¬†Hi, I‚Äôm Eric. I work on systems and product at Modal.</p> <p>We recently <a rel="nofollow" href="https://modal.com/blog/notebooks"><!--[--><!---->launched Modal Notebooks<!--]--></a><!---->, a new cloud Jupyter notebook that boots GPUs and arbitrary custom images in seconds, all with real-time collaboration.</p> <p>I want to share some of the engineering that made this experience possible. This post isn‚Äôt about features, but about the systems work behind running interactive, high-performance GPU workloads in the cloud while still feeling instantaneous.</p> <p><img src="https://modal-cdn.com/notebooks-teaser.gif" alt="Animated GIF showing a running notebook"/> <!--[!--><!--]--><!----></p> <p><strong>Notebooks began as a small experiment: could we build a hosted, collaborative notebook without sacrificing local speed?</strong> Most supercomputing workflows today involve forwarding a Jupyter server on a large, expensive devbox that has to stay warm (plus <a rel="nofollow" href="https://en.wikipedia.org/wiki/Slurm_Workload_Manager"><!--[--><!---->Slurm<!--]--></a><!----> for batch runs). We wanted something better‚Äîa modern editor that combines collaboration with Modal‚Äôs primitives like persistent storage, custom images, and instant access to GPUs.</p> <div><div><div><p>‚Äú</p><!----><p>Modal Notebooks have been incredible for sharing ML research across Suno. Engineers and designers can start playing with cutting-edge models within minutes after training runs are completed.</p><!----><p>‚Äù</p></div> <div><div><!--[--><p><span>‚Äî Victor Tao,</span> <span>ML Research Engineer</span></p><!--]--></div> <!--[--><!--]--></div></div></div><!----> <p>In this post I‚Äôll walk through how we did it, from the runtime (sandboxes, image loading, kernel protocol) to the collaboration layer, and finally the editor surface.</p> <h2 id="modal-sandboxes-for-stateful-backends">Modal Sandboxes for stateful backends</h2> <p><em>It all begins with kernels.</em></p> <p>At the heart of every Jupyter notebook is the <a rel="nofollow" href="https://jupyter-client.readthedocs.io/en/latest/messaging.html"><!--[--><!---->kernel protocol<!--]--></a><!---->, the language that you ‚Äúspeak‚Äù to the IPython kernel process to run code. It may look complicated at first, but the idea is simple: send a code cell in, get back the result, plus streams of stdout, stderr, and rich outputs.</p> <p><img src="https://modal-cdn.com/cdnbot/kernels-intro-2vhtem6w_15c6945d.webp" alt="Diagram of kernel where code is executed"/> <!--[!--><!--]--><!----></p> <p>In the standard setup, Jupyter runs on a single backend machine with a one-to-one link to the web editor. Running it reliably in a collaborative, multi-tenant cloud GPU environment required us to rethink that model.</p> <p>To start with the compute layer, kernels run inside <a rel="nofollow" href="https://modal.com/products/sandboxes"><!--[--><!---->Modal Sandboxes<!--]--></a><!---->: our abstraction for secure, isolated processes with their own filesystem, resources, and lifecycle. They‚Äôre a low-level API that can start containers around the world in seconds.</p> <p>Compared to other sandbox systems (Cloudflare, Vercel, e2b), Modal Sandboxes are a little bit different: they‚Äôre built to support high-performance workloads with hundreds of CPUs, top-tier Nvidia GPUs, gigabytes of disk, and a lazy-loading content-addressed FUSE filesystem. This means that Modal Sandboxes can run AI workloads.</p> <p>(Modal Sandboxes aren‚Äôt just for us; they‚Äôre a primitive we want others to use. For example, they‚Äôre how <a rel="nofollow" href="https://modal.com/blog/lovable-case-study"><!--[--><!---->Lovable runs AI developer environments<!--]--></a><!---->. And Marimo built their <a rel="nofollow" href="https://marimo.io/blog/announcing-molab"><!--[--><!---->Molab<!--]--></a><!----> cloud product on top of Sandboxes as well, a very different notebook experience from Modal‚Äôs own, which shows how Sandboxes can power many kinds of interactive computing.)</p> <p>To bridge the gap between the kernel and the outside world, we wrote a daemon called <strong><code>modal-kernelshim</code></strong> that sits inside each Sandbox. It translates Jupyter protocol messages over ZeroMQ into HTTP calls tunneled through our control plane. This lets us handle cell execution, interrupts, and shutdown in a way that looks like a local Jupyter kernel, but much simplified.</p> <p><img src="https://modal-cdn.com/cdnbot/kernelshim-arch-diagram-nq51cskm_35daf30a.webp" alt="Kernelshim architecture diagram"/> <!--[!--><!--]--><!----></p> <p>From the user‚Äôs perspective, you see <em>streaming outputs</em> appear in your browser as soon as your code runs. Behind the scenes, those outputs are sent over TLS from the Sandbox back to the frontend through a server component.</p> <p>That architecture (shim ‚Üí server ‚Üí frontend) is how we expose instantaneous, remote access to kernels, while providing a modern interface that multiple users can connect to.</p> <h2 id="instant-distributed-container-infrastructure">Instant, distributed container infrastructure</h2> <p>For the past 4 years, we‚Äôve been quietly building a lot of distributed systems and core infrastructure to power our container runtime. I wanted to share some of this work here, since it‚Äôs ultimately the low-level systems that make Notebooks tick.</p> <h3 id="lazy-loading-container-images">Lazy-loading container images</h3> <p>One of the biggest sources of latency in starting containers is <em>unpacking images</em>. In Docker or Kubernetes, bringing up an 8 GB Python/ML image means downloading and decompressing layers before you can run anything ‚Äî often close to a minute.</p> <p>That‚Äôs fine for long-lived services, but it kills the feedback loop in an interactive notebook.</p> <p>Our solution was to build a <em>lazy-loading container filesystem</em>. Instead of pulling in every file upfront, we load only a lightweight metadata index and mount it through a Rust FUSE server. The actual file contents are fetched on demand, the moment your process touches them.</p> <p>Those reads flow through a content-addressed <em>tiered cache</em>: memory page cache, local SSD, zonal cache servers, regional CDN, and finally blob storage. Most accesses hit one of the fast tiers; when they don‚Äôt, we stream only the files you actually need, often at speeds that can saturate the hardware.</p> <p>This file system and its associated container runtime are actually the first things I built when I joined Modal as a founding engineer: the very first lines of Rust at the company. It started as an experiment to improve benchmarks, and it‚Äôs now the foundation that makes fast container startup possible, from notebooks to production AI inference.</p> <h3 id="scheduling-and-capacity">Scheduling and capacity</h3> <p>Running notebooks efficiently isn‚Äôt just about fast container startup ‚Äî it‚Äôs about how those containers are placed. At Modal, notebooks share the same pool as our functions, backed by thousands of CPUs and GPUs. The scheduler balances workloads across this pool, so whether you start small with <strong>0.125 CPUs</strong> or scale up to <strong>multiple H100s or B200s</strong>, your container still gets placed instantly if there‚Äôs capacity.</p> <p>We‚Äôve previously written about how we <a rel="nofollow" href="https://modal.com/blog/resource-solver"><!--[--><!---->scale our fleet<!--]--></a><!----> automatically to match demand, spanning clouds and optimizing for price.</p> <p>Notebooks add another wrinkle: kernels are often idle. Leaving a giant GPU instance running idle is a fast way to burn money. Our solution is to pause them automatically; and when you come back, they start up again in seconds. You get the experience of a persistent machine without the overhead of keeping one alive.</p> <h3 id="volumes">Volumes</h3> <p>Modern AI workloads revolve around data. Training is data in, weights out. Inference pipelines move checkpoints, embeddings, and outputs through a chain of steps. None of this works without persistent storage ‚Äî and in a serverless world, where capacity shifts across regions and hardware, persistence is what keeps workloads coherent. To make Notebooks viable, we needed a storage system that was global, mutable, and fast.</p> <p>That‚Äôs what <strong>VolumeFS</strong> provides, the backbone of <a rel="nofollow" href="https://modal.com/docs/guide/volumes"><!--[--><!---->Modal Volumes<!--]--></a><!---->. It‚Äôs a FUSE filesystem designed for global access, built on a distributed network storing petabytes of data.</p> <p><img src="https://modal-cdn.com/nb-assets-sept-8/filesystem-viewer.png" alt="File viewer screenshot"/> <!--[!--><!--]--><!----></p> <p>Honestly, there‚Äôs too many components to describe VolumeFS in this blog post. But suffice it to say that it‚Äôs a core piece of infrastructure at Modal that allows the platform to come together. The quick summary is that file trees live in <a rel="nofollow" href="https://cloud.google.com/spanner"><!--[--><!---->Spanner<!--]--></a><!---->, operations are written to be eventually consistent, and we reuse our distributed content-addressed CDN. We look forward to discussing the internals of Modal Volumes later on.</p> <p>Like any good system, users never have to think about this! The result is that files feel local, but behave globally. You can spin up compute anywhere in the world and still have your data.</p> <h2 id="real-time-collaboration">Real-time collaboration</h2> <p>Notebooks are social by nature. They‚Äôre not just for running code, but for exploring ideas together and leaving behind a <a rel="nofollow" href="https://en.wikipedia.org/wiki/Literate_programming"><!--[--><!---->literate record<!--]--></a><!---->. So we need real-time collaborative editing.</p> <p><img src="https://modal-cdn.com/cdnbot/videoframe_41182i_aawmgv_8d32f575.webp" alt="Real-time collaboration in notebooks"/> <!--[!--><!--]--><!----></p> <p>For this piece, we leaned on <a rel="nofollow" href="https://github.com/ekzhang/rushlight"><!--[--><!---->Rushlight<!--]--></a><!---->, a small library I open-sourced a couple years ago when I was experimenting with operational transformation in <a rel="nofollow" href="https://codemirror.net/"><!--[--><!---->CodeMirror 6<!--]--></a><!---->. Back then, I wanted a simpler, self-hosted collaboration layer. Light and robust, with real-time storage and automatic compaction in your own database.</p> <p>Every edit flows through <a rel="nofollow" href="https://redis.io/docs/latest/develop/data-types/streams/"><!--[--><!---->Redis Streams<!--]--></a><!---->, where it‚Äôs broadcast to other clients. The OT layer ensures changes converge, even with several people typing at once. On the frontend, CodeMirror handles presence and multiple cursors.</p> <p>We separate editing state from execution state. When you run a cell, outputs stream back inline in real time, but anyone reconnecting later can always refetch the current state from the backend. Larger results‚Äîplots, videos, model outputs‚Äîare pushed into <a rel="nofollow" href="https://aws.amazon.com/s3/storage-classes/express-one-zone/"><!--[--><!---->S3 Express One Zone<!--]--></a><!----> so the editing stream stays fast, while outputs are still durable and retrievable.</p> <p>Some teams asked to share notebooks with stakeholders outside their Modal organization, so we added link-based sharing and allowed for embedding <a rel="nofollow" href="https://ipywidgets.readthedocs.io/en/latest/"><!--[--><!---->Jupyter Widgets<!--]--></a><!----> to add interactivity. That way a notebook isn‚Äôt just a scratchpad‚Äîit can double as a lightweight demo surface. We implemented the widget protocol on Modal Notebooks. To our knowledge, this is the first working implementation of widgets in a real-time collaborative editing environment!</p> <h2 id="editor-features-lsp-and-ai-completion">Editor features: LSP and AI completion</h2> <p>In 2025, editors need to be smart. Jupyter has historically lagged on developer ergonomics ‚Äî completions, inline docs, and semantic highlighting never quite worked out of the box. For Modal Notebooks, we wanted those capabilities to be native.</p> <p>We started by implementing core pieces of the <a rel="nofollow" href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/"><!--[--><!---->Language Server Protocol<!--]--></a><!----> (<code>textDocument/completions</code>, <code>textDocument/hover</code>, <code>textDocument/semanticTokens/full</code>) and wiring them up to Pyright. This nets you completions, documentation, and semantic highlighting. We <a rel="nofollow" href="https://github.com/modal-labs/waxtablet"><!--[--><!---->open-sourced<!--]--></a><!----> some of this implementation.</p> <video controls="" muted="" playsinline=""><source src="https://modal-cdn.com/nb-assets-sept-8/lsp-ai-suggestion.mp4" type="video/mp4"/></video> <p>We also integrated auto-formatting with Ruff, running its bleeding-edge <a rel="nofollow" href="https://www.npmjs.com/package/@astral-sh/ruff-wasm-web"><!--[--><!---->WebAssembly build<!--]--></a><!----> directly in our frontend.</p> <p>On the AI side, we experimented with edit prediction. Currently we use <a rel="nofollow" href="https://www.anthropic.com/news/claude-4"><!--[--><!---->Claude 4<!--]--></a><!----> for next-edit suggestions. We‚Äôve also tried pushing it further by hosting inference on Modal itself. That version runs <strong>Zed‚Äôs Zeta model</strong> on H100 GPUs, serving completions directly from our own cloud infra. It still needs some UI tweaks before we enable it as a default though.</p> <p>I think we‚Äôve really made something that actually feels like a first-class development environment, with modern editor features alongside GPU-backed execution.</p> <h2 id="conclusion">Conclusion</h2> <p>We‚Äôve been thinking about foundational infrastructure at Modal for years now, with systems goals at heart: speed, performance, efficiency, and ease of use. Each layer has built on the last. Modal Notebooks is the culmination of a lot of work across file systems, OS, distributed systems, scheduling, security, sandboxes, isolation, and more.</p> <p>Admittedly, although we‚Äôve invested in web interfaces and observability, Modal has never been <em>web-first</em>. Our starting point has always been the SDK, because we bet on programmers. But SDKs aren‚Äôt the right choice for all work, and this product is our first foray into a surface area that extends beyond the client library, our first ‚Äúnew product‚Äù separate from the rest.</p> <p>This work began as a solo prototype and grew into a small team effort. Contributors include:</p> <ul><li><strong>Engineering:</strong> <a rel="nofollow" href="https://x.com/ekzhang1"><!--[--><!---->Eric Zhang<!--]--></a><!---->, <a rel="nofollow" href="https://x.com/HowardHalim"><!--[--><!---->Howard Halim<!--]--></a><!---->, <a rel="nofollow" href="https://x.com/const_amit"><!--[--><!---->Amit Prasad<!--]--></a><!----></li> <li><strong>Product design:</strong> <a rel="nofollow" href="https://x.com/teenychairs"><!--[--><!---->Sona Dolasia<!--]--></a><!----></li> <li><strong>Product feedback:</strong> <a rel="nofollow" href="https://x.com/charles_irl"><!--[--><!---->Charles Frye<!--]--></a><!---->, <a rel="nofollow" href="https://x.com/michaelwaskom"><!--[--><!---->Michael Waskom<!--]--></a><!---->, <a rel="nofollow" href="https://x.com/luiscape"><!--[--><!---->Luis Capelo<!--]--></a><!---->, <a rel="nofollow" href="https://x.com/thomasjpfan"><!--[--><!---->Thomas Fan<!--]--></a><!----></li> <li><strong>Launch:</strong> <a rel="nofollow" href="https://x.com/kenny_ning"><!--[--><!---->Kenny Ning<!--]--></a><!---->, <a rel="nofollow" href="https://www.linkedin.com/in/rebeckastorm/"><!--[--><!---->Rebecka Storm<!--]--></a><!---->, <a rel="nofollow" href="https://www.linkedin.com/in/shababo/"><!--[--><!---->Ben Shababo<!--]--></a><!---->, <a rel="nofollow" href="https://www.linkedin.com/in/margaret-shen-5b93034a/"><!--[--><!---->Margaret Shen<!--]--></a><!----></li></ul> <p>And thanks especially to our design partners at world-class companies like <a rel="nofollow" href="https://suno.com/"><!--[--><!---->Suno<!--]--></a><!---->, who‚Äôve been using Notebooks since the very beginning.</p> <!--[--><a href="https://modal.com/notebooks" target="_blank"><!--[--><!---->Try notebooks<!--]--> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><!--[--><!----><path d="M5 12h14"><!----></path><!----><!----><path d="m12 5 7 7-7 7"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></a><!--]--><!----><!--]--></article><!----></div></div>
  </body>
</html>
