<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dispatch.techoversight.org/top-report-mark-zuckerberg-lied-to-congress-we-cant-trust-his-testimony/">Original</a>
    <h1>Mark Zuckerberg Lied to Congress. We Can&#39;t Trust His Testimony</h1>
    
    <div id="readability-page-1" class="page"><div>

    <article>

        <header>

            

            <div>
                <p><a href="https://dispatch.techoversight.org/author/top/">
                                <img src="https://dispatch.techoversight.org/content/images/size/w160/2025/07/Untitled-design-14-.png" alt="The Tech Oversight Project"/>
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2026-02-17">17 Feb 2026</time>
                            <span><span>—</span> 8 min read</span>
                    </p>
                </div>
            </div>

            
        </header>

        <section>
            <p><strong>WASHINGTON, DC – </strong>Today, The Tech Oversight Project issued the following report on the eve of Meta CEO Mark Zuckerberg’s testimony in the social media addiction trials. The report analyzes Zuckerberg’s <a href="https://www.techpolicy.press/transcript-us-senate-judiciary-committee-hearing-on-big-tech-and-the-online-child-sexual-exploitation-crisis/?ref=dispatch.techoversight.org"><u>testimony</u></a> in front of the Senate Judiciary Committee in 2024 against newly unsealed documents that show Zuckerberg lied and deceived the Committee. The Tech Oversight Project has compiled some of the most damning evidence against Meta on our Big Tech on Trial microsite, which will be updated throughout the proceedings. <a href="https://techoversight.org/bigtechontrial/?ref=dispatch.techoversight.org"><u>View the microsite here</u></a>.</p><p>“It’s important to remember that Meta has hidden behind Section 230 for so long that people like Mark Zuckerberg thought they were bulletproof. Meta’s team of attorneys bet on the fact that these documents would never see the light of day because a product liability case would never make it to trial, and they guessed wrong,” <strong>said Sacha Haworth, Executive Director of The Tech Oversight Project.</strong> “Never-before-seen documents prove that Zuckerberg lied to Congress. We know that they will lie, bury research, and continue recklessly harming young people until Congress forces them to clean up their act. The only way to outlaw Meta’s dangerous and egregious behavior is to pass legislation, like the <em>Kids Online Safety Act</em>, which will hold their feet to the fire and force them to protect children and teens.”</p><h2 id="mark-zuckerberg-lied">MARK ZUCKERBERG LIED</h2>
<!--kg-card-begin: html-->
<table><colgroup><col width="175"/><col width="449"/></colgroup><tbody><tr><td><p dir="ltr"><span>WHAT HE SAID </span></p></td><td><p dir="ltr"><span>WHAT THE EVIDENCE PROVES </span></p></td></tr><tr><td><p dir="ltr"><span>“No one should have to go through the things that your families have suffered and this is why we invest so much and are going to continue doing industry leading efforts to make sure that no one has to go through the types of things that your families have had to suffer,” Zuckerberg said directly to families who lost a child to Big Tech’s products in his now-infamous apology.</span></p></td><td><p dir="ltr"><span>Despite Zuckerberg’s claims during the 2024 US Senate Judiciary Committee hearing, Meta’s post-hearing investment in teen safety measures (i.e. Teen Accounts) are a PR stunt. A report conducted a comprehensive study of teen accounts, testing 47 of Instagram’s 53 listed safety features, finding that:</span></p></td></tr><tr><td><p dir="ltr"><span>“I don’t think that that’s my job is to make good tools.” Zuckerberg said when Senator Josh Hawley asked whether he would establish a fund to compensate victims.</span></p></td><td><p dir="ltr"><span>Expert findings in ongoing litigation directly challenge that claim. An expert report filed by Tim Ested, Founder and CEO of AngelQ AI, concluded that the defendants’ platforms were not designed to be safe for kids, citing broken child-safety features including weak age verification, ineffective parental controls, infinite scroll, autoplay, notifications, and appearance-altering filters, among others.</span></p></td></tr><tr><td><p dir="ltr"><span>“I think it’s important to look at the science. I know people widely talk about [social media harms] as if that is something that’s already been proven and I think that the bulk of the scientific evidence does not support that.” </span></p></td><td><p dir="ltr"><span>The 2021 Facebook Files investigation by WSJ revealed that both external studies and Meta’s own internal research consistently linked Instagram use to worsened teen mental health—especially around body image, anxiety, depression, and social comparison. </span></p></td></tr><tr><td><p dir="ltr"><span>“We don&#39;t allow sexually explicit content on the service for people of any age.”</span></p></td><td><p dir="ltr"><span>Meta knowingly allowed sex trafficking on its platform, and had a 17-strike policy for accounts known to engage in trafficking. “You could incur 16 violations for prostitution and sexual solicitation, and upon the 17th violation, your account would be suspended…by any measure across the industry, [it was] a very, very high strike threshold,” said Instagram’s former Head of Safety and Well-being Vaishnavi Jayakumar. </span></p></td></tr><tr><td><p dir="ltr"><span>“We don&#39;t allow people under the age of 13 on our service. So if we find anyone who&#39;s under the age of 13, we remove them from our service.”</span></p></td><td><p dir="ltr"><span>Internal document stating the goal for Meta to be the most relevant social products for kids worldwide. To do so, Meta will focus on “each youth life stage, ‘Kid’ (6-10), ‘Tween’ (10-13), and ‘Teen’ 13+.’”</span></p></td></tr><tr><td><p dir="ltr"><span>“The research that we’ve seen is that using social apps to connect with other people can have positive mental-health benefits,” CEO Mark Zuckerberg said at a congressional hearing in March 2021 when asked about children and mental health.&#34;</span></p></td><td><p dir="ltr"><span>Internal messages show that it was company policy to delete Meta Bad Experiences &amp; Encounters Framework (BEEF) research, which cataloged experience negative social comparison-promoting content; self-harm-promoting content; bullying content; unwanted advances. (Adam Mosseri’s Testimony on 2/11).</span></p></td></tr><tr><td><p dir="ltr"><span>“We are on the side of parents everywhere working hard to raise their kids”</span></p></td><td><p dir="ltr"><span>“If we tell teens’ parents and teachers about their live videos, that will probably ruin the product from the start (...) My guess is we’ll need to be very good about not notifying parents.” </span></p></td></tr><tr><td><p dir="ltr"><span>“We put special restrictions on teen accounts on Instagram. By default, accounts for under sixteens are set to private, have the most restrictive content settings and can&#39;t be messaged by adults that they don&#39;t follow or people they aren&#39;t connected to.”</span></p></td><td><p dir="ltr"><span>An internal 2022 audit allegedly found that Instagram’s Accounts You May Follow feature recommended 1.4 million potentially inappropriate adults to teenage users in a single day. </span></p></td></tr><tr><td><p dir="ltr"><span>“Mental health is a complex issue and the existing body of scientific work has not shown a causal link between using social media and young people having worse mental health outcomes. </span></p></td><td><p dir="ltr"><span>According to internal documents, Meta designed a “deactivation study,” which found that users who stopped using Facebook and Instagram for a week showed lower rates of anxiety, depression, and loneliness. Meta halted the study and did not publicly disclose the results – citing harmful media coverage as the reason for canning the study.</span></p></td></tr><tr><td><p dir="ltr"><span>“We&#39;re deeply committed to doing industry-leading work in this area. A good example of this work is Messenger Kids, which is widely recognized as better and safer than alternatives.”</span></p></td><td><p dir="ltr"><span>Despite Facebook’s promises, a flaw in Messenger Kids allowed thousands of children to be in group chats with users who hadn’t been approved by their parents. Facebook tried to quietly address the problem by closing violent group chats and notifying individual parents. The problems with Messenger Kids were only made public when they were covered by The Verge.</span></p></td></tr><tr><td><p dir="ltr"><span>“We want everyone who uses our services to have safe and positive experiences (...) I want to recognize the families who are here today who have lost a loved one or lived through some terrible things that no family should have to endure.</span></p></td><td><p dir="ltr"><span>An internal email from 2018 titled “Market Landscape Review: Teen Opportunity Cost and Lifetime Value,” stating that the “US lifetime value of a 13 y/o teen is roughly $270 per teen.” </span></p></td></tr></tbody></table>
<!--kg-card-end: html-->

<!--kg-card-begin: html-->
<table><colgroup><col width="174"/><col width="450"/></colgroup><tbody><tr><td><p dir="ltr"><span>WHAT THEY SAID </span></p></td><td><p dir="ltr"><span>WHAT THE EVIDENCE PROVES </span></p></td></tr><tr><td><p dir="ltr"><span>Instagram head Adam Mosseri told reporters that research he had seen suggests the app’s effects on teen well-being is likely “quite small.”</span></p></td><td><p dir="ltr"><span>An internal 2019 study titled “Teen Mental Health: Creatures of Habit” found the following: </span></p></td></tr><tr><td><p dir="ltr"><span>Meta says its automated systems are highly effective at detecting and removing harmful content. In its 2023 enforcement report, the company claimed its tools removed 87.8% of bullying and harassment content, 99% of child exploitation content, and 95% of hate speech before users reported it.</span></p></td><td><p dir="ltr"><span>In 2023, Meta whistleblower Arturo Béjar revealed those numbers only reflect the content Meta chose to remove—not the full amount of harm on the platform—and that the company relied on internal metrics that downplayed real-world experiences. He and other internal teams estimated the true accuracy of automated detection tools was to be lower than 5% and said leadership including Mark Zuckerberg, Chief Operating Officer Sheryl Sandberg, and Instagram head Adam Mosseri ignored proposed design-focused fixes, shut down the research, and fired most of the team behind it. </span></p></td></tr></tbody></table>
<!--kg-card-end: html-->

        </section>

    </article>


</div></div>
  </body>
</html>
