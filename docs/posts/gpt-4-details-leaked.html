<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://threadreaderapp.com/thread/1678545170508267522.html">Original</a>
    <h1>GPT-4 details leaked?</h1>
    
    <div id="readability-page-1" class="page"><div>
<div>
<div data-controller="mentions">

<div>
<p><a href="https://threadreaderapp.com/user/Yampeleg"><img src="https://pbs.twimg.com/profile_images/1505912788031623170/GC7LMHNp_bigger.jpg" alt="Yam Peleg Profile picture" data-controller="twitter-profile" data-twtrid="634339745" data-action="error-&gt;twitter-profile#error"/></a>
</p>

</div> 
<div id="tweet_1" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545170508267522" dir="auto"><p>
GPT-4&#39;s details are leaked. </p></div>

<div id="tweet_3" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545177282052098" dir="auto"><p>
Mixture Of Experts - Confirmed.</p></div>








<div id="tweet_12" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545200325558272" dir="auto"><p>
If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million.</p></div>
<div id="tweet_13" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545202477146113" dir="auto"><p>
Mixture of Expert Tradeoffs</p></div>
<div id="tweet_14" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545205442621440" dir="auto"><p>
This means parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates.</p></div>
<p>
There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with.
<sup><i></i></sup>
</p>
<p>
With such a large training run, OpenAI instead chose to be more conservative on the number of experts.
<sup><i></i></sup>
</p>

<p>
AN estimate of it&#39;s costs is $0.0049 cents per 1k tokens for 128 A100s to inference GPT-4 8k seqlen and $0.0021 cents per 1k tokens for 128 H100’s to inference GPT-4 8k seqlen. It should be noted, we assume decent high utilization, and keeping batch sizes high.
<sup><i></i></sup>
</p>



<p>
On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text.
<sup><i></i></sup>
</p>
<p>
One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video.
<sup><i></i></sup>
</p>
<div id="tweet_24" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545231300403200" dir="auto"><p>
Some of the data they train on is joint data (rendered LaTeX/text), screen shots of web page, youtube videos: sampling frames, and run Whisper around it to get transcript.</p></div>

<p>
If the small model was right about its predictions – the larger model agrees and we can decode several tokens in a single batch.
<sup><i></i></sup>
</p>
<p>
But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model.
<sup><i></i></sup>
</p>
<p>
The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.
<sup><i></i></sup>
</p>

<div id="tweet_30" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678548248326176771" dir="auto"><p>
The model has 120, so it fits in 15 different nodes.</p></div>
<div id="tweet_31" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678549234612674560" dir="auto"><p>
According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by chinchilla&#39;s optimal.</p></div>


<div id="tweet_34" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553048501633024" dir="auto"><p>
Which by this point we already get rumors that parts of it came from twitter, reddit &amp; youtube.</p></div>

<div id="tweet_36" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553055673872387" dir="auto"><p>
This creates the &#34;illusion&#34; that GPT-4 &#34;is smart&#34; no matter who use it.</p></div>
<div id="tweet_37" data-controller="thread" data-action="click-&gt;thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553507236835328" dir="auto"><p>
There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. </p></div>
<p>• • •</p>
<p><span>
Missing some Tweet in this thread? You can try to
<a id="force-click" href="#" data-category="refresh" data-action="1678545170508267522">force a refresh</a>
</span>
</p>
　
</div>
</div>
</div></div>
  </body>
</html>
