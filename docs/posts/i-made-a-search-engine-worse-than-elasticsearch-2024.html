<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://softwaredoug.com/blog/2024/08/06/i-made-search-worse-elasticsearch">Original</a>
    <h1>I made a search engine worse than Elasticsearch (2024)</h1>
    
    <div id="readability-page-1" class="page"><div>
	  <p>I want you to share in my shame at daring to make a search library. And in this shame, you too, can experience the humility and understanding of what a real, honest-to-goodness, not side-project, search engine does to make lexical search fast.</p>

<p><a href="https://github.com/beir-cellar/beir">BEIR</a> is a set of Information Retrieval benchmarks, oriented around question-answer use cases.</p>

<p>My side project, <a href="http://github.com/softwaredoug/searcharray">SearchArray</a> adds full text search to Pandas. So naturally, to see stand in awe at my amazing developer skills, I wanted to use BEIR to compare SearchArray to Elasticsearch (w/ same query + tokenization). So I spent a Saturday integrating SearchArray into BEIR, and measuring its relevence and performance on MSMarco Passage Retrieval corpus (8M docs).</p>

<p>‚Ä¶ and ü•Å</p>

<table>
  <thead>
    <tr>
      <th>Library</th>
      <th>Elasticsearch</th>
      <th>SearchArray</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NDCG@10</td>
      <td>0.2275</td>
      <td>0.225</td>
    </tr>
    <tr>
      <td>Search Throughput</td>
      <td>90 QPS</td>
      <td>~18 QPS</td>
    </tr>
    <tr>
      <td>Indexing Throughput</td>
      <td>10K Docs Per Sec</td>
      <td>~3.5K Docs Per Sec</td>
    </tr>
  </tbody>
</table>

<p>‚Ä¶ Sad trombone üé∫</p>

<p>It‚Äôs worse in every dimension</p>

<p>At least <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG@10</a> is nearly right, so our BM25 calculation is correct (probably due to negligible differences in tokenization)</p>

<h2 id="imposter-syndrome-anyone">Imposter Syndrome anyone?</h2>

<p>Instead of wallowing in my shame, I DO know exactly what‚Äôs going on‚Ä¶ And it‚Äôs fairly educational. Let‚Äôs chat about why a <strong>real</strong>, non side-project, search engine is fast.</p>

<h2 id="a-magic-wand">A Magic WAND</h2>

<p>(Or how SearchArray is top 8m retrieval while Elasticsearch == top K retrieval)</p>

<p>In lexical search systems, you search for multiple terms. You take the <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> score of each term, and then finally, combine those into a final score for the document. IE, a search for <code>luke skywalker</code> really means: <code>BM25(luke) ??? BM25(skywalker)</code> where ??? is some mathematical operator.</p>

<p>In a simple ‚ÄúOR‚Äù query, you just take the SUM of each term for each doc, IE, a search for <code>luke skywalker</code> is <code>BM25(luke) + BM25(skywalker)</code> like so:</p>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>Doc A (BM25)</th>
      <th>Doc B (BM25)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>luke</td>
      <td>1.90</td>
      <td>1.45</td>
    </tr>
    <tr>
      <td>skywalker</td>
      <td>11.51</td>
      <td>4.3</td>
    </tr>
    <tr>
      <td>Combined doc score  (SUM)</td>
      <td>13.41</td>
      <td>5.75</td>
    </tr>
  </tbody>
</table>

<p>SearchArray just does BM25 scoring. You get back big numpy arrays of <strong>every document</strong>‚Äôs BM25 score. Then you combine the scores ‚Äì literally using <code>np.sum</code>. Of course, that‚Äôs not what a search engine like Elasticsearch would do. Instead it has a different guarantee, it gets <strong>the highest scoring top N</strong> of your specified OR query.</p>

<p>This little bit of seemingly minute wiggle room gives search engines a great deal of latitude. Search engines can use an algorithm called <a href="https://www.elastic.co/blog/faster-retrieval-of-top-hits-in-elasticsearch-with-block-max-wand">Weak-AND or WAND</a> to avoid work when combining multiple term scores into the final top N results.</p>

<p>I won‚Äôt get into the full nitty gritty of the algorithm, but here‚Äôs a key intuition to noodle over:</p>

<p>A scoring system like BM25 depends heavily on document frequency of a term. So rare terms - a high (1 / document frequency) - have a higher likelihood of impacting the final score, and ending up in the top K. Luckily these terms (like <code>skywalker</code>) occur on fewer documents. So we can fetch these select, elite few docs quickly in the data structure that maps <code>skywalker -&gt; [... handful of matching doc ids...]</code> (aka postings). We can reach deeply into this list.</p>

<p>On the other hand, we can be much more circumspect about the boring, common term, <code>luke</code>. And that‚Äôs useful because <code>luke</code> has a very extensive postings list <code>luke -&gt; [... a giant honking list of documents...]</code>. We‚Äôd prefer to avoid scanning all of these.</p>

<p>We might imagine that these lists of document ids, also is paired with its <strong>term frequency</strong> how often that term occurs in that document - the other major input of BM25. And if its SORTED from highest -&gt; lowest term frequency, we can go down this list until its impossible for the BM25 score of a term to have any chance of making the top K results. Then exit early.</p>

<p>While WAND - and similar optimizations - helps Elasticsearch avoid work, SearchArray, gleefully does this work like an ignoramus happily giving you a giant idiotic array of BM25 scores.</p>

<p>When you look at this icicle graph of SearchArray‚Äôs performance doing an ‚ÄúOR‚Äù search, you can see all the time spent <strong>summing</strong> a giant array and also needlessly BM25 scoring many many documents.</p>

<p><img width="634" alt="image" src="https://softwaredoug.com/assets/media/2024/search-array-perf1.png"/></p>

<h2 id="searcharray-doesnt-directly-store-postings">SearchArray doesn‚Äôt directly store postings</h2>

<p>Unlike most search engines, SearchArray doesn‚Äôt have postings lists of terms -&gt; documents.</p>

<p>Instead, under the hood, SearchArray stores a positional index, built first-and-foremost for <strong>phrase matching</strong>. You give SearchArray a list of terms <code>[&#39;mary&#39;, &#39;had&#39;, &#39;a&#39;, &#39;little&#39;, &#39;lamb&#39;]</code>. It then finds every place <code>mary</code> occurs one position before <code>had</code>, etc. It does this by storing, for each terms, the positions as a <a href="https://softwaredoug.com/blog/2024/01/21/search-array-phrase-algorithm">roaring bitmap</a>.</p>

<p>In our roaring bitmap, each 64 bit word has a header indicating where the positions occur (document and region in the doc). Each bit position corresponds to a position in the document. A 1 indicates this term is present, a 0 missing.</p>

<p>So to collect phrase matches, for <code>mary had</code> we can simply find places where one term‚Äôs bits occur adjacent to another. This can be done very fast with simple bit arithmetic.</p>

<div><div><pre><code>mary
 &lt;Roaring Headers (doc id, etc)&gt;  00000010000000    | 00000000000010
had
 &lt;Roaring Headers (doc id, etc)&gt;  00000001000000    | 00000000000001      #&lt; Left shift 1, AND, count bits to get phrase freq
...
</code></pre></div></div>

<p>But a nice property of this, and alleviating maintenance for this one person project, is the fact that we can also use this to compute term frequencies. Simply by performing a <code>popcount</code> (counting the number of set bits), then collecting those documents for a term, we get a mapping of doc ids -&gt; term frequencies.</p>

<p>So we spend a fair amount of time doing that, as you can see here:</p>

<p><img width="634" alt="image" src="https://softwaredoug.com/assets/media/2024/search-array-perf2.png"/></p>

<p>Now I lied actually, while this is the core mechanism for storing term frequencies, we do cache. A cache that remembers the doc id -&gt; term frequencies when the roaring bit array is &gt; N 64 bit words. This lets users tune N to your memory / speed tradeoff, and get closer to a postings list.</p>

<h2 id="caching-non-term-dependent-bm25-components">Caching Non term-dependent BM25 components</h2>

<p>Take a look at this snippet for computing BM25:</p>

<div><div><pre><code>        bm25 = (
            term_freq / (term_freq + (k1 * ((1 - b) + (b * (doc_len / avg_doc_lens)))))
        ) * idf
</code></pre></div></div>

<p>Notice there is a BIG PART of this calculation that has nothing to do with the terms being searched:</p>

<div><div><pre><code> (k1 * ((1 - b) + (b * (doc_len / avg_doc_lens)))))
</code></pre></div></div>

<p>In my testing, a bit of latency (1ms on 1 million docs) can be shaved by caching everything in the <code>k1 + ... avg_doc_lens</code> somewhere. If <code>doc_len</code> corresponds to array with a doc length value for every document, you can create an array with this formula cached. But it‚Äôs a bit of a maintenance burden to have one additional, globally shared cache. So I have avoided this so far.</p>

<h2 id="caching-the-full-query-not-just-individual-bm25-term-scoring">Caching the FULL query, not just individual BM25 term scoring</h2>

<p>SearchArray is just a system for computing BM25 scores (or whatever similarity). You USE it to build up an ‚Äúor query‚Äù or whatever using numpy‚Ä¶ it doesn‚Äôt do it for you. IE the code implemented in BEIR is simply:</p>

<div><div><pre><code><span>def</span> <span>bm25_search</span><span>(</span><span>corpus</span><span>,</span> <span>query</span><span>,</span> <span>column</span><span>):</span>
    <span>tokenizer</span> <span>=</span> <span>corpus</span><span>[</span><span>column</span><span>].</span><span>array</span><span>.</span><span>tokenizer</span>
    <span>query_terms</span> <span>=</span> <span>tokenizer</span><span>(</span><span>query</span><span>)</span>
    <span>scores</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>len</span><span>(</span><span>corpus</span><span>))</span>
    <span>query_terms</span> <span>=</span> <span>set</span><span>(</span><span>query_terms</span><span>)</span>
    <span>for</span> <span>term</span> <span>in</span> <span>query_terms</span><span>:</span>
            <span>scores</span> <span>+=</span> <span>corpus</span><span>[</span><span>column</span><span>].</span><span>array</span><span>.</span><span>score</span><span>(</span><span>term</span><span>)</span>
    <span>return</span> <span>scores</span>
</code></pre></div></div>

<p>But in a regular search engine like Solr, Elasticsearch, OpenSearch, or Vespa, this logic is expressed in the search engine‚Äôs Query DSL. So the search engine can plan+cache the complete calculation, whereas SearchArray gives you all the tools to shoot yourself in the foot, performance wise (not to mention the earlier point about WAND, etc).</p>

<h2 id="thats-why-you-should-hug-a-search-engineer">That‚Äôs why you should hug a search engineer</h2>

<p>There you have it!</p>

<p>SearchArray is a tool for prototyping, using normal Pydata tooling, not for building giant retrieval systems like Elasticsearch. It‚Äôs good to know the tradeoffs behind your lexical system, as they focus on different tradeoffs. You might find it useful for dorking around on &lt; 10 million doc datasets.</p>

<p>What would be great would be if we COULD express our queries in such a dataframe-oriented DSL. IE a Polars-esque lazy top-N retrieval system that pulled from different retrieval sources, scored them, summed them, and did whatever <em>arbitrary math</em> to the underlying scores. I can cross my fingers such a thing might exist. So far people build these DAGs in less expressive ways: as part of their Torch model DAG, or some homegrown query-time DAG system.</p>

<p>In any case, I‚Äôm absolutely humbled by folks that work on big, large scale, distributed lexical search engines like (Vespa, Lucene, OpenSearch, Elasticsearch, Solr). These folks ought to be your hero too, they do this grunt work for us, and we should NOT take it for granted.</p>

<p>Below are some notes and appendices for BEIR and the different benchmarking scripts, in case you‚Äôre curious</p>

<hr/>

<h4 id="appendix-links-to-scripts">Appendix links to scripts</h4>

<ul>
  <li><a href="https://github.com/softwaredoug/searcharray-beir/blob/main/evaluate_elasticsearch.py">Elasticsearch Script</a> | <a href="https://github.com/beir-cellar/beir/blob/main/beir/retrieval/search/lexical/elastic_search.py">Elasticsearch Config</a></li>
  <li><a href="https://github.com/softwaredoug/searcharray-beir/blob/main/search_array.py">SearchArray Script</a></li>
</ul>

<h4 id="appendix---how-to-integrate-with-beir">Appendix - How to integrate with BEIR‚Ä¶</h4>

<p>BEIR has a set of built-in datasets and metrics tools, if you implement a <code>BaseSearch</code> class with the following signature:</p>

<div><div><pre><code>    <span>class</span> <span>SearchArraySearch</span><span>(</span><span>BaseSearch</span><span>):</span>

        <span>def</span> <span>search</span><span>(</span><span>self</span><span>,</span>
                   <span>corpus</span><span>:</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>]],</span>
                   <span>queries</span><span>:</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>],</span>
                   <span>top_k</span><span>:</span> <span>int</span><span>,</span>
                   <span>*</span><span>args</span><span>,</span>
                   <span>**</span><span>kwargs</span><span>)</span> <span>-&gt;</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>float</span><span>]]:</span>
</code></pre></div></div>

<p>The inputs:</p>

<ul>
  <li>Corpus: A dict pointing a document id to a set of fields to index, ie</li>
</ul>

<div><div><pre><code>{&#39;text&#39;: &#39;The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.&#39;,
 &#39;title&#39;: &#39;&#39;}
...
</code></pre></div></div>

<ul>
  <li>Queries: A dict pointing a query id -&gt; query:</li>
</ul>

<div><div><pre><code>{&#34;1&#34;: &#34;Who was the original governor of the plymouth colony&#34;}
...
</code></pre></div></div>

<p>Finally the output is a dictionary of query ids -&gt; {doc ids -&gt; scores} - each query w/ <code>top_k</code> scored.</p>

<p>So when search is called you need to</p>

<ol>
  <li>Index the corpus</li>
  <li>Issue all queries and gather scores</li>
</ol>

<p>Essentially this looks something like:</p>

<div><div><pre><code>def search(self,
           corpus: Dict[str, Dict[str, str]],
           queries: Dict[str, str],
           top_k: int,
           *args,
           **kwargs) -&gt; Dict[str, Dict[str, float]]:
    corpus = self.index_corpus(corpus)     # &lt;- add tokenized columns to dataframe
    results = {}

    for query_id, query in queries.items():    #&lt;- search and gather results
        results_for_query = some_search_function(corpus, query)
        results[query_id] = get_top_k(results_for_query)
    return results
</code></pre></div></div>

<p>How does this look for SearchArray?</p>

<p>To index, we loop over each str column, and add a SearchArray column to the DF. Below, tokenized with a snowball tokenizer:</p>

<div><div><pre><code>            for column in corpus.columns:
                if corpus[column].dtype == &#39;object&#39;:
                    corpus[column].fillna(&#34;&#34;, inplace=True)
                    corpus[f&#39;{column}_snowball&#39;] = SearchArray.index(corpus[column],
                                                                     data_dir=DATA_DIR,
                                                                     tokenizer=snowball_tokenizer)
</code></pre></div></div>

<p>Then replace <code>some_search_function</code> above w/ something that searches the SearchArray columns. Maybe this simple bm25_search:</p>

<div><div><pre><code>def bm25_search(corpus, query):
    query = snowball_tokenizer(query)
    scores = np.zeros(len(corpus))
    for q in query:
        scores += corpus[&#39;text_snowball&#39;].array.score(q)
    return scores
</code></pre></div></div>

<p>(Leaving out some annoying threading, but you can look at <a href="https://github.com/softwaredoug/searcharray-beir">the code all here</a> )</p>

      


	  </div><p>I hope you join me at <a href="https://maven.com/softwaredoug/cheat-at-search">Cheat at Search with LLMs</a> to learn how to apply LLMs to search applications. Check out <a href="https://github.com/softwaredoug/softwaredoug.com/edit/master/_includes/post.html">this post</a> for a sneak preview.

		</p></div>
  </body>
</html>
