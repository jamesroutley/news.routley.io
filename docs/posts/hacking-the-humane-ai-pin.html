<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://writings.agg.im/posts/hacking_ai_pin/">Original</a>
    <h1>Hacking the Humane AI Pin</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="#first-access">Skip to the technical stuff</a></p><h2 id="shutdown-announcement">Shutdown announcement</h2><p>On February 18, 2025, Humane announced it would be shutting down all services for the always online Humane Ai Pin in ten days, <a href="https://web.archive.org/web/20250219012718/https://humane.com/media/humane-hp">following a sale to HP</a>. I had been vaguely aware of the Ai Pin, definitely more so than the average tech person, as I have a close friend who was one of the earliest adopters and eventually joined the Humane team to actively work on the device, but I ultimately knew little about them. I did know that they were small, ran Android, had good industrial design, and had a cool laser. I also have dreamt of a personal, local always listening device since I was a young teenager. I was a little more confident in my ability to help shape a niche device’s ecosystem after my experiences driving the Analogue Pocket’s growth, so I decided to go looking for disgruntled users looking to sell their devices.</p><figure><img src="https://writings.agg.im/images/hacking_ai_pin/lunar_stock_photo.webp" alt="Stock image of the Lunar Ai Pin"/><figcaption>Courtesy Humane Press Kit</figcaption></figure><p>I relatively quickly was able to secure two Pins, three boosters, and a charging “egg” from Reddit for a total of $300 (a price that I originally thought might be too much, though I would later see them selling for $300-500 each). One of my Pins was supposed to be locked (no code from a previous owner), so possibly unusable, but that was OK, as we were here to learn and break things. I paid for expedited shipping so I could get the devices before the shutdown. Unfortunately for me, my two Pins arrived on the fated shutdown day February 28th… …about 2 hours <em>after</em> the servers were shut off. There was a big community call on the Humane community Discord server (they called themselves reHumane) counting down as the servers were turned off. Some big fans and current/previous Humane employees spoke about their experiences with the Ai Pin. I found it to be a weird experience overall, but it was interesting to see how passionate people were about these devices. Unfortunately with the devices I had purchased, the “working” device had been factory reset. Turns out that without Humane’s servers to talk to, you can never activate the device, so I was stuck in the onboarding screens. I would later acquire another Pin (a still sealed one) bringing the total to three, but all three were stuck on onboarding, thus I never got to experience what a working Ai Pin was like.</p><p>At least I now had hardware. Now I needed a mechanism to connect the Pin to a computer. The Pin has four pads hidden behind a moon sticker at the bottom of the device; these pads are just raw USB 2.0 and expose standard Android ADB. The only problem with connecting to them is they are 1.0mm pitch, which is quite small. To connect to test pads on devices like this you generally use a dock called an interposer, often using pressure sensitive probes to contact the pads. There were some existing connector components that would maybe work, but they weren’t 1.0mm pitch, so I was wary to pay for small quantities of a part that really required a PCB to be made. I ended up buying 0.68mm diameter, 16.55mm fully extended spring probes from Amazon, and spent entirely way too long designing and iterating on a mechanism to allow a human to place these pins 1.0mm apart while still being able to solder wires to them. A mechanical engineer friend pointed out the obvious better approach; fanning out the pins, which proved to be the best strategy by far. <a href="https://github.com/agg23/ai-pin-interposer">You can see my poor designs here</a>. Eventually community members <a href="https://www.etsy.com/listing/1904242117/ai-pin-usb-dock-slim-final-ver-woptions">GoinGhost</a> and <a href="https://openpin.org/about/interposers">@MaxMaeder</a> produced more robust designs with proper PCBs and components, and I used those from then on. <a href="https://github.com/PenumbraOS/3dprints">I later produced revised 3D models</a> for a better fit for Max’s interposer.</p><figure><img src="https://writings.agg.im/images/hacking_ai_pin/exposed_pads.jpg" alt="Bottom of an Ai Pin with exposed pads"/></figure><h2 id="examining-leaked-apks">Examining leaked APKs</h2><div><p>I am rather unfamiliar with Android, having last developed on it approximately 15 years ago (and only barely then). Gaining access to the Pin was a whole new set of things to learn. I am very skeptical about the future of generative AI as it is, its concerns for the future of humanity, copyright, and more, but I used this project as my first entry in trying to familiarize myself with applying LLMs for technical work. I can very confidently say that I could not have figured out what I did about Android without the help of LLMs. The codegen was of questionable worth, but the research was invaluable, even though there were several instances directed by LLM that wasted a double digit number of hours on something completely infeasible (but not trivially verifiable by someone uninformed like myself).</p></div><p>My Pins were both factory reset, and with Humane’s servers down, I couldn’t activate them, so there wasn’t much to play with. However, on the day of the Humane shutdown, there was some other excitement. Someone posted an Archive.org link to a number of leaked APKs ripped from a device. It’s important to note that no one from my side of the story ever had anything to do with any distribution. No sharing, no using decompiled code, nothing. Given the leaks, with no working Pins and the lack of an interposer design, there wasn’t much left for me to do besides scour through those leaked files. Since Android is primarily JVM bytecode based, it is trivial to decompile most applications into something rather resembling the original source code. Humane didn’t obfuscate their binaries, so all of the original names (variables, classes, enums) were present, making the code reasonably easy to read. At this point we had a number of people actively researching.</p><p>I was focused on getting past onboarding, since that’s where my Pins were stuck. The main things I expected to look for were logic errors where a stray argument could cause a different code path, test code still around (like Android intents that could be launched with an ADB command), global settings, and if all else fails, a way to bypass cert pinning, if that was enabled on the authentication server connection (it was). Ultimately I decided that if there was a way to bypass it, I would not be capable of it without some notable vulnerability to inject code or provide privilege escalation.</p><h2 id="magical-adb-cert">Magical ADB cert</h2><p>Suddenly one day about a week in I got a random anonymous message on Signal containing a single file of 1,704 bytes. I cautiously examine this rogue file in a hex editor and find that it looks like a real private key. I move it to <code>.android/adbkey</code>, run <code>adb kill-server</code>, then <code>adb shell</code> and suddenly I have an ADB connect prompt appearing on the laser display on the Pin. I was in. The private key appears to have been posted on the internet at about the same time and the rest of my development group quickly gained access as well.</p><h2 id="first-access">First access</h2><p>As soon as I gained ADB access to the Pin I tried running a demo app, and a few minutes later I had it projecting my cat profile picture and displaying a live video feed from the camera. That’s really good! Maybe we have full access to all of the hardware, and we’re basically free to modify the device at will. I dumped as much of the filesystem as possible (which is necessarily incomplete due to the limited access due to non-root user) and figured how to disable the onboarding experience, dropping me into the (very non-functional) base <code>SystemNavigation</code> UI. However, attempting to connect to a remote HTTP server resulted in an exception and <code>avc</code> errors logged to the console.</p><figure><img src="https://writings.agg.im/images/hacking_ai_pin/first_image.webp" alt="First custom image being displayed on my Pin"/></figure><h2 id="searching-for-a-crack">Searching for a crack</h2><p>Lots of probing showed that there were no obvious ways to get signals in/out of a user installed application. The Humane devs were super careful about security all across the device (which was part of their strange notion of “privacy” that sent stuff to remote servers all of the time, but at least your device was safe from penetration?), so they heavily relied on a newer Linux (and Android) permission system called SELinux. Those <code>avc</code> errors I saw when opening a TCP socket was SELinux denying the operation. Humane tweaked the default Android SELinux config so users and services had the exact most minimal access they would require at any time (which is a good practice, if surprising on an embedded device). Without root permissions or the platform key, the only type of app that you can install on Android, even with ADB access, belongs to the SELinux domain <code>untrusted_app</code>. This is the same permission level that a newly installed app from the Play Store would have. With that in mind Humane blocked all common communication methods between <code>untrusted_app</code> and any other SELinux domain that was not controlled by very specific system services (like SystemServer, the core system management process). No <code>tcp_socket</code>, named sockets, DNS queries, named pipes, etc. The filesystem dump has many <code>.cil</code> files that are the remnants of the SELinux compilation process. Using them, you can construct a partial picture of what access we have, which was very helpful in exploring the attack space.</p><p>As seen from the <code>.cil</code> files and a quick demo app, there was some limited access to the filesystem from our <code>untrusted_app</code>, so if we could run something else on the device pointing at the filesystem, we might be able to use that process as a bridge to talk to the world. Processes spawned as the <code>shell</code> user and domain (ADB’s default user) can open sockets (like when running <code>ping google.com</code>), so we could manually start a process on the Pin over ADB that uses the filesystem to talk to a normal Android app, giving it some semblance of communication. We can then proxy HTTP or whatever other APIs we want through this push/pull file connection. This would be slow and not persistent (would require rerunning the exploit after reboot; a “tethered jailbreak”), but it would get us going. This is how OpenPin worked at the time of writing. <a href="https://github.com/MaxMaeder/">@MaxMaeder</a> (the developer of OpenPin) and I both realized this would be possible at about the same time, but he had a working implementation much faster than I did. I became frustrated with how hacky my solution was and went looking for more powerful and “correct”…</p><h2 id="probing-vulernabilities">Probing vulernabilities</h2><p>A glance through recent reported Android vulnerabilities isolated two vulnerabilities that might be interesting and could theoretically be applied to the Pin. One of them in particular granted full access to <code>system</code> permissions, but it was somewhat finicky to use. This was <a href="https://rtx.meta.security/exploitation/2024/06/03/Android-Zygote-injection.html">CVE-2024-31317</a>, patched in Android security patch 2024-06-01. The latest version of the Ai Pin OS runs patch 2023-04-01, so it was within the valid window. The vulnerability is based around a global setting that just about anyone can set, including the ADB <code>shell</code> user and normal <code>untrusted_app</code>, that doesn’t have proper input sanitization. That setting feeds directly into Zygote, the system that spawns all of Android’s system and app processes. If you’re careful with how you construct your settings payload, you can launch processes or apps (but this is extra hard) with the identity of any SELinux “app” permission on the device. The highest level of app permission is <code>system</code>, but there are other interesting apps definitions as well, which is something we use later. <code>system</code> is the next best thing to root and it’s what many core Android systems operate under, so potentially gaining <code>system</code> access would grant us just about any permission we could want. Notably you can “clone” the SELinux permissions of an actual app on disk, so if Android checks if the app <code>com.android.settings</code> has special permissions allowing it modify eSIM settings, we can steal that permission and bypass the check.</p><p>I spent weeks studying this vulnerability, getting a Android build environment setup in my homelab, building VMs of Android 12L that I could test, tweaking print statements to trace the flow and timing of this convoluted multi-process communication system used in starting every process tied into the Android event system. This vulnerability is made much more complicated on Android 12 and later, and the Pin is on 12L, which meant the flow was extra confusing and there was a mix of native and Java code that needed to be traced. Eventually I understand the exact mechanics behind it thoroughly enough and (<a href="https://github.com/agg23/cve-2024-31317/">I do my best to document my findings</a>) to add to the public discussion of the vulnerability and hopefully add some clarity over existing articles that didn’t go as deep (I shortly thereafter found a group of people trying to apply this vulnerability, aided by my research, to another embedded Android device).</p><p>The exploit process (<a href="https://github.com/agg23/cve-2024-31317/">see here for a detailed explanation</a>):</p><ol><li>Attacker constructs a payload. On Android 12+ this payload needs to exploit specific timing and logic details to force the Zygote process launch system to execute our command. For simplicity sake, our payload looks something like:</li></ol><pre tabindex="0"><code>───────────────────────────────────────────────────────────────────────────────────────────────
BEGIN SETTINGS PAYLOAD
───────────────────────────────────────────────────────────────────────────────────────────────

\n                                      # Newlines (~3000x for padding)
\n
\n
...
\n
AAAAA...A                               # Any non-newline char; pads total to ~8,196 bytes
                                        # (includes some Android-added bytes not seen here)
───────────────────────────────────────────────────────────────────────────────────────────────
START OF ATTACKER-INJECTED COMMAND
───────────────────────────────────────────────────────────────────────────────────────────────

1234\n                                  # Number of arguments in command
--various-zygote-internal-arguments\n   # Determines permissions for new process
--invoke-with\n                         # Argument wrapper for custom command
[ATTACKER SH COMMAND]\n                 # Malicious shell command supplied by attacker
android.app.ActivityThread\n            # Expected entrypoint class to keep structure valid

───────────────────────────────────────────────────────────────────────────────────────────────
END OF ATTACKER-INJECTED COMMAND
───────────────────────────────────────────────────────────────────────────────────────────────

,,,,,,,,...,                            # Number of trailing commas is tuned for timing
                                        # (count of total commas + newlines) &lt;= 12,200 in
                                        # Android 12L

───────────────────────────────────────────────────────────────────────────────────────────────
END SETTINGS PAYLOAD
───────────────────────────────────────────────────────────────────────────────────────────────
</code></pre><ol start="2"><li>Attacker writes the payload to the vulnerable global setting <code>hidden_api_blacklist_exemptions</code>. This can be done over ADB as we see below, or in any app with the <code>WRITE_SECURE_SETTINGS</code> permission (which an app from the Play Store may possess, making this a very dangerous CVE).</li></ol><div><pre tabindex="0"><code data-lang="bash"><span><span><span># Set up exploit</span>
</span></span><span><span>settings put global hidden_api_blacklist_exemptions <span>[</span>payload<span>]</span>
</span></span><span><span>
</span></span><span><span><span># Start exploit if it didn&#39;t already trigger and provide a spawn command to eat (see below)</span>
</span></span><span><span>am start -n com.android.settings/com.android.settings.Settings
</span></span><span><span>
</span></span><span><span><span># Immediately clean up exploit; Zygote should have started processing, and we want to</span>
</span></span><span><span><span># protect against boot loops</span>
</span></span><span><span>settings delete global hidden_api_blacklist_exemptions
</span></span></code></pre></div><ol start="3"><li>Assuming the payload is correctly formed and you had favorable timing, the attacker’s process is now running with elevated privileges.</li></ol><h3 id="pin-specifics">Pin specifics</h3><p>This explanation can be seen in much more detail in <a href="https://github.com/penumbraOS/pinitd/issues/4">https://github.com/penumbraOS/pinitd/issues/4</a>.</p><p>Unfortunately, successfully triggering the vulnerability is not the full story on the Ai Pin due to an unfortunate way of how the Pin’s environment is set up. As seen in the attacker’s commands above, a start of another Android app is used to trigger the vulnerability. This is very intentional, as there’s actually a delicate IPC between the Zygote process and the Activity Manager Service (AMS). When AMS requests a process spawn (either via a user tapping an app icon or <code>am start</code>), it sends a command (like the attacker’s payload) to Zygote, asking it to spawn. Zygote then responds with some bytes indicating the spawned PID and an irrelevant boolean.</p><p>This represents a problem for the attacker. When Zygote executes the injected process spawn command, it will send those extra five bytes to AMS. AMS will get confused and kill Zygote, as it’s clearly broken. <code>init</code> will see it lost its <code>zygote</code> child, and run an init script that soft reboots the device by restarting Zygote, AMS, and most of the core Android systems. That ruins any exploit progress the attacker made. Instead as Meta showed, the attacker tricks the AMS-Zygote pair. The attacker builds their payload to say it contains more newlines than it has. If the <code>hidden_api_blacklist_exemptions</code> setting modification happens <em>immediately</em> before AMS requests an app spawn, Zygote will end up only processing one command (the vulnerable command), as it starts to consume the next command (the app spawn) as part of the previous one, discarding it as it doesn’t make any sense. The unconsumed part of the command is flushed from the command buffer due to an unrelated Android bug. Since we had AMS request an app spawn at the same time as our settings set, AMS was expecting the aforementioned five bytes from the new process, and Zygote provides them. The bytes are from the wrong process (the vulnerable process, not the app that never actually started), but AMS won’t know.</p><p>This is how Meta’s writeup of the initial CVE report concludes, and I wish it was that easy. Unfortunate the Ai Pin has to make everything hard. Due to how Zygote operates as a process that clones itself constantly, when an Android image supports both 32 and 64 bit architectures, it has to run two versions of Zygote, one for each bitness. The <code>hidden_api_blacklist_exemptions</code> setting goes to both Zygotes simultaneously. Thus on the Ai Pin, when we start our exploit, the 64 bit Zygote gets a payload and a normal app spawn (because we used a 64 bit app as our trigger), but the 32 bit Zygote gets just a payload. Since that payload says it has more arguments than it has, 32 bit Zygote crashes. As said above, when this happens <code>init</code> soft reboots the entire system. I could not find a timing that would allow starting a 64 bit and a 32 bit process in quick succession that didn’t result in a crash.</p><p>I managed to completely avoid this problem for a month of development time due to a quirk in timing. It turns out that if you set <code>hidden_api_blacklist_exemptions</code>, start your process, then clear <code>hidden_api_blacklist_exemptions</code> (necessary to prevent the exploit from breaking the system in general) <em>immediately</em> one after another, those commands will actually be executed out of order. AMS does not send its Zygote command synchronously with the CLI command, so we move to executing the next command before the spawn has actually been sent:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># We send</span>
</span></span><span><span>- Put exploit into hidden_api_blacklist_exemptions
</span></span><span><span>- Spawn app
</span></span><span><span>- Clear exploit from hidden_api_blacklist_exemptions
</span></span><span><span>
</span></span><span><span><span># Zygote sees</span>
</span></span><span><span>- Put exploit into hidden_api_blacklist_exemptions
</span></span><span><span>- Clear exploit from hidden_api_blacklist_exemptions
</span></span><span><span>- Spawn app
</span></span></code></pre></div><p>Since our exploit payload in the put <code>hidden_api_blacklist_exemptions</code> command states it has more arguments than it actually has, Zygote eats the next command, which is the clear, not the app spawn we expected. The execution proceeds as follows:</p><ol><li>Zygote executes the exploit setup command, the put <code>hidden_api_blacklist_exemptions</code>. Zygote eats the next command (the clear)</li><li>Zygote sends AMS four bytes for the result of the command. The write succeeded, so it writes <code>0x0000_0000</code>.</li><li>AMS is expecting five bytes for an app spawn; four bytes for the newly spawned process PID and a byte for a boolean. AMS sees four bytes, all being recorded as the process pid. The pid is 0. AMS is still waiting on another byte</li><li>Zygote executes the spawn app command (the clear was eaten)</li><li>Zygote sends AMS five bytes. Four bytes are whatever the PID is (let’s say <code>0xDEAD_BEEF</code>) and the boolean is 0</li><li>AMS is expecting one byte to complete the previous app spawn command and nothing else. AMS sees the first byte of the PID (<code>0xEF</code>) and says the boolean is set to <code>0xEF</code>. AMS now considers the app spawn complete, and is no longer waiting on anything</li><li>AMS is idle, but the Zygote -&gt; AMS buffer contains four bytes (<code>0x00DE_ADBE</code>)</li></ol><pre tabindex="0"><code>───────────────────────────────────────────────────────────────────────────────────────────────
[Attacker] -- Put settings exploit
[AMS] -- Start app spawn
[Zygote] -- Executes put settings command
   | Eats next command (settings clear) due to extra arg count
   | Sends: status[4]
   | Actual: 0x0000_0000
   v ──────────────────────────────────────────────────────────────────────────────────────────
[AMS] -- Waiting for spawn app result
   | Expects: 5 bytes (boolean[1] + PID[4])
   | Reads:   4 bytes only (0x0000_0000)
   | Interprets PID = 0, still waiting for boolean
   v ──────────────────────────────────────────────────────────────────────────────────────────
[Zygote] -- Executes app spawn (clear was eaten)
   | Sends: boolean[1] + PID[4]
   | Example: 0x00 + 0xDEAD_BEEF
   v ──────────────────────────────────────────────────────────────────────────────────────────
[AMS] -- Still waiting for boolean from previous spawn
   | Expects: 1 byte (boolean[1])
   | Reads:   1 byte (first byte of actual PID)
   | Interprets boolean = 0xEF
   |
   | Marks spawn complete. AMS is satisfied
   |
   | Remaining 4 bytes [0x00DE_ADBE] sit in buffer
───────────────────────────────────────────────────────────────────────────────────────────────
</code></pre><p>This fun mixes up our bytes and displaces the PIDs of newly spawned processes. This generally wouldn’t be a problem, at least not immediately, but AMS kills processes if it already has a process that has received that PID before. So after ~3 spawns, AMS is completely unable to start anything due to dirty/overlapping PIDs, and Android is incapable of launching new apps, services, or other integral systems.</p><h2 id="weaponizing-the-vulnerability">Weaponizing the vulnerability</h2><p>Therefore, successfully triggering the vulnerability isn’t really enough to make the device usable as we either crash and reboot the device or break starting of new Android processes. I continued development on other parts of the system, not specifically exploit related, while operating under the “broken Android spawning” pattern. Every code change I needed to test required a full reboot and re-exploit where I would launch all of my Android processes before starting the exploit. After a large number of hours spent trying to fix this issue, I turned towards trying to combine the two problems together into a fix. If I could figure out how to spawn a process and keep it around while the system “rebooted”, then everyone could be happy.</p><p>I was confused that since the vulnerability was returning the wrong PIDs to AMS every time, AMS shouldn’t know about the correct process to kill, which suggests that it was tracking the existence of the process in some other way. With that in mind, I tried various techniques of orphaning processes unsuccessfully, and at some point noticed this log message from AMS: <code>Killing process group -1234 in uid 1000 as part of process cgroup 1234</code>. UID 1000 is <code>system</code> and <code>1234</code> was my exploit process PID after the orphaning. Linux cgroups are intended to be used for resource management and was created by Google themselves. They contributed it to the Linux kernel and use it heavily in Android for managing battery life, memory, and killing rogue processes associated with apps. If there was a way for me to change my process’s parent cgroup <em>and</em> have that cgroup stay persistent across the automatic reboot when Zygote crashes, I might be able to avoid AMS killing my process as well as clearing any broken Zygote state.</p><p>Taking the previous example of our app being PID <code>1234</code>, we can see what cgroups we’re a part of:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ai-pin:/ $ cat /proc/1234/cgroup
</span></span><span><span>5:schedtune:/top-app
</span></span><span><span>4:memory:/
</span></span><span><span>3:cpuset:/system-background
</span></span><span><span>2:cpu:/
</span></span><span><span>1:blkio:/
</span></span><span><span>0::/uid_1000/pid_1221
</span></span></code></pre></div><p>The bottom line is the important one: <code>0::/uid_1000/pid_1221</code>. This shows that our process is parented under the primary UID 1000 group under a group named <code>pid_1221</code>. <code>1221</code> is the PID of the actual application that started our orphaned process. Given <code>uid_1000</code> is so high privilege, let’s assume that it will stick around in our soft reboot (after all, that reboot happens in Android land, not proper Linux). Will normal Linux permissions let us set anything in that cgroup?</p><div><pre tabindex="0"><code data-lang="bash"><span><span>system:/ $ ls -al /sys/fs/cgroup/uid_1000
</span></span><span><span>...
</span></span><span><span>-rw-rw-r-- <span>1</span> system system <span>0</span> 1970-05-03 15:06 cgroup.procs
</span></span><span><span>...
</span></span></code></pre></div><p>As seen above, generic <code>system</code> does actually have write permissions to the cgroup control interface (<code>cgroup.procs</code>) for the UID 1000 (<code>system</code>) user. Surprisingly, Humane’s SELinux configuration doesn’t block it (I don’t know if it normally would on AOSP, but given how locked down Humane’s Android was, I really expected this to not work). I’ll try transitioning my process:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>system:/ $ <span>echo</span> <span>1234</span> &gt; /sys/fs/cgroup/uid_1000/cgroup.procs
</span></span><span><span><span># Check if it worked</span>
</span></span><span><span>system:/ $ cat /proc/1234/cgroup
</span></span><span><span>5:schedtune:/top-app
</span></span><span><span>4:memory:/
</span></span><span><span>3:cpuset:/top-app
</span></span><span><span>2:cpu:/
</span></span><span><span>1:blkio:/
</span></span><span><span>0::/uid_1000/
</span></span></code></pre></div><p><code>0::/uid_1000/</code> showed us that we’re now successfully in the parent cgroup for all of user 1000. I trigger the vulnerability (with the correct timing, which crashes the system due to <code>zygote32</code> looking for extra arguments as detailed above) to cause the soft reboot. I check <code>ps</code> and find that <code>1234</code> has survived. I launch an Android app and it opens. I’ve successfully launched a process as <code>system</code> from the <code>shell</code> user <em>and</em> kept the system running and stable. We have real privileged access now, and can start reaching into the hardware.</p><h3 id="real-communication">Real communication</h3><p>But wait, how does having all of these normal Linux processes help us? This doesn’t get our properly launched app out from being <code>untrusted_app</code>/talking to the network nor does it let us present UI or do nice Androidy things from a native binary.</p><p>We can use these processes in other SELinux contexts as relays to transmit data around iff we can find a way to talk to <code>untrusted_app</code>. After pouring through the SELinux <code>.cil</code> files, I came to the conclusion there are two avenues from another SELinux domains to talk to <code>untrusted_app</code>; <code>radio</code> and <code>nfc</code>. Both of these user domains spawn Android services of the same names that have permissions to communicate via Binder to <code>untrusted_app</code>. <code>radio</code> is used for cellular and wifi, so we probably don’t to touch that, but there isn’t even a <code>nfc</code> process running on my device. If we could use the vulnerability to “become” the <code>nfc</code> “app” (as it’s defined in <code>seapp_contexts</code>, the file where SELinux defines app information), we could offer our own <code>nfc</code> service that actually is our magical router for everything Ai Pin, i.e. the <a href="https://github.com/penumbraOS/sdk">PenumbraOS SDK and bridge</a>.</p><p>So that’s exactly what I do. Due to how Humane restricted the SELinux policy, certain parts of the system can only do certain things. For example, both <code>system</code> and <code>shell</code> users can access the internet, but <code>nfc</code> and <code>radio</code> cannot. All of those domains can talk to each other though, so I end up building a tree of bridges:</p><pre tabindex="0"><code>              ┌──────────────────────────────┐
              │                              │
              │     User&#39;s untrusted_app     │
              │                              │
              └──────────────────────────────┘
                              ▲
                              │
                              ▼
              ┌──────────────────────────────┐
              │                              │
              │ nfc service (bridge-service) │
              │                              │
              └──────────────────────────────┘
                    ▲                   ▲
                    │                   │
                    ▼                   ▼
┌──────────────────────────┐     ┌──────────────────────────┐
│                          │     │                          │
│  bridge-system-service   │     │   bridge-shell-service   │
│                          │     │                          │
└──────────────────────────┘     └──────────────────────────┘
</code></pre><p><code>bridge-shell-service</code> can do ADB developer things that apps aren’t allowed to do. <code>bridge-system-service</code> has higher privileges than <code>shell</code> in most scenarios.</p><p><code>bridge-system-service</code> is the preferred implementation location as it’s the most “app like”. It has capabilities such as:</p><ul><li>DNS</li><li>HTTP/WebSockets</li><li>Hand tracking</li><li>Humane’s built in STT service</li><li>Input/hardware IO (Touchpad, LEDs, eSIM)</li></ul><p><code>bridge-shell-service</code> is mainly just there to let apps call the ADB shell if necessary.</p><h2 id="stealing-identities">Stealing identities</h2><p>This gives us the Linux level permissions, but we don’t necessarily have the Android level permissions, nor can we use almost any of the sample code on the internet for interacting with common Android services as it’s all in Java. But CVE-2024-31317 allows you to accomplish quite a lot if you’re careful. You can spawn a process as <code>system</code>, under the <code>com.android.settings</code> package name, making some parts of Android <em>actually think you are the real Settings app</em>. As you can imagine, the Settings app has permissions to do a lot of things. That’s somewhat limited when you’re running native binaries, however, as Android mostly lives in the land of JVM and an IPC system called Binder. A native process cannot easily communicate with system services, such as <code>phone</code> or <code>input</code> or any number of other things. Helpfully, here is a way to launch Java processes from CLI using <code>/system/bin/app_process</code>, allowing us to talk to Binder (mostly) like a real app. <code>app_process</code> exists almost entirely to start Zygote on system boot, but it has several arguments that make it very useful to us. Using a bunch of hackery and reflection, <a href="https://github.com/PenumbraOS/app_process-mocks">I built a number of ways to mock key classes and things used in making an app look “real”</a>. Using this, you can do something like:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>/system/bin/app_process -cp com.penumbraos.bridge /system/bin --application com.penumbraos.bridge.Entrypoint
</span></span></code></pre></div><p>where <code>com.penumbraos.bridge</code> is an APK built using aforementioned mocks, to launch something capable of talking to the Android Binder service system (granting access to those Android services and easy access to other APIs). You can also use that <code>app_process</code> to load code from another app using a <code>DexClassLoader</code>. Doing this we can pull in Humane’s eSIM code (without the copyright hazard of redistributing it), patch a notable bug they have, then use it to load our own eSIMs, even though the Humane environment wouldn’t normally allow this. <a href="#ps-esim">See the P.S. section on eSIM below</a>.</p><h2 id="recovering-userland">Recovering userland</h2><p>With a use case and the ability to spawn stable, privileged <code>system</code> processes, we need to package this into something that can be deployed on devices and set up the userland environment for us. I wrap that functionality up in a init system with <code>systemd</code>-style syntax named <a href="https://github.com/PenumbraOS/pinitd/"><code>pinitd</code></a>. Since the vulnerability can be triggered from a normal app, we install <code>pinitd</code> partially as a native binary that will be triggered by the vulnerability and partially as a normal app which will run some code on Android’s <code>BOOT_COMPLETED</code> broadcast in order to start <code>pinitd</code> automatically on boot. <code>pinitd</code> then does all of the stuff we need to set up worker processes that will live past a soft reboot under each of the SELinux domains, users, etc. required by the configuration. In general use this is spawning worker processes in the <code>nfc</code>, <code>shell</code>, and <code>system</code> environments (the latter two mimicking part of the <code>com.android.settings</code> identity), though <code>pinitd</code> supports more dynamic configuration as necessary. These are workers and not the actual services to be started by the init system as we can’t spawn “new” privileged processes after boot due to it requiring the vulnerability, which involves either corruption or a crash. We can only fork. So we create a fork worker in each environment that opens a TCP socket back to the controller node. After all of our environments have been set up, <code>pinitd</code> sends a crashing Zygote command to the system, causing the soft reboot. The entire Android system resets, and events like <code>BOOT_COMPLETED</code> are re-emitted. This starts the Android app <code>pinitd</code> again, which detects it is on the second run and dies. At the same time the <code>pinitd</code> binary (which stayed running through the soft reboot) attaches to all of the existing workers, and requests the spawning of our actual SDK environment.</p><p>The flow is:</p><ol><li>Normal Android boot. <code>BOOT_COMPLETED</code> is emitted to all registered Android apps</li><li><code>pinitd</code> the app receives <code>BOOT_COMPLETED</code>, waking up</li><li><code>pinitd</code> the app updates performs the vulnerability in the PID corrupting form, breaking AMS and future Android app spawns. The vulnerability is pointing at <code>pinitd</code> in native form as the binary to spawn</li><li>The native <code>pinitd</code> launches, sees this is the first boot, and starts all registered worker processes based on its configuration</li><li>Worker processes register themselves, and <code>pinitd</code> detaches the cgroups so it and the workers won’t get killed</li><li><code>pinitd</code> performs the vulnerability again, this time in the crashing form</li><li><code>zygote32</code> crashes, all core Android system processes are restarted. <code>pinitd</code> the native binary stays alive</li><li>Soft Android boot. <code>BOOT_COMPLETED</code> is emitted to all registered Android apps</li><li><code>pinitd</code> the app receives <code>BOOT_COMPLETED</code>, sees this is the second boot, marks boot completed and plays the boot chime (hardcoded to a recreation of the Macintosh LC boot chime)</li><li>The native <code>pinitd</code> checks that we’ve recovered from the crash and waits for all workers to attach after a delay</li><li><code>pinitd</code> sends requests to the necessary workers to fork their service processes based on its configuration</li><li>The workers start registered services</li></ol><p>Once this process is completed, we now have mostly working Android environment. Some operations have to be linked against the PenumbraOS SDK (most notably HTTP operations), but it’s functional and it exposes the APIs in a realistic way. From here, interested developers have a mechanism to play with these strange but interesting devices.</p><h2 id="conclusion">Conclusion</h2><p>That’s the state of efforts to save the Ai Pin from being complete ewaste. Obviously this took an insane amount of time that I should have dedicated towards more productive topics, but it was a very interesting learning experience to me. I don’t know how security researchers do this all of the time. If any of this interests you, or particularly if you’re interested in working on the userland applications on the Ai Pin, I would <a href="https://github.com/PenumbraOS/mabl">welcome help on MABL</a>, my launcher and voice assistant specifically designed for this environment. Turns out it’s a lot of work for one person to break into a device while trying to replicate a state of the art, low power, highly private, personal assistant. You can also feel free to reach out to me directly.</p><h3 id="ps-esim">P.S. eSIM</h3><p>Getting the eSIM to work was its own mini adventure. It’s obvious Humane did not intend anyone to ever connect to a different carrier than the MVNO they provided. They use a local profile assistant (LPA) from an unknown vendor (classes are of the form <code>es.com.valid.lib_lpa.controler</code>, and yes, that is how they spell “controller”) that amusingly has some significant bugs (which I later reported to HP/Humane). In order to properly connect to the eSIM hardware on the device, you must have the exact right permissions. This led me in circle for quite a while as I tried to replicate Humane’s own <code>humane.connectivity.esimlpa</code> APK’s permissions using the vulnerability. After fiddling with this for weeks on and off, I realize that Humane likely disabled the <code>esimlpa</code> app permissions in the user builds; it should never have been on my device in the first place. Unluckily for them, <code>com.android.settings</code> also had permissions to access the eSIM hardware, and by doing a bunch of mocking, dependency injection, then finally a Frida script on top of that to patch an infinite loop bug in the <code>es.com.valid.lib_lpa</code> implementation, I was able to activate an eSIM on my Pin. The mocking spins up an entire fake world in our process, ultimately letting us reach into the real <code>humane.connectivity.esimlpa</code> LPA and just call their code directly. At the time of writing I’ve only been able to get <a href="https://github.com/PenumbraOS/sdk/issues/3">eSIMs from a few vendors to work</a>, but this was a major obstacle that people weren’t sure if we’d surpass.</p></div></div>
  </body>
</html>
