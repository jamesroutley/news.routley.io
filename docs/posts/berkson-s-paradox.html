<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Original</a>
    <h1>Berkson&#39;s Paradox</h1>
    
    <div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><div>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Berkson_paradox_singers.svg"><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Berkson_paradox_singers.svg/170px-Berkson_paradox_singers.svg.png" decoding="async" width="170" height="302" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Berkson_paradox_singers.svg/255px-Berkson_paradox_singers.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Berkson_paradox_singers.svg/340px-Berkson_paradox_singers.svg.png 2x" data-file-width="512" data-file-height="910"/></a></p><div><p>An example of Berkson&#39;s paradox:</p></div></div></div>
<p><b>Berkson&#39;s paradox</b>, also known as <b>Berkson&#39;s bias</b>, <a href="https://en.wikipedia.org/wiki/Collider_(statistics)" title="Collider (statistics)">collider</a> bias, or <b>Berkson&#39;s fallacy</b>, is a result in <a href="https://en.wikipedia.org/wiki/Conditional_probability" title="Conditional probability">conditional probability</a> and <a href="https://en.wikipedia.org/wiki/Statistics" title="Statistics">statistics</a> which is often found to be <a href="https://en.wikipedia.org/wiki/Counterintuitive" title="Counterintuitive">counterintuitive</a>, and hence a <a href="https://en.wikipedia.org/wiki/Veridical_paradox" title="Veridical paradox">veridical paradox</a>. It is a complicating factor arising in statistical tests of proportions. Specifically, it arises when there is an <a href="https://en.wikipedia.org/wiki/Ascertainment_bias" title="Ascertainment bias">ascertainment bias</a> inherent in a study design. The effect is related to the <a href="https://en.wikipedia.org/wiki/Explaining_away" title="Explaining away">explaining away</a> phenomenon in <a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a>, and <a href="https://en.wikipedia.org/wiki/Collider_(statistics)" title="Collider (statistics)">conditioning on a collider</a> in <a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">graphical models</a>.
</p><p>It is often described in the fields of <a href="https://en.wikipedia.org/wiki/Medical_statistics" title="Medical statistics">medical statistics</a> or <a href="https://en.wikipedia.org/wiki/Biostatistics" title="Biostatistics">biostatistics</a>, as in the original description of the problem by <a href="https://en.wikipedia.org/wiki/Joseph_Berkson" title="Joseph Berkson">Joseph Berkson</a>.
</p>


<h2><span id="Examples">Examples</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=1" title="Edit section: Examples">edit</a><span>]</span></span></h2>
<h3><span id="Overview">Overview</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=2" title="Edit section: Overview">edit</a><span>]</span></span></h3>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Berkson.png"><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Berkson.png/220px-Berkson.png" decoding="async" width="220" height="363" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/75/Berkson.png/330px-Berkson.png 1.5x, //upload.wikimedia.org/wikipedia/commons/7/75/Berkson.png 2x" data-file-width="335" data-file-height="553"/></a></p><div><p>An illustration of Berkson&#39;s Paradox. The top graph represents the actual distribution, in which a positive correlation between quality of burgers and fries is observed. However, an individual who does not eat at any location where both are bad observes only the distribution on the bottom graph, which appears to show a negative correlation.</p></div></div></div>
<p>The most common example of Berkson&#39;s paradox is a false observation of a <i>negative</i> correlation between two desirable traits, i.e., that members of a population which have some desirable trait tend to lack a second. Berkson&#39;s paradox occurs when this observation appears true when in reality the two properties are unrelated—or even <i>positively</i> correlated—because members of the population where both are absent are not equally observed. For example, a person may observe from their experience that fast food restaurants in their area which serve good hamburgers tend to serve bad fries and vice versa; but because they would likely not eat anywhere where <i>both</i> were bad, they fail to allow for the large number of restaurants in this category which would weaken or even flip the correlation.
</p>
<h3><span id="Original_illustration">Original illustration</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=3" title="Edit section: Original illustration">edit</a><span>]</span></span></h3>
<p>Berkson&#39;s original illustration involves a retrospective study examining a <a href="https://en.wikipedia.org/wiki/Risk_factor" title="Risk factor">risk factor</a> for a disease in a <a href="https://en.wikipedia.org/wiki/Statistical_sample" title="Statistical sample">statistical sample</a> from a <a href="https://en.wikipedia.org/wiki/Hospital" title="Hospital">hospital</a> in-patient population. Because samples are taken from a hospital in-patient population, rather than from the general public, this can result in a spurious negative association between the disease and the risk factor. For example, if the risk factor is diabetes and the disease is <a href="https://en.wikipedia.org/wiki/Cholecystitis" title="Cholecystitis">cholecystitis</a>, a hospital patient <i>without</i> diabetes is <i>more</i> likely to have cholecystitis than a member of the general population, since the patient must have had some non-diabetes (possibly cholecystitis-causing) reason to enter the hospital in the first place. That result will be obtained regardless of whether there is any association between diabetes and cholecystitis in the general population.
</p>
<h3><span id="Ellenberg_example">Ellenberg example</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=4" title="Edit section: Ellenberg example">edit</a><span>]</span></span></h3>
<p>An example presented by <a href="https://en.wikipedia.org/wiki/Jordan_Ellenberg" title="Jordan Ellenberg">Jordan Ellenberg</a>: Suppose Alex will only date a man if his niceness plus his handsomeness exceeds some threshold. Then nicer men do not have to be as handsome to qualify for Alex&#39;s dating pool. So, <i>among the men that Alex dates</i>, Alex may observe that the nicer ones are less handsome on average (and vice versa), even if these traits are uncorrelated in the general population. Note that this does not mean that men in the dating pool compare unfavorably with men in the population.  On the contrary, Alex&#39;s selection criterion means that Alex has high standards.  The average nice man that Alex dates is actually more handsome than the average man in the population (since even among nice men, the ugliest portion of the population is skipped). Berkson&#39;s negative correlation is an effect that arises <i>within</i> the dating pool: the rude men that Alex dates must have been <i>even more</i> handsome to qualify.
</p>
<h3><span id="Quantitative_example">Quantitative example</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=5" title="Edit section: Quantitative example">edit</a><span>]</span></span></h3><p>
As a quantitative example, suppose a collector has 1000 <a href="https://en.wikipedia.org/wiki/Postage_stamp" title="Postage stamp">postage stamps</a>, of which 300 are pretty and 100 are rare, with 30 being both pretty and rare. 30% of all his stamps are pretty and 10% of his pretty stamps are rare, so prettiness tells nothing about rarity. He puts the 370 stamps which are pretty or rare on display.  Just over 27% of the stamps on display are rare (100/370), but still only 10% of the pretty stamps are rare (and 100% of the 70 not-pretty stamps on display are rare). If an observer only considers stamps on display, they will observe a spurious negative relationship between prettiness and rarity as a result of the <a href="https://en.wikipedia.org/wiki/Selection_bias" title="Selection bias">selection bias</a> (that is, not-prettiness strongly indicates rarity in the display, but not in the total collection).</p>
<h2><span id="Statement">Statement</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=6" title="Edit section: Statement">edit</a><span>]</span></span></h2>
<p>Two <a href="https://en.wikipedia.org/wiki/Statistical_independence" title="Statistical independence">independent</a> events become <a href="https://en.wikipedia.org/wiki/Conditional_independence" title="Conditional independence">conditionally dependent</a> given that at least one of them occurs.  Symbolically: 
</p>
<dl><dd>If <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8088a475fc93a01870c251eb9692bf1ba3ce6fb2" aria-hidden="true" alt="{\displaystyle 0&lt;P(A)&lt;1}"/></span>, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1ae63911a9b87060b0615654753bbd84e7474265" aria-hidden="true" alt="{\displaystyle 0&lt;P(B)&lt;1}"/></span>, and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/382b119cb6ac15ee4c4f3e31debc55a4d645ebf7" aria-hidden="true" alt="{\displaystyle P(A|B)=P(A)}"/></span>, then <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8862a7199681a55acccde9c65cfe76cc71f641a8" aria-hidden="true" alt="{\displaystyle P(A|B,A\cup B)=P(A)}"/></span> and hence <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/618d4bdd0e4906f14b14a4ffe4d9b1721178854e" aria-hidden="true" alt="{\displaystyle P(A|A\cup B)&gt;P(A)}"/></span>.</dd></dl>
<dl><dd></dd>
<dd>
</dd>
<dd></dd>
<dd>
</dd></dl>
<dl><dd></dd>
<dd></dd></dl>
<p>In other words, given two independent events, if you consider only outcomes where at least one occurs, then they become conditionally dependent, as shown above.
</p>
<h3><span id="Explanation">Explanation</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=7" title="Edit section: Explanation">edit</a><span>]</span></span></h3>
<p>The cause is that the <i>conditional</i> probability of event <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> occurring, <i>given</i> that it or <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> occurs, is inflated: it is higher than the <i>unconditional</i> probability, because we have <i>excluded</i> cases where <i>neither</i> occur.
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/618d4bdd0e4906f14b14a4ffe4d9b1721178854e" aria-hidden="true" alt="{\displaystyle P(A|A\cup B)&gt;P(A)}"/></span></dd>
<dd>conditional probability inflated relative to unconditional</dd></dl>
<p>One can see this in tabular form as follows: the yellow regions are the outcomes where at least one event occurs (and <b>~A</b> means &#34;not <b>A</b>&#34;).
</p>
<table>
<tbody><tr>
<th></th>
<th>A</th>
<th>~A
</th></tr>
<tr>
<th>B
</th>
<td>A &amp; B
</td>
<td>~A &amp; B
</td></tr>
<tr>
<th>~B
</th>
<td>A &amp; ~B
</td>
<td>~A &amp; ~B
</td></tr></tbody></table>
<p>For instance, if one has a sample of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0572cd017c6d7936a12737c9d614a2f801f94a36" aria-hidden="true" alt="100"/></span>, and both <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> occur independently half the time ( <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c5f8fd00c975ebfe0ac2f811c78ea916beef2200" aria-hidden="true" alt="{\displaystyle P(A)=P(B)=1/2}"/></span> ), one obtains:
</p>
<table>
<tbody><tr>
<th></th>
<th>A</th>
<th>~A
</th></tr>
<tr>
<th>B
</th>
<td>25
</td>
<td>25
</td></tr>
<tr>
<th>~B
</th>
<td>25
</td>
<td>25
</td></tr></tbody></table>
<p>So in <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ab258764362f14ae5c244475610f18012710c777" aria-hidden="true" alt="75"/></span> outcomes, either <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> or <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> occurs, of which <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/17e5f8966bed37734cd86d4fd3c302913bb6d48b" aria-hidden="true" alt="50"/></span> have <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> occurring. By comparing the conditional probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> to the unconditional probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i>: 
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f974917f681c6faeab581184a617d016726d369e" aria-hidden="true" alt="{\displaystyle P(A|A\cup B)=50/75=2/3&gt;P(A)=50/100=1/2}"/></span></dd></dl>
<p>We see that the probability of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span> is higher (<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f73d0fc4f429ca39387dffad817c8147cf73d0ff" aria-hidden="true" alt="{\displaystyle 2/3}"/></span>) in the subset of outcomes where (<i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> <i>or</i> <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span></i>) occurs, than in the overall population (<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e308a3a46b7fdce07cc09dcab9e8d8f73e37d935" aria-hidden="true" alt="{\displaystyle 1/2}"/></span>). On the other hand, the probability of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span> given both <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> and (<i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> or <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span></i>) is simply the unconditional probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i>, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f264d19e21604793c6dc54f8044df454db82744" aria-hidden="true" alt="P(A)"/></span>, since <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> is independent of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span></i>. In the numerical example, we have conditioned on being in the top row:
</p>
<table>
<tbody><tr>
<th></th>
<th>A</th>
<th>~A
</th></tr>
<tr>
<th>B
</th>
<td>25
</td>
<td>25
</td></tr>
<tr>
<th>~B
</th>
<td>25
</td>
<td>25
</td></tr></tbody></table>
<p>Here the probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> is <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8650867122d69e1ebd333eb69946861cec7311c9" aria-hidden="true" alt="{\displaystyle 25/50=1/2}"/></span>.
</p><p>Berkson&#39;s paradox arises because the conditional probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> given <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> <i>within the three-cell subset</i> equals the conditional probability in the overall population, but the unconditional probability within the subset is inflated relative to the unconditional probability in the overall population, hence, within the subset, the presence of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" aria-hidden="true" alt="B"/></span> decreases the conditional probability of <i><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" aria-hidden="true" alt="A"/></span></i> (back to its overall unconditional probability):
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e62af89037d80e9a390687d7573337a280decf5c" aria-hidden="true" alt="{\displaystyle P(A|B,A\cup B)=P(A|B)=P(A)}"/></span></dd>
<dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/618d4bdd0e4906f14b14a4ffe4d9b1721178854e" aria-hidden="true" alt="{\displaystyle P(A|A\cup B)&gt;P(A)}"/></span></dd></dl>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=8" title="Edit section: See also">edit</a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox" title="Simpson&#39;s paradox">Simpson&#39;s paradox</a></li>
<li><a href="https://en.wikipedia.org/wiki/Survivorship_bias" title="Survivorship bias">Survivorship bias</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=9" title="Edit section: References">edit</a><span>]</span></span></h2>
<ul><li><cite id="CITEREFBerkson1946">Berkson, Joseph (June 1946). &#34;Limitations of the Application of Fourfold Table Analysis to Hospital Data&#34;. <i><a href="https://en.wikipedia.org/wiki/Biometrics_(journal)" title="Biometrics (journal)">Biometrics Bulletin</a></i>. <b>2</b> (3): 47–53. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F3002000">10.2307/3002000</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a> <a rel="nofollow" href="https://www.jstor.org/stable/3002000">3002000</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/21001024">21001024</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biometrics+Bulletin&amp;rft.atitle=Limitations+of+the+Application+of+Fourfold+Table+Analysis+to+Hospital+Data&amp;rft.volume=2&amp;rft.issue=3&amp;rft.pages=47-53&amp;rft.date=1946-06&amp;rft_id=info%3Apmid%2F21001024&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F3002000%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.2307%2F3002000&amp;rft.aulast=Berkson&amp;rft.aufirst=Joseph&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABerkson%27s+paradox"></span> (The paper is frequently miscited as Berkson, J. (194<b>9</b>) <a href="https://en.wikipedia.org/wiki/The_Biological_Bulletin" title="The Biological Bulletin"><b>Biological</b> Bulletin</a> 2, 47–53.)</li>
<li>Jordan Ellenberg, &#34;<a rel="nofollow" href="http://www.slate.com/blogs/how_not_to_be_wrong/2014/06/03/berkson_s_fallacy_why_are_handsome_men_such_jerks.html">Why are handsome men such jerks?</a>&#34;</li></ul>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Berkson%27s_paradox&amp;action=edit&amp;section=10" title="Edit section: External links">edit</a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://www.youtube.com/watch?v=FUD8h9JpEVQ">Numberphile: Does Hollywood ruin books?</a> – An education video on Berkson&#39;s paradox in popular culture</li></ul>
<!-- 
NewPP limit report
Parsed by mw2277
Cached time: 20221120141734
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.205 seconds
Real time usage: 0.323 seconds
Preprocessor visited node count: 587/1000000
Post‐expand include size: 3503/2097152 bytes
Template argument size: 816/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 0/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 4910/5000000 bytes
Lua time usage: 0.069/10.000 seconds
Lua memory usage: 2559384/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  154.454      1 -total
 49.08%   75.806      1 Template:Cite_journal
 40.31%   62.267      1 Template:Short_description
 22.42%   34.635      2 Template:Pagetype
  9.74%   15.043      3 Template:Main_other
  9.46%   14.614      1 Template:Reflist
  8.39%   12.960      1 Template:SDcat
  1.78%    2.753      1 Template:Short_description/lowercasecheck
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1409541-0!canonical and timestamp 20221120141733 and revision id 1122667491.
 -->
</div>
</div></div>
  </body>
</html>
