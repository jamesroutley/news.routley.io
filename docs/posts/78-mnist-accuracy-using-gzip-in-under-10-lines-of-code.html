<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jakobs.dev/solving-mnist-with-gzip/">Original</a>
    <h1>Show HN: 78% MNIST accuracy using GZIP in under 10 lines of code</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://jakobs.dev/media/gzip.png" alt="GZIP"/></p>
<p>We can &#39;solve&#39; MNIST up to ~78% accuracy with the following code-golfed obscurity:</p>
<div data-language="python"><pre><code>c <span>=</span> <span>lambda</span> z<span>:</span> <span>len</span><span>(</span>gzip<span>.</span>compress<span>(</span>z<span>.</span>tobytes<span>(</span><span>)</span><span>)</span><span>)</span>

<span>def</span> <span>ncd</span><span>(</span>x<span>,</span> y<span>)</span><span>:</span>
    <span>return</span> <span>(</span>c<span>(</span>x <span>+</span> y<span>)</span> <span>-</span> <span>min</span><span>(</span>c<span>(</span>x<span>)</span><span>,</span> c<span>(</span>y<span>)</span><span>)</span><span>)</span> <span>/</span> <span>max</span><span>(</span>c<span>(</span>x<span>)</span><span>,</span> c<span>(</span>y<span>)</span><span>)</span>

cls <span>=</span> <span>[</span><span>(</span>x<span>,</span> c<span>(</span>x<span>)</span><span>,</span> l<span>)</span> <span>for</span> x<span>,</span> l <span>in</span> training_set<span>]</span>

correct_predictions <span>=</span> <span>sum</span><span>(</span><span>[</span>np<span>.</span>array_equal<span>(</span>Counter<span>(</span>
    <span>[</span>l <span>for</span> _<span>,</span> _<span>,</span> l <span>in</span> <span>sorted</span><span>(</span><span>[</span><span>(</span>ncd<span>(</span>x1<span>,</span> x<span>)</span><span>,</span> x<span>,</span> l<span>)</span> <span>for</span> x<span>,</span> _<span>,</span> l <span>in</span> cls<span>]</span><span>,</span>
     key<span>=</span><span>lambda</span> t<span>:</span> t<span>[</span><span>0</span><span>]</span><span>)</span><span>[</span><span>:</span><span>5</span><span>]</span><span>]</span><span>)</span><span>.</span>most_common<span>(</span><span>1</span><span>)</span><span>[</span><span>0</span><span>]</span><span>[</span><span>0</span><span>]</span><span>,</span> label<span>)</span>
     <span>for</span> x1<span>,</span> label <span>in</span> test_set<span>]</span><span>)</span></code></pre></div>
<p>If you just want to see the code sample, <a href="https://github.com/Jakob-98/mono/blob/main/python/gzip_mnist/mnist_gzip.ipynb">here</a> is a link to the Jupyter Notebook containing the code to run this experiment.</p>
<p>Lets dive into why and how: yesterday while in the one-hour train ride from Eindhoven to Rotterdam, I was inspired by the post <a href="http://pepijndevos.nl/2023/07/15/chatlmza.html">text generation from data compression</a> and the (quite controversial) paper on <a href="https://aclanthology.org/2023.findings-acl.426/">parameter free text classification</a> to play around with using compression as an image classification mechanism. Previously, I worked on image compression for computer vision on the edge, so interested in applying the technique to the most seminal yet basic dataset, I attempted to use GZIP + K-NN as a classification mechanism for the MNIST (handwritten digits) dataset.</p>
<p>Breaking down the technique, it boils down to two components: GZIP and NCD (Normalized Compression Distance) as a similarity metric, and k-NN (k-Nearest Neighbors) for classification. In this approach, GZIP is essentially our tool which gives us a way to measure the complexity or information content of individual data points. NCD provides a normalized measure of how similar two data points are, based on how much more (or less) effort it takes to compress them together compared to compressing them separately.</p>
<p>For each test sample, the algorithm computes its NCD with every training sample (in our case, 100 training samples), sorts them, and selects the k smallest distances. The majority class among these k=5 closest neighbors is then predicted as the label for the test sample. As this is quite computationally expensive, I only took a subset of the test images to arrive at my accuracy measure. Of course, it would be more correct to use the full set, but I leave this an an exercise to the reader ;).</p>
<p>Here is a less obscured version of the algorithm:</p>
<div data-language="python"><pre><code><span>def</span> <span>compute_ncd</span><span>(</span>x1<span>,</span> x2<span>)</span><span>:</span>
    <span>&#34;&#34;&#34;Compute the Normalized Compression Distance (NCD) between two samples.&#34;&#34;&#34;</span>
    Cx1 <span>=</span> <span>len</span><span>(</span>gzip<span>.</span>compress<span>(</span>x1<span>.</span>tobytes<span>(</span><span>)</span><span>)</span><span>)</span>
    Cx2 <span>=</span> <span>len</span><span>(</span>gzip<span>.</span>compress<span>(</span>x2<span>.</span>tobytes<span>(</span><span>)</span><span>)</span><span>)</span>
    Cx1x2 <span>=</span> <span>len</span><span>(</span>gzip<span>.</span>compress<span>(</span><span>(</span>x1 <span>+</span> x2<span>)</span><span>.</span>tobytes<span>(</span><span>)</span><span>)</span><span>)</span>
    
    <span>return</span> <span>(</span>Cx1x2 <span>-</span> <span>min</span><span>(</span>Cx1<span>,</span> Cx2<span>)</span><span>)</span> <span>/</span> <span>max</span><span>(</span>Cx1<span>,</span> Cx2<span>)</span>

<span>print</span><span>(</span><span>&#34;Classifying test samples...&#34;</span><span>)</span>

k <span>=</span> <span>5</span>  
correct_predictions <span>=</span> <span>0</span>  
actual_labels <span>=</span> <span>[</span><span>]</span>
predicted_labels <span>=</span> <span>[</span><span>]</span>


compressed_lengths <span>=</span> <span>[</span><span>(</span>x<span>,</span> <span>len</span><span>(</span>gzip<span>.</span>compress<span>(</span>x<span>.</span>tobytes<span>(</span><span>)</span><span>)</span><span>)</span><span>,</span> label<span>)</span> <span>for</span> x<span>,</span> label <span>in</span> training_set<span>]</span>

<span>for</span> <span>(</span>x1<span>,</span> actual_label<span>)</span> <span>in</span> tqdm<span>(</span>test_set<span>[</span><span>:</span><span>100</span><span>]</span><span>)</span><span>:</span>
    
    distances <span>=</span> <span>[</span><span>(</span>compute_ncd<span>(</span>x1<span>,</span> x<span>)</span><span>,</span> label<span>)</span> <span>for</span> x<span>,</span> _<span>,</span> label <span>in</span> compressed_lengths<span>]</span>
    
    
    neighbors <span>=</span> <span>sorted</span><span>(</span>distances<span>,</span> key<span>=</span><span>lambda</span> x<span>:</span> x<span>[</span><span>0</span><span>]</span><span>)</span><span>[</span><span>:</span>k<span>]</span>
    top_k_class <span>=</span> <span>[</span>label <span>for</span> _<span>,</span> label <span>in</span> neighbors<span>]</span>
    predicted_class <span>=</span> Counter<span>(</span>top_k_class<span>)</span><span>.</span>most_common<span>(</span><span>1</span><span>)</span><span>[</span><span>0</span><span>]</span><span>[</span><span>0</span><span>]</span>
    
    
    actual_labels<span>.</span>append<span>(</span>actual_label<span>)</span>
    predicted_labels<span>.</span>append<span>(</span>predicted_class<span>)</span>
    correct_predictions <span>+=</span> <span>(</span>predicted_class <span>==</span> actual_label<span>)</span></code></pre></div>
<p><strong>Note:</strong> after writing this post, I found <a href="https://www.blackhc.net/blog/2019/mnist-by-zip/">this article</a> by Andreas Kirsch taking a similar approach back in 2019. His approach reaches around 35% accuracy.</p></div></div>
  </body>
</html>
