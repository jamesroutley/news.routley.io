<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cloud.google.com/blog/products/compute/cloud-tpu-vms-are-generally-available">Original</a>
    <h1>Cloud TPU VMs are generally available</h1>
    
    <div id="readability-page-1" class="page"><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c55=""><div _ngcontent-c55="" innerhtml="&lt;p&gt;Earlier last year, &lt;a href=&#34;https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms&#34;&gt;Cloud TPU VMs on Google Cloud were introduced&lt;/a&gt; to make it easier to use the TPU hardware by providing direct access to TPU host machines. Today, we are excited to announce the general availability (GA) of TPU VMs.&lt;br&gt;&lt;/p&gt;&lt;p&gt;With Cloud TPU VMs you can work interactively on the same hosts where the physical TPU hardware is attached. Our rapidly growing TPU user community has enthusiastically adopted this access mechanism, because it not only makes it possible to have a better debugging experience, but it also enables certain training setups such as Distributed Reinforcement Learning which were not feasible with TPU Node (networks accessed) architecture.&lt;/p&gt;&lt;h3&gt;What&amp;#8217;s new for the GA release?&lt;/h3&gt;&lt;p&gt;Cloud TPUs are now optimized for large-scale ranking and recommendation workloads. We are also thrilled to share that &lt;a href=&#34;https://eng.snap.com/&#34; target=&#34;_blank&#34;&gt;Snap&lt;/a&gt;, an early adopter of this new capability, achieved about ~4.65x perf/TCO improvement to their business-critical ad ranking workload. Here are a few highlights from Snap&amp;#8217;s blog post on &lt;a href=&#34;https://eng.snap.com/training-models-with-tpus#:~:text=finally%2C%20we%20compare%20tpus%20with%20gpus.%20&#34; target=&#34;_blank&#34;&gt;Training Large Scale Recommendation Models&lt;/a&gt;:&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;gt; TPUs can offer much faster training speed and significantly lower training costs for recommendation system models than the CPUs;&lt;br&gt;&lt;/i&gt;&lt;i&gt;&amp;gt; TensorFlow for cloud TPU provides a powerful API to handle large embedding tables and fast lookups;&lt;br&gt;&lt;/i&gt;&lt;i&gt;&amp;gt; On TPU v3-32 slice, Snap was able to get a ~3x better throughput (-67.3% throughput on A100) with 52.1% lower cost compared to an equivalent A100 configuration (~4.65x perf/TCO)&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Ranking and recommendation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;With the TPU VMs GA release, we are introducing the new &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbedding&#34; target=&#34;_blank&#34;&gt;TPU Embedding API, which &lt;/a&gt;can accelerate ML Based ranking and recommendation workloads.&lt;/p&gt;&lt;p&gt;Many businesses today are built around ranking and recommendation use-cases, such as audio/video recommendations, product recommendations (apps, e-commerce), and ad ranking. These businesses rely on ranking and recommendation algorithms to serve their users and drive their business goals. In the last few years, the approaches to these algorithms have evolved from being purely statistical to deep neural network-based. These modern DNN-based algorithms offer greater scalability and accuracy, but they can come at a cost. They tend to use large amounts of data and can be difficult and expensive to train and deploy with traditional ML infrastructure.&lt;br&gt;&lt;/p&gt;&lt;p&gt;Embedding acceleration with Cloud TPU can solve this problem at a lower cost. Embedding APIs can efficiently handle large amounts of data, such as embedding tables, by automatically sharding across hundreds of Cloud TPU chips in a pod, all connected to one another via the custom-built interconnect.&lt;/p&gt;&lt;p&gt;To help you get started, we are releasing the &lt;a href=&#34;https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding&#34; target=&#34;_blank&#34;&gt;TF2 ranking and recommendation APIs&lt;/a&gt;, as part of the &lt;a href=&#34;https://www.tensorflow.org/recommenders&#34; target=&#34;_blank&#34;&gt;Tensorflow Recommenders&lt;/a&gt; library. We have also open sourced DLRM and DCN v2 &lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official/recommendation/ranking&#34; target=&#34;_blank&#34;&gt;ranking models&lt;/a&gt; in the TF2 &lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official#recommendation&#34; target=&#34;_blank&#34;&gt;model garden&lt;/a&gt; and the detailed tutorials are available &lt;a href=&#34;https://cloud.google.com/tpu/docs/tutorials/dlrm-dcn-2.x&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Framework support&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TPU VM GA Release supports the three major frameworks (TensorFlow, PyTorch and JAX) now offered through three optimized environments for ease of setup with the respective framework. GA release has been validated with TensorFlow v2-tf-stable, &lt;a href=&#34;https://cloud.google.com/tpu/docs/supported-tpu-versions#pytorch&#34;&gt;PyTorch/XLA v1.11 &lt;/a&gt;and &lt;a href=&#34;https://pypi.org/project/jax/0.3.6/&#34; target=&#34;_blank&#34;&gt;JAX [0.3.6]&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;TPU VMs Specific Features&lt;/h3&gt;&lt;p&gt;TPU VMs offer several additional capabilities over TPU Node architecture thanks to the local execution setup, i.e. TPU hardware connected to the same host that users execute the training workload(s).&lt;/p&gt;&lt;p&gt;&lt;b&gt;Local execution of input pipeline&amp;#160;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Input data pipeline executes directly on the TPU hosts. This functionality allows saving precious computing resources earlier used in the form of instance groups for PyTorch/JAX distributed training. In the case of Tensorflow, the distributed training setup required only one user VM and data pipeline executed directly on TPU hosts.&lt;/p&gt;&lt;p&gt;The following study summarizes the cost comparison for Transformer (FairSeq; PyTorch/XLA) training executed for 10 epochs on TPU VM vs TPU Node architecture (Network attached Cloud TPUs):&lt;br&gt;&lt;/p&gt;"><p>Earlier last year, <a href="https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms" track-metadata-module="post">Cloud TPU VMs on Google Cloud were introduced</a> to make it easier to use the TPU hardware by providing direct access to TPU host machines. Today, we are excited to announce the general availability (GA) of TPU VMs.<br/></p><p>With Cloud TPU VMs you can work interactively on the same hosts where the physical TPU hardware is attached. Our rapidly growing TPU user community has enthusiastically adopted this access mechanism, because it not only makes it possible to have a better debugging experience, but it also enables certain training setups such as Distributed Reinforcement Learning which were not feasible with TPU Node (networks accessed) architecture.</p><h3>What’s new for the GA release?</h3><p>Cloud TPUs are now optimized for large-scale ranking and recommendation workloads. We are also thrilled to share that <a href="https://eng.snap.com/" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://eng.snap.com" track-metadata-module="post">Snap</a>, an early adopter of this new capability, achieved about ~4.65x perf/TCO improvement to their business-critical ad ranking workload. Here are a few highlights from Snap’s blog post on <a href="https://eng.snap.com/training-models-with-tpus#:~:text=finally%2C%20we%20compare%20tpus%20with%20gpus.%20" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://eng.snap.com" track-metadata-module="post">Training Large Scale Recommendation Models</a>:<br/></p><p><i>&gt; TPUs can offer much faster training speed and significantly lower training costs for recommendation system models than the CPUs;<br/></i><i>&gt; TensorFlow for cloud TPU provides a powerful API to handle large embedding tables and fast lookups;<br/></i><i>&gt; On TPU v3-32 slice, Snap was able to get a ~3x better throughput (-67.3% throughput on A100) with 52.1% lower cost compared to an equivalent A100 configuration (~4.65x perf/TCO)</i></p><p><b>Ranking and recommendation</b></p><p>With the TPU VMs GA release, we are introducing the new <a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbedding" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://www.tensorflow.org" track-metadata-module="post">TPU Embedding API, which </a>can accelerate ML Based ranking and recommendation workloads.</p><p>Many businesses today are built around ranking and recommendation use-cases, such as audio/video recommendations, product recommendations (apps, e-commerce), and ad ranking. These businesses rely on ranking and recommendation algorithms to serve their users and drive their business goals. In the last few years, the approaches to these algorithms have evolved from being purely statistical to deep neural network-based. These modern DNN-based algorithms offer greater scalability and accuracy, but they can come at a cost. They tend to use large amounts of data and can be difficult and expensive to train and deploy with traditional ML infrastructure.<br/></p><p>Embedding acceleration with Cloud TPU can solve this problem at a lower cost. Embedding APIs can efficiently handle large amounts of data, such as embedding tables, by automatically sharding across hundreds of Cloud TPU chips in a pod, all connected to one another via the custom-built interconnect.</p><p>To help you get started, we are releasing the <a href="https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.tensorflow.org" track-metadata-module="post">TF2 ranking and recommendation APIs</a>, as part of the <a href="https://www.tensorflow.org/recommenders" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.tensorflow.org" track-metadata-module="post">Tensorflow Recommenders</a> library. We have also open sourced DLRM and DCN v2 <a href="https://github.com/tensorflow/models/tree/master/official/recommendation/ranking" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://github.com" track-metadata-module="post">ranking models</a> in the TF2 <a href="https://github.com/tensorflow/models/tree/master/official#recommendation" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://github.com" track-metadata-module="post">model garden</a> and the detailed tutorials are available <a href="https://cloud.google.com/tpu/docs/tutorials/dlrm-dcn-2.x" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/tpu/docs/tutorials/dlrm-dcn-2.x" track-metadata-module="post">here</a>.</p><p><b>Framework support</b></p><p>TPU VM GA Release supports the three major frameworks (TensorFlow, PyTorch and JAX) now offered through three optimized environments for ease of setup with the respective framework. GA release has been validated with TensorFlow v2-tf-stable, <a href="https://cloud.google.com/tpu/docs/supported-tpu-versions#pytorch" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/tpu/docs/supported-tpu-versions#pytorch" track-metadata-module="post">PyTorch/XLA v1.11 </a>and <a href="https://pypi.org/project/jax/0.3.6/" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://pypi.org" track-metadata-module="post">JAX [0.3.6]</a>.</p><h3>TPU VMs Specific Features</h3><p>TPU VMs offer several additional capabilities over TPU Node architecture thanks to the local execution setup, i.e. TPU hardware connected to the same host that users execute the training workload(s).</p><p><b>Local execution of input pipeline </b></p><p>Input data pipeline executes directly on the TPU hosts. This functionality allows saving precious computing resources earlier used in the form of instance groups for PyTorch/JAX distributed training. In the case of Tensorflow, the distributed training setup required only one user VM and data pipeline executed directly on TPU hosts.</p><p>The following study summarizes the cost comparison for Transformer (FairSeq; PyTorch/XLA) training executed for 10 epochs on TPU VM vs TPU Node architecture (Network attached Cloud TPUs):<br/></p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----><!----></div></div>
  </body>
</html>
