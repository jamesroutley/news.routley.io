<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stytch.com/blog/detecting-ai-agent-use-abuse/">Original</a>
    <h1>Detecting AI Agent Use and Abuse</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p>AI agents aren’t just indexing your content anymore. With tools like OpenAI’s Operator, Anthropic’s Computer Use API, and BrowserBase’s Open Operator, these agents can navigate the web, mimic real users, and even take actions at scale. The challenge? Knowing whether they’re enhancing your user experience—or opening the door to abuse.</p><p>In some scenarios, apps might encourage agent use if it improves usability and adoption, but in other cases, it could present unacceptable risks for application developers or be used as a method for malicious attacks (e.g. credential stuffing or fake account creation).</p><p>In either scenario, observability is paramount. Applications need to know what traffic is on their site (is this a human? A bot? A good bot or a bad one?) in order to make intelligent decisions about how to shape traffic and enforce desired usage patterns. AI agents add an additional wrinkle as users are already sharing their credentials with tools like Operator, meaning even a well-intentioned agent creates potential risk for these applications and their users.</p><p>The key question is: <strong>Can you detect AI agent traffic on your application today?</strong></p><p>We tested multiple AI agent toolkits across high-traffic consumer sites, and the results were clear—legacy detection techniques (CAPTCHAs, IP blocking, user-agent filtering) are largely ineffective. Here’s what we found.</p><h2 id="thenewrealityofbottrafficwhataiagenttrafficlookslike">The New Reality of Bot Traffic:<strong> </strong>What AI Agent Traffic Looks Like</h2><p>Traditionally, bot detection relied on CAPTCHAs, IP blocking, and user-agent filtering. But modern AI agents are engineered to look like actual users:</p><ul><li><strong>Realistic fingerprints:</strong> They use genuine IP addresses, user agents, and even simulate mouse movements.</li><li><strong>Headless browsing at human speeds:</strong> Their interactions mimic natural browsing behavior, evading rate-limit triggers.</li><li><strong>Datacenter origins—but not always:</strong> While some (like OpenAI’s Operator) come from known Azure datacenters, others (like Anthropic’s API) can run locally, borrowing your machine’s properties.</li></ul><div><figure><img alt="Reddit blocking AI agent operators." loading="lazy" width="726" height="753" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/3jwyzebk/production/e195fa64f9235588fb217cae233f3258d7ac3e9a-726x753.png?auto=format&amp;fit=max&amp;w=750&amp;q=75 1x, https://cdn.sanity.io/images/3jwyzebk/production/e195fa64f9235588fb217cae233f3258d7ac3e9a-726x753.png?auto=format&amp;fit=max&amp;w=1920&amp;q=75 2x" src="https://cdn.sanity.io/images/3jwyzebk/production/e195fa64f9235588fb217cae233f3258d7ac3e9a-726x753.png?auto=format&amp;fit=max&amp;w=1920&amp;q=75"/><figcaption></figcaption></figure></div><p>On the surface level, AI agent traffic can look quite similar to regular human user traffic. Here are examples of browser &amp; network properties across different AI agents:</p><table><thead><tr><td></td><th><h4>Open AI Operator</h4></th><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr><td colspan="3"><h4>User Agent</h4></td></tr><tr><td><h4>User Agent</h4></td><td><p>Chrome on Linux (standard fingerprint)</p></td><td><p>Firefox on Ubuntu (stable, unless run locally)</p></td><td><p>Chrome on Mac (with stealth features)</p></td></tr><tr><td colspan="3"><h4>IP Address</h4></td></tr><tr><td><h4>IP Address</h4></td><td><p>Known Azure Datacenter IP</p></td><td><p>Varies—depends on local vs. cloud deployment</p></td><td><p>Known AWS Datacenter IP</p></td></tr><tr><td colspan="3"><h4>Browser Version</h4></td></tr><tr><td><h4>Browser Version</h4></td><td><p>Chrome 130</p></td><td><p>Firefox 128</p></td><td><p>Chrome 124</p></td></tr><tr><td colspan="3"><h4>Location</h4></td></tr><tr><td><h4>Location</h4></td><td><p>San Francisco, California</p></td><td><p>San Jose, California</p></td><td><p>Boardman, Oregon</p></td></tr><tr><td colspan="3"><h4>ASN</h4></td></tr><tr><td><h4>ASN</h4></td><td><p>8075</p></td><td><p>398391</p></td><td><p>16509</p></td></tr><tr><td colspan="3"><h4>ASN Name</h4></td></tr><tr><td><h4>ASN Name</h4></td><td><p>Microsoft</p></td><td><p>Perimeter 81</p></td><td><p>Amazon</p></td></tr><tr><td colspan="3"><h4>Detection Difficulty</h4></td></tr><tr><td><h4>Detection Difficulty</h4></td><td><p>Easy</p></td><td><p>Harder</p></td><td><p>Moderate</p></td></tr></tbody></table><p>Both OpenAI’s Operator and BrowserBase’s Open Operator spin up <strong>remote datacenter or container‑hosted</strong> Chromium instances, rather than installing any software on your local machine. Because these solutions originate from a cloud‑hosted environment (with IP addresses and other signatures tied to the provider’s infrastructure), they can be <strong>easier to detect</strong> via methods like ASN lookups if the provider’s IP ranges are well known.</p><p>By contrast, Anthropic’s “Computer Use” functionality is available only through its API as of now, and you can choose how to run that API (e.g., directly on your own machine vs. inside a VM or container). If you run it locally, it inherits your local system’s properties like IP and ASN; if you host it in the cloud, it uses whichever environment that provider offers. Each approach has its own implications for fingerprinting and detection.</p><p>Currently, Anthropic provides two options for using their agent:</p><ol><li>Direct use of your computer environment (not recommended due to security concerns around unrestricted access).</li><li><a rel="noreferrer noopener" href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo">Setting up a virtual machine or container</a> with minimal privileges to run the API. You can either run this locally or deploy to a cloud provider using Docker, in which case it won’t inherit the user’s network properties.</li></ol><p>Whereas stricter sites like Reddit block OpenAI and BrowserBase outright, Anthropic’s approach allows it to successfully bypass even these strict sites when run locally:</p><div><figure><img alt="Reddit not detecting Anthropic Computer Use" loading="lazy" width="1999" height="1052" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/3jwyzebk/production/8f0baeec42be860a2e92d0b61501865cdc8f98fe-1999x1052.png?auto=format&amp;fit=max&amp;w=2048&amp;q=75 1x, https://cdn.sanity.io/images/3jwyzebk/production/8f0baeec42be860a2e92d0b61501865cdc8f98fe-1999x1052.png?auto=format&amp;fit=max&amp;w=3840&amp;q=75 2x" src="https://cdn.sanity.io/images/3jwyzebk/production/8f0baeec42be860a2e92d0b61501865cdc8f98fe-1999x1052.png?auto=format&amp;fit=max&amp;w=3840&amp;q=75"/><figcaption>Certain deployments of Anthropic’s approach make it harder for sites like Reddit and Youtube, which block OpenAI Operator, to detect this traffic.</figcaption></figure></div><p>And with tools like BrowserBase, there are also now open-sourced options for building browsing AI agents that allow potential attackers to increase the stealth of their headless browsing setup. This means we should expect continued, quick iteration on some agent use cases that will make them even more difficult to detect. As an example, BrowserBase offers premium plans with more advanced stealth to bypass CAPTCHA and other detection techniques:</p><div><figure><img alt="BrowserBase pricing plans." loading="lazy" width="1365" height="882" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/3jwyzebk/production/ac02c236c4b8f431348c6e0ec0505dd2042518ab-1365x882.png?auto=format&amp;fit=max&amp;w=1920&amp;q=75 1x, https://cdn.sanity.io/images/3jwyzebk/production/ac02c236c4b8f431348c6e0ec0505dd2042518ab-1365x882.png?auto=format&amp;fit=max&amp;w=3840&amp;q=75 2x" src="https://cdn.sanity.io/images/3jwyzebk/production/ac02c236c4b8f431348c6e0ec0505dd2042518ab-1365x882.png?auto=format&amp;fit=max&amp;w=3840&amp;q=75"/><figcaption>Premium plans on BrowserBase come with more advanced stealth features</figcaption></figure></div><p>Today, most sites are allowing agent traffic to navigate freely – whether these sites are detecting the abnormalities of this traffic (and choosing not to take action) is hard to say. For some like Reddit &amp; Youtube, their block on OpenAI Operator indicates they want to lock down agentic traffic. When these AI Agents bypass their restrictions (as detailed below), it’s a good signal those tools are actually flying below their radar today versus being officially sanctioned.</p><p>For others that allow this traffic, they may actually encourage it for user convenience or potentially have perverse incentives in some cases to allow a certain amount of bot traffic (Ticketmaster is a potential example of this given they can benefit from some level of bot activity increasing ticket purchase rates). Many more sites, however, simply don’t have the traffic intelligence to detect these AI agents, which explains why they can operate freely.</p><p>In testing the three primary AI Agent browsing toolkits on a set of high traffic consumer sites, we found that Youtube and Reddit were the only ones that consistently blocked this traffic:</p><table><thead><tr><td></td><th><h4>OpenAI Operator</h4></th><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr><td colspan="3"><h4>Youtube</h4></td></tr><tr><td><h4>Youtube</h4></td><td><p>❌</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>Reddit</h4></td></tr><tr><td><h4>Reddit</h4></td><td><p>❌</p></td><td><p>✅</p></td><td><p>❌</p></td></tr><tr><td colspan="3"><h4>LinkedIn</h4></td></tr><tr><td><h4>LinkedIn</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>Twitter/X</h4></td></tr><tr><td><h4>Twitter/X</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>Facebook.com</h4></td></tr><tr><td><h4>Facebook.com</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>OpenTable</h4></td></tr><tr><td><h4>OpenTable</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>Pinterest</h4></td></tr><tr><td><h4>Pinterest</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr><tr><td colspan="3"><h4>Nike</h4></td></tr><tr><td><h4>Nike</h4></td><td><p>✅</p></td><td><p>✅</p></td><td><p>✅</p></td></tr></tbody></table><p>Few websites are blocking popular AI Agents today – indicating either a lack of detection, ambivalence towards enforcement, or both. Reddit &amp; Youtube are a couple of key outliers that block this traffic when they can discern it.</p><p>Still, the differences are interesting. It’s worth understanding why Anthropic or BrowserBase are sometimes able to bypass sites that otherwise are blocking this traffic. In BrowserBase’s case, they generate a slightly different user-agent each session, which sometimes aligns with the underlying chromium runtime but sometimes attempts to be deceptive by claiming to be a different version, which leads to more inconsistency in whether companies detect it correctly.</p><p>Companies like LinkedIn and X are particularly sensitive to scraping on their sites, so the lack of enforcement is surprising. This suggests they’re not currently able to confidently discern this is programmatic traffic.</p><h2 id="howtotellanagentfromahuman">How to Tell an Agent from a Human</h2><p>The obvious question here is: <strong>shouldn&#39;t this be easy to detect by using just an IP or user agent?</strong></p><p>And the answer is yes and no. For OpenAI Operator, it’s pretty straightforward to detect given its stability &amp; origin point. For Anthropic and BrowserBase (and the new agents entering the market), it’s a bit more complicated.</p><table><thead><tr><td></td><th><h4>OpenAI Operator</h4></th><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr><td colspan="3"><h4>Browser Stack</h4></td></tr><tr><td><h4>Browser Stack</h4></td><td><p>Chromium</p></td><td><p>Firefox</p></td><td><p>Chromium</p></td></tr><tr><td colspan="3"><h4>IP address properties</h4></td></tr><tr><td><h4>IP address properties</h4></td><td><p>Known Azure Datacenter</p></td><td><div variant="listItem"><p>Depends on how you run it:</p><ul><li>Unlike Operator, you can run it locally directly on your machine or (recommended) on a dockerized container. In this case, it inherits your own device’s IP &amp; characteristics</li><li>In production, you’d likely host it in AWS/Azure/etc and traffic would come from known data centers</li></ul></div></td><td><p>Known AWS Datacenter</p></td></tr><tr><td colspan="3"><h4>IP stability</h4></td></tr><tr><td><h4>IP stability</h4></td><td><p>Remains stable</p></td><td><p>Remains stable</p></td><td><p>Churns</p></td></tr><tr><td colspan="3"><h4>User Agent</h4></td></tr><tr><td><h4>User Agent</h4></td><td><p>Remains stable</p></td><td><p>Remains stable</p></td><td><p>Churns</p></td></tr><tr><td colspan="3"><h4>CAPTCHA handling</h4></td></tr><tr><td><h4>CAPTCHA handling</h4></td><td><p>Hands off to user</p></td><td><p>Hands off to user</p></td><td><p>Offers options for automatically solving</p></td></tr></tbody></table><p>Some key elements worth knowing:</p><ul><li>OpenAI &amp; BrowserBase both operate on top of chromium – being able to discern chromium browsers vs. true chrome is key for detecting the latter since BrowserBase modifies things like IP, user agent, etc for increased stealth.</li><li>OpenAI originates out of a known Azure datacenter while BrowserBase originates from a known AWS datacenter.</li><li>OpenAI hands off non-invisible CAPTCHAs to the end user to complete, while BrowserBase offers options for automatically solving.</li></ul><p>Ideally, you want to detect this traffic beyond more coarse identifiers like known datacenter IPs so that your detection is fully resistant to adjustments agentic traffic will make in the future. And core to that is having high confidence in whether a browser interacting with your site is a real browser or a headless instrumentation.</p><p>This is where we’ve personally found Machine Learning to be incredibly useful by building a browser lie detector. We look at a very wide range of browser signals &amp; we have downloaded the entire historical archives of every browser version that has been released that we can find and trained our model on what authentic browsers look like vs. anomalous or deceitful browsers. </p><p>To provide this visibility, we’ve automated additions of new legitimate browser signatures upon new releases, we fingerprint it, run it past the model to establish an authentic set of signals for the current browser version, allowing us to detect anyone trying to emulate that version inorganically. With that fingerprinted dataset, machine learning can be used to detect anomalies with high accuracy. It’s this signal collection that has allowed us to see that OpenAI &amp; BrowserBase are full chromium builds, while Anthropic’s Firefox browser fails to emulate certain characteristics that would be present on a real user’s Firefox instance.</p><h2 id="chartingyourpathforward"><strong>Charting Your Path Forward</strong></h2><p>Whether you choose to block, restrict, or harness AI agent traffic, effective observability is non-negotiable. Here’s our current playbook:</p><ul><li><strong>Monitor actively:</strong> Start by detecting and monitoring AI agent fingerprints. Learn from their behavior before enforcing hard limits.</li><li><strong>Embrace legitimate use cases:</strong> Some users may leverage AI agents to streamline workflows (e.g., automating reporting on your dashboard). Recognize these opportunities while safeguarding against abuse.</li><li><strong>Iterate quickly:</strong> As AI agents evolve, so must your detection strategies. Invest in ML-based solutions that adapt alongside new agent behaviors.</li></ul><p>AI agents are rewriting the rules of web interaction. They offer exciting UX advancements—but also present new security challenges. The race is on: evolve your detection or risk being outpaced by bad actors and unintended misuse.</p><p>If you’re interested in learning more about how to detect these agents &amp; enforce certain behaviors, <a rel="noreferrer noopener" href="https://stytch.com/contact">reach out to us to chat more</a> (or take a look at the <a rel="noreferrer noopener" href="https://stytch.com/docs/fraud">fingerprinting techniques we’re using to detect them</a>).</p><div><div><div><div><div><h3>Try Device Fingerprinting</h3><p>99.99% bot detection, intelligent rate limiting, and reverse engineering protection.</p></div><p><a href="https://stytch.com/docs/fraud">Read the docs</a></p></div></div></div></div></div></div></div></div>
  </body>
</html>
