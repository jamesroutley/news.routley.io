<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.thealgorithmicbridge.com/p/google-is-winning-on-every-ai-front">Original</a>
    <h1>Google Is Winning on Every AI Front</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><span>Even in my most bullish days for OpenAI, I secretly preferred DeepMind. I felt Demis Hassabis was trustworthy in a way Sam Altman couldn&#39;t be—a true scientist, not a businessman. Also, </span><a href="https://deepmind.google/research/breakthroughs/alphago/" rel="">AlphaGo</a><span> and </span><a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/" rel="">AlphaZero</a><span>. To me, they&#39;re not historical milestones but nostalgia. ChatGPT is cool, but do you remember </span><em><a href="https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/" rel="">move 37</a></em><span>? And the </span><a href="https://www.youtube.com/watch?v=ZHfumZVPjVA&amp;list=PLDnx7w_xuguFDbrYDxxvPH-aoQkEX0rHv&amp;ab_channel=agadmator%27sChessChannel" rel="">AlphaZero-Stockfish 8 chess games</a><span>? My love and interest for AI grew parallel to DeepMind’s successes. I was rooting, almost like a sports fan, for them.</span></p><p><span>So, for years, I’ve been low-key saddened by their constant </span><em>fumbling</em><span>. They had the tech, the talent, the money, the infrastructure, the prestige, and the conviction to make ChatGPT—or whatever else they wanted—before OpenAI. They didn&#39;t. CEO Sundar Pichai was afraid to thwart Google’s main revenue source (search and ads). He chose prudence over boldness. Good—they didn’t shoot themselves in the foot.</span></p><p><span>Because they didn’t shoot </span><em>at all.</em></p><p>But that was the last mistake they made. Today, two and a half years after the ChatGPT debacle, Google DeepMind is winning. They are winning so hard right now that they’re screaming, “Please, please, we can’t take it anymore, it’s too much winning!” No, but really—I wonder if the only reason OpenAI, Anthropic, Meta, and Co. ever had the slightest chance to win is because Google fumbled that one time. They don’t anymore.</p><p><span>I’d been holding off on writing about Gemini 2.5. Focusing on the AI model didn’t feel like enough to tell the full story of Google’s comeback. Gemini 2.5 is only a piece—albeit a big one—of something much larger. Back in December 2024, I said they would </span><a href="https://www.thealgorithmicbridge.com/p/20-predictions-for-ai-in-2025" rel="">come out on top by the end of 2025</a><span>. We’re not even halfway there and it’s already happened. (For reasons I still don’t understand, some people genuinely thought </span><a href="https://polymarket.com/event/which-company-has-best-ai-model-end-of-april?tid=1744284231922" rel="">xAI had a shot</a><span>.)</span></p><p>Anyway, to avoid turning this post into an over-stylized narrative—which I do more often than I’d like—I’m keeping it to bullet points. It hits harder that way. You’ll see what I mean when the list just... doesn’t end.</p><p>Google and DeepMind fans: enjoy the long-overdue rebirth.</p><ul><li><p><a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking" rel="">Gemini 2.5 Pro Experimental</a><span> is the best model in the world. Number one on the </span><a href="https://lmarena.ai/?leaderboard" rel="">LMArena</a><span>, </span><a href="https://x.com/EpochAIResearch/status/1907519991252992508" rel="">GPQA Diamond</a><span>, </span><a href="https://scale.com/leaderboard/humanitys_last_exam" rel="">Humanity&#39;s Last Exam</a><span>, and AIME (math competition). It&#39;s also the best on private benchmarks like </span><a href="https://aider.chat/docs/leaderboards/" rel="">Aider Polyglot</a><span> (code), </span><a href="https://livebench.ai/#/" rel="">Live Bench</a><span> (diverse). It&#39;s better than Claude Sonnet at </span><a href="https://m.twitch.tv/gemini_plays_pokemon" rel="">playing Pokemon</a><span> (a promising agentic playground) and rising quickly on </span><a href="https://mcbench.ai/leaderboard" rel="">Minecraft Bench</a><span> (some </span><a href="https://x.com/wintermoat/status/1909787050640920894" rel="">examples</a><span>). It is decent at </span><a href="https://x.com/emollick/status/1904656593083396541" rel="">creative writing</a><span> (e.g., </span><a href="https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87" rel="">long-context comprehension</a><span>), which has been an elusive milestone for years.</span></p></li><li><p><span>Perhaps most importantly, the benchmark scores match the signal I receive from vibes checks, high-taste testers, and firsthand testimonials: </span><em><a href="https://x.com/MatthewBerman/status/1904714953095078004" rel="">people</a><span> </span><a href="https://www.reddit.com/r/Bard/s/TTXsRTWJdA" rel="">are</a></em><span> </span><em><a href="https://x.com/emollick/status/1910208856480768283" rel="">reporting</a><span> </span><a href="https://artificialanalysis.ai/models/gemini-2-5-pro?models=o1%2Cgpt-4o%2Co3-mini-high%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-35-sonnet%2Cclaude-3-7-sonnet%2Cdeepseek-r1%2Cgrok-beta#intelligence" rel="">en masse</a><span> </span></em><a href="https://x.com/cgarciae88/status/1907457306947702925" rel="">that</a><span> </span><a href="https://www.reddit.com/r/ClaudeAI/s/30nIFOtVMP" rel="">Gemini</a><span> </span><a href="https://www.reddit.com/r/GeminiAI/s/ihTsYvDxIY" rel="">2.5</a><span> </span><a href="https://www.reddit.com/r/ClaudeAI/s/khkte9Bq49" rel="">Pro</a><span> </span><a href="https://x.com/xf1280/status/1904587791868322036" rel="">is</a><span> </span><a href="https://x.com/emollick/status/1909748270249001248" rel="">indeed</a><span> </span><a href="https://x.com/daniel_mac8/status/1910058485150269797" rel="">the</a><span> </span><a href="https://x.com/petergyang/status/1906007718961492391" rel="">best</a><span> </span><a href="https://www.reddit.com/r/ChatGPTCoding/s/O3qJuHRsWA" rel="">model</a><span> </span><a href="https://www.reddit.com/r/GeminiAI/s/aLDvqH12oq" rel="">today</a><span>. A rare sight to witness. (Watch </span><a href="https://x.com/MatthewBerman/status/1904715099862098109" rel="">Matthew Berman’s clip</a><span> below.)</span></p></li><li><p><span>And that&#39;s just pure performance. Add to the above that Gemini 2.5, compared to models of its category, is </span><a href="https://artificialanalysis.ai/models/gemini-2-5-pro?models=o1%2Cgpt-4o%2Co3-mini-high%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-35-sonnet%2Cclaude-3-7-sonnet%2Cdeepseek-r1%2Cgrok-beta#speed" rel="">fast</a><span> and </span><a href="https://artificialanalysis.ai/models/gemini-2-5-pro?models=o1%2Cgpt-4o%2Co3-mini-high%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-35-sonnet%2Cclaude-3-7-sonnet%2Cdeepseek-r1%2Cgrok-beta#pricing" rel="">cheap</a><span>—I mean, they&#39;re </span><a href="https://x.com/sundarpichai/status/1908173216499093625" rel="">giving away free access</a><span>!—has a gigantic </span><a href="https://artificialanalysis.ai/models/gemini-2-5-pro?models=o1%2Cgpt-4o%2Co3-mini-high%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-35-sonnet%2Cclaude-3-7-sonnet%2Cdeepseek-r1%2Cgrok-beta#context-window" rel="">context window of 1 million tokens</a><span> (only recently surpassed by Meta’s Llama 4) and it’s connected to the entire Google suite of products (more on that soon).</span></p></li><li><p><span>Gemini 2.5 Flash (</span><a href="https://techcrunch.com/2025/04/09/googles-newest-gemini-ai-model-focuses-on-efficiency/" rel="">launching soon</a><span>) is Gemini 2.5 Pro’s little brother. It is just as amazing but for a different reason. Together with old versions of Flash (</span><a href="https://deepmind.google/technologies/gemini/flash/" rel="">2.0</a><span>, </span><a href="https://deepmind.google/technologies/gemini/flash-lite/" rel="">Lite</a><span>, </span><a href="https://deepmind.google/technologies/gemini/flash-thinking/" rel="">Thinking</a><span>), it will be </span><a href="https://artificialanalysis.ai/models/gemini-2-0-flash#speed" rel="">extremely fast</a><span> and </span><a href="https://x.com/deedydas/status/1883355957838897409" rel="">extremely cheap</a><span> (much cheaper than comparable models from competitors, </span><a href="https://artificialanalysis.ai/models/gemini-2-0-flash?models=gemini-2-0-flash%2Cdeepseek-r1%2Cdeepseek-v3&amp;endpoints=#pricing" rel="">even DeepSeek’s</a><span>, famous for </span><a href="https://x.com/EMostaque/status/1881310721746804810" rel="">being cheap</a><span>). They&#39;re also small, which makes them perfect for edge applications and phone integration.</span></p></li><li><p><span>Then there’s </span><a href="https://blog.google/technology/developers/gemma-3/" rel="">Gemma 3</a><span>—Google’s open source model, </span><a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf" rel="">competitive with the best open-source models</a><span>, including Llama 4 (</span><a href="https://x.com/jeremyphoward/status/1908607345393098878" rel="">too big</a><span>) and DeepSeek-V3.</span></p></li><li><p><a href="https://x.com/swyx/status/1908215411214344669" rel="">Swyx plotted a graph</a><span> revealing that “Google owns the Pareto frontier” with Gemini 2.0/2.5 (both Pro and Flash) on the two most important metrics: performance and cost. Not only that, but Google&#39;s most performant models </span><em>remain cost-effective,</em><span> and its most cost-effective models </span><em>remain performant</em><span>. Google is bullying the competition at this point; Gemini is off the charts, literally.</span></p></li><li><p><span>The LLMs that underlie chatbots like Gemini and ChatGPT are the main attraction but far from the only one. Google dominates the other generative AI areas just as clearly as it dominates text-based models. They announced they will integrate the other AI tools they have into </span><a href="https://cloud.google.com/blog/products/ai-machine-learning/expanding-generative-media-for-enterprise-on-vertex-ai" rel="">Vertex AI</a><span>: </span><a href="https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/" rel="">Lyria</a><span> (music), </span><a href="https://deepmind.google/technologies/imagen-3/" rel="">Imagen 3</a><span> (image), </span><a href="https://deepmind.google/technologies/veo/veo-2/" rel="">Veo 2</a><span> (video), and </span><a href="https://cloud.google.com/text-to-speech/docs/chirp3-hd" rel="">Chirp 3</a><span> (voice/speech). These are, in a way, side-projects for Google. Still, they’re world-class in their respective categories. Arguably, </span><a href="https://openai.com/index/introducing-4o-image-generation/" rel="">ChatGPT’s image generation</a><span> is better, but Veo 2, for instance, compelled me to write an article I entitled “</span><a href="https://www.thealgorithmicbridge.com/p/you-must-see-how-far-ai-video-has" rel="">You Must See How Far AI Video Has Come</a><span>,” whereas </span><a href="https://www.thealgorithmicbridge.com/p/openai-sora-turbo-a-very-expensive" rel="">OpenAI Sora is pure slop</a><span>. (The clip below is from Veo 2.)</span></p></li><li><p><span>What about agents? Gemini 2.5 Pro in </span><a href="https://x.com/demishassabis/status/1910143772635078754" rel="">Deep Research mode</a><span> it&#39;s </span><em><a href="https://x.com/AdvaitOnline/status/1909721368259960957" rel="">twice as good</a></em><span> as OpenAI’s Deep Research (until now, widely considered the most important AI product for investigative work). Google DeepMind is further cooking with </span><a href="https://deepmind.google/technologies/project-astra/" rel="">Project Astra</a><span> (assistant) and </span><a href="https://deepmind.google/technologies/project-mariner/" rel="">Project Mariner</a><span> (computer interaction).</span></p></li><li><p><span>If that wasn’t enough, they’ve built an </span><a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/" rel="">Agent2Agent protocol</a><span> complementary to and compatible with the </span><a href="https://www.anthropic.com/news/model-context-protocol" rel="">Model Context Protocol</a><span>, which they </span><em><a href="https://x.com/demishassabis/status/1910107859041271977" rel="">will also build</a></em><span> after listening to the community’s feedback. I can’t take any more </span><em>mogging.</em></p></li><li><p><span>To top it off, they keep chugging high-quality papers to </span><a href="https://www.nature.com/nature-index/institution-outputs/United%20Kingdom%20%28UK%29/Google%20DeepMind/54ee76b8140ba0f7058b4567" rel="">publish in Nature</a><span> or present at </span><a href="https://deepmind.google/discover/blog/google-deepmind-at-iclr-2024/" rel="">ICLR</a><span> or </span><a href="https://deepmind.google/discover/blog/google-deepmind-at-neurips-2024/" rel="">NeurIPS</a><span> or some other world-class journal or conference. And if by chance that doesn’t happen, you can either way tell they take both </span><a href="https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/" rel="">AGI efforts</a><span> as well as </span><a href="https://www.kaggle.com/whitepaper-prompt-engineering" rel="">industrial AI</a><span> very seriously.</span></p></li></ul><p>Is that all? Not really. Let&#39;s not forget that Google is a consumer software company as much as an AI company. They build better models than OpenAI and Anthropic, but they do plenty of other things no one else can do.</p><p><em><strong>Hello friend!</strong></em></p><p><em><strong>Before you read on, a quick note: I write this newsletter in an attempt to understand AI and offer that understanding to others who may find themselves similarly disoriented (who isn’t these days…)</strong></em></p><p><em><strong>The project continues thanks to a small group of generous readers who support it with ~$2/week (ChatGPT costs twice as much!). If you find value here—or simply wish for this quiet effort to persist—you are most welcome to join them.</strong></em></p><p><em><strong>If you already have, my sincere thanks. This exists because of you.</strong></em></p></div></div></div>
  </body>
</html>
