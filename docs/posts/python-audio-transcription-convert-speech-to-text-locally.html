<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pavlinbg.com/posts/python-speech-to-text-guide">Original</a>
    <h1>Show HN: Python Audio Transcription: Convert Speech to Text Locally</h1>
    
    <div id="readability-page-1" class="page"><article> <p>Last week, I faced a dilemma that many researchers, journalists, and content creators know all too well: I had hours of recordings that needed to be transcribed. I had serious privacy concerns about uploading sensitive content to commercial transcription services and their third-party servers.</p>
<p>Instead of risking it, I built a Python-based transcription system using OpenAI‚Äôs Whisper model. The result? All my audio files were transcribed in under 10 minutes with 96% accuracy‚Äîcompletely free and processed locally on my laptop.</p>
<p>In this post, I will show you how you can build a simple script for processing any audio data without recurring costs or privacy compromises.</p>
<h2 id="essential-setup-requirements">Essential Setup Requirements</h2>
<h3 id="1-ffmpeg-installation-critical-first-step">1. FFmpeg Installation (Critical First Step)</h3>
<p>FFmpeg handles audio processing and is required for all transcription methods. <strong>This is the #1 cause of setup failures.</strong></p>
<div id="note-uspausjbe" role="note" aria-labelledby="note-uspausjbe-title"> <div>  <div> <h4 id="note-uspausjbe-title"> <span> ‚ö†Ô∏è </span> Setup Priority </h4> <p>Install FFmpeg FIRST before any Python packages. Most transcription errors stem from missing or misconfigured FFmpeg. Don&#39;t skip this step‚Äîit will save you hours of debugging later.</p>  </div>  </div> </div> 
<p><strong>Windows:</strong></p>
<ol>
<li>Download from <a href="https://ffmpeg.org/download.html">ffmpeg.org/download.html</a></li>
<li>Extract to <code>C:\ffmpeg</code></li>
<li>Add <code>C:\ffmpeg\bin</code> to your PATH environment variable</li>
<li>Restart your terminal</li>
</ol>
<p><strong>macOS:</strong></p>
<pre tabindex="0" data-language="bash"><code><span><span># Using Homebrew (recommended)</span></span>
<span><span>brew</span><span> install</span><span> ffmpeg</span></span></code></pre>
<p><strong>Linux (Ubuntu/Debian):</strong></p>
<pre tabindex="0" data-language="bash"><code><span><span>sudo</span><span> apt</span><span> update</span><span> &amp;&amp; </span><span>sudo</span><span> apt</span><span> install</span><span> ffmpeg</span></span></code></pre>
<p><strong>Verify Installation:</strong></p>
<pre tabindex="0" data-language="bash"><code><span><span>ffmpeg</span><span> -version</span></span></code></pre>
<p>You should see version information. If you get ‚Äúcommand not found,‚Äù FFmpeg isn‚Äôt properly installed.</p>
<h3 id="2-python-environment-setup">2. Python Environment Setup</h3>
<div id="note-kse96golw" role="note" aria-labelledby="note-kse96golw-title"> <div>  <div> <h4 id="note-kse96golw-title"> <span> üîß </span> Virtual Environment Benefits </h4> <p>Using a virtual environment prevents package conflicts, keeps your system Python clean, and makes your setup reproducible across different machines. It&#39;s a best practice that will save you from dependency hell.</p>  </div>  </div> </div> 
<pre tabindex="0" data-language="bash"><code><span><span># Create isolated environment</span></span>
<span><span>python</span><span> -m</span><span> venv</span><span> whisper-env</span></span>
<span><span>cd</span><span> whisper-env</span></span>
<span></span>
<span><span># Activate environment</span></span>
<span><span># Windows:</span></span>
<span><span>Scripts\activate</span></span>
<span><span># macOS/Linux:</span></span>
<span><span>source</span><span> bin/activate</span></span>
<span></span>
<span><span># Install required packages</span></span>
<span><span>pip</span><span> install</span><span> openai-whisper</span></span></code></pre>
<h2 id="method-1-openai-whisper-recommended">Method 1: OpenAI Whisper (Recommended)</h2>
<p>Whisper is OpenAI‚Äôs state-of-the-art speech recognition model, trained on 680,000 hours of multilingual audio. It‚Äôs specifically designed for robust, real-world audio transcription and handles various accents, background noise, and audio quality issues remarkably well.</p>
<h3 id="choosing-the-right-whisper-model">Choosing the Right Whisper Model</h3>
<div id="note-0xi8mzwah" role="note" aria-labelledby="note-0xi8mzwah-title"> <div>  <div> <h4 id="note-0xi8mzwah-title"> <span> üéØ </span> Model Selection Guide </h4> <p>Start with &#39;base&#39; model for most use cases. It offers the best balance of speed, accuracy, and resource usage for typical projects. Only upgrade to &#39;small&#39; or &#39;medium&#39; if you specifically need higher accuracy and have the computational resources.</p>  </div>  </div> </div> 





















































<table><thead><tr><th>Model</th><th>Size</th><th>RAM Required</th><th>Speed</th><th>Accuracy</th><th>Best Use Case</th></tr></thead><tbody><tr><td>tiny</td><td>39 MB</td><td>390 MB</td><td>32x realtime</td><td>89%</td><td>Quick testing, real-time applications</td></tr><tr><td>base</td><td>74 MB</td><td>740 MB</td><td>16x realtime</td><td>94%</td><td><strong>General use (recommended)</strong></td></tr><tr><td>small</td><td>244 MB</td><td>2.4 GB</td><td>6x realtime</td><td>96%</td><td>High-quality transcription needs</td></tr><tr><td>medium</td><td>769 MB</td><td>5 GB</td><td>2x realtime</td><td>97%</td><td>Professional work, critical accuracy</td></tr><tr><td>large</td><td>1.5 GB</td><td>10 GB</td><td>1x realtime</td><td>98%</td><td>Maximum accuracy, research purposes</td></tr></tbody></table>
<h3 id="basic-whisper-implementation">Basic Whisper Implementation</h3>
<p>Here‚Äôs a clean, production-ready implementation:</p>
<pre tabindex="0" data-language="python"><code><span><span>import</span><span> whisper</span></span>
<span><span>import</span><span> os</span></span>
<span><span>from</span><span> pathlib </span><span>import</span><span> Path</span></span>
<span><span>import</span><span> time</span></span>
<span></span>
<span><span>class</span><span> AudioTranscriber</span><span>:</span></span>
<span><span>    def</span><span> __init__</span><span>(self, model_size</span><span>=</span><span>&#34;base&#34;</span><span>):</span></span>
<span><span>        &#34;&#34;&#34;Initialize transcriber with specified Whisper model&#34;&#34;&#34;</span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;Loading Whisper </span><span>{</span><span>model_size</span><span>}</span><span> model...&#34;</span><span>)</span></span>
<span><span>        self</span><span>.model </span><span>=</span><span> whisper.load_model(model_size)</span></span>
<span><span>        print</span><span>(</span><span>&#34;Model loaded successfully!&#34;</span><span>)</span></span>
<span><span>    </span></span>
<span><span>    def</span><span> transcribe_file</span><span>(self, audio_path, language</span><span>=</span><span>None</span><span>):</span></span>
<span><span>        &#34;&#34;&#34;</span></span>
<span><span>        Transcribe a single audio file</span></span>
<span><span>        </span></span>
<span><span>        Args:</span></span>
<span><span>            audio_path: Path to audio file</span></span>
<span><span>            language: Language code (&#39;en&#39;, &#39;es&#39;, &#39;fr&#39;, etc.) or None for auto-detect</span></span>
<span><span>        &#34;&#34;&#34;</span></span>
<span><span>        if</span><span> not</span><span> os.path.exists(audio_path):</span></span>
<span><span>            raise</span><span> FileNotFoundError</span><span>(</span><span>f</span><span>&#34;Audio file not found: </span><span>{</span><span>audio_path</span><span>}</span><span>&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;Transcribing: </span><span>{</span><span>Path(audio_path).name</span><span>}</span><span>&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        start_time </span><span>=</span><span> time.time()</span></span>
<span><span>        </span></span>
<span><span>        # Transcribe audio</span></span>
<span><span>        options </span><span>=</span><span> {</span><span>&#34;language&#34;</span><span>: language} </span><span>if</span><span> language </span><span>else</span><span> {}</span></span>
<span><span>        result </span><span>=</span><span> self</span><span>.model.transcribe(audio_path, </span><span>**</span><span>options)</span></span>
<span><span>        </span></span>
<span><span>        processing_time </span><span>=</span><span> time.time() </span><span>-</span><span> start_time</span></span>
<span><span>        </span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;‚úì Completed in </span><span>{</span><span>processing_time</span><span>:.1f</span><span>}</span><span> seconds&#34;</span><span>)</span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;‚úì Detected language: </span><span>{</span><span>result[</span><span>&#39;language&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        return</span><span> {</span></span>
<span><span>            &#39;text&#39;</span><span>: result[</span><span>&#39;text&#39;</span><span>].strip(),</span></span>
<span><span>            &#39;language&#39;</span><span>: result[</span><span>&#39;language&#39;</span><span>],</span></span>
<span><span>            &#39;segments&#39;</span><span>: result.get(</span><span>&#39;segments&#39;</span><span>, []),</span></span>
<span><span>            &#39;processing_time&#39;</span><span>: processing_time</span></span>
<span><span>        }</span></span>
<span><span>    </span></span>
<span><span>    def</span><span> save_transcription</span><span>(self, result, output_path):</span></span>
<span><span>        &#34;&#34;&#34;Save transcription to text file&#34;&#34;&#34;</span></span>
<span><span>        with</span><span> open</span><span>(output_path, </span><span>&#39;w&#39;</span><span>, </span><span>encoding</span><span>=</span><span>&#39;utf-8&#39;</span><span>) </span><span>as</span><span> f:</span></span>
<span><span>            f.write(</span><span>&#34;=== Transcription Results ===</span><span>\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(</span><span>f</span><span>&#34;Language: </span><span>{</span><span>result[</span><span>&#39;language&#39;</span><span>]</span><span>}\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(</span><span>f</span><span>&#34;Processing Time: </span><span>{</span><span>result[</span><span>&#39;processing_time&#39;</span><span>]</span><span>:.1f</span><span>}</span><span> seconds</span><span>\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(</span><span>&#34;=&#34;</span><span> *</span><span> 40</span><span> +</span><span> &#34;</span><span>\n\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(result[</span><span>&#39;text&#39;</span><span>])</span></span>
<span><span>        </span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;‚úì Transcription saved to: </span><span>{</span><span>output_path</span><span>}</span><span>&#34;</span><span>)</span></span>
<span></span>
<span><span># Usage example</span></span>
<span><span>def</span><span> transcribe_audio_file</span><span>(audio_path, model_size</span><span>=</span><span>&#34;base&#34;</span><span>, language</span><span>=</span><span>None</span><span>):</span></span>
<span><span>    &#34;&#34;&#34;Simple function to transcribe an audio file&#34;&#34;&#34;</span></span>
<span><span>    </span></span>
<span><span>    transcriber </span><span>=</span><span> AudioTranscriber(</span><span>model_size</span><span>=</span><span>model_size)</span></span>
<span><span>    result </span><span>=</span><span> transcriber.transcribe_file(audio_path, </span><span>language</span><span>=</span><span>language)</span></span>
<span><span>    </span></span>
<span><span>    # Save transcription</span></span>
<span><span>    audio_name </span><span>=</span><span> Path(audio_path).stem</span></span>
<span><span>    output_path </span><span>=</span><span> f</span><span>&#34;</span><span>{</span><span>audio_name</span><span>}</span><span>_transcript.txt&#34;</span></span>
<span><span>    transcriber.save_transcription(result, output_path)</span></span>
<span><span>    </span></span>
<span><span>    return</span><span> result</span></span>
<span></span>
<span><span># Example usage</span></span>
<span><span>if</span><span> __name__</span><span> ==</span><span> &#34;__main__&#34;</span><span>:</span></span>
<span><span>    # Transcribe a file</span></span>
<span><span>    audio_file </span><span>=</span><span> &#34;interview.wav&#34;</span><span>  # Replace with your audio file</span></span>
<span><span>    result </span><span>=</span><span> transcribe_audio_file(audio_file, </span><span>model_size</span><span>=</span><span>&#34;base&#34;</span><span>, </span><span>language</span><span>=</span><span>&#34;en&#34;</span><span>)</span></span>
<span><span>    </span></span>
<span><span>    print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Transcription preview:&#34;</span><span>)</span></span>
<span><span>    print</span><span>(result[</span><span>&#39;text&#39;</span><span>][:</span><span>200</span><span>] </span><span>+</span><span> &#34;...&#34;</span><span> if</span><span> len</span><span>(result[</span><span>&#39;text&#39;</span><span>]) </span><span>&gt;</span><span> 200</span><span> else</span><span> result[</span><span>&#39;text&#39;</span><span>])</span></span></code></pre>
<div id="note-bx6handsq" role="note" aria-labelledby="note-bx6handsq-title"> <div>  <div> <h4 id="note-bx6handsq-title"> <span> üéµ </span> Supported Audio Formats </h4> <p>Whisper supports most common audio formats out of the box: WAV, MP3, MP4, M4A, FLAC, OGG, and more. No need to convert files beforehand‚ÄîFFmpeg handles the conversion automatically in the background.</p>  </div>  </div> </div> 
<h3 id="batch-processing-multiple-files">Batch Processing Multiple Files</h3>
<p>For processing multiple audio files efficiently:</p>
<pre tabindex="0" data-language="python"><code><span><span>def</span><span> batch_transcribe</span><span>(audio_files, output_dir</span><span>=</span><span>&#34;transcripts&#34;</span><span>, model_size</span><span>=</span><span>&#34;base&#34;</span><span>):</span></span>
<span><span>    &#34;&#34;&#34;Transcribe multiple audio files&#34;&#34;&#34;</span></span>
<span><span>    </span></span>
<span><span>    os.makedirs(output_dir, </span><span>exist_ok</span><span>=</span><span>True</span><span>)</span></span>
<span><span>    transcriber </span><span>=</span><span> AudioTranscriber(</span><span>model_size</span><span>=</span><span>model_size)</span></span>
<span><span>    </span></span>
<span><span>    results </span><span>=</span><span> []</span></span>
<span><span>    </span></span>
<span><span>    for</span><span> i, audio_file </span><span>in</span><span> enumerate</span><span>(audio_files, </span><span>1</span><span>):</span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>--- Processing file </span><span>{</span><span>i</span><span>}</span><span>/</span><span>{len</span><span>(audio_files)</span><span>}</span><span> ---&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        try</span><span>:</span></span>
<span><span>            result </span><span>=</span><span> transcriber.transcribe_file(audio_file)</span></span>
<span><span>            </span></span>
<span><span>            # Save individual transcription</span></span>
<span><span>            file_name </span><span>=</span><span> Path(audio_file).stem</span></span>
<span><span>            output_path </span><span>=</span><span> os.path.join(output_dir, </span><span>f</span><span>&#34;</span><span>{</span><span>file_name</span><span>}</span><span>_transcript.txt&#34;</span><span>)</span></span>
<span><span>            transcriber.save_transcription(result, output_path)</span></span>
<span><span>            </span></span>
<span><span>            results.append(result)</span></span>
<span><span>            </span></span>
<span><span>        except</span><span> Exception</span><span> as</span><span> e:</span></span>
<span><span>            print</span><span>(</span><span>f</span><span>&#34;‚úó Failed to process </span><span>{</span><span>audio_file</span><span>}</span><span>: </span><span>{str</span><span>(e)</span><span>}</span><span>&#34;</span><span>)</span></span>
<span><span>            continue</span></span>
<span><span>    </span></span>
<span><span>    print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>‚úì Batch processing completed: </span><span>{len</span><span>(results)</span><span>}</span><span>/</span><span>{len</span><span>(audio_files)</span><span>}</span><span> files successful&#34;</span><span>)</span></span>
<span><span>    return</span><span> results</span></span>
<span></span>
<span><span># Usage</span></span>
<span><span>audio_files </span><span>=</span><span> [</span><span>&#34;interview1.wav&#34;</span><span>, </span><span>&#34;interview2.mp3&#34;</span><span>, </span><span>&#34;lecture.m4a&#34;</span><span>]</span></span>
<span><span>batch_transcribe(audio_files, </span><span>output_dir</span><span>=</span><span>&#34;my_transcripts&#34;</span><span>)</span></span></code></pre>
<h3 id="creating-subtitle-files-srt-format">Creating Subtitle Files (SRT Format)</h3>
<p>Generate subtitle files for videos:</p>
<pre tabindex="0" data-language="python"><code><span><span>def</span><span> create_srt_subtitles</span><span>(audio_path, output_path</span><span>=</span><span>None</span><span>):</span></span>
<span><span>    &#34;&#34;&#34;Create SRT subtitle file from audio&#34;&#34;&#34;</span></span>
<span><span>    </span></span>
<span><span>    transcriber </span><span>=</span><span> AudioTranscriber(</span><span>model_size</span><span>=</span><span>&#34;base&#34;</span><span>)</span></span>
<span><span>    result </span><span>=</span><span> transcriber.transcribe_file(audio_path)</span></span>
<span><span>    </span></span>
<span><span>    if</span><span> output_path </span><span>is</span><span> None</span><span>:</span></span>
<span><span>        output_path </span><span>=</span><span> Path(audio_path).stem </span><span>+</span><span> &#34;.srt&#34;</span></span>
<span><span>    </span></span>
<span><span>    with</span><span> open</span><span>(output_path, </span><span>&#39;w&#39;</span><span>, </span><span>encoding</span><span>=</span><span>&#39;utf-8&#39;</span><span>) </span><span>as</span><span> f:</span></span>
<span><span>        for</span><span> i, segment </span><span>in</span><span> enumerate</span><span>(result[</span><span>&#39;segments&#39;</span><span>], </span><span>1</span><span>):</span></span>
<span><span>            start_time </span><span>=</span><span> format_timestamp(segment[</span><span>&#39;start&#39;</span><span>])</span></span>
<span><span>            end_time </span><span>=</span><span> format_timestamp(segment[</span><span>&#39;end&#39;</span><span>])</span></span>
<span><span>            </span></span>
<span><span>            f.write(</span><span>f</span><span>&#34;</span><span>{</span><span>i</span><span>}\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(</span><span>f</span><span>&#34;</span><span>{</span><span>start_time</span><span>}</span><span> --&gt; </span><span>{</span><span>end_time</span><span>}\n</span><span>&#34;</span><span>)</span></span>
<span><span>            f.write(</span><span>f</span><span>&#34;</span><span>{</span><span>segment[</span><span>&#39;text&#39;</span><span>].strip()</span><span>}\n\n</span><span>&#34;</span><span>)</span></span>
<span><span>    </span></span>
<span><span>    print</span><span>(</span><span>f</span><span>&#34;‚úì SRT subtitles saved to: </span><span>{</span><span>output_path</span><span>}</span><span>&#34;</span><span>)</span></span>
<span></span>
<span><span>def</span><span> format_timestamp</span><span>(seconds):</span></span>
<span><span>    &#34;&#34;&#34;Convert seconds to SRT timestamp format&#34;&#34;&#34;</span></span>
<span><span>    hours </span><span>=</span><span> int</span><span>(seconds </span><span>//</span><span> 3600</span><span>)</span></span>
<span><span>    minutes </span><span>=</span><span> int</span><span>((seconds </span><span>%</span><span> 3600</span><span>) </span><span>//</span><span> 60</span><span>)</span></span>
<span><span>    secs </span><span>=</span><span> int</span><span>(seconds </span><span>%</span><span> 60</span><span>)</span></span>
<span><span>    millisecs </span><span>=</span><span> int</span><span>((seconds </span><span>%</span><span> 1</span><span>) </span><span>*</span><span> 1000</span><span>)</span></span>
<span><span>    return</span><span> f</span><span>&#34;</span><span>{</span><span>hours</span><span>:02d</span><span>}</span><span>:</span><span>{</span><span>minutes</span><span>:02d</span><span>}</span><span>:</span><span>{</span><span>secs</span><span>:02d</span><span>}</span><span>,</span><span>{</span><span>millisecs</span><span>:03d</span><span>}</span><span>&#34;</span></span>
<span></span>
<span><span># Usage</span></span>
<span><span>create_srt_subtitles(</span><span>&#34;presentation.mp4&#34;</span><span>)</span></span></code></pre>
<h2 id="method-2-alternative-with-speechrecognition-library">Method 2: Alternative with SpeechRecognition Library</h2>
<p>For scenarios requiring different recognition engines or more control over audio preprocessing:</p>
<pre tabindex="0" data-language="python"><code><span><span>import</span><span> speech_recognition </span><span>as</span><span> sr</span></span>
<span><span>from</span><span> pydub </span><span>import</span><span> AudioSegment</span></span>
<span><span>import</span><span> tempfile</span></span>
<span><span>import</span><span> os</span></span>
<span></span>
<span><span>class</span><span> FlexibleTranscriber</span><span>:</span></span>
<span><span>    def</span><span> __init__</span><span>(self, engine</span><span>=</span><span>&#34;google&#34;</span><span>):</span></span>
<span><span>        &#34;&#34;&#34;Initialize with specified recognition engine&#34;&#34;&#34;</span></span>
<span><span>        self</span><span>.recognizer </span><span>=</span><span> sr.Recognizer()</span></span>
<span><span>        self</span><span>.engine </span><span>=</span><span> engine</span></span>
<span><span>        </span></span>
<span><span>        # Optimize settings</span></span>
<span><span>        self</span><span>.recognizer.energy_threshold </span><span>=</span><span> 300</span></span>
<span><span>        self</span><span>.recognizer.dynamic_energy_threshold </span><span>=</span><span> True</span></span>
<span><span>        </span></span>
<span><span>    def</span><span> preprocess_audio</span><span>(self, audio_path):</span></span>
<span><span>        &#34;&#34;&#34;Optimize audio for better recognition&#34;&#34;&#34;</span></span>
<span><span>        audio </span><span>=</span><span> AudioSegment.from_file(audio_path)</span></span>
<span><span>        </span></span>
<span><span>        # Convert to mono and normalize</span></span>
<span><span>        if</span><span> audio.channels </span><span>&gt;</span><span> 1</span><span>:</span></span>
<span><span>            audio </span><span>=</span><span> audio.set_channels(</span><span>1</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        audio </span><span>=</span><span> audio.set_frame_rate(</span><span>16000</span><span>)  </span><span># Standard sample rate</span></span>
<span><span>        audio </span><span>=</span><span> audio.normalize()  </span><span># Normalize volume</span></span>
<span><span>        </span></span>
<span><span>        # Export to temporary WAV file</span></span>
<span><span>        temp_file </span><span>=</span><span> tempfile.NamedTemporaryFile(</span><span>delete</span><span>=</span><span>False</span><span>, </span><span>suffix</span><span>=</span><span>&#39;.wav&#39;</span><span>)</span></span>
<span><span>        audio.export(temp_file.name, </span><span>format</span><span>=</span><span>&#34;wav&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        return</span><span> temp_file.name</span></span>
<span><span>    </span></span>
<span><span>    def</span><span> transcribe_file</span><span>(self, audio_path, language</span><span>=</span><span>&#39;en-US&#39;</span><span>):</span></span>
<span><span>        &#34;&#34;&#34;Transcribe audio file using speech_recognition library&#34;&#34;&#34;</span></span>
<span><span>        </span></span>
<span><span>        # Preprocess audio</span></span>
<span><span>        processed_path </span><span>=</span><span> self</span><span>.preprocess_audio(audio_path)</span></span>
<span><span>        </span></span>
<span><span>        try</span><span>:</span></span>
<span><span>            with</span><span> sr.AudioFile(processed_path) </span><span>as</span><span> source:</span></span>
<span><span>                # Adjust for ambient noise</span></span>
<span><span>                self</span><span>.recognizer.adjust_for_ambient_noise(source, </span><span>duration</span><span>=</span><span>1</span><span>)</span></span>
<span><span>                audio_data </span><span>=</span><span> self</span><span>.recognizer.record(source)</span></span>
<span><span>            </span></span>
<span><span>            # Perform recognition</span></span>
<span><span>            if</span><span> self</span><span>.engine </span><span>==</span><span> &#34;google&#34;</span><span>:</span></span>
<span><span>                text </span><span>=</span><span> self</span><span>.recognizer.recognize_google(audio_data, </span><span>language</span><span>=</span><span>language)</span></span>
<span><span>            elif</span><span> self</span><span>.engine </span><span>==</span><span> &#34;sphinx&#34;</span><span>:</span></span>
<span><span>                text </span><span>=</span><span> self</span><span>.recognizer.recognize_sphinx(audio_data)</span></span>
<span><span>            </span></span>
<span><span>            return</span><span> {</span></span>
<span><span>                &#39;text&#39;</span><span>: text,</span></span>
<span><span>                &#39;success&#39;</span><span>: </span><span>True</span><span>,</span></span>
<span><span>                &#39;engine&#39;</span><span>: </span><span>self</span><span>.engine</span></span>
<span><span>            }</span></span>
<span><span>            </span></span>
<span><span>        except</span><span> sr.UnknownValueError:</span></span>
<span><span>            return</span><span> {</span></span>
<span><span>                &#39;text&#39;</span><span>: </span><span>&#34;&#34;</span><span>,</span></span>
<span><span>                &#39;success&#39;</span><span>: </span><span>False</span><span>,</span></span>
<span><span>                &#39;error&#39;</span><span>: </span><span>&#34;Could not understand audio&#34;</span></span>
<span><span>            }</span></span>
<span><span>        except</span><span> sr.RequestError </span><span>as</span><span> e:</span></span>
<span><span>            return</span><span> {</span></span>
<span><span>                &#39;text&#39;</span><span>: </span><span>&#34;&#34;</span><span>,</span></span>
<span><span>                &#39;success&#39;</span><span>: </span><span>False</span><span>,</span></span>
<span><span>                &#39;error&#39;</span><span>: </span><span>f</span><span>&#34;Recognition service error: </span><span>{str</span><span>(e)</span><span>}</span><span>&#34;</span></span>
<span><span>            }</span></span>
<span><span>        finally</span><span>:</span></span>
<span><span>            # Clean up temporary file</span></span>
<span><span>            os.unlink(processed_path)</span></span>
<span></span>
<span><span># Usage</span></span>
<span><span>transcriber </span><span>=</span><span> FlexibleTranscriber(</span><span>engine</span><span>=</span><span>&#34;google&#34;</span><span>)</span></span>
<span><span>result </span><span>=</span><span> transcriber.transcribe_file(</span><span>&#34;audio.wav&#34;</span><span>)</span></span>
<span></span>
<span><span>if</span><span> result[</span><span>&#39;success&#39;</span><span>]:</span></span>
<span><span>    print</span><span>(result[</span><span>&#39;text&#39;</span><span>])</span></span>
<span><span>else</span><span>:</span></span>
<span><span>    print</span><span>(</span><span>f</span><span>&#34;Transcription failed: </span><span>{</span><span>result[</span><span>&#39;error&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>)</span></span></code></pre>
<div id="note-qk70herjz" role="note" aria-labelledby="note-qk70herjz-title"> <div>  <div> <h4 id="note-qk70herjz-title"> <span> üîÑ </span> Engine Comparison </h4> <p>Google: High accuracy, requires internet connection and has usage limits. Sphinx: Completely offline, lower accuracy but no external dependencies. Choose Google for best results, Sphinx for complete privacy and offline use.</p>  </div>  </div> </div> 
<h2 id="common-issues-and-solutions">Common Issues and Solutions</h2>
<h3 id="issue-1-ffmpeg-not-found">Issue 1: FFmpeg Not Found</h3>
<p><strong>Error:</strong> <code>[WinError 2] The system cannot find the file specified</code></p>
<p><strong>Solution:</strong></p>
<ul>
<li>Verify FFmpeg installation: <code>ffmpeg -version</code></li>
<li>Windows: Ensure FFmpeg is in your PATH environment variable</li>
<li>Restart your terminal/command prompt after PATH changes</li>
<li>Try reinstalling FFmpeg if the problem persists</li>
</ul>
<h3 id="issue-2-out-of-memory-errors">Issue 2: Out of Memory Errors</h3>
<p><strong>Error:</strong> CUDA out of memory or system RAM exhausted</p>
<div id="note-ofnj2hr08" role="note" aria-labelledby="note-ofnj2hr08-title"> <div>  <div> <h4 id="note-ofnj2hr08-title"> <span> ‚ö° </span> Memory Management Tips </h4> <p>For files longer than 1 hour, use the &#39;tiny&#39; or &#39;base&#39; model. For files over 2 hours, consider chunking the audio or processing on a machine with more RAM. GPU acceleration helps with speed but requires more VRAM.</p>  </div>  </div> </div> 
<p><strong>Solutions:</strong></p>
<pre tabindex="0" data-language="python"><code><span><span># Use smaller model</span></span>
<span><span>transcriber </span><span>=</span><span> AudioTranscriber(</span><span>model_size</span><span>=</span><span>&#34;tiny&#34;</span><span>)</span></span>
<span></span>
<span><span># For very long audio files, process in chunks</span></span>
<span><span>def</span><span> transcribe_long_audio</span><span>(audio_path, chunk_duration</span><span>=</span><span>300</span><span>):  </span><span># 5 minutes</span></span>
<span><span>    audio </span><span>=</span><span> AudioSegment.from_file(audio_path)</span></span>
<span><span>    chunks </span><span>=</span><span> [audio[i:i</span><span>+</span><span>chunk_duration</span><span>*</span><span>1000</span><span>] </span><span>for</span><span> i </span><span>in</span><span> range</span><span>(</span><span>0</span><span>, </span><span>len</span><span>(audio), chunk_duration</span><span>*</span><span>1000</span><span>)]</span></span>
<span><span>    </span></span>
<span><span>    transcriptions </span><span>=</span><span> []</span></span>
<span><span>    for</span><span> i, chunk </span><span>in</span><span> enumerate</span><span>(chunks):</span></span>
<span><span>        chunk_path </span><span>=</span><span> f</span><span>&#34;temp_chunk_</span><span>{</span><span>i</span><span>}</span><span>.wav&#34;</span></span>
<span><span>        chunk.export(chunk_path, </span><span>format</span><span>=</span><span>&#34;wav&#34;</span><span>)</span></span>
<span><span>        </span></span>
<span><span>        result </span><span>=</span><span> transcriber.transcribe_file(chunk_path)</span></span>
<span><span>        transcriptions.append(result[</span><span>&#39;text&#39;</span><span>])</span></span>
<span><span>        </span></span>
<span><span>        os.remove(chunk_path)</span></span>
<span><span>    </span></span>
<span><span>    return</span><span> &#39; &#39;</span><span>.join(transcriptions)</span></span></code></pre>
<h3 id="issue-3-poor-accuracy-on-noisy-audio">Issue 3: Poor Accuracy on Noisy Audio</h3>
<p><strong>Problem:</strong> Low accuracy on recordings with background noise or poor quality</p>
<div id="note-6ur23yq5x" role="note" aria-labelledby="note-6ur23yq5x-title"> <div>  <div> <h4 id="note-6ur23yq5x-title"> <span> üé§ </span> Audio Quality Tips </h4> <p>Best results come from: 16kHz+ sample rate, minimal background noise, clear speech, and audio levels between -12dB to -6dB. Record in quiet environments when possible. Even small improvements in audio quality dramatically improve transcription accuracy.</p>  </div>  </div> </div> 
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Audio preprocessing:</strong></li>
</ol>
<pre tabindex="0" data-language="python"><code><span><span>def</span><span> enhance_audio</span><span>(audio_path):</span></span>
<span><span>    &#34;&#34;&#34;Basic audio enhancement&#34;&#34;&#34;</span></span>
<span><span>    audio </span><span>=</span><span> AudioSegment.from_file(audio_path)</span></span>
<span><span>    </span></span>
<span><span>    # Normalize volume</span></span>
<span><span>    audio </span><span>=</span><span> audio.normalize()</span></span>
<span><span>    </span></span>
<span><span>    # Apply high-pass filter to reduce low-frequency noise</span></span>
<span><span>    audio </span><span>=</span><span> audio.high_pass_filter(</span><span>80</span><span>)</span></span>
<span><span>    </span></span>
<span><span>    # Compress dynamic range</span></span>
<span><span>    audio </span><span>=</span><span> audio.compress_dynamic_range()</span></span>
<span><span>    </span></span>
<span><span>    return</span><span> audio</span></span></code></pre>
<ol start="2">
<li><strong>Specify language for better accuracy:</strong></li>
</ol>
<pre tabindex="0" data-language="python"><code><span><span>result </span><span>=</span><span> transcriber.transcribe_file(</span><span>&#34;audio.wav&#34;</span><span>, </span><span>language</span><span>=</span><span>&#34;en&#34;</span><span>)</span></span></code></pre>
<ol start="3">
<li><strong>Use higher-quality model:</strong></li>
</ol>
<pre tabindex="0" data-language="python"><code><span><span># Upgrade from &#39;base&#39; to &#39;small&#39; for better accuracy</span></span>
<span><span>transcriber </span><span>=</span><span> AudioTranscriber(</span><span>model_size</span><span>=</span><span>&#34;small&#34;</span><span>)</span></span></code></pre>
<h2 id="performance-benchmarks">Performance Benchmarks</h2>
<p>Based on testing with various audio types on a modern laptop:</p>
<p><strong>Whisper Model Performance (1-hour audio file):</strong></p>
<ul>
<li><strong>tiny</strong>: 1.9 minutes processing, 89% accuracy</li>
<li><strong>base</strong>: 3.8 minutes processing, 94% accuracy</li>
<li><strong>small</strong>: 10 minutes processing, 96% accuracy</li>
<li><strong>medium</strong>: 30 minutes processing, 97% accuracy</li>
</ul>
<p><strong>Hardware Impact:</strong></p>
<ul>
<li><strong>CPU only</strong>: Use base model maximum for reasonable speeds</li>
<li><strong>8GB RAM</strong>: Comfortable with small model</li>
<li><strong>16GB+ RAM</strong>: Can handle medium/large models without issues</li>
<li><strong>GPU acceleration</strong>: 3-5x speed improvement (requires CUDA setup)</li>
</ul>
<div id="note-03fyctqtz" role="note" aria-labelledby="note-03fyctqtz-title"> <div>  <div> <h4 id="note-03fyctqtz-title"> <span> üöÄ </span> Optimization Strategy </h4> <p>Start with &#39;base&#39; model on CPU for the best balance of speed and accuracy. If accuracy is insufficient for your use case, upgrade to &#39;small&#39;. Only use GPU acceleration if you&#39;re processing large volumes of audio regularly‚Äîthe setup complexity isn&#39;t worth it for occasional use.</p>  </div>  </div> </div> 
<h2 id="command-line-usage">Command-Line Usage</h2>
<p>Create a simple command-line script for easy usage:</p>
<pre tabindex="0" data-language="python"><code><span><span># transcribe.py</span></span>
<span><span>import</span><span> sys</span></span>
<span><span>import</span><span> argparse</span></span>
<span><span>from</span><span> pathlib </span><span>import</span><span> Path</span></span>
<span></span>
<span><span>def</span><span> main</span><span>():</span></span>
<span><span>    parser </span><span>=</span><span> argparse.ArgumentParser(</span><span>description</span><span>=</span><span>&#39;Transcribe audio files locally&#39;</span><span>)</span></span>
<span><span>    parser.add_argument(</span><span>&#39;audio_file&#39;</span><span>, </span><span>help</span><span>=</span><span>&#39;Path to audio file&#39;</span><span>)</span></span>
<span><span>    parser.add_argument(</span><span>&#39;--model&#39;</span><span>, </span><span>default</span><span>=</span><span>&#39;base&#39;</span><span>, </span><span>choices</span><span>=</span><span>[</span><span>&#39;tiny&#39;</span><span>, </span><span>&#39;base&#39;</span><span>, </span><span>&#39;small&#39;</span><span>, </span><span>&#39;medium&#39;</span><span>, </span><span>&#39;large&#39;</span><span>])</span></span>
<span><span>    parser.add_argument(</span><span>&#39;--language&#39;</span><span>, </span><span>help</span><span>=</span><span>&#39;Language code (e.g., en, es, fr)&#39;</span><span>)</span></span>
<span><span>    parser.add_argument(</span><span>&#39;--output&#39;</span><span>, </span><span>help</span><span>=</span><span>&#39;Output file path&#39;</span><span>)</span></span>
<span><span>    </span></span>
<span><span>    args </span><span>=</span><span> parser.parse_args()</span></span>
<span><span>    </span></span>
<span><span>    # Transcribe</span></span>
<span><span>    result </span><span>=</span><span> transcribe_audio_file(</span></span>
<span><span>        args.audio_file, </span></span>
<span><span>        model_size</span><span>=</span><span>args.model,</span></span>
<span><span>        language</span><span>=</span><span>args.language</span></span>
<span><span>    )</span></span>
<span><span>    </span></span>
<span><span>    # Save to custom output path if specified</span></span>
<span><span>    if</span><span> args.output:</span></span>
<span><span>        with</span><span> open</span><span>(args.output, </span><span>&#39;w&#39;</span><span>, </span><span>encoding</span><span>=</span><span>&#39;utf-8&#39;</span><span>) </span><span>as</span><span> f:</span></span>
<span><span>            f.write(result[</span><span>&#39;text&#39;</span><span>])</span></span>
<span><span>        print</span><span>(</span><span>f</span><span>&#34;Transcription saved to: </span><span>{</span><span>args.output</span><span>}</span><span>&#34;</span><span>)</span></span>
<span></span>
<span><span>if</span><span> __name__</span><span> ==</span><span> &#34;__main__&#34;</span><span>:</span></span>
<span><span>    main()</span></span></code></pre>
<p><strong>Usage examples:</strong></p>
<pre tabindex="0" data-language="bash"><code><span><span># Basic transcription</span></span>
<span><span>python</span><span> transcribe.py</span><span> interview.wav</span></span>
<span></span>
<span><span># Specify model and language</span></span>
<span><span>python</span><span> transcribe.py</span><span> lecture.mp3</span><span> --model</span><span> small</span><span> --language</span><span> en</span></span>
<span></span>
<span><span># Custom output file</span></span>
<span><span>python</span><span> transcribe.py</span><span> podcast.m4a</span><span> --output</span><span> transcript.txt</span></span></code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Local audio transcription with Python and Whisper offers a compelling alternative to commercial services. With a one-time setup, you get unlimited transcription capabilities, complete privacy, and often superior accuracy compared to cloud-based solutions.</p>
<p><strong>Key advantages:</strong></p>
<ul>
<li><strong>Zero ongoing costs</strong> after initial setup‚Äîno per-minute charges</li>
<li><strong>Complete privacy</strong>‚Äîaudio never leaves your machine</li>
<li><strong>High accuracy</strong>‚Äî94-98% depending on model choice and audio quality</li>
<li><strong>Fast processing</strong>‚Äîtypically 4-16x real-time speed</li>
<li><strong>Offline capability</strong>‚Äîworks without internet connection</li>
<li><strong>No usage limits</strong>‚Äîtranscribe as much as you want</li>
</ul>
<p>Whether you‚Äôre a researcher transcribing interviews, a journalist working with sensitive sources, or a content creator processing podcasts, this local solution gives you the control and privacy that cloud services can‚Äôt match.</p>
<p>The setup might take 30 minutes, but you‚Äôll save hours of time and potentially hundreds of dollars in transcription costs. Plus, you‚Äôll have the peace of mind that comes with keeping your audio data completely under your control.</p> <div> <h3> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"> <path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 01-2.25 2.25h-15a2.25 2.25 0 01-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0019.5 4.5h-15a2.25 2.25 0 00-2.25 2.25m19.5 0v.243a2.25 2.25 0 01-1.07 1.916l-7.5 4.615a2.25 2.25 0 01-2.36 0L3.32 8.91a2.25 2.25 0 01-1.07-1.916V6.75"></path> </svg> Stay up to date </h3> <p> Get notified when I publish something new, and unsubscribe at any time. </p>  </div>  <!-- Pass the prop here --> </article></div>
  </body>
</html>
