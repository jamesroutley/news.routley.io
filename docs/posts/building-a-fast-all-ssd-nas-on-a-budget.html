<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeffgeerling.com/blog/2022/building-fast-all-ssd-nas-on-budget">Original</a>
    <h1>Building a fast all-SSD NAS on a budget</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://www.jeffgeerling.com/sites/default/files/images/all-ssd-edit-nas-complete.jpeg" width="700" height="402" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-e155e0ec-6359-4de8-bbdf-bce6a0d820c6" data-insert-attach="{&#34;id&#34;:&#34;e155e0ec-6359-4de8-bbdf-bce6a0d820c6&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="All SSD Edit NAS build - completed"/></p>

<p>I edit videos non-stop nowadays. In a former life, I had a 2 TB backup volume and that stored my entire digital life—all my photos, family video clips, and every bit of code and text I&#39;d ever written.</p>

<p>Video is a different beast, entirely.</p>

<p>Every minute of 4K ProRes LT footage (which is a very lightweight format, compared to RAW) is 3 GB of space. A typical video I produce has between 30-60 minutes of raw footage (which brings the total project size up to around 100-200 GB).</p>

<p>To edit footage <em>well</em>, the data not only needs to move <em>fast</em> (1 Gbps barely cuts it for a single stream), it also needs to have very low latency, otherwise Final Cut Pro (my editor of choice) lags quite a bit while scrubbing over dozens of video clips.</p>

<p>Therefore, I always <em>used</em> to edit videos off my local SSD drive. And sometimes over the network using macOS&#39;s built-in file sharing. But as my video workflow matures, I find myself needing a central storage solution disconnected from my main workstation.</p>

<p>Thus, the all-SSD high-performance edit NAS—on a budget.</p>

<p>I had five <a href="https://amzn.to/3AXvJLs">8TB Samsung QVO SSDs</a> from my insane <a href="https://www.jeffgeerling.com/blog/2021/i-built-5000-raspberry-pi-server-yes-its-ridiculous">$5000 Raspberry Pi server</a> build. Until now, I had them inside my 2.5 Gbps NAS. But I wanted to build my own NAS capable of saturating a 10 Gbps connection, and allowing <em>extremely</em> low latency data access over the network to my two Macs, both of which are connected to my wired 10 Gbps home network.</p>

<h2>Used Server Parts</h2>

<h3>Summary</h3>

<p>This server build can be done with just as much capability (but a lower storage amount) on a more stringent budget, or can go &#39;all out&#39; maxing out the RAM and SSD storage space. I&#39;ll show a price comparison of both (noting that my needs—tons of RAM and tons of SSD space—may not match your own, if the main goal is &#39;very fast SSD NAS&#39;):</p>

<table>
<thead>
<tr>
  <th>Part</th>
  <th>Price (low)</th>
  <th>Price (high - as built)</th>
</tr>
</thead>
<tbody>
<tr>
  <td><a href="https://www.ebay.com/itm/254932365206">Supermicro X10SDV-4C-TLN2F motherboard / Xeon D / Dual 10 GbE</a> (eBay)</td>
  <td>$260</td>
  <td>$260</td>
</tr>
<tr>
  <td>DDR4-2133 ECC RAM (Didion Orf e-waste recycling)</td>
  <td>$55 (32GB)</td>
  <td>$220 (128GB)</td>
</tr>
<tr>
  <td><a href="https://www.microcenter.com/product/635353/supermicro-xg6-kxg60znv512g-512gb-ssd-3d-tlc-nand-m2-2280-pcie-nvme-30-x4-internal-solid-state-drive">Boot SSD</a></td>
  <td>$50 (<a href="https://amzn.to/3okVhuA">256GB USB Drive</a>)</td>
  <td>$125 (<a href="https://www.microcenter.com/product/635353/supermicro-xg6-kxg60znv512g-512gb-ssd-3d-tlc-nand-m2-2280-pcie-nvme-30-x4-internal-solid-state-drive">512GB XG6</a>)</td>
</tr>
<tr>
  <td><a href="https://www.myelectronics.nl/us/19-inch-2u-mini-itx-case-short-depth.html">2U Rackmount Case</a></td>
  <td>$200</td>
  <td>$200</td>
</tr>
<tr>
  <td><a href="https://amzn.to/3RSJ7Xi">2x Noctua 80mm case fans</a></td>
  <td>$34</td>
  <td>$34</td>
</tr>
<tr>
  <td>Storage SSDs</td>
  <td>$180 (<a href="https://amzn.to/3PlvaPQ">MX500 1TB x2</a>)</td>
  <td>$3490 (<a href="https://amzn.to/3PqRoQA">QVO 8TB x5</a>)</td>
</tr>
<tr>
  <td>Total</td>
  <td>$779</td>
  <td>$4,329</td>
</tr>
</tbody>
</table>

<p>When looking at the price discrepancy, you have to realize (a) I already had the 8TB SSDs, from some projects I tested last year... most people don&#39;t have those things laying around, and (b) I will actually <em>use</em> that much low-latency storage... most people probably don&#39;t and would be better off with less SSD storage and more spinning disks (which are <em>much</em> cheaper per TB).</p>

<h3>Details</h3>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/supermicro-mini-itx-server-motherboard-xeon-d-1521.jpeg" width="700" height="425" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-09b21672-391c-400e-be62-828db5d53090" data-insert-attach="{&#34;id&#34;:&#34;09b21672-391c-400e-be62-828db5d53090&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Supermicro Mini ITX server motherboard Xeon D-1521"/></p>

<p>The motivation for this build came from finding <a href="https://www.ebay.com/itm/254932365206">this Supermicro X10SDV-4C-TLN2F Mini ITX motherboard</a>. The price ($270 shipped) was low enough I could consider building with it, and it already included an older-but-not-too-power-hungry Xeon D SoC, two 10 Gbps Ethernet ports, and 5 SATA-III connectors, the basic components I needed for the build.</p>

<p>ServeTheHome <a href="https://www.servethehome.com/supermicro-x10sdv-4c-tln2f-review-xeon-d-1520/">gave this motherboard a good review</a> when it came out, and as long as things were in working order, it should still be a good choice, though less efficient and performant than a more expensive 2020s-era board.</p>

<p>I needed to find a Mini ITX rackmount enclosure, and luckily I&#39;d been in talks with MyElectronics after using their prototype &#39;blue&#39; enclosure for a <a href="https://www.jeffgeerling.com/blog/2022/hosting-website-on-farm-or-anywhere">remote Pi cluster installation</a> I was testing. They are working on a new <a href="https://www.myelectronics.nl/us/19-inch-2u-mini-itx-case-short-depth.html">2U mini ITX short-depth enclosure</a>, and they sent me an early revision to use in this build:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/myelectronics-mini-itx-2u-short-depth-enclosure.jpg" width="700" height="261" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-e3627b31-e080-44dc-bee2-67f568ceb00f" data-insert-attach="{&#34;id&#34;:&#34;e3627b31-e080-44dc-bee2-67f568ceb00f&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="MyElectronics 2U Mini ITX short depth enclosure"/></p>

<p>They said the enclosure should be available &#39;soon&#39; for around $200, but they&#39;re still working out a few details with port placement, the power supply configuration, and PCIe slot layout.</p>

<p>I already had the SSDs on hand, and to be honest, you could use much less expensive SSDs if you don&#39;t <em>need</em> many TB of flash storage—heck, right now I&#39;m only using a few percent of it! But those cost around $3500 new.</p>

<p>To round out the build, I bought a couple Noctua fans, and 128 GB of recycled ECC RAM from <a href="https://www.didionorfrecycling.com/it-assets">an e-waste recycler</a> only a dozen miles from where I live. They sold me four sticks of 32GB ECC DDR4-2133 RAM (HP branded) for $220.</p>

<h2>Yes there&#39;s a sponsor (for the SSD)</h2>

<p>To help fund this build, I got Micro Center to sponsor the <a href="https://www.youtube.com/watch?v=xvE4HNJZeIg">build video</a>, and they contributed a Kioxia XG6, which is sold under Supermicro&#39;s name:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/kioxia-xg6-nvme-ssd.jpeg" width="700" height="467" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-0e65e0b3-a5b8-4fdd-932d-c136917d826a" data-insert-attach="{&#34;id&#34;:&#34;0e65e0b3-a5b8-4fdd-932d-c136917d826a&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Kioxia XG6 Supermicro branded NVMe SSD provided by Micro Center"/></p>

<p>They sell the <a href="https://www.microcenter.com/product/635353/supermicro-xg6-kxg60znv512g-512gb-ssd-3d-tlc-nand-m2-2280-pcie-nvme-30-x4-internal-solid-state-drive">512GB model I used</a> for about $125—<em>in store</em>! I didn&#39;t realize they have some Supermicro (and sometimes Dell/EMC) server gear in stock at their stores in the US. I just wish they had more locations (both here and internationally!).</p>

<h2>BIOS and UEFI</h2>

<p>I put all the hardware together, and installed TrueNAS from a USB stick to the NVMe drive. If you&#39;re wondering why I chose TrueNAS core, check out the <a href="https://www.youtube.com/watch?v=xvE4HNJZeIg">full video</a> where I speak to Wendell from Level1Techs about performance considerations and the storage layout for this NAS.</p>

<p>But I had a problem: I couldn&#39;t get the Supermicro motherboard to <em>boot</em> off UEFI. I was getting errors about &#34;CPU with APIC ID 0 is not enabled&#34;, and I spent an hour messing around in the BIOS before (temporarily) giving up and throwing in a SATA boot SSD.</p>

<p>This motherboard is from around 2015, an era when NVMe boot on servers wasn&#39;t commonplace. Couple that with the M.2 slot accepting <em>either</em> SATA or NVMe drives, and most people preferring to boot off a <a href="https://www.supermicro.com/products/nfo/SATADOM.cfm">SuperDOM</a> instead, and I knew I was kind of on my own debugging it.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/zfs-data-copy-357mb-ps.jpg" width="700" height="249" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-10d712fb-1178-4bac-8861-d5a5010ec831" data-insert-attach="{&#34;id&#34;:&#34;10d712fb-1178-4bac-8861-d5a5010ec831&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="357 MBps copy over Samba on macOS"/></p>

<p>With the SATA SSD installed, I was able to get consistent performance using a striped RAIDZ mirror pool with four SSDs and a hot spare—but that performance stayed around 320 MB/sec writes over long periods of time! That&#39;s only a third of the available bandwidth, and only a little better than the 200-250 MB/sec I was getting on my 2.5 Gigabit NAS.</p>

<p>After some more consulting with Wendell <em>and</em> talking to Patrick from <a href="https://www.servethehome.com">ServeTheHome</a>, I was able to figure out both the boot issue (just had to mess with the UEFI boot settings a bit more), and the performance issue (it... ended up going away after I replugged everything).</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/zfs-data-copy-737mb-ps.jpg" width="700" height="297" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-a94d0427-155f-40ed-88d5-0f3c5cb815b5" data-insert-attach="{&#34;id&#34;:&#34;a94d0427-155f-40ed-88d5-0f3c5cb815b5&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="700 MBps copy over Samba on macOS"/></p>

<p>With the built-in SATA ports, I got around 700 MB/sec write speed over the network, and 1.1 GB/sec reads. Latency was great, and editing Final Cut Pro projects felt the same as if editing local.</p>

<p>Just to see what was possible, I also tossed in a <a href="https://amzn.to/3srcZOh">MegaRAID 9405W-16i HBA</a>, and re-tested, and could get 1.1 GB/sec <em>both</em> ways... but I went back to the onboard SATA when I measured 7-15W more power consumption using the HBA. A few hundred MB/sec isn&#39;t worth the extra power consumption—especially considering I&#39;d need to figure out how to properly cool the HBA:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/hba-with-fan-mini-itx-server-build.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-c31fd3ab-68d8-41f1-a552-8a19de0f6039" data-insert-attach="{&#34;id&#34;:&#34;c31fd3ab-68d8-41f1-a552-8a19de0f6039&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Fan directed at MegaRAID HBA installed in all SSD edit NAS for cooling"/></p>

<h2>First time using ZFS in production</h2>

<p>A few of the things I modified in my TrueNAS storage pool configuration, based on Wendell&#39;s recommendations:</p>

<ul>
<li><strong>Sync: Disabled</strong>: I disabled ZFS&#39;s sync option, meaning ZFS would report back to the OS a file was written even before it&#39;s fully written to disk. This is risky, but since I&#39;m always copying footage I have a golden copy of <em>already</em>, and since I&#39;m running the NAS on a UPS, the risk is minimized. With the &#39;Standard&#39; setting, ZFS would limp along for a period, then go fast (700+ MB/sec), then limp along again. Speed was very inconsistent.</li>
<li><strong>1M Record Size</strong>: Since I&#39;m mostly pushing video files (with tens of MB/sec) through the server, having a smaller block size would just result in more IO operations. If I were running a database on here, I would consider a smaller size, but this is optimized for video.</li>
<li><strong>Periodic maintenance</strong>: I set up nightly snapshots, weekly scrubs, and weekly S.M.A.R.T. scans, and made sure the server could email me a notification if anything went awry. One of the best benefits of an out-of-the-box solution like TrueNAS is the ease of setting these long-term automation tasks up.</li>
</ul>

<p>The nightly snapshots (and indeed, ZFS/RAID itself) is <em>NOT</em> at all a substitution for a rigorous backup plan. For now I&#39;ll be <code>rsync</code>ing the volume to my main NAS nightly, and that&#39;s backed up offsite to Amazon Glacier weekly. Eventually I might use ZFS&#39;s <code>sync</code> functionality to synchronize snapshots to my backup server. We&#39;ll see.</p>

<p>For now, I&#39;m happy having a server that can consistently write 700+ MB/sec, and read 1.1 GB/sec, with extremely low latency, over my 10 gigabit network.</p>

<p>Watch the <a href="https://www.youtube.com/watch?v=xvE4HNJZeIg">full video</a> to learn more about the thought process and some of the struggles I had.</p></div></div>
  </body>
</html>
