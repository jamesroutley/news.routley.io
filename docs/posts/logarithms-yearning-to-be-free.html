<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.johndcook.com/blog/2022/04/13/logarithms-yearning-to-be-free/">Original</a>
    <h1>Logarithms yearning to be free</h1>
    
    <div id="readability-page-1" class="page"><article id="post-102966">
	<!-- .entry-header -->

	<div>
		<p>I got an evaluation copy of <a href="https://amzn.to/3uzPqoS">The Best Writing on Mathematics 2021</a> yesterday. One article jumped out as I was skimming the table of contents: <em>A Zeroth Power Is Often a Logarithm Yearning to Be Free</em> by Sanjoy Mahajan. Great title.</p>
<p>There are quite a few theorems involving powers that have an exceptional case that involves a logarithm. The author opens with the example of finding the antiderivative of <em>x</em><sup><em>n</em></sup>. When <em>n</em> ≠ -1 the antiderivative is another power function, but when <em>n</em> = -1 it’s a logarithm.</p>
<p>Another example that the author mentions is that the limit of <a href="https://www.johndcook.com/blog/2009/03/23/inequalities-means/">power means</a> as the power goes to 0 is the geometric mean. i.e. the exponential of the mean of the logarithms of the arguments.</p>
<p>I tried to think of other examples where this pattern pops up, and I thought of a couple related to entropy.</p>
<h2><em>q</em>-logarithm entropy</h2>
<p>The definition of <em>q</em>-logarithm entropy takes Mahajan’s idea and runs it backward, turning a logarithm into a power. As I wrote about <a href="https://www.johndcook.com/blog/2021/05/17/qlog-tsallis-entropy/">here</a>,</p>
<blockquote><p>The natural logarithm is given by</p>
<p><img src="https://www.johndcook.com/lndef.svg" alt="\ln(x) = \int_1^x t^{-1}\,dt" width="132" height="41"/></p>
<p>and we can generalize this to the <em>q</em>-logarithm by defining</p>
<p><img loading="lazy" src="https://www.johndcook.com/lnqdef.svg" alt="\ln_q(x) = \int_1^x t^{-q}\,dt" width="139" height="41"/></p>
<p>And so ln<sub>1</sub> = ln.</p></blockquote>
<p>Then <em>q</em>-logarithm entropy is just Shannon entropy with natural logarithm replaced by <em>q</em>-logarithm.</p>
<h2>Rényi entropy</h2>
<p>Quoting from <a href="https://www.johndcook.com/blog/2018/11/21/renyi-entropy/">this post</a>,</p>
<blockquote><p>If a discrete random variable <em>X</em> has <em>n</em> possible values, where the <em>i</em>th outcome has probability <em>p</em><sub><em>i</em></sub>, then the Rényi entropy of order α is defined to be</p>
<p><img loading="lazy" src="https://www.johndcook.com/renyi_discrete.svg" alt="H_\alpha(X) = \frac{1}{1 - \alpha} \log_2 \left(\sum_{i=1}^n p_i^\alpha \right)" width="234" height="54"/></p>
<p>for 0 ≤ α ≤ ∞. In the case α = 1 or ∞ this expression means the limit as α approaches 1 or ∞ respectively.</p>
<p>…</p>
<p>When α = 1 we get the more familiar Shannon entropy:</p>
<p><img loading="lazy" src="https://www.johndcook.com/renyi_shannon.svg" alt="H_1(X) = \lim_{\alpha\to1} H_\alpha(X) = - \left(\sum_{i=1}^n p_i \log_2 p_i \right)" width="320" height="54"/></p></blockquote>
<p>In this case there’s already a logarithm in the definition, but it moves inside the parentheses in the limit.</p>
<p>And if you rewrite</p>
<p><em>p</em><sup>α</sup></p>
<p>as</p>
<p><em>p</em> <em>p</em><sup>α-1</sup></p>
<p>then as the exponent in <em>p</em><sup>α-1</sup> goes to zero, we have a logarithm yearning to be free.</p>
			</div><!-- .entry-content -->

	<!-- .entry-meta -->
</article></div>
  </body>
</html>
