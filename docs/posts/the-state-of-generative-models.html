<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nrehiew.github.io/blog/2024/">Original</a>
    <h1>The State of Generative Models</h1>
    
    <div id="readability-page-1" class="page"><div><details><summary><strong>Table of Contents</strong></summary><nav id="TableOfContents"><ol><li><a href="#the-state-of-generative-models">The State of Generative Models</a></li><li><a href="#language">Language</a><ol><li><a href="#architecture">Architecture</a><ol><li><a href="#dense-transformer">Dense Transformer</a></li><li><a href="#mixture-of-experts">Mixture-of-Experts</a></li></ol></li><li><a href="#tokenization">Tokenization</a></li><li><a href="#reasoning">Reasoning</a></li><li><a href="#distillation">Distillation</a></li></ol></li><li><a href="#image">Image</a><ol><li><a href="#architecture-1">Architecture</a></li><li><a href="#framework">Framework</a></li></ol></li><li><a href="#multimodality">Multimodality</a><ol><li><a href="#visual-language-models">Visual Language Models</a></li><li><a href="#omni-modal-models">Omni-Modal Models</a></li></ol></li><li><a href="#agents-and-human-ai-interfaces">Agents and Human-AI Interfaces</a></li><li><a href="#2025">2025</a></li></ol></nav></details><div><p>In the face of disruptive technologies, moats created by closed source are temporary.</p><p>-
Liang Wenfeng, CEO of DeepSeek</p></div><p>2024 has been a great year for AI. In both text and image generation, we have seen tremendous step-function like improvements in model capabilities across the board. A year that started with OpenAI dominance is now ending with Anthropic’s Claude being my used LLM and the introduction of several labs that are all attempting to push the frontier from xAI to Chinese labs like DeepSeek and Qwen.</p><p>The past 2 years have also been great for research. While much of the progress has happened behind closed doors in frontier labs, we have seen a lot of effort in the open to replicate these results. In this post, I share what I think the current state of generative models is, as well as research directions that I am personally excited for in 2025.</p><p>Large Language Models are undoubtedly the biggest part of the current AI wave and is currently the area where most research and investment is going towards. This year we have seen significant improvements at the frontier in capabilities as well as a brand new scaling paradigm.</p><h2 id="architecture">Architecture</h2><p>While we have seen attempts to introduce new architectures such as <a href="https://arxiv.org/abs/2312.00752">Mamba</a> and more recently <a href="https://arxiv.org/abs/2405.04517">xLSTM</a> to just name a few, it seems likely that the decoder-only transformer is here to stay - at least for the most part.</p><h3 id="dense-transformer">Dense Transformer</h3><p>The current “best” open-weights models are the Llama 3 series of models and Meta seems to have gone all-in to train the best possible vanilla Dense transformer. Dense transformers across the labs have in my view, converged to what I call the <a href="https://gist.github.com/nreHieW/a4ae05d216c5326c9fb9a70fcdda3274">Noam Transformer</a> (because of Noam Shazeer). This is essentially a stack of decoder-only transformer blocks using <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>, <a href="https://arxiv.org/abs/2305.13245">Group Query Attention</a>, some form of <a href="https://arxiv.org/abs/2002.05202">Gated Linear Unit</a> and <a href="https://arxiv.org/abs/2104.09864">Rotary Positional Embeddings</a>. Optionally, some labs also choose to interleave sliding window attention blocks.</p><div><p><img src="https://nrehiew.github.io/blog/2024/images/Dense.png" alt="Llama3"/></p><p>Llama 3 Architecture</p></div></div><div><p>© 2025. All rights reserved.</p></div></div>
  </body>
</html>
