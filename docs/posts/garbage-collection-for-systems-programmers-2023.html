<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bitbashing.io/gc-for-systems-programmers.html">Original</a>
    <h1>Garbage Collection for Systems Programmers (2023)</h1>
    
    <div id="readability-page-1" class="page"><article>
    <p><img src="https://assets.bitbashing.io/images/gofast.png" alt="gotta go fast"/></p>

<p>Let’s talk about one of the most performance-sensitive programs you run every day:
your operating system.
Since every speedup gives you more computer to compute with,
an OS is never fast enough, so you can always find
kernel and driver developers optimizing the bejesus out of their code.</p>

<p>Operating systems also need to be massively concurrent.
Not only is your OS scheduling all userspace processes and threads, but a kernel
has many threads of its own,
as well as interrupt handlers to interact with your hardware.
You want to minimize time spent waiting around, because again,
you’re robbing your users any time you do.</p>

<p>Put these two goals together and you’ll find many strange and magical methods
for locklessly sharing data between threads.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>
Let’s talk about one of those. Let’s talk about <em>RCU</em>.</p>

<h2 id="rcu">RCU</h2>

<p>Say we have data that is read <em>constantly</em> but written <em>rarely</em>—something
like the set of USB devices currently plugged in.
In computer years this set changes once a millennium, but it <em>can</em> change.
And when it does, it should change <em>atomically</em>, without blocking
any readers that happen to be taking a peak.</p>

<p>A surprisingly simple solution is to have the writer:</p>

<ol>
  <li>
    <p>Read the existing data from a pointer.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p>
  </li>
  <li>
    <p>Copy it, and apply the changes needed to make the next version.</p>
  </li>
  <li>
    <p>Atomically update the pointer so it points at the new version.</p>
  </li>
</ol>

<p><img src="https://assets.bitbashing.io/images/pointer-swap.jpg" alt="pointer swap" height="500"/></p>

<p>We might call this strategy, uh, <em>Read, Copy, Update</em>.
As code, it resembles something like:</p>

<div><div><pre><code><span>// Some big ball of state...</span>
<span>struct</span> <span>Foo</span> <span>{</span>
    <span>int</span> <span>lots</span><span>;</span>
    <span>string</span> <span>o</span><span>;</span>
    <span>big_hash_map</span> <span>fields</span><span>;</span>
<span>};</span>

<span>// ...is shared between readers and writer by this pointer.</span>
<span>atomic</span><span>&lt;</span><span>Foo</span><span>*&gt;</span> <span>sharedFoo</span><span>;</span>

<span>// Readers just... read the pointer.</span>
<span>const</span> <span>Foo</span><span>*</span> <span>readFoo</span><span>()</span> <span>{</span> <span>return</span> <span>shared_foo</span><span>.</span><span>load</span><span>();</span> <span>}</span>

<span>// The writer calls this to atomically update our shared state.</span>
<span>// (Wrap this in a mutex to make it multi-producer, multi-consumer,</span>
<span>// but let&#39;s assume the common single-producer scenario here.)</span>
<span>void</span> <span>updateFoo</span><span>()</span> <span>{</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>old</span> <span>=</span> <span>shared_foo</span><span>.</span><span>load</span><span>();</span> <span>// Read</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>updated</span> <span>=</span> <span>makeNewVersion</span><span>(</span><span>old</span><span>);</span> <span>// Copy</span>
    <span>sharedFoo</span><span>.</span><span>store</span><span>(</span><span>updated</span><span>);</span> <span>// Update</span>
<span>}</span>
</code></pre></div></div>

<p>Awesome! It’s easy to use, it’s wait-free, and it
<em>leaks like a sieve</em>.</p>

<p><img src="https://assets.bitbashing.io/images/leak.jpg" alt="leak" height="600"/></p>

<p>Well that’s bad. Could we just delete the data?</p>
<div><div><pre><code><span>void</span> <span>updateFoo</span><span>()</span> <span>{</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>old</span> <span>=</span> <span>shared_foo</span><span>.</span><span>load</span><span>();</span> <span>// Read</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>updated</span> <span>=</span> <span>makeNewVersion</span><span>(</span><span>old</span><span>);</span> <span>// Copy</span>
    <span>sharedFoo</span><span>.</span><span>store</span><span>(</span><span>updated</span><span>);</span> <span>// Update</span>
    <span>delete</span> <span>old</span><span>;</span> <span>// DANGER WILL ROBINSON</span>
<span>}</span>
</code></pre></div></div>
<p>No, actually. Not unless you like use-after-free bugs. This is all happening locklessly,
so how do we know there aren’t still readers looking at that <code>old</code> version?</p>

<figure>
<img src="https://assets.bitbashing.io/images/timeline-use-after-free.jpg" alt="Use after free timeline"/>
<figcaption>
Here a reader (<code>R2</code> in green) is still using the old version after the writer
(in purple) has updated the shared pointer. Subsequent readers (like <code>R3</code>) will
see the new version, but the writer doesn&#39;t know when <code>R2</code> will finish!
</figcaption>
</figure>

<p>Could readers, um, just tell us?</p>
<div><div><pre><code><span>void</span> <span>someReader</span><span>()</span> <span>{</span>
    <span>// Tell the writer that someone is reading.</span>
    <span>rcu_read_lock</span><span>();</span>

    <span>const</span> <span>Foo</span><span>*</span> <span>f</span> <span>=</span> <span>readFoo</span><span>();</span>
    <span>doThings</span><span>(</span><span>f</span><span>);</span>

    <span>// Tell the writer we&#39;re done.</span>
    <span>rcu_read_unlock</span><span>();</span>
<span>}</span>
</code></pre></div></div>
<p>This defines a sort of read-side critical section—readers still never block,
but they <em>can</em> make the writer wait to axe any data they’re still looking at.</p>
<div><div><pre><code><span>void</span> <span>updateFoo</span><span>()</span> <span>{</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>old</span> <span>=</span> <span>shared_foo</span><span>.</span><span>load</span><span>();</span> <span>// Read</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>updated</span> <span>=</span> <span>makeNewVersion</span><span>(</span><span>old</span><span>);</span> <span>// Copy</span>
    <span>sharedFoo</span><span>.</span><span>store</span><span>(</span><span>updated</span><span>);</span> <span>// Update</span>

    <span>// Wait for current readers to &#34;unlock&#34;</span>
    <span>// and leave their critical sections.</span>
    <span>rcu_synchronize</span><span>();</span>

    <span>delete</span> <span>old</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>And so,</p>
<figure>
<img src="https://assets.bitbashing.io/images/timeline-synchronize.jpg" alt="rcu_synchronize() timeline"/>
<figcaption>
Notice that we don&#39;t wait until there&#39;s <em>zero</em> readers—once again,
<code>R3</code> gets the new version of the data, so it doesn&#39;t care about the fate of
whatever came before it.
<code>rcu_synchronize()</code> just needs to wait for <em>previous</em> readers—ones which might
be looking at <code>old</code>—to finish.
</figcaption>
</figure>

<p>Normal people would be content with this solution,
but kernel developers aren’t normal people.
We’ve got a blocking writer now,
and even though we weren’t optimizing the writer side,
blocking still makes them very sad.</p>

<p>Suppose we don’t wait around <em>in</em> our update function to free the old data.
Our code is correct so long as that happens <em>eventually</em>,
right?. What if we “deferred” that?</p>
<div><div><pre><code><span>void</span> <span>updateFoo</span><span>()</span> <span>{</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>old</span> <span>=</span> <span>shared_foo</span><span>.</span><span>load</span><span>();</span> <span>// Read</span>
    <span>const</span> <span>Foo</span><span>*</span> <span>updated</span> <span>=</span> <span>makeNewVersion</span><span>(</span><span>old</span><span>);</span> <span>// Copy</span>
    <span>sharedFoo</span><span>.</span><span>store</span><span>(</span><span>updated</span><span>);</span> <span>// Update</span>

    <span>// Our cool library can free `old` any time after</span>
    <span>// current readers leave their critical sections.</span>
    <span>rcu_defer</span><span>(</span><span>old</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p><img src="https://assets.bitbashing.io/images/timeline-defer.jpg" alt="rcu_defer() timeline"/></p>

<p>All’s well if we free <code>old</code> anywhere in squiggly time.
We could even have a dedicated thread occasionally sweep through all the
old, unreferenced versions of the data and…</p>

<p>…wait, did we just build a generational garbage collector?
Of immutable data structures, no less?</p>

<h2 id="wat">Wat</h2>

<p>This isn’t some thought experiment—RCU is <em>very</em> real, and very useful.
Linux uses it tens of thousands of times.
It’s provided in
<a href="https://github.com/facebook/folly/blob/main/folly/synchronization/Rcu.h">Facebook’s Folly C++ library</a>.
And in Rust it goes by <a href="https://crates.io/crates/crossbeam-epoch"><code>crossbeam-epoch</code></a>
and underpins one of the most popular concurrency libraries.</p>

<figure>
<figcaption>
<b>Therapist:</b> Kernel garbage collection isn&#39;t real and it can&#39;t hurt you.</figcaption>
<img src="https://assets.bitbashing.io/images/linux-RCU.png"/>
</figure>

<p>At this point, some folks fire back with non-arguments about
how this isn’t “real” garbage collection.
Like, uh, because you manually mark the garbage!
I’m not here to argue taxonomy—whatever you want to call it,
RCU has the same shape as GC: memory is cleaned up <em>eventually</em>,
based on whether it’s still in use.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup> And it’s an interesting example that
cuts against the prevailing wisdom that garbage collection is:</p>

<ol>
  <li>
    <p>Slower than manual memory management</p>
  </li>
  <li>
    <p>Takes away the fine-grained control you need when writing systems software</p>
  </li>
</ol>

<p>These arguments are clearly bullshit for RCU, which is <em>motivated</em> by
performance and latency demands, not used as a convenience in spite of its costs.
And we’re not doing any extra work, we’re just moving it out of the critical path.</p>

<p>…Are these arguments just generally bullshit, too?</p>

<h2 id="gc-is-not-magically-slow-or-malloc-is-not-magically-fast">GC is not magically slow, OR: malloc() is not magically fast</h2>

<p>The common wisdom that garbage collectors are inherently less efficient
than traditional/manual memory management falls apart
pretty quickly when you look into the details of how these things
actually work. Consider:</p>

<ul>
  <li>
    <p><strong><code>free()</code> is not free.</strong> A general-purpose memory allocator has to maintain lots of internal,
global state. What pages have we gotten from the kernel? How did we split those
up into buckets for differently-sized allocations? Which of those buckets are in use?
This gives you frequent contention between threads as they try to lock
the allocator’s state,
or you do as <a href="https://jemalloc.net/">jemalloc</a> does and keep thread-local pools
that have to be synchronized with even more code.</p>

    <p>Tools to automate the “actually freeing the memory” part, like
lifetimes in Rust and RAII in C++, don’t solve these problems.
They absolutely aid <em>correctness</em>, something else you should care deeply about,
but they do nothing to simplify all this machinery.
<a href="https://bitbashing.io/async-rust.html">Many scenarios</a> also require you to fall back to
<a href="https://en.cppreference.com/w/cpp/memory/shared_ptr"><code>shared_ptr</code></a>/<a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc</code></a>,
and these in turn demand even more metadata (reference counts)
that bounces between cores and caches.
And they leak cycles in your liveness graph to boot.</p>
  </li>
  <li>
    <p><strong>Modern garbage collection offers optimizations that alternatives can not.</strong>
A moving, generational GC periodically recompacts the heap.
This provides insane throughput, since allocation is little more than a pointer bump!
It also gives sequential allocations great locality,
helping cache performance.</p>
  </li>
</ul>

<h2 id="the-illusion-of-control">The Illusion of Control</h2>

<p>Many developers opposed to garbage collection are building “soft” real-time systems.
They want to go as fast as possible—more FPS in my video game!
Better compression in my streaming codec!
But they don’t have <em>hard</em> latency requirements.
Nothing will break and nobody will die if the system occasionally takes an extra
millisecond.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>But even when we’re not on the <a href="https://www.usenix.org/system/files/1311_05-08_mickens.pdf">Night Watch</a>,
we don’t want to randomly stop the world for some garbage collector, right?</p>

<h3 id="lies-people-believe-about-memory-management">Lies people believe about memory management</h3>

<ul>
  <li>
    <p><strong>The programmer can decide when memory management happens.</strong>
The wonderful thing about an operating system is that it abstracts our interactions
with hardware. The terrible thing about an operating system is that it abstracts
interactions with hardware.
Linux, by default, does almost nothing when asked for memory, only handing it
out <a href="https://lwn.net/Articles/104185/">once you actually try to use it</a>.
In our wacky world of <a href="https://man7.org/linux/man-pages/man2/madvise.2.html"><code>madvise()</code></a>,
memory-mapped I/O, and file system caches, there’s no simple answer to,
“what’s allocated and when?”
We can only <em>hint</em> at our intentions,
then let the OS do its best.
Usually it does a great job, but on a bad day, a simple pointer access
can turn into disk I/O!</p>
  </li>
  <li>
    <p><strong>The programmer knows the best times to pause for memory management.</strong>
Sometimes there <em>are</em> obvious answers—like on the loading screen
of a video game. But the only obvious answer for lots of other
software is just, “whenever we’re not busy with more critical work.”
Our friends <code>shared_ptr</code> and <code>Arc</code> cloud our reasoning here, too—individual
pieces of code holding a reference-counted pointer can’t know <em>a priori</em> if
they’re going to be the last owner stuck with the cleanup.
(If they could know, we wouldn’t need reference counting there!)</p>
  </li>
  <li>
    <p><strong>Calling <code>free()</code> gives the memory back to the OS.</strong>
Memory is allocated from the operating system in pages,
and the allocator often holds onto those pages until the program exits.
It tries to reuse them, to avoid bugging the OS more than necessary.
Not to say the OS can’t
<a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html"><em>take</em> pages back by swapping them out…</a></p>
  </li>
</ul>

<h2 id="takeaways">Takeaways</h2>

<p>I’m not suggesting that <em>all</em> software would benefit from garbage collection.
Some certainly won’t.
But it’s almost 2024, and any mention of GC—especially in my
milieu of systems programmers—<em>still</em> drowns
in false dichotomies and FUD.
<em>GC is for dum dums,
too lazy or incompetent to write an “obviously” faster version in a language
with manual memory management.</em></p>

<p><img src="https://assets.bitbashing.io/images/just-use-gc.jpg" alt="Just use GC."/></p>

<p>It’s just not true. <a href="https://www.destroyallsoftware.com/talks/ideology">It’s ideology.</a>
And I bought it for over a decade until I joined a team that builds systems—systems
people bet their lives on—that provide sub-microsecond latency,
using a garbage-collected language that allocates on nearly every line.
It turns out modern GCs provide amazing throughput, and you don’t need to
throw that out for manual memory management just because <em>some</em> of your system
absolutely needs to run in <em>n</em> clock cycles.
(Those <em>specific parts</em> can be relegated to non-GC code, or even hardware!)</p>

<p>Garbage collection isn’t a silver bullet.
<a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">We don’t have those.</a>
But it’s another tool in the toolbox that we shouldn’t be afraid to use.</p>

<hr/>



  </article></div>
  </body>
</html>
