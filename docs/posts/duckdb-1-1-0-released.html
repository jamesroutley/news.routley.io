<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://duckdb.org/2024/09/09/announcing-duckdb-110.html">Original</a>
    <h1>DuckDB 1.1.0 Released</h1>
    
    <div id="readability-page-1" class="page"><div>
							
							
							
							
							
							
								<p><em>TL;DR: The DuckDB team is happy to announce that today we&#39;re releasing DuckDB version 1.1.0, codenamed “Eatoni”.</em></p>
							
							<p>To install the new version, please visit the <a href="https://duckdb.org/docs/installation/">installation guide</a>.
For the release notes, see the <a href="https://github.com/duckdb/duckdb/releases/tag/v1.1.0">release page</a>.</p>

<blockquote>
  <p>Some packages (R, Java) take a few extra days to release due to the reviews required in the release pipelines.</p>
</blockquote>

<p>We are proud to release DuckDB 1.1.0, our first release since we released version 1.0.0 three months ago.
This release is codenamed “Eatoni” after the <a href="https://en.wikipedia.org/wiki/Eaton%27s_pintail">Eaton&#39;s pintail (Anas eatoni)</a>,
a dabbling duck that occurs only on two very remote island groups in the southern Indian Ocean.</p>
      <h2 id="whats-new-in-110">
        
        <a href="#whats-new-in-110">What&#39;s New in 1.1.0</a>
        
      </h2>
    

<p>There have been far too many changes to discuss them each in detail, but we would like to highlight several particularly exciting features!</p>

<p>Below is a summary of those new features with examples.</p>

<ul>
  <li><a href="#breaking-sql-changes">Breaking SQL Changes</a></li>
  <li><a href="#community-extensions">Community Extensions</a></li>
  <li><a href="#friendly-sql">Friendly SQL</a>
    <ul>
      <li><a href="#unpacked-columns">Unpacked Columns</a></li>
      <li><a href="#query-and-query_table-functions"><code>query</code> and <code>query_table</code> Functions</a></li>
    </ul>
  </li>
  <li><a href="#performance">Performance</a>
    <ul>
      <li><a href="#dynamic-filter-pushdown-from-joins">Dynamic Filter Pushdown from Joins</a></li>
      <li><a href="#automatic-cte-materialization">Automatic CTE Materialization</a></li>
      <li><a href="#parallel-streaming-queries">Parallel Streaming Queries</a></li>
      <li><a href="#parallel-union-by-name">Parallel Union By Name</a></li>
      <li><a href="#nested-art-rework-foreign-key-load-speed-up">Nested ART Rework (Foreign Key Load Speed-Up)</a></li>
      <li><a href="#window-function-improvements">Window Function Improvements</a></li>
    </ul>
  </li>
  <li><a href="#spatial-features">Spatial Features</a>
    <ul>
      <li><a href="#geoparquet">GeoParquet</a></li>
      <li><a href="#r-tree">R-Tree</a></li>
    </ul>
  </li>
  <li><a href="#final-thoughts">Final Thoughts</a></li>
</ul>
      <h2 id="breaking-sql-changes">
        
        <a href="#breaking-sql-changes">Breaking SQL Changes</a>
        
      </h2>
    

<p><a href="https://github.com/duckdb/duckdb/pull/13493"><strong>IEEE-754 semantics for division by zero.</strong></a> The <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE-754 floating point standard</a> states that division by zero returns <code>inf</code>. Previously, DuckDB would return <code>NULL</code> when dividing by zero, also for floating point division. Starting with this release, DuckDB will return <code>inf</code> instead.</p>

<div><div><pre><code><span>SELECT</span> <span>1</span> <span>/</span> <span>0</span> <span>AS</span> <span>division_by_zero</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌──────────────────┐
│ division_by_zero │
│      double      │
├──────────────────┤
│              inf │
└──────────────────┘
</code></pre></div></div>

<p>The <code>ieee_floating_point_ops</code> can be set to <code>false</code> to revert this behavior:</p>

<div><div><pre><code><span>SET</span> <span>ieee_floating_point_ops</span> <span>=</span> <span>false</span><span>;</span>
<span>SELECT</span> <span>1</span> <span>/</span> <span>0</span> <span>AS</span> <span>division_by_zero</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌──────────────────┐
│ division_by_zero │
│      double      │
├──────────────────┤
│             NULL │
└──────────────────┘
</code></pre></div></div>

<p><a href="https://github.com/duckdb/duckdb/pull/13514"><strong>Error when scalar subquery returns multiple values.</strong></a> Scalar subqueries can only return a single value per input row. Previously, DuckDB would match SQLite&#39;s behavior and select an arbitrary row to return when multiple rows were returned. In practice this behavior often led to confusion. Starting with this release, an error is returned instead, matching the behavior of Postgres. The subquery can be wrapped with <code>ARRAY</code> to collect all of the results of the subquery in a list.</p>

<div><div><pre><code><span>SELECT</span> <span>(</span><span>SELECT</span> <span>unnest</span><span>(</span><span>range</span><span>(</span><span>10</span><span>)));</span>
</code></pre></div></div>

<div><div><pre><code>Invalid Input Error: More than one row returned by a subquery used as
an expression - scalar subqueries can only return a single row.
</code></pre></div></div>

<div><div><pre><code><span>SELECT</span> <span>ARRAY</span><span>(</span><span>SELECT</span> <span>unnest</span><span>(</span><span>range</span><span>(</span><span>10</span><span>)))</span> <span>AS</span> <span>subquery_result</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌────────────────────────────────┐
│        subquery_result         │
│            int64[]             │
├────────────────────────────────┤
│ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] │
└────────────────────────────────┘
</code></pre></div></div>

<p>The <code>scalar_subquery_error_on_multiple_rows</code> setting can be set to <code>false</code> to revert this behavior.</p>

<div><div><pre><code><span>SET</span> <span>scalar_subquery_error_on_multiple_rows</span><span>=</span><span>false</span><span>;</span>
<span>SELECT</span> <span>(</span><span>SELECT</span> <span>unnest</span><span>(</span><span>range</span><span>(</span><span>10</span><span>)))</span> <span>as</span> <span>result</span><span>;</span>
</code></pre></div></div>
<div><div><pre><code>┌────────┐
│ result │
│ int64  │
├────────┤
│      0 │
└────────┘
</code></pre></div></div>
      
    

<p>Recently we introduced <a href="https://duckdb.org/2024/07/05/community-extensions.html">Community Extensions</a>. Community extensions allow anyone to build extensions for DuckDB, that are then built and distributed by us. The <a href="https://community-extensions.duckdb.org/list_of_extensions.html">list of community extensions</a> has been growing since then.</p>

<p>In this release, we have been working towards making community extensions easier to build and produce. This release includes a new method of registering extensions <a href="https://github.com/duckdb/duckdb/pull/12682">using the C API</a> in addition to a lot of extensions to the C API allowing <a href="https://github.com/duckdb/duckdb/pull/11786">scalar functions</a>, <a href="https://github.com/duckdb/duckdb/pull/13229">aggregate functions</a> and <a href="https://github.com/duckdb/duckdb/pull/13499">custom types</a> to be defined. These changes will enable building extensions against a stable API, that are smaller in size, that will work across different DuckDB versions. In addition, these changes will enable building extensions in other programming languages in the future.</p>
      <h2 id="friendly-sql">
        
        <a href="#friendly-sql">Friendly SQL</a>
        
      </h2>
    

<p><a href="https://github.com/duckdb/duckdb/pull/12590"><strong>Histogram.</strong></a> This version introduces the <code>histogram</code> function that can be used to compute histograms over columns of a dataset. The histogram function works for columns of any type, and allows for various different binning strategies and a custom amount of bins.</p>

<div><div><pre><code><span>FROM</span> <span>histogram</span><span>(</span>
    <span>&#39;https://blobs.duckdb.org/data/ontime.parquet&#39;</span><span>,</span>
    <span>UniqueCarrier</span><span>,</span>
    <span>bin_count</span> <span>:</span><span>=</span> <span>5</span>
<span>);</span>
</code></pre></div></div>

<div><div><pre><code>┌────────────────┬────────┬──────────────────────────────────────────────────────────────────────────────────┐
│      bin       │ count  │                                       bar                                        │
│    varchar     │ uint64 │                                     varchar                                      │
├────────────────┼────────┼──────────────────────────────────────────────────────────────────────────────────┤
│ AA             │ 677215 │ ██████████████████████████████████████████████████████▏                          │
│ DL             │ 696931 │ ███████████████████████████████████████████████████████▊                         │
│ OO             │ 521956 │ █████████████████████████████████████████▊                                       │
│ UA             │ 435757 │ ██████████████████████████████████▉                                              │
│ WN             │ 999114 │ ████████████████████████████████████████████████████████████████████████████████ │
│ (other values) │ 945484 │ ███████████████████████████████████████████████████████████████████████████▋     │
└────────────────┴────────┴──────────────────────────────────────────────────────────────────────────────────┘
</code></pre></div></div>

<p><a href="https://github.com/duckdb/duckdb/pull/13084"><strong>SQL variables.</strong></a> This release introduces support for variables that can be defined in SQL. Variables can hold a single value of any type – including nested types like lists or structs. Variables can be set as literals, or from scalar subqueries.</p>

<p>The value stored within variables can be read using <code>getvariable</code>. When used in a query, <code>getvariable</code> is treated as a literal during query planning and optimization. This allows variables to be used in places where we normally cannot read values from within tables, for example, when specifying which CSV files to read:</p>

<div><div><pre><code><span>SET</span> <span>VARIABLE</span> <span>list_of_files</span> <span>=</span> <span>(</span><span>SELECT</span> <span>LIST</span><span>(</span><span>file</span><span>)</span> <span>FROM</span> <span>csv_files</span><span>);</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>read_csv</span><span>(</span><span>getvariable</span><span>(</span><span>&#39;list_of_files&#39;</span><span>),</span> <span>filename</span> <span>:</span><span>=</span> <span>true</span><span>);</span>
</code></pre></div></div>

<div><div><pre><code>┌───────┬───────────┐
│   a   │ filename  │
│ int64 │  varchar  │
├───────┼───────────┤
│    42 │ test.csv  │
│    84 │ test2.csv │
└───────┴───────────┘
</code></pre></div></div>
      <h3 id="unpacked-columns">
        
        <a href="#unpacked-columns">Unpacked Columns</a>
        
      </h3>
    

<p>The <a href="https://duckdb.org/docs/sql/expressions/star.html#columns-expression"><code>COLUMNS</code> expression</a> allows users to write dynamic SQL over a set of columns without needing to explicitly list the columns in the SQL string. Instead, the columns can be selected through either a regex or computed with a <a href="https://duckdb.org/2024/08/08/friendly-lists-and-their-buddies-the-lambdas.html">lambda function</a>.</p>

<p>This release expands this capability by <a href="https://github.com/duckdb/duckdb/pull/11872">allowing the <code>COLUMNS</code> expression to be <em>unpacked</em> into a function</a>.
This is especially useful when combined with nested functions like <code>struct_pack</code> or <code>list_value</code>.</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>many_measurements</span><span>(</span>
    <span>id</span> <span>INTEGER</span><span>,</span> <span>m1</span> <span>INTEGER</span><span>,</span> <span>m2</span> <span>INTEGER</span><span>,</span> <span>m3</span> <span>INTEGER</span>
<span>);</span>
<span>INSERT</span> <span>INTO</span> <span>many_measurements</span> <span>VALUES</span> <span>(</span><span>1</span><span>,</span> <span>10</span><span>,</span> <span>100</span><span>,</span> <span>20</span><span>);</span>

<span>SELECT</span> <span>id</span><span>,</span> <span>struct_pack</span><span>(</span><span>*</span><span>COLUMNS</span><span>(</span><span>&#39;m</span><span>\d</span><span>&#39;</span><span>))</span> <span>AS</span> <span>measurements</span>
<span>FROM</span> <span>many_measurements</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌───────┬────────────────────────────────────────────┐
│  id   │                measurements                │
│ int32 │ struct(m1 integer, m2 integer, m3 integer) │
├───────┼────────────────────────────────────────────┤
│     1 │ {&#39;m1&#39;: 10, &#39;m2&#39;: 100, &#39;m3&#39;: 20}            │
└───────┴────────────────────────────────────────────┘
</code></pre></div></div>
      <h3 id="query-and-query_table-functions">
        
        <a href="#query-and-query_table-functions"><code>query</code> and <code>query_table</code> Functions</a>
        
      </h3>
    

<p>The <a href="https://github.com/duckdb/duckdb/pull/10586"><code>query</code> and <code>query_table</code> functions</a> take a string literal, and convert it into a <code>SELECT</code> subquery or a table reference. Note that these functions can only take literal strings. As such, they are not as powerful (or dangerous) as a generic <code>eval</code>.</p>

<p>These functions are conceptually simple, but enable powerful and more dynamic SQL. For example, they allow passing in a table name as a prepared statement parameter:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>my_table</span><span>(</span><span>i</span> <span>INT</span><span>);</span>
<span>INSERT</span> <span>INTO</span> <span>my_table</span> <span>VALUES</span> <span>(</span><span>42</span><span>);</span>

<span>PREPARE</span> <span>select_from_table</span> <span>AS</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>query_table</span><span>(</span><span>$</span><span>1</span><span>);</span>
<span>EXECUTE</span> <span>select_from_table</span><span>(</span><span>&#39;my_table&#39;</span><span>);</span>
</code></pre></div></div>

<div><div><pre><code>┌───────┐
│   i   │
│ int32 │
├───────┤
│    42 │
└───────┘
</code></pre></div></div>

<p>When combined with the <code>COLUMNS</code> expression, we can write very generic SQL-only macros. For example, below is a custom version of <code>SUMMARIZE</code> that computes the <code>min</code> and <code>max</code> of every column in a table:</p>

<div><div><pre><code><span>CREATE</span> <span>OR</span> <span>REPLACE</span> <span>MACRO</span> <span>my_summarize</span><span>(</span><span>table_name</span><span>)</span> <span>AS</span> <span>TABLE</span>
<span>SELECT</span>
    <span>unnest</span><span>([</span><span>*</span><span>COLUMNS</span><span>(</span><span>&#39;alias_.*&#39;</span><span>)])</span> <span>AS</span> <span>column_name</span><span>,</span>
    <span>unnest</span><span>([</span><span>*</span><span>COLUMNS</span><span>(</span><span>&#39;min_.*&#39;</span><span>)])</span> <span>AS</span> <span>min_value</span><span>,</span>
    <span>unnest</span><span>([</span><span>*</span><span>COLUMNS</span><span>(</span><span>&#39;max_.*&#39;</span><span>)])</span> <span>AS</span> <span>max_value</span>
<span>FROM</span> <span>(</span>
    <span>SELECT</span>
        <span>any_value</span><span>(</span><span>alias</span><span>(</span><span>COLUMNS</span><span>(</span><span>*</span><span>)))</span> <span>AS</span> <span>&#34;alias_</span><span>\0</span><span>&#34;</span><span>,</span>
        <span>min</span><span>(</span><span>COLUMNS</span><span>(</span><span>*</span><span>))::</span><span>VARCHAR</span> <span>AS</span> <span>&#34;min_</span><span>\0</span><span>&#34;</span><span>,</span>
        <span>max</span><span>(</span><span>COLUMNS</span><span>(</span><span>*</span><span>))::</span><span>VARCHAR</span> <span>AS</span> <span>&#34;max_</span><span>\0</span><span>&#34;</span>
    <span>FROM</span> <span>query_table</span><span>(</span><span>table_name</span><span>::</span><span>VARCHAR</span><span>)</span>
<span>);</span>

<span>SELECT</span> <span>*</span>
<span>FROM</span> <span>my_summarize</span><span>(</span><span>&#39;https://blobs.duckdb.org/data/ontime.parquet&#39;</span><span>)</span>
<span>LIMIT</span> <span>3</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌─────────────┬───────────┬───────────┐
│ column_name │ min_value │ max_value │
│   varchar   │  varchar  │  varchar  │
├─────────────┼───────────┼───────────┤
│ year        │ 2017      │ 2017      │
│ quarter     │ 1         │ 3         │
│ month       │ 1         │ 9         │
└─────────────┴───────────┴───────────┘
</code></pre></div></div>
      <h2 id="performance">
        
        <a href="#performance">Performance</a>
        
      </h2>
    
      <h3 id="dynamic-filter-pushdown-from-joins">
        
        <a href="#dynamic-filter-pushdown-from-joins">Dynamic Filter Pushdown from Joins</a>
        
      </h3>
    

<p>This release adds a <em>very cool</em> optimization for joins: DuckDB now <a href="https://github.com/duckdb/duckdb/pull/12908">automatically creates filters</a> for the larger table in the join during execution. Say we are joining two tables <code>A</code> and <code>B</code>. <code>A</code> has 100 rows, and <code>B</code> has one million rows. We are joining on a shared key <code>i</code>. If there were any filter on <code>i</code>, DuckDB would already push that filter into the scan, greatly reducing the cost to complete the query. But we are now filtering on another column from <code>A</code>, namely <code>j</code>:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>A</span> <span>AS</span> <span>SELECT</span> <span>range</span> <span>i</span><span>,</span> <span>range</span> <span>j</span> <span>FROM</span> <span>range</span><span>(</span><span>100</span><span>);</span>
<span>CREATE</span> <span>TABLE</span> <span>B</span> <span>AS</span> <span>SELECT</span> <span>a</span><span>.</span><span>range</span> <span>i</span> <span>FROM</span> <span>range</span><span>(</span><span>100</span><span>)</span> <span>a</span><span>,</span> <span>range</span><span>(</span><span>10_000</span><span>)</span> <span>b</span><span>;</span>
<span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>A</span> <span>JOIN</span> <span>B</span> <span>USING</span> <span>(</span><span>i</span><span>)</span> <span>WHERE</span> <span>j</span> <span>&gt;</span> <span>90</span><span>;</span>
</code></pre></div></div>

<p>DuckDB will execute this join by building a hash table on the smaller table A, and then probe said hash table with the contents of B. DuckDB will now observe the values of i during construction of the hash table on A. It will then create a min-max range filter of those values of i and then <em>automatically</em> apply that filter to the values of i in B! That way, we early remove (in this case) 90% of data from the large table before even looking at the hash table. In this example, this leads to a roughly 10× improvement in query performance. The optimization can also be observed in the output of <code>EXPLAIN ANALYZE</code>.</p>
      <h3 id="automatic-cte-materialization">
        
        <a href="#automatic-cte-materialization">Automatic CTE Materialization</a>
        
      </h3>
    

<p>Common Table Expressions (CTE) are a convenient way to break up complex queries into manageable pieces without endless nesting of subqueries. Here is a small example for a CTE:</p>

<div><div><pre><code><span>WITH</span> <span>my_cte</span> <span>AS</span> <span>(</span><span>SELECT</span> <span>range</span> <span>AS</span> <span>i</span> <span>FROM</span> <span>range</span><span>(</span><span>10</span><span>))</span>
<span>SELECT</span> <span>i</span> <span>FROM</span> <span>my_cte</span> <span>WHERE</span> <span>i</span> <span>&gt;</span> <span>5</span><span>;</span>
</code></pre></div></div>

<p>Sometimes, the same CTE is referenced multiple times in the same query. Previously, the CTE would be “copied” wherever it appeared. This creates a potential performance problem: if computing the CTE is computationally expensive, it would be better to cache (“materialize”) its results instead of computing the result multiple times in different places within the same query. However, different filter conditions might apply for different instantiations of the CTE, which could drastically reduce their computation cost. A classical no-win scenario in databases. It was <a href="https://duckdb.org/docs/sql/query_syntax/with.html">already possible</a> to explicitly mark a CTE as materialized using the <code>MATERIALIZED</code> keyword, but that required manual intervention.</p>

<p>This release adds a feature where DuckDB <a href="https://github.com/duckdb/duckdb/pull/12290">automatically decides</a> whether a CTE result should be materialized or not using a heuristic. The heuristic currently is that if the CTE performs aggregation and is queried more than once, it should be materialized. We plan to expand that heuristic in the future.</p>
      <h3 id="parallel-streaming-queries">
        
        <a href="#parallel-streaming-queries">Parallel Streaming Queries</a>
        
      </h3>
    

<p>DuckDB has two different methods for fetching results: <em>materialized</em> results and <em>streaming</em> results. Materialized results fetch all of the data that is present in a result at once, and return it. Streaming results instead allow iterating over the data in incremental steps. Streaming results are critical when working with large result sets as they do not require the entire result set to fit in memory. However, in previous releases, the final streaming phase was limited to a single thread.</p>

<p>Parallelism is critical for obtaining good query performance on modern hardware, and this release adds support for <a href="https://github.com/duckdb/duckdb/pull/11494">parallel streaming of query results</a>. The system will use all available threads to fill up a query result buffer of a limited size (a few megabytes). When data is consumed from the result buffer, the threads will restart and start filling up the buffer again. The size of the buffer can be configured through the <code>streaming_buffer_size</code> parameter.</p>

<p>Below is a small benchmark using <a href="https://blobs.duckdb.org/data/ontime.parquet"><code>ontime.parquet</code></a> to illustrate the performance benefits that can be obtained using the Python streaming result interface:</p>

<div><div><pre><code><span>import</span> <span>duckdb</span>
<span>duckdb</span><span>.</span><span>sql</span><span>(</span><span>&#34;SELECT * FROM &#39;ontime.parquet&#39; WHERE flightnum = 6805;&#34;</span><span>).</span><span>fetchone</span><span>()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>v1.0</th>
      <th>v1.1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.17 s</td>
      <td>0.12 s</td>
    </tr>
  </tbody>
</table>
      <h3 id="parallel-union-by-name">
        
        <a href="#parallel-union-by-name">Parallel Union By Name</a>
        
      </h3>
    

<p>The <code>union_by_name</code> parameter allows combination of – for example – CSV files that have the same columns in them but not in the same order. This release <a href="https://github.com/duckdb/duckdb/pull/12957">adds support for parallelism</a> when using <code>union_by_name</code>. This greatly improves reading performance when using the union by name feature on multiple files.</p>
      <h3 id="nested-art-rework-foreign-key-load-speed-up">
        
        <a href="#nested-art-rework-foreign-key-load-speed-up">Nested ART Rework (Foreign Key Load Speed-Up)</a>
        
      </h3>
    

<p>We have <a href="https://github.com/duckdb/duckdb/pull/13373">greatly improved</a> index insertion and deletion performance for foreign keys. Normally, we directly inline row identifiers into the tree structure. However, this is impossible for indexes that contain a lot of duplicates, as is the case with foreign keys. Instead, we now actually create another index entry for each key that is itself another “recursive” index tree in its own right. That way, we can achieve good insertion and deletion performance inside index entries. The performance results of this change are drastic, consider the following example where a has 100 rows and b has one million rows that all reference a:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>a</span> <span>(</span><span>i</span> <span>integer</span><span>,</span> <span>PRIMARY</span> <span>KEY</span> <span>(</span><span>i</span><span>));</span>
<span>CREATE</span> <span>TABLE</span> <span>b</span> <span>(</span><span>i</span> <span>integer</span><span>,</span> <span>FOREIGN</span> <span>KEY</span> <span>(</span><span>i</span><span>)</span> <span>REFERENCES</span> <span>a</span><span>(</span><span>i</span><span>));</span>

<span>INSERT</span> <span>INTO</span> <span>a</span> <span>FROM</span> <span>range</span><span>(</span><span>100</span><span>);</span>
<span>INSERT</span> <span>INTO</span> <span>b</span> <span>SELECT</span> <span>a</span><span>.</span><span>range</span> <span>FROM</span> <span>range</span><span>(</span><span>100</span><span>)</span> <span>a</span><span>,</span> <span>range</span><span>(</span><span>10_000</span><span>)</span> <span>b</span><span>;</span>
</code></pre></div></div>

<p>On the previous version, this would take ca. 10s on a MacBook to complete. It now takes 0.2s thanks to the new index structure, a ca. 50x improvement!</p>
      <h3 id="window-function-improvements">
        
        <a href="#window-function-improvements">Window Function Improvements</a>
        
      </h3>
    

<p>Window functions see a lot of use in DuckDB, which is why we are continuously improving performance of executing Window functions over large datasets.</p>

<p>The <a href="https://github.com/duckdb/duckdb/pull/12311"><code>DISTINCT</code></a> and <a href="https://github.com/duckdb/duckdb/pull/12250"><code>FILTER</code></a> window function modifiers can now be executed in streaming mode. Streaming mode means that the input data for the operator does not need to be completely collected and buffered before the operator can execute. For large intermediate results, this can have a very large performance impact. For example, the following query will now use the streaming window operator:</p>

<div><div><pre><code><span>SELECT</span>
    <span>sum</span><span>(</span><span>DISTINCT</span> <span>i</span><span>)</span>
        <span>FILTER</span> <span>(</span><span>i</span> <span>%</span> <span>3</span> <span>=</span> <span>0</span><span>)</span>
        <span>OVER</span> <span>(</span><span>ROWS</span> <span>BETWEEN</span> <span>UNBOUNDED</span> <span>PRECEDING</span> <span>AND</span> <span>CURRENT</span> <span>ROW</span><span>)</span>
<span>FROM</span> <span>range</span><span>(</span><span>10</span><span>)</span> <span>tbl</span><span>(</span><span>i</span><span>);</span>
</code></pre></div></div>

<p>We have <a href="https://github.com/duckdb/duckdb/pull/12685">also implemented streaming mode</a> for positive <code>lead</code> offsets.</p>

<p>We can now <a href="https://github.com/duckdb/duckdb/pull/10932">push filters on columns through window functions that are partitioned by the same column</a>. For example, consider the following scenario:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>tbl2</span> <span>AS</span> <span>SELECT</span> <span>range</span> <span>i</span> <span>FROM</span> <span>range</span><span>(</span><span>10</span><span>);</span>
<span>SELECT</span> <span>i</span>
<span>FROM</span> <span>(</span><span>SELECT</span> <span>i</span><span>,</span> <span>SUM</span><span>(</span><span>i</span><span>)</span> <span>OVER</span> <span>(</span><span>PARTITION</span> <span>BY</span> <span>i</span><span>)</span> <span>FROM</span> <span>tbl</span><span>)</span>
<span>WHERE</span> <span>i</span> <span>&gt;</span> <span>5</span><span>;</span>
</code></pre></div></div>

<p>Previously, the filter on <code>i</code> could not be pushed into the scan on <code>tbl</code>. But we now recognize that pushing this filter “through” the window is safe and the optimizer will do so. This can be verified through <code>EXPLAIN</code>:</p>

<div><div><pre><code>┌─────────────────────────────┐
│┌───────────────────────────┐│
││       Physical Plan       ││
│└───────────────────────────┘│
└─────────────────────────────┘
              …
┌─────────────┴─────────────┐
│           WINDOW          │
│    ────────────────────   │
│        Projections:       │
│ sum(i) OVER (PARTITION BY │
│             i)            │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         SEQ_SCAN          │
│    ────────────────────   │
│            tbl            │
│                           │
│       Projections: i      │
│                           │
│          Filters:         │
│   i&gt;5 AND i IS NOT NULL   │
│                           │
│          ~2 Rows          │
└───────────────────────────┘
</code></pre></div></div>

<p>The blocking (non-streaming) version of the window operator <a href="https://github.com/duckdb/duckdb/pull/12907">now processes input data in parallel</a>. This greatly reduces the footprint of the window operator.</p>

<p>See also <a href="https://www.youtube.com/watch?v=QubE0u8Kq7Y&amp;list=PLzIMXBizEZjhbacz4PWGuCUSxizmLei8Y&amp;index=8">Richard&#39;s talk on the topic</a> at <a href="https://duckdb.org/2024/08/15/duckcon5.html">DuckCon #5</a> in Seattle a few weeks ago.</p>
      <h2 id="spatial-features">
        
        <a href="#spatial-features">Spatial Features</a>
        
      </h2>
    
      <h3 id="geoparquet">
        
        <a href="#geoparquet">GeoParquet</a>
        
      </h3>
    

<p>GeoParquet is an extension format of the ubiquitous Parquet format that standardizes how to encode vector geometries and their metadata in Parquet files. This can be used to store geographic data sets in Parquet files efficiently. When the <a href="https://duckdb.org/docs/extensions/spatial.html"><code>spatial</code> extension</a> is installed and loaded, reading from a GeoParquet file through DuckDB&#39;s normal Parquet reader will now <a href="https://github.com/duckdb/duckdb/pull/12503">automatically convert geometry columns to the <code>GEOMETRY</code> type</a>, for example:</p>

<div><div><pre><code><span>INSTALL</span> <span>spatial</span><span>;</span>
<span>LOAD</span> <span>spatial</span><span>;</span>

<span>FROM</span> <span>&#39;https://blobs.duckdb.org/data/geoparquet-example.parquet&#39;</span>
<span>SELECT</span> <span>GEOMETRY</span> <span>g</span>
<span>LIMIT</span> <span>10</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│                                                       g                                                        │
│                                                    geometry                                                    │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ MULTIPOLYGON (((180 -16.067132663642447, 180 -16.555216566639196, 179.36414266196414 -16.801354076946883, 17…  │
│ POLYGON ((33.90371119710453 -0.95, 34.07261999999997 -1.059819999999945, 37.69868999999994 -3.09698999999994…  │
│ POLYGON ((-8.665589565454809 27.656425889592356, -8.665124477564191 27.589479071558227, -8.684399786809053 2…  │
│ MULTIPOLYGON (((-122.84000000000003 49.000000000000114, -122.97421000000001 49.00253777777778, -124.91024 49…  │
│ MULTIPOLYGON (((-122.84000000000003 49.000000000000114, -120 49.000000000000114, -117.03121 49, -116.04818 4…  │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
</code></pre></div></div>
      <h3 id="r-tree">
        
        <a href="#r-tree">R-Tree</a>
        
      </h3>
    

<p>The spatial extension accompanying this release also implements initial support for creating “R-Tree” spatial indexes. An R-Tree index stores the approximate bounding boxes of each geometry in a column into an auxiliary hierarchical tree-like data structure where every “node” contains a bounding box covering all of its child nodes. This makes it really fast to check what geometries intersect a specific region of interest as you can quickly prune out a lot of candidates by recursively moving down the tree.</p>

<p>Support for spatial indexes has been a long-requested feature on the spatial extension roadmap, and now that we have one, a ton of new use cases and directions for further development are opening up. However, as of now they are only used to accelerate simple  queries that select from a table with a filter using one out of a hardcoded set of spatial predicate functions applied on an indexed geometry column and a constant geometry. This makes R-Tree indexes useful when you have a very large table of geometries that you repeatedly query, but you don&#39;t want to perform a full table scan when you&#39;re only interested in the rows whose geometries intersect or fit within a certain region anyway. Here is an example where we can see that the <code>RTREE_INDEX_SCAN</code> operator is used:</p>

<div><div><pre><code><span>INSTALL</span> <span>spatial</span><span>;</span>
<span>LOAD</span> <span>spatial</span><span>;</span>

<span>-- Create a table with 10_000_000 random points</span>
<span>CREATE</span> <span>TABLE</span> <span>t1</span> <span>AS</span> <span>SELECT</span> <span>point</span><span>::</span><span>GEOMETRY</span> <span>as</span> <span>geom</span>
<span>FROM</span> <span>st_generatepoints</span><span>(</span>
        <span>{</span><span>min_x</span><span>:</span> <span>0</span><span>,</span> <span>min_y</span><span>:</span> <span>0</span><span>,</span> <span>max_x</span><span>:</span> <span>10_000</span><span>,</span> <span>max_y</span><span>:</span> <span>10_000</span><span>}::</span><span>BOX_2D</span><span>,</span>
        <span>10_000_000</span><span>,</span>
        <span>1337</span>
    <span>);</span>

<span>-- Create an index on the table.</span>
<span>CREATE</span> <span>INDEX</span> <span>my_idx</span> <span>ON</span> <span>t1</span> <span>USING</span> <span>RTREE</span> <span>(</span><span>geom</span><span>);</span>

<span>-- Perform a query with a &#34;spatial predicate&#34; on the indexed geometry column</span>
<span>-- Note how the second argument in this case, the ST_MakeEnvelope call is a &#34;constant&#34;</span>
<span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>ST_Within</span><span>(</span><span>geom</span><span>,</span> <span>ST_MakeEnvelope</span><span>(</span><span>450</span><span>,</span> <span>450</span><span>,</span> <span>650</span><span>,</span> <span>650</span><span>));</span>
</code></pre></div></div>



<p>R-Tree indexes mostly share the same feature-set as DuckDB&#39;s built-in ART index. They are buffer-managed, persistent, lazily-loaded from disk and support inserts, updates and deletes to the base table. Although they can not be used to enforce constraints.</p>
      <h2 id="final-thoughts">
        
        <a href="#final-thoughts">Final Thoughts</a>
        
      </h2>
    

<p>These were a few highlights – but there are many more features and improvements in this release. The full release notes can be <a href="https://github.com/duckdb/duckdb/releases/tag/v1.1.0">found on GitHub</a>.</p>

<p>We would like to thank again our amazing community for using DuckDB, building cool projects on DuckDB and improving DuckDB by providing us feedback. Your contributions truly mean a lot!</p>

						</div></div>
  </body>
</html>
