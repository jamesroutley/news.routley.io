<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2202.06991">Original</a>
    <h1>Transformer Memory as a Differentiable Search Index</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tay%2C+Y">Yi Tay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tran%2C+V+Q">Vinh Q. Tran</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dehghani%2C+M">Mostafa Dehghani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ni%2C+J">Jianmo Ni</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mehta%2C+H">Harsh Mehta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin%2C+Z">Zhen Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hui%2C+K">Kai Hui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao%2C+Z">Zhe Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta%2C+J">Jai Gupta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schuster%2C+T">Tal Schuster</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cohen%2C+W+W">William W. Cohen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Metzler%2C+D">Donald Metzler</a></p></div>
      
    
  
    <p><a href="https://arxiv.org/pdf/2202.06991">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  In this paper, we demonstrate that information retrieval can be accomplished
with a single Transformer, in which all information about the corpus is encoded
in the parameters of the model. To this end, we introduce the Differentiable
Search Index (DSI), a new paradigm that learns a text-to-text model that maps
string queries directly to relevant docids; in other words, a DSI model answers
queries directly using only its parameters, dramatically simplifying the whole
retrieval process. We study variations in how documents and their identifiers
are represented, variations in training procedures, and the interplay between
models and corpus sizes. Experiments demonstrate that given appropriate design
choices, DSI significantly outperforms strong baselines such as dual encoder
models. Moreover, DSI demonstrates strong generalization capabilities,
outperforming a BM25 baseline in a zero-shot setup.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Yi Tay [<a href="https://arxiv.org/show-email/50602342/2202.06991">view email</a>]
      </p></div></div>
  </body>
</html>
