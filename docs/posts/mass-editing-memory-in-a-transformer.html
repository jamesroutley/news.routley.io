<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2210.07229">Original</a>
    <h1>Mass Editing Memory in a Transformer</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2210.07229">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Recent work has shown exciting promise in updating large language models with
new memories, so as to replace obsolete information or add specialized
knowledge. However, this line of work is predominantly limited to updating
single associations. We develop MEMIT, a method for directly updating a
language model with many memories, demonstrating experimentally that it can
scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),
exceeding prior work by orders of magnitude. Our code and data are at
<a href="https://memit.baulab.info" rel="external noopener nofollow">this https URL</a>.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Arnab Sen Sharma [<a href="https://arxiv.org/show-email/db98c49d/2210.07229">view email</a>]
      </p></div></div>
  </body>
</html>
