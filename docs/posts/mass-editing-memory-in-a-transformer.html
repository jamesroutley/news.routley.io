<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://memit.baulab.info/">Original</a>
    <h1>Mass editing memory in a transformer</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
<a href="https://arxiv.org/pdf/2210.07229.pdf" target="_blank"><img height="100" width="78" src="https://memit.baulab.info/images/paper-thumb.png" alt="ArXiv Preprint thumbnail" data-nothumb=""/></a>
<a href="https://memit.baulab.us/" target="_blank"><img height="100" width="78" src="https://memit.baulab.info/images/memit-demo.png" alt="ROME project thumbnail" data-nothumb=""/></a>
<a href="https://github.com/kmeng01/memit" target="_blank"><img height="100" width="78" src="https://memit.baulab.info/images/code-thumb.png" alt="Github code thumbnail" data-nothumb=""/></a>
<a href="https://rome.baulab.info/" target="_blank"><img height="100" width="78" src="https://memit.baulab.info/images/rome-thumb.png" alt="ROME project thumbnail" data-nothumb=""/></a>
</p>

<div>
<div>
<h3>How many memories can be added to deep network&#39;s weights?</h3>
<p>Large language model contain implicit knowledge of facts
in the world, but they have no built-in way to update that knowledge.
In <a href="https://rome.baulab.info/">previous work (ROME)</a>
we found that memorized factual associations can be located at a specific location
in a GPT network, and we developed a way to directly edit parameters to alter
that location to change the model&#39;s knowledge of a single fact.
</p>
<p>
In this paper, we develop an improved direct editing method (MEMIT) and scale it
up to perform many edits at once. We find that we can update <b>thousands of memories</b>
simultaneously, improving on previous approaches by orders of magnitude.
</p>
</div><!--card-block-->
</div><!--card-->

</div><div>
<div>
<figure>
  <img src="https://memit.baulab.info/images/Paper/MEMIT-graph-crop.png"/>
  <figcaption>(a) Language models can be viewed as knowledge bases containing memorized
    tuples (s, r, o), each connecting some subject <b><i>s</i></b> to an object
    <b><i>o</i></b> via a relation <b><i>r</i></b>, e.g., (s = Michael Jordan,
    r = plays sport, o = basketball).  (b) MEMIT modifies transformer weights to edit
    memories, e.g., &#34;Michael Jordan now plays the sport baseball,&#34; while (c) maintaining
    generalization, specificity, and fluency at scales beyond other methods.
    As detailed in Section 5.2.2 of the paper, editing score is the harmonic mean of
    efficacy, generalization, and specificity metrics.
  </figcaption>
</figure>

<h2>Why edit knowledge in a model?</h2>

<p>Large language models such as the GPT models contain some amount of
<em>world knowledge</em> since they can recall facts about real people, places,
and things.  For example, if you ask GPT-3 to complete the sentence

</p><center><em>Michael Jordan plays the sport...</em></center>


<p>the model will predict <em>&#34;<b>basketball</b>&#34;</em>, a word that
not only is grammatically correct, but that it is also consistent with
a true fact in the real world.

</p><p>However, the knowledge contained in a large language model is not perfect:
even the largest models will be missing specialized knowledge, and a model
will also contain obsolete knowledge that it learned from old text.

</p><p>
GPT-3 predicts: <em>Polaris is in the constellation <b>Ursa Minor</b></em>
  (<span>correct!</span>)
</p>
<p>
GPT-3 predicts: <em>Arneb is in the constellation of <b>Aquila</b></em>
  (<span>incorrect - should be Lepus</span>)
</p>
<p>
GPT-3 predicts: <em>The current Vice President of the United States is named <b>Mike Pence</b></em>
  (<span>obsolete</span>)
</p>


<p>To fix such problems, several <em>knowledge-editing</em> methods
have been proposed to insert new memories directly into model parameters.
Yet most of this these methods are focused on updating a single
memory in the model, and it has been a challenge to use those methods
to update more than a handful of facts. In practice we may want to insert
hundreds or thousands of new memories in order to update or improve
a large model.

</p><p>In this work, we propose <b>MEMIT</b>, a direct model editing method that
is capable of updating <b>thousands of memories at once</b>.
</p>

<h2>How does it work?</h2>

<p><b>MEMIT</b> is a successor to our previous work <a href="https://rome.baulab.info/">ROME</a>, which performs a <em>rank-one</em> modification of the 
  MLP weights of a single layer to directly write a memory into the model. MEMIT builds upon ROME to insert many memories by modifying the MLP weights 
  of a <em>range of</em> critical layers. We perform causal tracing to find a set of mediating MLP layers that recall memories about a certain subject.
  For GPT-J those layers are <span><b>ℛ</b></span> = {3, 4, 5, 6, 7, 8}.
</p>

<figure>
  <img src="https://memit.baulab.info/images/Paper/mrome-architecture-crop.png"/>
</figure>

<p>Then for a set of new memories we calculate the update  <span><b>Δ</b></span> and spread this  <span><b>Δ</b></span> across all the mediating MLP layers such that at 
the final layer the output of final mediating layer captures all the new memories.
</p>

<figure>
  <img src="https://memit.baulab.info/images/Paper/mrome-update-crop.png"/>
</figure>

<p>
<a href="https://arxiv.org/pdf/2210.07229.pdf">In our paper</a>, we derive and explain the method in detail.  We conduct benchmarks testing the ability of MEMIT to scale on a variety of batch knowledge-editing tasks, and we compare our method to other approaches.  <a href="https://github.com/kmeng01/memit">Our code is open-source and available on Github</a>.
</p>

<h2>How to cite</h2>

<p>This work is not yet peer-reviewed. The preprint can be cited as follows.
</p>

<div>

<p>
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. &#34;<em>Mass Editing Memory in a Transformer.</em>&#34; arXiv preprint <nobr><a href="https://arxiv.org/abs/2210.07229">arXiv:2210.07229</a> (2022).</nobr>
</p>

<div>
<pre>@article{meng2022memit,
  title={Mass Editing Memory in a Transformer},
  author={Kevin Meng and Sen Sharma, Arnab and Alex Andonian and Yonatan Belinkov and David Bau},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}
</pre>
</div>
</div>


</div>
</div></div>
  </body>
</html>
