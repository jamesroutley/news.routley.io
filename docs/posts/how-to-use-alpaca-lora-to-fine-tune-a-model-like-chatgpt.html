<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://replicate.com/blog/fine-tune-alpaca-with-lora">Original</a>
    <h1>How to use Alpaca-LoRA to fine-tune a model like ChatGPT</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>Low-rank adaptation (LoRA) is a technique for fine-tuning models that has some advantages over previous methods:</p>
<ul>
<li>It is faster and uses less memory, which means it can run on consumer hardware.</li>
<li>The output is much smaller (megabytes, not gigabytes).</li>
<li>You can combine multiple fine-tuned models together at runtime.</li>
</ul>
<p>Last month we blogged about <a href="https://replicate.com/blog/lora-faster-fine-tuning-of-stable-diffusion">faster fine-tuning of Stable Diffusion with LoRA</a>. Our friend Simon Ryu (aka <a href="https://github.com/cloneofsimo">@cloneofsimo</a>) applied the <a href="https://replicate.com/cloneofsimo/lora">LoRA technique to Stable diffusion</a>, allowing people to create custom trained styles from just a handful of training images, then mix and match those styles at prediction time to create highly customized images.</p>
<p>Fast-forward one month, and we’re seeing LoRA being applied elsewhere. Now it’s being used to fine-tune large language models like LLaMA. Earlier this month, Eric J. Wang released <a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a>, a project which contains code for reproducing the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a> results using <a href="https://github.com/huggingface/peft">PEFT</a>, a library that lets you take various transformers-based language models and fine-tune them using LoRA. What’s neat about this is that it allows you to fine-tune models cheaply and efficient on modest hardware, with smaller (and perhaps composable) outputs.</p>
<p>In this blog post, we’ll show you how to use LoRA to fine-tune LLaMA using Alpaca training data.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li><strong>GPU machine. T</strong>hanks to LoRA you can do this on low-spec GPUs like an NVIDIA T4 or consumer GPUs like a 4090. If you don&#39;t already have access to a machine with a GPU, check out our <a href="https://replicate.com/docs/guides/get-a-gpu-machine">guide to getting a GPU machine</a>.</li>
<li><strong>LLaMA weights</strong>. The weights for LLaMA have not yet been released publicly. To apply for access, fill out <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">this Meta Research form</a>.</li>
</ul>
<h2 id="step-1-clone-the-alpaca-lora-repo">Step 1: Clone the Alpaca-LoRA repo</h2>
<p>We’ve created a fork of <a href="https://github.com/tloen/alpaca-lora">the original Alpaca-LoRA repo</a> that adds support for Cog. <a href="https://github.com/replicate/cog">Cog is a tool to package machine learning models in containers</a> and we&#39;re using it to install the dependencies to fine-tune and run the model.</p>
<p>Clone the repository using Git:</p>
<pre><code>git clone https://github.com/daanelson/alpaca-lora
cd alpaca-lora
</code></pre>
<h2 id="step-2-get-llama-weights">Step 2: Get LLaMA weights</h2>
<p>Put your downloaded weights in a folder called <code>unconverted-weights</code>. The folder hierarchy should look something like this:</p>
<pre><code>unconverted-weights
├── 7B
│   ├── checklist.chk
│   ├── consolidated.00.pth
│   └── params.json
├── tokenizer.model
└── tokenizer_checklist.chk
</code></pre>
<p>Convert the weights from a PyTorch checkpoint to a transformers-compatible format using this command:</p>
<pre><code>cog run python -m transformers.models.llama.convert_llama_weights_to_hf \
  --input_dir unconverted-weights \
  --model_size 7B \
  --output_dir weights
</code></pre>
<p>You final directory structure should look like this:</p>
<pre><code>weights
├── llama-7b
└── tokenizermdki
</code></pre>
<h2 id="step-3-install-cog">Step 3: Install Cog</h2>
<pre><code>sudo curl -o /usr/local/bin/cog -L &#34;https://github.com/replicate/cog/releases/latest/download/cog_$(uname -s)_$(uname -m)&#34;
sudo chmod +x /usr/local/bin/cog
</code></pre>
<h2 id="step-4-fine-tune-the-model">Step 4: Fine-tune the model</h2>
<p>The fine-tuning script is configured by default to work on less powerful GPUs, but if you have a GPU with more memory, you can increase <code>MICRO_BATCH_SIZE</code> to 32 or 64 in <code>finetune.py</code> .</p>
<p>If you have your own instruction tuning dataset, edit <code>DATA_PATH</code> in <code>finetune.py</code> to point to your own dataset. Make sure it has the same format as <code>alpaca_data_cleaned.json</code>.</p>
<p>Run the fine-tuning script:</p>
<pre><code>cog run python finetune.py
</code></pre>
<p>This takes 3.5 hours on a 40GB A100 GPU, and more than that for GPUs with less processing power.</p>
<h2 id="step-5-run-the-model-with-cog">Step 5: Run the model with Cog</h2>
<pre><code>cog predict -i prompt=&#34;tell me something about alpacas&#34;
</code></pre>
<h2 id="next-steps">Next steps</h2>
<p>Here are some ideas for what you could do next:</p>
<ul>
<li>Bring your own dataset and fine-tune your own LoRA, like <a href="https://github.com/22-hours/cabrita">Cabrita: A portuguese finetuned instruction LLaMA</a>, or <a href="https://replicate.com/blog/fine-tune-llama-to-speak-like-homer-simpson">Fine-tune LLaMA to speak like Homer Simpson.</a></li>
<li><a href="https://replicate.com/docs/guides/push-a-model">Push the model to Replicate</a> to run it in the cloud. This is handy if you want an API to build interfaces, or to run large-scale evaluation in parallel. You&#39;ll need to keep it private so the weights aren&#39;t public.</li>
<li>Combine LoRAs. It is possible to combine different Stable Diffusion LoRAs to have a <a href="https://twitter.com/jakedahn/status/1628512624865320962">fine-tuned style and fine-tuned object in the same image</a>. What could be possible if this was done with language models?</li>
<li>Fine-tune the larger LLaMA models with the Alpaca dataset (or other datasets) and see how they perform. This should be possible with PEFT and LoRA, although it will need larger GPUs.</li>
</ul>
<p>We can&#39;t wait to see what you build.</p>

  </div></div>
  </body>
</html>
