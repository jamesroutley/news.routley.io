<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine">Original</a>
    <h1>The Path to Open-Sourcing the DeepSeek Inference Engine</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">A few weeks ago,
during <a href="https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file#202502-open-source-week">Open Source Week</a>,
we open-sourced several libraries.
The response from the community has been incredibly positive - sparking inspiring collaborations, productive
discussions, and valuable bug fixes.
Encouraged by this, we’ve decided to take another step forward: contributing our internal inference engine back to the
open-source community.</p>
<p dir="auto">We are deeply grateful for the open-source ecosystem, without which our progress toward AGI would not be possible.
Our training framework relies on <a href="https://github.com/pytorch/pytorch">PyTorch</a>, and our inference engine is built
upon <a href="https://github.com/vllm-project/vllm">vLLM</a>,
both of which have been instrumental in accelerating the training and deployment of DeepSeek models.</p>
<p dir="auto">Given the growing demand for deploying models like <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a>
and <a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a>, we want to give back to the community as much as we can.
While we initially considered open-sourcing our full internal inference engine, we identified several challenges:</p>
<ul dir="auto">
<li><strong>Codebase Divergence</strong>: Our engine is based on an early fork of vLLM from over a year ago. Although structurally
similar, we’ve heavily customized it for DeepSeek models, making it difficult to extend for broader use cases.</li>
<li><strong>Infrastructure Dependencies</strong>: The engine is tightly coupled with our internal infrastructure, including cluster
management tools, making it impractical for public deployment without significant modifications.</li>
<li><strong>Limited Maintenance Bandwidth</strong>: As a small research team focused on developing better models, we lack bandwidth to
maintain a large open-source project.</li>
</ul>
<p dir="auto">Considering these challenges, we’ve decided to collaborate with existing open-source projects as more sustainable alternatives.</p>
<p dir="auto">Moving forward, we will work closely with existing open-source projects to:</p>
<ul dir="auto">
<li><strong>Extract Standalone Features</strong>: Modularize and contribute reusable components as independent libraries.</li>
<li><strong>Share Optimizations</strong>: Contribute design improvements and implementation details directly.</li>
</ul>
<p dir="auto">We are profoundly grateful for the open-source movement - from operating systems and programming languages to machine
learning frameworks and inference engines. It’s an honor to contribute to this thriving ecosystem and to see our models
and code embraced by the community. Together, let’s push the boundaries of AGI and ensure its benefits serve all of
humanity.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto"><strong>To clarify, this article outlines our approach to open-sourcing of our DeepSeek-Inference-Engine codebase only.
Regarding future model releases, we maintain an open and collaborative stance towards both the open-source community
and hardware partners.
We commit to proactively synchronizing inference-related engineering efforts prior to new model launches, with the
goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0. Our ultimate aim is to foster a
synchronized ecosystem where cutting-edge AI capabilities can be seamlessly implemented across diverse hardware
platforms upon official model releases.</strong></p>
</div>
</article></div></div>
  </body>
</html>
