<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.pgvecto.rs/why-hnsw-is-not-the-answer">Original</a>
    <h1>Why HNSW is not the answer and disk-based alternatives might be more practical</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content-parent"><div id="post-content-wrapper"><p>HNSW (Hierarchical Navigable Small World) has become the go-to algorithm for many vector databases. Its multi-layered graph structure and ability to efficiently navigate vector embeddings make it particularly appealing. However, despite its apparent advantages, HNSW may not be the optimal solution for large-scale and dynamic vector similarity search. In this blog post, we challenge the dominance of HNSW and explore why disk-based alternatives, such as IVF (Inverted File Index), might be more practical for massive datasets.</p>
<hr/>
<h2 id="heading-the-appeal-of-hnsw">The Appeal of HNSW</h2>
<p>HNSW offers several advantages:</p>
<ul>
<li><p><strong>Efficient Search</strong>: Its graph-based structure enables quick nearest-neighbor searches, especially for smaller datasets.</p>
</li>
<li><p><strong>Incremental Updates</strong>: The ability to add new vectors incrementally without needing to rebuild the index is a major benefit for dynamic environments.</p>
</li>
<li><p><strong>High Recall</strong>: HNSW delivers high recall with relatively low latency, making it an ideal option for real-time applications.</p>
</li>
</ul>
<p>However, these benefits come with trade-offs that become more noticeable as dataset sizes increase.</p>
<hr/>
<h2 id="heading-the-problem-of-hnsw">The Problem of HNSW</h2>
<p>One of the main challenges of HNSW is its significant dependence on memory for indexing and search operations. Here are the primary issues that emerge:</p>
<ul>
<li><p><strong>Memory Overhead</strong>: Traversing HNSW’s graph structure involves highly random access patterns. The entire dataset must be stored in memory to achieve reasonable performance. This becomes infeasible for large-scale datasets with billions of high-dimensional vectors due to exorbitant memory requirements.</p>
</li>
<li><p><strong>Performance Sensitivity to Memory Size</strong>: HNSW’s performance degrades sharply if the memory is even slightly insufficient to hold all vectors. In such cases, swapping data to disk drastically impacts search speed, making it impractical for systems with constrained memory.</p>
</li>
<li><p><strong>Unsuitability for Disk-Based Environments</strong>: HNSW is fundamentally designed for in-memory operations and performs poorly in disk-oriented scenarios, such as PostgreSQL. Its reliance on frequent random access patterns makes it incompatible with the sequential access nature of disk storage.</p>
</li>
<li><p><strong>Insertion and Deletion Costs</strong>: Updating the index in HNSW involves cascading modifications throughout the graph, leading to significant computation and write amplification. This makes insertion and deletion operations both slow and resource-intensive.</p>
</li>
</ul>
<p>In contrast, disk-based solutions like IVF excel in scenarios requiring scalability and efficiency at the cost of minimal complexity.</p>
<hr/>
<h2 id="heading-why-ivf-can-be-faster-than-hnsw">Why IVF Can Be Faster Than HNSW</h2>
<p><img data-zoomable="true" loading="lazy" src="https://www.researchgate.net/publication/357875438/figure/fig15/AS:1113078611607560@1642390029247/Two-state-of-the-art-ANN-indexing-methods-the-IVF-index-Left-and-the-three-layer.ppm" alt="10: Two state-of-the-art ANN indexing methods: the IVF index (Left) and the (three-layer) HNSW index (Right). Figure credit: Li et al. [2020] ."/></p>
<p>A critical observation across all vector search algorithms is that their performance largely hinges on the number of distance computations they perform. Minimizing these computations is essential for improving search speed. Although the original IVF index required scanning 1% to 5% of the total vectors, modern advancements in quantization and optimization have significantly enhanced its efficiency, making IVF a strong competitor to HNSW.</p>
<h3 id="heading-quantization-the-game-changer">Quantization: The Game Changer</h3>
<p>Quantization compresses high-dimensional vectors into compact representations. For instance:</p>
<ul>
<li><p><strong>RaBitQ</strong>: Leverages concentration of measure phenomena to enhance binary and scalar quantization accuracy, quantizing each dimension into 1 bit for a 32x compression ratio.</p>
</li>
<li><p><strong>Product Quantization (PQ)</strong>: Works by dividing the vector space into subspaces, quantizing each subspace independently to minimize approximation error. This method offers flexible compression ratios from 4x to 64x in FAISS, enabling finer compression and faster approximate searches.</p>
</li>
<li><p><strong>Scalar Quantization (SQ)</strong>: Quantizes each vector dimension independently by dividing its range into discrete levels, typically achieving a 4x compression ratio by converting from float to int8.</p>
</li>
<li><p><strong>ScaNN</strong>: Uses anisotropic vector quantization to optimize inner product accuracy by penalizing errors in directions that impact high inner products, achieving superior recall and speed.</p>
</li>
</ul>
<p>Quantization reduces memory and disk space usage significantly—often by a factor of 32x when converting floats into bits—while drastically lowering computational overhead. Despite some loss of precision, quantization makes vector comparisons far more efficient. For example, by compressing floating-point numbers into bits, this complexity is reduced by a factor of 32. Additionally, binary operations are generally more efficient compared to floating-point operations, further enhancing performance. With fast-scan optimizations, these computations are further accelerated via efficient CPU register lookups. Combined with IVF, many quantization methods consistently outperform HNSW in both speed and scalability. The typical workflow involves:</p>
<ol>
<li><p><strong>Initial Search</strong>: Leveraging compressed representations to rapidly identify candidate vectors.</p>
</li>
<li><p><strong>Reranking</strong>: Refining results using full-precision distance calculations on a smaller subset of candidates.</p>
</li>
</ol>
<h3 id="heading-comparing-rabitqivf-and-hnsw">Comparing RaBitQ+IVF and HNSW</h3>
<p>IVF with quantization provides a highly efficient way to store all quantized vectors in memory. By leveraging RaBitQ, memory usage is reduced by a factor of 32 compared to full-size vectors, allowing the entire quantized dataset to fit in memory. This design enables the index to rapidly retrieve approximate nearest neighbors using bit vectors and rerank them with full-precision vectors fetched from disk. For a typical Top-10 query, IVF fetches only 100-200 vectors from disk, compared to HNSW, which may require 800-1000 vectors. This efficient approach achieves an optimal balance between memory usage and disk access, offering outstanding cost-performance benefits.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1734489380210/a2812b59-5ffd-4ee3-ac35-1681950ef550.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p>While similar quantization techniques could theoretically be applied to HNSW, practical constraints reduce their effectiveness:</p>
<ul>
<li><p><strong>Vector Packing</strong>: Fast scan optimization relies on packing 32 compressed vectors together, which is incompatible with HNSW’s graph structure.</p>
</li>
<li><p><strong>Random Access Costs</strong>: HNSW involves frequent random access across graph nodes and edges, making traversal inefficient. In contrast, IVF organizes vectors sequentially in posting lists, enabling faster prefetching and efficient sequential scans.</p>
</li>
<li><p><strong>Reranking Costs</strong>: Both IVF and HNSW share similar reranking computational costs due to their reliance on quantized representations in the first stage.</p>
</li>
</ul>
<p>Ultimately, the performance difference between quantized IVF and HNSW is likely minimal, but IVF stands out with its simplicity and efficiency.</p>
<hr/>
<div>
<table>
<thead>
<tr>
<td>Feature</td><td>IVF</td><td>IVF + RaBitQ</td><td>HNSW</td></tr>
</thead>
<tbody>
<tr>
<td>Indexing Method</td><td>KMeans can be offloaded to GPU</td><td>KMeans + Quantization</td><td>Nearest Neighbor Graph, need to keep everything in memory</td></tr>
<tr>
<td>Overlapped Data Across Query</td><td>Centroids</td><td>Quantized vectors</td><td>No</td></tr>
<tr>
<td>Scalability</td><td>Limited by CPU and memory</td><td>Outstanding</td><td>Limited by memory</td></tr>
<tr>
<td>Insertion/Deletion</td><td>Simple (updates posting lists)</td><td>Simple (updates posting lists)</td><td>Complex (cascading graph changes)</td></tr>
<tr>
<td>Search Speed</td><td>Slow</td><td>Extremely Fast with Quantization</td><td>Fast with sufficient memory</td></tr>
<tr>
<td>Overall Complexity</td><td>Low</td><td>Low</td><td>High</td></tr>
</tbody>
</table>
</div><hr/>
<h2 id="heading-operational-simplicity-of-ivf">Operational Simplicity of IVF</h2>
<p>IVF’s simplicity makes it a more practical choice for real-world applications:</p>
<ul>
<li><p><strong>Insertion and Deletion</strong>: In HNSW, these operations trigger cascading modifications throughout the graph, resulting in significant computation and write amplification. IVF, in contrast, only requires updating the relevant posting list.</p>
</li>
<li><p><strong>Disk-Based Storage</strong>: IVF’s reliance on disk-based indexing enables it to scale efficiently without the prohibitive memory costs associated with HNSW.</p>
</li>
<li><p><strong>Adaptability</strong>: IVF can be easily combined with advanced quantization techniques, enabling further optimizations.</p>
</li>
</ul>
<hr/>
<h2 id="heading-conclusion">Conclusion</h2>
<p>While HNSW has solidified its reputation as a fast and accurate algorithm for vector similarity search, it is not without limitations. Its memory-intensive nature and operational complexity make it less suitable for large-scale applications. In contrast, IVF offers a scalable and cost-effective alternative, particularly when paired with modern quantization techniques.</p>
<p>As the demand for vector search continues to grow, practitioners must carefully evaluate the trade-offs between memory-based and disk-based solutions. HNSW may dominate small to medium-scale applications, but for massive datasets, it’s time to look beyond HNSW and embrace simpler, scalable approaches like IVF.</p>
<p>If you&#39;re interested in such databases, check out VectorChord.</p>

</div></div></div>
  </body>
</html>
