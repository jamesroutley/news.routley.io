<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://umarbutler.com/how-i-built-the-largest-open-database-of-australian-law/">Original</a>
    <h1>Show HN: how I built the largest open database of Australian law</h1>
    
    <div id="readability-page-1" class="page"><div><div>
<p>Late last year, while attempting to train a large language model to solve legal problems, I made a surprising discovery — there weren’t any open databases of Australian law to train my model on. While there were certainly a few free-to-access legal databases, none were truly <em>open</em>, at least not in the sense of being able to just download their data and start training models without fear of infringing on anyone’s copyright. They all had policies against web scraping, and they were all either unable or unwilling to license their content.</p>



<p>So, before I could start training an LLM on Australian legal data, I’d need to get my hands on that data first. As with most of my projects, this sounded much easier than it would actually turn out to be. Almost a year later, and I am still hard at work on expanding my database to encompass all of Australia’s legal code.</p>



<p>In this article, I’ll walk you through the entire process of how I built the <a href="https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus">Open Australian Legal Corpus</a>, the largest open database of Australian law, from months-long negotiations with governments to reverse engineering ancient web technologies to hacking together a multitude of different solutions for extracting text from documents.</p>



<p>The first step was to ask Australian governments and courts for permission to scrape their data. For some jurisdictions like New South Wales and Queensland, which incidentally both use the same <a href="https://www.teratext.com/products/teratext-for-legislation.asp">legislation management system</a>, this was relatively easy, and I was helpfully directed to the endpoints of their public APIs. For others, however, the process was anything but simple. There were a few that imposed restrictions on my access to and use of their data, one that simply didn’t respond to my enquiries, and a great many that outright refused my requests, often times without giving a reason. Perhaps my most memorable interaction with a data source was when I was told that I would overburden their team by requiring them to manually sift through and package documents, despite my insistence that web scraping is an automated process that requires no additional work on the part of website owners.</p>



<p>In the end, after months of back and forth, I was granted access to the laws, regulations and bills of five out of the six states, one external territory and the Commonwealth. I was also able to secure permission to scrape judgments of three courts and five tribunals, all held by the Federal Court of Australia. To date, I am still in talks with my home state, Victoria, about being able to include their data in my database.</p>



<p>Once I had permission to access the various sources’ databases, the next step was to index and download their documents. Again, for those databases with public APIs, this was a cakewalk. But, for those without public APIs, it was a toss-up whether I’d need to block out a few days to reverse-engineer some ancient technology I had never heard of before. The Federal Government’s database was exceptionally difficult to work with, not only because of how slow it was but also because it was coded in ASP.NET and so made extensive use of <a href="https://asp-blogs.azurewebsites.net/infinitiesloop/Truly-Understanding-Viewstate">ViewStates</a>, requiring me to develop a <a href="https://github.com/umarbutler/open-australian-legal-corpus-creator/blob/1e05bd0a784e4b40703e9f072d723609540beade/src/oalc_creator/scrapers/federal_register_of_legislation.py#L83-L167">complex algorithm</a> just to be able to navigate between search engine results pages. Another notable mention is the Federal Court of Australia which, although easier to index, had used at least three different text encodings for various parts of its website (search results were in Unicode and judgments were a mix of Windows-1250, Windows-1252 and some were just encoded incorrectly). With enough trial and error, however, I was eventually able to create scrapers for every database.</p>



<p>Of course, even after getting my hands on the sources’ documents, I’d still need to convert them to a standardised format. I chose plain text for my database because it was what I’d be feeding my LLM, and it would be easy to convert documents to. Or so I thought. As it turns out, converting HTML to anything that isn’t HTML is a painful process. Especially if, like me, you want to preserve spacing, indentation, line breaks and tables. I went through <em>a lot</em> of Python packages before finally landing on <code><a href="https://github.com/weblyzard/inscriptis">Inscriptis</a></code>, an absolutely essential library for anyone wanting to extract text from HTML without mangling formatting. Still, some additional finetuning was necessary to get it to work well with every data source.</p>



<p>Unfortunately, mastering HTML parsing wasn’t enough to be able to build my database. I’d also need to learn how to work with RTFs, DOCXs and PDFs.</p>



<p>RTFs were the easiest of the lot. I let the <code><a href="https://github.com/joshy/striprtf">striprtf</a></code> Python library do the heavy lifting. DOCXs were a bit more difficult. I tested <code><a href="https://github.com/JessicaTegner/pypandoc">pypandoc</a></code>, <code><a href="https://github.com/python-openxml/python-docx">python-docx</a></code>, <code><a href="https://github.com/ankushshah89/python-docx2txt">docx2txt</a></code> and <code><a href="https://github.com/ShayHill/docx2python">docx2python</a></code> and none of them beat using <code><a href="https://github.com/mwilliamson/python-mammoth">mammoth</a></code> to convert DOCXs to HTML and then extracting text from the resulting HTML with <code>Inscriptis</code>.</p>



<p>Finally, when it came to PDFs, I tried out <code><a href="https://pypdf.readthedocs.io/en/stable/index.html">pypdf</a></code>, <code><a href="https://github.com/tesseract-ocr/tesseract">tesseract</a></code>, <code><a href="https://github.com/pdfminer/pdfminer.six">pdfminer.six</a></code> and <code><a href="https://github.com/jsvine/pdfplumber">pdfplumber</a></code>, and found that all of them had a tendency to insert line breaks where they didn’t belong. Essentially, they would interrupt whole sentences with newline characters, mimicking words wrapping around a page. Nevertheless, I found <code>pdfplumber</code> to perform the best out of all the available options, and also discovered that, for PDFs from certain sources, I could stich sentences back together by replacing newlines preceded by whitespace with just whitespace.</p>



<p>After enough experimentation with various Python libraries and post-processing techniques, I managed to convert all the documents I had collected into plain text. This resulted in a dataset of over 100k texts. I then packaged those documents together, attached metadata on their nature and origin, and publicly released my database on HuggingFace as the <a href="https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus">Open Australian Legal Corpus</a>. I also set up a <a href="https://github.com/umarbutler/open-australian-legal-corpus-creator">GitHub repository</a> to host my database creator, which I built in such a way that anyone else interested in open legal data can contribute their own scrapers to be integrated into the database.</p>



<p>As I go along, negotiating with more governments and courts, reverse engineering their ancient tech stacks, I intend to continue updating the database, expanding its sources and enriching its content. And hopefully, the next time someone wants to build an LLM for Australian law, they won’t need to go down a year-long journey of trying to find the right data.</p>
</div></div></div>
  </body>
</html>
