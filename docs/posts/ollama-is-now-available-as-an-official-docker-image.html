<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image">Original</a>
    <h1>Ollama is now available as an official Docker image</h1>
    
    <div id="readability-page-1" class="page"><section>
        <p><img src="https://github.com/jmorganca/ollama/assets/3325447/080f3a72-e2fd-4741-8070-ae79a06f943f"/>
</p>



<p>We are excited to share that Ollama is now available as an official Docker sponsored <a href="https://hub.docker.com/r/ollama/ollama">open-source image</a>, making it simpler to get up and running with large language models using Docker containers.</p>

<p>With Ollama, all your interactions with large language models happen locally without sending private data to third-party services.</p>

<h2>On the Mac</h2>

<p>Ollama handles running the model with GPU acceleration. It provides both a simple CLI as well as a REST API for interacting with your applications.</p>

<p>To get started, simply <a href="https://ollama.ai/download">download</a> and install Ollama.</p>

<p>We recommend running Ollama alongside Docker Desktop for macOS in order for Ollama to enable GPU acceleration for models.</p>

<h2>On Linux</h2>

<p>Ollama can run with GPU acceleration inside Docker containers for Nvidia GPUs.</p>

<p>To get started using the Docker image, please use the commands below.</p>

<h4>CPU only</h4>

<pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre>

<h4>Nvidia GPU</h4>

<ol>
<li>Install the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation">Nvidia container toolkit</a>.</li>
<li>Run Ollama inside a Docker container</li>
</ol>

<pre><code>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre>

<h3>Run a model</h3>

<p>Now you can run a model like Llama 2 inside the container.</p>

<pre><code>docker exec -it ollama ollama run llama2
</code></pre>

<p>More models can be found on the <a href="https://ollama.ai/library">Ollama library</a>.</p>

<p>Join Ollamaâ€™s <a href="https://discord.gg/ollama">Discord</a> to chat with other community members, maintainers, and contributors.</p>

<p>Follow <a href="https://twitter.com/ollama_ai">Ollama on Twitter</a> for updates.</p>

      </section></div>
  </body>
</html>
