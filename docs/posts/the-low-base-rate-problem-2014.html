<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.evanmiller.org/the-low-base-rate-problem.html">Original</a>
    <h1>The Low Base Rate Problem (2014)</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
        
        <div>

<p>By <a href="https://www.evanmiller.org/">Evan Miller</a></p>

<p><em>June 2, 2014</em></p>





<p><em>This post is the third in a series about A/B testing methodology. Other installments include <a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">How Not To Run an A/B Test</a>, <a href="https://www.evanmiller.org/lazy-assignment-and-ab-testing.html">Lazy Assignment and A/B Testing</a>, and <a href="https://www.evanmiller.org/sequential-ab-testing.html">Simple Sequential A/B Testing</a>.</em></p>

<p>If you’re running A/B tests on binary outcomes, and your conversion rate is in the single digits, there’s a good chance you’re wasting your time.</p><h2>Let’s talk about power</h2>

<p>Almost all testing frameworks report statistical significance, but very few talk about <em>power</em>. Significance is the probability of seeing an effect where no effect exists. Power is the flip side of the coin; it’s the probability of seeing an effect where an effect actually exists. Here’s a table summarizing the two concepts:</p>

<center>
<table>
    <thead>
        <tr>
            <th>Reality</th>
            <th>Test says</th>
            <th>Chance of</th>
            <th>Comment</th>
    </tr></thead>
    <tbody>
        <tr>
            <td>Effect exists</td>
            <td>Effect exists</td>
            <td>A</td>
            <td rowspan="2">A / (A + B) is power</td>
        </tr>
        <tr>
            <td>Effect exists</td>
            <td>No effect</td>
            <td>B</td>
        </tr>
        <tr>
            <td>No effect</td>
            <td>Effect exists</td>
            <td>C</td>
            <td rowspan="2">C / (C + D) is significance</td>
        </tr>
        <tr>
            <td>No effect</td>
            <td>No effect</td>
            <td>D</td>
        </tr>
    </tbody>
</table>
</center>


<p>To ignore power to ignore the top half of the table, that is, the world where an effect exists.</p>

<p>Power only exists in relation to the size of the effect that you wish to detect. The power level is typically set to 80%. That is, by setting power to 80% for some effect size (say, “10% increase in sales”), you ensure that <em>if</em> the experiment increases sales by <em>exactly</em> 10%, the test will detect the change 80% of the time. Power is important because without enough power, a statistical test will almost always come back with a null result, regardless of the significance level of your test.</p>

<h2>Some numbers</h2>

<p>You can find the handy power equations in <a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">How Not To Run an A/B Test</a>. I won’t frighten the reader by reprinting the equations, but I want to discuss an aspect of the equations that is devastating to small conversion rates.</p>

<p>The “effect size” in the equations are in <em>absolute</em> terms, but most investigators are interested in knowing <em>relative</em> effects. Let’s see what this means for detecting a 10% relative change:</p>

<center>
<table>
    <thead>
        <tr>
            <th>Old rate</th>
            <th>New rate</th>
            <th># Subjects needed</th>
        </tr>
        <tr>
            <td>50%</td>
            <td>55%</td>
            <td>1,567 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!50;80;5;10;1">ref</a>)</span></td>
        </tr>
        <tr>
            <td>20%</td>
            <td>22%</td>
            <td>6,347 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!20;80;5;10;1">ref</a>)</span></td>
        </tr>
        <tr>
            <td>10%</td>
            <td>11%</td>
            <td>14,313 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!10;80;5;10;1">ref</a>)</span></td>
        </tr>
        <tr>
            <td>5%</td>
            <td>5.5%</td>
            <td>30,244 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!5;80;5;10;1">ref</a>)</span></td>
        </tr>
        <tr>
            <td>1%</td>
            <td>1.1%</td>
            <td>157,697 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!1;80;5;10;1">ref</a>)</span></td>
        </tr>
</thead></table>
</center>

<p>That is, detecting a change off of a 1% baseline ends up requiring about a <em>hundred times</em> as many observations as detecting the same relative change off of a 50% baseline.</p>

<p>To put things in perspective, detecting a relative change of 10% off of a 1% base rate requires a total subject pool of over 300,000 — larger than the population of Pittsburgh — and even dialing the minimum detected effect all the way up to 50% — an epoch-making figure to anyone in marketing — an experiment with a 1% base rate would still need about 6,000 observations in each branch (<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!1;80;5;50;1">ref</a>).</p>

<p>The situation is even more depressing at lower conversion rates. Here’s what happens if you’re only converting one visitor in a thousand:</p>

<center>
<table>
    <thead>
        <tr>
            <th>Old rate</th>
            <th>New rate</th>
            <th># Subjects needed</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0.1%</td>
            <td>0.11%</td>
            <td>1,591,535 <span>(<a href="https://www.evanmiller.org/ab-testing/sample-size.html#!0.1;80;5;10;1">ref</a>)</span></td>
        </tr>
    </tbody>
</table>
</center>

<p>In other words, each branch of the experiment would need a subject pool about the size of Gabon.</p>

<h2>Calculate power in advance…</h2>

<p>If you’ve ever wondered why so many of your A/B experiments are coming back negative, it could be that you simply don’t have enough subjects to conduct proper tests. Before concluding that a treatment had no effect, it’s essential to calculate how much a power the statistical test actually had to begin with. Otherwise you run the risk of rejecting good changes without giving them a fair trial — or worse, concluding that the A/B methodology is somehow mystical or unsound.</p>

<p>To avoid a faulty conclusion, <strong>it is imperative to decide in advance the size of effect that you wish to detect</strong>. That number will determine the number of subjects needed before the test actually starts. Don’t try to guess what this number is and simply hope for the best. There’s a square root in the equation which makes intuition difficult. I recommend using my <a href="https://www.evanmiller.org/ab-testing/sample-size.html">Sample Size Calculator</a>, or working with the equations in my <a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">previous article</a> directly.</p>

<p>If the required number of subjects is prohibitively large, you might try to devise ways to redesign the experiment in a way that makes the baseline conversion rate larger. I describe this issue in detail in <a href="https://www.evanmiller.org/lazy-assignment-and-ab-testing.html">Lazy Assignment and A/B Testing</a>.</p>


<h2>…or else step away from the vehicle</h2>

<p>Because of the potential for erroneous conclusions and bad decision-making, it is not an exaggeration to say that <strong>anyone who lacks a firm understanding of statistical power should not be designing or interpreting A/B tests</strong>. This proscription may sound extreme, but designing a test without enough power is like designing a car without enough brakes.</p>

<p>So the next time you are presented with a negative test result, ask the experimenter: How much power did this test have? Did the test have enough subjects to reach a meaningful conclusion? Should the experiment have been run in the first place?</p>


<hr/>

<p><em>You’re reading <a href="https://www.evanmiller.org/">evanmiller.org</a>, a random collection of math, tech, and musings. If you liked this you might also enjoy:
    </em></p><ul><em>
        <li><a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">How Not To Run an A/B Test</a>
        </li><li><a href="https://www.evanmiller.org/lazy-assignment-and-ab-testing.html">Lazy Assignment and A/B Testing</a>
        </li><li><a href="https://www.evanmiller.org/sequential-ab-testing.html">Simple Sequential A/B Testing</a>
    </li></em></ul>

<hr/>
<p><em>Get new articles as they’re published, via <a href="https://twitter.com/EvMill">Twitter</a> or <a href="https://www.evanmiller.org/news.xml">RSS</a>.</em></p>

<hr/>

<p><em>Want to look for statistical patterns in your MySQL, PostgreSQL, or SQLite database? My desktop statistics software <strong><a href="https://www.wizardmac.com/">Wizard</a></strong> can help you analyze <strong>more data in less time</strong> and <strong>communicate discoveries visually</strong> without spending days struggling with pointless command syntax. Check it out!</em></p>
<div>
<p><a href="https://www.wizardmac.com/"><img height="128" width="128" src="https://www.evanmiller.org/images/index/wizard2.png"/></a></p></div>


<hr/>

<p><a href="https://www.evanmiller.org/">Back to Evan Miller’s home page</a> 
– <a href="https://www.evanmiller.org/news.xml">Subscribe to RSS</a>
– <a href="https://twitter.com/EvMill">Twitter</a> 
– <a href="https://www.youtube.com/c/EvanMiller">YouTube</a> 
</p>

<hr/>

</div>
</div></div>
  </body>
</html>
