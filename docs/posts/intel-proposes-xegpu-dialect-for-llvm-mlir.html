<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://discourse.llvm.org/t/rfc-add-xegpu-dialect-for-intel-gpus/75723">Original</a>
    <h1>Intel proposes XeGPU dialect for LLVM MLIR</h1>
    
    <div id="readability-page-1" class="page"><div id="post_1">
            <div>
              


              <p><span>
                  <time itemprop="datePublished" datetime="2023-12-16T01:11:15Z">
                    December 16, 2023,  1:11am
                  </time>
                  <meta itemprop="dateModified" content="2023-12-16T01:11:15Z"/>
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="articleBody">
              <p><strong>Motivation</strong></p>
<p>To support high-performance GEMM code generation on Intel GPU, we propose XeGPU dialect. XeGPU dialect provides an abstraction that closely models Xe instructions. XeGPU ops are introduced when a special Xe instruction can’t be expressed by LLVM/SPIR-V dialect, for example, like matrix instruction (AKA DPAS) and 2D block load. It matches the hardware instructions’ semantics including the matrix sizes. XeGPU dialect is similar to NVGPU and AMDGPU dialect and works as a bridge dialect providing target-specific operations on MLIR memref and vector data types.</p>
<p>XeGPU dialect models a subset of Xe GPU’s unique features focusing on GEMM performance. The operations include 2d load, dpas, atomic, scattered load, 1d load, named barrier, mfence, and compile-hint. These operations provide a minimum set to support high-performance MLIR GEMM implementation for a wide range of GEMM shapes. XeGPU dialect complements Arith, Math, Vector, and Memref dialects. This allows XeGPU based MLIR GEMM implementation fused with other operations lowered through existing MLIR dialects.</p>
<p><strong>Example</strong></p>
<pre data-code-wrap="MLIR"><code>%4 = xegpu.create_nd_tdesc %arg2[%2, %3] {mode = vc} : memref&lt;1024x1024xf32&gt; -&gt; !xegpu.tensor_desc&lt;8x16xf32&gt;

%5 = xegpu.load_nd %4 {mode = vc} : !xegpu.tensor_desc&lt;8x16xf32&gt; -&gt; vector&lt;8x16xf32&gt;

%7 = xegpu.create_nd_tdesc %arg0[%2, %c0] {mode=vc}: memref&lt;1024x1024xf16&gt; -&gt; !xegpu.tensor_desc&lt;8x16xf16&gt;

%8 = xegpu.create_nd_tdesc %arg1[%c0, %3] {mode=vc}: memref&lt;1024x1024xf16&gt; -&gt; !xegpu.tensor_desc&lt;16x16xf16&gt;

%6:3 = scf.for %arg3 = %c0 to %c1024 step %c16 iter_args(%arg4 = %5, %subA = %7, %subB = %8) -&gt; (vector&lt;8x16xf32&gt;, !xegpu.tensor_desc&lt;8x16xf16&gt;, !xegpu.tensor_desc&lt;16x16xf16&gt;) {

%9 = xegpu.load_nd %subA {mode=vc, vnni_axis = 1}: !xegpu.tensor_desc&lt;8x16xf16&gt; -&gt; vector&lt;8x8x2xf16&gt;

%10 = xegpu.load_nd %subB {mode=vc, vnni_axis = 0} : !xegpu.tensor_desc&lt;16x16xf16&gt; -&gt; vector&lt;8x16x2xf16&gt;

%11 = xegpu.dpas %9, %10, %arg4 {mode=vc}: vector&lt;8x8x2xf16&gt;, vector&lt;8x16x2xf16&gt;, vector&lt;8x16xf32&gt; -&gt; vector&lt;8x16xf32&gt;

%12 = xegpu.update_nd_offset %subA, [%c0, %c16] {mode=vc}: !xegpu.tensor_desc&lt;8x16xf16&gt; -&gt; !xegpu.tensor_desc&lt;8x16xf16&gt;

%13 = xegpu.update_nd_offset %subB, [%c16, %c0] {mode=vc}: !xegpu.tensor_desc&lt;16x16xf16&gt; -&gt; !xegpu.tensor_desc&lt;16x16xf16&gt;

scf.yield %11, %12, %13: vector&lt;8x16xf32&gt;, !xegpu.tensor_desc&lt;8x16xf16&gt;, !xegpu.tensor_desc&lt;16x16xf16&gt;

}

xegpu.store_nd %6#0, %4 {mode = vc}: vector&lt;8x16xf32&gt;, !xegpu.tensor_desc&lt;8x16xf32&gt;

</code></pre>
<p><strong>Reference</strong></p>
<p>See <a href="https://github.com/intel/mlir-extensions/blob/main/docs/rfcs/XeGPU.md" rel="noopener nofollow ugc">XeGPU Op definition</a> for details.</p>
            </div>

            

            

          </div><div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              
<p>I’m not familiar with Xe: is there a set of intrinsics in LLVM like NVVM and AMDGPU? The lowering path isn’t clear to me from your description?</p>
            </div>

            

            

          </div><div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discourse.llvm.org/u/JackW"><span itemprop="name">JackW</span></a>
                
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-12-16T04:24:43Z">
                    December 16, 2023,  4:24am
                  </time>
                  <meta itemprop="dateModified" content="2023-12-16T04:24:43Z"/>
              <span itemprop="position">3</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>From the <a href="https://github.com/intel/mlir-extensions/blob/main/docs/rfcs/XeGPU.md" rel="noopener nofollow ugc">RFC on Intel’s MLIR Extensions repo</a></p>
<blockquote>
<h4>Proposal</h4>
<p>XeGPU dialect models a subset of Xe GPU’s ISA. This is the counterpart of NVGPU and AMDGPU dialects, which provide a bridge dialect in the MLIR gradual lowering. XeGPU dialect works with MLIR memref and vector type and complements Arith, Math, Vector, and Memref dialects. XeGPU operations are introduced when there is a special Xe instruction not modeled by LLVM/SPIR-V dialect, for example, like DPAS and 2D block load. In some cases, one XeGPU op may lower to a sequence of instructions for a dedicated and performance-critical function. For example, create_tdesc is mapped to a fixed sequence of instructions to create an address description.</p>
<p>…</p>
<h4>Notes</h4>
<p>Currently, there is no lower-level dialect for the Intel GPU compiler toolchain to represent GPU ops with values based on LLVM data types such as NVVM dialect for the Nvidia GPU compiler toolchain. XeGPU dialect uses LLVM or SPIR-V intrinsic to access advanced intel GPU instructions. When the lower-level software changes, we expect XeGPU lowering passes to change accordingly.</p>
</blockquote>
            </div>

            

            

          </div><div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              <p>Thanks, I am still not sure on:</p>
<blockquote>
<p>for the Nvidia GPU compiler toolchain. XeGPU dialect uses LLVM or SPIR-V intrinsic to access advanced intel GPU instructions.</p>
</blockquote>
<p>Does this means that LLVM already has intrinsics for XeGPU?</p>
            </div>

            

            

          </div><div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              
<p>Can you elaborate? How would this dialect compose with other upstream dialects? In particular:</p>

<p>Presumably you’d like things like <a href="https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul-linalgmatmulop" rel="noopener nofollow ugc">linalg.matmul</a> to be lowered to XeGPU? What’s the roadmap for that? And would it be possible to have end-to-end tests upstream?</p>
<p>-Andrzej</p>
            </div>

            

            

          </div><div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              <p>The upstream LLVM doesn’t have intrinsic for Xe GPU yet.</p>
<p>The XeGPU op will be first lowered to LLVM dialect with an external function call to an Intel-specific function name, and then lower to LLVM bitcode and translate to SPIR-V binary.  These external function names will be recognized by Intel’s low-level SW stack (IGC) as intrinsic.</p>
<p>The current implementation in <a href="https://github.com/intel/mlir-extensions/tree/main/include/imex/Dialect/XeGPU" rel="noopener nofollow ugc">Intel Extension to MLIR github repo</a>  is lowered through SPRI-V dialect which generates SPIR-V IR directly.  But when we upstream, we plan to upstream the LLVM dialect lowering path.</p>
            </div>

            

            

          </div><div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              <p>XeGPU op interacts with memref and vector data type. Once it sets up the tensor address description with memref, it can load a 2d block from memref to vector.  With the data loaded to vector, it can be processed by any other dialect accepting vector data type.</p>
<p>Linalg.matmul would be able to lowered to XeGPU. The lowering could be gradual so it first lowered to a larger size 2d submatrix, and then lowered to the 2d block size to XeGPU level.  Internally we are experimenting with the gradual lowering and eventually we would like to upstream the dialect/passes out from the experiemnt.</p>
<p>We have a end-to-end XeGPU based GEMM implementation for 4Kx4K <a href="https://github.com/intel/mlir-extensions/blob/main/test/Integration/Dialect/XeGPU/gemm_4kx4kx4k_f16_f16_f16.mlir" rel="noopener nofollow ugc">here </a>, and we can upstream that test case to <a href="https://github.com/llvm/llvm-project/tree/main/mlir/test/Integration/Dialect" rel="noopener nofollow ugc">llvm-project/mlir/test/Integration/Dialect at main · llvm/llvm-project · GitHub</a> as part of the XeGPU lowering pass.</p>
            </div>

            

            

          </div></div>
  </body>
</html>
