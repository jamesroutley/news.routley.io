<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mistral-nemo/">Original</a>
    <h1>Mistral NeMo</h1>
    
    <div id="readability-page-1" class="page"><div><p>Today, we are excited to release Mistral NeMo, a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.</p><p>We have released pre-trained base and instruction-tuned checkpoints checkpoints under the Apache 2.0 license to promote adoption for researchers and enterprises. Mistral NeMo was trained with quantisation awareness, enabling FP8 inference without any performance loss.</p><p>The following table compares the accuracy of the Mistral NeMo base model with two recent open-source pre-trained models, Gemma 2 9B, and Llama 3 8B.</p><p><img src="https://mistral.ai/images/news/mistral-nemo/nemo-base-performance.png" alt="Mistral NeMo base model performance compared to Gemma 2 9B and Llama 3 8B" width="100%"/></p><p>Table 1: Mistral NeMo base model performance compared to Gemma 2 9B and Llama 3 8B.</p><h2 id="multilingual-model-for-the-masses">Multilingual Model for the Masses</h2><p>The model is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. This is a new step toward bringing frontier AI models to everyoneâ€™s hands in all languages that form human culture.</p><p><img src="https://mistral.ai/images/news/mistral-nemo/mistral-nemo-benchmarks.png" alt="Mistral NeMo performance on multilingual benchmarks" width="100%"/></p><p>Figure 1: Mistral NeMo performance on multilingual benchmarks.</p><h3 id="tekken-a-more-efficient-tokenizer">Tekken, a more efficient tokenizer</h3><p>Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. In particular, it is ~30% more efficient at compressing source code, Chinese, Italian, French, German, Spanish, and Russian. It is also 2x and 3x more efficient at compressing Korean and Arabic, respectively. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages.</p><p><img src="https://mistral.ai/images/news/mistral-nemo/new-tokenizer-tekken.png" alt="Tekken compression rate" width="100%"/></p><p>Figure 2: Tekken compression rate.</p><h2 id="instruction-fine-tuning">Instruction fine-tuning</h2><p>Mistral NeMO underwent an advanced fine-tuning and alignment phase. Compared to Mistral 7B, it is much better at following precise instructions, reasoning, handling multi-turn conversations, and generating code.</p><p><img src="https://mistral.ai/images/news/mistral-nemo/mistral-nemo-multilingual.png" alt="Mistral NeMo instruction-tuned model accuracy" width="100%"/></p><p>Table 2: Mistral NeMo instruction-tuned model accuracy. Evals done with GPT4o as judge on official references.</p><h2 id="links">Links</h2><p>Weights are hosted on HuggingFace both for the <a href="https://huggingface.co/mistralai/Mistral-Nemo-Base-2407">base</a> and for the <a href="https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407">instruct</a> models. You can try Mistral NeMo now with mistral-inference and adapt it with mistral-finetune. Mistral NeMo is exposed on la Plateforme under the name <code>open-mistral-nemo-2407</code>. This model is also packaged in a container as NVIDIA NIM inference microservice and available from <a href="https://ai.nvidia.com/">ai.nvidia.com</a>.</p></div></div>
  </body>
</html>
