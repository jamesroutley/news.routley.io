<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lightly.ai/post/self-supervised-learning-for-videos">Original</a>
    <h1>Self-Supervised Learning for Videos</h1>
    
    <div id="readability-page-1" class="page"><div><p>Self-supervised learning has emerged as a good alternative to supervised learning in recent years. It has been shown to beat Supervised Learning on Image Classification benchmarks and is a great option in cases where annotating/labeling is too expensive. However, its impact and performance on videos still need to be investigated since videos have an inherent multidimensional nature and complexity since they have both spatial and temporal dimensions.</p><p>In this article, we will briefly overview Masked Autoencoders as applied to images in a self-supervised setting, discuss why videos need special attention, and review the VideoMAE architecture and its follow-up work.<br/></p><h2>Brief Overview of Image Masked Autoencoders (ImageMAE)</h2><h2>‍<br/></h2><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/668e840c46bfbfffcf065e50_1*2Z6uQGTrBPQRuuI9-pltBQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Overview of the ImageMAE architecture. Source: <a href="https://arxiv.org/abs/2111.06377" target="_blank">https://arxiv.org/abs/2111.06377</a></figcaption></figure><p>‍</p><p>The ImageMAE architecture introduced by He et al. in <a href="https://arxiv.org/abs/2111.06377" target="_blank">Masked Autoencoders Are Scalable Vision Learners, 2022</a> was inspired by the success of Masked Modelling in NLP and was based on a straightforward idea:</p><blockquote>An image is converted into a set of non-overlapping patches and masked randomly. The visible subset of patches are fed into an encoder which projects them into a latent representation space. A lightweight decoder then operates on these latent representations and the masked tokens to reconstruct the original image.</blockquote><p>Two key design principles of this approach are:</p><ul role="list"><li>An <strong>asymmetric design</strong> in the sense that the encoder only operates on the visible tokens while the decoder operates on the latent representations and masked tokens.</li><li>A <strong>lightweight decoder</strong> is used to reconstruct the image.</li></ul><p><strong>Reconstruction Loss:</strong> The decoder is tasked with reconstructing the input image, and therefore predicts pixel values for each masked patch. Thus a natural loss formulation emerges where we compute the Mean Squared Error (MSE) between the reconstructed images and the original images in the pixel space. The authors also report improved performance when normalizing the predicted pixel values of each masked patch.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f70074101ab551f8da366_1*ykXF8fr5XSWC9tEiI48ySA.png" loading="lazy" alt=""/></p><figcaption><strong>Equation:</strong> Reconstruction Loss. “M” represents the set of masked pixels. </figcaption></figure><p>‍</p><p>Moreover, based on extensive studies the authors find that this method works well even with high masking proportions (&gt;75%) and improves efficiency in training high-capacity models that generalize well. The authors use Vision Transformers (ViTs) as the encoders. Because the encoder only operates on visible patches of the image (~25% of the original image) this enables them to train very large encoders with only a fraction of compute and memory.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f7026d2e841b6bd787eb8_1*EQXRDYNXVFGMYBp83gh5Ug.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Validation Accuracy of ImageMAE on ImageNet-1k with varying masking ratios. Source: <a href="https://arxiv.org/abs/2111.06377" target="_blank">https://arxiv.org/abs/2111.06377</a></figcaption></figure><p>‍</p><h2>Why Video Requires Special Attention<br/></h2><h3>Temporal Redundancy<br/></h3><p>Videos are often densely captured with a high refresh rate and therefore their semantics vary slowly over time. This phenomenon termed <strong>Temporal Redundancy</strong> leads to issues when applying masked modeling to videos.</p><ul role="list"><li>Keeping the original frame rate for pre-training is inefficient since consecutive frames are highly correlated and mostly redundant for representation learning.</li><li>Under standard masking ratios because of temporal redundancy reconstruction is simple because it’s mostly the same scene in every frame.<br/></li></ul><h3>Temporal Correlation<br/></h3><p>Videos can be seen as the evolution of a scene over time with correspondence between the consecutive frames. This correlation leads to information leakage during the reconstruction process. </p><p>Thus for a given masked part of the video (termed <em>cube</em>), it becomes easy to find an unmasked highly correlated copy in adjacent frames. This might lead to the model learning “shortcut” features that don’t generalize to new scenes.</p><h2>The VideoMAE Architecture<br/></h2><p>To overcome these unique properties of applying Masked Modeling techniques to videos Tong et al. introduced a new method in <a href="https://arxiv.org/abs/2203.12602" target="_blank">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training, 2022</a>. VideoMAE is a simple strategy that not only effectively increases the pre-training performance but also greatly reduces the computational cost due to the asymmetric encoder-decoder architecture. The models pre-trained with VideoMAE significantly outperform those trained from scratch or pre-trained with contrastive learning methods.</p><p>‍<br/></p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f7073273e436f269b369a_1*2kc1NdPsLkUhePO8CN23XA.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> VideoMAE architecture. Source: <a href="https://arxiv.org/abs/2203.12602" target="_blank">https://arxiv.org/abs/2203.12602</a></figcaption></figure><p>‍</p><p>This is a simple extension of ImageMAE but with two key properties:</p><ul role="list"><li><strong>Temporal Downsampling:</strong> The authors propose to use a strided temporal sampling strategy to obtain better frames for efficient Self-Supervised Video Pre-training (SSVP). Formally, a video clip with consecutive frames is randomly sampled from the original video. Temporal sampling is then used to compress this clip into fewer frames using strided temporal sampling.</li><li><strong>Cube Embedding: </strong>The authors use a joint space-time cube embedding where a cube represents a 3D token with three dimensions height, width, and time. These cubes are mapped to a token in the channel dimension. This decreases the spatial and temporal dimension of the input and helps alleviate the temporal redundancy.</li><li><strong>Extremely High Masking:</strong> To deal with temporal redundancy, the authors decide to use extremely high mask ratios (90–95%) to mitigate the information leakage during masked modeling. This masking strategy enforces a mask to expand over the whole temporal axis. This masking strategy enforces temporal neighbors of masked cubes to always be masked.</li></ul><p>The following Gradio Space visualises the masking process of VideoMAE</p><h3>Joint Space-Time Attention<br/></h3><p>The authors use a Joint Space-Time Attention policy to learn representations across tokens. However, this has a downside since similarity is computed for all pairs of tokens which is computationally costly due to the large number of patches in a video clip.</p><p>Instead of applying attention to the spatial domain within each frame, the authors use joint space-time attention to learn temporal dependencies across frames. </p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f70a56cf61b4f15de217a_1*jb3W04URpnYsCWbM6g_kHg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of various Space-Time attention policies. Blue patches denote the query patch while the other non-blue colors are the self-attention space-time neighborhood for each scheme. Source: <a href="https://arxiv.org/abs/2102.05095" target="_blank">https://arxiv.org/abs/2102.05095</a></figcaption></figure><p>‍</p><p>Several follow-up works have investigated the impact of space-time attention variants on video understanding tasks. Bertasius et al. in <a href="https://arxiv.org/abs/2102.05095">Is Space-Time Attention All You Need for Video Understanding?, 2021</a> proposed a more efficient architecture for spatiotemporal attention, termed <strong>Divided Space-Time Attention</strong> where temporal attention and spatial attention are separately applied one after the other.</p><h3>Results From VideoMAE</h3><p>The authors conduct extensive experiments and report that the VideoMAE architecture is a data-efficient learner for Self-Supervised Video Pre-training. Notably, even with only 3.5k training clips, VideoMAE obtains satisfying accuracy on the HMDB51 dataset, thus proving its effectiveness on limited data.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f70bdc63b8adcd64b0f09_1*FLgJKCDbMFN1j6_OxyJTqQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of VideoMAE (proposed method) and MoCo v3 (leading contrastive learning method).</figcaption></figure><p>‍</p><ul role="list"><li>Compared to training from scratch and the leading contrastive methods, VideoMAE performs significantly better even with significantly fewer video clips.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f70d313e91bf643f3feb5_1*160q0SuDGjWHDL2idFqntA.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> VideoMAE vs MoCo v3 in terms of efficiency and effectiveness on Something-Something V2</figcaption></figure><p>‍</p><ul role="list"><li>VideoMAE outperforms MoCo v3 in terms of both fine-tuning and linear probing accuracy with a 3.2x speedup.</li></ul><p>‍</p><h2>Follow Up Work<br/></h2><h3>VideoMAEv2</h3><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f7135ce94096cc72b2da9_1*iqU89bH_hHTYLCsBiTkkuA.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Overview of VideoMAEv2. Source: <a href="https://arxiv.org/abs/2303.16727" target="_blank">https://arxiv.org/abs/2303.16727</a></figcaption></figure><p>‍</p><p>In a follow-up work, Wang et al. propose a dual masking strategy for VideoMAE in <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf">VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking, 2023</a>. They further increase the efficiency of the VideoMAE model by applying a masking map to the decoder as well. The model then learns to reconstruct a subset of pixel cubes selected by the running cell masking.</p><p>This better enables large-scale VideoMAE pre-training under a limited computational budget by using the decoder mask to reduce the decoder input length for high efficiency yet attain similar information to the full reconstruction.</p><p>As opposed to VideoMAE which requires pre-training individual models specific to each dataset, the authors aim to learn a universal pre-trained model that could be transferred to different downstream tasks.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f7157183feb7e953f98d3_1*z2UqTaoD1vsvHWCts_MBYA.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Encoder-only masking vs Dual masking. Source: <a href="https://arxiv.org/abs/2303.16727" target="_blank">https://arxiv.org/abs/2303.16727</a></figcaption></figure><ul role="list"><li>This dual masking strategy outperforms encoder-only masking in terms of both performance and training time.</li></ul><p>‍<br/></p><h3>MGMAE</h3><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f71b208fe8d1ac78d7a02_1*qglphpL4P_btV_MUsUTTUQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of various masking strategies. Source: <a href="https://arxiv.org/abs/2308.10794" target="_blank">https://arxiv.org/abs/2308.10794</a></figcaption></figure><p>‍</p><p>Huang et al. in <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.pdf" target="_blank">MGMAE: Motion Guided Masking for Video Masked Autoencoding, 2023</a> introduced a new motion-guided masking strategy that explicitly incorporates motion information to build temporal consistent masking volume. This is based on the insight that motion is a general and unique prior in the video, which should be taken into account during masked pre-training.</p><p>The optical flow representation explicitly encodes the movement of each pixel from the current frame to the next one. This is then used to align masking maps between adjacent frames to build consistent masking volumes across time. In particular, the authors use an online and lightweight optical flow estimator to capture motion information.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f71cf34f7597227f94cd0_1*cuC_Sq6sbCqwNVwuiOe92Q.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Overview of MGMAE. Source: <a href="https://arxiv.org/abs/2308.10794" target="_blank">https://arxiv.org/abs/2308.10794</a></figcaption></figure><p>‍</p><p>Firstly, a masking map is randomly generated at the base frame (by default, middle frame). Estimated optical flow is then used to warp the initial masking map to adjacent frames. As a result of multiple warping operations, a temporally consistent masking volume is built for all frames in the video. Based on this masking volume, a set of visible tokens to the MAE encoder with top-k selection is sampled based on a frame-wise manner.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f71dd3c6f0666a087e1c0_1*7If70s23ubc_ktiVAkiuhw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> MGMAE vs VideoMAE on SSV2. Source: <a href="https://arxiv.org/abs/2308.10794" target="_blank">https://arxiv.org/abs/2308.10794</a></figcaption></figure><p>‍</p><p>With improved accuracy, MGMAE proves to be a more effective video representation learner. It benefits greatly from the harder task constructed with the motion-guided masking strategy.</p><h3>ARVideo</h3><p>Ren et al. in <a href="https://arxiv.org/abs/2405.15160">ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning, 2024</a> address the limitations of cube embeddings by proposing an autoregressive method termed <strong>ARVideo</strong>. Cube embeddings such as those adopted by VideoMAE and MGMAE often fail to encapsulate the rich semantics of the video. This is primarily because:</p><ul role="list"><li>video tokens are dimensionally limited, and</li><li>video inherently lacks a sequential order in its spatial dimensions, although it retains this feature in its temporal aspects.</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f720beece6ace921e9438_1*YbLEmkTd_3gBfdqiy-ct8g.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of video tokens and various clusters</figcaption></figure><p>‍</p><p>To address these limitations, the authors propose a novel autoregressive paradigm with two key design elements:</p><ol role="list"><li>Autoregressive video tokens are organized into spatiotemporal video clusters thus differentiating this method from conventional single-dimensional strategies like spatial video clusters or temporal video clusters. This improves semantic representation by aggregating more contextually relevant multidimensional information.</li><li>They adopt a randomized spatiotemporal prediction order to facilitate learning from multi-dimensional data, addressing the limitations of a handcrafted spatial-first or temporal-first sequence order. This random sequence order empirically yields significantly stronger results, suggesting that effectively capturing the inherent multidimensionality of video data is crucial for autoregressive modeling.</li></ol><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f722301d71e4d92227fbc_1*ESPGcfz4U8eD-RILgVz_yQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> ARVideo architecture</figcaption></figure><p>‍</p><p>They extend the Generative Pretrained Transformer (GPT) framework which autoregressively predicts the next element  given all preceding ones by minimizing the negative log-likelihood wrt model parameters. However, simply extending this framework to videos faces significant challenges, primarily due to the added temporal dimension. Moreover, pixels as autoregressive elements lack semantic richness compared to words in the language, further necessitating pixel grouping strategies to enhance representation learning. </p><p>In <strong>ARVideo</strong> we strategically group spatially neighbored tokens into spatial clusters and temporally adjacent into temporal clusters. These video tokens are then grouped into spatiotemporal clusters with no overlaps. They also apply a random rasterization approach that scrambles the order of clusters randomly during autoregressive pretraining. Such flexibility in autoregressive prediction orders not only captures the inherent multidimensionality of video data more effectively but also fosters a richer, more comprehensive video representation.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/669f723e7a00aeea7df0609a_1*WyY2NwsJEQzd2W_O9WZotg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure</strong>: SOTA methods on Kinetics-400</figcaption></figure><p>‍</p><p>When trained with the ViT-B backbone, ARVideo competitively attains 81.2% on Kinetics-400 while demonstrating higher training efficiency. ARVideo trains 14% faster and requires 58% less GPU memory compared to VideoMAE.</p><h3>Conclusion</h3><p>In conclusion, self-supervised learning for video understanding has made significant strides in recent years, addressing the unique challenges posed by the multidimensional nature of video data. From the foundational work of VideoMAE to innovative approaches like VideoMAEv2, MGMAE, and ARVideo, researchers have tackled issues such as temporal redundancy, information leakage, and the need for more efficient and effective representation learning. </p><p>The methods presented in this post demonstrate how self-supervised learning can be adapted to the video domain by employing strategies that exploit the spatio-temporal structure of videos. This not only results in more generalized models but also reduces the compute requirements for training video-based models significantly.</p></div></div>
  </body>
</html>
