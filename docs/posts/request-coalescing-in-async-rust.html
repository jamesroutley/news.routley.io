<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fasterthanli.me/articles/request-coalescing-in-async-rust">Original</a>
    <h1>Request Coalescing in Async Rust</h1>
    
    <div id="readability-page-1" class="page"><div>
  

  <p>As the popular saying goes, there are only two hard problems in computer
science: caching, off-by-one errors, and getting a Rust job that isn&#39;t
cryptocurrency-related.</p>
<p>Today, we&#39;ll discuss caching! Or rather, we&#39;ll discuss... &#34;request coalescing&#34;,
or &#34;request deduplication&#34;, or &#34;single-flighting&#34; - there&#39;s many names for that
concept, which we&#39;ll get into fairly soon.</p>
<p>That is, as soon as we set the stage...</p>
<h2>A simple web server</h2>
<p>I don&#39;t publish the source code for <a href="https://fasterthanli.me">my website</a>. I try
to concentrate on writing articles and producing videos. I do have a ton of
proprietary code, though: over 8000 lines (according to
<a href="https://lib.rs/crates/tokei">tokei</a>) for the web server alone.</p>
<p>But I don&#39;t want to spend the time to maintain it <em>for everyone</em>, and every
possible use case, and burn myself out just doing that maintenance work, instead
of writing new articles. So I can&#39;t point you at the repository and go &#34;here,
just do that&#34;!</p>
<p>Instead, let&#39;s write a simple version of just the parts we need to get into
today&#39;s topic.</p>
<p>And that part is... a web server. My actual website is powered by
<a href="https://lib.rs/crates/warp">warp</a>, but I&#39;m planning to migrate to
<a href="https://lib.rs/crates/axum">axum</a> pretty quickly, so let&#39;s pretend it&#39;s already
done and use that.</p>
<div>

<p>Living in fantasy land again, are we.</p>
</div>
<p>So first, a hello world:</p>
<pre><p>Shell session</p><p><code>$ cargo new plaque
     Created binary (application) `plaque` package

$ cd plaque

$ cargo run
   Compiling plaque v0.1.0 (/home/amos/bearcove/plaque)
    Finished dev [unoptimized + debuginfo] target(s) in 0.37s
     Running `target/debug/plaque`
Hello, world!
</code></p></pre>
<p>Okay, that was easy. It doesn&#39;t serve web requests though!</p>
<p>To serve web requests, we&#39;ll want to be able to do async I/O. We don&#39;t <em>have</em>
to: we could just do blocking I/O.</p>
<p>Here, let me show you:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{
    error<i>::</i>Error,
    io<i>::</i>{Read, Write},
    net<i>::</i>{SocketAddr, TcpListener},
}<i>;</i>

<i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i><i>mut</i> stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i>?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Accepted connection from {addr}&#34;</i><i>)</i><i>;</i>

        <i>let</i> <i>mut</i> incoming = <i>vec</i><i>!</i><i>[</i><i>]</i><i>;</i>

        <i>loop</i> {
            <i>let</i> <i>mut</i> buf = <i>vec</i><i>!</i><i>[</i><i>0u8</i>; <i>1024</i><i>]</i><i>;</i>
            <i>let</i> read = stream<i>.</i><i>read</i><i>(</i><i>&amp;</i><i>mut</i> buf<i>)</i>?<i>;</i>
            incoming<i>.</i><i>extend_from_slice</i><i>(</i><i>&amp;</i>buf<i>[</i>..read<i>]</i><i>)</i><i>;</i>

            <i>if</i> incoming<i>.</i><i>len</i><i>(</i><i>)</i> &gt; <i>4</i> &amp;&amp; <i>&amp;</i>incoming<i>[</i>incoming<i>.</i><i>len</i><i>(</i><i>)</i> - <i>4</i>..<i>]</i> == <i>b&#34;\r\n\r\n&#34;</i> {
                <i>break</i><i>;</i>
            }
        }

        <i>let</i> incoming = std<i>::</i>str<i>::</i><i>from_utf8</i><i>(</i><i>&amp;</i>incoming<i>)</i>?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Got HTTP request:\n{}&#34;</i>, incoming<i>)</i><i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;HTTP/1.1 200 OK\r\n&#34;</i><i>)</i>?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;\r\n&#34;</i><i>)</i>?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;Hello from plaque!\n&#34;</i><i>)</i>?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Closing connection for {addr}&#34;</i><i>)</i><i>;</i>
    }
}
</code></p></pre>
<p>This works! I had to repress <em>several</em> urges to finish writing this, but it
does, in fact, work:</p>
<pre><p>Shell session</p><p><code># In terminal 1
$ cargo run --quiet
Listening on http://0.0.0.0:3779

# In terminal 2
$ curl http://0.0.0.0:3779
Hello from plaque!

# Meanwhile, in terminal 1
Accepted connection from 127.0.0.1:34594
Got HTTP request:
GET / HTTP/1.1
Host: 0.0.0.0:3779
User-Agent: curl/7.79.1
Accept: */*


Closing connection for 127.0.0.1:34594
</code></p></pre>
<p>It even works from an actual web browser!</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/everyone-loves-edge-right.70557504f2fcdf6a.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/everyone-loves-edge-right.65c68eb37c2f4f4a.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/everyone-loves-edge-right.8840c51826622d31.jpg" title="A screenshot of Microsoft Edge (Chromium-based) that&#39;s opened to localhost:3779 and shows the text &#34;Hello from plaque!&#34;" ,="" alt="A screenshot of Microsoft Edge (Chromium-based) that&#39;s opened to localhost:3779 and shows the text &#34;Hello from plaque!&#34;"/>
            </picture>
            
<p>But it&#39;s not... good.</p>
<h2>The case for async</h2>
<p>There&#39;s many not-so-good things about this web server (turns out there&#39;s a lot
of depth to handling HTTP properly), but there&#39;s a glaring one: it can only
service one client at a time.</p>
<p>If we were to use netcat to open a TCP connection to our server, send a single
HTTP request and just wait, it would prevent anyone else from connecting to the
server.</p>
<p>See, at first our server is blocked in an <code>accept4</code> syscall:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -A 90 &#39;Thread 1&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
Thread 1 (Thread 0x7f7c481d7780 (LWP 775276) &#34;plaque&#34;):
#0  0x00007f7c482ed6aa in accept4 (fd=3, addr=..., addr_len=0x7ffdb31011ac, flags=524288) at ../sysdeps/unix/sysv/linux/accept4.c:32
#1  0x0000558267510329 in std::sys::unix::net::{impl#0}::accept::{closure#0} () at library/std/src/sys/unix/net.rs:225
#2  std::sys::unix::cvt_r&lt;i32, std::sys::unix::net::{impl#0}::accept::{closure#0}&gt; () at library/std/src/sys/unix/mod.rs:212
#3  std::sys::unix::net::Socket::accept () at library/std/src/sys/unix/net.rs:225
#4  std::sys_common::net::TcpListener::accept () at library/std/src/sys_common/net.rs:407
#5  std::net::tcp::TcpListener::accept () at library/std/src/net/tcp.rs:811
#6  0x00005582674fac75 in plaque::main () at src/main.rs:12
</code></p></pre><div>

<p>The command above runs GDB (well, a wrapper for GDB that loads some
Rust-specific scripts) in quiet mode, attaches to the given PID (process id),
which is the PID of the only running instance of &#34;plaque&#34;, our sample application,
then we run GDB command &#34;t a a bt&#34; (thread apply all backtrace), then exit gdb,
then pipe all of that through a couple greps to clean up the output a little.</p>
</div>
<p>And once it accepts a connection, it&#39;s blocked in a <code>read</code> syscall:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -A 90 &#39;Thread 1&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
Thread 1 (Thread 0x7f7c481d7780 (LWP 775276) &#34;plaque&#34;):
#0  0x00007f7c482ed030 in __libc_recv (fd=4, buf=0x558267afeba0, len=1024, flags=0) at ../sysdeps/unix/sysv/linux/recv.c:28
#1  0x0000558267510594 in std::sys::unix::net::Socket::recv_with_flags () at library/std/src/sys/unix/net.rs:245
#2  std::sys::unix::net::Socket::read () at library/std/src/sys/unix/net.rs:251
#3  std::sys_common::net::UdpSocket::recv () at library/std/src/sys_common/net.rs:638
#4  std::net::udp::UdpSocket::recv () at library/std/src/net/udp.rs:693
#5  0x00005582674faf26 in plaque::main () at src/main.rs:19
</code></p></pre><div>

<p>Yes, yes, that makes sen- wait is that <code>UdpSocket</code> in the backtrace?</p>
</div>
<div>

<p>Yeah so, funny story: you know how sometimes people say &#34;code optimizations
make it harder to debug&#34;?</p>
</div>

<div>

<p>That&#39;s one of these cases. Turns out if you have two functions with the exact
same code, LLVM <a href="https://godbolt.org/z/WhG8GnEc6">only generates one</a>.</p>
</div>
<p>Interestingly enough, if we dig a little with GDB:</p>
<pre><p>Shell session</p><p><code>(gdb) frame 3
#3  std::sys_common::net::UdpSocket::recv () at library/std/src/sys_common/net.rs:638
638     in library/std/src/sys_common/net.rs
(gdb) p $rip
$2 = (*mut fn ()) 0x558267510594 &lt;std::net::udp::UdpSocket::recv+20&gt;
(gdb) info symbol 0x558267510594
&lt;std::net::tcp::TcpStream as std::io::Read&gt;::read + 20 in section .text of /home/amos/bearcove/plaque/target/debug/plaque
(gdb) 
</code></p></pre><div>

<p>Oh good. But wait, isn&#39;t this a debug build? Why would LLVM do that?</p>
</div>
<div>

<p>Well, our application is a debug build, but the standard lib is prebuilt (and
optimized). Thanks to <a href="https://twitter.com/m_ou_se">Mara</a> for helping me solve
the puzzle.</p>
</div>
<p>Anyway! Let&#39;s pretend debugging is always ideal and that we&#39;re looking at this:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -A 90 &#39;Thread 1&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
Thread 1 (Thread 0x7f7c481d7780 (LWP 775276) &#34;plaque&#34;):
#0  0x00007f7c482ed030 in __libc_recv (fd=4, buf=0x558267afeba0, len=1024, flags=0) at ../sysdeps/unix/sysv/linux/recv.c:28
#1  0x0000558267510594 in std::sys::unix::net::Socket::recv_with_flags () at library/std/src/sys/unix/net.rs:245
#2  std::sys::unix::net::Socket::read () at library/std/src/sys/unix/net.rs:251
#3  std::sys_common::net::TcpSocket::recv () at library/std/src/sys_common/net.rs:638
#4  std::net::tcp::TcpSocket::recv () at library/std/src/net/tcp.rs:whatever
#5  0x00005582674faf26 in plaque::main () at src/main.rs:19
</code></p></pre>
<p>That&#39;s the problem with blocking syscalls (which are roughly &#34;function calls
into the kernel&#34;, by the way) - we can only do one thing at a time. Here, we&#39;re
either accepting new connections, or reading from an already-established
connection, but never both at the same time.</p>
<p>We can solve this with more <del>violence</del> threads:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{
    error<i>::</i>Error,
    io<i>::</i>{Read, Write},
    net<i>::</i>{SocketAddr, TcpListener, TcpStream},
}<i>;</i>

<i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i>stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i>?<i>;</i>
        std<i>::</i>thread<i>::</i><i>spawn</i><i>(</i><i>move</i> || {
            <i>if</i> <i>let</i> Err<i>(</i>e<i>)</i> = <i>handle_connection</i><i>(</i>stream, addr<i>)</i> {
                <i>println</i><i>!</i><i>(</i><i>&#34;Error handling connection {addr}: {e}&#34;</i><i>)</i><i>;</i>
            }
        }<i>)</i><i>;</i>
    }
}

<i>fn</i> <i>handle_connection</i><i>(</i><i>mut</i> <i>stream</i>: <i>TcpStream</i>, <i>addr</i>: <i>SocketAddr</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>println</i><i>!</i><i>(</i><i>&#34;Accepted connection from {addr}&#34;</i><i>)</i><i>;</i>

    <i>let</i> <i>mut</i> incoming = <i>vec</i><i>!</i><i>[</i><i>]</i><i>;</i>

    <i>loop</i> {
        <i>let</i> <i>mut</i> buf = <i>vec</i><i>!</i><i>[</i><i>0u8</i>; <i>1024</i><i>]</i><i>;</i>
        <i>let</i> read = stream<i>.</i><i>read</i><i>(</i><i>&amp;</i><i>mut</i> buf<i>)</i>?<i>;</i>
        incoming<i>.</i><i>extend_from_slice</i><i>(</i><i>&amp;</i>buf<i>[</i>..read<i>]</i><i>)</i><i>;</i>

        <i>if</i> incoming<i>.</i><i>len</i><i>(</i><i>)</i> &gt; <i>4</i> &amp;&amp; <i>&amp;</i>incoming<i>[</i>incoming<i>.</i><i>len</i><i>(</i><i>)</i> - <i>4</i>..<i>]</i> == <i>b&#34;\r\n\r\n&#34;</i> {
            <i>break</i><i>;</i>
        }
    }

    <i>let</i> incoming = std<i>::</i>str<i>::</i><i>from_utf8</i><i>(</i><i>&amp;</i>incoming<i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Got HTTP request:\n{}&#34;</i>, incoming<i>)</i><i>;</i>
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;HTTP/1.1 200 OK\r\n&#34;</i><i>)</i>?<i>;</i>
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;\r\n&#34;</i><i>)</i>?<i>;</i>
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;Hello from plaque!\n&#34;</i><i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Closing connection for {addr}&#34;</i><i>)</i><i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre>
<p>And now we can have <em>several</em> threads stuck on a syscall:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -E -A 90 &#39;Thread [0-9]*&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
[Thread debugging using libthread_db enabled]
Using host libthread_db library &#34;/lib64/libthread_db.so.1&#34;.
0x00007fd5eb1cd6f0 in accept4 (fd=3, addr=..., addr_len=0x7ffe2b2886fc, flags=524288) at ../sysdeps/unix/sysv/linux/accept4.c:32
32        return SYSCALL_CANCEL (accept4, fd, addr.__sockaddr__, addr_len, flags);

Thread 4 (Thread 0x7fd5eacb4640 (LWP 792836) &#34;plaque&#34;):
#0  __libc_recv (flags=&lt;optimized out&gt;, len=1024, buf=0x7fd5e0000c90, fd=6) at ../sysdeps/unix/sysv/linux/recv.c:28
(cut)
#20 0x00007fd5eb147b1a in start_thread (arg=&lt;optimized out&gt;) at pthread_create.c:443
#21 0x00007fd5eb1cc650 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81

Thread 3 (Thread 0x7fd5eaeb5640 (LWP 792671) &#34;plaque&#34;):
#0  __libc_recv (flags=&lt;optimized out&gt;, len=1024, buf=0x7fd5dc000c90, fd=5) at ../sysdeps/unix/sysv/linux/recv.c:28
(cut)
#20 0x00007fd5eb147b1a in start_thread (arg=&lt;optimized out&gt;) at pthread_create.c:443
#21 0x00007fd5eb1cc650 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81

Thread 2 (Thread 0x7fd5eb0b6640 (LWP 792521) &#34;plaque&#34;):
#0  __libc_recv (flags=&lt;optimized out&gt;, len=1024, buf=0x7fd5e4000c90, fd=4) at ../sysdeps/unix/sysv/linux/recv.c:28
(cut)
#20 0x00007fd5eb147b1a in start_thread (arg=&lt;optimized out&gt;) at pthread_create.c:443
#21 0x00007fd5eb1cc650 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81

Thread 1 (Thread 0x7fd5eb0b7780 (LWP 792264) &#34;plaque&#34;):
#0  0x00007fd5eb1cd6f0 in accept4 (fd=3, addr=..., addr_len=0x7ffe2b2886fc, flags=524288) at ../sysdeps/unix/sysv/linux/accept4.c:32
#1  0x000055d758e79de9 in std::sys::unix::net::{impl#0}::accept::{closure#0} () at library/std/src/sys/unix/net.rs:225
#2  std::sys::unix::cvt_r&lt;i32, std::sys::unix::net::{impl#0}::accept::{closure#0}&gt; () at library/std/src/sys/unix/mod.rs:212
#3  std::sys::unix::net::Socket::accept () at library/std/src/sys/unix/net.rs:225
#4  std::sys_common::net::TcpListener::accept () at library/std/src/sys_common/net.rs:407
#5  std::net::tcp::TcpListener::accept () at library/std/src/net/tcp.rs:811
#6  0x000055d758e5e0ec in plaque::main () at src/main.rs:12
</code></p></pre>
<p>Which is all well and good, until we start hitting some limits. Like, maybe we
have SO MANY CONNECTIONS that we run out of memory, because each of these
threads has its own stack, and that&#39;s not free. It&#39;s not <em>a lot</em>, but a lot of
&#34;not a lot&#34; quickly becomes a lot, as we all learn sooner or later.</p>
<p>Luckily, there&#39;s a way to make those syscalls in a non-blocking manner: you tell
them &#34;I would like to please not block&#34; and they sometimes reply immediately
with a result, and other times with &#34;well try again later, then&#34;.</p>
<p>But <em>when</em> should you try again? Well, you can subscribe to events that let you
know when a resource, say, a TCP socket, is ready to be read from, or written
to.</p>
<p>And... that all sounds like a lot of bookkeeping. Wouldn&#39;t it be convenient if
there was a system that took care of all that, and allowed you to code as if you
were writing blocking code, but it would still be able to do multiple things
concurrently with a single thread???</p>
<p>Well that&#39;s exactly what an async runtime is.</p>
<h2>Enter tokio</h2>
<p>So, let&#39;s make our program async! Well, let&#39;s make a simple async program and go
from there.</p>
<pre><p>Shell session</p><p><code>$ cargo add tokio --features full
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tokio v1.17.0 to dependencies with features: [&#34;full&#34;]
</code></p></pre>
<p>Because I don&#39;t want you to think <code>tokio::main</code> is magic, let&#39;s first go without
it:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>error<i>::</i>Error<i>;</i>

<i>use</i> tokio<i>::</i>net<i>::</i>TcpListener<i>;</i>

<i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> rt = tokio<i>::</i>runtime<i>::</i><i>Builder</i><i>::</i><i>new_current_thread</i><i>(</i><i>)</i>
        <i>.</i><i>enable_all</i><i>(</i><i>)</i>
        <i>.</i><i>build</i><i>(</i><i>)</i>?<i>;</i>
    rt<i>.</i><i>block_on</i><i>(</i>async {
        <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
        <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
        <i>loop</i> {
            <i>let</i> <i>(</i>stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
            <i>println</i><i>!</i><i>(</i><i>&#34;Accepted connection from {addr}&#34;</i><i>)</i><i>;</i>
            <i>// do nothing for now, it&#39;s a simple example</i>
            <i>drop</i><i>(</i>stream<i>)</i>
        }
    }<i>)</i>
}
</code></p></pre>
<p>This accepts a connection and immediately closes it! I bet HTTP user agents
(like curl and web browsers) won&#39;t be too happy about it.</p>
<pre><p>Shell session</p><p><code># In terminal 1
$ cargo run --quiet
Listening on http://0.0.0.0:3779

# In terminal 2
$ curl http://0.0.0.0:3779
curl: (56) Recv failure: Connection reset by peer

# Meanwhile, in terminal 1
Accepted connection from 127.0.0.1:34620
</code></p></pre>
            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/its-edge-again.1cb0032bf4cc1914.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/its-edge-again.5857e2498c65e402.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/its-edge-again.1fa576b36756f280.jpg" title="Another screenshot of Microsoft Edge, opened on the same address as before, showing a &#34;This page isn&#39;t working right now&#34; message, adding: localhost didn&#39;t send any data, and showing off code ERR_EMPTY_RESPONSE" ,="" alt="Another screenshot of Microsoft Edge, opened on the same address as before, showing a &#34;This page isn&#39;t working right now&#34; message, adding: localhost didn&#39;t send any data, and showing off code ERR_EMPTY_RESPONSE"/>
            </picture>
            
<p>And as promised, we&#39;re not &#34;blocked&#34; on a syscall. Well... we&#39;re not blocked on
<code>accept4</code>:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -E -A 90 &#39;Thread [0-9]*&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
[Thread debugging using libthread_db enabled]
Using host libthread_db library &#34;/lib64/libthread_db.so.1&#34;.
0x00007f0f6b8cabea in epoll_wait (epfd=3, events=0x560ec6a4b410, maxevents=1024, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30
Downloading 0.00 MB source file /usr/src/debug/glibc-2.34-25.fc35.x86_64/misc/../sysdeps/unix/sysv/linux/epoll_wait.c...
30        return SYSCALL_CANCEL (epoll_wait, epfd, events, maxevents, timeout);

Thread 1 (Thread 0x7f0f6b7b7cc0 (LWP 1435153) &#34;plaque&#34;):
#0  0x00007f0f6b8cabea in epoll_wait (epfd=3, events=0x560ec6a4b410, maxevents=1024, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30
#1  0x0000560ec62f4c55 in mio::sys::unix::selector::epoll::Selector::select (self=0x7ffd9ab57950, events=0x7ffd9ab56ce0, timeout=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/mio-0.8.0/src/sys/unix/selector/epoll.rs:68
#2  0x0000560ec62fa1c7 in mio::poll::Poll::poll (self=0x7ffd9ab57950, events=0x7ffd9ab56ce0, timeout=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/mio-0.8.0/src/poll.rs:337
#3  0x0000560ec6276776 in tokio::io::driver::Driver::turn (self=0x7ffd9ab57770, max_wait=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/io/driver/mod.rs:163
#4  0x0000560ec62770df in tokio::io::driver::{impl#3}::park (self=0x7ffd9ab57770) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/io/driver/mod.rs:234
#5  0x0000560ec62a42ca in tokio::signal::unix::driver::{impl#1}::park (self=0x7ffd9ab57770) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/signal/unix/driver.rs:150
#6  0x0000560ec62bdaaa in tokio::process::imp::driver::{impl#1}::park (self=0x7ffd9ab57770) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/process/unix/driver.rs:44
#7  0x0000560ec627c363 in tokio::park::either::{impl#0}::park&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt; (self=0x7ffd9ab57768) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/park/either.rs:30
#8  0x0000560ec62a59a4 in tokio::time::driver::Driver&lt;tokio::park::either::Either&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt;&gt;::park_internal&lt;tokio::park::either::Either&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt;&gt; (self=0x7ffd9ab57740, limit=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/time/driver/mod.rs:238
#9  0x0000560ec62a5da0 in tokio::time::driver::{impl#3}::park&lt;tokio::park::either::Either&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt;&gt; (self=0x7ffd9ab57740) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/time/driver/mod.rs:436
#10 0x0000560ec627c44d in tokio::park::either::{impl#0}::park&lt;tokio::time::driver::Driver&lt;tokio::park::either::Either&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt;&gt;, tokio::park::either::Either&lt;tokio::process::imp::driver::Driver, tokio::park::thread::ParkThread&gt;&gt; (self=0x7ffd9ab57738) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/park/either.rs:30
#11 0x0000560ec623fa47 in tokio::runtime::driver::{impl#1}::park (self=0x7ffd9ab57738) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/driver.rs:198
#12 0x0000560ec623e047 in tokio::runtime::basic_scheduler::{impl#4}::park::{closure#1} () at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:315
#13 0x0000560ec623ea80 in tokio::runtime::basic_scheduler::Context::enter&lt;(), tokio::runtime::basic_scheduler::{impl#4}::park::{closure#1}&gt; (self=0x7ffd9ab58518, core=0x560ec6a522b0, f=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:356
#14 0x0000560ec623dd39 in tokio::runtime::basic_scheduler::Context::park (self=0x7ffd9ab58518, core=0x560ec6a522b0) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:314
#15 0x0000560ec622b0bf in tokio::runtime::basic_scheduler::{impl#9}::block_on::{closure#0}&lt;core::pin::Pin&lt;&amp;mut core::future::from_generator::GenFuture&lt;plaque::main::{generator#0}&gt;&gt;&gt; (core=0x560ec6a522b0, context=0x7ffd9ab58518) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:522
#16 0x0000560ec622b8c2 in tokio::runtime::basic_scheduler::{impl#9}::enter::{closure#0}&lt;tokio::runtime::basic_scheduler::{impl#9}::block_on::{closure#0}, core::result::Result&lt;(), alloc::boxed::Box&lt;dyn std::error::Error, alloc::alloc::Global&gt;&gt;&gt; () at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555
#17 0x0000560ec622e6c6 in tokio::macros::scoped_tls::ScopedKey&lt;tokio::runtime::basic_scheduler::Context&gt;::set&lt;tokio::runtime::basic_scheduler::Context, tokio::runtime::basic_scheduler::{impl#9}::enter::{closure#0}, (alloc::boxed::Box&lt;tokio::runtime::basic_scheduler::Core, alloc::alloc::Global&gt;, core::result::Result&lt;(), alloc::boxed::Box&lt;dyn std::error::Error, alloc::alloc::Global&gt;&gt;)&gt; (self=0x560ec63aca88 &lt;tokio::runtime::basic_scheduler::CURRENT&gt;, t=0x7ffd9ab58518, f=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/macros/scoped_tls.rs:61
#18 0x0000560ec622b648 in tokio::runtime::basic_scheduler::CoreGuard::enter&lt;tokio::runtime::basic_scheduler::{impl#9}::block_on::{closure#0}, core::result::Result&lt;(), alloc::boxed::Box&lt;dyn std::error::Error, alloc::alloc::Global&gt;&gt;&gt; (self=..., f=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555
#19 0x0000560ec622aaf0 in tokio::runtime::basic_scheduler::CoreGuard::block_on&lt;core::pin::Pin&lt;&amp;mut core::future::from_generator::GenFuture&lt;plaque::main::{generator#0}&gt;&gt;&gt; (self=..., future=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:488
#20 0x0000560ec6229de5 in tokio::runtime::basic_scheduler::BasicScheduler::block_on&lt;core::future::from_generator::GenFuture&lt;plaque::main::{generator#0}&gt;&gt; (self=0x7ffd9ab58b68, future=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:168
#21 0x0000560ec622eda3 in tokio::runtime::Runtime::block_on&lt;core::future::from_generator::GenFuture&lt;plaque::main::{generator#0}&gt;&gt; (self=0x7ffd9ab58b60, future=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/mod.rs:475
#22 0x0000560ec6232af4 in plaque::main () at src/main.rs:9
</code></p></pre>
<p>We can see the guts of the async runtime here. From bottom to top, we have our
main function, our <code>block_on</code> invocation, some thread-local stuff (<a href="https://en.wikipedia.org/wiki/Thread-local_storage">this
TLS</a>, not <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">that
TLS</a>), we can see that
the thread is &#34;parked&#34;, which in this case means waiting for events coming out
of &#34;epoll&#34;, the mechanism that happens to be used by
<a href="https://lib.rs/crates/tokio">tokio</a> to perform I/O asynchronously on Linux.</p>
<p>Let&#39;s make the code shorter using the
<a href="https://docs.rs/tokio/latest/tokio/attr.main.html">#[tokio::main]</a> attribute
macro:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{error<i>::</i>Error, net<i>::</i>SocketAddr}<i>;</i>

<i>use</i> tokio<i>::</i>net<i>::</i>TcpListener<i>;</i>

<i>// 👇 this will take care of building the runtime</i>
<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>// note: our function is now `async fn`</i>

    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>//                       we can await from there!  👇</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i>stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Accepted connection from {addr}&#34;</i><i>)</i><i>;</i>
        <i>// just do nothing, it&#39;s a simple example</i>
        <i>drop</i><i>(</i>stream<i>)</i>
    }
}
</code></p></pre>
<p>There! That&#39;s exactly equivalent to the longer version, just... more readable.</p>
<p>Now that we&#39;ve convinced ourselves that <code>#[tokio::main]</code> is not too magic, let&#39;s
keep porting our code to async Rust, via tokio:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{error<i>::</i>Error, net<i>::</i>SocketAddr}<i>;</i>

<i>use</i> tokio<i>::</i>{
    io<i>::</i>{AsyncReadExt, AsyncWriteExt},
    net<i>::</i>TcpListener,
}<i>;</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>println</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i><i>mut</i> stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Accepted connection from {addr}&#34;</i><i>)</i><i>;</i>

        <i>let</i> <i>mut</i> incoming = <i>vec</i><i>!</i><i>[</i><i>]</i><i>;</i>

        <i>loop</i> {
            <i>let</i> <i>mut</i> buf = <i>vec</i><i>!</i><i>[</i><i>0u8</i>; <i>1024</i><i>]</i><i>;</i>
            <i>let</i> read = stream<i>.</i><i>read</i><i>(</i><i>&amp;</i><i>mut</i> buf<i>)</i><i>.</i>await?<i>;</i>
            incoming<i>.</i><i>extend_from_slice</i><i>(</i><i>&amp;</i>buf<i>[</i>..read<i>]</i><i>)</i><i>;</i>

            <i>if</i> incoming<i>.</i><i>len</i><i>(</i><i>)</i> &gt; <i>4</i> &amp;&amp; <i>&amp;</i>incoming<i>[</i>incoming<i>.</i><i>len</i><i>(</i><i>)</i> - <i>4</i>..<i>]</i> == <i>b&#34;\r\n\r\n&#34;</i> {
                <i>break</i><i>;</i>
            }
        }

        <i>let</i> incoming = std<i>::</i>str<i>::</i><i>from_utf8</i><i>(</i><i>&amp;</i>incoming<i>)</i>?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Got HTTP request:\n{}&#34;</i>, incoming<i>)</i><i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;HTTP/1.1 200 OK\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;Hello from plaque!\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        <i>println</i><i>!</i><i>(</i><i>&#34;Closing connection for {addr}&#34;</i><i>)</i><i>;</i>
    }
}
</code></p></pre>
<p>There! It looks deceivingly like the sync version, there&#39;s just a bunch of
<code>.await</code> sprinkled everywhere that might block: <code>bind</code>, <code>accept</code>, <code>read</code> and
<code>write_all</code>.</p>
<div>

<p>Why <code>write_all</code>, by the way?</p>
</div>
<div>

<p><code>write</code> doesn&#39;t have to write the whole buffer, that&#39;s why! <code>write_all</code> is a
version that issues as many <code>write</code> as needed to write... all.</p>
</div>
<div>

<p>That seems peculiar... why is the interface designed that way?</p>
</div>
<div>

<p>Well, it&#39;s buffers all the way down. There might be room in the <em>kernel</em>&#39;s TCP
socket buffer for half of our buffer right now, and we may have to wait until
it&#39;s drained, to write the rest.</p>
</div>
<div>

<p>Oh, that actually makes sense. I love it when things make sense!</p>
</div>
<p>So, that program behaves just like its sync counterpart, as in: it only handles
one connection at a time. If client A connects, client B won&#39;t be able to
connect until client A closes its connection (or has its connection closed by
the server):</p>
<pre><p>Shell session</p><p><code># In the server&#39;s terminal
$ cargo run --quiet
Listening on http://0.0.0.0:3779

# In client A&#39;s terminal
$ nc localhost 3779

# In the server&#39;s terminal
Accepted connection from 127.0.0.1:34696

# In client B&#39;s terminal
$ curl http://localhost:3779
</code></p></pre>
<p>But unlike the sync version of our code, it&#39;s not blocked on the <code>read</code> syscall.
It&#39;s, again, blocked on <code>epoll_wait</code>:</p>
<pre><p>Shell session</p><p><code>$ rust-gdb -quiet -pid $(pidof plaque) -ex &#34;t a a bt&#34; -ex &#34;set confirm off&#34; -ex &#34;q&#34; | 2&gt;/dev/null | grep -E -A 90 &#39;Thread [0-9]*&#39; | grep -vE &#39;[dD]etach(ing|ed)&#39;
[Thread debugging using libthread_db enabled]
Using host libthread_db library &#34;/lib64/libthread_db.so.1&#34;.
0x00007ff502a5fbea in epoll_wait (epfd=3, events=0x55ec9905b410, maxevents=1024, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30
30        return SYSCALL_CANCEL (epoll_wait, epfd, events, maxevents, timeout);

Thread 1 (Thread 0x7ff50294ccc0 (LWP 1441723) &#34;plaque&#34;):
#0  0x00007ff502a5fbea in epoll_wait (epfd=3, events=0x55ec9905b410, maxevents=1024, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30
#1  0x000055ec9871aef5 in mio::sys::unix::selector::epoll::Selector::select (self=0x7ffdc746ebf0, events=0x7ffdc746df80, timeout=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/mio-0.8.0/src/sys/unix/selector/epoll.rs:68
#2  0x000055ec987204c7 in mio::poll::Poll::poll (self=0x7ffdc746ebf0, events=0x7ffdc746df80, timeout=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/mio-0.8.0/src/poll.rs:337
#3  0x000055ec98699e66 in tokio::io::driver::Driver::turn (self=0x7ffdc746ea10, max_wait=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/io/driver/mod.rs:163
#4  0x000055ec9869a7cf in tokio::io::driver::{impl#3}::park (self=0x7ffdc746ea10) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/io/driver/mod.rs:234
(cut)
#21 0x000055ec9864d993 in tokio::runtime::Runtime::block_on&lt;core::future::from_generator::GenFuture&lt;plaque::main::{generator#0}&gt;&gt; (self=0x7ffdc74701f0, future=...) at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/mod.rs:475
#22 0x000055ec98651a8d in plaque::main () at src/main.rs:13
</code></p></pre>
<p>And so... from GDB, there&#39;s no great way to tell where it&#39;s stuck. </p>
<p>Async runtimes have, one way or another, their own stack traces to handle.</p>
<p>If we look at a somewhat equivalent Go program, for example:</p>
<pre><p>Go code</p><p><code><i>package</i> main

<i>import</i> (
	<i>&#34;fmt&#34;</i>
	<i>&#34;net&#34;</i>
	<i>&#34;strings&#34;</i>
)

<i>func</i> <i>main</i>() {
	<i>addr</i> <i>:=</i> <i>&#34;0.0.0.0:3779&#34;</i>
	<i>// the original example only listens on IPv4,</i>
	<i>listener</i>, <i>err</i> <i>:=</i> <i>net</i>.<i>Listen</i>(<i>&#34;tcp4&#34;</i>, <i>addr</i>)
	<i>if</i> <i>err</i> <i>!=</i> <i>nil</i> {
		<i>panic</i>(<i>err</i>)
	}
	<i>fmt</i>.<i>Printf</i>(<i>&#34;Listening on http://%s\n&#34;</i>, <i>addr</i>)

	<i>for</i> {
		<i>conn</i>, <i>err</i> <i>:=</i> <i>listener</i>.<i>Accept</i>()
		<i>if</i> <i>err</i> <i>!=</i> <i>nil</i> {
			<i>panic</i>(<i>err</i>)
		}
		<i>fmt</i>.<i>Printf</i>(<i>&#34;Accepted connection from %v\n&#34;</i>, <i>conn</i>.<i>RemoteAddr</i>())

		<i>var</i> <i>incoming</i> []<i>byte</i>
		<i>for</i> {
			<i>buf</i> <i>:=</i> <i>make</i>([]<i>byte</i>, 1024)
			<i>n</i>, <i>err</i> <i>:=</i> <i>conn</i>.<i>Read</i>(<i>buf</i>)
			<i>if</i> <i>err</i> <i>!=</i> <i>nil</i> {
				<i>panic</i>(<i>err</i>)
			}

			<i>incoming</i> <i>=</i> <i>append</i>(<i>incoming</i>, <i>buf</i>[:<i>n</i>]<i>...</i>)

			<i>if</i> <i>len</i>(<i>incoming</i>) <i>&gt;</i> 4 <i>&amp;&amp;</i> <i>strings</i>.<i>HasSuffix</i>(<i>string</i>(<i>incoming</i>), <i>&#34;\r\n\r\n&#34;</i>) {
				<i>break</i>
			}
		}

		<i>// Go doesn&#39;t let us shadow bindings, let&#39;s use a scope to stay close to the</i>
		<i>// original example.</i>
		{
			<i>incoming</i> <i>:=</i> <i>string</i>(<i>incoming</i>)
			<i>fmt</i>.<i>Printf</i>(<i>&#34;Got HTTP request:\n%s\n&#34;</i>, <i>incoming</i>)
			<i>conn</i>.<i>Write</i>([]<i>byte</i>(<i>&#34;HTTP/1.1 200 OK\r\n&#34;</i>))
			<i>conn</i>.<i>Write</i>([]<i>byte</i>(<i>&#34;\r\n&#34;</i>))
			<i>conn</i>.<i>Write</i>([]<i>byte</i>(<i>&#34;Hello from plaque!\n&#34;</i>))
			<i>fmt</i>.<i>Printf</i>(<i>&#34;Closing connection for %v\n&#34;</i>, <i>conn</i>.<i>RemoteAddr</i>())
			<i>err</i> <i>=</i> <i>conn</i>.<i>Close</i>()
			<i>if</i> <i>err</i> <i>!=</i> <i>nil</i> {
				<i>panic</i>(<i>err</i>)
			}
		}
	}
}
</code></p></pre>
<p>It exhibits the exact same behavior: because we&#39;re not spawning a goroutine per
connection, we can only service one connection at a time.</p>
<p>At startup, it&#39;s &#34;blocked&#34; on accepting connections, but we can&#39;t really see
that from gdb <em>just by looking at threads</em>:</p>
<pre><p>Shell session</p><p><code>$ go build ./main.go
$ gdb --quiet --args ./main
Reading symbols from ./main...
Loading Go Runtime support.
(gdb) r
Starting program: /home/amos/bearcove/plaque/main 
[Thread debugging using libthread_db enabled]
Using host libthread_db library &#34;/lib64/libthread_db.so.1&#34;.
Listening on http://0.0.0.0:3779
^C
Thread 1 &#34;main&#34; received signal SIGINT, Interrupt.
runtime.epollwait () at /usr/local/go/src/runtime/sys_linux_amd64.s:666
666             MOVL    AX, ret+24(FP)
(gdb) t a a bt

Thread 5 (Thread 0x7fffcb7fe640 (LWP 1461205) &#34;main&#34;):
#0  runtime.futex () at /usr/local/go/src/runtime/sys_linux_amd64.s:520
#1  0x000000000042f196 in runtime.futexsleep (addr=0xfffffffffffffe00, val=0, ns=4598179) at /usr/local/go/src/runtime/os_linux.go:44
#2  0x000000000040bd27 in runtime.notesleep (n=0xfffffffffffffe00) at /usr/local/go/src/runtime/lock_futex.go:160
#3  0x00000000004398d1 in runtime.templateThread () at /usr/local/go/src/runtime/proc.go:2385
#4  0x0000000000438393 in runtime.mstart1 () at /usr/local/go/src/runtime/proc.go:1407
#5  0x00000000004382d9 in runtime.mstart0 () at /usr/local/go/src/runtime/proc.go:1365
#6  0x000000000045e925 in runtime.mstart () at /usr/local/go/src/runtime/asm_amd64.s:248
#7  0x0000000000463025 in runtime.mstart () at &lt;autogenerated&gt;:1
#8  0x000000000040181b in runtime/cgo(.text) ()
#9  0x0000000000000000 in ?? ()

(Threads 4 through 2 are parked, like Thread 5)

Thread 1 (Thread 0x7ffff7d9b740 (LWP 1461198) &#34;main&#34;):
#0  runtime.epollwait () at /usr/local/go/src/runtime/sys_linux_amd64.s:666
#1  0x000000000042eefc in runtime.netpoll (delay=&lt;optimized out&gt;) at /usr/local/go/src/runtime/netpoll_epoll.go:127
#2  0x000000000043abd3 in runtime.findrunnable () at /usr/local/go/src/runtime/proc.go:2947
#3  0x000000000043bdd9 in runtime.schedule () at /usr/local/go/src/runtime/proc.go:3367
#4  0x000000000043c32d in runtime.park_m (gp=0xc0000ce000) at /usr/local/go/src/runtime/proc.go:3516
#5  0x000000000045e9a3 in runtime.mcall () at /usr/local/go/src/runtime/asm_amd64.s:307
#6  0x0000000000463109 in runtime.newproc (siz=4581669, fn=0x45e8db &lt;runtime.rt0_go+315&gt;) at &lt;autogenerated&gt;:1
#7  0x0000000000000000 in ?? ()
(gdb) 
</code></p></pre>
<p>All we can see <em>by looking at threads</em> is that there&#39;s a thread blocking on
<code>epollwait</code>.</p>
<p>As it turns out, there&#39;s GDB integration for the Go runtime (it ships with Go
itself, see <a href="https://go.dev/doc/gdb">this page</a> for more info), so we can also
print the backtraces for goroutines:</p>
<pre><p>Shell session</p><p><code>(gdb) goroutine all backtrace
#0  runtime.gopark (unlockf=&lt;optimized out&gt;, lock=&lt;optimized out&gt;, reason=&lt;optimized out&gt;, traceEv=&lt;optimized out&gt;, traceskip=&lt;optimized out&gt;) at /usr/local/go/src/runtime/proc.go:367
#1  0x000000000042e613 in runtime.netpollblock (pd=&lt;optimized out&gt;, mode=&lt;optimized out&gt;, waitio=&lt;optimized out&gt;) at /usr/local/go/src/runtime/netpoll.go:454
#2  0x000000000045c4e9 in internal/poll.runtime_pollWait (pd=&lt;optimized out&gt;, mode=0) at /usr/local/go/src/runtime/netpoll.go:234
#3  0x000000000048b932 in internal/poll.(*pollDesc).wait (pd=&lt;optimized out&gt;, mode=0, isFile=192) at /usr/local/go/src/internal/poll/fd_poll_runtime.go:84
#4  0x000000000048cbac in internal/poll.(*pollDesc).waitRead (isFile=192, pd=&lt;optimized out&gt;) at /usr/local/go/src/internal/poll/fd_poll_runtime.go:89
#5  internal/poll.(*FD).Accept (fd=0xc0000d4000, ~r0=&lt;optimized out&gt;, ~r1=..., ~r2=..., ~r3=...) at /usr/local/go/src/internal/poll/fd_unix.go:402
#6  0x00000000004ae7b5 in net.(*netFD).accept (fd=0xc0000d4000) at /usr/local/go/src/net/fd_unix.go:173
#7  0x00000000004beb48 in net.(*TCPListener).accept (ln=0xc00000e048) at /usr/local/go/src/net/tcpsock_posix.go:140
#8  0x00000000004bdf5d in net.(*TCPListener).Accept (l=0xfffffffffffffffc) at /usr/local/go/src/net/tcpsock.go:262
#9  0x00000000004c5b18 in main.main () at /home/amos/bearcove/plaque/main.go:19
No such goroutine:  17
#0  runtime.gopark (unlockf=&lt;optimized out&gt;, lock=&lt;optimized out&gt;, reason=&lt;optimized out&gt;, traceEv=&lt;optimized out&gt;, traceskip=&lt;optimized out&gt;) at /usr/local/go/src/runtime/proc.go:367
#1  0x00000000004359ed in runtime.goparkunlock (reason=&lt;optimized out&gt;, traceEv=&lt;optimized out&gt;, traceskip=&lt;optimized out&gt;, lock=&lt;optimized out&gt;) at /usr/local/go/src/runtime/proc.go:372
#2  runtime.forcegchelper () at /usr/local/go/src/runtime/proc.go:306
#3  0x0000000000460be1 in runtime.goexit () at /usr/local/go/src/runtime/asm_amd64.s:1581
#4  0x0000000000000000 in ?? ()
(cut: three other parked goroutines)
</code></p></pre>
<p>And <em>here</em> we can see where we&#39;re stuck on: <code>net.(*TCPListener).Accept</code>.</p>
<p>Can we get something similar for our Rust program?</p>
<p>Not really. At least, not yet.</p>
<p>As of early March 2022, here&#39;s what <em>I</em> would do to help with that problem.</p>
<h2>A bit of tracing</h2>
<p><a href="https://lib.rs/crates/tracing">tracing</a> is a crate ecosystem that lets you...
trace what&#39;s going on in your application. And it works for async code, too!</p>
<p>Right now, we&#39;ve been using <code>println!</code> to log what&#39;s happening. Let&#39;s see what
it would look like if we were using <code>tracing</code> instead:</p>
<pre><p><code>$ cargo add tracing   
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing v0.1.31 to dependencies

$ cargo add tracing-subscriber --features env-filter
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing-subscriber v0.3.9 to dependencies with features: [&#34;env-filter&#34;]
</code></p></pre><pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{error<i>::</i>Error, net<i>::</i>SocketAddr}<i>;</i>

<i>use</i> tokio<i>::</i>{
    io<i>::</i>{AsyncReadExt, AsyncWriteExt},
    net<i>::</i>TcpListener,
}<i>;</i>
<i>use</i> tracing<i>::</i>{debug, info}<i>;</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>// 👇 this will print tracing events to standard output for humans to read</i>
    tracing_subscriber<i>::</i>fmt<i>::</i><i>init</i><i>(</i><i>)</i><i>;</i>
    <i>// (you can configure a bunch of options, but we&#39;re going all defaults for now)</i>

    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>info</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i><i>mut</i> stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
        <i>info</i><i>!</i><i>(</i>%addr, <i>&#34;Accepted connection&#34;</i><i>)</i><i>;</i>

        <i>let</i> <i>mut</i> incoming = <i>vec</i><i>!</i><i>[</i><i>]</i><i>;</i>

        <i>loop</i> {
            <i>let</i> <i>mut</i> buf = <i>vec</i><i>!</i><i>[</i><i>0u8</i>; <i>1024</i><i>]</i><i>;</i>
            <i>let</i> read = stream<i>.</i><i>read</i><i>(</i><i>&amp;</i><i>mut</i> buf<i>)</i><i>.</i>await?<i>;</i>
            incoming<i>.</i><i>extend_from_slice</i><i>(</i><i>&amp;</i>buf<i>[</i>..read<i>]</i><i>)</i><i>;</i>

            <i>if</i> incoming<i>.</i><i>len</i><i>(</i><i>)</i> &gt; <i>4</i> &amp;&amp; <i>&amp;</i>incoming<i>[</i>incoming<i>.</i><i>len</i><i>(</i><i>)</i> - <i>4</i>..<i>]</i> == <i>b&#34;\r\n\r\n&#34;</i> {
                <i>break</i><i>;</i>
            }
        }

        <i>let</i> incoming = std<i>::</i>str<i>::</i><i>from_utf8</i><i>(</i><i>&amp;</i>incoming<i>)</i>?<i>;</i>
        <i>debug</i><i>!</i><i>(</i>%incoming, <i>&#34;Got HTTP request&#34;</i><i>)</i><i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;HTTP/1.1 200 OK\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;Hello from plaque!\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
        <i>info</i><i>!</i><i>(</i>%addr, <i>&#34;Closing connection&#34;</i><i>)</i><i>;</i>
    }
}
</code></p></pre>
<p>By default, we get timestamps, log levels (which we can filter with the
<code>RUST_LOG</code> environment variable - that&#39;s the <code>env-filter</code> feature we asked
<a href="https://lib.rs/crates/cargo-edit">cargo-edit</a> to enable earlier when we invoked
<code>cargo add</code>), and neat formatting:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=debug cargo run
    Finished dev [unoptimized + debuginfo] target(s) in 0.01s
     Running `target/debug/plaque`
2022-03-03T17:22:22.837683Z  INFO plaque: Listening on http://0.0.0.0:3779
2022-03-03T17:22:24.077415Z  INFO plaque: Accepted connection addr=127.0.0.1:34718
2022-03-03T17:22:24.077511Z DEBUG plaque: Got HTTP request incoming=GET / HTTP/1.1
Host: localhost:3779
User-Agent: curl/7.79.1
Accept: */*


2022-03-03T17:22:24.077579Z  INFO plaque: Closing connection addr=127.0.0.1:34718
</code></p></pre>
<p>Also, colors!</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-subscriber-colors.568dc75c64735f3a.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-subscriber-colors.dfd07d3b39ab9012.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tracing-subscriber-colors.56f8a2c79cfa8312.jpg" title="A screenshot of my terminal showing tracing-subscriber output: timestamps are grey, INFO is green, DEBUG is blue. Span attribute names are in italics" ,="" alt="A screenshot of my terminal showing tracing-subscriber output: timestamps are grey, INFO is green, DEBUG is blue. Span attribute names are in italics"/>
            </picture>
            
<p>That&#39;s just scratching the surface though. Those are events, but we can also
have spans: we could have a span for the whole server, another for accepting
connections, another for our connection handler, one for accepting incoming
connections, one for reading the incoming request and one for writing the
response:</p>
<pre><p>Rust code</p><p><code><i>use</i> std<i>::</i>{error<i>::</i>Error, net<i>::</i>SocketAddr}<i>;</i>

<i>use</i> tokio<i>::</i>{
    io<i>::</i>{AsyncRead, AsyncReadExt, AsyncWrite, AsyncWriteExt},
    net<i>::</i>{TcpListener, TcpStream},
}<i>;</i>
<i>use</i> tracing<i>::</i>{debug, info, info_span, Instrument}<i>;</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    tracing_subscriber<i>::</i>fmt<i>::</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await
}

<i>// 👇 here we use an attribute macro</i>
<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>run_server</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>info</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
    <i>loop</i> {
        <i>//                      here we use a Future extension trait 👇</i>
        <i>let</i> <i>(</i>stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i><i>instrument</i><i>(</i><i>info_span</i><i>!</i><i>(</i><i>&#34;accept&#34;</i><i>)</i><i>)</i><i>.</i>await?<i>;</i>
        <i>handle_connection</i><i>(</i>stream, addr<i>)</i><i>.</i>await?<i>;</i>
    }
}

<i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>stream<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>handle_connection</i><i>(</i><i>mut</i> <i>stream</i>: <i>TcpStream</i>, <i>addr</i>: <i>SocketAddr</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> req = <i>read_http_request</i><i>(</i><i>&amp;</i><i>mut</i> stream<i>)</i><i>.</i>await?<i>;</i>
    <i>debug</i><i>!</i><i>(</i>%req, <i>&#34;Got HTTP request&#34;</i><i>)</i><i>;</i>
    <i>write_http_response</i><i>(</i><i>&amp;</i><i>mut</i> stream<i>)</i><i>.</i>await?<i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}

<i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>stream<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>read_http_request</i><i>(</i><i>mut</i> <i>stream</i>: <i>impl</i> <i>AsyncRead</i> + <i>Unpin</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>String</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> <i>mut</i> incoming = <i>vec</i><i>!</i><i>[</i><i>]</i><i>;</i>

    <i>loop</i> {
        <i>let</i> <i>mut</i> buf = <i>vec</i><i>!</i><i>[</i><i>0u8</i>; <i>1024</i><i>]</i><i>;</i>
        <i>let</i> read = stream<i>.</i><i>read</i><i>(</i><i>&amp;</i><i>mut</i> buf<i>)</i><i>.</i>await?<i>;</i>
        incoming<i>.</i><i>extend_from_slice</i><i>(</i><i>&amp;</i>buf<i>[</i>..read<i>]</i><i>)</i><i>;</i>

        <i>if</i> incoming<i>.</i><i>len</i><i>(</i><i>)</i> &gt; <i>4</i> &amp;&amp; <i>&amp;</i>incoming<i>[</i>incoming<i>.</i><i>len</i><i>(</i><i>)</i> - <i>4</i>..<i>]</i> == <i>b&#34;\r\n\r\n&#34;</i> {
            <i>break</i><i>;</i>
        }
    }

    Ok<i>(</i><i>String</i><i>::</i><i>from_utf8</i><i>(</i>incoming<i>)</i>?<i>)</i>
}

<i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>stream<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>write_http_response</i><i>(</i><i>mut</i> <i>stream</i>: <i>impl</i> <i>AsyncWrite</i> + <i>Unpin</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;HTTP/1.1 200 OK\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;\r\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
    stream<i>.</i><i>write_all</i><i>(</i><i>b&#34;Hello from plaque!\n&#34;</i><i>)</i><i>.</i>await?<i>;</i>
    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre><pre><p>Shell session</p><p><code>$ RUST_LOG=debug cargo run
   Compiling plaque v0.1.0 (/home/amos/bearcove/plaque)
    Finished dev [unoptimized + debuginfo] target(s) in 1.19s
     Running `target/debug/plaque`
2022-03-03T17:47:10.565608Z  INFO run_server: plaque: Listening on http://0.0.0.0:3779
2022-03-03T17:47:11.203471Z DEBUG run_server:handle_connection{addr=127.0.0.1:34722}: plaque: Got HTTP request req=GET / HTTP/1.1
Host: localhost:3779
User-Agent: curl/7.79.1
Accept: */*
</code></p></pre>
<p>At first glance, there&#39;s not much of a difference... at least with the default
<code>tracing-subscriber</code> formatter.</p>
<p>But let&#39;s try something like <a href="https://lib.rs/crates/tracing-tree">tracing-tree</a>:</p>
<pre><p>Shell session</p><p><code>$ cargo add tracing-tree
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing-tree v0.2.0 to dependencies
</code></p></pre><pre><p>Rust code</p><p><code><i>// omitted: other `use` directives</i>

<i>use</i> tracing_subscriber<i>::</i>{layer<i>::</i>SubscriberExt, util<i>::</i>SubscriberInitExt, Registry}<i>;</i>
<i>use</i> tracing_tree<i>::</i>HierarchicalLayer<i>;</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>// 👇 new!</i>
    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await
}
</code></p></pre>
<p>Now we get hierarchical output!</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=debug cargo run
   Compiling plaque v0.1.0 (/home/amos/bearcove/plaque)
    Finished dev [unoptimized + debuginfo] target(s) in 1.25s
     Running `target/debug/plaque`
plaque::run_server{}
  0ms  INFO plaque Listening on http://0.0.0.0:3779
  plaque::accept{}
  
  plaque::handle_connection{addr=127.0.0.1:34728}
    plaque::read_http_request{}
    
    0ms DEBUG plaque Got HTTP request, req=GET / HTTP/1.1
    Host: localhost:3779
    User-Agent: curl/7.79.1
    Accept: */*
    
    
    plaque::write_http_response{}
    
  
  plaque::accept{}

</code></p></pre>
<p>Here it is with colors:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-tree.bccfed67ddf45530.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-tree.a8483a734ddf1ff6.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tracing-tree.99c7b4a8b0c8d6be.jpg" title="The same output as above, but with colors. The &#39;plaque&#39; part (crate name) is in grey, and so are timings. INFO is in green, DEBUG is in blue, as before" ,="" alt="The same output as above, but with colors. The &#39;plaque&#39; part (crate name) is in grey, and so are timings. INFO is in green, DEBUG is in blue, as before"/>
            </picture>
            
<p>I love <code>tracing-tree</code> because it shows not only events, but also when we
enter/exit spans. There&#39;s <a href="https://docs.rs/tracing-tree/0.2.0/tracing_tree/struct.HierarchicalLayer.html">a bunch of knobs you can turn</a>
to get the output juuuuust right.</p>
<p>And that means... we can kinda see where we get &#34;stuck&#34;! Here we can see it&#39;s in
&#34;accept&#34;.</p>
<p>Of course that&#39;s definitely not the same as a debugger: if there were other
tasks running in our async runtime, we wouldn&#39;t see them. We&#39;re not able to
list all the tasks, see their full backtrace, etc.</p>
<p>Here&#39;s another thing we can do with tracing: it&#39;s all structured, so instead of
printing human-readable, we can print newline-delimited JSON! And because it&#39;s
all composable, we can use that at the same time as tracing-tree: just add a
layer!</p>
<pre><p>Shell session</p><p><code>$ cargo add tracing-subscriber --features &#34;env-filter json&#34;
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing-subscriber v0.3.9 to dependencies with features: [&#34;env-filter&#34;, &#34;json&#34;]
 
</code></p></pre><pre><p>Rust code</p><p><code><i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>// 👇 new!</i>
        <i>.</i><i>with</i><i>(</i>
            tracing_subscriber<i>::</i>fmt<i>::</i><i>layer</i><i>(</i><i>)</i>
                <i>.</i><i>json</i><i>(</i><i>)</i>
                <i>.</i><i>with_writer</i><i>(</i>|| <i>File</i><i>::</i><i>create</i><i>(</i><i>&#34;/tmp/log.json&#34;</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await
}
</code></p></pre>
<p>Here&#39;s a request from Chrome logged as JSON, for example:</p>
<pre><p>json</p><p><code>{
  &#34;timestamp&#34;: &#34;2022-03-03T18:11:04.867737Z&#34;,
  &#34;level&#34;: &#34;DEBUG&#34;,
  &#34;fields&#34;: {
    &#34;message&#34;: &#34;Got HTTP request&#34;,
    &#34;req&#34;: &#34;GET /favicon.ico HTTP/1.1\r\nHost: localhost:3779\r\nConnection: keep-alive\r\nsec-ch-ua: \&#34; Not A;Brand\&#34;;v=\&#34;99\&#34;, \&#34;Chromium\&#34;;v=\&#34;98\&#34;, \&#34;Google Chrome\&#34;;v=\&#34;98\&#34;\r\nDNT: 1\r\nsec-ch-ua-mobile: ?0\r\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\r\nsec-ch-ua-platform: \&#34;Windows\&#34;\r\nAccept: image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\r\nSec-Fetch-Site: same-origin\r\nSec-Fetch-Mode: no-cors\r\nSec-Fetch-Dest: image\r\nReferer: http://localhost:3779/\r\nAccept-Encoding: gzip, deflate, br\r\nAccept-Language: en-US,en;q=0.9,fr-FR;q=0.8,fr;q=0.7,de;q=0.6\r\nCookie: admin=\r\n\r\n&#34;
  },
  &#34;target&#34;: &#34;plaque&#34;,
  &#34;span&#34;: {
    &#34;addr&#34;: &#34;127.0.0.1:34734&#34;,
    &#34;name&#34;: &#34;handle_connection&#34;
  },
  &#34;spans&#34;: [
    {
      &#34;name&#34;: &#34;run_server&#34;
    },
    {
      &#34;addr&#34;: &#34;127.0.0.1:34734&#34;,
      &#34;name&#34;: &#34;handle_connection&#34;
    }
  ]
}
</code></p></pre>
<p>But let&#39;s dream bigger. Let&#39;s dream <em>better</em>.</p>
<p>It&#39;s easy to imagine that, given the examples we&#39;ve seen, we have enough
information about spans and traces to be able to send them, in a structured way,
to various tools and services to allow us to dig even deeper into them.</p>
<p>For example, we could generate a Chrome trace out of it:</p>
<pre><p>Shell session</p><p><code>$ cargo add tracing-chrome
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing-chrome v0.4.0 to dependencies
</code></p></pre><pre><p>Rust code</p><p><code><i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> <i>(</i>chrome_layer, _guard<i>)</i> = tracing_chrome<i>::</i><i>ChromeLayerBuilder</i><i>::</i><i>new</i><i>(</i><i>)</i>
        <i>.</i><i>file</i><i>(</i><i>&#34;/shared/chrome-trace.json&#34;</i><i>.</i><i>into</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>build</i><i>(</i><i>)</i><i>;</i>
    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>with</i><i>(</i>chrome_layer<i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await
}
</code></p></pre>
<p>And visualize it from Chrome:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-chrome.8ef53790b010096c.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tracing-chrome.387035a5723e8352.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tracing-chrome.61ea5d352908b1f8.jpg" title="A screenshot of chrome&#39;s built-in tracing tool, showing some request being processed" ,="" alt="A screenshot of chrome&#39;s built-in tracing tool, showing some request being processed"/>
            </picture>
            
<p>Which is not even the best visualizer for these btw: there&#39;s a lot of tools out
there that process this format, like <a href="https://ui.perfetto.dev/">Perfetto</a> and
<a href="https://www.speedscope.app/">Speedscope</a>.</p>
<p>The Chrome tracing format isn&#39;t the best fit for what we&#39;re after though (which
is figuring out what&#39;s happening in our async app): we might want to look at a
number of <a href="https://opentelemetry.io/">OpenTelemetry</a> collectors.</p>
<p>We could go for <a href="https://www.honeycomb.io/">Honeycomb</a>,
<a href="https://www.datadoghq.com/">Datadog</a>, and many others. Heck, even
<a href="https://sentry.io/welcome/">Sentry</a> accepts traces now.</p>
<p>Because I&#39;m always trying to make my articles &#34;do-try-this-at-home&#34;, we&#39;ll go
with something self-hosted here:
<a href="https://www.jaegertracing.io/download/">Jaeger</a>.</p>
<p>Because of the OpenTelemetry indirection, the setup is a tad more involved, but
it&#39;s just that: setup. The code remains the same, and is instrumented the same
way.</p>
<pre><p>Rust code</p><p><code>$ cargo add opentelemetry --features rt-tokio
    Updating <i>&#39;</i>https:<i>//github.com/rust-lang/crates.io-index&#39; index</i>
      Adding opentelemetry v0<i>.</i><i>17</i><i>.</i><i>0</i> to dependencies with features: <i>[</i><i>&#34;rt-tokio&#34;</i><i>]</i>

$ cargo add opentelemetry-jaeger --features rt-tokio
    Updating <i>&#39;</i>https:<i>//github.com/rust-lang/crates.io-index&#39; index</i>
      Adding opentelemetry-jaeger v0<i>.</i><i>16</i><i>.</i><i>0</i> to dependencies with features: <i>[</i><i>&#34;rt-tokio&#34;</i><i>]</i>

$ cargo add tracing-opentelemetry
    Updating <i>&#39;</i>https:<i>//github.com/rust-lang/crates.io-index&#39; index</i>
      Adding tracing-opentelemetry v0<i>.</i><i>17</i><i>.</i><i>2</i> to dependencies
</code></p></pre><pre><p>Rust code</p><p><code><i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> tracer =
        opentelemetry_jaeger<i>::</i><i>new_pipeline</i><i>(</i><i>)</i><i>.</i><i>install_batch</i><i>(</i>opentelemetry<i>::</i>runtime<i>::</i>Tokio<i>)</i>?<i>;</i>

    <i>let</i> telemetry = tracing_opentelemetry<i>::</i><i>layer</i><i>(</i><i>)</i><i>.</i><i>with_tracer</i><i>(</i>tracer<i>)</i><i>;</i>

    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>with</i><i>(</i>telemetry<i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>

    opentelemetry<i>::</i>global<i>::</i><i>shutdown_tracer_provider</i><i>(</i><i>)</i><i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre>
<p>Hey that wasn&#39;t even half bad!</p>
<p>Now let&#39;s start Jaeger in the background, as a container:</p>
<pre><p>Shell session</p><p><code>$ docker run -d -p6831:6831/udp -p6832:6832/udp -p16686:16686 -p14268:14268 jaegertracing/all-in-one:latest
</code></p></pre><div>

<p>Huh. No output? Is it even running?</p>
</div>
<pre><p>Shell session</p><p><code>$ docker ps
CONTAINER ID   IMAGE                             COMMAND                  CREATED              STATUS              PORTS                                                                                                                                                                                        NAMES
3b0727ba7f92   jaegertracing/all-in-one:latest   &#34;/go/bin/all-in-one-…&#34;   About a minute ago   Up About a minute   5778/tcp, 0.0.0.0:14268-&gt;14268/tcp, :::14268-&gt;14268/tcp, 5775/udp, 14250/tcp, 0.0.0.0:6831-6832-&gt;6831-6832/udp, :::6831-6832-&gt;6831-6832/udp, 0.0.0.0:16686-&gt;16686/tcp, :::16686-&gt;16686/tcp   crazy_volhard
</code></p></pre>
<p>The Jaeger &#34;all-in-one&#34; container is serving its web UI on <a href="http://localhost:16686">http://localhost:16686</a>:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-empty.c25781d00d7159f1.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-empty.4d98265bd86762ec.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/jaeger-empty.56abbdb41c91d47a.jpg" title="An empty Jaeger view, on /search, showing an adventurous gopher with... an adventure hat" ,="" alt="An empty Jaeger view, on /search, showing an adventurous gopher with... an adventure hat"/>
            </picture>
            
<p>If we run our server and hit it with curl, then refresh Jaeger, we can see that
it received traces from two services: <code>jaeger-query</code> (jaeger instruments
itself!), and <code>unknown_service</code>: we haven&#39;t really done any configuration at
all, and OpenTelemetry accepts extra metadata compared to tracing, so we&#39;re gonna
see a bunch of default values here.</p>
<p>Clicking &#34;Search&#34; shows the traces we&#39;ve received so far:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-unknown-service.d953412b81bc1e46.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-unknown-service.fda5b9ec6f83d3e2.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/jaeger-unknown-service.bf45e1ded39990bc.jpg" title="A slightly less empty Jaeger view, with the service dropdown selected, with unknown_service selected. It shows one trace called &#39;accept&#39;, with four spans" ,="" alt="A slightly less empty Jaeger view, with the service dropdown selected, with unknown_service selected. It shows one trace called &#39;accept&#39;, with four spans"/>
            </picture>
            
<p>Clicking on a trace shows an expanded view, with all the associated spans:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-trace.17484820b069800c.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-trace.6dacc0e36cdc68d0.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/jaeger-trace.e0fab4e1bf8a26a5.jpg" title="The detail of a trace in Jaeger" ,="" alt="The detail of a trace in Jaeger"/>
            </picture>
            
<p>And notice that the attributes we&#39;ve set are there: they&#39;re called &#34;tags&#34; here.
<code>accept_connection</code> has the <code>addr</code>. The &#34;Got HTTP request&#34; event has the full
HTTP request header associated to it.</p>
<p>Because we have a top-level span, <em>all requests</em> are actually grouped together:
if we run several requests, we can see that they all have a parent in common,
and we can see us &#34;spending time&#34; in accept (in scare quotes because again, the
thread is actually parked, and so it&#39;s mostly idle - mostly because there&#39;s
background tasks now, flushing traces and whatnot).</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-many-traces.aff42e173f6463c3.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-many-traces.e9cb4ae53fbdcfd3.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/jaeger-many-traces.b78aa273a2bfdd0b.jpg" title="The same trace, but with many more requests. We can see in the middle there were several minutes of pause within accept" ,="" alt="The same trace, but with many more requests. We can see in the middle there were several minutes of pause within accept"/>
            </picture>
            
<p>If we remove some spans, by removing the <code>#[tracing::instrument]</code> attribute on
<code>run_server</code>, each connection has its own trace:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-no-toplevel.6530ae6973c0346b.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/jaeger-no-toplevel.f9752d7f66123918.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/jaeger-no-toplevel.e66c14313edc2a71.jpg" title="Jaeger search again, showing 15 traces for the &#39;handle_connection&#39; operation. There&#39;s a graph that shows the duration of these traces ranges roughly from 500 microseconds to 600 microseconds" ,="" alt="Jaeger search again, showing 15 traces for the &#39;handle_connection&#39; operation. There&#39;s a graph that shows the duration of these traces ranges roughly from 500 microseconds to 600 microseconds"/>
            </picture>
            
<p>On top of all traces, there&#39;s a graph that shows latency for the <code>handle_connection</code> &#34;operation&#34;: it&#39;s between 500 and 600 microseconds.</p>
<p>That&#39;s on a debug build. If I do <code>RUST_LOG=debug cargo run --release</code> instead, I
see latencies of around 150 microseconds instead.</p>
<p>There would be a lot more to say about OpenTelemetry, but for now, let&#39;s skip to
the last tool we&#39;ll see before addressing the topic at hand:
<a href="https://lib.rs/crates/tokio-console">tokio-console</a>.</p>
<pre><p>Shell session</p><p><code>$ cargo install tokio-console
    Updating crates.io index
     Ignored package `tokio-console v0.1.2` is already installed, use --force to override
</code></p></pre>
<p>It gives us a dynamic realtime view into what&#39;s going on within a Tokio runtime:
it&#39;s less GDB and more <a href="https://linux.die.net/man/1/top">top(1)</a>.</p>
<p>Unfortunately, for performance reasons, we can&#39;t really use that on a production
app, but since we&#39;re playing around locally, there&#39;s nothing stopping us.</p>
<p>The full instructions to enable it are <a href="https://lib.rs/crates/tokio-console">in tokio-console&#39;s
documentation</a>: I won&#39;t reproduce them here
because <code>tokio-console</code> is still in the early stages and I expect the
instructions to change.</p>
<p>At the time of this writing, <code>tokio-console</code> relies on
<a href="https://lib.rs/crates/console-subscriber">console-subscriber</a>, which is, you
guessed it, a <code>tracing</code> layer!</p>
<p>So, we can just add it to the pile:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> tracer =
        opentelemetry_jaeger<i>::</i><i>new_pipeline</i><i>(</i><i>)</i><i>.</i><i>install_batch</i><i>(</i>opentelemetry<i>::</i>runtime<i>::</i>Tokio<i>)</i>?<i>;</i>

    <i>let</i> telemetry = tracing_opentelemetry<i>::</i><i>layer</i><i>(</i><i>)</i><i>.</i><i>with_tracer</i><i>(</i>tracer<i>)</i><i>;</i>

    <i>let</i> <i>(</i>console, server<i>)</i> = console_subscriber<i>::</i><i>ConsoleLayer</i><i>::</i><i>builder</i><i>(</i><i>)</i><i>.</i><i>build</i><i>(</i><i>)</i><i>;</i>
    tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
        server<i>.</i><i>serve</i><i>(</i><i>)</i><i>.</i>await<i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>
    }<i>)</i><i>;</i>

    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>with</i><i>(</i>telemetry<i>)</i>
        <i>.</i><i>with</i><i>(</i>console<i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    <i>run_server</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>

    opentelemetry<i>::</i>global<i>::</i><i>shutdown_tracer_provider</i><i>(</i><i>)</i><i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre>
<p><em>There&#39;s other steps I&#39;m not showing: several features to enable, via <code>Cargo.toml</code>,
also <code>.cargo/config.toml</code>. Read the docs!</em></p>
<p>The instructions I&#39;m reading now state that we must have
<code>tokio=trace,runtime=trace</code> in our env filter, so, that&#39;s how we want to start
the app in order to let <code>console-subscriber</code> work:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tokio=trace,runtime=trace,debug cargo run
</code></p></pre>
<p>And now, after starting our application, we can run <code>tokio-console</code>.</p>
<p>By default, it opens on the tasks view:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-tasks.b989ce8fb3b8ad76.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-tasks.aca4fd665db96cab.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tokio-console-tasks.06d19126d60ee038.jpg" title="tokio-console&#39;s task view: there&#39;s four tasks, one running, three stopped. The first two are from console-subscriber itself, and the last two are from hyper" ,="" alt="tokio-console&#39;s task view: there&#39;s four tasks, one running, three stopped. The first two are from console-subscriber itself, and the last two are from hyper"/>
            </picture>
            
<p>We can see two of console-subscriber&#39;s internal tasks, and two hyper tasks. We
can scroll through tasks with the up and down arrows (or k and j, for the
<a href="https://en.wikipedia.org/wiki/Arrow_keys#HJKL_keys">vi-inclined</a>), and pressing
enter shows details for a task...</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-single-task.1289b1876330efe9.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-single-task.13bacaa36181faf9.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tokio-console-single-task.2be63f0e6338bca2.jpg" title="The details for " ,="" alt="The details for "/>
            </picture>
            
<p>...including its name, target, the location in code where it was spawned, how
long it&#39;s existed, and how much CPU time we&#39;ve spent on it. There&#39;s also
statistics about its wakers (think about asynchronous I/O operations on TCP
sockets again: we need to have a way to wake up a task when the socket is
ready to be read from/written to), and a wonderful poll time histogram, along
with percentiles: we can see it&#39;s usually done in under 5 milliseconds (that&#39;s
a debug build again).</p>
<p>Unfortunately, for now:</p>
<ul>
<li>There&#39;s no stack trace (unsurprising, stack traces are not recorded)</li>
<li>There&#39;s no <em>span trace</em> either: we don&#39;t know the context in which this was spanned</li>
<li>The only API to set the name of a task is internal and behind feature flags</li>
<li>I have no idea how fields are set: I have never seen a field other than &#34;kind=task&#34;</li>
</ul>
<p>I see those as hints of what&#39;s planned for <code>tokio-console</code> - and I can&#39;t wait to
see more!</p>
<p>Let&#39;s take a quick look at the &#34;resources&#34; view as well:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-resources.c88086bdbe0a259a.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-resources.5a5de16592e10ee8.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tokio-console-resources.27d84e738032dfcc.jpg" title="tokio-console&#39;s resources view" ,="" alt="tokio-console&#39;s resources view"/>
            </picture>
            
<p>This is a recent improvement: timers are only tracked since tokio 1.13.0, and
mutexes / rwlocks / channels are only tracked since tokio 1.15.0.</p>
<p>Just looking at this, I&#39;m guessing that our &#34;batched&#34; opentelemetry span
exporter exports... every thirty seconds! Since it seems to be sleeping for 30
seconds.</p>
<p>I have no idea what all those semaphores are used for, it seems like
<code>console-subscriber</code> is doing things at an interval (probably sending data to
<code>tokio-console</code>), and it seems tokio&#39;s intervals are implemented in terms of
<code>sleep</code>, if the last line is to be believed.</p>
<p>Like for tasks, scrolling to a specific resource and pressing enter shows us
a detail view: for the interval, we can see a list of async ops:</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-async-ops.1c5507b54e6ad238.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-async-ops.496e47233ce99d8b.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tokio-console-async-ops.2e83527357c5ab3f.jpg" title="tokio-console&#39;s detail view for one of console-subscriber&#39;s interval resource" ,="" alt="tokio-console&#39;s detail view for one of console-subscriber&#39;s interval resource"/>
            </picture>
            
<p>Which is neat, because it means the relationship between resources and tasks is
already being tracked. Again, there&#39;s a lot planned for tokio-console, you can
check out <a href="https://github.com/tokio-rs/console/">its repository</a> to keep track
of what&#39;s happening.</p>
<div>

<p>I can&#39;t help but notice there&#39;s nothing in there about us accepting
connections... right?</p>
</div>
<div>

<p>Ah, right... well it&#39;s not spawned as a task, maybe if we spawn it as a task?</p>
</div>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>// omitted: tracing + opentelemetry setup</i>

    <i>let</i> handle = tokio<i>::</i>task<i>::</i><i>spawn</i><i>(</i>async { <i>run_server</i><i>(</i><i>)</i><i>.</i>await<i>.</i><i>unwrap</i><i>(</i><i>)</i> }<i>)</i><i>;</i>
    handle<i>.</i>await?<i>;</i>

    opentelemetry<i>::</i>global<i>::</i><i>shutdown_tracer_provider</i><i>(</i><i>)</i><i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre>
<p>If we do that, then yes indeed, our task does show up! It has an empty name, but
the <code>location</code> is right: it&#39;s in <code>main.rs</code> somewhere.</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-main-task.27cc5fcce4f5f5b2.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/tokio-console-main-task.adb67c50edd03df6.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/tokio-console-main-task.61b61670f89ed25f.jpg" title="tokio-console&#39;s task detail view for our main task" ,="" alt="tokio-console&#39;s task detail view for our main task"/>
            </picture>
            
<p>And we can see from the poll time histogram that we&#39;re spending around 100
microseconds to 260 microseconds polling it: this is not the same as latency,
because it might need to be polled multiple times for a single HTTP request to
be handled, since it involves an accept, one or more reads, and multiple writes.</p>
<div>

<p>Looking at thread backtraces isn&#39;t too useful when we&#39;re using an async runtime.
This is true of Go, too, but they track enough information about goroutines and
have added enough support to GDB that you can list all goroutine backtraces
(also via <a href="https://pkg.go.dev/net/http/pprof">pprof</a> over HTTP), and that&#39;s
invaluable.</p>
<p>In Rust, with the tokio runtime, there is no such thing - yet. That&#39;s typically
something the <a href="https://rust-lang.github.io/wg-async/index.html">Async workgroup</a>
would be working on.</p>
<p>However, we&#39;ve discovered the <a href="https://lib.rs/keywords/tracing">tracing
ecosystem</a>, which makes it extremely easy to
collect structured spans and events and send them to various places: as
human-readable logs, JSON lines, profiler dumps, any OpenTelemetry tracing
solution, etc.</p>
<p>Crates like <a href="https://lib.rs/crates/color-eyre">color-eyre</a>, although not shown
here, also integrate nicely into the tracing ecosystem, showing not only
stacktraces, but also spantraces.</p>
<p><a href="https://lib.rs/crates/tokio-console">tokio-console</a> brands itself &#34;top(1) for
tokio&#34;, and although it&#39;s not usable in production yet, there are many exciting
plans around it, notably around answering questions like &#34;why is this task not
making progress? what resources does it depend on?&#34;. There&#39;s still a lot of open
questions about the exact design of the plumbing around all of this.</p>
</div>
<p>Before we move on beyond TCP APIs to a proper HTTP framework, let&#39;s just make a
version of our async program that can actually service multiple clients in
parallel, just for comparison with the sync version.</p>
<pre><p>Rust code</p><p><code><i>// omitted: everything else (which stays the same)</i>

async <i>fn</i> <i>run_server</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>info</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>
    <i>let</i> listener = <i>TcpListener</i><i>::</i><i>bind</i><i>(</i>addr<i>)</i><i>.</i>await?<i>;</i>
    <i>loop</i> {
        <i>let</i> <i>(</i>stream, addr<i>)</i> = listener<i>.</i><i>accept</i><i>(</i><i>)</i><i>.</i><i>instrument</i><i>(</i><i>info_span</i><i>!</i><i>(</i><i>&#34;accept&#34;</i><i>)</i><i>)</i><i>.</i>await?<i>;</i>
        tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
            <i>if</i> <i>let</i> Err<i>(</i>err<i>)</i> = <i>handle_connection</i><i>(</i>stream, addr<i>)</i><i>.</i>await {
                <i>error</i><i>!</i><i>(</i>%err, <i>&#34;Error handling connection&#34;</i> <i>)</i><i>;</i>
            }
        }<i>)</i><i>;</i>
    }
}
</code></p></pre>
<p>It&#39;s very similar, except it uses <code>tokio::spawn</code> rather than
<code>std::thread::spawn</code>. Just like we can move resources (a <code>TcpStream</code> and a
<code>SocketAddr</code>) into threads, we can move then into futures (which are then
spawned as tasks on a runtime, so that they get polled in the background).</p>
<p>I couldn&#39;t resist running <a href="https://lib.rs/crates/oha">oha</a> against a release
build of this program: the numbers are completely meaningless and should be
ignored, but look at the nice TUI (text user interface):</p>

            <picture>
                <source type="image/avif" srcset="/content/articles/request-coalescing-in-async-rust/assets/oha-tcp.e2afa283c29f2f65.avif"/>
                <source type="image/webp" srcset="/content/articles/request-coalescing-in-async-rust/assets/oha-tcp.85fec1638153349b.webp"/>
                <img loading="lazy" src="https://fasterthanli.me/content/articles/request-coalescing-in-async-rust/assets/oha-tcp.bcb0150a81a50afb.jpg" title="oha&#39;s text user interface, showing a progress bar on top, several panels showing how many requests have been sent, the slowest/fastest/average time, how much data was transferred, the number of open files, etc. There&#39;s also histograms showing how many requests it managed to send each second." ,="" alt="oha&#39;s text user interface, showing a progress bar on top, several panels showing how many requests have been sent, the slowest/fastest/average time, how much data was transferred, the number of open files, etc. There&#39;s also histograms showing how many requests it managed to send each second."/>
            </picture>
            
<p>oha&#39;s final report (again, just for show, this is all very unscientific and zero
effort went into making this fair or meaningful) reads:</p>
<pre><p>Shell session</p><p><code>$ oha -z 10s http://localhost:3779
Summary:
  Success rate: 1.0000
  Total:        10.0003 secs
  Slowest:      0.0135 secs
  Fastest:      0.0002 secs
  Average:      0.0011 secs
  Requests/sec: 46809.3602

  Total data:   8.48 MiB
  Size/request: 19 B
  Size/sec:     868.53 KiB

Response time histogram:
  0.000 [201]    |
  0.001 [1204]   |
  0.001 [277131] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.001 [167022] |■■■■■■■■■■■■■■■■■■■
  0.001 [12561]  |■
  0.002 [3754]   |
  0.002 [1728]   |
  0.002 [1146]   |
  0.002 [927]    |
  0.003 [333]    |
  0.003 [2101]   |

Latency distribution:
  10% in 0.0009 secs
  25% in 0.0010 secs
  50% in 0.0010 secs
  75% in 0.0011 secs
  90% in 0.0012 secs
  95% in 0.0013 secs
  99% in 0.0021 secs

Details (average, fastest, slowest):
  DNS+dialup:   0.0001 secs, 0.0000 secs, 0.0114 secs
  DNS-lookup:   0.0000 secs, 0.0000 secs, 0.0059 secs

Status code distribution:
  [200] 468108 responses
</code></p></pre>
<h2>And now, a proper http framework</h2>
<p>So we&#39;ve had our fun, implementing HTTP by hand - a tiny tiny fraction of it,
incorrectly, with terrible buffering hygiene, etc. It allowed us to compare
synchronous and asynchronous code, talk about why async code is hard to debug
in general, and discover the tracing ecosystem.</p>
<p>All of this comes in handy... right now!</p>
<p>Let&#39;s change our server application to use <a href="https://lib.rs/crates/axum">axum</a>
instead. It&#39;s built on top of <a href="https://lib.rs/crates/hyper">hyper</a>, which powers
<a href="https://fly.io/">fly.io</a>&#39;s edge network, among many other things.</p>
<pre><p>Shell session</p><p><code>$ cargo add axum
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding axum v0.4.8 to dependencies
</code></p></pre>
<p>We&#39;ll keep all our tracing setup intact, in a module:</p>
<pre><p>Rust code</p><p><code><i>// in `src/tracing_stuff.rs`</i>

<i>use</i> std<i>::</i>error<i>::</i>Error<i>;</i>
<i>use</i> tracing_subscriber<i>::</i>{layer<i>::</i>SubscriberExt, util<i>::</i>SubscriberInitExt, EnvFilter, Registry}<i>;</i>
<i>use</i> tracing_tree<i>::</i>HierarchicalLayer<i>;</i>

<i>pub</i><i>(</i>crate<i>)</i> <i>fn</i> <i>setup</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> tracer =
        opentelemetry_jaeger<i>::</i><i>new_pipeline</i><i>(</i><i>)</i><i>.</i><i>install_batch</i><i>(</i>opentelemetry<i>::</i>runtime<i>::</i>Tokio<i>)</i>?<i>;</i>
    <i>let</i> telemetry = tracing_opentelemetry<i>::</i><i>layer</i><i>(</i><i>)</i><i>.</i><i>with_tracer</i><i>(</i>tracer<i>)</i><i>;</i>
    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>.</i><i>with</i><i>(</i>telemetry<i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}

<i>pub</i><i>(</i>crate<i>)</i> <i>fn</i> <i>teardown</i><i>(</i><i>)</i> {
    opentelemetry<i>::</i>global<i>::</i><i>shutdown_tracer_provider</i><i>(</i><i>)</i><i>;</i>
}
</code></p></pre>
<p>...and write the simplest axum app that behaves exactly like our
cobbled-together HTTP server did:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>use</i> std<i>::</i>{error<i>::</i>Error, net<i>::</i>SocketAddr}<i>;</i>

<i>use</i> axum<i>::</i>{response<i>::</i>IntoResponse, routing<i>::</i>get, Router, Server}<i>;</i>
<i>use</i> tracing<i>::</i>info<i>;</i>

<i>mod</i> tracing_stuff<i>;</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    tracing_stuff<i>::</i><i>setup</i><i>(</i><i>)</i>?<i>;</i>
    <i>run_server</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
    tracing_stuff<i>::</i><i>teardown</i><i>(</i><i>)</i><i>;</i>
    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}

async <i>fn</i> <i>run_server</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>info</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>

    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i><i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i><i>;</i>
    <i>Server</i><i>::</i><i>bind</i><i>(</i><i>&amp;</i>addr<i>)</i><i>.</i><i>serve</i><i>(</i>app<i>.</i><i>into_make_service</i><i>(</i><i>)</i><i>)</i><i>.</i>await?<i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i><i>)</i> -&gt; <i>impl</i> <i>IntoResponse</i> {
    <i>&#34;Hello from plaque!\n&#34;</i>
}
</code></p></pre><div>

<p>Well, I like the looks of that.</p>
</div>
<div>

<p>Right??? I&#39;ve been waiting for something like <code>axum</code> forever.</p>
</div>
<p>It works fine in curl:</p>
<pre><p>Shell session</p><p><code>curl -v http://localhost:3779
*   Trying 127.0.0.1:3779...
* Connected to localhost (127.0.0.1) port 3779 (#0)
&gt; GET / HTTP/1.1
&gt; Host: localhost:3779
&gt; User-Agent: curl/7.79.1
&gt; Accept: */*
&gt; 
* Mark bundle as not supporting multiuse
&lt; HTTP/1.1 200 OK
&lt; content-type: text/plain; charset=utf-8
&lt; content-length: 19
&lt; date: Thu, 03 Mar 2022 20:44:39 GMT
&lt; 
Hello from plaque!
* Connection #0 to host localhost left intact
</code></p></pre>
<p>...and, you know, it actually sends some useful response headers.</p>
<p>Predictably, it also works fine in a proper web browser, and... I couldn&#39;t
resist running <a href="https://lib.rs/crates/oha">oha</a> on it as well:</p>
<pre><p>Shell session</p><p><code>(results shortened)

Response time histogram:
  0.000 [430]     |
  0.000 [3277]    |
  0.000 [233959]  |■■■■■■
  0.000 [1074275] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.000 [123655]  |■■■
  0.001 [7550]    |
  0.001 [1951]    |
  0.001 [1155]    |
  0.001 [519]     |
  0.001 [177]     |
  0.001 [7737]    |

Latency distribution:
  10% in 0.0003 secs
  25% in 0.0003 secs
  50% in 0.0003 secs
  75% in 0.0004 secs
  90% in 0.0004 secs
  95% in 0.0004 secs
  99% in 0.0005 secs
</code></p></pre><div>

<p>Wait... 4? Didn&#39;t our latency numbers end in 1 earlier?</p>
</div>
<div>

<p>Yeah. 0.001 vs 0.0004 seconds.</p>
</div>

<p>So, now that we&#39;ve reassured ourselves performance-wise, let&#39;s take a look at
tracing!</p>
<p>We still have <code>tracing-tree</code> set up and everything, and we&#39;ve instrumented the
<code>root</code> async function, but it doesn&#39;t have a lot of information:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=info cargo run --release
    Finished release [optimized] target(s) in 0.03s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
plaque::root{}

plaque::root{}

plaque::root{}

plaque::root{}

</code></p></pre>
<p>...because a lot of things are handled by <code>hyper</code> and <code>axum</code> now.</p>
<p>But what&#39;s cool is that <code>hyper</code> taps into the <code>tower</code> ecosystem, which is made
of composable layers, much like <code>tracing</code>!</p>
<p>So if we pull in <code>tower-http</code>, we can add some tracing easily:</p>
<pre><p>Shell session</p><p><code>$ cargo add tower
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tower v0.4.12 to dependencies

$ cargo add tower-http --features trace
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tower-http v0.2.3 to dependencies with features: [&#34;trace&#34;]
</code></p></pre><pre><p>Rust code</p><p><code>async <i>fn</i> <i>run_server</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> addr: <i>SocketAddr</i> = <i>&#34;0.0.0.0:3779&#34;</i><i>.</i><i>parse</i><i>(</i><i>)</i>?<i>;</i>
    <i>info</i><i>!</i><i>(</i><i>&#34;Listening on http://{}&#34;</i>, addr<i>)</i><i>;</i>

    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i><i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i><i>.</i><i>layer</i><i>(</i>
        <i>ServiceBuilder</i><i>::</i><i>new</i><i>(</i><i>)</i>
            <i>// 👇 new!</i>
            <i>.</i><i>layer</i><i>(</i><i>TraceLayer</i><i>::</i><i>new_for_http</i><i>(</i><i>)</i><i>)</i>
            <i>.</i><i>into_inner</i><i>(</i><i>)</i>,
    <i>)</i><i>;</i>
    <i>Server</i><i>::</i><i>bind</i><i>(</i><i>&amp;</i>addr<i>)</i><i>.</i><i>serve</i><i>(</i>app<i>.</i><i>into_make_service</i><i>(</i><i>)</i><i>)</i><i>.</i>await?<i>;</i>

    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre>
<p>There&#39;s a wealth of middleware to pick from, that&#39;s just one of them!</p>
<p>And now, our server emits events whenever it serves a request:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug cargo run --release
    Finished release [optimized] target(s) in 0.03s
     Running `target/release/plaque`
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  0ms DEBUG tower_http::trace::on_response finished processing request, latency=0 ms, status=200

tower_http::trace::make_span::request{method=GET, uri=/uh-oh, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  0ms DEBUG tower_http::trace::on_response finished processing request, latency=0 ms, status=404
</code></p></pre>
<p>...no matter whether it serves it over plaintext HTTP/1, HTTPS, HTTP/2, etc.</p>
<p>We&#39;d probably want to roll our own layer if we needed something a little more
detailed: and we&#39;d have all the required integration points to do so.</p>
<h2>A useful http service</h2>
<p>And now, to move beyond the hello world.</p>
<p>Let&#39;s make an HTTP request handler that returns the ID of the latest video
on <a href="https://www.youtube.com/c/fasterthanlime">my YouTube channel</a>.</p>
<p>I picked this example because:</p>
<ul>
<li>It can be done using the public YouTube API: no API key required!</li>
<li>I&#39;m somewhat scared of being rate-limited when doing this</li>
</ul>
<p>Because of potential rate-limiting going on, that&#39;s one of the first things I
deduplicated/memoized/single-flighted/cached in my site (not counting static
content).</p>
<p>So! We&#39;ll be using the <a href="https://www.youtube.com/feeds/videos.xml">https://www.youtube.com/feeds/videos.xml</a> API endpoint.
It accepts a <code>channel_id</code> query parameter, and my channel ID is
<code>UCs4fQRyl1TJvoeOdekW6lYA</code>.</p>
<p>We&#39;ll make all of that happen in a separate module named <code>youtube</code>:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>mod</i> youtube<i>;</i>
</code></p></pre>
<p>First we&#39;ll want to build the URL. And I don&#39;t feel like doing string
interpolation right now - I want only your strongest types, potion seller<a href="https://www.youtube.com/watch?v=R_FQU4KzN7A&amp;t=2s">.</a></p>
<pre><p>Shell session</p><p><code>$ cargo add url
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding url v2.2.2 to dependencie
</code></p></pre>
<p>Ok, let&#39;s build!</p>
<pre><p>Rust code</p><p><code><i>// in `src/youtube.rs`</i>

<i>use</i> std<i>::</i>error<i>::</i>Error<i>;</i>
<i>use</i> url<i>::</i>Url<i>;</i>

<i>const</i> YT_CHANNEL_ID: <i>&amp;</i><i>str</i> = <i>&#34;UCs4fQRyl1TJvoeOdekW6lYA&#34;</i><i>;</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
<i>pub</i><i>(</i>crate<i>)</i> async <i>fn</i> <i>fetch_video_id</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>String</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    <i>let</i> <i>mut</i> api_url = <i>Url</i><i>::</i><i>parse</i><i>(</i><i>&#34;https://www.youtube.com/feeds/videos.xml&#34;</i><i>)</i>?<i>;</i>
    {
        <i>let</i> <i>mut</i> q = api_url<i>.</i><i>query_pairs_mut</i><i>(</i><i>)</i><i>;</i>
        q<i>.</i><i>append_pair</i><i>(</i><i>&#34;channel_id&#34;</i>, YT_CHANNEL_ID<i>)</i><i>;</i>
    }

    <i>todo</i><i>!</i><i>(</i><i>&#34;fetch from {api_url}&#34;</i><i>)</i><i>;</i>
}
</code></p></pre>
<p>And we&#39;ll change our <code>root</code> endpoint to call it...</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i><i>)</i> -&gt; <i>impl</i> <i>IntoResponse</i> {
    youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>)</i><i>.</i>await<i>.</i><i>unwrap</i><i>(</i><i>)</i>
}
</code></p></pre><pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug cargo run --release
    Finished release [optimized] target(s) in 0.03s
     Running `target/release/plaque`
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
thread &#39;main&#39; panicked at &#39;not yet implemented: fetch from https://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA&#39;, src/youtube.rs:13:5
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
</code></p></pre>
<p>Okay! URL looks good (I checked by opening it in a browser).</p>
<p>I&#39;m not super happy with the error handling though - making the whole task panic
seems a little heavy-handed.</p>
<p>It also feels like the perfect time to showcase
<a href="https://lib.rs/crates/color-eyre">color-eyre</a>.</p>
<pre><p>Shell session</p><p><code>$ cargo add color-eyre 
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding color-eyre v0.6.1 to dependencies
</code></p></pre>
<p>To get spantrace capture to work, we&#39;ll also need to install a
<a href="https://lib.rs/crates/tracing-error">tracing-error</a> layer:</p>
<pre><p>Shell session</p><p><code>$ cargo add tracing-error
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tracing-error v0.2.0 to dependencies
</code></p></pre><pre><p>Rust code</p><p><code><i>// in `src/tracing_stuff.rs`</i>

<i>use</i> tracing_error<i>::</i>ErrorLayer<i>;</i>

<i>// in the `setup` function,</i>
<i>// deep in the `.with` chain:</i>

    <i>Registry</i><i>::</i><i>default</i><i>(</i><i>)</i>
        <i>.</i><i>with</i><i>(</i><i>EnvFilter</i><i>::</i><i>from_default_env</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>
            <i>HierarchicalLayer</i><i>::</i><i>new</i><i>(</i><i>2</i><i>)</i>
                <i>.</i><i>with_targets</i><i>(</i><i>true</i><i>)</i>
                <i>.</i><i>with_bracketed_fields</i><i>(</i><i>true</i><i>)</i>,
        <i>)</i>
        <i>// 👇 there!</i>
        <i>.</i><i>with</i><i>(</i><i>ErrorLayer</i><i>::</i><i>default</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>with</i><i>(</i>telemetry<i>)</i>
        <i>.</i><i>init</i><i>(</i><i>)</i><i>;</i>
</code></p></pre>
<p>And now, we install <code>color_eyre</code> as the default panic handler, in <code>main</code>:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>// (omitted: everything else)</i>

<i>#<i>[</i>tokio<i>::</i>main<i>(</i>flavor = <i>&#34;current_thread&#34;</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>main</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>(</i><i>)</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i> {
    color_eyre<i>::</i><i>install</i><i>(</i><i>)</i>?<i>;</i>

    tracing_stuff<i>::</i><i>setup</i><i>(</i><i>)</i>?<i>;</i>
    <i>run_server</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
    tracing_stuff<i>::</i><i>teardown</i><i>(</i><i>)</i><i>;</i>
    Ok<i>(</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre><pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug,info cargo run --release
    Finished release [optimized] target(s) in 0.03s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
The application panicked (crashed).
Message:  not yet implemented: fetch from https://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA
Location: src/youtube.rs:14

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::youtube::fetch_video_id
      at src/youtube.rs:6
   1: plaque::root
      at src/main.rs:35
   2: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>Well, isn&#39;t that delightful!</p>
<p>In case you&#39;re <em>not</em> following along at home, let&#39;s see what the thread
backtrace looks like. Because I&#39;m running a release build here (I&#39;m passing
<code>--release</code> to cargo), let&#39;s make sure just enough debug information is included
so that we get source file / line numbers info in our stacktrace:</p>
<pre><p>TOML markup</p><p><code><i># in `Cargo.toml`</i>

<i># omitted: everything else ([package], [dependencies], etc.)</i>

<i>[</i><i>profile</i><i>.</i><i>release</i><i>]</i>
<i># 2/true is too much, 0 is not enough, 1 is just right for backtraces</i>
<i>debug</i> <i>=</i> 1
</code></p></pre>
<p>And now:</p>
<pre><p>Shell session</p><p><code>$ RUST_BACKTRACE=1 cargo run --release
Finished release [optimized + debuginfo] target(s) in 0.03s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
The application panicked (crashed).
Message:  not yet implemented: fetch from https://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA
Location: src/youtube.rs:14

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::youtube::fetch_video_id
      at src/youtube.rs:6
   1: plaque::root
      at src/main.rs:35
   2: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ BACKTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
                                ⋮ 7 frames hidden ⋮                               
   8: plaque::youtube::fetch_video_id::{{closure}}::{{closure}}::hab8f271bc61b75d4
      at /home/amos/bearcove/plaque/src/youtube.rs:14
   9: &lt;core::future::from_generator::GenFuture&lt;T&gt; as core::future::future::Future&gt;::poll::h6e0e23eb00ce4823
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/mod.rs:84
  10: &lt;tracing::instrument::Instrumented&lt;T&gt; as core::future::future::Future&gt;::poll::h0be1db02d1392ae8
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tracing-0.1.31/src/instrument.rs:272
  11: plaque::youtube::fetch_video_id::{{closure}}::hf6d51b72f3f19269
      at /home/amos/bearcove/plaque/src/youtube.rs:6
  12: &lt;core::future::from_generator::GenFuture&lt;T&gt; as core::future::future::Future&gt;::poll::h6cb6341d279017f6
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/mod.rs:84
  13: plaque::root::{{closure}}::{{closure}}::h630b187fb3cddb8a
      at /home/amos/bearcove/plaque/src/main.rs:37
  14: &lt;core::future::from_generator::GenFuture&lt;T&gt; as core::future::future::Future&gt;::poll::h6ddf6f485ebbd74f
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/mod.rs:84
  15: &lt;tracing::instrument::Instrumented&lt;T&gt; as core::future::future::Future&gt;::poll::h0b761b74342ffab4
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tracing-0.1.31/src/instrument.rs:272
  16: plaque::root::{{closure}}::hf30871c07e78c6fd
      at /home/amos/bearcove/plaque/src/main.rs:35
  17: &lt;core::future::from_generator::GenFuture&lt;T&gt; as core::future::future::Future&gt;::poll::hc5fc9ae1f1172d28
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/mod.rs:84
  18: &lt;F as axum::handler::Handler&lt;(),B&gt;&gt;::call::{{closure}}::h4b190ad50776c570
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/handler/mod.rs:274
  19: &lt;core::future::from_generator::GenFuture&lt;T&gt; as core::future::future::Future&gt;::poll::hd4c31eb92d321c5c
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/mod.rs:84
  20: &lt;core::pin::Pin&lt;P&gt; as core::future::future::Future&gt;::poll::h194730287d082d5b
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/future.rs:123
  21: &lt;futures_util::future::future::map::Map&lt;Fut,F&gt; as core::future::future::Future&gt;::poll::hda5093c773501670
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.21/src/future/future/map.rs:55
  22: &lt;futures_util::future::future::Map&lt;Fut,F&gt; as core::future::future::Future&gt;::poll::h35098e19f97e2dfa
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.21/src/lib.rs:91
  23: &lt;axum::handler::future::IntoServiceFuture as core::future::future::Future&gt;::poll::h83968b7e939194bc
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/macros.rs:42
  24: &lt;core::pin::Pin&lt;P&gt; as core::future::future::Future&gt;::poll::h1d3bccc0ccc1322c
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/future.rs:123
  25: &lt;tower::util::oneshot::Oneshot&lt;S,Req&gt; as core::future::future::Future&gt;::poll::ha7c9f6e93564c479
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-0.4.12/src/util/oneshot.rs:97
  26: &lt;axum::routing::route::RouteFuture&lt;B,E&gt; as core::future::future::Future&gt;::poll::he8321bd83c70eaf0
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/routing/route.rs:108
  27: &lt;tower_http::trace::future::ResponseFuture&lt;Fut,C,OnResponseT,OnBodyChunkT,OnEosT,OnFailureT&gt; as core::future::future::Future&gt;::poll::h146cd7d5d1401fa8
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/future.rs:52
  28: &lt;tower_http::map_response_body::ResponseFuture&lt;Fut,F&gt; as core::future::future::Future&gt;::poll::h052add6c17b2403c
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/map_response_body.rs:204
  29: &lt;tower_http::map_response_body::ResponseFuture&lt;Fut,F&gt; as core::future::future::Future&gt;::poll::hf0cd011d85831398
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/map_response_body.rs:204
  30: &lt;core::pin::Pin&lt;P&gt; as core::future::future::Future&gt;::poll::h1d3bccc0ccc1322c
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/future/future.rs:123
  31: &lt;tower::util::oneshot::Oneshot&lt;S,Req&gt; as core::future::future::Future&gt;::poll::ha7c9f6e93564c479
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-0.4.12/src/util/oneshot.rs:97
  32: &lt;axum::routing::route::RouteFuture&lt;B,E&gt; as core::future::future::Future&gt;::poll::he8321bd83c70eaf0
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/routing/route.rs:108
  33: &lt;futures_util::future::either::Either&lt;A,B&gt; as core::future::future::Future&gt;::poll::h5c1aa3799a3a97a8
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.21/src/future/either.rs:89
  34: &lt;axum::routing::future::RouterFuture&lt;B&gt; as core::future::future::Future&gt;::poll::ha3aaa039e3a1cd54
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/macros.rs:42
  35: &lt;hyper::proto::h1::dispatch::Server&lt;S,hyper::body::body::Body&gt; as hyper::proto::h1::dispatch::Dispatch&gt;::poll_msg::hda35a226e6f560e0
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:491
  36: hyper::proto::h1::dispatch::Dispatcher&lt;D,Bs,I,T&gt;::poll_write::hae4fc7f534b34390
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:297
  37: hyper::proto::h1::dispatch::Dispatcher&lt;D,Bs,I,T&gt;::poll_loop::h45f8006b014e2845
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:161
  38: hyper::proto::h1::dispatch::Dispatcher&lt;D,Bs,I,T&gt;::poll_inner::h7b0e9f76871f96a5
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:137
  39: hyper::proto::h1::dispatch::Dispatcher&lt;D,Bs,I,T&gt;::poll_catch::h955db4e72e7b9ab8
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:120
  40: &lt;hyper::proto::h1::dispatch::Dispatcher&lt;D,Bs,I,T&gt; as core::future::future::Future&gt;::poll::h126968541a4386be
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/proto/h1/dispatch.rs:424
  41: &lt;hyper::server::conn::ProtoServer&lt;T,B,S,E&gt; as core::future::future::Future&gt;::poll::h0d8b641b8484c8a6
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/server/conn.rs:1142
  42: &lt;hyper::server::conn::upgrades::UpgradeableConnection&lt;I,S,E&gt; as core::future::future::Future&gt;::poll::h2a9fb2b64056d7c4
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/server/conn.rs:1346
  43: &lt;hyper::server::conn::spawn_all::NewSvcTask&lt;I,N,S,E,W&gt; as core::future::future::Future&gt;::poll::he87cab1ebceecb94
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.17/src/server/conn.rs:1283
  44: tokio::runtime::task::core::CoreStage&lt;T&gt;::poll::{{closure}}::hf838d353f1f39232
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/core.rs:161
  45: tokio::loom::std::unsafe_cell::UnsafeCell&lt;T&gt;::with_mut::h5c1a5f3e29a5a536
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/loom/std/unsafe_cell.rs:14
  46: tokio::runtime::task::core::CoreStage&lt;T&gt;::poll::h410a4d2378fe1425
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/core.rs:151
  47: tokio::runtime::task::harness::poll_future::{{closure}}::h7e66f49a70a48bf4
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/harness.rs:467
  48: &lt;core::panic::unwind_safe::AssertUnwindSafe&lt;F&gt; as core::ops::function::FnOnce&lt;()&gt;&gt;::call_once::ha4ede35dc8c09d23
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/panic/unwind_safe.rs:271
  49: std::panicking::try::do_call::h9bf1526b1481658f
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/std/src/panicking.rs:406
  50: std::panicking::try::hce34bb9cb3d6f762
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/std/src/panicking.rs:370
  51: std::panic::catch_unwind::hff0de339c046ca05
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/std/src/panic.rs:133
  52: tokio::runtime::task::harness::poll_future::h22a0571f341a8fe4
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/harness.rs:455
  53: tokio::runtime::task::harness::Harness&lt;T,S&gt;::poll_inner::he829968450070f3b
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/harness.rs:103
  54: tokio::runtime::task::harness::Harness&lt;T,S&gt;::poll::hf1ae4d7dc3894514
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/harness.rs:57
  55: tokio::runtime::task::LocalNotified&lt;S&gt;::run::h23a4057f42b678c2
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/task/mod.rs:347
  56: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::{{closure}}::h6bdc27d064499238
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:532
  57: tokio::coop::with_budget::{{closure}}::he21073af0a9fe50c
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:102
  58: std::thread::local::LocalKey&lt;T&gt;::try_with::ha3868a0b480c262b
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/std/src/thread/local.rs:412
  59: std::thread::local::LocalKey&lt;T&gt;::with::h3b38379aec814a78
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/std/src/thread/local.rs:388
  60: tokio::coop::with_budget::he33e37f59c7ff8b0
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:95
  61: tokio::coop::budget::h27f9e6eaf2d6cfe1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:72
  62: tokio::runtime::basic_scheduler::Context::run_task::{{closure}}::h36036a49569cf8fc
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:291
  63: tokio::runtime::basic_scheduler::Context::enter::h10fac43713f24837
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:356
  64: tokio::runtime::basic_scheduler::Context::run_task::hd09c9d487557d6ef
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:291
  65: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::he67b4b3fdfbc4ace
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:531
  66: tokio::runtime::basic_scheduler::CoreGuard::enter::{{closure}}::hde9b67ddaf06cae1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555
  67: tokio::macros::scoped_tls::ScopedKey&lt;T&gt;::set::hed655ba933b2ec0f
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/macros/scoped_tls.rs:61
  68: tokio::runtime::basic_scheduler::CoreGuard::enter::hb6b4338769eb9cba
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555
  69: tokio::runtime::basic_scheduler::CoreGuard::block_on::h8510ffa7213dec9a
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:488
  70: tokio::runtime::basic_scheduler::BasicScheduler::block_on::hc0e1922df1329249
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:168
  71: tokio::runtime::Runtime::block_on::h20934397d494fcbb
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/mod.rs:475
  72: plaque::main::h955bdc1216baefce
      at /home/amos/bearcove/plaque/src/main.rs:18
  73: core::ops::function::FnOnce::call_once::he99aaacc5fbd0766
      at /rustc/9d1b2106e23b1abd32fce1f17267604a5102f57a/library/core/src/ops/function.rs:227
                                ⋮ 15 frames hidden ⋮                              

Run with COLORBT_SHOW_HIDDEN=1 environment variable to disable frame filtering.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>It&#39;s a pretty noisy backtrace. The async working group is <a href="https://rust-lang.github.io/wg-async/vision/roadmap/polish/stacktraces.html">also thinking about
that</a>.</p>
<p>But we have <em>everything</em> here. Well, minus the hidden frames. We got
<code>Runtime::block_on</code> we used to call ourselves when we weren&#39;t using
<code>#[tokio::main]</code> (proof that it&#39;s not so magic!).</p>
<p>We got <code>tokio</code>&#39;s coop system that <a href="https://tokio.rs/blog/2020-04-preemption">improves tail
latencies</a>, we got <code>hyper</code>&#39;s http1
server, we got <code>axum</code>&#39;s router, some more <code>axum</code> handlers, and at the very top,
we have our very own <code>fetch_video_id</code>.</p>
<p>Okay, so now we have pretty good error reporting... but it still panics!</p>
<p>Well, let&#39;s do two things: one, let&#39;s make <code>fetch_video_id</code> return an error
not of type <code>Box&lt;dyn Error&gt;</code>, but of type <code>color_eyre::Report</code>:</p>
<pre><p>Rust code</p><p><code><i>// in `src/youtube.rs`</i>

<i>use</i> color_eyre<i>::</i>{eyre<i>::</i>eyre, Report}<i>;</i>
<i>use</i> url<i>::</i>Url<i>;</i>

<i>const</i> YT_CHANNEL_ID: <i>&amp;</i><i>str</i> = <i>&#34;UCs4fQRyl1TJvoeOdekW6lYA&#34;</i><i>;</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
<i>pub</i><i>(</i>crate<i>)</i> async <i>fn</i> <i>fetch_video_id</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>String</i>, <i>Report</i><i>&gt;</i> {
    <i>let</i> <i>mut</i> api_url = <i>Url</i><i>::</i><i>parse</i><i>(</i><i>&#34;https://www.youtube.com/feeds/videos.xml&#34;</i><i>)</i>?<i>;</i>
    {
        <i>let</i> <i>mut</i> q = api_url<i>.</i><i>query_pairs_mut</i><i>(</i><i>)</i><i>;</i>
        q<i>.</i><i>append_pair</i><i>(</i><i>&#34;channel_id&#34;</i>, YT_CHANNEL_ID<i>)</i><i>;</i>
    }

    Err<i>(</i><i>eyre</i><i>!</i><i>(</i><i>&#34;TODO: fetch from {api_url}&#34;</i><i>)</i><i>)</i>
}
</code></p></pre>
<p>And two, let&#39;s... no, wait, let&#39;s not do anything and see what kind of spantrace
we get now:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug,info cargo run --release
   Compiling plaque v0.1.0 (/home/amos/bearcove/plaque)
    Finished release [optimized + debuginfo] target(s) in 3.40s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
The application panicked (crashed).
Message:  called `Result::unwrap()` on an `Err` value: 
   0: TODO: fetch from https://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA

Location:
   src/youtube.rs:14

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::youtube::fetch_video_id
      at src/youtube.rs:6
   1: plaque::root
      at src/main.rs:35
   2: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
Location: src/main.rs:37

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::root
      at src/main.rs:35
   1: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>Okay! Nice! It shows us where the original error happened (first span trace),
and then where we unwrapped it (which just happens to be the same line).</p>
<p>That said, we&#39;re writing a web service. We don&#39;t want to panic, we probably want
to return an <a href="https://httpstatuses.com/">HTTP 500</a> instead.</p>
<p>Is there a way to do that in axum? Yes there is! And it&#39;s actually quite
interesting. Axum&#39;s <code>IntoReponse</code> trait is implemented for... <code>Result&lt;T, E&gt; where T: IntoResponse, E: IntoResponse</code>.</p>
<p>Which means we can write code like this:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>(</i><i>StatusCode</i>, <i>String</i><i>)</i><i>&gt;</i> {
    <i>let</i> res = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>)</i><i>.</i>await<i>.</i><i>map_err</i><i>(</i>|err| {
        <i>(</i>
            <i>StatusCode</i><i>::</i>INTERNAL_SERVER_ERROR,
            <i>format</i><i>!</i><i>(</i><i>&#34;youtube error: {err}&#34;</i><i>)</i>,
        <i>)</i>
    }<i>)</i>?<i>;</i>
    Ok<i>(</i>res<i>)</i>
}
</code></p></pre>
<p>Here, both the <code>Ok</code> and <code>Err</code> variant of <code>Result</code> contain something that can be
turned into a response! The <code>Ok</code> variant is just a string (which defaults to an
HTTP 200 status code), and the <code>Err</code> variant is straight up a <code>(StatusCode, String)</code>, and we&#39;re specifying HTTP 500 ourselves.</p>
<p>I don&#39;t love this though, I think we can improve the ergonomics a bit... because
unless I specifically match on the result of some fallible function or future,
I&#39;d want it to default to HTTP 500 (Internal Server Error) - and I&#39;d like the
<code>?</code> sigil to mean that.</p>
<p>Luckily, <code>axum</code>&#39;s design is flexible enough for that!</p>
<p>All we gotta do is make a little newtype around <code>eyre::Report</code> and then
implement <code>IntoResponse</code> for it!</p>
<pre><p>Rust code</p><p><code><i>struct</i> <i>ReportError</i><i>(</i><i>Report</i><i>)</i><i>;</i>

<i>impl</i> <i>From</i><i>&lt;</i><i>Report</i><i>&gt;</i> <i>for</i> <i>ReportError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>err</i>: <i>Report</i><i>)</i> -&gt; <i>Self</i> {
        ReportError<i>(</i>err<i>)</i>
    }
}

<i>impl</i> <i>IntoResponse</i> <i>for</i> <i>ReportError</i> {
    <i>fn</i> <i>into_response</i><i>(</i><i>self</i><i>)</i> -&gt; <i>Response</i> {
        <i>// {:?} shows the backtrace / spantrace, see </i>
        <i>// https://docs.rs/eyre/0.6.7/eyre/struct.Report.html#display-representations</i>
        <i>(</i>
            <i>StatusCode</i><i>::</i>INTERNAL_SERVER_ERROR,
            <i>format</i><i>!</i><i>(</i><i>&#34;Internal server error: {:?}&#34;</i>, <i>self</i>.<i>0</i><i>)</i>,
        <i>)</i>
            <i>.</i><i>into_response</i><i>(</i><i>)</i>
    }
}
</code></p></pre>
<p>And now, we can change the signature of <code>root</code>, and... tada!</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>let</i> res = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>)</i><i>.</i>await?<i>;</i>
    Ok<i>(</i>res<i>)</i>
}
</code></p></pre>
<p>And now, our handler doesn&#39;t panic anymore - instead, the failure is logged:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug,info cargo run --release
   Compiling plaque v0.1.0 (/home/amos/bearcove/plaque)
    Finished release [optimized + debuginfo] target(s) in 3.53s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
  
  
  0ms DEBUG tower_http::trace::on_response finished processing request, latency=0 ms, status=500
  0ms ERROR tower_http::trace::on_failure response failed, classification=Status code: 500 Internal Server Error, latency=0 ms 
</code></p></pre>
<p>And the spantrace is sent within the HTTP response:</p>
<pre><p>Shell session</p><p><code>$ curl http://localhost:3779/
Internal server error: 
   0: TODO: fetch from https://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA

Location:
   src/youtube.rs:14

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::youtube::fetch_video_id
      at src/youtube.rs:6
   1: plaque::root
      at src/main.rs:41
   2: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>In a real-world application, we&#39;d probably want to disable that for production.
But that&#39;s left as an exercise to the (very patient) reader.</p>
<p>Okay, well. Time to hit that YouTube API.</p>
<p>I normally prefer to use <a href="https://lib.rs/crates/hyper">hyper</a> directly instead
of <a href="https://lib.rs/crates/reqwest">reqwest</a>, but let&#39;s use the latter to keep it
short.</p>
<pre><p>Shell session</p><p><code>$ cargo add reqwest --no-default-features --features rustls-tls
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding reqwest v0.11.9 to dependencies with features: [&#34;rustls-tls&#34;]
</code></p></pre>
<p>The API response happens to be an Atom feed:</p>
<pre><p>XML</p><p><code><i>&lt;</i>?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?<i>&gt;</i>
<i>&lt;</i><i>feed</i> <i>xmlns:yt</i>=&#34;<i>http://www.youtube.com/xml/schemas/2015</i>&#34; <i>xmlns:media</i>=&#34;<i>http://search.yahoo.com/mrss/</i>&#34; <i>xmlns</i>=&#34;<i>http://www.w3.org/2005/Atom</i>&#34;<i>&gt;</i>
 <i>&lt;</i><i>link</i> <i>rel</i>=&#34;<i>self</i>&#34; <i>href</i>=&#34;<i>http://www.youtube.com/feeds/videos.xml?channel_id=UCs4fQRyl1TJvoeOdekW6lYA</i>&#34;/&gt;
 <i>&lt;</i><i>id</i><i>&gt;</i>yt:channel:UCs4fQRyl1TJvoeOdekW6lYA<i>&lt;/</i><i>id</i><i>&gt;</i>
 <i>&lt;</i><i>yt:channelId</i><i>&gt;</i>UCs4fQRyl1TJvoeOdekW6lYA<i>&lt;/</i><i>yt:channelId</i><i>&gt;</i>
 <i>&lt;</i><i>title</i><i>&gt;</i>fasterthanlime<i>&lt;/</i><i>title</i><i>&gt;</i>
 <i>&lt;</i><i>link</i> <i>rel</i>=&#34;<i>alternate</i>&#34; <i>href</i>=&#34;<i>https://www.youtube.com/channel/UCs4fQRyl1TJvoeOdekW6lYA</i>&#34;/&gt;
 <i>&lt;</i><i>author</i><i>&gt;</i>
  <i>&lt;</i><i>name</i><i>&gt;</i>fasterthanlime<i>&lt;/</i><i>name</i><i>&gt;</i>
  <i>&lt;</i><i>uri</i><i>&gt;</i>https://www.youtube.com/channel/UCs4fQRyl1TJvoeOdekW6lYA<i>&lt;/</i><i>uri</i><i>&gt;</i>
 <i>&lt;/</i><i>author</i><i>&gt;</i>
 <i>&lt;</i><i>published</i><i>&gt;</i>2019-10-16T09:57:58+00:00<i>&lt;/</i><i>published</i><i>&gt;</i>
 <i>&lt;!-- omitted: rest of the feed --&gt;</i>
<i>&lt;/</i><i>feed</i><i>&gt;</i>
</code></p></pre>
<p>...so let&#39;s bring in another crate to parse it:</p>
<pre><p>Shell session</p><p><code>$ cargo add feed-rs
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding feed-rs v1.0.0 to dependencies
</code></p></pre><div>

<p>Ooh, version 1.0.0, take a sip!</p>
</div>
<p>And just because I feel like being silly, let&#39;s try out
<a href="https://lib.rs/crates/tap">tap</a>:</p>
<pre><p>Shell session</p><p><code>$ cargo add tap
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding tap v1.0.1 to dependencies
</code></p></pre>
<p>And then... weeeeeeeeeeeEEEEEEeeeee:</p>
<pre><p>Rust code</p><p><code><i>// in `src/youtube.rs`</i>

<i>use</i> color_eyre<i>::</i>{eyre<i>::</i>eyre, Report}<i>;</i>
<i>use</i> reqwest<i>::</i>Client<i>;</i>
<i>use</i> tap<i>::</i>Pipe<i>;</i>
<i>use</i> url<i>::</i>Url<i>;</i>

<i>const</i> YT_CHANNEL_ID: <i>&amp;</i><i>str</i> = <i>&#34;UCs4fQRyl1TJvoeOdekW6lYA&#34;</i><i>;</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
<i>pub</i><i>(</i>crate<i>)</i> async <i>fn</i> <i>fetch_video_id</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>String</i>, <i>Report</i><i>&gt;</i> {
    Ok<i>(</i><i>Client</i><i>::</i><i>new</i><i>(</i><i>)</i>
        <i>.</i><i>get</i><i>(</i>{
            <i>let</i> <i>mut</i> url = <i>Url</i><i>::</i><i>parse</i><i>(</i><i>&#34;https://www.youtube.com/feeds/videos.xml&#34;</i><i>)</i>?<i>;</i>
            url<i>.</i><i>query_pairs_mut</i><i>(</i><i>)</i>
                <i>.</i><i>append_pair</i><i>(</i><i>&#34;channel_id&#34;</i>, YT_CHANNEL_ID<i>)</i><i>;</i>
            url
        }<i>)</i>
        <i>// make our mark on the world</i>
        <i>.</i><i>header</i><i>(</i><i>&#34;user-agent&#34;</i>, <i>&#34;cool/bear&#34;</i><i>)</i>
        <i>.</i><i>send</i><i>(</i><i>)</i>
        <i>.</i>await? <i>// this will cover connect errors</i>
        <i>.</i><i>error_for_status</i><i>(</i><i>)</i>? <i>// this will cover HTTP errors</i>
        <i>.</i><i>bytes</i><i>(</i><i>)</i>
        <i>.</i>await? <i>// errors while streaming the response body?</i>
        <i>.</i><i>pipe</i><i>(</i>|bytes| feed_rs<i>::</i>parser<i>::</i><i>parse</i><i>(</i><i>&amp;</i>bytes<i>[</i>..<i>]</i><i>)</i><i>)</i>? <i>// parse the feed</i>
        <i>.</i><i>pipe_ref</i><i>(</i>|feed| feed<i>.</i><i>entries</i><i>.</i><i>get</i><i>(</i><i>0</i><i>)</i><i>)</i>
        <i>.</i><i>ok_or</i><i>(</i><i>eyre</i><i>!</i><i>(</i><i>&#34;no entries in video feed&#34;</i><i>)</i><i>)</i>?
        <i>.</i><i>pipe_ref</i><i>(</i>|entry| entry<i>.</i><i>id</i><i>.</i><i>strip_prefix</i><i>(</i><i>&#34;yt:video:&#34;</i><i>)</i><i>)</i>
        <i>.</i><i>ok_or</i><i>(</i><i>eyre</i><i>!</i><i>(</i><i>&#34;first video feed item wasn&#39;t a video&#34;</i><i>)</i><i>)</i>?
        <i>.</i><i>to_string</i><i>(</i><i>)</i><i>)</i>
}
</code></p></pre><div>

<p>Whew, that&#39;s... that&#39;s a lot.</p>
</div>
<div>

<p>It&#39;s allllll suffix baby. Who needs statements?</p>
</div>
<p>Well? Let&#39;s try it out!</p>
<p>Oh, wait, no! Let&#39;s return it as JSON! Because we can! We&#39;ll just add the
<code>json</code> feature to <code>axum</code>:</p>
<pre><p>TOML markup</p><p><code><i># in `Cargo.toml`</i>

<i>[</i><i>dependencies</i><i>]</i>
<i>axum</i> <i>=</i> <i>{</i> <i>version</i> <i>=</i> <i>&#34;0.4.8&#34;</i><i>,</i> <i>features</i> <i>=</i> <i>[</i><i>&#34;json&#34;</i><i>]</i> <i>}</i>
</code></p></pre>
<p>Also add serde with its derive macros:</p>
<pre><p>Shell session</p><p><code>$ cargo add serde --features derive
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding serde v1.0.136 to dependencies with features: [&#34;derive&#34;]
</code></p></pre>
<p>And now our <code>root</code> handler becomes:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>// new imports:</i>
<i>use</i> axum<i>::</i>Json<i>;</i>
<i>use</i> serde<i>::</i>Serialize<i>;</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    Ok<i>(</i>Json<i>(</i><i>Response</i> {
        <i>video_id</i>: youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>)</i><i>.</i>await?,
    }<i>)</i><i>)</i>
}
</code></p></pre>
<p>And there we go! This is what we see from <code>curl</code>:</p>
<pre><p>Shell session</p><p><code>$ curl -v http://localhost:3779/
*   Trying 127.0.0.1:3779...
* Connected to localhost (127.0.0.1) port 3779 (#0)
&gt; GET / HTTP/1.1
&gt; Host: localhost:3779
&gt; User-Agent: curl/7.79.1
&gt; Accept: */*
&gt; 
* Mark bundle as not supporting multiuse
&lt; HTTP/1.1 200 OK
&lt; content-type: application/json
&lt; content-length: 26
&lt; date: Thu, 03 Mar 2022 23:57:30 GMT
&lt; 
* Connection #0 to host localhost left intact
{&#34;video_id&#34;:&#34;nKyf4gVE5n0&#34;}% 
</code></p></pre>
<p>And from the server side:</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug,info cargo run --release
    Finished release [optimized + debuginfo] target(s) in 0.05s
     Running `target/release/plaque`
 INFO plaque Listening on http://0.0.0.0:3779
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
    
  
  57ms DEBUG tower_http::trace::on_response finished processing request, latency=57 ms, status=200

tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  plaque::root{}
    plaque::youtube::fetch_video_id{}
    
  
  55ms DEBUG tower_http::trace::on_response finished processing request, latency=55 ms, status=200
</code></p></pre>
<p>Now we have our endpoint. How do we make it better?</p>
<p>Well, we&#39;re making a fresh <code>reqwest</code> <code>Client</code> for every request. That&#39;s not
ideal. This is arguably <em>yet another</em> distraction, because our plan involves
caching, but if we weren&#39;t aiming for that, this might be the first thing we
try: just re-use the same <code>Client</code> so we can re-use an existing connection to
the YouTube server.</p>
<p>But first, you know what I realized? We don&#39;t really need tower&#39;s
<code>ServiceBuilder</code> - I was looking at an example that had <em>fallible</em> layers,
so they needed that extra flexibility, but us? We can just turn this:</p>
<pre><p>Rust code</p><p><code>    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i><i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i><i>.</i><i>layer</i><i>(</i>
        <i>ServiceBuilder</i><i>::</i><i>new</i><i>(</i><i>)</i>
            <i>.</i><i>layer</i><i>(</i><i>TraceLayer</i><i>::</i><i>new_for_http</i><i>(</i><i>)</i><i>)</i>
            <i>.</i><i>into_inner</i><i>(</i><i>)</i>,
    <i>)</i><i>;</i>
</code></p></pre>
<p>Into this:</p>
<pre><p>Rust code</p><p><code>    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i>
        <i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i><i>TraceLayer</i><i>::</i><i>new_for_http</i><i>(</i><i>)</i><i>)</i><i>;</i>
</code></p></pre>
<p>Now: axum has a neat way to share state between handlers - and it&#39;s just another
layer that adds an extension to the request! That&#39;s pretty standard hyper stuff,
just wrapped nicely.</p>
<p>Our layer stack becomes:</p>
<pre><p>Rust code</p><p><code>    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i>
        <i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i><i>TraceLayer</i><i>::</i><i>new_for_http</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i>Extension<i>(</i>reqwest<i>::</i><i>Client</i><i>::</i><i>new</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
</code></p></pre>
<p>A single <code>reqwest</code> Client is constructed when we build the app&#39;s layer stack,
and it&#39;s accessible to any request handler that wants it, via axum extractor:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
<i>//               👇 here!</i>
async <i>fn</i> <i>root</i><i>(</i><i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    Ok<i>(</i>Json<i>(</i><i>Response</i> {
        <i>//                      passed here 👇</i>
        <i>video_id</i>: youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await?,
    }<i>)</i><i>)</i>
}
</code></p></pre>
<p>And we have to change up our <code>fetch_video_id</code> function as well:</p>
<pre><p>Rust code</p><p><code><i>// in `src/youtube.rs`</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>]</i></i>
<i>//                                    👇</i>
<i>pub</i><i>(</i>crate<i>)</i> async <i>fn</i> <i>fetch_video_id</i><i>(</i><i>client</i>: <i>&amp;</i>reqwest<i>::</i><i>Client</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>String</i>, <i>Report</i><i>&gt;</i> {
    Ok<i>(</i>client
        <i>.</i><i>get</i><i>(</i>{
            <i>// (etc.)</i>
        }<i>)</i>
        <i>// (etc.)</i>
    <i>)</i>
}
</code></p></pre>
<p>Will that improve latency? Theoretically, we shouldn&#39;t have to pay the cost of a
TCP handshake for subsequent requests - the connection should only be closed if
it remains idle for some time.</p>
<p>But this is the internet. Anything can happen.</p>
<pre><p>Shell session</p><p><code>$ RUST_LOG=tower_http=debug cargo run --release
    Finished release [optimized + debuginfo] target(s) in 0.05s
     Running `target/release/plaque`
tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  58ms DEBUG tower_http::trace::on_response finished processing request, latency=58 ms, status=200

tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  13ms DEBUG tower_http::trace::on_response finished processing request, latency=13 ms, status=200

tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  13ms DEBUG tower_http::trace::on_response finished processing request, latency=13 ms, status=200

tower_http::trace::make_span::request{method=GET, uri=/, version=HTTP/1.1}
  0ms DEBUG tower_http::trace::on_request started processing request
  13ms DEBUG tower_http::trace::on_response finished processing request, latency=13 ms, status=200
</code></p></pre>
<p>Heck yeah! What do you know! Sometimes things <em>do</em> work as expected.</p>
<p>But that&#39;s not what we came here for, though. It&#39;s good... but we want to be
able to process a lot more traffic! The kind of traffic that will make the
YouTube API servers go &#34;please go away for a while&#34;, and then the request fails
and we lose out on that sweet cross-promo. Or whatever marketing term is
appropriate.</p>
<p>So, let&#39;s come up with schemes.</p>
<h2>A little caching can&#39;t hurt</h2>
<p>Now that we have a way to share some state across request handlers, we could put
a cached version of the latest video ID in there, right?</p>
<p>And we&#39;d want it to expire at some point, so maybe we store an <code>Instant</code> as
well, and decide on a duration after which we consider the data stale.</p>
<p>Because multiple handlers can run concurrently, we want to protect that cached
value with something like an <code>std::sync::Mutex</code> (we could use an <code>RwLock</code>, but
let&#39;s keep it simple).</p>
<p>So maybe we make a new struct:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>use</i> std<i>::</i>sync<i>::</i>{Arc, Mutex}<i>;</i>
<i>use</i> std<i>::</i>time<i>::</i>Instant<i>;</i>

<i>#<i>[</i>derive<i>(</i>Clone, Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLatestVideo</i> {
    <i>value</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>String</i><i>)</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>And then we change the <code>root</code> handler to use the cache:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>client, cached<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i>
    <i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i>,
    <i>cached</i>: <i>Extension</i><i>&lt;</i><i>CachedLatestVideo</i><i>&gt;</i>,
<i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    {
        <i>if</i> <i>let</i> Some<i>(</i><i>(</i>cached_at, video_id<i>)</i><i>)</i> = cached<i>.</i><i>value</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
            <i>if</i> cached_at<i>.</i><i>elapsed</i><i>(</i><i>)</i> &lt; std<i>::</i>time<i>::</i><i>Duration</i><i>::</i><i>from_secs</i><i>(</i><i>5</i><i>)</i> {
                <i>return</i> Ok<i>(</i>Json<i>(</i><i>Response</i> {
                    <i>video_id</i>: video_id<i>.</i><i>clone</i><i>(</i><i>)</i>,
                }<i>)</i><i>)</i><i>;</i>
            } <i>else</i> {
                <i>// was stale, let&#39;s refresh</i>
                <i>debug</i><i>!</i><i>(</i><i>&#34;stale video, let&#39;s refresh&#34;</i><i>)</i><i>;</i>
            }
        } <i>else</i> {
            <i>debug</i><i>!</i><i>(</i><i>&#34;not cached, let&#39;s fetched&#34;</i><i>)</i><i>;</i>
        }
    }

    <i>let</i> video_id = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await?<i>;</i>
    cached
        <i>.</i><i>value</i>
        <i>.</i><i>lock</i><i>(</i><i>)</i>
        <i>.</i><i>unwrap</i><i>(</i><i>)</i>
        <i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, video_id<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>

    Ok<i>(</i>Json<i>(</i><i>Response</i> { video_id }<i>)</i><i>)</i>
}
</code></p></pre>
<p>Because our <code>Mutex</code> is synchronous, and not asynchronous, we don&#39;t want to hold
it across await points - otherwise we could deadlock: I discuss this at length
in <a href="https://fasterthanli.me/articles/a-rust-match-made-in-hell">A Rust match made in
hell</a>.</p>
<p>So, we lock it, check if there&#39;s a value we can use - if not, we start a fetch,
and then lock it again to store it in the cache.</p>
<p>Is that going to make requests faster? Let&#39;s find out!</p>
<p>Let&#39;s run 10 requests in sequence, using <code>xargs</code> and <code>curl</code>:</p>
<pre><p>Shell session</p><p><code>$ xargs -I %n -P 1 /usr/bin/time --format=&#34;request %n: %e seconds (wall time)&#34; curl -s http://localhost:3779 -o /dev/null &lt; &lt;(printf &#39;%s\n&#39; {0..9})
request 0: 0.06 seconds (wall time)
request 1: 0.00 seconds (wall time)
request 2: 0.00 seconds (wall time)
request 3: 0.00 seconds (wall time)
request 4: 0.00 seconds (wall time)
request 5: 0.00 seconds (wall time)
request 6: 0.00 seconds (wall time)
request 7: 0.00 seconds (wall time)
request 8: 0.00 seconds (wall time)
request 9: 0.00 seconds (wall time)
</code></p></pre>
<p>This appears to work! The first response is slow (it&#39;s a cache miss), and the
others are fast (they&#39;re cache hits).</p>
<p>If we wait 5 seconds and repeat, we see roughly the same distribution, except
the first request is faster, because it reuses the connection (0.01s instead of
0.06s).</p>
<p>But what happens if there&#39;s an onslaught of requests at first? Say, 5 at the
same time? (Or rather, very close together?)</p>
<pre><p>Shell session</p><p><code> xargs -I %n -P 5 /usr/bin/time --format=&#34;request %n: %e seconds (wall time)&#34; curl -s http://localhost:3779 -o /dev/null &lt; &lt;(printf &#39;%s\n&#39; {0..9})
request 0: 0.01 seconds (wall time)
request 1: 0.01 seconds (wall time)
request 2: 0.01 seconds (wall time)
request 7: 0.00 seconds (wall time)
request 6: 0.00 seconds (wall time)
request 5: 0.00 seconds (wall time)
rreeqquueests t8 :9 : 0.000.00  sseeccoonnddss  ((wwaallll  ttiimmee))

request 3: 0.02 seconds (wall time)
request 4: 0.03 seconds (wall time)
</code></p></pre>
<p>Uhhhhhhhhhhhhh...</p>

<p>...let&#39;s try that again:</p>
<pre><p>Shell session</p><p><code>$ xargs -I %n -P 5 /usr/bin/time --format=&#34;request %n: %e seconds (wall time)&#34; curl -s http://localhost:3779 -o /dev/null &lt; &lt;(printf &#39;%s\n&#39; {0..9})
request 1: 0.01 seconds (wall time)
request 0: 0.01 seconds (wall time)
request 5: 0.00 seconds (wall time)
request 6: 0.00 seconds (wall time)
request 7: 0.00 seconds (wall time)
request 8: 0.00 seconds (wall time)
request 9: 0.00 seconds (wall time)
request 2: 0.02 seconds (wall time)
request 3: 0.02 seconds (wall time)
request 4: 0.02 seconds (wall time)
</code></p></pre>
<p>The requests are printed out of order, but you can see that requests 0 through 4
(inclusive) are cache misses: we did multiple concurrent fetches!</p>
<p>And we&#39;d like to avoid that. They&#39;re all getting the same result anyway, so
there&#39;s no point in making multiple concurrent requests to the YouTube API.</p>
<p>We need to do... request deduplication (or request coalescing, or
single-flighting).</p>
<p>In our case, because we&#39;re caching the results anyway, there&#39;s really nothing
preventing us from just moving to an asynchronous <code>Mutex</code>. A mutex that would
let us hold guards across await points.</p>
<p>Since only one request can hold the lock to the cache entry, that would
effectively deduplicate requests:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>// instead of std::sync::Mutex</i>
<i>use</i> tokio<i>::</i>sync<i>::</i>Mutex<i>;</i>

<i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>client, cached<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i>
    <i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i>,
    <i>cached</i>: <i>Extension</i><i>&lt;</i><i>CachedLatestVideo</i><i>&gt;</i>,
<i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    <i>// keep that lock throughout. that&#39;s async: 👇</i>
    <i>let</i> <i>mut</i> cached_value = cached<i>.</i><i>value</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i>await<i>;</i>

    {
        <i>if</i> <i>let</i> Some<i>(</i><i>(</i>cached_at, video_id<i>)</i><i>)</i> = cached_value<i>.</i><i>as_ref</i><i>(</i><i>)</i> {
            <i>if</i> cached_at<i>.</i><i>elapsed</i><i>(</i><i>)</i> &lt; std<i>::</i>time<i>::</i><i>Duration</i><i>::</i><i>from_secs</i><i>(</i><i>5</i><i>)</i> {
                <i>return</i> Ok<i>(</i>Json<i>(</i><i>Response</i> {
                    <i>video_id</i>: video_id<i>.</i><i>clone</i><i>(</i><i>)</i>,
                }<i>)</i><i>)</i><i>;</i>
            } <i>else</i> {
                <i>// was stale, let&#39;s refresh</i>
                <i>debug</i><i>!</i><i>(</i><i>&#34;stale video, let&#39;s refresh&#34;</i><i>)</i><i>;</i>
            }
        } <i>else</i> {
            <i>debug</i><i>!</i><i>(</i><i>&#34;not cached, let&#39;s fetched&#34;</i><i>)</i><i>;</i>
        }
    }

    <i>let</i> video_id = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await?<i>;</i>
    cached_value<i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, video_id<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>

    Ok<i>(</i>Json<i>(</i><i>Response</i> { video_id }<i>)</i><i>)</i>
}
</code></p></pre>
<p>Let&#39;s try this again:</p>
<pre><p>Shell session</p><p><code>$ xargs -I %n -P 5 /usr/bin/time --format=&#34;request %n: %e seconds (wall time)&#34; curl -s http://localhost:3779 -o /dev/null &lt; &lt;(printf &#39;%s\n&#39; {0..9})
request 2: request 0: 0.01 seconds (warll etqiumees)t
 4: 0.01 seconds (wall time)
0.01 seconds (wall time)
request 1: 0.01 seconds (wall time)
request 3: 0.01 seconds (wall time)
request 5: 0.00 seconds (wall time)
request 8: 0.00 seconds (wall time)
rreeqquueesstt  67::  0.00 seconds (0.00w aslelc otnidmse )(
wall time)
request 9: 0.00 seconds (wall time)
</code></p></pre>
<p>Oh boy. Maybe using <code>xargs</code> for this was a mistake. We could probably just use
<code>oha</code> right? Make it do 10 requests with 5 clients, the latency distributions
should show us cached / uncached requests:</p>
<pre><p>Shell session</p><p><code>$ oha -n 10 -c 5 http://localhost:3779
(cut)
Response time histogram:
  0.007 [5] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.015 [0] |
  0.022 [0] |
  0.029 [0] |
  0.037 [0] |
  0.044 [0] |
  0.051 [0] |
  0.059 [0] |
  0.066 [0] |
  0.073 [0] |
  0.081 [5] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
</code></p></pre>
<p>There, beautiful! 5 cache misses, 5 cache hits.</p>
<p>But I don&#39;t like this method: it only really works if you have one cache key.</p>
<p>Otherwise, if you have something like that:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>derive<i>(</i>Clone, Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLatestVideos</i> {
    <i>value</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>HashMap</i><i>&lt;</i><i>ChannelId</i>, <i>(</i><i>Instant</i>, <i>LatestVideoId</i><i>)</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>(Where <code>ChannelId</code> and <code>LatestVideoId</code> are just newtypes for <code>String</code> to make
our type signature readable)</p>
<p>...then requests for the latest video of channel A would block requests for the
latest video of channel B, even if those were fresh and in cache.</p>
<p>A problem we can solve with <del>even more violence</del> even more mutexes, something
like that:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>derive<i>(</i>Clone, Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLatestVideos</i> {
    <i>value</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>HashMap</i><i>&lt;</i><i>ChannelId</i>, <i>CachedVideoSlot</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>
}

<i>#<i>[</i>derive<i>(</i>Clone, Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedVideoSlot</i> {
    <i>value</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>LatestVideoId</i><i>)</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>That might work, I just... I just find myself yearning for something a little
more elegant. I need a little more grace in my life, not more locks.</p>
<p>And I&#39;d also like to stick with synchronous locks: the shorter we hold them, the
better.</p>
<p>We should be able to:</p>
<ul>
<li>Acquire the lock</li>
<li>If there&#39;s a fresh value in cache, get it and release the lock</li>
<li>Otherwise, if there&#39;s an in-flight request, subscribe to it and release the lock</li>
<li>Otherwise, start our own request (and let other requests subscribe to it) and release the lock while it&#39;s in-flight</li>
</ul>
<p>And there&#39;s a surprising amount of subtlety around doing just that!</p>
<p>My instinct tells me &#34;we&#39;re gonna need channels&#34;: we essentially need a channel
from the &#34;task&#34;, the &#34;in-flight request&#34; that we can receive a value from.</p>
<p>But there could be multiple subscribers for the same in-flight request, so we
can&#39;t use an <a href="https://docs.rs/tokio/latest/tokio/sync/mpsc/fn.channel.html">mpsc
channel</a>, the most
common kind of channel: it&#39;s multi-producer (<code>Sender</code> is <code>Clone</code>), but single-consumer.</p>
<p>Instead, we want a <a href="https://docs.rs/tokio/latest/tokio/sync/broadcast/index.html">broadcast channel</a>, which is multi-producer, multi-consumer.</p>
<p>What&#39;s interesting here is that <code>Receiver</code> is not <code>Clone</code> - so we can&#39;t store
a <code>Receiver</code> in our state, we have to store a <code>Sender</code>, and subscribe to it.</p>
<p>Let&#39;s see what a naive version would look like:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>// back to sync mutexes (instead of `tokio::sync::Mutex`)</i>
<i>use</i> std<i>::</i>sync<i>::</i>Mutex<i>;</i>
<i>use</i> tokio<i>::</i>sync<i>::</i>broadcast<i>;</i>

<i>#<i>[</i>derive<i>(</i>Clone, Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLatestVideo</i> {
    <i>inner</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>CachedLastVideoInner</i><i>&gt;</i><i>&gt;</i>,
}

<i>#<i>[</i>derive<i>(</i>Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLastVideoInner</i> {
    <i>last_fetched</i>: <i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>String</i><i>)</i><i>&gt;</i>,
    <i>inflight</i>: <i>Option</i><i>&lt;</i>broadcast<i>::</i><i>Sender</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>String</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i><i>&gt;</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>Immediately, we run into issues: we can&#39;t send <code>Box&lt;dyn Error&gt;</code> between threads:
and we definitely will, because various request handlers all run in different
tasks, which might be scheduled on different OS threads.</p>
<p>So, let&#39;s ask for the <code>Send</code> and <code>Sync</code> bounds as well:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>derive<i>(</i>Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLastVideoInner</i> {
    <i>last_fetched</i>: <i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>String</i><i>)</i><i>&gt;</i>,
    <i>// here</i>
    <i>inflight</i>: <i>Option</i><i>&lt;</i>broadcast<i>::</i><i>Sender</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>String</i>, <i>Box</i><i>&lt;</i><i>dyn</i> <i>Error</i> + <i>Sync</i> + <i>Send</i><i>&gt;</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>Then, we run into some <em>more</em> problems because... whatever values are sent to a
broadcast channel need to be <code>Clone</code>. Which make sense: each consumer will get
its own copy (its own clone, I guess) of the value. And we didn&#39;t ask for
the Error type to be <code>Clone</code>, so let&#39;s add one more bound...</p>
<pre><p>Rust code</p><p><code>    inflight: Option&lt;broadcast<i>::</i>Sender&lt;Result&lt;String, Box&lt;dyn Error + Sync + Send + Clone&gt;&gt;&gt;&gt;,
</code></p></pre>
<p>But then...</p>
<pre><p>Rust code</p><p><code>error<i>[</i>E0225<i>]</i>: only auto traits can be used <i>as</i> <i>additional</i> traits <i>in</i> a <i>trait</i> object
  --&gt; src/main<i>.</i><i>rs</i>:<i>42</i>:<i>85</i>
   |
<i>42</i> |     inflight: Option&lt;broadcast<i>::</i>Sender&lt;Result&lt;String, Box&lt;dyn Error + Sync + Send + Clone&gt;&gt;&gt;&gt;,
   |                                                               -----                 ^^^^^ additional non-auto <i>trait</i>
   |                                                               |
   |                                                               first non-auto <i>trait</i>
   |
   = <i>help</i>: <i>consider</i> creating a new <i>trait</i> with all of these <i>as</i> supertraits and using that <i>trait</i> here instead: `<i>trait</i> <i>NewTrait</i>: <i>StdError</i> + <i>Clone</i> {}`
   = note: auto-traits like `Send` and `Sync` are traits that have special properties<i>;</i> <i>for</i> more information on them, visit &lt;https:<i>//doc.rust-lang.org/reference/special-types-and-traits.html#auto-traits&gt;</i>
</code></p></pre>
<p>Mhhh we might be on the wrong road here. I&#39;m not even sure all the errors we
could possibly get are <code>Clone</code> anyway (or <code>Send</code>, or <code>Sync</code>). Instead, let&#39;s
just make an error type that carries the human-readable string representation
of the error.</p>
<pre><p>Shell session</p><p><code>$ cargo add thiserror
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding thiserror v1.0.30 to dependencies
</code></p></pre><pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>#<i>[</i>derive<i>(</i>Debug, Clone, thiserror<i>::</i>Error<i>)</i><i>]</i></i>
<i>#<i>[</i>error<i>(</i><i>&#34;stringified error: {inner}&#34;</i><i>)</i><i>]</i></i>
<i>pub</i> <i>struct</i> <i>CachedError</i> {
    <i>inner</i>: <i>String</i>,
}

<i>impl</i> <i>CachedError</i> {
    <i>pub</i> <i>fn</i> <i>new</i><i>&lt;</i><i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i><i>&gt;</i><i>(</i><i>e</i>: <i>E</i><i>)</i> -&gt; <i>Self</i> {
        <i>Self</i> {
            <i>inner</i>: e<i>.</i><i>to_string</i><i>(</i><i>)</i>,
        }
    }
}

<i>impl</i> <i>From</i><i>&lt;</i><i>Report</i><i>&gt;</i> <i>for</i> <i>CachedError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>e</i>: <i>Report</i><i>)</i> -&gt; <i>Self</i> {
        <i>CachedError</i><i>::</i><i>new</i><i>(</i>e<i>)</i>
    }
}

<i>impl</i> <i>From</i><i>&lt;</i>broadcast<i>::</i>error<i>::</i><i>RecvError</i><i>&gt;</i> <i>for</i> <i>CachedError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>e</i>: broadcast<i>::</i>error<i>::</i><i>RecvError</i><i>)</i> -&gt; <i>Self</i> {
        <i>CachedError</i><i>::</i><i>new</i><i>(</i>e<i>)</i>
    }
}

<i>// and for completeness</i>

<i>impl</i> <i>From</i><i>&lt;</i><i>CachedError</i><i>&gt;</i> <i>for</i> <i>ReportError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>err</i>: <i>CachedError</i><i>)</i> -&gt; <i>Self</i> {
        ReportError<i>(</i>err<i>.</i><i>into</i><i>(</i><i>)</i><i>)</i>
    }
}
</code></p></pre>
<p>Now we can have this be our error type, which is automatically <code>Send</code> and
<code>Sync</code>, also <code>Clone</code> because we derived it, and implements <code>std::error::Error</code>
thanks to <a href="https://lib.rs/crates/thiserror">thiserror</a>.</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>derive<i>(</i>Default<i>)</i><i>]</i></i>
<i>struct</i> <i>CachedLastVideoInner</i> {
    <i>last_fetched</i>: <i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>String</i><i>)</i><i>&gt;</i>,
    <i>inflight</i>: <i>Option</i><i>&lt;</i>broadcast<i>::</i><i>Sender</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>String</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}
</code></p></pre>
<p>And now we &#34;just&#34; have to implement our deduplicating handler:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>client, cached<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i>
    <i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i>,
    <i>cached</i>: <i>Extension</i><i>&lt;</i><i>CachedLatestVideo</i><i>&gt;</i>,
<i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    <i>// keep that lock throughout</i>
    <i>let</i> <i>mut</i> inner = cached<i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>

    <i>if</i> <i>let</i> Some<i>(</i><i>(</i>fetched_at, video_id<i>)</i><i>)</i> = inner<i>.</i><i>last_fetched</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
        <i>// is it fresh?</i>
        <i>if</i> fetched_at<i>.</i><i>elapsed</i><i>(</i><i>)</i> &lt; std<i>::</i>time<i>::</i><i>Duration</i><i>::</i><i>from_secs</i><i>(</i><i>5</i><i>)</i> {
            <i>return</i> Ok<i>(</i>Json<i>(</i><i>Response</i> {
                <i>video_id</i>: video_id<i>.</i><i>clone</i><i>(</i><i>)</i>,
            }<i>)</i><i>)</i><i>;</i>
        } <i>else</i> {
            <i>// was stale, let&#39;s refresh</i>
            <i>debug</i><i>!</i><i>(</i><i>&#34;stale video, let&#39;s refresh&#34;</i><i>)</i><i>;</i>
        }
    }

    <i>// is there an in-flight request?</i>
    <i>if</i> <i>let</i> Some<i>(</i>inflight<i>)</i> = inner<i>.</i><i>inflight</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
        <i>// yes, subscribe to it!</i>
        <i>let</i> <i>mut</i> rx = inflight<i>.</i><i>subscribe</i><i>(</i><i>)</i><i>;</i>
        <i>let</i> video_id = rx
            <i>.</i><i>recv</i><i>(</i><i>)</i>
            <i>.</i>await
            <i>.</i><i>map_err</i><i>(</i>|_| <i>eyre</i><i>!</i><i>(</i><i>&#34;in-flight request died&#34;</i><i>)</i><i>)</i>??<i>;</i>
        <i>debug</i><i>!</i><i>(</i><i>&#34;received deduplicated fetch&#34;</i><i>)</i><i>;</i>
        <i>return</i> Ok<i>(</i>Json<i>(</i><i>Response</i> { video_id }<i>)</i><i>)</i><i>;</i>
    }

    <i>// there isn&#39;t, let&#39;s fetch</i>
    <i>let</i> <i>(</i>tx, <i>mut</i> rx<i>)</i> = broadcast<i>::</i><i>channel</i><i>::</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>String</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>(</i><i>1</i><i>)</i><i>;</i>
    tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
        <i>let</i> video_id = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await<i>;</i>
        <i>match</i> video_id {
            Ok<i>(</i>video_id<i>)</i> =&gt; {
                inner
                    <i>.</i><i>last_fetched</i>
                    <i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, video_id<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                tx<i>.</i><i>send</i><i>(</i>Ok<i>(</i>video_id<i>)</i><i>)</i>
            }
            Err<i>(</i>e<i>)</i> =&gt; tx<i>.</i><i>send</i><i>(</i>Err<i>(</i>e<i>.</i><i>into</i><i>(</i><i>)</i><i>)</i><i>)</i>,
        }<i>;</i>
    }<i>)</i><i>;</i>
    <i>let</i> video_id = rx
        <i>.</i><i>recv</i><i>(</i><i>)</i>
        <i>.</i>await
        <i>.</i><i>map_err</i><i>(</i>|_| <i>eyre</i><i>!</i><i>(</i><i>&#34;in-flight request died&#34;</i><i>)</i><i>)</i>??<i>;</i>
    <i>debug</i><i>!</i><i>(</i><i>&#34;received deduplicated fetch&#34;</i><i>)</i><i>;</i>
    Ok<i>(</i>Json<i>(</i><i>Response</i> { video_id }<i>)</i><i>)</i>
}
</code></p></pre>
<p>This code is wrong, and thankfully, the Rust compiler catches it:</p>
<pre><p>Shell session</p><p><code>$ cargo check
    Checking plaque v0.1.0 (/home/amos/bearcove/plaque)
error: future cannot be sent between threads safely
   --&gt; src/main.rs:124:5
    |
124 |     tokio::spawn(async move {
    |     ^^^^^^^^^^^^ future created by async block is not `Send`
    |
    = help: within `impl Future&lt;Output = [async output]&gt;`, the trait `Send` is not implemented for `std::sync::MutexGuard&lt;&#39;_, CachedLastVideoInner&gt;`
note: captured value is not `Send`
   --&gt; src/main.rs:128:17
    |
128 |                 inner
    |                 ^^^^^ has type `std::sync::MutexGuard&lt;&#39;_, CachedLastVideoInner&gt;` which is not `Send`
note: required by a bound in `tokio::spawn`
   --&gt; /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/task/spawn.rs:127:21
    |
127 |         T: Future + Send + &#39;static,
    |                     ^^^^ required by this bound in `tokio::spawn`

error: could not compile `plaque` due to previous error
</code></p></pre>
<p>Woops! We actually sent a <code>MutexGuard</code> across threads (by spawning a task, which
may be scheduled on any thread).</p>
<p>It&#39;s a good thing the compiler caught it, because that&#39;s not what we meant to
do: we didn&#39;t want to hang onto that lock, we wanted to release it and
re-acquire it when we fill in the result.</p>
<p>Also, we forgot to store the sender in <code>inner.inflight</code> and clear it when it&#39;s
done. But even more importantly: when we subscribe to an in-flight task, we keep
holding onto the <code>inner</code> lock! That would&#39;ve been a deadlock.</p>
<p>The compiler catches this, too, but the message isn&#39;t great this time:</p>
<pre><p>Shell session</p><p><code>$ cargo check
    Checking plaque v0.1.0 (/home/amos/bearcove/plaque)
error[E0277]: the trait bound `fn(Extension&lt;reqwest::Client&gt;, Extension&lt;CachedLatestVideo&gt;) -&gt; impl Future&lt;Output = Result&lt;Opaque(DefId(0:229 ~ plaque[f569]::root::{opaque#0}::{opaque#0}), []), ReportError&gt;&gt; {root}: axum::handler::Handler&lt;_, _&gt;` is not satisfied
   --&gt; src/main.rs:76:25
    |
76  |         .route(&#34;/&#34;, get(root))
    |                     --- ^^^^ the trait `axum::handler::Handler&lt;_, _&gt;` is not implemented for `fn(Extension&lt;reqwest::Client&gt;, Extension&lt;CachedLatestVideo&gt;) -&gt; impl Future&lt;Output = Result&lt;Opaque(DefId(0:229 ~ plaque[f569]::root::{opaque#0}::{opaque#0}), []), ReportError&gt;&gt; {root}`
    |                     |
    |                     required by a bound introduced by this call
    |
note: required by a bound in `axum::routing::get`
   --&gt; /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/axum-0.4.8/src/routing/method_routing.rs:394:1
    |
394 | top_level_handler_fn!(get, GET);
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ required by this bound in `axum::routing::get`
    = note: this error originates in the macro `top_level_handler_fn` (in Nightly builds, run with -Z macro-backtrace for more info)

For more information about this error, try `rustc --explain E0277`.
error: could not compile `plaque` due to previous error
</code></p></pre>
<p>So this time, we&#39;ll have to think a little: we <em>only</em> want to do synchronous
things while holding onto that lock guard: we <em>cannot</em> <code>.await</code> anything.</p>
<p>So, we&#39;ll have to get a little creative. To be entirely safe, we&#39;ll use a scope
to limit how long we hold the lock - to make sure that when we&#39;re out of this
scope, the lock is released.</p>
<p>Something like this:</p>
<pre><p><code>fn() {
    let whatever = {
        let inner = cached.inner.lock().unwrap();

        // do sync stuff with `inner`
    };

    // do async stuff with `whatever`
}
</code></p></pre>
<p>Giving, with real code, this:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>client, cached<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i>
    <i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i>,
    <i>cached</i>: <i>Extension</i><i>&lt;</i><i>CachedLatestVideo</i><i>&gt;</i>,
<i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    <i>let</i> <i>mut</i> rx = {
        <i>// only sync code in this block</i>
        <i>let</i> <i>mut</i> inner = cached<i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>

        <i>if</i> <i>let</i> Some<i>(</i><i>(</i>fetched_at, video_id<i>)</i><i>)</i> = inner<i>.</i><i>last_fetched</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
            <i>// is it fresh?</i>
            <i>if</i> fetched_at<i>.</i><i>elapsed</i><i>(</i><i>)</i> &lt; std<i>::</i>time<i>::</i><i>Duration</i><i>::</i><i>from_secs</i><i>(</i><i>5</i><i>)</i> {
                <i>return</i> Ok<i>(</i>Json<i>(</i><i>Response</i> {
                    <i>video_id</i>: video_id<i>.</i><i>clone</i><i>(</i><i>)</i>,
                }<i>)</i><i>)</i><i>;</i>
            } <i>else</i> {
                <i>// was stale, let&#39;s refresh</i>
                <i>debug</i><i>!</i><i>(</i><i>&#34;stale video, let&#39;s refresh&#34;</i><i>)</i><i>;</i>
            }
        }

        <i>// is there an in-flight request?</i>
        <i>if</i> <i>let</i> Some<i>(</i>inflight<i>)</i> = inner<i>.</i><i>inflight</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
            inflight<i>.</i><i>subscribe</i><i>(</i><i>)</i>
        } <i>else</i> {
            <i>// there isn&#39;t, let&#39;s fetch</i>
            <i>let</i> <i>(</i>tx, rx<i>)</i> = broadcast<i>::</i><i>channel</i><i>::</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>String</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>(</i><i>1</i><i>)</i><i>;</i>
            inner<i>.</i><i>inflight</i> = Some<i>(</i>tx<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>;</i>
            <i>let</i> cached = cached<i>.</i><i>clone</i><i>(</i><i>)</i><i>;</i>
            tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
                <i>let</i> video_id = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await<i>;</i>

                {
                    <i>// only sync code in this block</i>
                    <i>let</i> <i>mut</i> inner = cached<i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>
                    inner<i>.</i><i>inflight</i> = None<i>;</i>

                    <i>match</i> video_id {
                        Ok<i>(</i>video_id<i>)</i> =&gt; {
                            inner
                                <i>.</i><i>last_fetched</i>
                                <i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, video_id<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                            <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Ok<i>(</i>video_id<i>)</i><i>)</i><i>;</i>
                        }
                        Err<i>(</i>e<i>)</i> =&gt; {
                            <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Err<i>(</i>e<i>.</i><i>into</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                        }
                    }<i>;</i>
                }
            }<i>)</i><i>;</i>
            rx
        }
    }<i>;</i>

    <i>// if we reached here, we&#39;re waiting for an in-flight request (we weren&#39;t</i>
    <i>// able to serve from cache)</i>
    Ok<i>(</i>Json<i>(</i><i>Response</i> {
        <i>video_id</i>: rx
            <i>.</i><i>recv</i><i>(</i><i>)</i>
            <i>.</i>await
            <i>.</i><i>map_err</i><i>(</i>|_| <i>eyre</i><i>!</i><i>(</i><i>&#34;in-flight request died&#34;</i><i>)</i><i>)</i>??,
    }<i>)</i><i>)</i>
}
</code></p></pre>
<p>And that appears to work:</p>
<pre><p>Shell session</p><p><code>$ oha -n 10 -c 5 http://localhost:3779
(cut)
Response time histogram:
  0.005 [5] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.011 [0] |
  0.016 [0] |
  0.022 [0] |
  0.027 [0] |
  0.033 [0] |
  0.038 [0] |
  0.044 [0] |
  0.049 [0] |
  0.055 [0] |
  0.060 [5] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
</code></p></pre>
<h2>Making it generic</h2>
<p>Chances are, that&#39;s not the only thing we want to cache! And if we have <a href="https://en.wikipedia.org/wiki/Rule_of_three_(computer_programming)">three
or more</a>, we
won&#39;t want to copy-paste that code everywhere.</p>
<p>Well, no big deal, we&#39;ll just make it generic!</p>
<p>In fact, let&#39;s move all caching/deduplication-related code to its own module:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>mod</i> cached<i>;</i>
<i>use</i> cached<i>::</i>{CachedError, Cached}<i>;</i>
</code></p></pre><pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

<i>use</i> std<i>::</i>{
    sync<i>::</i>{Arc, Mutex},
    time<i>::</i>{Duration, Instant},
}<i>;</i>

<i>use</i> color_eyre<i>::</i>{eyre<i>::</i>eyre, Report}<i>;</i>
<i>use</i> tokio<i>::</i>sync<i>::</i>broadcast<i>;</i>
<i>use</i> tracing<i>::</i>debug<i>;</i>

<i>#<i>[</i>derive<i>(</i>Debug, Clone, thiserror<i>::</i>Error<i>)</i><i>]</i></i>
<i>#<i>[</i>error<i>(</i><i>&#34;stringified error: {inner}&#34;</i><i>)</i><i>]</i></i>
<i>pub</i> <i>struct</i> <i>CachedError</i> {
    <i>inner</i>: <i>String</i>,
}

<i>impl</i> <i>CachedError</i> {
    <i>pub</i> <i>fn</i> <i>new</i><i>&lt;</i><i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i><i>&gt;</i><i>(</i><i>e</i>: <i>E</i><i>)</i> -&gt; <i>Self</i> {
        <i>Self</i> {
            <i>inner</i>: e<i>.</i><i>to_string</i><i>(</i><i>)</i>,
        }
    }
}

<i>impl</i> <i>From</i><i>&lt;</i><i>Report</i><i>&gt;</i> <i>for</i> <i>CachedError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>e</i>: <i>Report</i><i>)</i> -&gt; <i>Self</i> {
        <i>CachedError</i><i>::</i><i>new</i><i>(</i>e<i>)</i>
    }
}

<i>impl</i> <i>From</i><i>&lt;</i>broadcast<i>::</i>error<i>::</i><i>RecvError</i><i>&gt;</i> <i>for</i> <i>CachedError</i> {
    <i>fn</i> <i>from</i><i>(</i><i>e</i>: broadcast<i>::</i>error<i>::</i><i>RecvError</i><i>)</i> -&gt; <i>Self</i> {
        <i>CachedError</i><i>::</i><i>new</i><i>(</i>e<i>)</i>
    }
}

<i>#<i>[</i>derive<i>(</i>Clone<i>)</i><i>]</i></i>
<i>pub</i> <i>struct</i> <i>Cached</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>inner</i>: <i>Arc</i><i>&lt;</i><i>Mutex</i><i>&lt;</i><i>CachedLastVideoInner</i><i>&lt;</i><i>T</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
    <i>refresh_interval</i>: <i>Duration</i>,
}

<i>struct</i> <i>CachedLastVideoInner</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>last_fetched</i>: <i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>T</i><i>)</i><i>&gt;</i>,
    <i>inflight</i>: <i>Option</i><i>&lt;</i>broadcast<i>::</i><i>Sender</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}

<i>impl</i><i>&lt;</i><i>T</i><i>&gt;</i> <i>Default</i> <i>for</i> <i>CachedLastVideoInner</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>fn</i> <i>default</i><i>(</i><i>)</i> -&gt; <i>Self</i> {
        <i>Self</i> {
            <i>last_fetched</i>: None,
            <i>inflight</i>: None,
        }
    }
}

<i>impl</i><i>&lt;</i><i>T</i><i>&gt;</i> <i>Cached</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>pub</i> <i>fn</i> <i>new</i><i>(</i><i>refresh_interval</i>: <i>Duration</i><i>)</i> -&gt; <i>Self</i> {
        <i>Self</i> {
            <i>inner</i>: <i>Default</i><i>::</i><i>default</i><i>(</i><i>)</i>,
            refresh_interval,
        }
    }

    <i>pub</i> async <i>fn</i> <i>get_cached</i><i>(</i><i>&amp;</i><i>self</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i> {
        <i>let</i> <i>mut</i> rx = {
            <i>// only sync code in this block</i>
            <i>let</i> <i>mut</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>

            <i>if</i> <i>let</i> Some<i>(</i><i>(</i>fetched_at, value<i>)</i><i>)</i> = inner<i>.</i><i>last_fetched</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
                <i>if</i> fetched_at<i>.</i><i>elapsed</i><i>(</i><i>)</i> &lt; <i>self</i><i>.</i><i>refresh_interval</i> {
                    <i>return</i> Ok<i>(</i>value<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>;</i>
                } <i>else</i> {
                    <i>debug</i><i>!</i><i>(</i><i>&#34;stale, let&#39;s refresh&#34;</i><i>)</i><i>;</i>
                }
            }

            <i>if</i> <i>let</i> Some<i>(</i>inflight<i>)</i> = inner<i>.</i><i>inflight</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
                inflight<i>.</i><i>subscribe</i><i>(</i><i>)</i>
            } <i>else</i> {
                <i>// there isn&#39;t, let&#39;s fetch</i>
                <i>let</i> <i>(</i>tx, rx<i>)</i> = broadcast<i>::</i><i>channel</i><i>::</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>(</i><i>1</i><i>)</i><i>;</i>
                inner<i>.</i><i>inflight</i> = Some<i>(</i>tx<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>;</i>
                <i>let</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>clone</i><i>(</i><i>)</i><i>;</i>
                tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
                    <i>let</i> res = <i>todo</i><i>!</i><i>(</i><i>&#34;how do we actually perform a request?&#34;</i><i>)</i><i>;</i>

                    {
                        <i>// only sync code in this block</i>
                        <i>let</i> <i>mut</i> inner = inner<i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>
                        inner<i>.</i><i>inflight</i> = None<i>;</i>

                        <i>match</i> res {
                            Ok<i>(</i>value<i>)</i> =&gt; {
                                inner<i>.</i><i>last_fetched</i><i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, value<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                                <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Ok<i>(</i>value<i>)</i><i>)</i><i>;</i>
                            }
                            Err<i>(</i>e<i>)</i> =&gt; {
                                <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Err<i>(</i>e<i>.</i><i>into</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                            }
                        }<i>;</i>
                    }
                }<i>)</i><i>;</i>
                rx
            }
        }<i>;</i>

        <i>// if we reached here, we&#39;re waiting for an in-flight request (we weren&#39;t</i>
        <i>// able to serve from cache)</i>
        Ok<i>(</i>rx
            <i>.</i><i>recv</i><i>(</i><i>)</i>
            <i>.</i>await
            <i>.</i><i>map_err</i><i>(</i>|_| <i>eyre</i><i>!</i><i>(</i><i>&#34;in-flight request died&#34;</i><i>)</i><i>)</i>??<i>)</i>
    }
}
</code></p></pre>
<p>There&#39;s just one little wrinkle... as the <code>todo!</code> mentions: how do we actually
perform a request? Where do we get our values from? That too, needs to be
generic.</p>
<p>We could take a future directly, but that means we&#39;d need to build a future
(e.g. call <code>youtube::fetch_video_id</code> without awaiting it) every time we want to
call <code>get_cached</code> - and that&#39;s unnecessary work.</p>
<p>Here&#39;s my take: instead, let&#39;s accept a closure that can build a future. Here
I&#39;ll assume that it&#39;s okay to heap-allocate in that code path, so we&#39;ll have it
return a boxed future for simplicity. Our implementation of
<code>youtube::fetch_video_id</code> is an async fn anyway, so its future type cannot
be named, as of Rust 1.59.0 stable, unless I&#39;m missing something.</p>
<p>So, quick type alias:</p>
<pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

<i>use</i> std<i>::</i>future<i>::</i>Future<i>;</i>

<i>pub</i> <i>type</i> <i>BoxFut</i><i>&lt;</i><i>&#39;</i><i>a</i>, <i>O</i><i>&gt;</i> = <i>Pin</i><i>&lt;</i><i>Box</i><i>&lt;</i><i>dyn</i> <i>Future</i><i>&lt;</i><i>Output</i> = <i>O</i><i>&gt;</i> + <i>Send</i> + <i>&#39;</i><i>a</i><i>&gt;</i><i>&gt;</i><i>;</i>
</code></p></pre>
<p>And let&#39;s accept a function returning a boxed future that returns a <code>T</code> (and
can fail! with any error type that we know how to display!)</p>
<pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

    <i>pub</i> async <i>fn</i> <i>get_cached</i><i>&lt;</i><i>F</i>, <i>E</i><i>&gt;</i><i>(</i><i>&amp;</i><i>self</i>, <i>f</i>: <i>F</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i>
    <i>where</i>
        <i>F</i>: <i>FnOnce</i><i>(</i><i>)</i> -&gt; <i>BoxFut</i><i>&lt;</i><i>&#39;</i><i>static</i>, <i>Result</i><i>&lt;</i><i>T</i>, <i>E</i><i>&gt;</i><i>&gt;</i> + <i>Send</i> + <i>&#39;</i><i>static</i>,
        <i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i> + <i>&#39;</i><i>static</i>,
    {
        <i>// (cut)</i>
    }
</code></p></pre>
<p>And our code becomes:</p>
<pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

            <i>if</i> <i>let</i> Some<i>(</i>inflight<i>)</i> = inner<i>.</i><i>inflight</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
                inflight<i>.</i><i>subscribe</i><i>(</i><i>)</i>
            } <i>else</i> {
                <i>// there isn&#39;t, let&#39;s fetch</i>
                <i>let</i> <i>(</i>tx, rx<i>)</i> = broadcast<i>::</i><i>channel</i><i>::</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>(</i><i>1</i><i>)</i><i>;</i>
                inner<i>.</i><i>inflight</i> = Some<i>(</i>tx<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>;</i>
                <i>let</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>clone</i><i>(</i><i>)</i><i>;</i>

                <i>// call the closure first, so we don&#39;t send _it_ across threads,</i>
                <i>// just the Future it returns</i>
                <i>let</i> fut = <i>f</i><i>(</i><i>)</i><i>;</i>

                tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
                    <i>let</i> res = fut<i>.</i>await<i>;</i>

                    {
                        <i>// only sync code in this block</i>
                        <i>let</i> <i>mut</i> inner = inner<i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>
                        inner<i>.</i><i>inflight</i> = None<i>;</i>

                        <i>match</i> res {
                            Ok<i>(</i>value<i>)</i> =&gt; {
                                inner<i>.</i><i>last_fetched</i><i>.</i><i>replace</i><i>(</i><i>(</i><i>Instant</i><i>::</i><i>now</i><i>(</i><i>)</i>, value<i>.</i><i>clone</i><i>(</i><i>)</i><i>)</i><i>)</i><i>;</i>
                                <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Ok<i>(</i>value<i>)</i><i>)</i><i>;</i>
                            }
                            Err<i>(</i>e<i>)</i> =&gt; {
                                <i>let</i> _ = tx<i>.</i><i>send</i><i>(</i>Err<i>(</i><i>CachedError</i> {
                                    <i>inner</i>: e<i>.</i><i>to_string</i><i>(</i><i>)</i>,
                                }<i>)</i><i>)</i><i>;</i>
                            }
                        }<i>;</i>
                    }
                }<i>)</i><i>;</i>
                rx
            }                
</code></p></pre>
<p>Now to use our beautifully generic code for real:</p>
<pre><p>Rust code</p><p><code><i>// in `src/main.rs`</i>

<i>#<i>[</i>derive<i>(</i>Clone<i>)</i><i>]</i></i>
<i>struct</i> <i>LatestVideo</i><i>(</i><i>String</i><i>)</i><i>;</i>
</code></p></pre>
<p>Our layer stack becomes this:</p>
<pre><p>Rust code</p><p><code>    <i>let</i> app = <i>Router</i><i>::</i><i>new</i><i>(</i><i>)</i>
        <i>.</i><i>route</i><i>(</i><i>&#34;/&#34;</i>, <i>get</i><i>(</i>root<i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i><i>TraceLayer</i><i>::</i><i>new_for_http</i><i>(</i><i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i>Extension<i>(</i>reqwest<i>::</i><i>Client</i><i>::</i><i>new</i><i>(</i><i>)</i><i>)</i><i>)</i>
        <i>.</i><i>layer</i><i>(</i>Extension<i>(</i><i>Cached</i><i>::</i><i>&lt;</i><i>LatestVideo</i><i>&gt;</i><i>::</i><i>new</i><i>(</i><i>Duration</i><i>::</i><i>from_secs</i><i>(</i>
            <i>5</i>,
        <i>)</i><i>)</i><i>)</i><i>)</i><i>;</i>
</code></p></pre>
<p>And <code>root</code>, just this:</p>
<pre><p>Rust code</p><p><code><i>#<i>[</i>tracing<i>::</i>instrument<i>(</i>skip<i>(</i>client, cached<i>)</i><i>)</i><i>]</i></i>
async <i>fn</i> <i>root</i><i>(</i>
    <i>client</i>: <i>Extension</i><i>&lt;</i>reqwest<i>::</i><i>Client</i><i>&gt;</i>,
    <i>cached</i>: <i>Extension</i><i>&lt;</i><i>Cached</i><i>&lt;</i><i>LatestVideo</i><i>&gt;</i><i>&gt;</i>,
<i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>impl</i> <i>IntoResponse</i>, <i>ReportError</i><i>&gt;</i> {
    <i>#<i>[</i>derive<i>(</i>Serialize<i>)</i><i>]</i></i>
    <i>struct</i> <i>Response</i> {
        <i>video_id</i>: <i>String</i>,
    }

    <i>let</i> LatestVideo<i>(</i>video_id<i>)</i> = cached
        <i>.</i><i>get_cached</i><i>(</i>|| {
            <i>Box</i><i>::</i><i>pin</i><i>(</i>async <i>move</i> {
                <i>let</i> video_id = youtube<i>::</i><i>fetch_video_id</i><i>(</i><i>&amp;</i>client<i>)</i><i>.</i>await?<i>;</i>
                Ok<i>::</i><i>&lt;</i><i>_</i>, <i>Report</i><i>&gt;</i><i>(</i>LatestVideo<i>(</i>video_id<i>)</i><i>)</i>
            }<i>)</i>
        }<i>)</i>
        <i>.</i>await?<i>;</i>

    Ok<i>(</i>Json<i>(</i><i>Response</i> { video_id }<i>)</i><i>)</i>
}
</code></p></pre>
<p>That&#39;s.. nice! Well, there&#39;s a box-pinned async-move block in a closure, and we
have to use a lil&#39; <a href="https://turbo.fish/">turbofish</a> to let the compiler know the
error type, but we&#39;ve separated concerns, so as far as I&#39;m concerned: mission
accomplished.</p>
<p>If the cached items were keyed, we could easily imagine that the outer <code>Cached</code>
structure had a <code>HashMap</code> of <code>CachedInner</code> instead: we&#39;d still have a single
lock, which would only ever be held for very short periods of time.</p>
<p>In a production app, we might actually want to have a <em>negative</em> cache as well:
if we can&#39;t fetch the video for some reason, and it fails <em>almost immedialy</em>, we
wouldn&#39;t want to hammer YouTube&#39;s API servers with requests. We&#39;d want to sleep
between retries, and have our handler either wait for some retries, or
immediately return an error.</p>
<p>We also might want to set a timeout on that HTTP request: I don&#39;t think there&#39;s
any timeouts (connection timeout, idle read/write timeout) on that default
reqwest client, and that usually makes me nervous. Timeout all the things,
always!</p>
<p>But that&#39;s not what I&#39;m concerned with - y&#39;all can solve all of these on your
own.</p>
<p>What I&#39;m concerned with is we left a terrific, horrific bug in there. One that,
in a past <del>life</del> day job, led to a pretty bad incident that was really hard to
investigate.</p>
<h2>Panic! at the in-flight request</h2>
<p>What happens if any part of our code panics? That&#39;s something we need to think
about, and that Rust protects us <em>less than usual</em> from. They&#39;re not safety
concerns, &#34;just&#34; logic errors.</p>
<p>Let&#39;s make... the 3rd request panic.</p>
<pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

<i>use</i> std<i>::</i>sync<i>::</i>atomic<i>::</i>{AtomicU64, Ordering}<i>;</i>

<i>static</i> DOOM_COUNTER: <i>AtomicU64</i> = <i>AtomicU64</i><i>::</i><i>new</i><i>(</i><i>0</i><i>)</i><i>;</i>

<i>impl</i><i>&lt;</i><i>T</i><i>&gt;</i> <i>Cached</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>pub</i> async <i>fn</i> <i>get_cached</i><i>&lt;</i><i>F</i>, <i>E</i><i>&gt;</i><i>(</i><i>&amp;</i><i>self</i>, <i>f</i>: <i>F</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i>
    <i>where</i>
        <i>F</i>: <i>FnOnce</i><i>(</i><i>)</i> -&gt; <i>BoxFut</i><i>&lt;</i><i>&#39;</i><i>static</i>, <i>Result</i><i>&lt;</i><i>T</i>, <i>E</i><i>&gt;</i><i>&gt;</i>,
        <i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i> + <i>&#39;</i><i>static</i>,
    {
        <i>if</i> DOOM_COUNTER<i>.</i><i>fetch_add</i><i>(</i><i>1</i>, <i>Ordering</i><i>::</i>SeqCst<i>)</i> == <i>2</i> {
            <i>panic</i><i>!</i><i>(</i><i>&#34;doom!&#34;</i><i>)</i><i>;</i>
        }

        <i>// etc.</i>
    }
}
</code></p></pre><pre><p>Shell session</p><p><code>$ oha -n 10 -c 5 http://localhost:3779

Status code distribution:
  [200] 9 responses

Error distribution:
  [1] connection closed before message completed
</code></p></pre>
<p>Okay, that&#39;s fine - only the third request was impacted. Because our handler
panicked, axum/hyper just closed the connection. Seems like a reasonable
response, even though if we wanted, we could probably catch the panic and turn
it into a 500 instead.</p>
<p>Now let&#39;s panic someplace else! Say, after we&#39;ve acquired the lock:</p>
<pre><p>Rust code</p><p><code>    <i>pub</i> async <i>fn</i> <i>get_cached</i><i>&lt;</i><i>F</i>, <i>E</i><i>&gt;</i><i>(</i><i>&amp;</i><i>self</i>, <i>f</i>: <i>F</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i>
    <i>where</i>
        <i>F</i>: <i>FnOnce</i><i>(</i><i>)</i> -&gt; <i>BoxFut</i><i>&lt;</i><i>&#39;</i><i>static</i>, <i>Result</i><i>&lt;</i><i>T</i>, <i>E</i><i>&gt;</i><i>&gt;</i>,
        <i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i> + <i>&#39;</i><i>static</i>,
    {
        <i>let</i> <i>mut</i> rx = {
            <i>// only sync code in this block</i>
            <i>let</i> <i>mut</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>.</i><i>unwrap</i><i>(</i><i>)</i><i>;</i>

            <i>if</i> DOOM_COUNTER<i>.</i><i>fetch_add</i><i>(</i><i>1</i>, <i>Ordering</i><i>::</i>SeqCst<i>)</i> == <i>2</i> {
                <i>panic</i><i>!</i><i>(</i><i>&#34;doom!&#34;</i><i>)</i><i>;</i>
            }

            <i>// etc.</i>
        }<i>;</i>

        <i>// if we reached here, we&#39;re waiting for an in-flight request (we weren&#39;t</i>
        <i>// able to serve from cache)</i>
        Ok<i>(</i>rx
            <i>.</i><i>recv</i><i>(</i><i>)</i>
            <i>.</i>await
            <i>.</i><i>map_err</i><i>(</i>|_| <i>eyre</i><i>!</i><i>(</i><i>&#34;in-flight request died&#34;</i><i>)</i><i>)</i>??<i>)</i>
    }
</code></p></pre>
<p>Now, oha never completes: it&#39;s stuck in the TUI with 8 &#34;connection closed before
the message completed&#34;, and 2 requests that just... never complete.</p>
<p>Our server logs tell the story:</p>
<pre><p>Shell session</p><p><code>The application panicked (crashed).
Message:  called `Result::unwrap()` on an `Err` value: PoisonError { .. }
Location: src/cached.rs:91

  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ SPANTRACE ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

   0: plaque::root
      at src/main.rs:49
   1: tower_http::trace::make_span::request with method=GET uri=/ version=HTTP/1.1
      at /home/amos/.cargo/registry/src/github.com-1ecc6299db9ec823/tower-http-0.2.3/src/trace/make_span.rs:116

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>Because we panicked while holding a <code>MutexGuard</code>, the mutex is now poisoned, and
nobody can ever acquire it again.</p>
<p>So what happens is:</p>
<ul>
<li>The first request finds nothing in cache, starts a task</li>
<li>The second task finds an in-flight request, subscribes to it</li>
<li>The third task acquires the lock, panics holding it</li>
<li>Tasks 4 through 10 panic as they try to acquire the poisoned lock</li>
</ul>
<p>The first and second requests are waiting for a task that will never complete,
because it too, panicked while trying to acquire the poisoned lock. So we get
eight connection resets, and two requests stuck forever.</p>
<div>

<p>I guess that&#39;s where tower&#39;s <a href="https://docs.rs/tower/latest/tower/builder/struct.ServiceBuilder.html#method.timeout">timeout facilities</a> would come in handy, to have those
requests eventually fail.</p>
</div>
<div>

<p>Yes! Timeout <em>all the things</em>.</p>
</div>
<p>Told you there was subtlety!</p>
<p>But that&#39;s not even the bug I&#39;m getting at. And that one&#39;s pretty easy to solve:
we can just switch to a non-poisoning <code>Mutex</code>, like <a href="https://lib.rs/crates/parking_lot">parking_lot</a>&#39;s.</p>
<pre><p>Shell session</p><p><code>$ cargo add parking_lot
    Updating &#39;https://github.com/rust-lang/crates.io-index&#39; index
      Adding parking_lot v0.12.0 to dependencies
</code></p></pre>
<p>We just need to swap <code>std::sync::Mutex</code> with <code>parking_lot::Mutex</code>, and remove
a couple <code>.unwrap()</code> calls, since <code>parking_lot::Mutex::unlock()</code> is infallible.</p>
<p>And now, still with the third request panicking while holding the lock, we have:</p>
<pre><p>Shell session</p><p><code>$ oha -n 10 -c 5 http://localhost:3779
(cut)
Status code distribution:
  [200] 9 responses

Error distribution:
  [1] connection closed before message completed
</code></p></pre>
<p>...just the third request failing, exactly what we wanted.</p>
<p>But now... what if we panic <em>in the task itself</em>? In the &#34;in-flight request&#34;?</p>
<p>Let&#39;s panic only the first time, this time:</p>
<pre><p>Rust code</p><p><code>                tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
                    <i>let</i> res = fut<i>.</i>await<i>;</i>

                    <i>if</i> DOOM_COUNTER<i>.</i><i>fetch_add</i><i>(</i><i>1</i>, <i>Ordering</i><i>::</i>SeqCst<i>)</i> == <i>0</i> {
                        <i>panic</i><i>!</i><i>(</i><i>&#34;doom!&#34;</i><i>)</i><i>;</i>
                    }

                    {
                        <i>// only sync code in this block</i>
                        <i>let</i> <i>mut</i> inner = inner<i>.</i><i>lock</i><i>(</i><i>)</i><i>;</i>
                        inner<i>.</i><i>inflight</i> = None<i>;</i>

                        <i>// etc.</i>
                    }
                }<i>)</i><i>;</i>
</code></p></pre>
<p>In <em>that</em> case, we do see the panic in the server logs:</p>
<pre><p>Shell session</p><p><code>The application panicked (crashed).
Message:  doom!
Location: src/cached.rs:118

Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.
Run with RUST_BACKTRACE=full to include source snippets.
</code></p></pre>
<p>But from the outside, all we see is that all requests are stuck.</p>
<p><em>All</em> requests.</p>
<p>Not just the first one, not just for the first five seconds, all
requests, forever.</p>
<p>And that&#39;s what my big bad dayjob incident was about: a bunch of in-flight
requests got in a bad state, and so we&#39;d be forever stuck waiting for them, long
past any chance they&#39;d ever complete, and never starting new requests.</p>
<p>To make matters worse, it wasn&#39;t even a panic! That&#39;s just the example I&#39;ve
chosen to show you the bug. A panic would&#39;ve been caught by the
<a href="https://sentry.io/welcome/">Sentry</a> reporter, and we&#39;d have received an
alert about it.</p>
<p>No, it was a timeout. The task was simply taking longer than a few seconds,
and it was timed out - which in Rust, amounts to just dropping the future.</p>
<p>And with an <a href="https://docs.rs/tokio/latest/tokio/sync/mpsc/index.html">MPSC channel</a>, you can detect that condition: if you hold a
<code>Receiver</code> and all <code>Sender</code>s are dropped, the future returned by <code>recv</code> just
yields <code>None</code> - that would be our cue that something went wrong with the
in-flight request, and we&#39;re at liberty to try again.</p>
<p>But here... there&#39;s no such thing! Because the only way to be able to subscribe
to the result of the in-flight request is to hold a <code>Sender</code>. So there&#39;s two
places holding a <code>Sender</code> for the same channel: the in-flight request <em>and</em>
our mutex-protected state.</p>
<p>If the in-flight request doesn&#39;t run to completion, it never replaces
<code>inner.inflight</code> with <code>None</code>, and the channel is never closed. A bug reminiscent
of the Go channel spaghetti I find everywhere I look. </p>
<div>

<p>So..... how do we solve this?</p>
</div>
<p>Well!</p>
<p>We&#39;d like the channel to close whether the task completes successfully or
panics. Which is to say, we&#39;d like the <code>Sender</code> to drop either way. <em>All</em>
senders for that channel.</p>
<p>We can&#39;t <em>not</em> hold a <code>Sender</code> in our internal state, because that&#39;s how we
subscribe to in-flight tasks (unless we want to go back to the two-level mutex
solution I hinted at above).</p>
<p>But maybe we can hold a reference to <code>Sender</code> that... doesn&#39;t prevent it from
dropping.</p>
<p>A reference that would let us access the <code>Sender</code> <em>if it still exists</em>, if it
hasn&#39;t been dropped by the in-flight request yet.</p>
<p>And that&#39;s exactly what
<a href="https://doc.rust-lang.org/stable/std/sync/struct.Weak.html">Weak</a> lets us do:
it&#39;s a version of
<a href="https://doc.rust-lang.org/stable/std/sync/struct.Arc.html">Arc</a> that holds a
<em>non-owning reference</em> to a <code>T</code>.</p>
<p>That way:</p>
<ul>
<li>The task will hold an <code>Arc&lt;Sender&gt;</code>, and it <em>will</em> drop it whether it panics
or completes successfully</li>
<li>The internal state will hold a <code>Weak&lt;Sender&gt;</code>, which won&#39;t prevent the <code>Sender</code>
from being dropped</li>
</ul>
<p>And to access the <code>Sender</code> from the state (when we want to subscribe to its
result), we simply use <a href="https://doc.rust-lang.org/stable/std/sync/struct.Weak.html#method.upgrade">Weak::upgrade</a>, which returns an <code>Option&lt;Arc&lt;T&gt;&gt;</code>. If we get a <code>Some</code>,
we know the task is still alive. If we get a <code>None</code>, we know it&#39;s not.</p>
<div>

<p>But isn&#39;t that a race condition? Couldn&#39;t we get an <code>Arc&lt;Sender&gt;</code> <em>right before
the task finishes</em>, and then we&#39;d never receive anything?</p>
</div>
<div>

<p>No, because we&#39;re only ever manipulating <code>inner.inflight</code> while holding a lock.</p>
</div>

<p>So, without further ado, here&#39;s my idea of a fix:</p>
<pre><p>Rust code</p><p><code><i>// in `src/cached.rs`</i>

<i>use</i> std<i>::</i>sync<i>::</i>Weak<i>;</i>

<i>struct</i> <i>CachedLastVideoInner</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>last_fetched</i>: <i>Option</i><i>&lt;</i><i>(</i><i>Instant</i>, <i>T</i><i>)</i><i>&gt;</i>,
    <i>// now weak!      👇</i>
    <i>inflight</i>: <i>Option</i><i>&lt;</i><i>Weak</i><i>&lt;</i>broadcast<i>::</i><i>Sender</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>&gt;</i><i>&gt;</i>,
}

<i>impl</i><i>&lt;</i><i>T</i><i>&gt;</i> <i>Cached</i><i>&lt;</i><i>T</i><i>&gt;</i>
<i>where</i>
    <i>T</i>: <i>Clone</i> + <i>Send</i> + <i>Sync</i> + <i>&#39;</i><i>static</i>,
{
    <i>pub</i> async <i>fn</i> <i>get_cached</i><i>&lt;</i><i>F</i>, <i>E</i><i>&gt;</i><i>(</i><i>&amp;</i><i>self</i>, <i>f</i>: <i>F</i><i>)</i> -&gt; <i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i>
    <i>where</i>
        <i>F</i>: <i>FnOnce</i><i>(</i><i>)</i> -&gt; <i>BoxFut</i><i>&lt;</i><i>&#39;</i><i>static</i>, <i>Result</i><i>&lt;</i><i>T</i>, <i>E</i><i>&gt;</i><i>&gt;</i>,
        <i>E</i>: std<i>::</i>fmt<i>::</i><i>Display</i> + <i>&#39;</i><i>static</i>,
    {
        <i>let</i> <i>mut</i> rx = {
            <i>let</i> <i>mut</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>lock</i><i>(</i><i>)</i><i>;</i>

            <i>if</i> <i>let</i> Some<i>(</i><i>(</i>fetched_at, value<i>)</i><i>)</i> = inner<i>.</i><i>last_fetched</i><i>.</i><i>as_ref</i><i>(</i><i>)</i> {
                <i>// etc.</i>
            }

            <i>// it&#39;s only in-flight if we can upgrade it to an `Arc`:  👇👇👇</i>
            <i>if</i> <i>let</i> Some<i>(</i>inflight<i>)</i> = inner<i>.</i><i>inflight</i><i>.</i><i>as_ref</i><i>(</i><i>)</i><i>.</i><i>and_then</i><i>(</i><i>Weak</i><i>::</i>upgrade<i>)</i> {
                inflight<i>.</i><i>subscribe</i><i>(</i><i>)</i>
            } <i>else</i> {
                <i>let</i> <i>(</i>tx, rx<i>)</i> = broadcast<i>::</i><i>channel</i><i>::</i><i>&lt;</i><i>Result</i><i>&lt;</i><i>T</i>, <i>CachedError</i><i>&gt;</i><i>&gt;</i><i>(</i><i>1</i><i>)</i><i>;</i>
                <i>// let&#39;s reference-count a single `Sender`:</i>
                <i>let</i> tx = <i>Arc</i><i>::</i><i>new</i><i>(</i>tx<i>)</i><i>;</i>
                <i>// and only store a weak reference in our state:</i>
                <i>//                            👇</i>
                inner<i>.</i><i>inflight</i> = Some<i>(</i><i>Arc</i><i>::</i><i>downgrade</i><i>(</i><i>&amp;</i>tx<i>)</i><i>)</i><i>;</i>
                <i>let</i> inner = <i>self</i><i>.</i><i>inner</i><i>.</i><i>clone</i><i>(</i><i>)</i><i>;</i>

                <i>let</i> fut = <i>f</i><i>(</i><i>)</i><i>;</i>

                tokio<i>::</i><i>spawn</i><i>(</i>async <i>move</i> {
                    <i>let</i> res = fut<i>.</i>await<i>;</i>

                    <i>// still testing that panic...</i>
                    <i>if</i> DOOM_COUNTER<i>.</i><i>fetch_add</i><i>(</i><i>1</i>, <i>Ordering</i><i>::</i>SeqCst<i>)</i> == <i>0</i> {
                        <i>panic</i><i>!</i><i>(</i><i>&#34;doom!&#34;</i><i>)</i><i>;</i>
                    }

                    {
                        <i>// etc.</i>
                    }
                }<i>)</i><i>;</i>
                rx
            }
        }<i>;</i>

        <i>// etc.</i>
    }
}
</code></p></pre>
<p>And let me tell you, I was pretty pleased with myself when I found this pattern.</p>
<p>(That&#39;s <em>not</em> the fix we used at that dayjob: I went with something a lot more
complicated with a <code>Drop</code> impl and everything. This solution is pretty elegant
in comparison).</p>
<p>Let&#39;s take it for a test-drive.</p>
<p>Here&#39;s our first batch of requests:</p>
<pre><p>Shell session</p><p><code>$ oha etc.
Status code distribution:
  [200] 5 responses
  [500] 5 responses
</code></p></pre>
<p>And the second:</p>
<pre><p>Shell session</p><p><code>$ oha etc.
Status code distribution:
  [200] 10 responses
</code></p></pre>
<p>Now, a task dying simply makes room for another one. And we get nice 500 errors
that look like this:</p>
<pre><p>Shell session</p><p><code>$ curl http://localhost:3779          
Internal server error: 
   0: stringified error: in-flight request died
</code></p></pre>
<p>...because it&#39;s the <code>rx.recv()</code> call that fails, and that doesn&#39;t panic, it just
propagates the error all the way up, which gets turned into a 500 by our <code>impl IntoResponse for ReportError</code>.</p>
<p>And that&#39;s all I wanted to show y&#39;all today!</p>
<p>Until next time, take excellent care of yourselves.</p>

</div><div>
  
    
    
      

  <div>
  
    <p>If you liked this article, please support my work on Patreon!</p>
    <p>
      <a href="https://www.patreon.com/bePatron?u=47556">
        <img src="https://fasterthanli.me/img/patreon/mark-white.png"/>
        <span>Become a Patron</span>
      </a>
    </p>
  
</div>


  

  
</div></div>
  </body>
</html>
