<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ashermancinelli.com/csblog/2025-7-20-Ideal-Array-Language.html">Original</a>
    <h1>My Ideal Array Language</h1>
    
    <div id="readability-page-1" class="page"><div>
                        
<p><em>2025-07-20</em></p>
<p>What do I think the ideal array language should look like?</p>
<ul>
<li><a href="#my-ideal-array-language">My Ideal Array Language</a></li>
<li><a href="#why-does-this-matter">Why does this matter?</a></li>
<li><a href="#user-extensible-rank-polymorphism">User-Extensible Rank Polymorphism</a>
<ul>
<li><a href="#user-extensibility-in-mojo">User Extensibility in Mojo</a></li>
</ul>
</li>
<li><a href="#value-semantics-and-automatic-bufferization">Value Semantics and Automatic Bufferization</a>
<ul>
<li><a href="#fortrans-array-semantics">Fortran’s Array Semantics</a></li>
<li><a href="#comparison-with-mlir-types-and-concepts">Comparison with MLIR Types and Concepts</a></li>
<li><a href="#fortran-array-semantics-in-mlir">Fortran Array Semantics in MLIR</a></li>
<li><a href="#aside-dependent-types-in-fortran">Aside: Dependent Types in Fortran</a></li>
</ul>
</li>
<li><a href="#compilation-step">Compilation Step</a>
<ul>
<li><a href="#offline-vs-online-compilation">Offline vs Online Compilation</a></li>
<li><a href="#compiler-transparency-and-inspectability">Compiler Transparency and Inspectability</a>
<ul>
<li><a href="#example-nvhpcs-user-facing-optimization-reporting">Example: NVHPC’s User-Facing Optimization Reporting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#simt-and-automatic-parallelization">SIMT and Automatic Parallelization</a>
<ul>
<li><a href="#simt-vs-simd">SIMT vs SIMD</a></li>
<li><a href="#default-modes-of-parallelism">Default Modes of Parallelism</a></li>
</ul>
</li>
<li><a href="#array-aware-type-system">Array-Aware Type System</a></li>
<li><a href="#syntax">Syntax</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#bonus-comparing-parallel-functional-array-languages">Bonus: <em>Comparing Parallel Functional Array Languages</em></a></li>
</ul>

<p>The fundamental units of computation available to users today are not the same as they were 20 years ago.
When users had at most a few cores on a single CPU, it made complete sense that every program was written with the assumption that it would only run on a single core.</p>
<p>Even in a high-performance computing (HPC) context, the default mode of parallelism was (for a long time) the Message Passing Interface (MPI), which is a <em>descriptive</em> model of multi-core and multi-node parallelism.
Most code was still basically written with the same assumtions: all units of computation were assumed to be uniform.</p>
<p>Hardware has trended towards heterogeneity in several ways:</p>
<ul>
<li>More cores per node</li>
<li>More nodes per system</li>
<li>More kinds of subsystems (GPUs, FPGAs, etc.)</li>
<li>More kinds of computational units on a single subsystem
<ul>
<li>CPUs have lots of vector units and specialized instructions</li>
<li>NVIDIA GPUs have lots of tensor cores specialized for matrix operations</li>
</ul>
</li>
<li>New paradigms at the assembly level
<ul>
<li>Scalable Vector Extensions (SVE) and Scalable Matrix Extensions (SMEs) on Arm</li>
</ul>
</li>
<li>Tight hardware release schedules, meaning less and less time in between changes in hardware and more and more rewrites required for hand-written code at the lowest level</li>
</ul>
<p>The old assumptions do not hold true anymore, and programming languages need to be aware of these changes and able to optimize around them.</p>
<p>Imagine the units of computation available in 2025 as a spectrum from SIMD units, to tensor cores, to CUDA cores, to small power-efficient Arm CPU cores, to large beefy AMD CPUs.
It is easy to imagine this spectrum filling in with more specialized hardware pushing the upper and lower boundaries and filling in the gaps.
One day it might be as natural to share work between nodes as it is between individual SIMD lanes on a single CPU core.
This level of heterogeneity is not something that can be ignored by a programming language or a programming language ecosystem.
I believe languages and compilers that do not consider the trajectory of hardware development will be left behind to some degree.</p>

<p>IMO this is what makes something an array language.
No language can be an array language without rank polymorphism.</p>
<p>Numpy provides <em>some</em> rank polymorphism, but it’s not a first-class <em>language</em> feature.
Numpy also needs to be paired with a JIT compiler to make python a real array language, so NUMBA or another kernel language is required for Python to make the list.
Otherwise, users would not be able to write their own polymorphic kernels (<code>ufunc</code>s).</p>
<p>Similarly, the JAX and XLA compilers provide an array language base, but without a kernel language like Pallas or Triton, it’s not extensible enough for every use case they care about.</p>
<div id="admonition-default" role="note">
<p>A proper array language will have an array language <em>base</em> with the ability to write <em>kernels</em> directly, either by exposing lower-level details the user can opt-in to, or by allowing primitives to be composed in a way the compiler can reason about.</p>
</div>
<h2 id="user-extensibility-in-mojo"><a href="#user-extensibility-in-mojo">User Extensibility in Mojo</a></h2>
<div id="admonition-todo-unfinished-section" role="note" aria-labelledby="admonition-todo-unfinished-section-title">
<div>
<p>TODO: unfinished section</p>
</div>
<div>
<ul>
<li>MLIR primitives exposed in the language</li>
<li>Pushing details out of the compiler and into the language</li>
<li>Helps extensibility; fewer uses of compiler builtins</li>
<li>Standard library contains lots of language features that are usually implemented in the compiler</li>
<li><a href="https://youtu.be/5gPG7SXoBag?si=kLnJhKjxxWo5udp2">recent talk on <em>GPUMODE</em></a></li>
</ul>
</div>
</div>

<p>Most of the major ML frameworks have value semantics for arrays by default, and it gives the compiler much more leeway when it comes to memory.
Not only is manual memory management a huge pain and a source of bugs, if the ownership semantics are not sufficiently represented in the language or the language defaults are not ammenable to optimization, the compiler will have a much harder time generating performant code.</p>
<p>My understanding of the Rust borrow checker is that its purpose is to handle the intersection of manual memory management and strict ownership.
Users choose between value and reference semantics, and the compiler helps you keep track of ownership.
Summarized as <em>“aliasing xor mutability”</em>.</p>
<p><a href="https://bsky.app/profile/steveklabnik.com/post/3lusthgcwcs2r"><em>Thanks Steve, for helping clarify the Rust bits!</em></a></p>
<h2 id="fortrans-array-semantics"><a href="#fortrans-array-semantics">Fortran’s Array Semantics</a></h2>
<p>In a way, value semantics and automatic bufferization of arrays is part of why Fortran compilers are able to generate such performant code.</p>
<p>When you use an array in Fortran, you are not just getting a pointer to a contiguous block of memory.
If you access an array from C FFI, you get access to an <em>array descriptor</em> or a <em>dopevector</em> which contains not only the memory backing the array, but also rich shape, stride, and bounds information.</p>
<p>This is not always how the array is represented in the final program however; because the low level representation of the array is only revealed to the user if they ask for it, the compiler is able to optimize around it in a way that is not possible with C.</p>
<p>In C, the compiler <em>must assume arrays alias each other</em> unless the user provides explicit aliasing information.
In Fortran, it is exactly the opposite: unless the user informs the compiler that they formed a pointer to an array, the compiler can assume that the array has exclusive ownership.
The language rules dictate that function arguments may not alias unless the user explicitly declares them to be aliasing.
This means the compiler can optimize around array operations in a way that is only possible in C with lots of extra effort from the user.</p>
<p>Additionally, Fortran represents rank polymorphism natively.
Just like Numpy universal functions (<code>ufunc</code>s), Fortran has the concept of <code>ELEMENTAL</code> procedures which can be called on arrays of any rank.</p>
<h2 id="comparison-with-mlir-types-and-concepts"><a href="#comparison-with-mlir-types-and-concepts">Comparison with MLIR Types and Concepts</a></h2>
<p><a href="https://mlir.llvm.org/">MLIR</a> is the compiler infrastructure backing many of the major ML frameworks.
I won’t go too deep into MLIR here, but many of the components of MLIR are designed to handle array descriptors and their associated shape, stride, and bounds information in various intermediate representations.</p>
<div id="admonition-intermediate-representations" role="note" aria-labelledby="admonition-intermediate-representations-title">
<div>
<p>Intermediate Representations</p>
</div>
<div>
<p>An intermediate representation (IR) is the language used inside of a compiler to represent the program.
There are often several IRs in a compiler, each with a different purpose and possibly different semantics.</p>
<p>MLIR provides infrastructure for heterogeneous IRs, meaning many different IRs can co-exist in the same program.
These IRs are called dialects, and each dialect defines its own operations and types, and often conforms with semantics defined in the core of MLIR so they can compose with other dialects that are not specific to the project.</p>
</div>
</div>
<p>MLIR has a few array representations, most prominently the <code>memref</code> and <code>tensor</code> types, <code>tensor</code> being the more abstract, unbufferized version of a <code>memref</code>.
The process of <em>bufferizing</em>, or converting ops with tensor semantics to ops with memref semantics (<a href="https://mlir.llvm.org/docs/Bufferization/">described here</a>) is how abstract operations on arrays are converted to a more concrete representation with explicit memory backing.</p>
<p>One might consider the <code>tensor</code> dialect to be a purely functional, un-bufferized array programming language that is primarily <em>internal</em> to the compiler.
In fact, <a href="https://pldb.io/blog/chrisLattner.html">see this quote from Chris Lattner</a>:</p>
<div id="admonition-_chris-lattner_" role="note" aria-labelledby="admonition-_chris-lattner_-title">
<div>
<p><em>Chris Lattner</em></p>
</div>
<div>
<p><em>What languages changed the way you think?</em></p>
<p>I would put in some of the classics like Prolog and APL. APL and Prolog are like a completely different way of looking at problems and thinking about them.</p>
<p>I love that, even though it’s less practical in certain ways. Though all the ML compilers today are basically reinventing APL.</p>
</div>
</div>
<p>Array languages lend themselves to compilation for a few reasons:</p>
<ul>
<li>The lack of bufferization is a great match for compiler optimizations; if the buffers do not exist in the user’s program, the compiler can optimize them and sometimes get rid of them entirely.</li>
<li>Functional array langauges are a closer match to the internal representation of array programs in many compilers, particularly MLIR-based ones.
<ul>
<li>The buffers are left entirely up to the compiler</li>
<li>Most modern compilers use <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form for their internal representation</a>, meaning every “value” (virtual register) is assigned to exactly once.</li>
<li>For example, to set an element in <code>tensor</code>-land, one must create an entirely new tensor. Which operations actually result in a new buffer being created is left up to the compiler.</li>
</ul>
</li>
</ul>
<p>Take this example from the MLIR docs:</p>
<pre><code>%r = tensor.insert %f into %t[%idx] : tensor&lt;5xf32&gt;
</code></pre>
<p>The the tensor with <code>%f</code> inserted into the <code>%t</code> tensor at index <code>%idx</code> is <code>%r</code>: an entirely new tensor.
Compare this with the bufferized version in the <code>memref</code> dialect:</p>
<pre><code>memref.store %f, %t[%idx] : memref&lt;5xf32&gt;
</code></pre>
<p>The <code>memref.store</code> operation has <em>side effects</em> and the backing memory is modified in place.
This is a lower-level representation, and typically harder to reason about.
The compiler may have to consider if the memory escaped the current scope before modifying it in place, for example.
The higher-level <code>tensor</code> dialect is much closer to function array programming languages, so a functional array language is a great match for an optimizing compiler.</p>
<!-- ~~~admonish todo
- lattner apl mlir https://pldb.io/chrisLattner.html
- An interview with Chris Lattner
- Why compilers lend themselves to functional? Bc ssa format, can only deal with values, but then have special language for loads/stores, sorta like refs in ocaml where you got special language for interacting with memory in that way
- Why pointer chasing so much more expensive?
~~~ -->
<h2 id="fortran-array-semantics-in-mlir"><a href="#fortran-array-semantics-in-mlir">Fortran Array Semantics in MLIR</a></h2>
<p>The LLVM Flang Fortran compiler was one of the first users of MLIR in the world outside of ML frameworks.
Flang has two primary MLIR dialects for representing Fortran programs: <code>hlfir</code> and <code>fir</code>, or high-level Fortran intermediate representation and Fortran intermediate representation, respectively.</p>
<p><code>hlfir</code> initially represents Fortran programs that are <em>not bufferized</em>, meaning the compiler can optimize around array operations without considering the details of memory management, except when required by the user’s program.</p>
<p>Take this Fortran program:</p>
<pre><code>subroutine axpy(a, b, c, n)
  implicit none
  integer, intent(in)    :: n
  real,    intent(in)    :: a(n,n), b(n,n)
  real,    intent(inout)   :: c(n,n)
  c = a * b + c
end subroutine
</code></pre>
<p><a href="https://godbolt.org/z/sMWvMoo5M">At this godbolt link</a>, we can see the IR for this program after every compiler pass in the bottom-right panel, which is MLIR in the <code>hlfir</code> and <code>fir</code> dialects.</p>
<p>A savy reader might notice that the <code>hlfir</code> dialect is not bufferized, meaning the memory backing the arrays is not represented in the IR.
The language semantics give the compiler this freedom.</p>
<p>After each pass, the IR is dumped with this message: <code>IR Dump After &lt;pass name&gt;</code>.
If you search for <code>IR Dump After BufferizeHLFIR</code>, you can see the IR after the compiler pass that introduces memory to back a temporary array used to calculate the result.</p>
<p>If you then turn the optimization level up to <code>-O3</code> by changing the compiler flags in the top-right, you can search for the pass <code>OptimizedBufferization</code> which leverages the language semantics to reduce and reuse memory in the program, and you’ll notice that the temporary array is no longer present in the IR.</p>
<p>An ideal array language should be able to represent this kind of dynamic shape information in the type system and leave space for the compiler to perform these sorts of optimizations.</p>
<h2 id="aside-dependent-types-in-fortran"><a href="#aside-dependent-types-in-fortran">Aside: Dependent Types in Fortran</a></h2>
<p>You may have also noticed that the shapes of the matrices passed to the function are dynamic - the parameter <code>n</code> is used to determine the shape of the arrays.</p>
<p>In the IR dumps, you can see that the shapes are used to determine the types of the arrays <em>at runtime</em>; the types of the arrays depends on the parameter passed into the function.</p>
<p>This is represented in the IR like this:</p>
<pre><code>func.func @_QPaxpy(
    %arg0: !fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt; {fir.bindc_name = &#34;a&#34;}, // ...
    ) {
    // ...
    %12 = fir.shape %6, %11 : (index, index) -&gt; !fir.shape&lt;2&gt;
    %13:2 = hlfir.declare %arg0(%12)
                {fortran_attrs = #fir.var_attrs&lt;intent_in&gt;, uniq_name = &#34;_QFaxpyEa&#34;}
                : (!fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt;, !fir.shape&lt;2&gt;, !fir.dscope)
                -&gt; (!fir.box&lt;!fir.array&lt;?x?xf32&gt;&gt;, !fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt;)
</code></pre>

<p>Whether offline or online compilation, there needs to be a compilation step.
Part of the beauty of array languages is the language semantics, but the real power comes from the <em>ability to optimize</em> around those semantics.</p>
<p>If a user adds two arrays together, it’s imperative that a compiler is able to see the high-level information in the user’s program and optimize around it.</p>
<h2 id="offline-vs-online-compilation"><a href="#offline-vs-online-compilation">Offline vs Online Compilation</a></h2>
<p>One might argue for either offline compilation (like Fortran, with an explicit compilation step) or online compilation (like Python, where the compiler is invoked at runtime).
For workloads with very heavy compute, it is likely that the process driving the computation can outpace the hardware, meaning it is not very costly to have the compiler invoked while the program is running.</p>
<p>It can be quite a downside to have the compiler on the hotpath, especially for smaller programs where it might become a bottleneck.
Compilers built for offline compilation can often get away with suboptimal performance.
As long as users can lauch <code>make -j</code> and get their program back after grabbing a coffee, it’s not usually a big deal.
Online compilation introduces an entirely new set of challanges, but the lower barrier to entry for users may be worth it.</p>
<p>All major ML frameworks driven by Python make this tradeoff, for example.</p>
<!-- ```admonish todo
- Offline v online compilation. As long as you can drive your heavy units of compute, it doesnt matter which model you use, but you do get the downside of putting the compiler on the hotpath with online compilation. Can sorta get away with a lot in offline. Users will make and forget unless its egregious.
``` -->
<div id="admonition-todo" role="note" aria-labelledby="admonition-todo-title">
<div>
<p>Todo</p>
</div>
<div>
<p>TODO: discuss <a href="https://youtu.be/-6no6N3i9Tg?si=QTDIhNiLRbNhDcTS">Marshall Lochbaum’s <em>Dyalog ’18: The Interpretive Advantage</em> talk.</a></p>
</div>
</div>
<h2 id="compiler-transparency-and-inspectability"><a href="#compiler-transparency-and-inspectability">Compiler Transparency and Inspectability</a></h2>
<p>Compiler optimizations are notoriously unreliable.
If there were a library that was as unreliable and opaque as most compilers, I do not believe users would be willing to adopt it.
In addition, most compiler reporting is built for compiler engineers, not users.
For a user to have any understanding of why their program is slow, they need to be able to understand the optimizations that the compiler is <em>not</em> performing, and be able to inspect compiler logs.</p>
<p>A good example of this is the LLVM remarks framework - I find this framework indispensible for debugging performance issues, especially when paired with a profiling tool like linux <code>perf</code>.
Pairing up the hot paths in the profile with the remarks from the compiler gives a good indication of what the compiler is <em>not</em> optimizing and why - but again this is built for compiler engineers, not users.</p>
<p>If a user finds that their C program is slow, they might look at Clang’s optimization remarks and find that a function in their hot loop was not inlined because of the cost of the stack space taken up by the a function in the inner loop, or that dependence analysis failed because the user did not provide enough aliasing information to the compiler.
Even if they manage to dump the logs and use LLVM’s remarks-to-html tool and generate a readable report of their program, they may still have problems finding actionable information in that report.
<em><strong>User-facing optimization reports and hints are a must.</strong></em></p>
<h3 id="example-nvhpcs-user-facing-optimization-reporting"><a href="#example-nvhpcs-user-facing-optimization-reporting">Example: NVHPC’s User-Facing Optimization Reporting</a></h3>
<p>This is one of my favorite features of the NVHPC compilers - they all have a user-facing optimization reporting framework.
Adding <code>-Minfo=all</code> and <code>-Mneginfo=all</code> to the command line gives a detailed report of the optimizations that the compiler is performing, optimizations that were missed, and why.</p>
<p><a href="https://godbolt.org/z/b4ePMK7zo">Take this C code for example</a>:</p>
<pre><code>void add_float_arrays(const float *a,
                      const float *b,
                            float *c,
                      size_t n)
{
    for (size_t i = 0; i &lt; n; ++i) {
        c[i] = a[i] + b[i];
    }
}

// -Minfo output:
// add_float_arrays:
//       8, Loop versioned for possible aliasing
//          Generated vector simd code for the loop
//          Vectorized loop was interleaved
//          Loop unrolled 4 times
</code></pre>
<p>It doesn’t take too savy of a user to see the <code>Loop versioned for possible aliasing</code> remark and wonder <em>“Well, how do I tell the compiler that these arrays are not aliasing?”</em></p>
<p>Of course, annotating the arrays with <code>restrict</code> gives the compiler this information:</p>
<pre><code>void add_float_arrays(const float *restrict a,
                      const float *restrict b,
                            float *restrict c,
                      size_t                n)
{
    for (size_t i = 0; i &lt; n; ++i) {
        c[i] = a[i] + b[i];
    }
}

// -Minfo output:
// add_float_arrays_restrict:
//      20, Generated vector simd code for the loop
//          Vectorized loop was interleaved
</code></pre>
<p>Of course, the language semantics should be enough to tell the compiler that arrays in a function like this do not alias, but this is an example of what friendly user-facing compiler reporting looks like, in my opinion.</p>

<h2 id="simt-vs-simd"><a href="#simt-vs-simd">SIMT vs SIMD</a></h2>
<p>SIMT is a programming model that allows for parallel execution of the same instruction on multiple threads.
Users typically write a function which recieves a thread identifier, performs operations on data, and writes to an output parameter.
It is nearly impossible to <em>not</em> achieve parallelism with SIMT; once you have described your function in this way, the compiler has to do not other work in order to achieve <strong>parallelism</strong>.
SIMT kernels often operate in <em>lockstep</em>, meaning that every instruction in a SIMT kernel is executed <em>by every thread</em>, but instructions in a thread that is not <em>active</em> are not committed to memory.</p>
<p>In this example, <em>every thread</em> executes <em>both</em> the <code>if</code> and the <code>else</code> branches, but only threads that are active in either region will actually write to <code>pointer</code>.</p>
<pre><code>if thread_id &lt; 2:           # [1, 1, 1, 1] - all threads active
    pointer[thread_id] = 1  # [1, 1, 0, 0] - 2 active threads
else:
    pointer[thread_id] = 2  # [0, 0, 1, 1] - 2 active threads
</code></pre>
<p>So, the user may write a <em>divergent</em> (meaning lots of <code>if</code>/<code>else</code> branches that differ between threads) or otherwise suboptimal program, but they do not have to worry about whether parallelism was achieved or not.</p>
<p>SIMD is a programming model that allows for parallel execution of the <em>S</em>ame <em>I</em>nstruction on <em>M</em>ultiple <em>D</em>ata elements.</p>
<p>Unless users achieve SIMD by writing platform-specific intrinsics directly by specifying they want to use a particular assembly instruction to add two vectors of 8 32-bit floats together, they are relying on the compiler to generate the SIMD code for them.</p>
<p>The lack of trust in compiler to generate near-optimal SIMD code was a major hurdle to adoption.
Users savy enough to write their own assembly were always able to take advantage of the latest hardware, but this basically necessitates rewriting their code each time they want to leverage a new hardware target.</p>
<p>I believe SIMT was a part of the success of the CUDA programming model in part because of how reliably it achieves parallelism (<a href="https://youtu.be/GmNkYayuaA4?si=7ZDmisps4eHxMgr3">Stephen Jones discusses this in his 2025 GTC talk</a>, <a href="https://www.youtube.com/watch?v=139UPjoq7Kw">and this talk on scaling ML workloads had some interesting points too</a>).
With CUDA, users described their programs in terms of SIMT kernels, functions which execute in parallel.</p>
<p>With that in mind, in my ideal array language, users must be able to <em>opt in to SIMT programming</em>, but <em>achieve</em> SIMT programming through automatic parallelization.</p>
<h2 id="default-modes-of-parallelism"><a href="#default-modes-of-parallelism">Default Modes of Parallelism</a></h2>
<p>In this ideal language, a <em>descriptive</em> paradigm for parallelism should be the default while allowing users to opt in to a more <em>prescriptive</em> paradigm if they desire.
A descriptive model should be the default because it gives the compiler a huge amount of flexibility without putting a large burden on the user.</p>
<p>Users should be able to write SIMT kernels with really specific information about how the compiler should map the code to the hardware, while relying on automatic parallelization for most cases.</p>

<p>The type system should be able to represent the shape, stride, and bounds of an array with automatic type inference.
I think the flexibility of OCaml’s type system would be a nice match.
Type annotations are not needed, but they are available if users or library authors would like to opt in to stricter constraints.
Some Hindley-Milner style type inference giving users the ability to opt in to type declarations while allowing the compiler to infer types and optimizer around array information when it’s available would be ideal.
This may be paired with a high-level LTO compilation system that allows the compiler to perform whole-program optimizations and infer more array information in the type system may allow for more aggressive optimizations.</p>
<p><a href="https://www.youtube.com/watch?v=3Lbs0pJ_OHI">This PLDI 2025 talk on representing array information in the type system</a> was really interesting - I don’t know if it’s my ideal, but it was fun to watch someone explore the space.</p>
<!-- Opt-out features:
- Automatic parallelization
- Automatic bufferization

Opt-in features:
- SIMT parallelism
- Manual memory management -->

<p>I purposefully do not have a strong opinion here.
I think the syntax is too dependent on the target audience and I don’t think my ideal language would succeed without a strong community of users.
That being said, the core algorithms of array programming must be representable in a <em>consistent</em> way.</p>
<p>Take numpy for example.
Coming to the language, one might expect a uniform way to perform generic algorithms like <code>reduce</code> and <code>map</code>.
You end up needing to define your own <code>ufunc</code> and call it like <code>&lt;ufunc&gt;.reduce()</code>.
Contrast this with a language like BQN, where a sum reduction is just <code>+´</code>.
I recognize APL-like languages are not approachable or maintainable for everyone, but the flexibility and uniformity of APL-like languages ought to be considered.</p>

<p>Hardware is getting more and more heterogeneous and strange, and programming languages need to be ready.
I believe that modelling programs as functional, unbufferized array programs is the most effective and flexible way to map a large domain of programs to the hardware of today and tomorrow.
As compiler engineers and folks generally working in the space of programming languages, the onus is on us to build programming languages and programming language facilities that are prepared for the hardware of the future.</p>

<p>After writing this post, my friend and colleague sent me the paper <a href="https://arxiv.org/abs/2505.08906"><em>Comparing Parallel Functional Array Languages: Programming and Performance</em></a>, which I found highly relevant.
These are the thoughts I jotted down while reading it.</p>
<p>I was unsure of this statement about APL:</p>
<div id="admonition-_25-the-apl-language_" role="note" aria-labelledby="admonition-_25-the-apl-language_-title">
<div>
<p><em>2.5. The APL Language</em></p>
</div>
<p>Almost all APL primitives have data parallel semantics making them a natural fit for SIMD architectures.
There are, however, two challenges: APL does not guarantee the associativity and referential transparency required for the safe parallel execution of operations like scan (\ or ⍀);
and, the high-level expression of parallel array operations may not necessarily map effectively to a specific SIMD architecture.</p>
</div>
<p>In my understanding, APL does not allow users to obtain reference to arrays directly, so every object has value semantics, and if the compiler elides buffers that are not needed, the user ought never notice.</p>
<p>I loved section <em>3.2. Array Representation</em>, and tables 1, 2, and 3.
The comparison of type systems in various array languages was extremely informative, and I left wanting to learn more about Futhark.</p>
<p>Section <em>3.3. Parallel Computation Paradigms</em> made note of the restrictions on higher-ordered functions in array programming languages, which I think is helpful to note for usual functional programmers who are perhaps used to higher-ordered functions all over the place.
One must not use higher-ordered functions too liberally in array languages for performance reasons; although functional programming <em>can</em> lend itself to highly performant parallel code, it is not due to thier support for higher-ordered functions.</p>
<p>If I could write another paper with these authors, I would title it <em>The Unreasonable Optimizability of Functional, Unbufferized Array Languages</em>.
I believe it captures the essence of what I care about when it comes to array languages.
While the paper was endlessly fascinating, I am less interested in the syntax of the languages and more interested in the semantics and the optimizations made available to the compiler by virtue of those semantics.
The lines-of-code comparisons were interesting and the ease with which the user interacts with the language is important, but the <em>optimizability</em> of languages which are capable of representing their programs as functional, unbufferized and array-oriented is what I find most compelling.</p>
<p>This is why I am still so interested in Fortran even though it is largely a procedural language - the Flang compiler is able to represent programs in an unbufferized array-oriented SSA intermediate representation, which lends itself so well to optimization.</p>

                    </div></div>
  </body>
</html>
