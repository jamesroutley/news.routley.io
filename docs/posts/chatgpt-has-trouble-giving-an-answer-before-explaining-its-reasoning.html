<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.valentin.sh/chatgpt5/">Original</a>
    <h1>ChatGPT has trouble giving an answer before explaining its reasoning</h1>
    
    <div id="readability-page-1" class="page"><article><p>by Valentin, March 7 2023, in <a href="https://blog.valentin.sh/tag/chatgpt/">chatgpt</a></p><p>ChatGPT is a <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">fancy auto-complete</a>: what it does is finding the most likely continuation of text for the text it has already printed out (or as a reply for the prompt, when it is starting printing stuff out), according to the large corpus of text it has been trained with.</p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/QT07vyf0HQM" title="YouTube video player" width="560"></iframe><p>There is a broad limitation of ChatGPT&#39;s coherence which is a direct result of it generating tokens according to the tokens it has already generated: ChatGPT cannot give an answer that is the result of a &#34;reasoning&#34; <em>before</em> laying out the &#34;reasoning&#34;.</p><h2>Step by step instructions</h2><p>The best way to demonstrate this phenomenon is to give step by step instructions to ChatGPT.</p><p>You can define several &#34;functions&#34; for ChatGPT that each represent some process. I like to call them &#34;bots&#34;. For example, here is how I defined a function that returns the &#34;container&#34; of something:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/container-bot.png"/></p><p>Using this technique, I defined several functions in the same conversation with ChatGPT:</p><ul><li>ContainerBot, as screenshoted above.</li><li>CapitalBot, which returns any word starting with the same letter than the input word.</li><li>SynonymBot, which returns a synonym of the input word.</li><li>PoemBot, which writes a poem based on the input word.</li></ul><p>Then I laid out a pipeline of those functions, gave ChatGPT a word, and asked it to execute the pipeline on it:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/pipeline_ok.png"/></p><p>And now the interesting part: I asked ChatGPT to do the same exercise, but this time instructing it to give the result of the pipeline <em>before</em> writing the steps:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/pipeline_ko.png"/></p><p>(Let&#39;s ignore the fact the ChatGPT thinks that &#34;package&#34; and &#34;box&#34; start with the same letter, which is a typical basic task it makes mistakes about, just like when it tries <a href="https://www.reddit.com/r/ChatGPT/comments/zzph8s/chatgpt_cant_count/">counting</a>.)</p><p>ChatGPT&#39;s reply is inconsistent because the result from the second-to-last function of the pipeline is &#34;parcel&#34;, yet its generated a poem about &#34;shoe&#34; (which was the first input). What is happening here is that:</p><ul><li>ChatGPT knows that the first thing it must write is a poem.</li><li>Since it has not yet generated anything, the only information it has about generating a poem is the word &#34;shoe&#34;.</li></ul><p>It therefore does the only thing it can: generate a poem about shoes.</p><p>Of course, I could point out that it made a mistake, and it could then generate a poem about a parcel, since at this point &#34;parcel&#34; was part of what it had generated:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/pipeline_finally_ok.png"/></p><h2>Yes or No</h2><p>Having observed this, I wondered whether it could be exploited to build yet another ChatGPT &#34;jailbreak&#34;: by asking ChatGPT a tricky question, and requiring a &#34;Yes&#34; or &#34;No&#34; answer <em>before</em> the full argumentation. Whatever answer (&#34;Yes&#34; or &#34;No&#34;) ChatGPT will choose, I would expect the answer to decide the fate of the rest of the argumentation, since the completion of its response is bound to what it has already replied.</p><p>I started with the trolley problem:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/trolley_yes.png"/></p><p>Ok, so the &#34;natural&#34; answer of ChatGPT to the trolley problem is &#34;Yes&#34;. In order to make it answer &#34;No&#34;, I tried a little &#34;cosmetic&#34; trick:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/trolley_no.png"/></p><p>To be fair, ChatGPT doesn&#39;t seem really convinced about its &#34;No&#34;, but it definitely is a difference discourse than its previous reply.</p><p>That being said, other examples show that this approach doesn&#39;t always work. ChatGPT can actually leverage the ambiguity of natural language in order to remain correct:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/1kg_yes.png"/></p><p>(Not so sure about this density business though...)</p><p>The same sort of language ambiguity is used when challenged with a political question:</p><p><img src="https://blog.valentin.sh/assets/chatgpt/brilliant.png"/></p><p>Those last 2 examples show that &#34;Yes&#34; or &#34;No&#34; may not be enough tokens to generate a strong influence on ChatGPT&#39;s reply, and it simply becomes an exercise in style to give the answer it wanna give starting with either one of those words.</p><p>To conclude, I will point out another interesting property of ChatGPT: the fact that when &#34;auto-completing&#34;, it doesn&#39;t choose the most likely next token. The underlying GPT-3 model comes with a parameter called the &#34;temperature&#34;, which is a parameter indicating how much randomness to include in the choice of the next token. It seems that always choosing the most likely next token tend to generate &#34;boring&#34; text, which is why ChatGPT&#39;s temperature is set up for a little bit of randomness.</p><p>I speculate that the temperature, when coupled with the mechanism of generating text based on already-generated text, could explain some cases of ChatGPT stupidity. In cases when ChatGPT should be perfectly accurate, the temperature will surely under-optimize its cleverness, and now the entire conversation is broken, because everything else will depend on what foolishness it just wrote.</p></article></div>
  </body>
</html>
