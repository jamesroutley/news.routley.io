<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.futureofhumanityinstitute.org">Original</a>
    <h1>Future of Humanity Institute shuts down</h1>
    
    <div id="readability-page-1" class="page"><div id="canvas">

    
    

    

    

    <!-- // page image or divider -->
    
      
        
      
    

    <section id="page" role="main" data-content-field="main-content" data-collection-id="660e95bc8f7f8975547c4714" data-collection-id="660e95bc8f7f8975547c4714" data-edit-main-image="Banner">

      <!-- // CATEGORY NAV -->
      

      <div data-type="page" data-updated-on="1713296249798" id="page-660e95bc8f7f8975547c4714"><div><div><div data-block-type="2" id="block-yui_3_17_2_1_1713107673603_21183"><div>

<div>
  <p><strong>(2005–2024)</strong></p><p>Established in 2005, initially for a 3-year period, the Future of Humanity Institute was a multidisciplinary research group at Oxford University.  It was founded by Prof Nick Bostrom and brought together a select set of researchers from disciplines such as philosophy, computer science, mathematics, and economics to study big-picture questions for human civilization, attempting to shield them from ordinary academic pressures and create an organizational culture conducive to creativity and intellectual progress.</p><p>During its 19-year existence, the team at FHI made a series of research contributions that helped change our conversation about the future and contributed to the creation of several new fields and paradigms.  FHI was involved in the germination of a wide range of ideas including existential risk, effective altruism, longtermism, AI alignment, AI governance, global catastrophic risk, grand futures, information hazards, the unilateralist’s curse, and moral uncertainty.  It also did significant work on anthropics, human enhancement ethics, systemic risk modeling, forecasting and prediction markets, the search for extraterrestrial intelligence, and on the attributes and strategic implications of key future technologies.  One major contribution was in showing that it was even possible to do rigorous research on big picture questions about humanity’s future.</p><p>Over time FHI faced increasing administrative headwinds within the Faculty of Philosophy (the Institute’s organizational home).  Starting in 2020, the Faculty imposed a freeze on fundraising and hiring.  In late 2023, the Faculty of Philosophy decided that the contracts of the remaining FHI staff would not be renewed.  On 16 April 2024, the Institute was closed down.</p><p>Over the course of its nineteen years, FHI inspired the emergence of a vibrant ecosystem of organizations where the kinds of questions that FHI investigated can be explored.  FHI alumni will continue to research these questions both within Oxford and at other places around the world.  Topics that once struggled to eke out a precarious existence at the margins of a single philosophy department are now pursued by leading AI labs, government agencies, nonprofits, and specialized academic research centers (with many more in the process of creation).</p>
</div>




















  
  



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1713107673603_13012"><div>

<div>
  <h2>FHI Books</h2><ul data-rte-list="default"><li><p><a href="https://en.wikipedia.org/wiki/Global_Catastrophic_Risks_(book)"><em>Global Catastrophic Risks</em></a>, Nick Bostrom and Milan Ćirković (eds.), 2008.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Human_Enhancement"><em>Human Enhancement</em></a>, Julian Savulescu and Nick Bostrom (eds.), 2009.</p></li><li><p><a href="https://www.amazon.co.uk/Radical-Abundance-Revolution-Nanotechnology-Civilization/dp/1610391136"><em>Radical Abundance: How a Revolution in Nanotechnology Will Change Civilization</em></a>, Eric Drexler, 2013.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><em>Superintelligence: Paths, Dangers, Strategies</em></a>, Nick Bostrom, 2014.</p></li><li><p><a href="https://theprecipice.com"><em>The Precipice: Existential Risk and the Future of Humanity</em></a>, Toby Ord, 2020.</p></li><li><p><a href="https://www.moraluncertainty.com"><em>Moral Uncertainty</em></a>, William MacAskill, Krister Bykvist, and Toby Ord, 2020.</p></li><li><p><a href="https://nickbostrom.com/booklink/deep-utopia"><em>Deep Utopia: Life and Meaning in a Solved World</em></a>, Nick Bostrom, 2024.</p></li></ul>
</div>




















  
  



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1713107673603_11413"><div>

<div>
  <h2>Key FHI Papers</h2><h3>Existential and Catastrophic Risk</h3><ul data-rte-list="default"><li><p><a href="https://nickbostrom.com/existential/risks.pdf">Existential risks: analyzing human extinction scenarios and related hazards</a>, Nick Bostrom, 2002 (pre-FHI).</p></li><li><p><a href="https://nickbostrom.com/papers/astronomical-waste/astronomical-waste.pdf">Astronomical waste: the opportunity cost of delayed technological development</a>, Nick Bostrom, 2003 (pre-FHI).</p></li><li><p><a href="https://arxiv.org/pdf/astro-ph/0512204.pdf">How unlikely is a doomsday catastrophe? </a>Max Tegmark and Nick Bostrom, 2005.</p></li><li><p><a href="https://nickbostrom.com/fut/singleton">What is a singleton?</a> Nick Bostrom, 2005.</p></li><li><p><a href="https://nickbostrom.com/papers/where-are-they/">Where are they? Why I hope the search for extraterrestrial life finds nothing</a>, Nick Bostrom, 2008.</p></li><li><p><a href="http://amirrorclear.net/files/probing-the-improbable.pdf" target="">Probing the improbable: methodological challenges for risks with low probabilities and high stakes</a>. Toby Ord, Rafaela Hillerbrand, and Anders Sandberg, 2010.</p></li><li><p><a href="https://nickbostrom.com/papers/anthropicshadow.pdf">Anthropic shadow: observation selection effects and human extinction risks</a>,   Milan Ćirković, Anders Sandberg, and Nick Bostrom, 2010.</p></li><li><p><a href="https://nickbostrom.com/information-hazards.pdf">Information hazards</a>, Nick Bostrom, 2011.</p></li><li><p><a href="https://existential-risk.com/concept.pdf">Existential risk prevention as global priority</a>, Nick Bostrom, 2013.</p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0016328714001888">How much could refuges help us recover from a global catastrophe? </a>Nick Beckstead, 2015.</p></li><li><p><a href="http://amirrorclear.net/files/existential-risk-and-existential-hope.pdf">Existential Risk and Existential Hope: Definitions</a>, Owen Cotton-Barratt and Toby Ord, 2015.</p></li><li><p><a href="https://nickbostrom.com/papers/unilateralist.pdf">The unilateralist’s curse and the case for a principle of conformity</a>, Nick Bostrom, Thomas Douglas, and Anders Sandberg, 2016.</p></li><li><p><a href="http://amirrorclear.net/files/an-upper-bound-for-the-background-rate-of-human-extinction.pdf" target="">An upper bound for the background rate of human extinction</a>, Andrew Snyder-Beattie, Toby Ord, and Michael Bonsall, 2019.</p></li><li><p><a href="https://nickbostrom.com/papers/vulnerable.pdf" target="">The vulnerable world hypothesis</a>, Nick Bostrom, 2019.</p></li><li><p><a href="https://www.taylorfrancis.com/chapters/edit/10.4324/9781003331384-24/lifespan-civilizations-anders-sandberg">The lifespan of civilizations: do societies “age,” or Is collapse just bad luck? </a>Anders Sandberg, 2023.</p></li></ul><h3>AI Safety</h3><ul data-rte-list="default"><li><p><a href="https://nickbostrom.com/papers/oracle.pdf">Thinking inside the box: controlling and using an oracle AI</a>, Stuart Armstrong, Anders Sandberg, and Nick Bostrom, 2012.</p></li><li><p><a href="https://nickbostrom.com/superintelligentwill.pdf">The superintelligent will: Motivation and instrumental rationality in advanced artificial agents</a>, Nick Bostrom, 2012.</p></li><li><p><a href="https://ora.ox.ac.uk/objects/uuid:17c0e095-4e13-47fc-bace-64ec46134a3f/files/m289fc3fabe51226b4cf6c8a09d0737f8">Safely interruptible agents</a>, Laurent Orseau and Stuart Armstrong, 2016.</p></li><li><p><a href="https://nickbostrom.com/papers/survey.pdf">Future progress in artificial intelligence: a survey of expert opinion</a>, Vincent Müller and Nick Bostrom, 2016.</p></li><li><p><a href="https://agentmodels.org"><em>Modeling agents with probabilistic programs</em></a>, Owain Evans et al., 2017.</p></li><li><p><a href="https://jair.org/index.php/jair/article/view/11222/26431">When will AI exceed human performance? Evidence from AI experts</a>, Katja Grace et al., 2018.</p></li><li><p><a href="https://www.futureofhumanityinstitute.org/s/2019-1.pdf"><em>Reframing superintelligence: comprehensive AI services as general intelligence</em></a>, Eric Drexler, 2019.</p></li><li><p><a href="https://arxiv.org/abs/2110.06674">Truthful AI: developing and governing AI that does not lie</a>, Owain Evans et al., 2021.</p></li></ul><h3>AI Governance</h3><ul data-rte-list="default"><li><p><a href="https://www.futureofhumanityinstitute.org/s/2013-1.pdf" target="">Racing to the precipice: a model of artificial intelligence development</a>, Stuart Armstrong, Carl Shulman, and Nick Bostrom, 2013.</p></li><li><p><a href="https://nickbostrom.com/papers/openness.pdf">Strategic implications of openness in AI development</a>, Nick Bostrom, 2017.</p></li><li><p><a href="https://cdn.governance.ai/GovAI-Research-Agenda.pdf">AI governance: a research agenda</a>, Allan Dafoe, 2018.</p></li><li><p><a href="https://maliciousaireport.com/">The malicious use of artificial intelligence: Forecasting, prevention, and mitigation</a>, Miles Brundage et al., 2018.</p></li><li><p><a href="https://arxiv.org/abs/2012.08347">Beyond privacy trade-offs with structured transparency</a>, Andrew Trask et al., 2020.</p></li><li><p><a href="https://www.governance.ai/research-paper/the-windfall-clause-distributing-the-benefits-of-ai-for-the-common-good">The windfall clause: distributing the benefits of AI for the common good</a>, Cullen O’Keefe et al., 2020.</p></li><li><p><a href="https://www.nature.com/articles/s42256-021-00298-y">Institutionalizing ethics in AI through broader impact requirements</a>, Carina Prunkl et al., 2021.</p></li><li><p><a href="https://www.governance.ai/research-paper/international-control-of-powerful-technology-lessons-from-the-baruch-plan-for-nuclear-weapons#:~:text=Nuclear%20Weapons%20%7C%20GovAI-,International%20Control%20of%20Powerful%20Technology%3A%20Lessons%20from%20the%20Baruch%20Plan,broad%20sharing%20of%20its%20benefits%3F">International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons</a>, Waqar Zaidi and Allan Dafoe, 2021.</p></li><li><p><a href="https://www.governance.ai/research-paper/lessons-atomic-bomb-ord">Lessons from the development of the atomic bomb</a>, Toby Ord. 2022.</p></li></ul><h3>Digital Minds</h3><ul data-rte-list="default"><li><p><a href="https://nickbostrom.com/papers/experience.pdf">Quantity of experience: brain-duplication and degrees of consciousness</a>, Nick Bostrom, 2006.</p></li><li><p><a href="https://nickbostrom.com/propositions.pdf">Propositions concerning digital minds and society</a>, Nick Bostrom and Carl Shulman, 2022</p></li><li><p><a href="https://arxiv.org/abs/2308.08708">Consciousness in artificial intelligence: insights from the science of consciousness</a>, Patrick Butlin et al., 2023.</p></li></ul><h3>Biological Risk</h3><ul data-rte-list="default"><li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/risa.13235">Information hazards in biotechnology</a>, Gregory Lewis et al., 2019.</p></li><li><p><a href="https://www.nature.com/articles/s41467-020-19149-2">The biosecurity benefits of genetic engineering attribution</a>, Gregory Lewis et al., 2020.</p></li><li><p><a href="https://pubmed.ncbi.nlm.nih.gov/35903214/">High-risk human-caused pathogen exposure events from 1975-2016</a>, David Manheim and Gregory Lewis, 2021.</p></li><li><p><a href="https://www.science.org/doi/10.1126/science.abd9338">Inferring the effectiveness of government interventions against COVID-19</a>, Jan Brauner et al. 2021.</p></li></ul><h3>Human Enhancement</h3><ul data-rte-list="default"><li><p><a href="https://nickbostrom.com/fable/dragon.pdf">The fable of the dragon tyrant</a>, Nick Bostrom, 2005.</p></li><li><p><a href="https://nickbostrom.com/ethics/statusquo.pdf">The reversal test</a>, Nick Bostrom and Toby Ord, 2006.</p></li><li><p><a href="https://nickbostrom.com/evolution.pdf">The wisdom of nature: an evolutionary heuristic for human enhancement</a>, Nick Bostrom and Anders Sandberg, 2009.</p></li><li><p><a href="https://nickbostrom.com/superintelligentwill.pdf">The superintelligent will: motivation and instrumental rationality in advanced artificial agents</a>, Nick Bostrom, 2012.</p></li><li><p><a href="https://nickbostrom.com/papers/embryo.pdf">Embryo selection for cognitive enhancement: curiosity or game-changer?</a> Carl Shulman and Nick Bostrom, 2014.</p></li></ul><h3>Moral Uncertainty</h3><ul data-rte-list="default"><li><p><a href="http://amirrorclear.net/files/statistical-normalization-methods.pdf" target="">Statistical normalization methods in interpersonal and intertheoretic comparisons</a>. William MacAskill, Owen Cotton-Barratt, and Toby Ord, 2020.</p></li><li><p><a href="http://amirrorclear.net/files/why-maximize-expected-choiceworthiness.pdf" target="">Why maximize expected choice-worthiness?</a> William MacAskill and Toby Ord, 2020.</p></li></ul><h3>Effective Altruism</h3><ul data-rte-list="default"><li><p><a href="http://amirrorclear.net/files/the-moral-imperative-towards-cost-effectiveness-in-global-health.pdf">The moral imperative towards cost-effectiveness in global health</a>, Toby Ord, 2013.</p></li><li><p><a href="http://amirrorclear.net/files/global-poverty-and-the-demands-of-morality.pdf" target="">Global poverty and the demands of morality</a>, Toby Ord, 2014. </p></li><li><p><a href="http://amirrorclear.net/files/moral-trade.pdf">Moral trade</a>, Toby Ord, 2015.</p></li></ul><h3>Grand Futures</h3><ul data-rte-list="default"><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148">Eternity in six hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox</a>, Stuart Armstrong and Anders Sandberg, 2013.</p></li><li><p><a href="https://arxiv.org/pdf/1705.03394.pdf">That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi&#39;s paradox</a>. Anders Sandberg, Stuart Armstrong, and Milan Ćirković, 2016.</p></li><li><p><a href="https://arxiv.org/abs/1806.02404">Dissolving the Fermi paradox</a>, Anders Sandberg, Eric Drexler, and Toby Ord, 2018.</p></li><li><p><a href="https://arxiv.org/abs/2104.01191">The edges of our universe</a>. Toby Ord, 2021.</p></li><li><p><a href="https://www.liebertpub.com/doi/full/10.1089/ast.2019.2149">The timing of evolutionary transitions suggests intelligent life is rare</a>, Andrew Snyder-Beattie et al., 2021.</p></li></ul><h3>Longtermism</h3><ul data-rte-list="default"><li><p><a href="http://amirrorclear.net/files/shaping-humanity&#39;s-longterm-trajectory.pdf" target="">Shaping humanity’s longterm trajectory</a>. Toby Ord, 2023.</p></li><li><p><a href="https://arxiv.org/abs/2308.09045">The Lindy effect</a>, Toby Ord, 2023.</p></li></ul><h3>Ethical Theory</h3><ul data-rte-list="default"><li><p><a href="http://amirrorclear.net/files/beyond-action.pdf" target=""><em>Beyond action: applying consequentialism to decision making and motivation</em></a><em>,</em> Toby Ord, 2009.</p></li><li><p><a href="https://nickbostrom.com/papers/pascal.pdf">Pascal’s mugging</a>, Nick Bostrom, 2009.</p></li><li><p><a href="https://nickbostrom.com/ethics/infinite.pdf">Infinite ethics</a>, Nick Bostrom, 2011.</p></li></ul><h3>Other Topics</h3><ul data-rte-list="default"><li><p><a href="https://simulation-argument.com">Are you living In a computer simulation?</a>, Nick Bostrom, 2003 (pre-FHI).</p></li><li><p><a href="https://www.futureofhumanityinstitute.org/s/2008-3.pdf" target=""><em>Whole brain emulation: a roadmap</em></a>, Anders Sandberg and Nick Bostrom, 2008.</p></li><li><p><a href="https://nickbostrom.com/lectures/crucial-considerations/">Crucial considerations and wise philanthropy</a>, Nick Bostrom, 2014.</p></li></ul>
</div>




















  
  



</div></div></div></div></div>

    </section>

    

    <!-- <div class="page-divider bottom-divider"></div> -->

    

    

  </div></div>
  </body>
</html>
