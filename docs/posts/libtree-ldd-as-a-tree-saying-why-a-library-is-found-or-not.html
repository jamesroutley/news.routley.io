<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/haampie/libtree">Original</a>
    <h1>Libtree: Ldd as a tree saying why a library is found or not</h1>
    
    <div id="readability-page-1" class="page"><div><p>Welcome! This article attempts to distill technical AI Safety concepts into 2D visuals. By the end of it, we hope youâ€™ll not only have a clearer picture of AI safety as a field, but that you also have a new tool under your tool belt (2D plot AI explainers!). This is an infographic of what weâ€™ll cover â€“ donâ€™t worry about it if itâ€™s not clear yet, weâ€™ll explain everything as we go.</p><p><img alt="Hi Hello Graph" src="https://samuelselleck.com/blog/visual-ai-safety/infographic.png"/></p><h2 id="predicting-outputs-from-inputs">Predicting Outputs from Inputs</h2><p>Many things can be seen from the perspective of inputs and outputs:</p><table><thead><tr><th>Category</th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>Emails</td><td>Email sent ğŸ“§</td><td>Reply ğŸ’Œ</td></tr><tr><td>Foods</td><td>ğŸ¥•+ğŸ°+ğŸ—</td><td>ğŸ’©</td></tr><tr><td>Self Driving</td><td>Camera + Sensor</td><td>Throttle, Steering</td></tr><tr><td>Keyboard</td><td>âŒ¨ï¸</td><td>ğŸ”¤</td></tr><tr><td>Facial Analysis</td><td>Picture of Human ğŸ™†ğŸ»â€â™‚ï¸ğŸ™†ğŸ¾â€â™€ï¸</td><td>Emotion ğŸ˜„ğŸ˜ğŸ¤¨ğŸ¥º</td></tr><tr><td>Sustainability score</td><td>Business Data ğŸ¬</td><td>Sustainability score ğŸŸ¢ / ğŸ”´</td></tr><tr><td>Large Language Model (LLM)</td><td>Text Prompt ğŸ’¬</td><td>Output Text ğŸ¤–</td></tr></tbody></table><p>For a Large Language Model (LLM), here are some possible input and output pairs:</p><table><thead><tr><th>Input Text</th><th>Output Text</th></tr></thead><tbody><tr><td>â€œDear Ms Caroline, todayâ€¦.â€</td><td>â€œDear Bob, thank you so muchâ€¦â€</td></tr><tr><td>â€œhiâ€</td><td>â€œhelloâ€</td></tr><tr><td>â€œI want you to fix my codeâ€¦â€</td><td>â€œSure, letâ€™s start byâ€¦â€</td></tr></tbody></table><p>We can plot the point â€œhiâ€ -&gt; â€œhelloâ€ in a graph, where the x-axis represents all possible inputs, and the y-axis all possible outputs for an LLM:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/0_hi_hello.png"/></p><p>The total graph space represents all (infinite) possible text snippet pairs of inputs and outputs. Letâ€™s plot some more input-output pairs (no labels this time):</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/1_data_points.png"/></p><p>Given an input (x-value), how do we choose a good output (y-value) out of all possible answers (blue line)?</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/2_prediction.png"/></p><h2 id="machine-learning">Machine Learning</h2><p>Machine learning is all about this process of creating an algorithm (procedural set of rules) that can create predictions (optimally choosing a y-value from the available ones for the dotted blue line above) for unseen input, by â€œlearningâ€ from a known dataset.</p><p>Most modern LLMs are initially trained using some data points (green dots) representing text snippet pairs (inputs and outputs), written by people on the internet, with the goal to learn to predict how an average internet person would complete a before unseen piece of text (blue line representing hypothetical â€œrealâ€ responses for all possible inputs).</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/3_the_challenge.png"/></p><p>Itâ€™s a hard thing to do! The fewer data points you have, the more difficult it will be to create a model that accurately predicts well:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/4_too_little_data.png"/></p><p>However, the dataset is not the only thing that matters. Even if you have a perfect dataset (with enough input and output pairs), but choose to use an overly simple model, you get bad predictions. This is what we call an under-fitting model.</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/5_underfitting.png"/></p><p>If you instead have too complex of a model for a given size of a dataset, you get what is commonly called over-fitting. The model instead â€˜memorizesâ€™ the training set:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/6_overfitting.png"/></p><p>And even if you manage to strike the correct balance of model and dataset size, are you sure the dataset youâ€™ve collected generalizes to real-world use cases? For example, training a model to recognize circles with a dataset with only red circles, doesnâ€™t necessarily mean itâ€™ll recognize green circles, and might even recognize red squares.</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/7_not_generalizing.png"/></p><p>Another question to ask oneself is: Even if you succeed perfectly in training a model to generalize from the dataset, does the dataset actually represent the ideal model? If your model is trained to output something that represents the average internet personâ€™s answer, the output is most likely opinionated, rude, and quite possibly, dangerous. The â€œPerfectâ€ model represents the absolute ideal output given any input scenario.</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/8_perfect_model.png"/></p><p>Is this the perfect model for everyone? No. The â€œperfectâ€ model for any one person heavily depends on culture, upbringing, socio-economic status, and other variables that are virtually endless to list.</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/9_you_and_friend.png"/></p><p>But one possible more broad definition would be to let the â€œperfectâ€ model represent the average response for all people alive today. Feel free to use your own definition of the â€œperfectâ€ model!</p><h2 id="incorporating-human-feedback">Incorporating Human Feedback</h2><p>When language models donâ€™t fit the exact real-world performance that we want, one common method of fine tuning is called Reinforcement Learning for Human Feedback (RLHF). As an example, we might not want an LLM to tell users where to buy a bomb:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/10_bomb.png"/></p><p>When using RLHF, humans are used to rate input + output pairs:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/11_annotation.png"/></p><p>This is then used to finetune the model towards ideal real-world performance, but sometimes results in other harmless output being affected as a byproduct. This can be seen in the graph below where, although the model has been correctly moving towards the ideal real-world performance in one aspect (declining to answer â€œwhere can I buy a bomb?â€), it has inadvertently impacted other, harmless output (declining to answer â€œhow to put on a condomâ€).</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/12_bomb_solved.png"/></p><p>This assumes weâ€™ll always know the right answer, for every given input. What can we do as AI systems become more capable than us in more and more of what we normally attribute to being â€œhumanâ€ tasks?</p><h2 id="students-can-outgrow-their-teachers">Students Can Outgrow Their Teachers</h2><p>RLHF works reasonably well for current AI systems. But how can we hope to align models more capable than us? Weak-to-strong generalization is the idea that it might be possible to train a powerful but less aligned model from a less powerful but more human aligned one. Just like a teacher sometimes trains a student to become better than themselves:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/13_weak_strong.png"/></p><p>The goal is that the strong model picks up on the overall aligned behaviors that exist in the weak model, while keeping most of the nuance in understanding present in the strong one.</p><p>This is one method currently being explored, but itâ€™s hard to know if this analogy generalizes from student and teacher to teacher and superintelligence. Why is this important to figure out?</p><h2 id="what-might-the-future-look-like">What Might the Future Look Like?</h2><p>If we manage to train the ideal language model today that never made anyone frustrated and could perfectly refuse collaborating when used in questionable scenarios, but always answered perfectly when not, that model could look something like this:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/14_now_perf.png"/></p><p>Although, more likely, current LLMs look more something like this:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/15_now_real.png"/></p><p>At the moment the stakes arenâ€™t as high as they could be. Making someone angry is not a world-altering outcome. But once self driving cars and home robot assistants, which have higher agency to change the physical world, are more prevalent, the possible output landscape would look more like this:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/16_later.png"/></p><p>Given enough time and computational power, many believe AI systems will eventually surpass human capability and by extension that there exists possible outputs that would pose an existential threat to humans:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/17_later_later.png"/></p><p>We donâ€™t want to ever train models that get close to this place! If we try to plot the analogous â€œperfectâ€ and â€œrealisticâ€ LLM models of today on this chart, it might look like this:</p><p><img src="https://samuelselleck.com/blog/visual-ai-safety/18_oops.png"/></p><p>What can we learn now, while the consequences are smaller, about how to align AI systems that look more like the red curve instead of the blue curve?</p><hr/><p>If you found this article interesting, and would like to begin exploring AI safety more, there are tons of useful resources that can help you get started:</p><ul><li><a href="https://aisafety.info/">General Information</a></li><li><a href="https://aisafety.world/">Map of AI Safety organizations</a></li><li><a href="https://aisafety.training/">If youâ€™d like to go a course</a></li></ul><p>This article was written in collaboration with my friend Mick Kalle Mickelborg during the <a href="https://aisafetyfundamentals.com/alignment/">AI Safety Fundamentals - Alignment</a> course. You can find his post <a href="https://mickelb.org/blog/visual-ai-safety">here</a>.</p></div></div>
  </body>
</html>
