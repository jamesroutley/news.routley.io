<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.indiehackers.com/post/claude-just-slashed-the-cost-of-building-ai-applications-lQHHc1S9NLLiPLJeyUWk">Original</a>
    <h1>Claude just slashed the cost of building AI applications</h1>
    
    <div id="readability-page-1" class="page"><div>
          <div>
            <p><img src="https://i.imgur.com/j0WcQBx.png" alt="ai agent person"/></p><p>Imagine you&#39;re creating an AI SaaS/app.</p><p>You&#39;re heavily dependent on OpenAI, Claude, or Google&#39;s API, and one of your key competitive advantages is your advanced prompt.</p><p>However, your advanced prompt is quite lengthy and includes many examples, which help the AI generate a useful output.</p><p>This can quickly drive up your API costs since you&#39;re constantly sending the same long input. Providers like OpenAI charge you based on <a target="_blank" href="https://openai.com/api/pricing/">input tokens</a>, meaning the more words you send for the AI to process, the more it costs.</p><p>Things can get expensive fast.</p><h2>Welcome to the world of prompt caching</h2><p>ClaudeAI, one of the top three AI providers in the world (alongside Google and OpenAI), <a target="_blank" href="https://blog.getbind.co/2024/08/15/what-is-claude-prompt-caching-how-does-it-work/">has recently </a>introduced a feature called Prompt Caching.</p><p><a target="_blank" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching">This feature</a> essentially allows you to &#34;reuse text&#34; across multiple prompts. </p><p>With this feature, you can &#34;cache&#34; the examples and only send the remaining 10% as the actual prompt.</p><p>The impact of this is dramatic. <strong>Up to 90% reduction in input API costs</strong>.</p><p>This also means that,<strong> as a developer, you can either lower your pricing or increase your profit margins for your SaaS/app.</strong></p><h2>What is prompt caching useful for?</h2><p>According to the creators of Claude, this is useful if you provide:</p><ul><li><p><strong>AI assistants, </strong>where you expect multiple users to enter the same prompt.</p></li><li><p><strong>Code generation </strong>where you need to reuse the same prompt or have multiple users working with the same template.</p></li><li><p><strong>Code reviews: </strong>When asking AI to review long chunks of code, you <a target="_blank" href="https://www.reddit.com/r/ClaudeAI/comments/1esto2i/anthropic_just_released_prompt_caching_making/">don&#39;t have to send </a>the same code over and over again. This can save both time and money</p></li><li><p><strong>Processing large documents</strong>: For instance, if you feed the AI a novel and want to ask questions about it,.</p></li><li><p><strong>Any search tool:</strong> You can input data from files and ask questions, etc.</p></li><li><p><strong>Any prompt with plenty of examples:</strong> You no longer need to worry about optimizing your prompt for length. You can focus on being thorough and getting the best results possible.</p></li></ul><p>Will OpenAI follow suit and release a similar feature soon?</p>
          </div>
        </div></div>
  </body>
</html>
