<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/">Original</a>
    <h1>Why I self host my servers and what I&#39;ve recently learned</h1>
    
    <div id="readability-page-1" class="page"><div><h2 id="introduction">Introduction</h2><p>I self host everything but email. I wrote about this <a href="https://chollinger.com/blog/2019/04/building-a-home-server/">here</a>, <a href="https://chollinger.com/blog/2023/04/migrating-a-home-server-to-proxmox-truenas-and-zfs-or-how-to-make-your-home-network-really-complicated-for-no-good-reason/">here</a>, or <a href="https://chollinger.com/blog/2023/10/moving-a-proxmox-host-with-a-sas-hba-as-pci-passthrough-for-zfs--truenas/">here</a>.</p><p>As a summary, at home, I run a 3 node Proxmox cluster with several services, powering a home network with Mikrotik router, Mikrotik switches, and UniFi WiFi, as well as an external VPS.</p><p>This article is about two things: Why I still bother and what it has recently taught me. Think of it as a brief retrospective and an encouragement for readers to go down the same rabbit hole.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825124525084.png" alt="heimdall.lan"/><figcaption><p>heimdall.lan
<a href="https://www.hbo.com/curb-your-enthusiasm"><small>[Background from HBO promo material]</small></a></p></figcaption></figure><h2 id="my-services">My Services</h2><p>I self host:</p><ul><li><code>PiHole</code> as DNS resolver (redundant)</li><li>RouterOS (probably stretching the ‚Äúself hosting‚Äù a bit, but it does DHCP, VLANs, Firewalls, DNS routing and such)</li><li><code>UniFi controller</code> as a WiFi controller</li><li><code>heimdall</code> as a landing page</li><li><code>TrueNAS</code> as a file server (redundant)</li><li><code>gitea</code> as a local git server</li><li><code>wiki.js</code> for general knowledge storing</li><li><code>VS Code</code> as a browser based editor</li><li>A <code>Ubuntu</code> dev VM for general code shenanigans (Debian otherwise)</li><li><code>mariadb</code> for database needs</li><li><code>redis</code> for forgetful database needs</li><li><code>InfluxDB</code> for specific database needs (aka ‚Äúprojects I mean to get back into‚Äù)</li><li><code>LibreNMS</code> as a network manager and monitoring suite</li><li><code>Calibre Web</code> for E-Books and papers</li><li><code>Komga</code> for comics</li><li><code>Jellyfin</code> for general media</li><li><code>Homebridge</code> for internet of sh*t (tm) devices I don‚Äôt need</li></ul><p>In addition to that, I <em>also</em> have had an external VPS for 10+ years, which hosts:</p><ul><li><code>nginx</code> for my website and this blog</li><li><code>firefoxsync-rs</code>, Firefox sync to sync Firefox synchronously (the new <code>rust</code> version, too!)</li><li><code>Nextcloud</code> to host files, calendar, and contacts</li></ul><p>As you can imagine, this can be quite a bit of work. I, despite common sentiment, also have a live outside of computers. Matter of fact, I‚Äôd rather spend my weekends outside (provided it‚Äôs not 90 degrees). Which sometimes conflicts with hosting all this.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825135445550.png" alt="For those unfamiliar: Outdoors! (Sample image from our garden)"/><figcaption><p>For those unfamiliar: Outdoors! (Sample image from our garden)
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>So‚Ä¶ why bother?</p><h2 id="why-i-self-host">Why I self host</h2><p>Most of this article is not purely about that question, but I dislike clickbait, so I‚Äôll actually answer the question from the title: Two reasons.</p><p>First of all, I like to be independent - or at least, as much as I can. Same reason we have backup power, why I know how to bake bread, preserve food, and generally LARP as a grandmother desperate to feed her 12 grandchildren until they are no longer capable of self propelled movement. It makes me <em>reasonably</em> independent of whatever evil scheme your local <code>$MEGA_CORP</code> is up to these days (<em>hint</em>: it‚Äôs probably a subscription).</p><p>It‚Äôs basically the Linux and Firefox argument - competition is good, and freedom is too.</p><p>If that‚Äôs too abstract for you, and what this article is <em>really</em> about, is the fact that it teaches you a lot and that is a truth I hold to be self-evident: Learning things is good &amp; useful.</p><p>Turns out, forcing yourself to either do something you don‚Äôt do every day, or to get better at something you do occasionally, or to simply learn something that sounds fun makes you better at it. Wild concept, I know.</p><p>In my real job, I‚Äôm not a sysadmin, but actually a software engineer. I work on large, distributed data processing systems. I‚Äôve come to genuinely despise the ‚ÄúData Engineer‚Äù title (story for another time), so I won‚Äôt use it. But, on an average day leading our data infrastructure efforts at <code>$coolNetworkingStartup</code> I would, generally, do one or multiple of the following:</p><ul><li>Meetings, office hours, architecture chats, that sort of thing</li><li>Write text with funny colors, these days usually <code>Python</code>, <code>Scala</code>, <code>go</code>, <code>typescript</code>, and <code>sql</code></li><li>Deal with actual infrastructure and tooling: K8s, Terraform, Docker, VPCs, EC2s, nix and other fun stuff</li><li>Perhaps related, stare at AWS Cost Explorer or spreadsheets</li><li>Babysit a bunch of OSS services, manage updates etc.</li><li>Keep my project management stuff reasonably up to date and presentable</li><li>And so on</li></ul><p>I do have some reasonable overlap with sysadmin tasks, mostly because I‚Äôm a proponent of open software and hence, decided to use and <a href="https://ngrok.com/blog-post/how-ngrok-uses-dagster-to-run-our-data-platform">host</a> various OSS tools as part of our data stack. While most of that is very stable, it does occasionally require some mild Linux wizardry to get back on track.</p><h2 id="reasoning-about-complex-systems">Reasoning about complex systems</h2><p>With that said, relatively rarely am I in the business of <code>ssh</code>‚Äòing into a random Linux machine to check on <code>nginx</code>. Not saying it <em>doesn‚Äôt</em> happen - it just doesn‚Äôt happen <em>very often</em>.</p><p>And when it <em>does</em> happen, being able to reason about complex systems by means of understanding the underlying technology on more than a very rough surface level is tremendously helpful.</p><p>Self hosting all these services - especially, if they take you a look deeper into a rabbit hole than just copy pasting commands - will teach you a lot of new things that help with this.</p><p>Let me give you a small handful of real-world examples - often times, simple Linux <em>administration</em> basics can be useful. The best ways to acquire those is to either daily drive Linux (personally, I‚Äôm on macOS these days)‚Ä¶ or self host and self manage servers.</p><p>For instance, knowing that setting <a href="https://www.howtogeek.com/449691/what-is-swapiness-on-linux-and-how-to-change-it/">swapiness</a> is seldom useful and calling that out in pull requests or knowing that messing with <a href="https://www.hpc.dtu.dk/?page_id=1180"><code>LD_LIBRARY_PATH</code></a> is a surefire way to frustration. However, being aware that both <em>can be</em> a valid tool to deal with obscure cases (for the latter, we mix <code>poetry</code> and <code>nix</code> and you can see where that is going) is helpful.</p><p>Of course babysitting and maintaining a distributed infrastructure (like Proxmox) or <a href="https://github.com/chollinger93/bridgefour">writing one from scratch</a> is also helpful to maintain, design, and/or implement <em>other</em> complex distributed systems that span more than one concept.</p><p>One of my pet projects at work is an <a href="https://flink.apache.org/">Apache Flink</a> pipeline (itself an inherently distributed framework), written in Scala 3, that uses typeclass derivation via <a href="https://github.com/softwaremill/magnolia">magnolia</a> and <a href="https://github.com/sksamuel/avro4s">avro4s</a> to turn <a href="https://scalapb.github.io/">protobuf</a> messages into <a href="https://iceberg.apache.org/">Apache Iceberg</a> via <a href="https://kafka.apache.org/">Kafka</a> (it does a bit more, but these are the basics). This project involved, in no particular order, a more-than-surface-level understanding of the following concepts that <em>aren‚Äôt</em> core ‚Äúprogramming‚Äù task, in <em>addition</em> to the core stuff (since it‚Äôs <code>scala</code>, stuff like tagless final):</p><p>VPCs and a bunch of networking, Kubernetes, GRPC, exactly-once/at-least-once semantics, eventual consistency, transactional locks, some <a href="https://github.com/scalapb/ScalaPB/pull/1674">compiler building</a>, protobufs &amp; schema evolution, Kafka (or general data storage replication and multi tenancy), blob storage, a ton of metrics &amp; monitoring (and hence, timeseries databases for lack of a better umbrella term) and so on.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825131026074.png" alt="Here&#39;s a picture out our neighborhood barn cat, so you don&#39;t get too bored while we talk about protobuf compilers"/><figcaption><p>Here&#39;s a picture out our neighborhood barn cat, so you don&#39;t get too bored while we talk about protobuf compilers
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Of course, if you are a reasonably seasoned engineer who has worked on distributed systems before, none of these topics will probably inherently new or scary - but the point is that a lot of them overlap with a lot of the things you deal with when you self host software - in fact, out of the 11 or so points I mentioned, I‚Äôm willing to wager I‚Äôve dealt with at least 8 of them during my self-hosting adventures.</p><p>Lastly, as hinted to above, at <code>$work</code> I also ‚Äúown‚Äù (isn‚Äôt that a horribly ‚Äúcorporate‚Äù word in this context?) several Open Source tools, which we self-host and dogfood on Kubernetes, such as <a href="https://superset.apache.org/">Apache Superset</a>. The parallels to what this article about should be relatively obvious. :-)</p><h2 id="things-that-broke-in-the-last-6-months">Things that broke in the last 6 months</h2><p>Okay, cool, seems useful. But remember how I said ‚Äúit‚Äôs a lot of work‚Äù? Well. Over the past 6 or so months, the following things happened (and I don‚Äôt believe that‚Äôs an exhaustive list):</p><ul><li>We had power outages and the UPS, which was supposed to be responsible for be backup power for the server + home network, just‚Ä¶ didn‚Äôt work</li><li>Because of that, our network - which has single point of failures - was down when a single server was down</li><li>My VPS, the one server I outsource the operations of, was randomly offline for almost a week</li><li>One of my Proxmox hosts crashed every single night, without fail, at 23:25 and came back online at 23:40</li></ul><h2 id="things-i-learned-or-recalled-in-the-last-6-months">Things I learned (or recalled) in the last 6 months</h2><p>So, allow me to rapid fire a bunch of things I did in the same timeframe, partially to fix some of these issues, and partially out of curiosity, and roughly what I learned and/or recalled by doing that.</p><h3 id="you-can-self-host-vs-code">You can self host VS Code</h3><p>This is a simple, but neat one: The OS part of VS Code is something that runs in a browser. Same concept that powers GitHub‚Äôs codespaces, but self hosted: <a href="https://github.com/coder/code-server">https://github.com/coder/code-server</a></p><p>Very useful to use VS Code on a device like an iPad or a Mac (or Windows) that wants a Linux box. Mine runs on <code>Ubuntu</code>.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825131634407.png" alt="Useful on an iPhone? Eh..."/><figcaption><p>Useful on an iPhone? Eh...
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>I stumbled upon this while wondering if I can make my overpriced iPad Pro a bit more useful. Turns out, you can! Not saying you <em>should</em>, but you <em>can</em>.</p><h3 id="ups-batteries-die-silently-and-quicker-than-you-think">UPS batteries die silently and quicker than you think</h3><p>UPS batteries, at least consumer ones like my simple 1U Cyberpower 1500VA only last about <em>3 years</em>. Mine, being 4 years old, were <em>completely</em> dead. While I‚Äôm conceptually aware that not everything is a long-lived lithium based powerhouse, I did <em>not</em> know they go to ‚Äúunusable capacity‚Äù that quickly.</p><p>I learned that the hard way after getting hit by brown outs and power outages that are pretty common here, and seemingly randomly, ‚Äúthe internet‚Äù was down. Turns out, without a redundant DNS, the internet doesn‚Äôt work.</p><p>Changing these batteries was actually pretty straightforward (thanks to RefurbUPS).</p><p>Nowadays, I actually have <code>powerst -test</code> scheduled monthly and will take testing my other, analog backup batteries more frequently.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240824101120003.png" alt="pwrstat"/><figcaption><p>pwrstat
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><h3 id="redundant-dns-is-good-dns">Redundant DNS is good DNS</h3><p>We all know DNS is inherently redundant, globally distributed, and eventually consistent. But I actually never <em>hosted</em> 2 DNS caches at once.</p><p>The reason I never did - I use Pi-Hole, which runs <code>dnsmasq</code>. I <em>also</em> used it as a DHCP server to see which machine ran which DNS queries. So, by way of laziness and convenience, moving DHCP out of the Pi-Hole interface was annoying.</p><p>To fix that, I used RouterOS (Mikrotik) to set static IPs per device and VLAN/IP pool and re-enabled the DHCP server. I <em>already</em> did that for the non-home VLANs, since they use a public DNS and don‚Äôt use the Pi-Hole.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607213853924.png" alt="VLAN"/><figcaption><p>VLAN
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>I also used this as an opportunity to map IP ranges to physical devices.</p><p>I then used that mapping to map hostnames to IPs w/in the Pi-Hole:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607214429699.png" alt="DNS Mapping"/><figcaption><p>DNS Mapping
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Then, it was just a matter of configuring the two Pi-Hole servers in RouterOS to make both available for clients. Poor man‚Äôs H/A!</p><p>Since we don‚Äôt add devices that often (and I add all services that <em>need</em> a DNS name to <a href="https://github.com/linuxserver/Heimdall">heimdall</a>), it works well enough to grok Pi-Hole logs to manually copy over configs when something changes.</p><p>I‚Äôve got it on my list to host <a href="https://github.com/mattwebbio/orbital-sync">orbital-sync</a> to simplify this further.</p><p>At this point, you <em>can</em> also enforce the use of said DNS servers (which may be a good idea with kids or in a work setting) by means of firewall rules around port 53 UDP traffic. RouterOS can do that easily.</p><h3 id="raspberry-pis-run-arn-proxmox-does-not">Raspberry PIs run ARN, Proxmox does not</h3><p>Normally, Proxmox is x86 only.</p><p>But, this <a href="https://github.com/jiangcuo/Proxmox-Port">repo</a> on GitHub preps Proxmox for ARM. Because of that, my little Raspberry Pi 5 is now a Proxmox node, which runs UniFi and one of the Pi-Holes.</p><p>Not recommended and unsupported? Sure! So is using <code>zfs</code> via an USB device. Still works (within reason)!</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607214732963.png" alt="Promox nodes"/><figcaption><p>Promox nodes
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>I mentioned ‚ÄúPoor man‚Äôs High Availability‚Äù a second ago: Funnily enough, <a href="https://www.youtube.com/watch?v=hNrr0aJgxig">Linus Tech Tips</a> recently made a video about Proxmox proper H/A VMs and failovers. My DNS setup is not that, but <em>could</em> be, since I now have 3 nodes, thanks to my frankenstein‚Äôd ARM node. And as they say: Three‚Äôs a quorum, baby!</p><p><em>As another little side tangent</em>: The first time I dealt with ‚Äúquorums‚Äù was in the early <code>hadoop</code> days, where manually configuring <code>zookeeper</code>, <code>hdfs</code> etc. and making sure your cluster can form a quorum was actually important. It was certainly more complicated than ‚Äújust deploy this job to AWS‚Äù (and I don‚Äôt want to do that again), but it certainly taught me a lot about how the sausage is made!</p><h3 id="zfs--proxmox-eat-memmory-and-will-oom-kill-your-vms"><code>zfs</code> + Proxmox eat memmory and will OOM kill your VMS</h3><p>Did you know OOM kills were ‚Äúa thing‚Äù in <code>$currentYear</code>? Sure are! I run <code>TrueNAS Scale</code> with a <a href="https://chollinger.com/blog/2023/10/moving-a-proxmox-host-with-a-sas-hba-as-pci-passthrough-for-zfs--truenas/">SAS HBA in PCI passthru mode</a> and had my machines OOM killed occasionally when both Proxmox backups + device backups where running.</p><p>Now, as a tangent on top of the tangent - running that config for TrueNAS is <em>also</em> unsupported/discouraged by TrueNAS, which wants to run on bare metal.</p><p>Anyways, Proxmox, by default, uses up to 50% of memory for the <code>zfs</code> cache, which, in combination with the VM being configured to use ~80%, didn‚Äôt work out too well in an environment with limited RAM - and one that consists of almost exclusively refurbished, old RAM sticks from other computers.</p><p>But, it turns out, you can <a href="https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Module%20Parameters.html#zfs-arc-max">configure</a> that in the ZFS kernel settings!</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span><span>echo</span> <span>&#39;options zfs zfs_arc_max=&#34;8589934592&#34;&#39;</span> &gt;&gt; /etc/modprobe.d/zfs.conf
</span></span><span><span>update-initramfs -u
</span></span></code></pre></td></tr></tbody></table></div></div><p>This limits <code>zfs</code> to 8 GiB.</p><h3 id="the-mystery-of-random-crashes-is-it-hardware-its-always-hardware">The mystery of random crashes (Is it hardware? It‚Äôs always hardware.)</h3><p>One of my Proxmox hosts crashed every single night, without fail, at 23:25 and came back online at 23:40. Here‚Äôs the monitoring:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607215257458.png" alt="TBD"/><figcaption><p>TBD
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>While it <em>did</em> restart automatically, including the VMs, it of course couldn‚Äôt decrypt the <code>zfs</code> pool. I only noticed when the TimeMachine backups on my Mac were failing!</p><p>Well, sorting through logs revealed: 23:25 is when another Proxmox nodes backs up its VM to that server, to a single, non-redundant, ancient harddrive, with SMART values like this:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span>Error <span>4</span> occurred at disk power-on lifetime: <span>27469</span> hours <span>(</span><span>1144</span> days + <span>13</span> hours<span>)</span>
</span></span><span><span>  When the <span>command</span> that caused the error occurred, the device was active or idle.
</span></span><span><span>
</span></span><span><span>  After <span>command</span> completion occurred, registers were:
</span></span><span><span>  ER ST SC SN CL CH DH
</span></span><span><span>  -- -- -- -- -- -- --
</span></span><span><span>  <span>04</span> <span>51</span> <span>00</span> ff ff ff 0f
</span></span><span><span>
</span></span><span><span>  Commands leading to the <span>command</span> that caused the error were:
</span></span><span><span>  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name
</span></span><span><span>  -- -- -- -- -- -- -- --  ----------------  --------------------
</span></span><span><span>  ea <span>00</span> <span>00</span> <span>00</span> <span>00</span> <span>00</span> a0 <span>00</span>   6d+03:38:11.728  FLUSH CACHE EXT
</span></span><span><span>  <span>61</span> <span>00</span> <span>08</span> ff ff ff 4f <span>00</span>   6d+03:38:11.726  WRITE FPDMA QUEUED
</span></span><span><span>  <span>61</span> <span>00</span> <span>08</span> ff ff ff 4f <span>00</span>   6d+03:38:11.726  WRITE FPDMA QUEUED
</span></span><span><span>  <span>61</span> <span>00</span> <span>08</span> ff ff ff 4f <span>00</span>   6d+03:38:11.725  WRITE FPDMA QUEUED
</span></span><span><span>  <span>61</span> <span>00</span> b0 ff ff ff 4f <span>00</span>   6d+03:38:11.725  WRITE FPDMA QUEUED
</span></span></code></pre></td></tr></tbody></table></div></div><p>‚Ä¶which then caused weird deadlocks and such and the machine became unresponsive and got killed.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607215539705.png" alt="LibreNMS monitoring"/><figcaption><p>LibreNMS monitoring
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Replace drive, problem solved. Funny how that works.</p><p><code>$BigHardDrive</code> will tell you to <em>only</em> use shiny, new, fancy, expensive drives, but the man with the server rack in the basement tells you that you can, in fact, re-use your ancient, assorted pieces of hardware for ultimately unimportant backups with <del>no</del> almost no issues!</p><h3 id="snmpv3-is-still-cool">SNMP(v3) is still cool</h3><p>I live precariously through reminders. One I had on my list for a while was ‚Äúautomate network mapping and alerts‚Äù. Turns out, a solved problem, even <em>without</em> the modern monitoring &amp; tracing solutions from Datadog (which, for the record, I love), Jaeger, Prometheus and the like.</p><p>These graphs above from the decaying HDD? <a href="https://www.librenms.org/">LibreNMS</a>!</p><p>Uses Simple Network Management Protocol (<a href="https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol?useskin=vector">SNMP</a>). An ancient protocol from the 80‚Äôs that does <em>exactly</em> that: Monitors your stuff. I just never bothered setting it up, since it is somewhat of an arcane protocol to modern eyes. Not that I allow inbound traffic (we have <a href="https://ngrok.com/">ngrok</a> for that), but a paranoid sysadmin is a good sysadmin.</p><blockquote><p>SNMP depends on secure strings (or ‚Äúcommunity strings‚Äù) that grant access to portions of devices‚Äô management planes. Abuse of SNMP could allow an unauthorized third party to gain access to a network device.</p><p>SNMPv3 should be the only version of SNMP employed because SNMPv3 has the ability to authenticate and encrypt payloads. When either SNMPv1 or SNMPv2 are employed, an adversary could sniff network traffic to determine the community string. This compromise could enable a man-in-the-middle or replay attack.</p><p>‚Ä¶</p><p>Simply using SNMPv3 is not enough to prevent abuse of the protocol. A safer approach is to combine SNMPv3 with management information base (MIB) whitelisting using SNMP views.</p><p><a href="https://www.cisa.gov/news-events/alerts/2017/06/05/reducing-risk-snmp-abuse">https://www.cisa.gov/news-events/alerts/2017/06/05/reducing-risk-snmp-abuse</a></p></blockquote><p>I have all devices (that support it) on the network in LibreNMS, using the most ‚Äúsecure‚Äù version of the protocol, V3.</p><p>It does everything a cool, modern, SaaS based tracing and monitoring service does, for free. Well, maybe not <em>all</em> of it - but it‚Äôs delightfully in depth. Other options (not necessarily mutually exclusive) are <code>zabbix</code> and <code>munin</code>, both of which I played with before, but never stuck to. This is a deceptively deep field, turns out, and not one I‚Äôm very knowledgeable about.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240824103028884.png" alt="LibreNMS"/><figcaption><p>LibreNMS
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>But I <em>do</em> like graphs and pretty dashboards.</p><h3 id="dont-trust-your-vps-vendor">Don‚Äôt trust your VPS vendor</h3><p>My VPS, hosted by Contabo, randomly went down for almost 4 days. You could not read this blog.</p><p><em>Only</em> VNC through a different IP worked. I was able to talk to my server, which I paid for continuously since <em>2014</em>, only via VNC. <em>Nothing</em> else worked - ping, SSH, nothing. And I pride myself in writing detailed support tickets and check everything I can myself - this time, it actually <em>wasn‚Äôt</em> DNS. :-)</p><p>For a short timeline of events: First, I raise a ticket outlining the problem, including a copy of the DNS config.</p><p>I‚Äôll refrain from pasting the verbatim interactions here, but their response boiled down to</p><blockquote><p>Your problem sounds like a case of not having the DNS server set.</p></blockquote><p>‚Ä¶ followed by a few steps to set the Google DNS servers via <code>resolv.conf</code>.</p><p>Naturally, DNS was the first thing I checked. See above. My response included another copy (this time a screenshot) of my DNS configuration and a friendly note that I can‚Äôt install <code>apt</code> packages, since the machine is cut off from anything outside their data center.</p><p>I tried to offer more insights by doing some more digging. Turns out, I couldn‚Äôt even ping the IP directly (i.e., without DNS involvement). It seems like my ISP and their data center could not talk.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607221106538.png" alt="External network path says no"/><figcaption><p>External network path says no
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>As well as the fun observation that the VPS couldn‚Äôt talk to anything external. The only reason VNC worked is that this would tunnel it through a different machine in the same DC.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607221115352.png" alt="Ping says no internally"/><figcaption><p>Ping says no internally
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>93.184.215.14 is <a href="http://example.org/">example.org</a>. This would never time out, since I let it run overnight. And yes, I did all of this debugging with <code>netcat</code> too, given the limitations of a simple <code>ping</code>.</p><p>More curiously, an <code>arp</code> scan gave me back 207.244.240.1, which is their data center in Missouri. Which, while available, was spotty to reach from the outside at best:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240607221122749.png" alt="Flakey"/><figcaption><p>Flakey
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Everything here pointed at a larger network issue on their end, and my server had been offline for 48 hrs.</p><p>2 days later, my own options exhausted, the machine is suddenly back online - in a different DC, from what I can tell. I made no further changes and worked on on a migration to Hetzner instead (see below). I get this email:</p><blockquote><p>Thank you for your patience. We checked your VPS M SSD/ It is reacting to Ping requests, SSH and VNC is accessible as well.
Therefore we assume that the issue is already solved. Kindly check your side again.</p></blockquote><p>‚Ä¶awesome. I did <em>nothing</em> (see above - don‚Äôt think I could have!) and it suddenly works.</p><p>I‚Äôm certainly not above admitting if I <em>did</em> mess something up - I‚Äôve done it in this article and on this blog many, many times and I <em>do</em> mess up sysadmin stuff frequently - but this time, I promise you I‚Äôve made zero changes to the box before this happened, neither before it magically was fixed.</p><p>I don‚Äôt expect <a href="https://uptime.is/99.999">99.999%</a> availability from a more budget VPS, and they do only ‚Äú<a href="https://contabo.com/en-us/legal/terms-and-conditions/">advertise</a>‚Äù (I use this term lightly - it‚Äôs hidden in their T&amp;C) and SLA of <a href="https://uptime.is/95">95%</a>, which means they allow for over 18 days of downtime a year.</p><blockquote><p>(1) The Provider will ensure that the physical connectivity of the object storage infrastructure, webspace packages, dedicated servers, virtual dedicated server and VPS is available at an annual average rate of 95%.</p></blockquote><p>Unfortunately, this <em>interaction</em>, not necessarily the downtime itself was so bad that I stopped recommending Contabo entirely and since moved to Hetzner - and, of course, my own physical hardware, which had 99.995% uptime:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825104016231.png" alt="Uptime"/><figcaption><p>Uptime
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Realistically, it‚Äôs probably more like 98% long-term, but that involves me actively breaking things. Make of that what you will.</p><h3 id="gotta-go-fast">Gotta go fast</h3><p>So, because of that, I looked for alternatives. I wanted a cheap VPS in the US for latency reasons, with basic creatue comfort. Eventually, after much deliberation, I moved to <a href="https://www.hetzner.com/">Hetzner</a>, another German company with at least some North American data centers, on their small, $7ish/mo, CPX21 VPS.</p><p>Much to my surprise, that machine was faster in benchmarks than my (nominally much more powerful) Contabo VPS:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span>sysbench --threads<span>=</span><span>4</span>   --time<span>=</span><span>30</span> --cpu-max-prime<span>=</span><span>20000</span> cpu run
</span></span></code></pre></td></tr></tbody></table></div></div><p>Got me</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/GPGtDkkXoAAW9XD.png" alt="VPS Benchmark 1/2"/><figcaption><p>VPS Benchmark 1/2
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>And</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/GPGtEuXXYAAdzJ4.png" alt="VPS Benchmark 2/2"/><figcaption><p>VPS Benchmark 2/2
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Yes, synthetic benchmarks aren‚Äôt perfect, but they also certainly aren‚Äôt completely meaningless.</p><p>Again, make of these numbers what you will.</p><h3 id="cifs-is-still-not-fast">CIFS is still not fast</h3><p>Unfortunately, the server came with a 80 GB SSD. That‚Äôs only about 29,000 3.5&#34; floppy disks! Remember, this server runs Nextcloud which, inherently, requires a lot of storage.</p><p>My MacBook uses 1.2/2 <strong>T</strong>B, my Proxmox cluster has something like 50 TB combined capacity (in fairness, lots of redundancy), and even my phone is using ~200GB.</p><p>I stumbled upon Hetzner‚Äôs ‚Äú<a href="https://www.hetzner.com/storage/storage-box/">StorageBox</a>‚Äù. Cool concept: Basically what <a href="https://rsync.net/">rsync.net</a> does, but German. You basically get a dumbed down terminal and a slow, shared disk for very little money at all: <strong>~$4/TB</strong>! Unfortunately, no US locations.</p><p>You can access these disks via FTP, SFTP or SCP (aka SSH), WebDAV, various backup tools like <code>borg</code> and so on.</p><p>Overall a pretty promising concept, but once you need to attach said disk to a server to make the local storage bigger and usable w/in Nextcloud, your options are somewhat limited and the official <a href="https://docs.hetzner.com/robot/storage-box/access/access-samba-cifs/">docs</a> recommend <code>samba</code>/<code>cifs</code>. <a href="https://github.com/libfuse/sshfs"><code>sshfs</code></a> is a valid alternative, the project is in maintenance mode, but I tested it anyways. I suppose conceptually you could make WebDAV work.</p><p>In any case, I tried using the official approach and benchmarked a bit because it certainly <em>felt</em> slow.</p><p>Well, that‚Äôs because it was. Here‚Äôs three benchmarks, two with <code>cifs</code> (V3), with a strict and loose cache, as well as <code>sshfs</code>.</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825111920987.png" alt="Storage Benchmark 1/2"/><figcaption><p>Storage Benchmark 1/2
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>Here‚Äôs what I used to test this:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span><span>SIZE</span><span>=</span><span>&#34;256M&#34;</span>
</span></span><span><span><span># Sequential</span>
</span></span><span><span>fio --name<span>=</span>job-r --rw<span>=</span><span>read</span> --size<span>=</span><span>$SIZE</span> --ioengine<span>=</span>libaio --iodepth<span>=</span><span>4</span> --bs<span>=</span>128K --direct<span>=</span><span>1</span>
</span></span><span><span>fio --name<span>=</span>job-w --rw<span>=</span>write --size<span>=</span><span>$SIZE</span> --ioengine<span>=</span>libaio --iodepth<span>=</span><span>4</span> --bs<span>=</span>128k --direct<span>=</span><span>1</span>
</span></span><span><span><span># Random</span>
</span></span><span><span>fio --name<span>=</span>job-randr --rw<span>=</span>randread --size<span>=</span><span>$SIZE</span> --ioengine<span>=</span>libaio --iodepth<span>=</span><span>32</span> --bs<span>=</span>4K --direct<span>=</span><span>1</span> 
</span></span><span><span>fio --name<span>=</span>job-randw --rw<span>=</span>randwrite --size<span>=</span><span>$SIZE</span> --ioengine<span>=</span>libaio --iodepth<span>=</span><span>32</span> --bs<span>=</span>4k --direct<span>=</span><span>1</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Not a new finding either, as I‚Äôm not the first to <a href="https://blog.ja-ke.tech/2019/08/27/nas-performance-sshfs-nfs-smb.html">benchmark</a> this.</p><p>In any case, that‚Äôs pretty slow. The combination of a slow protocol - <code>cifs</code> - a slow box, <em>and</em> cross-Atlantic latency didn‚Äôt make this particularly attractive.</p><p>To give you some perspective, here‚Äôs the same chart with the same benchmark run on a local Proxmox VM:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825134453828.png" alt="Storage Benchmark 2/2"/><figcaption><p>Storage Benchmark 2/2
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><h3 id="blob-storage-blob-fish-and-file-systems-its-all-meh">Blob storage, blob fish, and file systems: It‚Äôs all ‚Äúmeh‚Äù</h3><p>Now, not even <em>using</em> a ‚Äúdisk‚Äù, that‚Äôs what all the cool kids do nowadays. They simply throw away almost all the advantages of modern, fast, local storage with a high quality, tested, atomic, error correcting (‚Ä¶) file system by, get this, <em>not having</em> a file system and call it ‚Äúobject‚Äù (or ‚Äúblob‚Äù) storage and get none of those features, but a lot of storage for cheap!</p><p><em>Tangent</em>: Since the world ‚Äúblob‚Äù is now indexed, please remember the following before you shame a helpless fish for the failures of your storage architecture:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/blob.jpeg" alt="Poor fish"/><figcaption><p>Poor fish
<a href="https://nailalatif002.medium.com/on-ugliness-of-blobfish-culpable-ignorance-and-gods-guilt-14fd3b226908"><small>[by Naila Latif]</small></a></p></figcaption></figure><p>Anyways - despite the drawbacks, it‚Äôs really not a new concept. Professionally, I‚Äôd use fast NVMe drives for stuff like developer machines, databases and the like, regular SSDs for less intense application servers, and blob storage for slow-but-cheap data storage and distributed query engines, with optional caching where required. Most of my Data Platform project in the past ~10 years have been based on S3/GCS/etc., and often stored many hundreds of TiB (if not PiB) of data.</p><p>However - these storage systems are <em>not</em> file systems and trying to pretend <code>s3</code> and <code>zfs</code> are similar because they both store files (like some sort of deranged, surface level ü¶Ü-typing) is not going to end well.</p><p>But, given my access patterns - storing files and accessing them every once in a while by hand and not hosting a database on it, just like I would use a slow, HDD based local file server, I still went looking for blob storage options to store just my Nextcloud data.</p><p>Your options essentially are the big providers, like AWS, where 1TB runs you roughly <strong>~$20-24/mo</strong>, depending on the provider and region.</p><p>Third party alternatives that implement the same protocol, say <code>s3</code>, are mostly <a href="https://wasabi.com/">Wasabi</a> and <a href="https://www.backblaze.com/">Backblaze</a>.</p><p>Wasabi charges a minimum of <strong>$6.99/mo</strong> (which is the price per TB) <em>and</em> 90 days minimum storage (i.e., deleting data doesn‚Äôt help). Backblaze is cheaper on paper at <strong>$6/mo/TB</strong>, but they do charge $0.01/GB egress over 3x the average monthly data stored, which is included with Wasabi.</p><p>Given that I spent $35 egress to get my 500GB server backups out of S3 during the migration and after reading <a href="https://help.nextcloud.com/t/high-aws-s3-costs-due-to-nextcloud-requests/68687/4">this</a>, I went with Wasabi and moved my files there:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825112653613.png" alt="Wasabi"/><figcaption><p>Wasabi
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>And then used the Nextcloud <a href="https://docs.nextcloud.com/server/29/admin_manual/configuration_files/external_storage_configuration_gui.html">external storage</a> interface to mount the drive.</p><p>And what can I say, it‚Ä¶ works. It behaves just like S3 does: If you need to list a lot of files or do anything with a lot of small files, it‚Äôs a huge pain and will <em>not</em> be fast. That‚Äôs the reason the semi-official recommendation for deleting large amounts of small files is to use <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html">bucket lifecycle</a> policies.</p><p>For $6.99 a TiB, it works <em>okay</em>. Listing files on a cold cache (e.g., a new device) is slow as molasses, and searching for files puts you entirely at the mercy of your index (compared to a <code>find</code> or <code>grep</code> on a local NVMe storage).</p><p>Seeing this screen for several seconds is normal:</p><figure><img src="https://chollinger.com/blog/2024/08/why-i-still-self-host-my-servers-and-what-ive-recently-learned/assets/image-20240825114612035.png" alt="Nextcloud on iOS"/><figcaption><p>Nextcloud on iOS
<a href="https://chollinger.com"><small>[by me]</small></a></p></figcaption></figure><p>In all fairness, this is really only a problem on mobile devices, since I have all my files downloaded on my Mac anyways.</p><p>I might still try <a href="https://rsync.net/">rsync.net</a>, which clocks in at $12/mo/TB.</p><h3 id="crowdsec">CrowdSec</h3><p>Last but not least, do you remember <a href="https://github.com/fail2ban/fail2ban"><code>fail2ban</code></a>? You should, because it‚Äôs still very much an active project that should probably stay the default recommendation for simple intrusion prevention.</p><p>But, setting up a new server, I figured it would be worth checking some newer tools, namely <a href="https://github.com/crowdsecurity/crowdsec"><code>CrowdSec</code></a> (not to be confused with CrowdStrike :-) ) which describes itself as</p><blockquote><p>a free, modern &amp; collaborative behavior detection engine, coupled with a global IP reputation network. It stacks on fail2ban‚Äôs philosophy but is IPV6 compatible and 60x faster (Go vs Python) (‚Ä¶)</p></blockquote><p>The neat thing about CrowdSec is that, as the name implies, the threat detection patterns are being shared (anonymously) with their community, which (allegedly) yields a much better threat detection. It also has a plugin <a href="https://app.crowdsec.net/hub?q=next&amp;page=1">hub</a>, with ‚Äúbouncers‚Äù for various tools.</p><p>It also comes with a neat cli that gives you stats about what it‚Äôs doing:</p><p>Which, for instance, will tell me it banned 33 IPs for <code>crowdsecurity/CVE-2019-18935</code>, a rule for <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18935">this</a> CVE.</p><p>Overall, I‚Äôm a fan - it certainly feels like a natural evolution of <code>fail2ban</code> (even though I suspect they could co-exist) and as long as the company behind it doesn‚Äôt exploit its user base at a certain point, I‚Äôll stick to it.</p><h2 id="conclusion">Conclusion</h2><p>If you‚Äôre a software engineer, I recommend self hosting things. You learn a whole bunch of things through forced exposure to problems that you‚Äôll be less likely to encounter in your day job, which in itself is a benefit. Even better, I do believe you‚Äôll wind up using at least some of these things in your day job eventually, provided you work on something vaguely backend related.</p><p>By hosting stuff yourself, also get a reasonable level of autonomy - or, at the very least, some hedging - against the corporate dream of your entire life being a perpetually rented subscription. I think that‚Äôs nice.</p></div></div>
  </body>
</html>
