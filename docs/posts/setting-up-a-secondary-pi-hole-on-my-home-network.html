<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.dzombak.com/blog/2024/02/Setting-up-a-secondary-Pi-Hole-on-my-home-network.html">Original</a>
    <h1>Setting up a secondary Pi-Hole on my home network</h1>
    
    <div id="readability-page-1" class="page"><article><header><span> <time pubdate="" datetime="2024-02-06T15:44:41-05:00"> <span>February</span> <span>06,</span> <span>2024</span> </time> • <span>Tagged:</span> <a href="https://www.dzombak.com/blog/tag/linux/">linux</a> <a href="https://www.dzombak.com/blog/tag/series%3Aproject-logs/">series:project-logs</a> </span></header><p>I run <a href="https://pi-hole.net">Pi-Hole</a> as the DNS server for my home network. It provides ad and nuisance blocking for a subset of the systems in the house. Having a single DNS server for your network is very stressful; it’s a single point of failure, so even routine maintenance feels touch-and-go. I finally got around to setting up a secondary DNS server for my home network, running a second Pi-Hole instance and using <a href="https://github.com/vmstan/gravity-sync">Gravity Sync</a> to keep it in sync with the primary instance.</p><p>This (rather terse) blog post walks through how I set this up. Thankfully, there’s not that much to say, because both Pi-Hole and Gravity Sync are well-documented and easy to set up.</p><h2 id="the-machine"> The machine <a href="#the-machine"></a></h2><p>I opted to set this DNS server up as a VM on my home NAS rather than running Pi-Hole in Docker. My reasoning is:</p><ul><li>I want the DNS server to have its own IP, not just take over pot 53 on my home NAS</li><li>I have never used macvlan (or ipvlan) networking with Docker, I don’t know how to set it up, and I’ve read conflicting and confusing reports about how easy it is to use with Pi-Hole</li><li>the NAS has 64 GB of RAM and 48TB of storage, so this VM will take a tiny fraction of those resources</li></ul><p>First, I set up the NAS to support running persistent VMs with KVM. <a href="https://www.dzombak.com/blog/2024/02/Setting-up-KVM-virtual-machines-using-a-bridged-network.html">I wrote a blog post about that last week which you should read for more detail.</a> The following command started a VM with 2 VCPUs, 2GB of memory, and 32GB of disk space:</p><div><div><pre><code>virt-install <span>\</span>
  <span>--name</span> altdns <span>\</span>
  <span>--description</span> <span>&#34;alternate DNS server for the home network&#34;</span> <span>\</span>
  <span>--memory</span> 2048 <span>\</span>
  <span>--vcpus</span> 2 <span>\</span>
  <span>--disk</span> <span>path</span><span>=</span>/mnt/storage/vm/altdns/disk.qcow2,size<span>=</span>32 <span>\</span>
  <span>--cdrom</span> /mnt/scratch/ubuntu-22.04.3-live-server-amd64.iso <span>\</span>
  <span>--graphics</span> vnc <span>\</span>
  <span>--os-variant</span> ubuntu22.04 <span>\</span>
  <span>--virt-type</span> kvm <span>\</span>
  <span>--autostart</span> <span>\</span>
  <span>--network</span> <span>network</span><span>=</span>hostbridge
</code></pre></div></div><h2 id="ubuntu-setup"> Ubuntu setup <a href="#ubuntu-setup"></a></h2><p>I installed the “minimal” version of Ubuntu 22.04 LTS on the VM. This isn’t the interesting part of the project, so I’ll mostly skip over it. The only interesting parts are:</p><ul><li>I installed the packages <code>locales</code>, <code>dialog</code>, and <code>apt-utils</code>, which aren’t included in the minimal Ubuntu setup but make the terminal user experience better</li><li>I installed <a href="https://www.netdata.cloud">Netdata</a> on the VM for monitoring</li><li>I installed <a href="https://tailscale.com">Tailscale</a> on the VM for remote access</li><li>After installation, I assigned the VM a DHCP reservation on my router, ensuring it always gets a stable IP address</li></ul><h2 id="pi-hole-setup"> Pi-Hole setup <a href="#pi-hole-setup"></a></h2><p>I first installed Pi-Hole with <a href="https://docs.pi-hole.net/main/basic-install/">their one-step automated installation process</a>:</p><div><div><pre><code>curl <span>-sSL</span> https://install.pi-hole.net | bash
</code></pre></div></div><p><a href="https://github.com/vmstan/gravity-sync/blob/master/README.md#limitations">Gravity Sync doesn’t sync <em>everything</em></a>, so I needed to be sure that I’d configured everything on the new server similarly to the primary server.</p><p>First, I needed to copy <code>/etc/pihole/pihole-FTL.conf</code> from the primary to the new secondary server, since I have a few customizations in there.</p><p>Then I logged into the alternate server’s Pi-Hole web admin console and walked through all the settings, configuring them identically to the primary server. I paid particular attention to:</p><ul><li>upstream DNS servers</li><li>“advanced DNS settings” &amp; conditional forwarding</li><li>privacy settings</li></ul><p>Since Netdata is installed on this VM, I also set up <a href="https://www.dzombak.com/blog/2023/11/Fixing-excessive-Pi-Hole-lighttpd-access-log-size-when-Netdata-is-installed.html">my lighttpd config workaround for Netdata flooding the Pi-Hole admin console access logs</a>.</p><p>Finally, since 32GB is a relatively small disk, I installed <a href="https://github.com/cdzombak/apt-daily-clean">a daily cron job to clean the ￼<code>apt</code>￼ cache</a>:</p><div><div><pre><code><span>sudo </span>apt-get <span>install</span> <span>-y</span> apt-daily-clean
</code></pre></div></div><h2 id="gravity-sync-setup"> Gravity Sync setup <a href="#gravity-sync-setup"></a></h2><p>Gravity Sync’s wiki has excellent instructions, which I’m not going to repeat here. I followed the instructions to <a href="https://github.com/vmstan/gravity-sync/wiki/Installing">install &amp; configure</a> the software, <a href="https://github.com/vmstan/gravity-sync/wiki/Engaging">perform a dry run then a real synchronization</a> to the alternate DNS server, <a href="https://github.com/vmstan/gravity-sync/wiki/Automation">and schedule the sync process</a> to run frequently.</p><h3 id="gravity-sync-ssh-key-management"> Gravity Sync SSH key management <a href="#gravity-sync-ssh-key-management"></a></h3><p>Gravity Sync creates an SSH keypair on each system, which it uses for the sync process. By default, for the user it’s running under on each host, it adds the SSH public key from the <em>other</em> host to <code>~/.ssh/authorized_keys</code>.</p><p>For <a href="https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html#git-repo-for-ssh-configuration">reasons</a>, I don’t like having machine-specific contents in <code>~/.ssh/authorized_keys</code>. To move this key to another file, on each server, I:</p><ol><li>Moved the line containing the new SSH key to a new file, <code>~/.ssh/authorized_keys_gravity-sync</code></li><li><code>chmod 0644 ~/.ssh/authorized_keys_gravity-sync</code></li></ol><p>To make <code>sshd</code> pick up on this new <code>authorized_keys</code> file, on each server, I edited <code>/etc/ssh/sshd_config</code> so that the <code>AuthorizedKeysFile</code> line reads:</p><div><div><pre><code>AuthorizedKeysFile  .ssh/authorized_keys .ssh/authorized_keys_gravity-sync
</code></pre></div></div><p>Finally, I made that change take effect via <code>sudo systemctl reload sshd</code>.</p><h2 id="monitoring"> Monitoring <a href="#monitoring"></a></h2><p>In addition to using Netdata, I monitor various parts of my personal computing infrastructure with <a href="https://github.com/louislam/uptime-kuma">Uptime Kuma</a>, and DNS servers are no different — they’re one of the most important things to monitor.</p><p>I have the following Uptime Kuma monitors set up for both the primary and secondary DNS servers:</p><ul><li>Liveness (a simple ping monitor)</li><li>Tailscale (a simple ping monitor for the Tailscale IP address)</li><li>DNS server liveness (using Uptime Kuma’s built-in DNS monitor)</li><li>Pi-Hole admin console availability (an HTTP monitor)</li><li>Gravity Sync status (a push monitor)</li></ul><p>To monitor Gravity Sync, I created a systemd override file for the <code>gravity-sync</code> service which updates that Uptime Kuma push monitor every time the sync process completes without error. I did that via <code>sudo systemctl edit gravity-sync.service</code>, modifying the override file to add:</p><div><div><pre><code>[Service]
Type=oneshot
ExecStartPost=curl &#34;http://192.168.1.10:9001/api/push/XXXXXXXX?status=up&amp;msg=OK&amp;ping=&#34;
</code></pre></div></div><p>Changing the service from a <code>simple</code> type to <code>oneshot</code> is necessary because for a <code>simple</code> service systemd will run <code>ExecStartPost</code> immediately if the service <em>started</em> successfully. For a <code>oneshot</code> service, systemd waits for the task to exit successfully and <em>then</em> runs <code>ExecStartPost</code>.</p><p><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html#ExecStartPre=">See the systemd docs for more details.</a></p><h2 id="testing-and-network-setup"> Testing and network setup <a href="#testing-and-network-setup"></a></h2><p>I verified the new DNS server was working by running the <code>dig</code> command from my desktop. Assuming the new DNS server has the address <code>192.168.1.100</code>, this looks like:</p><div><div><pre><code>dig @192.168.1.100 dzombak.com
</code></pre></div></div><p>Once that was done, I just needed to configure my router to return both DNS servers in DHCP responses.</p></article></div>
  </body>
</html>
