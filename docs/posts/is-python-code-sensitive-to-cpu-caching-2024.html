<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lukasatkinson.de/2024/python-cpu-caching/">Original</a>
    <h1>Is Python Code Sensitive to CPU Caching? (2024)</h1>
    
    <div id="readability-page-1" class="page"><div> <p>Cache-aware programming can make a huge performance difference,
especially when writing code in C++ or Rust.
Python is a much more high-level language,
and doesn&#39;t give us that level of control over memory layout of our data structures.
So does this mean that CPU caching effects aren&#39;t relevant in Python?</p>
<p>In this post, we&#39;ll conduct some basic experiments to answer this question,
by accessing list element either in sequental order or in a random order.</p>
<p>Results indicate that randomized access is consistently slower in Python.
Especially as soon as problem sizes outgrow the CPU cache sizes,
random access is multiple times slower.
Thus, some degree of cache-aware programming may be relevant in interpreted environments like CPython 3.12 as well.</p>
<!-- more -->
<p>Disclaimers:
Results may vary on different software and hardware.
The software used for these tests is CPython 3.12.
The CPU uses the x86-64 (amd64) architecture, Zen 2 microarchitecture.
This CPU generation had a fairly complex cache hierarchy
due to grouping cores in separate &#34;core complexes&#34; that partially share some of their cache.
Each core should see up to 32KB L1, 512KB L2, 16MB L3.</p>
<h2 id="background">Background</h2>
<p>Programs store data in memory (RAM), but this is slow.
CPU caches are faster, but much smaller.
The CPU has to decide which parts of the RAM to load into the caches.
Of course explicitly accessed memory addresses will be loaded into the cache.
But the act of loading data is comparatively slow, so the CPU will try to pre-load relevant data.</p>
<p>For example, we can expect the CPU to detect linear array access patterns
and automatically pre-load the next chunks of an array.
So <code>foreach</code> style loops over an array where all relevant data is directly inside the array
can be quite cache-friendly,
whereas random access cannot be predicted.</p>
<p>In practice, many arrays contain <em>pointers</em> instead.
We must dereference the pointer to a different memory location in order to get to the actual data.
Maybe the pointed-to location is already in the cache, maybe not and we will have to wait
– effectively also a kind of random access.</p>
<p>In Java (OpenJDK) and Python (CPython), all normal objects are actually represented as pointers to a more complex data structure.
In C notation, an array wouldn&#39;t contain <code>PyObject</code> values, but <code>PyObject*</code> pointers.</p>
<p>So clearly, Python data structures won&#39;t be as cache-friendly as data structures in C++ or Rust could be.</p>
<p>But Python already has a ton of interpreter overhead compared to natively compiled languages,
so maybe this doesn&#39;t matter?</p>
<ul>
<li>In my experience, interpreter overhead tends to be in the 2× – 100× range, typically at least 10×,
and much worse if meaningful compiler optimizations are possible.</li>
<li>But typical values for the cost of accessing main memory vs L1 cached data also tend to be in the 100× range.</li>
</ul>
<p>All of this is very rule-of-tumb, back-of-napkin,
but this suggests that interpreter overhead wouldn&#39;t drown out cache effects.</p>
<h2 id="designing-an-experiment">Designing an experiment</h2>
<p>Our working hypothesis is: cache effects are detectable in Python programs.</p>
<p>This is not testable, so let&#39;s refine it based on the array iteration example above.
What we could try to observe is that the following program should take longer
when <code>indices = shuffled(range(len(values)))</code> (&#34;random&#34; access)
than when elements are accessed in order (<code>indices = list(range(len(values)))</code>):</p>
<pre tabindex="0" data-language="python"><code><span><span>sum</span><span>(values[i] </span><span>for</span><span> i </span><span>in</span><span> indices)</span></span>
<span></span></code></pre>
<p>But there are many different objects here at play:</p>
<ul>
<li><code>indices</code> is an array containing pointers to integer objects</li>
<li><code>i</code> is effectively a pointer to such an integer object</li>
<li><code>values</code> is an array containing pointers to objects</li>
<li><code>values[i]</code> is effectively one such pointer</li>
</ul>
<p>To drop to C notation for a moment, this means we have four different kinds of memory locations:</p>
<ul>
<li><code>indices[…]</code> / <code>i</code>, which contains a pointer</li>
<li><code>*i</code>, which contains the actual index</li>
<li><code>values[*i]</code>, which contains a pointer</li>
<li><code>*values[*i]</code>, which contains the actual value</li>
</ul>
<p>If we want to desing the experiment properly, we have to consider cache effects for all these memory locations.</p>
<ul>
<li>
<p>The <code>indices</code> array is likely unproblematic as it will be iterated in order.</p>
</li>
<li>
<p>Each index <code>*i</code> however requires a memory access.
To reduce the impact of cache effects, it would be nice if these values are laid out sequentially in memory.
Python doesn&#39;t let us control that,
but in practice objects are likely to be allocated mostly one after the other.
So we want to create those integer objects in the order in which they will be referenced.
That means we must not shuffle indices within the Python process that runs the benchmark,
but can read in already-shuffled test data.</p>
</li>
<li>
<p>The <code>values</code> array is unproblematic, as it is the main cache effect that we want to test.</p>
</li>
<li>
<p>The <code>*values[*i]</code> elements may or may not matter.
Either the CPU is able to understand that <code>values</code> are actually pointers to values
and can pre-load the pointed-to values.
Or, cache effects would mean that the example runs faster if the values are laid out
in the same order in which they are referenced in the <code>values</code> array.</p>
</li>
</ul>
<p>I&#39;m not sure how to test the impact of <code>values</code> order,
so let&#39;s stick to the same trick of using allocation-order to increase the chance of a potentially cache-friendly data layout.</p>
<p>We can generate test data in a format as follows:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>&lt;size&gt;,&lt;iterations&gt;,&lt;expected sum&gt;</span></span>
<span><span>&lt;indices&gt;,...</span></span>
<span><span>&lt;values&gt;,...</span></span>
<span><span></span></span></code></pre>
<p>For example, the following would describe an in-order access of <code>values</code>
(the order of indices matters, the order of values should not):</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>4,1,10</span></span>
<span><span>0,1,2,3,4</span></span>
<span><span>3,2,4,0,1</span></span>
<span><span></span></span></code></pre>
<p>I don&#39;t want to bother with strings and number parsing,
so let&#39;s use Python&#39;s <code>struct</code> and <code>array</code> modules to pack the test data into a binary representation.</p>
<p>Here is the main program that we will benchmark:</p>
<pre tabindex="0" data-language="python"><code><span><span>import</span><span> sys, struct, array</span></span>
<span><span>def</span><span> parse_array</span><span>(f, fmt, n):</span></span>
<span><span>    arr </span><span>=</span><span> array.array(fmt)</span></span>
<span><span>    arr.fromfile(f, n)</span></span>
<span><span>    return</span><span> arr.tolist()</span></span>
<span></span>
<span><span># parse the test case</span></span>
<span><span>header_layout </span><span>=</span><span> struct.Struct(</span><span>&#34;=4sLLQ&#34;</span><span>)</span></span>
<span><span>magic, size, iterations, expected_sum </span><span>=</span><span> \</span></span>
<span><span>    header_layout.unpack(sys.stdin.buffer.read(header_layout.size))</span></span>
<span><span>assert</span><span> magic </span><span>==</span><span> b</span><span>&#34;cach&#34;</span><span>,  </span><span>f</span><span>&#34;wrong magic number </span><span>{</span><span>magic</span><span>}</span><span>&#34;</span></span>
<span><span>indices </span><span>=</span><span> parse_array(sys.stdin.buffer, </span><span>&#34;L&#34;</span><span>, size)</span></span>
<span><span>values </span><span>=</span><span> parse_array(sys.stdin.buffer, </span><span>&#34;L&#34;</span><span>, size)</span></span>
<span></span>
<span><span># run the test case</span></span>
<span><span>for</span><span> _ </span><span>in</span><span> range</span><span>(iterations):</span></span>
<span><span>    s </span><span>=</span><span> sum</span><span>(values[i] </span><span>for</span><span> i </span><span>in</span><span> indices)</span></span>
<span><span>    assert</span><span> s </span><span>==</span><span> expected_sum</span></span>
<span></span></code></pre>
<p>Here&#39;s a program to generate a test case:</p>
<pre tabindex="0" data-language="python"><code><span><span>import</span><span> sys, struct, array, random, click</span></span>
<span></span>
<span><span>@click.command</span><span>()</span></span>
<span><span>@click.option</span><span>(</span><span>&#34;--size&#34;</span><span>, </span><span>type</span><span>=</span><span>int</span><span>, </span><span>required</span><span>=</span><span>True</span><span>)</span></span>
<span><span>@click.option</span><span>(</span><span>&#34;--iterations&#34;</span><span>, </span><span>type</span><span>=</span><span>int</span><span>, </span><span>required</span><span>=</span><span>True</span><span>)</span></span>
<span><span>@click.option</span><span>(</span><span>&#34;--shuffle/--no-shuffle&#34;</span><span>, </span><span>required</span><span>=</span><span>True</span><span>)</span></span>
<span><span>@click.option</span><span>(</span><span>&#34;--seed&#34;</span><span>, </span><span>type</span><span>=</span><span>int</span><span>, </span><span>required</span><span>=</span><span>True</span><span>)</span></span>
<span><span>def</span><span> cli</span><span>(size: </span><span>int</span><span>, iterations: </span><span>int</span><span>, shuffle: </span><span>bool</span><span>, seed: </span><span>int</span><span>):</span></span>
<span><span>    rng </span><span>=</span><span> random.Random(seed)</span></span>
<span><span>    values </span><span>=</span><span> list</span><span>(</span><span>range</span><span>(size))</span></span>
<span><span>    rng.shuffle(values)  </span><span># should not affect benchmarks, but best randomize</span></span>
<span><span>    indices </span><span>=</span><span> list</span><span>(</span><span>range</span><span>(size))</span></span>
<span><span>    if</span><span> shuffle:</span></span>
<span><span>        rng.shuffle(indices)</span></span>
<span></span>
<span><span>    header_layout </span><span>=</span><span> struct.Struct(</span><span>&#34;=4sLLQ&#34;</span><span>)</span></span>
<span><span>    sys.stdout.buffer.write(header_layout.pack(</span><span>b</span><span>&#34;cach&#34;</span><span>, size, iterations, </span><span>sum</span><span>(values)))</span></span>
<span><span>    array.array(</span><span>&#34;L&#34;</span><span>, indices).tofile(sys.stdout.buffer)</span></span>
<span><span>    array.array(</span><span>&#34;L&#34;</span><span>, values).tofile(sys.stdout.buffer)</span></span>
<span></span>
<span><span>if</span><span> __name__</span><span> ==</span><span> &#34;__main__&#34;</span><span>:</span></span>
<span><span>    cli()</span></span>
<span></span></code></pre>
<p>Btw why don&#39;t we just use an <code>array</code> instead of a Python <code>list</code> for our tests because after all an array lets us control the memory layout?</p>
<ul>
<li>For the <code>values</code>, we want to use a <code>list</code> because we&#39;re interested in the cache effects with typical Python datastructures.</li>
<li>For the <code>indices</code>, because accessing an <code>indices[…]</code> element would still have to create a <code>PyObject*</code>.
It&#39;s probably better to get all of these allocations out of the way before running the actual tests.</li>
</ul>
<p>How much memory will actually get used for a particular example size?</p>
<ul>
<li>While the above layouts <em>encode</em> integers as 32-bit or 64-bit ints (4 or 8 bytes),
the in-memory representation will be a <code>PyObject*</code>.</li>
<li>That&#39;s one pointer (8 bytes) for the pointer,</li>
<li>and another 28 bytes (3.5 pointers worth) for the <code>PyObject</code> data in its small integer mode
(calculated via <code>sys.getsizeof()</code>).</li>
<li>However, alignment considerations will probably round this up to 32 bytes (4 pointers worth).</li>
<li>In total, that is 5 pointers worth (40 bytes) per element in <code>values</code>, plus the same for the indices.</li>
<li>To observe meaningful results, our problem sizes should be a lot larger than the available cache size.
For 16MB cache, this means <code>values</code> should contain at least 420000 entries,
or half that amount if we also consider cache pressure from <code>indices</code>.</li>
</ul>
<p>This allows us to formulate some expectations:</p>
<ul>
<li>For problem sizes below around 200000, we expect linear and randomized access to work similarly well.</li>
<li>Hypothesis: for problem sizes above 420000, we expect linear array access to be significantly faster than randomized access.</li>
</ul>
<p>We can use the <a href="https://github.com/sharkdp/hyperfine"><code>hyperfine</code> tool (v1.18)</a> to run the individual benchmarks.
It supports parametrized benchmarks.</p>
<p>All experiments will use the following form, though we will vary <code>--iterations</code> and <code>size</code> parameters.</p>
<pre tabindex="0" data-language="bash"><code><span><span>hyperfine</span><span> \</span></span>
<span><span>  --setup</span><span> &#39;python generate_data.py --seed 1234 --iterations {iterations} --size {size} --{shuffle} &gt;example_{size}_{shuffle}.bin&#39;</span><span> \</span></span>
<span><span>  --command-name</span><span> &#39;size={size} iters={iterations} {shuffle}&#39;</span><span> \</span></span>
<span><span>  -L</span><span> size</span><span> 100,1000,10000,100000</span><span> \</span></span>
<span><span>  -L</span><span> iterations</span><span> 5</span><span> \</span></span>
<span><span>  -L</span><span> shuffle</span><span> shuffle,no-shuffle</span><span> \</span></span>
<span><span>  &#39;python run_benchmark.py &lt;example_{size}_{shuffle}.bin&#39;</span></span>
<span></span></code></pre>
<h2 id="results">Results</h2>
<p>We can now conduct a couple of experiments to investigate caching impact.</p>
<h3 id="small-data-sets">Small data sets</h3>
<p>Our above analysis suggests that problem sizes below around 200000 should not be affected by caching.
Let&#39;s ask <code>hyperfine</code> to compare shuffled access <code>for size in {50_000, 100_000, 200_000}</code>.
Note that we&#39;re also adjusting the iterations so that the total number of array accesses remains constant at 10 million,
which allows us also to compare results across sizes.</p>
<details><summary>Results for size=50_000</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=50000 iters=200 shuffle (iterations = 200)</span></span>
<span><span>  Time (mean ± σ):     462.0 ms ±  13.4 ms    [User: 456.7 ms, System: 5.2 ms]</span></span>
<span><span>  Range (min … max):   439.5 ms … 477.5 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=50000 iters=200 no-shuffle (iterations = 200)</span></span>
<span><span>  Time (mean ± σ):     374.1 ms ±  17.3 ms    [User: 369.2 ms, System: 4.8 ms]</span></span>
<span><span>  Range (min … max):   350.0 ms … 394.3 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=50000 iters=200 no-shuffle (iterations = 200) ran</span></span>
<span><span>    1.23 ± 0.07 times faster than size=50000 iters=200 shuffle (iterations = 200)</span></span>
<span><span></span></span></code></pre>
</details>
<details><summary>Results for size=100_000:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=100000 iters=100 shuffle (iterations = 100)</span></span>
<span><span>  Time (mean ± σ):     479.6 ms ±  19.6 ms    [User: 470.2 ms, System: 9.3 ms]</span></span>
<span><span>  Range (min … max):   453.7 ms … 519.8 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=100000 iters=100 no-shuffle (iterations = 100)</span></span>
<span><span>  Time (mean ± σ):     384.4 ms ±  14.0 ms    [User: 376.0 ms, System: 8.3 ms]</span></span>
<span><span>  Range (min … max):   361.4 ms … 405.8 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=100000 iters=100 no-shuffle (iterations = 100) ran</span></span>
<span><span>    1.25 ± 0.07 times faster than size=100000 iters=100 shuffle (iterations = 100)</span></span>
<span><span></span></span></code></pre>
</details>
<details><summary>Results for size=200_000:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=200000 iters=50 shuffle (iterations = 50)</span></span>
<span><span>  Time (mean ± σ):     567.8 ms ±  35.4 ms    [User: 557.7 ms, System: 9.9 ms]</span></span>
<span><span>  Range (min … max):   501.0 ms … 634.4 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=200000 iters=50 no-shuffle (iterations = 50)</span></span>
<span><span>  Time (mean ± σ):     385.2 ms ±  13.8 ms    [User: 377.3 ms, System: 7.8 ms]</span></span>
<span><span>  Range (min … max):   371.4 ms … 407.8 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=200000 iters=50 no-shuffle (iterations = 50) ran</span></span>
<span><span>    1.47 ± 0.11 times faster than size=200000 iters=50 shuffle (iterations = 50)</span></span>
<span><span></span></span></code></pre>
</details>
<p>Interpretation:</p>
<p>For all tested problem sizes, we see that randomized (<code>shuffle</code>) access is significantly slower than linear array access (<code>no-shuffle</code>).
For sizes <code>50_000</code> and <code>100_000</code> we see a factor of around 1.24.
But for the size <code>200_000</code> we&#39;re already up to a factor of <code>1.47</code>.</p>
<p>We can reformat the data to compare the mean elapsed time of <code>shuffle</code> and <code>no-shuffle</code> factors across different problem sizes:</p>





























<table><thead><tr><th>size</th><th>no-shuffle</th><th>shuffle</th><th>rel. slowdown</th></tr></thead><tbody><tr><td><code>50_000</code></td><td>374.1 ms ±  17.3 ms</td><td>462.0 ms ±  13.4 ms</td><td>1.23 ± 0.07</td></tr><tr><td><code>100_000</code></td><td>384.4 ms ±  14.0 ms</td><td>479.6 ms ±  19.6 ms</td><td>1.25 ± 0.07</td></tr><tr><td><code>200_000</code></td><td>385.2 ms ±  13.8 ms</td><td>567.8 ms ±  35.4 ms</td><td>1.47 ± 0.11</td></tr></tbody></table>
<p>We can see that the <code>no-shuffle</code> linear mode (left column) maintains constant speed across different problem sizes.
All of these timings are within each other&#39;s margin of error.</p>
<p>But for the <code>shuffle</code> mode, we see differences.
While the two smaller problem sizes have speeds within each other&#39;s margin of error,
the <code>size = 200_000</code> runs are significantly slower.</p>
<p>This is broadly in line with our expectations.</p>
<ul>
<li>We see that linear array access provides very consistent performance across problem sizes.</li>
<li>We see that random access is consistently slower, but the overhead isn&#39;t terrible (only a factor of about <code>1.24</code> at smaller problem sizes).</li>
<li>As we approach the region where our problem no longer fits into the cache, performance of random access starts to deteriorate.</li>
</ul>
<h3 id="large-data-sets">Large data sets</h3>
<p>Next, let&#39;s measure performance when we cross into the region of problem sizes that cannot fit into the cache.
In order to maintain a nice power-of-two sequence, the <code>iterations</code> were increased a bit,
keeping a constant volume of indexing operations at 12.8 million.</p>
<ul>
<li><code>size= 200_000 iters=64</code> (size matches largest experiment from the &#34;small data set&#34; round)</li>
<li><code>size= 400_000 iters=32</code></li>
<li><code>size= 800_000 iters=16</code></li>
<li><code>size=1600_000 iters= 8</code></li>
</ul>
<details><summary>Results for size=200_000:
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=200000 iters=64 shuffle (iterations = 64)</span></span>
<span><span>  Time (mean ± σ):     693.9 ms ±  40.1 ms    [User: 684.6 ms, System: 9.1 ms]</span></span>
<span><span>  Range (min … max):   641.0 ms … 751.8 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=200000 iters=64 no-shuffle (iterations = 64)</span></span>
<span><span>  Time (mean ± σ):     496.8 ms ±  10.3 ms    [User: 483.1 ms, System: 13.6 ms]</span></span>
<span><span>  Range (min … max):   482.2 ms … 515.4 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=200000 iters=64 no-shuffle (iterations = 64) ran</span></span>
<span><span>    1.40 ± 0.09 times faster than size=200000 iters=64 shuffle (iterations = 64)</span></span>
<span><span></span></span></code></pre>
</summary></details>
<details><summary>Results for size=400_000:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=400000 iters=32 shuffle (iterations = 32)</span></span>
<span><span>  Time (mean ± σ):      1.293 s ±  0.071 s    [User: 1.275 s, System: 0.018 s]</span></span>
<span><span>  Range (min … max):    1.217 s …  1.452 s    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=400000 iters=32 no-shuffle (iterations = 32)</span></span>
<span><span>  Time (mean ± σ):     515.2 ms ±  16.2 ms    [User: 497.3 ms, System: 17.7 ms]</span></span>
<span><span>  Range (min … max):   485.7 ms … 541.0 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=400000 iters=32 no-shuffle (iterations = 32) ran</span></span>
<span><span>    2.51 ± 0.16 times faster than size=400000 iters=32 shuffle (iterations = 32)</span></span>
<span><span></span></span></code></pre>
</details>
<details><summary>Results for size=800_000:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=800000 iters=16 shuffle (iterations = 16)</span></span>
<span><span>  Time (mean ± σ):      1.855 s ±  0.036 s    [User: 1.824 s, System: 0.030 s]</span></span>
<span><span>  Range (min … max):    1.805 s …  1.926 s    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=800000 iters=16 no-shuffle (iterations = 16)</span></span>
<span><span>  Time (mean ± σ):     550.0 ms ±  11.8 ms    [User: 521.5 ms, System: 28.4 ms]</span></span>
<span><span>  Range (min … max):   532.4 ms … 563.1 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=800000 iters=16 no-shuffle (iterations = 16) ran</span></span>
<span><span>    3.37 ± 0.10 times faster than size=800000 iters=16 shuffle (iterations = 16)</span></span>
<span><span></span></span></code></pre>
</details>
<details><summary>Results for size=1600_000:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=1600000 iters=8 shuffle (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):      2.379 s ±  0.034 s    [User: 2.327 s, System: 0.052 s]</span></span>
<span><span>  Range (min … max):    2.336 s …  2.428 s    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=1600000 iters=8 no-shuffle (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):     628.4 ms ±  12.2 ms    [User: 570.3 ms, System: 57.9 ms]</span></span>
<span><span>  Range (min … max):   606.3 ms … 647.2 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=1600000 iters=8 no-shuffle (iterations = 8) ran</span></span>
<span><span>    3.79 ± 0.09 times faster than size=1600000 iters=8 shuffle (iterations = 8)</span></span>
<span><span></span></span></code></pre>
</details>
<p>Table for comparing the mean elapsed time and the relative slowdown of random access:</p>



































<table><thead><tr><th>size</th><th>no-shuffle</th><th>shuffle</th><th>rel. slowdown</th></tr></thead><tbody><tr><td><code>200_000</code></td><td>496.8 ms ±  10.3 ms</td><td>693.9 ms ±  40.1 ms</td><td>1.40 ± 0.09</td></tr><tr><td><code>400_000</code></td><td>515.2 ms ±  16.2 ms</td><td>1.293 s ±  0.071 s</td><td>2.51 ± 0.16</td></tr><tr><td><code>800_000</code></td><td>550.0 ms ±  11.8 ms</td><td>1.855 s ±  0.036 s</td><td>3.37 ± 0.10</td></tr><tr><td><code>1600_000</code></td><td>628.4 ms ±  12.2 ms</td><td>2.379 s ±  0.034 s</td><td>3.79 ± 0.09</td></tr></tbody></table>
<p>Interpretation:</p>
<ul>
<li>For linear accesses (<code>no-shuffle</code> mode), we now see deteriorating performance as the problem sizes increase.
Part of this is due to the lower amount of iterations, so that the initialization overhead
(Python interpreter boot, parsing the input data) becomes more noticeable.</li>
<li>For random accesses (<code>shuffle</code> mode), we see dramatic slowdowns as problem sizes increase.
These cannot be explained solely by initialization overhead.</li>
<li>The relative slowdown between a linear access vs random access pair increases with the problem size.
At 1.6 million elements (a <code>values</code> list weighing around 60MB total, plus the same amount of data in <code>indices</code>),
we&#39;re up to a relative slowdown of about 3.8.
That means random access is 280% slower, or that linear access only needs 26% of the time.</li>
</ul>
<p>As the data set is equivalent between the linear-access and random-access variants,
this seems to confirm our experimental hypothesis that linear access is significantly faster,
which in turn provides evidence for our actual hypothesis that cache effects can have a significant impact in Python code.</p>
<h2 id="bonus-round-indirection-overhead">Bonus round: indirection overhead</h2>
<p>A problem with vanilla Python is the <code>PyObject*</code> pointer indirection,
and the overhead of the <code>PyObject</code> structure itself.
However, Python does offer facilities to circumvent this.
We can use the <code>numpy</code> module to obtain C-style flat arrays,
and perform the indexing and summing within native code:</p>
<pre tabindex="0" data-language="python"><code><span><span>import</span><span> sys, struct, numpy </span><span>as</span><span> np</span></span>
<span></span>
<span><span># parse the test case</span></span>
<span><span>header_layout </span><span>=</span><span> struct.Struct(</span><span>&#34;=4sLLQ&#34;</span><span>)</span></span>
<span><span>magic, size, iterations, expected_sum </span><span>=</span><span> \</span></span>
<span><span>    header_layout.unpack(sys.stdin.buffer.read(header_layout.size))</span></span>
<span><span>assert</span><span> magic </span><span>==</span><span> b</span><span>&#34;cach&#34;</span><span>,  </span><span>f</span><span>&#34;wrong magic number </span><span>{</span><span>magic</span><span>}</span><span>&#34;</span></span>
<span><span>indices </span><span>=</span><span> np.fromfile(sys.stdin.buffer, </span><span>dtype</span><span>=</span><span>&#34;L&#34;</span><span>, </span><span>count</span><span>=</span><span>size)</span></span>
<span><span>values </span><span>=</span><span> np.fromfile(sys.stdin.buffer, </span><span>dtype</span><span>=</span><span>&#34;L&#34;</span><span>, </span><span>count</span><span>=</span><span>size)</span></span>
<span><span>assert</span><span> sys.stdin.buffer.read() </span><span>==</span><span> b</span><span>&#34;&#34;</span><span>  # EOF</span></span>
<span></span>
<span><span># run the test case</span></span>
<span><span>for</span><span> _ </span><span>in</span><span> range</span><span>(iterations):</span></span>
<span><span>    s </span><span>=</span><span> np.sum(values[indices], </span><span>dtype</span><span>=</span><span>int</span><span>)</span></span>
<span><span>    assert</span><span> s </span><span>==</span><span> expected_sum</span></span>
<span></span></code></pre>
<p>We can now use Hyperfine to compare the Numpy-based implementation against vanilla Python
at a fixed problem size:</p>
<pre tabindex="0" data-language="bash"><code><span><span>hyperfine</span><span> \</span></span>
<span><span>  --setup</span><span> &#39;python generate_data.py --seed 1234 --iterations {iterations} --size {size} --{shuffle} &gt;example_{size}_{shuffle}.bin&#39;</span><span> \</span></span>
<span><span>  --command-name</span><span> &#39;size={size} iters={iterations} {shuffle} {implementation}&#39;</span><span> \</span></span>
<span><span>  -L</span><span> size</span><span> 1600000</span><span> \</span></span>
<span><span>  -L</span><span> iterations</span><span> 8</span><span> \</span></span>
<span><span>  -L</span><span> shuffle</span><span> shuffle,no-shuffle</span><span> \</span></span>
<span><span>  -L</span><span> implementation</span><span> benchmark,benchmark_numpy</span><span> \</span></span>
<span><span>  &#39;python run_{implementation}.py &lt;example_{size}_{shuffle}.bin&#39;</span></span>
<span></span></code></pre>
<details><summary>Results:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: size=1600000 iters=8 shuffle benchmark (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):      2.354 s ±  0.029 s    [User: 2.298 s, System: 0.055 s]</span></span>
<span><span>  Range (min … max):    2.315 s …  2.405 s    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: size=1600000 iters=8 no-shuffle benchmark (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):     618.1 ms ±  17.7 ms    [User: 567.3 ms, System: 50.6 ms]</span></span>
<span><span>  Range (min … max):   585.9 ms … 641.5 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 3: size=1600000 iters=8 shuffle benchmark_numpy (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):     214.8 ms ±  17.5 ms    [User: 1782.5 ms, System: 19.9 ms]</span></span>
<span><span>  Range (min … max):   195.8 ms … 255.0 ms    14 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 4: size=1600000 iters=8 no-shuffle benchmark_numpy (iterations = 8)</span></span>
<span><span>  Time (mean ± σ):     137.1 ms ±  11.0 ms    [User: 1726.8 ms, System: 18.6 ms]</span></span>
<span><span>  Range (min … max):   128.8 ms … 163.1 ms    18 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  size=1600000 iters=8 no-shuffle benchmark_numpy (iterations = 8) ran</span></span>
<span><span>    1.57 ± 0.18 times faster than size=1600000 iters=8 shuffle benchmark_numpy (iterations = 8)</span></span>
<span><span>    4.51 ± 0.38 times faster than size=1600000 iters=8 no-shuffle benchmark (iterations = 8)</span></span>
<span><span>   17.17 ± 1.39 times faster than size=1600000 iters=8 shuffle benchmark (iterations = 8)</span></span>
<span><span></span></span></code></pre>
</details>
<p>This looks like Numpy has a considerable performance advantage, which is rather unsurprising.
However, this is an apples-to-oranges comparison when it comes to cache effects.
Because Numpy arrays are densely packed with no pointer indirection and no per-item object overhead,
they take up much less memory.
Here, we only need 8 bytes per item with Numpy, compared to about 40 bytes per item in vanilla Python – a 5× difference.</p>
<p>Instead, we can run an experiment for the same memory usage,
but with different <code>iterations</code> to achieve a comparable number of indexing operations.
This is arguably not a valid comparison, but it&#39;s a nice illustration.
Here, let&#39;s target a volume of 20 million operations, divided as</p>
<ul>
<li>4 million values at 5 iterations for Numpy</li>
<li>0.8 million values at 25 iterations for vanilla Python</li>
</ul>
<pre tabindex="0" data-language="bash"><code><span><span>python</span><span> generate_data.py</span><span> --seed</span><span> 1234</span><span> --size</span><span> 800000</span><span> --iterations</span><span> 25</span><span> --shuffle</span><span>    &gt;</span><span>example_vanilla_shuffle.bin</span></span>
<span><span>python</span><span> generate_data.py</span><span> --seed</span><span> 1234</span><span> --size</span><span> 800000</span><span> --iterations</span><span> 25</span><span> --no-shuffle</span><span> &gt;</span><span>example_vanilla_no-shuffle.bin</span></span>
<span><span>python</span><span> generate_data.py</span><span> --seed</span><span> 1234</span><span> --size</span><span> 4000000</span><span> --iterations</span><span> 5</span><span> --shuffle</span><span>    &gt;</span><span>example_numpy_shuffle.bin</span></span>
<span><span>python</span><span> generate_data.py</span><span> --seed</span><span> 1234</span><span> --size</span><span> 4000000</span><span> --iterations</span><span> 5</span><span> --no-shuffle</span><span> &gt;</span><span>example_numpy_no-shuffle.bin</span></span>
<span><span>hyperfine</span><span> \</span></span>
<span><span>    --command-name</span><span> &#39;{implementation} {shuffle}&#39;</span><span> \</span></span>
<span><span>    -L</span><span> implementation</span><span> vanilla,numpy</span><span> \</span></span>
<span><span>    -L</span><span> shuffle</span><span> shuffle,no-shuffle</span><span> \</span></span>
<span><span>    &#39;python run_benchmark_{implementation}.py &lt;example_{implementation}_{shuffle}.bin&#39;</span></span>
<span></span></code></pre>
<details><summary>Results:</summary>
<pre tabindex="0" data-language="plaintext"><code><span><span>Benchmark 1: vanilla shuffle</span></span>
<span><span>  Time (mean ± σ):      2.864 s ±  0.016 s    [User: 2.828 s, System: 0.035 s]</span></span>
<span><span>  Range (min … max):    2.832 s …  2.884 s    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 2: numpy shuffle</span></span>
<span><span>  Time (mean ± σ):     336.8 ms ±  14.9 ms    [User: 1918.3 ms, System: 27.0 ms]</span></span>
<span><span>  Range (min … max):   319.3 ms … 369.8 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 3: vanilla no-shuffle</span></span>
<span><span>  Time (mean ± σ):     791.2 ms ±  21.7 ms    [User: 758.6 ms, System: 32.2 ms]</span></span>
<span><span>  Range (min … max):   758.3 ms … 822.7 ms    10 runs</span></span>
<span><span></span></span>
<span><span>Benchmark 4: numpy no-shuffle</span></span>
<span><span>  Time (mean ± σ):     167.8 ms ±   9.0 ms    [User: 1745.3 ms, System: 30.2 ms]</span></span>
<span><span>  Range (min … max):   152.7 ms … 187.6 ms    17 runs</span></span>
<span><span></span></span>
<span><span>Summary</span></span>
<span><span>  numpy no-shuffle ran</span></span>
<span><span>    2.01 ± 0.14 times faster than numpy shuffle</span></span>
<span><span>    4.71 ± 0.28 times faster than vanilla no-shuffle</span></span>
<span><span>   17.06 ± 0.92 times faster than vanilla shuffle</span></span>
<span><span></span></span></code></pre>
</details>
<p>Table for pairwise comparisons of each implementation:</p>























<table><thead><tr><th>implementation</th><th>no-shuffle</th><th>shuffle</th><th>rel. slowdown</th></tr></thead><tbody><tr><td>numpy</td><td>167.8 ms ±   9.0 ms</td><td>336.8 ms ±  14.9 ms</td><td>2.01 ± 0.14</td></tr><tr><td>vanilla</td><td>791.2 ms ±  21.7 ms</td><td>2.864 s ±  0.016 s</td><td>3.62 ± 0.10</td></tr></tbody></table>
<!-- vanilla slowdown:
f = 2864 / 791.2 = 3.619817997977755;
stdev = f * sqrt((16/2864)^2 + (21.7/791.2)^2) = 0.1013182812933839;
-->
<p>Now, we can clearly see that both the Numpy and vanilla Python implementations
are affected by cache pressure,
with the cache-friendly version being 2× and 3.6× faster, respectively.
That the pure-Python version suffered a greater slowdown
could indicate that the more complex <code>PyObject*</code> layout is substantially more cache-hostile,
but I have a hunch that this might not be the entire story,
e.g. because these measurements also include input parsing overhead.
Future work might involve using tools like Valgrind&#39;s <code>cachegrind</code>
to gain more detailed insight into how these programs interact with CPU caches,
or could use a Numpy array for both implementations,
and only compare whether the summing and indexing occurs in C code or Python code.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In these experiments, we demonstrated that even though Python may be slow compared to native code,
cache effects do have measurable impact on performance.
We used fairly large lists to compare sequential element access to random access.
We found that depending on list size, the random access was between 23% and 280% slower.
That is nontrivial overhead worth optimizing for, at least for CPU-bound Python code.</p>
<p>The practical effect of this should be negligible though,
as CPU-bound Python code using lists with lengths in the 10⁴+ range should be fairly rare.</p>
<h2 id="further-reading">Further reading</h2>
<p><a href="https://agner.org/optimize/">Agner Fog&#39;s optimization manuals</a>
are an indispensable and fairly accessible resource on microarchitecture details.
For example, the microarchitecture manual provides tables on cache latencies,
and the C++ and assembly optimization manuals each contain sections on optimizing memory access patterns.</p>
<p><a href="https://norvig.com/21-days.html#answers">Norvig&#39;s table of Latencies Every Programmer Should Know</a>
is a neat overview latencies.
The relative orders of magnitude remain relevant,
even if that resource is over a decade old.
<a href="https://github.com/sirupsen/napkin-math">Sirupsen&#39;s <em>Napkin Math</em> collection</a>
provides similar tables that are more up to date,
but is focused more on general system performance.
The collection to further performance resources is particularly nice.</p> </div></div>
  </body>
</html>
