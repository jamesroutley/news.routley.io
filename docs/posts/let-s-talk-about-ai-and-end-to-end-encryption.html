<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/">Original</a>
    <h1>Let&#39;s talk about AI and end-to-end encryption</h1>
    
    <div id="readability-page-1" class="page"><article id="post-7943">
	
	<div>
		
<p>Recently I came across a fantastic new paper by a group of NYU and Cornell researchers entitled “<a href="https://eprint.iacr.org/2024/2086">How to think about end-to-end encryption and AI</a>.” I’m extremely grateful to see this paper, because while I don’t agree with every one of its conclusions, it’s a good first stab at an incredibly important set of questions.</p>



<p>I was particularly happy to see people thinking about this topic, since it’s been on my mind in a half-formed state this past few months. On the one hand, my interest in the topic was piqued by the deployment of new AI assistant systems like <a href="https://security.googleblog.com/2024/11/new-real-time-protections-on-Android.html">Google’s scam call protection</a> and <a href="https://www.apple.com/apple-intelligence/">Apple Intelligence</a>, both of which aim to put AI basically everywhere on your phone — even, critically, right in the middle of your private messages. On the other hand, I’ve been thinking about the negative privacy implications of AI due to the recent European debate over “<a href="https://www.patrick-breyer.de/en/posts/chat-control/">mandatory content scanning</a>” laws that would require machine learning systems to scan virtually every private message you send.</p>



<p>While these two subjects arrive from very different directions, I’ve come to believe that they’re going to end up in the same place. And as someone who has been worrying about encryption — and debating the “crypto wars” for over a decade — this has forced me to ask some uncomfortable questions about what the future of end-to-end encrypted privacy will look like. Maybe, even, whether it actually <em>has</em> a future.</p>



<p>But let’s start from an easier place.</p>



<h3>What is end-to-end encryption, and what does AI have to do with it?</h3>



<p>From a privacy perspective, the most important story of the past decade or so has been the rise of end-to-end encrypted communications platforms. Prior to 2011, most cloud-connected devices simply uploaded their data in plaintext. For many people this meant their private data was <a href="https://en.wikipedia.org/wiki/2014_celebrity_nude_photo_leak">exposed to hackers</a>, civil subpoenas, government warrants, or business exploitation by the platforms themselves. Unless you were an advanced user who used tools like <a href="https://blog.cryptographyengineering.com/2014/08/13/whats-matter-with-pgp/">PGP</a> and <a href="https://otr.cypherpunks.ca/">OTR</a>, most end-users had to suffer the consequences.</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-7.png"><img data-attachment-id="8009" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-49/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-7.png" data-orig-size="2028,800" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-7.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-7.png?w=700" width="1024" height="403" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-7.png?w=1024" alt=""/></a><figcaption>Headline about the <a href="https://en.wikipedia.org/wiki/Salt_Typhoon">Salt Typhoon</a> group, an APT threat that has essentially owned US telecom infrastructure by compromising “wiretapping” systems. Oh how the worm has turned.</figcaption></figure></div>


<p>Around 2011 our approach to data storage began to evolve. This began with messaging apps like <a href="https://support.signal.org/hc/en-us/articles/360007320391-Is-it-private-Can-I-trust-it">Signal</a>, <a href="https://support.apple.com/guide/security/imessage-security-overview-secd9764312f/web">Apple iMessage</a> and <a href="https://faq.whatsapp.com/820124435853543">WhatsApp</a>, all of which began to roll out default end-to-end encryption for private messaging. This technology changed the way that keys are managed, to ensure that servers would <em>never see the plaintext content of your messages.</em> Shortly afterwards, phone (OS) designers like Google, Samsung and Apple began encrypting the data stored locally on your phone, a move that occasioned some famous <a href="https://epic.org/documents/apple-v-fbi-2/">arguments with law enforcement</a>. More recently, Google <a href="https://www.androidcentral.com/how-googles-backup-encryption-works-good-bad-and-ugly">introduced default end-to-end encryption for phone backups</a>, and (somewhat <a href="https://www.reuters.com/article/world/exclusive-apple-dropped-plan-for-encrypting-backups-after-fbi-complained-sour-idUSKBN1ZK1CO/">belatedly</a>) Apple has <a href="https://support.apple.com/en-us/108756">begun to follow.</a></p>



<p>Now I don’t want to minimize the engineering effort required here, which was massive. But these projects were also relatively simple. By this I mean: all of the data encrypted in these projects shared a common feature, which is that <em>none of it needed to be processed by a server.</em></p>



<p>The obvious limitation of end-to-end encryption is that while it can hide content from servers, it can also make it very difficult for servers to compute on that data.<sup>1</sup> This is basically fine for data like cloud backups and private messages, since that content is mostly interesting to clients. For data that actually requires serious processing, the options are much more limited. Wherever this is the case — for example, consider a phone that applies <a href="https://support.apple.com/guide/iphone/use-live-text-iphcf0b71b0e/ios">text recognition</a> to the photos in your camera roll — designers typically get forced into a binary choice. On the one hand they can (1) send plaintext off to a server, in the process resurrecting many of the earlier vulnerabilities that end-to-end encryption sought to close. Or else (2) they can limit their processing to whatever can be <a href="https://www.theverge.com/2017/9/13/16300464/apple-iphone-x-ai-neural-engine">performed</a> <a href="https://developers.google.com/ml-kit/vision/text-recognition/v2/android">on the device</a> itself.</p>



<p>The problem with the second solution is that your typical phone only has a limited amount of processing power, not to mention RAM and battery capacity. Even the top-end iPhone will typically perform photo processing while it’s plugged in at night, just to avoid burning through your battery when you might need it. Moreover, there is a huge amount of variation in phone hardware. Some flagship phones will run you $1400 or more, and contain onboard GPUs and neural engines. But these phones aren’t typical, in the US and particularly around the world. Even in the US it’s still possible to buy a decent Android phone for a couple hundred bucks, and a lousy one for much less. There is going to be a vast world of difference between these devices in terms of what they can process.</p>



<p>So now we’re finally ready to talk about AI.</p>



<p>Unless you’ve been living under a rock, you’ve probably noticed the explosion of powerful new AI models with amazing capabilities. These include Large Language Models (LLMs) which can generate and understand complex human text, as well as new image processing models that can do amazing things in that medium. You’ve probably also noticed Silicon Valley’s enthusiasm to find applications for this tech. And since phone and messaging companies <em>are</em> Silicon Valley, your phone and its apps are no exception. Even if these firms don’t quite know <em>how</em> AI will be useful to their customers, they’ve already decided that models are the future. In practice this has already manifested in the deployment of applications such as <a href="https://arstechnica.com/apple/2024/11/apple-intelligence-notification-summaries-are-honestly-pretty-bad/">text message summarization</a>, systems that listen to and detect <a href="https://support.google.com/pixelphone/thread/307725284/learn-more-about-the-new-scam-detection-beta?hl=en">scam phone calls</a>, models that detect <a href="https://support.apple.com/en-us/105069">offensive images</a>, as well as new systems that help you <a href="https://support.apple.com/guide/iphone/use-writing-tools-iph6f08da1d2/ios">compose text</a>.</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-2.png"><img data-attachment-id="7959" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-44/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-2.png" data-orig-size="1128,581" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-2.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-2.png?w=700" width="1024" height="527" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-2.png?w=1024" alt=""/></a></figure></div>


<p>And these are obviously just the beginning.</p>



<p>If you believe the AI <a href="https://www.youtube.com/watch?v=_-8j28KFXvQ">futurists</a> — and in this case, I actually think you should — all these toys are really just a palate cleanser for the<em> </em>main entrée still to come: the deployment of “AI agents,” aka, <a href="https://www.goodreads.com/quotes/985552-the-encyclopedia-galactica-defines-a-robot-as-a-mechanical-apparatus">Your Plastic Pal Who’s Fun To Be With</a>. </p>



<p><em>In theory</em> these new agent systems will remove your need to ever bother messing with your phone again. They’ll read your email and text messages and they’ll answer for you. They’ll order your food and find the best deals on shopping, swipe your dating profile, negotiate with your lenders, and generally anticipate your every want or need. The only ingredient they’ll need to realize this blissful future is <em>virtually unrestricted access to all your private data</em>, plus a whole gob of computing power to process it.</p>



<p>Which finally brings us to the sticky part. </p>



<p>Since most phones currently don’t have the compute to run very powerful models, and since models keep getting better and in some cases <a href="https://techcrunch.com/2025/01/10/vcs-say-ai-companies-need-proprietary-data-to-stand-out-from-the-pack/">more proprietary</a>, it is likely that much of this processing (and the data you need processed) will have to be offloaded to remote servers.</p>



<p>And that’s the first reason that I would say that AI is going to be the biggest privacy story of the decade. Not only will we soon be doing more of our compute off-device, but we’ll be sending a lot more of our private data. This data will be examined and summarized by increasingly powerful systems, producing relatively compact but valuable summaries of our lives. In principle those systems will eventually know everything about us and about our friends. They’ll read our most intimate private conversations, maybe they’ll even intuit our deepest innermost thoughts. We are about to face many hard questions about these systems, including some difficult questions about <em>whether they will actually be working for us at all.</em></p>



<p>But I’ve gone about two steps farther than I intended to. Let’s start with the easier questions.</p>



<h3>What does AI mean for end-to-end encrypted messaging?</h3>



<p>Modern end-to-end encrypted messaging systems provide a very specific set of technical guarantees. Concretely, an end-to-end encrypted system is designed to ensure that plaintext message content <span>in transit</span> is not available anywhere except for the end-devices of the participants and (here comes the Big Asterisk) <em>anyone the participants or their devices choose to share it with.</em></p>



<p>The last part of that sentence is very important, and it’s obvious that non-technical users get pretty confused about it. End-to-end encrypted messaging systems are intended to deliver data securely. <em>They don’t dictate what happens to it next.</em> If you take screenshots of your messages, back them up in plaintext, copy/paste your data onto Twitter, or hand over your device in response to a civil lawsuit — well, that has really nothing to do with end-to-end encryption. Once the data has been delivered from one end to the other, end-to-end encryption washes its hands and goes home</p>



<p>Of course there’s a difference between <em>technical guarantees</em> and the promises that individual service providers make to their customers. For example, Apple says this about its iMessage service:</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-3.png"><img data-attachment-id="7968" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-45/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-3.png" data-orig-size="2398,404" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-3.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-3.png?w=700" width="1024" height="172" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-3.png?w=1024" alt=""/></a></figure></div>


<p>I can see how these promises might get a little bit confusing! For example, imagine that Apple keeps its promise to deliver messages securely, but then your (Apple) phone goes ahead and uploads (the plaintext) message content to a different set of servers where <span>Apple really can</span> decrypt it. Apple is absolutely using end-to-end encryption in the dullest technical sense… yet is the statement above really accurate? Is Apple keeping its broader promise that it “can’t decrypt the data”?</p>



<p>Note: I am not picking on Apple here! This is just one paragraph from a <a href="https://support.apple.com/guide/security/welcome/web">security overview</a>. In a separate legal document, Apple’s lawyers have written a <a href="https://www.apple.com/legal/privacy/data/en/messages/">zillion words to cover their asses</a>. My point here is just to point out that a <em>technical guarantee</em> is different from a <em>user promise</em>.</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/img_9881.jpg"><img data-attachment-id="7970" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/img_9881/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/img_9881.jpg" data-orig-size="1059,375" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;1&#34;}" data-image-title="IMG_9881" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/img_9881.jpg?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/img_9881.jpg?w=700" loading="lazy" width="1024" height="362" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/img_9881.jpg?w=1024" alt=""/></a></figure></div>


<p>Making things more complicated, this might not be a question about your own actions. Consider the promise you receive every time you start a WhatsApp group chat (at right.) Now imagine that some other member of the group — not you, but one of your idiot friends — decides to turn on some service that uploads (your) received plaintext messages to WhatsApp. Would you still say the message you received still accurately describes how WhatsApp treats your data? If not, how should it change?</p>



<p>In general, what we’re asking here is a question about <em>informed</em> <em>consent. </em>Consent is complicated because it’s both a moral concept and also a legal one, and because the law is different in every corner of the world. The <a href="https://eprint.iacr.org/2024/2086.pdf">NYU/Cornell paper</a> does an excellent job giving an overview of the legal bits, but — and sorry to be a bummer here — I really don’t want you to expect the law to protect you. I imagine that some companies will do a good job informing their users, as a way to earn trust. Other companies, won’t. Here in the US, they’ll bury your consent inside of an inscrutable “terms of service” document you never read. In the EU they’ll probably just invent a new type of cookie banner.</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-4.png"><img data-attachment-id="7973" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-46/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-4.png" data-orig-size="2054,908" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-4.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-4.png?w=700" loading="lazy" width="1024" height="452" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-4.png?w=1024" alt=""/></a><figcaption><em>This is obviously a joke. But, like, maybe not?</em></figcaption></figure></div>


<p>So to summarize: in the near term we’re headed to a world where we should expect increasing amounts of our data to be processed by AI, and a lot of that processing will (very likely) be off-device. Good end-to-end encrypted services will actively inform you that this is happening, so maybe you’ll have the chance to opt-in or opt-out. But if it’s <em>truly ubiquitous</em> (as our futurist friends tell us it will be) then probably your options will be end up being pretty limited.</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-5.png"><img data-attachment-id="7979" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-47/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-5.png" data-orig-size="672,186" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-5.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-5.png?w=672" loading="lazy" width="672" height="186" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-5.png?w=672" alt=""/></a><figcaption>Some ideas for avoiding AI inference on your private messages (cribbed from <a href="https://www.usenix.org/system/files/1401_08-12_mickens.pdf">Mickens</a>.) </figcaption></figure></div>


<p>Fortunately all is not lost. In the short term, a few people have been thinking hard about this problem.</p>



<h3>Trusted Hardware and Apple’s “Private Cloud Compute”</h3>



<p>The good news is that our coming AI future isn’t going to be a total privacy disaster. And in large part that’s because a few privacy-focused companies like Apple have anticipated the problems that ML inference will create (namely the need to outsource processing to better hardware) and have tried to do something about it. This something is essentially a new type of cloud-based computer that Apple believes is trustworthy enough to hold your private data. </p>



<p>Quick reminder: as we already discussed, Apple can’t rely on every device possessing enough power to perform inference locally. This means inference will be outsourced to a remote cloud machine. Now the <em>connection</em> from your phone to the server can be encrypted, but the remote machine will still receive confidential data that it must see in cleartext. This poses a huge risk at that machine. It could be compromised and the data stolen by hackers, or worse, Apple’s business development executives could find out about it.</p>



<p>Apple’s <a href="https://security.apple.com/blog/private-cloud-compute/">approach to this problem</a> is called “Private Cloud Compute” and it involves the use of special <em>trusted hardware</em> devices that run in Apple’s data centers. A “trusted hardware device” is a kind of computer that is locked-down in both the physical and logical sense. Apple’s devices are house-made, and use custom silicon and software features. One of these is Apple’s <a href="https://support.apple.com/guide/security/boot-process-for-iphone-and-ipad-devices-secb3000f149/web">Secure Boot</a>, which ensures that only “allowed” operating system software is loaded. The OS then uses code-signing to verify that only permitted software images can run. Apple ensures that no long-term state is stored on these machines, and also load-balances your request to a different random server every time you connect. The machines “attest” to the application software they run, meaning that they announce a hash of the software image (which must itself be stored in a verifiable <a href="https://en.wikipedia.org/wiki/Certificate_Transparency">transparency log</a> to prevent any new software from surreptitiously being added.) Apple even says it will <a href="https://security.apple.com/blog/pcc-security-research/">publish its software</a> images (though unfortunately not [<strong>update:</strong> <a href="https://github.com/apple/security-pcc/">all of</a>] the source code) so that security researchers can check them over for bugs.</p>



<p>The goal of this system is to make it hard for both attackers <em>and </em>Apple employees to exfiltrate data from these devices. I won’t say I’m thrilled about this. It goes without saying that Apple’s approach is a vastly weaker guarantee than encryption — it still centralizes a huge amount of valuable data, and its security relies on Apple getting a bunch of complicated software and hardware security features right, rather than the mathematics of an encryption algorithm. Yet this approach is obviously much better than what’s being done at companies like OpenAI, where the data is processed by servers that employees (presumably) can log into and access.</p>



<p>Although PCC is currently unique to Apple, we can hope that other privacy-focused services will soon crib the idea — after all, imitation is the best form of flattery. In this world, if we absolutely must have AI everywhere, at least the result will be a bit more secure than it might otherwise be.</p>



<h3>Who does your AI agent actually work for?</h3>



<p>There are <em>so</em> many important questions that I haven’t discussed in this post, and I feel guilty because I’m not going to get to them. I have not, for example, discussed the very important problem of the data used to <em>train</em> (and fine-tune) future AI models, something that opens entire oceans of privacy questions. If you’re interested, these problems are discussed in the <a href="https://eprint.iacr.org/2024/2086.pdf">NYU/Cornell paper.</a>  </p>



<p>But rather than go into that, I want to turn this discussion towards a different question: namely, <em>who are these models actually going to be working <span>for</span>? </em></p>



<p>As I mentioned above, over the past couple of years I’ve been watching <a href="https://www.gov.uk/government/publications/end-to-end-encryption-and-child-safety/end-to-end-encryption-and-child-safety">the UK</a> and <a href="https://www.europarl.europa.eu/RegData/etudes/ATAG/2023/751473/EPRS_ATA(2023)751473_EN.pdf">EU</a> debate new laws that would mandate automated “scanning” of private, encrypted messages. The EU’s proposal is focused on detecting both existing and <em>novel</em> child sexual abuse material (CSAM); it has at various points also included proposals to detect <a href="https://www.edpb.europa.eu/system/files/2024-02/edpb_statement_202401_proposal_regulation_prevent_combat_child_sexual_abuse_en.pdf">audio and textual conversations</a> that represent “<a href="https://thehackernews.com/2022/05/eu-proposes-new-rules-for-tech.html">grooming behavior</a>.” The UK’s proposal is a bit wilder and covers many different types of illegal content, including hate speech, terrorism content and fraud — one proposed amendment even covered “<a href="https://edri.org/our-work/the-uk-will-treat-online-images-of-immigrants-crossing-the-channel-as-a-criminal-offence/">images of immigrants crossing the Channel in small boats</a>.”</p>


<div>
<figure><a href="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-6.png"><img data-attachment-id="7995" data-permalink="https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/image-48/" data-orig-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-6.png" data-orig-size="1500,1000" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-6.png?w=300" data-large-file="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-6.png?w=700" loading="lazy" width="1024" height="682" src="https://blog.cryptographyengineering.com/wp-content/uploads/2025/01/image-6.png?w=1024" alt=""/></a></figure></div>


<p>While scanning<em> </em>for known<em> </em>CSAM does not require AI/ML, detecting nearly every one of the other types of content would require powerful ML inference that can operate on your private data. (Consider, for example, what is involved in detecting <a href="https://blog.google/technology/safety-security/how-we-detect-remove-and-report-child-sexual-abuse-material/">novel CSAM content</a>.) Plans to detect audio or textual “grooming behavior” and hate speech will require even more powerful capabilities: not just speech-to-text capabilities, but also some ability to truly understand the topic of human conversations without false positives. It is hard to believe that politicians even understand what they’re asking for. And yet some of them <a href="https://www.euractiv.com/section/politics/news/swedish-parliament-divided-over-eus-new-controversial-chat-control-bill/"><span>are</span> asking for it</a>, in a few cases very eagerly.</p>



<p>These proposals haven’t been implemented yet. But to some degree this is because ML systems that securely process private data are very challenging to build, and technical platforms have <a href="https://www.politico.eu/article/meta-encryption-uk-fight/">been resistant to building them.</a> Now I invite you to imagine a world where we <em>voluntarily go ahead and build </em>general-purpose agents that are capable of all of these tasks and more. You might do everything in your technical power to keep them under the user’s control, but can you guarantee that they will remain that way?</p>



<p>Or put differently: would you even blame governments for demanding access to a resource like this? And how would you stop them? After all, think about how much time and money a law enforcement agency could save by asking your agent sophisticated questions about your behavior and data, questions like: “does this user have any potential CSAM,” or “have they written anything that could potentially be hate speech in their private notes,” or “do you think maybe they’re cheating on their taxes?” You might even convince yourself that these questions are “privacy preserving,” since no human police officer would ever rummage through your papers, and law enforcement would only learn the answer if you were (probably) doing something illegal.</p>



<p>This future worries me because it doesn’t really matter what technical choices we make around privacy. It does not matter if your model is running locally, or if it uses trusted cloud hardware — <em>once a sufficiently-powerful general-purpose agent has been deployed on your phone</em>, <em>the only question that remains is who is given access to talk to it.</em> Will it be only you? Or will we prioritize the government’s interest in monitoring its citizens over various fuddy-duddy notions of individual privacy.</p>



<p>And while I’d like to hope that we, as a society, will make the right political choice in this instance, frankly I’m just not that confident.</p>



<p><em>Notes:</em></p>



<ol>
<li>(A quick note: some will suggest that Apple should use <a href="https://blog.cryptographyengineering.com/2012/01/02/very-casual-introduction-to-fully/">fully-homomorphic encryption</a> [FHE] for this calculation, so the private data can remain encrypted. This is <em>theoretically</em> possible, but unlikely to be practical. The best FHE schemes we have today really only work for evaluating very tiny ML models, of the sort that would be practical to run on a weak client device. While schemes will get better and hardware will too, I suspect this barrier will exist for a long time to come.)</li>
</ol>








	</div><!-- .entry-content -->

			<!-- .entry-footer -->
	</article></div>
  </body>
</html>
