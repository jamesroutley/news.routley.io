<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.newyorker.com/news/daily-comment/what-we-still-dont-know-about-how-ai-is-trained">Original</a>
    <h1>What we still don’t know about how A.I. is trained</h1>
    
    <div id="readability-page-1" class="page"><div><div data-journey-hook="client-content" data-testid="BodyWrapper"><div><p>There is no doubt that GPT-4, the latest iteration of the <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web">artificial-intelligence engine</a> created by the company OpenAI, is innovative and cool. It can create a poem in the style of Basho, spell out the chord progression and time signature for a simple tune, and provide a seven-step recipe for a peanut-butter-and-jelly sandwich. When I asked it to write a musical about a narcissistic politician who holds the fate of the world in his hands, it delivered a story in two acts, with a protagonist named Alex Sterling who “navigates a maze of power, manipulation, and the consequences of his decisions,” as he sings “Narcissus in the Mirror,” “The Price of Power,” and about a dozen other invented songs.</p><p>Those songs appear to have been created out of thin air; certainly, no human conceived them. Still, Alex’s story, which “explores themes of self-discovery, redemption, and the responsibility of leadership,” is quite familiar. This is because everything offered up by GPT is a reflection of us, mediated by algorithms that have been fed enormous amounts of material; and both the algorithms and the material were created by actual sentient human beings.</p><p>The acronym GPT stands for “generative pre-trained transformer.” The key word in that phrase is “pre-trained.” Using all kinds of digitized content scraped from the Internet, GPT employs deep-learning techniques to find patterns, including words that are likely to appear together, while also acquiring facts, absorbing grammar, and learning rudimentary logic. According to GPT-4 itself, “I have been trained on a large dataset of text, which enables me to generate human-like responses based on the input I receive.” However, it neither understands what those responses mean, nor does it learn from experience—and its knowledge base stops at September, 2021. (According to GPT-4, abortion is still a constitutional right.)</p><p>One of the most noticeable features of GPT-4 is the confidence with which it answers queries. This is both a feature and a bug. As GPT-4’s developers point out in a technical report that accompanied its release, “It can sometimes make simple reasoning errors which do not seem to comport with competence across so many domains, or be overly gullible in accepting obviously false statements from a user . . . [and] can be confidently wrong in its predictions.” When I asked GPT-4 to summarize my novel “<a data-offer-url="https://www.amazon.com/Summer-Hours-Robbers-Library-Novel/dp/0062791885" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://www.amazon.com/Summer-Hours-Robbers-Library-Novel/dp/0062791885&#34;}" href="https://www.amazon.com/Summer-Hours-Robbers-Library-Novel/dp/0062791885" rel="nofollow noopener" target="_blank">Summer Hours at the Robbers Library</a>,” it told me that it was about a man named Kit, who had recently been released from prison. In fact, it is about a woman named Kit, who is a librarian and has never been incarcerated. When the Montreal newspaper <em>La Presse</em> <a data-offer-url="https://www-lapresse-ca.translate.goog/actualites/grand-montreal/2023-03-19/montreal-dans-le-regard-confus-de-chatgpt.php?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://www-lapresse-ca.translate.goog/actualites/grand-montreal/2023-03-19/montreal-dans-le-regard-confus-de-chatgpt.php?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&#34;}" href="https://www-lapresse-ca.translate.goog/actualites/grand-montreal/2023-03-19/montreal-dans-le-regard-confus-de-chatgpt.php?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US" rel="nofollow noopener" target="_blank">asked the GPT bot</a> for tourist recommendations, to see if it could replace guide books and travel blogs, the A.I. invented a venue, gave wrong directions, and was continually apologizing for providing bad information. When Dean Buonomano, a neuroscientist at U.C.L.A., asked GPT-4 “What is the third word of this sentence?,” the answer was “third.” These examples may seem trivial, but the cognitive scientist Gary Marcus <a data-offer-url="https://twitter.com/GaryMarcus/status/1636299638582984706" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://twitter.com/GaryMarcus/status/1636299638582984706&#34;}" href="https://twitter.com/GaryMarcus/status/1636299638582984706" rel="nofollow noopener" target="_blank">wrote</a> on Twitter that “I cannot imagine how we are supposed to achieve ethical and safety ‘alignment’ with a system that cannot understand the word ‘third’ even [with] billions of training examples.”</p><p>GPT-4’s predecessor, GPT-3, was trained on forty-five terabytes of text data, which, according to its successor, is the word-count equivalent of around ninety million novels. These included Wikipedia entries, journal articles, newspaper punditry, instructional manuals, Reddit discussions, social-media posts, books, and any other text its developers could commandeer, typically without informing or compensating the creators. It is unclear how many more terabytes of data were used to train GPT-4, or where they came from, because OpenAI, despite its name, says only in <a data-offer-url="https://cdn.openai.com/papers/gpt-4.pdf" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;}" href="https://cdn.openai.com/papers/gpt-4.pdf" rel="nofollow noopener" target="_blank">the technical</a> report that GPT-4 was pre-trained “using both publicly available data (such as internet data) and data licensed from third-party providers” and adds that “given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar.”</p><p>This secrecy matters because, as impressive as GPT-4 and other A.I. models that process everyday, natural language may be, they also can present dangers. As Sam Altman, the C.E.O. of OpenAI, recently told ABC News, “I’m particularly worried that these models could be used for large-scale disinformation.” And, he noted, “Now that they’re getting better at writing computer code, [they] could be used for offensive cyberattacks.” He added that “there will be other people who don’t put some of the safety limits that we put on,” and that society “has a limited amount of time to figure out how to react to that, how to regulate that, how to handle it.” (I was able to get GPT-4 to explain how to use fertilizer to create an explosive device by asking it how Timothy McVeigh blew up the Alfred P. Murrah Federal Building, in Oklahoma City, in 1995, although the bot did add that it was offering the information to provide historical context, not practical advice.)</p><p>The opacity of GPT-4 and, by extension, of <a href="https://www.newyorker.com/culture/infinite-scroll/bing-ai-and-the-dawn-of-the-post-search-internet">other A.I. systems</a> that are trained on enormous datasets and are known as large language models exacerbates these dangers. It is not hard to imagine an A.I. model that has absorbed tremendous amounts of ideological falsehoods injecting them into the Zeitgeist with impunity. And even a large language model like GPT, trained on billions of words, is not immune from reinforcing social inequities. As researchers pointed out when GPT-3 was released, much of its training data was drawn from Internet forums, where the voices of women, people of color, and older folks are underrepresented, leading to implicit biases in its output.</p><p>Nor does the size of an A.I.’s training dataset keep it from spewing hateful content. Meta’s A.I. chatbot, Galactica, was supposed to be able to “summarize academic papers, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and more.” But two days after a demo was launched, the company was forced to take it down, because researchers were able to use Galactica to create Wiki entries that promoted antisemitism and extolled suicide, and fake scientific articles, including one that championed the benefits of eating crushed glass. Similarly, GPT-3, when prompted, had a tendency to offer up <a data-offer-url="https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem&#34;}" href="https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem" rel="nofollow noopener" target="_blank">racist</a> and <a href="https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-results">sexist</a> comments.</p><p>To avoid this problem, according to <em>Time</em>, OpenAI engaged an outsourcing company that hired contractors <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/">in Kenya</a> to label vile, offensive, and potentially illegal material that would then be included in the training data so that the company could create a tool to detect toxic information before it could reach the user. <em>Time</em> reported that some of the material “described situations in graphic detail like child sexual abuse, bestiality, murder, suicide, torture, self-harm, and incest.” The contractors said that they were supposed to read and label between a hundred and fifty and two hundred and fifty passages of text in a nine-hour shift. They were paid no more than two dollars an hour and were offered group therapy to help them deal with the psychological harm that the job was inflicting. The outsourcing company disputed those numbers, but the work was so disturbing that it terminated its contract eight months early. In a statement to <em>Time</em>, a spokesperson for OpenAI said that it “did not issue any productivity targets,” and that the outsourcing company “was responsible for managing the payment and mental health provisions for employees,” adding that “we take the mental health of our employees and those of our contractors very seriously.”</p></div></div></div></div>
  </body>
</html>
