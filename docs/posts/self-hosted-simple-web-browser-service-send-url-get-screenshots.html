<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/US-Artificial-Intelligence/scraper">Original</a>
    <h1>Self-hosted, simple web browser service â€“ send URL, get screenshots</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">You run the API on your machine, you send it a URL, and you get back the website data as a file plus screenshots of the site. Simple as.</p>
<p dir="auto">This project was made to support <a href="https://github.com/US-Artificial-Intelligence/abbey">Abbey</a>, an AI platform. Its author is <a href="https://x.com/gkamer8" rel="nofollow">Gordon Kamer</a>.</p>
<p dir="auto">Some highlights:</p>
<ul dir="auto">
<li>Scrolls through the page and takes screenshots of different sections</li>
<li>Runs in a docker container</li>
<li>Browser-based (will run websites&#39; Javascript)</li>
<li>Gives you the HTTP status code and headers from the first request</li>
<li>Automatically handles 302 redirects</li>
<li>Handles download links properly</li>
<li>Tasks are processed in a queue with configurable memory allocation</li>
<li>Blocking API</li>
<li>Zero state or other complexity</li>
</ul>
<p dir="auto">This web scraper is resource intensive but higher quality than many alternatives. Websites are scraped using Playwright, which launches a Firefox browser context for each job.</p>

<p dir="auto">You should have Docker and <code>docker compose</code> installed.</p>
<ol dir="auto">
<li>Clone this repo</li>
<li>Run <code>docker compose up</code> (a <code>docker-compose.yml</code> file is provided for your use)</li>
</ol>
<p dir="auto">...and the service will be available at <code>http://localhost:5006</code>. See the Usage section below for details on how to interact with it.</p>

<p dir="auto">You may set an API key using a <code>.env</code> file inside the <code>/scraper</code> folder (same level as <code>app.py</code>).</p>
<p dir="auto">You can set as many API keys as you&#39;d like; allowed API keys are those that start with <code>SCRAPER_API_KEY</code>. For example, here is a <code>.env</code> file that has three available keys:</p>
<div data-snippet-clipboard-copy-content="SCRAPER_API_KEY=should-be-secret
SCRAPER_API_KEY_OTHER=can-also-be-used
SCRAPER_API_KEY_3=works-too"><pre><code>SCRAPER_API_KEY=should-be-secret
SCRAPER_API_KEY_OTHER=can-also-be-used
SCRAPER_API_KEY_3=works-too
</code></pre></div>
<p dir="auto">API keys are sent to the service using the Authorization Bearer scheme.</p>

<p dir="auto">The root path <code>/</code> returns status 200 if online, plus some Gilbert and Sullivan lyrics (you can go there in your browser to see if it&#39;s online).</p>
<p dir="auto">The only other path is <code>/scrape</code>, to which you send a JSON formatted POST request and (if all things go well) receive a <code>multipart/mixed</code> type response.</p>
<p dir="auto">The response will be either:</p>
<ul dir="auto">
<li>Status 200: <code>multipart/mixed</code> response where the first part is type <code>application/json</code> with information about the request; the second part is the website data (usually <code>text/html</code>); and the remaining parts are up to 5 screenshots.</li>
<li>Not status 200: <code>application/json</code> response with an error message under the &#34;error&#34; key.</li>
</ul>
<p dir="auto">Here&#39;s a sample cURL request:</p>
<div data-snippet-clipboard-copy-content="curl -X POST &#34;http://localhost:5006/scrape&#34;
    -H &#34;Content-Type: application/json&#34;
    -d &#39;{&#34;url&#34;: &#34;https://us.ai&#34;}&#39;"><pre><code>curl -X POST &#34;http://localhost:5006/scrape&#34;
    -H &#34;Content-Type: application/json&#34;
    -d &#39;{&#34;url&#34;: &#34;https://us.ai&#34;}&#39;
</code></pre></div>
<p dir="auto">Here is a code example using Python and the requests_toolbelt library to let you interact with the API properly:</p>
<div data-snippet-clipboard-copy-content="import requests
from requests_toolbelt.multipart.decoder import MultipartDecoder
import sys
import json

data = {
    &#39;url&#39;: &#34;https://us.ai&#34;
}
# Optional if you&#39;re using an API key
headers = {
    &#39;Authorization&#39;: f&#39;Bearer Your-API-Key&#39;
}

response = requests.post(&#39;http://localhost:5006/scrape&#39;, json=data, headers=headers, timeout=30)
if response.status_code != 200:
    my_json = response.json()
    message = my_json[&#39;error&#39;]
    print(f&#34;Error scraping: {message}&#34;, file=sys.stderr)
else:
    decoder = MultipartDecoder.from_response(response)
    resp = None
    for i, part in enumerate(decoder.parts):
        if i == 0:  # First is some JSON
            json_part = json.loads(part.content)
            req_status = json_part[&#39;status&#39;]  # An integer
            req_headers = json_part[&#39;headers&#39;]  # Headers from the request made to your URL
            metadata = json_part[&#39;metadata&#39;]  # Information like the number of screenshots and their compressed / uncompressed sizes
            # ...
        elif i == 1:  # Next is the actual content of the page
            content = part.content
            headers = part.headers  # Will contain info about the content (text/html, application/pdf, etc.)
            # ...
        else:  # Other parts are screenshots, if they exist
            img = part.content
            headers = part.headers  # Will tell you the image format
            # ..."><pre><code>import requests
from requests_toolbelt.multipart.decoder import MultipartDecoder
import sys
import json

data = {
    &#39;url&#39;: &#34;https://us.ai&#34;
}
# Optional if you&#39;re using an API key
headers = {
    &#39;Authorization&#39;: f&#39;Bearer Your-API-Key&#39;
}

response = requests.post(&#39;http://localhost:5006/scrape&#39;, json=data, headers=headers, timeout=30)
if response.status_code != 200:
    my_json = response.json()
    message = my_json[&#39;error&#39;]
    print(f&#34;Error scraping: {message}&#34;, file=sys.stderr)
else:
    decoder = MultipartDecoder.from_response(response)
    resp = None
    for i, part in enumerate(decoder.parts):
        if i == 0:  # First is some JSON
            json_part = json.loads(part.content)
            req_status = json_part[&#39;status&#39;]  # An integer
            req_headers = json_part[&#39;headers&#39;]  # Headers from the request made to your URL
            metadata = json_part[&#39;metadata&#39;]  # Information like the number of screenshots and their compressed / uncompressed sizes
            # ...
        elif i == 1:  # Next is the actual content of the page
            content = part.content
            headers = part.headers  # Will contain info about the content (text/html, application/pdf, etc.)
            # ...
        else:  # Other parts are screenshots, if they exist
            img = part.content
            headers = part.headers  # Will tell you the image format
            # ...
</code></pre></div>

<p dir="auto">Navigating to untrusted websites is a serious security issue. Risks are somewhat mitigated in the following ways:</p>
<ul dir="auto">
<li>Runs as isolated container (container isolation)</li>
<li>Each website is scraped in a new browser context (process isolation)</li>
<li>Strict memory limits and timeouts for each task</li>
<li>Checks the URL to make sure that it&#39;s not too weird (loopback, non http, etc.)</li>
</ul>
<p dir="auto">You may take additional precautions depending on your needs, like:</p>
<ul dir="auto">
<li>Only giving the API trusted URLs (or otherwise screening URLs)</li>
<li>Running this API on isolated VMs (hardware isolation)</li>
<li>Using one API instance per user</li>
<li>Not making any secret files or keys available inside the container (besides the API key for the scraper itself)</li>
</ul>
<p dir="auto"><strong>If you&#39;d like to make sure that this API is up to your security standards, please examine the code and open issues! It&#39;s not a big repo.</strong></p>

<p dir="auto">You can control memory limits and other variables at the top of <code>scraper/worker.py</code>. Here are the defaults:</p>
<div data-snippet-clipboard-copy-content="MEM_LIMIT_MB = 4_000  # 4 GB memory threshold for child scraping process
MAX_SCREENSHOTS = 5
SCREENSHOT_JPEG_QUALITY = 85
BROWSER_HEIGHT = 2000
BROWSER_WIDTH = 1280
USER_AGENT = &#34;Mozilla/5.0 (compatible; Abbey/1.0; +https://github.com/US-Artificial-Intelligence/scraper)&#34;"><pre><code>MEM_LIMIT_MB = 4_000  # 4 GB memory threshold for child scraping process
MAX_SCREENSHOTS = 5
SCREENSHOT_JPEG_QUALITY = 85
BROWSER_HEIGHT = 2000
BROWSER_WIDTH = 1280
USER_AGENT = &#34;Mozilla/5.0 (compatible; Abbey/1.0; +https://github.com/US-Artificial-Intelligence/scraper)&#34;
</code></pre></div>
</article></div></div>
  </body>
</html>
