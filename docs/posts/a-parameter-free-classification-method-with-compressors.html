<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aclanthology.org/2023.findings-acl.426/">Original</a>
    <h1>A Parameter-Free Classification Method with Compressors</h1>
    
    <div id="readability-page-1" class="page"><div id="main-container"><section id="main"><hr/><div><div><div><div><h5>Abstract</h5><p><span>Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like <i>gzip</i> with a <span>k</span>-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.</span></p></div></div><dl><dt>Anthology ID:</dt><dd>2023.findings-acl.426</dd><dt>Volume:</dt><dd><a href="https://aclanthology.org/volumes/2023.findings-acl/">Findings of the Association for Computational Linguistics: ACL 2023</a></dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2023</dd><dt>Address:</dt><dd>Toronto, Canada</dd><dt>Venue:</dt><dd><a href="https://aclanthology.org/venues/findings/">Findings</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>6810–6828</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href="https://aclanthology.org/2023.findings-acl.426">https://aclanthology.org/2023.findings-acl.426</a></dd><dt>DOI:</dt><dd></dd><dt>Bibkey:</dt><dd></dd><dt>Cite (ACL):</dt><dd><span id="citeACL">Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, and Jimmy Lin. 2023. <a href="https://aclanthology.org/2023.findings-acl.426">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</a>. In <i>Findings of the Association for Computational Linguistics: ACL 2023</i>, pages 6810–6828, Toronto, Canada. Association for Computational Linguistics.</span></dd><dt>Cite (Informal):</dt><dd><span id="citeRichText"><a href="https://aclanthology.org/2023.findings-acl.426">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</a> (Jiang et al., Findings 2023)</span></dd><dt>Copy Citation:</dt><dd>



</dd><dt>PDF:</dt><dd><a href="https://aclanthology.org/2023.findings-acl.426.pdf">https://aclanthology.org/2023.findings-acl.426.pdf</a></dd></dl></div></div><hr/><div id="citeModal" tabindex="-1" role="dialog" aria-labelledby="citeModalLabel" aria-hidden="true"><div role="document"><div><div><ul id="citeFormats" role="tablist"><li><a data-toggle="list" href="#citeBibtex" role="tab" aria-controls="citeBibtex" aria-selected="true">BibTeX</a></li><li><a data-toggle="list" href="#citeMods" role="tab" aria-controls="citeMods" aria-selected="false">MODS XML</a></li><li><a data-toggle="list" href="#citeEndnote" role="tab" aria-controls="citeEndnote" aria-selected="false">Endnote</a></li><li><a data-toggle="list" href="#citeMarkdown" role="tab" aria-controls="citeMarkdown" aria-selected="false">Preformatted</a></li></ul><div id="citeFormatsContent"><div id="citeBibtex" role="tabpanel"><pre id="citeBibtexContent">@inproceedings{jiang-etal-2023-low,
    title = &#34;{``}Low-Resource{&#39;&#39;} Text Classification: A Parameter-Free Classification Method with Compressors&#34;,
    author = &#34;Jiang, Zhiying  and
      Yang, Matthew  and
      Tsirlin, Mikhail  and
      Tang, Raphael  and
      Dai, Yiqin  and
      Lin, Jimmy&#34;,
    booktitle = &#34;Findings of the Association for Computational Linguistics: ACL 2023&#34;,
    month = jul,
    year = &#34;2023&#34;,
    address = &#34;Toronto, Canada&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://aclanthology.org/2023.findings-acl.426&#34;,
    pages = &#34;6810--6828&#34;,
    abstract = &#34;Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that{&#39;}s easy, lightweight, and universal in text classification: a combination of a simple compressor like \textit{gzip} with a $k$-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.&#34;,
}
</pre></div><div id="citeMods" role="tabpanel"><pre id="citeModsContent">﻿&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;jiang-etal-2023-low&#34;&gt;
    &lt;titleInfo&gt;
        &lt;title&gt;“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors&lt;/title&gt;
    &lt;/titleInfo&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Zhiying&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Jiang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Matthew&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Yang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Mikhail&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Tsirlin&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Raphael&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Tang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Yiqin&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Dai&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Jimmy&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Lin&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;originInfo&gt;
        &lt;dateIssued&gt;2023-07&lt;/dateIssued&gt;
    &lt;/originInfo&gt;
    &lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
    &lt;relatedItem type=&#34;host&#34;&gt;
        &lt;titleInfo&gt;
            &lt;title&gt;Findings of the Association for Computational Linguistics: ACL 2023&lt;/title&gt;
        &lt;/titleInfo&gt;
        &lt;originInfo&gt;
            &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt;
            &lt;place&gt;
                &lt;placeTerm type=&#34;text&#34;&gt;Toronto, Canada&lt;/placeTerm&gt;
            &lt;/place&gt;
        &lt;/originInfo&gt;
        &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt;
    &lt;/relatedItem&gt;
    &lt;abstract&gt;Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.&lt;/abstract&gt;
    &lt;identifier type=&#34;citekey&#34;&gt;jiang-etal-2023-low&lt;/identifier&gt;
    &lt;location&gt;
        &lt;url&gt;https://aclanthology.org/2023.findings-acl.426&lt;/url&gt;
    &lt;/location&gt;
    &lt;part&gt;
        &lt;date&gt;2023-07&lt;/date&gt;
        &lt;extent unit=&#34;page&#34;&gt;
            &lt;start&gt;6810&lt;/start&gt;
            &lt;end&gt;6828&lt;/end&gt;
        &lt;/extent&gt;
    &lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
</pre></div><div id="citeEndnote" role="tabpanel"><pre id="citeEndnoteContent">﻿%0 Conference Proceedings
%T “Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors
%A Jiang, Zhiying
%A Yang, Matthew
%A Tsirlin, Mikhail
%A Tang, Raphael
%A Dai, Yiqin
%A Lin, Jimmy
%S Findings of the Association for Computational Linguistics: ACL 2023
%D 2023
%8 July
%I Association for Computational Linguistics
%C Toronto, Canada
%F jiang-etal-2023-low
%X Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.
%U https://aclanthology.org/2023.findings-acl.426
%P 6810-6828

</pre></div><div id="citeMarkdown" role="tabpanel"><h5>Markdown (Informal)</h5><p id="citeMarkdownContent">[“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors](https://aclanthology.org/2023.findings-acl.426) (Jiang et al., Findings 2023)</p><ul><li><a href="https://aclanthology.org/2023.findings-acl.426">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</a> (Jiang et al., Findings 2023)</li></ul><h5>ACL</h5><ul><li id="citeACLstyleContent">Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, and Jimmy Lin. 2023. <a href="https://aclanthology.org/2023.findings-acl.426">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</a>. In <i>Findings of the Association for Computational Linguistics: ACL 2023</i>, pages 6810–6828, Toronto, Canada. Association for Computational Linguistics.</li></ul></div></div></div></div></div></div></section></div></div>
  </body>
</html>
