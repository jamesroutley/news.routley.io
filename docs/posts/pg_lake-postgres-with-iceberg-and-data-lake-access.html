<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Snowflake-Labs/pg_lake">Original</a>
    <h1>Pg_lake: Postgres with Iceberg and data lake access</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><code>pg_lake</code> integrates Iceberg and data lake files into Postgres. With the <code>pg_lake</code> extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.</p>
<p dir="auto">At a high level, <code>pg_lake</code> lets you:</p>
<ul dir="auto">
<li>Create and modify <a href="https://iceberg.apache.org/" rel="nofollow"><strong>Iceberg</strong></a> tables directly from PostgreSQL, with full transactional guarantees and query them from other engines</li>
<li>Query and import data from <a href="https://parquet.apache.org/" rel="nofollow"><strong>Parquet</strong></a>, <strong>CSV</strong>, <strong>JSON</strong>, and <strong>Iceberg</strong> files stored in S3 or other compatible object stores</li>
<li>Export query results back to S3 in <strong>Parquet</strong>, <strong>CSV</strong>, or <strong>JSON</strong> formats using COPY commands</li>
<li>Read <strong>geospatial formats</strong> supported by GDAL, such as <strong>GeoJSON</strong> and <strong>Shapefiles</strong></li>
<li>Use compression transparently with <strong>.gz</strong> and <strong>.zst</strong></li>
<li>Use the built-in <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/pg_map/README.md"><strong>map</strong> type</a> for semi-structured or key–value data</li>
<li>Combine <strong>heap</strong>, <strong>Iceberg</strong>, and external <strong>Parquet/CSV/JSON</strong> files in the same SQL queries and modifications — all with full transactional guarantees and no SQL limitations</li>
<li><strong>Infer table columns and types</strong> from external data sources such as <strong>Iceberg</strong>, <strong>Parquet</strong>, <strong>JSON</strong>, and <strong>CSV</strong> files</li>
<li><strong>Leverage DuckDB’s query engine</strong> underneath for fast execution without leaving Postgres</li>
</ul>

<p dir="auto">There are two ways to set up <code>pg_lake</code>:</p>
<ul dir="auto">
<li><strong>Using Docker</strong>, for an easy, ready-to-run test environment.</li>
<li><strong>Building from source</strong>, for a manual setup or development use.</li>
</ul>
<p dir="auto">Both approaches include the PostgreSQL extensions, the <code>pgduck_server</code> application and setting up S3-compatible storage.</p>

<p dir="auto">Follow the <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/docker/README.md">Docker README</a> to set up and run <code>pg_lake</code> with Docker.</p>

<p dir="auto">Once you’ve <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/docs/building-from-source.md">built and installed the required components</a>, you can initialize <code>pg_lake</code> inside Postgres.</p>

<p dir="auto">Create all required extensions at once using <code>CASCADE</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension &#34;pg_lake_table&#34;
NOTICE:  installing required extension &#34;pg_lake_engine&#34;
NOTICE:  installing required extension &#34;pg_extension_base&#34;
NOTICE:  installing required extension &#34;pg_lake_iceberg&#34;
NOTICE:  installing required extension &#34;pg_lake_copy&#34;
CREATE EXTENSION"><pre>CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension <span><span>&#34;</span>pg_lake_table<span>&#34;</span></span>
NOTICE:  installing required extension <span><span>&#34;</span>pg_lake_engine<span>&#34;</span></span>
NOTICE:  installing required extension <span><span>&#34;</span>pg_extension_base<span>&#34;</span></span>
NOTICE:  installing required extension <span><span>&#34;</span>pg_lake_iceberg<span>&#34;</span></span>
NOTICE:  installing required extension <span><span>&#34;</span>pg_lake_copy<span>&#34;</span></span>
CREATE EXTENSION</pre></div>

<p dir="auto"><code>pgduck_server</code> is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses <code>DuckDB</code> to execute queries.</p>
<p dir="auto">When you run <code>pgduck_server</code> it starts listening to port <code>5332</code> on unix domain socket:</p>
<div data-snippet-clipboard-copy-content="pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000"><pre><code>pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000
</code></pre></div>
<p dir="auto">As <code>pgduck_server</code> implements Postgres wire protocol, you can access it via <code>psql</code> on port <code>5332</code> and host <code>/tmp</code> and run commands via DuckDB.</p>
<p dir="auto">For example, you can get the DuckDB version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="psql -p 5332 -h /tmp

select version() as duckdb_version; 
duckdb_version 
---------------- 
v1.3.2 (1 row)"><pre>psql <span>-</span>p <span>5332</span> <span>-</span>h <span>/</span>tmp

<span>select</span> version() <span>as</span> duckdb_version; 
duckdb_version 
<span><span>--</span>-------------- </span>
<span>v1</span>.<span>3</span>.<span>2</span> (<span>1</span> row)</pre></div>
<p dir="auto">You can also provide some additional settings while starting the server, to see all:</p>

<p dir="auto">There are some important settings that should be adjusted, especially on production systems:</p>
<ul dir="auto">
<li><code>--memory_limit</code>: Optionally specify the maximum memory of pgduck_server similar to DuckDB&#39;s memory_limit, the default is 80 percent of the system memory</li>
<li><code>--init_file_path &lt;path&gt;</code>: Execute all statements in this file on start-up</li>
<li><code>--cache_dir</code>: Specify the directory to use to cache remote files (from S3)</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">Connecting <code>pg_lake</code> to s3 (or compatible)</h4><a id="user-content-connecting-pg_lake-to-s3-or-compatible" aria-label="Permalink: Connecting pg_lake to s3 (or compatible)" href="#connecting-pg_lake-to-s3-or-compatible"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>pgduck_server</code> relies on the DuckDB <a href="https://duckdb.org/docs/stable/configuration/secrets_manager" rel="nofollow">secrets manager</a> for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly — for example, by setting them in ~/.aws/credentials.</p>
<p dir="auto">Once you set up the credential chain, you should set the <code>pg_lake_iceberg.default_location_prefix</code>. This is the location where Iceberg tables are stored:</p>
<div dir="auto" data-snippet-clipboard-copy-content="SET pg_lake_iceberg.default_location_prefix TO &#39;s3://testbucketpglake&#39;;"><pre><span>SET</span> <span>pg_lake_iceberg</span>.<span>default_location_prefix</span> TO <span><span>&#39;</span>s3://testbucketpglake<span>&#39;</span></span>;</pre></div>
<p dir="auto">You can also set the credentials on <code>pgduck_server</code> for <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/docs/building-from-source.md#running-s3-compatible-service-minio-locally">local development with <code>minio</code></a>.</p>


<p dir="auto">You can create Iceberg tables by adding <code>USING iceberg</code> to your <code>CREATE TABLE</code> statements.</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE TABLE iceberg_test USING iceberg 
      AS SELECT 
            i as key, &#39;val_&#39;|| i  as val
         FROM 
            generate_series(0,99)i;"><pre><span>CREATE</span> <span>TABLE</span> <span>iceberg_test</span> USING iceberg 
      <span>AS</span> <span>SELECT</span> 
            i <span>as</span> key, <span><span>&#39;</span>val_<span>&#39;</span></span><span>||</span> i  <span>as</span> val
         <span>FROM</span> 
            generate_series(<span>0</span>,<span>99</span>)i;</pre></div>
<p dir="auto">Then, query it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT count(*) FROM iceberg_test;
 count 
-------
   100
(1 row)"><pre><span>SELECT</span> <span>count</span>(<span>*</span>) <span>FROM</span> iceberg_test;
 count 
<span><span>--</span>-----</span>
   <span>100</span>
(<span>1</span> row)</pre></div>
<p dir="auto">You can then see the Iceberg metadata location:</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT table_name, metadata_location FROM iceberg_tables;


    table_name     |                                                metadata_location
-------------------+--------------------------------------------------------------------------------------------------------------------
 iceberg_test      | s3://testbucketpglake/postgres/public/test/435029/metadata/00001-f0c6e20a-fd1c-4645-87c9-c0c64b92992b.metadata.json"><pre><span>SELECT</span> table_name, metadata_location <span>FROM</span> iceberg_tables;


    table_name     |                                                metadata_location
<span><span>--</span>-----------------+--------------------------------------------------------------------------------------------------------------------</span>
 iceberg_test      | s3:<span>//</span>testbucketpglake<span>/</span>postgres<span>/</span>public<span>/</span>test<span>/</span><span>435029</span><span>/</span>metadata<span>/</span><span>00001</span><span>-</span>f0c6e20a<span>-</span>fd1c<span>-</span><span>4645</span><span>-</span>87c9<span>-</span><span>c0c64b92992b</span>.<span>metadata</span>.json</pre></div>

<p dir="auto">You can import or export data directly using <code>COPY</code> in <strong>Parquet</strong>, <strong>CSV</strong>, or <strong>newline-delimited JSON</strong> formats.  The format is automatically inferred from the file extension, or you can specify it explicitly with <code>COPY</code> options like <code>WITH (format &#39;csv&#39;, compression &#39;gzip&#39;)</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Copy data from Postgres to S3 with format parquet
-- Read from any data source, including iceberg tables, heap tables or any query results
COPY (SELECT * FROM iceberg_test) TO &#39;s3://testbucketpglake/parquet_data/iceberg_test.parquet&#39;;

-- Copy back from S3 to any table in Postgres
-- This example copies into an iceberg table, but could be heap table as well
COPY iceberg_test FROM &#39;s3://testbucketpglake/parquet_data/iceberg_test.parquet&#39;;"><pre><span><span>--</span> Copy data from Postgres to S3 with format parquet</span>
<span><span>--</span> Read from any data source, including iceberg tables, heap tables or any query results</span>
COPY (<span>SELECT</span> <span>*</span> <span>FROM</span> iceberg_test) TO <span><span>&#39;</span>s3://testbucketpglake/parquet_data/iceberg_test.parquet<span>&#39;</span></span>;

<span><span>--</span> Copy back from S3 to any table in Postgres</span>
<span><span>--</span> This example copies into an iceberg table, but could be heap table as well</span>
COPY iceberg_test <span>FROM</span> <span><span>&#39;</span>s3://testbucketpglake/parquet_data/iceberg_test.parquet<span>&#39;</span></span>;</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Create foreign table for files on s3</h3><a id="user-content-create-foreign-table-for-files-on-s3" aria-label="Permalink: Create foreign table for files on s3" href="#create-foreign-table-for-files-on-s3"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can create a foreign table directly from a file or set of files without having to specify column names or types.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- use the files under the path, can use * for all files
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (path &#39;s3://testbucketpglake/parquet_data/*.parquet&#39;);

-- note that we infer the columns from the file
\d parquet_table
              Foreign table &#34;public.parquet_table&#34;
 Column |  Type   | Collation | Nullable | Default | FDW options 
--------+---------+-----------+----------+---------+-------------
 key    | integer |           |          |         | 
 val    | text    |           |          |         | 
Server: pg_lake
FDW options: (path &#39;s3://testbucketpglake/parquet_data/*.parquet&#39;)

-- and, query it
select count(*) from parquet_table;
 count 
-------
   100
(1 row)
"><pre><span><span>--</span> use the files under the path, can use * for all files</span>
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (<span>path</span> <span><span>&#39;</span>s3://testbucketpglake/parquet_data/*.parquet<span>&#39;</span></span>);

<span><span>--</span> note that we infer the columns from the file</span>
\d parquet_table
              Foreign table <span><span>&#34;</span>public.parquet_table<span>&#34;</span></span>
 Column |  Type   | Collation | Nullable | Default | FDW options 
<span><span>--</span>------+---------+-----------+----------+---------+-------------</span>
 key    | <span>integer</span> |           |          |         | 
 val    | <span>text</span>    |           |          |         | 
Server: pg_lake
FDW options: (<span>path</span> <span><span>&#39;</span>s3://testbucketpglake/parquet_data/*.parquet<span>&#39;</span></span>)

<span><span>--</span> and, query it</span>
<span>select</span> <span>count</span>(<span>*</span>) <span>from</span> parquet_table;
 count 
<span><span>--</span>-----</span>
   <span>100</span>
(<span>1</span> row)
</pre></div>

<p dir="auto">A <code>pg_lake</code> instance consists of two main components: <strong>PostgreSQL with the pg_lake extensions</strong> and <strong>pgduck_server</strong>.</p>
<p dir="auto">Users connect to PostgreSQL to run SQL queries, and the <code>pg_lake</code> extensions integrate with Postgres’s hooks to handle query planning, transaction boundaries, and overall orchestration of execution.</p>
<p dir="auto">Behind the scenes, <em>parts</em> of query execution are delegated to DuckDB through pgduck_server, a separate multi-threaded process that implements the PostgreSQL wire protocol (locally).  This process runs DuckDB together with our <strong>duckdb_pglake</strong> extension, which adds PostgreSQL-compatible functions and behavior.</p>
<p dir="auto">Users typically don’t need to be aware of <code>pgduck_server</code>; it operates transparently to improve performance. When appropriate, <code>pg_lake</code> delegates scanning of the data and the computation to DuckDB’s highly parallel, column-oriented execution engine.</p>
<p dir="auto">This separation also avoids the threading and memory-safety limitations that would arise from embedding DuckDB directly inside the Postgres process, which is designed around process isolation rather than multi-threaded execution. Moreover, it lets us interact with the query engine directly by connecting to it using standard Postgres clients.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Snowflake-Labs/pg_lake/blob/main/pglake-arch.png"><img src="https://github.com/Snowflake-Labs/pg_lake/raw/main/pglake-arch.png" alt="pg_lake Architecture"/></a></p>

<p dir="auto">The team behind pg_lake has a lot of experience building Postgres extensions (e.g. Citus, pg_cron, pg_documentdb). Over time, we’ve learned that large, monolithic PostgreSQL extensions are harder to evolve and maintain.</p>
<p dir="auto"><code>pg_lake</code> follows a modular design built around a <strong>set of interoperating components</strong> — mostly implemented as PostgreSQL extensions, others as supporting services or libraries.</p>
<p dir="auto">The current set of components are:</p>
<ul dir="auto">
<li><strong>pg_lake_iceberg</strong>: a PostgreSQL extension that implements the <a href="https://iceberg.apache.org/" rel="nofollow">Iceberg specification</a></li>
<li><strong>pg_lake_table</strong>: a PostgreSQL extension that implements a foreign data wrapper to query files in object storage</li>
<li><strong>pg_lake_copy</strong>: a PostgreSQL extension that implements COPY to/from your data lake</li>
<li><strong>pg_lake_engine</strong>: a common module for different pg_lake extensions</li>
<li><strong>pg_extension_base</strong>: A foundational building block for other extensions</li>
<li><strong>pg_extension_updater</strong>: An extension for updating all extensions on start-up. See <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/pg_extension_updater/README.md">README.md</a>.</li>
<li><strong>pg_lake_benchmark</strong>: a PostgreSQL extension that performs various benchmarks on lake tables. See <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/pg_lake_benchmark/README.md">README.md</a>.</li>
<li><strong>pg_map</strong>: A generic map type generator</li>
<li><strong>pgduck_server</strong>: a stand-alone server that loads DuckDB into the same server machine and exposes DuckDB via the PostgreSQL protocol</li>
<li><strong>duckdb_pglake</strong>: a DuckDB extension that adds missing PostgreSQL functions to DuckDB</li>
</ul>

<p dir="auto"><code>pg_lake</code> development started in early 2024 at <a href="https://www.crunchydata.com/" rel="nofollow">Crunchy Data</a> with the goal of bringing Iceberg to PostgreSQL. The first few months were focused on building a robust integration of an external query engine (DuckDB). To get to market early, we made the query/import/export features available to <a href="https://docs.crunchybridge.com/" rel="nofollow">Crunchy Bridge</a> customers as <a href="https://www.crunchydata.com/blog/crunchy-bridge-for-analytics-your-data-lake-in-postgresql" rel="nofollow">Crunchy Bridge for Analytics</a>.</p>
<p dir="auto">Next, we started building a comprehensive implementation of the Iceberg (v2) protocol with support for transactions and almost all PostgreSQL features. In November 2024, we relaunched Crunchy Bridge for Analytics as Crunchy Data Warehouse available on Crunchy Bridge and on-premises.</p>
<p dir="auto">In June 2025, <a href="https://www.crunchydata.com/blog/crunchy-data-joins-snowflake" rel="nofollow">Crunchy Data was acquired by Snowflake</a>. Following the acquisition, Snowflake decided to open source the project as <code>pg_lake</code> in November 2025. The initial version is 3.0 because of the two prior generations. If you’re currently a Crunchy Data Warehouse user there will be an automatic upgrade path, though some names will change.</p>

<p dir="auto">Full project documentation can be found in the <a href="https://github.com/Snowflake-Labs/pg_lake/blob/main/docs">docs</a> directory.</p>

<p dir="auto">Copyright (c) Snowflake Inc. All rights reserved.
Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache 2.0</a> license.</p>

<p dir="auto"><code>pg_lake</code> is dependent on third-party projects Apache Avro and DuckDB. During build, <code>pg_lake</code> applies patches to Avro and certain DuckDB extensions in order to provide the <code>pg_lake</code> functionality. The source code associated with the Avro and DuckDB extensions is downloaded from the applicable upstream repos and the source code associated with those projects remains under the original licenses. If you are packaging or redistributing packages that include <code>pg_lake</code>, please note that you should review those upstream license terms.</p>
</article></div></div>
  </body>
</html>
