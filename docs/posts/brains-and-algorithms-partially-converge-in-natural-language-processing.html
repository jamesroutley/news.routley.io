<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s42003-022-03036-1">Original</a>
    <h1>Brains and algorithms partially converge in natural language processing</h1>
    
    <div id="readability-page-1" class="page"><div>
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div id="Abs1-section"><h2 id="Abs1">Abstract</h2><p>Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.</p></div></section>
            

            
                
            
            
                <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>Deep learning algorithms have recently made considerable progress in developing abilities generally considered unique to the human species<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Turing, A. M. Parsing the Turing Test 23–65 (Springer, 2009)." href="#ref-CR1" id="ref-link-section-d184064578e405">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chomsky, N. Language and Mind (Cambridge University Press, 2006)." href="#ref-CR2" id="ref-link-section-d184064578e405_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Dehaene, S., Yann, L. &amp; Girardon, J. La plus belle histoire de l’intelligence: des origines aux neurones artificiels: vers une nouvelle étape de l’évolution (Robert Laffont, 2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR3" id="ref-link-section-d184064578e408">3</a></sup>. Language transformers, in particular, can complete, translate, and summarize texts with an unprecedented accuracy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vaswani, A. et al. Attention is all you need. In Proceedings on NIPS (Cornell University, 2017)." href="#ref-CR4" id="ref-link-section-d184064578e412">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (2019)." href="#ref-CR5" id="ref-link-section-d184064578e412_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lample, G. &amp; Conneau, A. Cross-lingual language model pretraining. In Adv. Neural Inf. Process. Syst. (2019)." href="#ref-CR6" id="ref-link-section-d184064578e412_2">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR7" id="ref-link-section-d184064578e415">7</a></sup>. These advances raise a major question: do these algorithms process words and sentences like the human brain?</p><p>Recent neuroimaging studies suggest that they might—at least partially<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lakretz, Y. et al. The emergence of number and syntax units in LSTM language models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (2019)." href="#ref-CR8" id="ref-link-section-d184064578e422">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Loula, J., Baroni, M. &amp; Lake, B. M. Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. In BlackboxNLP@ EMNLP (2018)." href="#ref-CR9" id="ref-link-section-d184064578e422_1">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hale, J. T. et al. Neuro-computational models of language processing." href="#ref-CR10" id="ref-link-section-d184064578e422_2">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lake, B. M. &amp; Murphy, G. L. Word meaning in minds and machines. Psychol. Rev. (2021)." href="#ref-CR11" id="ref-link-section-d184064578e422_3">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Marcus, G. Deep learning: a critical appraisal. Preprint at 
                  https://arXiv.org/1801.00631
                  
                 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR12" id="ref-link-section-d184064578e425">12</a></sup>. First, word embeddings—high dimensional dense vectors trained to predict lexical neighborhood<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bengio, Y., Ducharme, R. &amp; Vincent, P. in Advances in Neural Information Processing Systems (eds. Leen, T. K. et al.) vol. 13, 932–938 (MIT Press, 2003)." href="#ref-CR13" id="ref-link-section-d184064578e429">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mikolov, T., Chen, K., Corrado, G. &amp; Dean, J. Efficient estimation of word representations in vector space. Preprint at 
                  https://arxiv.org/1301.3781
                  
                 (2013)." href="#ref-CR14" id="ref-link-section-d184064578e429_1">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pennington, J., Socher, R. &amp; Manning, C. D. Glove: global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP) Conference 1532–1543 (2014)." href="#ref-CR15" id="ref-link-section-d184064578e429_2">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Bojanowski, P., Grave, E., Joulin, A. &amp; Mikolov, T. Enriching Word Vectors with Subword Information. In Transactions of the Association for Computational Linguistics (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR16" id="ref-link-section-d184064578e432">16</a></sup>—have been shown to linearly map onto the brain responses elicited by words presented either in isolation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008)." href="#ref-CR17" id="ref-link-section-d184064578e436">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Anderson, A. J. et al. Multiple regions of a cortical network commonly encode the meaning of words in multiple grammatical positions of read sentences. Cereb. Cortex 29, 2396–2411 (2019)." href="#ref-CR18" id="ref-link-section-d184064578e436_1">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Sassenhagen, J. &amp; Fiebach, C. J. Traces of meaning itself: Encoding distributional word vectors in brain activity. Neurobiology of Language 1.1, 54–76 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR19" id="ref-link-section-d184064578e439">19</a></sup> or within narratives<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. GPT-2’s Activations Predict the Degree of Semantic Comprehension in the Human Brain (Cold Spring Harbor Laboratory Section: New Results, 2021)." href="#ref-CR20" id="ref-link-section-d184064578e443">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Reddy Oota, S., Manwani, N. &amp; Raju S, B. fMRI semantic category decoding using linguistic encoding of word embeddings. In International Conference on Neural Information Processing (Springer, Cham, 2018)." href="#ref-CR21" id="ref-link-section-d184064578e443_1">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abnar, S., Ahmed, R., Mijnheer, M. &amp; Zuidema, W. H. Experiential, distributional and dependency-based word embeddings have complementary roles in decoding brain activity. In Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), (2018)." href="#ref-CR22" id="ref-link-section-d184064578e443_2">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ruan, Y. -P., Ling, Z. -H. &amp; Hu, Y. Exploring semantic representation in brain activity using word embeddings. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 669–679 (Association for Computational Linguistics, 2016)." href="#ref-CR23" id="ref-link-section-d184064578e443_3">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Brodbeck, C., Hong, L. E. &amp; Simon, J. Z. Rapid transformation from auditory to linguistic representations of continuous speech. Curr. Biol. 28, 3976–3983 (2018)." href="#ref-CR24" id="ref-link-section-d184064578e443_4">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gauthier, J. &amp; Ivanova, A. Does the brain represent words? an evaluation of brain decoding studies of language understanding. Preprint at 
                  https://arXiv.org/1806.00591
                  
                 (2018)." href="#ref-CR25" id="ref-link-section-d184064578e443_5">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wehbe, L., Vaswani, A., Knight, K. &amp; Mitchell, T. Aligning context-based statistical models of language with brain activity during reading. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) 233–243 (Association for Computational Linguistics, 2014)." href="#ref-CR26" id="ref-link-section-d184064578e443_6">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In Proceedings of the National Academy of Sciences (2021)." href="#ref-CR27" id="ref-link-section-d184064578e443_7">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Disentangling syntax and semantics in the brain with deep networks. ICML 2021-38th International Conference on Machine Learning (2021)." href="#ref-CR28" id="ref-link-section-d184064578e443_8">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. In EMNLP 2021—Conference on Empirical Methods in Natural Language Processing (2021)." href="#ref-CR29" id="ref-link-section-d184064578e443_9">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR30" id="ref-link-section-d184064578e446">30</a></sup>. Second, the contextualized activations of language transformers improve the precision of this mapping, especially in the prefrontal, temporal and parietal cortices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jain, S. &amp; Huth, A. in Advances in Neural Information Processing Systems (eds Bengio, S. et al.) vol. 31, 6628–6637 (Curran Associates, Inc., 2018)." href="#ref-CR31" id="ref-link-section-d184064578e450">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Athanasiou, N., Iosif, E. &amp; Potamianos, A. Neural activation semantic models: computational lexical semantic models of localized neural activations. In Proceedings of the 27th International Conference on Computational Linguistics 2867–2878 (Association for Computational Linguistics, 2018)." href="#ref-CR32" id="ref-link-section-d184064578e450_1">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR33" id="ref-link-section-d184064578e453">33</a></sup>. Third, specific computations of deep language models, such as the estimations of word surprisal (i.e., the probability of a word given its context) and the parsing of syntactic constituents have been shown to correlate with evoked related potentials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR30" id="ref-link-section-d184064578e458">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. &amp; de Lange, F. P. A hierarchy of linguistic predictions during natural language comprehension. bioRxiv 
                  https://doi.org/10.1101/2020.12.03.410399
                  
                 (2020)." href="#ref-CR34" id="ref-link-section-d184064578e461">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Brennan, J. R. &amp; Pylkkänen, L. Meg evidence for incremental sentence composition in the anterior temporal lobe. Cogn. Sci. 41, 1515–1531 (2017)." href="#ref-CR35" id="ref-link-section-d184064578e461_1">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Hale, J., Dyer, C., Kuncoro, A. &amp; Brennan, J. R. Finding syntax in human encephalography with beam search. Preprint at 
                  https://arXiv.org/1806.04127
                  
                 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR36" id="ref-link-section-d184064578e464">36</a></sup> and functional magnetic resonance imaging (fMRI)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Disentangling syntax and semantics in the brain with deep networks. ICML 2021-38th International Conference on Machine Learning (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR28" id="ref-link-section-d184064578e468">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Hale, J., Dyer, C., Kuncoro, A. &amp; Brennan, J. R. Finding syntax in human encephalography with beam search. Preprint at 
                  https://arXiv.org/1806.04127
                  
                 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR36" id="ref-link-section-d184064578e471">36</a></sup>. However, the above studies remain fragmentary: first, most only analyze a small number of subjects (although see refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. GPT-2’s Activations Predict the Degree of Semantic Comprehension in the Human Brain (Cold Spring Harbor Laboratory Section: New Results, 2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR20" id="ref-link-section-d184064578e475">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Disentangling syntax and semantics in the brain with deep networks. ICML 2021-38th International Conference on Machine Learning (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR28" id="ref-link-section-d184064578e478">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. In EMNLP 2021—Conference on Empirical Methods in Natural Language Processing (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR29" id="ref-link-section-d184064578e481">29</a></sup>). Second, most studies only explore the spatial but not the temporal properties of the brain responses to language (although see refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR30" id="ref-link-section-d184064578e485">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR33" id="ref-link-section-d184064578e488">33</a></sup>).</p><p>More critically, the principles that lead a deep language models to generate brain-like representations remain largely unknown. Indeed, past studies only investigated a small set of pretrained language models that typically vary in dimensionality, architecture, training objective, and training corpus. The inherent correlations between these multiple factors thus prevent identifying those that lead algorithms to generate brain-like representations.</p><p>To address this issue, we systematically compare a wide variety of deep language models in light of human brain responses to sentences (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig1">1</a>). Specifically, we analyze the brain activity of 102 healthy adults, recorded with both fMRI and source-localized magneto-encephalography (MEG). During these two 1 h-long sessions the subjects read isolated Dutch sentences composed of 9–15 words<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e501">37</a></sup>. After quantifying the signal-to-noise ratio of the brain responses (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>), we train a variety of deep learning algorithms, extract their responses to the very same sentences and compare their ability to linearly map onto the fMRI and MEG brain recordings. Finally, we assess how the training, the architecture, and the word-prediction performance independently explains the brain-similarity of these algorithms and localize this convergence in both space and time.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Approach."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Approach.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s42003-022-03036-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="259"/></picture></a></div><p><b>a</b> Subjects read isolated sentences while their brain activity was recorded with fMRI and MEG<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e522">37</a></sup>. <b>b</b> To compute the similarity between a deep language model and the brain, we (1) fit a linear regression <i>W</i> from the model’s activations <i>X</i> to predict brain responses <i>Y</i> and (2) evaluate this mapping with a correlation between the predicted and true brain responses to held-out sentences <i>Y</i><sub>test</sub>. <b>c</b> We consider different types of embedding depending on whether they vary with neighboring words during training and/or during inference. Visual embeddings refer, here, to the activations of a deep convolutional neural network trained on character recognition. Lexical embeddings refer, here, to the non-contextualized activations associated with a word independently of its context. Here, we use the word-embedding layer of language transformers (bottom green), as opposed to algorithms like Word2Vec<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 93" title="Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems 3111–3119 (MIT Press, 2013)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR93" id="ref-link-section-d184064578e546">93</a></sup> (middle, green). Compositional embeddings refer, here, to the context-dependent activations of a deep language model (see SI.4 for a discussion of our terminology). <b>d</b> The three panels represent three hypotheses on the link between deep language models and the brain. Each dot represents one embedding. Algorithm are said to <i>converge</i> to brain-like computations if their performance (<i>x</i>-axis: i.e., accuracy at predicting a word from its previous context) indexes their ability to map onto brain responses to the same stimuli (i.e., <i>y</i>-axis: brain score). High-dimensional neural networks can, in principle, capture relevant information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="Bingham, E. &amp; Mannila, H. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 245–250 (ACM, 2001)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR94" id="ref-link-section-d184064578e563">94</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="Frankle, J. &amp; Carbin, M. The lottery ticket hypothesis: finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR95" id="ref-link-section-d184064578e566">95</a></sup> and thus lead to a fortunate similarity with brain responses, and event a systematic divergence.</p></div></figure></div><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Average and shared response modeling (or noise ceiling)."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Average and shared response modeling (or noise ceiling).</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s42003-022-03036-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="1263"/></picture></a></div><p><b>a</b> Grand average MEG source estimates to word onset (<i>t</i> = 0 ms) for seven regions typically associated with reading (V1: purple, M1: green, fusiform gyrus: dark blue, supramarginal gyrus: light blue, superior temporal gyrus: orange, infero-frontal gyrus: yellow and fronto-polar gyrus: red), normalized to their peak response. Vertical bars indicate the peak time of each region. The full (not normalized) spatio-temporal time course of the whole-brain activity is displayed in Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM4">1</a>. <b>b</b> MEG shared response model (or noise ceilings), approximated by predicting brain responses of a given subject from those of all other subjects. Colored lines depict the mean noise ceiling in each region of interest. The gray line depicts the best noise ceiling across sources. <b>c</b> Same as <b>b</b> in sensor space. <b>d</b> Shared response model of fMRI recordings.</p></div></figure></div><p>We find that (1) a variety of deep learning algorithms linearly map onto the brain areas associated with reading (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>), (2) the best brain-mapping are obtained from the middle layers of deep language models and, critically, we show that (3) whether an algorithm maps onto the brain primarily depends on its ability to predict words context (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>).</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Brain-score comparison across embeddings."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Brain-score comparison across embeddings.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s42003-022-03036-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="288"/></picture></a></div><p>Lexical and compositional representations (see Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">4</a> for the definition of compositionality) can be isolated from (i) the word embedding layer (green) and (ii) one middle layer (red) of a typical language transformer (here, the ninth layer of a 12-layer causal transformer), respectively. We also report the brain scores of a convolutional neural network trained on visual character recognition (blue) to account for low-level visual representations. <b>a</b> Mean (across subjects) fMRI scores obtained with the visual, word, and compositional embeddings. All colored regions display significant fMRI scores across subjects (<i>n</i> = 100) after false discovery rate (FDR) correction. <b>b</b> Mean MEG scores averaged across all time samples and subjects (<i>n</i> = 95 subjects). <b>c</b> Left: mean MEG scores averaged across all sensors. Right: mean MEG gains averaged across all sensors: i.e., the gain in MEG score of one level relative to the level below (blue: <i>R</i>[visual]; green: <i>R</i>[word] − <i>R</i>[visual]; red: <i>R</i>[compositional] − <i>R</i>[word]). <b>d</b> Mean MEG gains in four regions of interest. For a whole-brain depiction of the MEG gains, see Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM5">2</a>. For the raw scores (without subtraction), see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">6</a>. For the distribution of scores across channels and voxels, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">4</a>.</p></div></figure></div><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Language transformers tend to converge towards brain-like representations."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Language transformers tend to converge towards brain-like representations.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s42003-022-03036-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42003-022-03036-1/MediaObjects/42003_2022_3036_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="495"/></picture></a></div><p><b>a</b> Bar plots display the average MEG score (across time and channels) of six representative transformers varying in tasks (causal vs. masked language modeling) and depth (4–12 layers). The green and red bars correspond to the word-embedding and middle layers, respectively. The star indicates the layer with the highest MEG score. <b>b</b> Average MEG scores (across subjects, time, and channels) of each of the embeddings (dots) extracted from 18 causal architectures, separately for the input layer (word embedding, green) and the middle layers (red). <b>c</b> Zoom of <b>b</b>, focusing on the best neural networks (i.e., word-prediction accuracy &gt;35%). The results reveal a plateau and/or a divergence of the middle and input layers. <b>d</b> Permutation importance quantifies the extent to which each property of the language transformers specifically contribute to making its embeddings more-or-less similar to brain activity (Δ<i>R</i>). All properties (training task. dimensionality <i>etc</i>.) significantly contribute to the brain scores (Δ<i>R</i> &gt; 0, all <i>p</i> &lt; 0.0001 across subjects). Ordered pairwise comparisons of the permutation scores are marked with a star (*<i>p</i> &lt; 0.05, **<i>p</i> &lt; 0.01, ***<i>p</i> &lt; 0.001). <b>e</b>–<b>h</b> Same as <b>a</b>–<b>d</b>, but evaluated on fMRI recordings. All error bars are the 95% confidence intervals across subjects (<i>n</i> = 95 for MEG, <i>n</i> = 100 for fMRI).</p></div></figure></div></div></div></section><section data-title="Results"><div id="Sec2-section"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Shared brain responses to words and sentences across subjects</h3><p>Before comparing deep language models to brain activity, we first aim to identify the brain regions recruited during the reading of sentences. To this end, we (i) analyze the average fMRI and MEG responses to sentences across subjects and (ii) quantify the signal-to-noise ratio of these responses, at the single-trial single-voxel/sensor level.</p><p>As expected<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fedorenko, E., Blank, I., Siegelman, M. &amp; Mineroff, Z. Lack of selectivity for syntax relative to word meanings throughout the language network. Cognition 203, 104348 (2020)." href="#ref-CR38" id="ref-link-section-d184064578e786">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dehaene, S. &amp; Cohen, L. The unique role of the visual word form area in reading. Trends Cogn. Sci. 15, 254–262 (2011)." href="#ref-CR39" id="ref-link-section-d184064578e786_1">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hagoort, P. The neurobiology of language beyond single-word processing. Science 366, 55–58 (2019)." href="#ref-CR40" id="ref-link-section-d184064578e786_2">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Hickok, G. &amp; Poeppel, D. The Cortical Organization of Speech Processing vol. 8, 393–402 (Nature Publishing Group, 2007)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR41" id="ref-link-section-d184064578e789">41</a></sup>, the average fMRI and MEG responses to words reveals a hierarchy of neural responses originating in V1 around 100 ms and continuing within the left posterior fusiform gyrus around 200 ms, the superior and middle temporal gyri, as well as the pre-motor and infero-frontal cortices between 150 and 500 ms after word onset (Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM4">1</a> and Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a> and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>a).</p><p>To quantify the proportion of these brain responses that depend on the specific content of sentences, we fit, for each subject separately, a shared response model across subjects (or noise-ceiling, see “Methods” section, Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">2</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a> and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>b–d). We then assess the accuracy of this model with a Pearson <i>R</i> correlation (hereafter referred to as “brain score” following<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Yamins, D. L. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. 111, 8619–8624 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR42" id="ref-link-section-d184064578e817">42</a></sup>) between the true and the predicted brain responses to held-out sentences, using a five-fold cross-validation. Finally, we assess the statistical significance of these brain scores with a two-sided Wilcoxon test across subjects, after testing for multiple comparison using false discovery rate (FDR) across voxels (see “Methods” section). Our shared response model confirms that the brain network classically associated with language processing elicits representations specific to words and sentences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR17" id="ref-link-section-d184064578e822">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Fedorenko, E. et al. Neural correlate of the construction of sentence meaning. Proc. Natl Acad. Sci. 113, E6256–E6262 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR43" id="ref-link-section-d184064578e825">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature 532, 453–458 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR44" id="ref-link-section-d184064578e828">44</a></sup>.</p><h3 id="Sec4">Deep language models reveal the hierarchical generation of language representations in the brain</h3><p>Where and when are the language representations of the brain similar to those of deep language models? To address this issue, we extract the activations (<i>X</i>) of a visual, a word and a compositional embedding (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig1">1</a>d) and evaluate the extent to which each of them maps onto the brain responses (<i>Y</i>) to the same stimuli. To this end, we fit, for each subject independently, an <i>ℓ</i><sub>2</sub>-penalized regression (<i>W</i>) to predict single-sample fMRI and MEG responses for each voxel/sensor independently. We then assess the accuracy of this mapping with a brain-score similar to the one used to evaluate the shared response model.</p><p>Overall, the brain scores of these trained models are largely above chance (all <i>p</i> &lt; 10<sup>−9</sup>, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>a, e). The modest correlation values are consistent with the high level of noise in single-sample single-voxel/channel neuroimaging data (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>b–d). For example, fMRI and MEG scores reach <i>R</i> = 0.048 and <i>R</i> = 0.041, respectively, for the compositional embedding, which is close to and even exceeds our shared response model (fMRI: <i>R</i> = 0.060, MEG: <i>R</i> = 0.020, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>).</p><p>In fMRI, the brain scores of the visual embedding peak in the early visual cortex (V1) (mean brain scores across voxels: <i>R</i> = 0.022 ± 0.003, <i>p</i> &lt; 10<sup>−11</sup>). By contrast, the brain scores of lexical embedding peak in the left superior temporal gyrus (<i>R</i> = 0.052 ± 0.004, <i>p</i> &lt; 10<sup>−13</sup>) as well as in the inferior temporal cortex and middle frontal gyrus (<i>R</i> = 0.053 ± 0.003, <i>p</i> &lt; 10<sup>−15</sup>) and are significant across the entire language and reading network (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>b). Finally, the brain scores of the compositional embedding are significantly higher than those of lexical of embeddings in the superior temporal gyrus (Δ<i>R</i> = 0.012 ± 0.001, <i>p</i> &lt; 10<sup>−16</sup>), the angular gyrus (Δ<i>R</i> = 0.010 ± 0.001, <i>p</i> &lt; 10<sup>−16</sup>), the infero-frontal cortex (Δ<i>R</i> = 0.016 ± 0.001, <i>p</i> &lt; 10<sup>−16</sup>) and the dorsolateral prefrontal cortex (Δ<i>R</i> = 0.012 ± 0.001, <i>p</i> &lt; 10<sup>−13</sup>). While these effects are lateralized (left hemisphere versus right hemisphere: Δ<i>R</i> = 0.010 ± 0.001, <i>p</i> &lt; 10<sup>−14</sup>), they are significant across a remarkably large number of bilateral areas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>b). Lexical and compositional embeddings accurately predict brain responses in the early visual cortex. This result is not necessarily surprising: language embeddings encode features (e.g., position of words in the sentence, beginning/end of the sentence) that correlate with visual information (words are flashed at a screen, and the sentences are separated by pauses). Critically, the gain (Δ<i>R</i>) of these embeddings remain very small, suggesting that this effect is mainly driven by the covariance between low-level and high-level representations of words.</p><h3 id="Sec5">Tracking the sequential generation of language representations over time and space</h3><p>To characterize the dynamics of these brain representations, we perform the same analysis using source-localized MEG recordings. The resulting brain scores are consistent with—although less spatially precise than—the above fMRI results (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>c, average brain score between 0 and 2 s). For clarity, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>d and Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM5">2</a> plot the gain in MEG scores: i.e., the difference of prediction performance between i) word and visual embeddings (green) and ii) the difference between compositional and word embedding (red). The brain scores of the visual embedding peak around 100 ms in V1 (<i>R</i> = 0.008 ± 0.002, <i>p</i> &lt; 10<sup>−3</sup>), and rapidly propagate to higher-level areas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>d and Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM5">2</a>). The gain achieved by the word embedding can be observed in the left posterior fusiform gyrus around 200 ms and peaks around 400 ms and in the left temporal and frontal cortices. Finally, the gain achieved by the compositional embedding is observed in a large number of bilateral brain regions, and peaks around 1 s after word onset (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>c, d).</p><p>After that period, brain areas outside the language network, such as area V1, appear to be better predicted by word and compositional embeddings than by visual ones (e.g., between visual and word in V1: Δ<i>R</i> = 0.016 ± 0.002, <i>p</i> &lt; 10<sup>−10</sup>). These effects could thus reflect feedback activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Seydell-Greenwald, A., Wang, X., Newport, E., Bi, Y. &amp; Striem-Amit, E. Spoken language comprehension activates the primary visual cortex. Preprint at bioRxiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR45" id="ref-link-section-d184064578e1012">45</a></sup> and explain why the corresponding fMRI responses are better accounted for by word and compositional embeddings than by visual ones.</p><p>Together with Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a>, these results show with unprecedented spatio-temporal precision, that the brain-mapping of our three representative embeddings automatically recovers the hierarchy of visual, lexical, and compositional representations of language in each cortical region.</p><h3 id="Sec6">Compositional embeddings best predict brain responses</h3><p>What computational principle leads these deep language models to generate brain-like activations? To address this issue, we generalize the above analyses and evaluate the brain scores of 36 transformer architectures (varying from 4 to 12 layers, each ranging from 128 to 512 dimensions, and each benefiting from 4 to 8 attention heads), trained on the same Wikipedia dataset either with a causal language modeling (CLM) or a masked language modeling task (MLM). While causal language models are trained to predict a word from its previous context, masked language models are trained to predict a randomly masked word from its both left and right context.</p><p>Overall, we observe that the corresponding brain scores largely vary as a function of the relative depth of the embedding within the language transformer. Specifically, both MEG and fMRI scores follow an inverted <i>U</i>-shaped pattern across layers for all architectures (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>a, e): the middle layers systematically outperform the output (fMRI: Δ<i>R</i> = 0.011 ± 0.001, <i>p</i> &lt; 10<sup>−18</sup>, MEG: Δ<i>R</i> = 0.003 ± 0.0005, <i>p</i> &lt; 10<sup>−13</sup>) and the input layers (fMRI: Δ<i>R</i>=.031 ± .001, <i>p</i> &lt; 10<sup>−18</sup>, MEG: Δ<i>R</i>=.009 ± .001, <i>p</i> &lt; 10<sup>−17</sup>). For simplicity, we refer to “middle layers” as the layers <i>l</i> <span>∈</span> [<i>n</i><sub>layers</sub>/2, 3<i>n</i><sub>layers</sub>/4] in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>a, e. This result confirms that the intermediary representations of deep language transformers are more brain-like than those of the input and output layers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR33" id="ref-link-section-d184064578e1088">33</a></sup>.</p><h3 id="Sec7">The emergence of brain-like representations predominantly depends on the algorithm’s ability to predict missing words</h3><p>The above findings result from trained neural networks. However, recent studies suggest that random (i.e., untrained) networks can significantly map onto brain responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In Proceedings of the National Academy of Sciences (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR27" id="ref-link-section-d184064578e1101">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kell, A., Yamins, D., Shook, E., Norman-Haignere, S. &amp; McDermott, J. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR46" id="ref-link-section-d184064578e1104">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Millet, J. &amp; King, J.-R. Inductive biases, pretraining and fine-tuning jointly account for brain responses to speech. Preprint at 
                  https://arXiv.org/2103.01032
                  
                 [cs, eess, q-bio] (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR47" id="ref-link-section-d184064578e1107">47</a></sup>. To test whether brain mapping specifically and systematically depends on the language proficiency of the model, we assess the brain scores of each of the 32 architectures trained with 100 distinct amounts of data. For each of these training steps, we compute the top-1 accuracy of the model at predicting masked or incoming words from their contexts. This analysis results in 32,400 embeddings, whose brain scores can be evaluated as a function of language performance, i.e., the ability to predict words from context (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>b, f).</p><p>We observe three main findings. First, random embeddings systematically lead to significant brain scores across subjects and architectures. The mean fMRI score across voxels is <i>R</i> = 0.019 ± 0.001, <i>p</i> &lt; 10<sup>−16</sup>. The mean MEG score across channels and time sample is <i>R</i> = 0.018 ± 0.0008, <i>p</i> &lt; 10<sup>−16</sup>. This result suggests that language transformers partially map onto brain responses independently of their language abilities.</p><p>Second, brain scores strongly correlate with language accuracy in both MEG (<i>R</i> = 0.77 Pearson’s correlation on average ± 0.01 across subjects) and fMRI (<i>R</i> = 0.57 ± 0.02, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>b, c). The correlation is highest for middle (fMRI: <i>R</i> = 0.81 ± 0.02; MEG: <i>R</i> = 0.86 ± 0.01) than input (fMRI: <i>R</i> = 0.39 ± 0.03; MEG: <i>R</i> = 0.73 ± 0.02) and output layers (fMRI: <i>R</i> = 0.63 ± 0.03; MEG:<i>R</i> = 0.78 ± 0.02). Beta coefficients for each particular layer and architecture are displayed in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a>a, b. Furthermore, single-voxel analyses show that this correlation between brain score and language performance is driven mainly by the superior temporal sulcus and gyrus for the embedding layer (mean <i>R</i> = 0.52 ± 0.06) and is widespread for the middle layers, exceeding a correlation of <i>R</i> = 0.85 in the superior temporal sulcus, infero-frontal, fusiform and angular gyri (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a>c). Overall, this result suggests that the better language models are at predicting words from context, the more their activations linearly map onto those of the brain.</p><p>Third, the highest brain scores are not achieved by the very best language transformers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>c, g). For instance, CLM transformers best map onto MEG (<i>R</i> = 0.039) and fMRI (<i>R</i> = 0.056) when they reach a language performance of 43% and 32%, respectively. By contrast, the very best transformers reach a language accuracy of 46%, but have significantly smaller brain scores (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>c, g).</p><h3 id="Sec8">Architectural and training factors impact brain scores too</h3><p>Language performance co-varies with the amount of training as well as with several architectural variables. To disentangle the contribution of each of these variables to the brain scores, we perform a permutation feature importance analysis. Specifically, we train a Random Forest estimator<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Breiman, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR48" id="ref-link-section-d184064578e1201">48</a></sup> to predict the average brain scores (across voxels or MEG sensors) of each subject independently, given the layer of the representation, the architectural properties (number of layers, dimensionality, and attention head), task (CLM and MLM), amount of training (number of steps) and language performance (top-1 accuracy) of the transformer. Permutation feature importance then estimates the unique contribution of each feature in explaining the variability of brain scores across models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Breiman, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR48" id="ref-link-section-d184064578e1205">48</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e1208">49</a></sup>. The results confirm that language performance is the most important factor that drives brain scores (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>d–h). This factor supersedes other covarying factors such as the amount of training, and the relative position of the embedding with regard to the architecture (“layer position”): Δ<i>R</i> = 0.56 ± 0.01 for fMRI, Δ<i>R</i> = 0.51 ± 0.02 for MEG. Nevertheless, these other factors contribute significantly to the prediction of brain scores (<i>p</i> &lt; 10<sup>−16</sup> across subjects for all variables).</p><p>Overall, these results show that the ability of deep language models to map onto the brain primarily depends on their ability to predict words from the context, and is best supported by the representations of their middle layers.</p></div></div></section><section data-title="Discussion"><div id="Sec9-section"><h2 id="Sec9">Discussion</h2><div id="Sec9-content"><p>Do deep language models and the human brain process sentences in the same way? Following a recent methodology<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR33" id="ref-link-section-d184064578e1239">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Yamins, D. L. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. 111, 8619–8624 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR42" id="ref-link-section-d184064578e1242">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature 532, 453–458 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR44" id="ref-link-section-d184064578e1245">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kell, A., Yamins, D., Shook, E., Norman-Haignere, S. &amp; McDermott, J. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR46" id="ref-link-section-d184064578e1248">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kell, A., Yamins, D., Shook, E., Norman-Haignere, S. &amp; McDermott, J. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR46" id="ref-link-section-d184064578e1251">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tang, H. et al. Recurrent computations for visual pattern completion. Proc. Natl Acad. Sci. 115, 8835–8840 (2018)." href="#ref-CR50" id="ref-link-section-d184064578e1254">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Khaligh-Razavi, S.-M. &amp; Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain it cortical representation. PLoS Comput. Biol. 10, e1003915 (2014)." href="#ref-CR51" id="ref-link-section-d184064578e1254_1">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446 (2015)." href="#ref-CR52" id="ref-link-section-d184064578e1254_2">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Güçlü, U. &amp; van Gerven, M. A. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. J. Neurosci. 35, 10005–10014 (2015)." href="#ref-CR53" id="ref-link-section-d184064578e1254_3">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Eickenberg, M., Gramfort, A., Varoquaux, G. &amp; Thirion, B. Seeing it all: convolutional network layers map the function of the human visual system. NeuroImage 152, 184–194 (2017)." href="#ref-CR54" id="ref-link-section-d184064578e1254_4">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yamins, D. L. &amp; DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. Nat. Neurosci. 19, 356 (2016)." href="#ref-CR55" id="ref-link-section-d184064578e1254_5">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Saxe, A., Nelli, S. &amp; Summerfield, C. If deep learning is the answer, what is the question? Nat. Rev. Neurosci. 22, 1–13 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR56" id="ref-link-section-d184064578e1258">56</a></sup>, we address this issue by evaluating whether the activations of a large variety of deep language models linearly map onto those of 102 human brains. Our study provides two main contributions.</p><p>First, our work complements previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Wehbe, L., Vaswani, A., Knight, K. &amp; Mitchell, T. Aligning context-based statistical models of language with brain activity during reading. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) 233–243 (Association for Computational Linguistics, 2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR26" id="ref-link-section-d184064578e1265">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In Proceedings of the National Academy of Sciences (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR27" id="ref-link-section-d184064578e1268">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020)." href="#ref-CR30" id="ref-link-section-d184064578e1271">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jain, S. &amp; Huth, A. in Advances in Neural Information Processing Systems (eds Bengio, S. et al.) vol. 31, 6628–6637 (Curran Associates, Inc., 2018)." href="#ref-CR31" id="ref-link-section-d184064578e1271_1">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Athanasiou, N., Iosif, E. &amp; Potamianos, A. Neural activation semantic models: computational lexical semantic models of localized neural activations. In Proceedings of the 27th International Conference on Computational Linguistics 2867–2878 (Association for Computational Linguistics, 2018)." href="#ref-CR32" id="ref-link-section-d184064578e1271_2">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="#ref-CR33" id="ref-link-section-d184064578e1271_3">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. &amp; de Lange, F. P. A hierarchy of linguistic predictions during natural language comprehension. bioRxiv 
                  https://doi.org/10.1101/2020.12.03.410399
                  
                 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR34" id="ref-link-section-d184064578e1274">34</a></sup> and confirms that the activations of deep language models significantly map onto the brain responses to written sentences (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>). This mapping peaks in a distributed and bilateral brain network (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>a, b) and is best estimated by the middle layers of language transformers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>a, e). The notion of representation underlying this mapping is formally defined as linearly-readable information. This operational definition helps identify brain responses that any neuron can differentiate—as opposed to entangled information, which would necessitate several layers before being usable<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Minsky, M. &amp; Papert, S. Perceptrons: An Introduction to Computational Geometry. (MIT Press, 1969)." href="#ref-CR57" id="ref-link-section-d184064578e1287">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cadieu, C. F. et al. Deep neural networks rival the representation of primate it cortex for core visual object recognition. PLoS Comput. Biol. 10, e1003963 (2014)." href="#ref-CR58" id="ref-link-section-d184064578e1287_1">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. A. Representational similarity analysis—connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="#ref-CR59" id="ref-link-section-d184064578e1287_2">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="King, J.-R. &amp; Dehaene, S. Characterizing the dynamics of mental representations: the temporal generalization method. Trends Cogn. Sci. 18, 203–210 (2014)." href="#ref-CR60" id="ref-link-section-d184064578e1287_3">60</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Cohen, U., Chung, S., Lee, D. D. &amp; Sompolinsky, H. Separability and geometry of object manifolds in deep neural networks. Nat. Commun. 11, 1–13 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR61" id="ref-link-section-d184064578e1290">61</a></sup>.</p><p>Furthermore, the comparison between visual, lexical, and compositional embeddings precise the nature and dynamics of these cortical representations. In particular, our results shows with unprecedented spatio-temporal precision that early visual responses (&lt;150 ms) are quasi-entirely accounted for by visual embeddings, and then transmitted to the posterior fusiform gyrus, which switches from visual to lexical representations around 200 ms (Movie 2). This finding strengthens the claim that this area is responsible for orthographic and morphemic computations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Dehaene, S. &amp; Cohen, L. The unique role of the visual word form area in reading. Trends Cogn. Sci. 15, 254–262 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR39" id="ref-link-section-d184064578e1297">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Hermes, D. et al. Electrophysiological responses in the ventral temporal cortex during reading of numerals and calculation. Cereb. Cortex 27, 567–575 (2017)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR62" id="ref-link-section-d184064578e1300">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Woolnough, O. et al. Spatiotemporal dynamics of orthographic and lexical processing in the ventral visual pathway. Nat. Hum. Behav. 5, 389–398 (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR63" id="ref-link-section-d184064578e1303">63</a></sup>. Then, around 400 ms, word embeddings predict a large fronto-temporo-parietal network which peaks in the left temporal gyrus; these word representations are then maintained for several seconds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR17" id="ref-link-section-d184064578e1307">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Sassenhagen, J. &amp; Fiebach, C. J. Traces of meaning itself: Encoding distributional word vectors in brain activity. Neurobiology of Language 1.1, 54–76 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR19" id="ref-link-section-d184064578e1310">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Jain, S. &amp; Huth, A. in Advances in Neural Information Processing Systems (eds Bengio, S. et al.) vol. 31, 6628–6637 (Curran Associates, Inc., 2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR31" id="ref-link-section-d184064578e1313">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). Advances in Neural Information Processing Systems 32 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR33" id="ref-link-section-d184064578e1316">33</a></sup>. This result not only confirms the wide spread distribution of meaning in the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature 532, 453–458 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR44" id="ref-link-section-d184064578e1320">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Price, C. J. The anatomy of language: a review of 100 fmri studies published in 2009. Ann. N. Y. Acad. Sci. 1191, 62–88 (2010)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR64" id="ref-link-section-d184064578e1323">64</a></sup>, but also reveals its remarkably long-lasting nature.</p><p>Finally, compositional embeddings peak in the brain regions associated with high-level language processing such as the infero-frontal and the anterior temporal cortices as well as the superior temporal cortex and the temporal-parietal junction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Brennan, J. R. &amp; Pylkkänen, L. Meg evidence for incremental sentence composition in the anterior temporal lobe. Cogn. Sci. 41, 1515–1531 (2017)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR35" id="ref-link-section-d184064578e1330">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Hickok, G. &amp; Poeppel, D. The Cortical Organization of Speech Processing vol. 8, 393–402 (Nature Publishing Group, 2007)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR41" id="ref-link-section-d184064578e1333">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Pallier, C., Devauchelle, A.-D. &amp; Dehaene, S. Cortical representation of the constituent structure of sentences. Proc. Natl Acad. Sci. 108, 2522–2527 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR65" id="ref-link-section-d184064578e1336">65</a></sup>. We confirm that these left-lateralized representations are significant in both hemispheres<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Fedorenko, E., Hsieh, P.-J., Nieto-Castañón, A., Whitfield-Gabrieli, S. &amp; Kanwisher, N. New method for fmri investigations of language: defining rois functionally in individual subjects. J. Neurophysiol. 104, 1177–1194 (2010)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR66" id="ref-link-section-d184064578e1340">66</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Cogan, G. B. et al. Sensory–motor transformations for speech occur bilaterally. Nature 507, 94–98 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR67" id="ref-link-section-d184064578e1343">67</a></sup>. Critically, MEG suggests that these compositional effects become dominant and clearly bilateral long after word onset (&gt;800 ms). We speculate that this surprisingly late responses may be due to the complexity of the sentences used in the present study, which may slow down compositional computations.</p><p>At this stage, however, these three levels representations remain coarsely defined. Further inspection of artificial<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Lakretz, Y. et al. The emergence of number and syntax units in LSTM language models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR8" id="ref-link-section-d184064578e1351">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. &amp; Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proc. Natl Acad. Sci. 117, 30046–30054 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR68" id="ref-link-section-d184064578e1354">68</a></sup> and biological networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Hale, J. T. et al. Neuro-computational models of language processing." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR10" id="ref-link-section-d184064578e1358">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Disentangling syntax and semantics in the brain with deep networks. ICML 2021-38th International Conference on Machine Learning (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR28" id="ref-link-section-d184064578e1361">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Reddy, A. J. &amp; Wehbe, L. Syntactic representations in the human brain: beyond effort-based metrics. Preprint at bioRXiv (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR69" id="ref-link-section-d184064578e1364">69</a></sup> remains necessary to further decompose them into interpretable features. In particular, it will be important to test whether the converging representations presently identified solely correspond to well-known linguistics phenomena as our supplementary analyses suggest (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">2</a> and Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">3</a>), or, on the contrary, whether they correspond to unknown language structures.</p><p>Second, our study shows that the similarity between deep language models and the brain primarily depends on their ability to predict words from their context. Specifically, we show that language performance is the most contributing factor explaining the variability of brain scores across embeddings (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>d, h). Analogous results have been reported in both vision and audition research, where best deep learning models tend to best map onto brain responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In Proceedings of the National Academy of Sciences (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR27" id="ref-link-section-d184064578e1380">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Yamins, D. L. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. 111, 8619–8624 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR42" id="ref-link-section-d184064578e1383">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Yamins, D. L. &amp; DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. Nat. Neurosci. 19, 356 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR55" id="ref-link-section-d184064578e1386">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Schrimpf, M. et al. Brain-score: which artificial neural network for object recognition is most brain-like? Preprint at bioRXiv (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR70" id="ref-link-section-d184064578e1389">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Kell, A. J. E., Yamins, D. L. K., Shook, E. N., Norman-Haignere, S. V. &amp; McDermott, J. H. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR71" id="ref-link-section-d184064578e1392">71</a></sup>. In addition, our results are consistent with the findings of Schrimpf et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In Proceedings of the National Academy of Sciences (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR27" id="ref-link-section-d184064578e1396">27</a></sup> reported simultaneously to ours. Together, these results suggest that deep learning algorithms converge—at least partially—to brain-like representations during their training. This result is not trivial: the representations that are optimal to predict masked or future words from large amounts of text could have been very distinct from those the brain learns to generate.</p><p>The mapping between deep language models and brain recordings reaches very low correlation values. This phenomenon is expected: i) neuroimaging is notoriously noisy and ii) we analyze and model here single-sample responses of single-voxel/sensor. However, the resulting brain scores are i) highly significant (all <i>p</i> &lt; 10<sup>−9</sup> on average across both all fMRI voxels and MEG sensors), including when compared to a permutation baseline (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">3</a>), and ii) in the same order of magnitude than a baseline shared-response model (or noise ceiling, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>) as well as previous reports (see e.g., <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature 532, 453–458 (2016)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR44" id="ref-link-section-d184064578e1414">44</a></sup> before correcting for the noise ceiling). Besides, we generally report brain scores averaged across all voxels or MEG channels, even though many brain areas do not strongly respond to language (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a>). Critically, the link between brain scores and language performance is strong: the correlation between the language performance and brain scores is above <i>R</i> = 0.90 for MEG and <i>R</i> = 0.80 for fMRI (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a>). Nevertheless, it is clear that improving the the signal-to-noise ratio, for instance by using increasingly large datasets<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. GPT-2’s Activations Predict the Degree of Semantic Comprehension in the Human Brain (Cold Spring Harbor Laboratory Section: New Results, 2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR20" id="ref-link-section-d184064578e1431">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Caucheteux, C., Gramfort, A. &amp; King, J.-R. Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. In EMNLP 2021—Conference on Empirical Methods in Natural Language Processing (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR29" id="ref-link-section-d184064578e1434">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Millet, J. &amp; King, J.-R. Inductive biases, pretraining and fine-tuning jointly account for brain responses to speech. Preprint at 
                  https://arXiv.org/2103.01032
                  
                 [cs, eess, q-bio] (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR47" id="ref-link-section-d184064578e1437">47</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Nastase, S. A. et al. Narratives: fmri data for evaluating models of naturalistic language comprehension. Trends in neurosciences 43, 271–273 (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR72" id="ref-link-section-d184064578e1440">72</a></sup> will be critical to precisely characterize the nature of brain representations.</p><p>Permutation feature importance shows that several factors such as the amount of training and the architecture significantly impact brain scores. This finding contributes to a growing list of variables that lead deep language models to behave more-or-less similarly to the brain. For example, Hale et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Hale, J., Dyer, C., Kuncoro, A. &amp; Brennan, J. R. Finding syntax in human encephalography with beam search. Preprint at 
                  https://arXiv.org/1806.04127
                  
                 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR36" id="ref-link-section-d184064578e1447">36</a></sup> showed that the amount and the type of corpus impact the ability of deep language parsers to linearly correlate with EEG responses. The present work complements this finding by evaluating the full set of activations of deep language models. It further demonstrates that the key ingredient to make a model more brain-like is, for now, to improve its language performance.</p><p>The conclusion that deep networks converge towards brain-like representations should be qualified: we show that the brain scores of the very best models tend to ultimately decrease with language performance, especially in fMRI (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>g). We speculate that this phenomenon (also observed in vision<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Schrimpf, M. et al. Brain-score: which artificial neural network for object recognition is most brain-like? Preprint at bioRXiv (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR70" id="ref-link-section-d184064578e1457">70</a></sup>) may rise because transformers overfit an inappropriate objective. Specifically, while there is growing evidence that the human brain does predict words from context<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR30" id="ref-link-section-d184064578e1461">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Keller, G. B. &amp; Mrsic-Flogel, T. D. Predictive processing: a canonical cortical computation. Neuron 100, 424–435 (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR73" id="ref-link-section-d184064578e1464">73</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. &amp; de Lange, F. P. A hierarchy of linguistic predictions during natural language comprehension. Preprint at bioRXiv (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR74" id="ref-link-section-d184064578e1467">74</a></sup>, this learning rule may not fully account for the complex (and potentially various) tasks performed by the brain (e.g., long-range<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="Wang, L. Dynamic predictive coding across the left fronto-temporal language hierarchy: evidence from MEG, EEG and fMRI29." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR75" id="ref-link-section-d184064578e1471">75</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Lee, C. S., Aly, M. &amp; Baldassano, C. Anticipation of temporally structured events in the brain. eLife 10, e64972 (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR76" id="ref-link-section-d184064578e1474">76</a></sup> and hierarchical predictions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR77" id="ref-link-section-d184064578e1478">77</a></sup>).</p><p>This discrepancy adds to the long-list of differences between deep language models and the brain: whereas the brain is trained (i) with a recurrent architecture and (ii) on a relatively small amount of grounded sentences, transformers are trained (i) with a massively feedforward architecture and (ii) on huge text databases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR7" id="ref-link-section-d184064578e1485">7</a></sup> (note that, given large-enough spaces, feedforward transformers may actually implement computations similar to recurrent networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Ramsauer, H. et al. Hopfield networks is all you need. Preprint at 
                  https://arXiv.org/2008.02217
                  
                 [cs, stat] (2021)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR78" id="ref-link-section-d184064578e1489">78</a></sup>). Consequently, while the similarity between deep networks and the brain provide a stepping stone to unravel the foundation of natural language processing, identifying the remaining differences between these two systems remains, by far, the major challenge to build algorithms that learn and think like humans<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (2020)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR7" id="ref-link-section-d184064578e1493">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Loula, J., Baroni, M. &amp; Lake, B. M. Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. In BlackboxNLP@ EMNLP (2018)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR9" id="ref-link-section-d184064578e1496">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Lake, B. M., Ullman, T. D., Tenenbaum, J. B. &amp; Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences 40 (2017)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR79" id="ref-link-section-d184064578e1499">79</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. &amp; Choi, Y. Hellaswag: can a machine really finish your sentence? Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR80" id="ref-link-section-d184064578e1502">80</a></sup></p></div></div></section><section data-title="Methods"><div id="Sec10-section"><h2 id="Sec10">Methods</h2><div id="Sec10-content"><h3 id="Sec11">Deep language transformers</h3><p>To model word and sentence representations, we trained a variety of transformers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Vaswani, A. et al. Attention is all you need. In Proceedings on NIPS (Cornell University, 2017)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR4" id="ref-link-section-d184064578e1517">4</a></sup>, and input them with the same sentences that the subject read. Transformers consist of multiple contextual transformer layers stacked onto one non-contextualized word embedding layer (a look-up table). Following the standard implementation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Vaswani, A. et al. Attention is all you need. In Proceedings on NIPS (Cornell University, 2017)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR4" id="ref-link-section-d184064578e1521">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR81" id="ref-link-section-d184064578e1524">81</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog 1, 9 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR82" id="ref-link-section-d184064578e1527">82</a></sup>, the word embedding layer is trained simultaneously with the contextual layers: the weights of the word embedding vary with the training, and so do their activations in response to fixed inputs. Thus, one representation can be extracted from each (contextual or non-contextual) layer. We always extract the activations in a causal way: for example, given the sentence “THE CAT IS ON THE MAT”, the brain response to “ON” would be solely compared to the activations of the transformer input with “THE CAT IS ON”, and extracted from the “ON” contextualized embeddings. Word embeddings and contextualized embeddings were generated for every word, by generating word sequences from the three previous sentences. We did not observe qualitatively different results when using shorter or longer contexts. It is to be noted that the sentences were isolated, and were not part of a narrative.</p><p>In total, we investigated 32 distinct architectures varying in their dimensionality (<span>∈</span> [128, 256, 512]), number of layers (<span>∈</span> [4, 8, 12]), attention heads (<span>∈</span> [4, 8]), and training task (causal language modeling and masked language modeling). While causal language transformers are trained to predict a word from its previous context, masked language transformers predict randomly masked words from a surrounding context. We froze the networks at ≈100 training stages (log distributed between 0 and 4, 5 M gradient updates, which corresponds to ≈35 passes over the full corpus), resulting in 3600 networks in total, and 32,400 word representations (one per layer). The training was early-stopped when the networks’ performance did not improve after five epochs on a validation set. Therefore, the number of frozen steps varied between 96 and 103 depending on the training length.</p><p>The algorithms were trained using XLM implementation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Lample, G. &amp; Conneau, A. Cross-lingual language model pretraining. In Adv. Neural Inf. Process. Syst. (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR6" id="ref-link-section-d184064578e1537">6</a></sup>. No hyper-parameter tuning was performed. Following <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Lample, G. &amp; Conneau, A. Cross-lingual language model pretraining. In Adv. Neural Inf. Process. Syst. (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR6" id="ref-link-section-d184064578e1541">6</a></sup>, each algorithm was trained on eight GPUs using early stopping with training perplexity criteria, 16 streams per batch, 128 words per stream, epoch size of 200,000 streams, 0.1 dropout, 0.1 attention dropout, gelu activation, inverse (sqrt) adam optimizer with learning rate 0.0001, 0.01 weight decay, on the same Wikipedia corpus of 278,386,651 words (in Dutch) extracted using WikiExtractor<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Attardi, G. Wikiextractor. 
                  https://github.com/attardi/wikiextractor
                  
                 (2015)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR83" id="ref-link-section-d184064578e1545">83</a></sup> and pre-processed using Moses tokenizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Koehn, P. et al. Moses: ppen source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions 177–180 (Association for Computational Linguistics, 2007)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR84" id="ref-link-section-d184064578e1549">84</a></sup>, with punctuation. We restricted the vocabulary to the 50,000 most frequent words, concatenated with all words used in the study (50,341 vocabulary words in total). These design choices enforce that the difference in brain scores observed across models cannot be explained by differences in corpora and text preprocessing.</p><p>To evaluate the language processing performance of the networks, we computed their performance (top-1 accuracy on word prediction given the context) using a test dataset of 180,883 words from Dutch Wikipedia. The list of architectures and their final performance at next-word prerdiction is provided in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">2</a>.</p><p>For clarity, we dissociate:</p><ul>
                  <li>
                    <p>The architectures (e.g., one transformer with 12 layers): there are 36 transformer architectures here (18 CLM and 18 MLM).</p>
                  </li>
                  <li>
                    <p>The models: one architecture, frozen at one particular learning step. Since we use 100 learning steps, there are 36 × 100 = 3600 networks here.</p>
                  </li>
                  <li>
                    <p>The embeddings: one word representation extracted from a network, at one particular layer. Since the number of layers varies with the architecture (twelve networks with 5, twelve networks with 9 and twelve networks with 13 twelve layers, including the non contextualized word embedding), there are 12 × (5 + 9 + 13) = 324 representations per step, so 324 × 100 = 3400 word embeddings in total.</p>
                  </li>
                </ul><h3 id="Sec12">Visual convolutional neural network</h3><p>To model visual representations, every word presented to the subjects was rendered on a gray 100 × 32 pixel background with a centered black Arial font, and input to a VGG network pretrained to recognize words from images<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Baek, J. et al. What is wrong with scene text recognition model comparisons? dataset and model analysis. In Proceedings of the IEEE International Conference on Computer Vision, 4715–4723 
                  https://github.com/clovaai/deep-text-recognition-benchmark
                  
                 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR85" id="ref-link-section-d184064578e1591">85</a></sup>, resulting in an 888-dimensional embedding. Specifically, this model was trained on real pictures of single words taken in naturalistic settings (e.g., ad, banner).</p><p>This embedding was used to replicate and extend previous work on the similarity between visual neural network activations and brain responses to the same images (e.g., <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Yamins, D. L. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. 111, 8619–8624 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR42" id="ref-link-section-d184064578e1598">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446 (2015)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR52" id="ref-link-section-d184064578e1601">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Güçlü, U. &amp; van Gerven, M. A. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. J. Neurosci. 35, 10005–10014 (2015)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR53" id="ref-link-section-d184064578e1604">53</a></sup>).</p><h3 id="Sec13">Neuroimaging protocol</h3><p>For all the analyses, we used the open-source dataset released by Schoffelen and colleagues<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e1616">37</a></sup>, gathering the functional magnetic resonance imaging (fMRI) and magneto-encephalography (MEG) recordings of 204 native Dutch speakers (100 males), aged from 18 to 33 years. Here, we focused on the 102 right-handed speakers who performed a reading task while being recorded by a CTF magneto-encephalography (MEG) and, in a separate session, with a SIEMENS Trio 3T Magnetic Resonance scanner<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e1620">37</a></sup>.</p><p>Words (in Dutch) were flashed one at a time with a mean duration of 351 ms (ranging from 300 to 1400 ms), separated with a 300 ms blank screen, and grouped into sequences of 9–15 words, for a total of approximately 2700 words per subject. Sequences were separated by a 5 s-long blank screen. We restricted our study to meaningful sentences (400 distinct sentences in total, 120 per subject). The exact syntactic structures of sentences varied across all sentences. Roughly, sentences were either composed of a main clause and a simple subordinate clause, or contained a relative clause. Twenty percent of the sentences were followed by a yes/no question (e.g., “Did grandma give a cookie to the girl?”) to ensure that subjects were paying attention. Questions were not included in the dataset, and thus excluded from our analyses. Sentences were grouped into blocks of five sequences. This grouping was used for cross-validation to avoid information leakage between the train and test sets.</p><h3 id="Sec14">Magnetic resonance imaging (MRI)</h3><p>Structural images were acquired with a T1-weighted magnetization-prepared rapid gradient-echo (MP-RAGE) pulse sequence. The full acquisition details, available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e1635">37</a></sup>, are summarized here simplicity: TR = 2300 ms, TE = 3.03 ms, 8 degree flip-angle, 1 slab, slice-matrix size = 256 × 256, slice thickness = 1 mm, field of view = 256 mm, isotropic voxel-size = 1.0 × 1.0 × 1.0 mm. Structural images were defaced by Schoffelen and colleagues. Preprocessing of the structural MRI was performed with Freesurfer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Fischl, B. Freesurfer. Neuroimage 62, 774–781 (2012)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR86" id="ref-link-section-d184064578e1639">86</a></sup>, using the <span>recon-all</span> pipeline and a manual inspection of the cortical segmentations, realigned to “fsaverage”. Region-of-interest analyses were selected from the PALS Brodmann’s Area atlas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Van Essen, D. C. A population-average, landmark-and surface-based (pals) atlas of human cerebral cortex. Neuroimage 28, 635–662 (2005)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR87" id="ref-link-section-d184064578e1646">87</a></sup> and the Destrieux atlas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title="Destrieux, C., Fischl, B., Dale, A. &amp; Halgren, E. Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature. Neuroimage 53, 1–15 (2010)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR88" id="ref-link-section-d184064578e1650">88</a></sup>.</p><p>Functional images were acquired with a T2<sup>*</sup>-weighted functional echo-planar blood oxygenation level-dependent (EPI-BOLD) sequence. The full acquisition details, available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e1659">37</a></sup>, are summarized here for simplicity: TR = 2.0 s, TE = 35 ms, flip angle = 90 degrees, anisotropic voxel size = 3.5 × 3.5 × 3.0 mm extracted from 29 oblique slices. fMRI was preprocessed with fMRIPrep with default parameters<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 89" title="Esteban, O. et al. fmriprep: a robust preprocessing pipeline for functional mri. Nat. Methods 16, 111–116 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR89" id="ref-link-section-d184064578e1663">89</a></sup>. The resulting BOLD times series were detrended and de-confounded from 18 variables (the six estimated head-motion parameters (<span>trans</span><sub>x,y,z</sub>, <span>rot</span><sub>x,y,z</sub>) and the first six noise components calculated using anatomical CompCorr<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Behzadi, Y., Restom, K., Liau, J. &amp; Liu, T. T. A component based noise correction method (compcor) for bold and perfusion based fmri. Neuroimage 37, 90–101 (2007)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR90" id="ref-link-section-d184064578e1676">90</a></sup> and six DCT-basis regressors using nilearn’s <span>clean_img</span> pipeline and otherwise default parameters<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Abraham, A. et al. Machine learning for neuroimaging with scikit-learn. Front. Neuroinform. 8, 14 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR91" id="ref-link-section-d184064578e1683">91</a></sup>. The resulting volumetric data lying along a 3 mm line orthogonal to the mid-thickness surface were linearly projected to the corresponding vertices. The resulting surface projections were spatially decimated by 10, and are hereafter referred to as voxels, for simplicity. Finally, each group of five sentences was separately and linearly detrended. It is noteworthy that our cross-validation never splits such groups of five consecutive sentences between the train and test sets. Two subjects were excluded from the fMRI analyses because of difficulties in processing the metadata, resulting in 100 fMRI subjects.</p><h3 id="Sec15">Magneto-encephalography (MEG)</h3><p>The MEG time series were preprocessed using MNE-Python and its default parameters except when specified<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Gramfort, A. et al. Mne software for processing meg and eeg data. NeuroImage 86, 446–460 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR92" id="ref-link-section-d184064578e1696">92</a></sup>. Signals were band-passed filtered between 0.1 and 40 Hz filtered, spatially corrected with a Maxwell Filter, clipped between the 0.01st and 99.99th percentiles, segmented between −500 ms to +2000 ms relative to word onset and baseline-corrected before <i>t</i> = 0. Reference channels and non-MEG channels were excluded from subsequent analyses, leading to 273 MEG channels per subject. We manually co-referenced (i) the skull segmentation of subjects’ anatomical MRI with (ii) the head markers digitized before MEG acquisition. A single-layer forward model was generated with the Freesurfer-wrapper implemented in MNE-Python<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Gramfort, A. et al. Mne software for processing meg and eeg data. NeuroImage 86, 446–460 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR92" id="ref-link-section-d184064578e1703">92</a></sup>. Due to the lack of empty-room recordings, the noise covariance matrix used for the inverse operator was estimated from the zero-centered 200 ms of baseline MEG activity preceding word onset. Subjects’ source space inverse operators were computed using a dSPRM. The average brain responses displayed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig1">1</a>d were computed as the square of the average evoked related field across all words for each subject separately, averaged across subjects, and finally divided by their respective maxima, to highlight temporal differences. Supplementary Movie <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM4">1</a> displays the average sources without normalization. Seven subjects were excluded from the MEG analyses because of difficulties in processing the metadata, resulting in 92 usable MEG recordings.</p><h3 id="Sec16">Shared response model: Brain → Brain mapping</h3><p>To estimate the amount of explainable signal in each MEG and fMRI recording, we trained and evaluated, through cross-validation, a linear mapping model <i>W</i> to predict the brain responses of a given subject to each sentence <i>Y</i> from the aggregated brain responses of all other subjects who read the same sentence <i>X</i>. Specifically, five cross-validation splits were implemented across 5-sentence blocks with scikit-learn GroupKFold<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e1730">49</a></sup>. For each word of each sentence <i>i</i>, all but one subject who read the corresponding sentence were averaged with one another to form a template brain response: <span>\({x}_{{{i}}}\in {{\mathbb{R}}}^{{{n}}}\)</span> with <i>n</i> the number of MEG channels or fMRI voxels, as well as a target brain response <span>\({y}_{{{i}}}\in {{\mathbb{R}}}^{{{n}}}\)</span> corresponding to the remaining subject. <i>X</i> and <i>Y</i> were normalized (mean = 0, std = 1) across sentences for each spatio-temporal dimension, using a robust scaler clipping below and above the 0.01st and 99.99th percentiles, respectively. A linear mapping <span>\(W\in {{\mathbb{R}}}^{{{n}}\times {{n}}}\)</span> was then fit with a ridge regression to best predict <i>Y</i> from <i>X</i> on the train set:</p><div id="Equ1"><p><span>$$W={({X}_{{{{{{{{\rm{train}}}}}}}}}^{{{T}}}{X}_{{{{{{{{\rm{train}}}}}}}}}+\lambda I)}^{-1}{X}_{{{{{{{{\rm{train}}}}}}}}}^{{{T}}}{Y}_{{{{{{{{\rm{train}}}}}}}}},$$</span></p><p>
                    (1)
                </p></div><p>with <i>λ</i> the <i>l</i>2 regularization parameter, chosen amongst 20 values log-spaced between 10<sup>−3</sup> and 10<sup>8</sup> with nested leave-one-out cross-validation for each dimension separately (as implemented in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e2009">49</a></sup>). Brain predictions <span>\(\hat{Y}=WX\)</span> were evaluated with a Pearson correlation on the test set:</p><div id="Equ2"><p><span>$$R={{{{{{{\rm{Corr}}}}}}}}({Y}_{{{{{{{{\rm{test}}}}}}}}},{\hat{Y}}_{{{{{{{{\rm{test}}}}}}}}}).$$</span></p><p>
                    (2)
                </p></div><p>For the MEG source noise estimate, the correlation was also performed after source projection:</p><div id="Equ3"><p><span>$$R={{{{{{{\rm{Corr}}}}}}}}(K{Y}_{{{{{{{{\rm{test}}}}}}}}},K{\hat{Y}}_{{{{{{{{\rm{test}}}}}}}}})$$</span></p><p>
                    (3)
                </p></div><p>with <span>\(K\in {{\mathbb{R}}}^{n\times m}\)</span> the inverse operator projecting the <i>n</i> MEG sensors onto <i>m</i> sources. Correlation scores were finally averaged across cross-validation splits for each subject, resulting in one correlation score (“brain score”) per voxel (or per MEG sensor/time sample) per subject.</p><h3 id="Sec17">Brain score and similarity: Network → Brain mapping</h3><p>To estimate the functional similarity between each artificial neural network and each brain, we followed the same analytical pipeline used for noise ceiling, but replaced <i>X</i> with the activations of the deep learning models. Specifically, using the same cross-validation, and for each subject separately, we trained a linear mapping <span>\(W\in {{\mathbb{R}}}^{{{o,n}}}\)</span> with <i>o</i> the number of activations, to predict brain responses <i>Y</i> from the network activations <i>X</i>. <i>X</i> was normalized across words (mean = 0, std = 1).</p><p>To account for the hemodynamic delay between word onset and the BOLD response recorded in fMRI, we used a finite impulse response (FIR) model with five delays (from 2 to 10 s) to build <i>X</i><sup>*</sup> from <i>X</i>. <i>W</i> was found using the same ridge regression described above, and evaluated with the same correlation scoring procedure. The resulting brain correlation scores measure the linear relationship between the brain signals of one subject (measured either by MEG or fMRI) and the activations of one artificial neural network (e.g., a word embedding). For MEG, we simply fit and evaluated the model activations <i>X</i> at each time sample independently.</p><p>In principle, one may orthogonalize low-level representations (e.g., visual features) from high-level network models (e.g., language model), to separate the specific contribution of each type of model. This is because middle layers have access to the word-embedding layer, and can, in principle, simply copy some of its activations. Similarly, word embedding can implicitly contain visual information: e.g., frequent words tend to be visually smaller than rare ones. In our case, however, the middle layers of transformers were much better than word embeddings, which were much better than visual embeddings. To quantify the gain Δ<i>R</i> achieved by a higher-level model <i>M</i><sub>1</sub> (e.g., the middle layers of a transformer) and a lower level model <i>M</i><sub>2</sub> (e.g., a word embedding) we thus simply compared the difference of their encoding scores:</p><div id="Equ4"><p><span>$${{\Delta }}{R}_{{{{{{{\mathrm{M}}}}}}}_{1}}={R}_{{{{{{{\mathrm{M}}}}}}}_{1}}-{R}_{{{{{{{\mathrm{M}}}}}}}_{2}}$$</span></p><p>
                    (4)
                </p></div><p>Results are consistent when using different orthogonalization methods (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">5</a>).</p><h3 id="Sec18">Convergence analysis</h3><p>All neural networks but the visual CNN were trained from scratch on the same corpus (as detailed in the first “Methods” section). We systematically computed the brain scores of their activations on each subject, sensor (and time sample in the case of MEG) independently. For computational reasons, we restricted model comparison on MEG encoding scores to ten time samples regularly distributed between [0, 2]s. Brain scores were then averaged across spatial dimensions (i.e., MEG channels or fMRI surface voxels), time samples, and subjects to obtain the results in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>. To evaluate the convergence of a model, we computed, for each subject separately, the correlation between (1) the average brain score of each network and (2) its performance or its training step (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM2">1</a>). Positive and negative correlations indicate convergence and divergence, respectively. Brain scores above 0 before training indicate a fortuitous relationship between the activations of the brain and those of the networks.</p><h3 id="Sec19">Permutation feature importance</h3><p>To systematically quantify how the architecture, language accuracy, and training of the language transformers impacted their ability to linearly map onto brain activity, we fitted, for each subject separately, a Random Forest across the models’ properties to predict their brain scores, using scikit-learn’s <span>RandomForest</span><sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Breiman, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR48" id="ref-link-section-d184064578e2470">48</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e2473">49</a></sup>. Specifically, we input the following features to the random forest: the training task (causal language modeling “CLM” vs. masked language modeling “MLM”), the number of attention heads <span>∈</span> [4, 8], the total number of layers <span>∈</span> [4, 8, 12], dimensionality <span>∈</span> [128, 256, 512], training step (number of gradient updates, <span>∈</span> [0, 4.5M]), language modeling accuracy (top-1 accuracy at predicting a masked word) and the relative position of the representation (a.k.a “layer position”, between 0 for the word-embedding layer, and 1 for the last layer). The performance of the Random Forest was evaluated for each subject separately with a Pearson correlation <i>R</i> using five-split cross-validation across models.</p><p>“Permutation feature importance” summarizes how each of the covarying properties of the models (their task, architecture, etc.) specifically impacts the brain scores<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Breiman, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR48" id="ref-link-section-d184064578e2483">48</a></sup>. Permutation feature importance was implemented with scikit-learn<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e2487">49</a></sup> and is summarized with Δ<i>R</i>: the decrease in <i>R</i> when shuffling one feature (using 50 repetitions). For each subject, we reported the average decrease across the cross-validation splits (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a>). The resulting scores (Δ<i>R</i>) are expected to be centered around 0 if the corresponding feature does not impact the brain scores, and positive otherwise.</p><h3 id="Sec20">Statistics and reproducibility</h3><p>To estimate the robustness of our results, we systematically performed second-level analyses across subjects. Specifically, we applied Wilcoxon signed-rank tests across subjects’ estimates to evaluate whether the effect under consideration was systematically different from the chance level. The <i>p</i>-values of individual voxel/source/time samples were corrected for multiple comparisons, using a False Discovery Rate (Benjamini/Hochberg) as implemented in MNE-Python<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Gramfort, A. et al. Mne software for processing meg and eeg data. NeuroImage 86, 446–460 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR92" id="ref-link-section-d184064578e2515">92</a></sup> (we use the default parameters). Error bars and ± refer to the standard error of the mean (SEM) interval across subjects.</p><h3 id="Sec21">Brain parcellation</h3><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>, we focus on particular regions of interest using the Brodmann’s areas from the PALS parcellation of freesurfer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Fischl, B. Freesurfer. Neuroimage 62, 774–781 (2012)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR86" id="ref-link-section-d184064578e2531">86</a></sup>. The superior temporal gyrus (BA22) is split into its anterior, middle and posterior parts to increase granularity. For clarity, we rename certain areas as specified in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Tab1">1</a>.</p><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Table 1 Brain parcellation Taxonomy used to label the regions of interest in the brain following the PALS Brodmann’s Area atlas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title="Destrieux, C., Fischl, B., Dale, A. &amp; Halgren, E. Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature. Neuroimage 53, 1–15 (2010)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR88" id="ref-link-section-d184064578e2548">88</a></sup>.</b></figcaption></figure></div><h3 id="Sec22">Ethics</h3><p>These data were provided (in part) by the Donders Institute for Brain, Cognition, and Behavior after having been approved by the local ethics committee (CMO—the local “Committee on Research Involving Human Subjects” in the Arnhem-Nijmegen region). As stated in the original paper<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. Sci. Data 6, 1–13 (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR37" id="ref-link-section-d184064578e2720">37</a></sup>, “In the informed consent procedure, [the subjects] explicitly consented for the anonymized collected data to be used for research purposes by other researchers. [..] The study was approved by the local ethics committee (CMO—the local “Committee on Research Involving Human Subjects” in the Arnhem-Nijmegen region) and followed guidelines of the Helsinki declaration.”</p><h3 id="Sec23">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM9">Nature Research Reporting Summary</a> linked to this article.</p></div></div></section>
            

            <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><p>The data are publicly available on request. They were provided by the Donders Institute for Brain, Cognition and Behavior after having been approved by the local ethics committee (CMO—the local “Committee on Research Involving Human Subjects” in the Arnhem-Nijmegen region). Link: <a href="https://data.donders.ru.nl/collections/di/dccn/DSC_3011020.09_236">https://data.donders.ru.nl/collections/di/dccn/DSC_3011020.09_236</a>. The aggregated data used to generate Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig2">2</a> (Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM6">1</a>), Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig4">4</a> (Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM7">2</a>), and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a> (Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>) are available jointly with the manuscript. In particular, to generate Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>, one needs Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>a (scores across layers for MEG, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>a), Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>b (scores across training for MEG, Figure Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>b), Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>c (permutation importance for MEG, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>e), Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>d (scores across layers for fMRI, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>e), Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>e (scores across training for fMRI, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>f), and Supplementary Data <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s42003-022-03036-1#MOESM8">3</a>f (permutation importance for fMRI, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s42003-022-03036-1#Fig3">3</a>h).</p></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><p>The code is available upon request. Data analysis was performed in Python using the scikit-learn open source library<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR49" id="ref-link-section-d184064578e2877">49</a></sup>. The MEG and fMRI data were processed using MNE-Python<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Gramfort, A. et al. Mne software for processing meg and eeg data. NeuroImage 86, 446–460 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR92" id="ref-link-section-d184064578e2881">92</a></sup>, nilearn<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Abraham, A. et al. Machine learning for neuroimaging with scikit-learn. Front. Neuroinform. 8, 14 (2014)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR91" id="ref-link-section-d184064578e2885">91</a></sup> and freesurfer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Fischl, B. Freesurfer. Neuroimage 62, 774–781 (2012)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR86" id="ref-link-section-d184064578e2889">86</a></sup>. The natural language processing algorithms were trained using the implementation from the XLM github repository (<a href="https://github.com/facebookresearch/XLM">https://github.com/facebookresearch/XLM</a>,<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Lample, G. &amp; Conneau, A. Cross-lingual language model pretraining. In Adv. Neural Inf. Process. Syst. (2019)." href="https://www.nature.com/articles/s42003-022-03036-1#ref-CR6" id="ref-link-section-d184064578e2901">6</a></sup>).</p></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol><li data-counter="1."><p id="ref-CR1">Turing, A. M. <i>Parsing the Turing Test</i> 23–65 (Springer, 2009).</p></li><li data-counter="2."><p id="ref-CR2">Chomsky, N. <i>Language and Mind</i> (Cambridge University Press, 2006).</p></li><li data-counter="3."><p id="ref-CR3">Dehaene, S., Yann, L. &amp; Girardon, J. <i>La plus belle histoire de l’intelligence: des origines aux neurones artificiels: vers une nouvelle étape de l’évolution</i> (Robert Laffont, 2018).</p></li><li data-counter="4."><p id="ref-CR4">Vaswani, A. et al. Attention is all you need. In <i>Proceedings on NIPS</i> (Cornell University, 2017).</p></li><li data-counter="5."><p id="ref-CR5">Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, Volume 1 (Long and Short Papers) (2019).</p></li><li data-counter="6."><p id="ref-CR6">Lample, G. &amp; Conneau, A. Cross-lingual language model pretraining. In <i>Adv. Neural Inf. Process. Syst.</i> (2019).</p></li><li data-counter="7."><p id="ref-CR7">Brown, T. B. et al. Language models are few-shot learners. In <i>Advances in Neural Information Processing Systems</i> (2020).</p></li><li data-counter="8."><p id="ref-CR8">Lakretz, Y. et al. The emergence of number and syntax units in LSTM language models. In <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, Volume 1 (Long and Short Papers) (2019).</p></li><li data-counter="9."><p id="ref-CR9">Loula, J., Baroni, M. &amp; Lake, B. M. Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. In <i>BlackboxNLP@ EMNLP</i> (2018).</p></li><li data-counter="10."><p id="ref-CR10">Hale, J. T. et al. Neuro-computational models of language processing.</p></li><li data-counter="11."><p id="ref-CR11">Lake, B. M. &amp; Murphy, G. L. Word meaning in minds and machines. <i>Psychol. Rev.</i> (2021).</p></li><li data-counter="12."><p id="ref-CR12">Marcus, G. Deep learning: a critical appraisal. Preprint at <a href="https://arXiv.org/1801.00631">https://arXiv.org/1801.00631</a> (2018).</p></li><li data-counter="13."><p id="ref-CR13">Bengio, Y., Ducharme, R. &amp; Vincent, P. in <i>Advances in Neural Information Processing Systems</i> (eds. Leen, T. K. et al.) vol. 13, 932–938 (MIT Press, 2003).</p></li><li data-counter="14."><p id="ref-CR14">Mikolov, T., Chen, K., Corrado, G. &amp; Dean, J. Efficient estimation of word representations in vector space. Preprint at <a href="https://arxiv.org/1301.3781">https://arxiv.org/1301.3781</a> (2013).</p></li><li data-counter="15."><p id="ref-CR15">Pennington, J., Socher, R. &amp; Manning, C. D. Glove: global vectors for word representation. In <i>Empirical Methods in Natural Language Processing (EMNLP) Conference</i> 1532–1543 (2014).</p></li><li data-counter="16."><p id="ref-CR16">Bojanowski, P., Grave, E., Joulin, A. &amp; Mikolov, T. Enriching Word Vectors with Subword Information. In <i>Transactions of the Association for Computational Linguistics</i> (2016).</p></li><li data-counter="17."><p id="ref-CR17">Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. <i>Science</i> <b>320</b>, 1191–1195 (2008).</p></li><li data-counter="18."><p id="ref-CR18">Anderson, A. J. et al. Multiple regions of a cortical network commonly encode the meaning of words in multiple grammatical positions of read sentences. <i>Cereb. Cortex</i> <b>29</b>, 2396–2411 (2019).</p></li><li data-counter="19."><p id="ref-CR19">Sassenhagen, J. &amp; Fiebach, C. J. Traces of meaning itself: Encoding distributional word vectors in brain activity. <i>Neurobiology of Language</i> 1.1, 54–76 (2020).</p></li><li data-counter="20."><p id="ref-CR20">Caucheteux, C., Gramfort, A. &amp; King, J.-R. <i>GPT-2’s Activations Predict the Degree of Semantic Comprehension in the Human Brain</i> (Cold Spring Harbor Laboratory Section: New Results, 2021).</p></li><li data-counter="21."><p id="ref-CR21">Reddy Oota, S., Manwani, N. &amp; Raju S, B. fMRI semantic category decoding using linguistic encoding of word embeddings. In <i>International Conference on Neural Information Processing</i> (Springer, Cham, 2018).</p></li><li data-counter="22."><p id="ref-CR22">Abnar, S., Ahmed, R., Mijnheer, M. &amp; Zuidema, W. H. Experiential, distributional and dependency-based word embeddings have complementary roles in decoding brain activity. In P<i>roceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics</i> (CMCL 2018), (2018).</p></li><li data-counter="23."><p id="ref-CR23">Ruan, Y. -P., Ling, Z. -H. &amp; Hu, Y. Exploring semantic representation in brain activity using word embeddings. In <i>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</i>, 669–679 (Association for Computational Linguistics, 2016).</p></li><li data-counter="24."><p id="ref-CR24">Brodbeck, C., Hong, L. E. &amp; Simon, J. Z. Rapid transformation from auditory to linguistic representations of continuous speech. <i>Curr. Biol.</i> <b>28</b>, 3976–3983 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitlKmu7jF" aria-label="CAS reference 24">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cub.2018.10.042" aria-label="Article reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Rapid%20transformation%20from%20auditory%20to%20linguistic%20representations%20of%20continuous%20speech&amp;journal=Curr.%20Biol.&amp;volume=28&amp;pages=3976-3983&amp;publication_year=2018&amp;author=Brodbeck%2CC&amp;author=Hong%2CLE&amp;author=Simon%2CJZ">
                    Google Scholar</a> 
                </p></li><li data-counter="25."><p id="ref-CR25">Gauthier, J. &amp; Ivanova, A. Does the brain represent words? an evaluation of brain decoding studies of language understanding. Preprint at <a href="https://arXiv.org/1806.00591">https://arXiv.org/1806.00591</a> (2018).</p></li><li data-counter="26."><p id="ref-CR26">Wehbe, L., Vaswani, A., Knight, K. &amp; Mitchell, T. Aligning context-based statistical models of language with brain activity during reading. In <i>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> 233–243 (Association for Computational Linguistics, 2014).</p></li><li data-counter="27."><p id="ref-CR27">Schrimpf, M. et al. The neural architecture of language: Integrative modeling converges on predictive processing. In <i>Proceedings of the National Academy of Sciences</i> (2021).</p></li><li data-counter="28."><p id="ref-CR28">Caucheteux, C., Gramfort, A. &amp; King, J.-R. Disentangling syntax and semantics in the brain with deep networks.<i> ICML 2021-38th International Conference on Machine Learning</i> (2021).</p></li><li data-counter="29."><p id="ref-CR29">Caucheteux, C., Gramfort, A. &amp; King, J.-R. Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. In <i>EMNLP 2021—Conference on Empirical Methods in Natural Language Processing</i> (2021).</p></li><li data-counter="30."><p id="ref-CR30">Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of language in humans and machines. Preprint at bioRxiv (2020).</p></li><li data-counter="31."><p id="ref-CR31">Jain, S. &amp; Huth, A. in <i>Advances in Neural Information Processing Systems</i> (eds Bengio, S. et al.) vol. 31, 6628–6637 (Curran Associates, Inc., 2018).</p></li><li data-counter="32."><p id="ref-CR32">Athanasiou, N., Iosif, E. &amp; Potamianos, A. Neural activation semantic models: computational lexical semantic models of localized neural activations. In <i>Proceedings of the 27th International Conference on Computational Linguistics</i> 2867–2878 (Association for Computational Linguistics, 2018).</p></li><li data-counter="33."><p id="ref-CR33">Toneva, M. &amp; Wehbe, L. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). <i>Advances in Neural Information Processing Systems</i> 32 (2019).</p></li><li data-counter="34."><p id="ref-CR34">Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. &amp; de Lange, F. P. A hierarchy of linguistic predictions during natural language comprehension. bioRxiv <a href="https://doi.org/10.1101/2020.12.03.410399">https://doi.org/10.1101/2020.12.03.410399</a> (2020).</p></li><li data-counter="35."><p id="ref-CR35">Brennan, J. R. &amp; Pylkkänen, L. Meg evidence for incremental sentence composition in the anterior temporal lobe. <i>Cogn. Sci.</i> <b>41</b>, 1515–1531 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fcogs.12445" aria-label="Article reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Meg%20evidence%20for%20incremental%20sentence%20composition%20in%20the%20anterior%20temporal%20lobe&amp;journal=Cogn.%20Sci.&amp;volume=41&amp;pages=1515-1531&amp;publication_year=2017&amp;author=Brennan%2CJR&amp;author=Pylkk%C3%A4nen%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="36."><p id="ref-CR36">Hale, J., Dyer, C., Kuncoro, A. &amp; Brennan, J. R. Finding syntax in human encephalography with beam search. Preprint at <a href="https://arXiv.org/1806.04127">https://arXiv.org/1806.04127</a> (2018).</p></li><li data-counter="37."><p id="ref-CR37">Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to study language processing. <i>Sci. Data</i> <b>6</b>, 1–13 (2019).</p></li><li data-counter="38."><p id="ref-CR38">Fedorenko, E., Blank, I., Siegelman, M. &amp; Mineroff, Z. Lack of selectivity for syntax relative to word meanings throughout the language network. <i>Cognition</i> <b>203</b>, 104348 (2020).</p></li><li data-counter="39."><p id="ref-CR39">Dehaene, S. &amp; Cohen, L. The unique role of the visual word form area in reading. <i>Trends Cogn. Sci.</i> <b>15</b>, 254–262 (2011).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.tics.2011.04.003" aria-label="Article reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20unique%20role%20of%20the%20visual%20word%20form%20area%20in%20reading&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=15&amp;pages=254-262&amp;publication_year=2011&amp;author=Dehaene%2CS&amp;author=Cohen%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">Hagoort, P. The neurobiology of language beyond single-word processing. <i>Science</i> <b>366</b>, 55–58 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXhvFWlurzM" aria-label="CAS reference 40">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.aax0289" aria-label="Article reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neurobiology%20of%20language%20beyond%20single-word%20processing&amp;journal=Science&amp;volume=366&amp;pages=55-58&amp;publication_year=2019&amp;author=Hagoort%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">Hickok, G. &amp; Poeppel, D. <i>The Cortical Organization of Speech Processing</i> vol. 8, 393–402 (Nature Publishing Group, 2007).</p></li><li data-counter="42."><p id="ref-CR42">Yamins, D. L. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. <i>Proc. Natl Acad. Sci.</i> <b>111</b>, 8619–8624 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXnslWnsb4%3D" aria-label="CAS reference 42">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1403112111" aria-label="Article reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance-optimized%20hierarchical%20models%20predict%20neural%20responses%20in%20higher%20visual%20cortex&amp;journal=Proc.%20Natl%20Acad.%20Sci.&amp;volume=111&amp;pages=8619-8624&amp;publication_year=2014&amp;author=Yamins%2CDL">
                    Google Scholar</a> 
                </p></li><li data-counter="43."><p id="ref-CR43">Fedorenko, E. et al. Neural correlate of the construction of sentence meaning. <i>Proc. Natl Acad. Sci.</i> <b>113</b>, E6256–E6262 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhsFKjsLzM" aria-label="CAS reference 43">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1612132113" aria-label="Article reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20correlate%20of%20the%20construction%20of%20sentence%20meaning&amp;journal=Proc.%20Natl%20Acad.%20Sci.&amp;volume=113&amp;pages=E6256-E6262&amp;publication_year=2016&amp;author=Fedorenko%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="44."><p id="ref-CR44">Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. <i>Nature</i> <b>532</b>, 453–458 (2016).</p></li><li data-counter="45."><p id="ref-CR45">Seydell-Greenwald, A., Wang, X., Newport, E., Bi, Y. &amp; Striem-Amit, E. Spoken language comprehension activates the primary visual cortex. Preprint at bioRxiv (2020).</p></li><li data-counter="46."><p id="ref-CR46">Kell, A., Yamins, D., Shook, E., Norman-Haignere, S. &amp; McDermott, J. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. <i>Neuron</i> <b>98</b>, 630–644 (2018).</p></li><li data-counter="47."><p id="ref-CR47">Millet, J. &amp; King, J.-R. Inductive biases, pretraining and fine-tuning jointly account for brain responses to speech. Preprint at <a href="https://arXiv.org/2103.01032">https://arXiv.org/2103.01032</a> [cs, eess, q-bio] (2021).</p></li><li data-counter="48."><p id="ref-CR48">Breiman, L. Random forests. <i>Mach. Learn.</i> <b>45</b>, 5–32 (2001).</p></li><li data-counter="49."><p id="ref-CR49">Pedregosa, F. et al. Scikit-learn: machine learning in python. <i>J. Mach. Learn. Res.</i> <b>12</b>, 2825–2830 (2011).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Scikit-learn%3A%20machine%20learning%20in%20python&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=12&amp;pages=2825-2830&amp;publication_year=2011&amp;author=Pedregosa%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="50."><p id="ref-CR50">Tang, H. et al. Recurrent computations for visual pattern completion. <i>Proc. Natl Acad. Sci.</i> <b>115</b>, 8835–8840 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVCkur%2FJ" aria-label="CAS reference 50">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1719397115" aria-label="Article reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Recurrent%20computations%20for%20visual%20pattern%20completion&amp;journal=Proc.%20Natl%20Acad.%20Sci.&amp;volume=115&amp;pages=8835-8840&amp;publication_year=2018&amp;author=Tang%2CH">
                    Google Scholar</a> 
                </p></li><li data-counter="51."><p id="ref-CR51">Khaligh-Razavi, S.-M. &amp; Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain it cortical representation. <i>PLoS Comput. Biol.</i> <b>10</b>, e1003915 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1371%2Fjournal.pcbi.1003915" aria-label="Article reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20supervised%2C%20but%20not%20unsupervised%2C%20models%20may%20explain%20it%20cortical%20representation&amp;journal=PLoS%20Comput.%20Biol.&amp;volume=10&amp;publication_year=2014&amp;author=Khaligh-Razavi%2CS-M&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a> 
                </p></li><li data-counter="52."><p id="ref-CR52">Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. <i>Annu. Rev. Vis. Sci.</i> <b>1</b>, 417–446 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1146%2Fannurev-vision-082114-035447" aria-label="Article reference 52">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%3A%20a%20new%20framework%20for%20modeling%20biological%20vision%20and%20brain%20information%20processing&amp;journal=Annu.%20Rev.%20Vis.%20Sci.&amp;volume=1&amp;pages=417-446&amp;publication_year=2015&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a> 
                </p></li><li data-counter="53."><p id="ref-CR53">Güçlü, U. &amp; van Gerven, M. A. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. <i>J. Neurosci.</i> <b>35</b>, 10005–10014 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.5023-14.2015" aria-label="Article reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20reveal%20a%20gradient%20in%20the%20complexity%20of%20neural%20representations%20across%20the%20ventral%20stream&amp;journal=J.%20Neurosci.&amp;volume=35&amp;pages=10005-10014&amp;publication_year=2015&amp;author=G%C3%BC%C3%A7l%C3%BC%2CU&amp;author=Gerven%2CMA">
                    Google Scholar</a> 
                </p></li><li data-counter="54."><p id="ref-CR54">Eickenberg, M., Gramfort, A., Varoquaux, G. &amp; Thirion, B. Seeing it all: convolutional network layers map the function of the human visual system. <i>NeuroImage</i> <b>152</b>, 184–194 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2016.10.001" aria-label="Article reference 54">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Seeing%20it%20all%3A%20convolutional%20network%20layers%20map%20the%20function%20of%20the%20human%20visual%20system&amp;journal=NeuroImage&amp;volume=152&amp;pages=184-194&amp;publication_year=2017&amp;author=Eickenberg%2CM&amp;author=Gramfort%2CA&amp;author=Varoquaux%2CG&amp;author=Thirion%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="55."><p id="ref-CR55">Yamins, D. L. &amp; DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. <i>Nat. Neurosci.</i> <b>19</b>, 356 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XjtVOjt7k%3D" aria-label="CAS reference 55">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.4244" aria-label="Article reference 55">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20goal-driven%20deep%20learning%20models%20to%20understand%20sensory%20cortex&amp;journal=Nat.%20Neurosci.&amp;volume=19&amp;publication_year=2016&amp;author=Yamins%2CDL&amp;author=DiCarlo%2CJJ">
                    Google Scholar</a> 
                </p></li><li data-counter="56."><p id="ref-CR56">Saxe, A., Nelli, S. &amp; Summerfield, C. If deep learning is the answer, what is the question? <i>Nat. Rev. Neurosci.</i> <b>22</b>, 1–13 (2020).</p></li><li data-counter="57."><p id="ref-CR57">Minsky, M. &amp; Papert, S. <i>Perceptrons: An Introduction to Computational Geometry</i>. (MIT Press, 1969).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;publication_year=1969&amp;author=Minsky%2CM&amp;author=Papert%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="58."><p id="ref-CR58">Cadieu, C. F. et al. Deep neural networks rival the representation of primate it cortex for core visual object recognition. <i>PLoS Comput. Biol.</i> <b>10</b>, e1003963 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1371%2Fjournal.pcbi.1003963" aria-label="Article reference 58">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20rival%20the%20representation%20of%20primate%20it%20cortex%20for%20core%20visual%20object%20recognition&amp;journal=PLoS%20Comput.%20Biol.&amp;volume=10&amp;publication_year=2014&amp;author=Cadieu%2CCF">
                    Google Scholar</a> 
                </p></li><li data-counter="59."><p id="ref-CR59">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. A. Representational similarity analysis—connecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p></li><li data-counter="60."><p id="ref-CR60">King, J.-R. &amp; Dehaene, S. Characterizing the dynamics of mental representations: the temporal generalization method. <i>Trends Cogn. Sci.</i> <b>18</b>, 203–210 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.tics.2014.01.002" aria-label="Article reference 60">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterizing%20the%20dynamics%20of%20mental%20representations%3A%20the%20temporal%20generalization%20method&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=18&amp;pages=203-210&amp;publication_year=2014&amp;author=King%2CJ-R&amp;author=Dehaene%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="61."><p id="ref-CR61">Cohen, U., Chung, S., Lee, D. D. &amp; Sompolinsky, H. Separability and geometry of object manifolds in deep neural networks. <i>Nat. Commun.</i> <b>11</b>, 1–13 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Separability%20and%20geometry%20of%20object%20manifolds%20in%20deep%20neural%20networks&amp;journal=Nat.%20Commun.&amp;volume=11&amp;pages=1-13&amp;publication_year=2020&amp;author=Cohen%2CU&amp;author=Chung%2CS&amp;author=Lee%2CDD&amp;author=Sompolinsky%2CH">
                    Google Scholar</a> 
                </p></li><li data-counter="62."><p id="ref-CR62">Hermes, D. et al. Electrophysiological responses in the ventral temporal cortex during reading of numerals and calculation. <i>Cereb. Cortex</i> <b>27</b>, 567–575 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26503267" aria-label="PubMed reference 62" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Electrophysiological%20responses%20in%20the%20ventral%20temporal%20cortex%20during%20reading%20of%20numerals%20and%20calculation&amp;journal=Cereb.%20Cortex&amp;volume=27&amp;pages=567-575&amp;publication_year=2017&amp;author=Hermes%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="63."><p id="ref-CR63">Woolnough, O. et al. Spatiotemporal dynamics of orthographic and lexical processing in the ventral visual pathway. <i>Nat. Hum. Behav.</i> <b>5</b>, 389–398 (2021).</p></li><li data-counter="64."><p id="ref-CR64">Price, C. J. The anatomy of language: a review of 100 fmri studies published in 2009. <i>Ann. N. Y. Acad. Sci.</i> <b>1191</b>, 62–88 (2010).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1749-6632.2010.05444.x" aria-label="Article reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20anatomy%20of%20language%3A%20a%20review%20of%20100%20fmri%20studies%20published%20in%202009&amp;journal=Ann.%20N.%20Y.%20Acad.%20Sci.&amp;volume=1191&amp;pages=62-88&amp;publication_year=2010&amp;author=Price%2CCJ">
                    Google Scholar</a> 
                </p></li><li data-counter="65."><p id="ref-CR65">Pallier, C., Devauchelle, A.-D. &amp; Dehaene, S. Cortical representation of the constituent structure of sentences. <i>Proc. Natl Acad. Sci.</i> <b>108</b>, 2522–2527 (2011).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXitFWns7o%3D" aria-label="CAS reference 65">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1018711108" aria-label="Article reference 65">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20representation%20of%20the%20constituent%20structure%20of%20sentences&amp;journal=Proc.%20Natl%20Acad.%20Sci.&amp;volume=108&amp;pages=2522-2527&amp;publication_year=2011&amp;author=Pallier%2CC&amp;author=Devauchelle%2CA-D&amp;author=Dehaene%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="66."><p id="ref-CR66">Fedorenko, E., Hsieh, P.-J., Nieto-Castañón, A., Whitfield-Gabrieli, S. &amp; Kanwisher, N. New method for fmri investigations of language: defining rois functionally in individual subjects. <i>J. Neurophysiol.</i> <b>104</b>, 1177–1194 (2010).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1152%2Fjn.00032.2010" aria-label="Article reference 66">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=New%20method%20for%20fmri%20investigations%20of%20language%3A%20defining%20rois%20functionally%20in%20individual%20subjects&amp;journal=J.%20Neurophysiol.&amp;volume=104&amp;pages=1177-1194&amp;publication_year=2010&amp;author=Fedorenko%2CE&amp;author=Hsieh%2CP-J&amp;author=Nieto-Casta%C3%B1%C3%B3n%2CA&amp;author=Whitfield-Gabrieli%2CS&amp;author=Kanwisher%2CN">
                    Google Scholar</a> 
                </p></li><li data-counter="67."><p id="ref-CR67">Cogan, G. B. et al. Sensory–motor transformations for speech occur bilaterally. <i>Nature</i> <b>507</b>, 94–98 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXjs1Ght7o%3D" aria-label="CAS reference 67">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature12935" aria-label="Article reference 67">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%E2%80%93motor%20transformations%20for%20speech%20occur%20bilaterally&amp;journal=Nature&amp;volume=507&amp;pages=94-98&amp;publication_year=2014&amp;author=Cogan%2CGB">
                    Google Scholar</a> 
                </p></li><li data-counter="68."><p id="ref-CR68">Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. &amp; Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. <i>Proc. Natl Acad. Sci.</i> <b>117</b>, 30046–30054 (2020).</p></li><li data-counter="69."><p id="ref-CR69">Reddy, A. J. &amp; Wehbe, L. Syntactic representations in the human brain: beyond effort-based metrics. Preprint at bioRXiv (2021).</p></li><li data-counter="70."><p id="ref-CR70">Schrimpf, M. et al. Brain-score: which artificial neural network for object recognition is most brain-like? Preprint at bioRXiv (2018).</p></li><li data-counter="71."><p id="ref-CR71">Kell, A. J. E., Yamins, D. L. K., Shook, E. N., Norman-Haignere, S. V. &amp; McDermott, J. H. A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. <i>Neuron</i> <b>98</b>, 630–644 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2018.03.044" aria-label="Article reference 71">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20task-optimized%20neural%20network%20replicates%20human%20auditory%20behavior%2C%20predicts%20brain%20responses%2C%20and%20reveals%20a%20cortical%20processing%20hierarchy&amp;journal=Neuron&amp;volume=98&amp;publication_year=2018&amp;author=Kell%2CAJE&amp;author=Yamins%2CDLK&amp;author=Shook%2CEN&amp;author=Norman-Haignere%2CSV&amp;author=McDermott%2CJH">
                    Google Scholar</a> 
                </p></li><li data-counter="72."><p id="ref-CR72">Nastase, S. A. et al. Narratives: fmri data for evaluating models of naturalistic language comprehension. <i>Trends in neurosciences</i> <b>43</b>, 271–273 (2020).</p></li><li data-counter="73."><p id="ref-CR73">Keller, G. B. &amp; Mrsic-Flogel, T. D. Predictive processing: a canonical cortical computation. <i>Neuron</i> <b>100</b>, 424–435 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVSisrnF" aria-label="CAS reference 73">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2018.10.003" aria-label="Article reference 73">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Predictive%20processing%3A%20a%20canonical%20cortical%20computation&amp;journal=Neuron&amp;volume=100&amp;pages=424-435&amp;publication_year=2018&amp;author=Keller%2CGB&amp;author=Mrsic-Flogel%2CTD">
                    Google Scholar</a> 
                </p></li><li data-counter="74."><p id="ref-CR74">Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. &amp; de Lange, F. P. A hierarchy of linguistic predictions during natural language comprehension. Preprint at bioRXiv (2020).</p></li><li data-counter="75."><p id="ref-CR75">Wang, L. Dynamic predictive coding across the left fronto-temporal language hierarchy: evidence from MEG, EEG and fMRI29.</p></li><li data-counter="76."><p id="ref-CR76">Lee, C. S., Aly, M. &amp; Baldassano, C. Anticipation of temporally structured events in the brain. <i>eLife</i> <b>10</b>, e64972 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXitlOgs7bP" aria-label="CAS reference 76">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.7554%2FeLife.64972" aria-label="Article reference 76">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 76" href="http://scholar.google.com/scholar_lookup?&amp;title=Anticipation%20of%20temporally%20structured%20events%20in%20the%20brain&amp;journal=eLife&amp;volume=10&amp;publication_year=2021&amp;author=Lee%2CCS&amp;author=Aly%2CM&amp;author=Baldassano%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="77."><p id="ref-CR77">Friston, K. The free-energy principle: a unified brain theory? <i>Nat. Rev. Neurosci.</i> <b>11</b>, 127–138 (2010).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXksFGktw%3D%3D" aria-label="CAS reference 77">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnrn2787" aria-label="Article reference 77">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 77" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20free-energy%20principle%3A%20a%20unified%20brain%20theory%3F&amp;journal=Nat.%20Rev.%20Neurosci.&amp;volume=11&amp;pages=127-138&amp;publication_year=2010&amp;author=Friston%2CK">
                    Google Scholar</a> 
                </p></li><li data-counter="78."><p id="ref-CR78">Ramsauer, H. et al. Hopfield networks is all you need. Preprint at <a href="https://arXiv.org/2008.02217">https://arXiv.org/2008.02217</a> [cs, stat] (2021).</p></li><li data-counter="79."><p id="ref-CR79">Lake, B. M., Ullman, T. D., Tenenbaum, J. B. &amp; Gershman, S. J. Building machines that learn and think like people. <i>Behavioral and brain sciences</i> 40 (2017).</p></li><li data-counter="80."><p id="ref-CR80">Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. &amp; Choi, Y. Hellaswag: can a machine really finish your sentence? <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i> (2019).</p></li><li data-counter="81."><p id="ref-CR81">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In <i>Proceedings of NAACL-HLT</i> (2019).</p></li><li data-counter="82."><p id="ref-CR82">Radford, A. et al. Language models are unsupervised multitask learners. <i>OpenAI Blog</i> <b>1</b>, 9 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 82" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20models%20are%20unsupervised%20multitask%20learners&amp;journal=OpenAI%20Blog&amp;volume=1&amp;publication_year=2019&amp;author=Radford%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="83."><p id="ref-CR83">Attardi, G. Wikiextractor. <a href="https://github.com/attardi/wikiextractor">https://github.com/attardi/wikiextractor</a> (2015).</p></li><li data-counter="84."><p id="ref-CR84">Koehn, P. et al. Moses: ppen source toolkit for statistical machine translation. In <i>Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</i> 177–180 (Association for Computational Linguistics, 2007).</p></li><li data-counter="85."><p id="ref-CR85">Baek, J. et al. What is wrong with scene text recognition model comparisons? dataset and model analysis. In <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 4715–4723 <a href="https://github.com/clovaai/deep-text-recognition-benchmark">https://github.com/clovaai/deep-text-recognition-benchmark</a> (2019).</p></li><li data-counter="86."><p id="ref-CR86">Fischl, B. Freesurfer. <i>Neuroimage</i> <b>62</b>, 774–781 (2012).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2012.01.021" aria-label="Article reference 86">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 86" href="http://scholar.google.com/scholar_lookup?&amp;title=Freesurfer&amp;journal=Neuroimage&amp;volume=62&amp;pages=774-781&amp;publication_year=2012&amp;author=Fischl%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="87."><p id="ref-CR87">Van Essen, D. C. A population-average, landmark-and surface-based (pals) atlas of human cerebral cortex. <i>Neuroimage</i> <b>28</b>, 635–662 (2005).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2005.06.058" aria-label="Article reference 87">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 87" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20population-average%2C%20landmark-and%20surface-based%20%28pals%29%20atlas%20of%20human%20cerebral%20cortex&amp;journal=Neuroimage&amp;volume=28&amp;pages=635-662&amp;publication_year=2005&amp;author=Essen%2CDC">
                    Google Scholar</a> 
                </p></li><li data-counter="88."><p id="ref-CR88">Destrieux, C., Fischl, B., Dale, A. &amp; Halgren, E. Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature. <i>Neuroimage</i> <b>53</b>, 1–15 (2010).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2010.06.010" aria-label="Article reference 88">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 88" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20parcellation%20of%20human%20cortical%20gyri%20and%20sulci%20using%20standard%20anatomical%20nomenclature&amp;journal=Neuroimage&amp;volume=53&amp;pages=1-15&amp;publication_year=2010&amp;author=Destrieux%2CC&amp;author=Fischl%2CB&amp;author=Dale%2CA&amp;author=Halgren%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="89."><p id="ref-CR89">Esteban, O. et al. fmriprep: a robust preprocessing pipeline for functional mri. <i>Nat. Methods</i> <b>16</b>, 111–116 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXisVyhurnN" aria-label="CAS reference 89">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fs41592-018-0235-4" aria-label="Article reference 89">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 89" href="http://scholar.google.com/scholar_lookup?&amp;title=fmriprep%3A%20a%20robust%20preprocessing%20pipeline%20for%20functional%20mri&amp;journal=Nat.%20Methods&amp;volume=16&amp;pages=111-116&amp;publication_year=2019&amp;author=Esteban%2CO">
                    Google Scholar</a> 
                </p></li><li data-counter="90."><p id="ref-CR90">Behzadi, Y., Restom, K., Liau, J. &amp; Liu, T. T. A component based noise correction method (compcor) for bold and perfusion based fmri. <i>Neuroimage</i> <b>37</b>, 90–101 (2007).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2007.04.042" aria-label="Article reference 90">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 90" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20component%20based%20noise%20correction%20method%20%28compcor%29%20for%20bold%20and%20perfusion%20based%20fmri&amp;journal=Neuroimage&amp;volume=37&amp;pages=90-101&amp;publication_year=2007&amp;author=Behzadi%2CY&amp;author=Restom%2CK&amp;author=Liau%2CJ&amp;author=Liu%2CTT">
                    Google Scholar</a> 
                </p></li><li data-counter="91."><p id="ref-CR91">Abraham, A. et al. Machine learning for neuroimaging with scikit-learn. <i>Front. Neuroinform.</i> <b>8</b>, 14 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3389%2Ffninf.2014.00014" aria-label="Article reference 91">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 91" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%20for%20neuroimaging%20with%20scikit-learn&amp;journal=Front.%20Neuroinform.&amp;volume=8&amp;publication_year=2014&amp;author=Abraham%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="92."><p id="ref-CR92">Gramfort, A. et al. Mne software for processing meg and eeg data. <i>NeuroImage</i> <b>86</b>, 446–460 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuroimage.2013.10.027" aria-label="Article reference 92">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 92" href="http://scholar.google.com/scholar_lookup?&amp;title=Mne%20software%20for%20processing%20meg%20and%20eeg%20data&amp;journal=NeuroImage&amp;volume=86&amp;pages=446-460&amp;publication_year=2014&amp;author=Gramfort%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="93."><p id="ref-CR93">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. Distributed Representations of Words and Phrases and their Compositionality. <i>Advances in Neural Information Processing Systems</i> 3111–3119 (MIT Press, 2013).</p></li><li data-counter="94."><p id="ref-CR94">Bingham, E. &amp; Mannila, H. Random projection in dimensionality reduction: applications to image and text data. In <i>Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> 245–250 (ACM, 2001).</p></li><li data-counter="95."><p id="ref-CR95">Frankle, J. &amp; Carbin, M. The lottery ticket hypothesis: finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1038/s42003-022-03036-1?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>This work was supported by ANR-17-EURE-0017, the Fyssen Foundation, and the Bettencourt and Fyssen Foundations to J.R.K. for his work at PSL.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Affiliations</h3><ol><li id="Aff1"><p>Facebook AI Research, Paris, France</p><p>Charlotte Caucheteux &amp; Jean-Rémi King</p></li><li id="Aff2"><p>Université Paris-Saclay, Inria, CEA, Palaiseau, France</p><p>Charlotte Caucheteux</p></li><li id="Aff3"><p>École normale supérieure, PSL University, CNRS, Paris, France</p><p>Jean-Rémi King</p></li></ol><h3 id="contributions">Contributions</h3><p>J.R.K. defined the line of research, C.C. conducted the analyses, both authors analyzed the results, designed the figures and wrote the paper.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:ccaucheteux@fb.com">Charlotte Caucheteux</a> or <a id="corresp-c2" href="mailto:jeanremi@fb.com">Jean-Rémi King</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Peer review"><div id="peer-review-section"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Communications Biology</i> thanks Blake Richards, Anna Ivanova and the other, anonymous, reviewer for their contribution to the peer review of this work. Primary Handling Editors: Enzo Tagliazucchi and George Inglis. Peer reviewer reports are available.</p>
              
            </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Supplementary information"><div id="Sec24-section"><h2 id="Sec24">Supplementary information</h2></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Brains%20and%20algorithms%20partially%20converge%20in%20natural%20language%20processing&amp;author=Charlotte%20Caucheteux%20et%20al&amp;contentID=10.1038%2Fs42003-022-03036-1&amp;copyright=The%20Author%28s%29&amp;publication=2399-3642&amp;publicationDate=2022-02-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s42003-022-03036-1" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s42003-022-03036-1" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Caucheteux, C., King, JR. Brains and algorithms partially converge in natural language processing.
                    <i>Commun Biol</i> <b>5, </b>134 (2022). https://doi.org/10.1038/s42003-022-03036-1</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="https://citation-needed.springer.com/v2/references/10.1038/s42003-022-03036-1?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2021-08-12">12 August 2021</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2021-12-29">29 December 2021</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2022-02-16">16 February 2022</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s42003-022-03036-1</span></p></li></ul></div></div></div></div></section>

            

            
                <section data-title="Comments"><div id="article-comments-section"><h2 id="article-comments">Comments</h2><p>By submitting a comment you agree to abide by our <a href="https://www.nature.com/info/tandc.html">Terms</a> and <a href="https://www.nature.com/info/community-guidelines.html">Community Guidelines</a>. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.</p></div></section>
                
            

            </div></div>
  </body>
</html>
