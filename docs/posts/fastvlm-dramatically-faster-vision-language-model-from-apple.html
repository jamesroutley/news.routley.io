<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/ml-fastvlm">Original</a>
    <h1>FastVLM: Dramatically Faster Vision Language Model from Apple</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This is the official repository of
<strong><a href="https://www.arxiv.org/abs/2412.13303" rel="nofollow">FastVLM: Efficient Vision Encoding for Vision Language Models</a>. (CVPR 2025)</strong></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-fastvlm/blob/main/docs/acc_vs_latency_qwen-2.png"><img src="https://github.com/apple/ml-fastvlm/raw/main/docs/acc_vs_latency_qwen-2.png" alt="Accuracy vs latency figure." width="400"/></a>
</p>

<ul dir="auto">
<li>We introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.</li>
<li>Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.</li>
<li>Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.</li>
<li>Demo iOS app to demonstrate the performance of our model on a mobile device.</li>
</ul>
<markdown-accessiblity-table></markdown-accessiblity-table>

<p dir="auto">We use LLaVA codebase to train FastVLM variants. In order to train or finetune your own variants,
please follow instructions provided in <a href="https://github.com/haotian-liu/LLaVA">LLaVA</a> codebase.
We provide instructions for running inference with our models.</p>

<div dir="auto" data-snippet-clipboard-copy-content="conda create -n fastvlm python=3.10
conda activate fastvlm
pip install -e ."><pre>conda create -n fastvlm python=3.10
conda activate fastvlm
pip install -e <span>.</span></pre></div>

<p dir="auto">For detailed information on various evaluations, please refer to our <a href="https://www.arxiv.org/abs/2412.13303" rel="nofollow">paper</a>.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Stage</th>
<th>Pytorch Checkpoint (url)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FastVLM-0.5B</td>
<td>2</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage2.zip" rel="nofollow">fastvlm_0.5b_stage2</a></td>
</tr>
<tr>
<td></td>
<td>3</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3.zip" rel="nofollow">fastvlm_0.5b_stage3</a></td>
</tr>
<tr>
<td>FastVLM-1.5B</td>
<td>2</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage2.zip" rel="nofollow">fastvlm_1.5b_stage2</a></td>
</tr>
<tr>
<td></td>
<td>3</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3.zip" rel="nofollow">fastvlm_1.5b_stage3</a></td>
</tr>
<tr>
<td>FastVLM-7B</td>
<td>2</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage2.zip" rel="nofollow">fastvlm_7b_stage2</a></td>
</tr>
<tr>
<td></td>
<td>3</td>
<td><a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3.zip" rel="nofollow">fastvlm_7b_stage3</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">To download all the pretrained checkpoints run the command below (note that this might take some time depending on your connection so might be good to grab ☕️ while you wait).</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash get_models.sh   # Files will be downloaded to `checkpoints` directory."><pre>bash get_models.sh   <span><span>#</span> Files will be downloaded to `checkpoints` directory.</span></pre></div>

<p dir="auto">To run inference of PyTorch checkpoint, follow the instruction below</p>
<div dir="auto" data-snippet-clipboard-copy-content="python predict.py --model-path /path/to/checkpoint-dir \
                  --image-file /path/to/image.png \
                  --prompt &#34;Describe the image.&#34;"><pre>python predict.py --model-path /path/to/checkpoint-dir \
                  --image-file /path/to/image.png \
                  --prompt <span><span>&#34;</span>Describe the image.<span>&#34;</span></span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Inference on Apple Silicon</h3><a id="user-content-inference-on-apple-silicon" aria-label="Permalink: Inference on Apple Silicon" href="#inference-on-apple-silicon"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To run inference on Apple Silicon, pytorch checkpoints have to be exported to format
suitable for running on Apple Silicon, detailed instructions and code can be found <a href="https://github.com/apple/ml-fastvlm/blob/main/model_export"><code>model_export</code></a> subfolder.
Please see the README there for more details.</p>
<p dir="auto">For convenience, we provide 3 models that are in Apple Silicon compatible format: <a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3_llm.fp16.zip" rel="nofollow">fastvlm_0.5b_stage3</a>,
<a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3_llm.int8.zip" rel="nofollow">fastvlm_1.5b_stage3</a>,
<a href="https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3_llm.int4.zip" rel="nofollow">fastvlm_7b_stage3</a>.
We encourage developers to export the model of their choice with the appropriate quantization levels following
the instructions in <a href="https://github.com/apple/ml-fastvlm/blob/main/model_export"><code>model_export</code></a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Inference on Apple Devices</h3><a id="user-content-inference-on-apple-devices" aria-label="Permalink: Inference on Apple Devices" href="#inference-on-apple-devices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To run inference on Apple devices like iPhone, iPad or Mac, see <a href="https://github.com/apple/ml-fastvlm/blob/main/app"><code>app</code></a> subfolder for more details.</p>

<p dir="auto">If you found this code useful, please cite the following paper:</p>
<div data-snippet-clipboard-copy-content="@InProceedings{fastvlm2025,
  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},
  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2025},
}"><pre><code>@InProceedings{fastvlm2025,
  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},
  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2025},
}
</code></pre></div>

<p dir="auto">Our codebase is built using multiple opensource contributions, please see <a href="https://github.com/apple/ml-fastvlm/blob/main/ACKNOWLEDGEMENTS">ACKNOWLEDGEMENTS</a> for more details.</p>

<p dir="auto">Please check out the repository <a href="https://github.com/apple/ml-fastvlm/blob/main/LICENSE">LICENSE</a> before using the provided code and
<a href="https://github.com/apple/ml-fastvlm/blob/main/LICENSE_MODEL">LICENSE_MODEL</a> for the released models.</p>
</article></div></div>
  </body>
</html>
