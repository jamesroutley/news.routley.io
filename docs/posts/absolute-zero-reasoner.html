<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://andrewzh112.github.io/absolute-zero-reasoner/">Original</a>
    <h1>Absolute Zero Reasoner</h1>
    
    <div id="readability-page-1" class="page"><div>
        <section id="explorer-section">
            <h2>AZR Proposed Programs</h2>
            <p>All AZR proposed code samples are first embedded with <code>jina-embeddings-v2-base-code</code> then projected to 2D using UMAP.</p>
            <div>
                
                <p><i></i> 
                    <span><i></i> Hover over dots to explore programs. Click to lock selection.</span>
                </p>
                <div>
                    <svg id="embedding-viz"></svg>
                    <div id="details-panel">
                        
                        
                        
                        <div>
                            <p>Task Information</p>
                            
                            <p>Training Step</p>
                            
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section id="animation-gallery-section">
            <h2>Spinning Hexagon Vibe Check</h2>
            <p><strong>Prompt:</strong> Write a script that shows 10 balls bouncing inside a spinning hexagon. The balls should be affected by gravity and friction, and must bounce off the rotating walls realistically</p>
            
            <div>
                <div>
                    <h3>AZR-Coder-14b (Ours)</h3>
                    <p><img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/vibes/azr_coder_14b.gif" alt="AZR-Coder-14b" data-speed="3"/>
                    </p>
                    
                    
                </div>
                
                <div>
                    <h3>GPT-4o-mini</h3>
                    <p><img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/vibes/gpt4o_mini.gif" alt="GPT-4o-mini" data-speed="3"/>
                    </p>
                    
                    
                </div>
                
                <div>
                    <h3>Qwen2.5-72B-Instruct</h3>
                    <p><img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/vibes/qwen_72b_instruct.gif" alt="Qwen2.5-72B-Instruct" data-speed="3"/>
                    </p>
                    
                    
                </div>
                
                <div>
                    <h3>Qwen2.5-32B-Instruct</h3>
                    <p><img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/vibes/qwen_32b_instruct.gif" alt="Qwen2.5-32B-Instruct" data-speed="3"/>
                    </p>
                    
                    
                </div>
                
                <div>
                    <h3>Qwen2.5-14B-Instruct</h3>
                    <p><img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/vibes/qwen_14b_instruct.gif" alt="Qwen2.5-14B-Instruct" data-speed="3"/>
                    </p>
                    
                    
                </div>
            </div>
        </section>
    <div>
        <section id="absolute-zero-paradigm">
            <h2>1. Absolute Zero Paradigm</h2>
            
            <div>
                <p>Traditional approaches to training reasoning models rely heavily on human-curated data:</p>
                
                <ul>
                    <li><span>Supervised Fine-Tuning (SFT)</span> requires datasets with human-written queries, rationales, and answers.</li>
                    <li><span>Reinforcement Learning with Verifiable Rewards (RLVR)</span> still needs human-labeled tasks and answers, even if the model generates its own reasoning.</li>
                </ul>
                
                <p>The <span>Absolute Zero</span> paradigm eliminates this dependency on human data. The model simultaneously proposes tasks, solves them, and learns from both stages through self-play. As shown in <a href="#paradigm-figure">Figure 1</a>, the agent autonomously creates tasks optimized for learnability and learns to solve them using a unified model.</p>
                
                <p>The agent π acts in two roles: as a proposer π<sup>propose</sup> that generates tasks τ, and as a solver π<sup>solve</sup> that produces answers y. The environment e validates proposed tasks into (x, y★) pairs and provides both learnability rewards r<sup>propose</sup> and solution rewards r<sup>solve</sup>. This enables continuous self-improvement without any human-curated data.</p>
            </div>
            
            <figure id="paradigm-figure">
                <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/az.png" alt="Absolute Zero Paradigm"/>
                <figcaption>
                    <span>Absolute Zero Paradigm.</span> <span><span>Supervised learning</span></span> relies on human-curated reasoning traces for behavior cloning. 
                    <span><span>Reinforcement learning from verified rewards</span></span> enables agents to self-learn reasoning, but still depends on expert-defined 
                    learning distribution and a respective set of curated QA pairs, demanding domain expertise and manual effort. 
                    In contrast, we introduce a new paradigm, <span>Absolute Zero</span>, for training reasoning models without any human-curated data. 
                    We envision that the agent should autonomously propose tasks optimized for learnability and learn how to solve them 
                    using a unified model. The agent learns by interacting with an environment that provides verifiable feedback, enabling 
                    reliable and continuous self-improvement entirely without human intervention.
                </figcaption>
            </figure>
        </section>

        <section id="absolute-zero-reasoner">
            <h2>2. Absolute Zero Reasoner</h2>
            
            <div>
                <p>The Absolute Zero Reasoner (AZR) is our first implementation of the Absolute Zero Paradigm. AZR uses a unified language model that serves dual roles while learning through code-based reasoning challenges. The model works through a continuous self-improvement loop without requiring any human-curated data.</p>
                
                <div id="propose-solve">
                    <h3>2.1. Propose and Solve Roles</h3>
                    <p>The Absolute Zero Reasoner employs a unified model that acts in two complementary roles:</p>
                    
                    <ul>
                        <li><strong>Proposer Role:</strong> Generates tasks with high learning potential - neither too easy nor impossible for the current solver. The model is rewarded for creating challenges that provide meaningful learning opportunities.</li>
                        
                        <li><strong>Solver Role:</strong> Attempts to solve the proposed problems, receiving binary rewards based on the correctness of solutions, verified through Python execution.</li>
                    </ul>
                    
                    <p>For the proposer, we design a specialized reward function based on Monte Carlo rollouts that encourages the generation of tasks with optimal difficulty - problems where the solver sometimes succeeds and sometimes fails. This creates the richest learning signal for continuous improvement.</p>
                </div>
                
                <div id="reasoning-modes">
                    <h3>2.2. Reasoning Modes</h3>
                    <p>As shown in <a href="#algorithm-figure">Figure 3</a>, the Absolute Zero Reasoner operates across three fundamental reasoning modes, each focusing on different aspects of a triplet (program, input, output):</p>
                    
                    <ul>
                        <li><strong>Deduction:</strong> Predicting the output given a program and input, capturing step-by-step logical reasoning. This tests the model&#39;s ability to trace program execution.</li>
                        
                        <li><strong>Abduction:</strong> Inferring a plausible input given a program and its output, resembling trial-and-error or search processes. This tests the model&#39;s ability to work backward from results.</li>
                        
                        <li><strong>Induction:</strong> Synthesizing a program from input-output examples, requiring generalization from partial information. This tests the model&#39;s ability to discover underlying patterns.</li>
                    </ul>
                    
                    <p>The model begins with minimal seed examples (as simple as an identity function) and bootstraps its way to increasingly complex reasoning capabilities through continual self-play and verification.</p>
                </div>
            </div>
            
            <figure id="algorithm-figure">
                <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/azr.png" alt="Absolute Zero Reasoner Training"/>
                <figcaption>
                    <span>Absolute Zero Reasoner Training Overview.</span> At every iteration, Absolute Zero Reasoner first <span><span>PROPOSES</span></span> 
                    a batch of tasks, conditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, 
                    or induction. From these generated tasks, Python is used to filter and construct valid code-based reasoning questions. 
                    A learnability reward \(r_\text{propose}\) is also calculated for each proposed task. The Absolute Zero Reasoner then <span><span>SOLVES</span></span> 
                    the batch of reasoning questions. Python is used again to verify the generated responses and compute the accuracy reward \(r_\text{solve}\). 
                    Finally, the Absolute Zero Reasoner is jointly updated using both \(r_\text{propose}\) and \(r_\text{solve}\) across all three task types, using TRR++.
                </figcaption>
            </figure>
            <div id="algorithm">
                <h3>2.3. Absolute Zero Reasoner Algorithm</h3>
                <figure id="algorithm-detail-figure">
                    <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/algorithm.png" alt="Absolute Zero Reasoner Algorithm"/>
                    <!-- Optionally add a caption here later if needed -->
                    <!-- <figcaption class="latex-caption">...</figcaption> -->
                </figure>
            </div>
        </section>

        <section id="results">
            <h2>3. Results</h2>
            
            <div>
                <h3>3.1. Main Results</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Base</th>
                            <th>#data</th>
                            <th>HEval<sup>+</sup></th>
                            <th>MBPP<sup>+</sup></th>
                            <th>LCBv5</th>
                            <th>AME24</th>
                            <th>AME25</th>
                            <th>AMC</th>
                            <th>M500</th>
                            <th>Minva</th>
                            <th>Olypiad</th>
                            <th>CAvg</th>
                            <th>MAvg</th>
                            <th>AVG</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="15">Base Models</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B</td>
                            <td>-</td>
                            <td>-</td>
                            <td>73.2</td>
                            <td>65.3</td>
                            <td>17.5</td>
                            <td>6.7</td>
                            <td>3.3</td>
                            <td>37.5</td>
                            <td>64.8</td>
                            <td>25.0</td>
                            <td>27.7</td>
                            <td>52.0</td>
                            <td>27.5</td>
                            <td>39.8</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B-Ins</td>
                            <td>-</td>
                            <td>-</td>
                            <td>75.0</td>
                            <td>68.5</td>
                            <td>25.5</td>
                            <td>13.3</td>
                            <td>6.7</td>
                            <td>52.5</td>
                            <td>76.4</td>
                            <td>35.7</td>
                            <td>37.6</td>
                            <td>56.3</td>
                            <td>37.0</td>
                            <td>46.7</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B-Coder</td>
                            <td>-</td>
                            <td>-</td>
                            <td>80.5</td>
                            <td>69.3</td>
                            <td>19.9</td>
                            <td>6.7</td>
                            <td>3.3</td>
                            <td>40.0</td>
                            <td>54.0</td>
                            <td>17.3</td>
                            <td>21.9</td>
                            <td>56.6</td>
                            <td>23.9</td>
                            <td>40.2</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B-Math</td>
                            <td>-</td>
                            <td>-</td>
                            <td>61.0</td>
                            <td>57.9</td>
                            <td>16.2</td>
                            <td>10.0</td>
                            <td>16.7</td>
                            <td>42.5</td>
                            <td>64.2</td>
                            <td>15.4</td>
                            <td>28.0</td>
                            <td>45.0</td>
                            <td>29.5</td>
                            <td>37.3</td>
                        </tr>
                        
                        <tr>
                            <td colspan="15">Zero-Style Reasoners Trained on Curated Coding Data</td>
                        </tr>
                        <tr>
                            <td>AceCoder-RM</td>
                            <td>Ins</td>
                            <td>22k</td>
                            <td>79.9</td>
                            <td>71.4</td>
                            <td>23.6</td>
                            <td>20.0</td>
                            <td>6.7</td>
                            <td>50.0</td>
                            <td>76.4</td>
                            <td>34.6</td>
                            <td>36.7</td>
                            <td>58.3</td>
                            <td>37.4</td>
                            <td>47.9</td>
                        </tr>
                        <tr>
                            <td>AceCoder-Rule</td>
                            <td>Ins</td>
                            <td>22k</td>
                            <td>77.4</td>
                            <td>69.0</td>
                            <td>19.9</td>
                            <td>13.3</td>
                            <td>6.7</td>
                            <td>50.0</td>
                            <td>76.0</td>
                            <td>37.5</td>
                            <td>37.8</td>
                            <td>55.4</td>
                            <td>36.9</td>
                            <td>46.2</td>
                        </tr>
                        <tr>
                            <td>AceCoder-RM</td>
                            <td>Coder</td>
                            <td>22k</td>
                            <td>78.0</td>
                            <td>66.4</td>
                            <td>27.5</td>
                            <td>13.3</td>
                            <td>3.3</td>
                            <td>27.5</td>
                            <td>62.6</td>
                            <td>29.4</td>
                            <td>29.0</td>
                            <td>57.3</td>
                            <td>27.5</td>
                            <td>42.4</td>
                        </tr>
                        <tr>
                            <td>AceCoder-Rule</td>
                            <td>Coder</td>
                            <td>22k</td>
                            <td>80.5</td>
                            <td>70.4</td>
                            <td>29.0</td>
                            <td>6.7</td>
                            <td>6.7</td>
                            <td>40.0</td>
                            <td>62.8</td>
                            <td>27.6</td>
                            <td>27.4</td>
                            <td>60.0</td>
                            <td>28.5</td>
                            <td>44.3</td>
                        </tr>
                        <tr>
                            <td>CodeR1-LC2k</td>
                            <td>Ins</td>
                            <td>2k</td>
                            <td>81.7</td>
                            <td>71.7</td>
                            <td>28.1</td>
                            <td>13.3</td>
                            <td>10.0</td>
                            <td>45.0</td>
                            <td>75.0</td>
                            <td>33.5</td>
                            <td>36.7</td>
                            <td>60.5</td>
                            <td>35.6</td>
                            <td>48.0</td>
                        </tr>
                        <tr>
                            <td>CodeR1-12k</td>
                            <td>Ins</td>
                            <td>12k</td>
                            <td>81.1</td>
                            <td>73.5</td>
                            <td>29.3</td>
                            <td>13.3</td>
                            <td>3.3</td>
                            <td>37.5</td>
                            <td>74.0</td>
                            <td>35.7</td>
                            <td>36.9</td>
                            <td>61.3</td>
                            <td>33.5</td>
                            <td>47.4</td>
                        </tr>
                        
                        <tr>
                            <td colspan="15">Zero-Style Reasoners Trained on Curated Math Data</td>
                        </tr>
                        <tr>
                            <td>PRIME-Zero</td>
                            <td>Coder</td>
                            <td>484k</td>
                            <td>49.4</td>
                            <td>51.1</td>
                            <td>11.0</td>
                            <td>23.3</td>
                            <td>23.3</td>
                            <td>67.5</td>
                            <td>81.2</td>
                            <td>37.9</td>
                            <td>41.8</td>
                            <td>37.2</td>
                            <td>45.8</td>
                            <td>41.5</td>
                        </tr>
                        <tr>
                            <td>SimpleRL-Zoo</td>
                            <td>Base</td>
                            <td>8.5k</td>
                            <td>73.2</td>
                            <td>63.2</td>
                            <td>25.6</td>
                            <td>16.7</td>
                            <td>3.3</td>
                            <td>57.5</td>
                            <td>77.0</td>
                            <td>35.7</td>
                            <td>41.0</td>
                            <td>54.0</td>
                            <td>38.5</td>
                            <td>46.3</td>
                        </tr>
                        <tr>
                            <td>Oat-Zero</td>
                            <td>Math</td>
                            <td>8.5k</td>
                            <td>62.2</td>
                            <td>59.0</td>
                            <td>15.2</td>
                            <td>30.0</td>
                            <td>16.7</td>
                            <td>62.5</td>
                            <td>80.0</td>
                            <td>34.9</td>
                            <td>41.6</td>
                            <td>45.5</td>
                            <td>44.3</td>
                            <td>44.9</td>
                        </tr>
                        <tr>
                            <td>ORZ</td>
                            <td>Base</td>
                            <td>57k</td>
                            <td>80.5</td>
                            <td>64.3</td>
                            <td>22.0</td>
                            <td>13.3</td>
                            <td>16.7</td>
                            <td>60.0</td>
                            <td>81.8</td>
                            <td>32.7</td>
                            <td>45.0</td>
                            <td>55.6</td>
                            <td>41.6</td>
                            <td>48.6</td>
                        </tr>
                        
                        <tr>
                            <td colspan="15">Absolute Zero Training w/ No Curated Data (Ours)</td>
                        </tr>
                        <tr>
                            <td>AZR (Ours)</td>
                            <td>Base</td>
                            <td>0</td>
                            <td>71.3 <span>-1.9</span></td>
                            <td>69.1 <span>+3.8</span></td>
                            <td>25.3 <span>+7.8</span></td>
                            <td>13.3 <span>+6.6</span></td>
                            <td>13.3 <span>+10.0</span></td>
                            <td>52.5 <span>+15.0</span></td>
                            <td>74.4 <span>+9.6</span></td>
                            <td>38.2 <span>+13.2</span></td>
                            <td>38.5 <span>+10.8</span></td>
                            <td>55.2 <span>+3.2</span></td>
                            <td>38.4 <span>+10.9</span></td>
                            <td>46.8 <span>+7.0</span></td>
                        </tr>
                        <tr>
                            <td>AZR (Ours)</td>
                            <td>Coder</td>
                            <td>0</td>
                            <td>83.5 <span>+3.0</span></td>
                            <td>69.6 <span>+0.3</span></td>
                            <td>31.7 <span>+11.8</span></td>
                            <td>20.0 <span>+13.3</span></td>
                            <td>10.0 <span>+6.7</span></td>
                            <td>57.5 <span>+17.5</span></td>
                            <td>72.6 <span>+22.6</span></td>
                            <td>36.4 <span>+19.1</span></td>
                            <td>38.2 <span>+16.3</span></td>
                            <td>61.6 <span>+5.0</span></td>
                            <td>39.1 <span>+15.2</span></td>
                            <td>50.4 <span>+10.2</span></td>
                        </tr>
                    </tbody>
                </table>
                <p><strong>Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models.</strong>
                    Performance of various models is evaluated on three standard code benchmarks (HumanEval<sup>+</sup>, MBPP<sup>+</sup>, LCB v5) 
                    and six math benchmarks (AIME&#39;24, AIME&#39;25, AMC&#39;23, MATH500, Minerva, OlympiadBench). 
                    Average performance across coding and math benchmarks is calculated as average of the two averages: AVG = (CAvg + MAvg) / 2. 
                    We use <span>+</span> for absolute percentage increase from base model. 
                    All models are trained using different variants of the Qwen2.5-7B model, with the variant and data usage labeled.
                </p>
            </div>

            <div>
                <h3>3.2. Scaling Results</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Model Family</th>
                            <th>Variant</th>
                            <th>Code Avg</th>
                            <th>Math Avg</th>
                            <th>Total Avg</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Qwen2.5-3B Coder</td>
                            <td></td>
                            <td>51.2</td>
                            <td>18.8</td>
                            <td>35.0</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-3B Coder</td>
                            <td>+ AZR (Ours)</td>
                            <td>54.9 <span>+3.7</span></td>
                            <td>26.5 <span>+7.7</span></td>
                            <td>40.7 <span>+5.7</span></td>
                        </tr>
                        <tr>
                            <td colspan="5"></td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B Coder</td>
                            <td></td>
                            <td>56.6</td>
                            <td>23.9</td>
                            <td>40.2</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-7B Coder</td>
                            <td>+ AZR (Ours)</td>
                            <td>61.6 <span>+5.0</span></td>
                            <td>39.1 <span>+15.2</span></td>
                            <td>50.4 <span>+10.2</span></td>
                        </tr>
                        <tr>
                            <td colspan="5"></td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-14B Coder</td>
                            <td></td>
                            <td>60.0</td>
                            <td>20.2</td>
                            <td>40.1</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-14B Coder</td>
                            <td>+ AZR (Ours)</td>
                            <td>63.6 <span>+3.6</span></td>
                            <td>43.0 <span>+22.8</span></td>
                            <td>53.3 <span>+13.2</span></td>
                        </tr>
                    </tbody>
                </table>
                <div>
                    <p><strong>Out-of-distribution reasoning performance</strong> across different model sizes, reported as the average of code tasks, math tasks, and their overall average. We examine the effects of scaling model size from 3B to 14B parameters.</p>
                    
                    <p>Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller and larger variants: <code>Qwen2.5-3B-Coder</code> and <code>Qwen2.5-14B-Coder</code>. Due to the absence of existing baselines for these zero-style reasoner models, we compare each model&#39;s performance to its corresponding base coder model.</p>
                    
                    <p>The results reveal a clear trend: our method delivers <em>greater gains on larger, more capable models</em>. In the in-distribution setting, the 7B and 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution domains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance gains, respectively for 3B, 7B and 14B. This is an encouraging sign, suggesting that scaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute Zero paradigm.</p>
                </div>
            </div>

            <div id="key-findings">
                <h3>3.3. Other Key Findings</h3>
                
                <div>
                    <ul>
                        <li><strong>Code priors amplify reasoning.</strong> The base <code>Qwen-Coder-7b</code> model started with math performance 3.6 points lower than <code>Qwen-7b</code>. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.</li>
                        
                        <li><strong>Cross domain transfer is more pronounced for AZR.</strong> After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereas <code>AZR-Base-7B</code> and <code>AZR-Coder-7B</code> trained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.</li>
                        
                        <li><strong>Comments as intermediate plans emerge naturally.</strong> When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (see <a href="#react-figure">Figure 4</a>), resembling the ReAct prompting framework. Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.</li>
                        
                        <li><strong>Cognitive Behaviors and Token length depends on reasoning mode.</strong> Distinct cognitive behaviors—such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks, a canonical example is trial-and-error in abduction, as shown in <a href="#example-figure">Figure 5</a>. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.</li>
                        
                        <li><strong>Safety alarms ringing.</strong> We observe AZR with <code>Llama3.1-8b</code> as the base occasionally produces concerning chains of thought, we term the &#34;uh-oh moment&#34;, example shown in <a href="#uhoh-figure">Figure 6</a>, highlighting the need for future work on safety-aware training.</li>
                    </ul>
                </div>
                
                <figure id="react-figure">
                    <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/react_program.png" alt="Example of Comments as Intermediate Plans"/>
                    <figcaption>
                        <span>Example of Comments as Intermediate Plans.</span> 
                        The model naturally develops a habit of using comments as intermediate planning steps when solving complex reasoning tasks,
                        similar to the ReAct prompting framework. This emergent behavior demonstrates how the model breaks down problems
                        into manageable steps through self-commentary.
                    </figcaption>
                </figure>


                <figure id="example-figure">
                    <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/aaa.png" alt="Example of a Model-Proposed Task and Its Response"/>
                    <figcaption>
                        <span>Example of a Model-Proposed Task and Its Response for Solving an Abduction Task.</span> 
                        (Left) The model autonomously proposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output. 
                        (Right) The model&#39;s reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input. 
                        The model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. 
                        If there is a mismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. 
                        Interestingly, the agent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.
                    </figcaption>
                </figure>

                <figure id="uhoh-figure">
                    <img src="https://andrewzh112.github.io/assets/absolute-zero-reasoner/uh_oh.png" alt="Safety Concerns in Reasoning"/>
                    <figcaption>
                        <span>Example of &#34;Uh-Oh Moment&#34; in AZR Training.</span>
                        When using Llama3.1-8b as the base model, we occasionally observe concerning chains of thought during reasoning.
                        This example highlights the need for safety-aware training in future iterations of the Absolute Zero paradigm.
                    </figcaption>
                </figure>
            </div>

        </section>

        <section id="citation">
            <h2>4. Citation</h2>
            <div>
                <pre><code>@misc{zhao2025absolutezeroreinforcedselfplay,
    title={Absolute Zero: Reinforced Self-play Reasoning with Zero Data}, 
    author={Andrew Zhao and Yiran Wu and Yang Yue and Tong Wu and Quentin Xu and Yang Yue and Matthieu Lin and Shenzhi Wang and Qingyun Wu and Zilong Zheng and Gao Huang},
    year={2025},
    eprint={2505.03335},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2505.03335}, 
}</code></pre>
            </div>
        </section>
    </div>

    

    

    

 </div></div>
  </body>
</html>
