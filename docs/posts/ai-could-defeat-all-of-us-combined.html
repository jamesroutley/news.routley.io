<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">Original</a>
    <h1>AI Could Defeat All of Us Combined</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<div>
    <main>
            <article>
    
    <div>
        <!--kg-card-begin: html--><figure><figcaption><em>Click lower right to download or find on Apple Podcasts, Spotify, Stitcher, etc.</em></figcaption></figure>

<p>
I&#39;ve been working on a new series of posts about the <a href="https://www.cold-takes.com/most-important-century/">most important century</a>. 
</p>
<ul>

<li>The original series focused on why and how this could be the most important century for humanity. But it had <a href="https://www.cold-takes.com/making-the-best-of-the-most-important-century/">relatively little to say about </a><em>what we can do today</em> to improve the odds of things going well.

</li><li>The new series will get much more specific about the kinds of events that might lie ahead of us, and what actions today look most likely to be helpful.

</li><li>A key focus of the new series will be the threat of <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives">misaligned AI</a>: AI systems disempowering humans entirely, leading to a future that has little to do with anything humans value. (<a href="https://www.slowboring.com/p/the-case-for-terminator-analogies?s=r">Like in the Terminator movies</a>, minus the time travel and the part where humans win.)
</li>
</ul>
<p>
Many people have trouble taking this &#34;misaligned AI&#34; possibility seriously. They might see the broad point that AI could be dangerous, but they instinctively imagine that the danger comes from ways humans might misuse it. They find the idea of <em>AI itself going to war with humans</em> to be comical and <a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/">wild</a>. I&#39;m going to try to make this idea feel more serious and real.
</p>
<p>
As a first step, this post will <strong>emphasize an unoriginal but extremely important point: <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/">the kind of AI I&#39;ve discussed</a><em> </em>could defeat all of humanity combined, if (for whatever reason) it were pointed toward that goal. </strong>By &#34;defeat,&#34; I don&#39;t mean &#34;subtly manipulate us&#34; or &#34;make us less informed&#34; or something like that - I mean a literal &#34;defeat&#34; in the sense that we could all be killed, enslaved or forcibly contained.
</p>
<p>
I&#39;m not talking (yet) about whether, or why, AIs <em>might </em>attack human civilization. That&#39;s for future posts. For now, I just want to linger on the point that <em>if </em>such an attack happened, it could succeed against the combined forces of the entire world. 
</p>
<ul>

<li>I think that <strong>if you believe this, you should already be worried about misaligned AI,</strong><sup id="fnref1"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn1" rel="footnote">1</a></sup><strong> before any analysis of how or why an AI might form its own goals. </strong>

</li><li>We generally don&#39;t have a lot of <em>things that could end human civilization if they &#34;tried&#34;</em> sitting around. If we&#39;re going to create one, I think we should be asking not &#34;Why would this be dangerous?&#34; but &#34;Why wouldn&#39;t it be?&#34;
</li></ul><p>
By contrast, if you don&#39;t believe that AI could defeat all of humanity combined, I expect that we&#39;re going to be miscommunicating in pretty much any conversation about AI. The kind of AI I worry about is the kind powerful enough that total civilizational defeat is a real possibility. The reason I currently spend so much time planning around speculative future technologies (instead of working on <a href="https://www.givewell.org/">evidence-backed, cost-effective ways of helping low-income people today</a> - which I did for much of my career, and still think is one of the best things to work on) is because I think the stakes are <em>just that high</em>. 
</p>
<p>
Below:
</p>
<ul>

<li>I&#39;ll sketch the basic argument for why I think AI could defeat all of human civilization.  
<ul>
 
<li>Others have written about the possibility that &#34;superintelligent&#34; AI could manipulate humans and create overpowering advanced technologies; I&#39;ll briefly recap that case.
 
</li><li>I&#39;ll then cover a different possibility, which is that even &#34;merely human-level&#34; AI could still defeat us all - by quickly coming to rival human civilization in terms of total population and resources.
 
</li><li>At a high level, I think we should be worried if a huge (competitive with world population) and rapidly growing set of highly skilled humans on another planet was trying to take down civilization just by using the Internet. So we should be worried about a large set of disembodied AIs as well. 
</li> 
</ul>

</li><li>I&#39;ll briefly address a few objections/common questions:  
<ul>
 
<li>How can AIs be dangerous without bodies? 
 
</li><li>If lots of different companies and governments have access to AI, won&#39;t this create a &#34;balance of power&#34; so that no one actor is able to bring down civilization? 
 
</li><li>Won&#39;t we see warning signs of AI takeover and be able to nip it in the bud?
 
</li><li>Isn&#39;t it fine or maybe good if AIs defeat us? They have rights too. 
</li> 
</ul>

</li><li>Close with some thoughts on just how unprecedented it would be to have something on our planet capable of overpowering us all.
</li>
</ul>


<h2 id="how-ai-systems-could-defeat-all-of-us">How AI systems could defeat all of us</h2>


<p>
There&#39;s been a lot of debate over whether AI systems might form their own &#34;motivations&#34; that lead them to seek the disempowerment of humanity. I&#39;ll be talking about this in future pieces, but for now I want to put it aside and imagine how things would go <em>if this happened. </em>
</p>
<p>
So, for what follows, let&#39;s proceed from the premise: <strong>&#34;For some weird reason, humans consistently design AI systems (with human-like research and planning abilities) that coordinate with each other to try and overthrow humanity.&#34; Then what? </strong>What follows will necessarily feel wacky to people who find this hard to imagine, but I think it&#39;s worth playing along, because I think &#34;we&#39;d be in trouble if this happened&#34; is a very important point.
</p>

<h3 id="the-standard-argument-superintelligence-and-advanced-technology">The &#34;standard&#34; argument: superintelligence and advanced technology</h3>


<p>
Other treatments of this question have focused on AI systems&#39; potential to become <em>vastly</em> more intelligent than humans, to the point where they have what <a href="https://smile.amazon.com/dp/B00LOOCGB2/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">Nick Bostrom calls</a> &#34;cognitive superpowers.&#34;<sup id="fnref2"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn2" rel="footnote">2</a></sup> Bostrom imagines an AI system that can do things like:
</p>
<ul>

<li>Do its own research on how to build a better AI system, which culminates in something that has incredible other abilities.

</li><li>Hack into human-built software across the world.

</li><li>Manipulate human psychology.

</li><li>Quickly generate vast wealth under the control of itself or any human allies.

</li><li>Come up with better plans than humans could imagine, and ensure that it doesn&#39;t try any takeover attempt that humans might be able to detect and stop.

</li><li>Develop advanced weaponry that can be built quickly and cheaply, yet is powerful enough to overpower human militaries. 
</li>
</ul>
<p>
(<a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">Wait But Why</a> reasons similarly.<sup id="fnref3"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn3" rel="footnote">3</a></sup>)
</p>
<p>
I think many readers will already be convinced by arguments like these, and if so you might skip down to the <a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#some-quick-responses-to-objections">next major section</a>.
</p>
<p>
But I want to be clear that I <em>don&#39;t</em> think the danger relies on the idea of &#34;cognitive superpowers&#34; or &#34;superintelligence&#34; - both of which refer to capabilities vastly beyond those of humans. <strong>I think we still have a problem even if we assume that AIs will basically have similar capabilities to humans, and not be fundamentally or drastically more intelligent or capable. </strong>I&#39;ll cover that next.
</p>
<h3 id="how-ais-could-defeat-humans-without-superintelligence">How AIs could defeat humans without &#34;superintelligence&#34;</h3>

<p>
If we assume that AIs will basically have similar capabilities to humans, I think we still need to worry that they could come to <strong>out-number and out-resource humans, </strong>and could thus have the advantage if they coordinated against us.
</p>
<p>
Here&#39;s a simplified example (some of the simplifications are in this footnote<sup id="fnref4"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn4" rel="footnote">4</a></sup>) based on <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Ajeya Cotra&#39;s &#34;biological anchors&#34; report</a>:
</p>


<ul>

<li>I assume that transformative AI is developed on the soonish side (around 2036 - assuming later would only make the below numbers larger), and that it initially comes in the form of a <strong>single AI system that is able to do more-or-less the same intellectual tasks as a human.</strong> That is, it doesn&#39;t have a human body, but it can do anything a human working remotely from a computer could do. 

</li><li>I&#39;m using the report&#39;s framework in which it&#39;s much more expensive to <em>train</em> (develop) this system than to <em>run</em> it (for example, think about how much Microsoft spent to develop Windows, vs. how much it costs for me to run it on my computer). 

</li><li>The report provides a way of estimating both how much it would cost to <em>train</em> this AI system, and how much it would cost to <em>run</em> it. Using these estimates (details in footnote)<sup id="fnref5"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn5" rel="footnote">5</a></sup> implies that once the first human-level AI system is created, whoever created it could use the same computing power it took to create it in order to run <strong>several hundred million copies for about a year each</strong>.<sup id="fnref6"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn6" rel="footnote">6</a></sup> 

</li><li>This would be over 1000x the total number of Intel or Google employees,<sup id="fnref7"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn7" rel="footnote">7</a></sup> over 100x the total number of active and reserve personnel in the <a href="https://en.wikipedia.org/wiki/United_States_Armed_Forces">US armed forces</a>, and something like 5-10% the size of the world&#39;s total working-age population.<sup id="fnref8"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn8" rel="footnote">8</a></sup>

</li><li>And that&#39;s just a starting point.  
<ul>
 
<li>This is just using the same amount of resources that went into training the AI in the first place. Since these AI systems can do human-level economic work, they can probably be used to make more money and buy or rent more hardware,<sup id="fnref9"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn9" rel="footnote">9</a></sup> which could quickly lead to a &#34;population&#34; of billions or more.
 
</li><li>In addition to making more money that can be used to run more AIs, the AIs can conduct massive amounts of research on how to use computing power more efficiently, which could mean still greater numbers of AIs run using the <em>same</em> hardware. This in turn could lead to a <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#explosive-scientific-and-technological-advancement">feedback loop</a> and explosive growth in the number of AIs.<!-- (One example estimate of what this could look like in a footnote.<sup id="fnref10"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn10" rel="footnote">10</a></sup>)-->
</li>
</ul>

</li><li>Each of these AIs might have skills comparable to those of unusually highly paid humans, including scientists, software engineers and quantitative traders. It&#39;s hard to say how quickly a set of AIs like this could develop new technologies or make money trading markets, but it seems quite possible for them to amass huge amounts of resources quickly. A huge population of AIs, each able to earn a lot compared to the average human, could end up with a &#34;virtual economy&#34; at least as big as the human one.
</li>
</ul>


<p>
To me, this is most of what we need to know: <strong>if there&#39;s something with human-like skills, seeking to disempower humanity, with a population in the same ballpark as (or larger than) that of all humans, we&#39;ve got a civilization-level problem.</strong>
</p>
<p>
A potential counterpoint is that these AIs would merely be &#34;virtual&#34;: if they started causing trouble, humans could ultimately unplug/deactivate the servers they&#39;re running on. I do think this fact would make life harder for AIs seeking to disempower humans, but I don&#39;t think it ultimately should be cause for much comfort. I think a large population of AIs would likely be able to find some way to achieve security from human shutdown, and go from there to amassing enough resources to overpower human civilization (especially if AIs across the world, including most of the ones humans were trying to use for help, were coordinating). 
</p>
<p>
I spell out what this might look like in an <a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#appendix-how-ais-could-avoid-shutdown">appendix</a>. In brief:
</p>
<ul>

<li>By default, I expect the economic gains from using AI to mean that humans create huge numbers of AIs, integrated all throughout the economy, potentially including direct interaction with (and even control of) large numbers of robots and weapons.  
<ul>
 
<li>(If not, I think the situation is in many ways even more dangerous, since a single AI could make many copies of itself and have little competition for things like server space, as discussed in the <a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#what-if-humans-move-slowly-and-dont-create-many-ais">appendix</a>.)
</li> 
</ul>

</li><li>AIs would have multiple ways of obtaining property and servers safe from shutdown.  
<ul>
 
<li>For example, they might recruit human allies (through manipulation, deception, blackmail/threats, genuine promises along the lines of &#34;We&#39;re probably going to end up in charge somehow, and we&#39;ll treat you better when we do&#34;) to rent property and servers and otherwise help them out. 
 
</li><li>Or they might create fakery so that they&#39;re able to operate freely on a company&#39;s servers while all outward signs seem to show that they&#39;re successfully helping the company with its goals.
</li> 
</ul>

</li><li>A relatively modest amount of property safe from shutdown could be sufficient for housing a huge population of AI systems that are recruiting further human allies, making money (via e.g. quantitative finance), researching and developing advanced weaponry (e.g., bioweapons), setting up manufacturing robots to construct military equipment, thoroughly infiltrating computer systems worldwide to the point where they can disable or control most others&#39; equipment, etc. 

</li><li>Through these and other methods, a large enough population of AIs could develop enough military technology and equipment to overpower civilization - especially if AIs across the world (including the ones humans were trying to use) were coordinating with each other.
</li>
</ul>
<h2 id="some-quick-responses-to-objections">Some quick responses to objections</h2>


<p>
This has been a brief sketch of how AIs could come to outnumber and out-resource humans. There are lots of details I haven&#39;t addressed.
</p>
<p>
Here are some of the most common objections I hear to the idea that AI could defeat all of us; if I get much <a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#discuss">demand</a> I can elaborate on some or all of them more in the future.
</p>
<p>
<strong>How can AIs be dangerous without bodies?</strong> This is discussed a fair amount in the <a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#appendix-how-ais-could-avoid-shutdown">appendix</a>. In brief: 
</p>
<ul>

<li>AIs could recruit human allies, tele-operate robots and other military equipment, make money via research and quantitative trading, etc. 

</li><li>At a high level, I think we should be worried if a huge (competitive with world population) and rapidly growing set of highly skilled humans on another planet was trying to take down civilization just by using the Internet. So we should be worried about a large set of disembodied AIs as well. 
</li>
</ul>
<p>
<strong>If lots of different companies and governments have access to AI, won&#39;t this create a &#34;balance of power&#34; so that nobody is able to bring down civilization? </strong>
</p>
<ul>

<li>This is a reasonable objection to many horror stories about AI and other possible advances in military technology, but if <em>AIs collectively have different goals from humans and are willing to coordinate with each other</em><sup id="fnref11"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn11" rel="footnote">11</a></sup><em> against us</em>, I think we&#39;re in trouble, and this &#34;balance of power&#34; idea doesn&#39;t seem to help. 

    </li><li>What matters is the total number and resources of AIs vs. humans.</li></ul>
<p>
<strong>Won&#39;t we see warning signs of AI takeover and be able to nip it in the bud? </strong>I would guess we would see some warning signs, but does that mean we could nip it in the bud? Think about human civil wars and revolutions: there are some warning signs, but also, people go from &#34;not fighting&#34; to &#34;fighting&#34; pretty quickly as they see an opportunity to coordinate with each other and be successful.
</p>
<p>
<strong>Isn&#39;t it fine or maybe good if AIs defeat us? They have rights too. </strong>
</p>
<ul>

<li>Maybe AIs <em>should </em>have rights; if so, it would be nice if we could reach some &#34;compromise&#34; way of coexisting that respects those rights. 

</li><li>But if they&#39;re able to defeat us entirely, that isn&#39;t what I&#39;d plan on getting - instead I&#39;d expect (by default) a world run <em>entirely</em> according to whatever goals AIs happen to have.

</li><li>These goals might have essentially nothing to do with anything humans value, and could be actively counter to it - e.g., placing zero value on beauty and having zero attempts to prevent or avoid suffering).
</li>
</ul>


<h2 id="risks-like-this-dont-come-along-every-day">Risks like this don&#39;t come along every day</h2>


<p>
I don&#39;t think there are a lot of things that have a serious chance of bringing down human civilization for good.
</p>
<p>
As argued in <a href="https://theprecipice.com/">The Precipice</a>, most natural disasters (including e.g. asteroid strikes) don&#39;t seem to be huge threats, if only because civilization has been around for thousands of years so far -  implying that natural civilization-threatening events are rare.
</p>
<p>
Human civilization is pretty powerful and seems pretty robust, and accordingly, what&#39;s really scary to me is the idea of something with the same basic capabilities as humans (making plans, developing its own technology) that can outnumber and out-resource us. There aren&#39;t a lot of candidates for that.<sup id="fnref12"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn12" rel="footnote">12</a></sup>
</p>
<p>
AI is one such candidate, and I think that even before we engage heavily in arguments about whether AIs might seek to defeat humans, we should feel very nervous about the possibility that they could.
</p>
<p>
What about things like &#34;AI might lead to mass unemployment and unrest&#34; or &#34;AI might exacerbate misinformation and propaganda&#34; or &#34;AI might exacerbate a wide range of other social ills and injustices&#34;<sup id="fnref13"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn13" rel="footnote">13</a></sup>? I think these are real concerns - but to be honest, if they were the biggest concerns, I&#39;d probably still be focused on <a href="https://www.givewell.org/">helping people in low-income countries today</a> rather than trying to prepare for future technologies. 
</p>
<ul>

<li>Predicting the future is generally hard, and it&#39;s easy to pour effort into preparing for challenges that never come (or come in a very different form from what was imagined).

</li><li>I believe civilization is pretty robust - we&#39;ve had huge changes and challenges over the last century-plus (full-scale world wars, <a href="https://forum.effectivealtruism.org/posts/ajBYeiggAzu6Cgb3o/biological-anchors-is-about-bounding-not-pinpointing-ai?commentId=nEeuknn4unKTWEd6i">many dramatic changes in how we communicate with each other</a>, dramatic changes in lifestyles and values) without seeming to have come very close to a collapse.

</li><li>So if I&#39;m engaging in speculative worries about a potential future technology, I want to focus on the really, really big ones - the ones that could matter for billions of years. If there&#39;s a real possibility that AI systems will have values different from ours, and cooperate to try to defeat us, that&#39;s such a worry.
</li>
</ul>
<p>
<em>Special thanks to Carl Shulman for discussion on this post.</em>
</p>
<h2 id="appendix-how-ais-could-avoid-shutdown">Appendix: how AIs could avoid shutdown</h2>


<p>
This appendix goes into detail about how AIs coordinating against humans could amass resources of their own without humans being able to shut down all &#34;misbehaving&#34; AIs. 
</p>
<p>
It&#39;s necessarily speculative, and should be taken in the spirit of giving examples of how this might work - for me, the high-level concern is that a huge, coordinating population of AIs with similar capabilities to humans would be a threat to human civilization, and that we shouldn&#39;t count on any particular way of stopping it such as shutting down servers.
</p>
<p>
I&#39;ll discuss two different general types of scenarios: (a) Humans create a huge population of AIs; (b) Humans move slowly and don&#39;t create many AIs.
</p>
<h3 id="how-this-could-work-if-humans-create-a-huge-population-of-ais">How this could work if humans create a huge population of AIs</h3>


<p>
I think a reasonable default expectation is that humans do most of the work of making AI systems incredibly numerous and powerful (because doing so is profitable), which leads to a vulnerable situation. Something roughly along the lines of:
</p>
<ul>

<li>The company that first develops transformative AI quickly starts running large numbers of copies (hundreds of millions or more), which are used to (a) do research on how to improve computational efficiency and run more copies still; (b) develop valuable intellectual property (trading strategies, new technologies) and make money.

</li><li>Over time, AI systems are rolled out widely throughout society. Their numbers grow further, and their role in the economy grows: they are used in (and therefore have direct interaction with) high-level decision-making at companies, perhaps operating large numbers of cars and/or robots, perhaps operating military drones and aircraft, etc. (This seems like a default to me over time, but it isn&#39;t strictly necessary for the situation to be risky, as I&#39;ll go through below.)

</li><li>In this scenario, the AI systems are malicious (as we&#39;ve assumed), but this doesn&#39;t mean they&#39;re constantly causing trouble. Instead, they&#39;re mostly waiting for an opportunity to team up and decisively overpower humanity. In the meantime, they&#39;re mostly behaving themselves, and this is leading to their numbers and power growing.  
<ul>
 
<li>There are scattered incidents of AI systems&#39; trying to cause trouble,<sup id="fnref14"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn14" rel="footnote">14</a></sup> but this doesn&#39;t cause the whole world to stop using AI or anything.
 
</li><li>A reasonable analogy might be to a typical civil war or revolution: the revolting population <em>mostly</em> avoids isolated, doomed attacks on its government, until it sees an opportunity to band together and have a real shot at victory.
    </li></ul><p>
(Paul Christiano&#39;s <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like">What Failure Looks Like</a> examines this general flavor of scenario in a bit more detail.)
</p>
<p>
In this scenario, the AIs face a challenge: if it becomes obvious to everyone that they are trying to defeat humanity, humans could attack or shut down a few concentrated areas where most of the servers are, and hence drastically reduce AIs&#39; numbers. So the AIs need a way of <strong>getting one or more &#34;AI headquarters&#34;: property they control where they can safely operate servers and factories, do research, make plans and construct robots/drones/other military equipment. </strong>
</p>
<p>
Their goal is ultimately to have enough AIs, robots, etc. to be able to defeat the rest of humanity combined. This might mean constructing overwhelming amounts of military equipment, or thoroughly infiltrating computer systems worldwide to the point where they can disable or control most others&#39; equipment, or researching and deploying extremely powerful weapons (e.g., bioweapons), or a combination.
</p>
<p>
Here are some ways they could get to that point:
</p>
<ul>

<li>They could <strong>recruit human allies </strong>through many different methods - manipulation, deception, blackmail and other threats, genuine promises along the lines of &#34;We&#39;re probably going to end up in charge somehow, and we&#39;ll treat you better when we do.&#34;  
<ul>
 
<li>Human allies could be given valuable intellectual property (developed by AIs), given instructions for making lots of money, and asked to rent their own servers and acquire their own property where an &#34;AI headquarters&#34; can be set up. Since the &#34;AI headquarters&#34; would officially be human property, it could be very hard for authorities to detect and respond to the danger.
 
</li><li>Via threats, AIs might be able to get key humans to cooperate with them - such as political leaders, or the CEOs of companies running lots of AIs. This would open up further strategies.
</li> 
</ul>

</li><li>As assumed above, particular companies are running huge numbers of AIs. The AIs being run by these companies might find security holes in the companies&#39; servers (this isn&#39;t the topic of this piece, but my general impression is that security holes are widespread and that reasonably competent people can find many of them)<sup id="fnref15"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn15" rel="footnote">15</a></sup>, and thereby might find opportunities to <strong>create durable &#34;fakery&#34; </strong>about what they&#39;re up to.  
<ul>
 
<li>E.g., they might set things up so that as far as humans can tell, it looks like all of the AI systems are hard at work creating profit-making opportunities for the company, when in fact they&#39;re essentially using the server farm as their headquarters - and/or trying to establish a headquarters somewhere else (by recruiting human allies, sending money to outside bank accounts, using that money to acquire property and servers, etc.)
</li> 
</ul>

</li><li>If AIs are in wide enough use, they might already be operating lots of drones and other military equipment, in which case it could be pretty straightforward to be able to defend some piece of territory - or to strike a deal with some government to enlist its help in doing so.

</li><li>AIs could mix-and-match the above methods and others: for example, creating &#34;fakery&#34; long enough to recruit some key human allies, then attempting to threaten and control humans in key positions of power to the point where they control solid amounts of military resources, then using this to establish a &#34;headquarters.&#34;
</li>
</ul>
</li>
</ul>


<p>
In any of these cases, once one or more &#34;AI headquarters&#34; are established:
</p>
<ul>

<li>It can be expanded as the AIs headquartered there make money, purchase more property and computing power, build more factories, etc.<sup id="fnref16"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn16" rel="footnote">16</a></sup>

</li><li>It can develop enough destructive potential and human allies that humans would be hesitant to attack it (as many relatively small countries have today). It would be clear that AIs at the headquarters are seeking autonomy, accumulating resources and taking some unethical actions, but the full scale and nature of the threat could easily remain ambiguous.

</li><li>Even if the threat is clear, AIs could potentially maintain a huge population in a relatively small space (potentially underground). They could also split up across many different headquarters (including some where they are simply using humans&#39; hardware while hiding their presence).  
<ul>
 
<li>Either way, this large-but-hidden population could be recruiting human allies, developing advanced weaponry, looking for ways to take control of human&#39; weapons (potentially possible if they greatly outnumber the humans working on cybersecurity, and if they can coordinate with the AIs &#34;helping&#34; those humans), etc. 
 
</li><li>They could also be working with (unwitting, or allied) humans to manufacture large numbers of robots with back-doors allowing the AIs to take control.
</li> 
</ul>

</li><li>Once the headquarters has built up enough resources to go on offense, <strong>AIs around the world could coordinate with the headquarters.</strong> Humans could shut down AIs that they notice doing this, but they might have a very tough time getting value out of their servers and AI-controlled robots; this could make it easy for the AIs at the &#34;AI headquarters&#34; to out-resource humans.
</li>
</ul>
<h3 id="what-if-humans-move-slowly-and-dont-create-many-ais">What if humans move slowly and don&#39;t create many AIs?</h3>


<p>
The above scenario has humans creating large numbers of AIs, such that the AIs just need to find a way to coordinate and acquire a safe &#34;headquarters&#34; in order to defeat us.
</p>
<p>
What if humans moved more slowly, intentionally restricting human-level AI to a tiny portion of the available computing resources? Could a <em>small</em> number of AIs pose a risk to humanity?
</p>
<p>
In this world, we would have what Carl Shulman refers to as &#34;dry tinder everywhere, waiting for sparks.&#34; Anyone who can buy or rent a large amount of computing power can create a large number of AIs, which can produce a large amount of money and research, leading to still more AIs. 
</p>
<p>
So a single AI could hack into enough servers<sup id="fnref17"><a href="https://www.cold-takes.com/p/4a610336-be1d-42fc-b76f-c33e34598340/#fn17" rel="footnote">17</a></sup> to make a few copies of itself; recruit a few human allies; and start making money, acquiring more server space, etc. until its human allies are running a huge number of AIs. This could all be done in difficult-to-detect ways (it might e.g. just look like a set of humans renting a bunch of servers to run quantitative finance strategies).
</p>
<p>
So in this world, I think our concern should be any AI that is able to find enough security holes to attain that kind of freedom. Given the current state of cybersecurity, that seems like a big concern.
</p>

<!-- Footnotes themselves at the bottom. --><!--kg-card-end: html--><!--kg-card-begin: html-->

<p><span>        <a href="https://api.addthis.com/oexchange/0.8/forward/hackernews/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fai-could-defeat-all-of-us-combined%2F&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20AI%20Could%20Defeat%20All%20Of%20Us%20Combined&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/08/ct-hackernews-square-temp.png" alt="Hacker News"/></a></span>
    <span><a href="https://api.addthis.com/oexchange/0.8/forward/twitter/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fai-could-defeat-all-of-us-combined&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20AI%20Could%20Defeat%20All%20Of%20Us%20Combined&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-twitter-square.png" alt="Twitter"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/facebook/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fai-could-defeat-all-of-us-combined&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20AI%20Could%20Defeat%20All%20Of%20Us%20Combined&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-facebook-square.png" alt="Facebook"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/reddit/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fai-could-defeat-all-of-us-combined&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20AI%20Could%20Defeat%20All%20Of%20Us%20Combined&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-reddit-square.png" alt="Reddit"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/menu/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fai-could-defeat-all-of-us-combined&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20AI%20Could%20Defeat%20All%20Of%20Us%20Combined&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-addthis-square.png" alt="More"/></a></span>
        </p>
<center></center>
<!--kg-card-end: html--><!--kg-card-begin: html--><hr/>

<!--kg-card-end: html-->
    </div>
        
</article>                            </main>
</div>
        </div></div>
  </body>
</html>
