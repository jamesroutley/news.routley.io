<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Tongyi-MAI/Z-Image">Original</a>
    <h1>Z-Image: Powerful and highly efficient image generation model with 6B parameters</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">


<p dir="auto">Z-Image is a powerful and highly efficient image generation model with <strong>6B</strong> parameters. Currently there are three variants:</p>
<ul dir="auto">
<li>
<p dir="auto">üöÄ <strong>Z-Image-Turbo</strong> ‚Äì A distilled version of Z-Image that matches or exceeds leading competitors with only <strong>8 NFEs</strong> (Number of Function Evaluations). It offers <strong>‚ö°Ô∏èsub-second inference latency‚ö°Ô∏è</strong> on enterprise-grade H800 GPUs and fits comfortably within <strong>16G VRAM consumer devices</strong>. It excels in photorealistic image generation, bilingual text rendering (English &amp; Chinese), and robust instruction adherence.</p>
</li>
<li>
<p dir="auto">üß± <strong>Z-Image-Base</strong> ‚Äì The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.</p>
</li>
<li>
<p dir="auto">‚úçÔ∏è <strong>Z-Image-Edit</strong> ‚Äì A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.</p>
</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Hugging Face</th>
<th>ModelScope</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Z-Image-Turbo</strong></td>
<td><a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo" rel="nofollow"><img src="https://camo.githubusercontent.com/c7b5297fa004103e381199ad34a108d5bd16e433688d9aeff6d78b4e44d9aa80/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f254630253946254134253937253230436865636b706f696e742532302d5a2d2d496d6167652d2d547572626f2d79656c6c6f77" alt="Hugging Face" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow"/></a> </td>
<td><a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo" rel="nofollow"><img src="https://camo.githubusercontent.com/284a4a2e68383a77f97b477bec44d98d625845964b362dd416cfcad139bc440a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f254630253946254134253936253230253230436865636b706f696e742d5a2d2d496d6167652d2d547572626f2d363234616666" alt="ModelScope Model" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%96%20%20Checkpoint-Z--Image--Turbo-624aff"/></a> </td>
</tr>
<tr>
<td><strong>Z-Image-Base</strong></td>
<td><em>To be released</em></td>
<td><em>To be released</em></td>
</tr>
<tr>
<td><strong>Z-Image-Edit</strong></td>
<td><em>To be released</em></td>
<td><em>To be released</em></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">üì∏ <strong>Photorealistic Quality</strong>: <strong>Z-Image-Turbo</strong> delivers strong photorealistic image generation while maintaining excellent aesthetic quality.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/showcase_realistic.png"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/showcase_realistic.png" alt="Showcase of Z-Image on Photo-realistic image Generation"/></a></p>
<p dir="auto">üìñ <strong>Accurate Bilingual Text Rendering</strong>: <strong>Z-Image-Turbo</strong> excels at accurately rendering complex Chinese and English text.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/showcase_rendering.png"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/showcase_rendering.png" alt="Showcase of Z-Image on Bilingual Text Rendering"/></a></p>
<p dir="auto">üí°  <strong>Prompt Enhancing &amp; Reasoning</strong>: Prompt Enhancer empowers the model with reasoning capabilities, enabling it to transcend surface-level descriptions and tap into underlying world knowledge.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/reasoning.png"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/reasoning.png" alt="reasoning.jpg"/></a></p>
<p dir="auto">üß† <strong>Creative Image Editing</strong>: <strong>Z-Image-Edit</strong> shows a strong understanding of bilingual editing instructions, enabling imaginative and flexible image transformations.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/showcase_editing.png"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/showcase_editing.png" alt="Showcase of Z-Image-Edit on Image Editing"/></a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">üèóÔ∏è Model Architecture</h3><a id="user-content-Ô∏è-model-architecture" aria-label="Permalink: üèóÔ∏è Model Architecture" href="#Ô∏è-model-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We adopt a <strong>Scalable Single-Stream DiT</strong> (S3-DiT) architecture. In this setup, text, visual semantic tokens, and image VAE tokens are concatenated at the sequence level to serve as a unified input stream, maximizing parameter efficiency compared to dual-stream approaches.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/architecture.webp"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/architecture.webp" alt="Architecture of Z-Image and Z-Image-Edit"/></a></p>

<p dir="auto">According to the Elo-based Human Preference Evaluation (on <a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=T2I" rel="nofollow"><em>Alibaba AI Arena</em></a>), Z-Image-Turbo shows highly competitive performance against other leading models, while achieving state-of-the-art results among open-source models.</p>
<p dir="auto">
  <a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=T2I" rel="nofollow">
    <img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/leaderboard.png" alt="Z-Image Elo Rating on AI Arena"/></a>
</p>

<div dir="auto"><h4 tabindex="-1" dir="auto">(1) PyTorch Native Inference</h4><a id="user-content-1-pytorch-native-inference" aria-label="Permalink: (1) PyTorch Native Inference" href="#1-pytorch-native-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Build a virtual environment you like and then install the dependencies:</p>

<p dir="auto">Then run the following code to generate an image:</p>


<p dir="auto">Install the latest version of diffusers, use the following command:</p>
<details>
  <summary>Click here for details for why you need to install diffusers from source</summary>
<p dir="auto">We have submitted two pull requests (<a href="https://github.com/huggingface/diffusers/pull/12703" data-hovercard-type="pull_request" data-hovercard-url="/huggingface/diffusers/pull/12703/hovercard">#12703</a> and <a href="https://github.com/huggingface/diffusers/pull/12704" data-hovercard-type="pull_request" data-hovercard-url="/huggingface/diffusers/pull/12704/hovercard">#12715</a>) to the ü§ó diffusers repository to add support for Z-Image. Both PRs have been merged into the latest official diffusers release.
Therefore, you need to install diffusers from source for the latest features and Z-Image support.</p>
</details>
<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/huggingface/diffusers"><pre>pip install git+https://github.com/huggingface/diffusers</pre></div>
<p dir="auto">Then, try the following code to generate an image:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import ZImagePipeline

# 1. Load the pipeline
# Use bfloat16 for optimal performance on supported GPUs
pipe = ZImagePipeline.from_pretrained(
    &#34;Tongyi-MAI/Z-Image-Turbo&#34;,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
)
pipe.to(&#34;cuda&#34;)

# [Optional] Attention Backend
# Diffusers uses SDPA by default. Switch to Flash Attention for better efficiency if supported:
# pipe.transformer.set_attention_backend(&#34;flash&#34;)    # Enable Flash-Attention-2
# pipe.transformer.set_attention_backend(&#34;_flash_3&#34;) # Enable Flash-Attention-3

# [Optional] Model Compilation
# Compiling the DiT model accelerates inference, but the first run will take longer to compile.
# pipe.transformer.compile()

# [Optional] CPU Offloading
# Enable CPU offloading for memory-constrained devices.
# pipe.enable_model_cpu_offload()

prompt = &#34;Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (‚ö°Ô∏è), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (Ë•øÂÆâÂ§ßÈõÅÂ°î), blurred colorful distant lights.&#34;

# 2. Generate Image
image = pipe(
    prompt=prompt,
    height=1024,
    width=1024,
    num_inference_steps=9,  # This actually results in 8 DiT forwards
    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models
    generator=torch.Generator(&#34;cuda&#34;).manual_seed(42),
).images[0]

image.save(&#34;example.png&#34;)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>ZImagePipeline</span>

<span># 1. Load the pipeline</span>
<span># Use bfloat16 for optimal performance on supported GPUs</span>
<span>pipe</span> <span>=</span> <span>ZImagePipeline</span>.<span>from_pretrained</span>(
    <span>&#34;Tongyi-MAI/Z-Image-Turbo&#34;</span>,
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>,
    <span>low_cpu_mem_usage</span><span>=</span><span>False</span>,
)
<span>pipe</span>.<span>to</span>(<span>&#34;cuda&#34;</span>)

<span># [Optional] Attention Backend</span>
<span># Diffusers uses SDPA by default. Switch to Flash Attention for better efficiency if supported:</span>
<span># pipe.transformer.set_attention_backend(&#34;flash&#34;)    # Enable Flash-Attention-2</span>
<span># pipe.transformer.set_attention_backend(&#34;_flash_3&#34;) # Enable Flash-Attention-3</span>

<span># [Optional] Model Compilation</span>
<span># Compiling the DiT model accelerates inference, but the first run will take longer to compile.</span>
<span># pipe.transformer.compile()</span>

<span># [Optional] CPU Offloading</span>
<span># Enable CPU offloading for memory-constrained devices.</span>
<span># pipe.enable_model_cpu_offload()</span>

<span>prompt</span> <span>=</span> <span>&#34;Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (‚ö°Ô∏è), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (Ë•øÂÆâÂ§ßÈõÅÂ°î), blurred colorful distant lights.&#34;</span>

<span># 2. Generate Image</span>
<span>image</span> <span>=</span> <span>pipe</span>(
    <span>prompt</span><span>=</span><span>prompt</span>,
    <span>height</span><span>=</span><span>1024</span>,
    <span>width</span><span>=</span><span>1024</span>,
    <span>num_inference_steps</span><span>=</span><span>9</span>,  <span># This actually results in 8 DiT forwards</span>
    <span>guidance_scale</span><span>=</span><span>0.0</span>,     <span># Guidance should be 0 for the Turbo models</span>
    <span>generator</span><span>=</span><span>torch</span>.<span>Generator</span>(<span>&#34;cuda&#34;</span>).<span>manual_seed</span>(<span>42</span>),
).<span>images</span>[<span>0</span>]

<span>image</span>.<span>save</span>(<span>&#34;example.png&#34;</span>)</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">üî¨ Decoupled-DMD: The Acceleration Magic Behind Z-Image</h2><a id="user-content--decoupled-dmd-the-acceleration-magic-behind-z-image" aria-label="Permalink: üî¨ Decoupled-DMD: The Acceleration Magic Behind Z-Image" href="#-decoupled-dmd-the-acceleration-magic-behind-z-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://arxiv.org/abs/2511.22677" rel="nofollow"><img src="https://camo.githubusercontent.com/df7e645e34f1e4fef4424f1a761a971632d46a2654f5b9bed792939ae77ae644/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323531312e32323637372d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2511.22677-b31b1b.svg"/></a></p>
<p dir="auto">Decoupled-DMD is the core few-step distillation algorithm that empowers the 8-step Z-Image model.</p>
<p dir="auto">Our core insight in Decoupled-DMD  is that the success of existing DMD (Distributaion Matching Distillation) methods is the result of two independent, collaborating mechanisms:</p>
<ul dir="auto">
<li><strong>CFG Augmentation (CA)</strong>: The primary <strong>engine</strong> üöÄ driving the distillation process, a factor largely overlooked in previous work.</li>
<li><strong>Distribution Matching (DM)</strong>: Acts more as a <strong>regularizer</strong> ‚öñÔ∏è, ensuring the stability and quality of the generated output.</li>
</ul>
<p dir="auto">By recognizing and decoupling these two mechanisms, we were able to study and optimize them in isolation. This ultimately motivated us to develop an improved distillation process that significantly enhances the performance of few-step generation.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/decoupled-dmd.webp"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/decoupled-dmd.webp" alt="Diagram of Decoupled-DMD"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">ü§ñ DMDR: Fusing DMD with Reinforcement Learning</h2><a id="user-content--dmdr-fusing-dmd-with-reinforcement-learning" aria-label="Permalink: ü§ñ DMDR: Fusing DMD with Reinforcement Learning" href="#-dmdr-fusing-dmd-with-reinforcement-learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://arxiv.org/abs/2511.13649" rel="nofollow"><img src="https://camo.githubusercontent.com/53b1fde83cdd0452453f1999256b182a263be735c4a40420c2d66eafdab7a9eb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323531312e31333634392d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2511.13649-b31b1b.svg"/></a></p>
<p dir="auto">Building upon the strong foundation of Decoupled-DMD, our 8-step Z-Image model has already demonstrated exceptional capabilities. To achieve further improvements in terms of semantic alignment, aesthetic quality, and structural coherence‚Äîwhile producing images with richer high-frequency details‚Äîwe present <strong>DMDR</strong>.</p>
<p dir="auto">Our core insight behind DMDR is that Reinforcement Learning (RL) and Distribution Matching Distillation (DMD) can be synergistically integrated during the post-training of few-step models. We demonstrate that:</p>
<ul dir="auto">
<li><strong>RL Unlocks the Performance of DMD</strong> üöÄ</li>
<li><strong>DMD Effectively Regularizes RL</strong> ‚öñÔ∏è</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongyi-MAI/Z-Image/blob/main/assets/DMDR.webp"><img src="https://github.com/Tongyi-MAI/Z-Image/raw/main/assets/DMDR.webp" alt="Diagram of DMDR"/></a></p>

<ul dir="auto">
<li><a href="https://github.com/vipshop/cache-dit">Cache-DiT</a> offers inference acceleration support for Z-Image with DBCache, Context Parallelism and Tensor Parallelism. Visit their <a href="https://github.com/vipshop/cache-dit/blob/main/examples/parallelism/run_zimage_cp.py">example</a> for more details.</li>
<li><a href="https://github.com/leejet/stable-diffusion.cpp">stable-diffusion.cpp</a> is a pure C++ diffusion model inference engine that supports fast and memory-efficient Z-Image inference across multiple platforms (CUDA, Vulkan, etc.). You can use stable-diffusion.cpp to generate images with Z-Image on machines with as little as 4GB of VRAM. For more information, please refer to <a href="https://github.com/leejet/stable-diffusion.cpp/wiki/How-to-Use-Z%E2%80%90Image-on-a-GPU-with-Only-4GB-VRAM">How to Use Z‚ÄêImage on a GPU with Only 4GB VRAM</a></li>
<li><a href="https://github.com/UnicomAI/LeMiCa">LeMiCa</a> provides a training-free, timestep-level acceleration method that conveniently speeds up Z-Image inference. For more details, see <a href="https://github.com/UnicomAI/LeMiCa/tree/main/LeMiCa4Z-Image">LeMiCa4Z-Image</a>.</li>
<li><a href="https://github.com/HellerCommaA/ComfyUI-ZImageLatent">ComfyUI ZImageLatent</a> provdes an easy to use latent of the official Z-Image resolutions</li>
</ul>

<p dir="auto"><a href="https://www.star-history.com/#Tongyi-MAI/Z-Image&amp;type=date&amp;legend=top-left" rel="nofollow"><img src="https://camo.githubusercontent.com/e4dc55d25622f057d0708871b84a93f929e99f64062b2ff270965fb3beb26a99/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d546f6e6779692d4d41492f5a2d496d61676526747970653d64617465266c6567656e643d746f702d6c656674" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=Tongyi-MAI/Z-Image&amp;type=date&amp;legend=top-left"/></a></p>

<p dir="auto">If you find our work useful in your research, please consider citing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{team2025zimage,
  title={Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer},
  author={Z-Image Team},
  journal={arXiv preprint arXiv:2511.22699},
  year={2025}
}

@article{liu2025decoupled,
  title={Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield},
  author={Dongyang Liu and Peng Gao and David Liu and Ruoyi Du and Zhen Li and Qilong Wu and Xin Jin and Sihan Cao and Shifeng Zhang and Hongsheng Li and Steven Hoi},
  journal={arXiv preprint arXiv:2511.22677},
  year={2025}
}

@article{jiang2025distribution,
  title={Distribution Matching Distillation Meets Reinforcement Learning},
  author={Jiang, Dengyang and Liu, Dongyang and Wang, Zanyi and Wu, Qilong and Jin, Xin and Liu, David and Li, Zhen and Wang, Mengmeng and Gao, Peng and Yang, Harry},
  journal={arXiv preprint arXiv:2511.13649},
  year={2025}
}"><pre><span>@article</span>{<span>team2025zimage</span>,
  <span>title</span>=<span><span>{</span>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Z-Image Team<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2511.22699<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}

<span>@article</span>{<span>liu2025decoupled</span>,
  <span>title</span>=<span><span>{</span>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Dongyang Liu and Peng Gao and David Liu and Ruoyi Du and Zhen Li and Qilong Wu and Xin Jin and Sihan Cao and Shifeng Zhang and Hongsheng Li and Steven Hoi<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2511.22677<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}

<span>@article</span>{<span>jiang2025distribution</span>,
  <span>title</span>=<span><span>{</span>Distribution Matching Distillation Meets Reinforcement Learning<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Jiang, Dengyang and Liu, Dongyang and Wang, Zanyi and Wu, Qilong and Jin, Xin and Liu, David and Li, Zhen and Wang, Mengmeng and Gao, Peng and Yang, Harry<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2511.13649<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>

<p dir="auto">We&#39;re actively looking for <strong>Research Scientists</strong>, <strong>Engineers</strong>, and <strong>Interns</strong> to work on foundational generative models and their applications. Interested candidates please send your resume to: <strong><a href="mailto:jingpeng.gp@alibaba-inc.com">jingpeng.gp@alibaba-inc.com</a></strong></p>
</article></div></div>
  </body>
</html>
