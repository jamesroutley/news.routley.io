<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/1029307/">Original</a>
    <h1>Following Up on the Python JIT</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
<a href="https://lwn.net/Archives/PythonIndex/#Performance">Performance of Python
programs</a> has been a major focus of development for the language over the last
five years or so; the <a href="https://github.com/faster-cpython/">Faster
CPython project</a> has been a big part of that effort.
One of its subprojects is to add an <a href="https://lwn.net/Articles/970397/">experimental just-in-time (JIT) compiler</a> to
the language;  at last year&#39;s PyCon US, project member Brandt Bucher <a href="https://lwn.net/Articles/977855/">gave an introduction</a> to the copy-and-patch JIT
compiler.  At
<a href="https://us.pycon.org/2025/">PyCon US
2025</a>, he followed that up with a talk on &#34;What they don&#39;t tell you
about building a JIT compiler for CPython&#34; to describe some of the things
he wishes he had known when he set out to work on that project.  There
was something of an elephant in the room, however, in that 
<a href="https://www.linkedin.com/posts/mdboom_its-been-a-tough-couple-of-days-microsofts-activity-7328583333536268289-p4Lp/">Microsoft
dropped support for the project</a> and laid off most of its
Faster CPython
team a few days before the talk. 
</p>

<!-- middle-ad -->

<p>
Bucher only alluded to that event in the talk, and elsewhere has <a href="https://discuss.python.org/t/community-stewardship-of-faster-cpython/92153/2">made
it clear</a> that he intends to continue working on the JIT compiler
whatever the fallout.  When he gave the talk back in May, he said that he
had been working with Python for around eight years, as a core developer
for six, part of the Microsoft CPython performance engineering team for
four, and has been working on the JIT compiler for the last two years.
While the team at Microsoft is often equated with the Faster CPython
project, it is really just a part of it; &#34;<q>our team collaborates with
lots of people outside of Microsoft</q>&#34;.
</p>

<h4>Faster CPython results</h4>

<p><a href="https://lwn.net/Articles/1029625/">
<img src="https://static.lwn.net/images/2025/pycon-bucher-sm.png" alt="[Brandt Bucher]" title="Brandt Bucher" width="201" height="280"/>
</a></p><p>
The project has seen some great results over the last few Python releases.
Its work first appeared in 2022 as part of Python 3.11, which averaged 25%
faster than 3.10, depending on the workload; &#34;<q>no need to change your
code, you just upgrade Python and everything works</q>&#34;.  In the years
since, there have been further improvements: Python 3.12 was 4% faster than
3.11, and 3.13 improved by 7% over 3.12.  Python 3.14, which is due in
October, will be around 8% faster than its predecessor.
</p>

<p>
In aggregate, that means Python has gotten nearly 50% faster in less than
four years, he said.  Around 93% of the benchmarks that the project uses
have improved their performance over that time; nearly half (46%) are more
than 50% faster.  20% of the benchmarks are more than 100% faster.  Those
are not simply micro-benchmarks, the benchmarks represent real workloads;
<a href="https://www.pylint.org/">Pylint</a> has gotten 100% faster, for
example.
</p>

<p>
All of those increases have come without the JIT; they come from all of the
<a href="https://lwn.net/Articles/930705/">other changes</a> that the team has been
working on, while &#34;<q>taking a kind of holistic approach to improving
Python performance</q>&#34;.   Those changes have a meaningful impact on
performance and were done in such a way that the community can maintain
them. &#34;<q>This is what happens when companies fund Python core
development</q>&#34;, he said, &#34;<q>it&#39;s a really special thing</q>&#34;.  On his
slides, that was followed by the crying emoji ðŸ˜¢ accompanied by an
uncomfortable laugh.
</p>

<p>
Moving on, he gave a &#34;<a href="https://en.wikipedia.org/wiki/Duck_typing">duck typing</a>&#34; example that he would refer to
throughout the talk.  It revolved around a duck simulator that would take
an <a href="https://wiki.python.org/moin/Iterator">iterator</a> of ducks and &#34;quack&#34; each one, then print the sound. As an additional feature, if a
duck has an &#34;echo&#34; attribute that evaluates to true, it would double the sound:
</p><pre>    def simulate_ducks(ducks):
        for duck in ducks:
            sound = duck.quack()
            if duck.echo:
                sound += sound
            print(sound)
</pre><p>
That was coupled with two classes that produced different sounds:
</p><pre>    class Duck:
        echo = False
        
        def quack(self):
            return &#34;Quack!&#34;

    class RubberDuck:
        echo = True

        def __init__(self, loud):
            self.loud = loud
            
        def quack(self):
            if self.loud:
                return &#34;SQUEAK!&#34;
            return &#34;Squeak!&#34;
</pre>


<p>
He stepped through an example execution of the loop in
<tt>simulate_ducks()</tt>.  He showed the bytecode for the stack-based
Python virtual machine that was generated by the interpreter and stepped
through one iteration of the loop describing the changes to the stack and
to the <tt>duck</tt> and <tt>sound</tt> local variables.  That process is
largely unchanged &#34;<q>since Python
was first created</q>&#34;.
</p>

<h4>Specialization</h4>

<p>
The 3.11 interpreter added specialized bytecode into the mix, where some of
the bytecode operations are changed to assume they are using a
specific typeâ€”chosen based on observing the execution of the code
a few times.  Python is a dynamic language, so the interpreter always needs
to be able to fall back to, say, looking up the proper binary operator for the
types. But after running the loop a few times, it can assume that
&#34;<tt>sound += sound</tt>&#34; will be 
operating on strings so it can switch to a bytecode with a fast path for
that explicit operation. &#34;<q>You actually have bytecode that can still
handle anything, but has inlined fast paths for the shape of your actual
objects and data structures and memory layout.</q>&#34;
</p>

<p>
All of that underlies the JIT compiler, which uses the specialized bytecode
interpreter, and can be viewed as being part of the same pipeline, Bucher
said.  The JIT compiler is not enabled by default in any build of Python,
however.  As he described in last year&#39;s talk, the specialized bytecode
instructions get further broken down into micro-ops, which are &#34;<q>smaller
units of work within an individual bytecode instruction</q>&#34;.  The
translation to micro-ops is completely automatic because the bytecodes are
defined in terms of them, &#34;<q>so this translation step is machine-generated
and very very fast</q>&#34;, he said.
</p>

<p>
The micro-ops can be optimized, that is basically the whole point of
generating them, he said.  Observing the different types and values that
are being encountered when executing through the micro-ops will show
optimizations that can be applied.  Some micro-ops can be replaced with
more efficient versions, others can be eliminated because they &#34;<q>are
doing work that is entirely redundant and that we can prove we can remove
without changing the semantics</q>&#34;.  He showed a slide full of micro-ops
that corresponded to the duck loop and slowly replaced and eliminated
something approaching 25% of them, which corresponds to what the 3.14
version of the JIT does.
</p>

<p>
The JIT will then translate the micro-ops into machine code one-by-one, but
it does so using the copy-and-patch mechanism.  The machine-code templates
for each of the micro-ops are generated at CPython compile time; it is
somewhat analogous to the way the micro-ops themselves are generated in a
table-driven fashion.  Since the templates are not hand-written, fixing
bugs in the micro-ops for the rest of the interpreter also fixes them for
the JIT; that helps with the maintainability of the JIT, but also helps lower
the barrier to entry for working on it, Bucher said.
</p>

<h4>Region selection</h4>

<p>
With that background out of the way, he moved on to some &#34;<q>interesting
parts of working on a JIT compiler</q>&#34; that are often overlooked, starting
with region selection.  Earlier, he had shown a sequence of micro-ops that
needed to be turned into machine code, but he did not describe how that
list was generated; &#34;<q>how did we get there in the first place?</q>&#34;
</p>

<p>
The JIT compiler does not start off with such a sequence, it starts with
code like in his duck simulation.  There are several questions that need to
be answered about that code based on its run-time activity. The first is:
&#34;<q>what do we want to compile?</q>&#34; If something
is running only a few times, it is not a good candidate for JIT
compilation, but something that is running a lot is. Another question is
where should it be compiled?  A function can be compiled in isolation or it
can be inlined into its callers and those can be compiled instead.  
</p>

<p>
When should the code be compiled?  There is a balance to be struck between
compiling things too early, wasting that effort because the code is not
actually running all that much, and too late, which may not actually make
the program any faster.  The final question is &#34;why?&#34;, he said; it only
makes sense to compile code if it is clear that compiling will make the
code more efficient. &#34;<q>If they are using really dynamic code patterns or
doing weird things that we don&#39;t actually compile well, then it&#39;s probably
not worth it.</q>&#34;
</p>

<p>
One approach that can be taken is to compile entire functions, which is
known as &#34;method at a time&#34; or &#34;method JIT&#34;.  It &#34;<q>maps naturally to the
way we think about compilers</q>&#34; because it is the way that many
ahead-of-time compilers work.  So, when the JIT looks at
<tt>simulate_ducks()</tt>, it can just compile the entire function (the
<tt>for</tt> loop) wholesale, but there are some other opportunities for
optimization.  If it recognizes that most of the time the loop operates on
<tt>Duck</tt> objects, it can inline the <tt>quack()</tt> function from it:
</p><pre>    for duck in ducks:
        if duck.__class__ is Duck:
            sound = &#34;Quack!&#34;
        else:
            sound = duck.quack()
        ...
</pre><p>
If there are lots of </p><tt>RubberDuck</tt><p> objects too, that class&#39;s
</p><tt>quack()</tt><p> method could be inlined as well.   Likewise, the attribute
lookup for </p><tt>duck.echo</tt><p> could be inlined for one or both cases, but
that all starts to get somewhat complicated, he said; &#34;<q>it&#39;s not always
super-easy to reason about, especially for something that is running while
you are compiling it</q>&#34;. 
</p>

<p>
Meanwhile, what if <tt>ducks</tt> is not a list, but is instead a <a href="https://wiki.python.org/moin/Generators">generator</a>?  In simple
cases, with a single <a href="https://docs.python.org/3/reference/expressions.html#yieldexpr"><tt>yield</tt>
expression</a>, it is not that much different from the list case, but with
multiple <tt>yield</tt> expressions and loops in the generator, it also
becomes hard to reason about.  That creates a kind of optimization barrier
and that kind of code is not uncommon, especially in <a href="https://lwn.net/Articles/726600/">asynchronous
programming contexts</a>. 
</p>

<p>
Another technique, and the one that is currently used in the CPython JIT,
is to use a &#34;tracing JIT&#34; instead of a method JIT.  The technique takes
linear traces of the program&#39;s execution, so it can use that information to
make optimization decisions.  If the first <tt>duck</tt> is a
<tt>Duck</tt>, the code can be optimized as it was earlier, with a guard
based on the class and inlining the <tt>sound</tt> assignment.  Next up is
a lookup for <tt>duck.echo</tt>, but the code in the guarded branch has
perfect type information; it already knows that it is processing a
<tt>Duck</tt>, so it knows <tt>echo</tt> is false, and that <tt>if</tt> can
be removed, leaving:
</p><pre>    for duck in ducks:
        if duck.__class__ is Duck:
            sound = &#34;Quack!&#34;
            print(sound)
</pre><p>
&#34;<q>This is pretty efficient.  If you have just a list of <tt>Duck</tt>s,
you&#39;re going to be doing kind of the bare minimum amount of work to
actually quack all those ducks.</q>&#34;
</p>

<p>
The code still needs to handle the case where the <tt>duck</tt> is not a
<tt>Duck</tt>, but it does not need to compile that piece; it can, instead,
just send it back to the interpreter if the class guard is false.  If the
code is also handling <tt>RubberDuck</tt> objects, though, eventually that
<tt>else</tt> branch will get &#34;hot&#34; because it is being taken frequently.
</p>

<p>
At that point, the tracing can be turned back on to see what the code is
doing.  If we assume that it mostly has non-<tt>loud</tt>
<tt>RubberDuck</tt> objects, the resulting code might look like:
</p><pre>    elif duck.__class__ is RubberDuck:
        if self.loud: ...
        sound = &#34;Squeak!Squeak!&#34;
        print(sound)
    else: ...
</pre><p>
The two branches that are not specified would simply return to the regular
interpreter when they are executed.  Since the tracing has perfect type
information, it knows that </p><tt>echo</tt><p> is true, so the sound should be
doubled, but there is no need to actually use &#34;</p><tt>+=</tt><p>&#34; to get the
result.  So, now the function has the minimum necessary code to quack
either a </p><tt>Duck</tt><p> or a non-</p><tt>loud</tt> <tt>RubberDuck</tt><p>.  If
those other branches start getting hot at some point, tracing can once
again be used optimize it further.
</p>

<p>
One downside of the tracing JIT approach is that it can compile duplicates
of the same code, as with &#34;<tt>print(sound)</tt>&#34;.  In &#34;<q>very branchy
code</q>&#34; Bucher said, &#34;<q>some things near the tail of those traces can be
duplicated quite a bit</q>&#34;.  There are ways to reduce that duplication,
but it is a downside to the technique.
</p>

<p>
Another technique for selecting regions is called &#34;meta tracing&#34;, but he
did not have time to go into it.  He suggested that attendees ask their LLM
of choice &#34;<q>about the &#39;first Futamura projection&#39; and don&#39;t misspell it
like me, it&#39;s not &#39;Futurama&#39;</q>&#34;, Bucher said to some chuckles around the
room. 
</p>

<h4>Memory management</h4>

<p>
JIT compilers &#34;<q>do really weird things with memory</q>&#34;.  C programmers
are familiar with readable (or read-only) data, such as a <tt>const</tt>
array, and data that is both readable and writable is the normal case.
Memory can be dynamically allocated using <a href="https://www.man7.org/linux/man-pages/man3/malloc.3.html"><tt>malloc()</tt></a>, but that kind
of memory cannot be executed; since a JIT compiler needs memory that it can
read, write, <i>and</i> execute, it requires &#34;<q>the big guns</q>&#34;: <a href="https://man7.org/linux/man-pages/man2/mmap.2.html"><tt>mmap()</tt></a>.
&#34;<q>If you know the right magic incantation, you can whisper to this thing
with all these secret flags and numbers</q>&#34; to get memory that is
readable, writable, and executable:
</p><pre>    char *data = mmap(NULL, 4096,
                      PROT_READ | PROT_WRITE | PROT_EXEC,
                      MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
</pre><p>
One caveat is that memory from </p><tt>mmap()</tt><p> comes in page-sized chunks,
which is 4KB on most systems but can be larger.  If the JIT code is, say,
four bytes in length, that can be wasteful, so it needs to be managed
carefully.  Once you have that memory, he asked, how do you actually
execute it?  It turns out that &#34;<q>C lets us do crazy things</q>&#34;:
</p><pre>    typedef int (*function)(int);
    ((function)data)(42);
</pre><p>
That first line creates a type definition named &#34;</p><tt>function</tt><p>&#34;, which
is a pointer to a function that takes an integer argument and returns an
integer.  The second line casts the </p><tt>data</tt><p> pointer to that type and
then calls the function with an argument of 42 (and ignores the return
value). &#34;<q>It&#39;s weird, but it works.</q>&#34;
</p>

<p>
He noted that the term &#34;executable data&#34; should be setting off alarm bells
in people&#39;s heads; &#34;<q>if you&#39;re a Rust programmer, this is what we call
&#39;unsafe code&#39;</q>&#34; he said to laughter.  Being able to write to memory that
can be executed is &#34;<q>a scary thing; at best you shoot yourself in the
foot, at worst it is a major security vulnerability</q>&#34;.  For this reason,
operating systems often require that memory not be in that state.  He said
that the memory should be mapped readable and writable, then filled in, and
switched to readable and executable using <a href="https://man7.org/linux/man-pages/man2/mprotect.2.html"><tt>mprotect()</tt></a>;
if there is a need to modify the data later, it can be switched back and
forth between the two states.
</p>

<h4>Debugging and profiling</h4>

<p>
When code is being profiled using one of the <a href="https://docs.python.org/3/library/profile.html">Python profilers</a>,
code that has been compiled should call all of the same profiling hooks.
The easiest way to do that, at least for now, is to not JIT code that has
profiler hooks installed.  In recent versions of Python, profiling is
implemented by using the specializing adaptive interpreter to change
certain bytecodes to other, instrumented versions of them, which will call
the profiler hooks.  If the tracing encounters one of these instrumented
bytecodes, it can shut the JIT down for that part of the code, but it can
still run in other, non-profiled parts of the code.
</p>

<p>
A related problem occurs when someone enables profiling for code that has
already been JIT-compiled.  In that case, Python needs to get out of the
JIT code as quickly as possible.  That is handled by placing special
<tt>_CHECK_VALIDITY</tt> micro-ops just before &#34;<q>known safe points</q>&#34;
where it can jump out of the JIT code and back to the interpreter.  That
micro-op checks a one-bit flag; if it is set, the execution bails out of
the JIT code.  That bit gets set when profiling is enabled, but it is also
used when code executes that could change the JIT optimizations (e.g. a
change of class attributes).
</p>

<p>
Something that just kind of falls out of that is the ability to support &#34;<q>the
weirder features of Python debuggers</q>&#34;.  The JIT code is created based
on what the tracing has seen, but someone running <a href="https://docs.python.org/3/library/pdb.html"><tt>pdb</tt></a> could
completely upend that state in various ways (e.g. &#34;<tt>duck =
Goose()</tt>&#34;).  The validity bit can be used to avoid problems of that
sort as well.
</p>

<p>
For native profilers and debuggers, such as perf and GDB, there is a need
to unwind the stack through JIT frames, and interact with JIT frames, but
&#34;<q>the short answer is that it&#39;s really really complicated</q>&#34;.  There
are lots of tools of this sort, for various platforms, that all work
differently and each has its own APIs for registering debug information in
different formats.  The project members are aware of the problem, but are
trying to determine which tools need to be supported and what level of
support they actually need.
</p>

<h4>Looking ahead</h4>

<p>
The current Python release is 3.13; the JIT can be built into it by using
the <tt>--enable-experimental-jit</tt> flag.  For Python 3.14, which is out
in beta form and will be released in October, the Windows and macOS builds
have the JIT built-in, but it must be enabled by setting
<tt>PYTHON_JIT=1</tt> in the environment.  He does not recommend enabling
it for production code, but the team would love to hear about any results
from using it: dramatic improvements or slowdowns, bugs, crashes, and so
on.  Other platforms, or people creating their own binaries, can enable the
JIT with the same flag as for 3.13.
</p>

<p>
For 3.15, which is in a pre-alpha stage at this point, there are two GitHub
issues they are focusing on: &#34;<a href="https://github.com/python/cpython/issues/126910">Supporting stack
unwinding in the JIT compiler</a>&#34; and &#34;<a href="https://github.com/python/cpython/issues/133171">Make the JIT
thread-safe</a>&#34;.  The first he had mentioned earlier with regard to
support for native debuggers and profilers.  The second is important since
the free-threaded build of CPython seems to be working out well and is
moving toward becoming the defaultâ€”see <a href="https://peps.python.org/pep-0779/">PEP 779</a> (&#34;Criteria for
supported status for free-threaded Python&#34;), which was recently <a href="https://discuss.python.org/t/pep-779-criteria-for-supported-status-for-free-threaded-python/84319/123">accepted
by the steering council</a>.  The Faster CPython developers think that
making the JIT thread-safe can be done without too much trouble; &#34;<q>it&#39;s
going to take a little bit of work and there&#39;s kind of a long tail of
figuring out what optimizations are actually still safe to do in a
free-threaded environment</q>&#34;.  Both of those issues are outside of his
domain of expertise, however, so he hoped that others who have those skills
would be willing to help out.
</p>

<p>
In addition, there is a lot of ongoing performance work that is going into
the 3.15 branch, of course.  He noted, pointedly, that fast progress,
especially on larger projects, will depend on the availability of
resources.  The words on his slide saying that changed to bold and he gave
a significant cough to further emphasize the point.
</p>

<p>
As he wrapped up, he suggested <a href="https://peps.python.org/pep-0659/">PEP
659</a> (&#34;Specializing Adaptive Interpreter&#34;) and <a href="https://peps.python.org/pep-0744/">PEP 744</a> (&#34;JIT Compilation&#34;)
for further information.  For those who would rather watch something,
instead of reading about it, he recommended videos of his talks (covered
by LWN and linked above) from 2023 <a href="https://www.youtube.com/watch?v=shQtrn1v7sQ">on the specializing
adaptive interpreter</a> and from 2024 on <a href="https://www.youtube.com/watch?v=kMO3Ju0QCDo&amp;t=3s">adding a JIT
compiler</a>.  The
<a href="https://www.youtube.com/watch?v=NE-Oq8I3X_w">YouTube video of this
year&#39;s talk</a> is available as well.
</p>

<p>
[Thanks to the Linux Foundation for its travel sponsorship that allowed me to travel to Pittsburgh for PyCon US.]
</p></div></div>
  </body>
</html>
