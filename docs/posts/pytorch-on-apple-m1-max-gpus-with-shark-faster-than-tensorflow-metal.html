<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nod.ai/pytorch-m1-max-gpu/">Original</a>
    <h1>PyTorch on Apple M1 MAX GPUs with SHARK – faster than TensorFlow-Metal</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<p><strong>SHARK</strong> is a portable High Performance Machine Learning Runtime for PyTorch. </p>



<p>In this blog we demonstrate <strong>PyTorch Training and Inference on the Apple M1Max GPU with SHARK</strong> with only a few lines of additional code and outperforming Apple’s <a href="https://developer.apple.com/metal/tensorflow-plugin/">Tensorflow-metal</a> plugin.  Though Apple has released GPU support for Tensorflow via the now deprecated <a rel="noreferrer noopener" href="https://github.com/apple/tensorflow_macos" target="_blank">tensorflow-macos</a> plugin and the newer <a rel="noreferrer noopener" href="https://developer.apple.com/metal/tensorflow-plugin/" target="_blank">tensorflow-metal</a> plugin the most popular machine learning framework <a rel="noreferrer noopener" href="https://github.com/pytorch/pytorch/issues/47702" target="_blank">PyTorch lacks GPU</a> support on Apple Silicon. Until now.</p>



<p>SHARK is built on <a href="https://mlir.llvm.org/" target="_blank" rel="noreferrer noopener">MLIR</a> and <a href="https://google.github.io/iree/" target="_blank" rel="noreferrer noopener">IREE</a> and can target various hardware seamlessly. Since SHARK generates kernels on the fly for each workload you can port to new architectures like the M1Max without the vendor provided handwritten / hand tuned library. </p>



<p><a rel="noreferrer noopener" href="https://github.com/google/iree/tree/main/experimental/rocm" target="_blank">Nod.ai has added AMD GPU support </a>to be able to retarget the code generation for AMD MI100/MI200 class devices and move machine learning workloads from Nvidia V100/A100 to AMD MI100 seamlessly. In the past <a rel="noreferrer noopener" href="https://nod.ai/outperforming-octoml-apache-tvm-and-intel-mkl/" data-type="post" data-id="5749" target="_blank">we demonstrated better codegen than Intel MKL and Apache/OctoML TVM on Intel Alderlake CPUs</a> and <a rel="noreferrer noopener" href="https://nod.ai/shark-the-fastest-runtime/" data-type="post" data-id="5772" target="_blank">outperforming Nvidia’s cuDNN/cuBLAS/CUTLASS used by ML frameworks such as Onnxruntime, Pytorch/Torchscript and Tensorflow/XLA</a>. Today we demonstrate SHARK targeting  Apple’s 32 Core GPU in the M1Max with PyTorch Models for BERT Inference and Training.  So if you love PyTorch and want to use those 32 GPU cores in your new Apple Silicon Macbook Pro read on. </p>



<p>For our experiment we will utilize a 14″ MacBook Pro with the Apple M1 Max with 64GB RAM. We have also run the same benchmarks on a 16″ MacBook Pro and notice the same performance and both don’t thermally throttle during our benchmarks.  </p>







<figure><img width="1024" height="521" src="https://nod.ai/wp-content/uploads/2022/02/image-1-1024x521.png" alt="" srcset="https://nod.ai/wp-content/uploads/2022/02/image-1-1024x521.png 1024w, https://nod.ai/wp-content/uploads/2022/02/image-1-300x153.png 300w, https://nod.ai/wp-content/uploads/2022/02/image-1-768x391.png 768w, https://nod.ai/wp-content/uploads/2022/02/image-1-1536x782.png 1536w, https://nod.ai/wp-content/uploads/2022/02/image-1-2048x1043.png 2048w, https://nod.ai/wp-content/uploads/2022/02/image-1-2000x1018.png 2000w, https://nod.ai/wp-content/uploads/2022/02/image-1-1000x509.png 1000w, https://nod.ai/wp-content/uploads/2022/02/image-1-1800x917.png 1800w, https://nod.ai/wp-content/uploads/2022/02/image-1-1100x560.png 1100w, https://nod.ai/wp-content/uploads/2022/02/image-1-600x306.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>SHARK on Apple M1MAX GPU</figcaption></figure>







<p>Here is the output of running the shark-bench tool on the microsoft/MiniLM-L12-H384 model. </p>



<pre><code>(base) anush@MacBook-Pro examples % ./shark-bench --module_file=minilm_jan6_m1max.vmfb --entry_function=predict  --function_input=1x128xi32 --function_input=1x128xi32 --function_input=1x128xi32 --benchmark_repetitions=10
2022-02-20T10:17:55-08:00
Running ./shark-bench
Run on (10 X 24.1214 MHz CPU s)
CPU Caches:
  L1 Data 64 KiB (x10)
  L1 Instruction 128 KiB (x10)
  L2 Unified 4096 KiB (x5)
Load Average: 2.14, 1.91, 1.81
-----------------------------------------------------------------------------------
Benchmark                                         Time             CPU   Iterations
-----------------------------------------------------------------------------------
BM_predict/process_time/real_time              11.7 ms         1.39 ms           58
BM_predict/process_time/real_time              11.5 ms         1.34 ms           58
BM_predict/process_time/real_time              11.7 ms         1.43 ms           58
BM_predict/process_time/real_time              11.6 ms         1.30 ms           58
BM_predict/process_time/real_time              11.5 ms         1.33 ms           58
BM_predict/process_time/real_time              11.7 ms         1.46 ms           58
BM_predict/process_time/real_time              11.6 ms         1.31 ms           58
BM_predict/process_time/real_time              11.5 ms         1.33 ms           58
BM_predict/process_time/real_time              11.7 ms         1.46 ms           58
BM_predict/process_time/real_time              11.6 ms         1.30 ms           58
<strong>BM_predict/process_time/real_time_mean         11.6 ms </strong>        1.36 ms           10
BM_predict/process_time/real_time_median       11.6 ms         1.34 ms           10
BM_predict/process_time/real_time_stddev      0.074 ms        0.063 ms           10
BM_predict/process_time/real_time_cv           0.64 %          4.65 %            10
(base) anush@MacBook-Pro examples % 
</code></pre>



<pre><code>Arguments: Namespace(models=[&#39;microsoft/MiniLM-L12-H384-uncased&#39;], model_source=&#39;pt&#39;, model_class=None, engines=[&#39;tensorflow&#39;], cache_dir=&#39;./cache_models&#39;, onnx_dir=&#39;./onnx_models&#39;, use_gpu=True, precision=&lt;Precision.FLOAT32: &#39;fp32&#39;&gt;, verbose=False, overwrite=False, optimize_onnx=False, validate_onnx=False, fusion_csv=&#39;fusion.csv&#39;, detail_csv=&#39;detail.csv&#39;, result_csv=&#39;result.csv&#39;, input_counts=[1], test_times=1000, batch_sizes=[1], sequence_lengths=[128], disable_ort_io_binding=False, num_threads=[10])
Metal device set to: Apple M1 Max

systemMemory: 64.00 GB
maxCacheSize: 24.00 GB

All model checkpoint layers were used when initializing TFBertModel.

All the layers of TFBertModel were initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
<strong>Run Tensorflow on microsoft/MiniLM-L12-H384-uncased</strong> with input shape [1, 128]
{&#39;engine&#39;: &#39;tensorflow&#39;, &#39;version&#39;: &#39;2.8.0&#39;, &#39;device&#39;: &#39;cuda&#39;, &#39;optimizer&#39;: &#39;&#39;, &#39;precision&#39;: &lt;Precision.FLOAT32: &#39;fp32&#39;&gt;, &#39;io_binding&#39;: &#39;&#39;, &#39;model_name&#39;: &#39;microsoft/MiniLM-L12-H384-uncased&#39;, &#39;inputs&#39;: 1, &#39;threads&#39;: 10, &#39;batch_size&#39;: 1, &#39;sequence_length&#39;: 128, &#39;datetime&#39;: &#39;2022-02-21 07:48:04.965312&#39;, &#39;test_times&#39;: 1000, &#39;latency_variance&#39;: &#39;0.00&#39;, &#39;latency_90_percentile&#39;: &#39;19.25&#39;, &#39;latency_95_percentile&#39;: &#39;22.10&#39;, &#39;latency_99_percentile&#39;: &#39;23.14&#39;, &#39;<strong>average_latency_ms&#39;: &#39;16.99&#39;</strong>, &#39;QPS&#39;: &#39;58.85&#39;}
</code></pre>







<h2 id="shark-on-m1-cpu-2x-faster-than-tf-1-5x-faster-than-pytorch-onnx">SHARK on M1 CPU: 2x faster than TF, ~1.5x faster than PyTorch, ONNX</h2>



<figure><img loading="lazy" width="1024" height="521" src="https://nod.ai/wp-content/uploads/2022/02/image-2-1024x521.png" alt="" srcset="https://nod.ai/wp-content/uploads/2022/02/image-2-1024x521.png 1024w, https://nod.ai/wp-content/uploads/2022/02/image-2-300x153.png 300w, https://nod.ai/wp-content/uploads/2022/02/image-2-768x391.png 768w, https://nod.ai/wp-content/uploads/2022/02/image-2-1536x782.png 1536w, https://nod.ai/wp-content/uploads/2022/02/image-2-2048x1043.png 2048w, https://nod.ai/wp-content/uploads/2022/02/image-2-2000x1018.png 2000w, https://nod.ai/wp-content/uploads/2022/02/image-2-1000x509.png 1000w, https://nod.ai/wp-content/uploads/2022/02/image-2-1800x917.png 1800w, https://nod.ai/wp-content/uploads/2022/02/image-2-1100x560.png 1100w, https://nod.ai/wp-content/uploads/2022/02/image-2-600x306.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>SHARK on Apple M1MAX CPUs</figcaption></figure>



<p>Here is a video of the demo runs. </p>



<figure><p>
<iframe loading="lazy" title="SHARK M1MAX GPU BERT Inference" width="930" height="523" src="https://www.youtube.com/embed/fheOHhZj6IM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p><figcaption>SHARK BERT Inference on M1Max at 1.5X Performance of TF-Metal</figcaption></figure>











<h2 id="bert-training">BERT Training</h2>



<h2 id="bert-minilm-inference">SHARK Runtime: 2x faster than TF-metal 2.8</h2>



<figure><img loading="lazy" width="1024" height="521" src="https://nod.ai/wp-content/uploads/2022/02/image-3-1024x521.png" alt="" srcset="https://nod.ai/wp-content/uploads/2022/02/image-3-1024x521.png 1024w, https://nod.ai/wp-content/uploads/2022/02/image-3-300x153.png 300w, https://nod.ai/wp-content/uploads/2022/02/image-3-768x391.png 768w, https://nod.ai/wp-content/uploads/2022/02/image-3-1536x782.png 1536w, https://nod.ai/wp-content/uploads/2022/02/image-3-2048x1043.png 2048w, https://nod.ai/wp-content/uploads/2022/02/image-3-2000x1018.png 2000w, https://nod.ai/wp-content/uploads/2022/02/image-3-1000x509.png 1000w, https://nod.ai/wp-content/uploads/2022/02/image-3-1800x917.png 1800w, https://nod.ai/wp-content/uploads/2022/02/image-3-1100x560.png 1100w, https://nod.ai/wp-content/uploads/2022/02/image-3-600x306.png 600w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>SHARK Training on Apple M1MAX GPU</figcaption></figure>



<p>In our tests we noticed the Tensorflow-metal plugin doesn’t seem to offload the backwards graph onto the GPU efficiently. Since Tensorflow-metal is a binary only release we have no way to debug it. The same <a href="https://github.com/NodLabs/shark-samples/blob/main/ModelCompiler/nlp_models/bert_small_tf_run.py">Tensorflow implementation</a> works well to offload onto CUDA GPUs.</p>



<pre><code>(base) anush@MacBook-Pro examples % ./shark-bench --module_file=bert_training_feb17.vmfb --function_input=1x512xi32 --function_input=1x512xi32 --function_input=1x512xi32 --function_input=1xi32 --entry_function=learn --benchmark_repetitions=10
2022-02-20T23:04:22-08:00
Running ./shark-bench
Run on (10 X 24.2416 MHz CPU s)
CPU Caches:
  L1 Data 64 KiB (x10)
  L1 Instruction 128 KiB (x10)
  L2 Unified 4096 KiB (x5)
Load Average: 2.03, 2.53, 2.31
---------------------------------------------------------------------------------
Benchmark                                       Time             CPU   Iterations
---------------------------------------------------------------------------------
BM_learn/process_time/real_time               104 ms         16.2 ms            5
BM_learn/process_time/real_time               104 ms         16.1 ms            5
BM_learn/process_time/real_time               105 ms         15.3 ms            5
BM_learn/process_time/real_time               104 ms         16.0 ms            5
BM_learn/process_time/real_time               103 ms         15.2 ms            5
BM_learn/process_time/real_time               105 ms         16.9 ms            5
BM_learn/process_time/real_time               104 ms         15.5 ms            5
BM_learn/process_time/real_time               104 ms         14.9 ms            5
BM_learn/process_time/real_time               105 ms         17.4 ms            5
BM_learn/process_time/real_time               105 ms         15.7 ms            5
<strong>BM_learn/process_time/real_time_mean          104 ms </strong>        15.9 ms           10
BM_learn/process_time/real_time_median        104 ms         15.8 ms           10
BM_learn/process_time/real_time_stddev      0.830 ms        0.774 ms           10
BM_learn/process_time/real_time_cv           0.80 %          4.86 %            10
(base) anush@MacBook-Pro examples % 
</code></pre>



<pre><code>(base) anush@MacBook-Pro nlp_models % python ./bert_small_tf_run.py
Metal device set to: Apple M1 Max

systemMemory: 64.00 GB
maxCacheSize: 24.00 GB

2022-02-21 07:58:36.593857: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-02-21 07:58:36.594010: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
1 Physical GPUs, 1 Logical GPU
Model: &#34;bert_classifier&#34;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_word_ids (InputLayer)    [(None, None)]       0           []                               
                                                                                                  
 input_mask (InputLayer)        [(None, None)]       0           []                               
                                                                                                  
 input_type_ids (InputLayer)    [(None, None)]       0           []                               
                                                                                                  
 bert_encoder_1 (BertEncoder)   [(None, None, 768),  15250176    [&#39;input_word_ids[0][0]&#39;,         
                                 (None, 768)]                     &#39;input_mask[0][0]&#39;,             
                                                                  &#39;input_type_ids[0][0]&#39;]         
                                                                                                  
 dropout_1 (Dropout)            (None, 768)          0           [&#39;bert_encoder_1[0][1]&#39;]         
                                                                                                  
 sentence_prediction (Classific  (None, 5)           3845        [&#39;dropout_1[0][0]&#39;]              
 ationHead)                                                                                       
                                                                                                  
==================================================================================================
Total params: 15,254,021
Trainable params: 15,254,021
Non-trainable params: 0
__________________________________________________________________________________________________
...
time: 1.7728650569915771
<strong>time/iter: 0.19698500633239746</strong></code></pre>



<figure><p>
<iframe loading="lazy" title="SHARK M1MAX GPU BERT Training" width="930" height="523" src="https://www.youtube.com/embed/l1zXDnglXfE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>















<h3 id="performance-watt-a100-vs-m1max">PERFORMANCE / WATT </h3>



<h4 id="performance-watt-a100-vs-m1max">A100 vs M1MAX for BERT Training</h4>



<p>Power measurements need to be done in a very controlled and instrumented environment. However this is a good approximation for running the same BERT training model on an A100 and M1MAX. </p>



<p>The A100 draws 131W peak during the training run, the M1MAX GPU draws a maximum of 15.4 W. The A100 runs an iteration at 7ms vs 104ms (with SHARK) and 196ms (with TF-Metal). Since TF-Metal doesn’t offload the Training graph well onto the GPU we remove it from our perf/watt comparisons.  </p>



<pre><code>Mon Feb 21 00:56:33 2022                                                       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   36C    P0   <strong>131W / 350W</strong> |  39341MiB / 40536MiB |     <strong>53%</strong>      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+</code></pre>



<pre><code>samples/ModelCompiler/nlp_models$ python bert_small_tf_run.py
..
Model: &#34;bert_classifier&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_word_ids (InputLayer)     [(None, None)]       0
__________________________________________________________________________________________________
input_mask (InputLayer)         [(None, None)]       0
__________________________________________________________________________________________________
input_type_ids (InputLayer)     [(None, None)]       0
__________________________________________________________________________________________________
bert_encoder_1 (BertEncoder)    [(None, None, 768),  15250176    input_word_ids[0][0]
                                                                 input_mask[0][0]
                                                                 input_type_ids[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           bert_encoder_1[0][1]
__________________________________________________________________________________________________
sentence_prediction (Classifica (None, 5)            3845        dropout_1[0][0]
==================================================================================================
Total params: 15,254,021
Trainable params: 15,254,021
Non-trainable params: 0
__________________________________________________________________________________________________
..                                                                            
time: 6.907362699508667
<strong>time/iter: 0.006977134039907744</strong></code></pre>



<pre><code><strong>Package Power: 2.48W (avg: 2.34W peak: 37.78W) </strong>throttle: no          
<strong>CPU: 1.37W (avg: 1.03W peak: 32.32W)  </strong>  <strong>GPU: 0.00W (avg: 0.00W peak: 15.41W)  </strong>                </code></pre>



<h2 id="software">SOFTWARE</h2>



<p>The SHARK Runtime is available as pip package and requires a few lines of code changes in your Pytorch/Python file. Here is an example of running <a href="https://github.com/NodLabs/shark-samples/blob/main/examples/resnet50.py">Resnet50</a> and <a href="https://github.com/NodLabs/shark-samples/blob/main/examples/bert.py">BERT</a> from Python using SHARK and we plan to add more Python examples there.  For the experiments in the post we will use a benchmark binary built with <a rel="noreferrer noopener" href="https://github.com/google/benchmark" target="_blank">Google Benchmark</a> support to run a controlled experiment though calling from Python adds a few microseconds.  We test a <a href="https://huggingface.co/microsoft/MiniLM-L12-H384-uncased">BERT MiniLM Model</a> (used in <a href="https://www.youtube.com/watch?v=jiftCAhOYQA">HuggingFace’s Infinity demos</a>) and a <a rel="noreferrer noopener" href="https://huggingface.co/bert-base-uncased" target="_blank">BERT Training model</a>. You can also try <a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a>, Resnet50, Mobilenetv3 etc and all the models are automatically offloaded to the GPU via torch-mlir. </p>



<p>The Torch-MLIR lowering for the BERT training graph is being integrated in this <a href="https://github.com/llvm/torch-mlir/tree/bert-staging">staging branch</a>.  All the code the recreate the tests are <a rel="noreferrer noopener" href="https://github.com/NodLabs/shark-samples/tree/main/examples" target="_blank">here</a> and <a href="https://github.com/NodLabs/shark-samples/blob/main/ModelCompiler/nlp_models/bert_small_tf_run.py">here</a>. However you will need to install PyTorch <code>torchvision</code> from source since torchvision <a rel="noreferrer noopener" href="https://github.com/pytorch/vision/issues/5171" target="_blank">doesn’t have support for M1 yet.</a> You will also need to build SHARK from the <em>apple-m1-max-support</em> branch from the <a rel="noreferrer noopener" href="https://github.com/nodlabs/shark/tree/AppleM1" target="_blank">SHARK repository.</a> </p>



<pre><code>(base) anush@MacBook-Pro examples % pip list | grep tensorflow
tensorflow-estimator          2.6.0
tensorflow-macos              2.8.0
tensorflow-metal              0.3.0
(base) anush@MacBook-Pro examples % pip list | grep onnx                                            
onnx                          1.10.1
onnxconverter-common          1.8.1
(base) anush@MacBook-Pro examples % pip list | grep tensor
tensorboard                   2.8.0
tensorboard-data-server       0.6.0
tensorboard-plugin-wit        1.8.0
tensorflow-estimator          2.6.0
tensorflow-macos              2.8.0
tensorflow-metal              0.3.0
(base) anush@MacBook-Pro examples % pip list | grep torch 
torch                         1.10.0
torchvision                   0.9.0a0
(base) anush@MacBook-Pro examples % pip list | grep onnxy
(base) anush@MacBook-Pro examples % pip list | grep onnx 
onnx                          1.10.1
onnxconverter-common          1.8.1
(base) anush@MacBook-Pro examples % </code></pre>



<p>We use <a href="https://github.com/tlkh/asitop" target="_blank" rel="noreferrer noopener">asitop</a> to monitor the Power Usage, GPU Usage, Core throttling etc which can be installed via pip.</p>



<p>Apple’s CoreML has the ability to target not just the CPU or GPU but also the Apple Neural Engine though only for inference. We did try to get CoreML to work for the inference comparison but ran into model conversion issues <a href="https://github.com/NodLabs/shark-samples/blob/main/examples/bert_coreml.log">here</a> and excluded it from the tests. The latest coremltools seems to require an older Tensorflow version 2.5.0 which is no longer the default when you install tensorflow with conda. </p>



<p>The OSX Window Server <a href="https://www.reddit.com/r/MacOS/comments/qibcaz/macos_monterey_windowserver_keeps_crashing_when/">crashes</a> when all GPUs are used to the maximum. We have filed a Feedback issue with Apple on this bug and hope it will be resolved soon but when recreating results we recommend ssh into the MacBook Pro and don’t turn on the display.</p>



<p>All the code in these repositories are very much work in progress and shown as technical previews which requires some level of polish before they are ready for non-technical users to be able to use. We plan to continue making it user friendly and add eager mode support to Torch-MLIR so PyTorch support on M1Max GPUs works out the box for seamless development to deployment. If putting all the open source pieces is not your thing or if you have a business case for deploying PyTorch with GPU support on M1 devices today and professional solution please sign up <a rel="noreferrer noopener" href="https://forms.gle/1C8zijXsb1SKmC6A9" target="_blank">here</a> and our solutions team will reach out to help.</p>



<h4 id="bert-minilm-mlir-gpubin">BERT MINILM Artifacts [ <a href="https://storage.googleapis.com/shark-public/m1gpudemo/minilm_jan6.mlir">MLIR</a> | <a href="https://storage.googleapis.com/shark-public/m1gpudemo/minilm_jan6_m1max.vmfb">GPUBIN</a> ]</h4>



<h4 id="bert-training-mlir-gpubin">BERT Training Artifacts [ <a href="https://storage.googleapis.com/shark-public/m1gpudemo/bert_training_feb17.mlir">MLIR</a> | <a href="https://storage.googleapis.com/shark-public/m1gpudemo/bert_training_feb17.mlir">GPUBIN</a> ]</h4>







<h3 id="conclusion-and-future-work">Conclusion and future work</h3>



<p>Using SHARK Runtime, we demonstrate high performance PyTorch models on Apple M1Max GPUs. It outperforms Tensorflow-Metal by 1.5x for inferencing and 2x in training BERT models.  In the near future we plan to enhance end user experience and add “eager” mode support so it is seamless from development to deployment on any hardware. </p>



<p>If you would like access to the commercial version of SHARK Runtime sign up for access <a rel="noreferrer noopener" href="https://forms.gle/1C8zijXsb1SKmC6A9" target="_blank">here</a> and if you have trouble recreating results please open an <a href="https://github.com/NodLabs/SHARK">issue</a>.</p>



<h3 id="acknowledgements">Acknowledgements</h3>



<p>SHARK is built on open source packages <a href="https://github.com/llvm/torch-mlir">Torch-MLIR</a>, <a href="https://mlir.llvm.org"> LLVM/MLIR</a> and <a href="https://google.github.io/iree/">Google IREE</a> and we are thank for all developers and community for their support. We specifically want to call out Ben Vanik, Lei Zhang, Stella Laurenzo and Thomas Raoux for their help, support and guidance on the MLIR/IREE GPU codegen paths.</p>
        </div></div>
  </body>
</html>
