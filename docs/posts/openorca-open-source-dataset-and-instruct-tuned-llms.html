<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://erichartford.com/openorca">Original</a>
    <h1>OpenOrca: open source dataset and instruct-tuned LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><section><div><div id="post-content-parent"><div id="post-content-wrapper"><p>Today I am announcing OpenOrca, an open-source dataset and series of instruct-tuned language models.</p>
<p>As I read <a target="_blank" href="https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/"><strong>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</strong></a> by Mukherjee et. al. of Microsoft, I had to consider the implications for Open Source AI.</p>
<p>This was pretty awesome stuff. But, I realized that while Microsoft would probably release their LLaMA-13b based model (as of the time of this writing they still haven&#39;t) I concluded that they might not release the dataset.</p>
<p>Therefore, I resolved to replicate their efforts, download the data myself, and train the model myself, so that OpenOrca can be released on other sizes of LLaMA as well as other foundational models such as Falcon, OpenLLaMA, RedPajama, MPT, RWKV.</p>
<p>This was a nontrivial undertaking. With the help of an all-star team of open-source AI/ML engineers, we have completed the OpenOrca dataset.</p>
<p>Our dataset consists of:</p>
<ul>
<li><p>~1 million of FLANv2 augmented with GPT-4 completions</p>
</li>
<li><p>~3.5 million of FLANv2 augmented with GPT-3.5 completions</p>
</li>
</ul>
<p>We followed the submix and system prompt distribution outlined in the Orca paper. With a few exceptions. We included all 75k of CoT in the FLAN-1m dataset rather than sampling that. Also, we found that many items were duplicated so we removed duplicates, resulting in 3.5m instructs in the ChatGPT dataset.</p>
<p>We are presently performing full weights fine-tuning of OpenOrca on the foundation of LLaMA-13b, so that our performance can be compared with Microsoft&#39;s model when it releases.</p>
<p>We expect to release OpenOrca-LLaMA-13b in mid-July 2023. At that time we will publish our evaluation findings and the dataset.</p>
<p>We are currently seeking GPU compute sponsors for training OpenOrca on the following platforms:</p>
<ul>
<li><p>Falcon 7b, 40b</p>
</li>
<li><p>LLaMA 7b, 13b, 33b, 65b</p>
</li>
<li><p>MPT-7b, 30b</p>
</li>
<li><p>Any other targets that get a sponsor. (RWKV, OpenLLaMA)</p>
</li>
</ul>
<p>From the Orca paper and our experiments, we roughly estimate the compute costs as follows:</p>
<div>
<table>
<thead>
<tr>
<td>Model Size</td><td>Compute Estimate</td></tr>
</thead>
<tbody>
<tr>
<td>7b</td><td>1k GPU-Hours</td></tr>
<tr>
<td>13b</td><td>2k GPU-Hours</td></tr>
<tr>
<td>30/33b</td><td>4k-6k GPU-Hours</td></tr>
<tr>
<td>40b</td><td>8k-10k GPU-Hours</td></tr>
<tr>
<td>65b</td><td>10k-15k GPU-Hours</td></tr>
</tbody>
</table>
</div><p>We will share our appreciation for sponsorship in this space, as well as the model cards.</p>
<p>Our current sponsors:</p>
<ul>
<li><p>Financial Contribution and mentorship - <a target="_blank" href="https://chirper.ai/">chirper.ai</a></p>
</li>
<li><p>LLaMA 7b - <a target="_blank" href="https://www.preemo.io/">preemo.io</a></p>
</li>
<li><p>LLaMA 33b - <a target="_blank" href="https://www.latitude.sh/">latitude.sh</a></p>
</li>
</ul>
<p>Please reach out to me if you are interested in providing compute sponsorship for any specific targets of OpenOrca.</p>
<p>I would like to thank the motley crew of Open Source AI/ML engineers who have worked beside me in this endeavor. Including:</p>
<ul>
<li><p>Wing &#34;Caseus&#34; Lian and NanoBit of OpenAccess AI Collective</p>
</li>
<li><p>AutoMeta, Entropi, AtlasUnified, and neverendingtoast of Alignment Lab AI</p>
</li>
<li><p>Rohan</p>
</li>
<li><p>Teknium</p>
</li>
<li><p>Pankaj Mathur</p>
</li>
<li><p>Tom &#34;TheBloke&#34; Jobbins for quantizing and amplifying</p>
</li>
<li><p>All the other people in the Open Source AI community who have taught me and helped me along the way.</p>
</li>
</ul>
</div></div></div></section></div></div>
  </body>
</html>
