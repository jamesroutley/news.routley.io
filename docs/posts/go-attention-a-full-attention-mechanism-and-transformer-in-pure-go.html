<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/takara-ai/go-attention">Original</a>
    <h1>Go-attention: A full attention mechanism and transformer in pure Go</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5ef5f94a9648b5afca0d5e381017252c2bde05d95e188cae63186277041ca20f/68747470733a2f2f74616b6172612e61692f696d616765732f6c6f676f2d32342f54616b61726141692e737667"><img src="https://camo.githubusercontent.com/5ef5f94a9648b5afca0d5e381017252c2bde05d95e188cae63186277041ca20f/68747470733a2f2f74616b6172612e61692f696d616765732f6c6f676f2d32342f54616b61726141692e737667" width="200" alt="Takara.ai Logo" data-canonical-src="https://takara.ai/images/logo-24/TakaraAi.svg"/></a></p>
<p dir="auto">From the Frontier Research Team at takara.ai we present the first pure Go implementation of attention mechanisms and transformer layers, designed for high performance and ease of use.</p>

<p dir="auto">Run our comprehensive examples:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Get the module
go get github.com/takara-ai/go-attention

# Run the examples
go run api_examples.go"><pre><span><span>#</span> Get the module</span>
go get github.com/takara-ai/go-attention

<span><span>#</span> Run the examples</span>
go run api_examples.go</pre></div>


<div dir="auto" data-snippet-clipboard-copy-content="type Vector []float64           // Represents a 1D vector of float64 values
type Matrix []Vector           // Represents a 2D matrix of float64 values"><pre><span>type</span> <span>Vector</span> []<span>float64</span>           <span>// Represents a 1D vector of float64 values</span>
<span>type</span> <span>Matrix</span> []<span>Vector</span>           <span>// Represents a 2D matrix of float64 values</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">1. Basic Dot-Product Attention</h3><a id="user-content-1-basic-dot-product-attention" aria-label="Permalink: 1. Basic Dot-Product Attention" href="#1-basic-dot-product-attention"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The simplest form of attention mechanism. Useful for basic sequence processing tasks.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import &#34;github.com/takara-ai/go-attention/attention&#34;

// Create query-key-value setup
query := attention.Vector{1.0, 0.0, 1.0, 0.0}  // Pattern to search for
keys := attention.Matrix{
    {1.0, 0.0, 1.0, 0.0},  // Similar to query
    {0.0, 1.0, 0.0, 1.0},  // Different from query
    {0.5, 0.5, 0.5, 0.5},  // Neutral pattern
}
values := attention.Matrix{
    {1.0, 2.0},  // Value for similar key
    {3.0, 4.0},  // Value for different key
    {5.0, 6.0},  // Value for neutral key
}

// Compute attention
output, weights, err := attention.DotProductAttention(query, keys, values)
if err != nil {
    log.Fatal(err)
}

// Output will be a weighted combination of values based on query-key similarity
// Weights will show how much attention each key received"><pre><span>import</span> <span>&#34;github.com/takara-ai/go-attention/attention&#34;</span>

<span>// Create query-key-value setup</span>
<span>query</span> <span>:=</span> attention.<span>Vector</span>{<span>1.0</span>, <span>0.0</span>, <span>1.0</span>, <span>0.0</span>}  <span>// Pattern to search for</span>
<span>keys</span> <span>:=</span> attention.<span>Matrix</span>{
    {<span>1.0</span>, <span>0.0</span>, <span>1.0</span>, <span>0.0</span>},  <span>// Similar to query</span>
    {<span>0.0</span>, <span>1.0</span>, <span>0.0</span>, <span>1.0</span>},  <span>// Different from query</span>
    {<span>0.5</span>, <span>0.5</span>, <span>0.5</span>, <span>0.5</span>},  <span>// Neutral pattern</span>
}
<span>values</span> <span>:=</span> attention.<span>Matrix</span>{
    {<span>1.0</span>, <span>2.0</span>},  <span>// Value for similar key</span>
    {<span>3.0</span>, <span>4.0</span>},  <span>// Value for different key</span>
    {<span>5.0</span>, <span>6.0</span>},  <span>// Value for neutral key</span>
}

<span>// Compute attention</span>
<span>output</span>, <span>weights</span>, <span>err</span> <span>:=</span> <span>attention</span>.<span>DotProductAttention</span>(<span>query</span>, <span>keys</span>, <span>values</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Output will be a weighted combination of values based on query-key similarity</span>
<span>// Weights will show how much attention each key received</span></pre></div>

<p dir="auto">More sophisticated attention mechanism that can capture different types of relationships in parallel.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import &#34;github.com/takara-ai/go-attention/attention&#34;

// Configure multi-head attention
config := attention.MultiHeadConfig{
    NumHeads:    4,        // Number of parallel attention heads
    DModel:      64,       // Size of input/output embeddings
    DKey:        16,       // Size per head (DModel/NumHeads)
    DValue:      16,       // Size per head (DModel/NumHeads)
    DropoutRate: 0.1,      // For regularization
}

// Create the attention module
mha, err := attention.NewMultiHeadAttention(config)
if err != nil {
    log.Fatal(err)
}

// Process sequences (batched input)
batchSize, seqLen := 2, 3  // Process 2 sequences, each with 3 tokens

// Create input matrices [batchSize × seqLen × DModel]
queries := make(attention.Matrix, batchSize*seqLen)
keys := make(attention.Matrix, batchSize*seqLen)
values := make(attention.Matrix, batchSize*seqLen)

// Initialize your matrices with actual data...

// Process through multi-head attention
output, err := mha.Forward(queries, keys, values)
if err != nil {
    log.Fatal(err)
}"><pre><span>import</span> <span>&#34;github.com/takara-ai/go-attention/attention&#34;</span>

<span>// Configure multi-head attention</span>
<span>config</span> <span>:=</span> attention.<span>MultiHeadConfig</span>{
    <span>NumHeads</span>:    <span>4</span>,        <span>// Number of parallel attention heads</span>
    <span>DModel</span>:      <span>64</span>,       <span>// Size of input/output embeddings</span>
    <span>DKey</span>:        <span>16</span>,       <span>// Size per head (DModel/NumHeads)</span>
    <span>DValue</span>:      <span>16</span>,       <span>// Size per head (DModel/NumHeads)</span>
    <span>DropoutRate</span>: <span>0.1</span>,      <span>// For regularization</span>
}

<span>// Create the attention module</span>
<span>mha</span>, <span>err</span> <span>:=</span> <span>attention</span>.<span>NewMultiHeadAttention</span>(<span>config</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Process sequences (batched input)</span>
<span>batchSize</span>, <span>seqLen</span> <span>:=</span> <span>2</span>, <span>3</span>  <span>// Process 2 sequences, each with 3 tokens</span>

<span>// Create input matrices [batchSize × seqLen × DModel]</span>
<span>queries</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)
<span>keys</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)
<span>values</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)

<span>// Initialize your matrices with actual data...</span>

<span>// Process through multi-head attention</span>
<span>output</span>, <span>err</span> <span>:=</span> <span>mha</span>.<span>Forward</span>(<span>queries</span>, <span>keys</span>, <span>values</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">3. Full Transformer Layer</h3><a id="user-content-3-full-transformer-layer" aria-label="Permalink: 3. Full Transformer Layer" href="#3-full-transformer-layer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Complete transformer layer with self-attention and feed-forward network.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import (
    &#34;github.com/takara-ai/go-attention/transformer&#34;
    &#34;github.com/takara-ai/go-attention/attention&#34;
)

// Configure transformer layer
config := transformer.TransformerConfig{
    DModel:      64,       // Size of token embeddings
    NumHeads:    4,        // Number of attention heads
    DHidden:     256,      // Size of feed-forward hidden layer
    DropoutRate: 0.1,      // For regularization
}

// Create transformer layer
layer, err := transformer.NewTransformerLayer(config)
if err != nil {
    log.Fatal(err)
}

// Create input sequence [seq_len × d_model]
seqLen := 3
input := make(attention.Matrix, seqLen)
for i := range input {
    input[i] = make(attention.Vector, config.DModel)
    // Fill with your embedding data...
}

// Process through transformer
output, err := layer.Forward(input)
if err != nil {
    log.Fatal(err)
}"><pre><span>import</span> (
    <span>&#34;github.com/takara-ai/go-attention/transformer&#34;</span>
    <span>&#34;github.com/takara-ai/go-attention/attention&#34;</span>
)

<span>// Configure transformer layer</span>
<span>config</span> <span>:=</span> transformer.<span>TransformerConfig</span>{
    <span>DModel</span>:      <span>64</span>,       <span>// Size of token embeddings</span>
    <span>NumHeads</span>:    <span>4</span>,        <span>// Number of attention heads</span>
    <span>DHidden</span>:     <span>256</span>,      <span>// Size of feed-forward hidden layer</span>
    <span>DropoutRate</span>: <span>0.1</span>,      <span>// For regularization</span>
}

<span>// Create transformer layer</span>
<span>layer</span>, <span>err</span> <span>:=</span> <span>transformer</span>.<span>NewTransformerLayer</span>(<span>config</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Create input sequence [seq_len × d_model]</span>
<span>seqLen</span> <span>:=</span> <span>3</span>
<span>input</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>seqLen</span>)
<span>for</span> <span>i</span> <span>:=</span> <span>range</span> <span>input</span> {
    <span>input</span>[<span>i</span>] <span>=</span> <span>make</span>(attention.<span>Vector</span>, <span>config</span>.<span>DModel</span>)
    <span>// Fill with your embedding data...</span>
}

<span>// Process through transformer</span>
<span>output</span>, <span>err</span> <span>:=</span> <span>layer</span>.<span>Forward</span>(<span>input</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}</pre></div>

<p dir="auto">When running the examples, you&#39;ll see:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Dot-Product Attention</strong>:</p>
<div data-snippet-clipboard-copy-content="Query: [1 0 1 0]
Attention Weights: [0.506 0.186 0.307]  // Shows focus on similar patterns
Output: [2.601 3.601]                   // Weighted combination of values"><pre><code>Query: [1 0 1 0]
Attention Weights: [0.506 0.186 0.307]  // Shows focus on similar patterns
Output: [2.601 3.601]                   // Weighted combination of values
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Multi-Head Attention</strong>:</p>
<div data-snippet-clipboard-copy-content="Input dimensions: [2 batches × 3 tokens × 64 features]
Output shape: [6×64]"><pre><code>Input dimensions: [2 batches × 3 tokens × 64 features]
Output shape: [6×64]
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Transformer Layer</strong>:</p>
<div data-snippet-clipboard-copy-content="Input shape: [3×64]
Output shape: [3×64]"><pre><code>Input shape: [3×64]
Output shape: [3×64]
</code></pre></div>
</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto"><strong>Text Processing</strong>:</p>
<ul dir="auto">
<li>Sequence-to-sequence translation</li>
<li>Document summarization</li>
<li>Sentiment analysis</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Time Series</strong>:</p>
<ul dir="auto">
<li>Financial forecasting</li>
<li>Sensor data analysis</li>
<li>Anomaly detection</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Structured Data</strong>:</p>
<ul dir="auto">
<li>Graph node embedding</li>
<li>Feature interaction modeling</li>
<li>Recommendation systems</li>
</ul>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">Performance Considerations</h2><a id="user-content-performance-considerations" aria-label="Permalink: Performance Considerations" href="#performance-considerations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Matrix operations are optimized for CPU</li>
<li>Memory allocations are minimized</li>
<li>Batch processing for better throughput</li>
<li>No external dependencies</li>
</ul>
<p dir="auto">For more detailed examples, see the <code>examples</code> directory in the repository.</p>

<p dir="auto">This module was created to provide a clean, efficient, and dependency-free implementation of attention mechanisms in Go. It&#39;s particularly useful for:</p>
<ul dir="auto">
<li><strong>Edge Computing</strong>: Zero external dependencies makes it perfect for edge devices where dependency management is crucial</li>
<li><strong>Real-time Processing</strong>: Pure Go implementation ensures predictable performance for real-time applications</li>
<li><strong>Cloud-native Applications</strong>: Efficient batched operations support high-throughput scaling in cloud environments</li>
<li><strong>Embedded Systems</strong>: Predictable resource usage and minimal memory allocations</li>
<li><strong>Production Systems</strong>: Comprehensive error handling and type safety for robust production deployments</li>
</ul>

<ul dir="auto">
<li>Efficient dot-product attention mechanism</li>
<li>Multi-head attention support</li>
<li>Full transformer layer implementation with:
<ul dir="auto">
<li>Layer normalization</li>
<li>Position-wise feed-forward networks</li>
<li>Residual connections</li>
</ul>
</li>
<li>Batched operations for improved performance</li>
</ul>

<p dir="auto">Future improvements may include:</p>
<ul dir="auto">
<li>Positional encoding implementations</li>
<li>Dropout support</li>
<li>CUDA acceleration support</li>
<li>Additional transformer variants</li>
<li>Pre-trained models</li>
<li>Training utilities</li>
</ul>

<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>

<p dir="auto">MIT License - see LICENSE file for details</p>
<hr/>
<p dir="auto">For research inquiries and press, please reach out to <a href="mailto:research@takara.ai">research@takara.ai</a></p>
<blockquote>
<p dir="auto">人類を変革する</p>
</blockquote>
</article></div></div>
  </body>
</html>
