<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.eleuther.ai/announcing-20b/">Original</a>
    <h1>Announcing GPT-NeoX-20B</h1>
    
    <div id="readability-page-1" class="page"><div>

<article>
   
  <div><p><strong>GPT-NeoX-20B will be publicly downloadable from The Eye on the <date datetime="2022-02-09">9th of February</date>.</strong>
In the meantime, you can already try out the model using CoreWeave’s and Anlatan’s new inference service, <a href="https://goose.ai/" title="We&#39;re dead serious, that is actually what it is called.">GooseAI</a>!</p>
<hr/>
<p>After a year-long odyssey through months of chip shortage-induced shipping delays, technical trials and tribulations, and aggressively boring debugging, we are happy to finally announce EleutherAI’s latest open-source language model: GPT-NeoX-20B, a 20 billion parameter model trained using our <a href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a> framework on GPUs generously provided by our friends at <a href="https://www.coreweave.com/">CoreWeave</a>.</p>
<p>GPT-NeoX-20B is, to our knowledge, the largest publicly accessible pretrained general-purpose autoregressive language model, and we expect it to perform well on many tasks.</p>
<p>We hope that the increased accessibility of models of this size will aid in <a href="https://blog.eleuther.ai/why-release-a-large-language-model/">research towards the safe use of AI systems</a>, and encourage anyone interested in working in this direction to reach out to us.</p>
<p>As a thank you to our generous compute donors, we are delaying the public downloadable release of the model by 7 days. On <date datetime="2022-02-09">February 9, 2022</date>, the full model weights will be downloadable for free under a permissive Apache 2.0 license from The Eye.</p>
<p>There will be a <span>#20b</span> channel set up in our Discord for discussions of this model. Please note that much like our other language models and codebases, GPT-NeoX and GPT-NeoX-20B are very much research artifacts and we <em>do not recommend deploying either in a production setting without careful consideration</em>. In particular, we strongly encourage those looking to use GPT-NeoX-20B to read the <a href="https://arxiv.org/abs/2101.00027">paper</a> and <a href="https://arxiv.org/abs/2201.07311">datasheet</a> on our training data. There are still bugs to be ironed out and many inefficiencies that could be addressed—but hey, we do this in our free time, give us a break lol</p>
<hr/>
<figure>
    <table>
<thead>
<tr>
<th>Task</th>
<th>Category</th>
<th>Babbage</th>
<th>Curie</th>
<th>GPT-J-6B</th>
<th>FairSeq-13B</th>
<th>GPT-NeoX-20B</th>
<th>DaVinci</th>
</tr>
</thead>
<tbody>
<tr>
<td>LAMBADA</td>
<td>Sentence Completion</td>
<td>62.49%</td>
<td>69.51%</td>
<td>68.29%</td>
<td>70.95%</td>
<td>71.98%</td>
<td>75.16%</td>
</tr>
<tr>
<td>ANLI R1</td>
<td>Natural Language Inference</td>
<td>32.40%</td>
<td>32.80%</td>
<td>32.40%</td>
<td>34.00%</td>
<td>33.50%</td>
<td>36.30%</td>
</tr>
<tr>
<td>ANLI R2</td>
<td>Natural Language Inference</td>
<td>30.90%</td>
<td>33.50%</td>
<td>34.00%</td>
<td>33.00%</td>
<td>34.40%</td>
<td>37.00%</td>
</tr>
<tr>
<td>ANLI R3</td>
<td>Natural Language Inference</td>
<td>33.75%</td>
<td>35.50%</td>
<td>35.50%</td>
<td>34.75%</td>
<td>35.75%</td>
<td>36.83%</td>
</tr>
<tr>
<td>WSC</td>
<td>Coreference Resolution</td>
<td>40.38%</td>
<td>54.81%</td>
<td>36.53%</td>
<td>57.69%</td>
<td>53.61%</td>
<td>63.46%</td>
</tr>
<tr>
<td>Winogrande</td>
<td>Coreference Resolution</td>
<td>59.51%</td>
<td>64.56%</td>
<td>64.01%</td>
<td>67.40%</td>
<td>65.27%</td>
<td>69.93%</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>Sentence Completion</td>
<td>54.54%</td>
<td>49.54%</td>
<td>49.54%</td>
<td>55.44%</td>
<td>49.04%</td>
<td>59.18%</td>
</tr>
<tr>
<td>Total</td>
<td></td>
<td>39.40%</td>
<td>42.57%</td>
<td>40.28%</td>
<td>44.67%</td>
<td>43.31%</td>
<td>48.40%</td>
</tr>
</tbody>
</table>

    <figcaption>
            <p>Accuracy on standard language modeling tasks.</p>
        </figcaption>
</figure>

<figure>
    <table>
<thead>
<tr>
<th>Subject Group</th>
<th>Babbage</th>
<th>Curie</th>
<th>GPT-J-6B</th>
<th>FairSeq-13B</th>
<th>GPT-NeoX-20B</th>
<th>DaVinci</th>
</tr>
</thead>
<tbody>
<tr>
<td>Humanities</td>
<td>27.01%</td>
<td>26.48%</td>
<td>28.07%</td>
<td>27.27%</td>
<td>28.70%</td>
<td>32.30%</td>
</tr>
<tr>
<td>Social Science</td>
<td>27.94%</td>
<td>29.24%</td>
<td>28.73%</td>
<td>27.94%</td>
<td>31.63%</td>
<td>35.87%</td>
</tr>
<tr>
<td>STEM</td>
<td>25.83%</td>
<td>24.25%</td>
<td>25.71%</td>
<td>24.63%</td>
<td>26.27%</td>
<td>28.60%</td>
</tr>
<tr>
<td>Other</td>
<td>26.86%</td>
<td>28.84%</td>
<td>27.95%</td>
<td>27.33%</td>
<td>29.83%</td>
<td>36.85%</td>
</tr>
<tr>
<td>Total</td>
<td>26.78%</td>
<td>26.90%</td>
<td>27.38%</td>
<td>26.53%</td>
<td>28.77%</td>
<td>32.86%</td>
</tr>
</tbody>
</table>

    <figcaption>
            <p>Accuracy of factual knowledge by subject group, as measured by the <a href="https://arxiv.org/abs/2009.03300">HendrycksTest</a> evaluation.</p>
        </figcaption>
</figure>



  </div>
  
</article>
    </div></div>
  </body>
</html>
