<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/iterative/datachain">Original</a>
    <h1>DataChain: DBT for Unstructured Data</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://pypi.org/project/datachain/" rel="nofollow"><img alt="PyPI" src="https://camo.githubusercontent.com/a4d123fc0f1691dd3957fd36b80adf23d5046768ad633924e1c58a4c3eaca629/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f64617461636861696e2e737667" data-canonical-src="https://img.shields.io/pypi/v/datachain.svg"/>
</a> <a href="https://pypi.org/project/datachain" rel="nofollow"><img alt="Python Version" src="https://camo.githubusercontent.com/e6c2bfa259931866ffac797b0b8caac7364e0eecb539d5f351e5593ec5a525de/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f64617461636861696e" data-canonical-src="https://img.shields.io/pypi/pyversions/datachain"/></a> <a href="https://codecov.io/gh/iterative/datachain" rel="nofollow"><img alt="Codecov" src="https://camo.githubusercontent.com/70e14e85f86115c3871d59d63025dd05b0a29f63ef2f7ed290bc77f13b421d01/68747470733a2f2f636f6465636f762e696f2f67682f6974657261746976652f64617461636861696e2f67726170682f62616467652e7376673f746f6b656e3d62796c69584747794742" data-canonical-src="https://codecov.io/gh/iterative/datachain/graph/badge.svg?token=byliXGGyGB"/></a> <a href="https://github.com/iterative/datachain/actions/workflows/tests.yml"><img alt="Tests" src="https://github.com/iterative/datachain/actions/workflows/tests.yml/badge.svg"/>
</a></p>
<p dir="auto">DataChain is a modern Pythonic data-frame library designed for artificial intelligence.
It is made to organize your unstructured data into datasets and wrangle it at scale on
your local machine. Datachain does not abstract or hide the AI models and API calls, but helps to integrate them into the postmodern data stack.</p>
<a name="user-content-key-features"></a>

<dl>
<dt>üìÇ <strong>Storage as a Source of Truth.</strong></dt>
<dd><ul dir="auto">
<li>Process unstructured data without redundant copies from S3, GCP, Azure, and local
file systems.</li>
<li>Multimodal data support: images, video, text, PDFs, JSONs, CSVs, parquet.</li>
<li>Unite files and metadata together into persistent, versioned, columnar datasets.</li>
</ul>
</dd>
<dt>üêç <strong>Python-friendly data pipelines.</strong></dt>
<dd><ul dir="auto">
<li>Operate on Python objects and object fields.</li>
<li>Built-in parallelization and out-of-memory compute without SQL or Spark.</li>
</ul>
</dd>
<dt>üß† <strong>Data Enrichment and Processing.</strong></dt>
<dd><ul dir="auto">
<li>Generate metadata using local AI models and LLM APIs.</li>
<li>Filter, join, and group by metadata. Search by vector embeddings.</li>
<li>Pass datasets to Pytorch and Tensorflow, or export them back into storage.</li>
</ul>
</dd>
<dt>üöÄ <strong>Efficiency.</strong></dt>
<dd><ul dir="auto">
<li>Parallelization, out-of-memory workloads and data caching.</li>
<li>Vectorized operations on Python object fields: sum, count, avg, etc.</li>
<li>Optimized vector search.</li>
</ul>
</dd>
</dl>
<a name="user-content-quick-start"></a>


<a name="user-content-selecting-files-using-json-metadata"></a>
<div dir="auto"><h2 tabindex="-1" dir="auto">Selecting files using JSON metadata</h2><a id="user-content-selecting-files-using-json-metadata" aria-label="Permalink: Selecting files using JSON metadata" href="#selecting-files-using-json-metadata"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A storage consists of images of cats and dogs (dog.1048.jpg, cat.1009.jpg),
annotated with ground truth and model inferences in the &#39;json-pairs&#39; format,
where each image has a matching JSON file like cat.1009.json:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;class&#34;: &#34;cat&#34;, &#34;id&#34;: &#34;1009&#34;, &#34;num_annotators&#34;: 8,
    &#34;inference&#34;: {&#34;class&#34;: &#34;dog&#34;, &#34;confidence&#34;: 0.68}
}"><pre>{
    <span>&#34;class&#34;</span>: <span><span>&#34;</span>cat<span>&#34;</span></span>, <span>&#34;id&#34;</span>: <span><span>&#34;</span>1009<span>&#34;</span></span>, <span>&#34;num_annotators&#34;</span>: <span>8</span>,
    <span>&#34;inference&#34;</span>: {<span>&#34;class&#34;</span>: <span><span>&#34;</span>dog<span>&#34;</span></span>, <span>&#34;confidence&#34;</span>: <span>0.68</span>}
}</pre></div>
<p dir="auto">Example of downloading only &#34;high-confidence cat&#34; inferred images using JSON metadata:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from datachain import Column, DataChain

meta = DataChain.from_json(&#34;gs://datachain-demo/dogs-and-cats/*json&#34;, object_name=&#34;meta&#34;)
images = DataChain.from_storage(&#34;gs://datachain-demo/dogs-and-cats/*jpg&#34;)

images_id = images.map(id=lambda file: file.path.split(&#39;.&#39;)[-2])
annotated = images_id.merge(meta, on=&#34;id&#34;, right_on=&#34;meta.id&#34;)

likely_cats = annotated.filter((Column(&#34;meta.inference.confidence&#34;) &gt; 0.93) \
                               &amp; (Column(&#34;meta.inference.class_&#34;) == &#34;cat&#34;))
likely_cats.export_files(&#34;high-confidence-cats/&#34;, signal=&#34;file&#34;)"><pre><span>from</span> <span>datachain</span> <span>import</span> <span>Column</span>, <span>DataChain</span>

<span>meta</span> <span>=</span> <span>DataChain</span>.<span>from_json</span>(<span>&#34;gs://datachain-demo/dogs-and-cats/*json&#34;</span>, <span>object_name</span><span>=</span><span>&#34;meta&#34;</span>)
<span>images</span> <span>=</span> <span>DataChain</span>.<span>from_storage</span>(<span>&#34;gs://datachain-demo/dogs-and-cats/*jpg&#34;</span>)

<span>images_id</span> <span>=</span> <span>images</span>.<span>map</span>(<span>id</span><span>=</span><span>lambda</span> <span>file</span>: <span>file</span>.<span>path</span>.<span>split</span>(<span>&#39;.&#39;</span>)[<span>-</span><span>2</span>])
<span>annotated</span> <span>=</span> <span>images_id</span>.<span>merge</span>(<span>meta</span>, <span>on</span><span>=</span><span>&#34;id&#34;</span>, <span>right_on</span><span>=</span><span>&#34;meta.id&#34;</span>)

<span>likely_cats</span> <span>=</span> <span>annotated</span>.<span>filter</span>((<span>Column</span>(<span>&#34;meta.inference.confidence&#34;</span>) <span>&gt;</span> <span>0.93</span>) \
                               <span>&amp;</span> (<span>Column</span>(<span>&#34;meta.inference.class_&#34;</span>) <span>==</span> <span>&#34;cat&#34;</span>))
<span>likely_cats</span>.<span>export_files</span>(<span>&#34;high-confidence-cats/&#34;</span>, <span>signal</span><span>=</span><span>&#34;file&#34;</span>)</pre></div>
<a name="user-content-data-curation-with-a-local-ai-model"></a>
<div dir="auto"><h2 tabindex="-1" dir="auto">Data curation with a local AI model</h2><a id="user-content-data-curation-with-a-local-ai-model" aria-label="Permalink: Data curation with a local AI model" href="#data-curation-with-a-local-ai-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Batch inference with a simple sentiment model using the transformers library:</p>

<p dir="auto">The code below downloads files the cloud, and applies a user-defined function
to each one of them. All files with a positive sentiment
detected are then copied to the local directory.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import pipeline
from datachain import DataChain, Column

classifier = pipeline(&#34;sentiment-analysis&#34;, device=&#34;cpu&#34;,
                model=&#34;distilbert/distilbert-base-uncased-finetuned-sst-2-english&#34;)

def is_positive_dialogue_ending(file) -&gt; bool:
    dialogue_ending = file.read()[-512:]
    return classifier(dialogue_ending)[0][&#34;label&#34;] == &#34;POSITIVE&#34;

chain = (
   DataChain.from_storage(&#34;gs://datachain-demo/chatbot-KiT/&#34;,
                          object_name=&#34;file&#34;, type=&#34;text&#34;)
   .settings(parallel=8, cache=True)
   .map(is_positive=is_positive_dialogue_ending)
   .save(&#34;file_response&#34;)
)

positive_chain = chain.filter(Column(&#34;is_positive&#34;) == True)
positive_chain.export_files(&#34;./output&#34;)

print(f&#34;{positive_chain.count()} files were exported&#34;)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>DataChain</span>, <span>Column</span>

<span>classifier</span> <span>=</span> <span>pipeline</span>(<span>&#34;sentiment-analysis&#34;</span>, <span>device</span><span>=</span><span>&#34;cpu&#34;</span>,
                <span>model</span><span>=</span><span>&#34;distilbert/distilbert-base-uncased-finetuned-sst-2-english&#34;</span>)

<span>def</span> <span>is_positive_dialogue_ending</span>(<span>file</span>) <span>-&gt;</span> <span>bool</span>:
    <span>dialogue_ending</span> <span>=</span> <span>file</span>.<span>read</span>()[<span>-</span><span>512</span>:]
    <span>return</span> <span>classifier</span>(<span>dialogue_ending</span>)[<span>0</span>][<span>&#34;label&#34;</span>] <span>==</span> <span>&#34;POSITIVE&#34;</span>

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>&#34;gs://datachain-demo/chatbot-KiT/&#34;</span>,
                          <span>object_name</span><span>=</span><span>&#34;file&#34;</span>, <span>type</span><span>=</span><span>&#34;text&#34;</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>8</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>is_positive</span><span>=</span><span>is_positive_dialogue_ending</span>)
   .<span>save</span>(<span>&#34;file_response&#34;</span>)
)

<span>positive_chain</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>&#34;is_positive&#34;</span>) <span>==</span> <span>True</span>)
<span>positive_chain</span>.<span>export_files</span>(<span>&#34;./output&#34;</span>)

<span>print</span>(<span>f&#34;<span><span>{</span><span>positive_chain</span>.<span>count</span>()<span>}</span></span> files were exported&#34;</span>)</pre></div>
<p dir="auto">13 files were exported</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ls output/datachain-demo/chatbot-KiT/
15.txt 20.txt 24.txt 27.txt 28.txt 29.txt 33.txt 37.txt 38.txt 43.txt ...
$ ls output/datachain-demo/chatbot-KiT/ | wc -l
13"><pre>$ ls output/datachain-demo/chatbot-KiT/
15.txt 20.txt 24.txt 27.txt 28.txt 29.txt 33.txt 37.txt 38.txt 43.txt ...
$ ls output/datachain-demo/chatbot-KiT/ <span>|</span> wc -l
13</pre></div>
<a name="user-content-llm-judging-chatbots"></a>

<p dir="auto">LLMs can work as universal classifiers. In the example below,
we employ a free API from Mistral to judge the <a href="https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot" rel="nofollow">publicly available</a> chatbot dialogs. Please get a free
Mistral API key at <a href="https://console.mistral.ai" rel="nofollow">https://console.mistral.ai</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pip install mistralai (Requires version &gt;=1.0.0)
$ export MISTRAL_API_KEY=_your_key_"><pre>$ pip install mistralai (Requires version <span>&gt;</span>=1.0.0)
$ <span>export</span> MISTRAL_API_KEY=_your_key_</pre></div>
<p dir="auto">DataChain can parallelize API calls; the free Mistral tier supports up to 4 requests at the same time.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mistralai import Mistral
from datachain import File, DataChain, Column

PROMPT = &#34;Was this dialog successful? Answer in a single word: Success or Failure.&#34;

def eval_dialogue(file: File) -&gt; bool:
     client = Mistral()
     response = client.chat.complete(
         model=&#34;open-mixtral-8x22b&#34;,
         messages=[{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: PROMPT},
                   {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: file.read()}])
     result = response.choices[0].message.content
     return result.lower().startswith(&#34;success&#34;)

chain = (
   DataChain.from_storage(&#34;gs://datachain-demo/chatbot-KiT/&#34;, object_name=&#34;file&#34;)
   .settings(parallel=4, cache=True)
   .map(is_success=eval_dialogue)
   .save(&#34;mistral_files&#34;)
)

successful_chain = chain.filter(Column(&#34;is_success&#34;) == True)
successful_chain.export_files(&#34;./output_mistral&#34;)

print(f&#34;{successful_chain.count()} files were exported&#34;)"><pre><span>from</span> <span>mistralai</span> <span>import</span> <span>Mistral</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>File</span>, <span>DataChain</span>, <span>Column</span>

<span>PROMPT</span> <span>=</span> <span>&#34;Was this dialog successful? Answer in a single word: Success or Failure.&#34;</span>

<span>def</span> <span>eval_dialogue</span>(<span>file</span>: <span>File</span>) <span>-&gt;</span> <span>bool</span>:
     <span>client</span> <span>=</span> <span>Mistral</span>()
     <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>complete</span>(
         <span>model</span><span>=</span><span>&#34;open-mixtral-8x22b&#34;</span>,
         <span>messages</span><span>=</span>[{<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>PROMPT</span>},
                   {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>file</span>.<span>read</span>()}])
     <span>result</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>
     <span>return</span> <span>result</span>.<span>lower</span>().<span>startswith</span>(<span>&#34;success&#34;</span>)

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>&#34;gs://datachain-demo/chatbot-KiT/&#34;</span>, <span>object_name</span><span>=</span><span>&#34;file&#34;</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>4</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>is_success</span><span>=</span><span>eval_dialogue</span>)
   .<span>save</span>(<span>&#34;mistral_files&#34;</span>)
)

<span>successful_chain</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>&#34;is_success&#34;</span>) <span>==</span> <span>True</span>)
<span>successful_chain</span>.<span>export_files</span>(<span>&#34;./output_mistral&#34;</span>)

<span>print</span>(<span>f&#34;<span><span>{</span><span>successful_chain</span>.<span>count</span>()<span>}</span></span> files were exported&#34;</span>)</pre></div>
<p dir="auto">With the instruction above, the Mistral model considers 31/50 files to hold the successful dialogues:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ls output_mistral/datachain-demo/chatbot-KiT/
1.txt  15.txt 18.txt 2.txt  22.txt 25.txt 28.txt 33.txt 37.txt 4.txt  41.txt ...
$ ls output_mistral/datachain-demo/chatbot-KiT/ | wc -l
31"><pre>$ ls output_mistral/datachain-demo/chatbot-KiT/
1.txt  15.txt 18.txt 2.txt  22.txt 25.txt 28.txt 33.txt 37.txt 4.txt  41.txt ...
$ ls output_mistral/datachain-demo/chatbot-KiT/ <span>|</span> wc -l
31</pre></div>
<a name="user-content-serializing-python-objects"></a>
<div dir="auto"><h2 tabindex="-1" dir="auto">Serializing Python-objects</h2><a id="user-content-serializing-python-objects" aria-label="Permalink: Serializing Python-objects" href="#serializing-python-objects"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">LLM responses may contain valuable information for analytics ‚Äì such as the number of tokens used, or the
model performance parameters.</p>
<p dir="auto">Instead of extracting this information from the Mistral response data structure (class
ChatCompletionResponse), DataChain can serialize the entire LLM response to the internal DB:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mistralai import Mistral
from mistralai.models import ChatCompletionResponse
from datachain import File, DataChain, Column

PROMPT = &#34;Was this dialog successful? Answer in a single word: Success or Failure.&#34;

def eval_dialog(file: File) -&gt; ChatCompletionResponse:
     client = MistralClient()
     return client.chat(
         model=&#34;open-mixtral-8x22b&#34;,
         messages=[{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: PROMPT},
                   {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: file.read()}])

chain = (
   DataChain.from_storage(&#34;gs://datachain-demo/chatbot-KiT/&#34;, object_name=&#34;file&#34;)
   .settings(parallel=4, cache=True)
   .map(response=eval_dialog)
   .map(status=lambda response: response.choices[0].message.content.lower()[:7])
   .save(&#34;response&#34;)
)

chain.select(&#34;file.name&#34;, &#34;status&#34;, &#34;response.usage&#34;).show(5)

success_rate = chain.filter(Column(&#34;status&#34;) == &#34;success&#34;).count() / chain.count()
print(f&#34;{100*success_rate:.1f}% dialogs were successful&#34;)"><pre><span>from</span> <span>mistralai</span> <span>import</span> <span>Mistral</span>
<span>from</span> <span>mistralai</span>.<span>models</span> <span>import</span> <span>ChatCompletionResponse</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>File</span>, <span>DataChain</span>, <span>Column</span>

<span>PROMPT</span> <span>=</span> <span>&#34;Was this dialog successful? Answer in a single word: Success or Failure.&#34;</span>

<span>def</span> <span>eval_dialog</span>(<span>file</span>: <span>File</span>) <span>-&gt;</span> <span>ChatCompletionResponse</span>:
     <span>client</span> <span>=</span> <span>MistralClient</span>()
     <span>return</span> <span>client</span>.<span>chat</span>(
         <span>model</span><span>=</span><span>&#34;open-mixtral-8x22b&#34;</span>,
         <span>messages</span><span>=</span>[{<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>PROMPT</span>},
                   {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>file</span>.<span>read</span>()}])

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>&#34;gs://datachain-demo/chatbot-KiT/&#34;</span>, <span>object_name</span><span>=</span><span>&#34;file&#34;</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>4</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>response</span><span>=</span><span>eval_dialog</span>)
   .<span>map</span>(<span>status</span><span>=</span><span>lambda</span> <span>response</span>: <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>.<span>lower</span>()[:<span>7</span>])
   .<span>save</span>(<span>&#34;response&#34;</span>)
)

<span>chain</span>.<span>select</span>(<span>&#34;file.name&#34;</span>, <span>&#34;status&#34;</span>, <span>&#34;response.usage&#34;</span>).<span>show</span>(<span>5</span>)

<span>success_rate</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>&#34;status&#34;</span>) <span>==</span> <span>&#34;success&#34;</span>).<span>count</span>() <span>/</span> <span>chain</span>.<span>count</span>()
<span>print</span>(<span>f&#34;<span><span>{</span><span>100</span><span>*</span><span>success_rate</span>:.1f<span>}</span></span>% dialogs were successful&#34;</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="     file   status      response     response          response
     name                  usage        usage             usage
                   prompt_tokens total_tokens completion_tokens
0   1.txt  success           547          548                 1
1  10.txt  failure          3576         3578                 2
2  11.txt  failure           626          628                 2
3  12.txt  failure          1144         1182                38
4  13.txt  success          1100         1101                 1

[Limited by 5 rows]
64.0% dialogs were successful"><pre>     file   status      response     response          response
     name                  usage        usage             usage
                   prompt_tokens total_tokens completion_tokens
0   1.txt  success           547          548                 1
1  10.txt  failure          3576         3578                 2
2  11.txt  failure           626          628                 2
3  12.txt  failure          1144         1182                38
4  13.txt  success          1100         1101                 1

[Limited by 5 rows]
64.0% dialogs were successful</pre></div>
<a name="user-content-iterating-over-python-data-structures"></a>
<div dir="auto"><h2 tabindex="-1" dir="auto">Iterating over Python data structures</h2><a id="user-content-iterating-over-python-data-structures" aria-label="Permalink: Iterating over Python data structures" href="#iterating-over-python-data-structures"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In the previous examples, datasets were saved in the embedded database
(<a href="https://www.sqlite.org/" rel="nofollow">SQLite</a> in folder .datachain of the working directory).
These datasets were automatically versioned, and can be accessed using
DataChain.from_dataset(&#34;dataset_name&#34;).</p>
<p dir="auto">Here is how to retrieve a saved dataset and iterate over the objects:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chain = DataChain.from_dataset(&#34;response&#34;)

# Iterating one-by-one: support out-of-memory workflow
for file, response in chain.limit(5).collect(&#34;file&#34;, &#34;response&#34;):
    # verify the collected Python objects
    assert isinstance(response, ChatCompletionResponse)

    status = response.choices[0].message.content[:7]
    tokens = response.usage.total_tokens
    print(f&#34;{file.get_uri()}: {status}, file size: {file.size}, tokens: {tokens}&#34;)"><pre><span>chain</span> <span>=</span> <span>DataChain</span>.<span>from_dataset</span>(<span>&#34;response&#34;</span>)

<span># Iterating one-by-one: support out-of-memory workflow</span>
<span>for</span> <span>file</span>, <span>response</span> <span>in</span> <span>chain</span>.<span>limit</span>(<span>5</span>).<span>collect</span>(<span>&#34;file&#34;</span>, <span>&#34;response&#34;</span>):
    <span># verify the collected Python objects</span>
    <span>assert</span> <span>isinstance</span>(<span>response</span>, <span>ChatCompletionResponse</span>)

    <span>status</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>[:<span>7</span>]
    <span>tokens</span> <span>=</span> <span>response</span>.<span>usage</span>.<span>total_tokens</span>
    <span>print</span>(<span>f&#34;<span><span>{</span><span>file</span>.<span>get_uri</span>()<span>}</span></span>: <span><span>{</span><span>status</span><span>}</span></span>, file size: <span><span>{</span><span>file</span>.<span>size</span><span>}</span></span>, tokens: <span><span>{</span><span>tokens</span><span>}</span></span>&#34;</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gs://datachain-demo/chatbot-KiT/1.txt: Success, file size: 1776, tokens: 548
gs://datachain-demo/chatbot-KiT/10.txt: Failure, file size: 11576, tokens: 3578
gs://datachain-demo/chatbot-KiT/11.txt: Failure, file size: 2045, tokens: 628
gs://datachain-demo/chatbot-KiT/12.txt: Failure, file size: 3833, tokens: 1207
gs://datachain-demo/chatbot-KiT/13.txt: Success, file size: 3657, tokens: 1101"><pre>gs://datachain-demo/chatbot-KiT/1.txt: Success, file size: 1776, tokens: 548
gs://datachain-demo/chatbot-KiT/10.txt: Failure, file size: 11576, tokens: 3578
gs://datachain-demo/chatbot-KiT/11.txt: Failure, file size: 2045, tokens: 628
gs://datachain-demo/chatbot-KiT/12.txt: Failure, file size: 3833, tokens: 1207
gs://datachain-demo/chatbot-KiT/13.txt: Success, file size: 3657, tokens: 1101</pre></div>
<a name="user-content-vectorized-analytics-over-python-objects"></a>
<div dir="auto"><h2 tabindex="-1" dir="auto">Vectorized analytics over Python objects</h2><a id="user-content-vectorized-analytics-over-python-objects" aria-label="Permalink: Vectorized analytics over Python objects" href="#vectorized-analytics-over-python-objects"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Some operations can run inside the DB without deserialization.
For instance, let&#39;s calculate the total cost of using the LLM APIs, assuming the Mixtral call costs $2 per 1M input tokens and $6 per 1M output tokens:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chain = DataChain.from_dataset(&#34;mistral_dataset&#34;)

cost = chain.sum(&#34;response.usage.prompt_tokens&#34;)*0.000002 \
           + chain.sum(&#34;response.usage.completion_tokens&#34;)*0.000006
print(f&#34;Spent ${cost:.2f} on {chain.count()} calls&#34;)"><pre><span>chain</span> <span>=</span> <span>DataChain</span>.<span>from_dataset</span>(<span>&#34;mistral_dataset&#34;</span>)

<span>cost</span> <span>=</span> <span>chain</span>.<span>sum</span>(<span>&#34;response.usage.prompt_tokens&#34;</span>)<span>*</span><span>0.000002</span> \
           <span>+</span> <span>chain</span>.<span>sum</span>(<span>&#34;response.usage.completion_tokens&#34;</span>)<span>*</span><span>0.000006</span>
<span>print</span>(<span>f&#34;Spent $<span><span>{</span><span>cost</span>:.2f<span>}</span></span> on <span><span>{</span><span>chain</span>.<span>count</span>()<span>}</span></span> calls&#34;</span>)</pre></div>
<p dir="auto">Output:</p>

<a name="user-content-pytorch-data-loader"></a>

<p dir="auto">Chain results can be exported or passed directly to PyTorch dataloader.
For example, if we are interested in passing image and a label based on file
name suffix, the following code will do it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from torch.utils.data import DataLoader
from transformers import CLIPProcessor

from datachain import C, DataChain

processor = CLIPProcessor.from_pretrained(&#34;openai/clip-vit-base-patch32&#34;)

chain = (
    DataChain.from_storage(&#34;gs://datachain-demo/dogs-and-cats/&#34;, type=&#34;image&#34;)
    .map(label=lambda name: name.split(&#34;.&#34;)[0], params=[&#34;file.name&#34;])
    .select(&#34;file&#34;, &#34;label&#34;).to_pytorch(
        transform=processor.image_processor,
        tokenizer=processor.tokenizer,
    )
)
loader = DataLoader(chain, batch_size=1)"><pre><span>from</span> <span>torch</span>.<span>utils</span>.<span>data</span> <span>import</span> <span>DataLoader</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>CLIPProcessor</span>

<span>from</span> <span>datachain</span> <span>import</span> <span>C</span>, <span>DataChain</span>

<span>processor</span> <span>=</span> <span>CLIPProcessor</span>.<span>from_pretrained</span>(<span>&#34;openai/clip-vit-base-patch32&#34;</span>)

<span>chain</span> <span>=</span> (
    <span>DataChain</span>.<span>from_storage</span>(<span>&#34;gs://datachain-demo/dogs-and-cats/&#34;</span>, <span>type</span><span>=</span><span>&#34;image&#34;</span>)
    .<span>map</span>(<span>label</span><span>=</span><span>lambda</span> <span>name</span>: <span>name</span>.<span>split</span>(<span>&#34;.&#34;</span>)[<span>0</span>], <span>params</span><span>=</span>[<span>&#34;file.name&#34;</span>])
    .<span>select</span>(<span>&#34;file&#34;</span>, <span>&#34;label&#34;</span>).<span>to_pytorch</span>(
        <span>transform</span><span>=</span><span>processor</span>.<span>image_processor</span>,
        <span>tokenizer</span><span>=</span><span>processor</span>.<span>tokenizer</span>,
    )
)
<span>loader</span> <span>=</span> <span>DataLoader</span>(<span>chain</span>, <span>batch_size</span><span>=</span><span>1</span>)</pre></div>
<a name="user-content-tutorials"></a>

<ul dir="auto">
<li><a href="https://datachain.dvc.ai/" rel="nofollow">Getting Started</a></li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/multimodal/clip_fine_tuning.ipynb">Multimodal</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/multimodal/clip_fine_tuning.ipynb" rel="nofollow">Colab</a>)</li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/llm/llm_chatbot_evaluation.ipynb">LLM evaluations</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/llm/llm_chatbot_evaluation.ipynb" rel="nofollow">Colab</a>)</li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/formats/json-metadata-tutorial.ipynb">Reading JSON metadata</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/formats/json-metadata-tutorial.ipynb" rel="nofollow">Colab</a>)</li>
</ul>
<a name="user-content-contributions"></a>

<p dir="auto">Contributions are very welcome.
To learn more, see the <a href="https://github.com/iterative/datachain/blob/main/CONTRIBUTING.rst">Contributor Guide</a>.</p>
<a name="user-content-community-and-support"></a>

<ul dir="auto">
<li><a href="https://datachain.dvc.ai/" rel="nofollow">Docs</a></li>
<li><a href="https://github.com/iterative/datachain/issues">File an issue</a> if you encounter any problems</li>
<li><a href="https://dvc.org/chat" rel="nofollow">Discord Chat</a></li>
<li><a href="mailto:support@dvc.org">Email</a></li>
<li><a href="https://twitter.com/DVCorg" rel="nofollow">Twitter</a></li>
</ul>

</article></div></div>
  </body>
</html>
