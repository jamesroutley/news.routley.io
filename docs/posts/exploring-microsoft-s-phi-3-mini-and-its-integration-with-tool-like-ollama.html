<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pieces.app/blog/phi-3-mini-integrations">Original</a>
    <h1>Exploring Microsoft&#39;s Phi-3-Mini and its integration with tool like Ollama</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>As a developer who has worked with various Large Language Models (LLMs), Iâ€™ve seen firsthand how the right model can transform a coding workflow. In the world of Large Language Models we often forget small language models and their capabilities.</p><p>Recently, Microsoft unveiled Phi-4, a new addition to their small language model series that is much better and comes with enhanced completion quality and smaller model sizes.Â </p><p>Phi-4 builds upon the groundwork established by Phi-3 and its variants.Â </p><p>While our primary focus in this article will be on phi-3-mini, itâ€™s encouraging to see Microsoft continuously innovating.Â </p><p>If you find phi-3-mini intriguing, you might want to keep Phi-4 in mind as a potential upgrade for even better capabilities.Â </p><p>For more details, check out Microsoftâ€™s <!--$--><a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090" rel="noopener">official announcement</a><!--/$-->.</p><p>In this blog, Iâ€™ll guide you through what makes the phi-3-mini an awesome choice.Â </p><p>We&#39;ll explore its core strengths, how to run it via Ollama, and how to integrate it with tools like the Pieces.app.Â </p><p>By the end, you&#39;ll have a clear understanding of why phi-3-mini â€“ especially variants like <code>phi-3-mini</code> or <code>phi-3-mini-128k-instruct</code>â€“ should be on your toolbox as an AI Practitioner.</p><h2>What is Phi-3-mini?</h2><p>Phi-3-mini is a 3.8 billion-parameter language model developed by Microsoft as part of the Phi-3 series.Â </p><p>Developed for efficiency, it provides performance that is comparable to that of larger models like the GPT-3.5, while being optimized for devices with limited computational resources, such as notebooks and smartphones.Â </p><p>Phi-3-mini excels in reasoning and coding, making it ideal for offline applications and systems which don&#39;t require a long context window and heavy computing.</p><h3>What sets Phi-3 Apart as an LLM?</h3><p>Phi-3 itself is a notable evolution in large language models, building upon the successes of GPT-3 and Phi-2.Â </p><p>Phi-3â€™s improvements arenâ€™t just incremental; the inclusion of extended context windows, such as in <code>phi-3-mini-128k-instruct</code>, fundamentally changes how you provide information to the model and its use casesÂ </p><p>With a phi 3 mini context window that can handle extensive documentation or multiple related files, you can maintain coherence in code suggestions even as complexity grows.</p><h3>What phi-3 is good for?</h3><p>From my experience, what makes <!--$--><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/01.Introduce/Phi3Family.md" rel="noopener">Phi-3 family models</a><!--/$--> particularly strong is their ability to handle vast, nuanced prompts effectively. It is relatively good for the given size for the tasks of language, reasoning, coding, and math benchmarks</p><p>By incorporating <strong>phi 3 mini synthetic data</strong> and employing architecture that supports extensive context (for example, <strong>phi 3 mini 128k instruct</strong> or <strong>phi 3 mini 4k instruct</strong> variants), these models deliver more accurate, context-aware code completions and reasoning.</p><h3>Can you generate code with Phi-3 Mini?</h3><p>Phi-3-mini emerges from Microsoftâ€™s Phi-3 family of models, designed to excel in both text and code generation.Â </p><p>This â€œmini&#34; variant channels the strengths of Phi-3 into a slimmer package, ensuring you retain robust performance without excessive resource demands.Â </p><p>Whether youâ€™re working within a constrained environment or looking to integrate smart code assistance into smaller workflows, phi-3-mini is engineered to adaptâ€”think scenarios like phi 3 mini android integrations or containerized services.Â </p><p>We have incorporated some part of phi 2 model in Pieces for on-device ml, more on that in a moment ðŸ˜‰</p><h3>How to run phi-3 mini</h3><p>Getting started with phi-3-mini is relatively straightforward.Â </p><p>You can download phi 3 mini directly from sources like<!--$--><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct" rel="noopener"> Hugging Face</a><!--/$-->. Check the phi 3 mini requirements to ensure compatibility.Â </p><p>Most teams find that a phi-3-mini robust structure translates into easier deployments on local machines or modest cloud setups. </p><p>For organizations needing full-scale enterprise solutions, consider <strong>azure deploy phi-3.5-mini.Â </strong></p><p>As highlighted in<!--$--><a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/" rel="noopener"> Azureâ€™s blog post</a><!--/$-->, Azureâ€™s support ensures production-grade stability, security, and the convenience of scaling up or down as per your project requirements.</p><h3>Integrating phi mini models with Pieces</h3><p>At Pieces, weâ€™ve previously showcased whatâ€™s possible by building copilots with earlier Phi models.Â </p><p>In our<!--$--><a href="https://pieces.app/blog/build-a-copilot-with-phi-2-using-pieces-client" rel="noopener"> copilot integration with Phi-2</a><!--/$-->, we demonstrated how to connect an LLMâ€™s capabilities to the Pieces Client seamlessly.Â </p><p>The same strategy applies to phi-3-mini: Pieces can store and manage a library of snippets, <!--$--><a href="https://docs.pieces.app/product-highlights-and-benefits/live-context" rel="noopener">supply real-time context</a><!--/$-->, and make feeding that context into phi-3-mini effortless.Â </p><p>This synergy accelerates tasks like <!--$--><a href="https://pieces.app/features/generate" rel="noopener">code generation</a><!--/$-->, refactoring, and maintaining a consistent coding style across your team.Â </p><p>Pieces CLI would be an awesome place to experiment with some of these capabilities.Â </p><h2>A practical example with Ollama and Pieces</h2><p>To illustrate phi-3-mini in action, I recommend exploring its integration with<!--$--><a href="https://ollama.com/library/phi3" rel="noopener"> Ollama</a><!--/$--> and Pieces CLI.Â </p><p>Ollama allows you to <!--$--><a href="https://pieces.app/blog/ollama-local-llm-powered" target="_blank" rel="noopener">interact with LLMs locally</a><!--/$-->, providing a straightforward environment for experimentation.Â </p><p>Pieces complements this by managing and organizing your code snippets, ensuring that whenever you return to phi-3-mini for refinement or a fresh snippet, everything is at your fingertips.</p><h4>Prerequisites:</h4><ul><li data-preset-tag="p"><p><!--$--><a href="https://ollama.com/download" rel="noopener">Ollama installed</a><!--/$--> and configured.</p></li></ul><p>The phi-3 model (or phi-3-mini variant) available locally:</p><ul><li data-preset-tag="p"><p><!--$--><a href="https://docs.pieces.app/extensions-plugins/cli/" rel="noopener">Pieces CLI</a><!--/$--> installed and authenticated.</p></li></ul><h3>Steps for generating code with Ollama</h3><p>Expect a code snippet that would do just this. Review it to ensure it meets your needs, and try prompting differently for a better outcome if you need to.</p><h3>Store snippet in Pieces</h3><p>Add the returned code to Pieces for easy retrieval later.</p><p>Make sure <!--$--><a href="https://pieces.app/plugins/cli" rel="noopener">Pieces CLI</a><!--/$--> is installed and properly configured on your machine.</p><p>Copy the code from the terminal. Pieces CLI can access your pastebin to store this code.</p><p>Run the Pieces create command in the terminal, and it&#39;ll get the code from your machine and store it in the Pieces platform, which you can use later on anywhere.</p><p>What to see your saved snippet? Use the below command or you can simply organise it in the Pieces for Developers Desktop App.</p><h3>Refine as neededÂ </h3><p>Retrieve the snippet, refine it with phi-3-mini through Ollama by adjusting the prompt, and store the improved version back in Pieces.Â </p><p>Over time, you build a curated library of production-ready snippets, shaped by your specific needs.</p><h3>What is the downside of the Phi-3 Mini?</h3><p>Though the Phi-3 model excels at a lot of things, one thing it still lacks is maintaining the context window overflow and when it does overflow the model replies back with gibberish.Â </p><p>This is <!--$--><a href="https://github.com/microsoft/Phi-3CookBook/issues/45" rel="noopener">reported by the community</a><!--/$--> and is likely to be fixed in the future ONNX release.</p><h2>Wrapping it up</h2><p>LLM phi-3-mini demonstrates that smaller doesnâ€™t mean weaker.Â </p><p>By delivering strong code generation and reasoning capabilities, handling extensive contexts gracefully, and offering adaptability through fine-tuning, phi-3-mini positions itself as an invaluable tool in modern development pipelines.Â </p><p>When combined with tools like Ollama and Pieces, it can significantly enhance productivity, consistency, and the overall developer experience.</p><p>If youâ€™re looking to streamline code generation, maintain context across complex projects, and integrate seamlessly with existing workflows, phi-3-mini deserves serious consideration.Â </p><p>With Microsoftâ€™s introduction of <!--$--><a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090" rel="noopener">Phi-4</a><!--/$-->, the Phi series continues to push the boundaries of what smaller language models can achieve.Â </p><p>As you integrate phi-3-mini into your workflow and potentially fine-tune it for specific tasks, know that thereâ€™s a natural progression toward even more capable models like Phi-4.Â </p><p>This ensures youâ€™re not just investing in a single model, but in an evolving ecosystem that grows with your needs.</p></div></div>
  </body>
</html>
