<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/coqui-ai/TTS">Original</a>
    <h1>Coqui TTS: a deep learning toolkit for Text-to-Speech</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS is a library for advanced Text-to-Speech generation. It&#39;s built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality.
<g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS comes with pretrained models, tools for measuring dataset quality and already used in <strong>20+ languages</strong> for products and research projects.</p>
<p dir="auto"><a href="https://gitter.im/coqui-ai/TTS?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge" rel="nofollow"><img src="https://camo.githubusercontent.com/a32e760bf6cc36c37b0179983490a428119d9d471b1002aa4c32a78b06a6c58f/68747470733a2f2f6261646765732e6769747465722e696d2f636f7175692d61692f5454532e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/coqui-ai/TTS.svg"/></a>
<a href="https://opensource.org/licenses/MPL-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/3084759be956bac908efca9e9d5e1923cdba04dc02bcbddb476f207cd92f74d4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d504c253230322e302d627269676874677265656e2e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg"/></a>
<a href="https://badge.fury.io/py/TTS" rel="nofollow"><img src="https://camo.githubusercontent.com/1935418575536d213b56d243e878e5f82b7d94434b72b9e10e1a21acb9337d07/68747470733a2f2f62616467652e667572792e696f2f70792f5454532e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/TTS.svg"/></a>
<a href="https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md"><img src="https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667" alt="Covenant"/></a>
<a href="https://pepy.tech/project/tts" rel="nofollow"><img src="https://camo.githubusercontent.com/6e898ee06b06605c0238e647672ee940748b6982de56bfb6b500caf79b987311/68747470733a2f2f706570792e746563682f62616467652f747473" alt="Downloads" data-canonical-src="https://pepy.tech/badge/tts"/></a>
<a href="https://zenodo.org/badge/latestdoi/265612440" rel="nofollow"><img src="https://camo.githubusercontent.com/c5dcc115831dde5596a7ba9b01490ac79acd621e3a39236297d7a3dbc033b682/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3236353631323434302e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/265612440.svg"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg" alt="GithubActions"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests.yml/badge.svg"><img src="https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests.yml/badge.svg" alt="GithubActions"/></a>
<a href="https://tts.readthedocs.io/en/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/63ec372fb79c41edff88abb9979b02499563180e85a3ef3f98886a1d89901e19/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7474732f62616467652f3f76657273696f6e3d6c6174657374267374796c653d706c6173746963" alt="Docs" data-canonical-src="https://readthedocs.org/projects/tts/badge/?version=latest&amp;style=plastic"/></a></p>
<p dir="auto"><g-emoji alias="newspaper" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png">ğŸ“°</g-emoji> <a href="https://coqui.ai/?subscription=true" rel="nofollow"><strong>Subscribe to <g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>Coqui.ai Newsletter</strong></a></p>
<p dir="auto"><g-emoji alias="loudspeaker" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png">ğŸ“¢</g-emoji> <a href="https://erogol.github.io/ddc-samples/" rel="nofollow">English Voice Samples</a> and <a href="https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2" rel="nofollow">SoundCloud playlist</a></p>
<p dir="auto"><g-emoji alias="page_facing_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png">ğŸ“„</g-emoji> <a href="https://github.com/erogol/TTS-papers">Text-to-Speech paper collection</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/21de59a73f538420fc4733927fab1d9e094448cb396a560714d54ab3a060cb4f/68747470733a2f2f7374617469632e73636172662e73682f612e706e673f782d707869643d63663331376665372d323138382d343732312d626330312d313234626235643564626232"><img src="https://camo.githubusercontent.com/21de59a73f538420fc4733927fab1d9e094448cb396a560714d54ab3a060cb4f/68747470733a2f2f7374617469632e73636172662e73682f612e706e673f782d707869643d63663331376665372d323138382d343732312d626330312d313234626235643564626232" data-canonical-src="https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2"/></a></p>
<h2 dir="auto"><a id="user-content--where-to-ask-questions" href="#-where-to-ask-questions" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png">ğŸ’¬</g-emoji> Where to ask questions</h2>
<p dir="auto">Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Platforms</th>
</tr>
</thead>
<tbody>
<tr>
<td><g-emoji alias="rotating_light" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a8.png">ğŸš¨</g-emoji> <strong>Bug Reports</strong></td>
<td><a href="https://github.com/coqui-ai/tts/issues">GitHub Issue Tracker</a></td>
</tr>
<tr>
<td><g-emoji alias="gift" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f381.png">ğŸ</g-emoji> <strong>Feature Requests &amp; Ideas</strong></td>
<td><a href="https://github.com/coqui-ai/tts/issues">GitHub Issue Tracker</a></td>
</tr>
<tr>
<td><g-emoji alias="woman_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-1f4bb.png">ğŸ‘©â€ğŸ’»</g-emoji> <strong>Usage Questions</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/discussions">Github Discussions</a></td>
</tr>
<tr>
<td><g-emoji alias="right_anger_bubble" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5ef.png">ğŸ—¯</g-emoji> <strong>General Discussion</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/discussions">Github Discussions</a> or <a href="https://gitter.im/coqui-ai/TTS?utm_source=share-link&amp;utm_medium=link&amp;utm_campaign=share-link" rel="nofollow">Gitter Room</a></td>
</tr>
</tbody>
</table>
<h2 dir="auto"><a id="user-content--links-and-resources" href="#-links-and-resources" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png">ğŸ”—</g-emoji> Links and Resources</h2>
<table>
<thead>
<tr>
<th>Type</th>
<th>Links</th>
</tr>
</thead>
<tbody>
<tr>
<td><g-emoji alias="briefcase" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bc.png">ğŸ’¼</g-emoji> <strong>Documentation</strong></td>
<td><a href="https://tts.readthedocs.io/en/latest/" rel="nofollow">ReadTheDocs</a></td>
</tr>
<tr>
<td><g-emoji alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png">ğŸ’¾</g-emoji> <strong>Installation</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/tree/dev#install-tts">TTS/README.md</a></td>
</tr>
<tr>
<td><g-emoji alias="woman_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-1f4bb.png">ğŸ‘©â€ğŸ’»</g-emoji> <strong>Contributing</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a></td>
</tr>
<tr>
<td><g-emoji alias="pushpin" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cc.png">ğŸ“Œ</g-emoji> <strong>Road Map</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/issues/378" data-hovercard-type="issue" data-hovercard-url="/coqui-ai/TTS/issues/378/hovercard">Main Development Plans</a></td>
</tr>
<tr>
<td><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">ğŸš€</g-emoji> <strong>Released Models</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/releases">TTS Releases</a> and <a href="https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models">Experimental Models</a></td>
</tr>
</tbody>
</table>
<h2 dir="auto"><a id="user-content--tts-performance" href="#-tts-performance" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="1st_place_medal" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f947.png">ğŸ¥‡</g-emoji> TTS Performance</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png"><img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png" width="800"/></a></p>
<p dir="auto">Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are <g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS models</p>

<h2 dir="auto"><a id="user-content-features" href="#features" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Features</h2>
<ul dir="auto">
<li>High-performance Deep Learning models for Text2Speech tasks.
<ul dir="auto">
<li>Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).</li>
<li>Speaker Encoder to compute speaker embeddings efficiently.</li>
<li>Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)</li>
</ul>
</li>
<li>Fast and efficient model training.</li>
<li>Detailed training logs on the terminal and Tensorboard.</li>
<li>Support for Multi-speaker TTS.</li>
<li>Efficient, flexible, lightweight but feature complete <code>Trainer API</code>.</li>
<li>Released and ready-to-use models.</li>
<li>Tools to curate Text2Speech datasets under<code>dataset_analysis</code>.</li>
<li>Utilities to use and test your models.</li>
<li>Modular (but not too much) code base enabling easy implementation of new ideas.</li>
</ul>
<h2 dir="auto"><a id="user-content-implemented-models" href="#implemented-models" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implemented Models</h2>
<h3 dir="auto"><a id="user-content-text-to-spectrogram" href="#text-to-spectrogram" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Text-to-Spectrogram</h3>
<ul dir="auto">
<li>Tacotron: <a href="https://arxiv.org/abs/1703.10135" rel="nofollow">paper</a></li>
<li>Tacotron2: <a href="https://arxiv.org/abs/1712.05884" rel="nofollow">paper</a></li>
<li>Glow-TTS: <a href="https://arxiv.org/abs/2005.11129" rel="nofollow">paper</a></li>
<li>Speedy-Speech: <a href="https://arxiv.org/abs/2008.03802" rel="nofollow">paper</a></li>
<li>Align-TTS: <a href="https://arxiv.org/abs/2003.01950" rel="nofollow">paper</a></li>
<li>FastPitch: <a href="https://arxiv.org/pdf/2006.06873.pdf" rel="nofollow">paper</a></li>
<li>FastSpeech: <a href="https://arxiv.org/abs/1905.09263" rel="nofollow">paper</a></li>
</ul>
<h3 dir="auto"><a id="user-content-end-to-end-models" href="#end-to-end-models" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>End-to-End Models</h3>
<ul dir="auto">
<li>VITS: <a href="https://arxiv.org/pdf/2106.06103" rel="nofollow">paper</a></li>
</ul>
<h3 dir="auto"><a id="user-content-attention-methods" href="#attention-methods" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Attention Methods</h3>
<ul dir="auto">
<li>Guided Attention: <a href="https://arxiv.org/abs/1710.08969" rel="nofollow">paper</a></li>
<li>Forward Backward Decoding: <a href="https://arxiv.org/abs/1907.09006" rel="nofollow">paper</a></li>
<li>Graves Attention: <a href="https://arxiv.org/abs/1910.10288" rel="nofollow">paper</a></li>
<li>Double Decoder Consistency: <a href="https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/" rel="nofollow">blog</a></li>
<li>Dynamic Convolutional Attention: <a href="https://arxiv.org/pdf/1910.10288.pdf" rel="nofollow">paper</a></li>
<li>Alignment Network: <a href="https://arxiv.org/abs/2108.10447" rel="nofollow">paper</a></li>
</ul>
<h3 dir="auto"><a id="user-content-speaker-encoder" href="#speaker-encoder" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Speaker Encoder</h3>
<ul dir="auto">
<li>GE2E: <a href="https://arxiv.org/abs/1710.10467" rel="nofollow">paper</a></li>
<li>Angular Loss: <a href="https://arxiv.org/pdf/2003.11982.pdf" rel="nofollow">paper</a></li>
</ul>
<h3 dir="auto"><a id="user-content-vocoders" href="#vocoders" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vocoders</h3>
<ul dir="auto">
<li>MelGAN: <a href="https://arxiv.org/abs/1910.06711" rel="nofollow">paper</a></li>
<li>MultiBandMelGAN: <a href="https://arxiv.org/abs/2005.05106" rel="nofollow">paper</a></li>
<li>ParallelWaveGAN: <a href="https://arxiv.org/abs/1910.11480" rel="nofollow">paper</a></li>
<li>GAN-TTS discriminators: <a href="https://arxiv.org/abs/1909.11646" rel="nofollow">paper</a></li>
<li>WaveRNN: <a href="https://github.com/fatchord/WaveRNN/">origin</a></li>
<li>WaveGrad: <a href="https://arxiv.org/abs/2009.00713" rel="nofollow">paper</a></li>
<li>HiFiGAN: <a href="https://arxiv.org/abs/2010.05646" rel="nofollow">paper</a></li>
<li>UnivNet: <a href="https://arxiv.org/abs/2106.07889" rel="nofollow">paper</a></li>
</ul>
<p dir="auto">You can also help us implement more models.</p>
<h2 dir="auto"><a id="user-content-install-tts" href="#install-tts" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install TTS</h2>
<p dir="auto"><g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS is tested on Ubuntu 18.04 with <strong>python &gt;= 3.7, &lt; 3.11.</strong>.</p>
<p dir="auto">If you are only interested in <a href="https://tts.readthedocs.io/en/latest/inference.html" rel="nofollow">synthesizing speech</a> with the released <g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS models, installing from PyPI is the easiest option.</p>

<p dir="auto">If you plan to code or train models, clone <g-emoji alias="frog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f438.png">ğŸ¸</g-emoji>TTS and install it locally.</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/coqui-ai/TTS
pip install -e .[all,dev,notebooks]  # Select the relevant extras"><pre>git clone https://github.com/coqui-ai/TTS
pip install -e .[all,dev,notebooks]  <span><span>#</span> Select the relevant extras</span></pre></div>
<p dir="auto">If you are on Ubuntu (Debian), you can also run following commands for installation.</p>
<div data-snippet-clipboard-copy-content="$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.
$ make install"><pre>$ make system-deps  <span><span>#</span> intended to be used on Ubuntu (Debian). Let us know if you have a different OS.</span>
$ make install</pre></div>
<p dir="auto">If you are on Windows, <g-emoji alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png">ğŸ‘‘</g-emoji>@GuyPaddock wrote installation instructions <a href="https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system" rel="nofollow">here</a>.</p>
<h2 dir="auto"><a id="user-content-use-tts" href="#use-tts" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Use TTS</h2>
<h3 dir="auto"><a id="user-content-single-speaker-models" href="#single-speaker-models" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Single Speaker Models</h3>
<ul dir="auto">
<li>
<p dir="auto">List provided models:</p>

</li>
<li>
<p dir="auto">Get model info (for both tts_models and vocoder_models):</p>
<ul dir="auto">
<li>
<p dir="auto">Query by type/name:
The model_info_by_name uses the name as it from the --list_models.</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;"><pre><code>$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts"><pre><code>$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2"><pre><code>$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2
</code></pre></div>
</li>
<li>
<p dir="auto">Query by type/idx:
The model_query_idx uses the corresponding idx from --list_models.</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_idx &#34;&lt;model_type&gt;/&lt;model_query_idx&gt;&#34;"><pre><code>$ tts --model_info_by_idx &#34;&lt;model_type&gt;/&lt;model_query_idx&gt;&#34;
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_idx tts_models/3 "><pre><code>$ tts --model_info_by_idx tts_models/3 
</code></pre></div>
</li>
</ul>
</li>
<li>
<p dir="auto">Run TTS with default models:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run a TTS model with its default vocoder model:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run with specific TTS and vocoder models from the list:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --vocoder_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --vocoder_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own TTS model (Using Griffin-Lim Vocoder):</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own TTS and Vocoder models:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_path path/to/config.json --config_path path/to/model.pth --out_path output/path/speech.wav
    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_path path/to/config.json --config_path path/to/model.pth --out_path output/path/speech.wav
    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json
</code></pre></div>
</li>
</ul>
<h3 dir="auto"><a id="user-content-multi-speaker-models" href="#multi-speaker-models" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-speaker Models</h3>
<ul dir="auto">
<li>
<p dir="auto">List the available speakers and choose as &lt;speaker_id&gt; among them:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --list_speaker_idxs"><pre><code>$ tts --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --list_speaker_idxs
</code></pre></div>
</li>
<li>
<p dir="auto">Run the multi-speaker TTS model with the target speaker ID:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --speaker_idx &lt;speaker_id&gt;"><pre><code>$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --speaker_idx &lt;speaker_id&gt;
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own multi-speaker TTS model:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth --speakers_file_path path/to/speaker.json --speaker_idx &lt;speaker_id&gt;"><pre><code>$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth --speakers_file_path path/to/speaker.json --speaker_idx &lt;speaker_id&gt;
</code></pre></div>
</li>
</ul>
<h2 dir="auto"><a id="user-content-directory-structure" href="#directory-structure" aria-hidden="true"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Directory Structure</h2>
<div data-snippet-clipboard-copy-content="|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)
|- utils/           (common utilities.)
|- TTS
    |- bin/             (folder for all the executables.)
      |- train*.py                  (train your target model.)
      |- distribute.py              (train your TTS model using Multiple GPUs.)
      |- compute_statistics.py      (compute dataset statistics for normalization.)
      |- ...
    |- tts/             (text to speech models)
        |- layers/          (model layer definitions)
        |- models/          (model definitions)
        |- utils/           (model specific utilities.)
    |- speaker_encoder/ (Speaker Encoder models.)
        |- (same)
    |- vocoder/         (Vocoder models.)
        |- (same)"><pre><code>|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)
|- utils/           (common utilities.)
|- TTS
    |- bin/             (folder for all the executables.)
      |- train*.py                  (train your target model.)
      |- distribute.py              (train your TTS model using Multiple GPUs.)
      |- compute_statistics.py      (compute dataset statistics for normalization.)
      |- ...
    |- tts/             (text to speech models)
        |- layers/          (model layer definitions)
        |- models/          (model definitions)
        |- utils/           (model specific utilities.)
    |- speaker_encoder/ (Speaker Encoder models.)
        |- (same)
    |- vocoder/         (Vocoder models.)
        |- (same)
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
