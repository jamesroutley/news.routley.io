<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://catalinionescu.dev/ai-agent/building-ai-agent-part-1/">Original</a>
    <h1>Building an AI agent inside a 7-year-old Rails monolith</h1>
    
    <div id="readability-page-1" class="page"><div data-toc-content=""><p>I’m a Director of Engineering at <a href="https://www.monami.io/">Mon Ami</a>, a US-based start-up building a SaaS solution for Aging and Disability Case Workers. We built a large Ruby on Rails monolith over the last 7 years.</p><p>It’s a multi-tenant solution where data sensitivity is crucial. We have multiple layers of access checks, but to simplify the story, we’ll assume it’s all abstracted away into a <a href="https://github.com/varvet/pundit">Pundit</a> policy.</p><p>While I would not describe us as a group dealing with Big Data problems, we do have <em>a lot</em> of data. Looking up clients’ records is, in particular, an action that is just not performant enough with raw database operations, so we built an Algolia index to make it work.</p><p>Given all that: the big monolith, complicated data access rules, and the nature of the business we are in, building an AI agent has not yet been a primary concern for us.</p><h2 id="sf-ruby-and-the-disconnect">SF Ruby, and the disconnect</h2><p>I was at <a href="https://sfruby.com/">SF Ruby</a>, in San Francisco, a few weeks ago. Most of the tracks were, of course, heavily focused on AI. Lots of stories from people building AIs into all sorts of products using Ruby and Rails,</p><p>They were good talks. But most of them assumed a kind of software I don’t work on — systems without strong boundaries, without multi-tenant concerns, without deeply embedded authorization rules.</p><p>I kept thinking: this is interesting, but it doesn’t map cleanly to my world. At Mon Ami, we can’t just release a pilot unless it passes strict data access checks.</p><p>Then I saw a talk about using the RubyLLM gem to build a RAG-like system. The conversation (LLM calls) context was augmented using function calls (tools). This is when it clicked. I could encode my complicated access logic into a specific function call and ensure the LLM gets access to some of our data without having to give it unrestricted access.</p><h2 id="rubyllm">RubyLLM</h2><p><a href="https://github.com/crmne/ruby_llm">RubyLLM</a> is a neat gem that abstracts away the interaction with many LLM providers with a clean API.</p><pre data-language="ruby" tabindex="0"><code><span><span>gem </span><span>&#34;ruby_llm&#34;</span></span></code></pre><p>It is configured in an initializer with the API keys for the providers you want to use.</p><pre data-language="ruby" tabindex="0"><code><span><span>RubyLLM</span><span>.</span><span>configure</span><span> do</span><span> |config|</span></span>
<span><span>  config.</span><span>openai_api_key</span><span> =</span><span> Rails</span><span>.</span><span>application</span><span>.</span><span>credentials</span><span>.</span><span>dig</span><span>(</span><span>:openai_api_key</span><span>)</span></span>
<span><span>  config.</span><span>anthropic_api_key</span><span> =</span><span> Rails</span><span>.</span><span>application</span><span>.</span><span>credentials</span><span>.</span><span>dig</span><span>(</span><span>:anthropic_api_key</span><span>)</span></span>
<span><span>  # config.default_model = &#34;gpt-4.1-nano&#34;</span></span>
<span></span>
<span><span>  # Use the new association-based acts_as API (recommended)</span></span>
<span><span>  config.</span><span>use_new_acts_as</span><span> =</span><span> true</span></span>
<span></span>
<span><span>  # Increase timeout for slow API responses</span></span>
<span><span>  config.</span><span>request_timeout</span><span> =</span><span> 600</span><span>  # 10 minutes (default is 300)</span></span>
<span><span>  config.</span><span>max_retries</span><span> =</span><span> 3</span><span>        # Retry failed requests</span></span>
<span><span>end</span></span>
<span></span>
<span><span># Load LLM tools from main app</span></span>
<span><span>Dir</span><span>[</span><span>Rails</span><span>.</span><span>root</span><span>.</span><span>join</span><span>(</span><span>&#39;app/tools/**/*.rb&#39;</span><span>)].</span><span>each</span><span> { |f| </span><span>require</span><span> f }</span></span></code></pre><p>It provides a Conversation model as an abstraction for an LLM thread. The Conversation contains a set of Messages. It also provides a way of defining structured responses and function calls available.</p><pre data-language="ruby" tabindex="0"><code><span><span>AVAILABLE_TOOLS</span><span> =</span><span> [</span></span>
<span><span>  Tools</span><span>::</span><span>Client</span><span>::</span><span>SearchTool</span></span>
<span><span>].</span><span>freeze</span></span>
<span></span>
<span><span>conversation</span><span> =</span><span> Conversation</span><span>.</span><span>find</span><span>(conversation_id)</span></span>
<span><span>chat</span><span> =</span><span> conversation.</span><span>with_tools</span><span>(</span><span>*</span><span>AVAILABLE_TOOLS</span><span>)</span></span>
<span></span>
<span><span>chat.</span><span>ask</span><span> &#39;What is the phone number for John Snow?&#39;</span></span></code></pre><p>A Conversation is initialized by passing a model (gpt-5, claude-sonnet-4.5, etc) and has a method for chatting to it.</p><pre data-language="ruby" tabindex="0"><code><span><span>conversation</span><span> =</span><span> Conversation</span><span>.</span><span>new</span><span>(</span><span>model:</span><span> RubyLLM</span><span>::</span><span>Model</span><span>.</span><span>find_by</span><span>(</span><span>model_id:</span><span> &#39;gpt-4o-mini&#39;</span><span>))</span></span></code></pre><p>RubyLLM comes with a neat DSL for defining accepted parameters (the descriptions are passed to the LLM as context since it needs to decide if the tool should be used based on the conversation). The tool implements an execute method returning a hash. The hash is then presented to the LLM. This is all the magic needed.</p><pre data-language="ruby" tabindex="0"><code><span><span>class</span><span> SearchTool</span><span> &lt; </span><span>BaseTool</span></span>
<span><span>  description </span><span>&#39;Search for clients by name, ID, or email address. Returns matching clients.&#39;</span></span>
<span></span>
<span><span>  param </span><span>:query</span><span>,</span></span>
<span><span>    desc:</span><span> &#39;Search query - can be client name, ID, or email address&#39;</span><span>,</span></span>
<span><span>    type:</span><span> :string</span></span>
<span></span>
<span><span>  def</span><span> execute</span><span>(</span><span>query:</span><span>)</span></span>
<span><span>  end</span></span>
<span><span>end</span></span></code></pre><p>We’ll now build a modest function call and a messaging interface. The function call allows searching a client using Algolia and ensuring the resulting set is visible to the user (by merging in the pundit policy).</p><pre data-language="ruby" tabindex="0"><code><span><span>def</span><span> execute</span><span>(</span><span>query:</span><span>)</span></span>
<span><span>  response</span><span> =</span><span> Algolia</span><span>::</span><span>SearchClient</span></span>
<span><span>    .</span><span>create</span><span>(app_id, search_key)</span></span>
<span><span>    .</span><span>search_single_index</span><span>(</span><span>Client</span><span>.</span><span>index_name</span><span>, {</span></span>
<span><span>      query:</span><span> query.</span><span>truncate</span><span>(</span><span>250</span><span>)</span></span>
<span><span>    })</span></span>
<span></span>
<span><span>  ids</span><span> =</span><span> response.</span><span>hits</span><span>.</span><span>map</span><span> { |hit| hit[</span><span>:id</span><span>] }.</span><span>compact</span></span>
<span></span>
<span><span>  base_scope</span><span> =</span><span> Client</span><span>.</span><span>where</span><span>(</span><span>id:</span><span> ids)</span></span>
<span><span>  client</span><span> =</span><span> Admin</span><span>::</span><span>Org</span><span>::</span><span>ClientPolicy</span><span>::</span><span>Scope</span><span>.</span><span>new</span><span>(base_scope).</span><span>resolve</span><span>.</span><span>first</span><span> or</span><span> return</span><span> {}</span></span>
<span></span>
<span><span>  {</span></span>
<span><span>    id:</span><span> client.</span><span>id</span><span>,</span></span>
<span><span>    ami_id:</span><span> client.</span><span>slug</span><span>,</span></span>
<span><span>    slug:</span><span> client.</span><span>slug</span><span>,</span></span>
<span><span>    name:</span><span> client.</span><span>full_name</span><span>,</span></span>
<span><span>    email:</span><span> client.</span><span>email</span></span>
<span><span>  }</span></span>
<span><span>end</span></span></code></pre><p>The LLM acts as the magic glue between the natural language input submitted by the user, decides which (if any) tool to use to augment the context, and then responds to the user. No model should ever know Jon Snow’s phone number from a SaaS service, but this approach allows this sort of retrieval.</p><p>The UI is built with a remote form that enqueues an Active Job.</p><pre data-language="haml" tabindex="0"><code><span><span>= turbo_stream_from @conversation, </span><span>:messages</span></span>
<span></span>
<span><span>.container-fluid.h-100.d-flex.flex-column</span></span>
<span><span>  .sticky-top</span></span>
<span><span>    %</span><span>h2</span><span>.mb-0</span></span>
<span><span>      Conversation ##{@conversation.id}</span></span>
<span></span>
<span><span>  .flex-grow-1</span></span>
<span><span>    = render @messages</span></span>
<span></span>
<span><span>  .p-3.border-top.bg-white.sticky-bottom#message-form</span></span>
<span><span>  = form_with </span><span>url:</span><span> path, </span><span>method:</span><span> :post</span><span>, </span><span>local:</span><span> false</span><span>, </span><span>data:</span><span> { </span><span>turbo_stream:</span><span> true</span><span> } </span><span>do</span><span> |f|</span></span>
<span><span>    = f.</span><span>text_area</span><span> :content</span></span>
<span><span>    = f.</span><span>submit</span><span> &#39;Send&#39;</span></span></code></pre><p>The job will process the Message.</p><pre data-language="ruby" tabindex="0"><code><span><span>class</span><span> ProcessMessageJob</span><span> &lt; </span><span>ApplicationJob</span></span>
<span><span>  queue_as </span><span>:default</span></span>
<span></span>
<span><span>  def</span><span> perform</span><span>(conversation_id, message)</span></span>
<span><span>    conversation</span><span> =</span><span> Conversation</span><span>.</span><span>find</span><span>(conversation_id)</span></span>
<span><span>    conversation.</span><span>ask</span><span> message</span></span>
<span><span>  end</span></span>
<span><span>end</span></span></code></pre><p>The conversation has broadcast refresh enabled to update the UI when the response is received.</p><pre data-language="ruby" tabindex="0"><code><span><span>class</span><span> Conversation</span><span> &lt; </span><span>RubyLLM::Conversation</span></span>
<span><span>  broadcasts_refreshes</span></span>
<span><span>end</span></span></code></pre><p>The form has a stimulus controller that checks for new messages being appended in order to scroll to the end of the conversation.</p><h2 id="a-note-on-selecting-the-model">A note on selecting the model</h2><p>I checked a few OpenAI models for this implementation: gpt-5, gpt-4o, gpt4. GPT-5 has a big context, meaning we could have long-running conversations, but because there are a number of round-trips, the delay to queries requiring 3+ consecutive tools made the Agent feel sluggish.</p><p>GPT-4, on the other hand, is interestingly very prone to hallucinations - rushing to respond to queries with made-up data instead of calling the necessary tools. GPT-4o strikes, so far, the best balance between speed and correctness.</p><h2 id="closing-thoughts">Closing thoughts</h2><p>Building this tool took probably about 2-3 days of Claude-powered development (AIs building AIs). The difficulty and the complexity of building such a tool were the things that surprised me the most. The tool service object is essentially an API controller action - pass inputs and get a JSON back. Interestingly.</p><p>Before building this Agent, I looked at the other gems in this space. ActiveAgent (a somewhat similar gem for interacting with LLMs) is a decent contender that moves the prompts to a view file. It didn’t fit my needs since it had no built-in support for defining tools or having long-running conversations.</p></div></div>
  </body>
</html>
