<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ollama/ollama/pull/5059">Original</a>
    <h1>Llama.cpp supports Vulkan. why doesn&#39;t Ollama?</h1>
    
    <div id="readability-page-1" class="page"><div disabled="" sortable="">
<div>
          <p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/pepijndevos/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/pepijndevos">@pepijndevos</a> Thanks for letting me know. After setting <code>GGML_VK_FORCE_MAX_ALLOCATION_SIZE</code>, I verified that <code>llama3.1 8B</code> works fine. However, I noticed a strange issue where models around 12–13 GiB in size fail to upload to the GPU. The CLI only shows the loading indicator continuously without any response.</p>
<ul dir="auto">
<li><strong>Successful:</strong>
<ul dir="auto">
<li><code>llama3.1:8b-instruct-q8_0</code> (7.95 GiB)</li>
<li><code>gemma2:27b-text-q3_K_S</code> (11.33 GiB)</li>
</ul>
</li>
<li><strong>Failed:</strong>
<ul dir="auto">
<li><code>gemma2:27b-instruct-q3_K_L</code> (13.52 GiB)</li>
<li><code>llama3.1:8b-instruct-fp16</code> (14.96 GiB)</li>
</ul>
</li>
</ul>
<p dir="auto">When the upload is successful, only one Vulkan device (the AMD GPU) is detected. However, when it fails, three Vulkan devices are detected instead. In <a href="https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-vulkan/ggml-vulkan.cpp#L2821">this section of the llama.cpp code</a>, the size of <code>device_indices</code> changes depending on the model size, and I wonder if this might be related to the error.</p>
<details>
    <summary>Successful Case</summary>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67185299/404688913-2ff829cb-189d-4b07-ba0d-00a6635accd8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgzMjk5MTcsIm5iZiI6MTczODMyOTYxNywicGF0aCI6Ii82NzE4NTI5OS80MDQ2ODg5MTMtMmZmODI5Y2ItMTg5ZC00YjA3LWJhMGQtMDBhNjYzNWFjY2Q4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMxVDEzMjAxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5MDUwMzcyMTU3ZDlhZjFhOGEyYTIzOTk4MTAzNWM4NTBiOTgwNjQwNTFlOTVhNmQ1Njc3YjFhYjE2N2IyZDImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.xTtTno7brV4Y01wBCfO5kWTaQnev71zM07-2idN9UvI"><img width="907" alt="image" src="https://private-user-images.githubusercontent.com/67185299/404688913-2ff829cb-189d-4b07-ba0d-00a6635accd8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgzMjk5MTcsIm5iZiI6MTczODMyOTYxNywicGF0aCI6Ii82NzE4NTI5OS80MDQ2ODg5MTMtMmZmODI5Y2ItMTg5ZC00YjA3LWJhMGQtMDBhNjYzNWFjY2Q4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMxVDEzMjAxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5MDUwMzcyMTU3ZDlhZjFhOGEyYTIzOTk4MTAzNWM4NTBiOTgwNjQwNTFlOTVhNmQ1Njc3YjFhYjE2N2IyZDImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.xTtTno7brV4Y01wBCfO5kWTaQnev71zM07-2idN9UvI"/></a>
</details>
<details>
    <summary>Failed Case</summary>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67185299/404688955-1d135dd9-1d6c-43bc-8c4a-605336bb3a72.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgzMjk5MTcsIm5iZiI6MTczODMyOTYxNywicGF0aCI6Ii82NzE4NTI5OS80MDQ2ODg5NTUtMWQxMzVkZDktMWQ2Yy00M2JjLThjNGEtNjA1MzM2YmIzYTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMxVDEzMjAxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ0MTZjNThhNGZmYjZiN2I2YmVmNDVmMmU5ZjUxNDM0MGRmZjViMjE2M2FiMWI4NzgwODMyYzMwNzE1ZmJmMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i77imQ9siDcsRcwBQxU3L3vaMkWoHETvRjZX1kus0qY"><img width="909" alt="image" src="https://private-user-images.githubusercontent.com/67185299/404688955-1d135dd9-1d6c-43bc-8c4a-605336bb3a72.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgzMjk5MTcsIm5iZiI6MTczODMyOTYxNywicGF0aCI6Ii82NzE4NTI5OS80MDQ2ODg5NTUtMWQxMzVkZDktMWQ2Yy00M2JjLThjNGEtNjA1MzM2YmIzYTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMxVDEzMjAxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ0MTZjNThhNGZmYjZiN2I2YmVmNDVmMmU5ZjUxNDM0MGRmZjViMjE2M2FiMWI4NzgwODMyYzMwNzE1ZmJmMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i77imQ9siDcsRcwBQxU3L3vaMkWoHETvRjZX1kus0qY"/></a>
</details>
<hr/>
<p dir="auto">The Maven core is fascinating, and I’m also interested in local LLMs due to data privacy concerns. Could you provide links to the Raspberry Pi images with AMD drivers and Vulkan LLM Docker images? If new Docker images are created, please let me know as well.</p>
<p dir="auto">For my GPU setup, I initially gained a lot of insights from the information shared by Geerlingguy. I’m using a Pineberry Pi UpCity HAT to connect a GPU, but the GPU is too large to securely mount on a Raspberry Pi. Recently, I purchased the AOOSTART AG01 dock, though it hasn’t arrived yet. This dock includes a power switch for the GPU. One thing to watch out for is the wattage, as the dock seems to support GPUs with a power consumption of up to 250W. For AMD GPUs, it appears the 7700 XT is the maximum supported model.</p>
<p dir="auto">For the power supply, I chose the MSI A850GL. It was one of the very few options that seemed capable of connecting two GPUs to a single supply due to its cable and port configuration. First, I tried using NVIDIA’s RTX 4060 Ti (the only 16GB memory option in the 4000 series), but I had issues getting the driver to recognize it properly. So, I switched to AMD’s RX 7600 XT instead. AMD also offers GPUs with over 20GB of memory, so it’s a viable option if you want to run larger LLMs.</p>
      </div>
</div></div>
  </body>
</html>
