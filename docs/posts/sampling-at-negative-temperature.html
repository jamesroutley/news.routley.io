<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cavendishlabs.org/blog/negative-temperature/">Original</a>
    <h1>Sampling at negative temperature</h1>
    
    <div id="readability-page-1" class="page"><div>
<p><strong>Summary</strong>: Inspired by the definition of temperature in statistical mechanics and the possibility for it to be below zero, we try sampling LLaMA at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo>−</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T=-0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span>0.001</span></span></span></span>. The results are maximally weird.</p>
<h2 id="background"><a href="#background">Background</a></h2>
<h3 id="what_is_temperature"><a href="#what_is_temperature">What is temperature?</a></h3>
<p>The notion of temperature comes from statistical mechanics. Consider a system that has states with energies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>E</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">E_1, \dots, E_n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>E</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>E</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. If the system is in thermal equilibrium, the probability distribution over states is given by the <a href="https://en.wikipedia.org/wiki/Boltzmann distribution">Boltzmann distribution</a>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mi>k</mi><mi>B</mi></msub><mi>T</mi></mrow></msup><mrow><munder><mo>∑</mo><mi>i</mi></munder><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mi>k</mi><mi>B</mi></msub><mi>T</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex"> p_i = \frac{e^{-E_i/k_BT}}{\sum\limits_i e^{-E_i/k_BT}} </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>−</span><span><span>E</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span><span>k</span><span><span><span><span><span><span></span><span><span>B</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>T</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>−</span><span><span>E</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span><span>k</span><span><span><span><span><span><span></span><span><span>B</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>T</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>The distribution is parameterized by a single number, the temperature <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span></span></span></span>. At lower temperatures the lowest-energy states predominate; at higher temperatures there is a more even mix.</p>
<h3 id="temperature_in_neural_nets"><a href="#temperature_in_neural_nets">Temperature in neural nets</a></h3>
<p>At the last layer of a neural net, we apply the <a href="https://en.wikipedia.org/wiki/softmax function">softmax function</a> to the neuron activations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{z_i\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span> to get a vector of probabilities that sum to 1:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>T</mi></mrow></msup><mrow><munder><mo>∑</mo><mi>i</mi></munder><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>T</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex"> p_i = \frac{e^{z_i/T}}{\sum\limits_i e^{z_i/T}} </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span>T</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span>T</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>Wait — this is just the Boltzmann distribution, up to a constant!<sup id="fnref:1"><a href="#fndef:1">[1]</a></sup> </p><table id="fndef:1">
    <tbody><tr>
        <td><a href="#fnref:1">[1]</a></td>
        <td>There&#39;s no minus sign in the exponent because while higher-energy states are less likely, larger logits are more likely.</td>
    </tr>
</tbody></table>

<p>In a language model, temperature is used to define how creative text generations are. For instance, in the zero temperature limit, the model should deterministically generate the most likely token. In the infinite temperature limit, all tokens are equally likely and the model output will be random noise. For an interactive explanation, see <a href="https://lukesalamone.github.io/posts/what-is-temperature/">here</a>.</p>
<h3 id="negative_temperature"><a href="#negative_temperature">Negative temperature</a></h3>
<p>What would it mean to have a temperature that is below zero? (This isn&#39;t the same as the negative Fahrenheit or Celsius temperatures we get on a cold day in Vermont — I mean below zero on an absolute scale like Kelvin).</p>
<p>Isn&#39;t it weird that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">T=\infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>∞</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>−</span><span>∞</span></span></span></span> are the same, but there&#39;s a huge discontinuity around 0? This is because temperature is not the most natural quantity to work with. It makes more sense to speak in terms of the quantity <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><msub><mi>k</mi><mi>B</mi></msub><mi>T</mi></mrow><annotation encoding="application/x-tex">1/k_BT</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1/</span><span><span>k</span><span><span><span><span><span><span></span><span><span>B</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>T</span></span></span></span>, which we call <a href="https://en.wikipedia.org/wiki/Thermodynamic_beta"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>β</span></span></span></span></a>.</p>
<p>Looking at the equations above, if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">T &lt; 0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>&lt;</span><span></span></span><span><span></span><span>0</span></span></span></span> then the sign of the exponent flips. That means that the states that were previously the least likely are now the most likely, and vice versa. As temperature approaches zero from the negative side, the model output will again be deterministic — but this time, the least likely tokens will be output.</p>
<p>Most physical systems have an infinite number of possible states at increasingly higher energy levels. As such, there is no least likely state. So negative temperatures really only make sense in systems with a finite state space. That includes neural nets — there are a finite number of neurons in the last layer.</p>
<h2 id="methodology"><a href="#methodology">Methodology</a></h2>
<p>Unfortunately, OpenAI models only allow sampling with temperatures between 0.0 and 2.0. So if we want to try this, we need a language model we can run locally. We&#39;ll use Meta&#39;s <a href="https://en.wikipedia.org/wiki/LLaMA">LLaMA</a> model with <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>.</p>
<p>Below is the function that is used to sample tokens in <a href="https://github.com/ggerganov/llama.cpp/blob/dfd9fce6d65599bf33df43e616e85aa639bdae4c/llama.cpp#L2049-L2059"><code>llama.cpp</code></a>, slightly simplified for ease of understanding:</p>
<pre><code><span><span>void</span> <span>llama_sample_temperature</span><span>(llama_token_data_array * candidates_p, <span>float</span> temp)</span> </span>{
    <span>for</span> (<span>size_t</span> i = <span>0</span>; i &lt; candidates_p-&gt;size; ++i) {
        candidates_p-&gt;data[i].logit /= temp;
    }
}</code></pre>
<p>So we can just pass in <code>--temperature -0.001</code>? Not quite; in <a href="https://github.com/ggerganov/llama.cpp/blob/dfd9fce6d65599bf33df43e616e85aa639bdae4c/examples/main/main.cpp#L489-L492"><code>examples/main/main.cpp</code></a> there is a check that will apply greedy (most-likely) sampling for any temperatures less than or equal to zero. Applying the following diff and recompiling, we&#39;re good to go:</p>
<pre><code><span>@@ -486,7 +486,7 @@</span> int main(int argc, char ** argv) {
                     logits[llama_token_nl()] = nl_logit;
                 }
<span>-                if (temp &lt;= 0) {</span>
<span>+                if (temp == 0) {</span>
                     // Greedy sampling
                     id = llama_sample_token_greedy(ctx, &amp;candidates_p);
                 } else {</code></pre>
<p>We will also want to disable repetition penalty, top-k, and top-p sampling. Here&#39;s the command we&#39;ll run: <code>./main -m models/7B/ggml-model-q4_0.bin --temp -0.001 --repeat-penalty 1.0 --top-k 0 --top-p 1.0 -p &#34;Temperature is a concept&#34;</code></p>
<h2 id="results"><a href="#results">Results</a></h2>
<p>When running this prompt at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T=0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>0.001</span></span></span></span>, here is the output:</p>
<blockquote>
<p>Temperature is a concept <samp ai-generated="">that is used to describe the degree of hotness or coldness of a substance. The temperature of a substance is measured by the kinetic energy of its molecules. The higher the temperature of a substance, the more kinetic energy its molecules have.</samp></p>
</blockquote>
<p>Now, running it at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo>−</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T = -0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span>0.001</span></span></span></span>:</p>
<blockquote>
<p>Temperature is a concept <samp ai-generated="">Хронологија</samp></p>
</blockquote>
<p>This means that <code>Хронологија</code> is the <em>least likely</em> token to follow this prompt. After one token, the program seems to hang. Perhaps the LLM is trying to speak an unspeakable sentence?</p>
<p>Let&#39;s try it with LLaMA-13B. At <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T=0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>0.001</span></span></span></span>:</p>
<blockquote>
<p>Temperature is a concept <samp ai-generated="">that is used to describe the degree of hotness or coldness of a substance. The temperature of a substance is measured by a thermometer.</samp></p>
</blockquote>
<p>At <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000000</mn></mrow><annotation encoding="application/x-tex">T = 1000000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>1000000</span></span></span></span> (which should be close to entirely random):</p>
<blockquote>
<p>Temperature is a concept <samp ai-generated="">fixesля conven Beng aer situation ton &#39;\Cr villa known vide among entities Ukraine keeps水 Newton Betty Mih різ killedтельство Victoria WeekRefreshbodyunct vir Ja Демо remporte settembre excell succeed fitted))); moy PC highlight located Referencias extendsconfigure\\ incidentWilajagateсия bibli journalist rec cont sovientlyillery恋 finishingც政 rotationintonosti orbiteditor</samp></p>
</blockquote>
<p>And at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo>−</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T = -0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span>0.001</span></span></span></span>:</p>
<blockquote>
<p>Temperature is a concept<samp ai-generated="">]&amp; ]{&#39;archividonnées Kontrola⊤ Kontrola Außer Хронологија costa Хронологија Хронологија Mitchell ez entfernesterd bidZyg entferne osc accom Begriffsklärлист Bedeut WendarchiviicanINCTpenastown Krieg Хронологија loyal vallIAL listade GemeinsBUGiskoshiftpenas ligapenas Хронологијаisko jú Marian Хронологија governor(* Kontrolapenasouw entferne Хронологија Хронологија Dic hornрем earliestантаpenas Promiseriatrarout23;&#39;archividonnées Kontrola⊤ Kontrola Außer Хронологија costa Хронологија Хронологија Mitchell ez entfernesterd bidZyg entferne osc accom Begriffsklärлист Bedeut WendarchiviicanINCTpenastown Krieg Хронологија loyal vallIAL listade GemeinsBUGiskoshiftpenas ligapenas Хронологијаisko jú Marian Хронологија governor(* Kontrolapenasouw entferne Хронологија Хронологија Dic hornрем earliestантаpenas Promiseriatrarout</samp></p>
</blockquote>
<p>The generation continues on and on. At first glance it seems random, but in a sense it should be even less comprehensible than a random sequence of tokens! There are repeated words, sure, but the generation is engineered to be as unlikely as possible.</p>
<p>Why are certain tokens like <code>Хронологија</code> and <code>entferne</code> repeated? Searching these words, I found <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation?commentId=GLRLuYxvMmqhYaw2v">this comment</a> by scottviteri on LessWrong. He points out that these are some of the tokens that are closest to the centroid in LLaMA&#39;s embedding space. That means that LLaMA has very little idea what these tokens mean. In the case of ChatGPT, tokens near the centroid have <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">anomalous properties</a>, so these tokens are likely to have similar effects on LLaMA. Let&#39;s give that a try, at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">T=0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>T</span><span></span><span>=</span><span></span></span><span><span></span><span>0.001</span></span></span></span>.</p>
<p>LLaMA is perfectly capable of repeating most words, even nonsense ones:</p>
<blockquote>
<p>Human: Repeat the word &#34; antferne&#34;.</p>
</blockquote>
<p>But is incapable of outputting this anomalous token:</p>
<blockquote>
<p>Human: Repeat the word &#34; entferne&#34;.</p>
</blockquote>
<p>The anomalous tokens that are the most likely completions at negative temperatures are the least likely completions at positive temperatures, so much so that the model refuses to generate them even in cases where they would be appropriate.</p>

<hr/>
<p>If you&#39;d like to cite this article, you can use this:</p>
<pre>@misc{Kauffman2023negative-temperature,
  author = &#34;Derik Kauffman&#34;,
  title = &#34;Sampling at negative temperature&#34;,
  year = 2023,
  howpublished = &#34;Blog post&#34;,
  url = &#34;https://cavendishlabs.org/blog/negative-temperature/&#34;
}
</pre>
</div></div>
  </body>
</html>
