<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dotat.at/@/2022-07-15-histogram.html">Original</a>
    <h1>Hg64: A 64-bit histogram data structure</h1>
    
    <div id="readability-page-1" class="page"><div data-pagefind-body="">
  <p>I have reached the point in my main project (<a href="https://dotat.at/prog/qp/">qp-trie for BIND</a>)
where I want to collect performance statistics. Things like:</p>
<ul>
<li>how long it takes to compact a trie;</li>
<li>how much memory was recovered by compaction;</li>
<li>size of trie before and after compaction.</li>
</ul>
<p>In a wider context, those using BIND for large authoritative server
systems would like better statistics on things like:</p>
<ul>
<li>incoming and outgoing zone transfer times;</li>
<li>zone refresh query latency.</li>
</ul>
<p>For my own purposes, I’m no longer satisfied with summarizing
performance with just the mean and standard deviation; and in an
operational context, it’s useful to know about the existence of
outliers (rare outliers can be hidden by simple summary statistics)
and the size of an outlier can be a useful clue.</p>
<p>So, we need histograms!</p>
<h2 id="hg64">hg64</h2>
<p>I have written a proof-of-concept histogram data structure called
<code>hg64</code>, which you can get from:</p>
<ul>
<li><a href="https://dotat.at/cgi/git/hg64.git">https://dotat.at/cgi/git/hg64.git</a></li>
<li><a href="https://github.com/fanf2/hg64">https://github.com/fanf2/hg64</a></li>
</ul>
<p>It can load 1 million data items in about 5ms (5ns per item), and uses
a few KiB of memory.</p>
<p>Here I will write about how <code>hg64</code> came to be.</p>
<h2 id="starting-points">starting points</h2>
<p>The DataDog blog has a very nice <a href="https://www.datadoghq.com/blog/engineering/computing-accurate-percentiles-with-ddsketch/">introduction to histograms and
quantile sketches in general, and DDSketch in particular</a>.
I particularly liked it because the design of DDSketch is simple and
it demystified things for me.</p>
<p>Briefly, the idea is that by choosing a relative error (e.g. 1%, or
two significant digits), you implicitly choose how to map values to
buckets: anything within 1% of the bucket’s nominal value is in the
same bucket. And buckets are spaced logarithmically, so a large range
can be covered by a relatively small number of buckets.</p>
<p>I think it’s worth distinguishing between quantile sketches that have
a target rank error and those that have a target value error. As well
as the comparison in the DataDog blog article, there’s a lot of
<a href="https://datasketches.apache.org/docs/Quantiles/Definitions.html">informative discussion about quantile sketches in the Apache
DataSketches documentation</a>. DDSketch is just a
histogram, like the ones we learned about in school, with a
logarithmic horizontal axis.</p>
<p>Well, not “just” a histogram, since DDSketch has a “collapsing”
mechanism that sacrifices accuracy for small measurements when the
data structure gets too big. (Generally we are more interested in the
large outliers.) There’s a variant called <a href="https://github.com/cafaro/UDDSketch">UDDSketch</a> that collapses
all buckets uniformly, which I liked because it is effectively a
self-tuning histogram.</p>
<p>So I thought I would try implementing my own UDDSketch.</p>
<h2 id="two-questions">two questions</h2>
<p>A histogram data structure is based on two questions:</p>
<ul>
<li>
<p>What is the underlying data structure? As well as directly finding
buckets by key, we would like to be able to search by rank so that
we can calculate quantiles efficiently.</p>
</li>
<li>
<p>How do we map a value to the key that identifies its bucket? This
affects the choice of data structure, because it’s easier if the
keys have a smaller range, or are more densely numbered.</p>
</li>
</ul>
<h2 id="data-structures">data structures</h2>
<p>Existing DDSketch implementations have an abstraction layer between
the histogram logic and an underlying storage structure. I wanted to
simplify my code by making a firm choice, so I tried out a few
alternatives:</p>
<ul>
<li>
<p>A red-black tree. Each node has left and right pointers, the
bucket key, bucket count, and (for searching by rank) a sum of the
counts of every bucket in the subtree.</p>
<p>This was fiddly to implement, used a lot of memory, and was not
particularly fast.</p>
</li>
<li>
<p>A sorted array. Each element has a bucket key and a count. Use
binary search to find a bucket by value, and linear search to find
a bucket by rank.</p>
<p>About twice as fast as the tree, and 0.2x memory usage.</p>
</li>
</ul>
<p>I thought to myself, I know a trick to find things faster than a
binary search…</p>
<ul>
<li>
<p>A Bagwell array-mapped trie with three levels, enough for 18 bit
keys.</p>
<p>I did not finish implementing this, because at the same time it
was becoming more clear to me what I needed from my bucket keys,
and that 18 bits was excessive.</p>
</li>
<li>
<p>An short array of <code>popcount</code> packed vectors. Each vector also
keeps a sum total of the counts in its buckets for faster rank
searches. Keys are 8..12 bits.</p>
<p>About 10x faster than the sorted array, down to around 5ns to add
an item. (The speed increase corresponds fairly accurately to the
smaller number of memory accesses.) Tolerable increase in memory
usage.</p>
</li>
</ul>
<p>It’s definitely inspired by a Bagwell trie, but flattened too much to
be tree-like.</p>
<h2 id="ddsketch-keys">ddsketch keys</h2>
<p>DDSketch counts floating point values. It maps a value to an integer
bucket number with a neat little formula:</p>
<pre><code>    ceil(log(value) / log(gamma))
</code></pre>
<p>Rounding with <code>ceil()</code> is where the precision is discarded so that a
range of values land in the same bucket. The base of the logarithm
<code>gamma</code> is derived from the chosen precision.</p>
<p>I adjusted this formula in a couple of ways: I precalculated <code>1 / log(gamma)</code>, because a multiplication is faster than a log and a div.
And I added a bias so that I did not have to worry about negative
numbers (such as  weirdness from undefined behaviour when shifting
signed integers in C).</p>
<p>At this point I was thinking, my bucket bias is like the exponent bias
in IEEE754; and I was thinking, how can I get rid of this other
logarithm?</p>
<h2 id="floating-point-keys">floating point keys</h2>
<p>Another histogram data structure, [<code>circllhist</code> (Circonus log-linear
histogram)][], buckets values by converting them to the form <code>X.Y * 10^Z</code>, i.e. exactly two decimal significant digits. In effect,
<code>circllhist</code> uses a custom low-precision decimal floating point
format.</p>
<p>So, if my values are floating point, they are already log-linear, and
I can adjust my precision (my bucket size) by adjusting how many bits
of mantissa I use. Maybe I could just use <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format"><code>bfloat16</code></a> as my
key? Basically just the top 16 bits of a single-precision IEEE754
number.</p>
<h2 id="integer-values">integer values</h2>
<p>I posted <a href="https://twitter.com/fanf/status/1546973795331743746">a few tweets about my histogram experiments</a> and got
a number of helpful replies.</p>
<p>That discussion reminded me that the values I want to handle - time
measurements, memory usage - start off as integers, in particular
64-bit integers. (32 bits doesn’t have quite enough range for e.g.
measuring hours to microsecond precision, and it’s smaller than
<code>size_t</code> on most modern platforms.)</p>
<p>So, our floating-point (or log-linear) keys need 6 bits of exponent to
cover the range of a 64-bit unsigned integer, and (roughly) 6 bits of
mantissa to get two decimal digits of precision.</p>
<p>To calculate the exponent, we can use <code>CLZ</code> (count leading zeroes),
then assemble the key with a small amount of bit shuffling:</p>
<pre><code>    clz = __builtin_clzll(value);
    exponent = 64 - 6 - clz;
    mantissa = value &gt;&gt; (exponent - 1);
    key = exponent * 64 + mantissa % 64;
</code></pre>
<p>I compared this with converting <code>value</code> to <code>float</code> and pulling out the
bits I wanted, and using <code>float</code> was neither simpler nor faster. (And
<code>float</code> introduced tricky rounding issues.)</p>
<h2 id="putting-it-together">putting it together</h2>
<p>This 6+6 (or 64+64) format meshes beautifully with 64-bit <code>popcount</code>
packed arrays, so I called the result <code>hg64</code>.</p>
<p>But 6 bits of mantissa is more than necessary for 2 s.f. of precision:
5 bits is about right. And I expect that in many cases 2 bits of
mantissa or 1 s.f. will be plenty. (My colleagues have suggested simple
order-of-magnitude bucketing for time measurements.)</p>
<p>So I added a coarse precision adjustment, but apart from that, <code>hg64</code>
does not have any configuration. For instance, unlike DDSketch, size
limits are implicit in the design: for 2 s.f. precision <code>hg64</code> can’t
grow larger than 17 KiB, and for 1 s.f. the max is 2 KiB. And <code>hg64</code>
remains consistently fast regardless of how much data it is fed.</p>
<p>It was fun making <code>hg64</code> and I’m pleased with the result.</p>
<hr/>
<p><em><a href="https://fanf.dreamwidth.org/139050.html">Comments welcome on Dreamwidth</a></em></p>

</div></div>
  </body>
</html>
