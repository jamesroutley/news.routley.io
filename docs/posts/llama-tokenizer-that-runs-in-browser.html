<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/belladoreai/llama-tokenizer-js">Original</a>
    <h1>Show HN: LLaMA tokenizer that runs in browser</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">The first JavaScript tokenizer for LLaMA which works client-side in the browser (and also in Node).</p>
<p dir="auto">Intended use case is calculating token count accurately on the client-side.</p>
<p dir="auto"><a href="https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/" rel="nofollow">Click here for demo</a></p>
<p dir="auto">Features:</p>
<ul dir="auto">
<li>Easy to use: 0 dependencies, code and data baked into a single file.</li>
<li>Compatible with most LLaMA-based models (see <a href="#compatibility">Compatibility</a>)</li>
<li>Optimized running time: tokenize a sentence in roughly 1ms, or 2000 tokens in roughly 20ms.</li>
<li>Optimized bundle size: 670KiB before minification and gzipping (the heaviest part of the tokenizer, merge data, has been compressed into a simple and efficient binary format, and then base64-encoded to bake it into the .js file)</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-import" aria-hidden="true" href="#import"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Import</h2>
<p dir="auto">Option 1: Install as an npm package and import as ES6 module</p>
<div data-snippet-clipboard-copy-content="npm install llama-tokenizer-js"><pre><code>npm install llama-tokenizer-js
</code></pre></div>
<div data-snippet-clipboard-copy-content="import llamaTokenizer from &#39;llama-tokenizer-js&#39;

console.log(llamaTokenizer.encode(&#34;Hello world!&#34;).length)"><pre><code>import llamaTokenizer from &#39;llama-tokenizer-js&#39;

console.log(llamaTokenizer.encode(&#34;Hello world!&#34;).length)
</code></pre></div>
<p dir="auto">Option 2: Load as ES6 module with <code>&lt;script&gt;</code> tags in your HTML</p>
<div data-snippet-clipboard-copy-content="&lt;script type=&#34;module&#34; src=&#34;https://belladoreai.github.io/llama-tokenizer-js/llama-tokenizer.js&#34;&gt;&lt;/script&gt;"><pre><code>&lt;script type=&#34;module&#34; src=&#34;https://belladoreai.github.io/llama-tokenizer-js/llama-tokenizer.js&#34;&gt;&lt;/script&gt;
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Once you have the module imported, you can encode or decode with it. Training is not supported.</p>
<p dir="auto">When used in browser, llama-tokenizer-js pollutes global namespace with <code>llamaTokenizer</code>.</p>
<p dir="auto">Encode:</p>
<div data-snippet-clipboard-copy-content="llamaTokenizer.encode(&#34;Hello world!&#34;)
&gt; [1, 15043, 3186, 29991]"><pre><code>llamaTokenizer.encode(&#34;Hello world!&#34;)
&gt; [1, 15043, 3186, 29991]
</code></pre></div>
<p dir="auto">Decode:</p>
<div data-snippet-clipboard-copy-content="llamaTokenizer.decode([1, 15043, 3186, 29991])
&gt; &#39;Hello world!&#39;"><pre><code>llamaTokenizer.decode([1, 15043, 3186, 29991])
&gt; &#39;Hello world!&#39;
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-tests" aria-hidden="true" href="#tests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tests</h2>
<p dir="auto">You can run tests with:</p>
<div data-snippet-clipboard-copy-content="llamaTokenizer.runTests()"><pre><code>llamaTokenizer.runTests()
</code></pre></div>
<p dir="auto">The test suite is small, but it covers different edge cases very well.</p>
<p dir="auto">Note that tests can be run both in browser and in Node (this is necessary because some parts of the code work differently in different environments).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-comparison-to-alternatives" aria-hidden="true" href="#comparison-to-alternatives"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Comparison to alternatives</h2>
<p dir="auto">As mentioned, llama-tokenizer-js is the first JavaScript tokenizer for LLaMA which works client-side in the browser. You might be wondering, what are people currently using to count tokens in web applications?</p>
<ul dir="auto">
<li>Many web applications currently use client-side JavaScript libraries for other, <em>incompatible</em> tokenizers. In particular, OpenAI&#39;s tokenizers are popular (see <a href="https://www.npmjs.com/package/@dqbd/tiktoken" rel="nofollow">tiktoken</a> and <a href="https://www.npmjs.com/package/gpt-tokenizer" rel="nofollow">gpt-tokenizer</a>). It&#39;s not entirely clear to me why people using LLaMA would want to count tokens with an OpenAI tokenizer that is not compatible with LLaMA. I guess people are assuming that there&#39;s not much difference between tokenizers? However, in my own testing I discovered that the token counts will commonly differ by as much as 20% between these tokenizers. So you can get a <em>very rough</em> approximation of LLaMA token count by using an OpenAI tokenizer.</li>
<li>Some web applications make network calls to Python applications that run the Huggingface transformers tokenizer. For example, the oobabooga-text-webui exposes an API endpoint for token count. The drawback of this approach is latency: although the Python tokenizer itself is very fast, oobabooga adds a lot of overhead. In my testing, making a network call to locally running oobabooga to count tokens for short Strings of text took roughly 300ms (compared to ~1ms when counting tokens client-side with llama-tokenizer-js). The latency will be even higher when a real web client is making requests over the internet. The latency issue is even worse if an application needs to iteratively trim down a prompt to get it to fit within a context limit, requiring multiple network calls.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-compatibility" aria-hidden="true" href="#compatibility"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Compatibility</h2>
<p dir="auto">The tokenizer is the same for all LLaMA models which have been trained on top of the checkpoints (model weights) leaked by Facebook in early 2023.</p>
<p dir="auto">Examples of compatible models:</p>
<ul dir="auto">
<li>wizard-vicuna-13b-uncensored-gptq</li>
<li>manticore-7b-ggml</li>
</ul>
<p dir="auto">Incompatible models are those which have been trained from scratch, not on top of the checkpoints leaked by Facebook. For example, <a href="https://github.com/openlm-research/open_llama">OpenLLaMA</a> models are incompatible. I&#39;d be happy to adapt this to any LLaMA models that people need, just open an issue for it.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-credit" aria-hidden="true" href="#credit"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Credit</h2>
<p dir="auto">You are free to use llama-tokenizer-js for basically whatever you want (MIT license).</p>
<p dir="auto">You are not required to give anything in exchange, but I kindly ask that you give back by linking to <a href="https://belladore.ai/tools" rel="nofollow">https://belladore.ai/tools</a> in an appropriate place in your website. For example, you might link with the text &#34;Using llama-tokenizer-js by belladore.ai&#34; or something similar.</p>
</article>
          </div></div>
  </body>
</html>
