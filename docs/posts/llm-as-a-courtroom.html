<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://falconer.com/notes/llm-as-a-courtroom/">Original</a>
    <h1>LLM-as-a-Courtroom</h1>
    
    <div id="readability-page-1" class="page"><div data-astro-cid-fezs4xpw=""> <p>Shared knowledge—decisions, plans, documentation, processes—is a living, breathing resource that evolves as companies do. Falconer is building a shared memory layer for cross-functional teams and their agents to maintain their shared knowledge. A core part of our solution means watching for code changes and proposing documentation updates automatically.</p>
<p>“Documentation rot” is a term used a lot by our users. The code changes, documents go stale, and the knowledge that was once accurate becomes a liability. Many tools now allow users to find information with AI. But <em>findability</em> alone doesn’t equate to accuracy. An easily findable doc that’s out of date doesn’t solve the problem. The hardest part of the problem isn’t finding right quanta of knowledge, rather it is the ability to trust it when you do find it.</p>
<h2 id="automating-documentation-updates">Automating documentation updates</h2>
<p>Falconer automatically updates documents based on changes to the code. But when a PR merges, how do you decide <em>which</em> documents to update?</p>
<p>This isn’t a simple pattern-matching problem. Yes, you can filter out obvious non-candidates—test files, lockfiles, CI configs. But after that, you’re depending on judgment. The PR code is complex and contextual, as are the documents. Different teams have different priorities. A change that’s critical for customer support documentation is probably irrelevant for engineering specs. And the audience reading the document matters as much as the change itself.</p>
<p>If a human were reading all code changes and updating docs manually—reading each PR, understanding the diffs, searching for affected documents, deliberating on whether each one needs updating, and making the actual updates—it would take days for a relatively small number of PRs. Falconer’s agent does this in seconds.</p>
<p>However, building an intelligent agent is just one piece of the puzzle. We also needed to build the infrastructure to process tens of thousands of PRs daily for our enterprise customers. And more importantly, we needed to build a judgment engine with a strong opinion of its own.</p>
<hr/>
<h2 id="societys-best-decision-framework">Society’s best decision framework</h2>
<p>We kept obsessing over the word “judgment,” and how it’s subjective, yet backed by reason. LLM-as-a-judge has been a useful technique, but far too rudimentary for our use case. Then it clicked. What better way to deliver better judgment than by constructing an entire courtroom? This led us to designing and building our “LLM-as-a-Courtroom” evaluation system.</p>
<p>Our first attempt was straightforward: categorical scoring. We asked our model to rate factors—relevance, feature addition, harm level—on numerical scales, then compared scores against configurable thresholds. We could handle the comparison logic ourselves; we just needed the model to assess.</p>
<pre>graph TD
    A[PR + Document] --&gt; B[LLM Evaluator]
    B --&gt; C{Categorical Scoring}
    C --&gt; D[Relevance: 7/10]
    C --&gt; E[Feature Impact: 5/10]
    C --&gt; F[Harm Level: 6/10]
    D --&gt; G{Score &gt; Threshold?}
    E --&gt; G
    F --&gt; G
    G --&gt;|Yes| H[Update Document]
    G --&gt;|No| I[Skip]

    J[Problem: Inconsistent ratings, no reasoning trail]

    style A fill:#f8f5eb,stroke:#0e1d0b
    style B fill:#cbeff5,stroke:#0e1d0b
    style C fill:#f2ecde,stroke:#0e1d0b
    style D fill:#f8f5eb,stroke:#0e1d0b
    style E fill:#f8f5eb,stroke:#0e1d0b
    style F fill:#f8f5eb,stroke:#0e1d0b
    style G fill:#f2ecde,stroke:#0e1d0b
    style H fill:#a7e3ec,stroke:#0e1d0b
    style I fill:#f8f5eb,stroke:#0e1d0b
    style J fill:#f2ecde,stroke:#0e1d0b</pre>
<p>It didn’t work as effectively as we needed it to. The ratings were inconsistent, often biased, and didn’t reflect the nuance of the actual decision. We were asking the model to do something it’s fundamentally not good at: rating things. Assigning a percentage or a hierarchical degree to a characteristic requires a kind of calibrated internal scale that LLMs simply don’t have.</p>
<p>But here’s what LLMs <em>are</em> good at: describing things in detail, providing explanations, and constructing arguments.</p>
<p>Unlike humans, who can often judge something quickly at a glance, models need a trail of thought in verbose terms to reach sound conclusions. They walk a path before reaching the end, facing gnarly crevices along the way. When you ask a model to output a single number, you’re asking it to skip that walk entirely. When you ask it to argue a position, you’re giving it the space to reason.</p>
<h3 id="from-requesting-ratings-to-constructing-arguments">From requesting ratings to constructing arguments</h3>
<p>So we shifted the objective. Instead of “rate this document’s relevance from 1-10,” we asked: “argue whether this document needs updating, and provide your evidence.”</p>
<p>As I constructed these argumentative prompts, the vernacular started to feel familiar. I was asking one agent to build a case. Another to challenge it. A third to weigh both sides and render a judgment. I realized I was mimicking a courtroom.</p>
<pre>%%{init: {&#39;theme&#39;: &#39;base&#39;, &#39;themeVariables&#39;: { &#39;actorBkg&#39;: &#39;#f8f5eb&#39;, &#39;actorBorder&#39;: &#39;#0e1d0b&#39;, &#39;actorTextColor&#39;: &#39;#0e1d0b&#39;, &#39;noteBkgColor&#39;: &#39;#0e1d0b&#39;, &#39;noteBorderColor&#39;: &#39;#0e1d0b&#39;, &#39;noteTextColor&#39;: &#39;#ffffff&#39;, &#39;signalColor&#39;: &#39;#0e1d0b&#39;, &#39;signalTextColor&#39;: &#39;#0e1d0b&#39; }}}%%
sequenceDiagram
    participant P as Prosecutor
    participant D as Defense
    participant JY as Jury
    participant JG as Judge

    P-&gt;&gt;JG: Exhibit A: Line 47 changed from async to sync execution
    P-&gt;&gt;JG: Document states &#34;all API calls are asynchronous&#34;
    P-&gt;&gt;JG: Harm: Developers will assume async behavior

    D-&gt;&gt;JG: Counter: Document section refers to v1 API, not v2
    D-&gt;&gt;JG: Change only affects internal implementation, not public contract
    D-&gt;&gt;JG: Harm overstated: existing UX doesn&#39;t change

    JY-&gt;&gt;JY: Deliberates independently
    JY-&gt;&gt;JG: Majority votes Not Guilty

    JG-&gt;&gt;JG: Weighs all evidence
    Note over P,JG: Verdict: Not Guilty</pre>
<p>This wasn’t accidental. The legal system is society’s most rigorous framework for checks and balances, argumentation, deliberation, judgment—and most of all, justice. It’s been refined over centuries for exactly the class of problem we faced: binary decisions under uncertainty, where evidence is imperfect and the costs of being wrong are asymmetric.</p>
<p>As a philosophy student, I was taught how to examine what makes for a sound argument. There are three components:</p>
<ul>
<li><strong>Validity</strong> (structure): The conclusion must follow logically from the premises, regardless of whether the premises are actually true.</li>
<li><strong>True premises</strong> (content): The statements used as the basis for the argument must be factually accurate.</li>
<li><strong>Soundness</strong> (the goal): An argument is sound if and only if it is both valid <em>and</em> all its premises are true. Only a sound argument guarantees a true conclusion.</li>
</ul>
<p>Legal argumentation, I’d argue, bears the closest resemblance to rigorous philosophical argument in the real world. It demands both logical structure and factual grounding. It enforces explicit evidence. It requires addressing counterarguments.</p>
<p>And there’s a key benefit to this approach: LLMs have consumed vast amounts of legal content in pre-training—journals, rulings, court transcripts, case analyses. When you frame a task using legal terminology, you’re activating a rich constellation of learned behaviors about how to argue, deliberate, and reason under scrutiny.</p>
<p>Legal argumentation elicits deep reasoning while making efficient use of language—and in turn, tokens.</p>
<hr/>
<h2 id="the-architecture-prosecution-defense-jury-judge">The architecture: Prosecution, Defense, Jury, Judge</h2>
<p>The courtroom paradigm is a structural framework with distinct roles, each designed to elicit specific reasoning behaviors.</p>
<p>Here’s how the roles interact.</p>
<pre>graph LR
    subgraph Input
        PR[PR Diff]
        DOC[Document Corpus]
    end

    subgraph Courtroom
        PROS[Prosecutor&lt;br/&gt;Builds Case]
        DEF[Defense&lt;br/&gt;Rebuts]
        JURY[Jury Pool&lt;br/&gt;5 Agents]
        JUDGE[Judge&lt;br/&gt;Final Verdict]
    end

    subgraph Output
        GUILTY[Guilty:&lt;br/&gt;Proposed Edits]
        NOTGUILTY[Not Guilty:&lt;br/&gt;No Action]
    end

    PR --&gt; PROS
    DOC --&gt; PROS
    PROS --&gt;|Exhibits +&lt;br/&gt;Harm Analysis| DEF
    DEF --&gt;|Counter-&lt;br/&gt;Arguments| JURY
    PROS -.-&gt;|Original Case| JURY
    JURY --&gt;|3/5 Guilty&lt;br/&gt;Threshold| JUDGE
    PROS -.-&gt;|All Evidence| JUDGE
    DEF -.-&gt;|All Evidence| JUDGE
    JUDGE --&gt; GUILTY
    JUDGE --&gt; NOTGUILTY

    style PR fill:#f8f5eb,stroke:#0e1d0b
    style DOC fill:#f8f5eb,stroke:#0e1d0b
    style PROS fill:#cbeff5,stroke:#0e1d0b
    style DEF fill:#a7e3ec,stroke:#0e1d0b
    style JURY fill:#f8f5eb,stroke:#0e1d0b
    style JUDGE fill:#f2ecde,stroke:#0e1d0b
    style GUILTY fill:#cbeff5,stroke:#0e1d0b
    style NOTGUILTY fill:#f8f5eb,stroke:#0e1d0b</pre>
<h3 id="the-prosecutor">The Prosecutor</h3>
<p>The prosecutor is our main GitHub agent. When a PR merges, this agent interprets the diffs, searches for potentially affected documents, and builds the case for updates. Like a real-world prosecutor, it’s responsible for constructing a burden of proof: what changed in the code, which documents are now inaccurate or incomplete, and what’s the harm of leaving them unchanged.</p>
<p>Critically, the prosecutor must provide <strong>exhibits</strong>—structured evidence with three enforced components:</p>
<pre>graph TD
    subgraph Exhibit Structure
        E[Exhibit] --&gt; Q1[1. Exact PR Quote]
        E --&gt; Q2[2. Exact Doc Quote]
        E --&gt; H[3. Concrete Harm]
    end

    subgraph Validation
        Q1 --&gt; V1{Exists in&lt;br/&gt;PR diff?}
        Q2 --&gt; V2{Exists in&lt;br/&gt;document?}
        H --&gt; V3{Specific&lt;br/&gt;consequence?}
    end

    V1 --&gt;|No| REJECT[Exhibit Rejected]
    V2 --&gt;|No| REJECT
    V3 --&gt;|No| REJECT
    V1 --&gt;|Yes| ACCEPT
    V2 --&gt;|Yes| ACCEPT
    V3 --&gt;|Yes| ACCEPT[Exhibit Accepted]

    style E fill:#cbeff5,stroke:#0e1d0b
    style Q1 fill:#f8f5eb,stroke:#0e1d0b
    style Q2 fill:#f8f5eb,stroke:#0e1d0b
    style H fill:#f8f5eb,stroke:#0e1d0b
    style V1 fill:#f2ecde,stroke:#0e1d0b
    style V2 fill:#f2ecde,stroke:#0e1d0b
    style V3 fill:#f2ecde,stroke:#0e1d0b
    style ACCEPT fill:#a7e3ec,stroke:#0e1d0b
    style REJECT fill:#f2ecde,stroke:#0e1d0b</pre>
<ol>
<li><strong>An exact quote from the PR diff</strong> — the specific code change being cited</li>
<li><strong>An exact quote from the document</strong> — text that must exist verbatim in the target document</li>
<li><strong>A concrete harm statement</strong> — not “this might confuse users” but “support agents will tell customers X when it’s now Y”</li>
</ol>
<p>The prosecutor must cite exact text from both the PR and the document, and articulate a specific harm. This requirement originates from the tenets of Retrieval Augmented Generation (RAG): an LLM’s context must be enriched with ground-truth information precisely curated toward a specific action.</p>
<p>But we’re also using the <em>terminology</em> strategically. Words like “exhibit” and “evidence” carry weight. They implicitly trigger learned behaviors from pre-training—the scrutiny, rigor, and specificity that legal contexts demand.</p>
<p>The prosecutor must prove that a document needs updating.</p>
<h3 id="the-defense">The Defense</h3>
<p>The defense is the adversarial counterweight. After reviewing the prosecution’s evidence and argumentation, the defense agent mounts a rebuttal. Its job is to challenge the prosecutor: Is the evidence actually conclusive? Does the document already cover this case? Is the alleged harm overstated?</p>
<p>The defense generates a structured rebuttal for each document, containing:</p>
<ul>
<li><strong>A core counter-argument</strong> — the central thesis for why no update is needed</li>
<li><strong>Specific challenges to each exhibit</strong> — questioning the evidence’s validity or relevance</li>
<li><strong>A harm dispute</strong> — why the alleged harm is overstated or nonexistent</li>
<li><strong>An alternative explanation</strong> — why the document may already be correct as written</li>
</ul>
<p>The presence of opposing perspectives is considered critical for reaching sound judgment. The defense provides the logical contrast that prevents groupthink and surfaces weaknesses in the prosecution’s case.</p>
<p>The roots of this go deep. In philosophy, we call it Socratic elenchus: a logical dialogue that chips away at abstractions slowly, reaching toward the core of a thesis. The defense forces the courtroom to stress-test its assumptions before rendering judgment.</p>
<h3 id="the-jury">The Jury</h3>
<p>The jury consists of multiple independent agents that evaluate the case after hearing both sides. They’re designed to be holistic bystanders—not advocates for either position, but impartial assessors weighing evidence and arguments.</p>
<p>Technical choices here are deliberate:</p>
<ul>
<li><strong>Parallel execution</strong>: All jurors run simultaneously, not sequentially</li>
<li><strong>High temperature</strong>: We want variance—each juror should bring a different perspective</li>
<li><strong>Light reasoning models</strong>: Cost-efficient for parallel execution, sufficient for evaluation</li>
<li><strong>Independent context</strong>: Each juror reasons in isolation, without seeing other jurors’ votes</li>
</ul>
<p>Each juror must explain their reasoning before casting a vote—guilty, not guilty, or abstain. This deliberate-then-vote pattern forces the model to think through the evidence before committing to a position.</p>
<pre>graph TD
    subgraph Jury Execution
        CASE[Case Evidence] --&gt; J1[Juror 1]
        CASE --&gt; J2[Juror 2]
        CASE --&gt; J3[Juror 3]
        CASE --&gt; J4[Juror 4]
        CASE --&gt; J5[Juror 5]
    end

    J1 --&gt; R1[Reasoning]
    J2 --&gt; R2[Reasoning]
    J3 --&gt; R3[Reasoning]
    J4 --&gt; R4[Reasoning]
    J5 --&gt; R5[Reasoning]

    R1 --&gt; V1[Guilty]
    R2 --&gt; V2[Guilty]
    R3 --&gt; V3[Not Guilty]
    R4 --&gt; V4[Guilty]
    R5 --&gt; V5[Abstain]

    V1 --&gt; TALLY{3/5 Guilty?}
    V2 --&gt; TALLY
    V3 --&gt; TALLY
    V4 --&gt; TALLY
    V5 --&gt; TALLY

    TALLY --&gt;|Yes| PROCEED[Proceed to Judge]
    TALLY --&gt;|No| DISMISS[Case Dismissed]

    style CASE fill:#f8f5eb,stroke:#0e1d0b
    style J1 fill:#f8f5eb,stroke:#0e1d0b
    style J2 fill:#f8f5eb,stroke:#0e1d0b
    style J3 fill:#f8f5eb,stroke:#0e1d0b
    style J4 fill:#f8f5eb,stroke:#0e1d0b
    style J5 fill:#f8f5eb,stroke:#0e1d0b
    style R1 fill:#f8f5eb,stroke:#0e1d0b
    style R2 fill:#f8f5eb,stroke:#0e1d0b
    style R3 fill:#f8f5eb,stroke:#0e1d0b
    style R4 fill:#f8f5eb,stroke:#0e1d0b
    style R5 fill:#f8f5eb,stroke:#0e1d0b
    style V1 fill:#cbeff5,stroke:#0e1d0b
    style V2 fill:#cbeff5,stroke:#0e1d0b
    style V3 fill:#f8f5eb,stroke:#0e1d0b
    style V4 fill:#cbeff5,stroke:#0e1d0b
    style V5 fill:#f2ecde,stroke:#0e1d0b
    style TALLY fill:#f2ecde,stroke:#0e1d0b
    style PROCEED fill:#a7e3ec,stroke:#0e1d0b
    style DISMISS fill:#f8f5eb,stroke:#0e1d0b</pre>
<p>The default configuration runs 5 jurors, requiring 3 guilty votes (a majority) to proceed to the judge. But this is tunable—some use cases might call for unanimous juries, others might accept a single guilty vote.</p>
<h3 id="the-judge">The Judge</h3>
<p>The judge serves a different role than in traditional jury trials. While the jury deliberates and reaches a preliminary verdict through majority vote, the judge acts as the final arbiter. It synthesizes all perspectives and renders an independent judgment, then determines the appropriate “sentencing.”</p>
<p>The judge operates differently from other agents:</p>
<ul>
<li><strong>Low temperature</strong>: We need predictability and consistency in final rulings</li>
<li><strong>Heavy reasoning model</strong>: This is where consequential reasoning happens</li>
<li><strong>Structured deliberation</strong>: The judge must reason through the case before issuing a verdict</li>
</ul>
<p>The judge produces a structured ruling containing: a full analysis synthesizing the prosecution’s case, the defense’s rebuttal, and the jury’s votes; a verdict (guilty, not guilty, or dismissed); a one-sentence rationale; and if guilty—specific edits to make to the document.</p>
<p>If the verdict is guilty, these edits are consolidated (max 2 per document by default) to avoid overwhelming document owners with granular changes.</p>
<h3 id="terminology-as-a-tool">Terminology as a tool</h3>
<p>One design principle runs throughout: the courtroom terminology is a <em>structural tool</em> to exploit LLM training sets, not serve as user-facing language. Internally, we talk about prosecutors, exhibits, verdicts. But this language is isolated and contained—it never leaks into the actual outputs that document owners see. They receive concise notifications about proposed updates.</p>
<hr/>
<h2 id="what-we-learned">What we learned</h2>
<p>We’ve built a useful evaluation system for our use case by leaning on what LLMs are already good at: legal comprehension. But we’re under no illusion that it’s a complete solution.</p>
<p><strong>Jury bias is real.</strong> Despite running independently with high temperature, jury agents sometimes converge on the same conclusion. This isn’t catastrophic—if evidence is genuinely strong or weak, agreement makes sense—but we’re investigating when convergence reflects true consensus versus shared bias. The variance we designed for isn’t always materializing.</p>
<p><strong>New paradigms need new testing infrastructure.</strong> You can’t unit test a courtroom simulation the way you test a function. The outputs are probabilistic. The “right answer” is often debatable. We’re developing evaluation strategies focused on observability: when a bad verdict happens, we need to trace back through the chain. Was it a weak prosecution? An ineffective defense? A judge that ignored the jury?</p>
<p><strong>Real usage surfaces real edge cases.</strong> The PRs that slip through, the documents flagged unnecessarily, the configurations that behave unexpectedly—these emerge from actual use, not internal testing. Our design partners have already surfaced issues we didn’t anticipate. That feedback loop is essential.</p>
<p>The legal system turned out to be a near-perfect match. And because LLMs have extensive exposure to legal reasoning through pre-training, we could activate that framework through terminology and structure rather than complex fine-tuning.</p>
<p>We’re doing more research to apply the LLM-as-a-Courtroom to a wider variety of complex problems. The complexity of the simulation should only grow from here—more nuanced roles, multi-turn debate, configurable appeals processes, domain-specific courts for different industries.</p>
<hr/>
<h2 id="the-verdict">The verdict</h2>
<p>After running LLM-as-a-Courtroom in production for 3 months, we saw 65% of PRs filtered before review, 95% of flagged PRs filtered before reaching Court, and 63% of Court cases dismissed without doc updates. When we do escalate to humans, we’re right 83% of the time.</p>
<p>Documentation rot thrives on neglect—the gap between code that changes and knowledge that doesn’t. The courtroom narrows that gap by filtering aggressively. We calibrated the system to skew strict, prioritizing precision over recall—false positives erode trust faster than false negatives. By keeping the bar high, we ensure every surfaced update deserves attention while building a curated dataset of high-quality updates to study and replicate at larger scale.</p>
<p>The architecture scales, the framework adapts.</p>
<p>We’re just getting started.</p>
<hr/>
<h2 id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/2408.08089">AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents</a> (ACL 2025)</li>
<li><a href="https://arxiv.org/abs/2305.14325">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a></li>
<li><a href="https://arxiv.org/abs/2410.04663">Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework</a></li>
<li><a href="https://gaudion.dev/blog/documentation-drift">Documentation Drift and How to Avoid It</a></li>
</ul> </div></div>
  </body>
</html>
