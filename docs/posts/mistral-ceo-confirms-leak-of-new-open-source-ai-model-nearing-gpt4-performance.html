<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/">Original</a>
    <h1>Mistral CEO confirms &#39;leak&#39; of new open source AI model nearing GPT4 performance</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-31T18:44:56+00:00" datetime="2024-01-31T18:44:56+00:00">January 31, 2024 10:44 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="420" src="https://venturebeat.com/wp-content/uploads/2024/01/cfr0z3n_overhead_view_of_the_eiffel_tower_rising_from_a_landsca_d374695a-bca7-4db7-abcd-d30ef5fd0e04.png?fit=750%2C420&amp;strip=all" alt="Overhead view of Eiffel tower in a Paris made of circuit boards."/></p><p><span>Credit: VentureBeat made with Midjourney V6</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">
		<div id="content" role="main">

			<article id="post-2932274">
				<div>
					<p>The past few days have been a wild ride for the growing open source AI community — even by its fast-moving and freewheeling standards.</p>



<p>Here’s the quick chronology: on or about January 28, a user with the handle “Miqu Dev” <a href="https://huggingface.co/miqudev/miqu-1-70b" target="_blank" rel="noreferrer noopener">posted a set of files on HuggingFace</a>, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled “miqu-1-70b.”</p>



<p>The HuggingFace entry, which is still up at the time of this article’s posting, noted that new LLM’s “Prompt format,” how users interact with it, was the same as <a href="https://mistral.ai/">Mistral</a>, the <a href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251" target="_blank" rel="noreferrer noopener">well-funded open source Parisian AI company</a> behind <a href="https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/" target="_blank" rel="noreferrer noopener">Mixtral 8x7b</a>, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta’s Llama 2. </p>



<h2 id="h-posted-on-4chan">Posted on 4chan</h2>



<p>The same day, an anonymous user on 4chan (possibly “Miqu Dev”) posted a <a href="https://boards.4chan.org/g/thread/98696032#p98697258" target="_blank" rel="noreferrer noopener">link to the miqu-1-70b files on 4chan</a>, the notoriously longstanding haven of online memes and toxicity, where users began to notice it.</p>



<p>Some took to X, Elon Musk’s social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI’s GPT-4 on the <a href="https://eqbench.com/" target="_blank" rel="noreferrer noopener">EQ-Bench</a>. </p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium – at Q4_K_M. I… <a href="https://t.co/0gOOPjxjPD">pic.twitter.com/0gOOPjxjPD</a></p>— N8 Programs (@N8Programs) <a href="https://twitter.com/N8Programs/status/1752441060133892503?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Might be late but I am now 100% convinced that Miqu is the same model that&#39;s accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain&#39;t no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. <a href="https://t.co/zZMcpspXch">pic.twitter.com/zZMcpspXch</a></p>— Teortaxes▶️ (@teortaxesTex) <a href="https://twitter.com/teortaxesTex/status/1752427812466593975?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<h2 id="h-mistral-quantized">Mistral quantized?</h2>



<p>Machine learning (ML) researchers took notice on <a href="https://www.linkedin.com/posts/maxime-labonne_new-open-source-llm-competes-with-gpt-activity-7158414579671199744-MFfA/" target="_blank" rel="noreferrer noopener">LinkedIn</a>, as well. </p>



<p><em>“Does ‘miqu’ stand for MIstral QUantized? We don’t know for sure, but this quickly became one of, if not the best open-source LLM,”</em> wrote Maxime Labonne, an ML scientist at JP Morgan &amp; Chase, one of the world’s largest banking and financial companies.<em> “Thanks to @152334H, we also now have a good unquantized version of miqu here: <a href="https://lnkd.in/g8XzhGSM">https://lnkd.in/g8XzhGSM</a></em></p>



<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization" target="_blank" rel="noreferrer noopener">Quantization</a> in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model’s architecture with shorter ones. </p>



<p>Users speculated “Miqu” might be a new Mistral model being covertly “leaked” by the company itself into the world — especially since Mistral is known for <a href="https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent-link-to-new-open-source-llm/" target="_blank" rel="noreferrer noopener">dropping new models and updates without fanfare</a> through esoteric and technical means — or perhaps an employee or customer gone rouge.</p>



<h2 id="h-confirmation-from-the-top">Confirmation from the top</h2>



<p>Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: <em>“An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly…</em></p>



<p><em>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster — the pretraining finished on the day of Mistral 7B release. We’ve made good progress since — stay tuned!<a href="https://twitter.com/arthurmensch/status/1752737462663684344/history"></a>“</em></p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly.</p>— Arthur Mensch (@arthurmensch) <a href="https://twitter.com/arthurmensch/status/1752737462663684344?ref_src=twsrc%5Etfw">January 31, 2024</a></blockquote>
</div></figure>



<p>Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster “might consider attribution.”</p>



<figure></figure>



<p>Still, with Mensch’s note to “stay tuned!” it appears that not only is Mistral training a version of this so-called “Miqu” model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. </p>



<h2 id="h-a-pivotal-moment-in-open-source-ai-and-beyond">A pivotal moment in open source AI and beyond?</h2>



<p>That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: <a href="https://openai.com/research/gpt-4" target="_blank" rel="noreferrer noopener">since its release back in March 2023</a>, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">Google’s presently available, long-rumored Gemini models</a> have been able to eclipse it — yet (according to some measures, the current Gemini models are <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">actually worse than the older OpenAI GPT-3.5 mode</a>l). </p>



<p>The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, <a href="https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/">as VentureBeat’s founder and CEO Matt Marshall recently reported</a>. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical “moat” with its <a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come/">GPT Store</a> and other features, to remain in the top spot for LLMs? </p>
<p><strong>VentureBeat&#39;s mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2932274 .article-wrapper -->


		</div><!-- #content -->
	</div></div>
  </body>
</html>
