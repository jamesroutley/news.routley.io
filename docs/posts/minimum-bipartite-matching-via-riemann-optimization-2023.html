<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ocramz.github.io/posts/2023-12-21-assignment-riemann-opt.html">Original</a>
    <h1>Minimum bipartite matching via Riemann optimization (2023)</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    <section>
        
<p><img src="https://ocramz.github.io/images/assignment_riemann_tri.png"/></p>
<p>Some time ago I ran into two separate instances of the same combinatorial optimization problem in the span of a few days, and decided to read up a little on the fundamentals. The two applications were object tracking in videos, and peak alignment in chromatography data.</p>
<p>By chasing down some definitions, I learned about the Birkhoff theorem, which led me to wonder whether we can turn the original combinatorial problem (<a href="https://math.mit.edu/~goemans/18433S13/matching-notes.pdf">minimum weight bipartite matching <em>aka</em> “assignment”</a> [0]) into a differentiable one. The other half of my investigation was about how to turn a constrained optimization problem into an unconstrained one over an appropriate smooth sub-manifold.</p>
<p>In this post I go over the background concepts and document my experiments which started from a “what if?” and ended up connecting some interesting areas of mathematics and computer science (and an empirical confirmation to my starting intuition, I should add).</p>
<h2 id="minimum-bipartite-matching">Minimum bipartite matching</h2>
<p>Given a bipartite graph between two sets <span><em>U</em></span>, <span><em>V</em></span> of <span><em>n</em></span> items each, and an edge cost matrix <span><em>C</em></span> with positive entries, the <a href="https://en.wikipedia.org/wiki/Assignment_problem">assignment problem</a> can be written as finding a permutation matrix <span><em>P</em><sup>⋆</sup></span> that minimizes the total cost:</p>
<p><span>$$
P^{\star} = \underset{P \in \mathbb{P}}{\mathrm{argmin}} \left( P C \right)
$$</span></p>
<p>Recall that a permutation matrix has binary entries, and exactly one <span>1</span> per row and column.</p>
<p>Finding the optimal permutation matrix <span><em>P</em><sup>⋆</sup></span> is a combinatorial optimization problem that has <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">well known polynomial-time solution algorithms</a>, e.g. Munkres that runs in <span><em>O</em>(<em>n</em><sup>3</sup>)</span> time.</p>
<p>The assignment problem is a special case of the larger family of <em>optimal transport</em> problems, which mathematicians have looked at since the 18th century.</p>

<p>The Birkhoff-von Neumann theorem states that the convex hull of the set of <span><em>n</em> × <em>n</em></span> permutation matrices is the set <span>𝔹</span> of doubly stochastic matrices [1]. Informally, there is a convex, continuous region of space “between” the permutation matrices, considered as points in <span>ℝ<sub>+</sub><sup><em>n</em> × <em>n</em></sup></span>. This convex set is called the Birkhoff polytope [2].</p>
<p>Can we perhaps use this result to solve the assignment problem with a convex, interior point approach?</p>
<p>We can rewrite the assignment problem such that the optimization variable ranges over the Birkhoff polytope; this rewritten form is equivalent to the original one since the cost function is linear in the argument <span><em>P</em></span>, so we expect the optimum to lie at a vertex of the admissible region <span>𝔹</span> (i.e. to be a permutation matrix).</p>
<p><span>$$
P^{\star} = \underset{P \in \mathbb{B}}{\mathrm{argmin}} \left( P C \right)
$$</span></p>
<p>That implicit constraint under the <span>argmin</span> looks hard. How to address it?
One way would be to write a Lagrangian and set up a primal-dual optimization scheme for it. In this post we’re <em>not</em> going to do that :)</p>
<p>In the following we’ll see how to turn this kind of constrained optimization problem into an unconstrained one over an appropriate <em>manifold</em>.</p>
<h2 id="optimization-on-manifolds">Optimization on manifolds</h2>
<p>Informally, a <i>manifold</i> is a version of Euclidean space <span>ℝ<sup><em>d</em></sup></span> that is only locally flat (unlike regular Euclidean space which is uniformly flat everywhere).</p>
<p>In order to “make progress” towards a minimum cost region over a manifold, we must define notions of vector addition over curved spaces, in a way. Riemann gradient descent proceeds by correcting the update steps, constraining the iterations to lie on the manifold, as we will see in the following.</p>
<p>The main technical devices for moving along a <a href="https://en.wikipedia.org/wiki/Riemannian_manifold">smooth manifold</a> <span>𝕄</span> are the <em>orthogonal projection</em> from <span>𝕄</span> to its tangent, and the <em>retraction</em> operation that assigns points on the tangent bundle <span><em>T</em>𝕄</span> to <span>𝕄</span>.</p>
<p>For an introduction to the relevant definitions I found the book and online course <a href="https://www.nicolasboumal.net/book/#lectures">“An introduction to optimization on smooth manifolds”</a> to be very accessible.</p>
<h2 id="the-manifold-of-doubly-stochastic-matrices">The manifold of doubly stochastic matrices</h2>
<p>Many interesting sets have manifold structure: the sphere, <a href="https://en.wikipedia.org/wiki/Statistical_manifold">the set of probability distributions</a>, the set of positive-definite matrices. You can construct manifolds from products of manifolds, too. With some furious handwaving on my part and by recognizing the similarities (e.g. positivity and unit norm constraint) you can convince yourself that DS matrices have manifold structure in turn. Alternatively, you can read the discussion in Section V of [4] for all proofs.</p>
<p>The definitions of the projection and retraction operators are taken from the literature ([4] is the single paper on the topic I’ve found):</p>
<ul>
<li><p>Riemann gradient (Lemma 1 of [4]):</p>
<p><span>∇<sub><em>R</em></sub><em>f</em>(<em>x</em>) = ∏<sub><em>x</em></sub>(∇<em>f</em>(<em>x</em>) ⊙ <em>x</em>)</span></p>
<ul>
<li><p>The projection <span>∏<sub><em>x</em></sub>(<em>z</em>)</span> of a vector <span><em>z</em></span> on <span><em>T</em>𝔹(<em>x</em>)</span> is defined (Eqns. B.9 - B.11 of [4]) as:</p>
<p><span>∏<sub><em>x</em></sub>(<em>z</em>) = <em>z</em> − (<em>α</em>1<sup>⊤</sup> + 1<em>β</em><sup>⊤</sup>) ⊙ <em>x</em></span></p>
<p>where</p>
<p><span><em>α</em> = (<em>I</em> − <em>x</em><em>x</em><sup>⊤</sup>)<sup>⊤</sup>(<em>z</em> − <em>x</em><em>z</em><sup>⊤</sup>)1</span></p>
<p><span><em>β</em> = <em>z</em><sup>⊤</sup>1 − <em>x</em><sup>⊤</sup><em>α</em></span></p></li>
</ul></li>
<li><p>The retraction <span><em>R</em><sub><em>x</em></sub>(<em>v</em>)</span> of a tangent vector <span><em>v</em> ∈ <em>T</em>𝔹</span> onto <span>𝔹</span> (Lemma 2 of [4]) is defined as:</p>
<p><span><em>R</em><sub><em>x</em></sub>(<em>v</em>) = <em>S</em>(<em>x</em> ⊙ <em>e</em><em>x</em><em>p</em>(<em>v</em> ⊘ <em>x</em>))</span></p></li>
</ul>
<p>where <span>⊙</span> and <span>⊘</span> are the elementwise (Hadamard) product and division, respectively.</p>
<p>In the retraction formula above, the <a href="https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem">Sinkhorn-Knopp iteration</a> <span><em>S</em></span> computes a diagonal scaling of a positive matrix such that its rows and columns are normalized (i.e. a doubly-stochastic matrix).</p>
<h2 id="first-order-optimization-on-manifolds">First-order optimization on manifolds</h2>
<p>My optimization code is based on <code>torch</code> with some code borrowed from <code>mctorch</code> [3], extended to implement the manifold of doubly-stochastic matrices. In the following I refer to Python modules and line numbers in my implementation at <a href="https://github.com/ocramz/assignment-riemann-opt/tree/aed67ed8be296f4e5850797c841cbcf19338192e">this commit</a> :</p>
<p>Disregarding some implementation details (e.g. how Pytorch handles mutable objects), the Riemann SGD step (<code>rsgd.py</code> line 57) follows the textbook definition:</p>
<ol type="1">
<li>compute the Riemann gradient (<code>egrad2rgrad</code>, from <code>parameter.py</code> line 31) via an orthogonal projection of the Euclidean gradient of the cost function onto <span><em>T</em>𝕄</span></li>
<li>scale it by the negative learning rate</li>
<li>compute the retraction of the current point along the scaled Riemann gradient, thereby moving to a new point on <span>𝕄</span>.</li>
</ol>
<p>The projection and retraction operators for the doubly-stochastic matrix manifold are implemented <a href="https://github.com/ocramz/assignment-riemann-opt/blob/aed67ed8be296f4e5850797c841cbcf19338192e/doublystochastic.py">here</a>, following the definitions given above from [4].</p>

<p>We start by generating a cost matrix <span><em>C</em></span> of rank <span><em>n</em> = 10</span>, and computing the optimal assignment with the Munkres algorithm, which provides us with a cost lower bound (<span><em>y</em><sub><em>L</em><em>B</em></sub></span>).
We then initialize the SGD optimizer at a random doubly stochastic matrix. The elements of both <span><em>C</em></span> and the starting point are sampled from the <a href="https://en.wikipedia.org/wiki/Folded_normal_distribution">“folded” normal distribution</a> <span>|𝒩(0, 1)|</span>.
The learning rate is set to 2e-2 (found empirically).</p>
<p><img src="https://ocramz.github.io/images/assign_movie_iter-1000_n-10_lr-0.02_1735984842.gif" width="500/"/></p>
<p>In the above animation we see the optimal assignment as dashed red edges, superimposed with the temporary solution in blue. As a visual cue, the thickness of the blue edges is proportional to the respective matrix coefficient, and we see the matrix “sparsifies” as it approaches the optimum at a vertex of the Birkhoff set.</p>
<p><img src="https://ocramz.github.io/images/assign_opt_gap_iter-1000_n-10_lr-0.02_1735984842.png" width="400/"/></p>
<p>As we can see above, the optimality gap between the current and the Munkres cost (<span><em>y</em> − <em>y</em><sub><em>L</em><em>B</em></sub></span>) converges smoothly to 0 (from most starting points, in my experiments). I still have to characterize some rare cases in which the bound worsens for a while before improving again.</p>

<p>I was quite surprised this approach works so well, to be honest. It would be interesting to look at the convergence of the method wrt the distribution of edge weights <span><em>C</em></span>.</p>
<p>Besides usual concerns of performance, convergence bounds etc., it would be also interesting to generalize this approach and look at the actual connections with optimal transport: what if we have two histograms or <em>distributions</em> rather than the sets <span><em>U</em></span> and <span><em>V</em></span>?</p>
<h2 id="code-repo">Code repo</h2>
<p>All scripts can be found on my GitHub profile <a href="https://github.com/ocramz/assignment-riemann-opt">here</a>.</p>

<ol start="0" type="1">
<li>Assignment problem <a href="https://en.wikipedia.org/wiki/Assignment_problem">wikipedia</a></li>
<li>Doubly-stochastic matrix <a href="https://en.wikipedia.org/wiki/Doubly_stochastic_matrix">wikipedia</a></li>
<li>Birkhoff polytope <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">wikipedia</a></li>
<li><code>mctorch</code> <a href="https://github.com/mctorch/mctorch">github</a></li>
<li>Douik, A. and Hassibi, B., Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry, 2018 <a href="https://arxiv.org/abs/1802.02628">arXiv</a></li>
<li>Boumal, N. <a href="https://www.nicolasboumal.net/book/#lectures">An introduction to optimization on smooth manifolds</a></li>
<li><a href="https://juliamanifolds.github.io/manifolds/stable/manifolds/multinomialdoublystochastic.html">manifolds.jl</a></li>
</ol>
    </section>
</article></div>
  </body>
</html>
