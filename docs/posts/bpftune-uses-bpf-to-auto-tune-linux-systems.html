<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/oracle/bpftune">Original</a>
    <h1>Bpftune uses BPF to auto-tune Linux systems</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">bpftune aims to provide lightweight, always-on auto-tuning of system
behaviour.  The key benefit it provides are</p>
<ul dir="auto">
<li>by using BPF observability features, we can continuously monitor
and adjust system behaviour</li>
<li>because we can observe system behaviour at a fine grain (rather
than using coarse system-wide stats), we can tune at a finer grain
too (individual socket policies, individual device policies etc)</li>
</ul>

<p dir="auto">The Linux kernel contains a large number of tunables; these
often take the form of sysctl(8) parameters, and are usually
introduced for situations where there is no one &#34;right&#34; answer
for a configuration choice.  The number of tunables available
is quite daunting.  On a 6.2 kernel we see</p>
<div data-snippet-clipboard-copy-content="# sysctl --all 2&gt;/dev/null|wc -l
1624"><pre><code># sysctl --all 2&gt;/dev/null|wc -l
1624
</code></pre></div>
<p dir="auto"><a href="https://github.com/leandromoreira/linux-network-performance-parameters">See here for an excellent writeup on network-related tunables.</a>.</p>
<p dir="auto">At the same time, individual systems get a lot less care
and adminstrator attention than they used to; phrases like
&#34;cattle not pets&#34; exemplify this.  Given the modern cloud
architectures used for most deployments, most systems never
have any human adminstrator interaction after initial
provisioning; in fact given the scale requirements, this
is often an explicit design goal - &#34;no ssh&#39;ing in!&#34;.</p>
<p dir="auto">These two observations are not unrelated; in an earlier
era of fewer, larger systems, tuning by administrators was
more feasible.</p>
<p dir="auto">These trends - system complexity combined with minimal
admin interaction suggest a rethink in terms of tunable
management.</p>
<p dir="auto">A lot of lore accumulates around these tunables, and to help
clarify why we developed bpftune, we will use a straw-man
version of the approach taken with tunables:</p>
<p dir="auto">&#34;find the set of magic numbers that will work for the
system forever&#34;</p>
<p dir="auto">This is obviously a caricature of how administrators
approach the problem, but it does highlight a critical
implicit assumption - that systems are static.</p>
<p dir="auto">And that gets to the &#34;BPF&#34; in bpftune; BPF provides means
to carry out low-overhead observability of systems. So
not only can we observe the system and tune appropriately,
we can also observe the effect of that tuning and re-tune
if necessary.</p>

<ul dir="auto">
<li>Minimize overhead.  Use observability features sparingly; do not
trace very high frequency events.</li>
<li>Be explicit about policy changes providing both a &#34;what&#34; - what
change was made - and a &#34;why&#34; - how does it help? syslog logging
makes policy actions explicit with explanations</li>
<li>Get out of the way of the administrator.  We can use BPF
observability to see if the admin sets tunable values that we
are auto-tuning; if they do, we need to get out of the way and
disable auto-tuning of the related feature set.</li>
<li>Don&#39;t replace tunables with more tunables! bpftune is designed to
be zero configuration; there are no options, and we try to avoid
magic numbers where possible.</li>
<li>Use push-pull approaches. For example, with tcp buffer sizing,
we often want to get out of the way of applications and bump
up tcp sndbuf and rcvbuf, but at a certain point we run the
risk of exhausting TCP memory.  We can however monitor if we
are approaching TCP memory pressure and if so we can tune down
values that we&#39;ve tuned up.  In this way, we can let the system
find a balance between providing resources and exhausting them.
In some cases, we won&#39;t need to tune up values; they may be fine
as they are. But in other cases these limits block optimal performance,
and if they are raised safely - with awareness of global memory
limits - we can get out the way of improved performance.  Another
concern is that increasing buffer size leads to latency - to
handle that, we correlate buffer size changes and TCP smoothed
round-trip time; if the correlation between these exceeds a
threshold (0.7) we stop increasing buffer size.</li>
</ul>

<p dir="auto">The key components are</p>
<ul dir="auto">
<li>
<p dir="auto">tuners: each tuner manages tunables and handles events sent
from BPF programs to userspace via the shared ring buffer.
Each tuner has an associated set of tunables that it manages.</p>
</li>
<li>
<p dir="auto">optional strategies: a tuner can specify multiple strategies;
after running for a while a strategy times out and we assess
if a better strategy is available.  Each strategy specifies a</p>
<ul dir="auto">
<li>name</li>
<li>description</li>
<li>timeout</li>
<li>evaluation function</li>
<li>set of BPF program names in tuner associated with strategy</li>
</ul>
<p dir="auto">Strategies are optional and should be set in the tuner init()
method via bpftune_strategies_add().  See test/strategy
for a coded example.  When a strategy times out, the various
evaluation functions are called and the highest-value evaluation
dictates the next stratgey.</p>
<p dir="auto">Strategies provide a way of providing multiple schemes for
auto-tuning the same set of tunables, where the choice is
guided by an evaluation of the effectiveness of the strategies.</p>
</li>
<li>
<p dir="auto">events specify a</p>
<ul dir="auto">
<li>tuner id: which tuner the event is destined for</li>
<li>a scenario: what happened</li>
<li>an associated netns (if supported)</li>
<li>information about the event (IP address etc)</li>
</ul>
</li>
<li>
<p dir="auto">the tuner then responds to the event guided by the active strategy;
increase or decrease a tunable value, etc.  Describing the event
in the log is key; this allows an admin to understand what
changed and why.</p>
</li>
</ul>

<ul dir="auto">
<li>bpftune is a daemon which manages a set of .so plugin tuners;
each of these is a shared object that is loaded on start-up.</li>
<li>tuners can be enabled or disabled; a tuner is automatically
disabled if the admin changes associated tunables manually.</li>
<li>tuners share a global BPF ring buffer which allows posting of
events from BPF programs to userspace.  For example, if the
sysctl tuner sees a systl being set, it posts an event.</li>
<li>each tuner has an associated id (set when it is loaded),
and events posted contain the tuner id.</li>
<li>each tuner has a BPF component (built using a BPF skeleton)
and a userspace component.  The latter has init(), fini()
and event_handler() entrypoints.  When an event is
received, the tuner id is used to identify the appropriate
event handler and its event_handler() callback function is run.</li>
<li>init, fini and event_handler functions are loaded from the
tuner .so object.</li>
<li>BPF components should include bpftune.bpf.h; it contains
the common map definitions (ringbuf, etc) and shared variables
such as learning rate and tuner ids that each tuner needs.</li>
</ul>

<ul dir="auto">
<li>TCP connection tuner: auto-tune choice of congestion control algorithm.
See bpftune-tcp-conn (8).</li>
<li>neighbour table tuner: auto-tune neighbour table sizes by growing
tables when approaching full. See bpftune-neigh (8).</li>
<li>route table tuner: auto-tune route table size by growing tables
when approaching full.  See bpftune-route (8).</li>
<li>sysctl tuner: monitor sysctl setting and if it collides with an
auto-tuned sysctl value, disable the associated tuner.  See
bpftune-sysctl (8).</li>
<li>TCP buffer tuner: auto-tune max and initial buffer sizes.  See
bpftune-tcp-buffer (8).</li>
<li>net buffer tuner: auto-tune tunables related to core networking.
See bpftune-net-buffer (8).</li>
<li>netns tuner: notices addition and removal of network namespaces,
which helps power namespace awareness for bpftune as a whole.
Namespace awareness is important as we want to be able to auto-tune
containers also.  See bpftune-netns (8).</li>
</ul>

<p dir="auto">Both core bpftune.c and individual tuners use the libbpftune library.
It handles logging, tuner init/fini, and BPF init/fini.</p>
<p dir="auto">Each tuner shared object defines an init(), fini() and event_handler()
function. These respectively set up and clean up BPF and handle events
that originate from the BPF code.</p>

<p dir="auto">If building the repository manually, simply run</p>
<div data-snippet-clipboard-copy-content="$ make ; sudo make install"><pre><code>$ make ; sudo make install
</code></pre></div>
<p dir="auto">at the top-level of the repository.  bpftune also supports a</p>

<p dir="auto">target, which will make a bpftune RPM.  See ./buildrpm/bpftune.spec</p>
<p dir="auto">We can also build with non-standard libdir for distros which do not
use /usr/lib64 like CachyOS; in this case to install to /usr/lib
instead</p>
<div data-snippet-clipboard-copy-content="$ make libdir=lib
$ sudo make install libdir=lib"><pre><code>$ make libdir=lib
$ sudo make install libdir=lib
</code></pre></div>
<p dir="auto">To build the following packages are needed (names may vary by distro);</p>
<ul dir="auto">
<li>libbpf, libbpf-devel &gt;= 0.6</li>
<li>libcap-devel</li>
<li>bpftool &gt;= 4.18</li>
<li>libnl3-devel</li>
<li>clang &gt;= 11</li>
<li>llvm &gt;= 11</li>
<li>python3-docutils</li>
</ul>
<p dir="auto">From the kernel side, the kernel needs to support BPF ring buffer
(around the 5.6 kernel, though 5.4 is supported on Oracle Linux
as ring buffer support was backported), and kernel BTF is
required (CONFIG_DEBUG_INFO_BTF=y).  Verify /sys/kernel/btf/vmlinux
is present.</p>
<p dir="auto">To enable bpftune as a service</p>
<div data-snippet-clipboard-copy-content="$ sudo service bpftune start"><pre><code>$ sudo service bpftune start
</code></pre></div>
<p dir="auto">...and to enable it by default</p>
<div data-snippet-clipboard-copy-content="$ sudo systemctl enable bpftune"><pre><code>$ sudo systemctl enable bpftune
</code></pre></div>
<p dir="auto">bpftune logs to syslog so /var/log/messages will contain details
of any tuning carried out.</p>
<p dir="auto">bpftune can also be run in the foreground as a program; to redirect
output to stdout/stderr, run</p>

<p dir="auto">On exit, bpftune will summarize any tuning done.</p>

<p dir="auto">Tests are supplied for each tuner in the tests/ subdirectory.
&#34;make test&#34; runs all the tests.  Tests use network namespaces
to simulate interactions with remote hosts. See ./TESTING.md
for more details.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Does my system support bpftune?</h2><a id="user-content-does-my-system-support-bpftune" aria-label="Permalink: Does my system support bpftune?" href="#does-my-system-support-bpftune"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Simply run &#34;bpftune -S&#34; to see:</p>
<div data-snippet-clipboard-copy-content="$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)"><pre><code>$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)
</code></pre></div>
<p dir="auto">Two aspects are important here</p>
<ul dir="auto">
<li>does the system support fentry/fexit etc? If so full support
is likely.</li>
<li>does the system support network namespace cookies? If so
per-network-namespace policy is supported.</li>
</ul>

<p dir="auto">Simply starting bpftune and observing changes made via /var/log/messages
can be instructive.  For example, on a standard VM with sysctl defaults,
I ran</p>

<p dir="auto">...and went about normal development activities such as cloning git
trees from upstream, building kernels, etc.  From the log we see
some of the adjustments bpftune made to accommodate these activities</p>
<div data-snippet-clipboard-copy-content="$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario &#39;specify bbr congestion control&#39; occurred for tunable &#39;TCP congestion control&#39; in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify &#39;bbr&#39; congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -&gt; (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -&gt; (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario &#39;specify bbr congestion control&#39; occurred for tunable &#39;TCP congestion control&#39; in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify &#39;bbr&#39; congestion control algorithm"><pre><code>$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario &#39;specify bbr congestion control&#39; occurred for tunable &#39;TCP congestion control&#39; in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify &#39;bbr&#39; congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -&gt; (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -&gt; (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario &#39;specify bbr congestion control&#39; occurred for tunable &#39;TCP congestion control&#39; in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify &#39;bbr&#39; congestion control algorithm
</code></pre></div>
<p dir="auto">To deterministically trigger bpftune behaviour, one approach we can
take is to download a large file with inappropriate settings.</p>
<p dir="auto">In one window, set tcp rmem max to a too-low value, and run bpftune
as a program logging to stdout/stderr (-s):</p>
<div data-snippet-clipboard-copy-content="$ sudo sysctl -w net.ipv4.tcp_rmem=&#34;4096 131072 1310720&#34;
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s"><pre><code>$ sudo sysctl -w net.ipv4.tcp_rmem=&#34;4096 131072 1310720&#34;
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s
</code></pre></div>
<p dir="auto">In another window, wget a large file:</p>
<div data-snippet-clipboard-copy-content="$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso"><pre><code>$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso
</code></pre></div>
<p dir="auto">In the first window, we see bpftune tuning up rmem:</p>
<div data-snippet-clipboard-copy-content="bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -&gt; (4096 131072 1638400)"><pre><code>bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario &#39;need to increase TCP buffer size(s)&#39; occurred for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -&gt; (4096 131072 1638400)
</code></pre></div>
<p dir="auto">This occurs multiple times, and on exit (Ctrl+C) we see
the summary of changes made:</p>
<div data-snippet-clipboard-copy-content="bpftune: Summary: scenario &#39;need to increase TCP buffer size(s)&#39; occurred 9 times for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl &#39;net.ipv4.tcp_rmem&#39; changed from (4096 131072 1310720 ) -&gt; (4096 131072 9765625 )"><pre><code>bpftune: Summary: scenario &#39;need to increase TCP buffer size(s)&#39; occurred 9 times for tunable &#39;net.ipv4.tcp_rmem&#39; in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl &#39;net.ipv4.tcp_rmem&#39; changed from (4096 131072 1310720 ) -&gt; (4096 131072 9765625 )
</code></pre></div>

<p dir="auto">See the docs/ subdirectory for manual pages covering bpftune
and associated tuners.</p>
<p dir="auto">bpftune was presented at the eBPF summit; <a href="https://www.youtube.com/watch?v=X0TvfH8hrQE&amp;t=420s" rel="nofollow">video here</a>.</p>
<p dir="auto">bpftune <a href="https://www.youtube.com/watch?v=3ylmGE6sW8w" rel="nofollow">was also discussed on Liz Rice&#39;s excellent eCHO eBPF podcast</a>, specifically in the context of using reinforcement learning in BPF</p>

<p dir="auto">This project welcomes contributions from the community. Before submitting a pull request, please <a href="https://corinfaife.co/oracle/bpftune/blob/main/CONTRIBUTING.md">review our contribution guide</a></p>

<p dir="auto">Please consult the <a href="https://corinfaife.co/oracle/bpftune/blob/main/SECURITY.md">security guide</a> for our responsible security vulnerability disclosure process</p>

<p dir="auto">Copyright (c) 2023 Oracle and/or its affiliates.</p>
<p dir="auto">This software is available to you under</p>
<p dir="auto">SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note</p>
<p dir="auto">Being under the terms of the GNU General Public License version 2.</p>
<p dir="auto">SPDX-URL: <a href="https://spdx.org/licenses/GPL-2.0.html" rel="nofollow">https://spdx.org/licenses/GPL-2.0.html</a></p>
<p dir="auto">See <a href="https://corinfaife.co/oracle/bpftune/blob/main/LICENSE.txt">the license file</a> for more details.</p>
</article></div></div>
  </body>
</html>
