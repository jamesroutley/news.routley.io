<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.arroyo.dev/blog/why-arrow-and-datafusion">Original</a>
    <h1>Building a streaming SQL engine with Arrow and DataFusion</h1>
    
    <div id="readability-page-1" class="page"><article><p>Arroyo is coming up on its one-year anniversary as an open-source project, and
we&#39;ve got something big planned. Our next release, 0.10, will feature an
entirely new SQL engine built around <a href="https://arrow.apache.org/">Apache Arrow</a>
and the <a href="https://arrow.apache.org/datafusion/">DataFusion</a> SQL toolkit.</p>
<div><div><div><p>New to Arroyo?</p><p>Arroyo is an
<a href="https://github.com/ArroyoSystems/arroyo">open-source</a> stream processing engine,
enabling users to transform, filter, aggregate, and join their data streams in
real-time with SQL queries. It&#39;s designed to be easy enough for any SQL user to
build correct, reliable, and scalable streaming pipelines.</p></div></div></div>
<p>This evolution reflects many lessons learned over the past two years of building
Arroyo and many years of working on streaming engines before that. This post
will go into detail on Arroyo&#39;s current implementation and why that&#39;s changing,
but in short:</p>
<ul>
<li><strong>Performance</strong>: Arrow is an in-memory columnar format, designed to leverage
the vector processing capabilities of modern CPUs; combined with
high-performance compute kernels, we can achieve state-of-the-art performance
for streaming, competitive with the best batch engines</li>
<li><strong>Architectural simplicity</strong>: Today Arroyo generates Rust code, which is then
compiled into a pipeline binary that performs data processing. Ahead-of-time
compilation provides good performance, but requires complex infrastructure to
compile pipelines. <strong>Arroyo 0.10 ships as a single, compact binary</strong> that can
be deployed in a variety of ways.</li>
<li><strong>Community</strong>: Arrow is fast becoming the center of the next-gen data stack,
and by adopting it Arroyo can seamlessly interact with other data systems and
even other languages like Python. By fully embracing DataFusion, we&#39;re able to
leverage (and contribute to) the incredible work of the emerging Rust data
ecosystem.</li>
</ul>
<p>Since data folks like numbers, here are some comparisons to Arroyo 0.9:</p>
<ul>
<li>Throughput: <strong>3x higher</strong></li>
<li>Pipeline startup: <strong>20x faster</strong></li>
<li>Docker image size: <strong>11x smaller</strong></li>
</ul>
<p>As of today, Arroyo 0.10 is available as a developer preview. You can get
started by running the docker container:</p>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="shellsession" data-theme="default"><code data-language="shellsession" data-theme="default"><span data-line=""><span>docker run -it -p 8000:8000 ghcr.io/arroyosystems/arroyo-single:0.10.0-dev</span></span></code></pre></div>
<p>We&#39;d love to hear your feedback on
<a href="https://github.com/ArroyoSystems/arroyo">Github</a> or
<a href="https://discord.com/invite/cjCr5rVmyR">Discord</a>!</p>
<p>So how did we get here, and why are we now making this change? Let&#39;s walk
through some Arroyo history and along the way cover some of the design decisions
that go into building a SQL engine.</p>
<h2 id="table-of-contents"><a aria-hidden="true" tabindex="-1" href="#table-of-contents"><span></span></a>Table of contents</h2>
<ul>
<li>
<p><a href="#arroyos-origins">Arroyo&#39;s origins</a></p>
</li>
<li>
<p><a href="#adding-sql">Adding SQL</a></p>
<ul>
<li><a href="#representing-data">Representing data</a></li>
<li><a href="#implementing-expressions">Implementing expressions</a></li>
</ul>
</li>
<li>
<p><a href="#building-a-managed-cloud">Building a managed cloud</a></p>
</li>
<li>
<p><a href="#improving-the-self-hosting-experience">Improving the self-hosting experience</a></p>
</li>
<li>
<p><a href="#the-new-arroyo">The new Arroyo</a></p>
<ul>
<li>
<p><a href="#arrow-dataflow">Arrow dataflow</a></p>
<ul>
<li><a href="#streaming-on-columns">Streaming on columns</a></li>
<li><a href="#interoperating">Interoperating</a></li>
</ul>
</li>
<li>
<p><a href="#embracing-datafusion">Embracing DataFusion</a></p>
</li>
<li>
<p><a href="#just-the-beginning">Just the beginning</a></p>
</li>
</ul>
</li>
<li>
<p><a href="#get-involved">Get involved</a></p>
</li>
</ul>
<h2 id="arroyos-origins"><a aria-hidden="true" tabindex="-1" href="#arroyos-origins"><span></span></a>Arroyo&#39;s origins</h2>
<p>Arroyo was inspired by my experience leading
<a href="https://flink.apache.org/">Apache Flink</a> teams at Lyft and Splunk. I&#39;d seen the
challenges of developing Flink streaming pipelines and how difficult they were
operate. Around the data ecosystems, projects like Redpanda and ScyllaDB were
successfully rethinking existing Java systems with simpler, higher-performance
implementations in non-managed languages, and I thought there was an opportunity
to do the same with Flink.</p>
<p>The initial prototype of Arroyo took Flink as a direct inspiration. Our new
system would be written in a faster language (Rust) and would fix some of its
shortcomings, particularly around state.</p>
<p>Other aspects we initially kept the same. For example, the core API of Flink is
called the Datastream API. It&#39;s a Java API used to directly define the
<em>dataflow graph</em>. This is a directed, acyclic graph on whose edges data messages
flow, between operators which implement the various parts of query logic (like
filter, joins, or windows).</p>
<p>In our initial prototype of Arroyo, this graph was similarly defined directly
via a Rust API.</p>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="rust" data-theme="default"><code data-language="rust" data-theme="default"><span data-line=""><span>Stream</span><span>::&lt;()&gt;::</span><span>with_parallelism</span><span>(</span><span>1</span><span>)</span></span>
<span data-line=""><span>    .</span><span>source</span><span>(</span><span>KafkaSource</span><span>::</span><span>new</span><span>(</span><span>&#34;localhost:9092&#34;</span><span>, </span><span>&#34;events&#34;</span><span>, </span><span>HashMap</span><span>::</span><span>new</span><span>()))</span></span>
<span data-line=""><span>    .</span><span>watermark</span><span>(</span><span>WatermarkType</span><span>::</span><span>Periodic</span><span> {</span></span>
<span data-line=""><span>        </span><span>period</span><span>: </span><span>Duration</span><span>::</span><span>from_secs</span><span>(</span><span>1</span><span>),</span></span>
<span data-line=""><span>        </span><span>max_lateness</span><span>: </span><span>Duration</span><span>::ZERO,</span></span>
<span data-line=""><span>    })</span></span>
<span data-line=""><span>    .</span><span>flat_map</span><span>(|</span><span>event</span><span>| {</span></span>
<span data-line=""><span>            </span><span>event</span><span>.</span><span>split</span><span>(</span><span>&#34; &#34;</span><span>).</span><span>map</span><span>(|</span><span>w</span><span>| (</span><span>w</span><span>.</span><span>to_string</span><span>(), </span><span>1</span><span>)).</span><span>collect</span><span>()</span></span>
<span data-line=""><span>    })</span></span>
<span data-line=""><span>    .</span><span>key_by</span><span>(|(</span><span>word</span><span>, </span><span>_</span><span>)| </span><span>word</span><span>)</span></span>
<span data-line=""><span>    .</span><span>window</span><span>(</span><span>TumblingWindow</span><span>::</span><span>new</span><span>(</span><span>Duration</span><span>::</span><span>from_secs</span><span>(</span><span>1</span><span>)))</span></span>
<span data-line=""><span>    .</span><span>sum_by</span><span>(|(</span><span>_</span><span>, </span><span>count</span><span>)| </span><span>count</span><span>)</span></span>
<span data-line=""><span>        .</span><span>sink</span><span>(</span><span>KeyedConsoleSink</span><span>::</span><span>new</span><span>());</span></span></code></pre></div>
<p>But a compiled language like Rust presents some challenges here compared to the
JVM. In Flink, pipelines are written in Java or Scala, compiled to JVM bytecode,
and then dynamically loaded into the Flink runtime. But such an approach
wouldn&#39;t fit well for Rust which expects statically compiled binaries<sup><a href="#user-content-fn-0" id="user-content-fnref-0" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p>
<p>Instead we structured the Arroyo runtime as a library, which would be invoked by
the actual pipeline code. Everything would then be compiled into a static binary
which would execute the pipeline.</p>
<h2 id="adding-sql"><a aria-hidden="true" tabindex="-1" href="#adding-sql"><span></span></a>Adding SQL</h2>
<p>At Lyft, SQL was the most requested feature from potential users of our
streaming platform. While Flink had a SQL API, user tests showed that it was
still too confusing and required too much Flink expertise for non-streaming
experts to pick up.</p>
<p>So we knew from the beginning we needed to have an excellent implementation of
SQL. We wanted data engineers and scientists who knew SQL to be able to build
correct, reliable, and performant streaming pipelines without needing much
expertise in building streaming systems.</p>
<p>When we began building the SQL interface we didn&#39;t want to start from scratch.
So we turned to DataFusion, which is both a complete batch SQL engine and a
composable library of SQL primitives. We decided to use just the <em>frontend</em>,
which takes raw strings SQL provided by users through several stages:</p>
<ol>
<li><strong>Parsing</strong>, where the SQL text is turned into an abstract-syntax tree (AST)</li>
<li><strong>Planning</strong>, which translates the AST into a <em>logical graph</em> of SQL
operators with data dependencies between them</li>
<li><strong>Optimization</strong>, where various rewrite rules are applied to the graph to
simplify it and make it more efficient to execute</li>
</ol>
<p>Once we had the optimized graph, we translated it into our own dataflow
representation (the <em>physical graph</em>) that we would execute in our runtime. This
dataflow representation is the same as the one that could be manually
constructed by using the Rust pipeline API described previously.</p>
<img src="https://www.arroyo.dev/posts/arrow-migration/sql_to_dataflow.png" alt="diagram showing the steps to transform a SQL query into a dataflow graph"/>
<p>Essentially, SQL support was layered on top of the existing graph API. This is
actually quite similar to how Flink SQL works—SQL is parsed and planned by an
external library (<a href="https://calcite.apache.org/">Apache Calcite</a>), then compiled
into a Flink Datastream program.</p>
<p>Once settling on this basic approach, there were two other design decisions we
had to make, which would determine much of our future development: how we
represented SQL data rows and how we implemented SQL expressions.</p>
<h3 id="representing-data"><a aria-hidden="true" tabindex="-1" href="#representing-data"><span></span></a>Representing data</h3>
<p>SQL engines operate on rows of tabular data<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. Tables are defined with
<em>schemas</em>, which tell the engine the names and data types of the columns.
Queries over those tables will <em>project</em> (transform) the data into other
structures of data.</p>
<p>Let&#39;s take this example schema:</p>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="sql" data-theme="default"><code data-language="sql" data-theme="default"><span data-line=""><span>CREATE</span><span> </span><span>TABLE</span><span> </span><span>products</span><span> (</span></span>
<span data-line=""><span>    id </span><span>INT</span><span>,</span></span>
<span data-line=""><span>    </span><span>name</span><span> </span><span>TEXT</span><span>,</span></span>
<span data-line=""><span>    </span><span>description</span><span> </span><span>TEXT</span><span>,</span></span>
<span data-line=""><span>    price </span><span>FLOAT</span><span>,</span></span>
<span data-line=""><span>    quantity </span><span>INT</span></span>
<span data-line=""><span>);</span></span></code></pre></div>
<p>We can project it into another form with a query like</p>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="sql" data-theme="default"><code data-language="sql" data-theme="default"><span data-line=""><span>SELECT</span><span> price * </span><span>0</span><span>.</span><span>92</span><span> </span><span>as</span><span> euros </span><span>FROM</span><span> products</span></span></code></pre></div>
<p>which will give us a new schema for the result with a single field <code>euros FLOAT</code>.</p>
<p>In the graph representation of this query, we have three nodes, or operators: a
<em>source</em> (the table), a projection (the select statement), and a sink. Between
each of those nodes is an edge, which has the data type of the records that flow
along that edge.</p>
<img src="https://www.arroyo.dev/posts/arrow-migration/typed-dataflow.png" alt="An illustration of a dataflow with types flowing along the edges"/>
<p>To implement this in a real system, we need to have actual data types—ways of
storing the data that operators produce and consume. There are two basic
approaches here:</p>
<ul>
<li>Dynamic typing: each row is represented by an array of fields, each of which
is an enum of all of the supported data types. In Rust this could look like</li>
</ul>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="rust" data-theme="default"><code data-language="rust" data-theme="default"><span data-line=""><span>enum</span><span> </span><span>Datum</span><span> {</span></span>
<span data-line=""><span>  </span><span>Text</span><span>(</span><span>String</span><span>),</span></span>
<span data-line=""><span>  </span><span>Int</span><span>(</span><span>i32</span><span>),</span></span>
<span data-line=""><span>  </span><span>Long</span><span>(</span><span>i64</span><span>),</span></span>
<span data-line=""><span>  </span><span>Float</span><span>(</span><span>f32</span><span>),</span></span>
<span data-line=""><span>  ...</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>struct</span><span> </span><span>Record</span><span> {</span></span>
<span data-line=""><span>	</span><span>fields</span><span>: </span><span>Vec</span><span>&lt;</span><span>Datum</span><span>&gt;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>// The query about would produce a record like</span></span>
<span data-line=""><span>Record</span><span> {</span></span>
<span data-line=""><span>  </span><span>fields</span><span>: </span><span>vec!</span><span>[</span><span>Datum</span><span>::</span><span>Float</span><span>(</span><span>50.4</span><span>)]</span></span>
<span data-line=""><span>}</span></span></code></pre></div>
<ul>
<li>Static typing: structs are generated for each unique data type, which are
compiled into the pipeline binary. For example we&#39;d generate this struct for
the return type of that query:</li>
</ul>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="rust" data-theme="default"><code data-language="rust" data-theme="default"><span data-line=""><span>struct</span><span> </span><span>GeneratedRecord2</span><span> {</span></span>
<span data-line=""><span>  </span><span>euros</span><span>: </span><span>f32</span></span>
<span data-line=""><span>}</span></span></code></pre></div>
<p>While dynamic typing is more popular in SQL engines (for example, used by
Materialize, RisingWave, and traditional databases like Postgres), we opted for
static representations. There were a few reasons for this, including better
performance (compilers can apply much more powerful optimizations if they know
the type of the data they&#39;re working with) and a better fit for the Rust API,
which supported user-defined struct types.</p>
<h3 id="implementing-expressions"><a aria-hidden="true" tabindex="-1" href="#implementing-expressions"><span></span></a>Implementing expressions</h3>
<p>SQL has an <em>expression language,</em> and any SQL engine needs to be able to
evaluate those expressions during execution. For example, in the query above we
have the expression <code>price * 0.92</code>, which takes a field from the row and
multiplies it by a constant. Modern SQL engines support a
<a href="https://www.postgresql.org/docs/9.2/functions.html">huge variety</a> of scalar
functions, aggregates, window functions, and other ways of transforming values.</p>
<p>Engine designers once again have two options<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>, one dynamic and one static:</p>
<ul>
<li><em>Interpretation:</em> we traverse through the expression tree at runtime and
evaluate each node against our data</li>
<li><em>Compilation</em>: at planning time we compile the expression into machine code
that is directly executed; this will generally be much faster than an
interpreter</li>
</ul>
<p>For traditional batch SQL engines there&#39;s a difficult tradeoff here. A user will
send a batch query to the engine and wait until it has returned the result.
Users thus care mostly about the end-to-end time a query takes to execute. Any
time spent compiling the query is time that could have been spent executing it.
For long-running queries (minutes to hours) the compile time gets amortized, but
for a small query (seconds) it will tend to dominate the total runtime. Thus
nearly all batch engines use some form of interpretation.</p>
<p>But for streaming engines the calculus is reversed. Streaming pipelines run
indefinitely—for days or months—and users care primarily about cost. That means
almost any upfront time is worth paying if it reduces the lifetime cost of
executing the pipeline. In streaming engines it&#39;s therefore much more common to
see ahead-of-time (AoT) compilation.</p>
<p>Arroyo already had a way to run Rust pipelines, so the straightforward answer
for us was to compile SQL expressions into Rust code, and then compile it with
the Rust compiler, which is very good at producing fast binaries thanks to the
sophisticated LLVM optimizer.</p>
<img src="https://www.arroyo.dev/posts/arrow-migration/expression_parsing.png" alt="Diagram showing the process of parsing an expression and producing Rust code from it"/>
<p>As a bonus, this approach also made it easy to add support for
<a href="https://doc.arroyo.dev/sql/udfs">user-defined functions</a> (UDFs). The Rust code
generated from SQL expressions could natively call UDFs without any translation
layer and could benefit from inlining and other compiler optimizations.</p>
<h2 id="building-a-managed-cloud"><a aria-hidden="true" tabindex="-1" href="#building-a-managed-cloud"><span></span></a>Building a managed cloud</h2>
<p>Arroyo was originally conceived and built as a managed cloud service. I&#39;d seen
that running streaming pipelines reliably was a huge challenge for companies,
who found they to needed to dedicate streaming ops folks. By offering a managed
service, we could completely free users from having to worry about operations
and enable many more organizations to adopt streaming.</p>
<p>In some ways, this early focus on providing a managed service has served us very
well. Excellent support for multi-tenancy is very hard to retrofit into an
existing engine, as I&#39;d experienced
<a href="https://www.arroyo.dev/blog/why-not-flink#flink-as-a-service">building managed services with Flink</a>.</p>
<p>But as a now open source project designed for self-hosting it held us back. When
we initially
<a href="https://www.arroyo.dev/blog/open-sourcing-the-arroyo-streaming-engine">open-sourced Arroyo last April</a>
we reworked a few systems to make it simpler to self-host, like writing a new
state backend that operated directly on S3. But other complex pieces of
infrastructure remained.</p>
<p>For a multi-tenant cloud, the cost of a piece of infrastructure—both in terms of
hardware and operations—is amortized across all users of the service. But for a
company trying to run a few pipelines the fixed costs tend to dominate.</p>
<p>This was particularly true of our reliance on Rust codegen and AoT compilation.
To start pipelines users would need a Rust compiler. And for quickly iterating
on pipelines it was important that the compiler have enough CPU to compile
pipelines in a reasonable amount of time. The initial user experience was an
even bigger issue: while Rust has good incremental compilation support, the cold
build will be very slow (5+ minutes)—a long time to wait to run your first
pipeline.</p>
<p>We worked around those issues by building out a compiler service which made good
use of incremental compilation by reusing the same workspace for each pipeline
build. To speed up the first pipeline, we built a
<a href="https://github.com/ArroyoSystems/arroyo/blob/555db2b2a2f3a1a812f5850618e5e19e5ea50d30/docker/cluster/compiler/Dockerfile">special docker image</a>
that included a pre-warmed compiler cache. This worked quite well, ensuring that
even the first pipeline could be started fairly quickly—typically under 10
seconds.</p>
<h2 id="improving-the-self-hosting-experience"><a aria-hidden="true" tabindex="-1" href="#improving-the-self-hosting-experience"><span></span></a>Improving the self-hosting experience</h2>
<p>Arroyo has seen enormous growth as a project in the past year with our current
approach. So why are making this change now?</p>
<p>Each of these design decisions—static typing, AoT code generation, using
cloud-scale infrastructure—came with costs as well as benefits. While they made
sense for our initial design target (building a managed cloud service) they were
preventing adoption of Arroyo as a self-hosted engine.</p>
<p>The core issue was the compiler service: at scale, the resources required to run
it would be amortized. But for a single company, this was a significant expense
and additional architectural complexity. It also limited how small Arroyo
deployments could be, since they relied on big pieces of common infrastructure.
And finally, managing this infrastructure required complex systems like
Kubernetes.</p>
<p>As developers working on the engine, code generation was also proving difficult.
Rust has excellent libraries like <a href="https://github.com/dtolnay/syn">syn</a> and
<a href="https://github.com/dtolnay/quote">quote</a> for parsing and producing Rust code,
but code gen is still <em>hard</em>. Developers need to think on multiple levels at
once, and understand how the generated code interacts with its environment.</p>
<p>You&#39;re also giving up one of the biggest benefits of Rust: the
<a href="https://www.arroyo.dev/blog/rust-for-data-infra#if-it-compiles-its-probably-correct">compiler is very good at catching mistakes</a>.
With code gen, compile-time errors are effectively made into run-time errors. We
built some
<a href="https://www.arroyo.dev/blog/end-to-end-sql-tests-with-rust-proc-macros">cool testing infra</a>
to help mitigate this, but runtime compile errors were a persistent problem.</p>
<p>Finally, we found that outside contributors really struggled to be productive
with this style of development, and found understanding the system and debugging
it very challenging.</p>
<h2 id="the-new-arroyo"><a aria-hidden="true" tabindex="-1" href="#the-new-arroyo"><span></span></a>The new Arroyo</h2>
<p>Last fall, we began thinking about what a version of Arroyo without AoT
compilation could look like. Many aspects of the system would need to change.
Static typing would no longer be possible; we&#39;d need to move to some form of
interpretation for SQL expressions. UDFs would become more challenging to
support.</p>
<p>This was going to be a huge project, reworking many fundamental aspects of the
engine. But after prototyping some possible approaches, we made the decision to
move forward.</p>
<h3 id="arrow-dataflow"><a aria-hidden="true" tabindex="-1" href="#arrow-dataflow"><span></span></a>Arrow dataflow</h3>
<p>Our first decision was to adopt <a href="https://arrow.apache.org">Apache Arrow</a><sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> as
our in-memory data representation, replacing the static Struct types. Arrow is a
columnar, in-memory format designed for analytical computations. The coolest
thing about Arrow is that it&#39;s a cross-language standard; it supports sharing
data directly between engines and even different languages without copying or
serialization overhead. For example, Pandas programs written in Python could
operate directly on data generated by Arroyo.</p>
<h4 id="streaming-on-columns"><a aria-hidden="true" tabindex="-1" href="#streaming-on-columns"><span></span></a>Streaming on columns</h4>
<p>Columnar representations<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup> have been adopted by nearly every OLAP
(analytics-oriented) engine over the past decade. There are a few reasons for
this:</p>
<ul>
<li>By storing all values in a column together, you can achieve better compression
ratios and make better use of CPU cache</li>
<li>Only the columns actually referenced in a query need to be read, reducing disk
and network IO</li>
<li>Columnar processing aligns well with the vector capabilities in modern CPUs,
providing 10x or more speedups</li>
</ul>
<p>However, row-oriented data remains the standard for streaming engines. There are
some inherent tradeoffs between latency (how quickly an event can traverse
through the pipeline) and throughput (how many events can be processed with a
given amount of CPU). By batching data we can get higher throughput at the
expense of latency. And columnar representations require that we batch a number
of events together before we see performance improvements (in fact, with a small
number of rows columnar processing will be much slower due to fixed overhead).</p>
<img src="https://www.arroyo.dev/posts/arrow-migration/row-vs-column.png" alt="row-oriented vs column oriented database design"/>
<p>Streaming engines like Flink and Arroyo operate—logically—on events one at a
time, with important guarantees around ordered processing. Initially they were
implemented <em>physically</em> operating on events one by one. But the benefits of
batching are hard to ignore and more recent versions of Flink do
<a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/tuning/#minibatch-aggregation">support some batching in SQL operators</a><sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup>.</p>
<p>But I think the argument for why batching makes sense in streaming is simple:
<em>at any given batch size, the higher the throughput the less time we must wait
to fill that batch.</em> For example, if we want at least 100 records in our batch
to overcome fixed costs, the amount of time we need to wait to receive 100
records will depend on our throughput:</p>
<ul>
<li>At 10 events/second, it takes 1 second</li>
<li>At 1,000 — 0.01 seconds (100ms)</li>
<li>At 1,000,000 — 0.0001 (0.1ms)</li>
</ul>
<p>Or looking at it from a fixed latency perspective (say, waiting at most 10ms):</p>
<ul>
<li>At 10 events/second, our batch size is 1</li>
<li>At 1,000 — 100</li>
<li>At 1,000,000 — 100,000</li>
</ul>
<p>The takeaway: we only have to pay high overhead of small batch sizes when our
data volume is very low. But if we&#39;re only handling 10 or 100 events per second,
the overall cost of processing will be very small in any case. And at high data
volumes (tens of thousands to millions of events per second) we can have our
cake and eat it too—achieve high throughput with batching and columnar data
while still maintaining low absolute latency.</p>
<h4 id="interoperating"><a aria-hidden="true" tabindex="-1" href="#interoperating"><span></span></a>Interoperating</h4>
<p>The other key benefit of Arrow—over building our own columnar format—is that
it&#39;s an open standard that is being increasingly adopted by modern data
libraries and engines. Unlike other standard data formats (e.g., Parquet) Arrow
is an <em>in-memory representation</em>. This means that it enables multiple engines,
libraries, and even languages to operate on in-memory data without serialization
or even copying overhead.</p>
<p>We&#39;re very excited about the possibilities that Arrow provides for interacting
with other parts of the data ecosystem. In a future version of Arroyo, it should
be possible to write performant UDFs and UDAFs in Python or Java that operate
seamlessly within the streaming computation, without needing to send results off
to another system.</p>
<p>More broadly we see Arrow as a sea change in how data systems are built, and are
excited to bring that revolution to the streaming world.</p>
<h3 id="embracing-datafusion"><a aria-hidden="true" tabindex="-1" href="#embracing-datafusion"><span></span></a>Embracing DataFusion</h3>
<p>I&#39;ve mentioned <a href="https://arrow.apache.org/datafusion/">DataFusion</a> a few times
already in this post, but it&#39;s worth at this point giving the full introduction.
DataFusion emerged out of the Arrow project with an ambitious set of goals:
provide both a state-of-the-art query engine for end users <em>and</em> a composable
toolkit for developers building their own SQL tools. And like Arroyo, it&#39;s
written in Rust,
<a href="https://www.arroyo.dev/blog/rust-for-data-infra">the best language for data infrastructure</a>.
Since starting in 2019 it&#39;s achieved both of these goals. As a query engine it
regularly leads benchmarks. As a library, numerous projects have successfully
built their SQL engines around it, including
<a href="https://www.influxdata.com/">InfluxDB</a>,
<a href="https://github.com/paradedb/paradedb">ParadeDB</a>,
<a href="https://github.com/GreptimeTeam/greptimedb">GreptimeDB</a> and
<a href="https://arrow.apache.org/DataFusion/user-guide/introduction.html#known-users">many others</a>.</p>
<img src="https://www.arroyo.dev/posts/arrow-migration/datafusion.svg" alt="The DataFusion logo"/>
<p>Arroyo has long relied on DataFusion for just its SQL frontend (responsible for
parsing and planning SQL). In other words, DataFusion operated only in the
control plane of Arroyo, while the data plane (where data is processed) was our
own engine, built from scratch. This meant implementing all of the SQL
operators, expressions, and functions. And since our existing implementations
rely on code generation and compilation, we were looking at redoing most of this
work for the next version.</p>
<p>Instead, we took a second look at DataFusion and asked ourselves…could we be
using DataFusion physical plans, operators, and expressions<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>? After some
prototyping, the answer turned out: yes! Despite having been designed for batch
systems, we found that it was flexible enough to reuse much of the code to
implement streaming versions of our operators. And the physical expression
code-responsible for evaluating SQL expression logic on Arrow data—was useable
more-or-less as is.</p>
<p>There is a huge amount of momentum in the Rust data community right now, and
much of it is centered around DataFusion. We&#39;re very excited to be part of that
effort, contributing back work to make it an excellent streaming engine in
addition to its batch capabilities.</p>
<h3 id="just-the-beginning"><a aria-hidden="true" tabindex="-1" href="#just-the-beginning"><span></span></a>Just the beginning</h3>
<p>We started Arroyo with the goal of making streaming easy enough that every
company can adopt it, and to empower every data user to build reliable, correct, and
efficient real-time data pipelines.</p>
<p>With 0.10, we&#39;re taking the next step towards that vision by greatly expanding the
environments in which Arroyo can be run. But we&#39;re just getting started.</p>
<p>Now that Arroyo compiles down to a single binary, we&#39;re working to remove the other
external dependencies, including Postgres and Prometheus; future releases of Arroyo
will have the option of running their control plane on an embedded sqlite database.</p>
<p>We&#39;re also working on new methods of distribution and configuration, including homebrew
support and the ability to run pipelines directly from the binary without the persistent
API and Web UI.</p>
<p>Beyond deployment and operations, we&#39;re also very excited to keep pushing the boundaries
of streaming performance. Arroyo 0.10 is already one of the fastest streaming engines
in the market, and there are many more optimizations we haven&#39;t implemented yet.</p>
<h2 id="get-involved"><a aria-hidden="true" tabindex="-1" href="#get-involved"><span></span></a>Get involved</h2>
<p>The Arroyo 0.10 release has been the culmination of months of effort, completely
rethinking how SQL streaming engines can work. We couldn&#39;t be more excited to
share this with our community and the world, and see what users build on it.</p>
<p>The developer preview, 0.10-dev, is available starting today. Getting started
with Docker is as easy as running</p>
<div data-rehype-pretty-code-fragment=""><pre tabindex="0" data-language="shellsession" data-theme="default"><code data-language="shellsession" data-theme="default"><span data-line=""><span>docker run -it -p 8000:8000 ghcr.io/arroyosystems/arroyo-single:0.10.0-dev</span></span></code></pre></div>
<p>Note that the developer preview still has a few missing features compared to 0.9,
including SQL window functions, struct joins, update tables, and async UDFs,
which will be added back in the coming weeks.</p>
<p>There are also a couple of breaking changes to the SQL syntax:</p>
<ul>
<li>Virtual field definitions must be followed by the <code>STORED</code> keyword, matching postgres syntax,
for example <code>watermark TIMESTAMP GENERATED ALWAYS AS (timestamp - INTERVAL &#39;1 minute&#39;) STORED</code></li>
<li>Re-aggregating a window aggregate now requires that the window be explicitly included in each GROUP BY.
Previously it was implicitly included.</li>
</ul>
<p>We&#39;d love to hear your feedback! Come join our
<a href="https://discord.gg/cjCr5rVmyR">Discord</a> or reach us via
<a href="mailto:founders@arroyo.systems">email</a>. We also love contributions from the
community. See the <a href="https://doc.arroyo.dev/developing/dev-setup">dev guide</a>
to get started, and come chat with the team in Discord with any questions.</p>
</article></div>
  </body>
</html>
