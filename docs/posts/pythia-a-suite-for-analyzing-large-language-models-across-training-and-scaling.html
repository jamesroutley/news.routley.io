<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2304.01373">Original</a>
    <h1>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Biderman%2C+S">Stella Biderman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schoelkopf%2C+H">Hailey Schoelkopf</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anthony%2C+Q">Quentin Anthony</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bradley%2C+H">Herbie Bradley</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=O%27Brien%2C+K">Kyle O&#39;Brien</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hallahan%2C+E">Eric Hallahan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khan%2C+M+A">Mohammad Aflah Khan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Purohit%2C+S">Shivanshu Purohit</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prashanth%2C+U+S">USVSN Sai Prashanth</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Raff%2C+E">Edward Raff</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Skowron%2C+A">Aviya Skowron</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sutawika%2C+L">Lintang Sutawika</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=van+der+Wal%2C+O">Oskar van der Wal</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2304.01373">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  How do large language models (LLMs) develop and evolve over the course of
training? How do these patterns change as models scale? To answer these
questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on
public data seen in the exact same order and ranging in size from 70M to 12B
parameters. We provide public access to 154 checkpoints for each one of the 16
models, alongside tools to download and reconstruct their exact training
dataloaders for further study. We intend \textit{Pythia} to facilitate research
in many areas, and we present several case studies including novel results in
memorization, term frequency effects on few-shot performance, and reducing
gender bias. We demonstrate that this highly controlled setup can be used to
yield novel insights toward LLMs and their training dynamics. Trained models,
analysis code, training code, and training data can be found at
<a href="https://github.com/EleutherAI/pythia" rel="external noopener nofollow">this https URL</a>.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Stella Biderman [<a href="https://arxiv.org/show-email/3e3ea3f1/2304.01373">view email</a>]
      </p></div></div>
  </body>
</html>
