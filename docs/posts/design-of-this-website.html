<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gwern.net/Design">Original</a>
    <h1>Design of This Website</h1>
    
    <div id="readability-page-1" class="page"><div id="markdownBody">
<div>
<blockquote>
<p>Gwern.net is implemented as a static website compiled via Hakyll from Pandoc Markdown and hosted on a dedicated server (due to expensive cloud bandwidth).</p>
<p>It stands out from your standard Markdown static website by aiming at good typography, fast performance, and advanced hypertext browsing features (at the cost of
great implementation complexity); the <a href="#principles">4 design principles</a> are: aesthetically-pleasing minimalism, accessibility/​​progressive-enhancement,
speed, and a ‘structural reading’ approach to hypertext use.</p>
<p>Unusual features include the monochrome esthetics, <a href="https://www.gwern.net/Sidenotes" id="gwern-sidenotes">sidenotes</a> instead of footnotes, efficient
dropcaps/​​smallcaps, collapsible sections, automatic inflation-adjusted currency, Wikipedia-style link icons &amp; infoboxes, custom <a href="https://en.wikipedia.org/wiki/Syntax_highlighting" data-link-icon="wikipedia" data-link-icon-type="svg">syntax highlighting</a>⁠,
extensive local archives to fight linkrot, and an ecosystem of “popup”/​​“popin” annotations &amp; previews of links for frictionless browsing—the net effect of
hierarchical structures with collapsing and instant popup access to excerpts enables iceberg-like pages where most information is hidden but the reader can easily
drill down as deep as they wish. (For a demo, see <a href="https://www.gwern.net/Lorem" id="gwern-lorem">Lorem Ipsum</a>⁠.)</p>
<p>Also discussed are the <a href="#abandoned">many failed experiments</a> or design changes made along the way.</p>
</blockquote>
</div>
<section id="benefit">


<p>The <a href="https://www.gwern.net/docs/www/thecodelesscode.com/caec649163afc08a327ff81f6c690dbef0971716.html" rel="archived alternate nofollow" data-url-original="http://thecodelesscode.com/case/96" title="The Codeless Code: Case 96: &#39;Stateless&#39; (Original URL: http://thecodelesscode.com/case/96 )">sorrow</a> of web design &amp;
typography is that it all can matter just a little how you present your pages. <span>A page can be terribly designed and render as typewriter text in 80-column
<span>ASCII</span> monospace, and readers will still read it, even if they complain about it.</span> <span>And the most tastefully-designed page,
with true smallcaps and correct use of em-dashes vs en-dashes vs hyphens vs minuses and all, which loads in a fraction of a second and is <span>SEO</span> optimized, is of little avail if the page has nothing worth reading; no amount of typography can rescue a page of dreck.</span> Perhaps 1% of
readers could even name any of these details, much less recognize them. If we added up all the small touches, they surely make a difference to the reader’s happiness,
 but it would have to be a small one—say, 5%.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> It’s hardly worth it for writing just a
few things.</p>
<p>But the joy of web design &amp; typography is that just its presentation can matter a little to all your pages. Writing is hard work, and any new piece of writing will
generally add to the pile of existing ones, rather than multiplying it all; it’s an enormous amount of work to go through all one’s existing writings and improve them
somehow, so it usually doesn’t happen. Design improvements, on the other hand, benefit one’s entire website &amp; all future readers, and so at a certain scale, can be quite
useful. I feel I’ve reached the point where it’s worth sweating the small stuff, typographically.</p>
</section>
<section id="principles">

<p>There are 4 design principles:</p>
<ol type="1">
<li>
<p>Aesthetically-pleasing <strong>Minimalism</strong></p>
<p>The design esthetic is minimalist. I believe that <a href="https://en.wikipedia.org/wiki/Minimalism" data-link-icon="wikipedia" data-link-icon-type="svg">minimalism</a> helps one focus on the content.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> Anything besides the content is <a href="https://www.jwz.org/gruntle/design.html">distraction and not design</a>⁠. ‘Attention!’, as
<a href="https://en.wikipedia.org/wiki/Ikkyu" data-link-icon="wikipedia" data-link-icon-type="svg">Ikkyu</a> would say.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> What does it take to present complex, highly-referenced, link-intensive, long-form text
online as effectively as possible, while conserving the reader’s time &amp; attention?</p>
<p>The palette is deliberately kept to grayscale as an experiment in consistency and whether this constraint permits a readable aesthetically-pleasing website.
Various classic typographical tools, like <a href="https://en.wikipedia.org/wiki/Initial" data-link-icon="wikipedia" data-link-icon-type="svg">drop caps</a> and <a href="https://en.wikipedia.org/wiki/Small_caps" data-link-icon="wikipedia" data-link-icon-type="svg">small caps</a> are used for emphasis.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</li>
<li>
<p>Accessibility &amp; <strong><a href="https://en.wikipedia.org/wiki/Progressive_Enhancement" data-link-icon="wikipedia" data-link-icon-type="svg">Progressive Enhancement</a></strong></p>
<p>Semantic markup is used where Markdown permits. JavaScript is <em>not</em> required for the core reading experience, only for optional features: comments,
table-sorting, <a href="https://www.gwern.net/Sidenotes" id="sidenotes-2">sidenotes</a>⁠, and so on. Pages can even be read without much problem in a smartphone or a
text browser like <code>elinks</code>.</p>
</li>
<li>
<p><strong>Speed &amp; Efficiency</strong></p>
<p>On an increasingly-bloated Internet, a website which is anywhere remotely as fast as it could be is a breath of fresh air. Readers deserve better. Gwern.net uses
many tricks to offer nice features like sidenotes or <span>L<span>a</span>T<span>e</span>X</span> math at minimal cost.</p>
</li>
<li>
<p><span id="structural-reading"><strong>Structural Reading</strong></span></p>
<p>How should we present texts online? A web page, unlike many mediums such as print magazines, lets us provide an unlimited amount of text. We need not limit
ourselves to overly concise constructions, which countenance contemplation but not conviction.</p>
<p>The problem then becomes taming complexity and length, lest we hang ourselves with our own rope. Some readers want to read every last word about a particular
topic, while most readers want the summary or are skimming through on their way to something else. A tree structure is helpful in organizing the concepts, but
doesn’t solve the presentation problem: a book or article may be hierarchically organized, but it still must present every last leaf node at 100% size. Tricks like
footnotes or appendices only go so far—having thousands of endnotes or 20 appendices to tame the size of the ‘main text’ is unsatisfactory as while any specific
reader is unlikely to want to read any specific appendix, they will certainly want to read <em>an</em> appendix &amp; possibly many. The classic hypertext paradigm of
simply having a rat’s-nest of links to hundreds of tiny pages to avoid any page being too big also breaks down, because how granular does one want to go? Should
every section be a separate page? (Anyone who attempted to read a <a href="https://en.wikipedia.org/wiki/Info_%28Unix%29" data-link-icon="wikipedia" data-link-icon-type="svg"><span><span>GNU</span> Info</span></a> manual knows how tedious that can be.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>) What about every reference in the bibliography, should there be 100 different pages for
100 different references?</p>
<p>A web page, however, can be dynamic. The solution to the length problem is to progressively expose more beyond the default as the user requests it, and make
requesting as easy as possible. For lack of a well-known term and by analogy to <a href="https://en.wikipedia.org/wiki/Code_folding" data-link-icon="wikipedia" data-link-icon-type="svg">code folding</a> in <a href="https://en.wikipedia.org/wiki/Structure_editor" data-link-icon="wikipedia" data-link-icon-type="svg">structural editors</a>⁠/​<a href="https://en.wikipedia.org/wiki/Outliner" data-link-icon="wikipedia" data-link-icon-type="svg">outliners</a>⁠, I call this <strong>structural reading</strong>: the
hierarchy is made visible &amp; malleable to allow reading at multiple levels of the structure.</p>
<p>A Gwern.net page can be read at multiple structural levels, high to low: title, metadata block, abstracts, section headers, margin notes, emphasized keywords in
list items, footnotes/​sidenotes, collapsed sections or paragraphs, internal cross-referencing links to other sections (such as appendices) which popup for
immediate reading, and fulltext links or internal links to other pages (also popping up).</p>
<p>So the reader can read (in increasing depth) the title/​metadata, or the page abstract, or skim the headers/​Table of Contents, then skim margin notes+item
summaries, then read the body text, then click to uncollapse regions to read in-depth sections too, and then if they still want more, they can mouse over references
to pull up the abstracts or excerpts, and then they can go even deeper by clicking the fulltext link to read the original. Thus, a page may look short, and the
reader can understand &amp; navigate it easily, but like an iceberg, those readers who want to know more about any specific point will find much more under the
surface.</p>
</li>
</ol>
<p>Miscellaneous principles: all visual differences should be semantic differences; every UI element that can react should visually change on hover, and have
tooltips/​summaries; hierarchies &amp; progressions should come in cycles of 3 (eg. bold &gt; smallcaps &gt; italics), otherwise, all numbers should be <a href="https://en.wikipedia.org/wiki/Zero_one_infinity_rule" data-link-icon="wikipedia" data-link-icon-type="svg">0, 1, or ∞</a>⁠; function
&gt; form; more &gt; less; self-contained &gt; fragmented; convention (linters/​checkers) &gt; constraints; hypertext is a great idea, we should try that!; local &gt;
remote—every link dies someday (archives are expensive short-term but cheap long-term); reader &gt; author; give the reader agency; speed is the second-most important
feature after correctness; always bet on text; <em>earn</em> your ornaments (if you go overboard on minimalism, you may barely be mediocre); “users won’t tell you when
it’s broken”; UI consistency is underrated (when in doubt, copy Wikipedia); if you find yourself doing something 3 times, fix it.</p>
</section>
<section id="features">

<p>Notable features (compared to standard Markdown static site):</p>
<ul>
<li>
<p>Link popup annotations (<a href="https://www.gwern.net/images/design/2021-03-30-sidenotes-gwern.net-popins.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="1368" data-image-width="685">‘popin’</a> on small screens or mobile):</p>
<p>Annotations are hand-written, and automatically extracted from <a href="https://en.wikipedia.org/wiki/ArXiv" data-link-icon="wikipedia" data-link-icon-type="svg" title="ArXiv">Arxiv</a>⁠/​BioRxiv/​MedRxiv/​Crossref or written by hand (formatting is kept consistent by an extensive series of rewrite rules &amp; checks); popups can be
recursive, and can be manipulated in many ways—moved, fullscreened, ‘stickied’ (<a href="https://en.wikipedia.org/wiki/Anchoring" data-link-icon="wikipedia" data-link-icon-type="svg" title="Anchoring">anchored</a> in place), etc.
<a href="https://www.gwern.net/images/design/2021-04-01-gwern.net-annotations-popups-recursivewikipediapopups.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="1398" data-image-width="1600">Wikipedia pages</a> <span>are specially-supported, enabling them to be recursively navigated as well. Local
Gwern.net pages &amp; whitelisted domains can be popped up and viewed in full; <span>PDFs</span> can be read inside a <span>PDF</span> viewer; and supported source code formats will pop up a syntax-highlighted version if available (</span><a href="https://www.gwern.net/static/build/LinkMetadata.hs" data-link-icon="code" data-link-icon-type="svg">eg</a>).</p>
</li>
<li>
<p><a href="https://www.gwern.net/Sidenotes" id="sidenotes-3">sidenotes</a> using both margins, fallback to floating footnotes</p>
<ul>
<li>
<a href="https://www.gwern.net/docs/www/edwardtufte.github.io/e43d8239ed3fa1d513e2b4d071b6a7c0c8a98bff.html#sidenotes" data-link-icon="ET" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://edwardtufte.github.io/tufte-css/#sidenotes" title="&#39;Tufte-CSS: Sidenotes: Footnotes and Marginal Notes&#39;, Liepmann 2022 (Original URL: https://edwardtufte.github.io/tufte-css/#sidenotes )">margin notes</a> (as
inline or sidenotes)
 </li>
</ul>
</li>
<li>
<p>source code syntax highlighting (<a href="#syntax-highlighting-algol">custom monochrome theme</a>)</p>
</li>
<li>
<p>code folding (collapsible sections/​code blocks/​tables)</p>
</li>
<li>
<p>JS-free <span>L<span>a</span>T<span>e</span>X</span> <span>math rendering (but where possible, <span>HTML</span>+Unicode is
used instead, as it is much more efficient &amp; natural-looking)</span></p>
</li>
<li>
<p><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-color-scheme" data-link-icon="FF" data-link-icon-type="text,sans">dark mode</a> (with a
<a href="https://www.gwern.net/static/js/darkmode.js" id="achmiz-2020" data-link-icon="code" data-link-icon-type="svg" title="&#39;&lt;code&gt;darkmode.js&lt;/code&gt;&#39;, Achmiz 2020">theme switcher</a>)</p>
</li>
<li>
<p>click-to-zoom images &amp; slideshows; full-width tables/​images</p>
</li>
<li>
<p>Disqus comments</p>
</li>
<li>
<p>sortable tables; tables of various sizes</p>
</li>
<li>
<p>automatically inflation-adjust dollar amounts, exchange-rate <a href="https://www.gwern.net/docs/bitcoin/2009-nakamoto.pdf" id="nakamoto-2009" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Bitcoin: A Peer-to-Peer Electronic Cash System&#39;, Nakamoto 2009">Bitcoin</a> amounts</p>
</li>
<li>
<p>link icons for filetype/​domain/​topic</p>
</li>
<li>
<p><a href="https://casual-effects.com/markdeep/features.md.html#basicformatting/admonitions" title="&#39;Markdeep Features: Admonitions&#39;, McGuire 2022">“admonitions”</a> infoboxes (Wikipedia-like by way of Markdeep)</p>
</li>
<li>
<p>lightweight drop caps</p>
</li>
<li>
<p>epigraphs</p>
</li>
<li>
<p>automatic smallcaps typesetting</p>
</li>
<li>
<p>multi-column lists</p>
</li>
<li>
<p>interwiki link syntax</p>
</li>
<li>
<p>automatic link-ification of keywords (<a href="https://www.gwern.net/static/build/LinkAuto.hs" id="linkauto-hs" data-link-icon="code" data-link-icon-type="svg">LinkAuto.hs</a>)</p>
</li>
</ul>
<p><span>Much of Gwern.net design and <span>JS/​CSS</span> was developed by</span> <a href="https://wiki.obormot.net/">Said
Achmiz</a>⁠, 2017–2020. Some inspiration has come from <a href="https://www.gwern.net/docs/www/edwardtufte.github.io/e43d8239ed3fa1d513e2b4d071b6a7c0c8a98bff.html" data-link-icon="ET" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://edwardtufte.github.io/tufte-css/" title="(Original URL: https://edwardtufte.github.io/tufte-css/ )"><span>Tufte <span>CSS</span></span></a> &amp; Matthew Butterick’s <a href="https://practicaltypography.com/"><em>Practical Typography</em></a>⁠.</p>
</section>
<section id="abandoned">

<p>Often the most interesting part of any design are the parts that are invisible—what was tried but did not work. Some post-mortems of things I tried but abandoned
(in chronological order):</p>
<ul>
<li>
<p><a href="https://github.com/jgm/gitit" data-link-icon="github" data-link-icon-type="svg"><strong>Gitit</strong></a> wiki: I preferred to edit files in <a href="https://en.wikipedia.org/wiki/Emacs" data-link-icon="wikipedia" data-link-icon-type="svg" title="Emacs">Emacs</a>⁠/​<span>Bash rather than a <span>GUI</span>/​browser-based wiki.</span></p>
<p>A Pandoc-based wiki using <a href="https://en.wikipedia.org/wiki/Darcs" data-link-icon="wikipedia" data-link-icon-type="svg" title="Darcs">Darcs</a> as a history mechanism, serving mostly as a demo; the requirement that ‘one page edit = one
<span>Darcs</span> revision’ <span>quickly became stifling, and I began editing my Markdown files directly and recording patches at the end
of each day, and syncing the <span>HTML</span> cache with my host (at the time, a personal directory on</span> <code>code.haskell.org</code>).
Eventually I got tired of that and figured that since I wasn’t using the wiki, but only the static compiled pages, I might as well switch to Hakyll and a normal
static website approach.</p>
</li>
<li>
<p><a href="https://www.gwern.net/docs/www/christophercliff.com/b5be664197dfdd7f1f3222c23360dff6776df2e9.html" rel="archived alternate nofollow" data-url-original="https://christophercliff.com/sausage/examples/couchdb.html" title="(Original URL: https://christophercliff.com/sausage/examples/couchdb.html )"><strong>jQuery
sausages</strong></a>: unhelpful UI visualization of section lengths.</p>
<p>A UI experiment, ‘sausages’ add a second scroll bar where vertical lozenges correspond to each top-level section of the page; it indicates to the reader how long
each section is and where they are. (They look like a long link of pale white sausages.) I thought it might assist the reader in positioning themselves, like the
popular ‘floating highlighted Table of Contents’ UI element, but without text labels, the sausages were meaningless. After a jQuery upgrade broke it, I didn’t bother
fixing it.</p>
</li>
<li>
<p><a href="https://www.gwern.net/AB-testing#beeline-reader-text-highlighting" id="gwern-ab-testing-beeline-reader-text-highlighting"><strong>Beeline
Reader</strong></a>: a ‘reading aid’ which just annoyed readers.</p>
<p><span><span>BLR</span> tries to aid reading by coloring the beginnings &amp; endings of lines to indicate the continuation and make it easier
for the reader’s eyes to saccade to the correct next line without distraction (apparently dyslexic readers in particular have issue correctly fixating on the
continuation of a line). The A/​B test indicated no improvements in the time-on-page metric, and I received many complaints about it; I was not too happy with the
browser performance or the appearance of it, either.</span></p>
<p><span>I’m sympathetic to the goal and think syntax highlighting aids are underused, but <span>BLR</span> was a bit half-baked and not worth
the cost compared more straightforward interventions like reducing paragraph lengths or more rigorous use of</span> <a href="#structural-reading">‘structural
reading’</a> formatting. (We may be able to do typography differently in the future with new technology, like VR/​AR headsets which come with <a href="https://en.wikipedia.org/wiki/Eye_tracking" data-link-icon="wikipedia" data-link-icon-type="svg">eye tracking</a> technology
intended for <a href="https://en.wikipedia.org/wiki/Foveated_rendering" data-link-icon="wikipedia" data-link-icon-type="svg">foveated rendering</a>—forget simple tricks like emphasizing the beginning of the next line as the reader reaches the end of the current line, do we need
‘lines’ at all if we can do things like just-in-time display the next piece of text in-place to create an ‘infinite line’?)</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Google_Programmable_Search_Engine" data-link-icon="wikipedia" data-link-icon-type="svg"><strong><span>Google <span>CSE</span></span></strong></a>: site search feature which too few people used.</p>
<p>A ‘custom search engine’<span>, a <span>CSE</span> is a souped-up</span> <code>site:gwern.net/</code> Google search query; I wrote one
covering Gwern.net and some of my accounts on other websites, and added it to the sidebar. <a href="https://www.gwern.net/AB-testing#cse" id="gwern-ab-testing-cse">Checking the analytics</a><span>, perhaps 1 in 227 page-views used the <span>CSE</span>, and a decent number of them used it only
by accident (eg. searching</span> “e”); an A/​B testing for a feature used so little would be powerless, and so I removed it rather than try to formally test
it.</p>
</li>
<li>
<p><strong><span>Tufte-<span>CSS</span> Sidenotes</span></strong>: fundamentally broken, and superseded.</p>
<p>An early admirer of <a href="https://en.wikipedia.org/wiki/Edward_Tufte" data-link-icon="wikipedia" data-link-icon-type="svg" title="Edward Tufte">Tufte</a>-<span><span>CSS</span> for its sidenotes, I gave a Pandoc plugin a try
only to discover a terrible drawback: the <span>CSS</span> didn’t support block elements &amp; so the plugin simply deleted them. This bug
apparently can be fixed, but the density of footnotes led to using</span> <code>sidenotes.js</code> instead.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/DjVu" data-link-icon="wikipedia" data-link-icon-type="svg"><strong>DjVu</strong></a>
document format use: DjVu is a space-efficient document format with the fatal drawback that Google ignores it, and “if it’s not in Google, it doesn’t exist.”</p>
<p><span>DjVu is a document format superior to <span>PDFs</span>, especially standard <span>PDFs</span>: I discovered
that space savings of 5× or more were entirely possible, so I used it for most of my book scans. It worked fine in my document viewers,</span> <a href="https://en.wikipedia.org/wiki/Internet_Archive" data-link-icon="wikipedia" data-link-icon-type="svg" title="Internet Archive">Internet Archive</a> <span>&amp; Libgen preferred them, and so why not? Until one day I wondered if anyone was linking them and tried
searching in Google Scholar for some. Not a single hit! (As it happens, GS seems to specifically filter out books.) Perplexed, I tried Google—also nothing. Huh‽ My
scans have been visible for years, DjVu dates to the 1990s and was widely used (if not remotely as popular as <span>PDF</span>), and G/​GS
picks up all my <span>PDFs</span> which are hosted identically. What about</span> <code>filetype:djvu</code>? I discovered to my horror that
on the entire Internet, Google indexed about 50 DjVu files. Total. While apparently at one time Google did index DjVu files, that time must be long past.</p>
<p><span>Loathe to take the space hit, which would noticeably increase my Amazon <span>AWS</span> S3 hosting costs, I looked into <span>PDFs</span> more carefully. I discovered <span>PDF</span> technology had advanced considerably over the default <span>PDFs</span> that</span> <a href="http://gscan2pdf.sourceforge.net/" id="ratcliffe-2019" title="&#39;gscan2pdf: A GUI to produce PDFs or DjVus from scanned documents&#39;, Ratcliffe 2019">gscan2pdf</a> generates, and with <a href="https://en.wikipedia.org/wiki/JBIG2" data-link-icon="wikipedia" data-link-icon-type="svg"><span>JBIG2</span></a> <span>compression, they were
closer to DjVu in size; I could conveniently generate such <span>PDFs</span> using</span> <a href="https://github.com/ocrmypdf/OCRmyPDF" data-link-icon="github" data-link-icon-type="svg">ocrmypdf</a>⁠.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> This let me
convert over at moderate cost and now my documents do show up in Google.</p>
</li>
<li>
<p><strong>Darcs/​<a href="https://en.wikipedia.org/wiki/Github" data-link-icon="wikipedia" data-link-icon-type="svg" title="Github">Github</a> repo</strong><span>: no useful contributions or patches submitted, added considerable process overhead, and I
accidentally broke the repo by checking in too-large <span>PDFs</span> from a failed post-DjVu optimization pass (I misread the result as
being smaller, when it was much larger).</span></p>
</li>
<li>
<p><strong><span>Spaces in <span>URLs</span></span></strong>: an OK idea but users are why we can’t have nice things.</p>
<p>Gitit assumed ‘<span>titles = filenames = <span>URLs</span></span>’<span>, which simplified things and I liked spaced-separated filenames;
I carried this over to Hakyll, but gradually, by monitoring analytics realized this was a terrible mistake—as straightforward as <span>URL</span>-encoding spaces as</span> <code>%20</code> may seem to be, <em>no one</em> <span>can do it properly. I didn’t want to fix it because by
the time I realized how bad the problem was, it would have required breaking, or later on, redirecting, hundreds of <span>URLs</span> and
updating all my pages. The final straw was when</span> <a href="https://thebrowser.com/" data-link-icon="TB" data-link-icon-type="text">The
Browser</a> linked a page incorrectly, sending ~1500 people to the 404 page. I finally gave in and replaced spaces with hyphens. (Underscores are the other main
option but because of Markdown, I worry that trades one error for another.) I suspect I should have also lower-cased all links while I was at it, but thus far it has
not proven <em>too</em> <span>hard to fix case errors &amp; lower-case <span>URLs</span> are ugly.</span></p>
<p>In retrospect, <a href="https://qntm.org/urls" title="On short URLs">Sam Hughes was right</a><span>: I should have made <span>URLs</span> as simple as possible (and then a bit simpler): a single word, lowercase alphanum, with no hyphens or underscores or spaces or
punctuation of any sort. I am, however, locked in to longer hyphen-separated mixed-case <span>URLs</span> now.</span></p>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/AdSense" data-link-icon="wikipedia" data-link-icon-type="svg">AdSense</a></strong> banner ads (and ads in general): reader-hostile and probably a net financial loss.</p>
<p>I hated running banner ads, but before my <a href="https://en.wikipedia.org/wiki/Patreon" data-link-icon="wikipedia" data-link-icon-type="svg" title="Patreon">Patreon</a> began working, it seemed the lesser of two evils. As my finances became less
parlous, I became curious as to <em>how</em> much lesser—but I could find no Internet research whatsoever measuring something as basic as the traffic loss due to
advertising! So I decided to <a href="https://www.gwern.net/Ads" id="gwern-ads">run an A /​ ​ ​B test myself</a>⁠, with a proper sample size and cost-benefit
analysis; the harm turned out to be so large that the analysis was unnecessary, and I removed AdSense permanently the first time I saw the results. Given the
measured traffic reduction, I was probably losing several times more in potential donations than I ever earned from the ads. (Amazon affiliate links appear to not
trigger this reaction, and so I’ve left them alone.)</p>
</li>
<li>
<p>Bitcoin/​<a href="https://en.wikipedia.org/wiki/PayPal" data-link-icon="wikipedia" data-link-icon-type="svg" title="PayPal">PayPal</a>⁠/​Gittip/​Flattr <strong>donation links</strong>: never worked well compared to <span>Patreon</span>.</p>
<p>These methods were either single-shot or never hit a critical mass. One-off donations failed because people wouldn’t make a habit if it was manual, and it was too
inconvenient. Gittip/​Flattr were similar to <span>Patreon</span> in bundling donators, and making it a regular thing, but never hit an
adequate scale.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Google_Fonts" data-link-icon="wikipedia" data-link-icon-type="svg"><strong>Google
Fonts</strong></a> web fonts: slow and buggy.</p>
<p>Google Fonts turned out to introduce noticeable latency in page rendering; further, its selection of fonts is limited, and the fonts outdated or incomplete. We
got both faster and nicer-looking pages by taking the <a href="https://github.com/adobe-fonts/source-serif" data-link-icon="github" data-link-icon-type="svg">master
<span>Github</span> versions</a> of Adobe Source Serif/​Sans Pro (the Google Fonts version was both outdated &amp; incomplete then) and
subsetting them for Gwern.net specifically.</p>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/MathJax" data-link-icon="wikipedia" data-link-icon-type="svg">MathJax</a>
JS</strong>: switched to static rendering during compilation for speed.</p>
<p>For math rendering, MathJax and <a href="https://en.wikipedia.org/wiki/KaTeX" data-link-icon="wikipedia" data-link-icon-type="svg">KaTeX</a> are reasonable options (inasmuch as <a href="https://en.wikipedia.org/wiki/MathML" data-link-icon="wikipedia" data-link-icon-type="svg">MathML</a> browser adoption is dead in the water). MathJax rendering is extremely slow on some pages: up to 6 seconds to load and render
all the math. Not a great reading experience. When I learned that it was possible to preprocess MathJax-using pages, I dropped MathJax JS use the same day.</p>
 </li>
<li>
<p><strong><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/q" data-link-icon="FF" data-link-icon-type="text,sans"><code>&lt;q&gt;</code></a> quote
tags</strong> for English <a href="https://en.wikipedia.org/wiki/Syntax_highlighting" data-link-icon="wikipedia" data-link-icon-type="svg">syntax highlighting</a>: divisive and a maintenance burden.</p>
<p>I like the idea of treating English as a little (not a lot!) more like a formal language, such as a programming language, as it comes with benefits like syntax
highlighting. In a program, the reader gets guidance from syntax highlighting indicating logical nesting and structure of the ‘argument’; in a natural language
document, it’s one damn letter after another, spiced up with the occasional punctuation mark or indentation. (If <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" title="Lisp (programming language)">Lisp</a> looks like “oatmeal with fingernail clippings mixed in” due to the lack of “<a href="https://en.wikipedia.org/wiki/Syntactic_sugar" data-link-icon="wikipedia" data-link-icon-type="svg">syntactic sugar</a>”, then
English must be plain oatmeal!) One of the most basic kinds of syntax highlighting is simply highlighting strings and other literals vs code: I learned early on that
syntax highlighting was worth it just to make sure you hadn’t forgotten a quote or parenthesis somewhere! The same is true of regular writing: if you are extensively
quoting or naming things, the reader can get a bit lost in the thickets of curly quotes and be unsure who said what.</p>
<p><span>I discovered an obscure <span>HTML</span> tag enabled by an obscurer Pandoc setting: the quote tag</span>
<code>&lt;q&gt;</code><span>, which replaces quote characters and is rendered by the browser as quotes (usually). Quote tags are parsed explicitly, rather than just
being opaque natural language text blobs, and so they, at least, can be manipulated easily by <span>JS/​CSS</span> and syntax-highlighted.
Anything inside a pair of quotes would be tinted a gray to visually set it off similarly to the blockquotes. I was proud of this tweak, which I’ve seen nowhere
else.</span></p>
<p><span>The problems with it was that not everyone was a fan (to say the least); it was not always correct (there are many double-quotes which are not literal
quotes of anything, like rhetorical questions); and it interacted badly with everything else. There were puzzling drawbacks: eg. web browsers delete them from
copy-paste, so we had to use JS to convert them to normal quotes. Even when it was worked out, all the <span>HTML/​CSS/​JS</span> had to be
constantly rejiggered to deal with interactions with them, browser updates would silently break what was working, and Said Achmiz hated the look. I tried manually
annotating quotes to ensure they were all correct and not used in dangerous ways, but even with interactive regexp search-and-replace to assist, the manual toil of
constantly marking up quotes was a major obstacle to writing. So I gave in. It was not meant to be.</span></p>
</li>
<li>
<p><a href="https://www.gwern.net/Red" id="gwern-red">Typographic <strong>rubrication</strong></a>: a solution in search of a problem.</p>
<p>Red emphasis is a visual strategy that works wonderfully well for many styles, but not Gwern.net that I could find. Using it on the regular website resulted in
<em>too</em> much emphasis and the lack of color anywhere else made the design inconsistent; we tried using it in dark mode to add some color &amp; preserve night vision
by making headers/​links/​drop-caps red, but it looked like, as one reader put it, “a vampire fansite”. It is a good idea, but we just haven’t found a use for it.
(Perhaps if I ever make another website, it will be designed around rubrication.)</p>
<ul>
<li>Another solution in search of a problem is <a href="https://www.gwern.net/Subscripts" id="gwern-subscripts" title="A typographic proposal: replace cumbersome inline citation formats like &#39;Foo et al. (2010&#39;) with subscripted dates/sources. Intuitive, easily implemented, consistent, and compact.">
<span>Subscripted Citations</span></a>⁠.
</li>
</ul>
</li>
<li>
<p><strong><code>wikipedia-popups.js</code></strong><span>: a JS library written to imitate Wikipedia popups, which used the <span>WP
API</span> to fetch article summaries; obsoleted by the faster &amp; more general local static link annotations.</span></p>
<p>I disliked the delay and as I thought about it, it occurred to me that it would be nice to have popups for other websites, like <span>Arxiv</span>/​BioRxiv links—but they didn’t <em>have</em> <span><span>APIs</span> which could be queried. If I fixed the
first problem by fetching WP article summaries while compiling articles and inlining them into the page, then there was no reason to include summaries for only
Wikipedia links, I could get summaries from any tool or service or <span>API</span>, and I could of course write my own! But that required an
almost complete rewrite to turn it into</span> <code>popups.js</code>.</p>
</li>
<li>
<p><strong>Link screenshot previews</strong>: automatic screenshots too low-quality, and unpopular.</p>
<p>To compensate for the lack of summaries for almost all links (even after I wrote the code to scrape various sites), I tried a feature I had seen elsewhere of
‘link previews’<span>: small thumbnail sized screenshots of a web page or <span>PDF</span>, loading using JS when the mouse hovered over a
link. (They were much too large, ~50kb, to inline statically like the link annotations.) They gave some indication of what the target content was, and could be
generated automatically using a headless browser. I used Chromium’s built-in screenshot mode for web pages, and took the first page of <span>PDFs</span>.</span></p>
<p><span>The <span>PDFs</span> worked fine, but the webpages often broke: thanks to ads, newsletters, and the <span>GDPR</span>, countless webpages will pop up some sort of giant modal</span> <a href="https://en.wikipedia.org/wiki/Blocking_%28statistics%29" data-link-icon="wikipedia" data-link-icon-type="svg" title="Blocking (statistics)">blocking</a> any
view of the page content, defeating the point. (I have extensions installed like <a href="https://git.sr.ht/~achmizs/AlwaysKillSticky.git">AlwaysKillSticky</a> to
block that sort of spam, but Chrome screenshot <em>cannot</em> <span>use any extensions or customized settings, and the Chrome devs refuse to improve it.) Even when
it did work and produced a reasonable screenshot, many readers disliked it anyway and complained. I wasn’t too happy either about having 10,000 tiny <span>PNGs</span> hanging around. So as I expanded link annotations steadily, I finally pulled the plug on the link previews. Too much for too
little.</span></p>
<ul>
<li>
<span>Link Archiving</span>: my link archiving improved on the link screenshots in several ways. First, <a href="https://github.com/gildas-lormeau/SingleFile/" data-link-icon="github" data-link-icon-type="svg" title="&#39;SingleFile&#39;, Lormeau 2022">SingleFile</a> saves pages inside a normal Chromium browsing instance, which <em>does</em> support extensions and user settings.
Killing stickies alone eliminates half the bad archives, ad block extensions eliminate a chunk more, and NoScript blacklists specific domains. (I initially used
NoScript on a whitelist basis, but disabling JS breaks too many websites these days.) Finally, I decided to manually review every snapshot before it went live to
catch bad examples and either fix them by hand or add them to the blacklist.
</li>
</ul>
</li>
<li>
<p><span id="auto-dark-mode"><strong>Auto-<a href="https://en.wikipedia.org/wiki/Dark_mode" data-link-icon="wikipedia" data-link-icon-type="svg">dark mode</a></strong></span>: a good idea but “users are why we can’t have nice things”.</p>
<p>OSes/​browsers have defined a ‘global dark mode’ toggle the user can set if they want dark mode everywhere, and this is available to a web page; if you are
implementing a dark mode for your website, it then seems natural to just make it a feature and turn on iff the toggle is on. There is no need for complicated
UI-cluttering widgets with complicated implementations. And yet—if you <em>do</em> do that, users will regularly complain about the website acting bizarre or being
dark in the daytime, having apparently forgotten that they enabled it (or never understood what that setting meant).</p>
<p>A widget is necessary to give readers control, although even there it can be screwed up: many websites settle for a simple negation switch of the global toggle,
but if you do that, someone who sets dark mode at day will be exposed to blinding white at night… Our widget works better than that. Mostly.</p>
</li>
<li>
<p><strong>Multi-column footnotes</strong>: mysteriously buggy and yielding overlaps.</p>
<p>Since most footnotes are short, and no one reads the endnote section, I thought rendering them as two columns, as many papers do, would be more space-efficient
and tidy. It was a good idea, but it didn’t work.</p>
</li>
<li>
<p><a href="https://github.com/mnater/Hyphenopoly" data-link-icon="github" data-link-icon-type="svg"><strong>Hyphenopoly</strong></a><span>: it turned out to be more
efficient (and not much harder to implement) to hyphenate the <span>HTML</span> during compilation than to run JS clientside.</span></p>
<p>To work around Google Chrome’s 2-decade-long refusal to ship hyphenation dictionaries on desktop and enable <a href="https://en.wikipedia.org/wiki/Typographic_alignment#Justified" data-link-icon="wikipedia" data-link-icon-type="svg">justified
text</a> (and incidentally use the better <a href="https://en.wikipedia.org/wiki/TeX#Hyphenation_and_justification" data-link-icon="wikipedia" data-link-icon-type="svg"><span>T<sub>e</sub>X</span></a> <a href="https://en.wikipedia.org/wiki/Hyphenation_algorithm" data-link-icon="wikipedia" data-link-icon-type="svg">hyphenation
algorithm</a>), the JS library Hyphenopoly will download the <span>T<sub>e</sub>X</span> English dictionary and typeset a webpage itself. While
the performance cost was surprisingly minimal, it was there, and it caused problems with obscurer browsers like Internet Explorer.</p>
<p>So we scrapped Hyphenopoly, and I later implemented a Hakyll function using <a href="https://hackage.haskell.org/package/hyphenation" id="kmett-2012" data-link-icon="𝛌" data-link-icon-type="text" title="&#39;hyphenation&#39;: Configurable Knuth-Liang hyphenation; uses the UTF8 encoded hyphenation patterns provided by hyph-utf8 from https://ctan.org/tex-archive/language/hyph-utf8">
a <span>Haskell</span> version</a> of the <span>T<sub>e</sub>X</span> hyphenation algorithm &amp; dictionary to insert at compile-time a ‘<a href="https://en.wikipedia.org/wiki/Soft_hyphen" data-link-icon="wikipedia" data-link-icon-type="svg">soft hyphen</a>’ everywhere a
browser could usefully break a word, which enables Chrome to hyphenate correctly, at the moderate cost of inlining them and a few edge cases.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Desktop Chrome <em>finally</em> shipped hyphen support in early 2020, and I removed the soft-hyphen hyphenation pass in April 2021 when <a href="https://caniuse.com/?search=hyphenate">CanIUse</a> indicated &gt;96% global support.</p>
<ul>
<li>
<p><span><a href="https://www.gwern.net/docs/design/1981-knuth.pdf" id="knuth-plass-1981" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Breaking paragraphs into lines&#39;, Knuth &amp; Plass 1981">Knuth-Plass</a> <a href="https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap" data-link-icon="wikipedia" data-link-icon-type="svg" title="Line wrap and word wrap">Linebreaking</a></span>: not to be
confused with Knuth-Liang <em>hyphenation</em> discussed before, which simply optimizes the set of legal hyphens, Knuth-Plass linebreaking tries to optimize the
actual chosen linebreaks.</p>
<p><a href="https://en.wikipedia.org/wiki/Typographic_alignment#Problems_with_justification" data-link-icon="wikipedia" data-link-icon-type="svg" title="Typographic alignment § Problems with justification">Particularly on narrow screens</a>⁠, justified text does not fit
well, and must be distorted to fit, by <a href="https://en.wikipedia.org/wiki/Microtypography" data-link-icon="wikipedia" data-link-icon-type="svg">microtypographic</a> <a href="https://www.gwern.net/docs/design/2000-thanh.pdf" id="thành-2000" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Micro-typographic extensions to the TeX typesetting system&#39;, Thành 2000">techniques</a> like inserting spaces between/​​within
words or changing glyph size. The default linebreaking that web browsers use is a bad one: it is a greedy algorithm, which produces many unnecessary poor
layouts, causing many stretched out words and blatant <a href="https://en.wikipedia.org/wiki/River_%28typography%29" data-link-icon="wikipedia" data-link-icon-type="svg">rivers</a>⁠. This bad layout gets worse the narrower the text, and so on Gwern.net lists on mobile, there are
a <em>lot</em> of bad-looking list items when fully-justified with greedy layout.</p>
<p>Knuth-Plass instead looks at paragraphs as a whole, and calculates every possible layout to pick the best one. As can be seen in any <span>T<sub>e</sub>X</span> output, the results are much better. Knuth-Plass (or its competitors) would solve the justified mobile layout problem.</p>
<p>Unfortunately, no browser implements any such algorithm (aside from a brief period where Internet Explorer, of all browsers, apparently did); there is a
property in <a href="https://www.w3.org/TR/css-text-4/#text-wrap" data-link-icon="W3" data-link-icon-type="text,sans"><span>CSS4</span></a>⁠, <code>text-wrap: pretty</code>, which might someday be implemented somehow by some browsers and be Knuth-Plass, but no one has
any idea when or how. And doing it in JavaScript is not an option, because the available JS prototypes fail on Gwern.net pages. (<a href="https://github.com/bramstein/typeset" data-link-icon="github" data-link-icon-type="svg">Bramstein’s <code>typeset</code></a> explicitly excludes lists and
blockquotes, Bramstein commenting in 2014 that “This is mostly a tech-demo, not something that should be used in production. I’m still hopeful browser will
implement this functionality natively at some point.” <a href="https://github.com/robertknight/tex-linebreak" data-link-icon="github" data-link-icon-type="svg">Knight’s <code>tex-linebreak</code></a> suffers from fatal bugs too. <a href="https://www.gwern.net/docs/www/mpetroff.net/6063c19769511f832d20062ff206cde3f2434cd2.html" rel="archived alternate nofollow" data-url-original="https://mpetroff.net/2020/05/pre-calculated-line-breaks-for-html-css/" title="Pre-calculated line breaks for HTML / CSS (Original URL: https://mpetroff.net/2020/05/pre-calculated-line-breaks-for-html-css/ )">Matthew Petroff</a> has a demo
which uses the brilliantly stupid brute-force approach of precalculating offline the Knuth-Plass linebreaks for every possible width—after all, monitor widths
can only range ~1–4000px, so it’s a small finite number of possibilities—but it’s unclear, to say the least, how I’d ever use such a thing for Gwern.net, and
doubtless has serious bugs of its own.) There are also questions about whether the performance on long pages would be acceptable, as the JS libraries rely on
inserting &amp; manipulating a lot of elements in order to force the browser to break where it should break, but those are largely moot when the prototypes are so
radically incomplete. (My prediction is that the cost would be acceptable with some optimizations and constraints like considering a maximum of <em>n</em>
lines.)</p>
<p>So the linebreaking situation is insoluble for the foreseeable future. We decided to disable full justification on narrow screens, and settle for
ragged-right.</p>
</li>
</ul>
</li>
<li>
<p><strong>Autopager keyboard shortcuts</strong>: binding Home/​PgUp &amp; End/​PgDwn keyboard shorcuts to go to the ‘previous’/​‘next’ logical page turned out to be
glitchy &amp; confusing.</p>
<p><span><span>HTML</span> supports previous/​next attributes on links which specify what <span>URL</span> is the
logical next or previous <span>URL</span>, which makes sense in many contexts like manuals or webcomics/​webserials or series of essays;
browsers make little use of this metadata—typically not even to</span> <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Link_types/preload" data-link-icon="FF" data-link-icon-type="text,sans">preload</a> the next page! (Opera apparently was one of the few exceptions.)</p>
<p>Such metadata was typically available in older hypertext systems by default, and so older more reader-oriented interfaces like pre-Web hypertext readers such
<a href="https://en.wikipedia.org/wiki/Info_%28Unix%29" data-link-icon="wikipedia" data-link-icon-type="svg">info</a>
<span>browsers frequently overloaded the standard page-up/​down keybindings to, if one was already at the beginning/​ending of a hypertext node, go to the logical
previous/​next node. This was convenient, since it made paging through a long series of info nodes fast, almost as if the entire info manual were a single long
page, and it was easy to discover: most users will accidentally tap them twice at some point, either reflexively or by not realizing they were already at the
top/​bottom (as is the case on most info nodes due to egregious shortness). In comparison, navigating the <span>HTML</span> version of an
info manual is frustrating: not only do you have to use the mouse to page through potentially dozens of 1-paragraph pages, each page takes noticeable time to load
(because of failure to exploit preloading) whereas a local info browser is instantaneous.</span></p>
<p>After defining a global sequence for Gwern.net pages, and adding a ‘navbar’ <span>to the bottom of each page with previous/​next <span>HTML</span> links encoding that sequence, I thought it’d be nice to support continuous scrolling through Gwern.net, and wrote some JS to detect
whether at the top/​bottom of page, and on each Home/​PgUp/​End/​PgDwn, whether that key had been pressed in the previous 0.5s, and if so, proceed to the
previous/​next page.</span></p>
<p><span>This worked, but proved buggy and opaque in practice, and tripped up even me occasionally. Since so few people know about that pre-<span>WWW</span> hypertext UI pattern (as useful as it is), would be unlikely to discover it, or use it much if they did discover it, I removed
it.</span></p>
</li>
</ul>
</section>
<section id="tools">

<p>Software tools &amp; libraries used in the site as a whole:</p>
<ul>
<li>
<p>The source files are written in <a href="https://pandoc.org/">Pandoc</a> <a href="https://en.wikipedia.org/wiki/Markdown" data-link-icon="wikipedia" data-link-icon-type="svg">Markdown</a> <span>(Pandoc: John MacFarlane et al; <span>GPL</span>) (source files: Gwern Branwen, <span>CC-0)</span>. The Pandoc Markdown uses a number of extensions; pipe tables
are preferred for anything but the simplest tables; and I use</span> <a href="https://www.gwern.net/docs/www/rhodesmill.org/952714f21809569d5d8d6243797e4f613c84ca70.html" rel="archived alternate nofollow" data-url-original="https://rhodesmill.org/brandon/2012/one-sentence-per-line/" title="(Original URL: https://rhodesmill.org/brandon/2012/one-sentence-per-line/ )">semantic linefeeds</a> (also called <a href="https://www.gwern.net/docs/www/sembr.org/ac853de1a0d7c7da21880a7b54fad27f27da70f1.html" rel="archived alternate nofollow" data-url-original="https://sembr.org/" title="(Original URL: https://sembr.org/ )">“semantic line breaks”</a> or <a href="https://www.gwern.net/docs/www/vanemden.wordpress.com/e7f5b9a1d5d4332192fe6174976d02d863c255cc.html" rel="archived alternate nofollow" data-url-original="https://vanemden.wordpress.com/2009/01/01/ventilated-prose/" title="(Original URL: https://vanemden.wordpress.com/2009/01/01/ventilated-prose/ )">“ventilated prose”</a>) formatting.</p>
</li>
<li>
 <p>math is written in <a href="https://en.wikipedia.org/wiki/LaTeX" data-link-icon="wikipedia" data-link-icon-type="svg"><span>L<span>a</span>T<span>e</span>X</span></a> which compiles to <a href="https://en.wikipedia.org/wiki/MathML" data-link-icon="wikipedia" data-link-icon-type="svg">MathML</a>⁠, rendered by MathJax (Apache)</p>
</li>
<li>
<p><span id="syntax-highlighting-algol">syntax highlighting</span>: we originally used <a href="https://github.com/jgm/skylighting" data-link-icon="github" data-link-icon-type="svg">Pandoc’s builtin</a> <a href="https://en.wikipedia.org/wiki/Kate_%28text_editor%29" data-link-icon="wikipedia" data-link-icon-type="svg">Kate</a>-derived themes, but most clashed with the overall appearance; after looking through all the existing themes, we took
inspiration from <a href="https://pygments.org/">Pygments’s</a> <a href="https://www.gwern.net/docs/www/xyproto.github.io/267fd99cab2edbe44dd02a8379b9dc73bfee8614.html" rel="archived alternate nofollow" data-url-original="https://xyproto.github.io/splash/docs/longer/algol_nu.html" title="(Original URL: https://xyproto.github.io/splash/docs/longer/algol_nu.html )">algol_nu</a> <span>(<span>BSD</span>) based on the
original</span> <a href="https://en.wikipedia.org/wiki/ALGOL_60" data-link-icon="wikipedia" data-link-icon-type="svg" title="ALGOL 60"><span>ALGOL</span></a> report, and typeset it in the <a href="https://en.wikipedia.org/wiki/IBM_Plex" data-link-icon="wikipedia" data-link-icon-type="svg" title="IBM Plex"><span><span>IBM</span>
Plex</span></a> Mono font<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</li>
<li>
<p>the site is compiled with the <a href="https://github.com/jaspervdj/Hakyll/" data-link-icon="github" data-link-icon-type="svg">Hakyll</a>v4+ static site
generator, used to generate Gwern.net, written in <a href="https://en.wikipedia.org/wiki/Haskell_%28programming_language%29" data-link-icon="wikipedia" data-link-icon-type="svg">Haskell</a> <span>(Jasper Van der Jeugt et al; <span>BSD</span>); for the gory details,
see</span> <a href="https://www.gwern.net/static/build/hakyll.hs" data-link-icon="code" data-link-icon-type="svg"><code>hakyll.hs</code></a> <span>which implements the compilation,
<span>RSS</span> feed generation, &amp; parsing of interwiki links as well. This just generates the basic website; I do many additional
optimizations/​tests before &amp; after uploading, which is handled by</span> <a href="https://www.gwern.net/static/build/sync-gwern.net.sh" data-link-icon="code" data-link-icon-type="svg"><code>sync-gwern.net.sh</code></a> <span>(Gwern Branwen, <span>CC-0)</span></span></p>
<p>My preferred method of use is to browse &amp; edit locally using <span>Emacs</span>, and then distribute using Hakyll. The simplest way to
use Hakyll is that you <code>cd</code> into your repository and <code>runhaskell hakyll.hs build</code> (with <code>hakyll.hs</code> <span>having whatever options
you like). Hakyll will build a static <span>HTML/​CSS</span> hierarchy inside</span> <code>_site/</code>; you can then do something like
<code>firefox _static/index</code><span>. (Because <span>HTML</span> extensions are not specified in the interest of</span> <a href="https://www.w3.org/TR/cooluris/" data-link-icon="W3" data-link-icon-type="text,sans"><span>cool <span>URIs</span></span></a>⁠, you cannot use the Hakyll <code>watch</code> webserver as of January 2014.) Hakyll’s main advantage for me is relatively
straightforward integration with the Pandoc Markdown libraries; Hakyll is not that easy to use, and so I do not recommend use of Hakyll as a general static site
generator unless one is already adept with <span>Haskell</span>.</p>
</li>
<li>
<p><span>the <span>CSS</span> is borrowed from a motley of sources and has been heavily modified, but its origin was the</span> <a href="https://www.gwern.net/docs/www/jaspervdj.be/18b65d43f1032749a84d76abef535857376ed541.html" data-link-icon="JVDJ" data-link-icon-type="text,quad,mono" rel="archived alternate nofollow" data-url-original="https://jaspervdj.be/hakyll/" title="(Original URL: https://jaspervdj.be/hakyll/ )">Hakyll homepage</a> &amp; <a href="https://github.com/jgm/gitit" data-link-icon="github" data-link-icon-type="svg">Gitit</a>⁠; for specifics, see <a href="https://www.gwern.net/static/css/default.css" data-link-icon="code" data-link-icon-type="svg"><code>default.css</code></a></p>
</li>
<li>
<p>Markdown syntax extensions:</p>
<ul>
<li>I implemented a Pandoc Markdown plugin for a custom syntax for interwiki links in Gitit, and then ported it to Hakyll (defined in <code>hakyll.hs</code>); it
allows linking to the English Wikipedia (among others) with syntax like <code>[malefits](!Wiktionary)</code> or <code>[antonym of &#39;benefits&#39;](!Wiktionary
&#34;Malefits&#34;)</code><span>. <span>CC-0.</span></span></li>
<li>inflation adjustment: <a href="https://www.gwern.net/static/build/Inflation.hs" id="gwern-static-build-inflation-hs" data-link-icon="code" data-link-icon-type="svg" title="&#39;InflationAdjuster&#39;, Branwen 2019"><code>Inflation.hs</code></a> provides a Pandoc Markdown plugin which allows automatic
inflation adjusting of dollar amounts, presenting the nominal amount &amp; a current real amount, with a syntax like <code>[$5]($1980)</code>.
</li>
<li>Book affiliate links are through an <a href="https://en.wikipedia.org/wiki/Amazon_Affiliates" data-link-icon="wikipedia" data-link-icon-type="svg">Amazon Affiliates</a> tag appended in the <code>hakyll.hs</code>
</li>
<li>image dimensions are looked up at compilation time &amp; inserted into <code>&lt;img&gt;</code> tags as browser hints</li>
</ul>
</li>
<li>
<p>JavaScript:</p>
<ul>
<li>
<p>Comments are outsourced to <a href="https://en.wikipedia.org/wiki/Disqus" data-link-icon="wikipedia" data-link-icon-type="svg">Disqus</a> (since I am not interested in writing a dynamic system to do it, and their anti-spam techniques are much better than mine).</p>
</li>
<li>
<p><span>the <span>HTML</span> tables are sortable via</span> <a href="https://www.gwern.net/docs/www/mottie.github.io/cc3241465b146702d90c231f8a6c27ae69ab83bf.html" rel="archived alternate nofollow" data-url-original="https://mottie.github.io/tablesorter/docs/" title="(Original URL: https://mottie.github.io/tablesorter/docs/ )">tablesorter</a> <span>(Christian Bach;
<span>MIT/​​GPL</span>)</span></p>
</li>
<li>
<p>the MathML is rendered using <a href="https://en.wikipedia.org/wiki/MathJax" data-link-icon="wikipedia" data-link-icon-type="svg">MathJax</a></p>
</li>
<li>
<p>analytics are handled by <a href="https://en.wikipedia.org/wiki/Google_Analytics" data-link-icon="wikipedia" data-link-icon-type="svg">Google Analytics</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/A%2FB_testing" data-link-icon="wikipedia" data-link-icon-type="svg">A / B
testing</a> is done using <a href="https://github.com/danmaz74/ABalytics" data-link-icon="github" data-link-icon-type="svg">ABalytics</a> <span>(Daniele Mazzini;
<span>MIT</span>) which hooks into Google Analytics (see</span> <a href="https://www.gwern.net/AB-testing" id="gwern-ab-testing-2">testing
notes</a>) for individual-level testing; when doing site-level long-term testing like in the <a href="https://www.gwern.net/Ads" id="ads-3">advertising
A / B tests</a>⁠, I simply write the JS manually.</p>
</li>
<li>
<p><a href="https://www.gwern.net/static/js/popups.js" id="achmiz-2019" data-link-icon="code" data-link-icon-type="svg" title="&#39;&lt;code&gt;popups.js&lt;/code&gt;&#39;, Achmiz 2019">Generalized tooltip popups</a> for loading introductions/​​summaries/​​previews of all links when one
mouses-over a link; reads annotations, which are manually written &amp; automatically populated from many sources (Wikipedia, Pubmed, BioRxiv, <span>Arxiv</span>, hand-written…), with special handling of YouTube videos (Said Achmiz, <a href="https://nitter.hu/theshawwn" data-link-icon="twitter" data-link-icon-type="svg">Shawn Presser</a>⁠; <span><span>MIT</span>).</span></p>
<p>Note that ‘links’ here is interpreted broadly: almost everything can be ‘popped up’<span>. This includes links to sections (or div IDs) on the current or
other pages, <span>PDFs</span> (often page-linked using the obscure but handy</span> <code>#page=N</code> feature), source code files
(which are syntax-highlighted by Pandoc), locally-mirrored web pages, footnotes/​​sidenotes, any such links within the popups themselves recursively…</p>
<ul>
<li>
<p>the floating footnotes are handled by the generalized tooltip popups (they were originally implemented via <a href="https://www.gwern.net/docs/www/ignorethecode.net/e40734db7dd14374d0eeea1e8b35c0b88fe9deb4.html" rel="archived alternate nofollow" data-url-original="http://ignorethecode.net/blog/2010/04/20/footnotes/" title="(Original URL: http://ignorethecode.net/blog/2010/04/20/footnotes/ )"><code>footnotes.js</code></a>); when the browser window is wide enough, the floating
footnotes are instead replaced with marginal notes/​​<em>sidenotes</em><a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>
using a custom library, <a href="https://www.gwern.net/static/js/sidenotes.js" data-link-icon="code" data-link-icon-type="svg"><code>sidenotes.js</code></a> <span>(Said Achmiz,
<span>MIT</span>)</span></p>
<figure>
<img alt="Demonstration of sidenotes on Radiance." decoding="async" height="1295" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.gwern.net/images/2002-radiance/sidenotes.png" srcset="/images/2002-radiance/sidenotes.png-768px.png 768w, /images/2002-radiance/sidenotes.png 1400w" width="1400"/>
 <figcaption aria-hidden="true">
Demonstration of sidenotes on <a href="https://www.gwern.net/docs/radiance/2002-scholz-radiance" id="scholz-2002-2"><em>Radiance</em></a>⁠.
</figcaption>
</figure>
</li>
</ul>
</li>
<li>
<p>image size: full-scale images (figures) can be clicked on to zoom into them with slideshow mode—useful for figures or graphs which do not comfortably fit into
the narrow body—using another custom library, <a href="https://www.gwern.net/static/js/image-focus.js" data-link-icon="code" data-link-icon-type="svg"><code>image-focus.js</code></a>
<span>(Said Achmiz; <span>GPL</span>)</span></p>
</li>
</ul>
</li>
<li>
<p>error checking: problems such as broken links are checked in 3 phases:</p>
<ul>
<li>
<a href="https://www.gwern.net/About#markdown-checker" id="gwern-about-markdown-checker"><code>markdown-lint.sh</code></a>: writing time
</li>
<li>
<a href="https://www.gwern.net/static/build/sync-gwern.net.sh" data-link-icon="code" data-link-icon-type="svg"><code>sync-gwern.net.sh</code></a><span>: during compilation,
sanity-checks file size &amp; count; greps for broken interwikis; runs <span>HTML</span> tidy over pages to warn about invalid <span>HTML</span>; tests liveness &amp; <span>MIME</span> types of various pages post-upload; checks for duplicates, read-only,
banned filetypes, too large or uncompressed images, etc.</span>
</li>
<li>
<a href="https://www.gwern.net/Archiving-URLs" id="archiving-2">Link rot tools</a>: <a href="https://github.com/linkchecker/linkchecker" data-link-icon="github" data-link-icon-type="svg"><code>linkchecker</code></a>⁠, <a href="https://github.com/pirate/ArchiveBox/" data-link-icon="github" data-link-icon-type="svg">ArchiveBox</a>⁠, and <a href="https://hackage.haskell.org/package/archiver" data-link-icon="𝛌" data-link-icon-type="text">archiver-bot</a>
</li>
</ul>
</li>
</ul>
<section id="implementation-details">
<h2><a href="#implementation-details" title="Link to section: § &#39;Implementation Details&#39;">Implementation Details</a></h2>
<p>There are a number of little tricks or details that web designers might find interesting.</p>
<p>Efficiency:</p>
<ul>
<li>
<p><strong>fonts</strong>:</p>
<ul>
<li>
<p>Adobe <a href="https://en.wikipedia.org/wiki/Source_Serif_Pro" data-link-icon="wikipedia" data-link-icon-type="svg">Source Serif</a>⁠/​​<a href="https://en.wikipedia.org/wiki/Source_Sans_Pro" data-link-icon="wikipedia" data-link-icon-type="svg">Sans</a>⁠/​​<a href="https://en.wikipedia.org/wiki/Source_Code_Pro" data-link-icon="wikipedia" data-link-icon-type="svg">Code Pro</a>: originally Gwern.net used <a href="https://en.wikipedia.org/wiki/Baskerville" data-link-icon="wikipedia" data-link-icon-type="svg">Baskerville</a>⁠, but system Baskerville fonts don’t have small caps. Adobe’s open-source “Source” font
family of <a href="https://blog.obormot.net/Screen-serif-fonts">screen serifs</a>⁠, however, is high quality and actively-maintained, comes
with good small caps and multiple sets of numerals (<a href="https://practicaltypography.com/alternate-figures.html#oldstyle-figures">‘old-style’</a>
<span>numbers for the body text and different numbers for tables), and looks particularly nice on Macs. (It is also subsetted to cut down the load time.) Small
caps <span>CSS</span> is automatically added to abbreviations/​​acronyms/​​initials by a Hakyll/​​Pandoc plugin, to avoid manual
annotation.</span></p>
</li>
<li>
<p>efficient drop caps by subsetting: 1 drop cap is used on every page, but a typical drop cap font will slowly download as much as a megabyte in order to
render 1 single letter.</p>
<p><span><span>CSS</span> font loads avoid downloading font files which are entirely unused, but they must download the entire font file
if anything in it is used, so it doesn’t matter that only one letter gets used. To avoid this, we split each drop cap font up into</span> <a href="https://github.com/TeX-Live/yinit/issues/1" data-link-icon="github" data-link-icon-type="svg">a single font file per letter</a> <span>and use <span>CSS</span> to load all the font files; since only 1 font file is used at all, only 1 gets downloaded, and it will be ~4kb rather than 168kb.
This has been done for all the drop cap fonts used (</span><a href="https://ctan.org/pkg/yinit?lang=en" data-link-icon="TₑX" data-link-icon-type="text"><code>yinit</code></a>⁠, <a href="https://www.dafont.com/cheshire-initials.font">Cheshire Initials</a>⁠, <a href="https://www.dafont.com/deutsche-zierschrif.font">Deutsche Zierschrift</a>⁠, <a href="https://www.gwern.net/docs/www/tug.org/4bcaf0f368ff4b1f3b54ccfc7f39296e865293f4.html" data-link-icon="TₑX" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://tug.org/FontCatalogue/goudyinitialen/" title="(Original URL: https://tug.org/FontCatalogue/goudyinitialen/ )">Goudy Initialen</a>⁠, <a href="https://www.dafont.com/kanzlei.font">Kanzlei
Initialen</a><span>), and the necessary <span>CSS</span> can be seen in</span> <a href="https://www.gwern.net/static/css/fonts.css" data-link-icon="code" data-link-icon-type="svg"><code>fonts.css</code></a><span>. To specify the drop cap for each page, a Hakyll metadata field is used to pick the class and
substituted into the <span>HTML</span> template.</span></p>
</li>
</ul>
</li>
<li>
<p><strong>lazy JavaScript loading</strong> by <a href="https://developer.mozilla.org/en-US/docs/Web/API/IntersectionObserver" data-link-icon="FF" data-link-icon-type="text,sans">IntersectionObserver</a>: several JS features are used rarely or not at all on many pages, but are responsible for much network
activity. For example, most pages have no tables but tablesorter must be loaded anyway, and many readers will never get all the way to the Disqus comments at the
bottom of each page, but Disqus will load anyway, causing much network activity and disturbing the reader because the page is not ‘finished loading’ yet.</p>
<p>To avoid this, IntersectionObserver can be used to write a small JS function which fires only when particular page elements are visible to the reader. The JS
then loads the library which does its thing. So an IntersectionObserver can be defined to fire only when an actual <code>&lt;table&gt;</code> element becomes
visible, and on pages with no tables, this never happens. Similarly for Disqus and <code>image-focus.js</code>. This trick is a little dangerous if a library
depends on another library because the loading might cause race conditions; fortunately, only 1 library, tablesorter, has a prerequisite, jQuery, so I simply
prepend jQuery to tablesorter and load tablesorter. (Other libraries, like sidenotes or WP popups, aren’t lazy-loaded because sidenotes need to be rendered as fast
as possible or the page will jump around &amp; be laggy, and WP links are so universal it’s a waste of time making them lazy since they will be in the first screen on
every page &amp; be loaded immediately anyway, so they are simply loaded asynchronously with the <code>defer</code> JS keyword.)</p>
</li>
<li>
<p><strong>image optimization</strong><span>: <span>PNGs</span> are optimized by</span> <code>pngnq</code>/​<code>advpng</code><span>,
<span>JPEGs</span> with</span> <code>mozjpeg</code><span>, <span>SVGs</span> are minified, <span>PDFs</span> are compressed with</span> <a href="https://github.com/ocrmypdf/OCRmyPDF" data-link-icon="github" data-link-icon-type="svg"><code>ocrmypdf</code>’s</a> <a href="https://en.wikipedia.org/wiki/JBIG2" data-link-icon="wikipedia" data-link-icon-type="svg"><span>JBIG2</span></a> <span>support. (<span>GIFs</span> are not used at all in favor of WebM/​MP4</span>
<code>&lt;video&gt;</code>s.)</p>
</li>
<li>
<p><strong><span><span>JS/​CSS</span> minification</span></strong><span>: because Cloudflare does Brotli compression, minification of
<span>JS/​CSS</span> has little advantage and makes development harder, so no minification is done; the font files don’t need any special
compression either.</span></p>
</li>
<li>
<p><strong>MathJax</strong>: getting well-rendered mathematical equations requires MathJax or a similar heavyweight JS library; worse, even after disabling
features, the load &amp; render time is extremely high—a page like <a href="https://www.gwern.net/Embryo-selection" id="es-2">the embryo selection page</a> which is both
large &amp; has a lot of equations can visibly take &gt;5s (as a progress bar that helpfully pops up informs the reader).</p>
<p>The solution here is to <a href="https://www.gwern.net/docs/www/joa.sh/290f53c78d4b287be8059260f613988a8abda677.html" rel="archived alternate nofollow" data-url-original="https://joa.sh/posts/2015-09-14-prerender-mathjax.html" title="(Original URL: https://joa.sh/posts/2015-09-14-prerender-mathjax.html )">prerender MathJax locally after Hakyll compilation</a>⁠, using the local tool <a href="https://github.com/pkra/mathjax-node-page/" data-link-icon="github" data-link-icon-type="svg"><code>mathjax-node-page</code></a> <span>to load the final
<span>HTML</span> files, parse the page to find all the math, compile the expressions, define the necessary <span>CSS</span>, and write the <span>HTML</span> back out. Pages still need to download the fonts but the overall speed goes
from &gt;5s to &lt;0.5s, and JS is not necessary at all.</span></p>
</li>
<li>
<p><strong>Automatic Link-Ification Regexps</strong>: I wrote <a href="#linkauto-hs"><code>LinkAuto.hs</code></a>⁠, a Pandoc library for automatically turning
user-defined regexp-matching strings into links, to automatically turn all the scientific jargon into Wikipedia or paper links. (There are too many to annotate by
hand, especially as new terms are added to the list or abstracts are generated for popups.)</p>
<p>“Test all strings against a list of regexps and rewrite if they match” may sound simple and easy, but the naive approach is exponential: <em>n</em> strings,
<em>r</em> regexps tested on each, so 𝑂(<em>n</em><sup><em>r</em></sup>) matches total. With &gt;600 regexps initially &amp; millions of words on Gwern.net… Regexp
matching is fast, but it’s not <em>that</em> fast. Getting this into the range of ‘acceptable’ (~3× slowdown) required a few tricks.</p>
<p>The major trick is that each document is converted to a simple plain text format, and the regexps are run against the <em>entire</em> <span>document; in the
average case (think of short pages or popup annotations), there will be zero matches, and the document can be skipped entirely. Only the matching regexps get used
in the full-strength <span>AST</span> traversal. While it is expensive to check a regexp against an entire document, it is an order of
magnitude or two less epxnesive than checking that regexp against every string node inside that document!</span></p>
</li>
</ul>
<p>Correctness:</p>
<ul>
<li>
<p><strong>Dark mode</strong>: our dark mode is custom, and tries to make dark mode a first-class citizen.</p>
<ol type="1">
<li>
<p><span>Avoiding Flashing &amp; Laggy Scrolling</span>: it is implemented in the standard best-practice way of creating two color palettes
(associating a set of color variables for every element, for a light-mode and then automatically-generating dark mode colors by inverting &amp; gamma-correcting),
and using JS to toggle the media-query to instantly enable that color.</p>
<p>This avoids the ‘flash of white’ <span>on page loads which regular JS-based approaches incur (because the <span>CSS</span>
media-queries can only implement auto-dark-mode, and the dark mode widget requires JS; however, the JS, when it decides to inject dark mode <span>CSS</span> into the page, is too late and that <span>CSS</span> will be rendered last after the reader has already been
exposed to the flash). The separate color palette approach also avoids the lag &amp; jank of using invert <span>CSS</span> filters (one
would think that</span> <code>invert(100%)</code> would be free from a performance standpoint—but it is not).</p>
</li>
<li>
<p><span>Native Dark Mode Color Scheme</span>: we modify the color scheme as necessary: some blue breaks the monochrome coloring, and the
source code syntax highlighting is tweaked for dark mode visibility.</p>
<p>Because of the changes in contrast, inverting the color scheme only <em>mostly</em> works. In particular, inline &amp; code blocks tend to disappear.</p>
</li>
<li>
<p><span>Inverted Images</span>: color images are desaturated &amp; grayscaled by default to reduce their brightness; grayscale/​​monochrome
images, are automatically inverted by an ImageMagick heuristic, falling back to manual annotations.</p>
<p>This avoids the common failure mode where a blog uses a dark mode library which implements the class approach correctly… but then all of their images still
have blinding bright white backgrounds or overall coloration, defeating the point! However, one cannot just blindly invert images because many images,
 photographs of people especially, are garbage as ‘photo-negatives’.</p>
</li>
<li>
<p><span>Three-Way Dark Mode Toggle</span>: Many dark modes are implemented with a simple binary on/​​off logic stored in a cookie, ignoring
browser/​​OS preferences, or simply defining ‘dark mode’ as the negation of the current browser/​​OS preference.</p>
<p>This is incorrect, and leads to odd situations like a website enabling dark mode during the day, and then light mode during the night! Using an
auto/​​dark/​​light three-way toggle means that users can force dark/​​light mode but also leave it on ‘auto’ to follow the browser/​​OS preference over
the course of the day. This requires a UI widget and it still incurs some of the problems of an <a href="#auto-dark-mode">auto-only dark mode</a>⁠, but overall
strikes the best balance between enabling dark mode unasked, user control/​​confusion, and avoiding dark mode at the wrong time.</p>
</li>
</ol>
</li>
<li>
<p><strong>collapsible sections</strong>: managing complexity of pages is a balancing act. It is good to provide all necessary code to reproduce results, but does
the reader <em>really</em> want to look at a big block of code? Sometimes they always would, sometimes only a few readers interested in the gory details will want
to read the code. Similarly, a section might go into detail on a tangential topic or provide additional justification, which most readers don’t want to plow
through to continue with the main theme. Should the code or section be deleted? No. But relegating it to an appendix, or another page entirely is not satisfactory
either—for code blocks particularly, one loses the literate programming aspect if code blocks are being shuffled around out of order.</p>
<p>A nice solution is to simply use a little JS to implement <a href="https://en.wikipedia.org/wiki/Code_folding" data-link-icon="wikipedia" data-link-icon-type="svg">code folding</a> <span>approach where sections or code blocks can be visually shrunk or collapsed, and
expanded on demand by a mouse click. Collapsed sections are specified by a <span>HTML</span> class (eg.</span> <code>&lt;div
class=&#34;collapse&#34;&gt;&lt;/div&gt;</code>), and summaries of a collapsed section can be displayed, defined by another class (<code>&lt;div
class=&#34;collapseSummary&#34;&gt;</code>). This allows code blocks to be collapse by default where they are lengthy or distracting, and for entire regions to be
collapsed &amp; summarized, without resorting to many appendices or forcing the reader to an entirely separate page.</p>
</li>
<li>
<p><strong>Sidenotes</strong>: one might wonder why <code>sidenotes.js</code> is necessary when most sidenote uses are like <a href="https://www.gwern.net/docs/www/edwardtufte.github.io/e43d8239ed3fa1d513e2b4d071b6a7c0c8a98bff.html#sidenotes" data-link-icon="ET" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://edwardtufte.github.io/tufte-css/#sidenotes" title="&#39;Tufte-CSS: Sidenotes: Footnotes and Marginal Notes&#39;, Liepmann 2022 (Original URL: https://edwardtufte.github.io/tufte-css/#sidenotes )"><span>Tufte-<span>CSS</span></span></a> <span>and use a static <span>HTML/​CSS</span> approach, which would avoid a JS library entirely and
visibly repainting the page after load?</span></p>
<p>The problem is that <span>Tufte</span>-<span><span>CSS</span>-style sidenotes do not reflow and are solely on
the right margin (wasting the considerable whitespace on the left), and depending on the implementation, may overlap, be pushed far down the page away from their,
break when the browser window is too narrow or not work on smartphones/​tablets at all. (This is</span> <a href="https://github.com/edwardtufte/tufte-css/issues/93#issuecomment-670695382" data-link-icon="ET" data-link-icon-type="text">fixable</a>⁠, <span>Tufte</span>-<span><span>CSS</span>’s maintainers just haven’t.) The JS library is able to handle all these and can
handle the most difficult cases like</span> <a href="#scholz-2002-2">my annotated edition of <em>Radiance</em></a>⁠. (<a href="https://www.gwern.net/docs/www/edwardtufte.github.io/e43d8239ed3fa1d513e2b4d071b6a7c0c8a98bff.html#epigraphs" data-link-icon="ET" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://edwardtufte.github.io/tufte-css/#epigraphs" title="(Original URL: https://edwardtufte.github.io/tufte-css/#epigraphs )"><span>Tufte-<span>CSS</span>-style epigraphs</span></a><span>,
however, pose no such problems and we take the same approach of defining an <span>HTML</span> class &amp; styling with <span>CSS</span>.)</span></p>
</li>
<li>
<p><strong>Link icons</strong>: icons are defined for all filetypes used in Gwern.net and many commonly-linked websites such as Wikipedia, Gwern.net (within-page
section links and between-page get ‘§’ &amp; logo icons respectively), or YouTube; all are inlined into <code>default.css</code> as <a href="https://en.wikipedia.org/wiki/Data_URI_scheme" data-link-icon="wikipedia" data-link-icon-type="svg"><span>data <span>URIs</span></span></a><span>; the <span>SVGs</span> are so small it would be absurd to have them be files.</span></p>
</li>
<li>
<p><strong>Redirects</strong><span>: static sites have trouble with redirects, as they are just static files. <span>AWS</span> 3S does not
support a</span> <code>.htaccess</code><span>-like mechanism for rewriting <span>URLs</span>. To allowing moving pages &amp; fix broken links, I
wrote</span> <a href="https://www.gwern.net/docs/www/jaspervdj.be/1c18943c199a07727fc395a4a1ebbfe4c90bb38d.html" data-link-icon="JVDJ" data-link-icon-type="text,quad,mono" rel="archived alternate nofollow" data-url-original="https://jaspervdj.be/hakyll/reference/Hakyll-Web-Redirect.html" title="(Original URL: https://jaspervdj.be/hakyll/reference/Hakyll-Web-Redirect.html )"><code>Hakyll.Web.Redirect</code></a> <span>for generating simple <span>HTML</span> pages with redirect metadata+JS, which simply redirect from <span>URL</span> 1 to <span>URL</span> 2. After moving to Nginx hosting, I converted all the redirects to regular Nginx rewrite rules.</span></p>
<p><span>In addition to page renames, I monitor 404 hits in Google Analytics to fix errors where possible, and Nginx logs. There are an astonishing number of ways
to misspell Gwern.net <span>URLs</span>, it turns out, and I have defined &gt;10k redirects so far (in addition to generic regexp rewrites
to fix patterns of errors).</span></p>
</li>
</ul>
</section>
</section>
<section id="appendix">

<section id="returns-to-design">
<h2><a href="#returns-to-design" title="Link to section: § &#39;Returns To Design?&#39;">Returns To Design?</a></h2>
<div>
<blockquote>
<p>What is the ‘shape’ of returns on investment in industrial design, UI/​​UX, typography etc? Is it a sigmoid with a golden mean of effort vs return… or a
parabola with an unhappy valley of mediocrity?</p>
<p>My experience with Gwern.net design improvements is that readers appreciated changes moderately early on in making its content more pleasant to read (if only by
comparison to the rest of the Internet!), but after a certain point, it all ‘came together’, in some sense, and readers started raving over the design and pointing
to Gwern.net’s <em>design</em> rather than its content. This is inconsistent with the default, intuitive model of ‘diminishing returns’, where each successive
design tweak should be worth less than the previous one. Is there a <a href="https://www.gwern.net/docs/psychology/2020-isaac.pdf" id="isaac-spangenberg-2020" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The Perfection Premium&#39;, Isaac &amp; Spangenberg 2020">’perfection</a> <a href="https://www.gwern.net/docs/psychology/writing/2020-blunden.pdf" id="blunden-brodsky-2020" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Beyond the Emoticon: Are There Unintentional Cues of Emotion in Email?&#39;, Blunden &amp; Brodsky 2020">premium’</a> (perhaps as a signal of <a href="https://www.gwern.net/notes/Regression" id="gwern-notes-regression">underlying unobservable quality</a>⁠, or perhaps user interaction is like an <a href="https://www.gwern.net/notes/Pipeline" id="gwern-notes-pipeline">O-ring process</a>)?</p>
</blockquote>
</div>
<p>Particularly with typography, there seems to be an infinite number of finicky details one could spend time on (much of which appears to be <a href="https://www.gwern.net/Fonts" id="gwern-fonts">for novelty’s sake</a>⁠, while vastly more important things like <a href="#ads-2">advertising harms</a> go ignored by so-called
designers). One’s initial guess is that it’d be <a href="https://en.wikipedia.org/wiki/Diminishing_returns" data-link-icon="wikipedia" data-link-icon-type="svg" title="Diminishing returns">diminishing returns</a>
like most things: it’d look something like a log curve, where every additional tweak costs more effort as one approaches the Platonic ideal. A more sophisticated guess
would be that it’d look like a <a href="https://en.wikipedia.org/wiki/Sigmoid_function" data-link-icon="wikipedia" data-link-icon-type="svg">sigmoid</a>: at first, something is <em>so</em> awful that any fixes are irrelevant to the user because that just means they suffer from a
<em>different</em> problem (it doesn’t matter much if a website doesn’t render because of a JS bug if the text when it does render is so light-shaded that one can’t
read it); then each improvements makes a difference to some users as it approaches a respectable mediocrity; and after that, it’s back to <span>diminishing returns</span>.</p>
<p>My experience with improving the design of Gwern.net &amp; reading about design has made me wonder if either of those is right. The shape may resemble more of a
parabola: the sigmoid, at some point, spikes up and returns <em>increase</em> rather than diminish?</p>
<p>I noticed that for the first half-decade or so, no one paid much attention to the tweaks I made, as it was an ordinary Markdown-based static site. As I kept
tinkering, a comment would be made once in a while. When Said Achmiz lent his talents to adding features &amp; enhancements and exploring novel tweaks, comments cropped up
much more frequently (consistent with the enormous increase in time spent on it); by 2019, the redesign had mostly stabilized and most of the signature features had
been implemented, and 2020 was more about bug fixes than adding pizzazz. Under the intuitive theories, the rate of comments would be about the same: while the bug
fixes may involve huge effort—the dark mode rewrite was a 3-month agony—the improvements are ever smaller—said rewrite had no user-visible change other than removing
slowness. But while <a href="https://www.gwern.net/Traffic" id="gwern-traffic">site traffic</a> remained steady, 2020 attracted more compliments than ever!</p>
<p>Similarly, the LW team put an unusual amount of effort into designing <a href="https://www.lesswrong.com/posts/TTPux7QFBpKxZtMKE/the-lesswrong-2018-book-is-available-for-pre-order" data-link-icon="LW" data-link-icon-type="text">a 2018 essay compilation</a>⁠, making it stylish (even redrawing all the images to match the color themes), and <a href="https://marginalrevolution.com/marginalrevolution/2020/12/what-is-the-meta-rational-thing-to-do-here.html#160189881" id="pace-2020" data-link-icon="M𝐑" data-link-icon-type="text" title="&#39;What is the meta-rational thing to do here? [comments]&#39;, Pace 2020">they were surprised</a> by unusually large
the preorders were: not a few percentage points, but many times. (There are many books on data visualization, but I suspect Edward Tufte’s books outsell them, even the
best, by similar magnitudes.) And what should we make of <a href="https://www.gwern.net/reviews/Movies#rams" id="gwern-reviews-movies-rams">Apple &amp; design</a>⁠, whose
devices &amp; software have glaring flaws and yet, by making more of an attempt, command a premium and are regarded well by the public? Or <a href="https://en.wikipedia.org/wiki/Stripe_%28company%29" data-link-icon="wikipedia" data-link-icon-type="svg" title="Stripe (company)">Stripe</a>?<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>If the sigmoid were right, just how much more effort would be necessary to elicit such jumps? Orders of magnitude more? I &amp; Said have invested effort, certainly,
but there are countless sites (even confining the comparison to just personal websites and excluding sites with professional full-time developers/​designers), whose
creators have surely invested more time; millions of books are self-published every year; and Apple is certainly not the only tech company which tries to design things
well.</p>
<p>What might be going on is related to the <a href="https://www.nngroup.com/articles/aesthetic-usability-effect/" id="moran-2017" data-link-icon="NN" data-link-icon-type="text,sans" title="&#39;The Aesthetic-Usability Effect&#39;, Moran 2017">“aesthetic-usability effect”</a>: at a certain level, the design itself
becomes noticeable to the user for its esthetic effect and the esthetics itself becomes a feature adding to the experience. That is, at the bottom of the sigmoid, on a
website strewn with typos and broken links and confusing colors, the user thinks “this website sucks!”, while in the middle, the user ceases to think of the website at
all and just gets on with using it, only occasionally irritated by design flaws; finally, at a certain level, when all the flaws have been removed and the site itself
is genuinely unironically beautiful, both the beauty &amp; absence of flaws themselves become noticeable, and the reader thinks, “this website, it is—pretty awesome!” The
spike is where suddenly the design itself is perceived as a distinct thing, not merely how the thing happens to be. Designers often aspire to an end-state of
<em>sprezzatura</em> or the “crystal goblet”, where they do their job so well the user doesn’t realize there was a job to be done at all—but in this fallen world,
where excellence seems so rare, the better one does the job, the more the contrast with all the botched jobs inevitably draws attention.</p>
<p>It is difficult for even the reader least interested in the topic to open a <span>Tufte</span> book, or walk into an Apple store, and
<em>not</em> be struck by first impressions of elegance and careful design—which is <a href="https://www.gwern.net/docs/www/asktog.com/efa892d330885607fe0f01dcd66193fd256ad056.html" id="tognazzini-2013" rel="archived alternate nofollow" data-url-original="https://asktog.com/atc/the-third-user/" title="&#39;The Third User, or, Exactly Why Apple Keeps Doing Foolish Things (Original URL: https://asktog.com/atc/the-third-user/ )">not necessarily</a> <a href="https://psyarxiv.com/dnr9s/" id="lin-thornton-2022" data-link-icon="ψ" data-link-icon-type="text" title="&#39;Fooled by beautiful data: Visualization aesthetics bias trust in science, news, and social media&#39;, Lin &amp; Thornton 2021">a good thing</a> if that cannot be lived
up to. (Any person struck by this must also realize that other people will be similarly impressed, using their own response as a proxy for the general reaction<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a>⁠, and will take it as a model for aspiration; liking Apple or <span>Tufte</span> signals your good taste, and that makes them luxury products as much as anything.)</p>
<p>This suggests a dangerous idea (dangerous because a good excuse for complacency &amp; mediocrity, especially for those who do not manage even mediocrity but believe
otherwise): if you are going to invest in design, half-measures yield less than half-results. If the design is terrible, then one should continue; but if the design is
already reasonable, then instead of there being substantial returns, the <span>diminishing returns</span> have already set in, and it may be
a too-long slog from where you are to the point where people are impressed enough by the design for the esthetic effect to kick in. Those moderate improvements may not
be worthwhile if one can only modestly improve on mediocrity; and a sufficiently-flawed design may not be able to reach the esthetic level at all, requiring a radical
new design.</p>
</section>
</section>

</div></div>
  </body>
</html>
