<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pi.website/research/real_time_chunking">Original</a>
    <h1>Real-time action chunking with large models</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><p>Published</p><p>June 9, 2025</p><p>Email</p><p><span>research@physicalintelligence.company</span><span>Kevin Black, Manuel Y. Galliker, Sergey Levine</span></p></div></div></div>


<div><p>Unlike chatbots or image generators, robots must operate in <em>real time</em>. While a robot is “thinking”, the world around it
evolves according to physical laws, so delays between inputs and outputs have a tangible impact on performance. For a
language model, the difference between fast and slow generation is a satisfied or annoyed user; for a
vision-language-action model (VLA), it could be the difference between a robot handing you a hot coffee or spilling it in
your lap. While VLAs have achieved <a href="https://www.pi.website/blog/pi05">promising results</a> in open-world
generalization, they can be slow to run. Like their cousins in language and vision, these models have billions of
parameters and require heavy-duty GPUs. On edge devices like mobile robots, that adds even more latency for network
communication between a centralized inference server and the robot<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p><p>To build a <a href="https://en.wikipedia.org/wiki/Real-time_computing" target="_blank">real-time system</a> with VLAs, we are going to need some form of asynchrony: that is, we must let a model think
about its future actions while executing a previous one. <em>Action chunking</em> — where a robot outputs and executes a
sequence of multiple actions for each inference call — provides a good starting point. <a href="https://www.pi.website/blog/pi0">Our
VLAs</a> all use a chunk size of 50 actions, corresponding to 1 second of real time.
However, chunking alone is not enough. When we switch chunks, the new actions might not “agree” with the old ones,
causing discontinuities and unsafe accelerations. Trying to naively smooth over these discontinuities is not guaranteed
to produce valid actions, and can have disastrous consequences.</p></div>
<div><p>The aforementioned disastrous consequences. These results are obtained by
attempting to do real-time inference with temporal ensembling to smooth over the discontinuities, a popular prior method
introduced in <a href="https://tonyzhaozh.github.io/aloha/">ACT</a> (Zhou et al., 2023).</p></div>
<div><p>As a result, in <span>π<sub>0</sub></span>, <span>π<sub>0</sub></span>-FAST, and <span>π<sub>0.5</sub></span>, we did not use a real-time strategy<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. We executed actions <em>synchronously</em>,
meaning that we would finish executing one chunk, wait for model inference, and then begin executing the next one. This
way, the robot started each chunk from rest, and we circumvented the issues that arise from switching chunks
while in motion. However, this introduces pauses between chunks that are still harmful — these pauses aren&#39;t in
the training data. Not to mention: they slow things down, are ugly to look at, and discourage us from scaling up the size
of our models.</p><p>To solve these problems, we developed an algorithm that we call <strong>real-time chunking</strong> <strong>(RTC)</strong>. It enables real-time
execution without discontinuities, and it works on any diffusion- or flow-based VLA — including <span>π<sub>0.5</sub></span> — with <strong>no
training-time changes</strong>. We found that RTC significantly sped up execution time for all the tasks we tested. It was also
very robust to latency, even when we injected artificial delays to cause much higher latencies than normal. RTC
completed dynamic and precise tasks, like striking a match or plugging in an Ethernet cable, with inference delays of
more than 300 milliseconds.</p><h3 id="thinking-while-moving"><a href="#thinking-while-moving" aria-hidden="true" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Thinking while moving</h3></div>
<div><div><div><div><p><img alt="Shirt grid" loading="lazy" width="3792" height="1710" decoding="async" data-nimg="1" srcset="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fshirt-grid.1f718e2c.jpg&amp;w=3840&amp;q=75 1x" src="https://www.pi.website/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fshirt-grid.1f718e2c.jpg&amp;w=3840&amp;q=75"/></p><div><p><img alt="Arm" loading="lazy" width="1002" height="810" decoding="async" data-nimg="1" srcset="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farm.b5360221.png&amp;w=1080&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farm.b5360221.png&amp;w=2048&amp;q=75 2x" src="https://www.pi.website/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farm.b5360221.png&amp;w=2048&amp;q=75"/></p><div><div><p>Action chunk 1</p><p>Action chunk 2</p></div></div></div></div></div></div></div>
<p>When there is an inference delay, real-time execution requires careful consideration. While a new chunk is generated
(red), the previous chunk (green) continues executing. If the new chunk is substantially different — which it often is —
switching to the new chunk results in disaster.</p>
<div><p>The core challenge of real-time execution is to maintain consistency between action chunks. By the time a new chunk has
been generated by the model, the previous one has already been executed partway. Without a specialized algorithm, the new
chunk might be totally incompatible with the robot&#39;s current trajectory — the model might be reacting to new information,
or it might just be sampling a different “strategy” from its learned distribution of behaviors. Thus, we must somehow
<em>use</em> the overlapping timesteps where we have access to the remaining actions of the previous chunk. A good real-time
algorithm should produce a new chunk that is <em>consistent</em> with these overlapping actions while still preserving the
model&#39;s reactivity to new information and ability to make intelligent decisions.</p><p>Our key insight is to pose real-time chunking as an inpainting problem. Let&#39;s say that our model takes 3 controller
timesteps to produce an action chunk after receiving an observation. The first 3 actions of a new chunk can&#39;t be
executed, since those timesteps will have already passed by the time the new chunk is available. Thus, it makes sense to
“freeze” those actions to the values from the previous chunk that we know <em>will</em> be executed; our goal is then to fill in
the remainder of the new chunk, much like inpainting a section of an image that has been removed. Depending on the chunk
size and how often we run inference, there will be some number of actions beyond the first 3 that also overlap with the
previous chunk. Rather than ignoring them completely and starting fresh, it also makes sense to <em>partially attend</em> to
these middle actions — encouraging the model to keep a consistent strategy, but allowing it to make updates based on new
information.</p></div>

<div><p>Luckily, diffusion and flow models happen to be really good at <a href="http://huggingface.co/docs/diffusers/en/using-diffusers/inpaint" target="_blank">image
inpainting</a>, even without being trained for it. By
adapting <a href="https://arxiv.org/abs/2310.04432" target="_blank">these algorithms</a> to our setting and adding our “partial attention” idea on
top, we can solve the chunk consistency problem without any training-time changes. This means we can easily apply our
method directly on top of models like <span>π<sub>0</sub></span> and <span>π<sub>0.5</sub></span>, benefiting from all the research that goes into training while
executing in real time!</p><h3 id="precision-and-speed-with-high-latency-models"><a href="#precision-and-speed-with-high-latency-models" aria-hidden="true" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Precision and speed with high-latency models</h3></div>

<div><p>We designed a set of experiments to see how real-time chunking performs compared to synchronous inference.
RTC should be faster, of course, since it eliminates the inference pauses between chunks. However, we also hypothesized that
RTC would help with <em>precision</em> — those pauses change the dynamics of the robot in a way that is not accounted for by the
model. Therefore, we tested two short, highly precise tasks (lighting a candle with a match and plugging in an Ethernet cable)
in addition to our standard longer-horizon tasks.</p><p>Part of the motivation for real-time chunking is to handle higher inference delays than we have right now. Whether from
scaling up model size, running inference on the cloud, or supporting different embodiments, latencies <em>will</em> get higher,
and handling them will become increasingly critical. Therefore, we also studied how our policies performed with +100ms and +200ms
of artificially injected latency. At +200ms, the total inference delay on a mobile manipulator is more than one-third of
a second.</p><div><table><thead><tr><th scope="col">Component</th><th scope="col">time (mobile)</th><th scope="col">time (static)</th></tr></thead><tbody><tr><th scope="row">Model</th><td>97ms</td><td>97ms</td></tr><tr><th scope="row">Network</th><td>21ms</td><td>6.9ms</td></tr><tr><th scope="row">Image resize</th><td>11ms</td><td>1.4ms</td></tr><tr><th scope="row">Other</th><td>9.7ms</td><td>3.0ms</td></tr></tbody><tfoot><tr><th scope="row">Total</th><td>139ms</td><td>108ms</td></tr></tfoot></table><p>Example latency numbers for a mobile and static robot running remote inference over the network. Even with a relatively small (3B parameter) model and a wired connection, latencies of 100ms or more are quite typical. Larger models or poorer network conditions will only make things worse.</p></div><p>Evaluating the “speed” of a policy is not trivial. We don&#39;t want to reward failure, no matter how speedy, nor do we want
to reward succeeding quickly but only rarely. We devised a <em>throughput</em> metric by dividing each task into substeps —
e.g., folding one item of clothing — and then defining throughput for an episode as the proportion of substeps completed
successfully divided by the duration of the episode. As shown below, synchronous inference suffers greatly from
increasing inference delays, whereas the performance of real-time chunking remains completely unchanged up to +200ms.</p><div><div><div><div><div><p>Average throughput (tasks/min)</p><p>Average throughput (tasks/min)</p></div></div></div><div><p>Inference delay vs. average throughput. Error bars are ±1 SEM, and each point is an average over 6 tasks for 10 episodes each (totaling 60 episodes). TE refers to temporal ensembling</p><!-- --> <p><a href="https://tonyzhaozh.github.io/aloha/">(Zhou et al., 2023)</a>, a naive smoothing method that averages together actions from multiple chunks. TE does not work at all at +100ms or +200ms — see the videos up above.</p></div></div></div><p>Of course, we expected RTC to be faster, since it eliminates the pauses between chunks. To test our hypothesis about
precision, we also measured policies by another metric: <em>controller steps</em>, or equivalently, elapsed time with the
inference pauses removed. This does not affect RTC, but gives synchronous inference an apparent boost in performance. As
shown below, the two methods measured by controller steps often reach similar final scores; however, RTC tends to make
more progress earlier in the episode, indicating that it makes fewer mistakes. Check out the <a href="https://www.pi.website/download/real_time_chunking.pdf">full
paper</a> for more in-depth results!</p><h3 id="perspectives-and-next-steps"><a href="#perspectives-and-next-steps" aria-hidden="true" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Perspectives and next steps</h3><p>VLAs and other robot foundation models must handle the physical world in real time. RTC provides a very simple and
effective strategy for real-time inference with current VLAs, and a surprising amount of resilience even to high
inference delays. There is a lot more to do here: future robot systems will need to make complex inferences at multiple
levels of abstraction and multiple time scales, plan out complex and quick dynamic movements, and pause to “think harder”
as needed. Getting this right will become more and more important as models increase in size, and the scale we need for
true physical intelligence will likely necessitate more sophisticated mechanisms for real-time inference.</p><p>If you are interested in joining us to work on these and other problems to bring physical intelligence into the real
world, <a href="https://www.pi.website/join-us">get in touch</a>! For researchers interested in our work, collaborations, or other
queries, please write to <a href="mailto:research@physicalintelligence.company" target="_blank">research@physicalintelligence.company</a>.</p></div>
</div></div>
  </body>
</html>
