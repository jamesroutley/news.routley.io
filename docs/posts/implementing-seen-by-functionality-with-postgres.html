<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://supabase.com/blog/2022/07/18/seen-by-in-postgresql">Original</a>
    <h1>Implementing &#34;seen by&#34; functionality with Postgres</h1>
    
    <div id="readability-page-1" class="page"><article><div><p><strong>tl;dr: Use HyperLogLog, it&#39;s a reasonable approach with great trade-offs and no large architectural liabilities. For a quick &amp; dirty prototype, use <code>hstore</code>, which also performs the best with integer IDs.</strong></p><p>The year is 2022. You&#39;re head DBA at the hot new social site, SupaBook... Your startup is seeing eye-boggling growth because everyone loves fitting their hot-takes in posts restricted to <code>VARCHAR(256)</code>.</p><p>Why <code>VARCHAR(256)</code>? No particular reason, but you don&#39;t have time to get hung up on that or ask why -- <strong>you just found out that the priority this quarter is tracking content views across all posts in the app</strong>.</p><p>&#34;It sounds pretty simple&#34; a colleague at the meeting remarks -- &#34;just an increment here and an increment there and we&#39;ll know which posts are seen the most on our platform&#34;. You start to explain why it will be non-trivial, but the meeting ends before you can finish.</p><p>Well, it&#39;s time to figure out how you&#39;re going to do it. There&#39;s been a complexity freeze at the company, so you&#39;re not allowed to bring in any new technology, but you don&#39;t mind that because for v1 you would have picked Postgres anyway. Postgres&#39;s open source pedigree, robust suite of features, stable internals, and awesome mascot <a href="https://www.vertabelo.com/blog/the-history-of-slonik-the-postgresql-elephant-logo/">Slonik</a> make it a strong choice, and it&#39;s what you&#39;re already running.</p><p><strong><em>(insert record scratch here)</em></strong></p><p>Sure, this scenario isn&#39;t real, but it could be - that last part about Postgres definitely is. Let&#39;s see how you might solve this problem, as that imaginary DBA.</p><h2 id="experiment-setup">Experiment setup</h2><p>We&#39;ve got the following simple table layout:</p><p><span><img alt="basic table layout diagram" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p><p>In SQL migration form:</p><pre></pre><p>This basic setup has taken the (imaginary) company quite far -- even though the <code>posts</code> table has millions and millions of entries, Postgres chugs along and serves our queries with impressive speed and reliability. Scaling up is the new (and old) scaling out.</p><h2 id="how-should-we-do-it">How should we do it?</h2><p>Well we can&#39;t pat ourselves for our miraculous and suspiciously simple DB architecture all day, let&#39;s move on to the task at hand.</p><p>Like any good tinkerer we&#39;ll start with the simplest solutions and work our way up in complexity to try and get to something outstanding, testing our numbers as we go.</p><h3 id="try-1-the-naive-way-a-simple-counter-on-every-post">Try #1: The naive way, a simple counter on every Post</h3><p>The easiest obvious way to do this is to maintain a counter on every tuple in the <code>posts</code> table. It&#39;s obvious, and it&#39;s almost guaranteed to work -- but maybe not <em>work well</em>.</p><p>The migration to make it happen isn&#39;t too difficult:</p><pre></pre><p>There&#39;s one <em>obvious</em> glaring issue here -- what if someone sees the same post twice? Every page reload would cause inflated counts in the <code>seen_by_count</code> column, not to mention a lot of concurrent database updates (which isn&#39;t necessarily Postgres&#39;s forte to begin with).</p><p>Clearly there&#39;s a better way to do things but before that...</p><h2 id="writing-a-test-suite-before-the-cpus-get-hot-and-heavy">Writing a test suite before the CPUs get hot and heavy</h2><p>How will we know which approach is better without numbers?! Measuring complexity and feeling can only get us so far -- we need to get some numbers that tell us the performance of the solution at the stated tasks -- we need benchmarks.</p><p>Before we can declare any solution the best, in particular we need a <em>baseline!</em>. The simplest possible incorrect solution (simply incrementing a counter on the Post) is probably a reasonable thing to use as a benchmark, so let&#39;s take a moment to write our testing suite.</p><p>Let&#39;s do this the simplest one might imagine:</p><ul><li>Generate a large amount of users<ul><li>Lets model for 1000, 10k, 100K, 1MM, and 10MM users</li></ul></li><li>Generate an even larger amount of fake posts attributed to those users<ul><li>This is a bit harder -- we need to define a general distribution for our users that&#39;s somewhat informed by real life...</li><li>An average/normalized distribution doesn&#39;t quite work here -- <a href="https://www.pewresearch.org/internet/2019/04/24/sizing-up-twitter-users/">on sites like twitter 10% of users create 80% of the tweets</a>!</li></ul></li><li>Generate a <em>description</em> of &#34;events&#34; that describe which post was seen by whom, which we can replay.<ul><li>We want the equivalent of an effect system or monadic computation, which is easier than it sounds -- we want to generate an encoding (JSON, probably) of <em>what to do</em>, without actually doing it</li><li>We&#39;ll just do consistent &#34;as fast as we can&#34; execution (more complicated analysis would burst traffic to be ab it closer to real life)</li></ul></li></ul><p>OK, let&#39;s roll our hands up and get it done:</p><h3 id="script-user-seeding">Script: User seeding</h3><p>Here&#39;s what that looks like:</p><pre></pre><p>Nothing too crazy in there -- we generate a bunch of JSON, and force it out to disk. It&#39;s best to avoid trying to keep it in memory so we can handle much larger volumes than we might be able to fit in memory.</p><p>If you&#39;d like to see the code, check out <a href="https://gitlab.com/mrman/supabase-seen-by/-/blob/main/scripts/generate/users.js"><code>scripts/generate/users.js</code> in the repo</a>.</p><h3 id="script-post-seeding">Script: Post seeding</h3><p>Along with users, we need to generate posts that they can view. We&#39;ll keep it simple and take an amount of posts to make, generating from 0 to <code>count</code> of those.</p><p>It&#39;s very similar to the user generation code, with the caveat that we can take into account the 80/20 lurker/poster rule. here&#39;s what that looks like:</p><p>It&#39;s a bit long so if you&#39;d like to see the code, check out <a href="https://gitlab.com/mrman/supabase-seen-by/-/blob/main/scripts/generate/posts.js"><code>scripts/generate/posts.js</code> in the repo</a>.</p><h3 id="script-action-api-call-seedinggeneration">Script: action (API call) seeding/generation</h3><p>This script is a bit tricky -- we need to inject some randomness in the performing of the following actions:</p><ul><li>Record a new view of a post</li><li>Retrieve just the count of a single post</li><li>Retrieve all the users who saw a post</li></ul><p>I&#39;ve chosen to use <a href="https://www.npmjs.com/package/autocannon"><code>autocannon</code></a> so I needed to write a request generation script which looks like this:</p><pre></pre><p>Nothing too crazy here, and some back of the envelope estimations on how often each operation would normally be called. These numbers could be tweaked more, but we <em>should</em> see a difference between approaches even if we messed up massively here.</p><p>If you&#39;d like to see the code, check out <a href="https://gitlab.com/mrman/supabase-seen-by/-/blob/main/scripts/setup-request.cjs"><code>scripts/setup-request.cjs</code> in the repo</a>.</p><h3 id="glue-it-all-together">Glue it all together</h3><p>Once we&#39;re done we need to glue this all together into one script, with roughly this format:</p><pre></pre><p>If you want to see what the code <em>actually</em> ended up looking like, check out <a href="https://gitlab.com/mrman/supabase-seen-by/-/blob/main/scripts/bench.js"><code>scripts/bench.js</code> in the repo</a>.</p><p>Along with the benchmark, we&#39;ll standardize on the following settings:</p><pre></pre><h3 id="our-first-run-on-the-naive-solution">Our first run, on the naive solution</h3><p>Alright, finally we&#39;re ready. Let&#39;s see what we get on our naive solution. We expect this to be <em>pretty fast</em>, because not only is it <em>wrong</em>, but it&#39;s just about the simplest thing you could do.</p><p>On my local machine, here&#39;s our baseline (output from <a href="https://www.npmjs.com/package/autocannon"><code>autocannon</code></a>):</p><pre></pre><p>As you might imagine, pretty darn good latency across all the requests.</p><h2 id="back-to-trying-things-out">Back to trying things out</h2><p>Now that we&#39;ve got a basic baseline of our tests, let&#39;s continue trying out ideas:</p><h3 id="try-2-storing-the-users-who-did-the-seeing-with-hstore">Try #2: Storing the users who did the &#34;see&#34;ing, with <code>hstore</code></h3><p>The next obvious thing (and probably a core requirement if we&#39;d asked around), is knowing <em>who</em> viewed each post. Well if we need to know who, then we probably need to store some more information!</p><p>Postgres has <a href="https://www.postgresql.org/docs/current/arrays.html">native support for arrays</a> and <a href="https://www.postgresql.org/docs/current/hstore.html">a data structure called a <code>hstore</code></a>, so let&#39;s try those. It&#39;s pretty obvious that having hundreds, thousands, or millions of entries in one of these data structures, inside a tuple isn&#39;t the <em>greatest</em> idea, but let&#39;s try it anyway and let the numbers speak for themselves.</p><p>Here&#39;s what the migration would look like:</p><pre></pre><p><code>hstore</code> provides support for both <a href="https://www.postgresql.org/docs/14/indexes-types.html#INDEXES-TYPE-GIST">GIST</a> and <a href="https://www.postgresql.org/docs/14/indexes-types.html#INDEXES-TYPES-GIN">GIN</a> indices, but after reading <a href="https://www.postgresql.org/docs/current/hstore.html#id-1.11.7.25.7">the documentation</a> we can conclude that we don&#39;t necessarily need those for the current set of functionality.</p><h4 id="caveats">Caveats</h4><p>Well as you might have imagined, this is obviously pretty bad and will eventually be hard to scale. If you expect only 0-50 entries in your column <code>text[]</code> is perfectly fine, but thousands or millions is another ballgame.</p><p>Thinking of how to scale this, a few ideas pop to mind:</p><ul><li>Compress our columns with <a href="https://github.com/lz4/lz4">LZ4</a> which is newly supported <a href="https://www.postgresql.org/docs/current/storage-toast.html"><code>TOAST</code> column compression</a> (I first heard about this thanks to <a href="https://www.postgresql.fastware.com/blog/what-is-the-new-lz4-toast-compression-in-postgresql-14">Fujitsu&#39;s fantastic blog post</a>)</li><li><code>PARTITION</code> our <code>posts</code> table</li></ul><h4 id="performance">Performance</h4><p>OK, time to get on with it, let&#39;s see how it performs with an <code>hstore</code>:</p><pre></pre><p>Not too far off! While we didn&#39;t try the pathological case(s) of millions of people liking the <em>same</em> post to hit breaking point, a slightly more random distribution seems to have done decently -- we actually have <em>lower</em> 99.999th percentile latency versus the simple counter.</p><p>An average of <code>2.15ms</code> versus <code>2.05ms</code> with the simpler counter is a ~4% increase in the average latency (though of course, the p99.999 is lower!).</p><h3 id="try-3-an-association-table-for-remembering-who-liked-what">Try #3: An Association table for remembering who liked what</h3><p>A likely requirement from the original scenario that we&#39;ve completely ignored is remembering <em>which</em> users liked a certain post to. The easiest solution here is an &#34;associative&#34; table like this one:</p><p><span><img alt="tables with associative table" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p><p>In SQL:</p><pre></pre><h4 id="caveats-1">Caveats</h4><p>In production, you&#39;re going to want to do a few things to make this even remotely reasonable long term:</p><ul><li><code>PARTITION</code> the table (consider using partition-friendly <a href="https://github.com/pgpartman/pg_partman"><code>pg_partman</code></a>)</li><li>Move old partitions off to slower/colder storage and maintain snapshots</li><li>Summarize older content that might be seen lots</li><li>Consider a partitioning key up front -- post IDs are probably a reasonable thing to use if they&#39;re sufficiently randomly distributed</li></ul><p>These are good initial stop-gaps, but a realistic setup will have many problems and many more solutions to be discovered.</p><p>(It will be a recurring theme but this is a spot where <em>we probably don&#39;t necessarily want to use stock Postgres</em> but instead want to use tools like <a href="https://docs.citusdata.com/en/stable/admin_guide/table_management.html#columnar-storage">Citus Columnar Storage</a>, <a href="https://github.com/greenplum-db/postgres/tree/zedstore">ZedStore</a>, or an external choice like <a href="https://supabase.com/blog/2022/07/18/clickhouse.tech/">ClickHouse</a>).</p><h4 id="performance-1">Performance</h4><p>Alright, enough dilly dally, let&#39;s run our test bench against this setup:</p><pre></pre><p>A little bit more divergence here -- 99.999%ile latency @ 30 which is almost double what it was for simple-hstore.</p><p>Average is coming in at <code>2.50ms</code> which is 16% slower than simple-hstore and 21% slower than simple-counter.</p><h3 id="try-4-getting-a-bit-more-serious-bringing-out-the-hyperloglog">Try #4: Getting a bit more serious: bringing out the HyperLogLog</h3><p><span><img alt="rest of the owl" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p><p>We&#39;ll just draw <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">the rest of the owl</a> now.</p><p>What&#39;s <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> you ask? Well it&#39;s just a probablistic data structure! Don&#39;t worry if you&#39;ve never heard of it before, it&#39;s a reasonably advanced concept.</p><p>You may have heard of <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filters</a> and they&#39;re <em>somewhat</em> related but they&#39;re not quite a great fit for the problem we&#39;re solving since we want to know how many people have seen a particular post. Knowing whether one user has seen a particular post is useful too -- but not quite what we&#39;re solving for here (and we&#39;d have to double-check our false positives anyway if we wanted to be absolutely sure).</p><p>HyperLogLog provides a probablistic data structure that is good at counting <em>distinct</em> entries, so that means that the count <em>will not</em> be exact, but be reasonably close (depending on how we tune). We won&#39;t have false positives (like with a bloom filter) -- we&#39;ll have a degree of error (i.e. the actual count may be 1000, but the HLL reports 1004).</p><p>We have to take this into account on the UI side but and maybe retrieve the full count if anyone ever <em>really</em> needs to know/view individual users that have seen the content, so we can fall back to our association table there.</p><p>Given that <a href="https://www.internetlivestats.com/twitter-statistics/">every second there are about 6000 tweets on Twitter(!)</a>, this is probably one of the only solutions that could actually work at massive scale with the limitations we&#39;ve placed on ourselves.</p><p>Here&#39;s what that looks like in SQL:</p><pre></pre><p>Here we need the <a href="https://github.com/citusdata/postgresql-hll"><code>citus/postgresql-hll</code></a> extension, which is generously made (<a href="https://github.com/citusdata/postgresql-hll/blob/master/LICENSE">truly</a>) open source by <a href="https://www.citusdata.com/">citusdata</a>.</p><p>NOTE that we still have access to the association table -- and while we still insert rows into it, we can <em>drop</em> the primary key index, and simply update our HLL (and leave ourselves a note on when we last updated it).</p><h3 id="caveats-2">Caveats</h3><p>There&#39;s not much to add to this solution, as the heavy lifting is mostly done by <code>postgresql-hll</code>, but there&#39;s one big caveat:</p><ul><li>This approach <em>will</em> need a custom Postgres image for this, since <code>hll</code> is not an official <code>contrib</code> module</li></ul><p>There are also a few optimizations that are easy to imagine:</p><ul><li>Batching inserts to the association table (storing them in some other medium in the meantime -- local disk, redis, etc)</li><li>Writing our association table entries in a completely different storage medium altogether (like object storage) and use <a href="https://www.postgresql.org/docs/current/postgres-fdw.html">Foreign Data Wrappers</a> and <a href="https://github.com/citusdata/pg_cron"><code>pg_cron</code></a> and delay or put off processing all together</li></ul><h3 id="performance-2">Performance</h3><p>The most complicated solution by far, let&#39;s see how it fares:</p><pre></pre><p>Another somewhat nuanced degradation in performance -- while the 99.99%ile latency was nearly 2x higher, the average latency was actually <em>lower</em> than the assoc-table approach @ <code>2.28ms</code>.</p><p>The average latency on the HLL approach is 11% worse than simple-counter, 6% worse than simple-hstore, and <em>faster</em> than assoc-table alone, which is an improvement.</p><h3 id="oh-the-other-places-we-could-go">Oh, the other places we could go</h3><p>One of the great things about Postgres is it&#39;s expansive ecosystem -- while Postgres may (and frankly <em>should not</em>) beat the perfect specialist tool for your use case, it often does an outstanding job in the general case.</p><p>Let&#39;s look into some more experiments that could be run -- maybe one day in the future we&#39;ll get some numbers behind these (community contributions are welcome!).</p><h4 id="incremental-view-maintenance-powered-by-pg_ivm">Incremental view maintenance powered by <code>pg_ivm</code></h4><p>If you haven&#39;t heard about <a href="https://github.com/sraoss/pg_ivm"><code>pg_ivm</code></a> it&#39;s an extension for handling Incremental View Maintenance -- updating <a href="https://www.postgresql.org/docs/14/sql-createview.html"><code>VIEW</code></a>s when underlying tables change.</p><p>IVM is a hotly requested feature whenever views (particularly materialized views) are mentioned, so there has been much fanfare to it&#39;s release.</p><p>There are a couple advantages we could gain by using <code>pg_ivm</code>:</p><ul><li>Ability to time constrain calculations (newer posts which are more likely to be seen can exist in instant-access views)</li><li>We could theoretically remove the complicated nature of the HLL all together by using <code>COUNT</code> with IVM</li></ul><p><code>pg_ivm</code> is quite new and cutting edge but looks to be a great solution -- it&#39;s worth giving a shot someday.</p><h3 id="doing-graph-computations-with-age">Doing graph computations with <a href="https://age.apache.org/">AGE</a></h3><p>As is usually the case in academia and practice, we can make our problem drastically easier by simply changing the data structures we use to model our problem!</p><p>One such reconfiguration would be storing the information as a graph:</p><p><span><img alt="graph of seen by relation" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p><p>As you might imagine, finding the number of &#34;seen-by&#34; relations would simply be counting the number of edges out of one of the nodes!</p><p>Well, the Postgres ecosystem has us covered here too! <a href="https://age.apache.org/">AGE</a> is an extension that allows you to perform graph related queries in Postgres.</p><p>We won&#39;t pursue it in this post but it would be a great way to model this problem as well -- thanks to the extensibility of Postgres, this data could live right next to our normal relational data as well.</p><h2 id="so-whats-the-best-way-to-do-it">So what&#39;s the best way to do it?</h2><p>OK, so what&#39;s the answer at the end of the day? What&#39;s the best way to get to that useful v1? Here are the numbers:</p><p><span><img alt="latency graph showing simple-hstore,hll,hll,and assoc table in speed order" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p><p>In tabular form:</p><table><thead><tr><th>Approach</th><th>Avg (ms)</th><th>99%ile (ms)</th><th>99.999%ile (ms)</th></tr></thead><tbody><tr><td>simple-counter</td><td>2.03</td><td>6</td><td>23</td></tr><tr><td>simple-hstore</td><td>2.15</td><td>6</td><td>16</td></tr><tr><td>assoc-table</td><td>2.5</td><td>8</td><td>30</td></tr><tr><td>hll</td><td>2.16</td><td>7</td><td>27</td></tr></tbody></table><p><strong>If we go strictly with the data, the best way <em>looks</em> to be the <code>hstore</code>-powered solution, but I think the HLL is probably the right choice.</strong></p><p>The HLL results were quite variable -- some runs were faster than others, so I&#39;ve taken the best of 3 runs.</p><p>Even though the data says <code>hstore</code>, knowing that posts will be seen by more and more people over time, I <em>might</em> choose the HLL solution for an actual implementation. It&#39;s far less likely to pose a bloated row problem, and it has the absolute correctness (and later recall) of the assoc-table solution, while performing better over all (as you can imagine, no need to <code>COUNT</code> rows).</p><p>Another benefit of the HLL solution is that <a href="https://www.postgresql.org/docs/current/manage-ag-tablespaces.html">PostgreSQL tablespaces</a> allow us to put the association table on a different, slower storage mechanism, and keep our <code>posts</code> table fast. Arguably in a real system we might have the HLL in something like <code>redis</code> but for a v1, it looks like Postgres does quite well!</p><p>I hope you enjoyed this look down the trunk hole, and you&#39;ve got an idea of how to implement solutions to surprisingly complex problems like this one with Postgres.</p><p>As usual, Postgres has the tools to solve the problem <em>reasonably</em> well (if not completely) before you reach out for more complicated/standalone solutions.</p><p><strong>See any problems with the code, solutions that haven&#39;t been tried? -- reach out, or open an issue!</strong></p><h2 id="more-postgres-resources">More Postgres resources</h2><ul><li><a href="https://supabase.com/blog/2022/06/28/partial-postgresql-data-dumps-with-rls">Partial data dumps using Postgres Row Level Security</a></li><li><a href="https://supabase.com/blog/2020/11/18/postgresql-views">Postgres Views</a></li><li><a href="https://supabase.com/blog/2022/03/08/audit">Postgres Auditing in 150 lines of SQL</a></li><li><a href="https://supabase.com/blog/2021/02/27/cracking-postgres-interview">Cracking PostgreSQL Interview Questions</a></li><li><a href="https://supabase.com/blog/2020/07/09/postgresql-templates">What are PostgreSQL Templates?</a></li><li><a href="https://supabase.com/blog/2021/12/01/realtime-row-level-security-in-postgresql">Realtime Postgres RLS on Supabase</a></li></ul></div></article></div>
  </body>
</html>
