<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://organicdonut.com/2024/06/cuda-four/">Original</a>
    <h1>CUDA – Four</h1>
    
    <div id="readability-page-1" class="page"><div>
			
<p>I’ve been busy with other things, but I woke up early and decided to get some CUDA studying in. I did talk with the hiring manager for the position that I’m interested in, who (as I expected) clarified that I didn’t actually need to know CUDA for this position. I’m still interested, though I should focus more on the <a href="https://leetcode.com/">Leetcode</a>-style exercises that are more likely to come up on the interivew.</p>



<p>That said, I haven’t been entirely ignoring this. I’ve been watching some 3Blue1Brown videos in my spare time, like <a href="https://www.youtube.com/watch?v=IaSGqQa5O-M">this one on convolution</a>. My calculus is definitely rusty (I don’t fully remember how to take an integral), but I’m mostly just trying to gain some intuition here so that I know what people are talking about if they say things like, “take a convolution”.</p>



<p>For today, I started by looking through the <a href="https://github.com/NVIDIA/cuda-samples/blob/master/Samples/0_Introduction/asyncAPI/asyncAPI.cu">source of the sample code</a> I got running <a href="https://organicdonut.com/2024/06/cuda-three/">last time</a>. Thanks to the book I’ve been reading, a lot of the code makes sense and I feel like I can at least skim the code and understand what’s going on at a syntax level, for example:</p>



<pre><code>__global__ void increment_kernel(int *g_data, int inc_value) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  g_data[idx] = g_data[idx] + inc_value;
}</code></pre>



<p>Writing this mostly for my own understanding: </p>



<p>The <code>__global</code> identifier marks this as a <em>Kernel</em> – code that is called from the host but runs on the device. It takes in a pointer to an array <code>g_data</code> and an int <code>inc_value</code>. This kernel will be run for each element in the <code>g_data</code> array and each instance of the kernel will operate on the element calculated in <code>idx</code>. Each thread block of <code>blockDim</code> threads will have a unique <code>blockIdx</code> and each thread in that block will have a unique <code>threadIdx</code>. Since we are working on 1D data (i.e. a single array, and not a 2D or 3D array), we only care about the <code>x</code> property of each of these index variables. Then, we increment the value at index <code>idx</code> by the <code>inc_value</code>.</p>



<p>Ok, writing this up I think I have one question, which is about the <code>.x</code> property. The book explains that you can use the <code>.x, .y, .z</code> properties to easily split up 2D or 3D data, but also talks about ways to turn 2D or 3D data into a 1D representation. So are the <code>.y, .z</code> properties just “nice” because they allow us to leave 2D data as 2D, or do they actually allow us to do something that re-representing the 2D data as 1D data and just using <code>.x</code> doesn’t?</p>



<p>Ok, continuing on:</p>



<pre><code>int main(int argc, char *argv[]) {
  int devID;
  cudaDeviceProp deviceProps;

  printf(&#34;[%s] - Starting...\n&#34;, argv[0]);</code></pre>



<p>Start the main function and set up some variables, as well as letting the user know that we’re starting.</p>



<pre><code>
  // This will pick the best possible CUDA capable device
  devID = findCudaDevice(argc, (const char **)argv);

  // get device name
  checkCudaErrors(cudaGetDeviceProperties(&amp;deviceProps, devID));
  printf(&#34;CUDA device [%s]\n&#34;, deviceProps.name);</code></pre>



<p>Some questions here. What does it mean by “best”? Fortunately, <a href="https://github.com/NVIDIA/cuda-samples/blob/master/Common/helper_cuda.h#L867">the source for findCudaDevice is available to us</a>. First it checks to see if a device is specified by command line flag, and if not, grabs the device “with “with highest Gflops/s”.</p>



<pre><code>  int n = 16 * 1024 * 1024;
  int nbytes = n * sizeof(int);
  int value = 26;

  // allocate host memory
  int *a = 0;
  checkCudaErrors(cudaMallocHost((void **)&amp;a, nbytes));
  memset(a, 0, nbytes);</code></pre>



<p>Setting some variables first, but then we allocate some host memory. I was curious about <code>cudaMallocHost</code>. In the other examples I’d seen, host memory was usually created by just using <code>malloc</code> (or simply assumed to already be allocated, in the book). <code>cudaMallocHost</code> creates “pinned” memory, which is locked into RAM and is not allowed to swap. This allows us to use e.g. <code>cudaMemcpy</code> without the performance overhead of constantly checking to make sure that the host memory has not been swapped to disk.</p>



<p>I’m still not used to the C convention of handling errors via macros like <code>checkCudaErrors</code> instead of language constructs like <code>try/catch</code> or <code>if (err != nil)</code>. It just <em>feels</em> like an obsolete way of doing error handling that’s easy to forget.</p>



<p>That’s all I had time for this morning, but it’s fun to understand more and more about this as I continue to learn!</p>
					</div></div>
  </body>
</html>
