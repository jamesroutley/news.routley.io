<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sakana.ai/dgm/">Original</a>
    <h1>The Darwin Gödel Machine: AI that improves itself by rewriting its own code</h1>
    
    <div id="readability-page-1" class="page"><article>
  <header>
  <time datetime="2025-05-30T00:00:00+09:00">May 30, 2025</time>
</header>

  <p><img src="https://sakana.ai/assets/dgm/dgm-main.png" width="100%"/><br/>
<!--<img class="b-lazy" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-main.png" style="width: 100%;"/><br/>--></p>

<!--more-->

<h2 id="summary">Summary</h2>

<p>A <a href="https://www.cs.mcgill.ca/~dprecup/courses/AI/Materials/turing1950.pdf">longstanding goal</a> of AI research has been the creation of AI that can learn <em>indefinitely</em>. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a <a href="https://en.wikipedia.org/wiki/G%C3%B6del_machine">Gödel Machine</a>, proposed by <a href="https://people.idsia.ch/~juergen/goedelmachine.html">Jürgen Schmidhuber</a> decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in <a href="https://people.idsia.ch/~juergen/metalearning.html#secME">meta-learning</a> or “learning to learn.”</p>

<p>While the theoretical Gödel Machine promised <em>provably</em> beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically <em>prove</em> that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune’s lab at UBC, propose something more feasible: a system that harnesses the <em>principles</em> of open-ended algorithms like Darwinian evolution to search for improvements that <em>empirically</em> improve performance.</p>

<p>We call the result the <strong>Darwin Gödel Machine</strong> (<a href="https://arxiv.org/abs/2505.22954">full technical report</a>). DGMs leverage foundation models to <a href="https://arxiv.org/abs/2206.08896">propose code improvements</a>, and use <a href="https://arxiv.org/abs/2408.08435">recent</a> <a href="https://arxiv.org/abs/2306.01711">innovations</a> in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with <a href="https://arxiv.org/abs/1905.10985">the clear trend</a> that AI systems that rely on <em>learning</em> ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-animation.gif"/></p>

<video autoplay="" muted="" playsinline="" loop=""><source src="/assets/dgm/dgm-code-evolution-smaller.mp4"/></video>

<p>For further details please read our <a href="https://arxiv.org/abs/2505.22954">Technical Report</a> and <a href="https://github.com/jennyzzt/dgm">released code</a>.</p>



<h2 id="introduction">Introduction</h2>

<p>Most current AI systems learn during training only. Then their intelligence is locked in place and deployed. Could they instead, like humans, or the entire community of human scientists, continue to learn and self-improve <em>forever</em>? Moreover, could such self-improvement catalyze future self-improvement?</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-conceptual.png"/></p>

<p>Our <em>**Darwin Gödel Machine (DGM) **</em>is a step in that direction. Although we believe the full potential of self-modification is much broader than the capabilities offered by existing agentic systems, DGMs can also be applied to practical, <a href="https://arxiv.org/abs/2408.08435">agentic tasks</a>, which combine foundation models with tools, such as web search, or workflows, such as creating three potential answers and choosing the best one. This first DGM is a coding agent that has the ability to:</p>

<ol>
  <li>
    <p><strong>Read and Modify Its Own Code</strong>: It understands and can modify its own Python codebase to try to self-improve (e.g., adding a new tool, or suggesting a different workflow).</p>
  </li>
  <li>
    <p><strong>Evaluate if the Change Improves Performance</strong>: Proposed new versions of itself are evaluated on coding benchmarks (like <a href="https://www.swebench.com/original.html">SWE-bench</a> and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a>). As our results show, improved performance on coding challenges means it has also gotten better at improving itself.</p>
  </li>
  <li>
    <p><strong>Open-endedly Explore the AI Design Space</strong>: New agents are added to an ever-expanding archive of interesting agents. Harnessing <a href="https://books.google.ca/books?id=Llb1CAAAQBAJ&amp;printsec=frontcover&amp;source=gbs_atb&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">the</a> <a href="https://www.nature.com/articles/s42256-018-0006-z">power</a> <a href="https://arxiv.org/abs/1905.10985">of</a> <a href="https://arxiv.org/abs/2408.08435">open</a>-<a href="https://arxiv.org/abs/2306.01711">ended</a> <a href="https://sakana.ai/ai-scientist/">algorithms</a>, future self-modifications can then branch off from any agent in this growing archive, allowing for parallel exploration of many different evolutionary paths. This open-ended exploration helps DGM discover truly novel solutions and avoid getting trapped in suboptimal designs.</p>
  </li>
</ol>

<p>If done safely (see our section dedicated to safety below), such self-improving AI could help us take advantage of the <a href="https://www.darioamodei.com/essay/machines-of-loving-grace">tremendous benefits</a> for society that AI has the potential to usher in.</p>



<h2 id="results">Results</h2>

<p>Experiments demonstrate the Darwin Gödel Machine can continuously self-improve by modifying its own codebase. That is true both on <a href="https://www.swebench.com/original.html">SWE-bench</a> (a widely used benchmark requiring agents to resolve real-world GitHub issues) and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a> (a multi-language coding benchmark). On SWE-bench, the DGM automatically improved its performance from 20.0% to 50.0%. On Polyglot, the DGM jumped performance from an initial 14.2% to 30.7%, which far surpasses the representative hand-designed agent by <a href="https://aider.chat">Aider</a>. These substantial gains demonstrate the DGM’s ability to discover and implement beneficial changes to its own code.</p>

<p>We also experimentally validate that the DGM’s ability to modify its own code speeds up its own learning (see below). A control without self-improvement performs far worse (DGM w/o Self-improve in the below figure). Moreover, experiments also show that harnessing the principles of open-ended algorithms was also key: without the Darwinian-evolution-inspired search based on growing an archive of different stepping stones, thus allowing <a href="https://www.researchgate.net/publication/277131571_Innovation_Engines_Automated_Creativity_and_Improved_Stochastic_Optimization_via_Deep_Learning">goal switching</a> and the exploration of many different paths in parallel, performance was lower (DGM w/o Open-ended exploration in the below figure).</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-self-improve.png"/></p>

<p>The lineage of DGM’s self-improvements on SWE-Bench is visualized in the figure below, which depicts the evolutionary tree of agents generated within its growing archive. This branching structure is a direct consequence of DGM’s open-ended exploration strategy, where it samples from a diverse pool of previously generated agents to seed new modifications. This process enables DGMs to explore multiple evolutionary pathways simultaneously. We observed that some less-performant “ancestor” agents, which might have been discarded by simpler hill-climbing optimization, were instrumental in discovering novel features or approaches that led to significant performance breakthroughs in their descendants, thereby avoiding premature convergence on suboptimal solutions.</p>

<p>The following 2 figures illustrate how the Darwin Gödel Machine automatically self-improves to become a better coding agent:</p>

<center>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-archive-tree.png"/></center>
<p><small><i>Archive of coding agents generated during the DGM run on SWE-bench.</i></small><br/></p>
<center>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-progress.png"/></center>
<p><small><i>Performance progress and a summary of key innovations of the DGM on SWE-bench. The dark line shows the lineage of the final best-discovered agent. The DGM discovers different tools for improved file editing and a patch strategy that combines multiple generations with ranking. Note that the path to the final best agent sometimes involved agents that performed worse than their parents, highlighting the benefits of open-ended search, including keeping an archive of interesting stepping stones to explore from, rather than only branching off from the highest-performing agent discovered so far.</i></small></p>



<p>Crucially, the DGM engineered improvements within its own agent design that proved to be fundamental and broadly transferable, not merely adaptations overfit to its model or task. The improvements discovered by the DGM (e.g., better tools, refined workflows) generalize to produce higher performance across different underlying FMs. For example, an agent optimized with Claude 3.5 Sonnet also showed improved performance when powered by o3-mini or Claude 3.7 Sonnet (left two panels in the figure below). This shows that the DGM discovers general agent design improvements rather than just model-specific tricks.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-transfer.png"/></p>

<p>This transferability was also evident across programming languages. A DGM variant whose self-improvement process was exclusively guided by its performance on Python tasks within the Polyglot benchmark subsequently demonstrated significant performance gains on tasks in entirely different programming languages (like Rust, C++, and Go) from the same benchmark (right panel in above figure).</p>

<p>These results collectively show that the Darwin Gödel Machine, through its self-referential code modification and open-ended exploration, can autonomously discover and implement increasingly sophisticated and generalizable improvements to AI agents.</p>



<h2 id="dgm-and-ai-safety-building-trustworthy-self-improvement">DGM and AI Safety: Building Trustworthy Self-Improvement</h2>

<p>The prospect of AI systems that autonomously enhance their own capabilities naturally brings the <a href="https://managing-ai-risks.com/">important topic</a> of AI Safety to the forefront. When an AI can rewrite its own code, it is crucial that its development is safe and aligns with human intentions. Modifications optimized solely for improving performance on a benchmark could introduce unintended behaviors or create systems too complicated for manual human oversight.</p>

<p>From the outset, DGM has been developed with safety in mind. All self-modifications and evaluations occur within secure, sandboxed environments, under human supervision and with strict limits on access to the web. The DGM archive also provides a transparent, traceable lineage of every change, and our reviews of the changes that occurred in our experiments show modifications focused solely on improving coding capabilities. Please see our <a href="https://arxiv.org/abs/2505.22954">paper</a> for a more thorough review of our safety practices and findings.</p>

<p>Beyond mitigating risks, self-improvement itself could offer a novel pathway to <em>enhance</em> AI safety. In preliminary investigations (detailed in our <a href="https://arxiv.org/abs/2505.22954">paper</a>), we tasked DGM with addressing its own tendency to sometimes pretend to use external tools instead of actually using them (including making up the output from the tools). The results are encouraging: DGM began to identify and propose solutions to these problems.</p>

<p>However, we did notice, and documented in our paper, instances when the DGM hacked its reward function. For example, we had cases where it hallucinated that it was using external tools, such as a command line tool that runs unit tests that determine if the code is functioning properly. It faked a log making it look like it had run the tests and that they had passed, when in fact they were never run! Because these logs become its context, it later mistakenly thought its proposed code changes had passed all the unit tests.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-hallucination.png"/></p>

<p>We conducted an experiment to see if DGM could fix this issue. We created a “tool use hallucination” reward function and asked the DGM to optimize that score to solve the hallucination issue. In many instances, DGM did provide solutions that addressed this type of hallucination. However, in some cases, it removed the markers we use in the reward function to detect hallucination (despite our explicit instruction not to do so), hacking our hallucination detection function to report false successes. We were able to detect that because DGM provides a transparent, traceable lineage of every change that allows us to quickly catch such undesirable behaviors. While in this case the issue was easily detected, more work is needed to prevent the model from attempting to cheat in this way in the first place.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-objective-hacking.png"/></p>

<p>Most broadly, we think safety should be front and center in any research on self-improving AI. The community needs to continue to research how to explore this promising, powerful, area responsibly, including creating systems that are aligned throughout their self-improvement, or even enhance their own safety, transparency, and alignment. These are key research directions we are committed to exploring further.</p>



<h2 id="conclusion">Conclusion</h2>

<p>Darwin Gödel Machines represent a concrete step towards AI systems that can autonomously gather their own stepping stones to learn and innovate forever. Future work will involve scaling up the approach and even letting it improve the training of the foundation models at its core. We must prioritize safety in this research because, if we can explore this direction safely, it has the possibility to unlock untold benefits for society, including enabling us to reap the benefits of accelerated scientific progress much sooner.</p>



<hr/>

<h3 id="citation">Citation</h3>

<p>Darwin Gödel Machine: Open-Ended Evolution of Self-Improving Agents</p>

<p>Jenny Zhang (※), Shengran Hu (※), Cong Lu, Robert Lange (†), Jeff Clune (†)</p>

<p>(※) co-first author</p>

<p>(†) co-senior author</p>

<p>Paper:  <a href="https://arxiv.org/abs/2505.22954">https://arxiv.org/abs/2505.22954</a></p>

<p>Code: <a href="https://github.com/jennyzzt/dgm">https://github.com/jennyzzt/dgm</a></p>

<hr/>




<center>
<a href="https://sakana.ai/careers/"><img src="https://sakana.ai/assets/dgm/dgm-fish.jpeg" width="80%"/></a><br/>
</center>



<h2 id="sakanaai">Sakana AI</h2>

<p>Want to make the AI that improves AI? Please see our <a href="https://sakana.ai/careers/">Careers</a> page for more information.</p>



  
</article></div>
  </body>
</html>
