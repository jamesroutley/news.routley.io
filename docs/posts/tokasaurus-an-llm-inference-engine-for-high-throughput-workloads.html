<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scalingintelligence.stanford.edu/blogs/tokasaurus/">Original</a>
    <h1>Tokasaurus: An LLM inference engine for high-throughput workloads</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div id="center">
      


      <div id="paper">
        <div id="content">
          
          

          

          <hr/>

          
            
              <p><img src="https://scalingintelligence.stanford.edu/imgs/teasers/tokasaurus.png" alt=""/>
                

              </p>
            

            

<p>We’re releasing Tokasaurus, a new LLM inference engine optimized for throughput-intensive workloads. With small models, Tokasaurus benefits from very low CPU overhead and dynamic <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> grouping to exploit shared prefixes. For larger models, Tokasaurus supports async tensor parallelism for GPUs with NVLink and a fast implementation of pipeline parallelism for GPUs without. On throughput-focused benchmarks, Tokasaurus can outperform vLLM and SGLang by up to 3x+.</p>

<hr/>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#intro">Intro</a></li>
  <li><a href="#optimizing-small-models">Optimizing Small Models</a></li>
  <li><a href="#optimizing-bigger-models">Optimizing Big Models</a></li>
  <li><a href="#try-it-out">Try it Out</a></li>
  <li><a href="#benchmarking-details">Benchmarking Details</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<hr/>

<h2 id="intro">Intro</h2>

<p>As LLMs get <a href="https://www.youtube.com/watch?v=gAjR4_CbPpQ">smarter, faster, and cheaper,</a> the community keeps finding new ways to use them. Our own recent work has explored using models to <a href="https://arxiv.org/abs/2501.14723">scan every file in a codebase</a>, <a href="https://arxiv.org/abs/2407.21787">sample 10,000 attempts for math and code problems</a>, and <a href="https://arxiv.org/abs/2502.15964">collaborate with other models to minimize cloud costs</a>. Inference is now also an important part of the training process, where we use models to <a href="https://arxiv.org/abs/2404.14219">generate synthetic data</a> or as part of <a href="https://arxiv.org/abs/2504.04736">RL pipelines</a> that generate and train on model completions.</p>

<p>Crucially, these new inference workloads look quite different than the original LLM use case of serving a chatbot. Here, we care primarily about the total time and cost required to complete a large batch of sequences, and we care much less (if at all) about the individual latency of a single generation. In other words, we want high throughput!</p>

<p>Open-source inference engines (i.e. dedicated systems for running efficient LLM inference) like <a href="https://arxiv.org/abs/2303.06865">FlexGen</a>, <a href="https://arxiv.org/abs/2309.06180">vLLM</a>, and <a href="https://arxiv.org/abs/2312.07104">SGLang</a> have been enormously valuable to the community. Inspired by and learning from these projects, we built a new engine, <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">Tokasaurus</a>, designed from the ground up to handle throughput-focused workloads. We’ve optimized Tokasaurus for efficiently serving large and small models alike, allowing it to outperform existing engines on throughput benchmarks. In the rest of this blog, we’ll walk through some of these optimizations and show off a few settings where Tokasaurus really shines.</p>

<hr/>

<h2 id="optimizing-small-models">Optimizing Small Models</h2>

<p>To benchmark Tokasaurus with small models, we’ll use two workloads:</p>

<ul>
  <li>Completing chatbot prompts from the ShareGPT dataset (this is a common benchmark for testing inference engines).</li>
  <li>Reproducing an experiment from <a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys</a>, where we take 128 problems from the <a href="https://arxiv.org/abs/2110.14168">GSM8K</a> math dataset and sample 1024 answers to every problem. The distinguishing feature of this workload is that there’s a lot of prefix sharing across sequences.</li>
</ul>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/small.png" alt="Tokasaurus small models"/>
  <img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/monkeys.png" alt="Tokasaurus large batch sampling"/>
</p>

<p>Tokasaurus outperforms vLLM and SGLang on both of these benchmarks, in particular achieving over 2x the throughput of other engines on the Large Language Monkeys workload. Two main features contribute to these wins with small models:</p>

<h3 id="minimizing-cpu-overhead">Minimizing CPU Overhead</h3>

<p>LLM engines perform many different tasks on the CPU, like handling web requests, tokenizing inputs/detokenizing outputs, managing KV cache allocation, and preparing inputs for the model. If these CPU-side tasks cause the GPU-side model to stall, throughput can take a <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">big hit</a>. To avoid these stalls, inference engines commonly make many CPU-side <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">tasks</a> <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">asynchronous</a>: while the GPU runs a forward pass for batch N, the CPU-side of the engine post-processes the results from batch N-1 and prepares the inputs for batch N+1.</p>

<p>Tokasaurus goes one step further, making the CPU-side of the engine (what we call the manager) both asynchronous and adaptive. The manager’s goal is to maintain a deep queue of inputs for the model to run forward passes on. The manager monitors the size of this queue and can detect if the model is close to exhausting it (and therefore stalling the GPU). In these cases, the manager will automatically start skipping optional steps (like checking for stop strings and onboarding new sequences) until the model’s input queue is sufficiently deep again. This combination of asynchrony and adaptivity lets Tokasaurus serve small models with much less CPU overhead.</p>

<h3 id="dynamic-prefix-identification-and-exploration">Dynamic Prefix Identification and Exploration</h3>

<p>Prefix sharing comes up all the time in LLM inference — not just when repeatedly sampling like in the Large Language Monkeys benchmark, but also when asking many questions about a long document or reusing a system prompt across many chatbot conversations.</p>

<p>Shared prefixes allow attention to be computed more efficiently. We first explored this idea last year with <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> (aka <a href="https://flashinfer.ai/2024/02/02/cascade-inference.html">cascade attention</a> and <a href="https://arxiv.org/abs/2403.08845">bifurcated attention</a>), but at the time we didn’t address the problem of detecting these shared prefixes in an engine where sequences are constantly starting and finishing. With Tokasaurus, we solve this detection problem by running a greedy depth-first search algorithm before every model forward pass that iteratively finds the longest shared prefixes possible. Hydragen is most impactful for small models, which spend a relatively larger fraction of total FLOPs on attention.</p>

<hr/>

<h2 id="optimizing-bigger-models">Optimizing Bigger Models</h2>

<p>Tokasaurus can also efficiently serve bigger models across multiple GPUs! Here, the most important optimizations are our implementations of pipeline parallelism (PP) and tensor parallelism (TP), which allow us to maximize throughput on GPUs with or without NVLink.</p>

<h3 id="pipeline-parallelism-for-the-gpu-poor">Pipeline Parallelism for the GPU Poor</h3>

<p>One of our original goals with Tokasaurus was to efficiently run multi-GPU inference on our lab’s L40S GPUs, which don’t have fast inter-GPU NVLink connections. Without NVLink, the communication costs incurred running TP across a node of eight GPUs are substantial. Therefore, efficient support for PP (which requires much less inter-GPU communication) was a high priority. PP needs a large batch in order to run efficiently, since batches from the manager are subdivided into microbatches that are spread out across pipeline stages. When optimizing for throughput, we’re generally already using the largest batch size that fits in GPU memory, so PP is often a natural fit for throughput-focused workloads. When benchmarking against vLLM’s and SGLang’s pipeline parallel implementations using Llama-3.1-70B on eight L40S GPUs, Tokasaurus improves throughput by over 3x:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/pipeline.png" alt="Tokasaurus small models"/>
</p>

<h3 id="async-tensor-parallel-for-the-gpu-rich">Async Tensor Parallel for the GPU Rich</h3>

<p>If you do have GPUs with NVLink (e.g. B200s and certain models of H100s and A100s), Tokasaurus has something for you too! Models in Tokasaurus can be torch compiled end-to-end, allowing us to take advantage of <a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">Async Tensor Parallelism (Async-TP)</a>. This is a relatively new feature in PyTorch that can overlap inter-GPU communication with other computations, partially hiding the cost of communication. In our benchmarks, we found that Async-TP adds a lot of CPU overhead to the model forward pass and only starts improving throughput with very large batch sizes (e.g. 6k+ tokens). Tokasaurus maintains torch-compiled versions of our models with and without Async-TP enabled, allowing us to automatically switch on Async-TP whenever the batch size is big enough:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/big.png" alt="Tokasaurus small models"/>
</p>

<hr/>

<h2 id="try-it-out">Try it Out</h2>

<p>Tokasaurus started as an internal lab effort to run our inference experiments faster, and we’re excited to share it more broadly! You can check out the Tokasaurus code on <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">GitHub</a> and install the package from PyPI with:</p>



<p>Currently, we support models from the Llama-3 and Qwen-2 families and support any combination of data, tensor, and pipeline parallel within a single node.</p>

<p>Tokasaurus is written in pure Python (although we do use attention and sampling ops from the excellent <a href="https://arxiv.org/abs/2501.01005">FlashInfer</a> package). We hope that this makes the engine easier to fork and hack on, à la <a href="https://github.com/pytorch-labs/gpt-fast">GPT-fast</a>.</p>

<h2 id="benchmarking-details">Benchmarking Details</h2>

<p>The commands for reproducing our benchmarks are available <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/main/logs/blog_commands.md">here</a>. For each benchmark, we configure all engines with the same KV cache size and maximum number of running requests. We’ve made a best effort to tune each engine’s remaining parameters. We report the average throughput across runs after completing a warmup run. For each benchmark, all engines are run on the same machine.</p>

<p>We use <a href="https://github.com/sgl-project/sglang/blob/7e257cd666c0d639626487987ea8e590da1e9395/python/sglang/bench_serving.py">this script</a> from SGLang for our ShareGPT benchmarks and <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/monkeys_gsm8k.py">this custom script</a> for the Large Language Monkeys benchmark. To standardize our benchmarking scripts and interface, all experiments send requests through the OpenAI API. We also experimented with vLLM’s Python API (i.e. <code>LLM.generate()</code>) on the Large Language Monkeys benchmark with Llama-1B and measured roughly a 5% throughput increase (thanks to the vLLM team for the tip!).</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Huge thanks to <a href="https://www.primeintellect.ai/">Prime Intellect</a> and <a href="https://www.together.ai/">Together AI</a> for providing us with compute for this project.</p>

<p>Also, we’re grateful to Dan Biderman, Simon Guo, Manat Kaur, and Avanika Narayan for beta testing the engine!</p>

<hr/>

<p>If you find Tokasaurus useful, please use the following citation:</p>

<div><div><pre><code><span>@misc</span><span>{</span><span>juravsky2025tokasaurus</span><span>,</span>
  <span>author</span>       <span>=</span> <span>{Jordan Juravsky and Ayush Chakravarthy and Ryan Ehrlich and Sabri Eyuboglu and Bradley Brown and Joseph Shetaye and Christopher R{\&#39;e} and Azalia Mirhoseini}</span><span>,</span>
  <span>title</span>        <span>=</span> <span>{Tokasaurus: An LLM Inference Engine for High-Throughput Workloads}</span><span>,</span>
  <span>year</span>         <span>=</span> <span>{2025}</span><span>,</span>
  <span>howpublished</span> <span>=</span> <span>{\url{https://scalingintelligence.stanford.edu/blogs/tokasaurus/}}</span>
<span>}</span>
</code></pre></div></div>


          
        </div>

        
      </div>
    </div>
  </div></div>
  </body>
</html>
