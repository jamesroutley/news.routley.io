<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.recall.ai/post/debugging-audio-artifacts-caused-by-a-serial-port">Original</a>
    <h1>Debugging audio artifacts caused by... a serial port?</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><h2>The joys of scaling a 10,000 node cluster</h2>
<p>At Recall.ai we run enormous infrastructure to process millions of meetings per month, in real-time.</p>
<p>At peak load, our cluster contains over 10,000 VMs at processing over 1TB / sec of raw video.</p>
<p>The first version of our cluster ran on Kubernetes, using Amazon&#39;s managed EKS control plane with self-managed nodes.</p>
<p>During 2023 we grew extremely rapidly and EKS&#39;s control plane could not keep up with our spiky load patterns, so we migrated off Kubernetes and wrote our own cluster scaler to replace it.</p>
<p>I deployed the migration on Friday night, and over Saturday and Sunday everything seemed to be running perfectly. </p>
<p>Because our customers do significantly more meetings on weekdays, we time our major deploys for Friday nights so we can monitor performance during the low-utilization periods on the weekend.</p>
<p>However, on Monday, we started getting customer reports that there were popping noises in the captured audio.</p>
<h2>The least-expected failure mode</h2>
<p>Out of all the ways this migration could have caused problems, this is <em>not</em> what I was expecting. Our audio recording code is completely unrelated to our cluster scaler.</p>
<p>On top of that, our new cluster runs exactly the same Docker containers as the Kubernetes cluster. Not a single line of application code was changed.</p>
<p>But we had not deployed any other changes, and there were zero affected bots in our un-migrated clusters. </p>
<p><img alt="Huh?" src="https://cdn.prod.website-files.com/633275e23914a500db413038/670d910b6d7a03bf0a923c4b_19b7bc2c.jpeg"/></p>
<h2>Problem solved?</h2>
<p>In the past, I had seen audio artifacts like popping sounds appearing during periods of CPU starvation.</p>
<p>There are a few processes in the bot that are especially latency sensitive, which we have tuned the <code>nice</code> value for. </p>
<p>No CPU spikes were showing up in our monitoring, but I hypothesized that there could be spikes too brief to show up in our coarse grained metrics. </p>
<p>I dug into this theory and struck gold almost immediately. The way I was setting <code>nice</code> <em>was</em> incorrect and priority wasn&#39;t being applied to the appropriate threads.</p>
<p>This was a quick fix, and I deployed the change into production. </p>
<p>Problem solved?</p>
<p>Unfortunately not. The next day, even more customers reported audio artifacts.</p>
<h2>Spooky action-at-a-distance</h2>
<p>So far, I had not been able to reproduce this issue at all. </p>
<p>Our customers confirmed the issue was definitely real, but none of the dozens of bots I tested had any audio artifacts whatsoever.</p>
<p>I found a breakthrough when I tried to launch bots with precisely the same config as one of the affected bots. With that specific config, one bot in a dozen had popping audio.</p>
<p>I bisected the config options, and through a trial-and-error-binary-search determined that enabling <a href="https://docs.recall.ai/docs/real-time-transcription"><em>real time transcription</em></a> was the critical factor in causing these artifacts to appear.</p>
<p>This was an extremely bizarre result. Transcription happens in a completely different part of the code from audio encoding and it is not particularly CPU intensive. </p>
<p>On top of that, the bots do transcription by sending the audio over WebSocket to <a href="https://www.recall.ai/partners">3rd party transcription partners</a>, so nothing transcription-related runs in our cluster anyway.</p>
<p><img alt="???" src="https://cdn.prod.website-files.com/633275e23914a500db413038/670d910b6d7a03bf0a923c4f_bbcf74e5.jpeg"/></p>
<h2>Spot the difference</h2>
<p>After figuring out a way to reproduce the issue, I went through our infrastructure with a fine-tooth comb. </p>
<p>There was <em>something</em> different between our new infrastructure and our old infrastructure that was causing a problem, and I just needed to figure out what that difference was.</p>
<p>The only difference I found was extremely far removed from our audio processing. The thought of this difference being the root cause of our audio artifacts was laughable. </p>
<p>Our logging system was different.</p>
<p>In our old Kubernetes cluster, we used the built-in Kubernetes logging infrastructure.</p>
<p>In our new self-built cluster, we send logs over a unix socket with Fluentbit.</p>
<p>Presumably, if Fluentbit logging was the culprit, disabling Fluentbit would resolve the problem.</p>
<p>So I disabled Fluentbit logging.</p>
<p>Launched a fresh bot.</p>
<p>And still heard popping audio.</p>
<h2>A lucky break</h2>
<p>Thoroughly confused, I spent a few hours just poking around on the new machines.</p>
<p>I noticed offhand that the Docker logs were ending up in SystemD&#39;s journal logs. </p>
<p>This was mysterious because I did not configure Docker to log to SystemD&#39;s journal.</p>
<p>Digging in to this, I realized this was happening because I run the container from a Docker command inside of our EC2 instance&#39;s <code>cloud-init</code>.</p>
<p>The <code>cloud-init</code> output gets redirected to a few places, including SystemD, as well as an AWS internal system that allows you to get system logs from a running instance.</p>
<p><img alt="The AWS feature in question." src="https://cdn.prod.website-files.com/633275e23914a500db413038/670d910b6d7a03bf0a923c52_57ace9fa.jpeg"/></p>
<h2>Back-back-back-back-pressure</h2>
<p>This was a thread to follow. Maybe, there was something in SystemD or in AWS&#39;s <code>cloud-init</code> logging system that was blocking somewhere.</p>
<p>So I tested this theory, that <code>cloud-init</code> logging was the root cause of our problems.</p>
<p>I disabled <code>cloud-init</code> logging.</p>
<p>Launched a fresh bot.</p>
<p>And heard pure and clear audio, without a hint of popping.</p>
<p>Success! Finally, after several hours, a dozen deploys, and chasing a ghost up and down several levels of the stack, I was close to finding the root cause.</p>
<p>Now that I had a single parameter that I confirmed was triggering the issue, I dove in to discover the root cause.</p>
<p>Comparing the <code>journalctl</code> outputs of affected and unaffected bots, I saw the following log line repeated over and over again in every bot that produced audio artifacts:</p>
<pre><code>...
serial8250: too much work for irq4
serial8250: too much work for irq4
serial8250: too much work for irq4
serial8250: too much work for irq4
serial8250: too much work for irq4
...
</code></pre>
<p>I searched this log line in Google, and found <a href="https://github.com/amzn/amzn-drivers/issues/84#issuecomment-424652979">the following delightful GitHub thread.</a>.</p>
<p>It turns out, that EC2&#39;s console logging system writes to a virtual serial device.</p>
<p>And if you write too much data to that serial device, it gets overwhelmed and will hang momentarily, blocking the writer from proceeding. </p>
<p><img alt="" src="https://cdn.prod.website-files.com/633275e23914a500db413038/670efa61afc092d67765a11d_22e849c3.jpeg"/></p>
<p>I traced this issue all the way back up the stack, to our audio encoding process, and I finally realized what was happening:</p>
<ol>
<li>Our transcription module emitted a large volume of debug logs, written to <code>stdout</code></li>
<li>Because we launched the Docker container in <code>cloud-init</code>, stdout was getting consumed by EC2&#39;s internal logging system</li>
<li>EC2&#39;s internal logging system writes to a virtual serial device</li>
<li>The interrupt handler used to write to the serial device cannot be interrupted</li>
<li>The time spent in the interrupt handler acted like CPU starvation for our audio processing. </li>
</ol>
<p><img alt="" src="https://cdn.prod.website-files.com/633275e23914a500db413038/670eec477a5ec55ebc019283_37d63efe.jpeg"/></p></div></div></div></div>
  </body>
</html>
