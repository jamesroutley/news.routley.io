<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://inflection.ai/inflection-2">Original</a>
    <h1>Inflection-2: the next step up</h1>
    
    <div id="readability-page-1" class="page"><div id=""><div justify="center"><div><div><p>Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs. This puts it into the same training compute class as Google’s flagship PaLM 2 Large model, which Inflection-2 outperforms on the majority of the standard AI performance benchmarks, including the well known MMLU, TriviaQA, HellaSwag &amp; GSM8k.</p>
<p>Designed with serving efficiency in mind, Inflection-2 will soon be powering <a href="https://pi.ai/" target="_blank" rel="noopener noreferrer">Pi</a>. Thanks to a transition from A100 to H100 GPUs, as well as our highly optimized inference implementation, we managed to reduce the cost and increase the speed of serving vs. Inflection-1 despite Inflection-2 being multiple times larger.</p>
<p>This is a big milestone on our path towards building a personal AI for everyone, and we’re excited about the new capabilities that Inflection-2 will enable in Pi. As our scaling journey continues, we are already looking forward to training even larger models on the full capacity of our 22,000 GPU cluster. Stay tuned!</p>
<p>Training very large models demands a <a href="https://inflection.ai/frontier-safety" target="_blank" rel="noopener noreferrer">special level of care and attention</a> to matters of safety, security, and trustworthiness. We take these responsibilities seriously at Inflection, and <a href="http://inflection.ai/safety" target="_blank" rel="noopener noreferrer">our safety team</a> continues to ensure that these models are rigorously evaluated and integrate best-in-class approaches to alignment. We were the first to sign up to the White House’s <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/" target="_blank" rel="noopener noreferrer">July 2023 voluntary commitments</a>, and continue to support efforts to create <a href="https://inflection.ai/g7-hiroshima-code-of-conduct" target="_blank" rel="noopener noreferrer">global alignment</a> and <a href="https://www.ft.com/content/d84e91d0-ac74-4946-a21f-5f82eb4f1d2d" target="_blank" rel="noopener noreferrer">governance mechanisms</a> for this critical technology.</p>
<p>We thank our partners NVIDIA, Microsoft, and CoreWeave for their collaboration and support in building <a href="https://inflection.ai/inflection-ai-announces-1-3-billion-of-funding" target="_blank" rel="noopener noreferrer">our AI cluster</a> that made the training of Inflection-1 and Inflection-2 possible.</p>
<p><strong>Results</strong></p>
<p>It is important to benchmark our models against the state of the art to validate our progress. The results below are from our pre-trained model and do not involve chain-of-thought prompting, except for GSM8k. Before Inflection-2 is released on <a href="https://pi.ai/" target="_blank" rel="noopener noreferrer">Pi</a>, it will undergo a series of alignment steps to become a helpful and safe personal AI.</p>
<p>We show Inflection-2’s performance on a wide range of benchmarks, comparing it to Inflection-1 and the most powerful external models LLaMA-2, Grok-1, PaLM-2, Claude-2 and GPT-4. We show the number of shots (examples given to the model) in parenthesis. If a result was not reported by the developers of a model, we show a ‘-’. Unless noted, all evaluations are reported in the same format as in the Inflection-1 <a href="https://inflection.ai/assets/Inflection-1.pdf" target="_blank" rel="noopener noreferrer">tech memo</a>.</p>
<p>We evaluate Inflection-2 on MMLU (5-shot), a diverse set of tasks ranging from high school to professional level. Inflection-2 is the most performant model outside of GPT-4, even outperforming Claude 2 with chain-of-thought reasoning.</p></div></div></div></div></div>
  </body>
</html>
