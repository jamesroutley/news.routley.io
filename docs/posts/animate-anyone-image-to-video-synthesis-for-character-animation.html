<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://humanaigc.github.io/animate-anyone/">Original</a>
    <h1>Animate Anyone: Image-to-video synthesis for character animation</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            

                  <p><span>Institute for Intelligent Computingï¼ŒAlibaba Group</span>
                  </p>

                  
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section>
  
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character&#39;s movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- Youtube video -->
<section>
  <div>
    <div>
      <!-- Paper video. -->
      <h2>Video</h2>
      <div>
        <div>
          
          <p>
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/8PCn5hLKNu4?si=8yvBeRNAJuxp77FZ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Image carousel -->
<section>
  <div>
    <div>
      <h2>Method</h2>
      <div>
        <!-- Your image here -->
        <p><img src="https://humanaigc.github.io/animate-anyone/static/images/f2_img.png" alt="MY ALT TEXT"/></p><h2>
          The overview of our method. The pose sequence is initially encoded using Pose Guider and fused with multi-frame noise, followed by the Denoising UNet conducting the denoising process for video generation. The computational block of the Denoising UNet consists of Spatial-Attention, Cross-Attention, and Temporal-Attention, as illustrated in the dashed box on the right. The integration of reference image involves two aspects. Firstly, detailed features are extracted through ReferenceNet and utilized for Spatial-Attention. Secondly, semantic features are extracted through the CLIP image encoder for Cross-Attention. Temporal-Attention operates in the temporal dimension. Finally, the VAE decoder decodes the result into a video clip.
       </h2>
     </div>
    </div>
  </div>


</section>
<!-- End image carousel -->




<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Animating Various Characters</h2>
      <h2>Human</h2>
      
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section>
  
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section>
  
</section>
<!-- End video carousel -->



<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Comparisons</h2>
      <h2>Fashion Video Synthesis</h2>
      
      <h2>
        Fashion Video Synthesis aims to turn fashion photographs into realistic, animated videos using a driving pose sequence. Experiments are conducted on the UBC fashion video dataset with same training data. 
    </h2></div>
  </div>
</section>
<!-- End video carousel -->



<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Human Dance Generation</h2>
      
      <h2>
        Human Dance Generation focuses on animating images in real-world dance scenarios. Experiments are conducted on the TikTok dataset with same training data. 
    </h2></div>
  </div>
</section>
<!-- End video carousel -->








<!--BibTex citation -->
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{hu2023animateanyone,
      title={Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation},
      author={Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},
      journal={arXiv preprint arXiv:2311.17117},
      website={https://humanaigc.github.io/animate-anyone/},
      year={2023}
}</code></pre>
  </div>
</section>


  



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
