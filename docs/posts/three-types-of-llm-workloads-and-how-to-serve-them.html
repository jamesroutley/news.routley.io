<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://modal.com/llm-almanac/workloads">Original</a>
    <h1>Three types of LLM workloads and how to serve them</h1>
    
    <div id="readability-page-1" class="page"><div><p>We hold this truth to be self-evident: not all workloads are created equal.</p> <p>But for large language models, this truth is far from universally
    acknowledged. Most organizations building LLM applications get their AI from
    an API, and these APIs hide the varied costs and engineering trade-offs of
    distinct workloads behind <a href="https://modal.com/blog/dollars-per-token-considered-harmful">deceptively flat per-token pricing</a>.</p> <p>The truth, however, will out. The era of model API dominance is ending,
    thanks to excellent work on open source models by DeepSeek and Alibaba Qwen
    (eroding the benefits of proprietary model APIs like OpenAI&#39;s) and excellent
    work on open source inference engines like vLLM and SGLang (eroding the
    benefits of open model APIs powered by proprietary inference engines).</p> <p>Engineers who wish to take advantage of this technological change must
    understand their workloads in greater detail in order to properly architect
    and optimize their systems.</p> <p>In this document, we&#39;ll walk through the workloads and requirements we&#39;ve
    seen in the market, working with leading organizations deploying inference
    to production at scale. We&#39;ll explain the challenges LLM engineers face when
    building for these workloads and how they solve those challenges. And we&#39;ll
    share a bit about how you can implement those solutions on <a href="https://modal.com/">our cloud platform</a>.</p> <h2>The breakdown: offline, online, and semi-online</h2> <blockquote><div> <p>Gallia est omnis divisa in partes tres.</p><!----> <!--[--><!--]--></div></blockquote><!----> <p>In the more mature world of databases, there is a well-known split between
    transaction processing (OLTP, think &#34;shopping carts&#34;) and analytical
    processing (OLAP, think &#34;Year Wrapped&#34;). In between are hybrid workloads
    (HTAP) with the characteristics of both.</p> <p>A similar three-part division has helped us organize LLM workloads:</p> <div><ul><li><em>offline</em> or analytical workloads, which operate in batch mode,
        write to data stores asynchronously, and <em>demand throughput</em> above all else,</li> <li><em>online</em> or interactive workloads, which operate in streaming
        mode, communicate synchronously with humans, and <em>demand low latency</em>, and</li> <li><em>semi-online</em> or bursty workloads, which operate on streams of
        batches, communicate with other live computer systems, and <em>demand flexible infrastructure</em>.</li></ul></div> <p><img src="https://modal-cdn.com/llm-almanac/workloads-breakdown.png" alt="Diagram depicting the three types of LLM workloads"/></p><p>Our recommendations for each are as follows:</p> <div><ul><li>for <em>offline workloads</em>, we recommend using vLLM via asynchronous
        RPC to ad hoc, auto-scaled compute capacity</li> <li>for <em>online workloads</em>, we recommend using SGLang with excess
        tensor parallelism and EAGLE-3 speculative decoding on live edge
        Hopper/Blackwell GPUs accessed via low-overhead, prefix-aware HTTP
        proxies</li> <li>for <em>semi-online workloads</em>, we recommend using either engine
        with rapid autoscaling of ad hoc compute capacity that can handle
        variable load per-replica</li></ul></div> <p>We will unpack and justify these recommendations, with reference to both
    specific applications &amp; workloads that run on our platform and sample code
    that you can work off of, in the remainder of this document.</p> <h2>Offline workloads demand throughput</h2> <blockquote><div> <p>The <em>law of increasing return</em> may be worded thus: An increase of labour
      and capital leads generally to improved organization, which increases the efficiency
      of the work of labour and capital.</p><!----> <!--[--><!--]--></div></blockquote><!----> <p>The <a href="https://weaviate.io/product/transformation-agent">Weaviate Transformation Agent</a> augments and updates entire datasets by applying an LLM to each row.</p> <p>A leading video transcription service needs to produce LLM summaries of a
    large volume of recorded calls for later search and retrieval.</p> <p>These systems are <em>offline</em>: they produce information for long-term
    storage in another computer system (like a filesystem or a database).
    Workloads are submitted as bulk &#34;jobs&#34; composed of many LLM requests. The
    entire job should be completed quickly, for cost reasons, but no single
    request requires immediate service. The scale of the job exposes substantial
    parallelism, which allows for economies of scale.</p> <p>Offline systems are generally easier to architect — computer systems began
    as offline batch-processing machines for a reason! But they still have their
    challenges.</p> <h3>Challenge: Maximizing throughput per dollar</h3> <p>The core challenge of offline, batch workloads is to maximize the throughput
    while controlling cost by taking advantage of intra-batch task parallelism.</p> <p>Fundamentally, this is good news. The most popular and readily-available
    hardware for running LLM inference, GPUs, are designed for <a href="https://modal.com/gpu-glossary/perf/latency-hiding">maximum throughput</a>, from their <a href="https://modal.com/gpu-glossary/device-hardware/warp-scheduler">per-clock-cycle context switching</a> and <a href="https://modal.com/gpu-glossary/device-hardware/tensor-core">large matrix multiplication units</a> to their <a href="https://modal.com/gpu-glossary/device-software/cuda-programming-model">task-parallel programming model</a>. That makes it relatively easy to write inference kernels that <a href="https://modal.com/gpu-glossary/perf/compute-bound">saturate compute resources</a>, and the open source and freely available kernels are satisfactory.
    Additionally, training of LLMs and other neural networks is an offline,
    batch workload, and training workloads have historically gotten the most and
    the quickest attention, e.g. when new hardware enters the market.</p> <p>But kernels are not the only code required to take advantage of parallelism
    in offline workloads. As one prominent example, batches must be constructed
    out of live and pending tasks (aka requests). Intra-task LLM inference work
    can be split into two phases: prefill (aka prompt processing) and decode
    (aka generation). Prefill work can be further split into chunks. With care,
    all of these kinds of work can be scheduled together for different tasks in
    the same batch.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-mixed-batching.png" alt="Diagram depicting faster completion of workloads with mixed batching"/> <small><em>With mixed batching, less-compute-intensive decode work (thinner lines)
      can piggyback on more compute-intensive prefill work (thicker lines).
      Colors indicate different tasks. For details, see <a href="https://arxiv.org/abs/2308.16369">the SARATHI paper</a>.</em></small></p><p>The vLLM inference engine has better support for these scheduling
    optimizations. For this reason, we currently recommend it for
    throughput-sensitive, offline workloads.</p> <h3>Implementation</h3> <p>We make the following choices to optimize for throughput (per dollar) in
    offline applications:</p> <div><ul><li>Run on vLLM with async scheduling and chunked prefill.</li> <li>Send large batches in each request to expose maximum parallelism to the
        engine. This is easiest with an offline interface, like the <code>LLM</code> abstraction in vLLM&#39;s Python SDK, rather than the online-serving-oriented
        HTTP server interfaces.</li> <li>On Modal, use asynchronous RPC with <a href="https://modal.com/docs/guide/job-queues"><code>.spawn</code></a> or <a href="https://modal.com/docs/guide/batch-processing"><code>.spawn_map</code></a> to queue
        up large numbers of requests for later retrieval or storage.</li> <li>Limit the number of GPUs per replica to the minimum required to run on a
        large enough batch to <a href="https://modal.com/gpu-glossary/perf/compute-bound">saturate the GPU&#39;s compute resources</a>. Excess available GPU capacity should be instead shifted to running
        more replicas.</li></ul></div> <p>You can find these patterns demonstrated and explained in detail in <a href="https://modal.com/docs/examples/vllm_throughput">this code sample</a>.</p> <h3>Future considerations</h3> <p>As the reliability of models increases and as their use becomes more
    commonplace, we expect more and more batch workloads to operate quietly in
    the background of many businesses, just as data analytics jobs, which
    started out as rare, heroic tabulations like censuses, are now humdrum table
    stakes.</p> <p>We&#39;ve noticed an interesting pattern in GPU pricing, which shows up in our
    own <a href="https://modal.com/pricing">current rates</a> at time of writing: the FLOPs per
    dollar is roughly constant, so older GPUs that might be easier to come by
    (in on-premises deployments) or available in larger quantities (on platforms
    like Modal) serve quite nicely for jobs that care about throughput per <em>dollar</em> more than they care about throughput per <em>second</em>.</p> <h2>Online workloads abhor latency</h2> <blockquote><div> <p>When a computer and its users interact at a pace that ensures that neither
      has to wait on the other, productivity soars, the cost of the work done on
      the computer tumbles, employees get more satisfaction from their work, and
      its quality tends to improve. Few online computer systems are this well
      balanced…</p><!----> <!--[--><!--]--></div></blockquote><!----> <p>Agents built with <a href="https://decagon.ai/product/voice">Decagon Voice</a> need to participate in phone calls with humans requesting support help.</p> <p>A leading AI IDE company needs to serve &#34;smart&#34; auto-completion in the brief
    intervals while human engineers consider what code to write next.</p> <p>These systems are <em>online</em>: a human user is interacting with the
    system, and they want responses that match their (and other humans&#39;)
    reaction time, on the order of at most a few hundred milliseconds. Human
    users create multi-turn contexts from their repeated interactions.</p> <p>Online systems are extremely challenging to build. They are extremely
    performance-sensitive, and performance melts abstractions and couples
    decoupled concerns. But they can be built, if you can solve the attendant
    challenges.</p> <h3>Challenge: Avoiding host overhead</h3> <p>The primary challenge of online workloads is that the system has only a few
    hundred milliseconds to respond.</p> <p>First, that means that the performance hits from using interpreted
    languages, like Python, start to matter. Leading LLM inference engines are
    written mostly in Python, for faster development, and so they need to be
    architected and implemented carefully to avoid the work on the CPU (or host)
    from blocking the work on the GPU — inflicting &#34;host overhead&#34;. We wrote
    about the kinds of host overhead inference workloads encounter, and how
    we&#39;ve contributed to inference engines to solve them, in <a href="https://modal.com/blog/host-overhead-inference-efficiency">this blog post</a>.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-host-overhead.png" alt="Diagram depicting CPU work blocking GPU kernels (host overhead) and not blocking (no host overhead)"/></p><p>In our experience, the SGLang inference engine wins here, since it generally
    exhibits lower host overhead.</p> <h3>Challenge: Reducing communications overhead</h3> <p>To repeat: the primary challenge of online workloads is that the system has
    only a few hundred milliseconds to respond.</p> <p>We are here considering LLM inference services, rather than local
    applications, and so communication between the client and the system can
    introduce notable latency at this timescale. Specifically, networks operate
    at a large fraction of the speed of light, but that means tens or even
    hundreds of milliseconds of latency for clients (assumed Earthbound) to a
    system implemented in a single geographic location.</p> <p>The solution is to deploy both routing proxies and accelerator capacity to
    &#34;the edge&#34;, i.e. into datacenters that are close to clients. Quite apart
    from narrow-sense technical issues, this can prove challenging due to market
    conditions, as not all cloud providers have available capacity of all GPU
    types in all regions. At Modal, we solve this by aggregating capacity across
    clouds, which support regionalized service deployments.</p> <h3>Challenge: Handling multiple turns</h3> <p>Online workloads are interactive not just in latency requirement but also in
    request patterns. Human users respond to the system&#39;s response, which the
    system must respond to in turn.</p> <p>Unlike a (nominally) stateless protocol like HTTP, efficient multi-turn LLM
    inference is stateful. It may not look that way, since clients generally
    provide the entire conversation history in their requests. But contemporary
    models based on the Transformer architecture have computation requirements
    that scale quadratically with conversation length. This can be exchanged for
    linear computation in exchange for storing a linear quantity of model
    activations, the &#34;key-value cache&#34; (originally and more descriptively known
    as the &#34;past cache&#34;).</p> <p>The solution is to route requests to LLM inference replicas based on the
    information used to populate the cache. For lossless cacheing, that means
    the prefix(es) of the request. This &#34;prefix-aware&#34; routing can look as
    simple as sticky sessions per conversation, which we provide native support
    for in Modal, or can involve deeper inspection of both request and cache
    contents.</p> <h3>Challenge: Wrangling the memory bound</h3> <p>The bottlenecking operations in LLM inference with KV caching have a low <a href="https://modal.com/gpu-glossary/perf/arithmetic-intensity">arithmetic intensity</a>, which means inference is <a href="https://modal.com/gpu-glossary/perf/memory-bound">bound by memory</a>.</p> <p>Intuitively, you can generate only one or a few tokens per forward pass per
    request, but you must load all the model weights (typically numbered in the
    billions) into <a href="https://modal.com/gpu-glossary/device-hardware/register-file">registers</a>. Per user, those weights get used for roughly one add and one multiply,
    but <a href="https://modal.com/gpu-glossary/perf/arithmetic-intensity">ridge point arithmetic intensities</a> for GPUs are in the hundreds or thousands of operations per byte — and so, much <a href="https://modal.com/gpu-glossary/perf/arithmetic-bandwidth">arithmetic bandwidth</a> will go to waste. Furthermore, even if the latency requirements and request load
    for the online service admit batching across requests, prefixes are generally
    mostly distinct per request for these workloads, and so distinct elements of
    the KV cache must be loaded per request. Cache contents start out negligible
    relative to model weights but grow with sequence length.</p> <p>The foregoing arguments focus on throughput, but in online workloads, where
    latency is the primary concern, the situation is even worse. Because running
    a forward pass on a batch requires loading billions of model weights and <a href="https://modal.com/gpu-glossary/perf/memory-bandwidth">memory bandwidths</a> are measured in trillions of bytes per second, forward passes on single accelerators
    necessarily take milliseconds. Individual users cannot see lower per-token latencies
    than this. Autoregressive, i.e. sequential, generation stacks these latencies,
    rapidly eating into the latency budget, even for short generated sequences (another
    instance of Amdahl&#39;s heartbreaking law).</p> <p>One resolution is to increase the memory bandwidth in the system
    (specifically, the <a href="https://modal.com/gpu-glossary/perf/memory-bandwidth">memory bandwidth</a> between model weights + KV cache and <a href="https://modal.com/gpu-glossary/device-hardware/tensor-core">arithmetic units</a>). On the hardware side, that means using the <a href="https://modal.com/blog/introducing-b200-h200">latest GPUs</a>, like H100s and B200s,
    which offer substantial improvements in memory bandwidth over past
    generations.</p> <p>Using multiple GPUs increases aggregate bandwidth. But just adding more GPUs
    isn&#39;t enough to cut latency. On the software side, systems must additionally
    take advantage of intra-task parallelism to split bandwidth demands across
    accelerators. The most common approach is tensor parallelism, which takes
    advantage of the inherent parallelism of matrix multiplications to split
    pieces of the multiplication onto different workers.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-tensor-parallel.png" alt="Diagram depicting the division of tensors across GPUs in tensor parallel matrix multiplication"/> <small><em>Tensor parallelism splits a single matrix multiplication (left-hand-side
      of equation) across GPUs (represented by color; shared data on all GPUs in
      gray).</em></small></p><p>This requires low latency and high bandwidth, so it is usually done only
    within the backend/&#34;scale-up&#34; network, typically NVLink for GPUs. For many
    useful open source models applied to specific tasks, the standard
    eight-accelerator NVLink domain provides sufficient memory bandwidth to hit
    interactive latency targets, but we anticipate a future where the larger,
    rack-scale NVLink domains offered by NVSwitch are required.</p> <p>In addition to increasing memory bandwidth, systems can also decrease memory
    requirements, typically at a cost to model quality. Whether this trade-off
    is sensible is application-dependent — another good reason to host your own
    inference!</p> <p>The first lever to pull is <a href="https://quant.exposed">floating point quantization</a>. Generally, the performance benefit is greater if the hardware supports
    native floating point operations on the quantized data type: eight bit (FP8)
    for Hopper GPUs, four bit (FP4) for Blackwell GPUs.</p> <p>For models above about seventy billion parameters, four bit quantization
    works well with minimal fuss. For smaller models, down to a billion
    parameters, only eight bit quantization retains sufficient model quality —
    with the notable exception of <a href="https://modal.com/docs/examples/gpt_oss_inference">gpt-oss 20B</a>.</p> <p>Finally, we note in passing that the reason for the mixture-of-experts (MoE)
    structure for feedforward layers in contemporary architectures is to reduce
    the demand on memory bandwidth. If you&#39;re comparing across models to
    determine memory requirements and serving cost, look at active parameters,
    not just total parameters!</p> <h3>Challenge: Cheating the speed of light</h3> <p>Eventually, the memory bound is inescapable, and latency cannot be reduced
    any further. The system has reached the metaphorical &#34;speed of light&#34; for
    the hardware.</p> <p>The speed of light cannot be broken, but it can be cheated.</p> <p>The key technique for memory-bound inference is speculative decoding, which
    takes advantage of some of the slack in <a href="https://modal.com/gpu-glossary/perf/arithmetic-bandwidth">arithmetic bandwidth</a> in naïve, single-token autoregressive inference.</p> <p>Specifically, we use a simpler language modeling system, the speculator, to
    provide multiple sequential output tokens for the larger, target system to
    judge in parallel. Because inference is memory-bound, there are extra FLOPs
    to be had for running the speculator. Because the larger model already
    outputs probabilities for each token in its input, engines can
    straightforwardly ensure that outputs are unchanged (cf. &#34;rejection
    sampling&#34; from statistical inference).</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-specdec.png" alt="Diagram depicting the relationship between speculator tokens and generated tokens"/> <small><em>In speculative decoding, a speculator model produces &#34;draft&#34; tokens (light
      green) that are validated in parallel by the target model. Those with
      sufficiently high probability under the target model are accepted (dark
      green, lower right) and the first token that is rejected is replaced with
      a generation from the target model (orange).</em></small></p><p>This idea is <a href="https://arxiv.org/abs/2302.01318">well</a>-<a href="https://arxiv.org/abs/2211.17192">worn</a> by LLM inference standards, but until relatively recently, using more sophisticated
    draft models was hamstrung by operational difficulties that offset the limited
    performance gains. That left only very simple speculators, like &#34;predict that
    the same subsequence will be repeated&#34; (aka n-gram speculation), which generally
    have lower rates of acceptance and so speed up inference less.</p> <p>The <a href="https://arxiv.org/abs/2503.01840">EAGLE-3 speculator training method</a> changed that for us. Not only does it produce simple speculators with good support
    in open source engines, but it also achieves very high quality, measured in acceptance
    lengths. We have found that just adding EAGLE-3 via open source inference engines
    is sufficient to match the performance achieved by model providers with proprietary
    inference stacks. At time of writing, SGLang has better support for speculative
    decoding, another reason we recommend it for low latency applications.</p> <h3>Implementation</h3> <p>We make the following choices to optimize for low latency in online
    applications:</p> <div><ul><li>run on SGLang to reduce host overhead and take full advantage of
        speculative decoding</li> <li>use FP8 for smaller memory footprint and fast prefill and decode kernels
        on H100/H200 GPUs</li> <li>apply extra tensor parallelism above any required by memory capacity in
        order to reduce memory read latency</li> <li>use an off-the-shelf or custom-trained EAGLE-3 speculator model</li> <li>on Modal, use the <code>modal.experimental.http_server</code> deployment
        decorator to create a regionalized, ultra-low-overhead web server with session-based
        routing</li></ul></div> <p>You can see this in action in a code sample <a href="https://modal.com/docs/examples/sglang_low_latency">here</a>.</p> <h3>Future considerations</h3> <p>Because of the tremendous investment in and excitement over chatbots, this
    workload has received substantial engineering work already and its future is
    slightly easier to chart.</p> <p>First, we expect more ways to &#34;cheat the speed of light&#34; to become important
    in the near future, in particular lossy optimizations that sacrifice some
    performance for a lot of speed. A few we didn&#39;t mention above, but which are
    the targets of current research: approximate KV cacheing, layer skipping,
    pruning, lossy compression of the KV cache, lossy speculation. Many of these
    techniques are already reasonably mature in the world of diffusion models,
    where other opportunities for speedups are limited (see our <a href="https://modal.com/blog/flux-3x-faster">blog post on accelerating Flux</a>).</p> <p>Note that because these optimizations are &#34;lossy&#34;, they change the hosted
    model, in the statistical sense. Behavior is guaranteed to change, if only
    slightly. That makes for a good economic reason to self-host: you can check
    which optimizations work for your workload.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-lossy-tradeoffs.png" alt="Diagram depicting the tradeoff between speed and quality with lossy optimizations"/> <small><em>For some workloads, fully lossless performance improvements like
      speculative decoding might be insufficient to achieve target latency
      (vertical arrow). The right lossy performance improvements (angled arrows)
      to achieve the target speed and latency (colored regions) differ between
      workloads (indicated by color).</em></small></p><p>In part due to the investments of existing hardware providers and the crop
    of inference hardware startups, we expect these workloads to move
    increasingly onto more exotic hardware that even less resembles a typical
    workstation or generic high-performance machine in the cloud.</p> <p>Nvidia is investing heavily in tightly-interconnected systems, e.g.
    &#34;rack-scale&#34; <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">NVL72</a>/<a href="https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference">CPX Rubin</a>. This architecture can achieve massive memory bandwidth at low latency
    without using components that are too exotic relative to existing systems
    (for instance, using <a href="https://modal.com/gpu-glossary/device-hardware/gpu-ram">HBM</a> for system memory). Following the same logic, Google is building large TPU systems
    with a similar architecture. Doing better requires deeper innovation at the silicon
    layer, likely in the form of application-specific integrated circuits for specific
    model architectures. To reach one billion tokens per second, for instance, would
    require tightly co-locating storage and compute, e.g. with analog elements.</p> <p>While we don&#39;t expect these systems to go without demand, we expect the <em>relative</em> importance of online/chat workloads to decrease over time.
    The current interest has been driven by the initial &#34;killer app&#34; for LLMs, OpenAI&#39;s
    ChatGPT. This has led to lots of imitation and herding behavior by capital providers,
    investors, founders, and even application developers.</p> <p>But we are already seeing the signs of a different &#34;killer app&#34; emerging —
    long-running background agents, like Claude Code, which have the patience of
    machines, rather than humans. These applications generate quite different
    workloads, to which we turn in the next section.</p> <h2>Semi-online workloads demand flexibility</h2> <blockquote><div> <p>Roughly speaking, the cost of a system scales with its (short-term) peak
      traffic, but for most applications the value the system generates scales
      with the (long-term) average traffic. The gap between &#34;paying for peak&#34;
      and &#34;earning on average&#34; is critical to understand how the economics of
      large-scale cloud systems differ from traditional single-tenant systems.</p><!----> <!--[--><!--]--></div></blockquote><!----> <p>Users of <a href="https://reducto.ai/">Reducto</a>&#39;s document processing
    platform sometimes upload a single form for immediate perusal and sometimes
    drop their business&#39;s entire document storage.</p> <p>An AI news analytics agency needs to scale up its news agents in minutes in
    response to breaking news, crawling a variety of sources to produce
    syntheses. It also needs to produce a &#34;daily newspaper&#34; on a longer cadence.</p> <p>These systems are <em>semi-online</em>: sometimes they must return to a
    waiting human; other times they pass their results on to another computer
    system in a pipeline (which might be another agent). Even when they directly
    serve human users, they are not as tightly interactive. Their workloads are
    bursty — sometimes load goes to hundreds of times baseline for minutes or
    tens of minutes.</p> <h3>Challenge: Taming peak-to-average load ratio</h3> <p>This high peak-to-average ratio creates a cost conundrum for systems serving
    these workloads, as alluded to by Marc Brooker of Amazon Web Services in the
    quote above. That is, costs are typically driven by requirements to service <em>peak</em> demand, but revenues are driven by servicing <em>average</em> demand.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-peak-to-average.png" alt="Diagram depicting a workload with a high peak-to-average ratio"/> <small><em>System costs are proportional to the allocated resources for peak demand
      (shaded area). Revenues are proportional to the realized demand for
      resources (area under the curve). When peak demand is much higher than
      average, systems without flexible resource allocations, as depicted in
      this figure, become uneconomical.</em></small></p><p>The solution we&#39;ve taken at Modal is the same taken by AWS: aggregation and
    multitenancy. That is, we service a variety of these workloads on shared
    hardware, whose uncorrelated peaks aggregate into a smooth timeline of
    demand for that hardware. The peak-to-average load ratio is diminished.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-multitenancy.png" alt="Diagram depicting the lower resource utilization of a multitenant system"/> <small><em>In a multi-tenant system (left, tenant resource demand indicated by
      colored lines), peak demand is reduced, cutting costs (shaded region). A
      group of single-tenant systems has, in the worst case scenario (right)
      cost per workload (each shaded region) close to the cost of the entire
      multi-tenant system.</em></small></p><p>We can then maintain a buffer sufficient to service resource requests
    immediately and acquire or release resources as the average changes. See <a href="https://modal.com/blog/resource-solver">this blog post</a> for details on that system.</p> <h3>Challenge: Cutting cold starts from minutes to seconds</h3> <p>Multi-tenant computer systems have their drawbacks, including the addition
    of cold start latency. That is, even if the request for serving resources is
    serviced out of a buffer, configuring those resources to start handling the
    request takes some time: containers or VMs must boot, then inference engines
    must start.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-slow-container-start.png" alt="Diagram depicting slow container starts during inference engine autoscaling"/></p><p>Without optimization, container startup time can run into several minutes
    for large container images.</p> <p>At Modal, we&#39;ve invested heavily in techniques to accelerate container
    startup time, like mixing eager pre-fetching of files that will be used and
    lazy-loading of files that are unlikely to be used.</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-slow-engine-init.png" alt="Diagram depicting fast container startups with slow engine initialization"/></p><p>After optimization, container startup can be reduced to seconds. But engine
    initialization can still take tens of seconds.</p> <p>For instance, the Torch JIT compiler can deliver massive speedups to
    inference passes, but it can take several minutes to run — during which time
    the inference server replica cannot service requests.</p> <p>Our solution is GPU memory snapshotting. Just before an inference server
    replica is ready to service requests, we dump the program state to the
    filesystem. When we spin up a new replica, it is loaded straight from the
    filesystem, skipping computations like Torch JIT compilation (and also
    converting a large number of small file I/Os into one large I/O, which is a
    better fit to storage systems).</p> <p><img src="https://modal-cdn.com/llm-almanac/workloads-memory-snapshot.png" alt="Diagram depicting fast container starts for snapshot inference engines"/> <small><em>Memory snapshotting can cut down inference server start times by a factor
      of 10, requiring only that the server be <code>restore</code>d from
      serialized snapshot storage (drum icon). Rapid scaleup improves response
      time under sudden bursts of load (yellow <code>/chat/completions</code> requests).</em></small></p><p>We have <a href="https://modal.com/blog/gpu-mem-snapshots">benchmarked GPU snapshotting across a wide class of models</a> and found that it can cut LLM inference server start times from minutes to
    seconds.</p> <h3>Implementation</h3> <p>We make the following choices to optimize for flexible scaling in
    semi-online applications:</p> <div><ul><li>Use fast-booting, autoscaling GPU resources to service variable load
        economically</li> <li>While spot and on-demand B200s are still relatively scarce from
        hyperscalers, prefer H100s or H200s, which further indicates the use of
        FP8-quantized models</li> <li>On Modal, use the <code>web_server</code> deployment decorator to turn a
        Python program exposing an OpenAI-compatible server into a web service</li> <li>Set auto-scaling policy to absorb small load bursts — on Modal, that&#39;s
        done by setting <code>max_inputs</code> to be higher than <code>target_inputs</code> in the <code>modal.concurrent</code> decorator</li> <li>The choice of engine, between vLLM and SGLang, depends on other factors
        like model availability.</li> <li>Use GPU memory snapshotting to speed up server boots, especially if your
        engine requires slow JIT compilation steps. Almost all programs can be
        snapshot, but many programs require some slight code rewrites to be
        snapshot.</li></ul></div> <p>You can find sample code for serving these workloads with SGLang <a href="https://modal.com/docs/examples/sglang_snapshot">here</a> and with vLLM <a href="https://modal.com/docs/examples/vllm_snapshot">here</a>.</p> <h3>Future considerations</h3> <p>We expect more of these semi-online applications to emerge as the field
    matures — offline/analytical and online/transaction workloads are the
    obvious things to do, but there are many more tasks in the interior,
    combining traits of both.</p> <p>In particular, we expect the salience of these workloads to increase as more
    work is done by long-running agents, which have the patience of computer
    systems, rather than humans. That is, human users will pay a large premium
    to avoid a small wait — and productivity studies like Doherty &amp;
    Thadhani&#39;s, quoted above, bear out that trade. But engineers architecting
    agents or systems of agents to complete long-running tasks will generally
    prefer the opposite trade. We look forward to servicing more of these
    workloads as builders and engineers discover and scale them.</p> <h2>What next?</h2> <p>We are still early in the era of LLM engineering, despite being several
    years into the era of LLMs, thanks to the head-start on capabilities
    achieved by proprietary model companies and proprietary inference engines.</p> <p>But as in other domains, the underlying technologies are spreading enough to
    become commodities. The additional benefits of customization and control
    then tilt the balance increasingly in favor of building LLM inference
    in-house.</p> <p>This requires additional engineering effort — and a community effort to
    distribute knowledge, to upskill, and to produce open models and open source
    software. At Modal, we&#39;re happy to contribute to all of these. If you&#39;re
    interested in deploying your own inference at scale, <a href="mailto:sales@modal.com">talk to us</a>.</p></div></div>
  </body>
</html>
