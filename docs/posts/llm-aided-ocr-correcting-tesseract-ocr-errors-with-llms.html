<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Dicklesworthstone/llm_aided_ocr">Original</a>
    <h1>Show HN: LLM-aided OCR â€“ Correcting Tesseract OCR errors with LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">The LLM-Aided OCR Project is an advanced system designed to significantly enhance the quality of Optical Character Recognition (OCR) output. By leveraging cutting-edge natural language processing techniques and large language models (LLMs), this project transforms raw OCR text into highly accurate, well-formatted, and readable documents.</p>

<p dir="auto">To see what the LLM-Aided OCR Project can do, check out these example outputs:</p>
<ul dir="auto">
<li><a href="https://github.com/Dicklesworthstone/llm_aided_ocr/blob/main/160301289-Warren-Buffett-Katharine-Graham-Letter.pdf">Original PDF</a></li>
<li><a href="https://github.com/Dicklesworthstone/llm_aided_ocr/blob/main/160301289-Warren-Buffett-Katharine-Graham-Letter__raw_ocr_output.txt">Raw OCR Output</a></li>
<li><a href="https://github.com/Dicklesworthstone/llm_aided_ocr/blob/main/160301289-Warren-Buffett-Katharine-Graham-Letter_llm_corrected.md">LLM-Corrected Markdown Output</a></li>
</ul>

<ul dir="auto">
<li>PDF to image conversion</li>
<li>OCR using Tesseract</li>
<li>Advanced error correction using LLMs (local or API-based)</li>
<li>Smart text chunking for efficient processing</li>
<li>Markdown formatting option</li>
<li>Header and page number suppression (optional)</li>
<li>Quality assessment of the final output</li>
<li>Support for both local LLMs and cloud-based API providers (OpenAI, Anthropic)</li>
<li>Asynchronous processing for improved performance</li>
<li>Detailed logging for process tracking and debugging</li>
<li>GPU acceleration for local LLM inference</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Detailed Technical Overview</h2><a id="user-content-detailed-technical-overview" aria-label="Permalink: Detailed Technical Overview" href="#detailed-technical-overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<ol dir="auto">
<li>
<p dir="auto"><strong>PDF to Image Conversion</strong></p>
<ul dir="auto">
<li>Function: <code>convert_pdf_to_images()</code></li>
<li>Uses <code>pdf2image</code> library to convert PDF pages into images</li>
<li>Supports processing a subset of pages with <code>max_pages</code> and <code>skip_first_n_pages</code> parameters</li>
</ul>
</li>
<li>
<p dir="auto"><strong>OCR Processing</strong></p>
<ul dir="auto">
<li>Function: <code>ocr_image()</code></li>
<li>Utilizes <code>pytesseract</code> for text extraction</li>
<li>Includes image preprocessing with <code>preprocess_image()</code> function:
<ul dir="auto">
<li>Converts image to grayscale</li>
<li>Applies binary thresholding using Otsu&#39;s method</li>
<li>Performs dilation to enhance text clarity</li>
</ul>
</li>
</ul>
</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto"><strong>Chunk Creation</strong></p>
<ul dir="auto">
<li>The <code>process_document()</code> function splits the full text into manageable chunks</li>
<li>Uses sentence boundaries for natural splits</li>
<li>Implements an overlap between chunks to maintain context</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Error Correction and Formatting</strong></p>
<ul dir="auto">
<li>Core function: <code>process_chunk()</code></li>
<li>Two-step process:
a. OCR Correction:
<ul dir="auto">
<li>Uses LLM to fix OCR-induced errors</li>
<li>Maintains original structure and content
b. Markdown Formatting (optional):</li>
<li>Converts text to proper markdown format</li>
<li>Handles headings, lists, emphasis, and more</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Duplicate Content Removal</strong></p>
<ul dir="auto">
<li>Implemented within the markdown formatting step</li>
<li>Identifies and removes exact or near-exact repeated paragraphs</li>
<li>Preserves unique content and ensures text flow</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Header and Page Number Suppression (Optional)</strong></p>
<ul dir="auto">
<li>Can be configured to remove or distinctly format headers, footers, and page numbers</li>
</ul>
</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto"><strong>Flexible LLM Support</strong></p>
<ul dir="auto">
<li>Supports both local LLMs and cloud-based API providers (OpenAI, Anthropic)</li>
<li>Configurable through environment variables</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Local LLM Handling</strong></p>
<ul dir="auto">
<li>Function: <code>generate_completion_from_local_llm()</code></li>
<li>Uses <code>llama_cpp</code> library for local LLM inference</li>
<li>Supports custom grammars for structured output</li>
</ul>
</li>
<li>
<p dir="auto"><strong>API-based LLM Handling</strong></p>
<ul dir="auto">
<li>Functions: <code>generate_completion_from_claude()</code> and <code>generate_completion_from_openai()</code></li>
<li>Implements proper error handling and retry logic</li>
<li>Manages token limits and adjusts request sizes dynamically</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Asynchronous Processing</strong></p>
<ul dir="auto">
<li>Uses <code>asyncio</code> for concurrent processing of chunks when using API-based LLMs</li>
<li>Maintains order of processed chunks for coherent final output</li>
</ul>
</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto"><strong>Token Estimation</strong></p>
<ul dir="auto">
<li>Function: <code>estimate_tokens()</code></li>
<li>Uses model-specific tokenizers when available</li>
<li>Falls back to <code>approximate_tokens()</code> for quick estimation</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Dynamic Token Adjustment</strong></p>
<ul dir="auto">
<li>Adjusts <code>max_tokens</code> parameter based on prompt length and model limits</li>
<li>Implements <code>TOKEN_BUFFER</code> and <code>TOKEN_CUSHION</code> for safe token management</li>
</ul>
</li>
</ol>

<ol dir="auto">
<li><strong>Output Quality Evaluation</strong>
<ul dir="auto">
<li>Function: <code>assess_output_quality()</code></li>
<li>Compares original OCR text with processed output</li>
<li>Uses LLM to provide a quality score and explanation</li>
</ul>
</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">Logging and Error Handling</h3><a id="user-content-logging-and-error-handling" aria-label="Permalink: Logging and Error Handling" href="#logging-and-error-handling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Comprehensive logging throughout the codebase</li>
<li>Detailed error messages and stack traces for debugging</li>
<li>Suppresses HTTP request logs to reduce noise</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Configuration and Customization</h2><a id="user-content-configuration-and-customization" aria-label="Permalink: Configuration and Customization" href="#configuration-and-customization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The project uses a <code>.env</code> file for easy configuration. Key settings include:</p>
<ul dir="auto">
<li>LLM selection (local or API-based)</li>
<li>API provider selection</li>
<li>Model selection for different providers</li>
<li>Token limits and buffer sizes</li>
<li>Markdown formatting options</li>
</ul>

<ol dir="auto">
<li><strong>Raw OCR Output</strong>: Saved as <code>{base_name}__raw_ocr_output.txt</code></li>
<li><strong>LLM Corrected Output</strong>: Saved as <code>{base_name}_llm_corrected.md</code> or <code>.txt</code></li>
</ol>
<p dir="auto">The script generates detailed logs of the entire process, including timing information and quality assessments.</p>

<ul dir="auto">
<li>Python 3.12+</li>
<li>Tesseract OCR engine</li>
<li>PDF2Image library</li>
<li>PyTesseract</li>
<li>OpenAI API (optional)</li>
<li>Anthropic API (optional)</li>
<li>Local LLM support (optional, requires compatible GGUF model)</li>
</ul>

<ol dir="auto">
<li>Install Pyenv and Python 3.12 (if needed):</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Pyenv and python 3.12 if needed and then use it to create venv:
if ! command -v pyenv &amp;&gt; /dev/null; then
    sudo apt-get update
    sudo apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev \
    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \
    xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git

    git clone https://github.com/pyenv/pyenv.git ~/.pyenv
    echo &#39;export PYENV_ROOT=&#34;$HOME/.pyenv&#34;&#39; &gt;&gt; ~/.zshrc
    echo &#39;export PATH=&#34;$PYENV_ROOT/bin:$PATH&#34;&#39; &gt;&gt; ~/.zshrc
    echo &#39;eval &#34;$(pyenv init --path)&#34;&#39; &gt;&gt; ~/.zshrc
    source ~/.zshrc
fi
cd ~/.pyenv &amp;&amp; git pull &amp;&amp; cd -
pyenv install 3.12"><pre><span><span>#</span> Install Pyenv and python 3.12 if needed and then use it to create venv:</span>
<span>if</span> <span>!</span> <span>command</span> -v pyenv <span>&amp;</span><span>&gt;</span> /dev/null<span>;</span> <span>then</span>
    sudo apt-get update
    sudo apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev \
    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \
    xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git

    git clone https://github.com/pyenv/pyenv.git <span>~</span>/.pyenv
    <span>echo</span> <span><span>&#39;</span>export PYENV_ROOT=&#34;$HOME/.pyenv&#34;<span>&#39;</span></span> <span>&gt;&gt;</span> <span>~</span>/.zshrc
    <span>echo</span> <span><span>&#39;</span>export PATH=&#34;$PYENV_ROOT/bin:$PATH&#34;<span>&#39;</span></span> <span>&gt;&gt;</span> <span>~</span>/.zshrc
    <span>echo</span> <span><span>&#39;</span>eval &#34;$(pyenv init --path)&#34;<span>&#39;</span></span> <span>&gt;&gt;</span> <span>~</span>/.zshrc
    <span>source</span> <span>~</span>/.zshrc
<span>fi</span>
<span>cd</span> <span>~</span>/.pyenv <span>&amp;&amp;</span> git pull <span>&amp;&amp;</span> <span>cd</span> -
pyenv install 3.12</pre></div>
<ol start="2" dir="auto">
<li>Set up the project:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Use pyenv to create virtual environment:
git clone https://github.com/Dicklesworthstone/llm_aided_ocr    
cd llm_aided_ocr          
pyenv local 3.12
python -m venv venv
source venv/bin/activate
python -m pip install --upgrade pip
python -m pip install wheel
python -m pip install --upgrade setuptools wheel
pip install -r requirements.txt"><pre><span><span>#</span> Use pyenv to create virtual environment:</span>
git clone https://github.com/Dicklesworthstone/llm_aided_ocr    
<span>cd</span> llm_aided_ocr          
pyenv <span>local</span> 3.12
python -m venv venv
<span>source</span> venv/bin/activate
python -m pip install --upgrade pip
python -m pip install wheel
python -m pip install --upgrade setuptools wheel
pip install -r requirements.txt</pre></div>
<ol start="3" dir="auto">
<li>
<p dir="auto">Install Tesseract OCR engine (if not already installed):</p>
<ul dir="auto">
<li>For Ubuntu: <code>sudo apt-get install tesseract-ocr</code></li>
<li>For macOS: <code>brew install tesseract</code></li>
<li>For Windows: Download and install from <a href="https://github.com/UB-Mannheim/tesseract/wiki">GitHub</a></li>
</ul>
</li>
<li>
<p dir="auto">Set up your environment variables in a <code>.env</code> file:</p>
<div data-snippet-clipboard-copy-content="USE_LOCAL_LLM=False
API_PROVIDER=OPENAI
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key"><pre><code>USE_LOCAL_LLM=False
API_PROVIDER=OPENAI
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
</code></pre></div>
</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto">Place your PDF file in the project directory.</p>
</li>
<li>
<p dir="auto">Update the <code>input_pdf_file_path</code> variable in the <code>main()</code> function with your PDF filename.</p>
</li>
<li>
<p dir="auto">Run the script:</p>

</li>
<li>
<p dir="auto">The script will generate several output files, including the final post-processed text.</p>
</li>
</ol>

<p dir="auto">The LLM-Aided OCR project employs a multi-step process to transform raw OCR output into high-quality, readable text:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>PDF Conversion</strong>: Converts input PDF into images using <code>pdf2image</code>.</p>
</li>
<li>
<p dir="auto"><strong>OCR</strong>: Applies Tesseract OCR to extract text from images.</p>
</li>
<li>
<p dir="auto"><strong>Text Chunking</strong>: Splits the raw OCR output into manageable chunks for processing.</p>
</li>
<li>
<p dir="auto"><strong>Error Correction</strong>: Each chunk undergoes LLM-based processing to correct OCR errors and improve readability.</p>
</li>
<li>
<p dir="auto"><strong>Markdown Formatting</strong> (Optional): Reformats the corrected text into clean, consistent Markdown.</p>
</li>
<li>
<p dir="auto"><strong>Quality Assessment</strong>: An LLM-based evaluation compares the final output quality to the original OCR text.</p>
</li>
</ol>

<ul dir="auto">
<li><strong>Concurrent Processing</strong>: When using API-based models, chunks are processed concurrently to improve speed.</li>
<li><strong>Context Preservation</strong>: Each chunk includes a small overlap with the previous chunk to maintain context.</li>
<li><strong>Adaptive Token Management</strong>: The system dynamically adjusts the number of tokens used for LLM requests based on input size and model constraints.</li>
</ul>

<p dir="auto">The project uses a <code>.env</code> file for configuration. Key settings include:</p>
<ul dir="auto">
<li><code>USE_LOCAL_LLM</code>: Set to <code>True</code> to use a local LLM, <code>False</code> for API-based LLMs.</li>
<li><code>API_PROVIDER</code>: Choose between &#34;OPENAI&#34; or &#34;CLAUDE&#34;.</li>
<li><code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>: API keys for respective services.</li>
<li><code>CLAUDE_MODEL_STRING</code>, <code>OPENAI_COMPLETION_MODEL</code>: Specify the model to use for each provider.</li>
<li><code>LOCAL_LLM_CONTEXT_SIZE_IN_TOKENS</code>: Set the context size for local LLMs.</li>
</ul>

<p dir="auto">The script generates several output files:</p>
<ol dir="auto">
<li><code>{base_name}__raw_ocr_output.txt</code>: Raw OCR output from Tesseract.</li>
<li><code>{base_name}_llm_corrected.md</code>: Final LLM-corrected and formatted text.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">Limitations and Future Improvements</h2><a id="user-content-limitations-and-future-improvements" aria-label="Permalink: Limitations and Future Improvements" href="#limitations-and-future-improvements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>The system&#39;s performance is heavily dependent on the quality of the LLM used.</li>
<li>Processing very large documents can be time-consuming and may require significant computational resources.</li>
</ul>

<p dir="auto">Contributions to this project are welcome! Please fork the repository and submit a pull request with your proposed changes.</p>

<p dir="auto">This project is licensed under the MIT License.</p>
</article></div></div>
  </body>
</html>
