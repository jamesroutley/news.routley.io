<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/ainews-why-openai-should-build-slack">Original</a>
    <h1>OpenAI should build Slack</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><span>We’re still not over </span><a href="https://www.latent.space/p/ainews-sam-altmans-ai-combinator?utm_source=publication-search" rel="">the Sam Altman town hall</a><span>; at the town hall he said “</span><a href="https://www.youtube.com/live/Wpxv-8nG8ec?si=xu-CMSR2uPrZ_p0R&amp;t=3560" rel="">tell us what we should build, we’ll probably build it!</a><span>” and today at Stanford Treehacks </span><a href="https://x.com/swyx/status/2022529414547017757" rel="">he said</a><span> another thing about how he chooses projects: he thinks of himself as having made a career out of doing things people think are hard, but would be a big deal if it came true.</span></p><p><strong>well okay, Sam: You Should Build Slack. </strong><span>It fits your criteria: it is hard for anyone else without the clout of OpenAI to pull off, it will be very well received by the tech community, and it is an obvious progression of ChatGPT for both your </span><a href="https://www.bigtechnology.com/p/enterprise-will-be-a-top-openai-priority" rel="">Enterprise</a><span> -and- your Coding push and build permanent entrenchment in your customers.</span></p><p><span>Slack </span><a href="https://x.com/swyx/status/1262062882050596864" rel="">rejected developer community</a><span> and went upmarket in 2019, then </span><a href="https://finance.yahoo.com/news/salesforce-completes-slack-acquisition-132247106.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9rYWdpLmNvbS8&amp;guce_referrer_sig=AQAAAIHn6aL6ECAJH2dSErr8YVZLehWRdwRA_q2KzFp8_WzVMfX6CWRlOPG8iQwhJU7OBw8yR61sFu8By2DQp7HpizjBEq4q0OYH62Quw_FMZcYvFIE9B26OylhW0vEdtcOfyNQL7fKiQ-NS_4FL-V3dPP5JEh0CfF7PDggqv3JfrJfZ" rel="">Salesforce bought it for $27.7B in 2021</a><span>, and ever since then Slack has been on a slow rachet up in prices and has struggled to introduce compelling new AI features (</span><a href="https://slack.com/features/ai" rel="">Slack AI</a><span> is occasionally useful but impossible to discover/learn/personalize) while facing </span><a href="https://slack-status.com/calendar" rel="">constant outages</a><span>. NPS feels low, and yet every organization in tech uses it.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!XQAE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XQAE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 424w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 848w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 1272w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!XQAE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png" width="560" height="803.8461538461538" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:2090,&#34;width&#34;:1456,&#34;resizeWidth&#34;:560,&#34;bytes&#34;:755061,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/187930523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!XQAE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 424w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 848w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 1272w, https://substackcdn.com/image/fetch/$s_!XQAE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div></div></div></a></figure></div><p><strong>Everything could be better</strong><span>. Developers routinely complain about Slack’s API costs and permissions (even 3rd or 4th Uber investor and famed vibe coder Jason Calacanis </span><a href="https://youtu.be/CnaegIpkenA?si=dizG0NIKTYXG0Lwz&amp;t=832" rel="">complained on the latest All In podcast</a><span>). Founders routinely complain about the pricing. Slack users complain about channel fatigue and find the Recap tooling and notifications spam woefully inadequate. Huddles could offer far better realtime multimodal AI features.</span></p><p>Slack Connect is great though, definitely just clone that.</p><p><span>Sure, </span><a href="https://openai.com/index/group-chats-in-chatgpt/" rel="">ChatGPT launched group chats</a><span> 3 months ago and probably the usage isn’t great outside of OpenAI. It’d be a mistake to think that repeated </span><a href="https://www.theverge.com/openai/648130/openai-social-network-x-competitor" rel="">half hearted attempts in consumer social AI</a><span> means that you can’t build a successful business social network if you took it as seriously as you do everything else. Microsoft did, and Teams is by all reports a solid success (after </span><a href="https://news.ycombinator.com/item?id=47018386" rel="">a rocky start</a><span>).</span></p><p>In the desktop wars, Anthropic has pursued a far more cohesive strategy than OpenAI: one app for Chat, Cowork, and Claude Code, with optional control of the browser via Claude in Chrome. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!tKgz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!tKgz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 424w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 848w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!tKgz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png" width="1456" height="1195" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1195,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:186761,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/187930523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!tKgz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 424w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 848w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!tKgz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee585153-ea30-423a-a0ab-ee335df4fbc8_1528x1254.png 1456w" sizes="100vw"/></picture><div></div></div></a></figure></div><p><span>By contrast, OpenAI has </span><a href="https://betterthanrandom.substack.com/p/shipping-the-org-chart" rel="">shipped the org chart</a><span> to every user’s desktop: get our </span><a href="https://chatgpt.com/download" rel="">chat app here</a><span>, get our </span><a href="https://openai.com/index/introducing-chatgpt-atlas/" rel="">browser app</a><span> here, get our </span><a href="https://openai.com/index/introducing-the-codex-app/" rel="">coding app</a><span> there. Log in fresh every single time. Even doing a unification at some point probably still leaves you behind; you need to lead, not be a slow follower of what Anthropic already did.</span></p><p><span>“OpenAI Slack” is your chance to retake the initiative. Of course you’re going to be good at chat AI. Of course </span><a href="https://latent.space/p/noam-brown" rel="">you care about the multiagent</a><span> UX of the future. Why not build your own version of the existing multiagent UX we all know to work between humans? Heck, forgot you even hired </span><a href="https://www.cnbc.com/2025/12/09/openai-slack-ceo-denise-dresser-chief-revenue-officer.html" rel="">Slack CEO Denise Dresser in Dec</a><span>. Great!</span></p><p><span>Oh another thing: I bet OpenAI employees would have 10,000 ideas to improve Slack if you owned your own Slack. After all, </span><a href="https://calv.info/openai-reflections" rel="">you guys use this thing more than email</a><span>. The feedback dogfood loop on this one will be the craziest thing since Claude Code.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6DLA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6DLA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 424w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 848w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 1272w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!6DLA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png" width="598" height="580.7167630057803" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1344,&#34;width&#34;:1384,&#34;resizeWidth&#34;:598,&#34;bytes&#34;:798160,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/187930523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6DLA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 424w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 848w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 1272w, https://substackcdn.com/image/fetch/$s_!6DLA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96ecb132-a3fd-4116-976d-853805724dbd_1384x1344.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a></figure></div><p><span>The killer part of course is that this could </span><em>also</em><span> be the coding agent interface you always wanted anyway. The main remaining thing missing from the admittedly very good Codex app is the ability to be truly multiplayer. You haven’t felt the AGI until you have </span><a href="https://x.com/swyx/status/2021498862012334274" rel="">given your designer access to your coding agent</a><span> and let him rip all night with you occasionally chiming in to guide things. You can see swarms of humans and swarms of agents all working together in </span><strong>God’s given orchestration interface: chat</strong><span>.</span></p><p><span>Put another way, it is now time to layer a customer organization’s social graph and work graph onto ChatGPT, and then lather every interface with agents and AI in the way that OpenAI does best. The network effect makes it 10000x harder to leave you for a competitor, and sure, you could do it atop Slack as you currently do, but it’s easy to switch and won’t give you access to freely </span><strong>reinvent the future of work</strong><span>.</span></p><p>To recap:</p><ul><li><p><strong>Is it hard to do? </strong><span>yes for almost everyone except you</span></p></li><li><p><strong>Is it a big deal if you get it right?</strong><span> yes for us users, but an even bigger deal for your business</span></p></li><li><p><strong><span>Will you have lots of low hanging fruit to build new agentic interfaces and a </span><a href="https://www.latent.space/p/ainews-context-graphs-hype-or-actually" rel="">context graph</a><span>/system of record to power </span><a href="https://www.latent.space/p/ainews-ai-vs-saas-the-unreasonable" rel="">Frontier</a><span> and everything else you do in SMB and Enterprise?</span></strong><span> yeah.</span></p></li></ul><p>/fin</p><blockquote><p><em><strong>Feb 14 update: </strong><span>This header is usually at the start of the post, but since it is causing some confusion on HN and </span><a href="https://x.com/swyx/status/2022580899737673810" rel="">Twitter</a><span>, I am moving it down. The editorial written above is always human written, the recaps below are human reviewed.</span></em></p></blockquote><blockquote><p><em><span>AI News for 2/12/2026-2/13/2026. We checked 12 subreddits, </span><a href="https://twitter.com/i/lists/1585430245762441216" rel="">544 Twitters</a><span> and 24 Discords (</span><strong>256</strong><span> channels, and </span><strong>7993</strong><span> messages) for you. Estimated reading time saved (at 200wpm): </span><strong>675</strong><span> minutes. </span><a href="https://news.smol.ai/" rel="">AINews’ website</a><span> lets you search all past issues. As a reminder, </span><a href="https://www.latent.space/p/2026" rel="">AINews is now a section of Latent Space</a><span>. You can </span><a href="https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack" rel="">opt in/out</a><span> of email frequencies!</span></em></p></blockquote><p><span>It’s a pretty quiet day — the </span><a href="https://www.dwarkesh.com/p/dario-amodei-2" rel="">new Dwarkesh-Dario pod </a><span>is worthwhile but hasn’t generated much new conversation on day 1, and </span><a href="https://news.ycombinator.com/item?id=47006594" rel="">OpenAI claimed a big result</a><span> in theoretical physics that is mostly getting questioned by some physicists. This means we get to go back to our backlog of mini-editorial ideas for AINews subscribers!</span></p><p><strong>MiniMax M2.5 open-sourcing: agent-native RL, speed/cost, and rapid ecosystem uptake</strong></p><ul><li><p><strong>MiniMax-M2.5 is now open source</strong><span>: MiniMax released </span><strong>MiniMax-M2.5</strong><span> weights + code, positioning it as an “agent-native” model trained with </span><strong>RL across hundreds of thousands of real-world environments</strong><span> for coding, tool use, search, and office workflows (</span><a href="https://twitter.com/MiniMax_AI/status/2022310932693897628" rel="">MiniMax announcement</a><span>). vLLM highlights day‑0 support and reports key benchmark numbers: </span><strong>80.2% SWE‑Bench Verified</strong><span>, </span><strong>76.3% BrowseComp</strong><span>, plus claims around training scale (200k+ RL environments) and speed/cost characteristics (</span><a href="https://twitter.com/vllm_project/status/2022311342225678757" rel="">vLLM</a><span>). SGLang similarly ships day‑0 support and frames the model as production-grade for “always-on” agents (</span><a href="https://twitter.com/lmsysorg/status/2022319102560555401" rel="">lmsys</a><span>).</span></p></li><li><p><strong>The practical headline is economics + throughput, not just score</strong><span>: MiniMax repeatedly markets </span><strong>“$1 per hour at 100 tps”</strong><span> (interpretable as a “long-horizon agent budget”), which shows up both in their own posts (</span><a href="https://twitter.com/MiniMax_AI/status/2022379949336957254" rel="">MiniMax</a><span>) and in community recaps emphasizing that low activated-parameter count makes self-hosting plausible (</span><a href="https://twitter.com/omarsar0/status/2022384166034190528" rel="">omarsar0</a><span>). Early local runs suggest unusually strong on-device viability for its class: MLX users report ~</span><strong>50 tok/s</strong><span> shortly after release (</span><a href="https://twitter.com/pcuenq/status/2022336556326060341" rel="">pcuenq</a><span>), and a single </span><strong>M3 Ultra 512GB</strong><span> run at </span><strong>6‑bit</strong><span> reports ~</span><strong>40 tok/s</strong><span> with ~</span><strong>186GB</strong><span> peak memory (</span><a href="https://twitter.com/ivanfioravanti/status/2022338870172684655" rel="">ivanfioravanti</a><span>).</span></p></li><li><p><strong>Forge RL training system details leak into the narrative</strong><span>: A Zhihu-derived writeup summarizes MiniMax’s “Forge” RL stack as still </span><strong>CISPO-like</strong><span>, using </span><strong>process reward + completion-time reward</strong><span>, with infrastructure tricks like </span><strong>multi-level prefix cache</strong><span> and high rollout compute share (claimed </span><strong>~60%</strong><span> of compute) generating </span><strong>millions of trajectories/day</strong><span> (</span><a href="https://twitter.com/YouJiacheng/status/2022339475049947576" rel="">YouJiacheng</a><span>). MiniMax leadership explicitly answers parameterization tradeoffs (“</span><strong>10B active</strong><span> intentional”), claims proximity to “</span><strong>infinite agent scaling</strong><span>” with </span><strong>knowledge capacity</strong><span> as the limiter, and teases structural + pretraining innovation focus for </span><strong>M3</strong><span> (</span><a href="https://twitter.com/MiniMax_AI/status/2022370086397624476" rel="">MiniMax reply</a><span>).</span></p></li><li><p><strong>Independent reviews: “viable for multi-turn work” but token-hungry</strong><span>: A Chinese review thread claims M2.5 corrects M2.1’s imbalance (coding up, logic down), with overall improvements and better stability; it notes </span><strong>high token usage</strong><span> (nearly </span><strong>2× Sonnet</strong><span> in one comparison) but frames pricing/compute as making it usable day-to-day (</span><a href="https://twitter.com/ZhihuFrontier/status/2022214461415993817" rel="">ZhihuFrontier</a><span>). Another summary calls it “≤Sonnet for coding, but close,” and emphasizes multi-turn viability as the key break from “toy” open models (</span><a href="https://twitter.com/teortaxesTex/status/2022223441005621556" rel="">teortaxesTex</a><span>).</span></p></li><li><p><strong>Ecosystem uptake is immediate</strong><span>: weights mirrored and packaged across tooling (Hugging Face release pings, GGUF/quant drops, etc.), including Intel-hosted quantized artifacts like a </span><strong>2‑bit GGUF</strong><span> for MiniMax‑M2 and </span><strong>INT4</strong><span> for Qwen3‑Coder‑Next (</span><a href="https://twitter.com/HaihaoShen/status/2022293472796180676" rel="">HaihaoShen</a><span>).</span></p></li></ul><p><strong>GLM‑5 and the “near-frontier” open model wave: performance, infra constraints, and eval chatter</strong></p><ul><li><p><strong>GLM‑5 positioning</strong><span>: Together markets GLM‑5 as best-in-class open-source for long-horizon agents and systems engineering, quoting metrics like </span><strong>77.8% SWE‑Bench Verified</strong><span>, </span><strong>50.4% HLE w/ tools</strong><span>, and a MoE efficiency story with “DeepSeek Sparse Attention” (as described in the tweet) (</span><a href="https://twitter.com/togethercompute/status/2022354579858289052" rel="">Together</a><span>). W&amp;B promotes an interview claiming </span><strong>744B params</strong><span>, a “new RL framework,” and “fully open source under MIT” (as stated in the post) (</span><a href="https://twitter.com/wandb/status/2022389206572765697" rel="">W&amp;B</a><span>). Community members also notice dataset fingerprints like “truthy‑dpo” appearing in GLM‑5 outputs (</span><a href="https://twitter.com/jon_durbin/status/2022291772617945546" rel="">jon_durbin</a><span>).</span></p></li><li><p><strong>GLM‑5 qualitative review highlights</strong><span>: A detailed Zhihu-based comparison frames GLM‑5 as a substantial improvement over GLM‑4.7, especially on hallucination control, programming fundamentals, and character processing—but also more verbose/token-expensive and prone to “overthinking,” suggesting a trade between long-horizon reasoning and compute burn (</span><a href="https://twitter.com/ZhihuFrontier/status/2022161058321047681" rel="">ZhihuFrontier on GLM‑5</a><span>).</span></p></li><li><p><strong>Benchmarks as a moving target</strong><span>: There’s persistent meta-discussion about whether leaderboards/evals are saturated or misleading. Examples: concerns that tokens/latency tradeoffs hide true capability; skepticism about inferring model size from TPS; and the observation that past “SWE‑bench saturation” claims were premature (</span><a href="https://twitter.com/jyangballin/status/2022367240293949772" rel="">jyangballin</a><span>, </span><a href="https://twitter.com/teortaxesTex/status/2022255213394948360" rel="">teortaxesTex</a><span>).</span></p></li><li><p><strong>Cross-checking with alternative evals</strong><span>: SWE‑rebench is cited as “brutal” for some recent releases and shows different relative rankings than SWE‑bench Verified; a caution is made to treat it as “additional signal” (</span><a href="https://twitter.com/maximelabonne/status/2022401174549512576" rel="">maximelabonne</a><span>).</span></p></li></ul><p><strong>Agent engineering in practice: file-based coordination, terminal-first workflows, and “agent OS” framing</strong></p><ul><li><p><strong>Claude Code “Agent Teams” internals are surprisingly simple</strong><span>: A reverse-engineering summary claims Claude Code’s multi-agent comms use </span><strong>JSON files on disk</strong><span> (inboxes under </span><code>~/.claude/teams/inboxes/{agent}.json</code><span>), with polling between turns and JSON-in-JSON protocol messages; the argument is that this is a pragmatic CLI design (no Redis/queues) and improves observability at the cost of atomicity/backpressure (</span><a href="https://twitter.com/peter6759/status/2022156692985983266" rel="">peter6759</a><span>).</span></p></li><li><p><strong>Terminal agents are becoming the default UX</strong><span>: Cline launches </span><strong>Cline CLI 2.0</strong><span>, an open-source terminal coding agent featuring a redesigned interactive TUI, parallel agents with isolated state, headless CI/CD mode, and broad editor support (ACP for Zed/Neovim/Emacs) (</span><a href="https://twitter.com/cline/status/2022341254965772367" rel="">cline</a><span>, </span><a href="https://twitter.com/cline/status/2022341258979717196" rel="">cline details</a><span>). Community framing: “open-source strikes back” due to free/low-barrier access to strong models (</span><a href="https://twitter.com/testingcatalog/status/2022348951459172604" rel="">testingcatalog</a><span>, </span><a href="https://twitter.com/dr_cintas/status/2022387444189139367" rel="">dr_cintas</a><span>). One Cline team member describes a full rewrite (Go → TypeScript) driven by architecture/UX pain and the need to run evals reliably (</span><a href="https://twitter.com/arafatkatze/status/2022415192932651302" rel="">arafatkatze</a><span>).</span></p></li><li><p><strong>Agent scaffolds may matter less than expected (for some horizons)</strong><span>: METR-related discussion suggests Claude Code / Codex scaffolds don’t strongly outperform METR’s “simple OS scaffolds” on measured time horizons so far (</span><a href="https://twitter.com/nikolaj2030/status/2022398669337825737" rel="">nikolaj2030</a><span>), with Ajeya Cotra noting surprise at the small delta (</span><a href="https://twitter.com/ajeya_cotra/status/2022419978495127828" rel="">ajeya_cotra</a><span>). In contrast, others note that for longer, harder tasks, scaffold choice can matter materially (e.g., </span><strong>~10% success</strong><span> swings) (</span><a href="https://twitter.com/gneubig/status/2022451119310655909" rel="">gneubig</a><span>).</span></p></li><li><p><strong>“Agents as OS / filesystem as substrate”</strong><span>: Several posts converge on the idea that file systems are the natural environment for agents (observability, unstructured data manipulation). Box announces integration as a “cloud filesystem” into LangChain deepagents (</span><a href="https://twitter.com/levie/status/2022375298097111160" rel="">levie</a><span>). WebMCP pushes “browser is the API” for web automation without UI perception, with a DoorDash-like starter template (</span><a href="https://twitter.com/skirano/status/2022387763421810989" rel="">skirano</a><span>).</span></p></li><li><p><strong>Key operational lesson: make codebases “agent-ready”</strong><span>: A crisp observation is that agents have “zero tolerance” for entropy humans route around; they treat dead code/outdated docs literally, forcing engineering hygiene that humans always needed but often deferred (</span><a href="https://twitter.com/dok2001/status/2022339274767520246" rel="">dok2001</a><span>).</span></p></li></ul><p><strong>RL/post-training research themes: process rewards, exploration, and rubric-based evaluation</strong></p><ul><li><p><strong>Length-Incentivized Exploration (LIE) for reasoning</strong><span>: A research summary introduces the “Shallow Exploration Trap” (long reasoning trajectories become exponentially unlikely under AR sampling), and proposes LIE: a length reward + redundancy penalty to encourage broader in-context exploration without filler. Reported gains include </span><strong>AIME25 20.5%→26.7%</strong><span> in one setup and small but consistent improvements across other benchmarks/models (</span><a href="https://twitter.com/dair_ai/status/2022360649817526275" rel="">dair_ai</a><span>).</span></p></li><li><p><strong>DPPO vs PPO and “trust region” framing</strong><span>: A long algorithm breakdown contrasts PPO’s token-ratio clipping with DPPO’s distribution-shift control via divergence measures (TV/KL), plus approximations (binary/top‑K) to reduce compute, arguing DPPO is more proportional on rare tokens and better constrains large probability-mass moves (</span><a href="https://twitter.com/TheTuringPost/status/2022326245745377562" rel="">TheTuringPost</a><span>).</span></p></li><li><p><strong>Rubrics-as-rewards and evolving rubrics</strong><span>: A thread describes </span><strong>RLER</strong><span> (RL with evolving rubrics) in Dr. Tulu: seed rubrics with search-grounded criteria, maintain an evolving rubric buffer per prompt, and keep the most discriminative rubrics by reward variance to combat reward hacking and adapt evaluation on-policy (</span><a href="https://twitter.com/cwolferesearch/status/2022384365049892974" rel="">cwolferesearch</a><span>). Separately, a take argues “rubrics as rewards” can beat verifiers-as-reward even in formal-verification settings, recommending verifiers in the loop/harness but not as the sole reward signal (</span><a href="https://twitter.com/davidad/status/2022361016995319850" rel="">davidad</a><span>).</span></p></li><li><p><strong>∆Belief‑RL / information-seeking agents</strong><span>: A new approach rewards actions by how much they increase belief in a target (logprob-based), aiming for long-horizon information seeking without a critic/reward model; claims include generalization from “20 questions” training to new tasks and continued improvement when scaling interaction time (</span><a href="https://twitter.com/ShashwatGoel7/status/2022341054939185345" rel="">ShashwatGoel7</a><span>).</span></p></li><li><p><strong>Human simulation as an RL target</strong><span>: Stanford’s </span><strong>HumanLM</strong><span> + </span><strong>Humanual</strong><span> benchmark propose training LLMs to simulate user responses accurately (human-centric evaluation, preference shaping, policy justification), positioning user simulation as a capability primitive for product/agent design (</span><a href="https://twitter.com/ShirleyYXWu/status/2022374624676421676" rel="">ShirleyYXWu</a><span>).</span></p></li></ul><p><strong>Systems/infra and tooling: FP4 MoE kernels, faster ZeRO loads, model “skills,” and observability</strong></p><ul><li><p><strong>vLLM on GB300 + FP4 MoE acceleration</strong><span>: vLLM reports DeepSeek R1 on </span><strong>GB300</strong><span> with </span><strong>22.5K prefill TGS</strong><span> and </span><strong>3K decode TGS per GPU</strong><span>, claiming large improvements over Hopper, and highlights a recipe including </span><strong>NVFP4 weights</strong><span> and </span><strong>FlashInfer FP4 MoE kernel</strong><span> (</span><code>VLLM_USE_FLASHINFER_MOE_FP4=1</code><span>) plus disaggregated prefill and tuning notes (</span><a href="https://twitter.com/vllm_project/status/2022308974150975792" rel="">vllm_project</a><span>).</span></p></li><li><p><strong>DeepSpeed ZeRO load-time fix</strong><span>: A rework moves tensor flattening from CPU to GPU, significantly improving multi-GPU load times for huge models under ZeRO 1+2 (</span><a href="https://twitter.com/StasBekman/status/2022354880049082658" rel="">StasBekman</a><span>).</span></p></li><li><p><strong>Gemini “Skills” and multimodal tool calling</strong><span>: Google’s Gemini API work includes a “skills” repo teaser (</span><a href="https://twitter.com/osanseviero/status/2022259577232785866" rel="">osanseviero</a><span>) and an Interactions API update enabling </span><strong>multimodal function calling</strong><span> where tools can return </span><strong>images</strong><span> and Gemini can process returned images natively (</span><a href="https://twitter.com/_philschmid/status/2022349886318928158" rel="">philschmid</a><span>). AI Studio billing/upgrade UX is streamlined (upgrade to paid without leaving Studio, usage tracking, spend filters) (</span><a href="https://twitter.com/OfficialLoganK/status/2022409335465480346" rel="">OfficialLoganK</a><span>, </span><a href="https://twitter.com/GoogleAIStudio/status/2022409735287537999" rel="">GoogleAIStudio</a><span>).</span></p></li><li><p><strong>Agent harness instrumentation</strong><span>: ArtificialAnalysis adds end-to-end speed tracking to their agent harness </span><strong>Stirrup</strong><span>, plus per-model breakdowns and tool-call latency metrics—explicitly treating wall-clock completion time as a first-class agent metric (</span><a href="https://twitter.com/ArtificialAnlys/status/2022358995739254800" rel="">ArtificialAnlys</a><span>).</span></p></li><li><p><strong>Local fine-tuning &amp; Apple Silicon workflows</strong><span>: Notable tooling for MLX: real-time transcription with Voxtral Mini 4B in MLX Swift (</span><a href="https://twitter.com/awnihannun/status/2022322714548338962" rel="">awnihannun</a><span>), a no-code local fine-tuning tool exporting to Ollama (</span><a href="https://twitter.com/awnihannun/status/2022327214218657948" rel="">awnihannun</a><span>), and a repo of MLX-LM LoRA examples including GRPO/ORPO/DPO variants (</span><a href="https://twitter.com/ActuallyIsaak/status/2022414004623479014" rel="">ActuallyIsaak</a><span>).</span></p></li></ul><p><strong>“AI accelerates science” moment: GPT‑5.2 + QFT result and the scaffolding narrative</strong></p><ul><li><p><strong>OpenAI claims a novel theoretical physics result with GPT‑5.2</strong><span>: OpenAI announces a preprint showing a gluon interaction previously assumed not to occur can arise under a specific “half-collinear” regime, framed as AI-assisted discovery (</span><a href="https://twitter.com/OpenAI/status/2022390096625078389" rel="">OpenAI</a><span>; preprint link is shared in-thread: </span><a href="https://twitter.com/OpenAI/status/2022390104237707667" rel="">arXiv pointer</a><span>). Kevin Weil adds color: GPT‑5.2 Pro suggested a general formula; an internal scaffolded model then </span><strong>proved it after ~12 hours</strong><span> of continuous work (</span><a href="https://twitter.com/kevinweil/status/2022388305434939693" rel="">kevinweil</a><span>). Discussion emphasizes that pattern-finding + sustained scaffolded reasoning is the differentiator, not just a single chat completion.</span></p></li><li><p><strong>Community reactions range from “significant journal-paper tier” to skepticism about interpretation</strong><span>: Some report physicists calling it a meaningful contribution roughly equivalent to a solid journal paper (</span><a href="https://twitter.com/polynoamial/status/2022413904757035167" rel="">polynoamial</a><span>); others focus on the implications of long-duration productive reasoning and how to measure it in tokens/time (</span><a href="https://twitter.com/teortaxesTex/status/2022401945429000685" rel="">teortaxesTex</a><span>). There’s also meta-discussion about how many employees (or outsiders) can actually evaluate the proof/result, underscoring the evaluation gap for domain-elite work (</span><a href="https://twitter.com/scaling01/status/2022401147110318586" rel="">scaling01</a><span>).</span></p></li></ul><ul><li><p><strong>GitHub adds ability to disable PRs</strong><span> (</span><a href="https://twitter.com/joshmanders/status/2022170444116414790" rel="">joshmanders</a><span>, </span><a href="https://twitter.com/jaredpalmer/status/2022395520623480970" rel="">jaredpalmer</a><span>).</span></p></li><li><p><strong>OpenAI’s GPT‑5.2 physics announcement</strong><span> (</span><a href="https://twitter.com/OpenAI/status/2022390096625078389" rel="">OpenAI</a><span>).</span></p></li><li><p><strong>MiniMax M2.5 open-source release</strong><span> (</span><a href="https://twitter.com/MiniMax_AI/status/2022310932693897628" rel="">MiniMax</a><span>).</span></p></li><li><p><strong>Cline CLI 2.0 launch / open-source terminal agent</strong><span> (</span><a href="https://twitter.com/cline/status/2022341254965772367" rel="">cline</a><span>, </span><a href="https://twitter.com/testingcatalog/status/2022348951459172604" rel="">testingcatalog</a><span>).</span></p></li><li><p><strong>“I am the bottleneck now” (agent-era productivity reflection)</strong><span> (</span><a href="https://twitter.com/thorstenball/status/2022310010391302259" rel="">thorstenball</a><span>).</span></p></li><li><p><strong>Humanoid robotics hands progress (Figure)</strong><span> (</span><a href="https://twitter.com/adcock_brett/status/2022353637964751221" rel="">adcock_brett</a><span>).</span></p></li></ul><ul><li><p><strong><a href="https://www.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/" rel="">MiniMaxAI/MiniMax-M2.5 · Hugging Face</a></strong><span> (Activity: 531): </span><strong><span>MiniMaxAI has released the MiniMax-M2.5 model on </span><a href="https://huggingface.co/models?sort=modified&amp;search=minimax+m2.5" rel="">Hugging Face</a><span>, which is noted for its advanced performance in coding, tool use, and office tasks. The model maintains a size of </span></strong><code>220 billion</code><strong> parameters, contrary to expectations of an increase to </strong><code>800 billion</code><strong> like the GLM5 model. It offers a cost-effective operation at </strong><code>$1 per hour</code><strong> for </strong><code>100 tokens per second</code><strong>, and is enhanced by the Forge reinforcement learning framework, which improves training efficiency and task generalization.</strong><span> Commenters express surprise at the model’s size remaining at </span><code>220 billion</code><span> parameters, highlighting its impressive performance despite not increasing in size. There is also anticipation for the release of a </span><strong>GGUF</strong><span> quantization format, which is not yet available.</span></p><ul><li><p>A user expressed surprise at the model’s size, noting that while they expected an increase to 800 billion parameters to compete with models like GLM5, the MiniMax-M2.5 remains at 220 billion parameters. This is considered impressive given its ‘frontier strength’, suggesting high performance despite the parameter count.</p></li><li><p>Another user mentioned the model’s Q4_K_XL size, which is approximately 130GB. This size is significant as it places the model just beyond the capabilities of some hardware, indicating a need for more robust systems to fully utilize the model’s potential.</p></li><li><p>There is anticipation for the release of FP4/AWQ, indicating that users are looking forward to further advancements or optimizations in the model’s performance or efficiency. This suggests a community eager for improvements that could enhance usability or reduce resource requirements.</p></li></ul></li><li><p><strong><a href="https://www.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/" rel="">MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters</a></strong><span> (Activity: 523): </span><strong>OpenHands has announced the release of the MiniMaxAI MiniMax-M2.5 model, which features </strong><code>230 billion</code><strong> parameters with </strong><code>10 billion</code><strong> active parameters. This model is noted for its performance, ranking 4th in the OpenHands Index, and is </strong><code>13x</code><strong><span> more cost-effective than Claude Opus. It excels in long-running tasks and issue resolution but requires improvements in generalization and task execution accuracy. The model is available for free on the OpenHands Cloud for a limited time. </span><a href="https://huggingface.co/cerebras" rel="">Source</a></strong><span> Commenters are optimistic about the potential of a </span><code>~160B</code><span> REAP/REAM hybrid version, which could be optimized for machines with </span><code>128GB</code><span> of RAM, suggesting a focus on quantization and performance efficiency.</span></p><ul><li><p>The MiniMax-M2.5 model by Moonshot is notable for its architecture, which utilizes 230 billion parameters but only activates 10 billion at a time. This design choice is likely aimed at optimizing computational efficiency, allowing the model to perform well on less powerful hardware, such as GPUs that are not top-of-the-line. This approach could potentially offer a balance between performance and resource usage, making it accessible for more users.</p></li><li><p>A comparison is drawn between MiniMax-M2.5 and other large models like GLM and Kimi. GLM has had to double its parameters to maintain performance, while Kimi has reached 1 trillion parameters. The implication is that MiniMax-M2.5 achieves competitive performance with fewer active parameters, which could be a significant advancement in model efficiency and scalability.</p></li><li><p>The potential for further optimization through quantization is highlighted, suggesting that MiniMax-M2.5 could be made even more efficient. Quantization could reduce the model’s size and increase its speed, making it feasible to run on machines with 128GB of RAM while still leaving room for additional tasks such as deep-context tool use. This could make the model particularly attractive for users with limited computational resources.</p></li></ul></li><li><p><strong><a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/" rel="">Minimax M2.5 Officially Out</a></strong><span> (Activity: 765): </span><strong>Minimax M2.5 has been officially released, showcasing impressive benchmark results: </strong><code>SWE-Bench Verified</code><strong> at </strong><code>80.2%</code><strong>, </strong><code>Multi-SWE-Bench</code><strong> at </strong><code>51.3%</code><strong>, and </strong><code>BrowseComp</code><strong> at </strong><code>76.3%</code><strong>. The model is noted for its cost efficiency, with operational costs significantly lower than competitors like Opus, Gemini 3 Pro, and GPT-5. Specifically, running M2.5 at </strong><code>100 tokens per second</code><strong> costs </strong><code>$1 per hour</code><strong>, and at </strong><code>50 TPS</code><strong>, it costs </strong><code>$0.3 per hour</code><strong><span>, making it a cost-effective solution for continuous operation. More details can be found on the </span><a href="https://www.minimax.io/news/minimax-m25" rel="">official Minimax page</a><span>.</span></strong><span> Commenters highlight the potential game-changing nature of Minimax M2.5 due to its low operational costs compared to other models. There is also anticipation for the release of open weights on platforms like Hugging Face.</span></p><ul><li><p>The Minimax M2.5 model is highlighted for its cost-effectiveness, with operational costs significantly lower than competitors like Opus, Gemini 3 Pro, and GPT-5. Specifically, running M2.5 at 100 tokens per second costs $1 per hour, and at 50 tokens per second, it costs $0.3 per hour. This translates to an annual cost of $10,000 for four instances running continuously, making it a potentially disruptive option in terms of affordability.</p></li><li><p>There is anticipation for the release of open weights on Hugging Face, which would allow for broader experimentation and integration into various applications. This suggests a community interest in transparency and accessibility for further development and benchmarking.</p></li><li><p>The potential impact of Minimax M2.5 on existing models like GLM 5.0 and Kimi 2.5 is discussed, with some users suggesting that if the reported benchmarks are accurate, M2.5 could surpass these models in popularity due to its ease of use and cost advantages. This indicates a shift in preference towards models that offer better performance-to-cost ratios.</p></li></ul></li></ul></div></div></div>
  </body>
</html>
