<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://borretti.me/article/language-pragmatics">Original</a>
    <h1>Language Pragmatics Engineering</h1>
    
    <div id="readability-page-1" class="page"><article>
    <p>Summary:</p>

<ol>
  <li>The code that gets written is the code that’s easier to write.</li>
  <li>Anything not forbidden by the language semantics will be done as a “temporary
fix”.</li>
  <li>Codebases decay along the gradient of expedient hacks.</li>
</ol>

<p>Programming languages have syntax, semantics, and <a href="https://en.wikipedia.org/wiki/Pragmatics">pragmatics</a>: how the
language is used in practice. The latter is harder to design for. Language
pragmatics is tooling, best practices, and the code you see in the
wild. Pragmatics matter because they determine the shape of the efficient
frontier between expedience and engineering quality, and from then on it’s
gradient descent.</p>



<ol>
  <li><a href="#types">Types</a></li>
  <li><a href="#shotgun">Shotgun I/O</a></li>
  <li><a href="#tooling">Tooling</a></li>
  <li><a href="#tedium">Tedium</a></li>
</ol>



<p><strong>Summary:</strong> make types easier to define.</p>

<p>One fundamental principle of language pragmatics: the code that gets written is
the code that’s easier to write. Consider Java and Haskell.</p>

<p>In Java, defining a class is a monumental task. The language certainly makes it
feel that way. You have to create a whole new file just for that class. The
fields must be written in quadriplicate: the fields themselves then the
constructor, the getters, and setters. <code>equals</code> and <code>hashCode</code> methods must be
implemented. The consequence: Java projects have fewer classes than they
otherwise would have. Classes tend towards <a href="https://en.wikipedia.org/wiki/God_object">God classes</a>: large classes
with multiple incompatible uses and responsibilities. They accumulate, for
example, fields that start out as <code>null</code> and become non-<code>null</code> after certain
operations are performed on the class.</p>

<p>Whereas in Haskell a type is a few lines of code. You <code>derive</code> the equivalents
of <code>equals</code> and <code>hashCode</code>. The consequence: Haskell projects have more types
than equivalent Java projects have classes. Those types are narrower, more
specific. Instead of mutating fields on the same class, you can have distinct
types to represent the different stages that a value goes through as it
traverses the codebase. <code>null</code> goes away.</p>

<p>In Java the problem is compounded by the fact that the simulationist school of
OOP endorses the idea that each concept in the domain of discourse object should
correspond to a class. So while in a Haskell project you may have fiften types
to represent a purchase order (representing different stages of
validation/enrichment, like the intermediate representations in a compiler), in
Java you’d have one omnipresent <code>Order</code> class that is tied to every component in
the system. In Haskell the types are <a href="https://martinfowler.com/bliki/AnemicDomainModel.html">anemic</a> and proudly so, in Java
this would be called an antipattern.</p>



<p><strong>Summary:</strong> ban “quick fixes” at the level of language semantics, or make them
hard to write.</p>

<p>By analogy to <a href="http://langsec.org/papers/langsec-cwes-secdev2016.pdf">shotgun parsers</a>, shotgun IO is when effectful code is
smeared all over the codebase.</p>

<p>Consider Haskell vs. everything else. Many language communities have some best
practice about <a href="http://eweise.com/post/sideeffects/">moving IO to the edges</a>. But there is no strict
language-level enforcement of this rule. Because of expedience, because you have
to release this tonight or else, someone puts some database access code deep
inside otherwise pure data transform code, along with a <code>FIXME</code> comment.</p>

<p>Gradually you go down the slope of pragmatics: I/O is spread across the
codebase. In Haskell, there is no way out: you have to use the <code>IO</code> monad. And
so if you’re mixing IO and pure code, you know it, because it’s right there in
the function signature, and that knowledge gives you an opposite gradient, you
know the direction you have to move to make the code cleaner.</p>

<p>Or consider something like the <a href="https://docs.djangoproject.com/en/4.2/topics/db/">Django ORM</a>: you can write some database
access code that returns a <a href="https://docs.djangoproject.com/en/4.2/ref/models/querysets/"><code>QuerySet</code></a>. Then you can either read from that
<code>QuerySet</code>, or pass it on to some other function, which may perform further
queries on it.</p>

<p>A consequence of this is that anything that touches a <code>QuerySet</code> can kick off
database access. There is no requirement that database access be
centralized—in a <a href="https://en.wikipedia.org/wiki/Data_access_object">DAO</a> class, for example—and so because of the
pressures of expedience database queries crawl, hack after hack, like hydrogen
molecules through the interstitial spaces of a crystal, over the entire
codebase.</p>



<p><strong>Summary:</strong> bad tooling goes unused but is hard to displace, lowering economic
productivity.</p>

<p>In C and C++ there are many build systems and package managers, but none is the
obvious <a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)">focal point</a> in the way <a href="https://doc.rust-lang.org/cargo/">cargo</a> is for Rust. So people
vendor their dependencies, sometimes downloading them by hand. Smaller libraries
are often made <a href="https://en.wikipedia.org/wiki/Header-only">header-only</a>, this is advetised as a feature because
they require less build system overhead to integrate into your projects. This
discourages dependencies, which is basically the same as discouraging the
division of labour, with obvious costs.</p>

<p>And even bad tooling is hard to replace, again, because of coordination costs
and the permanence of focal points.</p>

<p>Consider Common Lisp: <a href="https://asdf.common-lisp.dev/">ASDF</a> is the standard build system, and
<a href="https://www.quicklisp.org/beta/">Quicklisp</a> the standard package manager. ASDF doesn’t support specifying
version bounds, and even if it did, Quicklisp doesn’t support using them. And
Quicklisp, despite being over 10 years old, is still in beta and feels like
alpha quality software: Quicklisp downloads packages over HTTP, and you are at
the mercy of God and the feds. So while the rest of the world has moved on to
reproducible builds, determinism, and SAT solving for dependency resolution, in
the Common Lisp world you can’t even fix dependency versions.</p>

<p>And who’s going to replace this? You’re going to rewrite everyone’s ASDF system
definition files? There’s an alternative to Quicklisp, called <a href="https://www.clpm.dev/">CLPM</a>. It’s
abandonware. We’ll fix Common Lisp tooling the day after <a href="https://borretti.me/article/youre-not-going-anywhere">we all move to
Mastodon</a>.</p>

<p>And somehow Common Lisp tooling is a pleasure to use compared to the tooling for
Python, a language that has multiple orders of magnitude more headcount and
investment.</p>

<p>Fixing entrenched tooling requires building an alternative that’s 10x better,
making the transition process as seamless as possible, and then reaching out and
opening PRs in every repository under the sun making the necessary
changes. People could hate the old tooling and it’s still costly for them to
change.</p>



<p><strong>Summary:</strong> programmers will go to great lengths to avoid writing tedious code,
to the detriment of other things.</p>

<p><a href="http://www.loper-os.org/?p=165">Erik Naggum</a> and the <a href="https://groups.google.com/g/comp.lang.lisp/c/Vn31kjztWpQ#52564cc186195b05">parable of the catapult</a>:</p>

<blockquote>
  <p>People use C because it <em>feels</em> faster. Like, if you build a catapult strong
enough that it can hurl a bathtub with someone crouching inside it from London
to New York, it will feel <em>very</em> fast both on take-off and landing, and
probably during the ride, too, while a comfortable seat in business class on a
transatlantic airliner would probably take less time (except for getting to and
from the actual plane, of course, what with all the “security”¹) but you would
not <em>feel</em> the speed nearly as much.</p>
</blockquote>

<p>The argument here is there’s two kinds of productivity: productivity in the
small (immediately) and productivity in the large (when you add up the cost of
testing, bugfixes, refactoring, onboarding etc.). C is productive in the small
and unproductive in the large.</p>

<p>You observe this with text editing. Moving with the arrow keys, erasing text by
holding backspace for an eternity <em>feels</em> slow, but it is predictable and
reliable. Vim or Emacs-style editing, where you fly through the buffer with
keybindings, feels faster, but a single wrong keypress puts your editor in some
unpredictable state it can be hard to recover from. Sometimes the midwit with
Notepad gets to the destination faster because they’re not fiddling with their
psychotically-optimized Emacs <code>neuralink-mode</code> bindings.</p>

<p>You observe this with types. Dynamic types feel faster, at the REPL, when you’re
<a href="https://medium.com/hackernoon/software-development-at-1-hz-5530bb58fc0e">coding at 1Hz</a>, because you’re not factoring in the (unseen) cost of future
bugs, the cost of refactors you <em>won’t</em> do because you don’t have the confidence
to refactor which static types give you, the cost of legacy software that can’t
be replaced because it can’t be understood by anyone, the cost of a Python
server doing four requests per second while you pay five figures to AWS every
month.</p>

<p>You observe this with macros. To avoid writing four lines of
almost-but-not-quite duplicated code programmers will reach for a macro
system. One-tenth of every OCaml or Rust codebase by mass is <code>#[attributes()]</code>
to tell the macro system to generate e.g. JSON serialization or pretty-printing
code or database interfaces. I do it too. In the grand scheme of things this
doesn’t save that much time: tedious code is fast to write but it only <em>feels</em>
slow. What is the cost? The cost is the codebase becomes harder and harder to
understand, because all of the interesting behaviour is <em>outside the source
code</em>.</p>

<p>It’s like those photographs of distant quasars: the “photograph” is the output
of a pipeline of Fortran codes written to interpret the data from six different
instruments written between 1960 and 2005.</p>

<p>With metaprogramming, your codebase is an input to a vast and unseen system: a
pipeline of build-time macroexpansion that spits out something that may very
well be unrecognizable. And for what? To save a minute of typing?  Because we
are programmers, not typists, and it is below our dignity to type?</p>

<p>Some languages do macros right, like Common Lisp, and you should take advantage
of them. For essentially every other language, metaprogramming should produce
outputs you can check into source control. That is: generated code is fine, but
you must be able to see it.</p>

<p>The central, underlying fallacy here is <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)#Salience_bias">salience bias</a>: measuring what is
seen and not what is unseen.</p>


  </article></div>
  </body>
</html>
