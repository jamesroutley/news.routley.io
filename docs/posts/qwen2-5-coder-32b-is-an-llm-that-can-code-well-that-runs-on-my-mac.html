<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2024/Nov/12/qwen25-coder/">Original</a>
    <h1>Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac</h1>
    
    <div id="readability-page-1" class="page"><div>


<div data-permalink-context="/2024/Nov/12/qwen25-coder/">

<p>12th November 2024</p>



<p>There’s a whole lot of buzz around the new <a href="https://qwenlm.github.io/blog/qwen2.5-coder-family/">Qwen2.5-Coder Series</a> of open source (Apache 2.0 licensed) LLM releases from Alibaba’s Qwen research team. On first impression it looks like the buzz is well deserved.</p>
<p>Qwen claim:</p>
<blockquote>
<p>Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o.</p>
</blockquote>
<p>That’s a <em>big</em> claim for a 32B model that’s small enough that it can run on my 64GB MacBook Pro M2. The Qwen published scores look impressive, comparing favorably with GPT-4o and Claude 3.5 Sonnet (October 2024) edition across various code-related benchmarks:</p>
<p><img src="https://static.simonwillison.net/static/2024/qwen-scores.jpg" alt="In benchmark comparisons, Qwen 2.5 Coder (32B Instruct) outperforms both GPT-4o and Claude 3.5 Sonnet on LiveCodeBench, Spider, and BIRD-SQL metrics, falls behind on MBPP, Aider, and CodeArena, shows mixed results on MultiPL-E, and performs similarly on HumanEval and McEval benchmarks."/></p>
<p>How about benchmarks from other researchers? Paul Gauthier’s <a href="https://aider.chat/docs/leaderboards/">Aider benchmarks</a> have a great reputation and <a href="https://twitter.com/paulgauthier/status/1856018124031832236">Paul reports</a>:</p>
<blockquote>
<p>The new Qwen 2.5 Coder models did very well on aider’s code editing benchmark. The 32B Instruct model scored in between GPT-4o and 3.5 Haiku.</p>
<p>84% 3.5 Sonnet,
75% 3.5 Haiku,
74% Qwen2.5 Coder 32B,
71% GPT-4o,
69% Qwen2.5 Coder 14B,
58% Qwen2.5 Coder 7B</p>
<p><img src="https://static.simonwillison.net/static/2024/qwen-paul.jpg" alt="Those numbers as a chart"/></p>
</blockquote>
<p>That was for the Aider “whole edit” benchmark. The “diff” benchmark <a href="https://twitter.com/paulgauthier/status/1856042640279777420">scores well</a> too, with Qwen2.5 Coder 32B tying with GPT-4o (but a little behind Claude 3.5 Haiku).</p>
<p>Given these scores (and the <a href="https://www.reddit.com/r/LocalLLaMA/comments/1gp84in/qwen25coder_32b_the_ai_thats_revolutionizing/">positive buzz on Reddit</a>) I had to try it for myself.</p>
<p>My attempts to run the <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF">Qwen/Qwen2.5-Coder-32B-Instruct-GGUF</a> Q8 using <a href="https://github.com/simonw/llm-gguf">llm-gguf</a> were a bit too slow, because I don’t have that compiled to use my Mac’s GPU at the moment.</p>
<p>But both the <a href="https://ollama.com/">Ollama</a> version <em>and</em> the <a href="https://github.com/ml-explore/mlx">MLX</a> version worked great!</p>
<p>I installed the Ollama version using:</p>
<pre><code>ollama pull qwen2.5-coder:32b
</code></pre>
<p>That fetched a 20GB quantized file. I ran a prompt through that using my <a href="https://llm.datasette.io/">LLM</a> tool and Sergey Alexandrov’s <a href="https://github.com/taketwo/llm-ollama">llm-ollama</a> plugin like this:</p>
<pre><code>llm install llm-ollama
llm models # Confirming the new model is present
llm -m qwen2.5-coder:32b &#39;python function that takes URL to a CSV file and path to a SQLite database, fetches the CSV with the standard library, creates a table with the right columns and inserts the data&#39;
</code></pre>
<p>Here’s <a href="https://gist.github.com/simonw/0a47f9e35a50d4e25a47826f4ab75dda">the result</a>. The code worked, but I had to work around a frustrating <code>ssl</code> bug first (which wouldn’t have been an issue if I’d allowed the model to use <code>requests</code> or <code>httpx</code> instead of the standard library).</p>
<p>I also tried running it using the Apple Silicon fast array framework MLX using the <a href="https://github.com/riccardomusmeci/mlx-llm">mlx-llm</a> library directly, run via <a href="https://github.com/astral-sh/uv">uv</a> like this:</p>
<pre><code>uv run --with mlx-lm \
  mlx_lm.generate \
  --model mlx-community/Qwen2.5-Coder-32B-Instruct-8bit \
  --max-tokens 4000 \
  --prompt &#39;write me a python function that renders a mandelbrot fractal as wide as the current terminal&#39;
</code></pre>
<p>That gave me a <em>very</em> <a href="https://gist.github.com/simonw/1cc1e0418a04dbd19cd281cf9b43666f">satisfying result</a>—when I ran the code it generated in a terminal I got this:</p>
<p><img src="https://static.simonwillison.net/static/2024/mlx-fractal.jpg" alt="macOS terminal window displaying a pleasing mandelbrot fractal as ASCII art"/></p>
<p>This is a really promising development. 32GB is just small enough that I can run the model on my Mac without having to quit every other application I’m running, and both the speed and the quality of the results feel genuinely competitive with the current best of the hosted models.</p>

<p>Given that code assistance is probably around 80% of my LLM usage at the moment this is a meaningfully useful release for how I engage with this class of technology.</p>


</div>


</div></div>
  </body>
</html>
