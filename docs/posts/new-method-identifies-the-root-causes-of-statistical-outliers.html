<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.amazon.science/blog/new-method-identifies-the-root-causes-of-statistical-outliers">Original</a>
    <h1>New method identifies the root causes of statistical outliers</h1>
    
    <div id="readability-page-1" class="page"><div>
                            
                                <div>
    
        <div><p><a href="https://en.wikipedia.org/wiki/Outlier" target="_blank" data-cms-ai="0">Outliers</a> are rare observations where a system deviates from its usual behavior. They arise in many real-world applications (e.g., medicine, finance) and present a greater demand for explanation than ordinary events. How can we identify the &#34;root causes&#34; of outliers once they are detected?</p><p>The problem of outliers is one of the oldest problems in statistics. It has been the subject of academic investigation for more than a century. Although a lot has been done on <a href="https://link.springer.com/book/10.1007/978-94-015-3994-4" target="_blank" data-cms-ai="0">detecting outliers</a>, a formal way to define the “root causes” of outliers has been lacking.</p><p>This week, at the International Conference on Machine Learning (<a href="https://www.amazon.science/conferences-and-events/icml-2022" data-cms-ai="0">ICML</a>), we are presenting <a href="https://www.amazon.science/publications/causal-structure-based-root-cause-analysis-of-outliers" data-cms-ai="0">our work</a> on identifying the root causes of outliers. Our first task was to introduce a formal definition of “root cause”, because we were not able to find one in the academic literature.</p><p>Our definition includes a formalization of the <i>quantitative causal contribution </i>of each of the root causes of an observed outlier. In other words, the contribution describes the <i>degree</i> to which a variable is responsible for the outlier event. This also relates to philosophical questions; even the purely qualitative question of whether an event is an “actual cause” of others is an ongoing debate among philosophers.</p><p>Our approach is based on <a href="http://bayes.cs.ucla.edu/WHY/" target="_blank" data-cms-ai="0">graphical causal models</a>, a formal framework developed by Turing Award winner <a href="https://en.wikipedia.org/wiki/Judea_Pearl" target="_blank" data-cms-ai="0">Judea Pearl</a> to model cause-effect relationships between variables in a system. It has two key ingredients. The first is a causal diagram, which visually represents the cause-effect relationships among observed variables, with arrows from the nodes representing causes to the nodes representing effects. The second is a set of causal mechanisms, which describe how the values of each node are generated from the values of its parents (i.e., direct causes) in the causal diagram.</p><p>Imagine, for instance, a retail website powered by distributed web services. A customer experiences an unusually slow loading time. Why? Is it a slow database in the back end? A malfunctioning buying service?</p><div data-align-center-expanded="">
            <div><figure>
    

    
        <p><figcaption>At left, we have the dependencies between the distributed web services that power a simple hypothetical retail website. In the middle, a customer (with ID 5) experiences a very slow loading time. Our goal is to identify its root causes among the distributed services <i>(right)</i>.</figcaption></p>
    
</figure></div>
        </div><p>There exist many <a href="https://link.springer.com/book/10.1007/978-94-015-3994-4" target="_blank" data-cms-ai="0">outlier detection algorithms</a>. To identify the root causes of outliers detected by one of these algorithms, we first introduce an information-theoretic (IT) outlier score, which probabilistically calibrates existing outlier scores.</p><p>Our outlier score relies on the notion of the <i>tail probability</i> — the probability that a random variable exceeds a threshold value. The IT outlier score of an event is the negative logarithm of the event’s tail probability under some transformation. It is inspired by Claude Shannon’s definition of the <a href="https://en.wikipedia.org/wiki/Information_content" target="_blank" data-cms-ai="0">information content</a> of a random event in <a href="https://en.wikipedia.org/wiki/Information_theory" target="_blank" data-cms-ai="0">information theory</a>.</p><p>The lower the likelihood of observing events more extreme than the event in question, the more information that event carries, and the larger its IT outlier score. Probabilistic calibration also renders IT outlier scores comparable across variables with different dimension, range, and scaling.</p><h2>Counterfactuals</h2><p>To attribute the outlier event to a variable, we ask the counterfactual question “Would the event not have been an outlier had the causal mechanism of that variable been normal?” The counterfactuals are the third rung on <a href="http://bayes.cs.ucla.edu/WHY/why-ch1.pdf" target="_blank" data-cms-ai="0">Pearl’s ladder of causation</a> and hence require <a href="http://bayes.cs.ucla.edu/BOOK-2K/ch1-4.pdf" target="_blank" data-cms-ai="0">functional causal models (FCMs)</a> as the causal mechanisms of variables.</p><p>In an FCM, each variable <i>X<sub>j</sub></i> is a function of its observed parents <i>PA<sub>j</sub></i> (with direct arrows to <i>X<sub>j</sub></i>) in the causal diagram and an unobserved noise variable <i>N<sub>j</sub></i>. As root nodes — those without observed parents — have only noise variables, the joint distribution of noise variables gives rise to the stochastic properties of observed variables.</p><p>The unobserved noise variables play a special role: we can think of <i>N<sub>j</sub></i> as a random switch that selects a deterministic function (or mechanism) from a set of functions <i>F<sub>j</sub></i> defined from direct causes <i>PA<sub>j</sub></i> to their effect <i>X<sub>j</sub></i>. If, instead of fixing the value of the noise term <i>N<sub>j</sub></i>, we set it to random values drawn from some distribution, then the functions from the set <i>F<sub>j</sub></i><sub> </sub>are also selected at random, and we can use this procedure to assign normal deterministic mechanisms to <i>Xj</i>.</p><p>Although this randomization operation might seem infeasible if we think of the noise variable as something not under our control — and even worse, not even observable — we can interpret it as an intervention on the observed variable.</p><div data-align-center-expanded="">
            <div><figure>
    

    
        <p><figcaption>On the left, for the observed pair (<i>x<sub>j</sub>, pa<sub>j</sub></i>) of variable <i>X<sub>j</sub></i> and its parents <i>PA<sub>j</sub></i>, the deterministic mechanism <i>f<sub>j</sub><sup>(1)</sup></i> of variable <i>X<sub>j</sub></i> is identified by the noise value (<i>N<sub>j</sub></i> = 1) corresponding to the pair (<i>x<sub>j</sub>, pa<sub>j</sub></i>). In the middle, a different value of noise (<i>N<sub>j </sub>= n</i>) identifies a <b>counterfactual</b> deterministic mechanism <i>f<sub>j</sub></i>(<i>n</i>). On the right, by drawing random samples of the noise term <i>N<sub>j</sub></i> according to some distribution, we assign “normal” deterministic mechanisms to <i>X<sub>j</sub></i>.</figcaption></p>
    
</figure></div>
        </div><div data-align-right="">
            <div><figure>
    

    
        <p><figcaption>To attribute the outlier event <i>x<sub>n</sub></i> of target variable <i>X<sub>n</sub></i> to a variable <i>X<sub>j</sub></i>, we first replace the deterministic mechanism of <i>X<sub>j</sub></i><sub> </sub>by normal causal mechanisms (the orange background indicates the replacement). Then we measure the impact of this replacement on the log tail probability of the outlier event.</figcaption></p>
    
</figure></div>
        </div><p>To attribute the outlier event <i>x<sub>n</sub></i> (of target variable <i>X<sub>n</sub></i>) to a variable <i>X<sub>j</sub></i>, we first replace the deterministic mechanism corresponding to its observation <i>x<sub>j</sub></i> by normal mechanisms. The impact of this replacement on the log tail probability defines the contribution of <i>X<sub>j</sub></i> to the outlier event. In particular, the contribution measures the factor by which replacing the causal mechanism of <i>X<sub>j</sub></i> with normal mechanisms (by drawing random samples of the noise <i>N<sub>j</sub></i>) decreases the likelihood of the outlier event. But the contribution computed this way depends on the order in which we replace the causal mechanisms. This dependence on ordering introduces arbitrariness into the attribution procedure.</p><p>To get rid of the dependence on the ordering of variables, we take the average contribution over all orderings, which is also the idea behind the <a href="https://en.wikipedia.org/wiki/Shapley_value" target="_blank" data-cms-ai="0">Shapley value approach</a> in game theory. The Shapley contributions sum up to the IT outlier score of the outlier event.</p><p>To get a high-level idea of how our approach works, consider again the retail-website example mentioned above. Dependencies between web services are typically available as a dependency graph. By inverting the arrows in the dependency graph, we obtain the causal graph of latencies of services. From training samples of observed latencies, we learn the causal mechanisms. The causal mechanisms may also be established directly using subject matter expertise. Our approach uses those to attribute the slow loading time for the specific client to its most likely root causes among the web services.</p><div data-align-center-expanded="">
            <div><figure>
    

    
        <p><figcaption>On the left, we have the causal graph of latencies of services, which is obtained by inverting the arrows of the dependency graph of services. By learning the causal mechanisms of nodes from training data, our approach yields the contributions of each node to the outlier event — here, the unusually high latency of the web service. As the Shapley contributions sum up to the IT outlier score of the outlier event, we are able to show the relative contribution of ancestors — here, the services.</figcaption></p>
    
</figure></div>
        </div><p>If you would like to apply our approach to your use case, the implementation is available in the <a href="https://arxiv.org/abs/2206.06821" target="_blank" data-cms-ai="0">“gcm” package</a> in the Python <a href="https://py-why.github.io/dowhy/" target="_blank" data-cms-ai="0">DoWhy</a> library. To get started quickly, you can check out our <a href="https://py-why.github.io/dowhy/example_notebooks/rca_microservice_architecture.html" target="_blank" data-cms-ai="0">example notebook</a>.</p></div>
    
</div>

                            
                        </div></div>
  </body>
</html>
