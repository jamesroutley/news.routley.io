<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.math3ma.com/blog/the-tensor-product-demystified">Original</a>
    <h1>The tensor product, demystified (2018)</h1>
    
    <div id="readability-page-1" class="page"><div href=""><p><a href="https://www.math3ma.com/blog/limits-and-colimits-part-1">Previously on the blog</a>, we&#39;ve discussed a recurring theme throughout mathematics: making new things from old things. Mathematicians do this all the time:</p><ul role="list"><li>When you have two integers, you can find their greatest common divisor or least common multiple.</li><li>When you have some sets, you can form their Cartesian product or their union.</li><li>When you have two groups, you can construct their direct sum or their free product.</li><li>When you have a topological space, you can look for a subspace or a quotient space.</li><li>When you have some vector spaces, you can ask for their direct sum or their intersection.</li><li>The list goes on!</li></ul><p>Today, I&#39;d like to focus on a particular way to build a new vector space from old vector spaces: <em>the tensor product</em>. This construction often come across as scary and mysterious, but I hope to help shine a little light and dispel some of the fear. In particular, we won&#39;t talk about axioms, universal properties, or <a href="https://www.math3ma.com/blog/commutative-diagrams-explained">commuting diagrams</a>. Instead, we&#39;ll take an elementary, concrete look:</p><p>Given two vectors $\mathbf{v}$ and $\mathbf{w}$, we can build a new vector, called the <em>tensor product</em> $\mathbf{v}\otimes \mathbf{w}$. But what is that vector, <em>really</em>? Likewise, given two vector spaces $V$ and $W$, we can build a new vector space, also called their <em>tensor product</em> $V\otimes W$. But what is that vector space, <em>really</em>? </p><h2>Making new vectors from old</h2><p>In this discussion, we&#39;ll assume $V$ and $W$ are finite dimensional vector spaces. That means we can think of $V$ as $\mathbb{R}^n$ and $W$ as $\mathbb{R}^m$ for some positive integers $n$ and $m$. So a vector $\mathbf{v}$ in $\mathbb{R}^n$ is really just a list of $n$ numbers, while a vector $\mathbf{w}$ in $\mathbb{R}^m$ is just a list of $m$ numbers.</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bc29557faf7dc810b681e2d_vw.jpg" alt=""/></p></figure><p>Let&#39;s try to make new, third vector out of $\mathbf{v}$ and $\mathbf{w}$. But how? Here are two ideas: We can stack them on top of each other, or we can first multiply the numbers together and <em>then</em> stack them on top of each other.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf17a1638d56499ff9ff2ad_option%2012.jpg" alt=""/></p></figure><p>The first option gives a new list of $n+m$ numbers, while the second option gives a new list of $nm$ numbers. The first gives a way to build a new space where the dimensions <em>add;</em> the second gives a way to build a new space where the dimensions <em>multiply</em>. The first is a vector $(\mathbf{v},\mathbf{w})$ in the <strong>direct sum </strong>$V\oplus W$ (this is the same as their direct product $V\times W$); the second is a vector $\mathbf{v}\otimes \mathbf{w}$ in the <strong>tensor product</strong> $V\otimes W$.</p><p>And that&#39;s it! </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf17bd763157b3c9879bd06_analogy.jpg" alt=""/></p></figure><p>Forming the tensor product $\mathbf{v}\otimes \mathbf{w}$ of two vectors is <em>a lot</em> like forming the Cartesian product of two sets $X\times Y$. In fact, that&#39;s exactly what we&#39;re doing if we think of $X$ as the set whose elements are the entries of $\mathbf{v}$ and similarly for $Y$. </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf17e62cb149a31764a85fa_XY.jpg" alt=""/></p></figure><p>So a tensor product is like a grown-up version of multiplication. It&#39;s what happens when you systematically multiply a bunch of numbers together, then organize the results into a list. It&#39;s multi-multiplication, if you will.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf192dbd917b20e519b6304_examples.jpg" alt=""/></p></figure><h2>There&#39;s a little more to the story.</h2><p>Does <em>every</em> vector in $V\otimes W$ look like $\mathbf{v}\otimes\mathbf{w}$ for some $\mathbf{v}\in V$ and $\mathbf{w}\in W$? Not quite. Remember, a vector in a vector space can be written as a weighted sum of <em>basis vectors</em>, which are like the space&#39;s building blocks. This is another instance of making new things from existing ones: we get a new vector by taking a weighted sum of some special vectors!</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf18109d730c7217fb4b1a3_basis.jpg" alt=""/></p></figure><p>So a typical vector in $V\otimes W$ is a weighted sum of basis vectors. W<em>hat are those basis vectors</em>? Well, there must be exactly $nm$ of them, since the dimension of $V\otimes W$ is $nm$. Moreover, we&#39;d expect them to be built up from the basis of $V$ <em>and</em> the basis of $W$. This brings us again to the &#34;How can we construct new things from old things?&#34; question. Asked explicitly: If we have $n$ bases $\mathbf{v}_1,\ldots,\mathbf{v}_n$ for $V$ and if we have $m$ bases $\mathbf{w}_1,\ldots,\mathbf{w}_m$ for $W$ then how can we combine them to get a new set of $nm$ vectors?</p><p>This is totally analogous to the construction we saw above: given a list of $n$ things and a list of $m$ things, we can obtain a list of $nm$ things by multiplying them all together. So we&#39;ll do the same thing here! We&#39;ll simply multiply the $\mathbf{v}_i$ together with the $\mathbf{w}_j$ in all possible combinations, <em>except</em> &#34;multiply $\mathbf{v}_i$ and $\mathbf{w}_j$ &#34; now means &#34;take the tensor product of $\mathbf{v}_i$ and $\mathbf{w}_j$.&#34; </p><p>Concretely, a basis for $V\otimes W$ is the set of all vectors of the form $\mathbf{v}_i\otimes\mathbf{w}_j$ where $i$ ranges from $1$ to $n$ and $j$ ranges from $1$ to $m$. As an example, suppose $n=3$ and $m=2$ as before. Then we can find the six basis vectors for $V\otimes W$ by forming a &#39;multiplication chart.&#39; (The sophisticated way to say this is: &#34;$V\otimes W$ is the free vector space on $A\times B$, where $A$ is a set of generators for $V$ and $B$ is a set of generators for $W$.&#34;)</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bc29013b7167189bc20f12e_tensor%20chart.jpg" alt=""/></p></figure><p>So $V\otimes W$ is the six-dimensional space with basis</p><p>$$\{\mathbf{v}_1\otimes\mathbf{w}_1,\;\mathbf{v}_1\otimes\mathbf{w}_2,\; \mathbf{v}_2\otimes\mathbf{w}_1,\;\mathbf{v}_2\otimes\mathbf{w}_2,\;\mathbf{v}_3\otimes\mathbf{w}_1,\;\mathbf{v}_3\otimes\mathbf{w}_2 \}$$</p><p>This might feel a little abstract with all the $\otimes$ symbols littered everywhere. But don&#39;t forget—we know exactly what each $\mathbf{v}_i\otimes\mathbf{w}_j$ looks like—it&#39;s just a list of numbers! <em>Which list of numbers?</em> Well, </p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf1822ec4891ebb5d70a2ba_6bases.jpg" alt=""/></p></figure><p>So what is $V\otimes W$? It&#39;s the vector space whose vectors are linear combinations of the $\mathbf{v}_i\otimes\mathbf{w}_j$. For example, here are a couple of vectors in this space:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf183c7d8afa54244f7b4ee_examples2.jpg" alt=""/></p></figure><h2>Well, <em>technically</em>...</h2><p> Technically, $\mathbf{v}\otimes\mathbf{w}$ is called the <strong>outer product</strong> of $\mathbf{v}$ and $\mathbf{w}$ and is defined by $$\mathbf{v}\otimes\mathbf{w}:=\mathbf{v}\mathbf{w}^\top$$ where $\mathbf{w}^\top$ is the same as $\mathbf{w}$ but written as a row vector. (And if the entries of $\mathbf{w}$ are complex numbers, then we also replace each entry by its complex conjugate.) So <em>technically </em>the tensor product of vectors is matrix:</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bc2961cdbf2a30ad3de11a3_outerprod.jpg" alt=""/></p></figure><p>This may seem to be in conflict with what we did above, but it&#39;s not! The two go hand-in-hand. Any $m\times n$ matrix can be reshaped into a $nm\times 1$ column vector and vice versa. (So thus far, we&#39;ve exploiting the fact that $\mathbb{R}^3\otimes\mathbb{R}^2$ is <em>isomorphic</em> to $\mathbb{R}^6$.) You might refer to this as <em>matrix-vector duality. </em></p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bc292a6ea24b52c94ed7392_reshape.jpg" alt=""/></p></figure><p>It&#39;s a little like a <strong>process-state duality</strong>. On the one hand, a matrix $\mathbf{v}\otimes\mathbf{w}$ is a <em>process</em>—it&#39;s a concrete representation of a (linear) <em>transformation. </em>On the other hand, $\mathbf{v}\otimes\mathbf{w}$ is, abstractly speaking, a vector. And a <em>vector</em> is the mathematical gadget that physicists use to describe the <em>state</em> of a quantum system. So matrices encode processes; vectors encode states. The upshot is that a vector in a tensor product $V\otimes W$ can be viewed in <em>either</em> <em>way</em> simply by reshaping the numbers as a list or as a rectangle.</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bc2931fea24b51eb6ed7397_reshape2.jpg" alt=""/></p></figure><p>By the way, this idea of viewing a matrix as a process can easily be generalized to <em>higher dimensional arrays</em>, too. These arrays are called <em>tensors</em> and whenever you do a bunch of <em>these</em> processes together, the resulting mega-process gives rise to a <strong>tensor network</strong>. But manipulating high-dimensional arrays of numbers can get very messy very quickly: there are lots of numbers that <em>all </em>have to be multiplied together. This is like multi-multi-multi-multi...plication. Fortunately, tensor networks come with lovely pictures that make these computations very simple. (It goes back to <a href="https://en.wikipedia.org/wiki/Penrose_graphical_notation" target="_blank">Roger Penrose&#39;s graphical calculus</a>.) This is a conversation I&#39;d like to have here, but it&#39;ll have to wait for <a href="https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams">another day</a>!</p><h2>In quantum physics</h2><p>One application of tensor products is related to the brief statement I made above: &#34;A <em>vector</em> is the mathematical gadget that physicists use to describe the <em>state</em> of a quantum system.&#34; To elaborate: if you have a little quantum particle, perhaps you’d like to know what it’s doing. Or what it’s capable of doing. Or the probability that it’ll be doing something. In essence, you&#39;re asking: What’s its status? What’s its <em>state</em>? The answer to this question— provided by a postulate of quantum mechanics—is given by a unit vector in a vector space. (Really, a Hilbert space, say $\mathbb{C}^n$.) That unit vector encodes information about that particle.</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf18deabdfdc38254963c66_quantum1.jpg" alt=""/></p></figure><p>The dimension $n$ is, loosely speaking, the number of different things you could observe after making a measurement on the particle. But what if we have two little quantum particles? The state of that two-particle system can be described by something called a <em>density matrix</em> $\rho$ on the tensor product of their respective spaces $\mathbb{C}^n\otimes\mathbb{C}^n$. A density matrix is a generalization of a unit vector—it accounts for interactions between the two particles. </p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf18ccccb149a45764a8667_quantum2.jpg" alt=""/></p></figure><p>The same story holds for $N$ particles—the state of an $N$-particle system can be described by a density matrix on an $N$-fold tensor product.</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf18e7b1e03a4ec5d964fd8_quantum3.jpg" alt=""/></p></figure><p><em>But why the tensor product?</em> Why is it that this construction—out of all things—describes the interactions within a quantum system so well, so naturally? I don’t know the answer, but perhaps the appropriateness of tensor products shouldn&#39;t be too surprising. The tensor product itself captures all ways that basic things can &#34;interact&#34; with each other!</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5bf18a33dc7cd56fcae2ad0b_analogy0.jpg" alt=""/></p></figure><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5b527d2844acede360b8e7ae_hline.jpg" alt=""/></p></figure><p>Of course, there&#39;s lots more to be said about tensor products. I&#39;ve only shared a snippet of basic arithmetic. For a deeper look into the mathematics, I recommend reading through Jeremy Kun&#39;s wonderfully lucid <a href="https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/" target="_blank">How to Conquer Tensorphobia</a> and <a href="https://jeremykun.com/2016/03/28/tensorphobia-outer-product/" target="_blank">Tensorphobia and the Outer Product</a>. Enjoy!</p><p>‍</p></div></div>
  </body>
</html>
