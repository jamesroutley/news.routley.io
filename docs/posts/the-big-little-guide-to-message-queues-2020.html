<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sudhir.io/the-big-little-guide-to-message-queues">Original</a>
    <h1>The Big Little Guide to Message Queues (2020)</h1>
    
    <div id="readability-page-1" class="page"><article><p>A guide to the fundamental concepts that underlie message queues, and how they apply to popular queueing systems available today.</p><p>Message Queues are now fairly prevalent—there are so many of them showing up so fast you&#39;d think they were <a href="https://www.rabbitmq.com">rabbits</a> with an unlimited supply of <a href="https://docs.celeryproject.org/en/stable/">celery</a>, resulting in an <a href="https://kafka.apache.org">kafkaesque</a> situation where making a decision is like trying to catch a <a href="https://redis.io/topics/streams-intro">stream</a> in your hands. If only there were fewer <a href="https://aws.amazon.com/sns">simple</a> <a href="https://aws.amazon.com/sqs/">services</a> that could help with <a href="https://cloud.google.com/pubsub">publishing and subscribing</a>, it would be so much easier to make a <a href="https://zeromq.org">zero-effort</a> choice 😕</p><p>Whether we use them by themselves as-is to move data between parts of our application, or as an integral part of the architecture (like event driven systems), message queues are here to stay. In a way, they&#39;ve been here all along—just without as many names. But what are they? Why are they useful? And how do we use them effectively? Which implementation do we pick? Does it even matter which one we use? And do we need to learn each of them individually, or are there more general concepts that apply to all message queues?</p><p>In this guide, we&#39;ll talk about:</p><ul><li>What message queues are and their history.</li><li>Why they&#39;re useful and what mental models to use when reasoning about them.</li><li>Delivery guarantees that the queuing systems make (at-least-once, at-most-once, and exactly-once semantics).</li><li>Ordering and FIFO guarantees and how they effect sequencing, parallelism and performance.</li><li>Patterns for fan-out and fan-in: delivering one message to many systems or messages from many systems into one.</li><li>Notes on the pros and cons of many popular systems available today.</li></ul><h2>What are message queues?</h2><p>Message Queues are a way to transfer information between two systems. This information—a message—can be data, metadata, signals, or a combination of all three. The systems that are sending and receiving messages could be processes on the same computer, modules of the same application, services that might be running on different computers or technology stacks, or entirely different kinds of systems altogether—like transferring information from your software into an email or an SMS on the cellphone network.</p><p>The idea of a messaging system has been around a very long time, from the message boxes used for moving information between people or office departments (literally where the words <em>inbox</em> and <em>outbox</em> come from), to telegrams, to your local postal or courier service. The messaging systems in the physical world that come closest to what we have in computing are probably the <a href="https://en.wikipedia.org/wiki/Pneumatic_tube">pnuematic</a> <a href="https://www.google.com/search?q=pneumatic+tubes&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;biw=2560&amp;bih=1366">tubes</a> that moved messages through buildings and cities using compressed air until a few decades ago (and are still used in some places today).</p><p>The kinds of messages we transfer today might be a note that something technical happened, like CPU usage exceeding a limit; or a business event of interest, like a customer placing an order; or a signal, like a command that tells another service to do something. The contents of each message will be driven entirely by the architecture of your application and its purposes—so for the rest of this guide, we don&#39;t need to be concerned about what&#39;s inside a message—we&#39;re more concerned with how the message gets from the system where it originates (the <em>producer</em>, <em>source, publisher</em> or <em>sender</em>) to the system where&#39;s it&#39;s supposed to go (the <em>consumer</em>, <em>subscriber</em>, <em>destination</em> or <em>receiver</em>).</p><h2>And Why Do We Need Them?</h2><p>We need message queues because no system exists or works in isolation—all systems need to communicate with other systems in structured ways that they both can understand, and at a controlled speed that they both can handle. Any non-trivial process needs a way to move information between each stage of the process; any workflow needs a way to move the intermediate product between the stages of that workflow. Message queues are a great way to handle this movement. There are plenty of ways of getting these messages around using API calls, file systems, or many other abuses of the natural order of things; but all of these are ad-hoc implementations of the message queue that we sometimes refuse to acknowledge we need.</p><p>The simplest mental model for a message queue is a very long tube that you can roll a ball into. You write your message on a ball, roll it into the tube, and someone or something else receives it at the other end. There are a lot of interesting benefits with this model, some of which are:</p><ul><li>We don&#39;t need to worry about <em>who or what</em> is going to receive the message – that&#39;s one less responsibility for the sender to be concerned about.</li><li>We don&#39;t need to worry about <em>when</em> the receiver is going to pick up the message.</li><li>We can put <em>as many</em> messages as we want into the tube (let&#39;s assume we have a infinitely long tube) at whatever <em>speed</em> is comfortable to us.</li><li>The receiver will <em>never be impacted</em> by our actions—they will pull out as many messages as they want at whatever rate is comfortable to them.</li><li>Neither the sender nor the receiver are concerned with <em>how</em> the other works.</li><li>Neither the sender nor the receiver are concerned with the <em>capacity or load</em> of the other.</li><li>Neither system is concerned with <em>where</em> the other one is – they may or many not reside on the same computer, network, continent or even the same planet.</li></ul><p>Each of these advantages (and this isn&#39;t even an exhaustive list) has very important benefits in software development—what they all have in common is <em>decoupling</em>. One system is decoupled from the other in terms of responsibility, time, bandwidth, internal workings, load and geography. And decoupling is a very desirable part of any distributed or complex system—the more decoupled the parts of the system are, the easier it is to independently build, test, run, maintain and scale them.</p><p>Most systems interact with other outside or third-party systems as well—if we build a shopping site we might interact with a payment processor, and let’s say we attempt to directly communicate with the payment processor on each user click. If our system is under heavy load, we&#39;re also subjecting the other system to the same load. And vice versa—if our payment provider needs to send us millions of pieces of information about our past payment statuses, our system better be ready. The two systems are now <em>coupled</em>. The decisions and actions made by one system have a significant impact on the other, so the needs of both need to be taken into account while making every decision. Add enough other systems into the mix, like logistics or delivery systems, and we quickly have a paralysing mess that makes it difficult to decide anything at all. If one system goes down, the other systems have effectively gone down as well, for no fault of their own.</p><p>We’re also in trouble if we want to switch out any one of these systems for another one, like a new payment processor or delivery system. We’d have to make deep changes in multiple places in our application, and it’s even more difficult to build code to split our messages between multiple providers—we may want to use a ratio to load balance them or split them by geography; or dynamically switch between them based on each provider’s availability or cost.</p><p>Message queues offer the decoupling that solves a lot of these problems. If we set up a queue between two systems that need to communicate with each other, they can now go about their work without having to worry about each other at all—we put our messages aimed at any system into a queue, and we expect information from the other system to come to us through another queue as well. We now have clear points at which we can add rules or make the changes we require, without either system knowing or caring about what&#39;s different.</p><h3>So what&#39;s the catch?</h3><p>Are message queues the holy grail of computing, though? Do they solve all the world&#39;s problems? No, of course not. There are plenty of situations where we might not want to use them. And we certainly don&#39;t want to use a queue just because we have one easily available and think it might be fun. There are some systems that are really simple that just don&#39;t require it—a message queue is a way to reduce complexity of communicating systems, but two communicating systems will always be more complex than one system that doesn&#39;t have to communicate. If you have a system that’s simple enough to not require communication with any others, there simply isn&#39;t any reason to reach for a queue.</p><p>There are also systems that communicate with each other, but where the complexity added by that communication added is insignificant and not worth worrying about. Or more often the systems are already coupled, in the sense that they all need to work together to function. A really common example is an application server and a data service (in an <em>OLTP</em> system). There&#39;s not much point in decoupling them with a queue, because neither can do anything useful without the direct involvement of the other.</p><p>Then there&#39;s performance to consider as well—the whole point of decoupling two systems with regards to time and load is so that they can each process information at their own pace—but we certainly would <em>not</em> want this to happen in performance sensitive applications or real-time systems. A queue might help us process more work at the same time (the receiver might have many processes working in parallel on the messages you send) but will remove any guarantees we need about the exact time taken for each piece of work. If predictability is more important than throughput, we&#39;re better off without a queue.</p><p>Using a queue might increase the time taken to process each <em>individual</em> message, but will allow you process many more messages at the same time across different computers—so your total number of messages processed per minute or hour, or <em>throughput</em>, will increase.</p><p>If we do have multiple systems that need to communicate, and that communication needs to be <em>durable</em> (if we’ve put a message into a queue, we want to be sure that the messaging system isn’t going to ‘forget’ about it) and decoupled, a message queue is indispensable.</p><h2>Arguing Semantics</h2><p>There&#39;s simply no way to learn about message queues without reading and/or arguing about delivery guarantees and semantics, so we might as well get to that quickly. People who build message queues will claim that their system offers one of three delivery guarantees—that each message you put into the queue will be delivered:</p><ul><li><em>at-least</em> once.</li><li><em>at-most</em> once.</li><li><em>exactly</em> once.</li></ul><p>Which guarantees we&#39;re using will have a massive impact on the design and working of our system, so let&#39;s unpack each of them one by one.</p><h3>At-Least Once</h3><p>This is the most common delivery mechanism, and it&#39;s the simplest to reason about and implement. If I have a message for you, I will read it to you, and keep doing so again and again until you acknowledge it. That&#39;s it. In a system which works on an at-least-once basis, this means that when you receive a message from the queue and don&#39;t delete/acknowledge it, you will receive it again in the future, and will keep receiving it until you explicitly delete/acknowledge it.</p><p>The reason this is the most common guarantee is that it&#39;s simple and gets the job done 100% of the time—there&#39;s no edge case in which the message gets lost. Even if the receiver crashes before acknowledging the message, it will simply receive the same message again. The flip side is that you as the receiver need to plan on receiving the same message multiple times—even if you haven&#39;t necessarily experienced a crash. This is because offering at-least-once is the simplest way to protect the queueing service from missing out messages as well—if your acknowledgement doesn&#39;t reach the queueing system over the network, the message will be sent again. If there&#39;s a problem persisting your acknowledgement, the message will be sent again. If the queuing system restarts before it can properly keep track of what&#39;s been sent to you, the message will be sent again. This simple remedy of sending the message again in case of any problem on any side is what makes this guarantee so reliable.</p><p>But is message duplication/repetition a problem? That&#39;s really up to you and your application or use-case. If the message is a timestamp and a measurement, for example, there&#39;s no problem with receiving a million duplicates. But if you&#39;re moving money based on the messages, it definitely is a problem. In these cases you&#39;ll need to have a transactional (ACID) database at the receiving end, and maybe record the message ID in a unique index so that it can&#39;t be repeated. This is called using an <em>idempotency token</em> or _tombstone—_when you act on a message you store a unique permanent marker to keep track of your actions, often in the same database transaction as taking the action itself. The prevents you from repeating that action again even if the message is duplicated.</p><p>If you handle duplication, or if your messages are naturally resistant to duplication, your systems are said to be <em>idempotent</em>. This means you can safely handle receiving the same message multiple times, without corrupting your work. It also often means you can tolerate the sender sending the same message multiple times—remember that senders will usually operate on the at-least-once principle when sending messages as well. If senders are unable to record the fact that they&#39;ve sent a particular message, they&#39;ll simply send it again. The senders are then responsible for making sure that they use the same tombstone or idempotency token if and when they re-send messages.</p><h3>At Most Once</h3><p>This is a pretty rare semantic, used for messages where duplication is so horribly explosive (or the message so utterly unimportant) that we&#39;d prefer not to send the message at all, rather than send it twice. At-most-once once implies that the queuing system will attempt to deliver the message to you once, but that&#39;s it. If you receive and acknowledge the message all is well, but if you don&#39;t, or anything goes wrong, that message will be lost forever—either because the queuing system has taken great pains to record the delivery to you before attempting to send it (in case the message is horribly explosive), or has not even bothered to record the message at all, and is just passing it on like a router passes on a UDP packet.</p><p>This semantic usually comes into play for messaging systems that are either acting as stateless information routers; or in those cases where a repeat message is so destructive that an investigation or reconciliation is necessary in case there&#39;s any failure.</p><h3>Exactly Once</h3><p>This is the holy grail of messaging, and also the fountain of a lot of snake-oil. It implies that every message is guaranteed to be delivered and processed exactly once, no more and no less. Everyone who builds or uses distributed systems has a point in their lives where they think “how hard can this be?”, and then they either (1) learn why it&#39;s impossible, figure out idempotency, and use at-least-once, or (2) they try to build a half-assed “exactly-once” system and sell it for lots of money to those who haven&#39;t figured out (1) yet.</p><p>The impossibility of exactly-once delivery arises from two basic facts:</p><ul><li>senders and receivers are imperfect</li><li>networks are imperfect</li></ul><p>If you think about the problem deeply, there are a lot of things that can go wrong:</p><ul><li>a sender might be unable to record (they <em>forget</em>) that they&#39;ve sent the message</li><li>the network call to send the message might fail</li><li>the messaging system’s database might not be able to record the message</li><li>the acknowledgement that the messaging system has recorded the message might not reach the sender over the network</li><li>the sender might not be able to record the acknowledgement that the messaging system has received the message</li></ul><p>Let&#39;s say all goes well while sending the message—when the messaging system tries to deliver the message to the receiver:</p><ul><li>the message might not reach the receiver over the network</li><li>the receiver might not be able to record the message in its database</li><li>the acknowledgement from the receiver might not reach the messaging system over the network</li><li>the messaging system’s database might not be able to record that the message has been delivered</li></ul><p>Given all the things that can go wrong, it&#39;s impossible for any messaging system to guarantee exactly-once delivery. Even if the messaging system is godlike in its perfection, most of the things that can go wrong are outside of it or in the interconnecting networks. Some systems do attempt to use the phrase “exactly once” anyway, usually because they claim their implementation will never have any of the messaging system problems mentioned above—but that doesn&#39;t mean the whole system is magically blessed with exactly-once semantics, even if the claims are actually true. It usually means that the queuing system has some form of ordering, locking, hashing, timers and idempotency tokens that will ensure it never re-delivers a messsage that&#39;s already been deleted/acknowledged—but this doesn&#39;t mean that the whole system including publisher + queue + subscriber has gained full exactly-once guarantees.</p><p>Most good messaging system engineers understand this and will <a href="https://www.lightbend.com/blog/how-akka-works-exactly-once-message-delivery">explain</a> to their users why this semantic is unworkable. The simpler and more reliable way to handle messages is go back to the basics and embrace at-least-once with idempotency measures at every point on the sending, receiving and queuing process: if at first you don&#39;t succeed, retry, retry, retry...</p><h2>Ordering vs Parallelism</h2><p>After delivery semantics, another common question on peoples’ minds is “why can’t we just process messages in parallel while also making sure we process them in order?”. Unfortunately this is another tradeoff imposed on us by the tyranny of logic. Doing work in a sequence and doing multiple pieces of work at the same time are always at conflict with each other. Most message queue systems will ask you to pick one—AWS SQS started by prioritising parallelism over strict ordering; but recently introduced a separate FIFO (first in, first out) queuing system as well, which maintains strict sequential ordering. Before making a choice between the two, let’s go over what the difference is and why there needs to be a difference at all.</p><p>Returning to our earlier metaphor for a queue—a long tube into which we roll messages written on a ball—we probably imagined the tube to be just a little wider than a single ball. There&#39;s really no way the balls could overtake or pass each other inside the tube, so the only way a receiver could get these messages out is one-by-one, in the order they were put in. This guarantees strict ordering, but places strong limitations on our receiver. There can <em>only be one</em> <em>agent</em> on the receiver side that&#39;s processing each message—if there was more than one, there would be no guarantee that the messages were processed in order. Because each new agent could processes each message independently, they could each finish and start on the next message at any time. If the are two agents, A &amp; B, and Agent A receives the first message and Agent B the second; Agent B could finish processing the second message and start on the third message even before Agent A is finished processing the first message. Though the messages were <em>received from the queue</em> strictly in the order that they were put in, if there are multiple receiving agents there’s no way to say the messages will <em>be processed in that order</em>.</p><p>The agents could use a <a href="https://redis.io/topics/distlock">distributed</a> <a href="https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS">lock</a> of some kind to co-ordinate with each other, but this is basically the same as having only one agent—the lock would only allow one agent to work at any given time. This also means that one agent crashing would result in a <em>deadlock</em> with no work being done.</p><p>One way for the messaging system to guarantee order would be for the tube to refuse to give out the next ball until and unless the last ball that was received has been destroyed (the last message has been deleted/acknowledged). This is what FIFO queues in general will do—they&#39;ll provide the next message only after the last one has been acknowledged or deleted—but this means that only one agent can possibly be working at a time, even if there are <em>N</em> agents waiting to receive messages from the queue.</p><p>Sometimes, this is exactly what we want. Some operations are easier to control effectively when we only have to deal with a single agent, like enforcing rules on financial transactions; respecting <a href="https://redis.io/commands/incr#pattern-rate-limiter">rate limits</a>; or generally processing messages whose formats have been designed assuming they would always be processed in order. But a lot of these “benefits” are not really coming from the decision to use FIFO ordering—any scenario where we have <em>N</em> receivers that must somehow co-ordinate their work with each other will benefit from the special case of <em>N = 1</em>. The key takeaway is that requiring a guaranteed order means we have to process messages sequentially on only one receiver at a time.</p><p>This restriction also places severe pressure on the queuing system, so you&#39;ll find that FIFO queues are often more expensive and have less capacity than their parallel counterparts. This is because the same logical limits apply to the internal implementation of queuing system as well—most work needs to be constrained to a single agent or server, and that system needs to be kept reliable. Any effort to add redundancy requires synchronous co-ordination between the master and the backup services in order to maintain the ordering guarantees. In AWS SQS, the FIFO queues are about 2X more expensive than the parallel queues, and are constrained to a few hundred messages per second when strict FIFO ordering is required.</p><p>So the only way to move forward with a FIFO message queue is to accept that the entire message processing architecture is going to have an intrinsic speed limit. Many systems will support <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html">group headings</a> inside the queue to denote what messages we want strict ordering on—we might say that all messages under the heading “payments” need to be FIFO, and all the messages under “orders” need to be FIFO, but they don&#39;t need to be FIFO with respect to each other. This allows some parallelisation inside the queue (like having two FIFO tubes instead of one), but we need to remember that the message bandwidth inside each group heading will still be limited.</p><h3>Parallel != Random</h3><p>Does that mean that the ordering in parallel queues is completely random? Sometimes, yes, but most often no. In SQS, the analogy is more that instead of having one tube from the sender to receiver, there are multiple tubes. They might also branch or join each other along the way. This doesn&#39;t mean that the order of the messages you roll in are intentionally randomised in any way—across a large number of messages you&#39;d still expect that earlier messages are generally received before the later ones. This is more a <em>best-effort</em> ordering, where some effort is make to keep the ordering intact, but because it&#39;s already logically impossible, it&#39;s simply not a big priority for the system. This also allows a messaging system like SQS to scale up to nearly infinite capacity—because if you&#39;re rolling in a lot of messages the queueing system can simply add more tubes. And as you can imagine, this will support any number of receivers at the same time, and any number of senders as well. This simplicity is what allows SQS to scale to mind-boggling numbers, including a case where <a href="https://twitter.com/timbray/status/1246157403663388672">there was a queue</a> with over 250 billion messages waiting to be consumed, with the receiver reading and acknowledging over a million messages a second. And that’s just one queue operated by one customer.</p><p>Most problems that seem like they have a hard FIFO requirement can often lend themselves to parallelism and out-of-order delivery with a little bit of creativity. The sender adding a timestamp into the message is one way to help with this, like in the case where the messages are measurements where only the last one matters. In a more transactional system, the sender can often add a <a href="https://www.postgresql.org/docs/current/functions-sequence.html">monotonically increasing counter</a> into the messages. If that&#39;s impossible, we might be able to handle this based on the contents of the message—if we&#39;re messaging the percentage of a file downloaded, for example, seeing 41%, 42% and 43% always means that the current value is 43%—even if we see them out of order as 41%, 43% and 42%.</p><p>While it&#39;s often a bad idea to change our systems to accommodate the tools we use, designing our messages to allow for out-of-order delivery and idempotency makes the system more resilient in general, while also letting use more parallel messaging systems—often saving time, money and a lot of operational work.</p><h2>Fan Out / In</h2><p>When building a distributed system there&#39;s often a need to have the same message sent to multiple receivers—besides the usual receiver of the message, we also often want the same message sent to other places, like an archive, an audit log (for compliance and security checks) or an analyzer for our dashboards. If you&#39;re using an event driven architecture with many services, you might want to use a single <em>event bus</em> in your application, where all the messages posted into this event bus are automatically sent to all of your services. This is called a <em>fan-out</em> problem, where a message from one producer needs to reach many consumers.</p><p>The inverse problem, where a single receiver is tasked with reading the messages posted to multiple queues is also common—in the example we considered above, a receiver that was archiving all messages or creating an audit log would probably receive all the messages generated in an organisation, on every queue. It&#39;s also common in service architectures to have a function like notifications handled separately—so a notification system might need to receive messages about a new confirmed orders, failed payments, successful shipping and many more. This is a <em>fan-in</em> problem, where the messages from many producers need to reach the same consumer.</p><p>If all the producers are putting their messages directly into queues, this would be a really difficult problem to solve—we&#39;d have to somehow intercept our queues, and reliably copy the messages into multiple queues. Building, configuring and maintaining this switchboard simply isn&#39;t worth the time or the effort—especially when we could just use <em>topics</em> instead.</p><p>One way to think about topics is that they&#39;re similar to the headings you&#39;d see on a notice board at a school or an office. Producers post messages under a specific topic on a board, and everyone interested in that topic will see the message. The most common way messaging systems send the messages to interested receivers is an HTTP(S) request, sometimes also called a <em>webhook</em>. In a push-based system like a HTTP request, the message is pushed into the receiver whether it&#39;s ready or not. This re-introduces the coupling that we talked about earlier which we want to avoid—we don&#39;t want a situation where our receiver collapses under the crushing load of tens / hundreds / thousands / millions of webhooks over a short span of time. The answer here, again, is to just use a message queue to soak up the messages from the topics at whatever rate they&#39;re generated. The receivers can then process them at their own pace.</p><p>Automatically copying a message from one topic into one or more queues isn&#39;t strictly a message queue feature, but it is complementary—most full-featured messaging systems will offer a way to do this. Producers will still continue to put messages into a single place as usual, but this will be a topic, and internally the messages will be copied to multiple queues, each of which will be read by their respective receivers.</p><p>In AWS, the service that provides topic based messaging is the Simple Notification Service (<a href="https://aws.amazon.com/sns/">SNS</a>). Here you create a topic and publish messages into it—the API to publish a message into an SNS topic is very similar to that of publishing a message into an SQS queue, and most producers don&#39;t have to care about the difference. SNS then has options available to publish that message into any number of <em>subscribed</em> SQS queues (at no extra charge). Each of these subscribed SQS queues would then be processed by their respective receivers.</p><p>If you&#39;re working with a different system like Apache Kafka, you&#39;ll see similar concepts there as well - you&#39;ll have <em>topics</em> that you publish messages into, and any number of consumers can each read all the messages in a topic. Google&#39;s Pub/Sub system integrates topics and queues as well.</p><p>This combination of these scenarios is common enough that there&#39;s a simple well established pattern to handle it:</p><ul><li>Publish every message to one appropriate topic.</li><li>Create a queue for each receiver.</li><li>Link up each receiver&#39;s queue to the topics that the receiver is interested in.</li></ul><p>Since it&#39;s usually possible to subscribe a queue to any number of topics, there&#39;s no extra plumbing required at a receiver to process messages from multiple topics. And of course, it&#39;s possible to have any number of message queues subscribed to a single topic. This kind of setup supports both fan-out as well as fan-in, and keeps your architecture open to expansion and changes in the future.</p><h3>Poison Pills &amp; Dead Letters</h3><p>As morbid as that sounds, when setting up systems to talk to multiple other systems there are bound to be mistakes that happen. The usual problem is that a subscriber is hooked up to receive messages from a topic it knows nothing about in a message format it doesn&#39;t understand. What happens? Does the subscriber ignore the message? Or does it acknowledge/delete it? Wouldn&#39;t be wrong for it to ignore it, because the message would just keep coming back again and again in an at-least-once system? But isn&#39;t it worse to delete/acknowledge a message that we aren&#39;t handling? Before we reach for philosophy books made from trees fallen in the woods, we might want to configure a <em>dead letter queue</em> on our queue. This is a feature that many queue systems give us, where if the system sees a message being sent out for processing repeatedly, unsuccessfully each time, it&#39;ll move it out into a special queue called a <em>dead letter queue</em>. We&#39;d want to hook this queue up to an alarm of some sort, so we&#39;ll quickly know something weird is going on.</p><p>A much worse scenario is one in which the message is explosive in some way—maybe it&#39;s formatted in XML instead of JSON, or contains user generated content carrying a malformed input attack that causes your parsing code to crash... your subscriber has just swallowed a <em>poison pill</em>. What happens when this pill reaches the subscriber is heavily dependent on your technology stack, so needless to say you want to think carefully about error handling and exceptions in the subscriber code. The good news is that if you&#39;ve configured a <em>dead letter queue</em>, just failing silently can be a fine option. The poison pill will eventually show up in the <em>dead letter queue</em> and can be examined. Even if the poison message is crashing your subscriber, running an automated restart with a process manager is often enough to retry the message so many times that it moves it to the dead letter queue. You do need to make sure there are no security implications, though, and remember that this is an easy <a href="https://en.wikipedia.org/wiki/Denial-of-service_attack">DoS attack</a>.</p><p>Remember to always verify your incoming messages, both in terms of whether the message is structured the way you expect it to be, and if you&#39;re the intended recipient.</p><h2>The Q-List</h2><p>Here&#39;s a list of some of the more popular message queuing systems available right now, with a list of how the concepts we&#39;ve seen so far apply to each of them. This isn&#39;t an exhaustive list of course, so let me know <a href="https://twitter.com/sudhirj">@sudhirj</a> if you think there are any that are missing or misrepresented.</p><h3>AWS SNS &amp; SQS</h3><p>AWS runs two services that integrate with each other to provide full message queuing functions. The <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html">SQS</a> service is a pure message queue—it allows you to create a queue, send a message, and receive a message. That&#39;s it. The <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html"><code>ReceiveMessage</code></a> API on on SQS queue is pull-only, so you&#39;ll need to call it whenever your receiver is ready to process a message. There&#39;s a <code>WaitTimeSeconds</code> option to block on the call wait for a message for up to 20 seconds, so an effective pattern is to poll the <code>ReceiveMessage</code> API in an infinite loop with the 20 second wait turned on.</p><p>The topics and fan-out / fan-in functions come with the integration of <a href="https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html">SNS</a>, which works on the construct of a <em>topic</em>. This allows a message to be posted into a topic, as opposed to a queue. You can then subscribe any number of SQS queues into a topic, so messages published to the topic are copied to all subscribed queues quickly at no additional cost. You&#39;ll want to turn on  the <a href="https://docs.aws.amazon.com/sns/latest/dg/sns-large-payload-raw-message-delivery.html"><em>raw message</em></a> option, which makes posting a message into an SNS topic effectively equivalent to posting it into an SQS queue—no transformations or packaging of any sort will be applied onto the message.</p><p>SQS &amp; SNS are both fully managed services, so there are no servers to maintain or software to install. You&#39;re charged based on the number of messages you send and receive, and AWS handles scaling to any load.</p><p>FIFO options are available on <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-sns-fifo-first-in-first-out-pub-sub-messaging/">SNS</a> and <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">SQS</a>, with different pricing and capacity guarantees. AWS uses the term <em>message group ID</em> to denote a group heading under which all messages are FIFO. Messages inside a group heading are delivered in order by not giving out the next message until the previous message is deleted.</p><h3>Google Pub/Sub</h3><p>Google provides the <a href="https://cloud.google.com/pubsub/docs/overview">Pub/Sub</a> service as part of their cloud platform to handle message queues and topics in an integrated service. The concept of topics exists as you&#39;d expect it, while a queue is called a <em>subscription</em>. As expected, associating multiple subscriptions with a topic will copy the message to all associated subscriptions. Besides allowing subscribers to poll, or <em>pull</em>, messages from the subscription, Pub/Sub can also do a <a href="https://cloud.google.com/pubsub/docs/push"><em>webhook</em></a> style POST of the message to your server, letting you acknowledge it with a success return status code.</p><p>This is also a fully managed system, like AWS. You&#39;re charged based on the number of messages you send, and Google handles scaling the system to whatever capacity you need. It also has a few features that aren&#39;t available in the SNS+SQS combo, like allowing you to look into your historical record using timestamps and <a href="https://cloud.google.com/pubsub/docs/replay-overview">replay messages</a>.</p><p><a href="https://cloud.google.com/pubsub/docs/ordering">FIFO functionality</a> exists inside the context of an <em>ordering key</em>, which allows you to ensure that messages inside an ordering key are processed in sequence by not giving you the next message until the previous one has been acknowledged.</p><h3>AWS EventBridge</h3><p>A new offering from AWS, <a href="https://aws.amazon.com/eventbridge/">EventBridge</a> offers a fully managed <em>event bus</em>—this is a varition on the concept of queues and topics, where all messages are posted into a single bus with no visible topic separation. Instead, every message needs to be structured according to a standard format that has information about the message topic inside it. The bus will then read the message, and route it to whichever subscribers have expressed an interest in receiving the messages about that topic. The actual delivery mechanism from the bus to the subscriber can be an SQS queue, webhooks, or many other platform specific options. This makes it easy to manage the event bus as a separately configurable rule-based switchboard, while also allowing easy plugins for archival, auditing, monitoring, alerting, replay, etc.</p><h3>Redis Streams</h3><p>Redis has a relatively new <a href="https://redis.io/topics/streams-intro">Streams</a> feature that works great for message queues. It works by creating topics on the fly and adding messages to them with the <code>XADD</code> command. Reading the messages directly off the topic is possible with <code>XREAD</code>, so each subscriber can maintain its own state (the last read offset) to read through the messages. To avoid each subscriber having to maintain its current state, it makes more sense to create <em>consumer groups</em> using <code>XGROUP CREATE</code>, which are the equivalents of queues. Each message sent to a topic becomes available independently in each consumer group, which can then be subscribed to with <code>XREADGROUP</code>. Messages can be acknowledged with <code>XACK</code>.</p><p>The Redis Streams are automatically FIFO ordered using a timestamp that can either be auto-generated or manually set. This also means that messages can only be processed by one consumer agent at a time. To get around this limitation and work with many consumer agents in parallel, there&#39;s a separate non-streams-based pattern described in the documentation for <code>[RPOPLPUSH](https://redis.io/commands/rpoplpush#pattern-reliable-queue)</code>—basically <code>LPUSH</code> messages into a topic, and then <code>RPOPLPUSH</code> them into other lists, each representing a queue, or more accurately its work in progress. <code>LREM</code> works to delete/acknowledge the message.</p><p>Redis is an open source system that you can install and maintain yourself or find managed hosting for. Depending on how durable you need the system to be you might want to figure out the best <a href="https://redis.io/topics/persistence">persistence</a> mechanism to use.</p><h3>Apache Kafka</h3><p><a href="https://kafka.apache.org/documentation/">Kakfa</a> is a popular message broker that works on the concepts of <em>producers</em> publishing messages, called <em>events</em>, to <em>topics</em>. The events in a topic are split into <em>partitions</em>, using a partition key inside of the topic, and FIFO ordering is maintained inside every partition. Events can be streamed to <em>consumers</em> over a socket, or queried by the consumers for a more decoupled approach. For consumers that don&#39;t want to maintain state, the concept of a <em>consumer group</em> applies, same as Redis Streams. A <em>consumer group</em> is effectively a queue, where every event posted to a topic is available for processing in every associated <em>consumer group</em>.</p><p>Kafka is open source, but is a complicated to install and maintain, which makes it suitable for larger projects and teams. It scales based on how well you split your events in to partitions—the more partitions you have the more Kafka can distribute work, and each partition has only as much capacity as the server that&#39;s in charge of managing it. Managed hosting options are avaiable, but they tend to have high base costs compared to managed services like SNS+SQS, Pub/Sub or RabbitMQ.</p><h3>RabbitMQ</h3><p>RabbitMQ is a popular open source message broker that supports a variety of <a href="https://www.rabbitmq.com/protocols.html">protocols</a>, with <a href="https://www.rabbitmq.com/tutorials/tutorial-five-python.html">direct support</a> for the concepts of topics and queues. RabbitMQ operates under both <em>at-most-once</em> and <em>at-least-once</em> modes, with <em>at-most-once</em> being a fast memory based mode that occasionally writes messages to the disk if necessary (you can choose between pesisted or transient queues). If you want a more reliable, but slower, at-least-once system, you can use the operations described in the <a href="https://www.rabbitmq.com/reliability.html">reliability guide</a> to ask for <em>confirmations</em> when publishing messages, and require mandatory <em>acknowledgements</em> when reading them. Queues are FIFO by default, with the option to enforce sequential processing with acknowledgements.</p><h3>NSQ</h3><p>The first truly distributed message queue in this list, <a href="https://nsq.io">NSQ</a> is interesting because it&#39;s built from the group up to be decentralized. There&#39;s no single point to connect to to publish or subscribe to messages—every NSQ node is effectively a full server and talks to every other node. The nodes allow you to publish messages to <em>topics</em>, and each topic can be linked to one or more <em>channels</em>—which are the equivalent of queues. Every message published to a topic is available in all its linked <em>channels</em>.</p><p>NSQ is <a href="https://nsq.io/overview/features_and_guarantees.html">defaults</a> to non-durable, at-least-once, un-ordered messaging, but has a few configuration options to tweak things. It&#39;s specially worth considering if your servers are highly networked with each other and you want a system without a single point of failure.</p><h3>NATS</h3><p><a href="https://docs.nats.io/">NATS</a> is a high performance distributed messaging system that&#39;s made for fast in-memory messaging. It supports topic based <a href="https://docs.nats.io/nats-concepts/pubsub">broadcast</a> (topics are called <em>subjects</em>), where all messages sent to the <em>subject</em> are sent to all subscriber agents; and <a href="https://docs.nats.io/nats-concepts/queue">distributed queues</a>, where each message in the queue is sent to any one subscriber agent. There isn&#39;t a built-in way to have topics linked to queues, but that should be possible to do programmatically.</p><p>NATS supports both <em>at-most-once</em> and <em>at-least-once</em> delivery, and by providing a <a href="https://docs.nats.io/nats-streaming-concepts/intro">streaming system</a> and an <a href="https://github.com/nats-io/jetstream">experimental persistence system</a>. It also has support for subscribing to multiple topics based on subject name patterns, which makes it easier to do fan-in and multi-tenancy.</p><p>NATS works great when you need a high throughput distributed system—it&#39;s also pretty easy to run, and supports complex network topology, like having regional clusters with connections between them.</p><h2>The Tail Message</h2><p>These are just a few of the options available right now, and still more are being developed as distributed computing develops and cloud providers grow. I&#39;ve found that the important thing when evaluating or using queueing systems is to understand the semantics &amp; guarantees they offer. I do this by reading their architectural overviews to get rough idea of how they&#39;re implemented. Beneath the surface, the same concepts apply to all of them, just under different name and configuration options.</p><p>If you&#39;re running your workloads in a particular cloud provider, the default topic/queue system they offer will usually work fine, as long as you understand what semantics they&#39;re offering in each mode. If you&#39;re managing your own installation of a queue system, the same thing applies—except you need to be a lot more concerned about the limit imposed by the operating decisions you&#39;re making, like how many nodes you&#39;re running, failover configuration, drive space, etc.</p><p>Thanks for reading, reach out to me <a href="https://twitter.com/sudhirj">@sudhirj</a> or join the discussion on <a href="https://news.ycombinator.com/item?id=25591492">Hacker News</a> if you have questions or disagree with anything.</p><p>Special thanks to <a href="https://twitter.com/svethacvl">@svethacvl</a> for proofreading, and <a href="https://twitter.com/wallyqs">@wallyqs</a> for notes on NATS.</p></article></div>
  </body>
</html>
