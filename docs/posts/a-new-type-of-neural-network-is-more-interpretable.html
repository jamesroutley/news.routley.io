<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/kan-neural-network">Original</a>
    <h1>A new type of neural network is more interpretable</h1>
    
    <div id="readability-page-1" class="page"><div data-headline="A New Type of Neural Network Is More Interpretable"><div><p><a href="https://spectrum.ieee.org/what-is-deep-learning" target="_blank">Artificial neural networks</a>—algorithms inspired by biological brains—are at the center of modern <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">artificial intelligence</a>, behind both chatbots and image generators. But with their many neurons, they can be <a href="https://www.nature.com/articles/d41586-024-01314-y" rel="noopener noreferrer" target="_blank">black boxes</a>, their inner workings uninterpretable to users. </p><p>Researchers have now created a fundamentally new way to make <a href="https://spectrum.ieee.org/tag/neural-networks">neural networks</a> that in some ways surpasses traditional systems. These new networks are more interpretable and also more accurate, proponents say, even when they’re smaller. Their developers say the way they learn to represent physics data concisely could help scientists uncover new laws of nature. </p><p>“It’s great to see that there is a new architecture on the table.” <strong></strong><strong>—Brice Ménard, Johns Hopkins University</strong><strong></strong></p><p>For the past decade or more, engineers have mostly tweaked neural-network designs through trial and error, says Brice Ménard, a physicist at Johns Hopkins University who studies how neural networks operate but was not involved in the new work, which <a href="https://arxiv.org/abs/2404.19756" rel="noopener noreferrer" target="_blank">was posted on arXiv</a> in April. “It’s great to see that there is a new architecture on the table,” he says, especially one designed from first principles.</p><p>One way to think of neural networks is by analogy with neurons, or nodes, and synapses, or connections between those nodes. In traditional neural networks, called multi-layer perceptrons (MLPs), each synapse learns a weight—a number that determines <em>how strong</em> the connection is between those two neurons. The neurons are arranged in layers, such that a neuron from one layer takes input signals from the neurons in the previous layer, weighted by the strength of their synaptic connection. Each neuron then applies a simple function to the sum total of its inputs, called an activation function.</p><p><img alt="black text on a white background with red and blue lines connecting on the left and black lines connecting on the right " data-rm-shortcode-id="16c6aa72c8d9515c82ff8f3ee8448e30" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/black-text-on-a-white-background-with-red-and-blue-lines-connecting-on-the-left-and-black-lines-connecting-on-the-right.png?id=53100120&amp;width=980" height="1128" id="e42c9" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%201768%201128&#39;%3E%3C/svg%3E" width="1768"/><small placeholder="Add Photo Caption...">In traditional neural networks, sometimes called multi-layer perceptrons [left], each synapse learns a number called a weight, and each neuron applies a simple function to the sum of its inputs. In the new Kolmogorov-Arnold architecture [right], each synapse learns a function, and the neurons sum the outputs of those functions.</small><small placeholder="Add Photo Credit...">The NSF Institute for Artificial Intelligence and Fundamental Interactions</small></p><p>In the new architecture, the synapses play a more complex role. Instead of simply learning <em>how strong</em> the connection between two neurons is, they learn the <em>full nature</em> of that connection—the function that maps input to output. Unlike the activation function used by neurons in the traditional architecture, this function could be more complex—in fact a “spline” or combination of several functions—and is different in each instance. Neurons, on the other hand, become simpler—they just sum the outputs of all their preceding synapses. The new networks are called Kolmogorov-Arnold Networks (KANs), after two mathematicians who studied how functions could be combined. The idea is that KANs would provide greater flexibility when learning to represent data, while using fewer learned parameters. </p><p>“It’s like an alien life that looks at things from a different perspective but is also kind of understandable to humans.” <strong>—Ziming Liu, Massachusetts Institute of Technology</strong></p><p><span></span>The researchers tested their KANs on relatively simple scientific tasks. In some experiments, they took simple physical laws, such as the velocity with which two relativistic-speed objects pass each other. They used these equations to generate input-output data points, then, for each physics function, trained a network on some of the data and tested it on the rest. They found that increasing the size of KANs improves their performance at a faster rate than increasing the size of MLPs did. When solving partial differential equations, a KAN was 100 times as accurate as an MLP that had 100 times as many parameters.</p><p>In another experiment, they trained networks to predict one attribute of topological knots, called their signature, based on other attributes of the knots. An MLP achieved 78 percent test accuracy using about 300,000 parameters, while a KAN achieved 81.6 percent test accuracy using only about 200 parameters.</p><p>What’s more, the researchers could visually map out the KANs and look at the shapes of the activation functions, as well as the importance of each connection. Either manually or automatically they could prune weak connections and replace some activation functions with simpler ones, like sine or exponential functions. Then they could summarize the entire KAN in an intuitive one-line function (including all the component activation functions), in some cases perfectly reconstructing the physics function that created the dataset.</p><p>“In the future, we hope that it can be a <a href="https://spectrum.ieee.org/ai-for-science" target="_self">useful tool for everyday scientific research</a>,” says Ziming Liu, a computer scientist at the Massachusetts Institute of Technology and the paper’s first author. “Given a dataset we don’t know how to interpret, we just throw it to a KAN, and it can <a href="https://www.nature.com/articles/d41586-023-03596-0" target="_blank">generate some hypothesis</a> for you. You just stare at the brain [the KAN diagram] and you can even perform surgery on that if you want.” You might get a tidy function. “It’s like an alien life that looks at things from a different perspective but is also kind of understandable to humans.”</p><p>Dozens of papers have already cited the KAN preprint. “It seemed very exciting the moment that I saw it,” says Alexander Bodner, an undergraduate student of computer science at the University of San Andrés, in Argentina. Within a week, he and three classmates had combined KANs with convolutional neural networks, or CNNs, a popular architecture for processing images. They tested their <a href="https://arxiv.org/abs/2406.13155" target="_blank">Convolutional KANs</a> on their ability to categorize handwritten digits or pieces of clothing. The best one approximately matched the performance of a traditional CNN (99 percent accuracy for both networks on digits, 90 percent for both on clothing) but using about 60 percent fewer parameters. The datasets were simple, but Bodner says other teams with more computing power have begun scaling up the networks. Other people are combining KANs with transformers, an architecture popular in <a href="https://www.nature.com/articles/d41586-021-00530-0" target="_blank">large language models</a>.</p><p>One downside of KANs is that they take longer per parameter to train—in part because they can’t take advantage of <a href="https://spectrum.ieee.org/tag/gpus">GPUs</a>. But they need fewer parameters. Liu notes that even if KANs don’t replace giant CNNs and transformers for processing images and language, training time won’t be an issue at the smaller scale of many physics problems. He’s looking at ways for experts to insert their prior knowledge into KANs—by manually choosing activation functions, say—and to easily extract knowledge from them using a simple interface. Someday, he says, KANs could help physicists discover high-temperature <a href="https://spectrum.ieee.org/tag/superconductors">superconductors</a> or ways to control nuclear fusion.</p></div></div></div>
  </body>
</html>
