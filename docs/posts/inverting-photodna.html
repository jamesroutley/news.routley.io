<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.anishathalye.com/2021/12/20/inverting-photodna/">Original</a>
    <h1>Inverting PhotoDNA</h1>
    
    <div id="readability-page-1" class="page"><div id="__next"><div><div><article><p>Microsoft <a rel="noopener" href="https://www.microsoft.com/en-us/photodna">PhotoDNA</a> creates a “unique digital signature” of an image which can
be matched against a database containing signatures of previously identified
illegal images like CSAM. The technology is <a rel="noopener" href="https://www.makeuseof.com/what-is-photodna-how-does-it-work/">used</a> by
companies including Google, Facebook, and Twitter. Microsoft says:</p><blockquote><p>A PhotoDNA hash is not reversible, and therefore cannot be used to recreate
an image.</p></blockquote><p><a rel="noopener" href="https://github.com/anishathalye/ribosome">Ribosome</a> inverts PhotoDNA hashes using machine learning.</p><div><div><div><div><div><div><p><img alt="PhotoDNA inversion demonstration" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAHCAYAAAAxrNxjAAAAAklEQVR4AewaftIAAAD5SURBVBXBvUsCYQDA4d/7cV2c0aAnEUZQCA41uBRiW1s09282NzQIDoWQRDQkQZkZeJ4fnOdxl+/5Gj2PmC6yjedqMrPGcxXtu1uSYEildoJOI36H7xRrdbRWkjg1mLUlmk3ptFoceIKdUpl+94HQ7nFdHKGVFDhK4rkaPJ+zZpNVNGfQ/+a8ccFwNOe194W2JmUSBPyrVqucNi8Jx2PqJR8hBW68RGmFTtcO5f1DTG7JVjlvj/f89J5JNorG1Q0vrTbjKEBqJbAbkFIQhhMGnx84WYJfKNB96mCXGUeVY/T2lsLkFiklJd8ntZoFuzjGJV8kxCsD05g/t11tyhgxUikAAAAASUVORK5CYII="/></p><picture><source srcset="/_next/static/images/demo-50683fa27b48ade96ca918f534951301.png.webp" type="image/webp"/><source srcset="/_next/static/images/demo-bbfeefc00dbf9d997419f319bdef1b24.png"/><img alt="PhotoDNA inversion demonstration" src="https://www.anishathalye.com/_next/static/images/demo-bbfeefc00dbf9d997419f319bdef1b24.png"/></picture></div></div></div></div></div></div><p>This demonstration uses provocative images to make a point: rough body shapes
and faces can be recovered from the PhotoDNA hash. The image in the top row is
from Sports Illustrated, and the image in the bottom row is <a rel="noopener" href="https://thispersondoesnotexist.com/">not a real
person</a>. The first column shows a portion of the
144-byte PhotoDNA hash, and the center column shows the image that can be
reconstructed from this hash using Ribosome.</p><p>Like any other lossy function, the PhotoDNA hash is not perfectly invertible,
but the hash leaks plenty of information about the original input, as evidenced
by these image recreations.</p><p>Neither details of the PhotoDNA algorithm nor an implementation is officially
available to the public, but the algorithm has been
<a rel="noopener" href="https://hackerfactor.com/blog/index.php?/archives/931-PhotoDNA-and-Limitations.html">reverse-engineered</a>
based on public documents, and a <a rel="noopener" href="https://github.com/jankais3r/pyPhotoDNA">compiled library</a> for computing
PhotoDNA hashes has been leaked around the time
<a rel="noopener" href="https://github.com/anishathalye/neural-hash-collider">collisions</a> were found
in Apple’s NeuralHash algorithm.</p><p>Likely due to the closed-source nature of PhotoDNA, there has not been much
work studying the hash function. In 2019, <a rel="noopener" href="https://kar.kent.ac.uk/77165/">Nadeem et
al.</a> in collaboration with Microsoft
investigated the privacy protection capability of PhotoDNA by testing it
against ML classification. The paper claimed that “PhotoDNA is resistant to
machine-learning-based classification attacks”. More recently, in November
2021, <a rel="noopener" href="https://eprint.iacr.org/2021/1531">Prokos et al.</a> performed targeted
second-preimage attacks on PhotoDNA.</p><p>Ribosome is an inversion attack on the PhotoDNA hash function and investigates
the claim that a PhotoDNA hash “cannot be used to recreate an image.”</p><p>Ribosome treats PhotoDNA as a black box and attacks the hash function using
machine learning. Because an implementation of PhotoDNA has been <a rel="noopener" href="https://github.com/jankais3r/pyPhotoDNA">leaked</a>, it
is possible to produce a dataset of image/hash pairs. Ribosome trains a neural
net on such a dataset to learn to synthesize an image given its hash.</p><p>PhotoDNA hashes are 144-element vectors of bytes. Ribosome uses a neural
network similar to the <a rel="noopener" href="https://arxiv.org/pdf/1511.06434/">DCGAN</a> generator and
the <a rel="noopener" href="https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf">Fast Style
Transfer</a>
network, using residual blocks followed by fractionally-strided convolutions
for learned upscaling, to turn this into a 100x100 image.</p><p>The choice of dataset used to train the model affects the results. Ideally, the
images in the dataset should be drawn from the same distribution as the images
whose hashes are being inverted. For example, when inverting a hash of an image
that is expected to contain a person, training the model on the
<a rel="noopener" href="http://places.csail.mit.edu/">Places</a> dataset may not produce optimal results.</p><p>Ribosome can produce good results even when the exact distribution that the
image is drawn from is not known. The following figure illustrates hash
inversions computed by models trained on different test sets:</p><div><div><div><div><div><div><p><img alt="Comparing training/evaluation across datasets" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAALCAYAAABGbhwYAAAAAklEQVR4AewaftIAAAF+SURBVAXBTWvTYADA8X+SZ23Srkmzds3KSnWdY6CrInXTg0fvfhJPgh/Dox/Bu7CbOPDg1e4NhkNBkL6t7fMkpm+pefH304LAz6LViiRNmc9CTLOAYegkacpqucBxHEhTxMcP77E3LUa+j1exyWKD2vYW/cEdXrVMJCd4Ox76s5PnWFaJzouXGKZNu3OMJvK0Oyfo+RLNo2PUEsSvgUTOYwhmhLHGj999pFRM1xp/fZ9esGA8niA+n34ivBtQPzxk0O9RKDiEcorrOsTzkEUCcthHvHn7jolUVGsV5FRSLrtMportiotSipyh0RuNEd++nhHHMXajhS5yBOMhecuie/2HDFjEefSNDcS+5xBFMe6uR7SOqFo2GVAsFUmSBLNok2Up4umr1yilcMo2vlKUHQdfSnZcF6V8cqbJcDRCdL+ccnV5RePBPc7Puzx5+Jibiy6Pjtr8vL2ldXDA98sLhNvYo/lPp7brsR8L6vdbLDULb6/JynTY3KpQn635D7YDplV9X0VYAAAAAElFTkSuQmCC"/></p><picture><source srcset="/_next/static/images/datasets-0cc6dc9f2b8162d314683f1926a30327.png.webp" type="image/webp"/><source srcset="/_next/static/images/datasets-ba81b96221fb42f056e2bd0a7ce88207.png"/><img alt="Comparing training/evaluation across datasets" src="https://www.anishathalye.com/_next/static/images/datasets-ba81b96221fb42f056e2bd0a7ce88207.png"/></picture></div></div></div></div></div></div><p>In the above figure, held-out test images from each of the datasets, plus an
extra image from the internet, are hashed and then reproduced using Ribosome
models trained on different datasets. Training datasets include <a rel="noopener" href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>,
<a rel="noopener" href="https://cocodataset.org/">COCO</a>, and a dataset of 100K images scraped from SFW and NSFW subreddits. A
fourth dataset was created by combining images from the three datasets (taking
only 40K images from CelebA).</p><p>The figure illustrates some interesting phenomena:</p><ul><li>Using a large and diverse dataset allows the model to produce good
reproductions from a PhotoDNA hash (fourth column)</li><li>The better the prior, the better the result, e.g. reproducing a face with a
model trained on CelebA (first column, second row)</li><li>Ribosome can handle some distribution shift, e.g. reproducing a face when
trained on COCO (second column, second row)</li><li>Unsurprisingly, training the model on a narrowly-scoped dataset gives it a
strong bias towards producing images like those in that dataset, e.g. training
on CelebA biases the model to produce images containing faces (first column)</li><li>The model learns to recolor the image (PhotoDNA converts the image to
greyscale before processing it)</li><li>The model sometimes has amusing failure behaviors, e.g. the Reddit dataset
was not carefully curated and had many “image unavailable” images, and
artifacts from this are visible in some of the results (third column)</li></ul><p>The images above are manually selected examples, chosen for the quality of the
result, diversity of images (e.g. different poses), and being SFW. The
reconstruction is not always perfect. To give a sense of how the model works on
average, here are 5 randomly-selected test datapoints from COCO shown along
with their original images:</p><div><div><div><div><div><div><p><img alt="Randomly-selected test data from COCO" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAYAAAC3OK7NAAAAAklEQVR4AewaftIAAACtSURBVAGkAFv/AV+p6OD+/gIMH8x79RUaHQr59/n32tnUCEhTYPnu6uYGpJ6Z+yIjIgQEHuW5+w0VGvvJ2dz7FRYY++jl4fsEAwIDs6ec+wIDAgE3NDP7ztHT/wLRBDUHyPQhBxwmHgfp/gwHHhMNB/Xm5QdbZnQHP0dTB9nUxgcjGhIHBCT+xPkjGScFtbPl+TULDgP3uNL5GP37AbyacfkLAwL/JiUU++De7f1v1UzkkMoJmgAAAABJRU5ErkJggg=="/></p><picture><source srcset="/_next/static/images/random-6f367ee9bd8c4963b00fb395cee733a9.png.webp" type="image/webp"/><source srcset="/_next/static/images/random-20c25bde6854b308852486b979d13d30.png"/><img alt="Randomly-selected test data from COCO" src="https://www.anishathalye.com/_next/static/images/random-20c25bde6854b308852486b979d13d30.png"/></picture></div></div></div></div></div></div><p>Ribosome shows that PhotoDNA does not perfectly hide information about the
source image used to compute the signature, and that in fact, a PhotoDNA hash
can be used to produce thumbnail-quality reproductions of the original image.</p><p>Code and pre-trained models are available on <a rel="noopener" href="https://github.com/anishathalye/ribosome">GitHub</a>.</p></article></div></div></div></div>
  </body>
</html>
