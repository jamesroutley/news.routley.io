<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.facebook.com/blog/ai-self-supervised-learning-data2vec/">Original</a>
    <h1>Data2vec 2.0: Highly efficient self-supervised learning for vision, speech, text</h1>
    
    <div id="readability-page-1" class="page"><div><div><p> Many recent breakthroughs in AI have been powered by <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">self-supervised learning</a>, which enables machines to learn without relying on labeled data. But current algorithms have several significant limitations, often including being specialized for a single modality (such as images or text) and requiring lots of computational power. This contrasts with human learning: People appear to learn much more efficiently than current AI, and also learn from different kinds of information in a similar way, rather than relying on separate learning mechanisms for text, speech, and other modalities. </p><p> Meta AI addressed one of these limitations earlier this year when we released <a href="https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/">data2vec, the first high-performance self-supervised algorithm</a> to learn the same way for three different modalities: speech, vision, and text. Data2vec made it much easier to apply research advances in, say, text understanding to an image segmentation or speech translation task. </p><p> Today, we’re sharing data2vec 2.0, a new algorithm that is vastly more efficient and outperforms its predecessor’s strong performance. It achieves the same accuracy as the most popular existing self-supervised algorithm for computer vision but does so 16x faster. </p><p> To make our research accessible to other researchers, we are now sharing the code and pretrained models. </p><h3>How data2vec 2.0 works</h3><p> The general idea of self-supervised learning is for machines to learn the structure of images, speech, and text simply by observing the world. Advances in this area have led to many breakthroughs in speech (e.g., <a href="https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/">wav2vec 2.0</a>), computer vision (e.g., <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2111.06377&amp;h=AT22-55SV2Cx3eSIEdqAo_Q6G7iW43KHBAL1h-Yi1cFZ0JnEIWeg0g9Qn9VSHA48NZCIfW65lIEGw2WBvW5ZeDjOhrwDjrERfDdhTfgTARF12vp4eNz_3O7esAVbo7Ck5Z-Jh5eY5HyCc5iZ" rel="nofollow" target="_blank" data-lynx-mode="hover">masked autoencoders</a>), and natural language processing (e.g., <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&amp;h=AT2fHEq31sO7sU6hL7FXL9Ylc5xCrXcRXoXZ_Os1iqR3jYp2DPxWVVUFdksuI4XGFEdjpCIQhS8ozfIa_G4cph4HUOor0pAr8wBI1V8h8bRFHbIf47YXAw0V6vmzvp2DiOL8y3OYTE5vmmA8" rel="nofollow" target="_blank" data-lynx-mode="hover">BERT</a>). But modern systems can be computationally demanding, as training very large models requires many GPUs. </p><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/318123618_1092569024787137_2206011311141320511_n.png?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=UB4YGmzu2pYAX-Sd3oA&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfCEEjpm8TtD0ydQJpfN_IzDHsXOk3OzHxLCYPg8LDsveQ&amp;oe=639E4F3C" alt=""/></p><div><p> Illustration of how data2vec 2.0 training works. It can be trained separately on text, speech, or images.</p></div><p> Similar to the original data2vec algorithm, data2vec 2.0 predicts contextualized representations of the data — or the layers of a neural network — instead of the pixels of an image, the words of a text passage, or the sounds of speech. Unlike with most other algorithms, these so-called target representations are <i>contextualized</i>, meaning they take the entire training example into account. For instance, the representation of the word <i>bank</i> is based on the entire sentence that the word appears in, and it is therefore easier to represent the correct meaning of the word (“financial institution” or “ground next to river”). We believe that contextualized targets lead to a richer learning task and enable data2vec 2.0 to learn faster than other algorithms. </p><p> We improved the efficiency of the original data2vec algorithm in several ways: First, we take target representations built for a particular training example and reuse them for masked versions (where we hide different random parts of the training example). We take each version and feed it into the student model, which predicts the same contextualized target representation for the different masked versions. This effectively amortizes the computational effort required to create target representations. Second, and similar to <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2111.06377&amp;h=AT0vRz-iCZe2DXKmV_rtW7lLBU1h4L2BuOGx1dXJSYJmhRT7s2L-3tncCy2Pzi9su9tkTO4DsU1L_ffoRvEqqxAG_ulnm2NSiujWHOOvvlh8lI9mClym1vVzlqyfpz5kxZf7htXYoGTY4k7i" rel="nofollow" target="_blank" data-lynx-mode="hover">masked autoencoders</a>, we do not run the student encoder network for the parts of the training examples that are blanked out (which is about 80 percent of an image in our case), thereby saving significant compute cycles. Finally, we use a more efficient decoder model that relies not on Transformer networks but on a multilayer convolutional network. </p><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/319186157_534351931941993_3307301578232989223_n.png?_nc_cat=102&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=3WQ822IFm9QAX_Jzvfq&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfDgCw0jnb8A-nutfuiyDyP5x4o2KkyZR69R57KEuuir-A&amp;oe=639D78EB" alt=""/></p><div><p>Relative training time improvements when training data2vec 2.0 to the same accuracy as popular existing algorithms on the same hardware.</p></div><h2>Efficiency gains with data2vec 2.0 </h2><p> To get a better understanding of how much more efficient data2vec 2.0 is than its predecessor and other algorithms, we tested it on computer vision, speech, and text tasks on widely used benchmarks. We were looking at the final accuracy and the time it took to pretrain the model. We measured the speed of algorithms on the same hardware (number of GPUs, etc.). </p><p> For computer vision, we evaluated data2vec 2.0 on the standard ImageNet-1K image classification benchmark, where it learned to represent images. Data2vec 2.0 can equal the accuracy of masked autoencoders (MAE) but is 16x faster (measured in wall clock time in a like-for-like setting). If we give the algorithm more time, it can achieve even higher accuracy while still being faster than MAE. </p><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/318807517_927742464857899_4459922671946972048_n.png?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=79gCGB4-kEoAX8BRab9&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfCW3VPxVd-Rz95eCH8jab8qGMipWVRJ1Urd6V1RwalxPw&amp;oe=639EDB2C" alt=""/></p><div><p>Data2vec 2.0 for computer vision: The graph shows speed vs. image classification accuracy for different algorithms on the popular ImageNet-1K benchmark.</p></div><p> For speech, we tested it on the LibriSpeech speech recognition benchmark, where it performed more than 11 times faster than wav2vec 2.0 with similar accuracy. For natural language processing (NLP), we evaluated data2vec 2.0 on the popular General Language Understanding Evaluation (GLUE) benchmark, where it achieved the same accuracy as <a href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/">RoBERTa</a>, a reimplementation of <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&amp;h=AT1ChNQ-V5jDIRgs_DkNk2rAmVaxvlF1zNbWRPbe3-Fn2ygxFXo4TH3e7y4NxKbr9-wMwStvcHHWW6NUDcUSJHvEO7KCPPQcxtCbqFlyLjuL2WQ2FLKmRdxu0ukGS7R4CBIucuZ6gLFDqlvL" rel="nofollow" target="_blank" data-lynx-mode="hover">BERT</a>, in half the training time. </p><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/317978784_2344920385665802_1248577109920944318_n.png?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=4-G2XrPWXIgAX_Cjn0y&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfDp4SfvWu4YbafIbsK5CSk_Prw1xBu1yOYjMBWZYd6i9g&amp;oe=639F20B2" alt=""/><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/317805791_3369791179938238_3976527047057626004_n.png?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=SIPR63aU1LsAX9VB60k&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfCOt2CZccVrj3irlK4NSsBfYZEu7InWo47e65_DQkPgKg&amp;oe=639E89CC" alt=""/></p><div><p>Data2vec 2.0 for speech and NLP: The top graph shows speed vs. speech recognition word error rate for models pretrained on LibriSpeech, fine-tuned on 10 hours of Libri-light data, and then evaluated on dev-other. The bottom graph shows natural language understanding accuracy on the GLUE benchmark when using the original BERT setup.</p></div><h2>Toward machines that learn efficiently</h2><p> We are on a journey to build more general and efficient self-supervised algorithms that can use a single learning objective to learn from different modalities. The ability to learn more efficiently is particularly important for modalities such as video, which require a lot of computational effort to process. We hope that more efficient self-supervised learning algorithms such as data2vec 2.0 will lead to machines that can deeply understand extremely complex data, such as the contents of an entire movie. </p><p> Access the open source code and pretrained models here, and read the paper hear. </p><h2>Get it on GitHub:</h2><p><a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Ffairseq%2Ftree%2Fmaster%2Fexamples%2Fdata2vec&amp;h=AT16bZDNfZIh4Vdt2a2wK8Xb9yl4odNKx_nrhXphe5GyG-MRPCU4vRrotUozyoMtCVflyneLVJR9zfa2kQ-VjKpua9p4DBj1AE0betcG3e1uyYLwLfgC6TjAopniITZ3eMeMSpj1IAJSnN0v" rel="nofollow" target="_blank" data-lynx-mode="hover">https://github.com/facebookresearch/fairseq/tree/master/examples/data2vec</a></p><h2>Read the paper:</h2><p><a href="https://ai.facebook.com/research/publications/efficient-self-supervised-learning-with-contextualized-target-representations-for-vision-speech-and-language">Efficient self-supervised learning with contextualized target representations for speech, vision, and language</a></p><p><i>This blog post was made possible by the work of Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli.</i></p><p><i>The second graphic in this post has been updated to correct a typographical error.</i></p></div></div></div>
  </body>
</html>
