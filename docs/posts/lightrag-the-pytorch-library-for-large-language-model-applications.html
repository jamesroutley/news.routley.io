<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/SylphAI-Inc/LightRAG">Original</a>
    <h1>LightRAG: The PyTorch Library for Large Language Model Applications</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/LightRAG-logo-doc.jpeg"><img src="https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/LightRAG-logo-doc.jpeg" alt="LightRAG Logo"/></a></p>


<p dir="auto"><a href="https://opensource.org/license/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/9a0e4adadbc6dc2fac84eca16d8239657c9ebed14cfb924622944c05df66c8d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f53796c706841492d496e632f4c69676874524147" alt="License" data-canonical-src="https://img.shields.io/github/license/SylphAI-Inc/LightRAG"/></a>
<a href="https://pypi.org/project/lightRAG/" rel="nofollow"><img src="https://camo.githubusercontent.com/84c52dd3c717990b50e3e1ac919b5c267efd3773afa5ca1615d56fb61532766c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c696768745241473f7374796c653d666c61742d737175617265" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/lightRAG?style=flat-square"/></a>
<a href="https://pypistats.org/packages/lightRAG" rel="nofollow"><img src="https://camo.githubusercontent.com/18511efa8b7fa2509c45ad10fa4b97f9045f4ded8758a2e3e9e1f18c0310ddc1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c696768745241473f7374796c653d666c61742d737175617265" alt="PyPI - Downloads" data-canonical-src="https://img.shields.io/pypi/dm/lightRAG?style=flat-square"/></a>
<a href="https://star-history.com/#SylphAI-Inc/LightRAG" rel="nofollow"><img src="https://camo.githubusercontent.com/1679ccaf2785ea535275ad60dedaaed67d9f0b6f677947adf2fa19f8952cade0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53796c706841492d496e632f4c696768745241473f7374796c653d666c61742d737175617265" alt="GitHub star chart" data-canonical-src="https://img.shields.io/github/stars/SylphAI-Inc/LightRAG?style=flat-square"/></a>
<a href="https://github.com/SylphAI-Inc/LightRAG/issues"><img src="https://camo.githubusercontent.com/e7b83495312bf804356719d83fdc7aec569b64eef10bef1d61018c6559f9a1d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f53796c706841492d496e632f4c696768745241473f7374796c653d666c61742d737175617265" alt="Open Issues" data-canonical-src="https://img.shields.io/github/issues-raw/SylphAI-Inc/LightRAG?style=flat-square"/></a>
<a href="https://discord.gg/zt2mTPcu" rel="nofollow"><img src="https://camo.githubusercontent.com/a6dcb35cccabcba4f9c369666186f66eb0a37932e30728db23dbe493a684f650/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f7a74326d545063753f636f6d706163743d74727565267374796c653d666c6174" alt="" data-canonical-src="https://dcbadge.vercel.app/api/server/zt2mTPcu?compact=true&amp;style=flat"/></a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">⚡ The PyTorch Library for Large Language Model Applications ⚡</h3><a id="user-content--the-pytorch-library-for-large-language-model-applications-" aria-label="Permalink: ⚡ The PyTorch Library for Large Language Model Applications ⚡" href="#-the-pytorch-library-for-large-language-model-applications-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><em>LightRAG</em> helps developers with both building and optimizing <em>Retriever-Agent-Generator (RAG)</em> pipelines.
It is <em>light</em>, <em>modular</em>, and <em>robust</em>, with a 100% readable codebase.</p>

<p dir="auto">LLMs are like water; they can almost do anything, from GenAI applications such as chatbots, translation, summarization, code generation, and autonomous agents to classical NLP tasks like text classification and named entity recognition. They interact with the world beyond the model’s internal knowledge via retrievers, memory, and tools (function calls). Each use case is unique in its data, business logic, and user experience.</p>
<p dir="auto">Because of this, no library can provide out-of-the-box solutions. Users must build towards their own use case, which requires the library to be modular, robust, and have a clean and readable codebase. The only code you should put into production is code you either trust 100% or are 100% clear about how to customize and iterate.</p>
<p dir="auto">LightRAG is born to be light, modular, and robust, with a 100% readable codebase.</p>
<p dir="auto">Further reading: <a href="https://lightrag.sylph.ai/" rel="nofollow">Introduction</a>, <a href="https://lightrag.sylph.ai/developer_notes/lightrag_design_philosophy.html" rel="nofollow">Design Philosophy</a> and <a href="https://lightrag.sylph.ai/developer_notes/class_hierarchy.html" rel="nofollow">Class hierarchy</a>.</p>


<p dir="auto">We will ask the model to respond with <code>explanation</code> and <code>example</code> of a concept. And we will build a pipeline to get the structured output as <code>QAOutput</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
from dataclasses import dataclass, field

from lightrag.core import Component, Generator, DataClass
from lightrag.components.model_client import GroqAPIClient
from lightrag.components.output_parsers import JsonOutputParser

@dataclass
class QAOutput(DataClass):
    explanation: str = field(
        metadata={&#34;desc&#34;: &#34;A brief explanation of the concept in one sentence.&#34;}
    )
    example: str = field(metadata={&#34;desc&#34;: &#34;An example of the concept in a sentence.&#34;})



qa_template = r&#34;&#34;&#34;&lt;SYS&gt;
You are a helpful assistant.
&lt;OUTPUT_FORMAT&gt;
{{output_format_str}}
&lt;/OUTPUT_FORMAT&gt;
&lt;/SYS&gt;
User: {{input_str}}
You:&#34;&#34;&#34;

class QA(Component):
    def __init__(self):
        super().__init__()

        parser = JsonOutputParser(data_class=QAOutput, return_data_class=True)
        self.generator = Generator(
            model_client=GroqAPIClient(),
            model_kwargs={&#34;model&#34;: &#34;llama3-8b-8192&#34;},
            template=qa_template,
            prompt_kwargs={&#34;output_format_str&#34;: parser.format_instructions()},
            output_processors=parser,
        )

    def call(self, query: str):
        return self.generator.call({&#34;input_str&#34;: query})

    async def acall(self, query: str):
        return await self.generator.acall({&#34;input_str&#34;: query})"><pre><span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span>, <span>field</span>

<span>from</span> <span>lightrag</span>.<span>core</span> <span>import</span> <span>Component</span>, <span>Generator</span>, <span>DataClass</span>
<span>from</span> <span>lightrag</span>.<span>components</span>.<span>model_client</span> <span>import</span> <span>GroqAPIClient</span>
<span>from</span> <span>lightrag</span>.<span>components</span>.<span>output_parsers</span> <span>import</span> <span>JsonOutputParser</span>

<span>@<span>dataclass</span></span>
<span>class</span> <span>QAOutput</span>(<span>DataClass</span>):
    <span>explanation</span>: <span>str</span> <span>=</span> <span>field</span>(
        <span>metadata</span><span>=</span>{<span>&#34;desc&#34;</span>: <span>&#34;A brief explanation of the concept in one sentence.&#34;</span>}
    )
    <span>example</span>: <span>str</span> <span>=</span> <span>field</span>(<span>metadata</span><span>=</span>{<span>&#34;desc&#34;</span>: <span>&#34;An example of the concept in a sentence.&#34;</span>})



<span>qa_template</span> <span>=</span> <span>r&#34;&#34;&#34;&lt;SYS&gt;</span>
<span>You are a helpful assistant.</span>
<span>&lt;OUTPUT_FORMAT&gt;</span>
<span>{{output_format_str}}</span>
<span>&lt;/OUTPUT_FORMAT&gt;</span>
<span>&lt;/SYS&gt;</span>
<span>User: {{input_str}}</span>
<span>You:&#34;&#34;&#34;</span>

<span>class</span> <span>QA</span>(<span>Component</span>):
    <span>def</span> <span>__init__</span>(<span>self</span>):
        <span>super</span>().<span>__init__</span>()

        <span>parser</span> <span>=</span> <span>JsonOutputParser</span>(<span>data_class</span><span>=</span><span>QAOutput</span>, <span>return_data_class</span><span>=</span><span>True</span>)
        <span>self</span>.<span>generator</span> <span>=</span> <span>Generator</span>(
            <span>model_client</span><span>=</span><span>GroqAPIClient</span>(),
            <span>model_kwargs</span><span>=</span>{<span>&#34;model&#34;</span>: <span>&#34;llama3-8b-8192&#34;</span>},
            <span>template</span><span>=</span><span>qa_template</span>,
            <span>prompt_kwargs</span><span>=</span>{<span>&#34;output_format_str&#34;</span>: <span>parser</span>.<span>format_instructions</span>()},
            <span>output_processors</span><span>=</span><span>parser</span>,
        )

    <span>def</span> <span>call</span>(<span>self</span>, <span>query</span>: <span>str</span>):
        <span>return</span> <span>self</span>.<span>generator</span>.<span>call</span>({<span>&#34;input_str&#34;</span>: <span>query</span>})

    <span>async</span> <span>def</span> <span>acall</span>(<span>self</span>, <span>query</span>: <span>str</span>):
        <span>return</span> <span>await</span> <span>self</span>.<span>generator</span>.<span>acall</span>({<span>&#34;input_str&#34;</span>: <span>query</span>})</pre></div>
<p dir="auto">Run the following code for visualization and calling the model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
qa = QA()
print(qa)

# call
output = qa(&#34;What is LLM?&#34;)
print(output)"><pre><span>qa</span> <span>=</span> <span>QA</span>()
<span>print</span>(<span>qa</span>)

<span># call</span>
<span>output</span> <span>=</span> <span>qa</span>(<span>&#34;What is LLM?&#34;</span>)
<span>print</span>(<span>output</span>)</pre></div>
<p dir="auto"><strong>Structure of the pipeline</strong></p>
<p dir="auto">Here is what we get from <code>print(qa)</code>:</p>
<div data-snippet-clipboard-copy-content="QA(
  (generator): Generator(
    model_kwargs={&#39;model&#39;: &#39;llama3-8b-8192&#39;},
    (prompt): Prompt(
      template: &lt;SYS&gt;
      You are a helpful assistant.
      &lt;OUTPUT_FORMAT&gt;
      {{output_format_str}}
      &lt;/OUTPUT_FORMAT&gt;
      &lt;/SYS&gt;
      User: {{input_str}}
      You:, prompt_kwargs: {&#39;output_format_str&#39;: &#39;Your output should be formatted as a standard JSON instance with the following schema:\n```\n{\n    &#34;explanation&#34;: &#34;A brief explanation of the concept in one sentence. (str) (required)&#34;,\n    &#34;example&#34;: &#34;An example of the concept in a sentence. (str) (required)&#34;\n}\n```\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n-Use double quotes for the keys and string values.\n-Follow the JSON formatting conventions.&#39;}, prompt_variables: [&#39;output_format_str&#39;, &#39;input_str&#39;]
    )
    (model_client): GroqAPIClient()
    (output_processors): JsonOutputParser(
      data_class=QAOutput, examples=None, exclude_fields=None, return_data_class=True
      (json_output_format_prompt): Prompt(
        template: Your output should be formatted as a standard JSON instance with the following schema:
        ```
        {{schema}}
        ```
        {% if example %}
        Examples:
        ```
        {{example}}
        ```
        {% endif %}
        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!
        -Use double quotes for the keys and string values.
        -Follow the JSON formatting conventions., prompt_variables: [&#39;schema&#39;, &#39;example&#39;]
      )
      (output_processors): JsonParser()
    )
  )
)"><pre><code>QA(
  (generator): Generator(
    model_kwargs={&#39;model&#39;: &#39;llama3-8b-8192&#39;},
    (prompt): Prompt(
      template: &lt;SYS&gt;
      You are a helpful assistant.
      &lt;OUTPUT_FORMAT&gt;
      {{output_format_str}}
      &lt;/OUTPUT_FORMAT&gt;
      &lt;/SYS&gt;
      User: {{input_str}}
      You:, prompt_kwargs: {&#39;output_format_str&#39;: &#39;Your output should be formatted as a standard JSON instance with the following schema:\n```\n{\n    &#34;explanation&#34;: &#34;A brief explanation of the concept in one sentence. (str) (required)&#34;,\n    &#34;example&#34;: &#34;An example of the concept in a sentence. (str) (required)&#34;\n}\n```\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n-Use double quotes for the keys and string values.\n-Follow the JSON formatting conventions.&#39;}, prompt_variables: [&#39;output_format_str&#39;, &#39;input_str&#39;]
    )
    (model_client): GroqAPIClient()
    (output_processors): JsonOutputParser(
      data_class=QAOutput, examples=None, exclude_fields=None, return_data_class=True
      (json_output_format_prompt): Prompt(
        template: Your output should be formatted as a standard JSON instance with the following schema:
        ```
        {{schema}}
        ```
        {% if example %}
        Examples:
        ```
        {{example}}
        ```
        {% endif %}
        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!
        -Use double quotes for the keys and string values.
        -Follow the JSON formatting conventions., prompt_variables: [&#39;schema&#39;, &#39;example&#39;]
      )
      (output_processors): JsonParser()
    )
  )
)
</code></pre></div>
<p dir="auto"><strong>The output</strong></p>
<p dir="auto">Here is what we get from <code>print(output)</code>:</p>
<div data-snippet-clipboard-copy-content="GeneratorOutput(data=QAOutput(explanation=&#39;LLM stands for Large Language Model, which refers to a type of artificial intelligence designed to process and generate human-like language.&#39;, example=&#39;For instance, LLMs are used in chatbots and virtual assistants, such as Siri and Alexa, to understand and respond to natural language input.&#39;), error=None, usage=None, raw_response=&#39;```\n{\n  &#34;explanation&#34;: &#34;LLM stands for Large Language Model, which refers to a type of artificial intelligence designed to process and generate human-like language.&#34;,\n  &#34;example&#34;: &#34;For instance, LLMs are used in chatbots and virtual assistants, such as Siri and Alexa, to understand and respond to natural language input.&#34;\n}&#39;, metadata=None)"><pre><code>GeneratorOutput(data=QAOutput(explanation=&#39;LLM stands for Large Language Model, which refers to a type of artificial intelligence designed to process and generate human-like language.&#39;, example=&#39;For instance, LLMs are used in chatbots and virtual assistants, such as Siri and Alexa, to understand and respond to natural language input.&#39;), error=None, usage=None, raw_response=&#39;```\n{\n  &#34;explanation&#34;: &#34;LLM stands for Large Language Model, which refers to a type of artificial intelligence designed to process and generate human-like language.&#34;,\n  &#34;example&#34;: &#34;For instance, LLMs are used in chatbots and virtual assistants, such as Siri and Alexa, to understand and respond to natural language input.&#34;\n}&#39;, metadata=None)
</code></pre></div>
<p dir="auto"><strong>See the prompt</strong></p>
<p dir="auto">Use the following code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="
qa2.generator.print_prompt(
        output_format_str=qa2.generator.output_processors.format_instructions(),
        input_str=&#34;What is LLM?&#34;,
)"><pre><span>qa2</span>.<span>generator</span>.<span>print_prompt</span>(
        <span>output_format_str</span><span>=</span><span>qa2</span>.<span>generator</span>.<span>output_processors</span>.<span>format_instructions</span>(),
        <span>input_str</span><span>=</span><span>&#34;What is LLM?&#34;</span>,
)</pre></div>
<p dir="auto">The output will be:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&lt;SYS&gt;
You are a helpful assistant.
&lt;OUTPUT_FORMAT&gt;
Your output should be formatted as a standard JSON instance with the following schema:
```
{
    &#34;explanation&#34;: &#34;A brief explanation of the concept in one sentence. (str) (required)&#34;,
    &#34;example&#34;: &#34;An example of the concept in a sentence. (str) (required)&#34;
}
```
-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!
-Use double quotes for the keys and string values.
-Follow the JSON formatting conventions.
&lt;/OUTPUT_FORMAT&gt;
&lt;/SYS&gt;
User: What is LLM?
You:"><pre>&lt;<span>SYS</span>&gt;
You are a helpful assistant.
&lt;OUTPUT_FORMAT&gt;
Your output should be formatted as a standard JSON instance with the following schema:
<span>```</span><span></span>
<span>{</span>
<span>    &#34;explanation&#34;: &#34;A brief explanation of the concept in one sentence. (str) (required)&#34;,</span>
<span>    &#34;example&#34;: &#34;An example of the concept in a sentence. (str) (required)&#34;</span>
<span>}</span>
<span></span><span>```</span>
-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!
-Use double quotes for the keys and string values.
-Follow the JSON formatting conventions.
&lt;/OUTPUT_FORMAT&gt;
&lt;/<span>SYS</span>&gt;
User: What is LLM?
You:</pre></div>

<p dir="auto">Install LightRAG with pip:</p>

<p dir="auto">Please refer to the <a href="https://lightrag.sylph.ai/get_started/installation.html" rel="nofollow">full installation guide</a> for more details.</p>

<p dir="auto">LightRAG full documentation available at <a href="https://lightrag.sylph.ai/" rel="nofollow">lightrag.sylph.ai</a>:</p>
<ul dir="auto">
<li><a href="https://lightrag.sylph.ai/" rel="nofollow">Introduction</a></li>
<li><a href="https://lightrag.sylph.ai/get_started/installation.html" rel="nofollow">Full installation guide</a></li>
<li><a href="https://lightrag.sylph.ai/developer_notes/lightrag_design_philosophy.html" rel="nofollow">Design philosophy</a></li>
<li><a href="https://lightrag.sylph.ai/developer_notes/class_hierarchy.html" rel="nofollow">Class hierarchy</a></li>
<li><a href="https://lightrag.sylph.ai/developer_notes/index.html" rel="nofollow">Tutorials</a></li>
<li><a href="https://lightrag.sylph.ai/apis/index.html" rel="nofollow">API reference</a></li>
</ul>

<p dir="auto"><a href="https://github.com/SylphAI-Inc/LightRAG/graphs/contributors"><img src="https://camo.githubusercontent.com/ce7b6b814ec41a8d16b437507948d469e5853e7bb2068339b1c6526dcda149eb/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d53796c706841492d496e632f4c69676874524147266d61783d32303030" alt="contributors" data-canonical-src="https://contrib.rocks/image?repo=SylphAI-Inc/LightRAG&amp;max=2000"/></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="@software{Yin2024LightRAG,
  author = {Li Yin},
  title = {{LightRAG: The PyTorch Library for Large Language Model (LLM) Applications}},
  month = {7},
  year = {2024},
  doi = {10.5281/zenodo.12639531},
  url = {https://github.com/SylphAI-Inc/LightRAG}
}"><pre><span>@software</span>{<span>Yin2024LightRAG</span>,
  <span>author</span> = <span><span>{</span>Li Yin<span>}</span></span>,
  <span>title</span> = <span><span>{</span>{LightRAG: The PyTorch Library for Large Language Model (LLM) Applications}<span>}</span></span>,
  <span>month</span> = <span><span>{</span>7<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
  <span>doi</span> = <span><span>{</span>10.5281/zenodo.12639531<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/SylphAI-Inc/LightRAG<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
