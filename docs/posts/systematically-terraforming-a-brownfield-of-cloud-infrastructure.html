<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.evalapply.org/posts/systems-approach-to-infrastructure-as-code/index.html">Original</a>
    <h1>Systematically Terraforming a Brownfield of Cloud Infrastructure</h1>
    
    <div id="readability-page-1" class="page"><div id="the-very-top">
        
  <main id="main">
    <article id="blog-post">
  <header>
    <div>
      <p>Systematically Terraforming a Brownfield of Cloud Infrastructure</p>
      
      <p>Some thinking, trade-offs, theory building, and method-making one might ended up doing, in the course of bringing Infrastructure as Code (IaC) discipline to brownfield (and greenfield) services, at a small regulated fintech company, having a smaller engineering team that serves several business units, including one of India&#39;s largest national tax gateways. Only somewhat easier than reading a long compound sentence without pausing for breath. Phew.</p>
      
      <hr/>
    </div>
  </header>
  <section>
      
<hr/>
  
<p>Take what is useful, discard the rest.</p>
<ul>
<li>This post was salvaged from draft hell <a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Parts of it may not have aged well. Other parts may be repetitive and repetitive.</li>
<li>It is <em>not</em> specifically about Hashicorp&#39;s Terraform <a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> (or about me <a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>), though both of us are players on the board.</li>
<li>Follow the ToC for a quick topical summary.</li>
<li>The sample Terraform <a href="#use-a-mono-repo-for-great-good">monorepo layout here</a> illustrates what I ended up making.</li>
</ul>
<p>This is a <em>long</em> read.</p>
<p>Suit up!</p>
<figure>
<img src="https://www.evalapply.org/posts/systems-approach-to-infrastructure-as-code/suits-meme.jpg"/>
<figcaption>Harvey Specter, about to underestimate Mike Ross.</figcaption>
</figure>

<blockquote>
<p>A Fortune 500 CTO counts many zeroes (and nines). A Startup CTO counts fewer. Yet they are equally sleep-deprived.</p>
<p>— Zen koan</p>
</blockquote>
<p>&#39;twas the <em>Before Times</em>, at a small <em>&#34;fintech&#34;</em> company with a smaller engineering team. &#39;twas a CTO one really wanted to work for. Shortly after joining his team, one was bidden; <em>&#34;Go forth! Terraform all our Infrastructure.&#34;</em>.</p>
<h2 id="the-goal">The Goal…</h2>
<p>… <a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> was to replace the extant ad-hoc semi-automation, with a comprehensive <em>system</em> for use company-wide, over the long haul.</p>
<blockquote>
<p><em>&#34;Mnhmm, so like, we&#39;re a 50-ish person company? Total? How hard could this be?&#34;</em></p>
<p>— My brain.</p>
</blockquote>
<p>Such pleasant thoughts.</p>
<p>One looked closer.</p>
<p>And closer.</p>
<p>Closer…</p>
<p>Until one was staring into the abyss, haunted by the specter of abject failure. Because, complexity manifests stunningly easily, even in a small company.</p>
<h2 id="a-conglomerate-in-startup-clothing">A conglomerate in startup clothing</h2>
<p>Though small, the company had (has) many business interests (and consequently, business units); supply chain finance, corporate tax compliance, services that touch receivables and payments. This means…</p>
<ul>
<li>Handling many different categories of customer financial records.</li>
<li>Working with banks, NBFCs, government IT infrastructure (private MPLS lines, anyone?), and enterprise IT systems (a lot of SAP!).</li>
<li>Working under lavishly sprawling Banking and Finance regulations.</li>
<li>Keeping Enterprise IT certification current (PCI-DSS, ISO…).</li>
</ul>
<p>Smallness, alas, is no excuse to the powers that be. So. Many. Audits. Plus, each business unit has its own distinct customer base, operating model, and third party integrations.</p>
<p>From a DevOps point of view, such business diversification would have been merely onerous in a less regulated industry. However in a heavily regulated one, it spells trouble. All businesses units must be siloed not just legally, but also computationally (repo owners, builds and deploys, storage, servers, networking etc.).</p>
<p>If everything is siloed, how to make One System to rule them all?</p>
<h2 id="no-stopping-the-world">No stopping the world</h2>
<p>While we build out this grand new all-encompassing vision, business continuity demands that the business continue to use the legacy mix bag of semi-automated scripts and manual AWS console processes. And let people make their new stuff while our new stuff is still not up to snuff.</p>
<h2 id="organisation-wide-impact">Organisation-wide impact</h2>
<blockquote>
<p>&#34;Uhhh, so like, if this project fails it will, at the very least, nuke several person-years of productivity? Because several teams are counting on it, and they will each be set back at least several months?&#34;</p>
<p>— My brain, staring into the abyss.</p>
</blockquote>
<p>The un-system we had was getting unwieldy and risky by the day, owing to the increasing pace and diversification of the company&#39;s business interests. Some, like the national tax compliance gateway, were growing fast. Meanwhile, extant infrastructure management and IT operations had accreted over many years, trailing the accretion of business interests.</p>
<p>But the un-system we had <em>worked</em>. So if the new thing didn&#39;t, then the old thing would eventually fail, but by that time nobody would have the time or budget to make <em>another</em> new thing, without taking away from whatever they were actually supposed to be building. Not that I would know, because then I&#39;d be out of a job.</p>
<h2 id="no-move-fast-and-break-things">No move fast and break things</h2>
<p>One of the unique delights of regulation / compliance is that a CTO must sit on their hands for six months before they can allow people like you and I to touch production. Nobody is allowed to move fast and breaking things here, sorry.</p>
<p>Luckily there is <em>not-production</em> which you and I <em>can</em> break at will. We know because we <em>have</em>, in fact, broken non-prod. Some of us (not I, sadly) have done it often enough to earn a custom emoji in the corporate Slack.</p>
<h2 id="one-more-thing">One more thing…</h2>
<p>The number one priority for this Infra-as-code project was to &#34;lift and shift&#34; a tax gateway service from its crufty docker swarm situation to <em>&#34;something that lets everybody sleep better at night&#34;</em>.</p>
<p>A fifth of the whole country&#39;s corporate tax filings were flowing through said gateway. This was spiky, bi-weekly, aligned with routine corporate tax compliance deadlines. However, the service was gearing for &#34;e-way bill&#34; tax compliance traffic, which would be <em>live</em>, and cause at least an order of magnitude jump in total traffic. Goods carriers must generate an &#34;e-way bill&#34; for the exact shipment, and register it with the tax gateway on a &#34;just in time&#34; basis. Transit is not allowed sans that document.</p>
<p>In other words, this service absolutely could not afford to go offline at any point in the infrastructure migration (and certainly not after!). Otherwise a large fraction of India&#39;s corporate tax filings, and worse, goods transit would be disrupted. When it comes to things that must <em>flow</em>, even small disruptions have large knock-on effects <a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>So no pressure, really.</p>
<p><img src="https://www.evalapply.org/posts/systems-approach-to-infrastructure-as-code/indiana-jones-swap-meme.jpg"/></p>

<p>Our CTO had evaluated options and chosen Hashicorp&#39;s Terraform <a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> for our IaC requirements.</p>

<p>Any tool imposes design trade-offs. Choosing it means we have chosen its baked-in assumptions and opinions about what IaC means. Design dominates everything, and so, to have a shot at succeed, one must deeply understand <em>the tool&#39;s model of reality</em>.</p>
<p>In the case of Hashicorp Terraform, understanding its state model, and learning the ins and outs of its behaviour under live fire is critical one&#39;s thinking about change, <u>and</u> one&#39;s process and workflow of change management.</p>
<h2 id="all-green-fields-turn-brown">All green fields turn brown</h2>
<p>My experience is certainly not unique. Most of us have had to introduce Terraform in an already-live, maybe semi-automated runtime. Even if one starts &#34;greenfield&#34;, change is inevitable. One&#39;s system will change in small and big ways, and one&#39;s codebase will grow and be refactored. Terraform itself will change too. One will have to keep up with all these changes. For example, I started using Terraform circa HCL 0.11.x and then had to migrate our codebase to HCL 0.12.x, which introduced breaking changes in HCL itself.</p>
<h2 id="operator-knowledge-is-tacit">Operator knowledge is tacit</h2>
<p>As with any operator tool, real-world Terraform usage involves a <strong>lot</strong> of tacit and tribal knowledge, in addition to a solid grasp of one&#39;s local operating context. The official documentation and manuals can do only so much for an operator.</p>
<p>It is therefore important to discover ways to model reality with the tools at hand, and construct a closely reasoned theory of change suited to the needs of the business.</p>
<h2 id="learn-from-all-available-prior-art">Learn from all available prior art</h2>
<p>A ton of prior art—tribal knowledge—that helped me understand how to use Terraform effectively, while meeting my system design goals. Thanks to all these people for publicly sharing how they think. When there are no text books, stories are all we have.</p>
<ul>
<li><p><strong>Design sensibilities:</strong> My first devops role (we called our practice &#34;production engineering&#34;) shaped my thinking about infrastructure automation. My former colleague and team lead explains in his talk.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/9aWclomg4d0?si=sNMjqDHgRJF0Oqfl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</li>
<li><p><strong>Terraform mastery:</strong> advanced users sharing how they solved various real-world problems; simple as well as sophisticated.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=G06j6HLWyYo">Happy Terraforming! Real Life Experience and Proven Best Practices</a></li>
<li><a href="https://cdn.oreillystatic.com/en/assets/1/event/275/Lessons%20learned%20migrating%20HealthCare_gov%20to%20Terraform%20Presentation.pdf">PDF: Migrating HealthCare.gov to Terraform:Lessons Learned</a></li>
<li><a href="https://www.antonbabenko.com/2016/09/21/how-i-structure-terraform-configurations.html">How I structure terraform configurations</a></li>
<li><a href="https://opencredo.com/blogs/terraform-infrastructure-design-patterns/">Terraform Infrastructure design patterns</a></li>
<li><a href="https://manage.rackspace.com/aws/docs/product-guide/iac_beta/terraform-standards.html">Terraform Standards from Rackspace</a></li>
</ul></li>
</ul>

<p>The regulators created mandatory bench time, my CTO provided resources and liberty to run all sorts of experiments, and modern clouds make it cheap to run them. So I had leeway to figure out:</p>
<ul>
<li>What the heck &#34;immutable infrastructure&#34; is supposed to be?</li>
<li>Ought we even treat infra as immutable, if yes, what are the rules?</li>
<li>The language constraints and sharp edges of the HCL language.</li>
<li>The ins and outs of state management with Terraform.</li>
<li>Design and tooling for safe change management, operations procedures and operator safety, staff onboarding, compliance audits, code migrations, error recovery etc.</li>
</ul>
<p>The intention was to:</p>
<ul>
<li>Locate any hard constraints and one-way streets (architecture/service lock-ins)
<ul>
<li>As far as possible eschew one-way choices</li>
<li>Find and validate at least one migration path away from any foundational one-way choice</li>
</ul></li>
<li>Identify stated and unstated system boundaries</li>
<li>Find and mitigate limitations, warts, and idiosyncrasies of the chosen tools</li>
<li>Discover safe workflows and operational properties of the new management system</li>
<li>Articulate &#34;how-to&#34; <em>and</em> &#34;why-to&#34; knowledge for operators and maintainers</li>
</ul>
<p>With 20/20 hindsight, here is a list of exercises to do, to acquire <em>das Fingerspitzengefühl</em> with Terraform. In real life, I did these things as they occurred to me.</p>
<h2 id="bare-bones-setup">Bare-bones Setup</h2>
<p>Write some terraform to set up some infrastructure:</p>
<ul>
<li>Write everything in a single directory</li>
<li>Let it be something small but representative; let&#39;s say, three or four distinct resources. e.g. a VPC with public/private subnet, an S3 bucket, maybe a datastore.</li>
<li>Do NOT use third-party modules at this time. Just keep it simple and write everything yourself.</li>
</ul>
<h2 id="progressively-riskier-code-refactoring">Progressively riskier code refactoring</h2>
<ul>
<li><strong>Small refactor:</strong> Using the bare-bones setup, try to improve names of some resources (run-of-the-mill refactoring). Run terraform plan and observe the changes.</li>
<li><strong>Small run-time change:</strong> Next, go to the AWS console and alter some resource properties there, such as resource names, or resource config parameters. Try to fix your terraform to the new reality (e.g. someone did an emergency fix via the AWS console).</li>
<li><strong>Medium refactor:</strong> Now try to split your terraform code into a better directory structure… assume the codebase has grown and needs to be factored better. Pass dependencies from one directory to another. Consider what order would you need to do terraform plan/apply if something changes in the directories.</li>
<li><strong>Large refactor:</strong> Try to bring in a third-party module to manage one of the pieces. This emulates a condition where you originally decided said third party module was too much. Eventually your usage grew such that you would rather pick the module as it&#39;s now a better choice.</li>
<li><strong>Add environments:</strong> Now try to support a &#34;test&#34;, &#34;staging&#34;, and &#34;prod&#34; setup, with the same structure but varied configuration, without duplicating code. The two setups must be controlled by variable files only. Configs can change (e.g. you want cheaper resources in staging), but the architecture must be the same. This is because changes to infra must be testable in a safe environment before propagating to &#34;staging&#34;, and then finally to &#34;production&#34;.</li>
</ul>
<h2 id="migration-exercise">Migration Exercise</h2>
<p>Try to change something fundamental, like your VPC&#39;s CIDR block (e.g. new requirement to migrate to a different VPC structure). Once a VPC is created, its allocations cannot be modified. There is no way to do an in-place upgrade, and a &#34;whole data center migration&#34; is needed.</p>
<ul>
<li>Think through what all must be accounted for, in order to make the migration succeed?</li>
<li>Assume the current VPC is a live system, that cannot go down while the migration is in progress.</li>
</ul>
<h2 id="disaster-exercise">Disaster Exercise</h2>
<p>Lose your state file, but keep your infrastructure. Try to rebuild your Terraform state file from your live infrastructure.</p>
<p>Destroy critical parts of your infrastructure, but keep your state file. Try to restore your infrastructure from the state file.</p>
<p>Take copious notes about the steps taken, issues faced, manual workarounds needed. And at the very least, make notes about how to:</p>
<ul>
<li>Prevent this from happening.</li>
<li>Recover if the worst happens.</li>
<li>Teach colleagues how to respond.</li>
</ul>
<p>I didn&#39;t attempt to solve for the whole thing, only attempted prevention. This, I did with a combination of S3 bucket versioning policies + IAM policies + scripted CLI to defend against typos / accidental command invocations + checklists for change management procedures + some operator training (more like admonishment: <em>&#34;If we lose that state file, we&#39;re dead.&#34;</em>).</p>

<p>This is a bonus exercise. I did it, by chance, without wanting to.</p>
<p>Pick the most complicated service from your cloud provider, that you want to use in &#34;guru&#34; mode; configured to the hilt. See if Terraform supports it fully. OR trawl the AWS provider issues to find a service that it does not yet support fully. Figure out what you&#39;d do in that case. Legitimate options include:</p>
<ul>
<li>Patch the provider source.</li>
<li>Work around with an out-of-band script and/or checklist.</li>
<li>Do nothing. Defer until the provider rolls out an update.</li>
</ul>
<p>In my case the Terraform AWS Provider did not support certain advanced routing rules for the AWS Application Load Balancer. If memory serves, this was being discussed in the AWS provider Github at the time. Since I had a one-time-only use case (a live migration), I got away by scripting the rules, because AWS CLI had the requisite support.</p>

<p>Though I didn&#39;t invent anything new, all the outside lessons coalesced through <em>The Force</em> within <a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>, into a (I think) rather neat solution, over the weeks and months it took to find it. In other words, I made it up as I went along and what follows could be totally <u>not</u> industry standard practice!</p>
<p>Here is what I thunk (and think).</p>
<h2 id="large-scale-architecture-must-be-functional-and-composable">Large-scale Architecture must be Functional and Composable</h2>
<p>However, Terraform itself is <u>not</u> functional. It is a highly stateful, object oriented system, with what amounts to remote procedure calls. Complects structure + behaviour + state. The ansible system is imperative in the small, but functional in the large because it is an <em>idempotent</em> transform of infra -&gt; infra, with UNIX-like composable workflows, dynamic in-process state as a reference (dynamic inventory).</p>
<h2 id="constrain-change-not-people">Constrain change, not people</h2>
<p>As a designer, I strongly prefer to build a system that helps us:</p>
<ul>
<li><em>Strictly gate-keep, structure, and micro-manage</em> changes to live production, at one extreme of the management spectrum.</li>
<li><em>And</em> at the other extreme, enable a fresh engineer to play with infrastructure willy-nilly. On day one of employment, a newcomer should be able to clone the codebase and follow a bunch of instructions to spin up, modify, destroy production-like infrastructure without any supervision whatsoever.</li>
</ul>
<p>Any infrastructure automation system must limit the &#34;blast radius&#34; of changes. The longer a thing lives, the more the odds that <em>something</em> is going to go awry. At the same time, one must make experimentation easy and safe, to ease change management, and to onboard new people through tons of hands-on practice.</p>
<p>If a system has such a capability, it signals to me that it has solved for strategic risk management <em>as well as</em> daily-driver operator safety.</p>
<h2 id="change-service-infrastructure-to-suit-the-management-model">Change service infrastructure to suit the management model</h2>
<p>A key change was to migrate application runtime to a &#34;serverless&#34; model, because this gels better with the convenient fiction of &#34;Immutable&#34; infrastructure.</p>
<p>The company had already chosen to switch to managed databases, and had previously adopted containers. However, we operated our own EC2 VMs and/or our own Docker Swarms. We adopted <a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> AWS Fargate , and it worked out quite well for us.</p>
<p>So if there is a general design principle for Terraformers, it would be <em>&#34;Subtract explicit management of mutable infrastructure (e.g. pool of EC2 VMs).&#34;</em>. Managed infrastructure bills can rack up fast, so use some good ol&#39; napkin arithmetic to make a sound tradeoff (management complexity <code>X</code> operating expense <code>X</code> degree of control and observability needed <code>X</code> vendor lock-in).</p>

<p>There are many frameworks and tools out there to help operators create and maintain their terraform code, as well as to apply it safely against live services.</p>
<p>However, I felt Terraform itself was a huge variable. Third-party tools and frameworks impose their own bespoke ideas and abstractions. Outsourcing critical thinking (<em>why</em> to do <em>what</em>?) to industrial automation (frameworks) is quick and convenient in the short term, but risky in the long run. I wanted to ensure we acquired enough first-party operational knowledge about Terraform, <em>specific to our unique constraints</em>, before locking ourselves into some third-party approach.</p>
<p>This paid off because, as I progressively discovered what worked for us, it became clearer that none of the extant third party tools / frameworks / guides would have served us in the long run. Further, because we needed to cater to only our specific workflow automation needs, only in AWS, we could tool up with just a handful of tiny shell scripts.</p>
<h2 id="find-a-core-unit-of-abstraction-and-define-it-clearly">Find a core unit of abstraction and define it clearly</h2>
<ul>
<li><strong>We &#34;Layer&#34; infrastructure:</strong> On-disk, a &#34;Layer&#34; is just a directory containing Terraform files, and crucially, has its own state file. The files follow a certain naming convention to help separate functionally different pieces for ease of maintenance. A <em>Layer</em> includes a notion of &#39;main&#39; code by convention, which can be hand-rolled Terraform, or a third party &#34;Module&#34;.</li>
<li><strong>A &#34;Layer&#34; is a stateful object:</strong> Each &#34;Layer&#34; defines some piece infrastructure (say, a VPC), or a &#34;service&#34; (cluster of apps, or data store), or a prerequisite (e.g. IAM). The art lies in choosing what to put in a Layer, because it is a unit of composition and a unit of change management of some logical unit of one&#39;s live runtime. Layers can look different for different systems.</li>
<li><strong>A &#34;Layer&#34; is NOT a &#34;Module&#34;:</strong> To reiterate, in this architecture, <strong>a Layer is a <u>stateful object</u></strong>. It is <strong><u>not</u> to be confused with a &#34;Module&#34;</strong>, which is also just a directory containing Terraform files. However, a Module is supposed to be a unit of HCL code reusability, and so, it <u>must not</u> be associated with any state. i.e. a &#34;Module&#34; must be function-like.</li>
</ul>

<p>Each business unit is a silo by necessity. It owns and operates its own infrastructure and services. Each unit needs total control over its secrets, its specific system architecture, its pace of development, and crucially who can effect changes in production.</p>
<p><em>However</em> a small company of this type can&#39;t afford to — in fact, it simply cannot — have each unit build their automation from the ground up. Thus, each business unit <em>also</em> needs to use and share the meta-stuff; tooling, techniques, workflows, operator experience.</p>
<h2 id="think-like-terraform-when-laying-out-code">Think like Terraform when laying out code</h2>
<p>Terraform&#39;s programming and state model firmly dictated directory layout, entity naming convention, coding convention, etc. All of this is version controlled under git, of course, which in turn allows us to safely run tooling that modifies code en-mass (e.g. recreate auto-generated files as needed).</p>
<h2 id="use-a-mono-repo-for-great-good">Use a mono-repo for great good</h2>
<p>A single repository can contain a directory for each department. It&#39;s always possible to split out each department into its own repo later, if some hard constraint dictates it be so. Absent that, it&#39;s far better to put it all in one repo. Doing this well is invaluable in helping departments <a href="#share-practices-protect-secrets">share practices while protecting secrets</a>.</p>
<pre><code># Each &#34;department&#34; runs / owns its own infrastructure,
# but benefit from shared tooling.
project-repo-root
|
|  # shared tooling and scripts
|_ tools/*.sh
|
|  # Each &#34;department&#34; owns and maintains its own infra.
|  # in a dedicated directory.
|_ department-A
|_ department-B
|_ department-C
   |
   |  # Global Vars must be common to two or more &#34;layers&#34;,
   |  # e.g. CIDR blocks, security groups, load balancers etc.
   |_ global-vars/{test,dev,prod}.tfvars
   |
   |_ vpc-layer
   |
   |_ iam-layer
   |
   |_ common-load-balancer-layer
   |
   |_ alpha-app-servers-layer # uses common load balancer
   |
   |_ delta-app-servers-layer # uses common load balancer
   |
   |_ job-runners-layer # does not need load balancer
   |
   |_ search-service-layer # has own dedicated load balancer
   |
   |_ ... # more layers
   |
   |_ ... # more layers
   |
   |_ foobar-layer
      |  # variable declarations auto-generated using
      |  # tfvars declared in department-wide global-vars
      |- input-global.tf
      |
      |  # variables declared and maintained manually
      |  # by THIS layer&#39;s department / owner
      |- input-local.tf
      |
      |  # terraform version etc. must be hard-coded
      |  # because of HCL&#39;s bootstrap problem
      |- main-hardcoded.tf
      |
      |  # &#34;main&#34; has ALL the resources for the layer.
      |  # Everything is parameterised based on env name.
      |- main.tf
      |
      |  # Export state to layer N+1 that uses THIS layer
      |- output.tf
      |
      |  # Per-environment variables and input-local overrides,
      |  # PLUS mandatory state isolation for each environment.
      |- env-test.tfvars
      |- env-test-s3-backend.conf
      |
      |- env-stage.tfvars
      |- env-stage-s3-backend.conf
      |
      |- env-prod.tfvars
      |- env-prod-s3-backend.conf
</code></pre>
<p>This is a classic monorepo structure, albeit with the following rationale / utility:</p>
<ul>
<li><strong>At the top level</strong>, each department&#39;s code is namespaced in a directory named after it. All shared workflows/scripts go into a top-level directory &#34;tools&#34; directory too.</li>
<li><strong>Inside a &#34;department&#34;</strong>, the directory-per-Layer structure is precisely because Terraform treats all files in a directory as one thing.</li>
<li><strong>Inside each layer</strong>, files follow a definite naming convention to tell maintainers where to look for what. e.g. &#34;input-*.tf&#34; and &#34;output.tf&#34; files are interface boundaries between a layer and the &#34;global&#34; parts of the system, and also between Layers. Some boilerplate HCL code is also separated from a maintenance point of view. E.g. main-hardcoded.tf can simply be overwritten using a tooling script, if we need to bump Terraform&#39;s version. &#34;input-global.tf&#34; can be regenerated for all layers, whenever global-vars change.</li>
<li><strong>Within each .tf file</strong>, <u>every</u> configuration parameter is written as a variable. There are no hard-coded values whatsoever.</li>
<li><strong>Each Layer has environment-specific configurations</strong>: The environment-specific .tfvars files control the value of variables that are environment specific. e.g. The number of VMs in a cluster, or the configuration of a load balancer, or a database. The vars files can optionally be encrypted on disk by department / team.</li>
<li><strong>Unified command-line interface</strong>: And last but not least, given a good directory name-spacing and file naming convention, one can design a standard, unified command-line workflow to help operators of all departments safely run all the tooling, be it for maintenance, update, or plan/apply. Pragmatic tooling serves as living documentation of operations practices, and multiplies benefits across departments.</li>
</ul>

<p>Big systems are made of little systems. The overall system (design convention + code layout + tooling + ops workflow) helped addresses several problems operators face.</p>
<p>Over the years, I&#39;ve learned some new terms and concepts that helped me reflect on what I was doing and frame it in ways I can explain to others. This section outlines various levels of abstraction to think at, as a designer of an infra-as-code <em>system</em>.</p>
<p>Though most design choices were intentional, not everything was clear and organised a-priori in my head. Much was synthesised while in the thick of things, wrought of common sense, trial and error, a bunch of luck, and prior operator experience. A gray beard helps, I am told :)</p>
<h2 id="think-brownfield-first">Think brownfield-first</h2>
<ul>
<li>Design for graded migrations.</li>
<li>Expect multiple tactical lift-and-shift operations. Use these as learning / improvement opportunities.</li>
<li><em>Discover</em> the right IaC architecture and abstractions.</li>
<li>Find a balance between services, procedures, tooling and operational complexity. e.g. your team already has infra, but wants to adopt terraform (prettymuch every team in the world)
<ul>
<li>don&#39;t bother with VPC/IAM/crazy-critical stuff, start replacing replacable services (e.g. things that can be blue/green deployed)</li>
<li>meanwhile each non-migrated layer can simply be a stub, importing state from live env, and exporting only relevant state via output.tf</li>
<li>then slowly terraform in rising order of mission-criticality / global side effects (e.g. borking IAM can blow up everything)</li>
</ul></li>
</ul>
<h2 id="create-environment-for-elite-devops-culture">Create environment for elite devops culture</h2>
<p><strong>Be hardcore about &#34;full-systems&#34; IaC</strong></p>
<ul>
<li>No more button-clicking in AWS console</li>
<li>Fine-grained IAM and security controls</li>
<li>Centralized Logging and Monitoring</li>
<li>CI/CD pipelines and push-button deploys</li>
<li>Auto-scaling</li>
</ul>
<p><strong>Promote Safety Culture. Security derives from this.</strong></p>
<ul>
<li>Psychological Safety</li>
<li>Operator Safety</li>
<li>System Safety</li>
</ul>
<p><strong>Deliver vastly better capabilities than extant un-system.</strong></p>
<ul>
<li>Zero-downtime live infrastructure switching</li>
<li>Composability</li>
<li>System Invariants</li>
<li>Dev/stage/prod parity of <em>architecture</em> and subsystem relationships</li>
</ul>
<p><strong>Promote tool ownership.</strong></p>
<ul>
<li>Learn/teach trade-offs imposed by Terrafrom&#39;s model.</li>
<li>Embrace limitations and work around them.</li>
</ul>
<p><strong>Craft procedures and tools for safe incremental changes.</strong></p>
<p>Incremental change is routine. Errors happen when routine tasks are poorly understood. Every operator should be trained in routine procedures, but nobody should have to operate relying solely on their own memory. e.g. In the system I arrived at, introducing new common vars meant running through a checklist like this:</p>
<ul>
<li>Manually add var(s) to the global environment file(s) (<code>tfvars</code> files located via standard file and directory naming convention).</li>
<li>Use a script to auto-generate input-global.tf for all layers.</li>
<li>Check git diff to verify it&#39;s OK.</li>
<li>Commit.</li>
<li>Plan/Apply, and
<ul>
<li>If success, <em>immediately</em> push commit to remote</li>
<li>If failure, recover from failure, commit a fix</li>
<li>GOTO Plan/Apply</li>
</ul></li>
</ul>
<p>Ditto for migrating main-hardcoded.tf (e.g. doing HCL version upgrades).</p>
<p>The acid test for a rock-solid routine procedure is: can any operator run it flawlessly under live fire?</p>
<h2 id="figure-out-decision-making-at-various-levels-of-abstraction">Figure out decision-making at various levels of abstraction</h2>
<p><strong>Decisions about separation of concerns based on &#34;unit-ness&#34; of a thing:</strong></p>
<p>Examples:</p>
<ul>
<li>If multiple services are to share a load-balancer, then the load-balancer must be its own layer.</li>
<li>If a service has its own load balancer, the the &#34;layer&#34; is service+load-balancer.</li>
<li>Each datastore gets its own layer, and if a store is multi-tenant, it may make sense to further isolate each tenant to its own layer</li>
</ul>
<p><strong>Decisions about separation of layers based on rate of change:</strong></p>
<p>Examples:</p>
<ul>
<li>A VPC once defined almost never changes, and it is shared by everything, so it goes in its own layer</li>
<li>IAM are added/updated (rarely removed), and shared, so they go into an IAM layer. Also good for IAM visibility.</li>
<li>Security groups are more frequently changed, and are shared, so they go into a single layer</li>
</ul>
<p><strong>Decisions about separation of environments based on access level of operator:</strong></p>
<p>Example: allow anybody in the ops team to CRUD the &#39;test&#39; environment. These are by definition throwaway, and encourage experimentation — &#34;fearless devops&#34;.</p>
<ul>
<li>enforce access to s3 state — newbies don&#39;t get access to &#34;staging&#34; and &#34;prod&#34; state — even terraform plan will fail</li>
<li>some staff get access to &#34;staging&#34;, once they become familiar with team ops</li>
<li>no individual user gets IAM on &#34;prod&#34; state; this is a controlled user, probably on a jump box or some such place, accessible only via a change management process, and where durable audit logs note who used it when, and why</li>
</ul>
<h2 id="factor-in-department-specific-change-management">Factor in department-specific change management</h2>
<p>e.g. create some explicit order of layers and perform plan/apply operations only in that order (and destroy in exactly the reverse order)</p>
<ul>
<li>I simply committed a file with a whitelist of layer names — a util script would run terraform commands only in that order</li>
<li>this little trick also let me add / prototype a new layer, without messing with the existing ones … the new layer would make it to the whitelist, after it was OK in test/stage</li>
</ul>
<h2 id="choose-a-sexy-code-name-for-branding-reasons">Choose a sexy code name for branding reasons</h2>
<p>I called the project <em>&#34;Skywalker&#34;</em>.</p>
<p>Okay, that may too pun-y to be sexy. But it did make people chuckle. And I could joke around that we couldn&#39;t possibly fail because <em>The Source</em> was with us :)</p>

<p>Implementation details of the concrete solution and tooling I developed must have changed a lot. Though I hope the core design and architecture hasn&#39;t (i.e. it has been a continued success).</p>
<h2 id="the-smell-of-success-is-wonderful">The smell of success is wonderful</h2>
<p>Anyone who has done a project with <em>lots</em> of moving parts and a nebulous end state knows that <em>so many things</em> can go wrong at any time. What does &#34;done&#34; even mean? Will the gnawing self-doubt leave already, please? So imagine my utter disbelief, when it not only worked, but it worked <em>well</em>!</p>
<ul>
<li>The project fulfilled the design intention of generating a reference architecture that the whole company could adopt. Also tools, checklists, runbooks, and procedures to play with it in our AWS test account.</li>
<li>Various teams were able to start adopting it immediately to create and operate their own bespoke infrastructure for greenfield services. They gained confidence that they could port over their legacy systems at their pace.</li>
<li>The whole thing proved itself in production (design, implementation, tooling, checklists). I ported and live-migrated the company&#39;s most mission-critical system. Then we ran the spanking new system without hitch for several months. And then, after I had moved on from the company into a sabbatical <a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> other operators entirely deprecated the old system without having to call me back for assistance.</li>
</ul>
<h2 id="computers-and-software-are-amazing">Computers and software are amazing</h2>
<p>Our tools and clouds are extremely leveraged. When used with even a bit of care and forethought, a single person can get a <em>lot</em> done. This, I knew from experience <a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<p>Even so, it took literal first-hand experience for the insight to sink in <em>viscerally</em>… the power available to a single person willing to plough through several hundred pages of manuals, incur hair-loss due to insane APIs, and doggedly try failed experiment after failed experiment. Until it all just… works!.</p>
<p>That person was I, and that guy still can&#39;t believe he ended up pulling off this whole terraforming exercise almost entirely by himself (like I mentioned, very small team!). This work would have required a whole department in the early 2000s.</p>
<h2 id="none-of-it-is-possible-without-supportive-bosses-and-peers">None of it is possible without supportive bosses and peers</h2>
<p>Thanks! You know who you are!</p>


  </section>
  
</article>
  </main>
      
      <!-- Cloudflare Web Analytics -->
      
      <!-- End Cloudflare Web Analytics -->
    </div></div>
  </body>
</html>
