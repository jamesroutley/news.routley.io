<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.zach.be/p/yc-is-wrong-about-llms-for-chip-design">Original</a>
    <h1>YC is wrong about LLMs for chip design</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div><div><div dir="auto"><p><span>YC recently released their latest </span><a href="https://www.ycombinator.com/rfs-build" rel="">Request for Startups</a><span>, and one specific item has already been getting some buzz in chip design communities. It’s exciting that an organization as high-profile as YC is taking chip design more seriously… or at least it’s exciting until you actually start reading what their proposal is. Unfortunately, YC seems to have fundamentally misunderstood the key challenges in the world of chip design, and where both LLMs and AI tools more broadly can help.</span></p><p>YC makes the following argument:</p><blockquote><p>Design of customized digital systems [...] has typically been costly because of the amount of custom design, development and testing necessary to bring such a system online. With the advent of large language models, these costs are coming down significantly, such that ever more specialized types of computation could be done.</p><p>We know there is a clear engineering trade-off: it is possible to optimize especially specialized algorithms or calculations such as cryptocurrency mining, data compression, or special-purpose encryption tasks such that the same computation would happen faster (5x to 100x), and using less energy (10x to 100x).</p></blockquote><p><span>If Gary Tan and YC believe that LLMs will be able to design chips 100x better than humans currently can, they’re significantly underestimating the difficulty of chip design, and the expertise of chip designers. While LLMs are </span><a href="https://arxiv.org/abs/2407.10424v4" rel="">capable</a><span> of writing </span><a href="https://arxiv.org/abs/2309.07544" rel="">functional Verilog</a><span> </span><em>sometimes</em><span>, their performance is still subhuman. More importantly, LLMs aren’t capable of designing novel chip architectures, which is the primary driver of performance improvements for modern accelerator chips. LLMs primarily pump out mediocre Verilog code.</span></p><p><span>The more charitable version of YC’s proposal is that LLM-based tools will significantly reduce the </span><em>cost</em><span> of chip design, which will make hardware acceleration more viable for applications where it’s currently cost-prohibitive. Unfortunately, the semiconductor industry already tried something similar. It was called high-level synthesis, and it resoundingly failed.</span></p><p><span>High-level synthesis, or HLS, was born in 1998, when </span><a href="https://en.wikipedia.org/wiki/Forte_Design_Systems" rel="">Forte Design Systems</a><span> was founded. They developed a tool called Cynthesizer, which could automatically translate SystemC to Verilog given certain timing constraints. More generally, HLS tools let engineers write code in a higher-level language like C, C++, or Scala, and automatically convert that code to Verilog, which can in turn be synthesized into logic gates for ASICs and FPGAs. The idea is to make silicon development cheaper and more accessible, especially for teams that don’t have access to deep Verilog expertise.</span></p><p><span>Despite Cynthesizer’s cool name, it never really caught on. Forte was still </span><a href="https://www.eetimes.com/the-future-is-high-level-synthesis/?_ga" rel="">desperately trying to argue</a><span> that HLS was the future in 2011. In 2014, they were acquired by Cadence, and Cynthesizer was rolled into Cadence’s new HLS tool, Stratus.</span></p><p><span>Xilinx, now AMD, has been a major proponent of HLS, specifically targeting FPGA acceleration. This makes some sense, as FPGA development is usually done on smaller teams where the additional firepower HLS promises could be super useful. Their </span><a href="https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vitis/vitis-hls.html" rel="">Vitis</a><span> tool is one of the best HLS tools in the industry -- unfortunately, that’s a bit of a low bar.</span></p><p><span>The current state of HLS is best summed up by Lan Huang in the </span><a href="https://link.springer.com/article/10.1007/s11390-020-9414-8" rel="">introduction to his survey of the current state-of-the-art in HLS tools</a><span>:</span></p><blockquote><p>The performance of HLS tools still has limitations. For example, designers remain exposed to various aspects of hardware design, development cycles are still time consuming, and the quality of results (QoR) of HLS tools is far behind that of RTL flows.</p></blockquote><p><span>Other researchers have reported </span><a href="https://trepo.tuni.fi/bitstream/handle/10024/126643/Are_We_There_Yet.pdf?sequence=1" rel="">similar disappointing results</a><span> in their own surveys. </span></p><p>Ultimately, while HLS makes designers more productive, it reduces the performance of the designs they make. And if you’re designing high-value chips in a crowded market, like AI accelerators, performance is one of the major metrics you’re expected to compete on. So it makes sense to spend the additional upfront cost to hire talented RTL designers, build chips with better performance, and deliver more successful products.</p><p>So HLS tools failed to gain any traction for high-value, high-volume chips, where performance and efficiency requirements necessitated human engineers writing high-quality Verilog. LLMs, which also produce poor-quality Verilog, will likely face the same challenges But what about other kinds of chips? Could LLMs help there?</p><p>The few successful applications of HLS enabled engineers without silicon expertise to leverage hardware acceleration. For example, HLS has seen success in FPGA acceleration of genomics workloads and CFD workloads.</p><p>In both cases, HLS is valuable because the engineers developing the hardware accelerators don’t have the RTL experience to build optimized RTL-level designs from scratch. But this is only the case because the markets for hardware-accelerated genomics and CFD are relatively small. GPUs already serve these use cases somewhat well, and neither use-case would drive even a fraction of the sales volume of AI chips or cryptographic accelerators. If these markets were bigger, it would make economic sense to dedicate talented hardware engineers to build optimized silicon.</p><p><span>The idea that YC is proposing falls into the same trap. LLMs can reduce the cost to develop silicon. This might make dedicated ASICs for applications like genomics economically viable, when they previously weren’t. But they weren’t viable for a reason: there isn’t a huge market there. Ultimately trying to leverage LLMs to build hardware accelerators for underserved applications is a loser’s bet.</span><span> If an application doesn’t warrant hardware acceleration yet, it’s probably because it’s a small market, and that makes it a poor target for a startup.</span></p><p>If LLM-based chip design primarily unlocks low-value markets, why are so many startups and established EDA companies trying to leverage LLMs? Well, it turns out that LLMs are also pretty valuable when it comes to chips for lucrative markets -- but they won’t be doing most of the design work. LLM copilots for Verilog are, at best, mediocre. But leveraging an LLM to write small snippets of simple code can still save engineers time, and ultimately save their employers money.</p><p>More importantly, though, the chip design world is experiencing a massive talent shortage in verification. Normally, you want two verification engineers for every designer, because silicon bugs are incredibly costly. But in the modern era, good verification engineers are hard to come by.</p><p><span>If you could leverage LLMs to make verification faster, easier, or more effective, that could be massively valuable to established semiconductor companies and semiconductor startups alike. But actually ensuring that LLMs can actually understand and reason about chip designs specifications is no small task, which is why I’m skeptical of startups like </span><a href="https://www.bronco.ai/" rel="">Bronco</a><span> or </span><a href="https://www.getinstachip.com/" rel="">Instachip</a><span> who are naively throwing fine-tuned LLMs at chip verification.</span></p><p><span>First of all, chip specifications are often complex, incomplete, labyrinthine monstrosities -- they were written by people like me, after all. But more importantly, successful verification of a chip requires an LLM to understand the expected internal state of the chip. In essence, the language needs to have some formal model of the chip: think </span><a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/" rel="">AlphaProof</a><span>, but for semiconductor verification instead of math. This is one of the things we’re working on at </span><a href="https://www.normalcomputing.com/" rel="">Normal Computing</a><span>, alongside our novel thermodynamic chip architectures.</span></p><p>Ultimately, LLMs will make chip design cheaper. But this will primarily benefit three kinds of companies: large semiconductor companies who can reduce their verification workforce, conventional chip startups who can operate leaner teams, and then the EDA software startups selling LLM-based tools. LLMs won’t build 100x better chips, or enable hardware startups to tackle markets lacking hardware acceleration, because the economics just don’t make sense.</p></div></div></div></article></div></div></div>
  </body>
</html>
