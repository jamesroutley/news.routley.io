<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developer.apple.com/videos/play/wwdc2023/10050/?time=590">Original</a>
    <h1>Bfloat16 support coming to Apple&#39;s Metal and PyTorch [video]</h1>
    
    <div id="readability-page-1" class="page"><div data-supplement-id="transcript" data-shortcut-base-url="/videos/play/wwdc2023/10050/">
							<section>
								<p><span id="get-transcript">Download</span></p>
								<p><span><span data-start="0.0">♪ ♪ </span></span><span><span data-start="10.0">Denis: Hi, my name is Denis Vieriu, and I&#39;m a software engineer in the GPU, </span></span><span><span data-start="15.0">Graphics, and Display Software group at Apple. </span></span><span><span data-start="18.0">Today I will present to you all the new features and enhancements </span></span><span><span data-start="21.0">introduced to machine learning this year in Metal. </span></span><span><span data-start="25.0">I&#39;ll first recap the existing machine learning backends. </span></span><span><span data-start="28.0">The Metal machine learning APIs are exposed </span></span><span><span data-start="31.0">through Metal Performance Shaders framework. </span></span><span><span data-start="34.0">MPS is a collection of high-performance GPU primitives for various fields, </span></span><span><span data-start="38.0">like image processing, linear algebra, and machine learning. </span></span><span><span data-start="43.0">MPSGraph is a general purpose compute graph, </span></span><span><span data-start="46.0">which sits on top of the MPS framework </span></span><span><span data-start="49.0">and extends support to multi-dimensional tensors. </span></span><span><span data-start="53.0">machine learning inference frameworks, like CoreML, </span></span><span><span data-start="56.0">build on top of the MPSGraph backend. </span></span><span><span data-start="59.0">MPSGraph also supports training frameworks, </span></span><span><span data-start="62.0">like TensorFlow and PyTorch. </span></span><span><span data-start="65.0">To learn more about MPSGraph </span></span><span><span data-start="67.0">and ML Frameworks, please refer </span></span><span><span data-start="69.0">to the previous Metal WWDC talks listed here.</span></span></p><p><span><span data-start="74.0">This session focuses on the updates and enhancements added to PyTorch </span></span><span><span data-start="78.0">and TensorFlow Metal backends, the new GPU acceleration for JAX, </span></span><span><span data-start="83.0">and the features added this year to MPSGraph for ML Inference.</span></span></p><p><span><span data-start="89.0">PyTorch and TensorFlow Metal acceleration enable you to use </span></span><span><span data-start="92.0">the highly efficient kernels from MPS to get the best performance on your Mac. </span></span><span><span data-start="96.0">PyTorch Metal acceleration has been available since version 1.12 </span></span><span><span data-start="101.0">through the MPS backend. </span></span><span><span data-start="102.0">This was introduced last year into the PyTorch ecosystem, </span></span><span><span data-start="106.0">and since then, multiple improvements have been made </span></span><span><span data-start="109.0">for optimizing memory usage and view tensors. </span></span><span><span data-start="112.0">This year, PyTorch 2.0 MPS Backend made a great leap forward </span></span><span><span data-start="117.0">and has been qualified for the Beta Stage. </span></span><span><span data-start="120.0">But these were not all the improvements. </span></span><span><span data-start="123.0">Latest PyTorch builds contain lots of new updates, </span></span><span><span data-start="126.0">such as MPS operation profiling, custom kernel, </span></span><span><span data-start="130.0">and Automatic Mixed precision support. </span></span><span><span data-start="132.0">Before covering all the nightly build features, </span></span><span><span data-start="135.0">I&#39;ll start with what is new in PyTorch 2.0.</span></span></p><p><span><span data-start="140.0">There is support for the top 60 most used Torch operators, </span></span><span><span data-start="144.0">including ops such as grid sampler, triangular solve, topk, and many more.</span></span></p><p><span><span data-start="152.0">The testing coverage has greatly improved. </span></span><span><span data-start="155.0">This includes tests for most of the Torch operators, </span></span><span><span data-start="158.0">gradient testing, and ModuleInfo based testing.</span></span></p><p><span><span data-start="163.0">Since release, the network coverage has expanded </span></span><span><span data-start="166.0">as multiple popular models adopted MPS as their official backend on macOS. </span></span><span><span data-start="171.0">This includes foundation models, such as WhisperAI, </span></span><span><span data-start="175.0">object detection models such as YOLO, stable diffusion models, and many more. </span></span><span><span data-start="181.0">Let&#39;s check one of these models in action using latest PyTorch 2.0. </span></span><span><span data-start="186.0">For this example, I am using YoloV5, </span></span><span><span data-start="189.0">an object detection network running on an M2 Max. </span></span><span><span data-start="193.0">On the left side, I have the network running </span></span><span><span data-start="196.0">and generating live images using the PyTorch MPS backend, </span></span><span><span data-start="199.0">while on the right, I have the exact same model, </span></span><span><span data-start="202.0">but running on the CPU. </span></span><span><span data-start="204.0">The left side, using the MPS backend, is running at noticeably higher framerate.</span></span></p><p><span><span data-start="211.0">And furthermore, the developers not only adopted </span></span><span><span data-start="214.0">the PyTorch MPS backend in their external networks, </span></span><span><span data-start="217.0">but also have contributed code for multiple new operators, </span></span><span><span data-start="220.0">including histogram, group_norm, signbit, and more.</span></span></p><p><span><span data-start="227.0">Next, I&#39;ll cover the new features </span></span><span><span data-start="229.0">available in the latest PyTorch builds, </span></span><span><span data-start="231.0">starting with profiling support </span></span><span><span data-start="233.0">for MPS operations. </span></span><span><span data-start="235.0">PyTorch nightly builds have profiling support </span></span><span><span data-start="238.0">that uses OS signposts to show the exact running time </span></span><span><span data-start="241.0">for operation executions, copies between CPU and GPU, </span></span><span><span data-start="245.0">and fallbacks to the CPU caused by unsupported operators. </span></span><span><span data-start="249.0">You will be able to visualize the profiling data in a very familiar tool, </span></span><span><span data-start="253.0">Metal System Trace, which is part of Instruments. </span></span><span><span data-start="257.0">To learn more about profiling ML applications using Metal System Trace, </span></span><span><span data-start="261.0">I recommend you watch the session from last year, </span></span><span><span data-start="264.0">&#34;Accelerate machine learning with Metal.&#34; </span></span><span><span data-start="267.0">Using the profiler is a very simple process. </span></span><span><span data-start="271.0">Call the start method on the MPS profiler package to enable tracing, </span></span><span><span data-start="275.0">and, at the end of your script, use the stop method to end profiling. </span></span><span><span data-start="280.0">Now I will walk through the profiler to debug an example.</span></span></p><p><span><span data-start="285.0">This sample network uses a Sequential model composed </span></span><span><span data-start="288.0">of linear transformations and Softshrink activation functions </span></span><span><span data-start="292.0">with a total of seven layers in the model.</span></span></p><p><span><span data-start="296.0">The current performance of this model is not satisfying. </span></span><span><span data-start="300.0">In this case, the profiler can be used </span></span><span><span data-start="302.0">to find the bottleneck.</span></span></p><p><span><span data-start="305.0">In the Metal System Trace, first, make sure to enable the os_signpost. </span></span><span><span data-start="310.0">This will allow you to capture the PyTorch operator information. </span></span><span><span data-start="314.0">Next, check that the device and the right executable are set, </span></span><span><span data-start="319.0">in this case, the Python binary. </span></span><span><span data-start="321.0">Then click on the record button.</span></span></p><p><span><span data-start="324.0">Instruments is now recording the PyTorch execution. </span></span><span><span data-start="328.0">I&#39;ll let it run for couple of seconds to make sure I capture enough data. </span></span><span><span data-start="332.0">Then I click Stop.</span></span></p><p><span><span data-start="336.0">In the os_signpost tab, </span></span><span><span data-start="338.0">disclose the PyTorch Intervals timeline.</span></span></p><p><span><span data-start="342.0">This timeline displays the execution time of an operator, </span></span><span><span data-start="345.0">alongside PyTorch Metadata, such as string identifiers, </span></span><span><span data-start="349.0">data types, and copy lengths.</span></span></p><p><span><span data-start="353.0">Zooming into the timeline reveals PyTorch operators used by this example. </span></span><span><span data-start="358.0">The pattern from this trace can be easily identified </span></span><span><span data-start="361.0">to the custom Sequential model made of seven layers.</span></span></p><p><span><span data-start="365.0">From the trace, it&#39;s clear that the bottleneck </span></span><span><span data-start="368.0">is in the Softshrink fallback to the CPU. </span></span><span><span data-start="371.0">This process is very inefficient. </span></span><span><span data-start="373.0">The model incurs the overhead from the CPU execution </span></span><span><span data-start="377.0">of the Softshrink operator and the additional copies, </span></span><span><span data-start="380.0">while the GPU is starved. </span></span><span><span data-start="382.0">Most of the gaps in the GPU timeline are coming </span></span><span><span data-start="385.0">from the Softshrink activation function falling back to CPU. </span></span><span><span data-start="389.0">In order to fix this, I&#39;ll write a custom kernel to improve the performance. </span></span><span><span data-start="394.0">There are four steps to write a custom operation. </span></span><span><span data-start="397.0">First, implement the operation in Objective-C and Metal. </span></span><span><span data-start="402.0">Next, create the Python bindings for your Objective-C code </span></span><span><span data-start="406.0">and compile your extension. </span></span><span><span data-start="408.0">Finally, once your extension is built, </span></span><span><span data-start="411.0">import the operation into your training script and begin using it. </span></span><span><span data-start="415.0">I&#39;ll start with the operation implementation.</span></span></p><p><span><span data-start="420.0">Start by importing the Torch extension header. </span></span><span><span data-start="423.0">This includes all the necessary PyTorch bits to write C++ extensions.</span></span></p><p><span><span data-start="429.0">Then define the compute function </span></span><span><span data-start="433.0">and use the get_command_buffer MPS backend API to get a reference </span></span><span><span data-start="437.0">to the MPSStream Command Buffer. </span></span><span><span data-start="440.0">Similarly, use the get_dispatch_queue API to get a reference to the serial queue.</span></span></p><p><span><span data-start="446.0">Next, create an encoder using the command buffer and define the custom GPU kernel.</span></span></p><p><span><span data-start="453.0">You encode the kernel inside the dispatch queue </span></span><span><span data-start="456.0">to ensure that submissions from multiple threads are serialized.</span></span></p><p><span><span data-start="461.0">After all the work is encoded, use the synchronize API to wait </span></span><span><span data-start="465.0">until the current command buffer is done running, </span></span><span><span data-start="467.0">so you can observe serialized submissions. </span></span><span><span data-start="470.0">Or if you don&#39;t need serialization, use the commit API.</span></span></p><p><span><span data-start="475.0">Next, bind your custom functions. </span></span><span><span data-start="478.0">You can use PYBIND11 to bind the Objective-C functions </span></span><span><span data-start="482.0">into Python in a very simple manner. </span></span><span><span data-start="484.0">For this extension, the necessary binding code spans only two lines.</span></span></p><p><span><span data-start="490.0">After binding, compile your extension. </span></span><span><span data-start="494.0">First import torch.utils.cpp_extension.</span></span></p><p><span><span data-start="498.0">This provides a load function which you can use to compile your extension. </span></span><span><span data-start="503.0">Next, pass the name of your extension to build, </span></span><span><span data-start="506.0">then a list of relative or absolute paths to the source code files. </span></span><span><span data-start="511.0">Optionally, you can list additional compiler flags to forward to the build. </span></span><span><span data-start="517.0">The load function will compile the source files into a shared library, </span></span><span><span data-start="521.0">which will be subsequently loaded into the current Python process as a module.</span></span></p><p><span><span data-start="526.0">Finally, import the operator into your script to begin using it.</span></span></p><p><span><span data-start="532.0">Start by importing the compiled library and change the previous Sequential model </span></span><span><span data-start="536.0">to use the custom Softshrink kernel.</span></span></p><p><span><span data-start="541.0">Let&#39;s run the same model again and check the result.</span></span></p><p><span><span data-start="546.0">With the newly added custom operator, the model runs much more efficiently.</span></span></p><p><span><span data-start="552.0">All the copies and intermediate tensors created by the fallback </span></span><span><span data-start="555.0">to the CPU are gone, and the Sequential model runs much faster. </span></span><span><span data-start="560.0">Now let&#39;s explore more ways your network can be further improved.</span></span></p><p><span><span data-start="566.0">PyTorch MPS backend now supports automatic mixed precision, </span></span><span><span data-start="570.0">which allows you to train faster using less memory </span></span><span><span data-start="573.0">and without loss in quality. </span></span><span><span data-start="575.0">To understand mixed precision, </span></span><span><span data-start="577.0">I will first review the supported data types. </span></span><span><span data-start="580.0">Mixed precision training is a mode that allows training </span></span><span><span data-start="583.0">deep learning models with a mix of single precision floating point </span></span><span><span data-start="587.0">and half precision floating point. </span></span><span><span data-start="590.0">Starting with macOS Sonoma, </span></span><span><span data-start="592.0">MPSGraph adds support for a new data type, bfloat16.</span></span></p><p><span><span data-start="597.0">bfloat16 is a 16-bit floating point format for deep learning. </span></span><span><span data-start="602.0">It is comprised of 1 sign bit, 8 exponent bits, </span></span><span><span data-start="606.0">and 7 mantissa bits. </span></span><span><span data-start="608.0">This is different from the standard IEEE 16-bit floating point format, </span></span><span><span data-start="613.0">which was not designed with deep learning applications in mind. </span></span><span><span data-start="616.0">Automatic Mixed Precision will be enabled for both float16 and bfloat16.</span></span></p><p><span><span data-start="623.0">Automatic mixed precision chooses the right precision per layer </span></span><span><span data-start="627.0">by measuring the performance of the network in default precision, </span></span><span><span data-start="631.0">then it runs again, with mixed precision settings </span></span><span><span data-start="634.0">to optimize the performance without impacting the accuracy. </span></span><span><span data-start="638.0">Some layers of the neural networks can be executed at lower precision, </span></span><span><span data-start="642.0">such as convolutional or linear layers. </span></span><span><span data-start="645.0">Other layers such as reductions will often require a higher precision level.</span></span></p><p><span><span data-start="652.0">Adding Automatic Mixed Precision support to your network is a very easy process. </span></span><span><span data-start="657.0">First, add autocast. </span></span><span><span data-start="659.0">Both float16 and bfloat16 are supported. </span></span><span><span data-start="664.0">Autocast serves as a context manager that allows a region of the script </span></span><span><span data-start="669.0">to run in mixed precision.</span></span></p><p><span><span data-start="672.0">In this region, MPS ops run in a data type chosen by autocast </span></span><span><span data-start="676.0">to improve performance while maintaining accuracy.</span></span></p><p><span><span data-start="681.0">The MPS backend has also been significantly optimized. </span></span><span><span data-start="684.0">With PyTorch 2.0 and macOS Sonoma, </span></span><span><span data-start="687.0">the MPS backend is up to five times faster </span></span><span><span data-start="689.0">compared to our previous release. </span></span><span><span data-start="692.0">That&#39;s it for PyTorch. Now let&#39;s move on to TensorFlow. </span></span><span><span data-start="696.0">The TensorFlow Metal backend has matured to a stable 1.0 release version. </span></span><span><span data-start="701.0">In this release, a grappler remapping optimizer pass </span></span><span><span data-start="704.0">has been added to the plugin. </span></span><span><span data-start="707.0">The Metal plugin also gets mixed precision support, </span></span><span><span data-start="710.0">and the installation process is now simpler than before.</span></span></p><p><span><span data-start="715.0">The performance of the TensorFlow Metal backend has been improved </span></span><span><span data-start="719.0">through addition of automatic fusion of recognized computation patterns. </span></span><span><span data-start="724.0">These computations include fused convolutions </span></span><span><span data-start="726.0">and matrix multiplications, optimizer operations, and RNN cells. </span></span><span><span data-start="732.0">This optimization happens automatically through the grappler pass </span></span><span><span data-start="736.0">when the computation graph is created.</span></span></p><p><span><span data-start="740.0">Here I have an example of a common computation </span></span><span><span data-start="742.0">of a two-dimensional convolution operation. </span></span><span><span data-start="745.0">The convolution is often followed by an addition function, </span></span><span><span data-start="749.0">a common pattern in convolutional neural networks. </span></span><span><span data-start="752.0">By identifying this pattern, </span></span><span><span data-start="754.0">the grappler pass can remap the computation.</span></span></p><p><span><span data-start="759.0">This allows you to use a more optimized kernel to achieve the same output, </span></span><span><span data-start="763.0">leading to better performance. </span></span><span><span data-start="765.0">Like in PyTorch, TensorFlow also gets mixed precision support. </span></span><span><span data-start="770.0">TensorFlow allows setting mixed precision globally. </span></span><span><span data-start="773.0">This enables all network layers to be automatically created </span></span><span><span data-start="777.0">with the requested data type policy, so enabling this change </span></span><span><span data-start="780.0">in your standard workflow requires minimal changes to existing code.</span></span></p><p><span><span data-start="786.0">Global policy can be set to use either Float16 or BFloat16.</span></span></p><p><span><span data-start="794.0">In addition to improvements in performance, </span></span><span><span data-start="797.0">the user experience in enabling the Metal acceleration has been streamlined. </span></span><span><span data-start="801.0">From now on, simply following the usual path </span></span><span><span data-start="804.0">of installing the TensorFlow wheel and the TensorFlow-Metal plugin </span></span><span><span data-start="808.0">through a package manager will enable the Metal acceleration. </span></span><span><span data-start="812.0">For those who want to stay on the bleeding edge of TensorFlow development, </span></span><span><span data-start="816.0">the Metal acceleration support is now also available </span></span><span><span data-start="819.0">on the nightly releases of TensorFlow. </span></span><span><span data-start="821.0">Now let&#39;s talk about the new GPU acceleration for JAX. </span></span><span><span data-start="826.0">This year, JAX GPU acceleration will be supported </span></span><span><span data-start="829.0">through the Metal backend, similar to PyTorch and TensorFlow.</span></span></p><p><span><span data-start="833.0">JAX is a Python library for high-performance numerical computing </span></span><span><span data-start="838.0">and machine learning research. </span></span><span><span data-start="840.0">It is based on the popular NumPy framework for working with large arrays, </span></span><span><span data-start="844.0">with three key extensions for machine learning research.</span></span></p><p><span><span data-start="849.0">First, it supports automatic differentiation using the grad function. </span></span><span><span data-start="854.0">It can differentiate through a large subset of Python&#39;s features, </span></span><span><span data-start="857.0">and it can even take high order derivatives. </span></span><span><span data-start="861.0">JAX also supports fast and efficient vectorization. </span></span><span><span data-start="865.0">Given a function apply_matrix, </span></span><span><span data-start="867.0">you could loop over a batch dimension in Python, </span></span><span><span data-start="869.0">but it may run at sub-optimal performance. </span></span><span><span data-start="873.0">In this case, vmap can be used to add batching support automatically.</span></span></p><p><span><span data-start="878.0">And further, JAX lets you just-in-time compile </span></span><span><span data-start="881.0">your function into optimized kernels using an API called jit. </span></span><span><span data-start="885.0">In the same case, jit is used to transform the function </span></span><span><span data-start="889.0">on top of vmap to make it run faster.</span></span></p><p><span><span data-start="893.0">On a MacBook Pro with M2 Max, </span></span><span><span data-start="895.0">JAX Metal acceleration provides amazing speedups, </span></span><span><span data-start="899.0">with an average of ten times faster than the CPU across these networks. </span></span><span><span data-start="903.0">For more details on environment setup and installation of JAX, </span></span><span><span data-start="907.0">please refer to the Metal Developer Resources web page.</span></span></p><p><span><span data-start="912.0">Let&#39;s switch gears and move to ML inference. </span></span><span><span data-start="915.0">I will start by introducing a new serialization format </span></span><span><span data-start="918.0">for MPSGraph that you use to optimize your load times. </span></span><span><span data-start="922.0">This new serialization format can be generated </span></span><span><span data-start="925.0">from your existing serialized networks from other frameworks. </span></span><span><span data-start="928.0">Finally, I will show you how to optimize the memory footprint </span></span><span><span data-start="932.0">of your network by leveraging 8-bit integer quantization. </span></span><span><span data-start="936.0">Let&#39;s begin. </span></span><span><span data-start="938.0">An MPSGraph can be created using the high level APIs with full flexibility, </span></span><span><span data-start="942.0">layer by layer. </span></span><span><span data-start="944.0">Please refer to the video on building customized ML models </span></span><span><span data-start="947.0">with Metal Performance Shaders Graph for details. </span></span><span><span data-start="951.0">After defining and compiling your custom graph, </span></span><span><span data-start="954.0">it will then execute through the MPSGraphExecutable to get results. </span></span><span><span data-start="959.0">Normally, this process works great. </span></span><span><span data-start="962.0">However, in complex graphs with many layers, </span></span><span><span data-start="965.0">this initial compilation can lead to high application launch times.</span></span></p><p><span><span data-start="970.0">MPSGraph has a new serialization format called MPSGraphPackage, </span></span><span><span data-start="974.0">to address exactly this problem. </span></span><span><span data-start="977.0">This new serialization format allows you to create </span></span><span><span data-start="980.0">the MPSGraphExecutable ahead of time. </span></span><span><span data-start="983.0">Once created, the optimized MPSGraphExecutable </span></span><span><span data-start="986.0">can be loaded directly from a MPSGraphPackage file. </span></span><span><span data-start="990.0">Creating a MPSGraphPackage is very simple.</span></span></p><p><span><span data-start="994.0">All you need to do is to create a serialization descriptor </span></span><span><span data-start="997.0">and pass it to the serialize function </span></span><span><span data-start="999.0">of the MPSGraphExecutable you want to serialize. </span></span><span><span data-start="1002.0">You&#39;ll also need to pass a path to store it. </span></span><span><span data-start="1006.0">After creating the package, </span></span><span><span data-start="1007.0">this is how you load the graph into your app. </span></span><span><span data-start="1011.0">You need a compilation descriptor and the path to your stored package. </span></span><span><span data-start="1015.0">Then use them to initialize the MPSGraphExecutable. </span></span><span><span data-start="1019.0">If you&#39;ve been already using MPSGraph, you can easily adopt </span></span><span><span data-start="1023.0">the new serialization format using the APIs we have presented. </span></span><span><span data-start="1027.0">But if you&#39;re coming from other frameworks, </span></span><span><span data-start="1029.0">you can now easily migrate to MPSGraphPackage </span></span><span><span data-start="1031.0">using the new MPSGraphTool. </span></span><span><span data-start="1034.0">For users of CoreML, </span></span><span><span data-start="1036.0">you can pass your ML Programs </span></span><span><span data-start="1038.0">tp the MPSGraphTool, which will create </span></span><span><span data-start="1040.0">a MPSGraphPackage for you. </span></span><span><span data-start="1042.0">The same goes for ONNX, </span></span><span><span data-start="1043.0">where you can use your ONNX file </span></span><span><span data-start="1045.0">as the input. </span></span><span><span data-start="1046.0">This new tool lets you quickly include your existing models </span></span><span><span data-start="1050.0">to your MPSGraph application without the need </span></span><span><span data-start="1052.0">to encode the inference model manually.</span></span></p><p><span><span data-start="1055.0">Here&#39;s how you use the command line tool. </span></span><span><span data-start="1058.0">You give the MPSGraphTool a flag to declare the input model type, </span></span><span><span data-start="1062.0">in this case, CoreML Package. </span></span><span><span data-start="1064.0">You also provide it with the path to your output destination </span></span><span><span data-start="1067.0">and the name of your output model. </span></span><span><span data-start="1070.0">Additionally, you define the target platform </span></span><span><span data-start="1072.0">and minimum OS version. </span></span><span><span data-start="1075.0">After conversion, the produced MPSGraphPackages </span></span><span><span data-start="1078.0">can be loaded to your app and executed directly.</span></span></p><p><span><span data-start="1082.0">Next, lets discuss how you can improve the efficiency </span></span><span><span data-start="1085.0">of your computations using the 8-bit integer quantizations. </span></span><span><span data-start="1090.0">It&#39;s common to use floating point formats to do training and inference, </span></span><span><span data-start="1094.0">such as 16-bit floating point format. </span></span><span><span data-start="1096.0">However, at inference, these models may take a longer time to predict results. </span></span><span><span data-start="1103.0">Instead, it&#39;s better in many cases to use reduced precision </span></span><span><span data-start="1107.0">or 8-bit integer numbers. </span></span><span><span data-start="1109.0">This will help you in saving memory bandwith </span></span><span><span data-start="1112.0">and reduce the memory footprint of your model.</span></span></p><p><span><span data-start="1116.0">For 8-bit integer formats, there are two types of quantization: </span></span><span><span data-start="1120.0">symmetric and asymmetric. </span></span><span><span data-start="1122.0">MPSGraph now supports APIs for both of them. </span></span><span><span data-start="1126.0">Compared to the symmetric quantization, the asymmetric one lets you specify </span></span><span><span data-start="1130.0">a quantization bias, denoted by zeroPoint here.</span></span></p><p><span><span data-start="1135.0">Now let&#39;s delve into using quantized computations through an example, </span></span><span><span data-start="1140.0">starting with activation and weights in an Int8 format as the inputs. </span></span><span><span data-start="1145.0">These inputs are dequantized to floating point format </span></span><span><span data-start="1148.0">using the dequantizeTensor op in MPSGraph. </span></span><span><span data-start="1152.0">Now the floating point inputs can be fed </span></span><span><span data-start="1155.0">into a convolution operation. </span></span><span><span data-start="1157.0">The resulting floating point tensor </span></span><span><span data-start="1159.0">can then be quantized back to Int8 </span></span><span><span data-start="1161.0">using the quantizeTensor op. </span></span><span><span data-start="1163.0">MPSGraph will automatically fuse all of these kernels into a single operation, </span></span><span><span data-start="1168.0">therefore saving memory bandwidth and potentially improving performance.</span></span></p><p><span><span data-start="1173.0">And this is how you can use the quantization support in MPSGraph.</span></span></p><p><span><span data-start="1179.0">In addition to the previous new features, </span></span><span><span data-start="1181.0">MPSGraph supports even more machine learning operators. </span></span><span><span data-start="1185.0">Starting this year, complex types are supported for most graph operations. </span></span><span><span data-start="1190.0">You can use complex numbers either with single precision </span></span><span><span data-start="1193.0">or half precision floating point formats.</span></span></p><p><span><span data-start="1196.0">Building on the complex data type, </span></span><span><span data-start="1198.0">MPSGraph adds operators for computing Fast Fourier Transformations. </span></span><span><span data-start="1202.0">You can apply complex to complex, complex to real, </span></span><span><span data-start="1205.0">and real to complex transformations up to four dimensions. </span></span><span><span data-start="1209.0">These are very common in audio, video, and image processing applications. </span></span><span><span data-start="1215.0">Furthermore, using MPSGraph, </span></span><span><span data-start="1218.0">you can now perform three-dimensional convolutions, </span></span><span><span data-start="1221.0">grid sampling, Sort and ArgSort, and cumulative operations, </span></span><span><span data-start="1225.0">including sums, products, minima, and maxima. </span></span><span><span data-start="1230.0">And this concludes the discussion about the new features in MPSGraph. </span></span><span><span data-start="1235.0">Let&#39;s review what was presented today in this session.</span></span></p><p><span><span data-start="1238.0">I went over the improvements in accelerating popular ML frameworks </span></span><span><span data-start="1242.0">like PyTorch and TensorFlow through Metal. </span></span><span><span data-start="1245.0">Now you can also take advantage of the new Metal accelerated JAX framework. </span></span><span><span data-start="1251.0">We also discussed how to seamlessly integrate your existing models </span></span><span><span data-start="1254.0">from other frameworks to MPSGraph using the new serialization tools. </span></span><span><span data-start="1259.0">And this concludes our talk. </span></span><span><span data-start="1261.0">We can&#39;t wait to see the amazing content that you will create </span></span><span><span data-start="1264.0">using all of these features. </span></span><span><span data-start="1265.0">Thanks for watching. </span></span><span><span data-start="1267.0">♪ ♪</span></span></p>
							</section>
						</div></div>
  </body>
</html>
