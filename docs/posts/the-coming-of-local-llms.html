<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nickarner.com/notes/the-coming-of-local-llms-march-23-2023/">Original</a>
    <h1>The Coming of Local LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>While there’s been a truly remarkable advance in large language models as they continue to scale up, facilitated by being trained and run on larger and larger GPU clusters, there is still a need to be able to run smaller models on devices that have constraints on memory and processing power.</p><p>Being able to run models at the edge enables creating applications that may be more sensitive to user privacy or latency considerations - ensuring that user data does not leave the device.</p><p>This also enables the application to be able to always work without concerns over server outages or degradation, or upstream provider policy or API changes.</p><h2 id="llama-changes-everything">LLaMA Changes Everything</h2><p>Recently, the weights of <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Facebook’s LLaMA model</a> <a href="https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse">leaked via a torrent posted to 4Chan</a>. This sparked a flurry of open-source activity, including <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Authored by the <a href="https://ggerganov.com">creator</a> of <a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a>, it quickly showed that <a href="https://twitter.com/ggerganov/status/1634320862722551809">it’s possible to get an LLM running on an M1 Mac</a>:</p><p><img src="https://nickarner.com/blog_assets/2023/LLaMA_On_M1.png" alt="LLaMa_On_M1"/></p><p>Soon, Anih Thite <a href="https://twitter.com/thiteanish/status/1635188333705043969">posted a video</a> of it running on a Google Pixel 6 phone. It was incredibly slow, at 1 token per second, but it was a start.</p><p>The next day, <a href="https://twitter.com/thiteanish/status/1635678053853536256">he posted a new video</a> - showing it running at 5 tokens a second.</p><p><img src="https://nickarner.com/blog_assets/2023/LLaMA_Pixel6.png" alt="LLaMA_Pixel6"/></p><p><a href="https://twitter.com/miolini">Artem Andreenko</a> soon was able to get LLaMA running on a Raspberry Pi 4:</p><p><img src="https://nickarner.com/blog_assets/2023/LLaMA_On_Raspberry_Pi.png" alt="LLaMa_On_Raspberry_Pi"/></p><p><a href="https://twitter.com/antimatter15">Kevin Kwok</a> created a <a href="https://github.com/antimatter15/alpaca.cpp">fork of llama.cpp</a> that included an add-on for a Chat-GPT style interface:</p><p><img src="https://nickarner.com/blog_assets/2023/AlpacaCPPChat.png" alt="AlpacaCPPChat"/></p><p>And finally, yesterday, he <a href="https://twitter.com/antimatter15/status/1638695917514784775?s=20">posted a demonstration of Alpaca running on an iPhone</a>:</p><p><img src="https://nickarner.com/blog_assets/2023/Alpacca_On_iPhone.png" alt="AlpacaCPPChat"/></p><h2 id="consumer-electronics-companies-and-llms">Consumer Electronics Companies and LLMs</h2><p>With the rapid advances being made for running local LLMs in the open source community, the question is bound to be asked - what is Apple doing? While Apple does have significant ML capabilities and talent, a look at their <a href="https://www.apple.com/careers/us/machine-learning-and-ai.html">ML Jobs Listings</a> does not indicate they are actively hiring for any LLM projects. The cream of the crop of expertise in LLM’s are at OpenAI, Anthropic, and DeepMind.</p><p>Apple is generally always late to deploying big technological advancements into their products. If and when Apple does develop on-device LLM capabilities, it could either arrive in the form of <a href="https://developer.apple.com/documentation/coreml">CoreML</a> models that are embedded in individual apps and come with different flavors; such as summarization, sentiment analysis, and text generation.</p><p>Or, alternatively, they could deploy a single LLM as part of an OS update. Apps could then interact with the LLM through system frameworks, <a href="https://machinelearning.apple.com/research/face-detection">similar to how the Vision SDK works</a>.</p><p>This, to me, seems the more likely of the two approaches - both to be sure that each app on a user’s phone does not embed their own, large model bundled with them, and - more importantly - that would allow Apple to have a much more capable version of Siri.</p><p>It’s incredibly unlikely that Apple will ever license the use of LLMs from outside parties like OpenAI or Anthropic. Their strategy is much more the style of either building it all in-house, or to acquire small startups that they integrate into their products. This is what happened with the <a href="https://www.forbes.com/sites/parmyolson/2011/10/06/steve-jobs-leaves-a-legacy-in-a-i-with-siri/?sh=4354901d5dd3">Siri acquisition</a>.</p><p>One consumer electronics company that isn’t shy about having a partnership with OpenAI is <a href="http://hu.ma.ne/">Humane</a>. They recently <a href="https://hu.ma.ne/media/humane-raises-100m-in-series-c-round-as-it-builds-device-and-services-platform-for-the-ai-era">announced a $100M Series C round of financing</a>. While still in stealth, they’re widely considered to be a laser-projector based AR wearable. Such a device would surely have major constraints on power, due to the nature of the laser projector. This would probably have a major restriction on running an embedded LLM, at least in their first version.</p><p>As part of their fundraising announcement, they did disclose that they’re partnering with several companies, including OpenAI. <a href="https://twitter.com/nickarner/status/1633519507997335552?s=20">My guess</a> is that they’re going to use a combination of <a href="https://openai.com/research/whisper">Whisper</a> and <a href="https://openai.com/research/gpt-4">GPT4</a> for some kind of personal assistant as part of their hardware product. While Whisper has been shown to be quite capable of running on-device, it will probably be some time until a powerful language model will be able to do so in production.</p><h2 id="closing-thoughts">Closing Thoughts</h2><p>I think we’re going to eventually see a demo showing an open source model running on an iPhone as well. I don’t have intuition as to how long it will take until we start seeing these models being baked into production apps, and eventually, into the OS’s themselves. I do anticipate this to eventually happen, opening the door to extremely personal and personalized ML models that we will carry with us in our pockets - having intelligent assistants with us at all times.</p><p>If you’re doing work in the area of local LLMs, particularly ones that may be able to run on phones or other embedded devices, please reach out.</p><h2 id="further-reading">Further Reading</h2><ul><li><a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a></li><li><a href="https://arstechnica.com/information-technology/2023/03/you-can-now-run-a-gpt-3-level-ai-model-on-your-laptop-phone-and-raspberry-pi/?utm_social-type=owned&amp;utm_medium=social&amp;utm_source=twitter&amp;utm_brand=ars">You can now run a GPT-3-level AI model on your laptop, phone, and Raspberry Pi</a></li><li><a href="https://github.com/ggerganov/llama.cpp/discussions/205">Inference at the edge</a></li></ul></div></div></div>
  </body>
</html>
