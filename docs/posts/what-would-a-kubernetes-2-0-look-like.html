<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matduggan.com/what-would-a-kubernetes-2-0-look-like/">Original</a>
    <h1>What would a Kubernetes 2.0 look like</h1>
    
    <div id="readability-page-1" class="page"><div>
        
        <main id="main-content" role="main">
            
<article>
    

    
    <section>
        <div>
            <p>Around 2012-2013 I started to hear a <em>lot</em> in the sysadmin community about a technology called &#34;Borg&#34;. It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a &#34;Borglet&#34; inside of clusters with &#34;cells&#34; but the basics started to leak. There was a concept of &#34;services&#34; and a concept of &#34;jobs&#34;, where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. </p><p>Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for &#39;helmsman&#39; that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) </p><p>Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to &#34;maybe this is a real product?&#34; On July 21st 2015 we got the v1.0 release as well as the creation of the CNCF. </p><p>In the ten years since that initial commit, Kubernetes has become a large part of my professional life. I use it at home, at work, on side projects—anywhere it makes sense. It&#39;s a tool with a steep learning curve, but it&#39;s also a massive force multiplier. We no longer &#34;manage infrastructure&#34; at the server level; everything is declarative, scalable, recoverable and (if you’re lucky) self-healing.</p><p>But the journey hasn&#39;t been without problems. Some common trends have emerged, where mistakes or misconfiguration arise from where Kubernetes isn&#39;t opinionated enough. Even ten years on, we&#39;re still seeing a lot of churn inside of ecosystem and people stepping on well-documented landmines. So, knowing what we know now, what could we do differently to make this great tool even more applicable to more people and problems? </p><h3 id="what-did-k8s-get-right">What did k8s get right?</h3><p>Let&#39;s start with the positive stuff. Why are we still talking about this platform now? </p><p><strong>Containers at scale</strong></p><p>Containers as a tool for software development make perfect sense. Ditch the confusion of individual laptop configuration and have one standard, disposable concept that works across the entire stack. While tools like Docker Compose allowed for some deployments of containers, they were clunky and still required you as the admin to manage a lot of the steps. I set up a Compose stack with a deployment script that would remove the instance from the load balancer, pull the new containers, make sure they started and then re-added it to the LB, as did lots of folks. </p><p>K8s allowed for this concept to scale out, meaning it was possible to take a container from your laptop and deploy an identical container across thousands of servers. This flexibility allowed organizations to revisit their entire design strategy, dropping monoliths and adopting more flexible (and often more complicated) micro-service designs. </p><p><strong>Low-Maintenance</strong></p><p>If you think of the history of Operations as a sort of &#34;naming timeline from pets to cattle&#34;, we started with what I affectionately call the &#34;Simpsons&#34; era. Servers were bare metal boxes set up by teams, they often had one-off names that became slang inside of teams and everything was a snowflake. The longer a server ran, the more cruft it picked up until it became a scary operation to even reboot them, much less attempt to rebuild them. I call it the &#34;Simpsons&#34; era because among the jobs I was working at the time, naming them after Simpsons characters was surprisingly common. Nothing fixed itself, everything was a manual operation. </p><p>Then we transition into the &#34;01 Era&#34;. Tools like Puppet and Ansible have become common place, servers are more disposable and you start to see things like bastion hosts and other access control systems become the norm. Servers aren&#39;t all facing the internet, they&#39;re behind a load balancer and we&#39;ve dropped the cute names for stuff like &#34;app01&#34; or &#34;vpn02&#34;. Organizations designed it so they could lose some of their servers some of the time. However failures still weren&#39;t self-healing, someone still had to SSH in to see what broke, write up a fix in the tooling and then deploy it across the entire fleet. OS upgrades were still complicated affairs. </p><p>We&#39;re now in the &#34;UUID Era&#34;. Servers exist to run containers, they are entirely disposable concepts. Nobody cares about how long a particular version of the OS is supported for, you just bake a new AMI and replace the entire machine. K8s wasn&#39;t the only technology enabling this, but it was the one that accelerated it. Now the idea of a bastion server with SSH keys that I go to the underlying server to fix problems is seen as more of a &#34;break-glass&#34; solution. Almost all solutions are &#34;destroy that Node, let k8s reorganize things as needed, make a new Node&#34;. </p><p>A lot of the Linux skills that were critical to my career are largely nice to have now, not need to have. You can be happy or sad about that, I certainly switch between the two emotions on a regular basis, but it&#39;s just the truth. </p><p><strong>Running Jobs </strong></p><p>The k8s jobs system isn&#39;t perfect, but it&#39;s so much better than the &#34;snowflake cron01 box&#34; that was an extremely common sight at jobs for years. Running on a cron schedule or running from a message queue, it was now possible to reliably put jobs into a queue, have them get run, have them restart if they didn&#39;t work and then move on with your life. </p><p>Not only does this free up humans from a time-consuming and boring task, but it&#39;s also simply a more efficient use of resources. You are still spinning up a pod for every item in the queue, but your teams have a lot of flexibility inside of the &#34;pod&#34; concept for what they need to run and how they want to run it. This has really been a quality of life improvement for a lot of people, myself included, who just need to be able to easily background tasks and not think about them again. </p><p><strong>Service Discoverability and Load Balancing</strong></p><p>Hard-coded IP addresses that lived inside of applications as the template for where requests should be routed has been a curse following me around for years. If you were lucky, these dependencies weren&#39;t based on IP address but were actually DNS entries and you could change the thing behind the DNS entry without coordinating a deployment of a million applications. </p><p>K8s allowed for simple DNS names to call other services. It removed an entire category of errors and hassle and simplified the entire thing down. With the Service API you had a stable, long lived IP and hostname that you could just point things towards and not think about any of the underlying concepts. You even have concepts like ExternalName that allow you to treat external services like they&#39;re in the cluster. </p><h2 id="what-would-i-put-in-a-kubernetes-20">What would I put in a Kubernetes 2.0?</h2><h3 id="ditch-yaml-for-hcl">Ditch YAML for HCL</h3><p>YAML was appealing because it wasn&#39;t JSON or XML, which is like saying your new car is great because it&#39;s neither a horse nor a unicycle. It demos nicer for k8s, looks nicer sitting in a repo and has the <em>illusion</em> of being a simple file format. In reality. YAML is just too much for what we&#39;re trying to do with k8s and it&#39;s not a safe enough format. Indentation is error-prone, the files don&#39;t scale great (you really don&#39;t want a super long YAML file), debugging can be annoying. YAML has <em>so many</em> subtle behaviors outlined in its spec.</p><p>I still remember not believing what I was seeing the first time I saw the Norway Problem. For those lucky enough to not deal with it, the Norway Problem in YAML is when &#39;NO&#39; gets interpreted as false. Imagine explaining to your Norwegian colleagues that their entire country evaluates to false in your configuration files. Add in accidental numbers from lack of quotes, the list goes on and on. There are much better posts on why YAML is crazy than I&#39;m capable of writing: <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell">https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell</a></p><p><strong>Why HCL?</strong></p><p>HCL is already the format for Terraform, so at least we&#39;d only have to hate one configuration language instead of two. It&#39;s strongly typed with explicit types. There&#39;s already good validation mechanisms. It is specifically designed to do the job that we are asking YAML to do and it&#39;s not much harder to read. It has built-in functions people are already using that would allow us to remove some of the third-party tooling from the YAML workflow. </p><p>I would wager 30% of Kubernetes clusters today are <em>already</em> being managed with HCL via Terraform. We don&#39;t need the Terraform part to get a lot of the benefits of a superior configuration language. </p><p>The only downsides are that HCL is slightly more verbose than YAML, and its Mozilla Public License 2.0 (MPL-2.0) would require careful legal review for integration into an Apache 2.0 project like Kubernetes. However, for the quality-of-life improvements it offers, these are hurdles worth clearing.</p><p><strong>Why HCL is better</strong></p><p>Let&#39;s take a simple YAML file. </p><pre><code># YAML doesn&#39;t enforce types
replicas: &#34;3&#34;  # String instead of integer
resources:
  limits:
    memory: 512  # Missing unit suffix
  requests:
    cpu: 0.5m    # Typo in CPU unit (should be 500m)</code></pre><p>Even in the most basic example, there are footguns everywhere. HCL and the type system would catch all of these problems. </p><pre><code>replicas = 3  # Explicitly an integer

resources {
  limits {
    memory = &#34;512Mi&#34;  # String for memory values
  }
  requests {
    cpu = 0.5  # Number for CPU values
  }
}</code></pre><p>Take a YAML file like this that you probably have 6000 in your k8s repo. Now look at HCL without needing external tooling. </p><pre><code># Need external tools or templating for dynamic values
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Can&#39;t easily generate or transform values
  DATABASE_URL: &#34;postgres://user:password@db:5432/mydb&#34;
  API_KEY: &#34;static-key-value&#34;
  TIMESTAMP: &#34;2023-06-18T00:00:00Z&#34;  # Hard-coded timestamp</code></pre><pre><code>resource &#34;kubernetes_config_map&#34; &#34;app_config&#34; {
  metadata {
    name = &#34;app-config&#34;
  }
  
  data = {
    DATABASE_URL = &#34;postgres://${var.db_user}:${var.db_password}@${var.db_host}:${var.db_port}/${var.db_name}&#34;
    API_KEY      = var.api_key != &#34;&#34; ? var.api_key : random_string.api_key.result
    TIMESTAMP    = timestamp()
  }
}

resource &#34;random_string&#34; &#34;api_key&#34; {
  length  = 32
  special = false
}</code></pre><p>Here&#39;s all the pros you get with this move. </p><ol><li><strong>Type Safety</strong>: Preventing type-related errors before deployment</li><li><strong>Variables and References</strong>: Reducing duplication and improving maintainability</li><li><strong>Functions and Expressions</strong>: Enabling dynamic configuration generation</li><li><strong>Conditional Logic</strong>: Supporting environment-specific configurations</li><li><strong>Loops and Iteration</strong>: Simplifying repetitive configurations</li><li><strong>Better Comments</strong>: Improving documentation and readability</li><li><strong>Error Handling</strong>: Making errors easier to identify and fix</li><li><strong>Modularity</strong>: Enabling reuse of configuration components</li><li><strong>Validation</strong>: Preventing invalid configurations</li><li><strong>Data Transformations</strong>: Supporting complex data manipulations<br/></li></ol><h3 id="allow-etcd-swap-out">Allow etcd swap-out</h3><p>I know, I&#39;m the 10,000 person to write this. Etcd has done a fine job, but it&#39;s a little crazy that it is the only tool for the job. For smaller clusters or smaller hardware configuration, it&#39;s a large use of resources in a cluster type where you will never hit the node count where it pays off. It&#39;s also a strange relationship between k8s and etcd now, where k8s is basically the only etcd customer left. </p><p>What I&#39;m suggesting is taking the work of <a href="https://github.com/k3s-io/kine" rel="noreferrer">kine</a> and making it official. It makes sense for the long-term health of the project to have the ability to plug in more backends, adding this abstraction means it (should) be easier to swap in new/different backends in the future and it also allows for more specific tuning depending on the hardware I&#39;m putting out there. </p><p>What I suspect this would end up looking like is much like this: <a href="https://github.com/canonical/k8s-dqlite">https://github.com/canonical/k8s-dqlite</a>. Distributed SQlite in-memory with Raft consensus and almost zero upgrade work required that would allow cluster operators to have more flexibility with the persistence layer of their k8s installations. If you have a conventional server setup in a datacenter and etcd resource usage is not a problem, great! But this allows for lower-end k8s to be a nicer experience and (hopefully) reduces dependence on the etcd project. </p><h3 id="beyond-helm-a-native-package-manager">Beyond Helm: A Native Package Manager</h3><p>Helm is a perfect example of a temporary hack that has grown to be a permanent dependency. I&#39;m grateful to the maintainers of Helm for all of their hard work, growing what was originally a hackathon project into the de-facto way to install software into k8s clusters. It has done as good a job as something could in fulfilling that role without having a deeper integration into k8s. </p><p>All that said, Helm is a nightmare to use. The Go templates are tricky to debug, often containing complex logic that results in really confusing error scenarios. The error messages you get from those scenarios are often gibberish. Helm isn&#39;t a very good package system because it fails at some of the basic tasks you need a package system to do, which are transitive dependencies and resolving conflicts between dependencies. </p><p><strong>What do I mean?</strong></p><p>Tell me what this conditional logic is trying to do:</p><pre><code># A real-world example of complex conditional logic in Helm
{{- if or (and .Values.rbac.create .Values.serviceAccount.create) (and .Values.rbac.create (not .Values.serviceAccount.create) .Values.serviceAccount.name) }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ template &#34;myapp.fullname&#34; . }}
  labels:
    {{- include &#34;myapp.labels&#34; . | nindent 4 }}
{{- end }}</code></pre><p>Or if I provide multiple values files to my chart, which one wins:</p><pre><code>helm install myapp ./mychart -f values-dev.yaml -f values-override.yaml --set service.type=NodePort</code></pre><p>Ok, what if I want to manage my application and all the application dependencies with a Helm chart. This makes sense, I have an application that itself has dependencies on other stuff so I want to put them all together. So I define my sub-charts or umbrella charts inside of my Chart.yaml. <br/></p><pre><code>dependencies:
- name: nginx
  version: &#34;1.2.3&#34;
  repository: &#34;&lt;https://example.com/charts&gt;&#34;
- name: memcached
  version: &#34;1.2.3&#34;
  repository: &#34;&lt;https://another.example.com/charts&gt;&#34;
</code></pre><p>But assuming I have multiple applications, it&#39;s entirely possible that I have 2 services both with a dependency on nginx or whatever like this:<br/></p><figure><img src="https://matduggan.com/content/images/2025/06/image-2.png" alt="" loading="lazy" width="367" height="176"/></figure><p>Helm doesn&#39;t handle this situation gracefully because template names are global with their templates loaded alphabetically. Basically you need to:</p><ul><li>Don&#39;t declare a dependency on the same chart more than once (hard to do for a lot of microservices)</li><li>If you do have the same chart declared multiple times, has to use the exact same version</li></ul><p>The list of issues goes on and on. </p><ul><li>Cross-Namespace installation stinks</li><li>Chart verification process is a pain and nobody uses it</li></ul><p>Let&#39;s just go to the front page of artifacthub:</p><figure><img src="https://matduggan.com/content/images/2025/06/image-3.png" alt="" loading="lazy" width="978" height="794" srcset="https://matduggan.com/content/images/size/w600/2025/06/image-3.png 600w, https://matduggan.com/content/images/2025/06/image-3.png 978w" sizes="(min-width: 720px) 720px"/></figure><p>I&#39;ll grab elasticsearch cause that seems important. </p><figure><img src="https://matduggan.com/content/images/2025/06/image-4.png" alt="" loading="lazy" width="504" height="296"/></figure><figure><img src="https://matduggan.com/content/images/2025/06/image-5.png" alt="" loading="lazy" width="522" height="404"/></figure><p>Seems <em>pretty bad</em> for the Official Elastic helm chart. Certainly <code>ingress-nginx</code> will be right, it&#39;s an absolute critical dependency for the entire industry. </p><figure><img src="https://matduggan.com/content/images/2025/06/image-6.png" alt="" loading="lazy" width="751" height="184" srcset="https://matduggan.com/content/images/size/w600/2025/06/image-6.png 600w, https://matduggan.com/content/images/2025/06/image-6.png 751w" sizes="(min-width: 720px) 720px"/></figure><p>Nope. Also how is the maintainer of the chart &#34;Kubernetes&#34; and it&#39;s <em>still</em> not marked as a <code>verified publisher</code>. Like Christ how much more verified does it get.</p><ul><li>No metadata in chart searching. You can only search by name and description, not by features, capabilities, or other metadata.</li></ul><figure><img src="https://matduggan.com/content/images/2025/06/image-7.png" alt="" loading="lazy" width="1138" height="929" srcset="https://matduggan.com/content/images/size/w600/2025/06/image-7.png 600w, https://matduggan.com/content/images/size/w1000/2025/06/image-7.png 1000w, https://matduggan.com/content/images/2025/06/image-7.png 1138w" sizes="(min-width: 720px) 720px"/></figure><ul><li>Helm doesn&#39;t strictly enforce semantic versioning</li></ul><pre><code># Chart.yaml with non-semantic version
apiVersion: v2
name: myapp
version: &#34;v1.2-alpha&#34; </code></pre><ul><li>If you uninstall and reinstall a chart with CRDs, it might delete resources created by those CRDs. This one has screwed me <em>multiple times</em> and is crazy unsafe. </li></ul><p>I could keep writing for another 5000 words and still wouldn&#39;t have outlined all the problems. There isn&#39;t a way to make Helm good enough for the task of &#34;package manager for all the critical infrastructure on the planet&#34;. </p><h4 id="what-would-a-k8s-package-system-look-like">What would a k8s package system look like?</h4><p>Let&#39;s call our hypothetical package system KubePkg, because if there&#39;s one thing the Kubernetes ecosystem needs, it&#39;s another abbreviated name with a &#39;K&#39; in it. We would try to copy as much of the existing work inside the Linux ecosystem while taking advantage of the CRD power of k8s. My idea looks something like this:</p><figure><img src="https://matduggan.com/content/images/2025/06/image-8.png" alt="" loading="lazy" width="600" height="356" srcset="https://matduggan.com/content/images/2025/06/image-8.png 600w"/></figure><p>The packages are bundles like a Linux package:<br/></p><figure><img src="https://matduggan.com/content/images/2025/06/image-9.png" alt="" loading="lazy" width="528" height="196"/></figure><p>There&#39;s a definition file that accounts for as many of the real scenarios that you actually encounter when installing a thing. </p><pre><code>apiVersion: kubepkg.io/v1
kind: Package
metadata:
  name: postgresql
  version: 14.5.2
spec:
  maintainer:
    name: &#34;PostgreSQL Team&#34;
    email: &#34;<a href="https://interjectedfuture.com/cdn-cgi/l/email-protection" data-cfemail="422f232b2c36232b2c27303102322d313625302731332e6c273a232f322e276c212d2f">[email protected]</a>&#34;
  description: &#34;PostgreSQL database server&#34;
  website: &#34;https://postgresql.org&#34;
  license: &#34;PostgreSQL&#34;
  
  # Dependencies with semantic versioning
  dependencies:
    - name: storage-provisioner
      versionConstraint: &#34;&gt;=1.0.0&#34;
    - name: metrics-collector
      versionConstraint: &#34;^2.0.0&#34;
      optional: true
  
  # Security context and requirements
  security:
    requiredCapabilities: [&#34;CHOWN&#34;, &#34;SETGID&#34;, &#34;SETUID&#34;]
    securityContextConstraints:
      runAsUser: 999
      fsGroup: 999
    networkPolicies:
      - ports:
        - port: 5432
          protocol: TCP
    
  # Resources to be created (embedded or referenced)
  resources:
    - apiVersion: v1
      kind: Service
      metadata:
        name: postgresql
      spec:
        ports:
        - port: 5432
    - apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: postgresql
      spec:
        # StatefulSet definition
  
  # Configuration schema using JSON Schema
  configurationSchema:
    type: object
    properties:
      replicas:
        type: integer
        minimum: 1
        default: 1
      persistence:
        type: object
        properties:
          size:
            type: string
            pattern: &#34;^[0-9]+[GMK]i$&#34;
            default: &#34;10Gi&#34;
  
  # Lifecycle hooks with proper sequencing
  hooks:
    preInstall:
      - name: database-prerequisites
        job:
          spec:
            template:
              spec:
                containers:
                - name: init
                  image: postgres:14.5
    postInstall:
      - name: database-init
        job:
          spec:
            # Job definition
    preUpgrade:
      - name: backup
        job:
          spec:
            # Backup job definition
    postUpgrade:
      - name: verify
        job:
          spec:
            # Verification job definition
    preRemove:
      - name: final-backup
        job:
          spec:
            # Final backup job definition
  
  # State management for stateful applications
  stateManagement:
    backupStrategy:
      type: &#34;snapshot&#34;  # or &#34;dump&#34;
      schedule: &#34;0 2 * * *&#34;  # Daily at 2 AM
      retention:
        count: 7
    recoveryStrategy:
      type: &#34;pointInTime&#34;
      verificationJob:
        spec:
          # Job to verify recovery success
    dataLocations:
      - path: &#34;/var/lib/postgresql/data&#34;
        volumeMount: &#34;data&#34;
    upgradeStrategies:
      - fromVersion: &#34;*&#34;
        toVersion: &#34;*&#34;
        strategy: &#34;backup-restore&#34;
      - fromVersion: &#34;14.*.*&#34;
        toVersion: &#34;14.*.*&#34;
        strategy: &#34;in-place&#34;</code></pre><p>There&#39;s a real signing process that would be required and allow you more control over the process. <br/></p><pre><code>apiVersion: kubepkg.io/v1
kind: Repository
metadata:
  name: official-repo
spec:
  url: &#34;https://repo.kubepkg.io/official&#34;
  type: &#34;OCI&#34;  # or &#34;HTTP&#34;
  
  # Verification settings
  verification:
    publicKeys:
      - name: &#34;KubePkg Official&#34;
        keyData: |
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvF4+...
          -----END PUBLIC KEY-----
    trustPolicy:
      type: &#34;AllowList&#34;  # or &#34;KeyRing&#34;
      allowedSigners:
        - &#34;KubePkg Official&#34;
        - &#34;Trusted Partner&#34;
    verificationLevel: &#34;Strict&#34;  # or &#34;Warn&#34;, &#34;None&#34;</code></pre>
<p>Like how great would it be to have something where I could automatically update packages without needing to do anything on my side. </p><pre><code>apiVersion: kubepkg.io/v1
kind: Installation
metadata:
  name: postgresql-main
  namespace: database
spec:
  packageRef:
    name: postgresql
    version: &#34;14.5.2&#34;
  
  # Configuration values (validated against schema)
  configuration:
    replicas: 3
    persistence:
      size: &#34;100Gi&#34;
    resources:
      limits:
        memory: &#34;4Gi&#34;
        cpu: &#34;2&#34;
  
  # Update policy
  updatePolicy:
    automatic: false
    allowedVersions: &#34;14.x.x&#34;
    schedule: &#34;0 2 * * 0&#34;  # Weekly on Sunday at 2am
    approvalRequired: true
  
  # State management reference
  stateRef:
    name: postgresql-main-state
    
  # Service account to use
  serviceAccountName: postgresql-installer</code></pre><p>What k8s needs is a system that meets the following requirements:</p><ol><li><strong>True Kubernetes Native</strong>: Everything is a Kubernetes resource with proper status and events</li><li><strong>First-Class State Management</strong>: Built-in support for stateful applications</li><li><strong>Enhanced Security</strong>: Robust signing, verification, and security scanning</li><li><strong>Declarative Configuration</strong>: No templates, just structured configuration with schemas</li><li><strong>Lifecycle Management</strong>: Comprehensive lifecycle hooks and upgrade strategies</li><li><strong>Dependency Resolution</strong>: Linux-like dependency management with semantic versioning</li><li><strong>Audit Trail</strong>: Complete history of changes with who, what, and when, not what Helm currently provides. </li><li><strong>Policy Enforcement</strong>: Support for organizational policies and compliance. </li><li><strong>Simplified User Experience</strong>: Familiar Linux-like package management commands. It seems wild that we&#39;re trying to go a different direction from the package systems that have worked for decades. </li></ol><h3 id="ipv6-by-default">IPv6 By Default</h3><p>Try to imagine, across the entire globe, how much time and energy has been invested in trying to solve any one of the following three problems. </p><ol><li>I need this pod in this cluster to talk to that pod in that cluster. </li><li>There is a problem happening somewhere in the NAT traversal process and I need to solve it</li><li>I have run out of IP addresses with my cluster because I didn&#39;t account for how many you use. Remember: A company starting with a /20 subnet (4,096 addresses), deploys 40 nodes with 30 pods each, and suddenly realizes they&#39;re approaching their IP limit. Not that many nodes!</li></ol><p>I am not suggesting the entire internet switches over to IPv6 and right now k8s happily supports IPv6-only if you want and a dualstack approach. But I&#39;m saying now is the time to flip the default and just go IPv6. You eliminate a huge collection of problems all at once. </p><ul><li>Flatter, less complicated network topology inside of the cluster. </li><li>The distinction between multiple clusters becomes a thing organizations can choose to ignore if they want if they want to get public IPs.</li><li>Easier to understand exactly the flow of traffic inside of your stack. </li><li>Built-in IPSec</li></ul><p>It has nothing to do with driving IPv6 adoption across the entire globe and just an acknowledgement that we no longer live in a world where you have to accept the weird limitations of IPv4 in a universe where you may need 10,000 IPs suddenly with very little warning. </p><p>The benefits for organizations with public IPv6 addresses is pretty obvious, but there&#39;s enough value there for cloud providers and users that even the corporate overlords might get behind it. AWS never needs to try and scrounge up more private IPv4 space inside of a VPC. That&#39;s gotta be worth something. </p><h3 id="conclusion">Conclusion </h3><p>The common rebuttal to these ideas is, &#34;Kubernetes is an open platform, so the community can build these solutions.&#34; While true, this argument misses a crucial point: <strong>defaults are the most powerful force in technology.</strong> The &#34;happy path&#34; defined by the core project dictates how 90% of users will interact with it. If the system defaults to expecting signed packages and provides a robust, native way to manage them, that is what the ecosystem will adopt.</p><p>This is an ambitious list, I know. But if we&#39;re going to dream, let&#39;s dream big. After all, we&#39;re the industry that thought naming a technology &#39;Kubernetes&#39; would catch on, and somehow it did!</p><p>We see this all the time in other areas like mobile developer and web development, where platforms assess their situation and make <em>radical</em> jumps forward. Not all of these are necessarily projects that the maintainers or companies <em>would</em> take on but I think they&#39;re all ideas that <em>someone</em> should at least revisit and think &#34;is it worth doing now that we&#39;re this nontrivial percentage of all datacenter operations on the planet&#34;? </p><p>Questions/feedback/got something wrong? Find me here: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
        </div>
    </section>



</article>

        </main>
        
    </div></div>
  </body>
</html>
