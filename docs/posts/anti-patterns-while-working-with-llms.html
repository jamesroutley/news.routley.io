<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://instavm.io/blog/llm-anti-patterns">Original</a>
    <h1>Anti-patterns while working with LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><p>After working with LLMs for the last 15 months, these are some of the anti-patterns I have discovered.</p>
<blockquote>
<p>By anti-patterns, I simply mean patterns or behaviors we should avoid when working with LLMs.</p>
</blockquote>
<h2>1. Did I tell you that already?</h2>
<p>Context is a scarce resource and probably worth its weight in gold, we need to use it wisely. One of the learnings is to not send the same information/text multiple times in the same session.</p>
<p>For example, during <a href="https://www.anthropic.com/news/3-5-models-and-computer-use">computer-use</a> sending each and every image frame when a mouse is going from point A to point B on the screen as screenshots with barely anything changing between a lot of consecutive frames (mouse pointer moving 1 millimeter for example) in each API call, when just one new and final screenshot showing current context is enough.</p>
<p>It&#39;s sort of an irony that the same company has come up with a context management <a href="https://www.anthropic.com/news/context-management">tool/api</a>, which helps you reduce/compress the context by removing redundant messages while it did exact opposite for computer-use and sent all previous almost duplicated screenshots in every new LLM api call again. We built open-source <a href="https://github.com/instavm/clickclickclick">click3</a> which does it without sending any possibly duplicate screenshots in API calls - screenshots with significant differences (or taken at state changes) are enough for the LLM to decide next course of action.</p>
<h2>2. Asking a fish to climb a tree</h2>
<p>Should we ask the fish to climb a tree? Sure sometimes they can climb a tree, but better ask them do things they are good at. For example, asking Gemini Banana to generate an image on a wooden plank with a text starting with prefix 1AA..(notice the double A) always ended up with 1A.. (single A) after 13 tries or so, i decided to give up. Later, I had an idea - to write the text in a google doc, take its picture and then give the picture and ask it to merge it on a wooden plank picture (also given by me) -- It did it in 1 shot.</p>
<p>Similarly we should not ask LLMs how many Rs are there in BLUEBERRY - we should ask it to write a code which counts the Rs. Coding ability &gt; Counting ability - atleast for the current LLMs.</p>
<p>Take another example, <a href="https://blog.cloudflare.com/code-mode/">Cloudflare recently realised</a> that tool calling is better when its written as <code>code</code> that calls them. So, it seems we should ask it to generate code whenever we expect more accurate answers.</p>
<p><img src="https://instavm.io/blog-images/fish.png"/></p>
<p><em>The climbing perch - A tree climbing fish</em></p>
<h2>3. Asking LLM to speak, when its drowning (in context)</h2>
<p>LLMs do best when it&#39;s not nearly full with 128k tokens. For long running sessions, which go beyond the 128k token count - it can be even worse, we then depend on the ability of the Claude to compress or discard information based on its whim. For example, the other day, it completely forgot about a database connection URL I had given it and started spitting someone else&#39;s database URL in the same session. Thankfully(for them) that URL didn&#39;t work. Unfortunately, some tasks do need big contexts, my only advice in that case is to be aware of its accuracy decline.</p>
<p><img src="https://instavm.io/blog-images/psql.png"/>
            <em>Some random database url, from its memory</em></p>
<h2>4. The squeaky wheel gets the grease</h2>
<p>LLMs don&#39;t perform well on obscure topics. Similarly and as expected, on topics which were invented after their training cut-off dates, for the simple reason of them not being trained on those topics. They perform well on topics which have been widely discussed. So if your topic is an obscure one, assume less accuracy and figure out ways to make it accurate. Here is an instance of Claude-CLI giving up on Stripe integration which btw has one of the nicest documentation -</p>
<p><img src="https://instavm.io/blog-images/gaveup.jpg"/></p>
<h2>5. You don&#39;t want to be a vibe-coder</h2>
<p>It&#39;s easy to slip into a manager (or as Andrej Karpathy calls it - a vibe-coder) mode with Claude Code like tool but in my observation if you lose the sight of what the LLM is writing, it will eventually be a net loss. Never lose the thread of what&#39;s going on. For example, in the <code>/invoices</code> api, Claude decided it was fine to put the <code>User</code> object in the response json, since it is part of the invoice object. Only I could see it was exposing the <code>password_hash</code> unnecessarily. Although not a security issue immediately, but if something goes wrong, and the attackers gets access to the invoice jsons, this will only help the attackers get more important information. Or imagine someone not even hashing the password and getting exposed. You get the point.</p>
<p><img src="https://instavm.io/blog-images/passwordhash.png"/></p>
<p>References:</p>
<ol>
<li><p><a href="https://www.anthropic.com/news/context-management">https://www.anthropic.com/news/context-management</a></p>
</li>
<li><p><a href="https://github.com/instavm/clickclickclick">https://github.com/instavm/clickclickclick</a></p>
</li>
<li><p><a href="https://blog.cloudflare.com/code-mode/">https://blog.cloudflare.com/code-mode/</a></p>
</li>
<li><p><a href="https://github.com/instavm/coderunner">https://github.com/instavm/coderunner</a></p>
</li>
</ol>
</div></div>
  </body>
</html>
