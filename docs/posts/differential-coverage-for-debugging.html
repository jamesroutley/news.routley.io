<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.swtch.com/diffcover">Original</a>
    <h1>Differential Coverage for Debugging</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        
       


<p>
I have been debugging some code I did not write and was reminded of this technique.
I’m sure it’s a very old debugging technique (like <a href="https://research.swtch.com/bisect">bisection</a>),
but it should be more widely known.
Suppose you have one test case that’s failing.
You can get a sense of what code might be involved by comparing the code coverage
of successful tests with the code coverage of the failing test.

</p><p>
For example, I’ve inserted a bug into my development copy of <code>math/big</code>:
</p><pre>$ <b>go test</b>
--- FAIL: TestAddSub (0.00s)
    int_test.go:2020: addSub(-0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff, 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff) = -0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe, want 0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe
FAIL
exit status 1
FAIL	math/big	7.528s
$
</pre>


<p>
Let’s collect a passing and failing profile:
</p><pre>$ <b>go test -coverprofile=c1.prof -skip=&#39;TestAddSub$&#39;</b>
PASS
coverage: 85.0% of statements
ok  	math/big	8.373s
% <b>go test -coverprofile=c2.prof -run=&#39;TestAddSub$&#39;</b>
--- FAIL: TestAddSub (0.00s)
    int_test.go:2020: addSub(-0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff, 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff) = -0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe, want 0x0, -0x1fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe
FAIL
coverage: 4.7% of statements
exit status 1
FAIL	math/big	0.789s
$
</pre>


<p>
Now we can diff them to make a profile showing what’s unique about the failing test:
</p><pre>$ <b>(head -1 c1.prof; diff c[12].prof | sed -n &#39;s/^&gt; //p&#39;) &gt;c3.prof</b>
$ <b>go tool cover -html=c3.prof</b>
</pre>


<p>
The <code>head -1</code> is preserving the one-line coverage profile header. The <code>diff | sed</code> saves only the lines unique to the failing test’s profile, and the <code>go tool cover -html</code> opens the profile in a web browser.

</p><p>
In the resulting profile, “covered” (green) means it ran in the failing test but not the passing ones, making it something to take a closer look at.
Looking at the file list, only <code>natmul.go</code> has a non-zero coverage percentage, meaning it contains lines that are unique to the failing test.

</p><p>
<img name="diffcover2" width="627" height="653" src="https://research.swtch.com/diffcover2.png" srcset="diffcover2.png 1x, diffcover2@1.5x.png 1.5x, diffcover2@2x.png 2x"/>

</p><p>
If we open <code>natmul.go</code>, we can see various lines in red (“uncovered”).

</p><p>
<img name="diffcover3" width="659" height="583" src="https://research.swtch.com/diffcover3.png" srcset="diffcover3.png 1x, diffcover3@1.5x.png 1.5x, diffcover3@2x.png 2x"/>

</p><p>
These lines ran in passing tests but not in the failing test. They are exonerated, although the fact that the lines normally run but were skipped in the failing test may prompt useful questions about what logic led to them being skipped. In this case, it’s just that the test does not exercise them: the <code>nat.mul</code> method has not been called at all.

</p><p>
Scrolling down, we find the one section of green.

</p><p>
<img name="diffcover4" width="762" height="893" src="https://research.swtch.com/diffcover4.png" srcset="diffcover4.png 1x, diffcover4@1.5x.png 1.5x, diffcover4@2x.png 2x"/>

</p><p>
This code is where I inserted the bug: the <code>else</code> branch is missing <code>za.neg = false</code>, producing the <code>-0x0</code> in the test failure.
Differential coverage is cheap to compute and display, and when it’s right, it can save a lot of time.
Out of over 15,000 lines of code, differential coverage identified 10, including the two relevant ones.

</p><p>
Of course, this technique is not foolproof: a passing test can still execute buggy code if the
bug is data-dependent, or if the test is not sensitive to the specific mistake in the code.
But a lot of the time, buggy code only triggers failures.
In those cases, differential coverage pinpoints the code blocks that merit a closer look.

</p><p>
You can <a href="https://research.swtch.com/bigcover.html">see the full profile here</a>.

</p><p>
A simpler but still useful technique is to view the basic coverage profile for a single failing test.
That gives you an accurate picture of which sections of code ran in the test, which can guide your
debugging: code that didn’t run is not the problem.
And if you are confused about how exactly a particular function returned an error,
the coverage pinpoints the exact error line.
In the example above, the failing test covered only 4.7% of the code.

</p><p>
Differential coverage also works for passing tests. Want to find the <a href="https://research.swtch.com/httpcover.html">code that implements the SOCK5 proxy in net/http</a>?
</p><pre>$ <b>go test -short -skip=SOCKS5 -coverprofile=c1.prof net/http</b>
$ <b>go test -short -run=SOCKS5 -coverprofile=c2.prof net/http</b>
$ <b>(head -1 c1.prof; diff c[12].prof | sed -n &#39;s/^&gt; //p&#39;) &gt;c3.prof</b>
$ <b>go tool cover -html=c3.prof</b>
</pre>


<p>
Have fun!
      </p></div>
    </div></div>
  </body>
</html>
