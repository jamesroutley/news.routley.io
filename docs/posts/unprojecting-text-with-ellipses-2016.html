<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mzucker.github.io/2016/10/11/unprojecting-text-with-ellipses.html">Original</a>
    <h1>Unprojecting text with ellipses (2016)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Using transformed ellipses to estimate perspective transformations of text.</p>

<p>How do you automatically turn this:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_input.png" alt="input image"/></p>

<p>..into this?</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_output.png" alt="input image"/></p>

<p>Find out below, and be sure to check out the code on <a href="https://github.com/mzucker/unproject_text">github</a> afterwards…</p>



<p>A while before my <a href="https://mzucker.github.io/2016/08/15/page-dewarping.html">page dewarping</a> project, I was already thinking
about how to undo the effects of 3D projection on photographs of
text. In the spring of 2016, I advised a couple of students working on
a combined optical character recognition (OCR) and translation
smartphone app for their Engineering senior design project. Their app
was pretty cool – it used <a href="https://github.com/tesseract-ocr">Tesseract OCR</a> along with the
<a href="https://cloud.google.com/translate/docs/">Google Translate API</a> to translate images in the user’s photo
library. Users had to be careful, however, to photograph the text they
wanted to translate head-on, so as to avoid strong perspective
distortion of the type evidenced beautifully in the opening crawl for
each of the Star Wars films:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/ESBOpeningCrawl.png" alt="tesseract OCR hates this"/></p>

<p>Yep, just try running that thru OCR. Won’t work.</p>

<p>One my students found an
<a href="http://www.sciencedirect.com/science/article/pii/S0262885613001066">interesting paper by Carlos Merino-Gracia et al.</a> on how to
undo this type of keystoning. When I looked it over, I wasn’t
surprised that the students felt like they didn’t have time to
implement it – to my eyes, the method seems sophisticated but also
a bit complex.</p>

<p><img src="https://mzucker.github.io/images/unproject_text/fig2.png" alt="grouping"/></p>

<p><img src="https://mzucker.github.io/images/unproject_text/fig46.png" alt="grouping"/></p>

<p>As these figures from the paper show, the method fits a quadrilateral
to each line of the input text, which can then be rectified. This is
useful but difficult because it requires detecting lines of text as
the very first step. It’s kind of a “chicken and egg” problem: if the
text were laid out horizontally, it would be trivial to detect lines;
but the approach uses the detected lines in order to make the text
horizontal!</p>

<p>My goal was to solve the perspective recovery problem based upon a
much simpler model of text appearance, namely that <em>on average, all
letters should be about the same size</em>.  I was really happy with the
approach I ended up with in this project, especially because I got to
learn some interesting math along the way.  Although I’m sure its
neither as accurate or as useful as the Merino-Gracia approach, it
ended up producing persuasive results on my test inputs in pretty
rapid order.</p>



<p>Let’s restate the principle that’s going to guide our approach to the
perspective removal problem:</p>

<blockquote>
  <p>On average, all letters should be about the same size.</p>
</blockquote>

<p>Because I don’t have a fancy text detector hanging around like
Merino-Gracia et al.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>, I’m going to make a huge simplifying
assumption that that the image we’re processing basically contains
only the text that we want to rectify, like for example, this one:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_input.png" alt="input image"/></p>

<p>In this case, we can use <a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)">thresholding</a> to obtain a clean bi-level
image that separates letters from the background:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_threshold.png" alt="thresholded"/></p>

<p>…and then use <a href="https://en.wikipedia.org/wiki/Connected-component_labeling">connected component labeling</a> to obtain the outline
– or <em>contour</em>, in image processing lingo – of each individual
letter, like so:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_contours.png" alt="contours with areas"/></p>

<p>Here I’ve tagged each detected contour with its enclosed area, to make
it clear that area correlates significantly with the position in the
image.  For example, the <strong>N</strong> and the <strong>c</strong> way over on the left
cover drastically fewer pixels than the <strong>x</strong> and the <strong>y</strong> on the far
right. If you plot the position-area relationship along with the
best-fit regression lines, the correlation becomes even more apparent:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/position-vs-area.png" alt="position vs. area"/></p>

<p>Our task is now pretty simple: if we want to remove the effects of
perspective, all we have to do is to transform the image in such a way
that the letter shapes tend to have uniform areas across the entire
image – independent of their position.</p>

<p>But before we get started, we had better define what type of
mathematical transformation we’re going to apply to our image to
correct it.</p>



<p>The mapping that turns normal-looking 2D text into the exciting <em>Star
Wars</em> crawl that seems to zoom past the camera in 3D is called a
<a href="https://en.wikipedia.org/wiki/Homography_(computer_vision)">homography</a>. Due to the geometric optics underlying image formation,
any time we take a photograph of a planar object like a chessboard or
a street sign, it gets mapped through one of these. Mathematically, we
can think about a homography transforming some original \((x, y)\)
point on the surface of our planar object to a destination point
\((x&#39;, y&#39;)\) inside of the image, like this:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/homography.png" alt="action of a homography"/></p>

<p>There are a few different ways to represent homographies
mathematically. Here’s a simple definition in terms of eight
parameters \(a\) through \(h\):</p><p>

\[x&#39;  = \frac{ax + by + c}{gx + hy + 1} ,
\quad 
y&#39;  = \frac{dx + ey + f}{gx + hy + 1}\]

</p><p>As a superset of <a href="https://en.wikipedia.org/wiki/Rotation">rotations</a>, <a href="https://en.wikipedia.org/wiki/Translation">translations</a>,
<a href="https://en.wikipedia.org/wiki/Scaling_(geometry)">scaling transformations</a>, and <a href="https://en.wikipedia.org/wiki/Shear_mapping">shear transformations</a>,
homographies encompass a wide variety of effects. We can consider the
influence of each parameter individually:</p>

<ul>
  <li>\(a\) and \(e\) control scale along the \(x\) and \(y\) axes, respectively</li>
  <li>\(b\) and \(d\) control shear along each axis (together with \(a\) and \(e\) they also influence rotation, too)</li>
  <li>\(c\) and \(f\) control translation along each axis</li>
  <li>\(g\) and \(h\) control perspective distortion along each axis</li>
</ul>

<p>The Shadertoy below shows how the appearance of <a href="http://knowyourmeme.com/memes/nyan-cat-pop-tart-cat">Nyan Cat</a> is altered as
each parameter of the homography (from \(a\) to \(h\)) is changed
individually. Press play in the bottom left corner to see the
animation (you can also hit <code>A</code> to toggle extra animations because wheeee Nyan Cat).</p>



<p>By jointly changing all eight parameters, we can represent every
possible perspective distortion of a planar object. Furthermore, since
homographies are <a href="https://en.wikipedia.org/wiki/Inverse_function">invertible</a>, we can also use them to warp <em>back</em>
from a distorted image to the original, undistorted planar object.</p>



<p><em>Note: this section assumes a bit of linear algebra knowledge – you
 can skim it, but the math here will shore up the ensuing section
 about ellipses.</em></p>

<p>Remember how we said there are multiple ways to write down homographies?
Well, here’s a matrix-based representation:</p><p>

\[\tilde{\mathbf{p}}&#39; = 
\left[\begin{array}{c} \tilde{x}&#39; \\ \tilde{y}&#39; \\ \tilde{w}&#39; \end{array}\right]
= 
\mathbf{H} \tilde{\mathbf{p}}
=
\left[\begin{array}{ccc} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; 1 \end{array}\right]
\left[\begin{array}{c} x \\ y \\ 1 \end{array}\right]
= 
\left[\begin{array}{c} ax + by + c \\ dx + ey + f \\ gx + hy + 1 \end{array}\right]\]

</p><p>Denote by \(\mathbf{H}\) the \(3 \times 3\) matrix of parameters in
the middle of the equation above. When we map the vector
\(\tilde{\mathbf{p}} = (x, y, 1)\) through it, we get three outputs
\(\tilde{x}&#39;\), \(\tilde{y}&#39;\), and \(\tilde{w}&#39;\). In order to obtain
our final coordinates \(x&#39;\) and \(y&#39;\), we simply divide the former two by
\(\tilde{w}&#39;\):</p><p>

\[x&#39;  = \frac{\tilde{x}&#39;}{\tilde{w}&#39;} = \frac{ax + by + c}{gx + hy + 1},
\quad
y&#39;  = \frac{\tilde{y}&#39;}{\tilde{w}&#39;} = \frac{dx + ey + f}{gx + hy + 1}\]

</p><p>You can verify that this is exactly the same as our first definition
of a homography <a href="#homography">above</a>, just expressed a little bit more
baroquely. Go ahead and make sure, I’ll wait…</p>

<p>Did it check out? Good.</p>

<p>Fine, the two definitions are the same – who cares? Well, it turns
out we just wrote down the the homography using
<a href="https://en.wikipedia.org/wiki/Homogeneous_coordinates">homogeneous coordinates</a>, which establish a beautiful mathematical
correspondence between matrices and a large family of non-linear
transformations like homographies.</p>

<p>Anything you can do with the underlying transformations – such as
composing two of them together – you can do in homogeneous
coordinates with simple operations like matrix multiplication.  And if
the homogeneous representation of a transformation is an
<a href="https://en.wikipedia.org/wiki/Invertible_matrix">invertible matrix</a>, then the parameters of the inverse transformation
are straightforwardly obtained from the matrix inverse! So given our
homography parameters \(a\) through \(h\), if we want to find the
parameters of the inverse homography, all we need to do is compute
\(\mathbf{H}^{-1}\) and grab its elements.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p>



<p>Let’s get back to our perspective recovery problem: we need to
estimate a homography that will equalize the areas of all of the
letters, once they’re all warped through it.  Well, since \(g\) and
\(h\) are the homography parameters that that determine the
correlation between a shape’s position and its projected area, we
<em>should</em> be able to find some setting for them that eliminates the
correlation as much as possible. Therefore, we’ll fix the other six
parameters for now, and just worry about producing the best possible
perspective transformation of the form</p><p>

\[\mathbf{H}_P = 
\left[\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ g &amp; h &amp; 1 \end{array}\right]\]

</p><p>By “best”, we mean the matrix that minimizes the
<a href="https://en.wikipedia.org/wiki/Total_sum_of_squares">total sum of squares</a> of the warped contours’ areas, defined as:</p><p>

\[SS_{total} = \sum_{i=1}^n \left(A_i - \bar{A}\right)^2\]

</p><p>where \(A_i\) is the area of the \(i^\text{th}\) warped contour (of
which there are \(n\) total), and \(\bar{A}\) is the mean of all of
their areas. Minimizing this total sum of squares is akin to minimizing
the <a href="http://mathworld.wolfram.com/SampleVariance.html">sample variance</a> of the contour areas.</p>

<p>How do we accomplish this in practice? We just described an
<a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization problem</a>: find the pair of parameters \((g, h)\) that
minimizes the quantity above. There’s <em>tons</em> of ways to solve
optimization problems, but when I’m in a hurry, I just hand them off
to good old <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code>scipy.optimize.minimize</code></a>. For all intents and purposes,
we can consider it a “black box” that tries lots of different
parameters, iteratively refining them until it finds the best combination.</p>



<p>To evaluate our optimization objective, we’ll need to compute the areas
of a bunch of contours.  OpenCV accomplishes this via a variant of the
<a href="https://en.wikipedia.org/wiki/Shoelace_formula">shoelace formula</a>, processing a contour consisting of \(m\) points in
linear time of \(O(m)\). Despite this apparent efficiency, that’s
actually bad news for us, because <code>scipy.optimize.minimize</code> has to
compute the projected area for \(n\) contours (one for each letter)
<em>every time</em> it wants to evaluate the optimization objective. If each
contour consists of \(m\) points on average, our objective function
would therefore take \(O(mn)\) time to run. To speed things up, we can
replace each contour by a simpler “proxy” shape that is much easier to
reason about.  Here’s our letter contours once more:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_contours_only.png" alt="contours with areas"/></p>

<p>We’ll be replacing them with these <a href="https://en.wikipedia.org/wiki/Ellipse">ellipses</a>:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_ellipses.png" alt="contours with areas"/></p>

<p>In the two images above, each red ellipse has the same area – that
is, it covers the same number of pixels – as the green outlined
letter that it replaces. There’s a few reasons I specifically chose
ellipses as the proxy shape:</p>

<ul>
  <li>
    <p>They’re simple to describe – we can fully specify an ellipse with
just five numbers.</p>
  </li>
  <li>
    <p>We can use them to not only match the area, but also the general
aspect ratio and orientation of a letter (i.e. skinny or round,
horizontal, vertical, or diagonal).</p>
  </li>
  <li>
    <p>Mapping an ellipse through a homography generally 
produces another ellipse.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>
  </li>
</ul>

<p>There are a couple of ways we can parameterize ellipses – let’s start
with the canonical parameters \((x_c, y_c, a, b, \theta)\), where
\((x_c, y_c)\) is the center point, \(a\) and \(b\) are the semi-major
and semi-minor axis lengths, and \(\theta\) is the counterclockwise
rotation of the ellipse from horizontal, as illustrated here:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/ellipse.png" alt="ellipse parameters"/></p>

<p>Given a contour outlining a letter, we can find the “closest” matching
ellipse by choosing these parameters such that the center point and
areas match up. We can also examine the second-order <a href="https://en.wikipedia.org/wiki/Image_moment">shape moments</a>
of the contour to match the letter’s aspect ratio and orientation as
well.</p>

<p>We can also represent an ellipse as the set of \((x, y)\) points that satisfy the 
<a href="https://en.wikipedia.org/wiki/Implicit_function">implicit function</a></p><p>

\[f(x, y) = Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0\]

</p><p>where \(A\) through \(F\) are the parameters of our ellipse.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup> We can
<a href="https://en.wikipedia.org/wiki/Ellipse#General_ellipse">switch back and forth</a> between the two representations without changing
the underlying mathematical object we’re describing – it’s just a
matter of which one is more useful at the moment.</p>

<p>Just like we did with the homography, we can express the implicit
function parameters as elements of a matrix that operates on
homogeneous coordinates. The new function looks like this:</p><p>

\[f(\tilde{\mathbf{p}}) = 
\tilde{\mathbf{p}}^T \mathbf{M} \tilde{\mathbf{p}} =
\left[\begin{array}{ccc} x &amp; y &amp; 1 \end{array}\right]
\left[ \begin{array}{ccc}
A &amp; B/2 &amp; D/2 \\
B/2 &amp; C &amp; E/2 \\
D/2 &amp; E/2 &amp; F \end{array}\right]
\left[\begin{array}{ccc} x \\ y \\ 1 \end{array}\right] = 0\]

</p><p>As it turns out, if we want to map the entire ellipse through some
homography \(\mathbf{H}\) represented as a \(3 \times 3\) matrix in
homogeneous coordinates, we can compute the matrix</p><p>

\[\mathbf{M}&#39; = \mathbf{H}^{-T} \mathbf{M} \mathbf{H}^{-1}\]

</p><p>and then straightforwardly obtain the parameters \(A&#39;\) through \(F&#39;\)
of the the implicit function corresponding to the transformed ellipse
by grabbing them out of the appropriate elements of \(\mathbf{M}&#39;\).<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup></p>

<p>To illustrate how ellipses get mapped through homographies, I created
another Shadertoy.  When the faint rectangle and ellipse at the center
of the display through a homography we see that the rectangle may
transform into an arbitrary quadrilateral; however, the inscribed
ellipse is just transformed into another ellipse. Interestingly, the
center point of the new ellipse (red dot) is not the same as the
transformed center point of the original ellipse (green dot). Once
again, press the play button in the lower left to see the figure
animate.</p>



<p>The bottom line of all this is that since there’s a closed-form
formula for expressing the result of mapping an ellipse through a
perspective transformation, it’s super efficient to model each letter
as an ellipse for the purposes of running our objective function.</p>

<p>Here is what the optimization process looks like as it refines the
warp parameters to improve the objective function:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/ellipse_optimization.gif" alt="optimization in action"/></p>

<p>The post-optimization, distortion-corrected image looks like this:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_prerotate_noline.png" alt="after undistorting"/></p>

<p>You can see we have equalized the letters’ areas quite a bit, just by
considering the action of the homography \(\mathbf{H}_P\) on the
collection of proxy ellipses.</p>



<p>Once the perspective distortion has been removed by finding the
optimal \((g, h)\) parameters of the homography, we need to choose
good values for the remaining parameters. In particular, we are
concerned with the parameters \((a, b, d, e)\), which control
rotation, scale, and skew. We will do this by composing the
perspective transformation \(\mathbf{H}_P\) discovered in the previous
step with two additional transformations: a rotation-only
transformation \(\mathbf{H}_R\), and a skew transformation
\(\mathbf{H}_S\). To find the optimal rotation angle, we will take a
<a href="https://en.wikipedia.org/wiki/Hough_transform">Hough transform</a> of the contours after being mapped through
\(\mathbf{H}_P\). Our input image is a binary mask indicating the edge
pixels:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_edges.png" alt="edge pixels"/></p>

<p>…and here is the corresponding Hough transform:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_hough.png" alt="Hough transform"/></p>

<p>The Hough transform relates every possible <em>line</em> in the input image
to a single <em>pixel</em> in the output image. Lines are parameterized by
their orientation angle \(\theta\) (with 0° being horizontal and
±90° being vertical), as well as their distance \(r\) from
the image origin. In the Hough transform output image, the brightness
of a pixel at \((\theta, r)\) corresponds to the number of edge pixels
detected along the line in the input image with angle \(\theta\) and
distance \(r\) from the origin.</p>

<p>If a particular angle correlates well to the rotation of the text in
the input image, its corresponding column in the Hough transform
should be mostly zero pixels, with a small number of very bright
pixels corresponding to the tops and bottoms of letters along parallel
lines of the same angle.  Conversely, angles which do not correlate
well to the text rotation should have a more or less random spread of
energy over all distances \(r\). To find the optimal rotation angle
\(\theta\), we simply identify the column (highlighted above in blue)
of the Hough transform containing the most zero pixels. We can then
create a <a href="https://en.wikipedia.org/wiki/Rotation_matrix">rotation matrix</a> of the form</p><p>

\[\mathbf{H}_R = \left[ \begin{array}{ccc}
\phantom{-}\cos \theta &amp; \sin \theta &amp; 0 \\
-\sin \theta &amp; \cos \theta &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}\right]\]

</p><p>to rotate the image back by the detected \(\theta\) value. Here is the
resulting image after warping first through \(\mathbf{H}_P\) and then
\(\mathbf{H}_R\):</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_preskew.png" alt="after perspective transform and rotation"/></p>

<p>Finally, taking a cue from Merino-Gracia et al., we create a skew
transformation</p><p>

\[\mathbf{H}_S = \left[ \begin{array}{ccc}

1 &amp; b &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1

\end{array}\right]\]

</p><p>parameterized by a single skew parameter \(b\), that aims to reduce
the width of the letters – this time using the <a href="https://en.wikipedia.org/wiki/Convex_hull">convex hull</a> of each
detected contour as a proxy shape. Instead of minimizing the width of
the widest letter, as Merino-Gracia et al. do, I found that on my
inputs at least, using the <a href="http://www.johndcook.com/blog/2010/01/13/soft-maximum/">soft maximum</a> over hull widths gave nicer
results than a straight-up maximum. Here’s the convex hulls after the
rotation, but before the skew:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_convex_hulls_before.png" alt="convex hulls before"/></p>

<p>And the same convex hulls after discovering the optimal skew with 
<a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html"><code>scipy.optimize.minimize_scalar</code></a>:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example0_convex_hulls_after.png" alt="convex hulls before"/></p>

<p>The final homography is given by composing the transformations we
identified, applied in right-to-left order:<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup></p><p>

\[\mathbf{H}_{final} = \mathbf{H}_S \, \mathbf{H}_R \, \mathbf{H}_P\]

</p>

<p>Here are a couple of other before/after image pairs. Input:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example1_input.png" alt="example 1 input"/></p>

<p>Output:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example1_output.png" alt="example 1 output"/></p>

<p>Input:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example2_input.png" alt="example 2 input"/></p>

<p>Output:</p>

<p><img src="https://mzucker.github.io/images/unproject_text/example2_output.png" alt="example 2 output"/></p>



<p>What started out as an interesting alternative take and/or
simplification of an existing paper’s approach turned into a fun deep
dive into the math underlying homographies and ellipses. I especially
enjoyed producing the visualizations and animations underlying my own
approach. Again, I don’t want to claim that the work I did is
state-of-the-art or even that it’s superior to existing methods like
Merino-Gracia et al. – I just relish the process of wrapping my head
around a technical challenge and carving it up into a sequence of
well-defined optimization problems, as I’ve done in the past with my
<a href="https://mzucker.github.io/2016/08/01/gabor-2.html">image fitting</a> and <a href="https://mzucker.github.io/2016/08/15/page-dewarping.html">page dewarping</a> posts.</p>

<p>I hope you enjoyed scrolling through the blog post as much as I did creating it!
Feel free to check out the code up on <a href="https://github.com/mzucker/unproject_text">github</a>.</p>



  </div>
  <!-- load mathjax -->
  
</article>


      </div>
    </div></div>
  </body>
</html>
