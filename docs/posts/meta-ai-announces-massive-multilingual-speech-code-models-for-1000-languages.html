<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms">Original</a>
    <h1>Meta AI announces Massive Multilingual Speech code,  models for 1000&#43; languages</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">The Massively Multilingual Speech (MMS) project expands speech technology from about 100 languages to over 1,000 by building a single multilingual speech recognition model supporting over 1,100 languages (more than 10 times as many as before), language identification models able to identify over <a href="https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html" rel="nofollow">4,000 languages</a> (40 times more than before), pretrained models supporting over 1,400 languages, and text-to-speech models for over 1,100 languages. Our goal is to make it easier for people to access information and to use devices in their preferred language.</p>
<p dir="auto">You can find details in the paper <a href="https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/" rel="nofollow">Scaling Speech Technology to 1000+ languages</a> and the <a href="https://ai.facebook.com/blog/multilingual-model-speech-recognition/" rel="nofollow">blog post</a>.</p>
<p dir="auto">An overview of the languages covered by MMS can be found <a href="https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html" rel="nofollow">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-pretrained-models" aria-hidden="true" href="#pretrained-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pretrained models</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMS-300M</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/pretraining/base_300m.pt" rel="nofollow">download</a></td>
</tr>
<tr>
<td>MMS-1B</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/pretraining/base_1b.pt" rel="nofollow">download</a></td>
</tr>
</tbody>
</table>
<p dir="auto">Example commands to finetune the pretrained models can be found <a href="https://github.com/fairinternal/fairseq-py/tree/mms_release/examples/wav2vec#fine-tune-a-pre-trained-model-with-ctc">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-finetuned-models" aria-hidden="true" href="#finetuned-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Finetuned models</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-asr" aria-hidden="true" href="#asr"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ASR</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Languages</th>
<th>Dataset</th>
<th>Model</th>
<th>Supported languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMS-1B:FL102</td>
<td>102</td>
<td>FLEURS</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_fl102.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_fl102_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>MMS-1B:L1107</td>
<td>1107</td>
<td>MMS-lab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_l1107.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_l1107_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>MMS-1B-all</td>
<td>1162</td>
<td>MMS-lab + FLEURS </td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_all.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/asr/mms1b_all_langs.html" rel="nofollow">download</a></td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto"><a id="user-content-tts" aria-hidden="true" href="#tts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TTS</h3>
<ol dir="auto">
<li>Download the list of <a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html" rel="nofollow">iso codes</a> of 1107 languages.</li>
<li>Find the iso code of the target language and download the checkpoint. Each folder contains 3 files: <code>G_100000.pth</code>,  <code>config.json</code>, <code>vocab.txt</code>. The <code>G_100000.pth</code> is the generator trained for 100K updates, <code>config.json</code> is the training config, <code>vocab.txt</code> is the vocabulary for the TTS model.</li>
</ol>
<div data-snippet-clipboard-copy-content="# Examples:
wget https://dl.fbaipublicfiles.com/mms/tts/eng.tar.gz # English (eng)
wget https://dl.fbaipublicfiles.com/mms/tts/azj-script_latin.tar.gz # North Azerbaijani (azj-script_latin)"><pre><code># Examples:
wget https://dl.fbaipublicfiles.com/mms/tts/eng.tar.gz # English (eng)
wget https://dl.fbaipublicfiles.com/mms/tts/azj-script_latin.tar.gz # North Azerbaijani (azj-script_latin)
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-lid" aria-hidden="true" href="#lid"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LID</h3>
<table>
<thead>
<tr>
<th># Languages</th>
<th>Dataset</th>
<th>Model</th>
<th>Dictionary</th>
<th>Supported languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>126</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l126/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>256</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l256.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l256/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l256_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>512</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l512.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l512/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l512_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>1024</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l1024.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l1024/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l1024_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>2048</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l2048.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l2048/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l2048_langs.html" rel="nofollow">download</a></td>
</tr>
<tr>
<td>4017</td>
<td>FLEURS + VL + MMS-lab-U + MMS-unlab</td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l4017.pt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/dict/l4017/dict.lang.txt" rel="nofollow">download</a></td>
<td><a href="https://dl.fbaipublicfiles.com/mms/lid/mms1b_l4017_langs.html" rel="nofollow">download</a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto"><a id="user-content-commands-to-run-inference" aria-hidden="true" href="#commands-to-run-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Commands to run inference</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-asr-1" aria-hidden="true" href="#asr-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ASR</h3>
<p dir="auto">Run this command to transcribe one or more audio files:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd /path/to/fairseq-py/
python examples/mms/asr/infer/mms_infer.py --model &#34;/path/to/asr/model&#34; --lang lang_code --audio &#34;/path/to/audio_1.wav&#34; &#34;/path/to/audio_1.wav&#34;"><pre><span>cd</span> /path/to/fairseq-py/
python examples/mms/asr/infer/mms_infer.py --model <span><span>&#34;</span>/path/to/asr/model<span>&#34;</span></span> --lang lang_code --audio <span><span>&#34;</span>/path/to/audio_1.wav<span>&#34;</span></span> <span><span>&#34;</span>/path/to/audio_1.wav<span>&#34;</span></span></pre></div>
<p dir="auto">For more advance configuration and calculate CER/WER, you could prepare manifest folder by creating a folder with this format:</p>
<div data-snippet-clipboard-copy-content="$ ls /path/to/manifest
dev.tsv
dev.wrd
dev.ltr
dev.uid

# dev.tsv each line contains &lt;audio&gt;  &lt;number_of_sample&gt;
$ cat dev.tsv
/
/path/to/audio_1  180000
/path/to/audio_2  200000

$ cat dev.ltr
t h i s | i s | o n e |
t h i s | i s | t w o |

$ cat dev.wrd
this is one
this is two

$ cat dev.uid
audio_1
audio_2"><pre><code>$ ls /path/to/manifest
dev.tsv
dev.wrd
dev.ltr
dev.uid

# dev.tsv each line contains &lt;audio&gt;  &lt;number_of_sample&gt;
$ cat dev.tsv
/
/path/to/audio_1  180000
/path/to/audio_2  200000

$ cat dev.ltr
t h i s | i s | o n e |
t h i s | i s | t w o |

$ cat dev.wrd
this is one
this is two

$ cat dev.uid
audio_1
audio_2
</code></pre></div>
<p dir="auto">Followed by command below:</p>
<div data-snippet-clipboard-copy-content="lang_code=&lt;iso_code&gt;

PYTHONPATH=. PREFIX=INFER HYDRA_FULL_ERROR=1 python examples/speech_recognition/new/infer.py -m --config-dir examples/mms/config/ --config-name infer_common decoding.type=viterbi dataset.max_tokens=4000000 distributed_training.distributed_world_size=1 &#34;common_eval.path=&#39;/path/to/asr/model&#39;&#34; task.data=&#39;/path/to/manifest&#39; dataset.gen_subset=&#34;${lang_code}:dev&#34; common_eval.post_process=letter
"><pre><code>lang_code=&lt;iso_code&gt;

PYTHONPATH=. PREFIX=INFER HYDRA_FULL_ERROR=1 python examples/speech_recognition/new/infer.py -m --config-dir examples/mms/config/ --config-name infer_common decoding.type=viterbi dataset.max_tokens=4000000 distributed_training.distributed_world_size=1 &#34;common_eval.path=&#39;/path/to/asr/model&#39;&#34; task.data=&#39;/path/to/manifest&#39; dataset.gen_subset=&#34;${lang_code}:dev&#34; common_eval.post_process=letter

</code></pre></div>
<p dir="auto">Available options:</p>
<ul dir="auto">
<li>
<p dir="auto">To get the raw character-based output, user can change to <code>common_eval.post_process=none</code></p>
</li>
<li>
<p dir="auto">To maximize GPU efficiency or avoid out-of-memory (OOM), user can tune <code>dataset.max_tokens=???</code> size</p>
</li>
<li>
<p dir="auto">To run language model decoding, install flashlight python bindings using</p>
<div data-snippet-clipboard-copy-content="git clone --recursive git@github.com:flashlight/flashlight.git
cd flashlight; 
git checkout 035ead6efefb82b47c8c2e643603e87d38850076 
cd bindings/python 
python3 setup.py install"><pre><code>git clone --recursive git@github.com:flashlight/flashlight.git
cd flashlight; 
git checkout 035ead6efefb82b47c8c2e643603e87d38850076 
cd bindings/python 
python3 setup.py install
</code></pre></div>
<p dir="auto">Train a <a href="https://github.com/flashlight/wav2letter/tree/main/recipes/rasr#language-model">KenLM language model</a> and prepare a lexicon file in <a href="https://dl.fbaipublicfiles.com/wav2letter/rasr/tutorial/lexicon.txt" rel="nofollow">this</a> format.</p>
<div data-snippet-clipboard-copy-content=" LANG=&lt;iso&gt; # for example - &#39;eng&#39;, &#39;azj-script_latin&#39;
 PYTHONPATH=. PREFIX=INFER HYDRA_FULL_ERROR=1  python examples/speech_recognition/new/infer.py  --config-dir=examples/mms/asr/config \
    --config-name=infer_common decoding.type=kenlm  distributed_training.distributed_world_size=1  \ 
    decoding.unique_wer_file=true   decoding.beam=500 decoding.beamsizetoken=50  \
    task.data=&lt;MANIFEST_FOLDER_PATH&gt;   common_eval.path=&#39;&lt;MODEL_PATH.pt&gt;&#39; decoding.lexicon=&lt;LEXICON_FILE&gt; decoding.lmpath=&lt;LM_FILE&gt; \  
    decoding.results_path=&lt;OUTPUT_DIR&gt; dataset.gen_subset=${LANG}:dev decoding.lmweight=??? decoding.wordscore=???"><pre><code> LANG=&lt;iso&gt; # for example - &#39;eng&#39;, &#39;azj-script_latin&#39;
 PYTHONPATH=. PREFIX=INFER HYDRA_FULL_ERROR=1  python examples/speech_recognition/new/infer.py  --config-dir=examples/mms/asr/config \
    --config-name=infer_common decoding.type=kenlm  distributed_training.distributed_world_size=1  \ 
    decoding.unique_wer_file=true   decoding.beam=500 decoding.beamsizetoken=50  \
    task.data=&lt;MANIFEST_FOLDER_PATH&gt;   common_eval.path=&#39;&lt;MODEL_PATH.pt&gt;&#39; decoding.lexicon=&lt;LEXICON_FILE&gt; decoding.lmpath=&lt;LM_FILE&gt; \  
    decoding.results_path=&lt;OUTPUT_DIR&gt; dataset.gen_subset=${LANG}:dev decoding.lmweight=??? decoding.wordscore=???
</code></pre></div>
<p dir="auto">We typically sweep <code>lmweight</code> in the range of 0 to 5 and <code>wordscore</code> in the range of -3 to 3.  The output directory will contain the reference and hypothesis outputs from decoder.</p>
<p dir="auto">For decoding with character-based language models, use empty lexicon file (<code>decoding.lexicon=</code>), <code>decoding.unitlm=True</code> and sweep over <code>decoding.silweight</code> instead of <code>wordscore</code>.</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-tts-1" aria-hidden="true" href="#tts-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TTS</h3>
<p dir="auto">Note: clone and install <a href="https://github.com/jaywalnut310/vits">VITS</a> before running inference.</p>
<div dir="auto" data-snippet-clipboard-copy-content="## English TTS
$ PYTHONPATH=$PYTHONPATH:/path/to/vits python examples/mms/tts/infer.py --model-dir /path/to/model/eng \
--wav ./example.wav --txt &#34;Expanding the language coverage of speech technology \
has the potential to improve access to information for many more people&#34;

## Maithili TTS
$ PYTHONPATH=$PYTHONPATH:/path/to/vits python examples/mms/tts/infer.py --model-dir /path/to/model/mai \
--wav ./example.wav --txt &#34;मुदा आइ धरि ई तकनीक सौ सं किछु बेसी भाषा तक सीमित छल जे सात हजार \ 
सं बेसी ज्ञात भाषाक एकटा अंश अछी&#34;"><pre><span><span>#</span># English TTS</span>
$ PYTHONPATH=<span>$PYTHONPATH</span>:/path/to/vits python examples/mms/tts/infer.py --model-dir /path/to/model/eng \
--wav ./example.wav --txt <span><span>&#34;</span>Expanding the language coverage of speech technology <span>\</span></span>
<span>has the potential to improve access to information for many more people<span>&#34;</span></span>

<span><span>#</span># Maithili TTS</span>
$ PYTHONPATH=<span>$PYTHONPATH</span>:/path/to/vits python examples/mms/tts/infer.py --model-dir /path/to/model/mai \
--wav ./example.wav --txt <span><span>&#34;</span>मुदा आइ धरि ई तकनीक सौ सं किछु बेसी भाषा तक सीमित छल जे सात हजार \ </span>
<span>सं बेसी ज्ञात भाषाक एकटा अंश अछी<span>&#34;</span></span></pre></div>
<p dir="auto"><code>example.wav</code> contains synthesized audio for the language.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-lid-1" aria-hidden="true" href="#lid-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LID</h3>
<p dir="auto">Prepare two files in this format</p>
<div data-snippet-clipboard-copy-content="#/path/to/manifest.tsv
/
/path/to/audio1.wav
/path/to/audio2.wav
/path/to/audio3.wav

# /path/to/manifest.lang
eng 1
eng 1
eng 1"><pre><code>#/path/to/manifest.tsv
/
/path/to/audio1.wav
/path/to/audio2.wav
/path/to/audio3.wav

# /path/to/manifest.lang
eng 1
eng 1
eng 1
</code></pre></div>
<p dir="auto">Download model and the corresponding dictionary file for the LID model.
Use the following command to run inference -</p>
<div dir="auto" data-snippet-clipboard-copy-content="$  PYTHONPATH=&#39;.&#39;  python3  examples/mms/lid/infer.py /path/to/dict/l126/ --path /path/to/models/mms1b_l126.pt \
  --task audio_classification  --infer-manifest /path/to/manifest.tsv --output-path &lt;OUTDIR&gt;"><pre>$  PYTHONPATH=<span><span>&#39;</span>.<span>&#39;</span></span>  python3  examples/mms/lid/infer.py /path/to/dict/l126/ --path /path/to/models/mms1b_l126.pt \
  --task audio_classification  --infer-manifest /path/to/manifest.tsv --output-path <span>&lt;</span>OUTDIR<span>&gt;</span></pre></div>
<p dir="auto">The above command assumes there is a file named <code>dict.lang.txt</code> in <code>/path/to/dict/l126/</code>. <code>&lt;OUTDIR&gt;/predictions.txt</code> will contain the predictions from the model for the audio files in <code>manifest.tsv</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-forced-alignment-tooling" aria-hidden="true" href="#forced-alignment-tooling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Forced Alignment Tooling</h2>
<p dir="auto">We also developed an efficient forced alignment algorithm implemented on GPU which is able to process very long audio files. This algorithm is open sourced and we provide instructions on how to use it <a href="https://blog.jonnew.com/facebookresearch/fairseq/blob/main/examples/mms/data_prep">here</a>. We also open source a multilingual alignment model trained on 31K hours of data in 1,130 languages, as well as text normalization scripts.</p>

<p dir="auto">The MMS code and model weights are released under the CC-BY-NC 4.0 license.</p>

<p dir="auto"><strong>BibTeX:</strong></p>
<div data-snippet-clipboard-copy-content="@article{pratap2023mms,
  title={Scaling Speech Technology to 1,000+ Languages},
  author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},
  journal={arXiv},
  year={2023}
}
"><pre><code>@article{pratap2023mms,
  title={Scaling Speech Technology to 1,000+ Languages},
  author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},
  journal={arXiv},
  year={2023}
}

</code></pre></div>
</article>
          </div></div>
  </body>
</html>
