<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/LMCache/LMCache">Original</a>
    <h1>Lossless LLM 3x Throughput Increase by LMCache</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/3913558/460239059-50c58c75-f37a-45e8-bf82-793439480f0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTExMzU0MDksIm5iZiI6MTc1MTEzNTEwOSwicGF0aCI6Ii8zOTEzNTU4LzQ2MDIzOTA1OS01MGM1OGM3NS1mMzdhLTQ1ZTgtYmY4Mi03OTM0Mzk0ODBmMGYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MjhUMTgyNTA5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI1YmEzY2YyMDAwMGNlM2E1OTZhMmJhNTQzMWUxOTRlODczMzdkYzQzYWJjOGE4ZmEyOTNhMjZlOTc2MTVlZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.jrsMH_KGRu4vnrOY-els-05P3gqjsAqQWhFetGPmOr4"><img src="https://private-user-images.githubusercontent.com/3913558/460239059-50c58c75-f37a-45e8-bf82-793439480f0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTExMzU0MDksIm5iZiI6MTc1MTEzNTEwOSwicGF0aCI6Ii8zOTEzNTU4LzQ2MDIzOTA1OS01MGM1OGM3NS1mMzdhLTQ1ZTgtYmY4Mi03OTM0Mzk0ODBmMGYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MjhUMTgyNTA5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI1YmEzY2YyMDAwMGNlM2E1OTZhMmJhNTQzMWUxOTRlODczMzdkYzQzYWJjOGE4ZmEyOTNhMjZlOTc2MTVlZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.jrsMH_KGRu4vnrOY-els-05P3gqjsAqQWhFetGPmOr4" width="720" alt="lmcache logo"/></a>

</p>
<p dir="auto">
  <a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ" rel="nofollow">
    <img height="40" alt="Join Slack" src="https://camo.githubusercontent.com/254cacb2e9f33dfb83351d52242a54da298711fb32b84bd5ec99d3f8ed837af6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c4d43616368652d4a6f696e253230536c61636b2d626c75653f6c6f676f3d736c61636b" data-canonical-src="https://img.shields.io/badge/LMCache-Join%20Slack-blue?logo=slack"/>
  </a>
  <a href="https://docs.lmcache.ai/" rel="nofollow">
    <img height="40" alt="Documentation" src="https://camo.githubusercontent.com/222f2aeaccf413b620e0a401cc73d002517988edaf18f1fe4006a94199ae3b2c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d626c75653f6c6f676f3d72656164746865646f6373266c6f676f436f6c6f723d663066386666" data-canonical-src="https://img.shields.io/badge/docs-blue?logo=readthedocs&amp;logoColor=f0f8ff"/>
  </a>
</p>
<p dir="auto">
  <a href="https://deepwiki.com/LMCache/LMCache" rel="nofollow">
    <img height="30" src="https://camo.githubusercontent.com/e7d4bb1a32530e373bb53fbe8eea825440ad27c7531d8f144d561acdd20c093a/68747470733a2f2f6465657077696b692e636f6d2f62616467652e737667" alt="Ask DeepWiki" data-canonical-src="https://deepwiki.com/badge.svg"/>
  </a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8ce9681df2833cc6e67a050d94dcbdade581bf76e4227057b46f816bf766606c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f772f4c4d43616368652f4c4d4361636865"><img height="30" alt="GitHub commit activity" src="https://camo.githubusercontent.com/8ce9681df2833cc6e67a050d94dcbdade581bf76e4227057b46f816bf766606c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f772f4c4d43616368652f4c4d4361636865" data-canonical-src="https://img.shields.io/github/commit-activity/w/LMCache/LMCache"/></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ecf742efb3e318188ed0046c1fe5307b3017ecd72b5adf8efdb72a5237b44a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f4c4d4361636865"><img height="30" alt="PyPI - Downloads" src="https://camo.githubusercontent.com/4ecf742efb3e318188ed0046c1fe5307b3017ecd72b5adf8efdb72a5237b44a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f4c4d4361636865" data-canonical-src="https://img.shields.io/pypi/dm/LMCache"/></a>
  <a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA" rel="nofollow">
    <img height="30" alt="YouTube Channel Views" src="https://camo.githubusercontent.com/8435a420c23950d4a974cdbd392454e375b26ca13d1e4b9a194082069d22a52c/68747470733a2f2f696d672e736869656c64732e696f2f796f75747562652f6368616e6e656c2f76696577732f554335387a4d7a35356e373072746631416b3250554c4a41" data-canonical-src="https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA"/>
  </a>
</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
    Redis for LLMs - Infinite and Ultra-Fast
</h3><a id="user-content-----redis-for-llms---infinite-and-ultra-fast" aria-label="Permalink: 
    Redis for LLMs - Infinite and Ultra-Fast
" href="#----redis-for-llms---infinite-and-ultra-fast"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<hr/>
<p dir="auto">LMCache is an <strong>LLM</strong> serving engine extension to <strong>reduce TTFT</strong> and <strong>increase throughput</strong>, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of <strong><em>any</em></strong> reused text (not necessarily prefix) in <strong><em>any</em></strong> serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.</p>
<p dir="auto">By combining LMCache with vLLM, LMCache achieves 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.</p>
<p dir="auto">Try LMCache with pre-built vllm docker images <a href="https://docs.lmcache.ai/developer_guide/docker_file.html" rel="nofollow">here</a>.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/118159393/454068318-86137f17-f216-41a0-96a7-e537764f7a4c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTExMzU0MDksIm5iZiI6MTc1MTEzNTEwOSwicGF0aCI6Ii8xMTgxNTkzOTMvNDU0MDY4MzE4LTg2MTM3ZjE3LWYyMTYtNDFhMC05NmE3LWU1Mzc3NjRmN2E0Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjI4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYyOFQxODI1MDlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05YTdiMTM1NWNlMTdlNzhjNGI3YTMwOWJlZDY2YjAzYTBjZjgwYWQ4NDczZDYwZTJiYjBlZTE5YWQwYzU0M2Q4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.7hduNx8HytSmWxig9Axq_VMYu-iBGw4Q9eQGpn9VB8U"><img src="https://private-user-images.githubusercontent.com/118159393/454068318-86137f17-f216-41a0-96a7-e537764f7a4c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTExMzU0MDksIm5iZiI6MTc1MTEzNTEwOSwicGF0aCI6Ii8xMTgxNTkzOTMvNDU0MDY4MzE4LTg2MTM3ZjE3LWYyMTYtNDFhMC05NmE3LWU1Mzc3NjRmN2E0Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjI4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYyOFQxODI1MDlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05YTdiMTM1NWNlMTdlNzhjNGI3YTMwOWJlZDY2YjAzYTBjZjgwYWQ4NDczZDYwZTJiYjBlZTE5YWQwYzU0M2Q4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.7hduNx8HytSmWxig9Axq_VMYu-iBGw4Q9eQGpn9VB8U" alt="performance"/></a></p>

<p dir="auto">Please refer to our detailed documentation for <a href="https://docs.lmcache.ai/getting_started/installation.html#install-from-source-v1" rel="nofollow">LMCache V1</a> and <a href="https://docs.lmcache.ai/getting_started/installation.html#install-from-source-v0" rel="nofollow">LMCache V0</a></p>

<p dir="auto">Fill out the <a href="https://forms.gle/mQfQDUXbKfp2St1z7" rel="nofollow">interest form</a>, <a href="https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter" rel="nofollow">sign up for our newsletter</a>, or <a href="https://github.com/LMCache/LMCache/blob/dev/contact@lmcache.ai">drop an email</a>, and our team will reach out to you!</p>

<ul>
<li> LMCache V1 with vLLM integration with following features is live ðŸ”¥
<ul dir="auto">
<li>High performance CPU KVCache offloading</li>
<li>Disaggregated prefill</li>
<li>P2P KVCache sharing</li>
</ul>
</li>
<li> LMCache is supported in the <a href="https://github.com/vllm-project/production-stack/tree/main">vLLM production stack ecosystem</a></li>
<li> User and developer documentation</li>
<li> Stable support for non-prefix KV caches</li>
<li> Support installation through pip install and integrate with latest vLLM</li>
<li> First release of LMCache</li>
</ul>

<p dir="auto">Our latest <a href="https://lmcache.github.io" rel="nofollow">blog posts</a> and the <a href="https://docs.lmcache.ai/" rel="nofollow">documentation</a> pages are available online</p>

<p dir="auto">The community meeting for LMCache is hosted weekly.
Meeting Details:</p>
<ul dir="auto">
<li>
<p dir="auto">Tuesdays at 9:00 AM PT â€“ <a href="https://drive.google.com/file/d/15Xz8-LtpBQ5QgR7KrorOOyfuohCFQmwn/view?usp=drive_link" rel="nofollow">Add to Calendar</a></p>
</li>
<li>
<p dir="auto">Tuesdays at 6:30 PM PT â€“ <a href="https://drive.google.com/file/d/1WMZNFXV24kWzprDjvO-jQ7mOY7whqEdG/view?usp=drive_link" rel="nofollow">Add to Calendar</a></p>
</li>
</ul>
<p dir="auto">Meetings <strong>alternate weekly</strong> between the two times. All are welcome to join!</p>

<p dir="auto">We welcome and value any contributions and collaborations.  Please check out <a href="https://github.com/LMCache/LMCache/blob/dev/CONTRIBUTING.md">CONTRIBUTING.md</a> for how to get involved.</p>

<p dir="auto">If you use LMCache for your research, please cite our papers:</p>
<div data-snippet-clipboard-copy-content="@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}"><pre><code>@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}
</code></pre></div>

<p dir="auto">This project is licensed under Apache License 2.0. See the <a href="https://github.com/LMCache/LMCache/blob/dev/LICENSE">LICENSE</a> file for details.</p>
</article></div></div>
  </body>
</html>
