<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vram.asmirnov.xyz/">Original</a>
    <h1>Show HN: I made a GPU VRAM calculator for transformer-based models</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><h4>VRAM Estimator</h4><h6>Estimate GPU VRAM usage of transformer-based models.</h6></p><div><div><h5>Running Parameters</h5><div><p>Precision: </p><p><span>mixed</span></p><p><span>full (fp32)</span></p></div><div><p>Optimizer: </p><p><span>Adam</span></p><p><span>SGD</span></p><p><label><span><svg focusable="false" aria-hidden="true" viewBox="0 0 24 24" data-testid="CheckBoxIcon"><path d="M19 3H5c-1.11 0-2 .9-2 2v14c0 1.1.89 2 2 2h14c1.11 0 2-.9 2-2V5c0-1.1-.89-2-2-2zm-9 14l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"></path></svg></span><span>momentum</span></label></p></div><h5>Model Parameters</h5><p>Model Parameters could be taken from <code>config.json</code> on HuggingFace or directly from model via<!-- --> <code>model.config</code></p><div><p><label data-shrink="true" for=":Rr9anla:" id=":Rr9anla:-label">Number of Parameters (billions)</label></p><div><fieldset aria-hidden="true"><legend><span>Number of Parameters (billions)</span></legend></fieldset></div></div><div><div><div><p><label data-shrink="true" for=":R2kv9anla:" id=":R2kv9anla:-label">Number of Attention Heads</label></p><div><fieldset aria-hidden="true"><legend><span>Number of Attention Heads</span></legend></fieldset></div></div></div></div><div><div><div><p><label data-shrink="true" for=":R2d19anla:" id=":R2d19anla:-label">Intermediate Size</label></p><p id=":R2d19anla:-helper-text">Expanding dimensionality within MLP block. Usually it is 4 × hidden size.</p></div></div><div><div><p><label data-shrink="true" for=":R2l19anla:" id=":R2l19anla:-label">Number of Key Value Heads</label></p><div><fieldset aria-hidden="true"><legend><span>Number of Key Value Heads</span></legend></fieldset></div><p id=":R2l19anla:-helper-text">Might be different from number of attention heads when using Grouped Query Attention</p></div></div></div></div></div><div><div><h5>Estimation Result</h5><ul><li><div><p>Total VRAM usage is<!-- --> <b>27836<!-- --> </b> <!-- -->MiB<!-- --> </p></div></li><li><div><p><span><span><span>CUDA Kernels</span> use<span> <!-- -->1000<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>When PyTorch uses CUDA for the first time, it allocates between 300 MiB and 2 GiB of VRAM</p></div></li><li><div><p><span><span><span>Parameters</span> use<span> <!-- -->8114<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>Number of Parameters (1.418 billion) × number of bytes per parameter (6; parameters are stored in both full precision and half precision) </p></div></li><li><div><p><span><span><span>Activations</span> use<span> <!-- -->7104<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>Sum of sizes of all intermediate tensors during forward pass across all 24 layers. Activations size have quadratic dependence on Sequence Length.</p></div></li><li><div><p><span><span><span>Gradients</span> use<span> <!-- -->5409<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>Gradient is stored for each parameter in full precision, so it is Number of Parameters (1.418 billion) × number of bytes per parameter (4) </p></div></li><li><div><p><span><span><span>First Moments</span> use<span> <!-- -->5409<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>Optimizer stores moving average of gradients for each parameter in full precision, so it is Number of Parameters (1.418 billion) × number of bytes per parameter (4) </p></div></li><li><div><p><span><span><span>Output tensor</span> uses<span> <!-- -->800<!-- --> </span>MiB<!-- --> of VRAM </span></span></p><p>Batch Size (4) × Sequence Length (512) × Vocabulary Size (51200) × number of bytes per parameter (4) × 2 (storing probabilities after softmax output which are the same size as output)</p></div></li></ul></div></div><hr/><h6>While estimates might not be completely precise, they reflect my current understanding of the topic. For an in-depth explanation and the logic behind these numbers, feel free to check out my<!-- --> <a href="https://asmirnov.xyz/vram" rel="noreferrer" target="_blank">detailed post</a> <!-- -->and the<!-- --> <a href="https://github.com/furiousteabag/vram-calculator/blob/main/app/_lib/index.ts" rel="noreferrer" target="_blank">calculation code</a> <!-- -->in the source repo. If you feel something is wrong please reach out via email<!-- --> <a href="mailto:alex@asmirnov.xyz">alex@asmirnov.xyz</a> or create an issue/PR in the repo. Cheers!</h6></div></div></div>
  </body>
</html>
