<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/">Original</a>
    <h1>Beyond OpenMP in C&#43;&#43; and Rust: Taskflow, Rayon, Fork Union</h1>
    
    <div id="readability-page-1" class="page"><div><blockquote><p>TL;DR: Most C++ and Rust thread-pool libraries leave significant performance on the table - often running 10× slower than <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> on classic fork-join workloads and <a href="https://github.com/ashvardanian/ParallelReductionsBenchmark">micro-benchmarks</a>.
So I’ve drafted a minimal ~300-line library called <a href="https://github.com/ashvardanian/fork_union">Fork Union</a> that lands within 20% of OpenMP.
It does not use advanced <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> tricks; it uses only the C++ and Rust standard libraries and has no other dependencies.</p><p>Update (Sep 2025): Since the <a href="https://github.com/ashvardanian/fork_union/releases/tag/v2.0.0">v2 release</a>, Fork Union supports NUMA and Huge Pages, as well as <code>tpause</code>, <code>wfet</code>, and other “pro” features.
Check the <a href="https://github.com/ashvardanian/fork_union?tab=readme-ov-file#pro-tips">README for details</a>.</p></blockquote><p><a href="https://github.com/ashvardanian/fork_union"><img alt="Fork Union for Rust and C++" loading="lazy" src="https://liquidbrain.net/beyond-openmp-in-cpp-rust/fork_union.jpg"/></a></p><p><a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> has been the industry workhorse for coarse-grain parallelism in C and C++ for decades.
I lean on it heavily in projects like <a href="https://github.com/unum-cloud/usearch">USearch</a>, yet I avoid it in larger systems because:</p><ul><li><strong>Fine-grain parallelism</strong> with independent subsystems doesn’t map cleanly to OpenMP’s global runtime.</li><li><strong>Portability</strong> of the C++ STL and the Rust standard library is better than OpenMP.</li><li><strong>Meta-programming</strong> with OpenMP is a pain - mixing <code>#pragma omp</code> with templates quickly becomes unmaintainable.</li></ul><p>So I went looking for ready-made thread pools in C++ and Rust — only to realize <strong>most of them implement asynchronous task queues, a much heavier abstraction than OpenMP’s fork-join model</strong>.
Those extra layers introduce what I call the four horsemen of low performance:</p><ol><li><a href="#locks-and-mutexes">Locks &amp; mutexes</a> with syscalls in the hot path.</li><li><a href="#memory-allocations">Heap allocations</a> in queues, tasks, futures, and promises.</li><li><a href="#atomics-and-cas">Compare-and-swap</a> (CAS) stalls in the pessimistic path.</li><li><a href="#alignment">False sharing</a> unaligned counters thrashing cache lines.</li></ol><p>With today’s dual-socket AWS machines pushing 192 physical cores, I needed something leaner than <a href="https://github.com/taskflow/taskflow/">Taskflow</a>, <a href="https://github.com/rayon-rs/rayon/">Rayon</a>, or <a href="https://github.com/tokio-rs/tokio/">Tokio</a>.
Enter <a href="https://github.com/ashvardanian/fork_union">Fork Union</a>.</p><h2 id="benchmarks">Benchmarks</h2><p>Hardware: AWS Graviton 4 metal (single NUMA node, 96× Arm v9 cores, 1 thread/core).
Workload: <a href="https://github.com/ashvardanian/ParallelReductionsBenchmark">“ParallelReductionsBenchmark”</a> - summing single-precision floats in parallel.
In this case, just one cache line (<code>float[16]</code>) per core—small enough to stress synchronization cost of the thread pool rather than arithmetic throughput of the CPU.
In other words, we are benchmarking kernels similar to:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-0-1"><a href="#hl-0-1">1</a>
</span><span id="hl-0-2"><a href="#hl-0-2">2</a>
</span><span id="hl-0-3"><a href="#hl-0-3">3</a>
</span><span id="hl-0-4"><a href="#hl-0-4">4</a>
</span><span id="hl-0-5"><a href="#hl-0-5">5</a>
</span><span id="hl-0-6"><a href="#hl-0-6">6</a>
</span><span id="hl-0-7"><a href="#hl-0-7">7</a>
</span><span id="hl-0-8"><a href="#hl-0-8">8</a>
</span><span id="hl-0-9"><a href="#hl-0-9">9</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;array&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>float</span> <span>parallel_sum</span><span>(</span><span>std</span><span>::</span><span>array</span><span>&lt;</span><span>float</span><span>,</span> <span>96</span> <span>*</span> <span>16</span><span>&gt;</span> <span>const</span> <span>&amp;</span><span>data</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> <span>result</span> <span>=</span> <span>0.0f</span><span>;</span>
</span></span><span><span><span>#pragma omp parallel for reduction(+:result) </span><span>// Not how we profile OpenMP
</span></span></span><span><span><span></span>    <span>for</span> <span>(</span><span>std</span><span>::</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>data</span><span>.</span><span>size</span><span>();</span> <span>++</span><span>i</span><span>)</span>
</span></span><span><span>        <span>result</span> <span>+=</span> <span>data</span><span>[</span><span>i</span><span>];</span>
</span></span><span><span>    <span>return</span> <span>result</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p><a href="https://github.com/google/benchmark">Google Benchmark</a> numbers for the C++ version of Fork Union, compared to OpenMP, <a href="https://github.com/taskflow/taskflow/">Taskflow</a>, and allocating 96× <code>std::thread</code> objects on-demand, are as follows:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-1-1"><a href="#hl-1-1">1</a>
</span><span id="hl-1-2"><a href="#hl-1-2">2</a>
</span><span id="hl-1-3"><a href="#hl-1-3">3</a>
</span><span id="hl-1-4"><a href="#hl-1-4">4</a>
</span><span id="hl-1-5"><a href="#hl-1-5">5</a>
</span><span id="hl-1-6"><a href="#hl-1-6">6</a>
</span><span id="hl-1-7"><a href="#hl-1-7">7</a>
</span><span id="hl-1-8"><a href="#hl-1-8">8</a>
</span><span id="hl-1-9"><a href="#hl-1-9">9</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span><span>PARALLEL_REDUCTIONS_LENGTH</span><span>=</span><span>1536</span> build_release/reduce_bench
</span></span><span><span>
</span></span><span><span>-----------------------------------
</span></span><span><span>Benchmark           UserCounters...
</span></span><span><span>-----------------------------------
</span></span><span><span>std::threads        bytes/s<span>=</span>3.00106 MB/s
</span></span><span><span>tf::taskflow        bytes/s<span>=</span>76.2837 MB/s
</span></span><span><span>av::fork_union      bytes/s<span>=</span>467.714 MB/s
</span></span><span><span>openmp              bytes/s<span>=</span>585.492 MB/s
</span></span></code></pre></td></tr></tbody></table></div></div><blockquote><p>I’ve cleaned up the output, focusing only on the relevant rows and the reduction throughput.</p></blockquote><p><a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> numbers for the Rust version of Fork Union, compared to <a href="https://github.com/rayon-rs/rayon/">Rayon</a>, <a href="https://github.com/tokio-rs/tokio/">Tokio</a>, and Smol’s <a href="https://github.com/smol-rs/async-executor">Async Executors</a>, are as follows:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-2-1"><a href="#hl-2-1">1</a>
</span><span id="hl-2-2"><a href="#hl-2-2">2</a>
</span><span id="hl-2-3"><a href="#hl-2-3">3</a>
</span><span id="hl-2-4"><a href="#hl-2-4">4</a>
</span><span id="hl-2-5"><a href="#hl-2-5">5</a>
</span><span id="hl-2-6"><a href="#hl-2-6">6</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>$ <span>PARALLEL_REDUCTIONS_LENGTH</span><span>=</span><span>1536</span> cargo +nightly bench -- --output-format bencher
</span></span><span><span>
</span></span><span><span><span>test</span> fork_union ... bench:  5,150 ns/iter <span>(</span>+/- 402<span>)</span>
</span></span><span><span><span>test</span> rayon ... bench:      47,251 ns/iter <span>(</span>+/- 3,985<span>)</span>
</span></span><span><span><span>test</span> smol ... bench:       54,931 ns/iter <span>(</span>+/- 10<span>)</span>
</span></span><span><span><span>test</span> tokio ... bench:     240,707 ns/iter <span>(</span>+/- 921<span>)</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>The timing methods used in those two executables are different, but the relative observations should hold.</p><ul><li>Spawning new threads is obviously too expensive.</li><li>Most reusable thread pools are still 10x slower to sync than OpenMP.</li><li>OpenMP isn’t easy to compete with and still outperforms Fork Union by 20%.</li></ul><p>This clearly shows, how important it is to chose the right tool for the job.
Don’t pick an asynchronous task pool for a fork-join blocking workload!</p><h2 id="four-horsemen-of-performance">Four Horsemen of Performance</h2><blockquote><p>This article won’t be a deep dive into those topics.
Each deserves its own article and a proper benchmark, with some good ones already available and linked.</p></blockquote><h3 id="locks-and-mutexes">Locks and Mutexes</h3><p>Unlike the <a href="https://en.cppreference.com/w/cpp/atomic/atomic"><code>std::atomic</code></a>, the <a href="https://en.cppreference.com/w/cpp/thread/mutex"><code>std::mutex</code></a> update may result in a system call, and it can be expensive to acquire and release.
Its implementations generally have 2 executable paths:</p><ul><li>the fast path, where the mutex is not contended, where it first tries to grab the mutex via a compare-and-swap operation, and if it succeeds, it returns immediately.</li><li>the slow path, where the mutex is contended, and it has to go through the kernel to block the thread until the mutex is available.</li></ul><p>On Linux, the latter translates to a <a href="https://en.wikipedia.org/wiki/Futex">“futex” syscall</a> and an expensive <a href="https://lwn.net/Articles/940944/#:~:text=shared%20between%20at%20least%20two,system%20call">context switch</a>.
In Rust, the same applies to <a href="https://doc.rust-lang.org/std/sync/atomic/"><code>std::async::atomic</code></a> and <a href="https://doc.rust-lang.org/std/sync/struct.Mutex.html"><code>std::sync::Mutex</code></a>.
Prefer the former when possible.</p><h3 id="memory-allocations">Memory Allocations</h3><p>Most thread-pools use classes like <a href="https://en.cppreference.com/w/cpp/thread/future"><code>std::future</code></a>, <a href="https://en.cppreference.com/w/cpp/thread/packaged_task"><code>std::packaged_task</code></a>, <a href="https://en.cppreference.com/w/cpp/utility/functional/function"><code>std::function</code></a>, <a href="https://en.cppreference.com/w/cpp/container/queue"><code>std::queue</code></a>, <a href="https://en.cppreference.com/w/cpp/thread/condition_variable"><code>std::conditional_variable</code></a>.</p><blockquote><p>In Rust land, there will often be a <a href="https://doc.rust-lang.org/std/boxed/struct.Box.html"><code>std::Box</code></a>, <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>std::Arc</code></a>, <a href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html"><code>std::collections::VecDeque</code></a>, <a href="https://doc.rust-lang.org/std/sync/mpsc/index.html"><code>std::sync::mpsc</code></a> or even <a href="https://doc.rust-lang.org/std/sync/mpmc/index.html"><code>std::sync::mpmc</code></a>.</p></blockquote><p>Most of those, I believe, aren’t unusable in Big-Data applications, where you always operate in memory-constrained environments:</p><ul><li>Raising a <a href="https://en.cppreference.com/w/cpp/memory/new/bad_alloc"><code>std::bad_alloc</code></a> exception when there is no memory left and just hoping that someone up the call stack will catch it is not a great design idea for Systems Engineering.</li><li>The threat of having to synchronize ~200 physical CPU cores across 2-8 sockets and potentially dozens of NUMA nodes around a shared global memory allocator practically means you can’t have predictable performance.</li></ul><p>As we focus on a simpler <del>concurrency</del> parallelism model, we can avoid the complexity of allocating shared states, wrapping callbacks into some heap-allocated “tasks”, and a lot of other boilerplates.</p><p>Less work = more performance.</p><h3 id="atomics-and-cas">Atomics and CAS</h3><p>Once you get to the lowest-level primitives on concurrency, you end up with the <code>std::atomic</code> and a small set of hardware-supported atomic instructions.
Hardware implements it differently:</p><ul><li>x86 is built around the “Total Store Order” (TSO) <a href="https://en.wikipedia.org/wiki/Memory_ordering">memory consistency model</a> and provides <code>LOCK</code> variants of the <code>ADD</code> and <code>CMPXCHG</code>. These variants act as full-blown “fences” — no loads or stores can be reordered across them. This makes atomic operations on x86 straightforward but heavyweight.</li><li>Arm, on the other hand, has a “weak” memory model and provides a set of atomic instructions that are not fenced and match the C++ concurrency model. It offers <code>acquire</code>, <code>release</code>, and <code>acq_rel</code> variants of each atomic instruction — such as <code>LDADD</code>, <code>STADD</code>, and <code>CAS</code> — which allow precise control over visibility and order, especially with the introduction of <a href="https://learn.arm.com/learning-paths/servers-and-cloud-computing/lse/intro/">“Large System Extension” (LSE)</a> instructions in Armv8.1-A.</li></ul><p>A locked atomic on x86 requires the cache line in the Exclusive state in the requester’s L1 cache.
This would incur a coherence transaction (Read-for-Ownership) if another core had the line.
Both Intel and AMD handle this similarly.</p><p>It makes <a href="https://arangodb.com/2021/02/cpp-memory-model-migrating-from-x86-to-arm">Arm and Power much more suitable for lock-free programming</a> and concurrent data structures, but some observations hold for both platforms.
Most importantly, “Compare and Swap” (CAS) is costly and should be avoided at all costs.</p><p>On x86, for example, the <code>LOCK ADD</code> <a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs">can easily take 50 CPU cycles</a>.
It is 50x slower than a regular <code>ADD</code> instruction but still easily 5-10x faster than a <code>LOCK CMPXCHG</code> instruction.
Once the contention rises, the gap naturally widens, further amplified by the increased “failure” rate of the CAS operation when the value being compared has already changed.
That’s why, for the “dynamic” mode, we resort to using an additional atomic variable rather than more typical CAS-based implementations.</p><h3 id="alignment">Alignment</h3><p>Assuming a thread pool is a heavy object anyway, nobody will care if it’s a bit larger than expected.
That allows us to over-align the internal counters to <a href="https://en.cppreference.com/w/cpp/thread/hardware_destructive_interference_size"><code>std::hardware_destructive_interference_size</code></a> or <a href="https://en.cppreference.com/w/cpp/types/max_align_t"><code>std::max_align_t</code></a> to avoid false sharing.
In that case, even on x86, where the entire cache will be exclusively owned by a single thread, in eager mode, we end up effectively “pipelining” the execution, where one thread may be incrementing the “in-flight” counter while the other is decrementing the “remaining” counter.
Others are executing the loop body in between.</p><h2 id="comparing-apis">Comparing APIs</h2><h3 id="fork-union">Fork Union</h3><p>Fork Union has a straightforward goal, so its API is equally clear.
There are only 4 core interfaces:</p><ul><li><code>for_each_thread</code> - to dispatch a callback per thread, similar to <code>#pragma omp parallel</code>.</li><li><code>for_each_static</code> - for individual evenly-sized tasks, similar to <code>#pragma omp for schedule(static)</code>.</li><li><code>for_each_slice</code> - for slices of evenly-sized tasks, similar to nested <code>#pragma omp for schedule(static)</code>.</li><li><code>for_each_dynamic</code> - for individual unevenly-sized tasks, similar to <code>#pragma omp for schedule(dynamic, 1)</code>.</li></ul><p>They all receive a C++ lambda or a Rust closure and a range of tasks to execute.
The construction of the thread pool itself is a bit trickier than typically in standard libraries, as “exceptions” and “panics” are not allowed.
So, the constructor can’t perform any real work.
In C++, the <code>try_spawn</code> method can be called to allocate all the threads:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-3-1"><a href="#hl-3-1"> 1</a>
</span><span id="hl-3-2"><a href="#hl-3-2"> 2</a>
</span><span id="hl-3-3"><a href="#hl-3-3"> 3</a>
</span><span id="hl-3-4"><a href="#hl-3-4"> 4</a>
</span><span id="hl-3-5"><a href="#hl-3-5"> 5</a>
</span><span id="hl-3-6"><a href="#hl-3-6"> 6</a>
</span><span id="hl-3-7"><a href="#hl-3-7"> 7</a>
</span><span id="hl-3-8"><a href="#hl-3-8"> 8</a>
</span><span id="hl-3-9"><a href="#hl-3-9"> 9</a>
</span><span id="hl-3-10"><a href="#hl-3-10">10</a>
</span><span id="hl-3-11"><a href="#hl-3-11">11</a>
</span><span id="hl-3-12"><a href="#hl-3-12">12</a>
</span><span id="hl-3-13"><a href="#hl-3-13">13</a>
</span><span id="hl-3-14"><a href="#hl-3-14">14</a>
</span><span id="hl-3-15"><a href="#hl-3-15">15</a>
</span><span id="hl-3-16"><a href="#hl-3-16">16</a>
</span><span id="hl-3-17"><a href="#hl-3-17">17</a>
</span><span id="hl-3-18"><a href="#hl-3-18">18</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;fork_union.hpp&gt;</span><span>   </span><span>// `fork_union_t`
</span></span></span><span><span><span></span><span>#include</span> <span>&lt;cstdio&gt;</span><span>           </span><span>// `stderr`
</span></span></span><span><span><span></span><span>#include</span> <span>&lt;cstdlib&gt;</span><span>          </span><span>// `EXIT_SUCCESS`
</span></span></span><span><span><span></span>
</span></span><span><span><span>namespace</span> <span>fun</span> <span>=</span> <span>ashvardanian</span><span>::</span><span>fork_union</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>int</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>    <span>fun</span><span>::</span><span>fork_union_t</span> <span>pool</span><span>;</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>!</span><span>pool</span><span>.</span><span>try_spawn</span><span>(</span><span>std</span><span>::</span><span>thread</span><span>::</span><span>hardware_concurrency</span><span>()))</span> <span>{</span>
</span></span><span><span>        <span>std</span><span>::</span><span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>&#34;Failed to fork the threads</span><span>\n</span><span>&#34;</span><span>);</span>
</span></span><span><span>        <span>return</span> <span>EXIT_FAILURE</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>pool</span><span>.</span><span>for_each_thread</span><span>([</span><span>&amp;</span><span>](</span><span>std</span><span>::</span><span>size_t</span> <span>thread_index</span><span>)</span> <span>noexcept</span> <span>{</span>
</span></span><span><span>        <span>std</span><span>::</span><span>printf</span><span>(</span><span>&#34;Hello from thread # %zu (of %zu)</span><span>\n</span><span>&#34;</span><span>,</span> <span>thread_index</span> <span>+</span> <span>1</span><span>,</span> <span>pool</span><span>.</span><span>count_threads</span><span>());</span>
</span></span><span><span>    <span>});</span>
</span></span><span><span>    <span>return</span> <span>EXIT_SUCCESS</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><blockquote><p>As you may have noticed, the lambdas are forced to be <code>noexcept</code> and can’t return anything.
This is a design choice that vastly simplifies the implementation.</p></blockquote><p>In Rust, similarly, the <code>try_spawn</code> method can be used:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-4-1"><a href="#hl-4-1"> 1</a>
</span><span id="hl-4-2"><a href="#hl-4-2"> 2</a>
</span><span id="hl-4-3"><a href="#hl-4-3"> 3</a>
</span><span id="hl-4-4"><a href="#hl-4-4"> 4</a>
</span><span id="hl-4-5"><a href="#hl-4-5"> 5</a>
</span><span id="hl-4-6"><a href="#hl-4-6"> 6</a>
</span><span id="hl-4-7"><a href="#hl-4-7"> 7</a>
</span><span id="hl-4-8"><a href="#hl-4-8"> 8</a>
</span><span id="hl-4-9"><a href="#hl-4-9"> 9</a>
</span><span id="hl-4-10"><a href="#hl-4-10">10</a>
</span><span id="hl-4-11"><a href="#hl-4-11">11</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>#![feature(allocator_api)]</span><span>
</span></span></span><span><span><span></span><span>use</span><span> </span><span>std</span>::<span>error</span>::<span>Error</span><span>;</span><span>
</span></span></span><span><span><span></span><span>use</span><span> </span><span>fork_union</span>::<span>ForkUnion</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span> <span>main</span><span>()</span><span> </span>-&gt; <span>Result</span><span>&lt;</span><span>(),</span><span> </span><span>Box</span><span>&lt;</span><span>dyn</span><span> </span><span>Error</span><span>&gt;&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>pool</span><span> </span><span>=</span><span> </span><span>ForkUnion</span>::<span>try_spawn</span><span>(</span><span>4</span><span>)</span><span>?</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>pool</span><span>.</span><span>for_each_thread</span><span>(</span><span>|</span><span>thread_index</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>println!</span><span>(</span><span>&#34;Hello from thread # </span><span>{}</span><span> (of </span><span>{}</span><span>)&#34;</span><span>,</span><span> </span><span>thread_index</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>pool</span><span>.</span><span>count_threads</span><span>());</span><span>
</span></span></span><span><span><span>    </span><span>});</span><span>
</span></span></span><span><span><span>    </span><span>Ok</span><span>(())</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div></div><p>Assuming Rust has no function overloading, there are a few alternatives:</p><ul><li><code>try_spawn</code> - to spawn a thread pool with the main allocator.</li><li><code>try_spawn_in</code> - to spawn a thread pool with a custom allocator.</li><li><code>try_named_spawn</code> - to spawn a thread pool with the main allocator and a name.</li><li><code>try_named_spawn_in</code> - to spawn a thread pool with a custom allocator and a name.</li></ul><h3 id="rayon">Rayon</h3><p>Rayon is the go-to Rust library for data parallelism.
It suffers from the same core design issues as every other thread pool I’ve looked at on GitHub, but it’s fair to say that at the high level, it provides outstanding coverage for various parallel iterators!
As such, there is <a href="https://github.com/ashvardanian/fork_union/issues/2">an open call to explore similar “Map-Reduce” and “Map-Fork-Reduce” patterns in Fork Union</a> to see if they can be implemented efficiently.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-5-1"><a href="#hl-5-1">1</a>
</span><span id="hl-5-2"><a href="#hl-5-2">2</a>
</span><span id="hl-5-3"><a href="#hl-5-3">3</a>
</span><span id="hl-5-4"><a href="#hl-5-4">4</a>
</span><span id="hl-5-5"><a href="#hl-5-5">5</a>
</span><span id="hl-5-6"><a href="#hl-5-6">6</a>
</span><span id="hl-5-7"><a href="#hl-5-7">7</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>use</span><span> </span><span>rayon</span>::<span>prelude</span>::<span>*</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span> <span>sum_of_squares</span><span>(</span><span>input</span>: <span>&amp;</span><span>[</span><span>i32</span><span>])</span><span> </span>-&gt; <span>i32</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>input</span><span>.</span><span>par_iter</span><span>()</span><span> </span><span>// &lt;-- just change that!
</span></span></span><span><span><span></span><span>         </span><span>.</span><span>map</span><span>(</span><span>|&amp;</span><span>i</span><span>|</span><span> </span><span>i</span><span> </span><span>*</span><span> </span><span>i</span><span>)</span><span>
</span></span></span><span><span><span>         </span><span>.</span><span>sum</span><span>()</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div></div><p>The default <code>.par_iter()</code> API of Rayon, <a href="https://github.com/rayon-rs/rayon/blob/ae07384e3e0b238cea89f0c14891f351c65a5cee/README.md?plain=1#L26-L33">at the start of the README.md</a>, is not how I’ve used it in “Parallel Reductions Benchmark”.
To ensure that we are benchmarking the actual synchronization cost of the thread pool, I’ve gone directly to the underlying <code>rayon::ThreadPool</code> API:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-6-1"><a href="#hl-6-1"> 1</a>
</span><span id="hl-6-2"><a href="#hl-6-2"> 2</a>
</span><span id="hl-6-3"><a href="#hl-6-3"> 3</a>
</span><span id="hl-6-4"><a href="#hl-6-4"> 4</a>
</span><span id="hl-6-5"><a href="#hl-6-5"> 5</a>
</span><span id="hl-6-6"><a href="#hl-6-6"> 6</a>
</span><span id="hl-6-7"><a href="#hl-6-7"> 7</a>
</span><span id="hl-6-8"><a href="#hl-6-8"> 8</a>
</span><span id="hl-6-9"><a href="#hl-6-9"> 9</a>
</span><span id="hl-6-10"><a href="#hl-6-10">10</a>
</span><span id="hl-6-11"><a href="#hl-6-11">11</a>
</span><span id="hl-6-12"><a href="#hl-6-12">12</a>
</span><span id="hl-6-13"><a href="#hl-6-13">13</a>
</span><span id="hl-6-14"><a href="#hl-6-14">14</a>
</span><span id="hl-6-15"><a href="#hl-6-15">15</a>
</span><span id="hl-6-16"><a href="#hl-6-16">16</a>
</span><span id="hl-6-17"><a href="#hl-6-17">17</a>
</span><span id="hl-6-18"><a href="#hl-6-18">18</a>
</span><span id="hl-6-19"><a href="#hl-6-19">19</a>
</span><span id="hl-6-20"><a href="#hl-6-20">20</a>
</span><span id="hl-6-21"><a href="#hl-6-21">21</a>
</span><span id="hl-6-22"><a href="#hl-6-22">22</a>
</span><span id="hl-6-23"><a href="#hl-6-23">23</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>sum_rayon</span><span>(</span><span>pool</span>: <span>&amp;</span><span>rayon</span>::<span>ThreadPool</span><span>,</span><span> </span><span>data</span>: <span>&amp;</span><span>[</span><span>f32</span><span>],</span><span> </span><span>partial_sums</span>: <span>&amp;</span><span>mut</span><span> </span><span>[</span><span>f64</span><span>])</span><span> </span>-&gt; <span>f64</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>cores</span><span> </span><span>=</span><span> </span><span>pool</span><span>.</span><span>current_num_threads</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>chunk_size</span><span> </span><span>=</span><span> </span><span>scalars_per_core</span><span>(</span><span>data</span><span>.</span><span>len</span><span>(),</span><span> </span><span>cores</span><span>);</span><span>       </span><span>// Defined elsewhere
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>partial_sums_ptr</span><span> </span><span>=</span><span> </span><span>partial_sums</span><span>.</span><span>as_mut_ptr</span><span>()</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>  </span><span>// Pointers aren&#39;t safe to pass around
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>    </span><span>pool</span><span>.</span><span>broadcast</span><span>(</span><span>|</span><span>context</span>: <span>rayon</span>::<span>BroadcastContext</span><span>&lt;</span><span>&#39;_</span><span>&gt;|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>thread_index</span><span> </span><span>=</span><span> </span><span>context</span><span>.</span><span>index</span><span>();</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>start</span><span> </span><span>=</span><span> </span><span>thread_index</span><span> </span><span>*</span><span> </span><span>chunk_size</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>start</span><span> </span><span>&gt;=</span><span> </span><span>data</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>stop</span><span> </span><span>=</span><span> </span><span>std</span>::<span>cmp</span>::<span>min</span><span>(</span><span>start</span><span> </span><span>+</span><span> </span><span>chunk_size</span><span>,</span><span> </span><span>data</span><span>.</span><span>len</span><span>());</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>partial_sum</span><span> </span><span>=</span><span> </span><span>sum_unrolled</span><span>(</span><span>&amp;</span><span>data</span><span>[</span><span>start</span><span>..</span><span>stop</span><span>]);</span><span>
</span></span></span><span><span><span>        </span><span>unsafe</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>ptr</span>::<span>write</span><span>(</span><span> </span><span>// Cast back to a pointer:
</span></span></span><span><span><span></span><span>                </span><span>(</span><span>partial_sums_ptr</span><span> </span><span>as</span><span> </span><span>*</span><span>mut</span><span> </span><span>f64</span><span>).</span><span>add</span><span>(</span><span>thread_index</span><span>),</span><span>
</span></span></span><span><span><span>                </span><span>partial_sum</span><span>,</span><span>
</span></span></span><span><span><span>            </span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>});</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>partial_sums</span><span>.</span><span>iter</span><span>().</span><span>copied</span><span>().</span><span>sum</span><span>()</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div></div><h3 id="taskflow">Taskflow</h3><p>Taskflow is one of the most popular C++ libraries for parallelism.
It has many features, including async execution graphs on CPUs and GPUs.
<a href="https://github.com/taskflow/taskflow/blob/b3c1e5fd8e2d67eaead944a8d869f87e6b58bbbe/README.md?plain=1#L84-L106">The most common example</a> looks like this:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-7-1"><a href="#hl-7-1"> 1</a>
</span><span id="hl-7-2"><a href="#hl-7-2"> 2</a>
</span><span id="hl-7-3"><a href="#hl-7-3"> 3</a>
</span><span id="hl-7-4"><a href="#hl-7-4"> 4</a>
</span><span id="hl-7-5"><a href="#hl-7-5"> 5</a>
</span><span id="hl-7-6"><a href="#hl-7-6"> 6</a>
</span><span id="hl-7-7"><a href="#hl-7-7"> 7</a>
</span><span id="hl-7-8"><a href="#hl-7-8"> 8</a>
</span><span id="hl-7-9"><a href="#hl-7-9"> 9</a>
</span><span id="hl-7-10"><a href="#hl-7-10">10</a>
</span><span id="hl-7-11"><a href="#hl-7-11">11</a>
</span><span id="hl-7-12"><a href="#hl-7-12">12</a>
</span><span id="hl-7-13"><a href="#hl-7-13">13</a>
</span><span id="hl-7-14"><a href="#hl-7-14">14</a>
</span><span id="hl-7-15"><a href="#hl-7-15">15</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;taskflow/taskflow.hpp&gt;</span><span>
</span></span></span><span><span><span></span><span>int</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Executor</span> <span>executor</span><span>;</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Taskflow</span> <span>taskflow</span><span>;</span>
</span></span><span><span>    <span>auto</span> <span>[</span><span>A</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>,</span> <span>D</span><span>]</span> <span>=</span> <span>taskflow</span><span>.</span><span>emplace</span><span>(</span> <span>// create four tasks
</span></span></span><span><span><span></span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>&#34;TaskA</span><span>\n</span><span>&#34;</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>&#34;TaskB</span><span>\n</span><span>&#34;</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>&#34;TaskC</span><span>\n</span><span>&#34;</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>&#34;TaskD</span><span>\n</span><span>&#34;</span><span>;</span> <span>}</span> 
</span></span><span><span>    <span>);</span>                                  
</span></span><span><span>    <span>A</span><span>.</span><span>precede</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>);</span> <span>// A runs before B and C
</span></span></span><span><span><span></span>    <span>D</span><span>.</span><span>succeed</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>);</span> <span>// D runs after  B and C
</span></span></span><span><span><span></span>    <span>executor</span><span>.</span><span>run</span><span>(</span><span>taskflow</span><span>).</span><span>wait</span><span>();</span> 
</span></span><span><span>    <span>return</span> <span>0</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Despite being just an example, it clearly shows how different Taskflow’s core objectives are from OpenMP and Fork Union.
It is still probably mainly used for simple static parallelism, similar to our case without complex dependencies and the <code>taskflow</code> can be reused.
Here is how “Parallel Reductions Benchmark” wraps Taskflow:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-8-1"><a href="#hl-8-1"> 1</a>
</span><span id="hl-8-2"><a href="#hl-8-2"> 2</a>
</span><span id="hl-8-3"><a href="#hl-8-3"> 3</a>
</span><span id="hl-8-4"><a href="#hl-8-4"> 4</a>
</span><span id="hl-8-5"><a href="#hl-8-5"> 5</a>
</span><span id="hl-8-6"><a href="#hl-8-6"> 6</a>
</span><span id="hl-8-7"><a href="#hl-8-7"> 7</a>
</span><span id="hl-8-8"><a href="#hl-8-8"> 8</a>
</span><span id="hl-8-9"><a href="#hl-8-9"> 9</a>
</span><span id="hl-8-10"><a href="#hl-8-10">10</a>
</span><span id="hl-8-11"><a href="#hl-8-11">11</a>
</span><span id="hl-8-12"><a href="#hl-8-12">12</a>
</span><span id="hl-8-13"><a href="#hl-8-13">13</a>
</span><span id="hl-8-14"><a href="#hl-8-14">14</a>
</span><span id="hl-8-15"><a href="#hl-8-15">15</a>
</span><span id="hl-8-16"><a href="#hl-8-16">16</a>
</span><span id="hl-8-17"><a href="#hl-8-17">17</a>
</span><span id="hl-8-18"><a href="#hl-8-18">18</a>
</span><span id="hl-8-19"><a href="#hl-8-19">19</a>
</span><span id="hl-8-20"><a href="#hl-8-20">20</a>
</span><span id="hl-8-21"><a href="#hl-8-21">21</a>
</span><span id="hl-8-22"><a href="#hl-8-22">22</a>
</span><span id="hl-8-23"><a href="#hl-8-23">23</a>
</span><span id="hl-8-24"><a href="#hl-8-24">24</a>
</span><span id="hl-8-25"><a href="#hl-8-25">25</a>
</span><span id="hl-8-26"><a href="#hl-8-26">26</a>
</span><span id="hl-8-27"><a href="#hl-8-27">27</a>
</span><span id="hl-8-28"><a href="#hl-8-28">28</a>
</span><span id="hl-8-29"><a href="#hl-8-29">29</a>
</span><span id="hl-8-30"><a href="#hl-8-30">30</a>
</span><span id="hl-8-31"><a href="#hl-8-31">31</a>
</span><span id="hl-8-32"><a href="#hl-8-32">32</a>
</span><span id="hl-8-33"><a href="#hl-8-33">33</a>
</span><span id="hl-8-34"><a href="#hl-8-34">34</a>
</span><span id="hl-8-35"><a href="#hl-8-35">35</a>
</span><span id="hl-8-36"><a href="#hl-8-36">36</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>template</span> <span>&lt;</span><span>typename</span> <span>serial_at</span> <span>=</span> <span>stl_accumulate_gt</span><span>&lt;</span><span>float</span><span>&gt;&gt;</span>
</span></span><span><span><span>class</span> <span>taskflow_gt</span> <span>{</span>
</span></span><span><span>    <span>float</span> <span>const</span> <span>*</span><span>const</span> <span>begin_</span> <span>=</span> <span>nullptr</span><span>;</span>
</span></span><span><span>    <span>float</span> <span>const</span> <span>*</span><span>const</span> <span>end_</span> <span>=</span> <span>nullptr</span><span>;</span>
</span></span><span><span>    <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>cores_</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Executor</span> <span>executor_</span><span>;</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Taskflow</span> <span>taskflow_</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>struct</span> <span>alignas</span><span>(</span><span>128</span><span>)</span> <span>thread_result_t</span> <span>{</span>
</span></span><span><span>        <span>double</span> <span>partial_sum</span> <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>    <span>};</span>
</span></span><span><span>    <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>thread_result_t</span><span>&gt;</span> <span>sums_</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>public</span><span>:</span>
</span></span><span><span>    <span>taskflow_gt</span><span>()</span> <span>=</span> <span>default</span><span>;</span>
</span></span><span><span>    <span>taskflow_gt</span><span>(</span><span>float</span> <span>const</span> <span>*</span><span>b</span><span>,</span> <span>float</span> <span>const</span> <span>*</span><span>e</span><span>)</span>
</span></span><span><span>        <span>:</span> <span>begin_</span> <span>{</span><span>b</span><span>},</span> <span>end_</span> <span>{</span><span>e</span><span>},</span> <span>cores_</span> <span>{</span><span>total_cores</span><span>()},</span> <span>executor_</span> <span>{</span><span>static_cast</span><span>&lt;</span><span>unsigned</span><span>&gt;</span><span>(</span><span>cores_</span><span>)},</span> <span>sums_</span> <span>{</span><span>cores_</span><span>}</span> <span>{</span>
</span></span><span><span>
</span></span><span><span>        <span>auto</span> <span>const</span> <span>input_size</span> <span>=</span> <span>static_cast</span><span>&lt;</span><span>std</span><span>::</span><span>size_t</span><span>&gt;</span><span>(</span><span>end_</span> <span>-</span> <span>begin_</span><span>);</span>
</span></span><span><span>        <span>auto</span> <span>const</span> <span>chunk_size</span> <span>=</span> <span>scalars_per_core</span><span>(</span><span>input_size</span><span>,</span> <span>cores_</span><span>);</span>
</span></span><span><span>        <span>for</span> <span>(</span><span>std</span><span>::</span><span>size_t</span> <span>thread_index</span> <span>=</span> <span>0</span><span>;</span> <span>thread_index</span> <span>&lt;</span> <span>cores_</span><span>;</span> <span>++</span><span>thread_index</span><span>)</span> <span>{</span>
</span></span><span><span>            <span>taskflow_</span><span>.</span><span>emplace</span><span>([</span><span>this</span><span>,</span> <span>input_size</span><span>,</span> <span>chunk_size</span><span>,</span> <span>thread_index</span><span>]</span> <span>{</span>
</span></span><span><span>                <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>start</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>thread_index</span> <span>*</span> <span>chunk_size</span><span>,</span> <span>input_size</span><span>);</span>
</span></span><span><span>                <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>stop</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>start</span> <span>+</span> <span>chunk_size</span><span>,</span> <span>input_size</span><span>);</span>
</span></span><span><span>                <span>sums_</span><span>[</span><span>thread_index</span><span>].</span><span>partial_sum</span> <span>=</span> <span>serial_at</span> <span>{</span><span>begin_</span> <span>+</span> <span>start</span><span>,</span> <span>begin_</span> <span>+</span> <span>stop</span><span>}();</span>
</span></span><span><span>            <span>});</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>double</span> <span>operator</span><span>()()</span> <span>{</span>
</span></span><span><span>        <span>executor_</span><span>.</span><span>run</span><span>(</span><span>taskflow_</span><span>).</span><span>wait</span><span>();</span>
</span></span><span><span>        <span>return</span> <span>std</span><span>::</span><span>accumulate</span><span>(</span><span>sums_</span><span>.</span><span>begin</span><span>(),</span> <span>sums_</span><span>.</span><span>end</span><span>(),</span> <span>0.0</span><span>,</span>
</span></span><span><span>                               <span>[](</span><span>double</span> <span>acc</span><span>,</span> <span>thread_result_t</span> <span>const</span> <span>&amp;</span><span>x</span><span>)</span> <span>noexcept</span> <span>{</span> <span>return</span> <span>acc</span> <span>+</span> <span>x</span><span>.</span><span>partial_sum</span><span>;</span> <span>});</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>};</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Only the <code>operator()</code> method is timed, leaving the construction costs out of the equation.</p><h2 id="conclusions--observations">Conclusions &amp; Observations</h2><p>Fork Union shows that a lean, 300-line fork-join pool can sit within ~20% of OpenMP, while more functional pools trail by an order of magnitude.
That margin will shift as more workloads, CPUs, and compilers are tested, so treat today’s numbers as directional, not gospel.
There may still be subtle memory-ordering bugs lurking in Fork Union, but the core observations should hold: <strong>dodge mutexes, dynamic queues, likely-pessimistic CAS paths, and false sharing — regardless of language or framework</strong>.</p><p>Rust is still new territory for me.
The biggest surprise is the <a href="https://github.com/rust-lang/rust/issues/32838">missing allocator support in <code>std::collections</code></a> on the stable toolchain.
Nightly’s <code>Vec::try_reserve_in</code> helps, but until stable lands, ergonomic custom allocation remains tricky.
The machinery exists in C++, yet most projects ignore it — so the culture needs to catch up.</p><hr/><p>PS: Spot dubious memory-ordering?
<a href="https://github.com/ashvardanian/fork_union/issues">Open an issue</a>.
Want to close the remaining 20% gap?
Happy forking 🤗</p><blockquote><p lang="en" dir="ltr">Fork Union, arguably the most unusual parallel-processing library on GitHub, just crossed its first 100 stars — my 12th project to reach that milestone 🥳</p>— Ash Vardanian (@ashvardanian) <a href="https://twitter.com/ashvardanian/status/1964634808635539904?ref_src=twsrc%5Etfw">September 7, 2025</a></blockquote></div></div>
  </body>
</html>
