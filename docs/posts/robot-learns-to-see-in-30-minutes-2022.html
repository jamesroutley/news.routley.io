<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://antonilo.github.io/vision_locomotion/">Original</a>
    <h1>Robot Learns to See in 30 Minutes (2022)</h1>
    
    <div id="readability-page-1" class="page"><div>

    <center>
    <iframe width="70%" height="400" src="https://www.youtube.com/embed/d7I34YIdMdk" title="Learning to See" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </center>


    <center>
        <p>
    <b>TL;DR: Learning to walk from pixels in the real world by using proprioception as supervision. </b>
    </p></center>


    <!--------------------- abstract --------------------->
    
    <h2> Abstract </h2>
    
    <center>
        <p> 
In this work, we show how to learn a visual walking policy that only uses a monocular RGB camera and proprioception to walk. Since simulating RGB is hard, we necessarily have to learn vision in the real world. We start with a blind walking policy trained in simulation. This policy can traverse some terrains in the real world but often struggles since it lacks knowledge of the upcoming geometry. This can be resolved with the use of vision. We train a visual module in the real world to predict the upcoming terrain with our proposed algorithm Cross-Modal Supervision (CMS). CMS uses time-shifted proprioception to supervise vision and allows the policy to continually improve with more real-world experience. 
We evaluate our vision-based walking policy over a diverse set of terrains including stairs (up to 19cm high), slippery slopes (inclination of 35 degrees), curbs and tall steps (up to 20cm), and complex discrete terrains. We achieve this performance with less than 30 minutes of real-world data.
Finally, we show that our policy can adapt to shifts in the visual field with a limited amount of real-world experience.</p>
    </center>


    <h2> Visual Plasticity: The Prism-Adaptation Experiment </h2>

    
    <center><p>
      We study how quickly the policy can adapt to shifts in the visual field. To do so, we change the camera orientation. This results in a large variation in the field of view, as shown in the image below. Note that after rotation, the robot cannot see the terrain in front of it.
    </p></center>

    <center>
		<img src="https://antonilo.github.io/vision_locomotion/images/prism-expt_camera.png" width="98%" alt=""/><br/>
	</center>



        
    <center><p>
Before shifting the camera&#39;s visual field (pre-test), the policy can climb the testing staircase perfectly. However, after rotating the camera, the visual policy stumbles on stairs and drifts in the horizontal direction (exposure). After only three trials (approximately 80 seconds of data), the policy can again anticipate steps and walk without drifting (adaptation).
    </p></center>


    <div>
        <p>
          <center> <b>Pre-Test</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/pre-test.mp4" width="97%"></video>
        </p>
        <p>
          <center> <b>Exposure</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/exposure.mp4" width="97%"></video>
        </p>
        <p>
          <center> <b>Adaptation</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/adaptation.mp4" width="97%"></video>
        </p>
    </div>


        
    <center><p>
      In the final session of the experiment (post-test), we switch back the camera to its original position. We observe that after training on only two trials, the policy can re-adapt to the original visual field.
    </p></center>

    <div>
        <p>
          <center> <b>Post-Test: Trial I</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/post-test_initial.mp4" width="97%"></video>
        </p>
        <p>
          <center> <b>Post-Test: Trial II</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/post-test_after_one.mp4" width="97%"></video>
        </p>
        <p>
          <center> <b>Post-Test: Trial III</b></center>
            <video controls="" playsinline="" autoplay="" loop="" muted="" src="res/prism/post-test_adapted.mp4" width="97%"></video>
        </p>
    </div>


    	<hr/>
    <h2> Generalization Results </h2>

    
    <center><p>
        We show that our vision-based policy can walk on previously unseen terrains.
    </p></center>

    
    
    <hr/>


    <h2> Generalization Results: Trip to Stanford </h2>

    
    <center><p>
        After training our robot on a large set of terrains in the Berkeley campus, we verified generalization of the visual policy in the Stanford campus.
    </p></center>

    
    


    <hr/>


    <hr/>
    <h2> Blind vs Visual Locomotion </h2>

    
    <center><p>
        We show that our policy is better than the blind policy in several environments.
    </p></center>


    
    
    
    


    <hr/>



      	<h2> Bibtex </h2>
	<pre>

  @InProceedings{loquercio2022learn,
   author={Loquercio, Antonio and Kumar, and Malik, Jitendra},
   title={{Learning Visual Locomotion with Cross-Modal Supervision}},
   booktitle={arXiv},
   year={2022}
  }

  </pre>
	<hr/>

	<p><b><span>Acknowledgements:</span></b></p><!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    

</div></div>
  </body>
</html>
