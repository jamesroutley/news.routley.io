<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://payments.posthaven.com/rc-w2d4-comparing-performance-of-left-vs-right-fold">Original</a>
    <h1>RC W2D4 - Comparing performance of left vs right fold</h1>
    
    <div id="readability-page-1" class="page"><div id="post_body_1945016">
    
      <div><p>Today I prepared slides for Friday’s presentation. I always find it helpful to a dry run the day before, I’m told <a href="https://www.youtube.com/watch?v=f84n5oFoZBc">magic</a> happens when you sleep on it.<br/></p><p>I was happy to draw a line connecting SICP to Haskell, and it’s a good periodic reminder when delving into functional programming. What&#39;s new is I feel pulled into a lot of theory (lambda calculus, type theory, category theory, proofs), discover this is normal, and spending time on all this doesn’t even cover what the compiler does in practice.</p><p>Re: compiler, I was previously under the impression that lazy evaluation can be inefficient because you’re doing the same calculation more than once. For example, in calculating `square (1 + 2)`, you have to do `(1 + 2)` twice in `((1 + 2) * (1 + 2))` instead of just once when do the sum first before you square. This is from CIS 194 <a href="https://www.cis.upenn.edu/~cis1940/spring13/lectures/06-laziness.html">lecture notes</a>.</p><blockquote><p>In particular, GHC uses a technique called graph reduction, where the expression being evaluated is actually represented as a graph, so that different parts of the expression can share pointers to the same subexpression.</p></blockquote><p>My read is Haskell builds abstractions that doesn’t translate as naturally to what the processor does and has to do heavy lifting for performance reasons. The code still runs fast, but maybe you’ll need to know the compiler a bit better when you’re trying to reason about your program or diagnose bottlenecks. This theme came up in the FP study group, on the topic of additional &#39;bookkeeping&#39; that Clojure does to ensure your dictionaries are immutable under the hood.</p><p>In any case I tried running locally the left fold vs right fold example <a href="https://stackoverflow.com/questions/3429634/foldl-is-tail-recursive-so-how-come-foldr-runs-faster-than-foldl">here</a>. This involves concatenating a 100mm integers and summing them up (I didn’t get a stack overflow like in the post, which is impressive heavy lifting!). This is for the right fold.</p><p><code>stack exec cis194-exe  2.27s user 0.85s system 116% cpu 2.672 total</code></p><p>Now the left.<br/>
</p><p><code>stack exec cis194-exe  13.14s user 2.97s system 101% cpu 15.849 total</code></p><p>That&#39;s 6x! Reminder to self to <a href="https://github.com/hasura/graphql-engine/pull/2933#discussion_r328821960" title="Link: https://github.com/hasura/graphql-engine/pull/2933#discussion_r328821960">read up</a> more on the weak-head normal form (WHNF). My understanding so far is that left folds are more expensive here because you need to expand everything out, whereas in right folds you get to keep things in WHNF until the eval happens.</p><p>One of the best memories at work was about Python code my previous manager wrote in more functional style using `functools`. On discovering that it actually runs slower than writing it in idiomatic Python, another colleague said &#34;It&#39;s harder to read, it&#39;s slower, so we&#39;re writing it this way for funsies?&#34;. Everyone laughed. My stomach hurt. It was amazing.<br/>
</p></div>
    
  </div></div>
  </body>
</html>
