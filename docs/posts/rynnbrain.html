<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/alibaba-damo-academy/RynnBrain">Original</a>
    <h1>RynnBrain</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/logo.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/logo.png"/></a>
</p>
<p dir="auto">
       ğŸ’« <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/" rel="nofollow"><b>Project Page</b></a>Â Â  | Â Â  Models &amp; Bench <a href="https://huggingface.co/collections/Alibaba-DAMO-Academy/rynnbrain" rel="nofollow"><b> ğŸ¤—</b></a> <a href="https://www.modelscope.cn/collections/DAMO_Academy/RynnBrain" rel="nofollow"><b>ğŸ¤–</b></a>  Â |Â  ğŸš€ <a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnBrain" rel="nofollow"><b>Demo</b></a> Â Â  | Â Â ğŸ“š <a href="https://github.com/alibaba-damo-academy/RynnBrain/tree/main/cookbooks">Cookbooks</a>Â Â 
</p>

<ul dir="auto">
<li><strong>[2026.02.15]</strong>  ğŸ”¥ğŸ”¥ Release our <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/assets/RynnBrain_Report.pdf" rel="nofollow">Technical Report</a> !!</li>
<li><strong>[2026.02.09]</strong>  ğŸ”¥ğŸ”¥ Release our code and model checkpoints!!</li>
</ul>

<p dir="auto">We present <strong>RynnBrain</strong>, an embodied foundation model grounded in physical reality. RynnBrain is available in two dense variants (2B and 8B) and one mixture-of-experts (MoE) model (30B-A3B).
In addition, we release three postâ€‘trained models: RynnBrainâ€‘Plan (<strong>robot task planning</strong>), RynnBrainâ€‘Nav (<strong>vision-language navigation</strong>), and RynnBrainâ€‘CoP (<strong>chain-of-point reasoning</strong>).</p>




<ul dir="auto">
<li><strong>Comprehensive egocentric understanding</strong>:
Excels in fine-grained video understanding and egocentric cognition, covering tasks such as embodied QA, counting, and OCR.</li>
<li><strong>Diverse spatio-temporal localization</strong>:
Possesses powerful localization capabilities across episodic memory, enabling precise identification of objects, target areas, and motion trajectories.</li>
<li><strong>Physical-space reasoning</strong>:
Employs an interleaved reasoning strategy that alternates between textual and spatial grounding, ensuring that its reasoning processes are firmly rooted in the physical environment.</li>
<li><strong>Physics-aware precise planning</strong>:
Integrates located affordances and object information into planning, enabling downstream VLA models to execute intricate tasks with fine-grained instructions.</li>
</ul>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/intro.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/intro.png"/></a>
</p>

<p dir="auto">RynnBrain employs a unified encoder-decoder architecture (supporting both Dense and MoE variants) to transform omni-vision inputs and textual instructions into multi-modal outputs, including spatial trajectories, physical pointing, and action planning.Â 
Through massive training on rich spatio-temporal, physical-space, and general knowledge data, RynnBrain maintains robust general-purpose capabilities while specializing in diverse, fine-grained embodied reasoning and complex planning tasks.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/framework.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/framework.png"/></a>
</p>

<ul dir="auto">
<li>General Embodied Understanding</li>
</ul>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/performance_general_2B_8B.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/performance_general_2B_8B.png"/></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/performance_general_30B.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/performance_general_30B.png"/></a>
</p>
<ul dir="auto">
<li>Robot Task Planning</li>
</ul>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/18526640/547478674-488da393-a828-4bd9-8813-ace6164a1d2c.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzExOTQ1ODEsIm5iZiI6MTc3MTE5NDI4MSwicGF0aCI6Ii8xODUyNjY0MC81NDc0Nzg2NzQtNDg4ZGEzOTMtYTgyOC00YmQ5LTg4MTMtYWNlNjE2NGExZDJjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMjE1VDIyMjQ0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiNzYwZmY5ZWMwNDAxMTA4NGJkZDQxMWQ5NmFjN2ViNGEzYjNjYzAyMzY1OTZmNjFlZWNlMTBiNzg0YjQyMTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YYdKZx1l6Hem3QH6tbXtbP4dxWuljpg4m0HALooh54M"><img src="https://private-user-images.githubusercontent.com/18526640/547478674-488da393-a828-4bd9-8813-ace6164a1d2c.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzExOTQ1ODEsIm5iZiI6MTc3MTE5NDI4MSwicGF0aCI6Ii8xODUyNjY0MC81NDc0Nzg2NzQtNDg4ZGEzOTMtYTgyOC00YmQ5LTg4MTMtYWNlNjE2NGExZDJjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMjE1VDIyMjQ0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiNzYwZmY5ZWMwNDAxMTA4NGJkZDQxMWQ5NmFjN2ViNGEzYjNjYzAyMzY1OTZmNjFlZWNlMTBiNzg0YjQyMTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YYdKZx1l6Hem3QH6tbXtbP4dxWuljpg4m0HALooh54M"/></a>
</p>
<ul dir="auto">
<li>Vision-Language Navigation</li>
</ul>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/18526640/547479499-78c36b4e-0ea8-42e2-a3fd-692d7c2fb4a7.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzExOTQ1ODEsIm5iZiI6MTc3MTE5NDI4MSwicGF0aCI6Ii8xODUyNjY0MC81NDc0Nzk0OTktNzhjMzZiNGUtMGVhOC00MmUyLWEzZmQtNjkyZDdjMmZiNGE3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMjE1VDIyMjQ0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiMTM1N2MzNTI0MmRjNTcxM2UzODI5MmNjMDU4ZGIxNmZkZThiMGQ2YzEzNTA2MTJlZjcxYjVmNTQwYTZkOGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6PT2CfW4sPciGyVxmi1OQvrloeb-Po8BLdG5VDB50_0"><img src="https://private-user-images.githubusercontent.com/18526640/547479499-78c36b4e-0ea8-42e2-a3fd-692d7c2fb4a7.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzExOTQ1ODEsIm5iZiI6MTc3MTE5NDI4MSwicGF0aCI6Ii8xODUyNjY0MC81NDc0Nzk0OTktNzhjMzZiNGUtMGVhOC00MmUyLWEzZmQtNjkyZDdjMmZiNGE3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMjE1VDIyMjQ0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiMTM1N2MzNTI0MmRjNTcxM2UzODI5MmNjMDU4ZGIxNmZkZThiMGQ2YzEzNTA2MTJlZjcxYjVmNTQwYTZkOGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6PT2CfW4sPciGyVxmi1OQvrloeb-Po8BLdG5VDB50_0"/></a>
</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Base Model</th>
<th>HuggingFace</th>
<th>ModelScope</th>
</tr>
</thead>
<tbody>
<tr>
<td>RynnBrain-2B</td>
<td>Qwen3-VL-2B-Instruct</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-2B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-2B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrain-8B</td>
<td>Qwen3-VL-8B-Instruct</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-8B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-8B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrain-30B-A3B</td>
<td>Qwen3-VL-30B-A3B-Instruct</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-30B-A3B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-30B-A3B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrainâ€‘CoP-8B</td>
<td>RynnBrain-8B</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-CoP-8B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-CoP-8B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrainâ€‘Plan-8B</td>
<td>RynnBrain-8B</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-Plan-8B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-Plan-8B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrainâ€‘Plan-30B-A3B</td>
<td>RynnBrain-30B-A3B</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-Plan-30B-A3B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-Plan-30B-A3B" rel="nofollow">Link</a></td>
</tr>
<tr>
<td>RynnBrainâ€‘Nav-8B</td>
<td>RynnBrain-8B</td>
<td><a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnBrain-Nav-8B" rel="nofollow">Link</a></td>
<td><a href="https://www.modelscope.cn/models/DAMO_Academy/RynnBrain-Nav-8B" rel="nofollow">Link</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Minimal dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install transformers==4.57.1"><pre>pip install transformers==4.57.1</pre></div>
<p dir="auto">Run text generation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(&#34;&#34;)
..."><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForImageTextToText</span>

<span>model</span> <span>=</span> <span>AutoModelForImageTextToText</span>.<span>from_pretrained</span>(<span>&#34;&#34;</span>)
...</pre></div>

<p dir="auto">Checkout the <a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks">cookbooks</a> that showcase RynnBrain&#39;s capabilities in cognition, localization, reasoning, and planning.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Cookbook name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cognition</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/1_spatial_understanding.ipynb">1_spatial_understanding.ipynb</a></td>
<td>Shows the model&#39;s ability for spatial understanding in the video scene.</td>
</tr>
<tr>
<td>Cognition</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/2_object_understanding.ipynb">2_object_understanding.ipynb</a></td>
<td>Shows how the model understands object categories, attributes, and relations and counting ability.</td>
</tr>
<tr>
<td>Cognition</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/3_ocr.ipynb">3_ocr.ipynb</a></td>
<td>Examples of optical character recognition and text understanding in videos.</td>
</tr>
<tr>
<td>Location</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/4_object_location.ipynb">4_object_location.ipynb</a></td>
<td>Locates specific objects with bounding boxes in an image or video based on instructions.</td>
</tr>
<tr>
<td>Location</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/5_area_location.ipynb">5_area_location.ipynb</a></td>
<td>Identifies and marks specified regions by points in an image or video.</td>
</tr>
<tr>
<td>Location</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/6_affordance_location.ipynb">6_affordance_location.ipynb</a></td>
<td>Finds areas or objects with specific affordances in an image or video.</td>
</tr>
<tr>
<td>Location</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/7_trajectory_location.ipynb">7_trajectory_location.ipynb</a></td>
<td>Infers and annotates trajectories or motion paths in an image or video.</td>
</tr>
<tr>
<td>Location</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/8_grasp_pose.ipynb">8_grasp_pose.ipynb</a></td>
<td>Presents the model&#39;s ability to predict robotic grasp poses from images.</td>
</tr>
<tr>
<td>Reasoning</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/9_thinking_with_time_space.ipynb">9_thinking_with_time_space.ipynb</a></td>
<td>Explores an interleaved reasoning mechanism that alternates between textual reasoning and spatial grounding.</td>
</tr>
<tr>
<td>Planning</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/10_manipulate_planning.ipynb">10_manipulate_planning.ipynb</a></td>
<td>Performs multi-step task decomposition and action planning from goals and scenes.</td>
</tr>
<tr>
<td>Planning</td>
<td><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/11_visual_language_navigation.ipynb">11_visual_language_navigation.ipynb</a></td>
<td>Combines vision and language instructions to perform navigation and path planning.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><strong>Pretraining &amp; Evaluation</strong></p>
<p dir="auto">Please refer to <a href="https://github.com/alibaba-damo-academy/RynnScale/tree/main/projects/rynn_brain">RynnScale</a> for details of pretraining and evaluation.</p>
<p dir="auto"><strong>Finetuning</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/reasoning">Reasoning</a>: RynnBrain introduces an <strong>interleaved reasoning approach that combines grounding with textual information</strong> directly within egocentric video streams. This paradigm effectively bridges the cognitive gap between language and the physical world, ensuring the reasoning process is robustly anchored
in reality.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/navigation">Navigation</a>:
We trained a vision-language navigation model based on the RynnBrain base model. Empirical evaluation demonstrates that fine-tuning the vision-language model on RynnBrain yields superior performance compared to fine-tuning on other foundational models.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/planning">Planning</a>:
RynnBrain <strong>integrates the location information of affordance, areas, and objects directly
into its planning outputs</strong>. Consequently, even highly intricate and fine-grained tasks can be effectively addressed within our hierarchical RynnBrain-VLA system architecture.</p>
</li>
</ul>

<p dir="auto">We introduceÂ <strong>RynnBrain-Bench</strong>, a high-dimensional benchmark for embodied understanding that evaluates models across four key dimensions: <em>object cognition</em>, <em>spatial cognition</em>, <em>grounding</em>, and <em>pointing</em>â€”highlightingÂ fine-grained understandingÂ andÂ spatiotemporal localizationÂ across episodic video sequences.</p>
<p dir="auto">For details, please refer to <a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/rynnbrain-bench/README.md">RynnBrain-Bench</a>.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/cookbooks/assets/RynnBrain-Bench.png"><img src="https://github.com/alibaba-damo-academy/RynnBrain/raw/main/cookbooks/assets/RynnBrain-Bench.png"/></a>
</p>
<details open=""><summary>ğŸ’¡ Some other multimodal-LLM projects from our team may interest you âœ¨. </summary><blockquote>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnEC"><strong>RynnEC: Bringing MLLMs into Embodied World</strong></a> </p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnScale"><strong>RynnScale</strong></a> </p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://arxiv.org/abs/2509.15212" rel="nofollow"><strong>RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</strong></a> </p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://arxiv.org/abs/2511.17502" rel="nofollow"><strong>RynnVLA-002: A Unified Vision-Language-Action and World Model</strong></a> </p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnRCP"><strong>RynnRCP: Open Robotics Context Protocol and RobotMotion</strong></a> </p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://github.com/alibaba-damo-academy/RynnMotion"><strong>RynnMotion: All-In-One Toolkit for Fast Robot Prototyping and Heterogeneous Teleoperation</strong></a> </p>
</blockquote>
</details>

<p dir="auto">Our RynnBrain is built on top of <a href="https://github.com/QwenLM/Qwen3-VL"><strong>Qwen3-VL</strong></a>. We also learned a lot from the implementation of <a href="https://github.com/alibaba-damo-academy/RynnEC"><strong>RynnEC</strong></a> and <a href="https://github.com/DAMO-NLP-SG/VideoRefer"><strong>VideoRefer</strong></a>. If your work is used in RynnBrain but not mentioned in either this repo or the technical report, feel free to let us know â¤ï¸.</p>

<p dir="auto">This project is licensed under the Apache License 2.0 - see the <a href="https://github.com/alibaba-damo-academy/RynnBrain/blob/main/LICENSE">LICENSE</a> file for details.</p>
</article></div></div>
  </body>
</html>
