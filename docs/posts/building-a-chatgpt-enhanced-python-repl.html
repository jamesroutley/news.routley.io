<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/">Original</a>
    <h1>Building a ChatGPT-enhanced Python REPL</h1>
    
    <div id="readability-page-1" class="page"><div data-pagefind-body="">
      <p>In this blog I share my experience in building a Python REPL augmented with ChatGPT. I explore how the application is built, and speculate on software engineering patterns and paradigms that might emerge in systems built on Large Language Models (LLMs).</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/intro-example.png" alt="GEPL - Generate, Evaluate, Print, Loop"/><figcaption>
            <h4>GEPL - Generate, Evaluate, Print, Loop</h4></figcaption>
</figure>

<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#introduction">
    <h2 id="introduction">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Introduction
    </h2>
</a>
<p>The Lisp programming language made REPLs (Read, Evaluate, Print, Loop) famous. REPLs are interactive programming environments where the programmer gets immediate feedback on lines of code they just typed. Today REPLs are common in Python, F#, and nearly every mainstream language.</p>
<p>While using ChatGPT through the OpenAI website I noticed parallels to a REPL. Both setup ongoing dialogues between a user and their computer system. The concept of REPL and ChatGPT sessions is that a single idea or concept can be declared and then refined until it works. The key feature is that the context of the conversation is preserved within a session. For REPLs this means symbols, state, and functions. For ChatGPT, it’s the thread of discussion.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/chatgpt.png" alt="ChatGPT - conversations have context."/><figcaption>
            <h4>ChatGPT - conversations have context.</h4></figcaption>
</figure>

<p>I wanted to explore how these two technologies could augment each other. I did this by creating <a href="https://github.com/lmortimer/gepl">GEPL</a> - Generate, Evaluate, Print Loop. It has the normal functionality of a Python REPL, you can type lines of code and execute them in the session. It also allows you to prompt the ChatGPT API to generate code for you. The ChatGPT prompt has context of code you’ve entered locally, so you can ask it to generate new code, or modify code you’ve written.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/weather-example.png" alt="GEPL"/><figcaption>
            <h4>GEPL</h4></figcaption>
</figure>

<p>Behind the scenes it uses the Python framework LangChain and OpenAI’s ChatGPT. However the code isn’t coupled to OpenAI’s implementation, and can be swapped out for other Chat Model LLMs as they’re released.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#architecture">
    <h2 id="architecture">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Architecture
    </h2>
</a>
<p>GEPL’s architecture unifies the state between a Python interpreter and a ChatGPT conversation. This enables ChatGPT to manipulate and design its answers around we’ve written locally.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/interop.png" alt="GEPL can modify code you&amp;rsquo;ve declared locally"/><figcaption>
            <h4>GEPL can modify code you&#39;ve declared locally</h4></figcaption>
</figure>

<p>GPT-3, GPT-4, and other APIs wouldn’t work because there’s no way to carry context across multiple prompts within a session. The type signature for those APIs are <code>str -&gt; str</code>, they are essentially functions which take in a string (the <em>prompt</em>) and return another string (the answer).</p>
<p>Chat Model APIs are also technically stateless in that every request is independent, however the API could be modelled as <code>List[Message] -&gt; str</code>, where it takes a list of messages and returns some answer. These messages can be one of two types:</p>
<ul>
<li><code>SystemMessage</code> - Messages from GEPL instructing ChatGPT how to behave.</li>
<li><code>HumanMessage</code> - Messages from the user prompting ChatGPT to respond.</li>
</ul>
<p>We’ll get into the details on the prompts of the messages below, but to grasp the magic of how this architecture works we need to understand that:</p>
<ol>
<li>GEPL maintains a local state of every command that has been typed into it and the result of execution.</li>
<li>Every time ChatGPT is called this historical state is passed as a list of <code>SystemMessage</code>.</li>
<li>The current prompt is sent as a <code>HumanMessage</code>.</li>
</ol>
<p>This allows ChatGPT to operate on code that either it or the user has written. Chat Model APIs are still very new and OpenAI’s ChatGPT is currently the only implementation. If you’re interested in more about the Chat Model API and how it differs to the other LLM APIs (eg. GPT-3, GPT-4) then read the Chat Models <a href="https://blog.langchain.dev/chat-models/">LangChain blog</a>.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#prompts">
    <h2 id="prompts">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Prompts
    </h2>
</a>
<p>Sometimes we want ChatGPT to generate some Python code. Other times we just want to tell it what has been executed in the REPL so that it maintains the state of the session. How do we do this? There’s nothing intrinsic in ChatGPT that it knows it’s a Python REPL. <strong>LLMs aren’t programmed through an API or configuration, they’re programmed through natural language called prompts</strong>. Prompts are equal parts powerful and fragile. What they allow us to do is amazing, but from an engineering and reliability point of view they can trip us up.</p>
<p>GEPL has <a href="https://github.com/lmortimer/gepl/blob/main/prompts.py">four types of prompts</a>:</p>
<ol>
<li><strong>Initial Prompt</strong> – A one off SystemMessage to bootstrap the conversation.</li>
<li><strong>Prompt for Code Generation</strong> - HumanMessage where the user prompts the LLM to write code.</li>
<li><strong>Generated Code Executed Prompt</strong> - SystemMessage passed back to the LLM to record execution of code it has generated.</li>
<li><strong>User Code Executed Prompt</strong> - SystemMessage passed back to the LLM to record execution of code the user wrote.</li>
</ol>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/interop.png" alt="Input and code execution add prompts to the stack."/><figcaption>
            <h4>Input and code execution add prompts to the stack.</h4></figcaption>
</figure>

<p>For this simple example, at the time of <code>say_hi(&#34;Hektor&#34;, &#34;Priam&#34;)</code> the prompt stack is as follows:</p>
<ol>
<li>Initial Prompt Message</li>
<li>User Code Executed Prompt: </li>
<li>User Code Executed Prompt: <code>say_hi(&#34;Hektor&#34;)</code></li>
<li>Prompt for Code Generation: <code> `rewrite say_hi to include the parameter last_name</code></li>
<li>Generated Code Execute Prompt: For when the above line was executed.</li>
</ol>
<p>Without these prompts ChatGPT would not know the state of code that either it or the user wrote, nor the symbols and side effects that are present in the GEPL. Now we’ll look at the four prompts in detail.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#initial-prompt">
    <h3 id="initial-prompt">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Initial Prompt
    </h3>
</a>
<p>Whenever GEPL calls the ChatGPT API, this is the first message it sees.</p>
<blockquote>
<p>You are a python code generator. Write well-written python 3 code.</p>
</blockquote>
<p>Some of those words look superfluous, some look bizarre, but every single one is required. These instruct the LLM …</p>
<ul>
<li>What it is (a code generator), what the code it generates will be used for, and that it should write well-written code.</li>
<li>What to do if it can’t generate the code. This acts as permission for it to ‘give up’ on a task, rather than hallucinate some answer that makes no sense.</li>
<li>The format in which it should reply. Without this the <code>str</code> returned by the API would be on one of four formats – with code blocks and text blocks in different locations making it a challenge to parse.</li>
</ul>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#prompt-for-code-generation">
    <h3 id="prompt-for-code-generation">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Prompt for Code Generation
    </h3>
</a>
<p>This prompt is always the last in the list of Messages passed to the ChatGPT API. It’s a direct pass through of what the user entered into the GEPL. eg.</p>
<blockquote>
<p>rewrite say_hi to include the parameter last_name</p>
</blockquote>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#generated-code-executed-prompt">
    <h3 id="generated-code-executed-prompt">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Generated Code Executed Prompt
    </h3>
</a>
<p>This SystemMessage records code that has been generated and the result of execution. It has the following prompt template.</p>
<blockquote>
<p>Previously the user asked you {message} and you generated code</p>
</blockquote>
<p>Where the bracketed parameters are substituted in. From the example above, once the line has been executed, the SystemMessage will be appended to the prompt stack and passed to the next call to the ChatGPT API with the following parameters.</p>
<ul>
<li><em>message</em> = <code>rewrite say_hi to include the parameter last_name</code></li>
<li><em>code</em> = <code>say_hi = lambda first_name, last_name: print(f&#34;Hello {first_name} {last_name}&#34;)</code></li>
<li><em>result</em> = <code>None</code> – as a function was defined.</li>
</ul>
<p>This template approach is implemented using LangChain’s <a href="https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html">PromptTemplate</a> abstraction.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#user-code-executed-prompt">
    <h3 id="user-code-executed-prompt">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    User Code Executed Prompt
    </h3>
</a>
<p>This SystemMessage records code that the user wrote and the result of execution. It has the following prompt template.</p>
<blockquote>
<p>The user has executed code. </p>
</blockquote>
<p>Substitution works identically to the Generated Code Executed Prompt.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#prompts-and-determinism">
    <h2 id="prompts-and-determinism">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Prompts and Determinism
    </h2>
</a>
<p>In the fifteen years I’ve been writing code this is the first time I’ve come across anything like the paradigm of prompts. In the same way that Lisp treats code as data, LLM applications treat natural language prompts as code. It’s a fundamentally different model of programming to what we’re used to. There’s no API to follow, just instruction and imagination.</p>
<p>Although powerful, LLMs instructed through natural language are very frail. Changing the wording in the prompt could result in radically different behaviour, both in terms of the logic the LLM applies, or the format in which it returns data. This is made more complex by the non-deterministic behaviour of LLMs <sup><a href="https://community.openai.com/t/a-question-on-determinism/8185">1</a></sup>. Even when setting the <a href="https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature">temperature</a>, a setting that controls how deterministic the generated responses are, to <code>0</code> the LLM still often replies with different answers to the same prompt across sessions.</p>
<p>For remotely hosted LLMs like ChatGPT, a separate concern is if the LLM itself is swapped out or upgraded without us knowing. Models will have optimisations and compromises, and be trained on different data sets. When an LLM is upgraded will my prompts respond in the same way? This highlights the importance of being able to pin a model version, and raises the question for engineers – how do we validate prompts across different LLMs?</p>
<p>From a software engineering perspective this lack of determinism is a problem. Today’s quality engineering practices such as unit tests and mocking seem inappropriate to validate natural language prompts on LLMs. As the technology evolves I see there being a greater demand for deterministic responses from LLMs. Toy Python REPLs are one thing, but medical and financial applications will have greater demands on the behaviour, predictability, and reliability of LLM responses.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#prompts-in-software-engineering">
    <h2 id="prompts-in-software-engineering">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Prompts in Software Engineering
    </h2>
</a>
<p>Construction of the prompt is also informal. Over time we’ll see best practices emerge. Some patterns exist today such as prompting the LLM to parse unstructured text and return data in a structured format like JSON. I can imagine a future where prompts become constructed through formal APIs in an ORM or fluent-style interface. This would allow for easier testing and to smooth out differences and features across LLMs.</p>
<div><pre tabindex="0"><code data-lang="fsharp"><span><span><span>Prompt</span>.new
</span></span><span><span><span>|&gt;</span> <span>Accepts</span>.types <span>[(</span>code<span>:</span> <span>string</span><span>);</span> <span>(</span>result<span>:</span> <span>string</span><span>)]</span>
</span></span><span><span><span>|&gt;</span> <span>Accepts</span>.from_prompt <span>&#34;The user has executed code and the result of that code being evaluated in a python3 interpreter&#34;</span>
</span></span><span><span><span>|&gt;</span> Must <span>&#34;do not run this code again&#34;</span>
</span></span><span><span><span>|&gt;</span> Must <span>&#34;remember the symbols, functions, and variables it defines&#34;</span>
</span></span><span><span><span>|&gt;</span> Returns ()
</span></span></code></pre></div><p>When I write software systems I start with type definitions. These are the core of the system and the rest of the code describes and enables this data to change over time. Implementation and logic emerges around the types, and I can then build the system in a maintainable manner. In writing GEPL the prompts seemed as important as the types. Less-so about the data format a given prompt returns, but more on the phrasing of the natural language that makes up the prompt. This equivalence of importance was reflected in the implementation, where prompts sit in equal importance to types.</p>
<p>The engineering paradigm <em>functional core, imperative shell</em> gives us sensible guidance to keep the core of our systems free of side effects and to push all state management to the edge of the application. Systems which call out to an LLM as a simple API would use this architecture. GEPL is tightly coupled to the LLM. I noticed that the core is actually the prompts, and the types need to react and wrap to whatever it is the LLM returns.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/functional-core.png" alt="Speculating on future architectures. Prompt core, functional wrapper, imperative shell."/><figcaption>
            <h4>Speculating on future architectures. Prompt core, functional wrapper, imperative shell.</h4></figcaption>
</figure>

<p><a href="https://langchain.com/">LangChain</a> is the first mover as an open source framework in which to build Python or Typescript applications that interact with LLMs. It’s what I used with GEPL, and allows you to abstract away from anything specific to a given vendor (OpenAI, Azure, Google, etc). OpenAI is the elephant in the room. They have both the most powerful LLMs, as well as the most mature APIs for interacting with the models. As Google and Amazon ramp up their availability of LLMs I expect to see some push and pull between the vendor APIs and LangChain.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#undefined-behaviour">
    <h2 id="undefined-behaviour">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Undefined Behaviour
    </h2>
</a>
<p>Decades of work has gone into developing debugging and observability tools for computer systems. With LLMs we start again from scratch. LLMs are complex black boxes which take in a prompt and return an answer.</p>
<p>Here’s an example of unexpected behaviour that I ran into while writing GEPL.</p>
<p>Below is an early version of the initial prompt. Key line bolded.</p>
<blockquote>
<p>You are a python code generator. Write well-written python 3 code.</p>
</blockquote>
<p>My thinking was that if the LLM can’t generate code then it should return a value like an exit code. This would be distinct from the success case of returning <code>STARTDESC</code> and <code>STARTCODE</code> blocks that I can parse. I test it out, and throw some unanswerable prompts at it and see that it’s working as intended.</p>
<p>Back to normal development, and I start seeing NOOPs where I don’t expect them.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/set-x-10-noop.png" alt="This sequence of commands returns a NOOP."/><figcaption>
            <h4>This sequence of commands returns a NOOP.</h4></figcaption>
</figure>

<p>Starting a brand new GEPL and calling the <code>set x to 10</code> without the <code>print</code> worked fine. Why would it consistently fail to generate code for <code>set x to 10</code> after I printed the integer 10?</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/set-x-10-success.png" alt="This works on its own."/><figcaption>
            <h4>This works on its own.</h4></figcaption>
</figure>

<p>At this stage I think that the LLM <em>thinks</em> that it cannot generate code for the simple task. Unlike every other computer API in existence <strong>we can prompt the LLM to tell us why it responded in the way it does</strong>. I replaced the bolded line of the prompt with:</p>
<blockquote>
<p>If you cannot return executable python code return set the reason why in the description and return no code.</p>
</blockquote>
<p>Re-ran, the problematic sequence of commands, and ChatGPT explains itself.</p>
<figure><img src="https://static.isthisit.nz/images/2023-04-18-gepl/set-x-10-bug.png" alt="Now the LLM tells us why it can&amp;rsquo;t generate the code."/><figcaption>
            <h4>Now the LLM tells us why it can&#39;t generate the code.</h4></figcaption>
</figure>

<p>There’s a peculiar asymmetry here. The same complexity that allows the LLM to <em>tell us</em> why it can’t do something also drives the reason why it can’t do it in the first place.</p>
<p>For this particular task it mistakenly thinks that it has already executed this line of code, and for some reason this prevents it from generating it again. Despite the former being false, I would still not expect the behaviour of <em>“It is not necessary to run it again”</em> to emerge. This could be fixed by tweaking the prompt template to tell it that it can, but without running into this bug I wouldn’t have predicted it emerging.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#conclusion">
    <h2 id="conclusion">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Conclusion
    </h2>
</a>
<p>Prompt-powered LLMs are a new paradigm in software engineering. It expands the class of systems we think are possible to make, but introduces inherent complexity and risk. On one hand we get massive benefits – behaviour that would otherwise be thousands of lines of code to implement, and systems which can tell us why they can’t do something. On the other hand we need to deal with the fragility that is prompts, and the behaviour of LLMs to do things even when unprompted.</p>
<p>Working on this project was a lot of fun. If you’re a software engineer I highly recommend trying out LangChain, LLMs, and experimenting with prompts.</p>
<p>Full source code of GEPL is <a href="https://github.com/lmortimer/gepl">available on Github</a>.</p>
<a href=" https://isthisit.nz/posts/2023/building-a-chat-gpt-enhanced-python-repl/#further-reading">
    <h2 id="further-reading">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="black" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <title>Link to this section</title>
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
        <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
    </svg>
    Further Reading
    </h2>
</a>
<ul>
<li><a href="https://huyenchip.com/2023/04/11/llm-engineering.html">LLM Engineering</a> - Chip Huyen.</li>
<li><a href="https://www.youtube.com/@samwitteveenai/videos">Exploring ML and AI</a> - Sam Witteveen</li>
<li><a href="https://blog.langchain.dev/chat-models/">Chat Models</a> - LangChain.</li>
</ul>

    </div></div>
  </body>
</html>
