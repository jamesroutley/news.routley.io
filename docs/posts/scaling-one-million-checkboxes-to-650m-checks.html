<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eieio.games/essays/scaling-one-million-checkboxes/">Original</a>
    <h1>Scaling One Million Checkboxes to 650M checks</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>On June 26th 2024 I <a href="https://eieio.games/nonsense/game-14-one-million-checkboxes/">launched a website</a> called One Million Checkboxes (OMCB). It had one million <em>global</em> checkboxes on it - checking a box checked it for everyone on the site, immediately.</p>

<div>
<div>
    
    
    <video playsinline="" controls="" muted="" poster=" /assets/images/scaling-omcb/30mins-activity-firstframe.png ">
    
        <source src="/assets/images/scaling-omcb/30mins-activity.mp4" type="video/mp4"/>
    </video>
    
    <p> The site, 30 minutes after launch </p>
    
</div>

</div>

<p>I built the site in 2 days. I thought I’d get a few hundred users, max. That is <em>not</em> what happened.</p>

<p>Instead, within hours of launching, tens of thousands of users checked millions of boxes. They piled in from <a href="https://news.ycombinator.com/item?id=40975509">Hacker News</a>, <a href="https://www.reddit.com/r/InternetIsBeautiful/comments/1dp4t7y/onemillioncheckboxescom_a_webpage_with_one/">/r/InternetIsBeautiful</a>, <a href="https://mastodon.gamedev.place/@eieio/112683641983412025">Mastodon</a> and <a href="https://x.com/itseieio/status/1805986839058079896">Twitter</a>. A few days later OMCB appeared in the <a href="https://www.washingtonpost.com/technology/2024/07/02/one-million-checkboxes-pointless-fun/">Washington Post</a> and the <a href="https://www.nytimes.com/2024/07/03/style/one-million-checkboxes-game.html">New York Times</a>.</p>

<p>Here’s what activity looked like on the first day (I launched at 11:30 AM EST).</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/first-day-hourly-unique-visitors.png" alt="boxes checked hourly during OMCB&#39;s first day. The first few hours are missing."/>
    <img src="https://eieio.games/assets/images/scaling-omcb/first-day-hourly-checked-boxes.png" alt="boxes checked hourly during OMCB&#39;s first day. The first few hours are missing."/>
</p>
<p>I don&#39;t have logs for checked boxes from the first few hours because I originally only kept the latest 1 million logs for a given day(!)</p>

<p>I wasn’t prepared for this level of activity. The site crashed a lot. But by day 2 I started to stabilize things and people checked over 50 million boxes. We passed 650 million before I sunset the site 2 weeks later.</p>

<p>Let’s talk about how I kept the site (mostly) online!
<!-- excerpt-end --></p>
<h2 id="the-original-architecture">The original architecture</h2>
<p>Here’s the gist of the original architecture:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/simple-original-arch.png" alt="The original architecture of OMCB. The client maintains a bitset tracking which boxes are checked and renders checkboxes lazily. The server has 1 million bits of state - one for each checkbox. The server flips bit 3 when box 3 is checked or unchecked."/>
</p>
<p>Our checkbox state is just one million bits (125KB). A bit is “1” if the corresponding checkbox is checked and “0” otherwise.</p>

<p>Clients store the bits in a <a href="https://en.wikipedia.org/wiki/Bit_array">bitset</a> (an array of bytes that makes it easy to store, access, and flip raw bits) and reference that bitset when rendering checkboxes. Clients tell the server when they check a box; the server flips the relevant bit and broadcasts that fact to all connected clients.</p>

<p>To avoid throwing a million elements into the DOM, clients only render the checkboxes in view (plus a small buffer) using <a href="https://www.npmjs.com/package/react-window">react-window</a>.</p>

<p>I could have done this with a single process,I wanted an architecture that I <em>could</em> scale (and an excuse to use Redis for the first time in years). So the actual server setup looked like this:</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/original-arch-reverse-proxy.png" alt="The original architecture of OMCB. A client connects to nginx, which reverse proxies the client to a a Flask instance. Flask interfaces with Redis."/>
</p>

<p>Clients hit nginx for static content, and then make a GET for the bitset state and a websocket connection (for updates); nginx (acting as a reverse proxy) forwards those requests to one of two Flask servers (run via gunicorn).</p>

<p>State is stored in Redis, which has good primitives for flipping individual bits. Clients tell Flask when they check a box; Flask updates the bits in Redis and writes an event to a pubsub (message queue). Both Flask servers read from that pubsub and notify connected clients when checkboxes are checked/unchecked.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/original-arch-request.png" alt="The original architecture of OMCB. A client connects to nginx, which reverse proxies the client to a a Flask instance. Flask interfaces with Redis."/>
</p>
<p>We need the pubsub because we&#39;ve got two Flask instances; a Flask instance can&#39;t just broadcast &#34;box 2 was checked&#34; to its own clients.</p>

<p>Finally, the Flask servers do simple rate-limiting (on requests per session and new sessions per IP<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> - foolishly stored in Redis!) and regularly send full state snapshots to connected clients (in case a client missed an update because, say, the tab was backgrounded).</p>

<details>
    <summary>Here&#39;s an (abbreviated) implementation of the Flask side.</summary>
    
<figure><pre><code data-lang="python"><span># This is long, so I&#39;ve skipped Redis initialization, 
# my rate limiting and logging implementation, etc.
</span>
<span># You can run lua inside Redis. The lua runs atomically.
# It&#39;s pretty sweet.
</span><span>set_bit_script</span> <span>=</span> <span>&#34;&#34;&#34;
local key = KEYS[1]
local index = tonumber(ARGV[1])
local value = tonumber(ARGV[2])
local current = redis.call(&#39;getbit&#39;, key, index)
local diff = value - current
redis.call(&#39;setbit&#39;, key, index, value)
redis.call(&#39;incrby&#39;, &#39;count&#39;, diff)
return diff&#34;&#34;&#34;</span>
<span>set_bit_sha</span> <span>=</span> <span>redis_client</span><span>.</span><span>script_load</span><span>(</span><span>set_bit_script</span><span>)</span>

<span>def</span> <span>get_bit</span><span>(</span><span>index</span><span>):</span>
    <span>return</span> <span>bool</span><span>(</span><span>redis_client</span><span>.</span><span>getbit</span><span>(</span><span>&#39;bitset&#39;</span><span>,</span> <span>index</span><span>))</span>

<span>def</span> <span>set_bit</span><span>(</span><span>index</span><span>,</span> <span>value</span><span>):</span>
    <span>diff</span> <span>=</span> <span>redis_client</span><span>.</span><span>evalsha</span><span>(</span><span>set_bit_sha</span><span>,</span> <span>1</span><span>,</span> <span>&#39;bitset&#39;</span><span>,</span> <span>index</span><span>,</span> <span>int</span><span>(</span><span>value</span><span>))</span>
    <span>return</span> <span>diff</span> <span>!=</span> <span>0</span>

<span>def</span> <span>get_full_state</span><span>():</span>
    <span>raw_data</span> <span>=</span> <span>redis_client</span><span>.</span><span>get</span><span>(</span><span>&#34;bitset&#34;</span><span>)</span>
    <span>return</span> <span>base64</span><span>.</span><span>b64encode</span><span>(</span><span>raw_data</span><span>).</span><span>decode</span><span>(</span><span>&#39;utf-8&#39;</span><span>)</span>

<span>def</span> <span>get_count</span><span>():</span>
    <span>return</span> <span>int</span><span>(</span><span>redis_client</span><span>.</span><span>get</span><span>(</span><span>&#39;count&#39;</span><span>)</span> <span>or</span> <span>0</span><span>)</span>

<span>def</span> <span>publish_toggle_to_redis</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>):</span>
    <span>redis_client</span><span>.</span><span>publish</span><span>(</span><span>&#39;bit_toggle_channel&#39;</span><span>,</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>&#39;index&#39;</span><span>:</span> <span>index</span><span>,</span> <span>&#39;value&#39;</span><span>:</span> <span>new_value</span><span>}))</span>

<span>def</span> <span>state_snapshot</span><span>():</span>
    <span>full_state</span> <span>=</span> <span>get_full_state</span><span>()</span>
    <span>count</span> <span>=</span> <span>get_count</span><span>()</span>
    <span>return</span> <span>{</span><span>&#39;full_state&#39;</span><span>:</span> <span>full_state</span><span>,</span> <span>&#39;count&#39;</span><span>:</span> <span>count</span><span>}</span>

<span>@</span><span>app</span><span>.</span><span>route</span><span>(</span><span>&#39;/api/initial-state&#39;</span><span>)</span>
<span>def</span> <span>get_initial_state</span><span>():</span>
    <span>return</span> <span>jsonify</span><span>(</span><span>state_snapshot</span><span>())</span>

<span>def</span> <span>emit_full_state</span><span>():</span>
    <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>&#39;full_state&#39;</span><span>,</span> <span>state_snapshot</span><span>())</span>

<span>@</span><span>socketio</span><span>.</span><span>on</span><span>(</span><span>&#39;toggle_bit&#39;</span><span>)</span>
<span>def</span> <span>handle_toggle</span><span>(</span><span>data</span><span>):</span>
    <span>if</span> <span>not</span> <span>allow_toggle</span><span>(</span><span>request</span><span>.</span><span>sid</span><span>):</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;Rate limiting toggle request for </span><span>{</span><span>request</span><span>.</span><span>sid</span><span>}</span><span>&#34;</span><span>)</span>
        <span>return</span> <span>False</span>
    <span># There&#39;s a race here. It bit me pretty early on; I fixed it by 
</span>    <span># moving this logic into the lua script.
</span>    <span>count</span> <span>=</span> <span>get_count</span><span>()</span>
    <span>if</span> <span>count</span> <span>&gt;=</span> <span>1_000_000</span><span>:</span>
        <span>print</span><span>(</span><span>&#34;DISABLED TOGGLE EXCEEDED MAX&#34;</span><span>)</span>
        <span>return</span> <span>False</span>
    <span>index</span> <span>=</span> <span>data</span><span>[</span><span>&#39;index&#39;</span><span>]</span>
    <span>current_value</span> <span>=</span> <span>get_bit</span><span>(</span><span>index</span><span>)</span>
    <span>new_value</span> <span>=</span> <span>not</span> <span>current_value</span>
    <span>print</span><span>(</span><span>f</span><span>&#34;Setting bit </span><span>{</span><span>index</span><span>}</span><span> to </span><span>{</span><span>new_value</span><span>}</span><span> from </span><span>{</span><span>current_value</span><span>}</span><span>&#34;</span><span>)</span>
    <span>set_bit</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>)</span>
    <span>forwarded_for</span> <span>=</span> <span>request</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>&#39;X-Forwarded-For&#39;</span><span>)</span> <span>or</span> <span>&#34;UNKNOWN_IP&#34;</span>
    <span># This only keeps the most recent 1 million logs for the day
</span>    <span>log_checkbox_toggle</span><span>(</span><span>forwarded_for</span><span>,</span> <span>index</span><span>,</span> <span>new_value</span><span>)</span>
    <span>publish_toggle_to_redis</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>)</span>

<span>def</span> <span>emit_state_updates</span><span>():</span>
    <span>scheduler</span><span>.</span><span>add_job</span><span>(</span><span>emit_full_state</span><span>,</span> <span>&#39;interval&#39;</span><span>,</span> <span>seconds</span><span>=</span><span>30</span><span>)</span>
    <span>scheduler</span><span>.</span><span>start</span><span>()</span>

<span>emit_state_updates</span><span>()</span>

<span>def</span> <span>handle_redis_messages</span><span>():</span>
    <span>message_count</span> <span>=</span> <span>0</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>message</span> <span>=</span> <span>pubsub</span><span>.</span><span>get_message</span><span>(</span><span>timeout</span><span>=</span><span>0.01</span><span>)</span>
        <span>if</span> <span>message</span> <span>is</span> <span>None</span><span>:</span>
            <span>break</span>

        <span>if</span> <span>message</span><span>[</span><span>&#39;type&#39;</span><span>]</span> <span>==</span> <span>&#39;message&#39;</span><span>:</span>
            <span>try</span><span>:</span>
                <span>data</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>message</span><span>[</span><span>&#39;data&#39;</span><span>])</span>
                <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>&#39;bit_toggled&#39;</span><span>,</span> <span>data</span><span>)</span>
                <span>message_count</span> <span>+=</span> <span>1</span>
            <span>except</span> <span>json</span><span>.</span><span>JSONDecodeError</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>&#34;Failed to decode message: </span><span>{</span><span>message</span><span>[</span><span>&#39;data&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>)</span>

        <span>if</span> <span>message_count</span> <span>&gt;=</span> <span>100</span><span>:</span>
            <span>break</span>

    <span>if</span> <span>message_count</span> <span>&gt;</span> <span>0</span><span>:</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;Processed </span><span>{</span><span>message_count</span><span>}</span><span> messages&#34;</span><span>)</span>

<span>def</span> <span>setup_redis_listener</span><span>():</span>
    <span>scheduler</span><span>.</span><span>add_job</span><span>(</span><span>handle_redis_messages</span><span>,</span> <span>&#39;interval&#39;</span><span>,</span> <span>seconds</span><span>=</span><span>0.1</span><span>)</span>

<span>setup_redis_listener</span><span>()</span>
    </code></pre></figure>

</details>

<p>This code isn’t great! It’s not even async. I haven’t shipped production Python in like 8 years! But I was fine with that. I didn’t think the project would be popular. This was good enough.</p>

<p>I changed a lot of OMCB but the basic architecture - nginx reverse proxy, API workers, Redis for state and message queues - remained.</p>

<h2 id="principles-for-scaling">Principles for scaling</h2>
<p>Before I talk about what changed, let’s look at the principles I had in mind while scaling.</p>

<h4 id="bound-my-costs">Bound my costs</h4>
<p>I needed to be able to math out an upper bound on my costs. I aimed to <a href="https://x.com/jonchurch/status/1809307525524631621">let things break when they broke my expectations</a> instead of going serverless and scaling into bankruptcy.</p>

<h4 id="embrace-the-short-term">Embrace the short-term</h4>
<p>I assumed the site’s popularity was fleeting. I took on technical debt and aimed for ok solutions that I could hack out in hours over great solutions that would take me days or weeks.</p>

<h4 id="use-simple-self-hosted-tech">Use simple, self-hosted tech</h4>
<p>I’m used to running my own servers. I like to log into boxes and run commands. I tried to only add dependencies that I could run and debug on my own.</p>

<h4 id="have-fun">Have fun</h4>
<p>I optimized for fun, not money. Scaling the site my way was fun. So was saying no to advertisers.</p>

<h4 id="keep-it-global">Keep it global</h4>
<p>The magic of the site was jumping <em>anywhere</em> and seeing immediate changes. So I didn’t want to scale by, for example, sending clients a view of only the checkboxes they were looking at<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<h2 id="day-1-the-pop">Day 1: the pop</h2>

<p>Within <a href="https://x.com/itseieio/status/1805994700760830066">30 minutes of launch</a>, activity looked like this:</p>


<p>The site was still up, but I knew it wouldn’t tolerate the load for much longer.</p>

<p>The most obvious improvement was more servers. Fortunately this was easy - nginx could easily reverse-proxy to Flask instances on another VM, and my state was already in Redis. I started spinning up more boxes.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/load-second-server.jpeg" alt="Load on the second OMCB server is 100%"/>
</p>
<p>I spun up the second server around 12:30 PM. Load immediately hit 100%</p>

<p>I originally assumed another server or two would be sufficient. Instead traffic grew as I scaled. I hit #1 on Hacker News; activity on my tweet skyrocketed. I looked for bigger optimizations.</p>

<p>My Flask servers were struggling. Redis was running out of connections (did you notice I wasn’t using a connection pool?). My best idea was to batch updates - I hacked something in that looked like this:</p>

<details>
<summary>Batching logic</summary>

<figure><pre><code data-lang="python"><span>def</span> <span>handle_Redis_messages</span><span>():</span>
    <span>message_count</span> <span>=</span> <span>0</span>
    <span>updates</span> <span>=</span> <span>[]</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>message</span> <span>=</span> <span>pubsub</span><span>.</span><span>get_message</span><span>(</span><span>timeout</span><span>=</span><span>0.01</span><span>)</span>
        <span>if</span> <span>message</span> <span>is</span> <span>None</span><span>:</span>
            <span># No more messages available
</span>            <span>break</span>

        <span>if</span> <span>message</span><span>[</span><span>&#39;type&#39;</span><span>]</span> <span>==</span> <span>&#39;message&#39;</span><span>:</span>
            <span>try</span><span>:</span>
                <span>data</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>message</span><span>[</span><span>&#39;data&#39;</span><span>])</span>
                <span>updates</span><span>.</span><span>append</span><span>(</span><span>data</span><span>)</span>
                <span>message_count</span> <span>+=</span> <span>1</span>
            <span>except</span> <span>json</span><span>.</span><span>JSONDecodeError</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>&#34;Failed to decode message: </span><span>{</span><span>message</span><span>[</span><span>&#39;data&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>)</span>

        <span>if</span> <span>message_count</span> <span>&gt;=</span> <span>100</span><span>:</span>
            <span>break</span>

    <span>if</span> <span>message_count</span> <span>&gt;</span> <span>0</span><span>:</span>
        <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>&#39;batched_bit_toggles&#39;</span><span>,</span> <span>updates</span><span>)</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;Processed </span><span>{</span><span>message_count</span><span>}</span><span> messages&#34;</span><span>)</span></code></pre></figure>

</details>

<p>I didn’t bother with backwards compatibility. I figured folks were used to the site breaking and would just refresh.</p>

<p>I also added a connection pool. This <em>definitely</em> did not play nicely with gunicorn and Flask<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>, but it did seem to reduce the number of connections to Redis<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>.</p>

<p>I also beefed up my Redis box - easy to do since I was using Digital Ocean’s managed Redis - from a tiny (1 shared CPU; 2 GB RAM) instance to a box with 4 dedicated CPUs and 32 GB of RAM (I did this after Redis mysteriously went down). The resizing took about 30 minutes; the server came back up.</p>

<p>And then things got trickier.</p>

<h3 id="it-was-not-a-good-night-to-have-plans">It was not a good night to have plans</h3>

<p>At around 4:30 PM I accepted it: <em>I had plans.</em> I had spent June at <a href="https://tisch.nyu.edu/itp/camp">a camp at ITP</a> - a school at NYU<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup>. And the night of the 26th was our final show. I had signed up to display a <a href="https://www.instagram.com/p/C8dJ_ZXP3yr/">face-controlled Pacman game</a> and invited some friends - I had to go!</p>

<p>I brought an iPad and put OMCB on it. I spun up servers while my friend <a href="https://x.com/UriBram">Uri</a> and my girlfriend Emma kindly stepped in to explain what I was doing to strangers when they came by my booth.</p>

<p>I had no automation for spinning up servers (oops) so my naming conventions evolved as I worked.</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/dropletnames.png" alt="Droplet names for OMCB. The names become increasingly terse."/>
</p>
<p>My servers. I ended up with 8 worker VMs</p>

<p>I got home from the show around midnight. I was tired. But there was still more work to do, like:</p>
<ul>
  <li>Reducing the number of Flask processes on each box (I originally had more workers than the number of cores on a box; this didn’t work well)</li>
  <li>Increasing the batch size of my updates - I found that doubling the batch size substantially reduced load. I tried doubling it again. This appeared to help even more. I don’t know how to pick a principled number here.</li>
</ul>

<h3 id="bandwidth">Bandwidth</h3>
<p>I pushed the updates. I was feeling good! And then I got a text from my friend <a href="https://greg.technology">Greg Technology</a>.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/gregtext.png" alt="A screenshot of a text message. It says &#39;I AM SO PROUD. R u gonna get hit w a server bill for bandwidth or is the banw not cray cray cray cray.&#39;"/>
</p>
<p>gulp</p>

<p>I realized <em>I hadn’t thought hard enough about bandwidth.</em>  Digital Ocean’s bandwidth pricing is pretty sane ($0.01/GB after a pretty generous per-server compounding free allowance). I had a TB of free bandwidth from <a href="https://eieio.games/nonsense/game-12-stranger-video/">past work</a> and (pre-launch) didn’t think OMCB would make a dent.</p>

<p>I did back of the envelope math. I send state snapshots (1 million bits; 1 Mbit) every 30 seconds. With 1,000 clients that’s already 2GB a minute! Or 120GB an hour. And we’re probably gonna have more clients than that. And we haven’t even started to think about updates.</p>

<p>It was 2 AM. I was very tired. I did some bad math - maybe I confused GB/hour with GB/minute? - and  <em>freaked out</em>. I thought I was already on the hook for thousands of dollars!</p>

<p>So I did a couple of things:</p>
<ul>
  <li>Frantically texted Greg, who helped me realize that my math was way off.</li>
  <li>Ran <code>ip -s link show dev eth0</code> on my nginx box to see how many bytes I had sent, confirming that my math was way off<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>.</li>
  <li>Started thinking about how to reduce bandwidth - and how to cap my costs.</li>
</ul>

<p>I immediately reduced the frequency of my state snapshots, and then (with some help from Greg) pared down the size of the incremental updates I sent to clients.</p>

<div><div><pre><code><span>// Original batch implementation</span>
<span>// (I just took the individual updates I was sending to clients and put them in a list)</span>
<span>[{</span> <span>&#34;</span><span>index</span><span>&#34;</span><span>:</span> <span>123</span><span>,</span> <span>&#34;</span><span>value</span><span>&#34;</span><span>:</span> <span>true</span> <span>},</span> <span>{</span> <span>&#34;</span><span>index</span><span>&#34;</span><span>:</span> <span>124</span><span>,</span> <span>&#34;</span><span>value</span><span>&#34;</span><span>:</span> <span>false</span> <span>},</span> <span>{</span><span>&#34;</span><span>index</span><span>&#34;</span><span>:</span> <span>125</span><span>,</span> <span>&#34;</span><span>value</span><span>&#34;</span><span>:</span> <span>true</span><span>}]</span>
<span>// Final implementation - array of true values, array of false values</span>
<span>[[</span><span>123</span><span>,</span> <span>125</span><span>],</span> <span>[</span><span>124</span><span>]]</span>
</code></pre></div></div>
<p>I moved from stuffing a bunch of dicts into a list to sending two arrays of indices with <code>true</code> and <code>false</code> implied. This was <em>five times shorter</em> than my original implementation!</p>

<p>And then I used linux’s <a href="https://en.wikipedia.org/wiki/Tc_(Linux)"><code>tc</code></a> utility to slam a hard cap on the amount of data I could send per second. <code>tc</code> is famously hard to use<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>, so I wrote my configuration script with Claude’s help.</p>

<details>
<summary>the tc script</summary>

<figure><pre><code data-lang="bash"><span>INTERFACE</span><span>=</span>eth0
<span>LIMIT</span><span>=</span>250mbit
tc qdisc del dev <span>$INTERFACE</span> root
tc qdisc add dev <span>$INTERFACE</span> root handle 1: htb default 10
tc class add dev <span>$INTERFACE</span> parent 1: classid 1:1 htb rate <span>$LIMIT</span>
tc class add dev <span>$INTERFACE</span> parent 1:1 classid 1:10 htb rate <span>$LIMIT</span>
tc filter add dev <span>$INTERFACE</span> protocol ip parent 1:0 prio 1 u32 match ip dst 0.0.0.0/0 flowid 1:10</code></pre></figure>

</details>

<p>This just limits traffic flowing over eth0 (my public interface) to 250Mbit a second. That’s a lot of bandwidth - ~2GB/min, or just under 3 TB a day. But it let me reason about my costs, and at $0.01/GB I knew I wouldn’t go bankrupt overnight.</p>

<p>At around 3:30 AM I got in bed.</p>

<p>My server was pegged at my 250 Mb/s limit for much of the night. I originally thought I was lucky to add limits when I did; I now realize someone probably saw <a href="https://x.com/itseieio/status/1806206039945060364">my tweet </a> about reducing bandwidth and tried to give me a huge bill.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/nginx-bandwidth-cropped.png" alt="Bandwidth usage from nginx. The server is pegged at the bandwidth lmiit for several hours"/>
    <img src="https://eieio.games/assets/images/scaling-omcb/bandwidth-tweet.png" alt="A tweet. It says &#39;doing this again, sorry! I am significantly reducing message size to keep things snappier and avoid bankrupting myself on bandwidth costs lol&#39;"/>
</p>
<p>Blue is traffic from my workers to nginx, purple is nginx out to the world. The timing is suspicious</p>

<h2 id="day-2-its-still-growing">Day 2: it’s still growing</h2>

<p>I woke up a few hours later. The site was down. I hadn’t been validating input properly.</p>

<p>The site didn’t prevent folks from checking boxes above 1 million. Someone had checked boxes in the hundred million range! This let them push the count of checked boxes to 1 million, tricking the site into thinking things were over<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" rel="footnote">8</a></sup>.</p>

<p>Redis had also added millions of 0s (between bit one million and bit one hundred million), which 100x’d the data I was sending to clients.</p>

<p>This was embarrassing - I’m new to building for the web but like…I know you should validate your inputs! But it was a quick fix. I stopped nginx, copied the first million bits of my old bitset to a new truncated bitset (I wanted to keep the old one for debugging), taught my code to reference the new bitset, and added proper validation.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/redis-truncated.png" alt="The size of the old bitset and new bitset in redis. The old bitset has 12500000 bytes; the new one has 125000"/>
</p>
<p>much better</p>

<p>Not too bad! I brought the site back up.</p>

<h3 id="adding-a-replica-and-portscanning-my-vpc">Adding a replica and portscanning my VPC</h3>

<p>The site was <em>slow</em>. The number of checked boxes per hour quickly exceeded the day 1 peak.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/hourly-unique-players-day-2.png" alt="Unique visitors to OMCB on day 2. The site peaks with over 20,000 unique visitors in an hour."/>
    <img src="https://eieio.games/assets/images/scaling-omcb/hourly-checked-boxes-day-2.png" alt="Boxes checked during OMCB&#39;s second day. Millions of boxes are being checked an hour."/>
</p>

<p>The biggest problem was the initial page load. This made sense - we had to hit Redis, which was under a lot of load (and we were making too many connections to it due to bugs in my connection pooling).</p>

<p>I was tired and didn’t feel equipped to debug my connection pool issues. So I embraced the short term and spun up a Redis replica to take load off the primary and spread my connections out.</p>

<p>But there was a problem - after spinning up the replica, I couldn’t find its private IP!</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/omcb-connection-details.gif" alt="Obfuscated connection details for the OMCB redis instance. Selecting &#39;VPC network&#39; gives a DNS entry with the &#39;private&#39; prefix"/>
</p>
<p>I got my Redis instance&#39;s private IP by prepending &#34;private-&#34; to its DNS entry</p>

<p>To connect to my primary, I used a DNS record - there were records for its public and private IPs. Digital Ocean told me to prepend <code>replica-</code> to those records to get my replica IP. This worked for the public one, but didn’t exist for the private DNS record! And I really wanted the private IP.</p>

<p>I thought sending traffic to a public IP would risk traversing the public internet, which would mean being billed for way more bandwidth<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">9</a></sup>.</p>

<p>Since I couldn’t figure out how to find the replica’s private IP in an official way (I’m sure you can! Tell me how!), I took a different approach and starting making connections to private IPs close to the IPs of my Redis primary and my other servers. This worked on the third or fourth try.</p>

<p>Then I hardcoded that IP as my replica IP!</p>

<h3 id="stabilizing">Stabilizing</h3>

<p>My Flask processes kept crashing, requiring me to babysit the site. The crashes seemed to be from running out of Redis connections. I’m wincing as I type this now, but I still didn’t want to debug what was going on there - it was late and the problem was fuzzy.</p>

<p>So I wrote a script that looked at the number of running Flask processes and bounced my systemd unit if too many were down<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" rel="footnote">10</a></sup>.</p>

<details>
    <summary>My bash restart script</summary>
    
<figure><pre><code data-lang="bash"><span>#!/bin/bash</span>

<span># We have 4 running processes. This script only restarts them if fewer than 3 are running;</span>
<span># I figured this might help us reach a stable equilibirum.</span>

<span># jitter</span>
<span>x</span><span>=</span><span>$((</span>RANDOM <span>%</span> <span>300</span><span>))</span>
<span>echo</span> <span>&#34;sleeping for </span><span>$x</span><span>&#34;</span>
<span>sleep</span> <span>&#34;</span><span>$x</span><span>&#34;</span>

count_running_servers<span>()</span> <span>{</span>
    ps aux | <span>grep </span>gunicorn | <span>grep</span> <span>-oP</span> <span>&#39;bind \K0\.0\.0\.0:\d+&#39;</span> | <span>sort</span> | <span>uniq</span> | <span>wc</span> <span>-l</span>
<span>}</span>

<span>running_servers</span><span>=</span><span>$(</span>count_running_servers<span>)</span>

<span>if</span> <span>[</span> <span>$running_servers</span> <span>-lt</span> 3 <span>]</span><span>;</span> <span>then
    </span><span>echo</span> <span>&#34;Less than 3 servers running. Restarting all servers...&#34;</span>

    <span>sudo </span>systemctl stop one-million-checkboxes.service

    <span>sleep </span>10

    <span>sudo </span>systemctl start one-million-checkboxes.service

    <span>echo</span> <span>&#34;Servers restarted.&#34;</span>
<span>else
    </span><span>echo</span> <span>&#34;At least 3 servers are running. No action needed.&#34;</span>
<span>fi</span></code></pre></figure>

</details>

<p>I threw that into the crontab on my boxes and updated my nginx config to briefly take servers out of rotation if they were down (I should have done this sooner!). This appeared to work pretty well. The site stabilized.</p>

<h3 id="stale-updates">Stale updates</h3>

<p>At around 12:30 AM I posted some stats on Twitter and got ready to go to bed. And then a user reported an issue:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/ohgod.png" alt="A user reports a bug around the site changing surprisingly quickly"/>
</p>
<p>ahhhhhhhhhhhh</p>

<p>I had written a classic bug.</p>

<p>To keep client checkbox state synchronized, I did two things:</p>
<ul>
  <li>Sent clients incremental updates when checkboxes were checked or unchecked</li>
  <li>Sent clients occasional full-state snapshots in case they missed an update</li>
</ul>

<p>These updates didn’t have timestamps. A client could receive a new full-state snapshot and then apply an old incremental update - resulting in them having a totally wrong view of the world until the next full-state snapshot<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" rel="footnote">11</a></sup>.</p>

<p>I was embarrassed by this - I’ve written a whole lot of state machine code and know better. It was almost 1 AM and I had barely slept the night before; it was a struggle to write code that I (ironically) thought I could write in my sleep. But I:</p>
<ul>
  <li>Timestamped each full state snapshot</li>
  <li>Timestamped each update written to my Redis pubsub</li>
  <li>Added the <em>max</em> timestamp of each incremental update in the batches I sent to clients</li>
  <li>Taught clients to drop update batches if their timestamp was behind the timestamp of the last full-state snapshot</li>
</ul>

<p>This isn’t perfect (clients can apply a batch of mostly-stale updates as long as one update is new) but it’s substantially better.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/claude-im-tired.png" alt="The author asking Claude to double check their work. They say they&#39;re very tired."/>
</p>
<p>me to claude, 1 AM</p>

<p>I ran my changes by Claude before shipping to prod. Claude’s suggestions weren’t actually super helpful, but talking through why they were wrong gave me more confidence.</p>

<h2 id="rewrite-in-go">Rewrite in go</h2>

<p>I woke up the next morning and the site was still up! Hackily restarting your servers is great.  This was great timing - the site was attracting more mainstream media attention (I woke up to an email from the Washington Post).</p>

<p>I moved my attention from keeping the site up to thinking about how to wind it down. I was still confident folks wouldn’t be interested in the site forever, and I wanted to provide a real ending before <em>everyone</em> moved on<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" rel="footnote">12</a></sup>.</p>

<p>I came up with a plan - I’d make checked boxes freeze if they weren’t unchecked quickly. I wasn’t sure that my current setup could handle this - it might result in a spike of activity plus I’d be asking my servers to do more work.</p>

<p>So (after taking a break for a day) I got brunch with my friend <a href="https://github.com/EliotHedeman">Eliot</a> - a super talented performance engineer - and asked if he was down to give me a hand. He was, and from around 2 PM to 2 AM on Sunday we discussed implementations of my sunsetting plan and then rewrote the whole backend in go!</p>

<p>The go rewrite was straightforward; we ported without many changes. Lots of our sticking points were things like “finding a go <code>socketio</code> library that supports the latest version of the protocol.”</p>

<p>The speedup was staggering.</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/cropped-cpu-go-deployment.png" alt="CPU usage on a worker VM. It drops dramatically after the go deployment"/>
</p>
<p>CPU usage on a worker VM</p>

<p>Things were actually so much faster that we ended up needing to add better rate-limiting; originally we scaled too well and bots on the site were able to push absurd amounts of traffic through the site.</p>

<p>The site <em>was</em> DDOS’d on Sunday night, but addressing this was pretty simple - I just threw the site behind CloudFlare and updated my nginx configs a bit.</p>

<h2 id="sunsetting-the-site">Sunsetting the site</h2>

<p>The site was rock-solid after the go rewrite. I spent the next week doing interviews, enjoying the attention, and trying to relax.</p>

<p>And then I got to work on sunsetting. Checked boxes would freeze if they weren’t unchecked quickly, which would eventually leave the site totally frozen. The architecture here ended up being pretty simple - mostly some more state in Redis:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/sunset-arch.png" alt="Architecture diagram. It shows a hashtable that maps box # to the last time it was checked, a frozen bitset that tracks which cells are disabled, and a time to freeze variable that represents how long before a box should be frozen"/>
</p>
<p>god i love making diagrams in tldraw</p>

<p>I added a hashtable that tracked the last time that a box was checked (this would be too much state to pass to clients, but was fine to keep in Redis), along with a “time to freeze” value. When trying to uncheck a box, we’d first check whether <code>now - last_checked &gt; time_to_freeze</code> - if it is, we don’t uncheck the box and instead update <code>frozen_bitset</code> to note that the relevant checkbox is now frozen.</p>

<p>I distributed <code>frozen_bitset</code> state to clients the same way that I distributed which boxes were checked, and taught clients to disable a checkbox if it was in the frozen bitset. And I added a job to periodically search for bits that <em>should</em> be frozen (but weren’t yet because nobody had tried to uncheck them) and freeze those.</p>

<p>Redis made it soooo easy to avoid race conditions with this implementation - I put all the relevant logic into a Lua script, meaning that it all ran atomically! Redis is great.</p>

<details>
    <summary>my lua script</summary>
    
<figure><pre><code data-lang="lua"><span>local</span> <span>bitset_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>1</span><span>]</span>
<span>local</span> <span>count_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>2</span><span>]</span>
<span>local</span> <span>frozen_bitset_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>3</span><span>]</span>
<span>local</span> <span>frozen_count_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>4</span><span>]</span>
<span>local</span> <span>freeze_time_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>5</span><span>]</span>
<span>local</span> <span>index</span> <span>=</span> <span>tonumber</span><span>(</span><span>ARGV</span><span>[</span><span>1</span><span>])</span>
<span>local</span> <span>max_count</span> <span>=</span> <span>tonumber</span><span>(</span><span>ARGV</span><span>[</span><span>2</span><span>])</span>

<span>local</span> <span>UNCHECKED_SENTINEL</span> <span>=</span> <span>0</span>
<span>local</span> <span>redis_time</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;TIME&#39;</span><span>)</span>
<span>local</span> <span>current_time</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis_time</span><span>[</span><span>1</span><span>])</span> <span>*</span> <span>1000</span> <span>+</span> <span>math.floor</span><span>(</span><span>tonumber</span><span>(</span><span>redis_time</span><span>[</span><span>2</span><span>])</span> <span>/</span> <span>1000</span><span>)</span>
<span>local</span> <span>freeze_time</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;get&#39;</span><span>,</span> <span>freeze_time_key</span><span>)</span> <span>or</span> <span>&#34;0&#34;</span><span>)</span>

<span>local</span> <span>current_count</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;get&#39;</span><span>,</span> <span>count_key</span><span>)</span> <span>or</span> <span>&#34;0&#34;</span><span>)</span>
<span>local</span> <span>current_bit</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;getbit&#39;</span><span>,</span> <span>bitset_key</span><span>,</span> <span>index</span><span>)</span>
<span>local</span> <span>frozen_bit</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;getbit&#39;</span><span>,</span> <span>frozen_bitset_key</span><span>,</span> <span>index</span><span>)</span>

<span>if</span> <span>frozen_bit</span> <span>==</span> <span>1</span> <span>then</span>
    <span>-- Return current bit value, 0 for no change, and 0 to indicate not newly frozen</span>
    <span>return</span> <span>{</span><span>current_bit</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>}</span>
<span>end</span>

<span>if</span> <span>current_count</span> <span>&gt;=</span> <span>max_count</span> <span>then</span>
    <span>return</span> <span>{</span><span>current_bit</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>}</span>
<span>end</span>

<span>local</span> <span>new_bit</span> <span>=</span> <span>1</span> <span>-</span> <span>current_bit</span>
<span>local</span> <span>diff</span> <span>=</span> <span>new_bit</span> <span>-</span> <span>current_bit</span>

<span>-- If we&#39;re unchecking (new_bit == 0), check the freeze_time</span>
<span>if</span> <span>new_bit</span> <span>==</span> <span>0</span> <span>then</span>
    <span>local</span> <span>last_checked</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;hget&#39;</span><span>,</span> <span>&#39;last_checked&#39;</span><span>,</span> <span>index</span><span>)</span> <span>or</span> <span>UNCHECKED_SENTINEL</span><span>)</span>
    <span>if</span> <span>last_checked</span> <span>~=</span> <span>UNCHECKED_SENTINEL</span> <span>and</span> <span>current_time</span> <span>-</span> <span>last_checked</span> <span>&gt;=</span> <span>freeze_time</span> <span>then</span>
        <span>-- Box is frozen, update frozen bitset and count</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;setbit&#39;</span><span>,</span> <span>frozen_bitset_key</span><span>,</span> <span>index</span><span>,</span> <span>1</span><span>)</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;incr&#39;</span><span>,</span> <span>frozen_count_key</span><span>)</span>
        <span>-- Return 1 (checked), 0 for no change, and 1 to indicate newly frozen</span>
        <span>return</span> <span>{</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>}</span>
    <span>else</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;hset&#39;</span><span>,</span> <span>&#39;last_checked&#39;</span><span>,</span> <span>index</span><span>,</span> <span>UNCHECKED_SENTINEL</span><span>)</span>
    <span>end</span>
<span>else</span>
    <span>-- We&#39;re checking the box, update last_checked time</span>
    <span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;hset&#39;</span><span>,</span> <span>&#39;last_checked&#39;</span><span>,</span> <span>index</span><span>,</span> <span>current_time</span><span>)</span>
<span>end</span>

<span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;setbit&#39;</span><span>,</span> <span>bitset_key</span><span>,</span> <span>index</span><span>,</span> <span>new_bit</span><span>)</span>
<span>local</span> <span>new_count</span> <span>=</span> <span>current_count</span> <span>+</span> <span>diff</span>
<span>redis</span><span>.</span><span>call</span><span>(</span><span>&#39;set&#39;</span><span>,</span> <span>count_key</span><span>,</span> <span>new_count</span><span>)</span>

<span>-- new bit value, the change (-1, 0, or 1), and 0 to indicate not newly frozen</span>
<span>return</span> <span>{</span><span>new_bit</span><span>,</span> <span>diff</span><span>,</span> <span>0</span><span>}</span>  </code></pre></figure>

</details>

<p>I rolled the sunsetting changes 2 weeks and 1 day after I launched OMCB. Box 491915 was checked at 4:35 PM Eastern on July 11th, closing out the site.</p>

<h2 id="so-whatd-i-learn">So what’d I learn?</h2>
<p>Well, a lot. This was the second time that I’d put a server with a ‘real’ backend on the public internet, and <a href="https://eieio.games/nonsense/game-12-stranger-video/">the last one</a> barely counted. Learning in a high-intensity but low-stakes environment is great<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" rel="footnote">13</a></sup>.</p>

<p>Building the site in two days with little regard for scale was a good choice. It’s so hard to know what will do well on the internet - nobody I explained the site to seemed that excited about it - and I doubt I would have launched at all if I spent weeks thinking about scale. Having a bunch of eyes on the site energized me to keep it up and helped me focus on what mattered.</p>

<p>I’m happy with the tech that I used. Redis and nginx are incredible. Running things myself made debugging and fixing things so much easier (it was a little painful to not have full control of my Redis instance). The site cost me something like $850 to run - <a href="https://buymeacoffee.com/eieio">donations</a> came pretty close to that, so I’m not in the hole for too much.</p>

<p>In the future I might spend some time trying to decode the pricing pages for <a href="https://www.convex.dev/pricing">convex</a> or <a href="https://developers.cloudflare.com/durable-objects/platform/pricing/">durable objects</a>. But I’m proud of scaling it on my own terms, and I think it’s worth noting that things worked out ok.</p>

<p>This also validated my belief that people are hungry for constrained anonymous interactions with strangers. I love building sites like this (and I was gonna build more regardless!) but I’m more confident than ever that it’s a good idea.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>This was an absolute blast.</p>

<p>I’ve got one more story to tell about the site. It’s about teens doing cool things. This blog is too long to tell it right now, so stay tuned (I’ll update my <a href="https://eieio.substack.com/">newsletter</a>, <a href="https://twitter.com/itseieio">twitter</a>, and <a href="https://eieio.games/whats-my-deal/">various other platforms</a>) when it’s live.</p>

<p>And in the meantime - build more stupid websites! The internet can still be fun :)</p>



    </div></div>
  </body>
</html>
