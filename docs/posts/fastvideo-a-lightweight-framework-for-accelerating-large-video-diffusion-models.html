<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/hao-ai-lab/FastVideo">Original</a>
    <h1>FastVideo: a lightweight framework for accelerating large video diffusion models</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/hao-ai-lab/FastVideo/blob/main/assets/logo.jpg"><img src="https://github.com/hao-ai-lab/FastVideo/raw/main/assets/logo.jpg" width="30%"/></a>
</p>
<p dir="auto">FastVideo is a lightweight framework for accelerating large video diffusion models.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description FastMochi-Demo.mp4">FastMochi-Demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/147024991/396602614-5fbc4596-56d6-43aa-98e0-da472cf8e26c.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjAyNjE0LTVmYmM0NTk2LTU2ZDYtNDNhYS05OGUwLWRhNDcyY2Y4ZTI2Yy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNTQ4M2UwNjA0ZWJhOWJmOWVhMzJhYTY1MDg2N2Y4MGY2MzAxMTZjZmI3M2M1M2EzMDhjYjA1NzM5NmJkNmRlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.iYI8_O3e04rg9rICEWocz8CSN6AYHBjfvCYX4P3OzeU" data-canonical-src="https://private-user-images.githubusercontent.com/147024991/396602614-5fbc4596-56d6-43aa-98e0-da472cf8e26c.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjAyNjE0LTVmYmM0NTk2LTU2ZDYtNDNhYS05OGUwLWRhNDcyY2Y4ZTI2Yy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNTQ4M2UwNjA0ZWJhOWJmOWVhMzJhYTY1MDg2N2Y4MGY2MzAxMTZjZmI3M2M1M2EzMDhjYjA1NzM5NmJkNmRlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.iYI8_O3e04rg9rICEWocz8CSN6AYHBjfvCYX4P3OzeU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">
    ü§ó <a href="https://huggingface.co/FastVideo/FastMochi-diffusers" rel="nofollow">FastMochi</a> | ü§ó <a href="https://huggingface.co/FastVideo/FastHunyuan" rel="nofollow">FastHunyuan</a>  | üîç <a href="https://discord.gg/REBzDQTWWt" rel="nofollow"> Discord </a>
</p> 
<p dir="auto">FastVideo currently offers: (with more to come)</p>
<ul dir="auto">
<li>FastHunyuan and FastMochi: consistency distilled video diffusion models for 8x inference speedup.</li>
<li>First open distillation recipes for video DiT, based on <a href="https://github.com/G-U-N/Phased-Consistency-Model">PCM</a>.</li>
<li>Support distilling/finetuning/inferencing state-of-the-art open video DiTs: 1. Mochi 2. Hunyuan.</li>
<li>Scalable training with FSDP, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs.</li>
<li>Memory efficient finetuning with LoRA, precomputed latent, and precomputed text embeddings.</li>
</ul>
<p dir="auto">Dev in progress and highly experimental.</p>

<p dir="auto">Fast-Hunyuan comparison with original Hunyuan, achieving an 8X diffusion speed boost with the FastVideo framework.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description FastHunyuan-Demo.mp4">FastHunyuan-Demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/147024991/396606115-064ac1d2-11ed-4a0c-955b-4d412a96ef30.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjA2MTE1LTA2NGFjMWQyLTExZWQtNGEwYy05NTViLTRkNDEyYTk2ZWYzMC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02NjVmYzg2ZDQwY2ZlZWI3NzNhNmY2NzQ0NDY5ODVjMjUyOTQzM2JiYzJlOGVkMWMyYzQ5ODk2NTIwYmI2MWRjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.t1M0mjLFdf8M-7AFhdqe0DainzRtPUEDp4BIYoSbOBA" data-canonical-src="https://private-user-images.githubusercontent.com/147024991/396606115-064ac1d2-11ed-4a0c-955b-4d412a96ef30.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjA2MTE1LTA2NGFjMWQyLTExZWQtNGEwYy05NTViLTRkNDEyYTk2ZWYzMC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02NjVmYzg2ZDQwY2ZlZWI3NzNhNmY2NzQ0NDY5ODVjMjUyOTQzM2JiYzJlOGVkMWMyYzQ5ODk2NTIwYmI2MWRjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.t1M0mjLFdf8M-7AFhdqe0DainzRtPUEDp4BIYoSbOBA" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Comparison between OpenAI Sora, original Hunyuan and FastHunyuan</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description sora-verse-fasthunyuan.mp4.mp4">sora-verse-fasthunyuan.mp4.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/147024991/396652769-d323b712-3f68-42b2-952b-94f6a49c4836.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjUyNzY5LWQzMjNiNzEyLTNmNjgtNDJiMi05NTJiLTk0ZjZhNDljNDgzNi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xOTVjYjRmMjZmZjEzNGI3NGQ5MDQyODgyN2I4NTkwMTYxZmY3Nzg3OTQ1MjUxYzAzZTgyM2Y5NmEzZjRlYWIyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9._oh8gbDWcf23Mlty0ZLWROh5ik5Cf2ojhN7kJtg8mfk" data-canonical-src="https://private-user-images.githubusercontent.com/147024991/396652769-d323b712-3f68-42b2-952b-94f6a49c4836.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0OTI3ODIsIm5iZiI6MTczNDQ5MjQ4MiwicGF0aCI6Ii8xNDcwMjQ5OTEvMzk2NjUyNzY5LWQzMjNiNzEyLTNmNjgtNDJiMi05NTJiLTk0ZjZhNDljNDgzNi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxOFQwMzI4MDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xOTVjYjRmMjZmZjEzNGI3NGQ5MDQyODgyN2I4NTkwMTYxZmY3Nzg3OTQ1MjUxYzAzZTgyM2Y5NmEzZjRlYWIyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9._oh8gbDWcf23Mlty0ZLWROh5ik5Cf2ojhN7kJtg8mfk" controls="controls" muted="muted">

  </video>
</details>


<ul dir="auto">
<li><code>2024/12/17</code>: <code>FastVideo</code> v0.1 is released.</li>
</ul>

<p dir="auto">The code is tested on Python 3.10.0, CUDA 12.1 and H100.</p>


<p dir="auto">We recommend using a GPU with 80GB of memory. To run the inference, use the following command:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Download the model weight
python scripts/huggingface/download_hf.py --repo_id=FastVideo/FastHunyuan --local_dir=data/FastHunyuan --repo_type=model
# CLI inference
sh scripts/inference/inference_hunyuan.sh"><pre><span><span>#</span> Download the model weight</span>
python scripts/huggingface/download_hf.py --repo_id=FastVideo/FastHunyuan --local_dir=data/FastHunyuan --repo_type=model
<span><span>#</span> CLI inference</span>
sh scripts/inference/inference_hunyuan.sh</pre></div>
<p dir="auto">You can also inference FastHunyuan in the <a href="https://github.com/Tencent/HunyuanVideo">official Hunyuan github</a>.</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Download the model weight
python scripts/huggingface/download_hf.py --repo_id=FastVideo/FastMochi-diffusers --local_dir=data/FastMochi-diffusers --repo_type=model
# CLI inference
bash scripts/inference/inference_mochi_sp.sh"><pre><span><span>#</span> Download the model weight</span>
python scripts/huggingface/download_hf.py --repo_id=FastVideo/FastMochi-diffusers --local_dir=data/FastMochi-diffusers --repo_type=model
<span><span>#</span> CLI inference</span>
bash scripts/inference/inference_mochi_sp.sh</pre></div>

<p dir="auto">Our distillation recipe is based on <a href="https://github.com/G-U-N/Phased-Consistency-Model">Phased Consistency Model</a>. We did not find significant improvement using multi-phase distillation, so we keep the one phase setup similar to the original latent consistency model&#39;s recipe.
We use the <a href="https://huggingface.co/datasets/LanguageBind/Open-Sora-Plan-v1.1.0/tree/main/all_mixkit" rel="nofollow">MixKit</a> dataset for distillation. To avoid running the text encoder and VAE during training, we preprocess all data to generate text embeddings and VAE latents.
Preprocessing instructions can be found <a href="https://github.com/hao-ai-lab/FastVideo/blob/main/docs/data_preprocess.md">data_preprocess.md</a>. For convenience, we also provide preprocessed data that can be downloaded directly using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/huggingface/download_hf.py --repo_id=FastVideo/HD-Mixkit-Finetune-Hunyuan --local_dir=data/HD-Mixkit-Finetune-Hunyuan --repo_type=dataset"><pre>python scripts/huggingface/download_hf.py --repo_id=FastVideo/HD-Mixkit-Finetune-Hunyuan --local_dir=data/HD-Mixkit-Finetune-Hunyuan --repo_type=dataset</pre></div>
<p dir="auto">Next, download the original model weights with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/huggingface/download_hf.py --repo_id=FastVideo/hunyuan --local_dir=data/hunyuan --repo_type=model"><pre>python scripts/huggingface/download_hf.py --repo_id=FastVideo/hunyuan --local_dir=data/hunyuan --repo_type=model</pre></div>
<p dir="auto">To launch the distillation process, use the following commands:</p>
<div data-snippet-clipboard-copy-content="bash scripts/distill/distill_mochi.sh # for mochi
bash scripts/distill/distill_hunyuan.sh # for hunyuan"><pre><code>bash scripts/distill/distill_mochi.sh # for mochi
bash scripts/distill/distill_hunyuan.sh # for hunyuan
</code></pre></div>
<p dir="auto">We also provide an optional script for distillation with adversarial loss, located at <code>fastvideo/distill_adv.py</code>. Although we tried adversarial loss, we did not observe significant improvements.</p>


<p dir="auto">Ensure your data is prepared and preprocessed in the format specified in <a href="https://github.com/hao-ai-lab/FastVideo/blob/main/docs/data_preprocess.md">data_preprocess.md</a>. For convenience, we also provide a mochi preprocessed Black Myth Wukong data that can be downloaded directly:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/huggingface/download_hf.py --repo_id=FastVideo/Mochi-Black-Myth --local_dir=data/Mochi-Black-Myth --repo_type=dataset"><pre>python scripts/huggingface/download_hf.py --repo_id=FastVideo/Mochi-Black-Myth --local_dir=data/Mochi-Black-Myth --repo_type=dataset</pre></div>
<p dir="auto">Download the original model weights with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/huggingface/download_hf.py --repo_id=genmo/mochi-1-preview --local_dir=data/mochi --repo_type=model
python scripts/huggingface/download_hf.py --repo_id=FastVideo/hunyuan --local_dir=data/hunyuan --repo_type=model"><pre>python scripts/huggingface/download_hf.py --repo_id=genmo/mochi-1-preview --local_dir=data/mochi --repo_type=model
python scripts/huggingface/download_hf.py --repo_id=FastVideo/hunyuan --local_dir=data/hunyuan --repo_type=model</pre></div>
<p dir="auto">Then you can run the finetune with:</p>
<div data-snippet-clipboard-copy-content="bash scripts/finetune/finetune_mochi.sh # for mochi"><pre><code>bash scripts/finetune/finetune_mochi.sh # for mochi
</code></pre></div>
<p dir="auto"><strong>Note that for finetuning, we did not tune the hyperparameters in the provided script</strong></p>

<p dir="auto">Currently, we only provide Lora Finetune for Mochi model, the command for Lora Finetune is</p>
<div data-snippet-clipboard-copy-content="bash scripts/finetune/finetune_mochi_lora.sh"><pre><code>bash scripts/finetune/finetune_mochi_lora.sh
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Minimum Hardware Requirement</h3><a id="user-content-minimum-hardware-requirement" aria-label="Permalink: Minimum Hardware Requirement" href="#minimum-hardware-requirement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>40 GB GPU memory each for 2 GPUs with lora</li>
<li>30 GB GPU memory each for 2 GPUs with CPU offload and lora.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Finetune with Both Image and Video</h3><a id="user-content-finetune-with-both-image-and-video" aria-label="Permalink: Finetune with Both Image and Video" href="#finetune-with-both-image-and-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our codebase support finetuning with both image and video.</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash scripts/finetune/finetune_hunyuan.sh
bash scripts/finetune/finetune_mochi_lora_mix.sh"><pre>bash scripts/finetune/finetune_hunyuan.sh
bash scripts/finetune/finetune_mochi_lora_mix.sh</pre></div>
<p dir="auto">For Image-Video Mixture Fine-tuning, make sure to enable the --group_frame option in your script.</p>

<p dir="auto">We learned and reused code from the following projects: <a href="https://github.com/G-U-N/Phased-Consistency-Model">PCM</a>, <a href="https://github.com/huggingface/diffusers">diffusers</a>, <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">OpenSoraPlan</a>, and <a href="https://github.com/xdit-project/xDiT">xDiT</a>.</p>
<p dir="auto">We thank MBZUAI and Anyscale for their support throughout this project.</p>
</article></div></div>
  </body>
</html>
