<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/DeepSeek-OCR">Original</a>
    <h1>DeepSeek OCR</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">


<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/logo.svg"><img src="https://github.com/deepseek-ai/DeepSeek-OCR/raw/main/assets/logo.svg" width="60%" alt="DeepSeek AI"/></a>
</p>
<hr/>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-OCR/raw/main/assets/badge.svg"/>
  </a>
  <a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white"/>
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da"/>
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white"/>
  </a>
</p>
<p dir="auto">
  <a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR" rel="nofollow"><b>üì• Model Download</b></a> |
  <a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"><b>üìÑ Paper Link</b></a> |
  <a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"><b>üìÑ Arxiv Paper Link</b></a> |
</p>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/fig1.png"><img src="https://github.com/deepseek-ai/DeepSeek-OCR/raw/main/assets/fig1.png"/></a>
</p>
<p dir="auto">
<a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main">Explore the boundaries of visual-text compression.</a>       
</p>

<ul dir="auto">
<li>[2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.</li>
</ul>

<ul dir="auto">
<li><a href="#install">Install</a></li>
<li><a href="#vllm-inference">vLLM Inference</a></li>
<li><a href="#transformers-inference">Transformers Inference</a></li>
</ul>

<blockquote>
<p dir="auto">Our environment is cuda11.8+torch2.6.0.</p>
</blockquote>
<ol dir="auto">
<li>Clone this repository and navigate to the DeepSeek-OCR folder</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/deepseek-ai/DeepSeek-OCR.git"><pre>git clone https://github.com/deepseek-ai/DeepSeek-OCR.git</pre></div>
<ol start="2" dir="auto">
<li>Conda</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr"><pre>conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr</pre></div>
<ol start="3" dir="auto">
<li>Packages</li>
</ol>
<ul dir="auto">
<li>download the vllm-0.8.5 <a href="https://github.com/vllm-project/vllm/releases/tag/v0.8.5">whl</a></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation"><pre>pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation</pre></div>
<p dir="auto"><strong>Note:</strong> if you want vLLM and transformers codes to run in the same environment, you don&#39;t need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&gt;=4.51.1</p>

<ul dir="auto">
<li>VLLM:</li>
</ul>
<blockquote>
<p dir="auto"><strong>Note:</strong> change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="cd DeepSeek-OCR-master/DeepSeek-OCR-vllm"><pre><span>cd</span> DeepSeek-OCR-master/DeepSeek-OCR-vllm</pre></div>
<ol dir="auto">
<li>image: streaming output</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python run_dpsk_ocr_image.py"><pre>python run_dpsk_ocr_image.py</pre></div>
<ol start="2" dir="auto">
<li>pdf: concurrency ~2500tokens/s(an A100-40G)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python run_dpsk_ocr_pdf.py"><pre>python run_dpsk_ocr_pdf.py</pre></div>
<ol start="3" dir="auto">
<li>batch eval for benchmarks</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python run_dpsk_ocr_eval_batch.py"><pre>python run_dpsk_ocr_eval_batch.py</pre></div>

<ul dir="auto">
<li>Transformers</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#39;0&#39;
model_name = &#39;deepseek-ai/DeepSeek-OCR&#39;

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation=&#39;flash_attention_2&#39;, trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = &#34;&lt;image&gt;\nFree OCR. &#34;
prompt = &#34;&lt;image&gt;\n&lt;|grounding|&gt;Convert the document to markdown. &#34;
image_file = &#39;your_image.jpg&#39;
output_path = &#39;your/output/dir&#39;

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModel</span>, <span>AutoTokenizer</span>
<span>import</span> <span>torch</span>
<span>import</span> <span>os</span>
<span>os</span>.<span>environ</span>[<span>&#34;CUDA_VISIBLE_DEVICES&#34;</span>] <span>=</span> <span>&#39;0&#39;</span>
<span>model_name</span> <span>=</span> <span>&#39;deepseek-ai/DeepSeek-OCR&#39;</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>model</span> <span>=</span> <span>AutoModel</span>.<span>from_pretrained</span>(<span>model_name</span>, <span>_attn_implementation</span><span>=</span><span>&#39;flash_attention_2&#39;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>, <span>use_safetensors</span><span>=</span><span>True</span>)
<span>model</span> <span>=</span> <span>model</span>.<span>eval</span>().<span>cuda</span>().<span>to</span>(<span>torch</span>.<span>bfloat16</span>)

<span># prompt = &#34;&lt;image&gt;\nFree OCR. &#34;</span>
<span>prompt</span> <span>=</span> <span>&#34;&lt;image&gt;<span>\n</span>&lt;|grounding|&gt;Convert the document to markdown. &#34;</span>
<span>image_file</span> <span>=</span> <span>&#39;your_image.jpg&#39;</span>
<span>output_path</span> <span>=</span> <span>&#39;your/output/dir&#39;</span>

<span>res</span> <span>=</span> <span>model</span>.<span>infer</span>(<span>tokenizer</span>, <span>prompt</span><span>=</span><span>prompt</span>, <span>image_file</span><span>=</span><span>image_file</span>, <span>output_path</span> <span>=</span> <span>output_path</span>, <span>base_size</span> <span>=</span> <span>1024</span>, <span>image_size</span> <span>=</span> <span>640</span>, <span>crop_mode</span><span>=</span><span>True</span>, <span>save_results</span> <span>=</span> <span>True</span>, <span>test_compress</span> <span>=</span> <span>True</span>)</pre></div>
<p dir="auto">or you can</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py"><pre><span>cd</span> DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py</pre></div>

<p dir="auto">The current open-source model supports the following modes:</p>
<ul dir="auto">
<li>Native resolution:
<ul dir="auto">
<li>Tiny: 512√ó512 Ôºà64 vision tokensÔºâ‚úÖ</li>
<li>Small: 640√ó640 Ôºà100 vision tokensÔºâ‚úÖ</li>
<li>Base: 1024√ó1024 Ôºà256 vision tokensÔºâ‚úÖ</li>
<li>Large: 1280√ó1280 Ôºà400 vision tokensÔºâ‚úÖ</li>
</ul>
</li>
<li>Dynamic resolution
<ul dir="auto">
<li>Gundam: n√ó640√ó640 + 1√ó1024√ó1024 ‚úÖ</li>
</ul>
</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# document: &lt;image&gt;\n&lt;|grounding|&gt;Convert the document to markdown.
# other image: &lt;image&gt;\n&lt;|grounding|&gt;OCR this image.
# without layouts: &lt;image&gt;\nFree OCR.
# figures in document: &lt;image&gt;\nParse the figure.
# general: &lt;image&gt;\nDescribe this image in detail.
# rec: &lt;image&gt;\nLocate &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image.
# &#39;ÂÖàÂ§©‰∏ã‰πãÂøßËÄåÂøß&#39;"><pre><span># document: &lt;image&gt;\n&lt;|grounding|&gt;Convert the document to markdown.</span>
<span># other image: &lt;image&gt;\n&lt;|grounding|&gt;OCR this image.</span>
<span># without layouts: &lt;image&gt;\nFree OCR.</span>
<span># figures in document: &lt;image&gt;\nParse the figure.</span>
<span># general: &lt;image&gt;\nDescribe this image in detail.</span>
<span># rec: &lt;image&gt;\nLocate &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image.</span>
<span># &#39;ÂÖàÂ§©‰∏ã‰πãÂøßËÄåÂøß&#39;</span></pre></div>

<markdown-accessiblity-table></markdown-accessiblity-table>

<p dir="auto">We would like to thank <a href="https://github.com/Ucas-HaoranWei/Vary/">Vary</a>, <a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0/">GOT-OCR2.0</a>, <a href="https://github.com/opendatalab/MinerU">MinerU</a>, <a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a>, <a href="https://github.com/LingyvKong/OneChart">OneChart</a>, <a href="https://github.com/Ucas-HaoranWei/Slow-Perception">Slow Perception</a> for their valuable models and ideas.</p>
<p dir="auto">We also appreciate the benchmarks: <a href="https://github.com/ucaslcl/Fox">Fox</a>, <a href="https://github.com/opendatalab/OmniDocBench">OminiDocBench</a>.</p>

<p dir="auto">coming soonÔºÅ</p>
</article></div></div>
  </body>
</html>
