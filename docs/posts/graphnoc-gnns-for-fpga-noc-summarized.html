<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aidenfoxivey.com/graphnoc/">Original</a>
    <h1>GraphNoC: GNNs for FPGA NoC Summarized</h1>
    
    <div id="readability-page-1" class="page"><div>
    
<p>
(Disclaimer: I am taking ECE493, which requires a discussion of this paper. This blog post on its own is <em>not</em> submitted to the course, but is understandably linked to what I did choose to submit.)</p>
<div id="outline-container-headline-1">
<h3 id="headline-1">
Introduction
</h3>
<div id="outline-text-headline-1">
<p>
<em>GraphNoC: Graph Neural Networks for Application-Specific FPGA NoC Performance Prediction</em> is a 2024 paper by Gurshaant Malik and Dr. Nachiket Kapre. Notably, it won Best-Paper Award at <a href="https://fpt2024.org">https://fpt2024.org</a> in Sydney, NSW, AU.</p>
<p>
It describes a paradigm by which GNNs are used to take a design for an FPGA NoC and then effectively &#39;simulate&#39; the latency that a given NoC design will have.</p>
<p>
Before we move on, I will summarize and define key terms.</p>
<div id="outline-container-headline-2">
<h4 id="headline-2">
NoC: Networks on Chip
</h4>
<div id="outline-text-headline-2">
<p>
Semiconductors have come a long way from being the focal points upon which a circuit routed signals. Now, having increased density chip density, there is clear motivation to connect IP blocks and route data between them in an efficient way.</p>
<p>
IP blocks are defined as independent units of configurable logic that an integrated circuit or (in this case) an FPGA designer can use for some functionality within their overall design.</p>
<p>
For those who are familiar with software, the clearest connection here is a &#39;library&#39;, since IP blocks often have verified behaviour and an interface.</p>
<p>
An interface is effectively a &#39;contract&#39; that states that one can expect certain behaviour if they follow certain rules. Given that IP blocks incur a high level of expertise to design and are often sold for some fee, their interfaces are frequently specified in a more comprehensive way than the average software library.</p>
</div>
</div>
<div id="outline-container-headline-3">
<h4 id="headline-3">
GNNs
</h4>
<div id="outline-text-headline-3">
<p>
Graph Neural Networks are a special form of the classic neural network that you may have worked with before. In <em>Multilayer Feedforward Networks are Universal Approximators</em> by Hornik, Stinchcombe, and White, we see that &#34;feedforward networks are capable of arbitrarily accurate approximation to any real-valued continuous function over a compact set&#34;. We should note though that <em>capability</em> does not imply <em>suitability</em>.</p>
<p>
Some problems, namely network problems, lend themselves to being represented in the form of a graph. Using a classic neural network for a problem represented in form of a graph will naturally lead to loss of &#39;dimensionality&#39;, since the classic neural network can only ingest &#39;flat&#39; tensors.</p>
<div id="outline-container-headline-4">
<h5 id="headline-4">
Message Passing
</h5>
<div id="outline-text-headline-4">
<p>
A key distinction between a classic neural net and graph neural net is <strong>message passing</strong>. This is a procedure by which a given node <strong>aggregates</strong> messages from its neighbours. The locality of a given node informs what message it will get from its neighbours.</p>
<p>
In <em>GRAPH LEARNING AT SCALE: CHARACTERIZING AND OPTIMIZING PRE-PROPAGATION GNNS</em>, Yue, Deng, and Zhang define message passing as being defined by an aggregation function $f_k$ that a node $v$ uses to collect and &#34;aggregate&#34; embeddings from its one-hop neighbours.</p>
<p>
The follow equation explains it more thoroughly:</p>
<p>
$$
h_v^{(k)} = l_k \left( h_v^{(k-1)}, f_k\left(\left\{(h_u^{(k-1)}, e_{vu}) \mid \forall u \in N(v)\right\}\right)\right)
$$</p>
<p>
This equation statees that the new embedding for node $v$ in layer $k$ is the transformation function ($l_k$) applied on the previous layer&#39;s node embedding for $v$ and the aggregation function $f_k$ applied on the set of pairs of the neighbouring nodes to $v$ in the $k-1$th layer and the edge attribute between them.</p>
<p>
Node that $N(v)$ is the set of one-hop neighbours from node $v$.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-5">
<h3 id="headline-5">
Motivation
</h3>
<div id="outline-text-headline-5">
<p>
Now that some of the underlying pieces are clear, let&#39;s consider GraphNoC&#39;s raison d&#39;Ãªtre: the time it takes to simulate a given NoC design for an FPGA.</p>
<p>
RTL (register-transfer level) simulations of FPGA NoCs take <em>lots</em> of time to run. According to GraphNoC, they are usually on the order of tens of minutes. As anyone who&#39;s ever baked, written a big C++ (or Rust!) codebase, or designed a NoC themselves knows, a feedback cycle above a minute is pretty much soul crushing.</p>
<p>
The next logical choice here would be to opt for analytical models, which are in effect a mathematical model of the NoC by virtue of knowing its architecture. Examples of this include HopliteRT, which is another work by Dr. Kapre. One predicament with these models is that they tend to be bad at generalizing across different NoC architectures <em>and</em> specific application workflows.</p>
<p>
GraphNoC also considers standard artifical neural nets, which tend to be faster than even analytical models (of course after training). A major benefit for ANNs is that they can <em>learn</em> specific details about a given NoC without being explicitly taught. </p>
<p>
The major limiting factor for them, however, is the inherent topology of artificial neural nets, which is biased towards &#39;highly regular&#39; and &#39;symmetric&#39; NoC topologies such as meshes or tori.</p>
<p>
Thus, GraphNoC comes to the conclusion that the most reasonable option would be graph neural networks, since they have approximately the same inference time, are compatible with the topologies that ANNs cannot manage with, and are feasible to use with different NoC architectures.</p>
<p>
<img src="https://aidenfoxivey.com/graphnoc/graphnoc.png" alt="./graphnoc.png" title="./graphnoc.png"/></p>
<p>
As is shown above, the loss (a measure of how accurately the prediction is emperically found to be) plateaus for ANNs whereas GNNs are sufficient to reduce it further.</p>
</div>
</div>
<p id="outline-container-headline-6">
<h3 id="headline-6">
Mechanics
</h3>
</p>
<p id="outline-container-headline-7">
<h3 id="headline-7">
Conclusion
</h3>
</p>

    
  </div></div>
  </body>
</html>
