<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning">Original</a>
    <h1>From deep to long learning?</h1>
    
    <div id="readability-page-1" class="page"><div><figure><img src="https://hazyresearch.stanford.edu/static/posts/2023-03-27-long-learning/tweet.png"/></figure>
<p>For the last two <a href="https://hazyresearch.stanford.edu/blog/2022-06-09-longer-sequences-next-leap-ai">years</a>, a <a href="https://arxiv.org/abs/2111.00396">line</a> of <a href="https://arxiv.org/abs/2205.14135">work</a> in <a href="https://arxiv.org/abs/2212.14052">our</a> <a href="https://arxiv.org/abs/2302.10866">lab</a> has been to increase sequence length. We thought longer sequences would enable a new era of machine learning foundation models: they could learn from longer contexts, multiple media sources, complex demonstrations, and more. All data ready and waiting to be learned from in the world! It’s been amazing to see the progress there. As an aside, we’re happy to play a role with the introduction of FlashAttention (<a href="https://github.com/HazyResearch/flash-attention">code</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-01-12-flashattention-long-sequences">blog</a>, <a href="https://arxiv.org/abs/2205.14135">paper</a>) by Tri Dao and Dan Fu from our lab, who showed that sequence lengths of 32k are possible–and now <a href="https://github.com/HazyResearch/flash-attention/blob/main/usage.md">widely available</a> in this era of foundation models (and we’ve heard <a href="https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py">OpenAI</a>, <a href="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-collaborates-with-hazy-research-and-nvidia-to-achieve/ba-p/3667511">Microsoft</a>, <a href="https://github.com/NVIDIA/Megatron-LM/pull/267">NVIDIA</a>, and others use it for their models too–awesome!).</p>
<figure><img id="context-length-img" src="https://hazyresearch.stanford.edu/static/posts/2023-03-27-long-learning/fm_context_length.png"/><figcaption><p>The context lengths of foundation models have been growing recently (and <a href="#alternate-explanations">alternate explanations abound</a>)! What&#39;s next?</p></figcaption></figure>
<p>As the GPT4 press release noted, this has allowed almost 50 pages of text as context–and tokenization/patching ideas like those in Deepmind’s <a href="https://www.deepmind.com/publications/a-generalist-agent">Gato</a> are able to use images as context. So many amazing ideas coming together, awesome!</p>
<p>This article is about another approach to increasing sequence length at a high level, and the connection to a new set of primitives.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2023-03-27-long-learning/flowchart.png"/><figcaption><p>This blog is about another approach to increasing sequence length!</p></figcaption></figure>
<p>One fundamental issue we ran into was that the attention layers in Transformers scale quadratically in sequence length: going from 32k length to 64k length isn’t 2x as expensive, but 4x more expensive. This led us to investigate models that are nearly linear time in sequence length. For our lab, this started with Hippo, followed by S4, H3, and now Hyena. These models hold the promise to have context lengths of millions… or maybe even a billion!</p>
<h2>Some Recent History and Progress</h2>
<h3>Long Range Arena and S4</h3>
<p>The Long Range Arena <a href="https://github.com/google-research/long-range-arena">benchmark</a> was introduced by Google researchers in 2020 to evaluate how well different models can handle long-range dependencies. LRA tests a suite of tasks covering different data types and modalities such as text, images, and mathematical expressions, with sequence lengths up to 16K (Path-X: classifying images that have been unrolled into pixels, without any spatial inductive bias). There’s been a <a href="https://arxiv.org/abs/2001.04451">lot</a> of <a href="https://arxiv.org/abs/2006.16236">great</a> <a href="https://arxiv.org/abs/2009.14794">work</a> on scaling Transformers to <a href="https://arxiv.org/abs/2007.14062">longer</a> sequences, but many of them seem to sacrifice accuracy. And there’s that pesky Path-X column: all these Transformer methods and their variants struggled to do better than random guessing.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2023-03-27-long-learning/lra.png"/><figcaption><p>Transformer variants benchmarked on Long Range Arena, along with S4.</p></figcaption></figure>
<p><a href="https://stanford.edu/~albertgu/">Enter</a> S4, led by the amazing Albert Gu! Inspired by the results from the LRA benchmark, Albert wanted to figure out how to better model long-range dependencies. Building on a long line of work on <a href="https://arxiv.org/abs/2008.07669">orthogonal polynomials</a> and the relationships between <a href="https://arxiv.org/abs/2110.13985">recurrent and convolutional</a> models, we introduced <a href="https://arxiv.org/abs/2111.00396">S4</a> – a new sequence model based on structured state space models (SSMs).</p>
<p>Critically, SSMs scale with <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>log</mi><mo>⁡</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \log N)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>lo<span>g</span></span><span></span><span>N</span><span>)</span></span></span></span></span> in sequence length <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span>, instead of quadratically like attention. S4 was able to successfully model the long-range dependencies in LRA, and was also the <strong>first model</strong> to achieve better than average performance on Path-X (and can now get 96.4% accuracy!). Since releasing S4, we’ve been super excited by how people are building on the ideas and making the space richer: with models like <a href="https://arxiv.org/abs/2208.04933">S5</a> from Scott Linderman’s group, <a href="https://arxiv.org/abs/2203.14343">DSS</a> from Ankit Gupta (and our own follow-on collaboration <a href="https://arxiv.org/abs/2206.11893">S4D</a>), <a href="https://arxiv.org/abs/2209.12951">Liquid-S4</a> from Hasani &amp; Lechner, and more – and of course we are always indebted to Sasha Rush and Sidd Karamcheti for the amazing <a href="https://srush.github.io/annotated-s4/">Annotated S4</a>!</p>
<p>As an aside: when we released <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a>, we were able to increase the sequence length of Transformers. We found that Transformers could also get non-trivial performance (63%) on Path-X – simply by increasing the sequence length to 16K!</p>
<h3>The Gap with Language</h3>
<p>But S4 still had a gap in quality on language modeling – up to 5 perplexity points (for context, that’s the gap between a 125M model and a 6.7B model). To close this gap, we looked at <a href="https://hazyresearch.stanford.edu/blog/2023-01-20-h3">synthetic languages</a> like associative recall to figure out what properties you should need for language. We ended up designing <a href="https://arxiv.org/abs/2212.14052">H3</a> (Hungry Hungry Hippos) – a new layer that stacked two SSMs, and multiplied their outputs together with a multiplicative gate.</p>
<p>Using H3, we replaced almost all the attention layers in GPT-style Transformers, and were able to match Transformers on both perplexity and downstream evaluations, when trained on 400B tokens from the Pile:</p>
<table><thead><tr><th>Model</th><th>Pile PPL</th><th>SuperGlue Zero-Shot</th></tr></thead><tbody><tr><td>GPT-Neo-1.3B</td><td>6.2</td><td>52.1</td></tr><tr><td><strong>H3, 2 attn (1.3B)</strong></td><td><strong>6.0</strong></td><td><strong>56.5</strong></td></tr><tr><td>GPT-Neo-2.7B</td><td>5.7</td><td>54.6</td></tr><tr><td><strong>H3, 2 attn (2.7B)</strong></td><td><strong>5.4</strong></td><td><strong>56.8</strong></td></tr></tbody></table>
<p>Since the H3 layer is built on SSMs, it also has compute that grows in <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>log</mi><mo>⁡</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \log N)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>lo<span>g</span></span><span></span><span>N</span><span>)</span></span></span></span></span> in sequence length. The two attention layers still make the whole model <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N^2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>N</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span> overall, but more on that in a bit...</p>
<p>Of course, we weren’t the only folks thinking in this direction: <a href="https://arxiv.org/abs/2206.13947">GSS</a> also found that SSMs with gating could work well in concert with attention in language modeling (which inspired H3), Meta released their <a href="https://arxiv.org/abs/2209.10655">Mega</a> model which also combined an SSM with attention, the <a href="https://arxiv.org/abs/2212.10544">BiGS model</a> replaced attention in BERT-style models, and our <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a> friends have been looking at completely recurrent approaches. Very exciting work in this area!</p>
<h3>The Next Advance: Hyena</h3>
<p>The next architecture in this line of work is <a href="https://arxiv.org/abs/2302.10866">Hyena</a> – we wanted to see if it was possible to get rid of those last two attention layers in H3, and get a model that grows nearly linearly in sequence length. Turns out, two simple insights led us to the answer:</p>
<ul>
<li>Every SSM can be viewed as a convolution filter the length of the input sequence – so we can replace the SSM with a convolution the size of the input sequence, to get a strictly more powerful model for the same compute. In particular, we parametrize the convolutional filters implicitly via another small neural network, borrowing powerful methods from the <a href="https://arxiv.org/abs/2006.09661">neural fields</a> literature, and the great <a href="https://arxiv.org/abs/2102.02611">CKConv</a> / <a href="https://arxiv.org/abs/2110.08059">FlexConv</a> line of work. Plus, the convolution can be computed in <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>log</mi><mo>⁡</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \log N)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>lo<span>g</span></span><span></span><span>N</span><span>)</span></span></span></span></span> time in sequence length – nearly-linear scaling!</li>
<li>The gating behavior in H3 can be generalized: H3 takes three projections of the input, and iteratively takes convolutions and applies a gate. In Hyena, we simply add more projections and more gates, which helps generalize to more expressive architectures and closes the gap to attention.</li>
</ul>
<p>In Hyena, we proposed the first fully near linear-time convolutional models that could match Transformers on perplexity and downstream tasks, with promising results in initial scaling experiments. We trained small- and medium-sized models on subsets of the PILE, and saw that val PPL matched Transformers:</p>
<table><thead><tr><th>Model</th><th>5B</th><th>10B</th><th>15B</th></tr></thead><tbody><tr><td>GPT-2 Small (125M)</td><td>13.3</td><td>11.9</td><td>11.2</td></tr><tr><td>Pure H3 (153M)</td><td>14.8</td><td>13.5</td><td>12.3</td></tr><tr><td>Hyena (153M)</td><td><strong>13.1</strong></td><td><strong>11.8</strong></td><td><strong>11.1</strong></td></tr><tr><td>GPT-2 Medium (355M)</td><td>11.4</td><td><strong>9.8</strong></td><td>9.3</td></tr><tr><td>Hyena (355M)</td><td><strong>11.3</strong></td><td><strong>9.8</strong></td><td><strong>9.2</strong></td></tr></tbody></table>
<p>With some optimizations (more on that below), Hyena models are slightly slower than Transformers of the same size at sequence length 2K – but get a lot faster at longer sequence lengths.</p>
<p>We’re super excited to see how far we can take these models, and excited to scale them up to the full size of the PILE (400B tokens): what happens if we combine the best ideas from H3 and Hyena, and how long can we go?</p>
<h3>A Common Primitive: the FFT... or Something More Basic?</h3>
<p>A common primitive in all these models is the FFT – that’s how we can efficiently compute a convolution as long as the input sequence in <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>log</mi><mo>⁡</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \log N)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>N</span><span></span><span>lo<span>g</span></span><span></span><span>N</span><span>)</span></span></span></span></span> time. However, the FFT is poorly supported on modern hardware, which is dominated by specialized matrix multiplication units and GEMMs (e.g., tensor cores on NVIDIA GPUs).</p>
<p>We can start to close the efficiency gap by <a href="https://arxiv.org/abs/2302.06646">rewriting</a> the FFT as a series of matrix multiplication operations – using a <a href="https://arxiv.org/abs/2302.06646">connection</a> to <a href="http://proceedings.mlr.press/v97/dao19a.html">Butterfly</a> matrices that folks in our group have used to explore <a href="https://openreview.net/forum?id=Nfl-iXa-y7R">sparse</a> <a href="https://arxiv.org/abs/2204.00595">training</a>. In our recent work, we’ve used this connection to build fast convolution algorithms like <a href="https://www.together.xyz/blog/h3">FlashConv</a> and <a href="https://arxiv.org/abs/2302.06646">FlashButterfly</a>, by using a Butterfly decomposition to compute the FFT as a series of matmul operations.</p>
<p>But we can draw on the prior work to make a deeper connection:  you can also let these matrices be <strong>learned</strong> – which takes the same wall-clock time, but gives you extra parameters! We’ve started exploring this connection on some small datasets with promising initial results, and we’re excited to see where else this connection can take us (how can we make it work for language models?):</p>
<table><thead><tr><th>Block Size</th><th>sCIFAR Acc</th></tr></thead><tbody><tr><td>Baseline</td><td>91.0</td></tr><tr><td>16x16 Learned</td><td>91.8</td></tr><tr><td>32x32 Learned</td><td>92.4</td></tr><tr><td>256x256 Learned</td><td><strong>92.5</strong></td></tr></tbody></table>
<p>We’re looking forward to exploring this more deeply. What class of transforms does this extension learn, and what can it allow you to do? What happens when we apply it to language?</p>
<h2>What&#39;s Next</h2>
<p>We are super excited by these directions, and what’s next: longer and longer sequences, new architectures that allow us to explore this new regime. We’re especially motivated by applications that could benefit from longer-sequence models – high-resolution imaging, new modalities of data, language models that can read entire books. Imagine giving a language model an entire book and having it summarize the plot, or conditioning a code generation model on all the code you’ve ever written. The possibilities are wild – and we’re excited.</p>
<p>You can find model code to play around with the synthetics languages we used to develop H3 &amp; Hyena <a href="https://github.com/HazyResearch/safari">here</a>. If you’re also excited by these directions, please reach out – we would love to chat!</p>
<p>Dan Fu: <a href="mailto:danfu@cs.stanford.edu">danfu@cs.stanford.edu</a>; Michael Poli: <a href="mailto:poli@stanford.edu">poli@stanford.edu</a></p>
<h2>Acknowledgements</h2>
<p>Thanks to Alex Tamkin, Percy Liang, Albert Gu, Michael Zhang, Eric Nguyen, and Elliot Epstein for their comments and feedback on this post.</p>
<h3 id="alternate-explanations">Alternate Explanations Abound</h3>
<p>H/t to <a href="https://twitter.com/typedfemale/status/1642622599380500480">@typedfemale</a> for bringing this to our attention. <a href="#context-length-img">↩</a></p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2023-03-27-long-learning/alternate_context_length.jpeg"/></figure></div></div>
  </body>
</html>
