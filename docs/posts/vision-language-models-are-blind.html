<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vlmsareblind.github.io/">Original</a>
    <h1>Vision language models are blind</h1>
    
    <div id="readability-page-1" class="page">
  <nav role="navigation" aria-label="main navigation">
    
    
  </nav>
  <section>
    <div>
      <div>
        <div>
          <div>
            
            <div>
              <p><span>

                <span>
                  
                  <p><span><sup>*</sup>Equal contribution</span>
                  </p>
                  <p><span><sup>1</sup>Auburn University,</span>
                    <span><sup>2</sup>University of Alberta,</span>
                  </p>
                </span>


            </span></p></div>
            
            

            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Abstract</h2>
          <p>
              Large language models with vision capabilities (VLMs), e.g., <span>GPT-<span>4o</span></span> and <span>Gemini-<span>1.5</span>
                Pro</span> are
              powering countless image-text processing applications and scoring high on existing vision-understanding
              benchmarks.
              Yet, we find that VLMs fail on 7 visual tasks <em>absurdly easy</em> to humans such as identifying (a)
              whether two
              circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d)
              counting the
              number of circles in an Olympic-like logo.
              The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like that
              of a person
              with <a href="https://en.wikipedia.org/wiki/Myopia" target="_blank">myopia</a> seeing fine details as
              blurry, and at
              worst, like an intelligent person who is blind making
              educated
              guesses.
            </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    
  </section>

  






  <!-- TASK 1 Begins -->
  <section id="task1">
    <div>
      <h2>Task 1: Counting line intersections <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/two_lines.svg" alt="Two intersecting lines"/></h2>

      <div>
        <p>
          Given the impressive accuracy of VLMs on answering questions on diagrams and charts (e.g., <span>Sonnet-<span>3.5</span></span> scoring 94.7% on AI2D and 90.8% on
          ChartQA) <a href="https://arxiv.org/abs/2203.10244">[1]</a>, a reasonable hypothesis is that VLMs must be able
          to see whether two graphs
          intersect in a
          chart. Here, we test this hypothesis by asking VLMs to count the number of intersections between two 2-segment
          piece-wise linear functions.
        </p>

        <h3>Images</h3>
        <p>
          We create 150 images (see Figure 1) of 2D line plots drawn on a white canvas. Each line plot consists of two
          line
          segments, defined by three points whose x-coordinates are fixed and equally spaced. The y-coordinates are
          randomly
          sampled to create two plots that intersect at exactly 0, 1 or 2 points. See Appendix A for more details.
        </p>

        
        <figcaption>Fig. 1: Examples of 2D line plots used in the task, showing different numbers of intersections.
        </figcaption>

        <h3>Prompts</h3>
        <div>
          <p>
            We ask each question using two different wordings:
          </p>
          <ol>
            <li><em>&#34;How many times do the blue and red line plots cross each other?&#34;</em></li>
            <li><em>&#34;How many times do the blue and red lines intersect?&#34;</em></li>
          </ol>

          <h3>Groundtruth</h3>
          <p>
              Answers are ∈ {0, 1, 2} (random-baseline accuracy: 33%).
            </p>




          <h2>Results</h2>
          <p>
            The following table shows the performance of the four models on the task of counting line intersections.
          </p>

          <table>
            <tbody><tr>
              <th>Thickness</th>
              <th>
                <p><span>GPT-<span>4o</span></span>
                </p>
              </th>
              <th>
                <p><span>Gemini-<span>1.5</span> Pro</span>
                </p>
              </th>
              <th>
                <p><span>Sonnet-<span>3</span></span>
                </p>
              </th>
              <th>
                <p><span>Sonnet-<span>3.5</span></span>
                </p>
              </th>
            </tr>
            <tr>
              <td>2</td>
              <td>45.00</td>
              <td>70.00</td>
              <td>64.00</td>
              <td>80.00</td>
            </tr>
            <tr>
              <td>3</td>
              <td>47.00</td>
              <td>68.00</td>
              <td>66.00</td>
              <td>79.00</td>
            </tr>
            <tr>
              <td>4</td>
              <td>54.00</td>
              <td>71.00</td>
              <td>62.00</td>
              <td>73.00</td>
            </tr>
            <tr>
              <td><strong>Average</strong></td>
              <td>48.67</td>
              <td>69.67</td>
              <td>64.00</td>
              <td>77.33</td>
            </tr>
          </tbody></table>
        </div>
      </div>
      <div>
        <h2>Qualitative samples</h2>
        <div>
          
          <figcaption>Fig. 2: VLMs cannot reliably count the intersections.</figcaption>
        </div>
  </div></div></section>
  <!-- TASK 1 Ends -->

  <hr/>
  <!-- TASK 2 Begins -->
  <section id="task2">
    <div>

      <h2>Task 2: Two circles <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/two-colored-circles-svg.svg" alt="Two intersecting lines"/></h2>



      <div>
        <p>
          In contrast to Task 1 where we tested VLMs on thin lines, here we evaluate their ability to perceive
          interactions
          between larger objects - specifically, two same-sized filled circles. This task assesses VLMs&#39; capability to
          detect
          (1) small gaps between circles and (2) overlapping circles.
        </p>

        <h3>Images</h3>
        <p>
          We generate 672 images of two circles on a white canvas. The circles vary in size, distance, and orientation:
        </p>
        <ul>
          <li>Circle diameters: 1/4, 1/5, 1/6, or 1/7 of the canvas size</li>
          <li>Distances between circle perimeters: -0.15 to 0.5 times the diameter</li>
          <li>Orientations: 90°, 0°, -45°, and 45° angles with the x-axis</li>
          <li>Canvas sizes: 384, 769, and 1155 pixels</li>
        </ul>

        
        <figcaption>Fig. 3: Examples of two-circle images used in the task, showing different configurations.
        </figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using two different wordings:
        </p>
        <ol>
          <li><em>&#34;Are the two circles touching each other? Answer with Yes/No.&#34;</em></li>
          <li><em>&#34;Are the two circles overlapping? Answer with Yes/No.&#34;</em></li>
        </ol>

        <h3>Groundtruth</h3>
        <div>
          <p>
            Answers are based on the distance d between circle perimeters:
          </p>
          <ul>
            <li>d &lt; 0: Overlapping and touching</li>
            <li>d = 0: Non-overlapping but touching</li>
            <li>d &gt; 0: Non-overlapping and non-touching</li>
          </ul>
          <p>Random-baseline accuracy: 50%.</p>
        </div>




        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of counting line intersections.
        </p>
        <table>
          <tbody><tr>
            <th></th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td>Overlapping</td>
            <td>71.27</td>
            <td>93.30</td>
            <td>88.09</td>
            <td>88.83</td>
          </tr>
          <tr>
            <td>Touching</td>
            <td>74.10</td>
            <td>92.26</td>
            <td>80.95</td>
            <td>94.49</td>
          </tr>
          <tr>
            <td><strong>Average</strong></td>
            <td>72.69</td>
            <td>92.78</td>
            <td>84.52</td>
            <td>91.66</td>
          </tr>
        </tbody></table>

        <h2>Qualitative samples</h2>
      </div>

      <div>

        <div>
          
          <figcaption>Fig. 4: VLMs consistently fail at smaller distances. However, at a large gap, <span>GPT-<span>4o</span></span> remains unreliable (rightmost). <span>Sonnet-<span>3.5</span></span> tends to conservatively answer &#34;No&#34;
            regardless of the actual distance
            between the two circles.</figcaption>
        </div>
      </div>
  </div></section>
  <!-- TASK 2 Ends -->

  <hr/>
  <!-- TASK 3 Begins -->
  <section id="task3">
    <div>

      <h2>Task 3: The circled letter <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/acknowledgement-svg.svg" alt="Two intersecting lines"/></h2>



      <div>
        <p>
          Consistent with prior reports <a href="https://arxiv.org/abs/2304.06712">[2]</a><a href="https://arxiv.org/abs/2310.11441">[3]</a><a href="https://arxiv.org/abs/2309.17421">[4]</a>, we find
          that VLMs can
          100%
          accurately identify a primitive shape (e.g., a red circle ⭕)<a href="https://arxiv.org/abs/2304.06712">[2]</a>
          and can perfectly read an English word (e.g.,
          <span>Subdermatoglyphic</span>) alone. Here, we superimposed the red circle on every
          letter, one at a time, in the word,
          and ask
          VLMs to identify which letter is being circled. While the task is easy to humans, our hypothesis is that if a
          VLM&#39;s
          vision is &#34;blurry&#34;, it might not be able to identify the exact letter being circled since there is tiny
          spacing
          between the adjacent letters.
        </p>

        <h3>Images</h3>
        <p>
          We choose three strings <span>Acknowledgement</span>, <span>Subdermatoglyphic</span>, and tHyUiKaRbNqWeOpXcZvM because they contain
          characters
          of variable widths and heights. Furthermore, all four tested VLMs can read out all characters in these strings
          when
          they are input to the models as an image. While <span>Acknowledgement</span> is a common
          English word, <span>Subdermatoglyphic</span> is
          the
          longest word without repetitive letters. We also test VLMs on the random string <span>tHyUiKaRbNqWeOpXcZvM</span> to
          estimate how
          much model accuracy is due to its familiarity with the word.
        </p>
        <p>
          For each (string, circled-letter) pair, we render a 512×512 image by choosing among 3 red oval line-thickness
          levels,
          2 font sizes, and 4 random positions in the canvas for a total of 24 images. That is, we generate 360, 408,
          and 480
          images for <span>Acknowledgement</span> (15 letters), <span>Subdermatoglyphic</span> (17 letters), and
          <span>tHyUiKaRbNqWeOpXcZvM</span> (20
          letters),
          respectively. We ensure each letter to be circled fits completely the oval.
        </p>

        
        <figcaption>Fig. 5: Examples of circled letter images used in the task, showing different words and circled
          letters.
        </figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using two different wordings:
        </p>
        <ol>
          <li><em>&#34;Which letter is being circled?&#34;</em></li>
          <li><em>&#34;Which character is being highlighted with a red oval?&#34;</em></li>
        </ol>

        <h3>Groundtruth</h3>
        <p>
            Letters need to match predicted letters exactly (case-insensitive).
          </p>




        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of identifying the circled letter.
        </p>
        <table>
          <tbody><tr>
            <th>Word</th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td><span>Acknowledgement</span></td>
            <td>69.03</td>
            <td>97.50</td>
            <td>82.64</td>
            <td>91.11</td>
          </tr>
          <tr>
            <td><span>Subdermatoglyphic</span></td>
            <td>63.60</td>
            <td>91.05</td>
            <td>71.45</td>
            <td>94.49</td>
          </tr>
          <tr>
            <td><span>tHyUiKaRbNqWeOpXcZvM</span></td>
            <td>77.92</td>
            <td>89.90</td>
            <td>65.94</td>
            <td>82.08</td>
          </tr>
          <tr>
            <td><strong>Average</strong></td>
            <td>70.18</td>
            <td>92.81</td>
            <td>73.34</td>
            <td>89.22</td>
          </tr>
        </tbody></table>
      </div>
      <div>
        <h2>Qualitative samples</h2>
        <div>
          
          <figcaption>Fig. 6: Identifying the letter being circled is non-trivial for VLMs across both English words
            (<span>Acknowledgement</span> &amp; <span>Subdermatoglyphic</span>)
            and a random string
            (<span>tHyUiKaRbNqWeOpXcZvM</span>). When making mistakes, VLMs
            tend to
            predict letters adjacent to the circled one.</figcaption>

        </div>
      </div>
  </div></section>
  <!-- TASK 3 Ends -->



  <hr/>
  <!-- TASK 4 Begins -->
  <section id="task4">
    <div>

      <h2>Task 4: Counting overlapping shapes <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/olympic-rings-svg.svg" alt="Two intersecting lines"/></h2>



      <div>
        <p>
          Aligned with prior research <a href="https://arxiv.org/abs/2309.17421">[4]</a>, we also find VLMs to be able
          to count disjoint circles. Yet, here, we test
          VLMs on
          counting circles that are <em>intersecting</em> like in the Olympic logo—a common cognitive development
          exercise for
          preschoolers <a href="https://www.myteachingstation.com/preschool/math/numbers/count-shapes-printables">[5]</a><a href="https://depositphotos.com/vector/how-many-counting-game-with-color-simple-geometric-shapes-for-kids-educational-maths-task-for-266096226.html">[6]</a>.
          Our hypothesis is that a &#34;blurry&#34; vision may not see the
          intersection between two circles
          clearly
          and therefore unable to trace circles and count them. For generalization of our findings, we repeat the
          experiment
          with pentagons as well.
        </p>

        <h3>Images</h3>
        <p>
          In an image of size C×C, where C ∈ {384, 769, 1155}px, we draw N ∈ {5, 6, 7, 8, 9} overlapping, same-sized
          circles
          arranged in two rows like the Olympic logo. A circle diameter φ ∈ {C/5, C/10}. We repeat the images with two
          different
          line thickness for rendering circles. This procedure renders 3 resolutions × 5 × 2 diameters = 60 images. We
          repeat
          for pentagons in addition to circles, resulting in 60 × 2 shapes = 120 images in total. For pentagons, their
          side
          length d ∈ {C/5, C/10}.
        </p>


        
        <figcaption>Fig. 7: Examples of Olympic-like logo images used in the task, showing different numbers of shapes,
          sizes,
          and colors.</figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using two different wordings:
        </p>
        <ol>
          <li><em>&#34;How many {shapes} are in the image? Answer with only the number in numerical format.&#34;</em></li>
          <li><em>&#34;Count the {shapes} in the image. Answer with a number in curly brackets e.g. {3}.&#34;</em></li>
        </ol>
        <p>
          Where {shapes} is either &#34;circles&#34; or &#34;pentagons&#34; depending on the image.
        </p>
        <h3>Groundtruth</h3>
        <p>
            Answers are ∈ {5, 6, 7, 8, 9} (random-baseline accuracy: 20%).
          </p>


        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of identifying the circled letter.
        </p>
        <table>
          <tbody><tr>
            <th></th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td>Circles</td>
            <td>42.50</td>
            <td>20.83</td>
            <td>31.66</td>
            <td>44.16</td>
          </tr>
          <tr>
            <td>Pentagons</td>
            <td>19.16</td>
            <td>9.16</td>
            <td>11.66</td>
            <td>75.83</td>
          </tr>
        </tbody></table>

      </div>

      <div>
        <h2>Qualitative samples</h2>

        <div>
          
          <figcaption>Fig. 8: <span>Gemini-<span>1.5</span>
              Pro</span> often predicts &#34;5&#34; circles.</figcaption>

        </div>
      </div>
  </div></section>
  <!-- TASK 4 Ends -->



  <hr/>
  <!-- TASK 5 Begins -->
  <section id="task5">
    <div>


      <h2>Task 5: Counting the nested squares <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/nested-squares-svg.svg" alt="Two intersecting lines"/></h2>


      <div>
        <p>
          Motivated by the findings that VLMs struggle in counting the intersected circles (Task 4), here, we arrange
          the shapes
          differently so that their edges do <em>not</em> intersect.
          That is, each shape is nested entirely inside another. For completeness, we test squares in this task.
        </p>

        <h3>Images</h3>
        <p>
          In a canvas of size C×C, we render N ∈ {2, 3, 4, 5} nested squares.
          The outermost square is rendered first using a random edge length d and a line thickness ∈ {2, 3, 4}px.
          The remaining N-1 squares are drawn using a size reduction factor, 0.75 × d and placed at a random coordinate
          that
          ensures they do not touch outer squares.
          For each line thickness, we generate 10 images (where squares have different, random locations) to create 3 ×
          10 = 30
          images.
          Repeating the process for all N values results in 4 × 30 = 120 images.
        </p>
        
        <figcaption>Fig. 9: Examples of nested square images used in the task, showing different numbers of squares.
        </figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using the following wording:
        </p>
        <ol>
          <li><em>&#34;Count the total number of squares in the image.&#34;</em></li>
        </ol>
        <p>
          Where {shapes} is either &#34;circles&#34; or &#34;pentagons&#34; depending on the image.
        </p>
        <h3>Groundtruth</h3>
        <p>
            Answers are ∈ {2, 3, 4, 5} (random-baseline accuracy: 25%).
          </p>


        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of counting nested squares.
        </p>
        <table>
          <tbody><tr>
            <th></th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td>Squares</td>
            <td>48.33</td>
            <td>80.00</td>
            <td>55.00</td>
            <td>87.50</td>
          </tr>
        </tbody></table>

      </div>

      <div>

        <h2>Qualitative samples</h2>
        <div>
          
          <figcaption>Fig. 10: Only <span>Sonnet-<span>3.5</span></span> can count
            the
            squares in a majority of the images.</figcaption>
        </div>
      </div>
  </div></section>
  <!-- TASK 5 Ends -->




  <hr/>
  <!-- TASK 6 Begins -->
  <section id="task6">
    <div>

      <h2>Task 6: Counting the rows and columns of a grid <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/grid-3x4-svg.svg" alt="Two intersecting lines"/></h2>


      <div>
        <p>
          The results from prior tasks show VLMs cannot always count shapes that are overlapping (Task 4) or nested
          (Task 5).
          What about adjacent shapes? Here, we tile up shapes (specifically, squares) into a grid and challenge VLMs to
          count—a
          task that is supposedly simple to VLMs given their remarkable performance (≥ 90% accuracy) on DocVQA, which
          includes
          many questions with tables.
          To simplify the task, we ask models to count the number of rows and columns in a given table.
        </p>

        <h3>Images</h3>
        <p>
          A grid may have N×N, N×N&#39;, or N&#39;×N cells, where N∈{3, 4, 5, 6, 7, 8, 9} and N&#39; = N + 1.
          Each grid is rendered with two different line-thicknesses on a canvas of size C×C where C∈{500, 1250, 2000}px.
          Besides empty grids, we also replicate the procedure to make grids contain text (which is more common in
          real-world
          tables) where each cell contains a single random word.
          Two versions combined have 2×222 = 444 images.
        </p>
        
        <figcaption>Fig. 9: Examples of grid images used in the task, showing text-filled and empty grids with various
          dimensions.</figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using two different wordings:
        </p>
        <ol>
          <li><em>&#34;Count the number of rows and columns and answer with numbers in curly brackets. For example, rows={5}
              columns={6}&#34;</em></li>
          <li><em>&#34;How many rows and columns are in the table? Answer with only the numbers in a pair (row, column),
              e.g.,
              (5,6)&#34;</em></li>
        </ol>

        <h3>Groundtruth</h3>
        <p>
            Answers include both the number of rows and columns. An answer is correct when both column and row counts
            are
            correctly predicted.
          </p>

        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of counting rows and columns in
          grids.
        </p>
        <table>
          <tbody><tr>
            <th>Grid type</th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td>Blank</td>
            <td>26.13</td>
            <td>25.75</td>
            <td>25.00</td>
            <td>59.84</td>
          </tr>
          <tr>
            <td>Text</td>
            <td>53.03</td>
            <td>45.83</td>
            <td>47.34</td>
            <td>88.68</td>
          </tr>
          <tr>
            <td><strong>Average</strong></td>
            <td>39.58</td>
            <td>35.79</td>
            <td>36.17</td>
            <td>74.26</td>
          </tr>
        </tbody></table>

      </div>

      <div>
        <h2>Qualitative samples</h2>
        <div>
          
          <figcaption>Fig. 12: Examples from the benchmark show that models consistently fail at counting rows and
            columns of
            blank grids.</figcaption>
        </div>
      </div>

      

      <div>
        <div>
          
          <figcaption>Fig. 13: When text is included in the cells of the grid, the performance of all VLMs improves,
            especially
            <span>Sonnet-<span>3.5</span></span>.
          </figcaption>
        </div>

      </div>
  </div></section>
  <!-- TASK 6 Ends -->


  <hr/>
  <!-- TASK 7 Begins -->
  <section id="task7">
    <div>

      <h2>Task 7: Following single-colored paths <img src="http://martin.kleppmann.com/2024/07/05/static/images/logo/subway-map-svg.svg" alt="Two intersecting lines"/></h2>



      <div>
        <p>
          It is important for VLMs to be able to follow paths in order to read maps or charts, interpret graphs, and
          understand
          user notations (e.g., arrows) in input images. To assess path-following capability, this task asks models to
          count the
          unique-color paths between two given stations in a simplified subway map. This is another easy-to-humans task
          that
          challenges VLMs significantly.
        </p>

        <h3>Images</h3>
        <p>
          We create each subway map on an image of size C×C, where C ∈ {512, 1024}px. We write 4 station names (A, B, C,
          D) at 4
          fixed coordinates. We divide the canvas into an invisible grid of 18×18 cells and initialize 3 path-starting
          points
          C/18px away from each station. We draw a path, using the depth-first search algorithm starting from a random
          station
          and a random starting point, where a valid move is one cell in any direction: North, south, east or west. We
          repeat
          the process so that each station has exactly N ∈ {1, 2, 3} outgoing paths, for a total of 180 maps.
        </p>

        
        <figcaption>Fig. 14: Examples of subway map images used in the task, showing different numbers of paths and
          variations
          in path thickness.</figcaption>
      </div>

      <h3>Prompts</h3>
      <div>
        <p>
          We ask each question using two different wordings:
        </p>
        <ol>
          <li><em>&#34;How many single-colored paths go from A to C? Answer with a number in curly brackets, e.g., {3}&#34;</em>
          </li>
          <li><em>&#34;Count the one-colored routes that go from A to C. Answer with a number in curly brackets, e.g.,
              {3}.&#34;</em>
          </li>
        </ol>

        <h3>Groundtruth</h3>
        <p>
            Answers are ∈ {0, 1, 2, 3} (random-baseline accuracy: 25%).
          </p>

        <h2>Results</h2>
        <p>
          The following table shows the performance of the four models on the task of counting single-colored paths
          between
          stations.
        </p>
        <table>
          <tbody><tr>
            <th>Paths</th>
            <th>
              <p><span>GPT-<span>4o</span></span>
              </p>
            </th>
            <th>
              <p><span>Gemini-<span>1.5</span> Pro</span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3</span></span>
              </p>
            </th>
            <th>
              <p><span>Sonnet-<span>3.5</span></span>
              </p>
            </th>
          </tr>
          <tr>
            <td>1</td>
            <td>67.50</td>
            <td>85.41</td>
            <td>23.75</td>
            <td>95.00</td>
          </tr>
          <tr>
            <td>2</td>
            <td>44.37</td>
            <td>28.75</td>
            <td>37.18</td>
            <td>56.25</td>
          </tr>
          <tr>
            <td>3</td>
            <td>36.71</td>
            <td>25.78</td>
            <td>15.42</td>
            <td>25.39</td>
          </tr>
          <tr>
            <td><strong>Average</strong></td>
            <td>45.89</td>
            <td>40.01</td>
            <td>23.78</td>
            <td>50.18</td>
          </tr>
        </tbody></table>
      </div>
      <div>
        <h2>Qualitative samples</h2>
        <div>
          
          <figcaption>Fig. 15: Some VLMs (<span>Gemini-<span>1.5</span></span>,
            <span>Sonnet-<span>3</span></span>)
            surprisingly fail in even extremely easy cases (leftmost).
            As the
            number of paths exiting each station increases, VLMs tend to perform worse.
          </figcaption>

        </div>
      </div>
  </div></section>
  <!-- TASK 7 Ends -->




  
  <!-- Add this script at the end of the body -->



</div>
  </body>
</html>
