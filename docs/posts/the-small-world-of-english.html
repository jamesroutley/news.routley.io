<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.inotherwords.app/linguabase/">Original</a>
    <h1>The Small World of English</h1>
    
    <div id="readability-page-1" class="page"><div>
            
            
    <p>Building a word game forced us to solve a measurement problem: how do you rank 40+ ways to associate any given word down to exactly 17 playable choices? We discovered that combining human-curated thesauri, book cataloging systems, and carefully constrained LLM queries creates a navigable network where 76% of random word pairs connect in ≤7 hops—but only when you deprecate superconnectors and balance multiple ranking signals. The resulting network of 1.5 million English terms reveals that nearly any two common words connect in 6-7 hops through chains of meaningful associations. The mean path length of 6.43 hops held true across a million random word pairs—shorter than we’d guessed, and remarkably stable.</p>

            <div>
                <p><span>1.5M</span>
                    <span>Headwords</span>
                </p>
                <p><span>100M</span>
                    <span>Relationships</span>
                </p>
                <p><span>&lt;7</span>
                    <span>Degrees of Separation</span>
                </p>
            </div>

<!--            <p>This is consistent with the small-world structure and near-universal connectivity seen in lexical network research on smaller datasets.<sup>1,2</sup> The network’s structure makes intuitive semantic navigation possible—players can <em>feel their way</em> from ‘sugar’ to ‘peace’ because the intermediate steps (sweet → harmony) make sense.</p>
-->
<p>This is consistent with the small-world structure and near-universal connectivity seen in lexical network research on smaller datasets.<sup>1,2</sup> The network&#39;s structure makes intuitive semantic navigation possible—players can <em>feel their way</em> through meaningful transitions: a crown&#39;s gemstones lead to emerald&#39;s foliage and finally to a forest canopy, or a flame becomes an ember, then a glowing memory, a mental recall, and finally the action to cancel.</p>


<!--
            <div class="illustration">
                <p style="font-size: 1.4rem; margin: 0; font-weight: 300;">
                    <span class="word-path-item">sugar</span>
                    <span class="word-path-arrow">→</span>
                    <span class="word-path-item">sweet</span>
                    <span class="word-path-arrow">→</span>
                    <span class="word-path-item">harmony</span>
                    <span class="word-path-arrow">→</span>
                    <span class="word-path-item">peace</span>
                </p>
            </div>
-->

<p data-path-1="Batman|vigilante|watchful|circumspect|inspect" data-path-2="crown|gemstones|emerald|foliage|canopy" data-path-3="fire|ember|memory|recall|cancel" data-path-4="nail|claw|hook|tune|song" data-path-5="salt|crystals|facet|component|hardware" data-current="1">
        <span>Batman</span>
        <span>→</span>
        <span>vigilante</span>
        <span>→</span>
        <span>watchful</span>
        <span>→</span>
        <span>circumspect</span>
        <span>→</span>
        <span>inspect</span>
    </p>





            <h2>The Mathematics of Semantic Distance</h2>

<p>English exhibits network effects remarkably similar to social networks—nearly any random pair of words can reach each other in just a few hops through chains of meaningful associations. This “small world” phenomenon was first measured in word co-occurrence networks,<sup>3</sup> and persists even after we deprioritize superconnector words that might otherwise dominate many paths.

</p><p>To probe this, we randomly sampled 1 million word pairs (4 days processing on 32 cores), to get a strong statistical sampling of the connected core of English.</p>

            <div>
                <h3>How to connect any random 2 words?</h3>
                <div>
                    <p><span>1</span>
                        <span>0.01%</span>
                    </p>
                    <p><span>2</span>
                        <span>0.15%</span>
                    </p>
                    <p><span>3</span>
                        <span>2.07%</span>
                    </p>
                    <p><span>4</span>
                        <span>9.97%</span>
                    </p>
                    <p><span>5</span>
                        <span>21.58%</span>
                    </p>
                    <p><span>6</span>
                        <span>24.15%</span>
                    </p>
                    <p><span>7</span>
                        <span>18.25%</span>
                    </p>
                    <p><span>8</span>
                        <span>11.18%</span>
                    </p>
                    <p><span>9</span>
                        <span>6.19%</span>
                    </p>
                    <p><span>10+</span>
                        <span>6.45%</span>
                    </p>
                </div>
                <p>Hop Distance Between Words</p>
            </div>

            <p>This bell curve centered at 5-6 hops creates ideal puzzle parameters. 
            
            </p><p>Here are some <b>random</b> examples at three distances. Conjugations lead to longer paths.</p>

            <!-- START ANIMATED HOPS SECTION -->



<div>
    
    <div>
        <div id="hop-2">
            
            <p><span data-path-1="rhyme|beat|percussion" data-path-2="opinionated|biased|discrimination" data-path-3="anticipation|yearning|reminiscence" data-current="1">
                <span>rhyme</span>
                <nobr><span>→</span>
                <span>beat</span></nobr>
                <nobr><span>→</span>
                <span>percussion</span></nobr>
            </span>
        </p></div>
        
        <!--
        <div class="path-example-row" id="hop-4">
            <div class="hops-label">
                <div class="hops-number">4</div>
                <div class="hops-hops">HOPS</div>
            </div>
            <span class="flip-path" 
                  data-path-1="salt lakes|brine|cucumbers|capers|plunder"
                  data-path-2="self-reliance|self-respect|honor|graduation|cap and gown"
                  data-path-3="anarchist|radical|modernize|gentrify|gentrified"
                  data-path-4="impact|affect|scowl|frustration|rage"
                  data-path-5="ports|dock|platform|campaign|skirmishes"
                  data-path-6="machinations|operations|joint|elbow|funny bone"
                  data-path-7="OK|mediocre|inferior|underling|henchman"
                  data-current="1">
                <span class="word-pill first-node">salt lakes</span>
                <nobr><span class="path-arrow">→</span>
                <span class="word-pill">brine</span></nobr>
                <nobr><span class="path-arrow">→</span>
                <span class="word-pill">cucumbers</span></nobr>
                <nobr><span class="path-arrow">→</span>
                <span class="word-pill">capers</span></nobr>
                <nobr><span class="path-arrow">→</span>
                <span class="word-pill last-node">plunder</span></nobr>
            </span>
        </div>
        -->
        
        <div id="hop-5">
            
            <p><span data-path-1="outbreak|strife|contenders|finalists|runners-up|grand prizes" data-path-2="nourish|health|malady|hypochondria|self-diagnosis|paranoia" data-path-3="reveler|merrymaker|pageant|sacrament|transubstantiation|chalice" data-path-4="overture|invitation|summons|complaint|grouse|capercaillie" data-current="1">
                <span>outbreak</span>
                <nobr><span>→</span>
                <span>strife</span></nobr>
                <nobr><span>→</span>
                <span>contenders</span></nobr>
                <nobr><span>→</span>
                <span>finalists</span></nobr>
                <nobr><span>→</span>
                <span>runners-up</span></nobr>
                <nobr><span>→</span>
                <span>grand prizes</span></nobr>
            </span>
        </p></div>
        
        <div id="hop-8">
            
            <p><span data-path-1="grounding|anchoring|berth|sleeping|nocturnal|nightjar|chirring|bombylious|dronelike" data-path-2="double-locks|padlocks|panic button|911|hijackers|coercers|ostracizers|expellers|dislodgers" data-path-3="squeeze|hug|cuddle|fireside|mantelpiece|ledge|precipice|abyss|black hole|supernova" data-current="1">
                <span>grounding</span>
                <nobr><span>→</span>
                <span>anchoring</span></nobr>
                <nobr><span>→</span>
                <span>berth</span></nobr>
                <nobr><span>→</span>
                <span>sleeping</span></nobr>
                <nobr><span>→</span>
                <span>nocturnal</span></nobr>
                <nobr><span>→</span>
                <span>nightjar</span></nobr>
                <nobr><span>→</span>
                <span>chirring</span></nobr>
                <nobr><span>→</span>
                <span>bombylious</span></nobr>
                <nobr><span>→</span>
                <span>dronelike</span></nobr>
            </span>
        </p></div>
    </div>
</div>



            <!-- END ANIMATED HOPS SECTION -->

            <div>

<!--
                <img src="images/visual-thesaurus-screenshot.jpg" 
                     alt="Screenshot of the visual thesaurus mode showing word connections"
                     style="width: 100%; height: auto; border-radius: 16px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
                <p style="font-size: 0.9rem; font-weight: 300; margin: 0; color: rgba(255, 255, 255, 0.7); text-align: right; margin-top: 0.5rem; font-style: italic; line-height: 1.3;">
-->
<picture>
    <source type="image/webp" srcset="images/resized/webp/visual-thesaurus-screenshot_std.webp 1x,
                    images/resized/webp/visual-thesaurus-screenshot_retina.webp 2x"/>
    <source type="image/jpeg" srcset="images/resized/visual-thesaurus-screenshot_std.jpg 1x,
                    images/resized/visual-thesaurus-screenshot_retina.jpg 2x"/>
    <img src="https://www.inotherwords.app/linguabase/images/resized/visual-thesaurus-screenshot_std.jpg" alt="Screenshot of the visual thesaurus mode showing word connections" loading="lazy"/>
</picture><p>

                    Visual thesaurus mode reveals multiple senses and weighted connections for any word.
                </p>
            </div>

<h2>Network Construction and Coverage</h2>

<p>When we started this project, we tried the obvious approach: combining existing resources like WordNet with early-generation AI tools including LDA topic modeling and static word vectors. WordNet gave us clean synonym sets but lacked the associative richness players expect (“coffee” → “morning” not just “beverage”). LDA found topical clusters but mixed unrelated terms that happened to co-occur. Word vectors collapsed all senses into single points, making “bank” (river) indistinguishable from “bank” (financial). These produced fragmented, overly generic relationships that lacked the nuance our game needed.</p>

<p>We capture 40 associations per term (enough for algorithmic flexibility) and display 17 in our interfaces (what users can reasonably process). This depth provides flexibility for both puzzle generation and reference use.</p>

<h2>1,525,522 headwords</h2>

<p>We built a semantic network of 1.5 million English terms by casting a wider net than traditional resources. Where academic dictionaries drew sharp boundaries—excluding slang, technical jargon, compound phrases, and proper nouns—we included what people actually say and write. From “ice cream” to “thermodispersion,” from “ghosting” to “Khao-I-Dang.”</p>

<p>This scale would have cost tens of millions to achieve manually. Consider the monumental pre-LLM efforts:</p>

<ul>
<li><strong>WordNet (1985-2010)</strong> - Princeton’s 25-year project produced 155,000 words in synonym groups. Became the NLP standard despite missing everyday compounds.</li>
<li><strong><a href="https://www.oed.com/information/about-the-oed">OED</a> (1857-1928, ongoing)</strong> - The definitive historical dictionary with 500,000+ entries. Took 70 years and thousands of contributors.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Webster%27s_Third_New_International_Dictionary">Webster’s Third</a> (1961)</strong> - America’s unabridged dictionary with 476,000 entries. Required 757 editor-years and $3.5 million ($50M+ today).</li>
<li><strong>Roget’s Thesaurus (1852)</strong> - The original meaning-based reference with 15,000 words in 1,000 conceptual categories.</li>
</ul>

<p>Word counts become arbitrary at this scale. Include every technical term, place name, and slang variant, and the count explodes. Whether we have 1.5 million or 2 million depends entirely on where you draw the line.</p>


<h3>Atlas of Connected Meaning</h3>
<p>Our inclusion criteria cast a wide net: all terms that volunteer lexicographers at Wiktionary have <a href="https://en.wiktionary.org/wiki/Wiktionary:Criteria_for_inclusion">included</a> (slightly more liberal than typical unabridged dictionaries), plus high-importance Wikipedia topics that are 1-3 words long (measured by PageRank), plus frequently produced compound terms generated by LLMs when analyzing 648,460 <a href="https://www.loc.gov/catdir/cpso/lcco/">Library of Congress book classifications</a>. Compound terms like “local governance” (appearing in 44,507 classifications) and “literary criticism” (19,417) were included, while “wild equids” (5 occurrences) did not. <!--The semantic network is accessible through two interfaces: a game client and a visual thesaurus tool. --></p>

<h3>What kinds of words?</h3>
<p>We include all these kinds of words, and to illustrate that there’s no clear redline to include or exclude, here’s a gradation of <b>common</b> and <b>obscure</b> examples of each...</p>

            <div>
                <div>
                    <h4>Compounds &amp; Phrases</h4>
                    <ul>
                        <li><strong>“health care”</strong> <span>(standard compound)</span></li>
                        <li><strong>“cut the rug”</strong> <span>(dated slang for dancing)</span></li>
                        <li><strong>“blow one’s nose”</strong> <span>(phrasal verb)</span></li>
                        <li><strong>“hatch, match, and dispatch”</strong> <span>(British newspaper jargon)</span></li>
                        <li><strong>“make the welkin ring”</strong> <span>(archaic for loud noise)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Slang &amp; Neologisms</h4>
                    <ul>
                        <li><strong>“ghosting”</strong> <span>(suddenly ending communication)</span></li>
                        <li><strong>“panda huggers”</strong> <span>(political slang)</span></li>
                        <li><strong>“Devil’s buttermilk”</strong> <span>(euphemism for alcohol)</span></li>
                        <li><strong>“drungry”</strong> <span>(drunk + hungry)</span></li>
                        <li><strong>“brass neck”</strong> <span>(British for audacity)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Technical Jargon</h4>
                    <ul>
                        <li><strong>“antibiotics”</strong> <span>(medical term)</span></li>
                        <li><strong>“barber-chaired”</strong> <span>(logging accident)</span></li>
                        <li><strong>“lead plane”</strong> <span>(wildfire aviation)</span></li>
                        <li><strong>“hemicorpectomy”</strong> <span>(surgical removal)</span></li>
                        <li><strong>“photonephograph”</strong> <span>(kidney imaging)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Species &amp; Taxonomy</h4>
                    <ul>
                        <li><strong>“German shepherd”</strong> <span>(dog breed)</span></li>
                        <li><strong>“dwarf sirens”</strong> <span>(salamander family)</span></li>
                        <li><strong>“northern raccoons”</strong> <span>(regional variant)</span></li>
                        <li><strong>“Angoumois moths”</strong> <span>(<em>Sitotroga cerealella</em>)</span></li>
                        <li><strong>“grass crab spider”</strong> <span>(specific arachnid)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Historical Language</h4>
                    <ul>
                        <li><strong>“thou”</strong> <span>(archaic second person)</span></li>
                        <li><strong>“oftimes”</strong> <span>(Middle English)</span></li>
                        <li><strong>“mean’st”</strong> <span>(archaic conjugation)</span></li>
                        <li><strong>“crurifragium”</strong> <span>(Roman execution)</span></li>
                        <li><strong>“naumachies”</strong> <span>(staged naval battles)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Word Variations</h4>
                    <ul>
                        <li><strong>“running”</strong> <span>(present participle)</span></li>
                        <li><strong>“rotavates”</strong> <span>(tills with rotary blades)</span></li>
                        <li><strong>“masculises”</strong> <span>(British spelling)</span></li>
                        <li><strong>“disappoynts”</strong> <span>(16-17th century)</span></li>
                        <li><strong>“mattifies”</strong> <span>(makes matte)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Acronyms</h4>
                    <ul>
                        <li><strong>“GPS”</strong> <span>(Global Positioning System)</span></li>
                        <li><strong>“CICUs”</strong> <span>(Coronary Intensive Care Units)</span></li>
                        <li><strong>“HKPF”</strong> <span>(Hong Kong Police Force)</span></li>
                        <li><strong>“MIMO-OFDM”</strong> <span>(telecom standard)</span></li>
                        <li><strong>“3DTDS”</strong> <span>(3-D structural term)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Places &amp; Culture</h4>
                    <ul>
                        <li><strong>“Broadway”</strong> <span>(NYC theater district)</span></li>
                        <li><strong>“Harsimus”</strong> <span>(Jersey City district)</span></li>
                        <li><strong>“Altai kray”</strong> <span>(Russian federal subject)</span></li>
                        <li><strong>“Khao-I-Dang”</strong> <span>(refugee camp)</span></li>
                        <li><strong>“ballybethagh”</strong> <span>(Irish land measurement)</span></li>
                    </ul>
                </div>
                
                <div>
                    <h4>Rare &amp; Nonce</h4>
                    <ul>
                        <li><strong>“selfie”</strong> <span>(once nonce, now standard)</span></li>
                        <li><strong>“greppable”</strong> <span>(programmer slang)</span></li>
                        <li><strong>“kiteboating”</strong> <span>(water sport)</span></li>
                        <li><strong>“quattrocentists”</strong> <span>(1400s scholars)</span></li>
                        <li><strong>“noitamrofni”</strong> <span>(information backwards)</span></li>
                    </ul>
                </div>
            </div>

            <p>Our analysis revealed a fundamental division in the network:</p>

            <ul>
                <li><strong>Reachable terms (56.8%):</strong> 870,522 words that appear in the top-40 associations of at least one other word</li>
                <li><strong>Unreachable terms (43.2%):</strong> 662,903 words that never appear in any other word’s top-40 list</li>
            </ul>

            <p>The unreachable terms include rare compounds (“stewing in one’s own grease”), technical terminology (“thermodispersion”), proper nouns (“Besisahar”), and alternative capitalizations. While these terms can point to other words, no words point back to them strongly enough to rank in any top-40 list. This doesn’t affect puzzles—which start from common words—but reveals an interesting property of the semantic network.</p>

            <h2>Beyond Traditional Thesauri</h2>

            <div>

<!--
                <img src="images/thesaurus-page.jpg" 
                     alt="A close-up of an open page from a traditional thesaurus"
                     style="width: 100%; height: auto; border-radius: 16px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
-->
<picture>
    <source type="image/webp" srcset="images/resized/webp/thesaurus-page_std.webp 1x,
                    images/resized/webp/thesaurus-page_retina.webp 2x"/>
    <source type="image/jpeg" srcset="images/resized/thesaurus-page_std.jpg 1x,
                    images/resized/thesaurus-page_retina.jpg 2x"/>
    <img src="https://www.inotherwords.app/linguabase/images/resized/thesaurus-page_std.jpg" alt="A close-up of an open page from a traditional thesaurus" loading="lazy"/>
</picture>

                <p>
                    Traditional thesauri focus on synonyms for abstract concepts while excluding concrete objects because they had limited paper pages.
                </p>
            </div>

            <p>Our visual thesaurus presents up to 8 contextual senses per term, each showing its own 17-word neighborhood. Just as our headword inclusion is necessarily arbitrary, so too is our sense distinction. LLMs identified these senses by querying with various prompts for different meanings and contextual flavors, then merging similar results. We capped it at 8 senses as more became unwieldy in the user interface. Whether “bank” gets 2 senses or 5, whether “coffee” as beverage differs from “coffee” as social ritual—these are judgment calls.</p>

            <p>Beyond homographs (words with identical spelling but different meanings, like “bass” for sound versus fish), we capture what we call “contextual flavors” within single senses. ‘Coffee’ connects to ‘café’ (location), ‘beverage’ (category), and ‘espresso’ (variety)—same core meaning, different facets.</p>

            <p>Our design philosophy centered on how people think of word associations—pools of related meanings that don’t necessarily align with how dictionaries split formal senses or define when meanings relate. This approach yields an average of 70 semantically connected words per headword across multiple senses, compared to 10-20 in traditional resources. Examples of our relationship types include:</p>

            <ul>
                <li><strong>Similar meanings:</strong> house → domicile, lodge</li>
                <li><strong>Category members:</strong> house → bungalow, villa</li>
                <li><strong>Functional relationships:</strong> horse → saddle, bridle</li>
                <li><strong>Cultural associations:</strong> breakfast → coffee, pastries</li>
                <li><strong>Taxonomic connections:</strong> quark → boson, fermion</li>
                <li><strong>Domain crossings:</strong> quark → Feynman (physics) or quark → cheese (food)</li>
                <li><strong>Thematic groupings:</strong> hike, nature, trail</li>
            </ul>

            <p>This approach yielded approximately 100 million directed edges connecting our 1.5 million terms.</p>

            <!-- KEEP THE INTERACTIVE WORD SELECTOR AS IS -->
            <div>
                <h3>Try it yourself: What relates to <span>“music”</span>?</h3>
                <p>
                        Pick any 10 words from the pink box that you think best relate to “music.” There’s no perfect answer—that’s the point.
                    </p>
                <p id="completion-message">
                    Great choices! You’ve captured your unique perspective on music.
                </p>
                
                
            </div>

            

            

<h2>Multiple Meanings as Network Bridges</h2>

            <p>English words often carry multiple meanings, creating natural bridges in the network:</p>

            <div>
                <div>
                    <h3>Double Meanings</h3>
                    <p>Words with entirely different definitions: “bass” (sound/fish), “tear” (eye/rip)</p>
                </div>
                <div>
                    <h3>Related Meanings</h3>
                    <p>Connected definitions: “head” as body part, leadership role, or ship’s bow</p>
                </div>
                <div>
                    <h3>Contextual Flavors</h3>
                    <p>“Hiking” as nature experience vs. physical exercise</p>
                </div>
            </div>

            <p>These multi-sense words create semantic bridges between seemingly unrelated concepts. Words like “ground” can connect earth, coffee, and electrical circuits in a single conceptual leap.</p>


<!-- new about semantic bridges --> 

            <p>You’d think words with multiple meanings would connect distant parts of the network faster. Turns out they don’t—they just give you more creative ways to navigate the same distance. Our analysis of 100k homograph-containing paths shows they average 6.57 hops versus the 6.43 random baseline. Instead of creating shortcuts, they exist in densely connected regions, offering creative routing options rather than efficiency gains.</p>

            <h3>The Bridges That Remain </h3>

            <p>To prevent too many paths from routing through generic hubs like “general” or “study,” we systematically penalized superconnectors throughout our workflow. But which words still emerge as natural bridges after this filtering?</p>

            <div>
                <h3>
                    Try it yourself: Explore the <span>bridges</span> that survived
                </h3>
                <p>
                    After filtering out generic connectors, which words still bridge English&#39;s network?
                </p>
                
                <p><label for="bridge-slider">
                        Showing bridges ranked <span id="slider-range">1-2</span>
                    </label>
                    
                </p>
                
                
            </div>

            

            

            <p>These survivors represent genuine conceptual bridges—words that naturally connect different domains through polysemy (“polish” as verb/nationality), historical significance (“Renaissance”), or conceptual richness (“jazz” connecting musical techniques, cultural movements, and time periods). Their average position of ~2.2 hops from path origins shows they typically serve as the critical pivot point between disparate concepts.</p>

<!-- -->

            
            <p>So where did we get our data?</p>




            <h2>Five Data Sources</h2>

            <div>
<!--
                <img src="images/knowledge-sources.jpg" 
                     alt="A visualization of the five knowledge sources combining into the Linguabase"
                     style="width: 100%; height: auto; border-radius: 16px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
-->
<picture>
    <source type="image/webp" srcset="images/resized/webp/knowledge-sources_std.webp 1x,
                    images/resized/webp/knowledge-sources_retina.webp 2x"/>
    <source type="image/jpeg" srcset="images/resized/knowledge-sources_std.jpg 1x,
                    images/resized/knowledge-sources_retina.jpg 2x"/>
    <img src="https://www.inotherwords.app/linguabase/images/resized/knowledge-sources_std.jpg" alt="A visualization of the five knowledge sources combining into the Linguabase" loading="lazy"/>
</picture>

                <p>
                    The Linguabase integrates five complementary knowledge sources into a unified semantic network.
                </p>
            </div>

            <p>The Linguabase integrates five complementary knowledge sources, each contributing unique strengths to our amalgam scoring system that uses multiple ranking signals—from word frequency and co-occurrence patterns to manually curated relationship scores:</p>

            <h3>1. In-House Lexicographic Work</h3>
            <p>Our lexicographer and a team of freelance grad students manually created specialized word lists for 5k varied topics, and associations for polysemous terms and word types like interjections that traditional lexicography treats as “stopwords.” These lists cover many of the most important common terms with multiple meanings.</p>


<!-- Visual 4: LLM Generation vs Recognition -->
<div>
    <h4>LLM Generation vs. Recognition</h4>
    <div>
        <div>
            <h5>Generation Mode</h5>
            <p>“What relates to schizophrenia?”</p>
            
        </div>
        <div>
            <h5>Recognition Mode</h5>
            <p>“Is ‘shamanism’ related?”</p>
            <div><p>
                → Yes, through cultural</p></div>
        </div>
    </div>
</div>



            <h3>2. Mining 125 Years of Library Wisdom</h3>
<p>We discovered that LLMs are much better at recognizing valid semantic relationships than generating them from scratch. Ask an LLM “What relates to coffee?” and you’ll get predictable answers: beverage, caffeine, morning. But the <a href="https://www.loc.gov/catdir/cpso/lcco/">Library of Congress classification system</a> revealed that ‘coffee’ appears in 2,542 different book classifications—linking to ‘fair trade certification’ in economic texts, ‘coffee berry borer’ in Hawaiian agriculture books, and ‘import-export tariffs’ in 487 trade policy publications. These connections capture how coffee actually intersects with global commerce, agriculture, and regulation.</p>

<!-- Visual 2: Coffee's 2,542 Contexts -->
<div>
    <h4>Coffee’s 2,542 Library Contexts</h4>
    <div>
        <div>
            <p>487</p>
            <p>Economics</p>
            <p>fair trade, tariffs, commodity markets</p>
        </div>
        <div>
            <p>312</p>
            <p>Agriculture</p>
            <p>berry borer, arabica, soil</p>
        </div>
        <div>
            <p>208</p>
            <p>Culture</p>
            <p>café society, coffeehouse politics</p>
        </div>
        <div>
            <p>89</p>
            <p>Chemistry</p>
            <p>caffeine extraction, roasting</p>
        </div>
    </div>
    <p>
        + 1,446 more classifications across history, law, art, medicine...
    </p>
</div>


<p>Since 1897, LOC catalogers have encoded the intellectual connections between 17 million books, creating what’s essentially a 125-year collaborative knowledge graph built by thousands of subject experts. Each classification represents a moment when a human expert decided “these concepts belong together”—and unlike web text, these decisions were expensive and permanent, made before SEO or engagement metrics existed.</p>

<!-- Visual 5: Expert Curation vs Crowd Wisdom -->
<div>
    <h4>Expert Curation vs. Crowd Wisdom</h4>
    <div>
        <div>
            <h5>Web Text</h5>
            <div>
                <p>“Coffee is life! ♥”</p>
                <p>(1 million tweets)</p>
            </div>
            <p>↓</p>
            
        </div>
        <div>
            <h5>LOC Classifications</h5>
            <div>
                <p>“Coffee industry—Labor—Guatemala”</p>
                <p>(47 scholarly books)</p>
            </div>
            <p>↓</p>
            
        </div>
    </div>
</div>




<p>We gave an LLM a focused task: generate word lists for each of LOC’s 648,460 classifications. A classification like “Hawaiian coffee trade” triggered specific, expert-like outputs: “kona coffee, arabica beans, coffee tariffs, pacific trade routes, coffee auctions”—far richer than asking generically about coffee. Each classification acted as a pre-engineered prompt that specified exactly which semantic neighborhood we wanted. “Schizophrenia—medical aspects” surfaced “atypical antipsychotic, dopamine antagonist,” while “Schizophrenia—fiction” yielded “asylum writings, trauma memoirs, neurodivergent voices,” capturing the full dimensionality of concepts.</p>

<!-- Visual 6: Context Matters - Schizophrenia -->
<div>
    <h4>Context Shapes Connections: Schizophrenia</h4>
    
</div>



<p>The real magic came from inverting the index. When we asked “Which classifications contain ‘algorithm’?” we found it appearing not just in computer science but in “aleatory electronic music” (alongside John Cage and stochastic processes), “mathematics in arts” (with fractals and Fibonacci sequences), and “investment mathematics” (with portfolio optimization). The system surfaced connections that require domain expertise: ‘Las Vegas’ linking to ‘Colorado River water rights’ through 12 books about Nevada’s water crisis, or ‘origami’ connecting to ‘shell structures’ and ‘stress analysis’ through engineering texts on deployable structures.</p>

<!-- Visual 1: The Double Inversion Process -->
<div>
    <h4>The Double Inversion Process</h4>
    <div>
        <div>
            <p>STEP 1: Classification → Terms</p>
            <p>
                “Hawaiian coffee trade” → kona, arabica, tariffs, pacific routes...
            </p>
        </div>
        <p>↓</p>
        <div>
            <p>STEP 2: Which terms co-occur with “algorithm”?</p>
            <p>
                Found with: John Cage, fractals, portfolio optimization...
            </p>
        </div>
        <p>↓</p>
        <div>
            <p>STEP 3: Build co-occurrence network</p>
            <p>
                algorithm → stochastic music (8.4) | Fibonacci (7.2) | fractals (6.8)
            </p>
        </div>
    </div>
</div>



<p>This approach gave us 3.1 million unique terms weighted by intellectual effort—a monograph on ‘bank equipment’ that mentions ‘pneumatic tubes’ (still used in 15 classifications!) counts more than casual blog mentions. Terms like “cultural heritage” appearing in 53,833 classifications became superconnectors we could appropriately down-rank, while preserving the “boring but essential” connections found in specialized journals like “sewer pipe periodicals” that link urban infrastructure to public health.</p>

<!-- Visual 7: Superconnector Deprecation -->
<div>
    <h4>Superconnector Term Penalties</h4>
    <div>
        <div>
            <p>cultural heritage</p>
            
            <p>
                53,833 × 0.15
            </p>
        </div>
        <div>
            <p>local governance</p>
            
            <p>
                44,507 × 0.26
            </p>
        </div>
        
        <div>
            <p>pneumatic tubes</p>
            
            <p>
                15 × 0.95
            </p>
        </div>
    </div>
    <p>
        Higher frequency → Lower multiplier → Pushed down in rankings
    </p>
</div>
<p>The process also revealed what we call the “Montreal effect”—where ‘bagels’ incorrectly associates with ‘Expo 67,’ ‘McGill University,’ and ‘French-speaking’ simply because Montreal is famous for its bagels. Our initial algorithm strengthened these geographic contaminations throughout the data. We resolved these spurious connections through subsequent LLM reviews that could distinguish true semantic relationships (“bagels → boiled dough → chewy texture”) from coincidental geographic co-occurrence (“bagels → Montreal Canadiens”).</p>

<!-- Visual 3: The Montreal Effect -->
<div>
    <h4>The Montreal Effect: Geographic Contamination</h4>
    <div>
        <div>
            <h5>❌ Geographic Co-occurrence</h5>
            <div>
                <p>Bagels ↔ Expo 67</p>
                <p>Bagels ↔ McGill University</p>
                <p>Bagels ↔ Montreal Canadiens</p>
                <p>Bagels ↔ French-speaking</p>
            </div>
        </div>
        <div>
            <h5>✓ True Semantic Relations</h5>
            <div>
                <p>Bagels ↔ boiled dough</p>
                <p>Bagels ↔ Jewish cuisine</p>
                <p>Bagels ↔ sesame seeds</p>
                <p>Bagels ↔ chewy texture</p>
            </div>
        </div>
    </div>
</div>




            <h3>3. Human-Curated Resources</h3>
            <p>Over 70 existing references contributed—dictionaries, thesauri, and encyclopedias from Wiktionary and WordNet to specialized resources like NASA’s thesaurus and the National Library of Medicine’s UMLS. Relationships appearing across multiple sources received higher weights. </p>
            
            <div>
                <div>
                    <h3>General Sources</h3>
                    <ul>
                        <li>Wiktionary</li>
                        <li>WordNet, ConceptNet, FrameNet</li>
                        <li>Roget’s Thesaurus</li>
                        <li>SWOW-EN18</li>
                    </ul>
                </div>
                <div>
                    <h3>Specialized Sources</h3>
                    <ul>
                        <li>Getty Art &amp; Architecture</li>
                        <li>NASA Thesaurus</li>
                        <li>UMLS Metathesaurus</li>
                        <li>AGROVOC Thesaurus</li>
                    </ul>
                </div>
            </div>

<h3>4. Pre-LLM Topic Extraction</h3>

<p> Before the rise of modern LLMs, we applied Latent Dirichlet Allocation (LDA) in 2013-2014 to discover eight context clusters for every headword in an in-house corpus of notable literary works. The algorithm scans large text collections and groups words that appear in similar contexts. Running it took 200,000 super-computer hours on the NSF’s Extreme Science and Engineering Discovery Environment (XSEDE)—decades on a single machine. Results were noisy: a delightful mix of intuitive associations and oddities (often caused by treating compound terms as separate words). Still, the run surfaced relationships that pure frequency analysis and today’s LLMs miss.
</p>

<p>We skipped early word-embedding vectors—numeric coordinates that place context-similar words near one another but merge all senses into one point—because, as games like <a href="https://www.inotherwords.app/semantic-games/">Semantle</a> show, their distances rarely match human intuition. We also evaluated word embeddings but their single-vector-per-word approach couldn’t handle our need for multiple senses—a fundamental limitation that various researchers tried to patch.<sup>5</sup></p>


            <h3>5. Large Language Model Enhancement</h3>
        
<p>Starting in 2023, frontier models finally provided the semantic understanding we needed—they could distinguish “bank” (river) from “bank” (money) and generate contextually appropriate associations for each. These models could handle:</p>
            <ul>
                <li>Everyday compound terms (“apple pie”, “department store”)</li>
                <li>Morphological variations across parts of speech</li>
                <li>Contextual dimensions of common words</li>
                <li>Capitalization distinctions (“China” vs. “china”)</li>
            </ul>

<p>Still, left to their own devices, LLMs are banal and formulaic, wallowing in cliche, latching onto what they think prompts intend. We ran over 80 million API calls (~$200k in Azure API costs, with minor xAi costs) across dozens of workflows to combat this tendency. Beyond the LOC classifications, we applied focused-prompt strategies across our entire corpus: extracting distinct senses for each headword, generating contextual word lists per sense, prompting for cultural variations and regional differences. Each workflow fed into the next—outputs from sense detection became inputs for association generation, which informed cultural expansion passes. The key was always the same: constrained, specific prompts yielded far better results than open-ended queries.</p>

<p>Even with careful prompting, the Montreal effect persisted. Geographic contamination appeared throughout: ‘Broadway’ linked to ‘taxis’ through New York; ‘grits’ to ‘jazz’ through the American South. We resolved these spurious connections through iterative LLM reviews that learned to distinguish true semantic relationships from coincidental geographic co-occurrence. This research and computational scale was made possible by $295k NSF SBIR seed funding (#2329817) and $150k Microsoft Azure compute resources.</p>


            <h2>Understanding Our Biases</h2>

            <p>Every semantic network encodes particular worldviews about which words relate to each other and how strongly they connect. Here are six key sources of bias that shape our network’s rankings and inclusions:</p>

            <div>
                <table>
                    <tbody><tr>
                        <th>Editorial Choices</th>
                        <th>AI Training Data</th>
                    </tr>
                    <tr>
                        <td>
                            Our lexicographer and team manually crafted relationships for common polysemous terms, inevitably encoding their linguistic backgrounds, cultural contexts, and conceptual frameworks about how meaning connects.
                            <em>Examples: “market” → includes “variety” and “retail,” omits “souk,” “bazaar” • “breakfast” → includes “cereal,” “toast,” omits “congee,” “idli” • “music” → includes “jazz,” “consonance,” omits “gamelan,” “qawwali”</em>
                        </td>
                        <td>
                            GPT-4o’s training data shapes its semantic associations, while its guardrails suppress certain connections. We supplemented with Grok-3 specifically for vulgar and offensive terms that GPT-4o wouldn’t adequately cover.
                            <em>Examples: “sex” → clinical terms favored over colloquial language • “death” → euphemisms like “passing” prioritized over direct terms like “corpse,” “decay”</em>
                        </td>
                    </tr>
                    <tr>
                        <th>Superconnector Deprecation</th>
                        <th>Prompting Cascades</th>
                    </tr>
                    <tr>
                        <td>
                            No matter how a large thesaurus is constructed, certain terms seem to be ubiquitous. This is partially author bias, partially natural language structure, and worse with repetitive LLMs. We down-rank ubiquitous words like “heritage” and “surname”—a low-key version of inverse-frequency normalization. Our graduated penalty system scores 59,112 terms with an inverse document frequency (IDF) variant that down-ranks common terms (1-18). Surprisingly, penalty correlates with conceptual breadth, not raw frequency: “heritage” (penalty 18) appears only 201 times, while “tourism” (penalty 14) appears 8,520 times.
                            <em>Examples: At one processing stage, 2 words get maximum penalty (18): “surname” and “heritage” • 46,445 words get minimal penalty (1) • “heritage” can connect to almost anything cultural, historical, or traditional</em>
                        </td>
                        <td>
                            Our multi-pass LLM workflow (listing senses → expanding culturally → reprocessing) introduces systematic preferences that affect both what gets included and how highly it ranks.
                            <em>Examples: Geographic diversity emphasized, so “dance” includes global forms equally • Cultural foods given comparable rankings to Western staples</em>
                        </td>
                    </tr>
                    <tr>
                        <th>Frequency ≠ Importance</th>
                        <th>Morphological and Similarity Filters</th>
                    </tr>
                    <tr>
                        <td>
                            Frequency is a useful but flawed proxy for word importance. It captures actual usage but creates artifacts: ‘pandas’ outranking ‘panda,’ ‘cheesecake’ outranking ‘cheesecakes,’ literary corpora overweighting ‘thee,’ technical terms underrepresented. Different corpora (books vs. screenplays) produce subtly different hierarchies, and none capture a word’s actual utility for learners or gameplay.
                            <em>Examples: “pandas” plural outranks “panda” • “thee” elevated by Shakespeare • “cheesecake” singular is more common than “cheesecakes” • Literary bias from <a href="https://books.google.com/ngrams/info">Google N-grams</a></em>.
                        </td>
                        <td>
                            Nobody wants word clouds full of plurals and variants. Our filter pushes ‘baguettes’ down 30 positions if ‘baguette’ already appears, ‘rolls’ down 23 if ‘roll’ exists (&gt;90% string similarity gets +12 penalty, plural/singular differences +17, reordered compounds +17). Length penalties also apply progressively.
                            <em>Examples: In “bagels” - “baguette” drops 30 positions because “baguettes” appears earlier • “roll” drops 23 positions when “rolls” is present • Singular forms consistently cascade downward when plurals exist</em>
                        </td>
                    </tr>
                </tbody></table>
            </div>

            <p>These biases shape which connections appear, how strongly they’re weighted, and where they rank in each word cloud. We’ve made deliberate choices to create a semantic network optimized for engaging gameplay—favoring conceptual diversity over raw frequency, meaningful connections over statistical noise. The Linguabase represents one coherent mapping of English’s semantic landscape, designed to reveal the surprising paths that connect all words.</p>

            <h2>How Network Properties Enable Gameplay</h2>

            <p>The mathematical properties of our semantic network create natural game parameters:</p>

            <div>
                <p><span>17</span>
                    <span>Word choices per hop</span>
                </p>
                <p><span>7</span>
                    <span>Maximum path length</span>
                </p>
                <p><span>3</span>
                    <span>Minimum puzzle distance</span>
                </p>
                <p><span>27</span>
                    <span>Genius solutions per puzzle</span>
                </p>
            </div>

            <p>We tested various difficulties and settled on 3-7 hops. Below 3 felt trivial; above 7, players gave up. The 3-hop puzzles naturally yield 27 solutions when we maintain 3 strong choices per step.</p>

            <p>For virtually any common word selected as a puzzle origin, there are ~370 million outward paths within 7 hops (about 10% less than the 17&amp;sup7;=410 million theoretical maximum due to natural graph loops). Within those paths, only 200k-1 million reach the target—a random success rate of 0.05-0.27%. Players succeed at much higher rates because they navigate semantically rather than randomly. Our puzzles are engineered to ensure at least 3 good choices per hop, creating exactly 27 optimal three-hop “Genius” solutions (3³ paths).</p>

            <div>
                <div>
                    <h4>Theoretical Maximum (if no word overlap)</h4>
                    <pre>3 hops: 17³ = 4,913 paths
4 hops: 17&amp;sup4; = 83,521 paths  
5 hops: 17&amp;sup5; = 1,419,857 paths
6 hops: 17&amp;sup6; = 24,137,569 paths
7 hops: 17&amp;sup7; = 410,338,673 paths</pre>
                </div>
                <div>
                    <h4>Measured Reality (with semantic overlap)</h4>
                    <pre>Total paths: ~370 million (90% of theoretical)
Winning paths: 200k-1 million
Beyond game limit: ~94% require 8+ hops</pre>
                </div>
            </div>

            <p>
                Curious how we transformed this linguistic database into a daily word game? Read <a href="https://www.inotherwords.app/making/">Making the Game</a> to discover how we found the perfect game mechanic and balanced the difficulty.
            </p>
        </div></div>
  </body>
</html>
