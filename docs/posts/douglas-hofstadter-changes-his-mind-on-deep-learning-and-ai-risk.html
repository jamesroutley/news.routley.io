<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai">Original</a>
    <h1>Douglas Hofstadter changes his mind on Deep Learning and AI risk</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><div><p id="block0"><a href="https://www.youtube.com/watch?v=lfXxzAVtdpU&amp;t=1763s">A podcast interview</a> (posted 2023-06-29) with noted AI researcher Douglas Hofstadter discusses his career and current views on AI (via <a href="https://twitter.com/kmett/status/1675378198345834496">Edward Kmett</a>).</p>
<p id="block1">Hofstadter has <a href="https://www.economist.com/by-invitation/2022/06/09/artificial-neural-networks-today-are-not-conscious-according-to-douglas-hofstadter">previously</a> <a href="https://www.theatlantic.com/ideas/archive/2023/06/generative-artificial-intelligence-universities/674473/">energetically</a> criticized GPT-2/3 models (and <a href="https://qz.com/1088714/qa-douglas-hofstadter-on-why-ai-is-far-from-intelligent">deep</a> <a href="https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/">learning</a> and <a href="https://www.theatlantic.com/magazine/archive/2013/11/the-man-who-would-teach-machines-to-think/309529/">compute-heavy GOFAI</a>).
These criticisms were widely circulated &amp; cited, and apparently many people found Hofstadter a convincing &amp; trustworthy authority when he was negative on deep learning capabilities &amp; prospects, and so I found his most-recent comments (which amplify things he has been saying in private since <a href="https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai?commentId=JkD8BvTxLp6BivdhT">at least 2014</a>  of considerable interest.</p>
<p id="block2">Below I excerpt from the second half where he discusses DL progress &amp; AI risk:</p>
<blockquote id="block3">
<ul>
<li id="block4">
<ul>
<li id="block5">
<p id="block6"><strong>Q</strong>: ...Which ideas from
<a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach"><em>GEB</em></a> are most relevant today?</p>
</li>
<li id="block7">
<p id="block8"><a href="https://en.wikipedia.org/wiki/Douglas_Hofstadter"><strong>Douglas
Hofstadter</strong></a>: ...In my book, <a href="https://en.wikipedia.org/wiki/I_Am_a_Strange_Loop"><em>I Am a
Strange
Loop</em></a>, I tried to set forth what
it is that really makes a self or a soul. I like to use the word
&#34;soul&#34;, not in the religious sense, but as a synonym for &#34;I&#34;, a
human &#34;I&#34;, capital letter &#34;I.&#34; So, what is it that makes a human
being able to validly say &#34;I&#34;? What justifies the use of that
word? When can a computer say &#34;I&#34; and we feel that there is a
genuine &#34;I&#34; behind the scenes?</p>
<p id="block9">I don&#39;t mean like when you call up the drugstore and the
chatbot, or whatever you want to call it, on the phone says,
&#34;Tell me what you want. I know you want to talk to a human
being, but first, in a few words, tell me what you want. I can
understand full sentences.&#34; And then you say something and it
says, &#34;Do you want to refill a prescription?&#34; And then when I
say yes, it says, &#34;Gotcha&#34;, meaning &#34;I got you.&#34; So it acts as
if there is an &#34;I&#34; there, but I don&#39;t have any sense whatsoever
that there is an &#34;I&#34; there. It doesn&#39;t feel like an &#34;I&#34; to me,
it feels like a very mechanical process.</p>
<p id="block10">But in the case of more advanced things like
<a href="https://openai.com/blog/chatgpt/">ChatGPT</a>-3 or
<a href="https://openai.com/research/gpt-4">GPT-4</a>, it feels like there
is something more there that merits the word &#34;I.&#34; The question
is, when will we feel that those things actually deserve to be
thought of as being full-fledged, or at least partly fledged,
&#34;I&#34;s?</p>
<p id="block11">I personally worry that this is happening right now. But it&#39;s
not only happening right now. It&#39;s not just that certain things
that are coming about are similar to human consciousness or
human selves. They are also very different, and in one way, it
is extremely frightening to me. They are extraordinarily much
more knowledgeable and they are extraordinarily much faster. So
that if I were to take an hour in doing something, the ChatGPT-4
might take one second, maybe not even a second, to do exactly
the same thing.</p>
<p id="block12">And that suggests that these entities, whatever you want to
think of them, are going to be very soon, right now they still
make so many mistakes that we can&#39;t call them more intelligent
than us, but very soon they&#39;re going to be, they may very well
be more intelligent than us and far more intelligent than us.
And at that point, we will be receding into the background in
some sense. We will have handed the baton over to our
successors, for better or for worse.</p>
<p id="block13">And I can understand that if this were to happen over a long
period of time, like hundreds of years, that might be okay. But
it&#39;s happening over a period of a few years. It&#39;s like a tidal
wave that is washing over us at unprecedented and unimagined
speeds. And to me, it&#39;s quite terrifying because it suggests
that everything that I used to believe was the case is being
overturned.</p>
</li>
</ul>
</li>
<li id="block14">
<ul>
<li id="block15">
<p id="block16"><strong>Q</strong>: What are some things specifically that terrify you? What
are some issues that you&#39;re really...</p>
</li>
<li id="block17">
<p id="block18"><strong>D. Hofstadter</strong>: When I started out studying cognitive science
and thinking about the mind and computation, you know, this was
many years ago, around 1960, and I knew how computers worked and
I knew how extraordinarily rigid they were. You made the
slightest typing error and it completely ruined your program.
Debugging was a very difficult art and you might have to run
your program many times in order to just get the bugs out. And
then when it ran, it would be very rigid and it might not do
exactly what you wanted it to do because you hadn&#39;t told it
exactly what you wanted to do correctly, and you had to change
your program, and on and on.</p>
<p id="block19">Computers were very rigid and I grew up with a certain feeling
about what computers can or cannot do. And I thought that
artificial intelligence, when I heard about it, was a very
fascinating goal, which is to make rigid systems act fluid. But
to me, that was a very long, remote goal. It seemed infinitely
far away. It felt as if artificial intelligence was the art of
trying to make very rigid systems behave as if they were fluid.
And I felt that would take enormous amounts of time. I felt it
would be hundreds of years before anything even remotely like a
human mind would be asymptotically approaching the level of the
human mind, but from beneath.</p>
<p id="block20">I never imagined that computers would rival, let alone surpass,
human intelligence. And in principle, I thought they could rival
human intelligence. I didn&#39;t see any reason that they couldn&#39;t.
But it seemed to me like it was a goal that was so far away, I
wasn&#39;t worried about it. But when certain systems started
appearing, maybe 20 years ago, they gave me pause. And then this
started happening at an accelerating pace, where unreachable
goals and things that computers shouldn&#39;t be able to do started
toppling. The defeat of Gary Kasparov by Deep Blue, and then
going on to Go systems, Go programs, well, systems that could
defeat some of the best Go players in the world. And then
systems got better and better at translation between languages,
and then at producing intelligible responses to difficult
questions in natural language, and even writing poetry.</p>
<p id="block21">And my whole intellectual edifice, my system of beliefs... It&#39;s
a very traumatic experience when some of your most core beliefs
about the world start collapsing. And especially when you think
that human beings are soon going to be eclipsed. It felt as if
not only are my belief systems collapsing, but it feels as if
the entire human race is going to be eclipsed and left in the
dust soon. People ask me, &#34;What do you mean by &#39;soon&#39;?&#34; And I
don&#39;t know what I really mean. I don&#39;t have any way of knowing.
But some part of me says 5 years, some part of me says 20 years,
some part of me says, &#34;I don&#39;t know, I have no idea.&#34; But the
progress, the accelerating progress, has been so unexpected, so
completely caught me off guard, not only myself but many, many
people, that there is a certain kind of terror of an oncoming
tsunami that is going to catch all humanity off guard.</p>
<p id="block22">It&#39;s not clear whether that will mean the end of humanity in the
sense of the systems we&#39;ve created destroying us. It&#39;s not clear
if that&#39;s the case, but it&#39;s certainly conceivable. If not, it
also just renders humanity a very small phenomenon compared to
something else that is far more intelligent and will become
incomprehensible to us, as incomprehensible to us as we are to
cockroaches.</p>
</li>
<li id="block23">
<p id="block24"><strong>Q</strong>: That&#39;s an interesting thought. [nervous laughter]</p>
</li>
<li id="block25">
<p id="block26"><strong>Hofstadter</strong>: Well, I don&#39;t think it&#39;s interesting. I think
it&#39;s terrifying. I hate it. I think about it practically all the
time, every single day. [<strong>Q</strong>: Wow.] And it overwhelms me and depresses
me in a way that I haven&#39;t been depressed for a very long time.</p>
</li>
<li id="block27">
<p id="block28"><strong>Q</strong>: Wow, that&#39;s really intense. You have a unique
perspective, so knowing you feel that way is very powerful.</p>
</li>
</ul>
</li>
<li id="block29">
<ul>
<li id="block30">
<p id="block31"><strong>Q</strong>: How have LLMs, large language models, impacted your view
of how human thought and creativity works?</p>
</li>
<li id="block32">
<p id="block33"><strong>D H</strong>: Of course, it reinforces the idea that human creativity
and so forth come from the brain&#39;s hardware. There is nothing
else than the brain&#39;s hardware, which is neural nets. But one
thing that has completely surprised me is that these LLMs and
other systems like them are all feed-forward. It&#39;s like the
firing of the neurons is going only in one direction. And I
would never have thought that deep thinking could come out of a
network that only goes in one direction, out of firing neurons
in only one direction. And that doesn&#39;t make sense to me, but
that just shows that I&#39;m naive.</p>
<p id="block34">It also makes me feel that maybe the human mind is not so
mysterious and complex and impenetrably complex as I imagined it
was when I was writing <em>Gödel, Escher, Bach</em> and writing <em>I Am a
Strange Loop</em>. I felt at those times, quite a number of years
ago, that as I say, we were very far away from reaching anything
computational that could possibly rival us. It was getting more
fluid, but I didn&#39;t think it was going to happen, you know,
within a very short time.</p>
<p id="block35">And so it makes me feel diminished. It makes me feel, in some
sense, like a very imperfect, flawed structure compared with
these computational systems that have, you know, a million times
or a billion times more knowledge than I have and are a billion
times faster. It makes me feel extremely inferior. And I don&#39;t
want to say deserving of being eclipsed, but it almost feels
that way, as if we, all we humans, unbeknownst to us, are soon
going to be eclipsed, and rightly so, because we&#39;re so imperfect
and so fallible. We forget things all the time, we confuse
things all the time, we contradict ourselves all the time. You
know, it may very well be that that just shows how limited we
are.</p>
</li>
</ul>
</li>
<li id="block36">
<ul>
<li id="block37">
<p id="block38"><strong>Q</strong>: Wow. So let me keep going through the questions. Is there
a time in our history as human beings when there was something
analogous that terrified a lot of smart people?</p>
</li>
<li id="block39">
<p id="block40"><strong>D H</strong>: Fire.</p>
</li>
<li id="block41">
<p id="block42"><strong>Q</strong>: You didn&#39;t even hesitate, did you? So what can we learn
from that?</p>
</li>
<li id="block43">
<p id="block44"><strong>D H</strong>: No, I don&#39;t know. Caution, but you know, we may have
already gone too far. We may have already set the forest on
fire. I mean, it seems to me that we&#39;ve already done that. I
don&#39;t think there&#39;s any way of going back.</p>
<p id="block45">When I saw an interview with <a href="https://en.wikipedia.org/wiki/Geoff_Hinton">Geoff
Hinton</a>, who was probably the most
central person in the development of all of these kinds of
systems, he said something striking. He said he might regret his
life&#39;s work. He said, &#34;Part of me regrets all of my life&#39;s
work.&#34; The interviewer then asked him how important these
developments are. &#34;Are they as important as the <a href="https://en.wikipedia.org/wiki/Industrial_Revolution">Industrial
Revolution</a>? Is there something
analogous in history that terrified people?&#34; Hinton thought for
a second and he said, &#34;Well, maybe as important as the wheel.&#34;</p>
</li>
</ul>
</li>
</ul>
</blockquote></div></div></div></div><p><span><div><div><div><div><div id="MqibdzYojXW5gBxKN"><div><div><div><div><div><div><div><blockquote>
<p>But one thing that has completely surprised me is that these LLMs and other systems like them are all feed-forward. It&#39;s like the firing of the neurons is going only in one direction. And I would never have thought that deep thinking could come out of a network that only goes in one direction, out of firing neurons in only one direction. And that doesn&#39;t make sense to me, but that just shows that I&#39;m naive.</p>
</blockquote>
<p>I felt exactly the same, until I had read this June 2020 paper: <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a>.</p>
<p>It turns out that using Transformers in the autoregressive mode (with output tokens being added back to the input by concatenating the previous input and the new output token, and sending the new versions of the input through the model again and again) results in them <em>emulating dynamics of recurrent neural networks</em>, and that clarifies things a lot...</p>
</div></div></div></div></div></div></div><div><div><div id="uEPNEtayDnh82QAi9"><div><div><div><div><div><div><div><p>Yeah, there&#39;s obviously SOME recursion there but it&#39;s still surprising that such a relatively low bandwidth recursion can still work so well. It&#39;s more akin to me writing down my thoughts and then rereading them to gather my ideas than the kind of loops I imagine our neurons might have.</p>
<p>That said, who knows, maybe the loops in our brain are superfluous, or only useful for learning feedback purposes, and so a neural network trained by an external system doesn&#39;t need them.</p>
</div></div></div></div></div></div></div><div><div><div id="6qou6bJxkup6DHjiN"><div><div><div><div><div><div><div><p>Pondering this particular recursion, I noticed that it looks like things change not too much from iteration to iteration of this autoregressive dynamics, because we just add one token each time.</p>
<p>The key property of those artificial recurrent architectures which successfully fight the vanishing gradient problem is that a single iteration of recurrence looks like <em>Identity + epsilon</em> (so, <em>X -&gt; X + deltaX</em> for a  small <em>deltaX</em> on each iteration, see, for example, this 2018 paper, <a href="https://arxiv.org/abs/1801.06105">Overcoming the vanishing gradient problem in plain recurrent networks</a> which explains how this is the case for LSTMs and such, and explains how to achieve this for plain recurrent networks; for a brief explanation see my review of the first version of this paper, <a href="https://anhinga.github.io/brandeis-mirror/recurrent-identity-networks.html">Understanding Recurrent Identity Networks</a>).</p>
<p>So, I strongly suspect that it is also the case for the recurrence which is happening in Transformers used in the autoregressive mode (because the input is changing mildly from iteration to iteration).</p>
<p>But I don&#39;t know to which extent this is also true for biological recurrent networks. On one hand, our perceptions seem to change smoothly with time, and that seems to be an argument for gradual change of the <em>X -&gt; X + deltaX</em> nature in the biological case as well. But we don&#39;t understand the biological case all that well...</p>
<hr/>
<p>I think recurrence is actually quite important for LLMs. Cf. Janus&#39; Simulator theory which is now relatively well developed (see e.g. the original <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">Simulators</a> or brief notes I took on the recent status of that theory <a href="https://github.com/anhinga/2022-notes/blob/main/Generative-autoregressive-models-are-similators/May-23-2023-status-update.md">May-23-2023-status-update</a>). The fact that this is an autoregressive simulation is playing the key role.</p>
<p>But we indeed don&#39;t know whether complexity of biological recurrences vs. relative simplicity of artificial recurrent networks matters much...</p>
</div></div></div></div></div></div></div><div><div><div id="DRptZti6cXnzA2a8e"><div><div><div><div><div><div><p>I&#39;d speculate that our perceptions just seem to change smoothly because we encode second-order (or even third-order) dynamics in our tokens. From what I layman-understand of consciousness, I&#39;d be surprised if it wasn&#39;t discrete.</p></div></div></div></div></div></div></div></div></div></div></div><div><div id="cr6JqCvoyM9ngxaEg"><div><div><div><div><div><div><p>It could also be that LLMs don&#39;t do it like we do it and simply offer a computationally sufficient platform.</p></div></div></div></div></div></div></div></div></div></div></div><div><div id="99qdTpzHfFJjAMHLX"><div><div><div><div><div><div><p>In what sense do they emulate these dynamics?</p></div></div></div></div></div></div></div></div></div></div></div><div><div id="jMauchMfjgGuSnrzH"><div><div><div><div><div><div><p>It is beautiful to see that many of our greatest minds are willing to Say Oops, even about their most famous works. It may not score that many winning-points, but it does restore quite a lot of dignity-points I think.</p></div></div></div></div></div></div></div></div><div><div id="4Rjbpmjx3YBQGRmmo"><div><div><div><div><div><div><div><blockquote>
<p>It&#39;s not clear whether that will mean the end of humanity in the sense of the systems we&#39;ve created destroying us. It&#39;s not clear if that&#39;s the case, but it&#39;s certainly conceivable. If not, it also just renders humanity a very small phenomenon compared to something else that is far more intelligent and will become incomprehensible to us, as incomprehensible to us as we are to cockroaches.</p>
</blockquote>
<blockquote>
<p>Q: That&#39;s an interesting thought. [nervous laughter]</p>
</blockquote>
<blockquote>
<p>Hofstadter: Well, I don&#39;t think it&#39;s interesting. I think it&#39;s terrifying. I hate it. I think about it practically all the time, every single day. [Q: Wow.] And it overwhelms me and depresses me in a way that I haven&#39;t been depressed for a very long time.</p>
</blockquote>
<p>I don&#39;t think I&#39;ve ever seen a better description of how I feel about the coming creation of artificial superintelligence. I find myself returning over and over again to that post by benkuhn about <a href="https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill#:~:text=Staring%20into%20the%20abyss%20means,breaking%20up%20with%20your%20partner.">&#34;Staring into the abyss as a core life skill&#34;</a> I think that is going to become a necessary core life skill for almost everyone in the coming years.</p>
<p>It has been morbidly gratifying to see more and more people develop the same feelings about AI as I have had for about a year now. Like validation in the worst possible way. I think if people actually understood what was coming there would be a near total call to ban improvements in this technology and only allow advancement under very strict conditions. But almost no one has really thought through the consequences of making a general purpose replacement for human beings.</p>
</div></div></div></div></div></div></div></div></div><div><div id="PzacuReZhtDwhWotm"><div><div><div><div><div><div><p>It has become slightly more plausible that Melanie Mitchell could come around. </p></div></div></div></div></div></div><div><div><div id="JkD8BvTxLp6BivdhT"><div><div><div><div><div><div><div><p>But only slightly. Hofstadter&#39;s doubts have been building for a long time in private, to an extent that his op-eds don&#39;t convey (compare his comments in OP to his comments <a href="https://www.theatlantic.com/ideas/archive/2023/06/generative-artificial-intelligence-universities/674473/">published in the Atlantic just a week before</a>! they are so drastically different I was wondering if this was some sort of bizarre deepfake prank, but some cursory searching made it seemed legit and no one like Mitchell was saying it was fake and the text <em>sounds</em> like Hofstadter). On Twitter, <a href="https://twitter.com/JonTeets005/status/1675824214161477633">John Teets</a> helpfully notes that Mitchell has a 2019 book <a href="https://melaniemitchell.me/aibook/"><em>Artificial Intelligence: A Guide for Thinking Humans</em></a> where she records some private Hofstadter material I was unfamiliar with:</p>
<blockquote>
<p><strong>Prologue: Terrified</strong> ...The meeting, in May 2014, had been organized by <a href="https://en.wikipedia.org/wiki/Blaise_Ag%C3%BCera_y_Arcas">Blaise Agüera y Arcas</a>, a young computer scientist who had recently left a top position at Microsoft to help lead Google’s machine intelligence effort...The meeting was happening so that a group of select Google AI researchers could hear from and converse with <a href="https://en.wikipedia.org/wiki/Douglas_Hofstadter">Douglas Hofstadter</a>, a legend in AI and the author of a famous book cryptically titled <em>Gödel, Escher, Bach: an Eternal Golden Braid</em>, or more succinctly, GEB (pronounced “gee-ee-bee”). If you’re a computer scientist, or a computer enthusiast, it’s likely you’ve heard of it, or read it, or tried to read it...<em>Chess and the First Seed of Doubt</em>: The group in the hard-to-locate conference room consisted of about 20 Google engineers (plus Douglas Hofstadter and myself), all of whom were members of various Google AI teams. The meeting started with the usual going around the room and having people introduce themselves. Several noted that their own careers in AI had been spurred by reading GEB at a young age. They were all excited and curious to hear what the legendary Hofstadter would say about AI.</p>
<p>Then Hofstadter got up to speak. “I have some remarks about AI research in general, and here at Google in particular.” His voice became passionate. “I am terrified. Terrified.”</p>
<p>Hofstadter went on. [2. In the following sections, quotations from Douglas Hofstadter are from a follow-up interview I did with him after the Google meeting; the quotations accurately capture the content and tone of his remarks to the Google group.] He described how, when he first started working on AI in the 1970s, it was an exciting prospect but seemed so far from being realized that there was no “danger on the horizon, no sense of it actually <em>happening</em>.” Creating machines with human-like intelligence was a profound intellectual adventure, a long-term research project whose fruition, it had been said, lay at least <a href="http://giancarlorota.org/hotair/light.html">“one hundred Nobel prizes away.”</a> [Jack Schwartz, quoted in G.-C. Rota, <em>Indiscrete Thoughts</em> (Boston: Berkhäuser, 1997), pg22.] Hofstadter believed AI was possible in principle: “The ‘enemy’ were people like John Searle, Hubert Dreyfus, and other skeptics, who were saying it was impossible. They did not understand that a brain is a hunk of matter that obeys physical law and the computer can simulate anything … the level of neurons, neurotransmitters, et cetera. In theory, it can be done.” Indeed, Hofstadter’s ideas about simulating intelligence at various levels---from neurons to consciousness---were discussed at length in GEB and had been the focus of his own research for decades. But in practice, until recently, it seemed to Hofstadter that general “human-level” AI had no chance of occurring in his (or even his children’s) lifetime, so he didn’t worry much about it.</p>
<p>Near the end of GEB, Hofstadter had listed “10 Questions and Speculations” about artificial intelligence. Here’s one of them: “Will there be chess programs that can beat anyone?” Hofstadter’s speculation was “no.” “There may be programs which can beat anyone at chess, but they will not be exclusively chess players. They will be programs of <em>general</em> intelligence.”&lt;sup&gt;4&lt;/sup&gt;</p>
<p>At the Google meeting in 2014, Hofstadter admitted that he had been “dead wrong.” The rapid improvement in chess programs in the 1980s and ’90s had sown the first seed of doubt in his appraisal of AI’s short-term prospects. Although the AI pioneer Herbert Simon] had predicted in 1957 that a chess program would be world champion “within 10 years”, by the mid-1970s, when Hofstadter was writing GEB, the best computer chess programs played only at the level of a good (but not great) amateur. Hofstadter had befriended <a href="https://en.wikipedia.org/wiki/Eliot_S._Hearst">Eliot Hearst</a>, a chess champion and psychology professor who had written extensively on how human chess experts differ from computer chess programs. Experiments showed that expert human players rely on quick recognition of patterns on the chessboard to decide on a move rather than the extensive brute-force look-ahead search that all chess programs use. During a game, the best human players can perceive a configuration of pieces as a particular “kind of position” that requires a certain “kind of strategy.” That is, these players can quickly recognize particular configurations and strategies as instances of higher-level concepts. Hearst argued that without such a general ability to perceive patterns and recognize abstract concepts, chess programs would never reach the level of the best humans. Hofstadter was persuaded by Hearst’s arguments.</p>
<p>However, in the 1980s and ’90s, computer chess saw a big jump in improvement, mostly due to the steep increase in computer speed. The best programs still played in a very unhuman way: performing extensive look-ahead to decide on the next move. By the mid-1990s, IBM’s Deep Blue machine, with specialized hardware for playing chess, had reached the Grandmaster level, and in 1997 the program defeated the reigning world chess champion, Garry Kasparov, in a 6-game match. Chess mastery, once seen as a pinnacle of human intelligence, had succumbed to a brute-force approach.</p>
<p><em>Music: The Bastion of Humanity</em>... Hofstadter had been wrong about chess, but he still stood by the other speculations in GEB...Hofstadter described this speculation as “one of the most important parts of GEB---I would have staked my life on it.”</p>
<blockquote>
<p>I sat down at my piano and I played one of <a href="https://en.wikipedia.org/wiki/Emily_Howell">EMI’s</a> mazurkas “in the style of Chopin.” It didn’t sound exactly like Chopin, but it sounded enough like Chopin, and like coherent music, that I just felt <em>deeply</em> troubled.</p>
</blockquote>
<p>Hofstadter then recounted a lecture he gave at the prestigious Eastman School of Music, in Rochester, New York. After describing EMI, Hofstadter had asked the Eastman audience---including several music theory and composition faculty---to guess which of two pieces a pianist played for them was a (little-known) mazurka by Chopin and which had been composed by EMI. As one audience member described later, “The first mazurka had grace and charm, but not ‘true-Chopin’ degrees of invention and large-scale fluidity … The second was clearly the genuine Chopin, with a lyrical melody; large-scale, graceful chromatic modulations; and a natural, balanced form.” [ 6.  Quoted in D. R. Hofstadter, <a href="https://www.cogsci.indiana.edu/pub/drh-emi.pdf">“Staring Emmy Straight in the Eye—and Doing My Best Not to Flinch,”</a> in <em>Creativity, Cognition, and Knowledge</em>, ed. T. Dartnell (Westport, Conn.: Praeger, 2002), 67–100.] Many of the faculty agreed and, to Hofstadter’s shock, voted EMI for the first piece and “real-Chopin” for the second piece. The correct answers were the reverse.</p>
<p>In the Google conference room, Hofstadter paused, peering into our faces. No one said a word. At last he went on. “I was terrified by EMI. Terrified. I hated it, and was extremely threatened by it. It was threatening to destroy what I most cherished about humanity. I think EMI was the most quintessential example of the fears that I have about artificial intelligence.”</p>
<p><em>Google and the Singularity</em>: Hofstadter then spoke of his deep ambivalence about what Google itself was trying to accomplish in AI---self-driving cars, speech recognition, natural-language understanding, translation between languages, computer-generated art, music composition, and more. Hofstadter’s worries were underlined by Google’s embrace of Ray Kurzweil and his vision of the Singularity, in which AI, empowered by its ability to improve itself and learn on its own, will quickly reach, and then exceed, human-level intelligence. Google, it seemed, was doing everything it could to accelerate that vision. While Hofstadter strongly doubted the premise of the Singularity, he admitted that Kurzweil’s predictions still disturbed him. “I was terrified by the scenarios. Very skeptical, but at the same time, I thought, maybe their timescale is off, but maybe they’re right. We’ll be completely caught off guard. We’ll think nothing is happening and all of a sudden, before we know it, computers will be smarter than us.” If this actually happens, “we will be superseded. We will be relics. We will be left in the dust. Maybe this is going to happen, but I don’t want it to happen soon. I don’t want my children to be left in the dust.”</p>
<p>Hofstadter ended his talk with a direct reference to the very Google engineers in that room, all listening intently: “I find it very scary, very troubling, very sad, and I find it terrible, horrifying, bizarre, baffling, bewildering, that people are rushing ahead blindly and deliriously in creating these things.”</p>
<p><em>Why Is Hofstadter Terrified?</em> I looked around the room. The audience appeared mystified, embarrassed even. To these Google AI researchers, none of this was the least bit terrifying. In fact, it was old news...Hofstadter’s terror was in response to something entirely different. It was not about AI becoming too smart, too invasive, too malicious, or even too useful. Instead, he was terrified that intelligence, creativity, emotions, and maybe even consciousness itself would be too easy to produce---that what he valued most in humanity would end up being nothing more than a “bag of tricks”, that a superficial set of brute-force algorithms could explain the human spirit.</p>
<p>As GEB made abundantly clear, Hofstadter firmly believes that the mind and all its characteristics emerge wholly from the physical substrate of the brain and the rest of the body, along with the body’s interaction with the physical world. There is nothing immaterial or incorporeal lurking there. The issue that worries him is really one of complexity. He fears that AI might show us that the human qualities we most value are disappointingly simple to mechanize. As Hofstadter explained to me after the meeting, here referring to Chopin, Bach, and other paragons of humanity, “If such minds of infinite subtlety and complexity and emotional depth could be trivialized by a small chip, it would destroy my sense of what humanity is about.”</p>
<p>...Several of the Google researchers predicted that general human-level AI would likely emerge within the next 30 years, in large part due to Google’s own advances on the brain-inspired method of “deep learning.”</p>
<p>I left the meeting scratching my head in confusion. I knew that Hofstadter had been troubled by some of Kurzweil’s Singularity writings, but I had never before appreciated the degree of his emotion and anxiety. I also had known that Google was pushing hard on AI research, but I was startled by the optimism several people there expressed about how soon AI would reach a general “human” level. My own view had been that AI had progressed a lot in some narrow areas but was still nowhere close to having the broad, general intelligence of humans, and it would not get there in a century, let alone 30 years. And I had thought that people who believed otherwise were vastly underestimating the complexity of human intelligence. I had read Kurzweil’s books and had found them largely ridiculous. However, listening to all the comments at the meeting, from people I respected and admired, forced me to critically examine my own views. While assuming that these AI researchers underestimated humans, had I in turn underestimated the power and promise of current-day AI?</p>
<p>...Other prominent thinkers were pushing back. Yes, they said, we should make sure that AI programs are safe and don’t risk harming humans, but any reports of near-term superhuman AI are greatly exaggerated. The entrepreneur and activist Mitchell Kapor advised, <a href="https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory">“Human intelligence is a marvelous, subtle, and poorly understood phenomenon. There is no danger of duplicating it anytime soon.”</a> The roboticist (and former director of MIT’s AI Lab) Rodney Brooks agreed, stating that we <a href="https://www.edge.org/response-detail/26057">“grossly overestimate the capabilities of machines---those of today and of the next few decades.”</a> The psychologist and AI researcher Gary Marcus went so far as to assert that in the quest to create “strong AI”---that is, <em>general</em> human-level AI---<a href="https://www.forbes.com/sites/gilpress/2016/10/31/12-observations-about-artificial-intelligence-from-the-oreilly-ai-conference/">“there has been almost no progress.”</a></p>
<p>I could go on and on with dueling quotations. In short, what I found is that the field of AI is in turmoil. Either a huge amount of progress has been made, or almost none at all. Either we are within spitting distance of “true” AI, or it is centuries away. AI will solve all our problems, put us all out of a job, destroy the human race, or cheapen our humanity. It’s either a noble quest or “summoning the demon.”</p>
</blockquote>
<p>That is, whatever the snarky <a href="https://gwern.net/doc/existential-risk/1940-sciam-harrington-nuclearweapons-dontworryitcanthappen.pdf">&#34;don&#39;t worry, it can&#39;t happen&#34;</a> tone of his public writings about DL has been since ~2010, Hofstadter has been saying these things in private for at least a decade*, starting somewhere around Deep Blue which clearly falsified a major prediction of his, and his worries about the scaling paradigm intensifying ever since; what has happened is that only one of two paradigms can be true, and Hofstadter has finally flipped to the other paradigm.
Mitchell, however, has heard all of this firsthand long before this podcast and appears to be completely immune to Hofstadter&#39;s concerns (publicly), so I wouldn&#39;t expect it to change her mind.</p>
<p>* I wonder what other experts &amp; elites have different views on AI than their public statements would lead you to believe?</p>
</div></div></div></div></div></div></div></div></div></div></div></div><div><div id="zjeBF3fQx2DdgrbLy"><div><div><div><div><div><div><div><p>At the time of Hofstadter&#39;s<a href="https://www.youtube.com/watch?v=LyrmwNN5RdY"> Singularity Summit talk </a>, I wondered why he wasn&#39;t &#34;getting with the program&#34;, and it became clear he was a mysterian:  He believed -- without being a dualist --  that some things, like the mind, are ultimately, basically, essentially, impossible to understand or describe.</p><p>This 2023 interview shows that the new generation of AI has done more than chagne his mind about the potential of AI: it has struck at the core of his mysterianism</p><blockquote><p>the human mind is not so mysterious and complex and impenetrably complex as I imagined it was when I was writing <i>Gödel, Escher, Bach</i> and writing <i>I Am a Strange Loop</i>.</p></blockquote></div></div></div></div></div></div></div><div><div><div id="hRLh4iTmnC3eip3oR"><div><div><div><div><div><div><p>He was only a de facto mysterian: thought mind is so complicated that it may as well be mysterious (but ofc he believed it&#39;s ultimately just physics). This position is updateable, and he clearly updated.</p></div></div></div></div></div></div></div></div></div></div></div><div><div id="p9RcZfbvbLqiRbbgg"><div><div><div><div><div><div><div><blockquote><p>But one thing that has completely surprised me is that these LLMs and other systems like them are all feed-forward. It&#39;s like the firing of the neurons is going only in one direction. And I would never have thought that deep thinking could come out of a network that only goes in one direction, out of firing neurons in only one direction. And that doesn&#39;t make sense to me, but that just shows that I&#39;m naive.</p></blockquote><p>What was the argument that being feed-forward limited the potential for deep thought <i>in principle</i>? It makes sense that multi-directional nets could do more with fewer neurons but Hofstader seemed to think there were things that feed-forward system fundamentally couldn&#39;t do. </p></div></div></div></div></div></div></div></div></div><div><div id="sKDPq6EMCQNFnoMTf"><div><div><div><div><div><div><div><blockquote><p>It&#39;s not clear whether that will mean the end of humanity in the sense of the systems we&#39;ve created destroying us. It&#39;s not clear if that&#39;s the case, but it&#39;s certainly conceivable. If not, it also just renders humanity a very small phenomenon compared to something else that is far more intelligent and will become incomprehensible to us, as incomprehensible to us as we are to cockroaches.</p></blockquote><p>It&#39;s interesting that he seems so in despair over this now. To the extent that he&#39;s worried about existential/catastrophic risks, I wonder if he is unaware of efforts to mitigate those, or if he is aware but thinks they are hopeless (or at least not guaranteed to succeed, which -- fair enough). To the extent that he&#39;s more broadly worried about human obsolescence (or anyway something more metaphysical), well, there are people trying to slow/stop AI, and others trying to enhance human capabilities -- maybe he&#39;s pessimistic about those efforts, too.</p></div></div></div></div></div></div></div></div></div></div></div></div></span></p></div></div>
  </body>
</html>
