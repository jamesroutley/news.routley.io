<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tratt.net/laurie/blog/2023/why_split_lexing_and_parsing_into_two_separate_phases.html">Original</a>
    <h1>Why Split Lexing and Parsing into Two Separate Phases?</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body"><p>



&#34;lex and yacc&#34; are two terms that are used together so often that they can seem
indivisible. <code>lex</code> takes a string as input and divides it up
into lexemes (roughly speaking &#34;words&#34;) and <code>yacc</code> takes a sequence
of lexemes and parses them.  Since some parsing approaches such as
recursive descent parsing unify these phases, why do <code>lex</code> and <code>yacc</code>
split them apart?


</p><h2>Running example</h2>
<p>Let&#39;s start with a simple language, which allows us to assign simple
expressions to variables:

</p><pre>x = 1;
y = x;
</pre>
<p>Using <code>lrlex</code> from the Rust <a href="https://github.com/softdevteam/grmtools/">grmtools</a> libraries, I can
define a lexer file  for this fragment as follows:

</p><pre>%%
[a-z]+ &#34;ID&#34;
[0-9]+ &#34;INT&#34;
= &#34;=&#34;
; &#34;;&#34;
[ \t\n\r]+ ;
</pre><p>

If there were any declarations (i.e. settings) they would come before the
&#34;<code>%%</code>&#34; line: there are none in this example. Each line after the
&#34;<code>%%</code>&#34; defines a lexing rule. Lexing rules consist of a regular
expression and either an identifier or &#34;;&#34;. For example, the rule <code>[0-9]+
&#34;INT&#34;</code> says that every non-empty sequence of lower-case digits defines an
integer, assigning it the token type &#34;<code>INT</code>&#34;. If a lexing rule has
&#34;;&#34; instead of an identifier, any text matching the regular expression is
discarded: as is common, in this example the lexer simply discards whitespace
(spaces, tabs, and newlines).

</p><p>Helpfully, we can run <code>lrlex</code> on our lex file and input to see
what lexemes it produces:

</p><pre>$ lrlex /tmp/t.l /tmp/t
ID x
= =
INT 1
; ;
ID y
= =
ID x
; ;
</pre><p>

Each line tells us the token type and the lexeme (i.e. the text matched).

</p><p>We can now define a simple yacc grammar which allows us to write
files with multiple &#34;statements&#34; (i.e. variable definitions or assignments):

</p><pre>%%
stmts: stmts stmt | stmt ;
stmt: &#34;ID&#34; &#34;=&#34; expr &#34;;&#34; ;
expr: &#34;INT&#34; | &#34;ID&#34; ;
</pre><p>

If you&#39;re unfamiliar with yacc grammars, the <code>stmts</code> rules is a long-winded
way of writing the more regex-like &#34;<code>stmts: stmt+</code>&#34; i.e. that a valid input is one or more
statements.

</p><p>We can can then use grmtools&#39; <code>nimbleparse</code> to parse
our input relative to our lexer and grammar:

</p><pre>$ nimbleparse /tmp/t.l /tmp/t.y /tmp/t
stmts
 stmts
  stmt
   ID x
   = =
   expr
    INT 1
   ; ;
 stmt
  ID y
  = =
  expr
   ID x
  ; ;
</pre><p>

The indentation in <code>nimbleparse</code> shows us the tree structure: if
you&#39;re not sure what you&#39;re looking at, you might find it helpfully to mentally
rotate the tree 90° clockwise.


</p><h2>Overlapping lexing rules</h2>
<p>Let&#39;s now assume that I want to introduce mathematical constants into our
language, specifically the constant &#34;pi&#34;:

</p><pre>z = pi;
</pre><p>

Since <code>pi</code> is a constant, I want to forbid people from
assigning to it. In other words I want this expression to cause an error:

</p><pre>pi = 3;
</pre><p>

I could do this in a &#34;type checking&#34; phase after parsing, but there are good
reasons why I might want to make my parser reject such inputs, including: I&#39;m
lazy and don&#39;t want to add a type checking phase; it&#39;s easier and faster for
external tools (e.g. your IDE) to spot incorrect input if it&#39;s encoded
in the parser; and it&#39;s generally easier for users to understand valid inputs
from a grammar than it is from a description of type rules.

</p><p>Let&#39;s give it a go by changing our lexer to explicitly identify pi:

</p><pre>%%
[a-z]+ &#34;ID&#34;
[0-9]+ &#34;INT&#34;
= &#34;=&#34;
; &#34;;&#34;
[ \t\n\r]+ ;
pi &#34;PI&#34;
</pre><p>

and our grammar to only allow pi in the right hand side of an expression:

</p><pre>%%
stmts: stmts stmt | stmt ;
stmt: &#34;ID&#34; &#34;=&#34; expr &#34;;&#34; ;
expr: &#34;INT&#34; | &#34;ID&#34; | &#34;PI&#34; ;
</pre><p>

Unfortunately when I run it on the input I expected to be rejected, it is parsed without error:

</p><pre>$ nimbleparse /tmp/t.l /tmp/t.y /tmp/t
stmts
 stmt
  ID pi
  = =
  expr
   INT 3
  ; ;
</pre><p>

Clearly I&#39;ve done <em>something</em> wrong, but what?

</p><p>From experience, when a yacc grammar doesn&#39;t do what&#39;s expected, a lot of
people immediately start trying to understand the LR parsing algorithm.
I&#39;ve surprised 3 people in the last month by saying, quite truthfully, that I
no longer remember how LR parsing works. I&#39;m sure I could refresh my memory
fairly quickly if I really needed to, but I have not found my forgetfulness to
be a problem because fixing these sorts of problems rarely
requires understanding the parsing algorithm.

</p><p>In this case, it turns out that I don&#39;t need to understand anything about
LR parsing, because the problem is in the lexer! We can see that by running
<code>lrlex</code> on our input:

</p><pre>$ lrlex /tmp/t.l /tmp/t
ID pi
= =
INT 3
; ;
</pre><p>

This clearly shows that <code>pi</code> has the token type <code>ID</code>
rather than the expected token type <code>PI</code>.

</p><p>At this point, it&#39;s clear that we need to know at least a little about how
<code>lex</code> (and <code>lrlex</code>) lex inputs. Fortunately,
the rules are surprisingly simple. Summarising briefly:

</p><ul>
<li>At each point in the input, <code>lex</code> tries <em>every</em> rule in
the lex file — it does not stop at the first match.
</li><li>If multiple rules match, the &#34;winner&#34; is decided in two steps:
  <ol>
<li>The rule(s) that match the most number of characters &#34;win&#34;.
  </li><li>If multiple rules match the same (most!) number of characters, the
  earliest rule in the lex file &#34;wins&#34;.
  </li></ol>
</li></ul>
<p>In our case, the regular expressions &#34;<code>[a-z]+</code>&#34; and
&#34;<code>pi</code>&#34; both matched the input &#34;<code>pi</code>&#34; but since
&#34;<code>[a-z]+</code>&#34; came earlier in the lex file, it &#34;won&#34;. As this shows
– and contrary to what people often suspect is the case – rules in a lex
file are ordered, and that order is important.

</p><p>The fix in our case is thus easy — we just need to put the lexing
rule for pi earlier than the rule for ID:

</p><pre>%%
pi &#34;PI&#34;
[a-z]+ &#34;ID&#34;
[0-9]+ &#34;INT&#34;
= &#34;=&#34;
; &#34;;&#34;
[ \t\n\r]+ ;
</pre><p>

We can then check that our input is lexed as we expect, and that the incorrect
input is rejected:

</p><pre>$ lrlex /tmp/t.l /tmp/t
ID pi
= =
INT 3
; ;
$ nimbleparse /tmp/t.l /tmp/t.y /tmp/t
stmts
 stmt
  ID 
  = =
  expr
   INT 3
  ; ;

Parsing error at line 1 column 1. Repair sequences found:
   1: Insert ID, Delete pi
</pre><p>

Since I&#39;ve left grmtools&#39; error recovery on, it has both recognised &#34;<code>pi =
...</code>&#34; as incorrect and realised that we should have written &#34;<code>&lt;id&gt; =
...</code>&#34; .

</p><p>Simple though this example is, it shows us one reason why it is
useful to separate lexing from parsing. To see that, let&#39;s imagine a tool called <code>lecc</code>
which merges together <code>lex</code> and <code>yacc</code>. If we take the
first lexer and grammar from this post and adapt it for <code>lecc</code>
we might write something like:

</p><pre>%%
stmts: stmts stmt | stmt ;
stmt: &#34;[a-z]+&#34; &#34;=&#34; expr &#34;;&#34; ;
expr: &#34;[0-9]+&#34; | &#34;[a-z]+&#34; ;
</pre><p>

Because of the nature of this combined lexer/grammar, it&#39;s fairly easy to
see how <code>lecc</code> would be able to lex/parse it in a way that&#39;s
equivalent to our separate lexer and grammars from earlier.

</p><p>However, what happens if I want to write a
<code>lecc</code> file for the lexer and grammar which forbids me assigning
to &#34;<code>pi</code>&#34;? I now need a regular expression which matches every
non-empty sequence of lower-case ASCII characters except the string
&#34;<code>pi</code>&#34;. Since <code>lex</code> doesn&#39;t have things like negative
lookahead assertions , I have to be quite
creative in how I write this:

</p><pre>%%
stmts: stmts stmt | stmt ;
stmt: &#34;[a-oq-z][a-z]*|p[a-hj-z][a-z]*|pi[a-z]+|p&#34; &#34;=&#34; expr &#34;;&#34; ;
expr: &#34;[0-9]+&#34; | &#34;[a-z]+&#34; ;
</pre><p>

Have I got the big regular expression right? I think I might have, but even
after I&#39;ve lightly tested it, I&#39;m not entirely confident. Imagine if I also
wanted to handle other constants such as &#34;<code>e</code>&#34; or &#34;<code>G</code>&#34;
and so on — I would very quickly exhaust my ability to write such a
regular expression, even though it&#39;s always theoretically possible to do so.
If I were to generate the necessary regular expression automatically, it&#39;s
quite probable that the result would end up being so large and complex that
it would have poor performance.

</p><p>We thus see the first argument for separating lexing from parsing: it makes
it much easier for us to deal with &#34;overlapping&#34; lexing rules. Our first lexing
file only worked because at any point in the input at most one rule could
match. As soon as we introduced pi, we created overlap between two rules (ID
and pi) . Real languages have many instances of
overlap (keywords and identifiers being an obvious, but by no means the only,
example).


</p><h2>Scannerless parsing</h2><p>

What my imaginary version of <code>lecc</code> is doing is really a form of <a href="https://en.wikipedia.org/wiki/Scannerless_parsing">scannerless
parsing</a>. Astute readers might have realised that by combining a lexer and
parser, I&#39;ve accidentally introduced ambiguity problems into the seemingly
unambiguous LR formalism. We can see this if I translate the <code>expr</code>
from earlier as follows:

</p><pre>expr: &#34;[0-9]+&#34; | &#34;[a-z]+&#34; | &#34;pi&#34; ;
</pre><p>

What does the input &#34;pi&#34; match? Well, it matches the second and third productions in that rule!
In other words, we&#39;ve transferred what I called the &#34;overlapping&#34; problem from
the lexer to the parser.

</p><p>Fundamentally, the &#34;overlapping&#34; problem in lexing is a form of ambiguity
. We now see a second argument (which is in a sense a generalisation of the
first argument) for separating lexing and parsing: it allows us to push
the inherent ambiguity of most languages into the lexer, keeping the grammar
unambiguous.

</p><p>What happens, however, if we embrace ambiguity and push scannerless parsing
further? The modern form of scannerless parsing came into being thanks to the
late – and much missed by me and others – Eelco Visser  as part of his <a href="https://eelcovisser.org/publications/1997/Visser97.pdf">PhD thesis</a>.
Amongst other ideas he introduced the idea of &#34;reject&#34; rules which we might
represent as follows:

</p><pre>%%
stmts: stmts stmt | stmt ;
stmt: id_except_pi &#34;=&#34; expr &#34;;&#34; ;
expr: &#34;[0-9]+&#34; | &#34;[a-z]+&#34; ;
id_except_pi: &#34;[a-z]+&#34; ;
id_except_pi reject: &#34;pi&#34; ;
</pre><p>

In essence, the idea is to say that &#34;<code>id_except_pi</code> matches all
identifiers but then rejects those identifiers which match &#39;<code>pi</code>&#39;&#34;.
This achieves the same outcome as my fiercely complex regular expression from
earlier, and is much easier to use!

</p><p>However, there are problems. In essence, &#34;reject&#34; rules interfere with
the compositionality of context-free grammars, making them slightly context
sensitive. A <a href="https://citeseerx.ist.psu.edu/doc/10.1.1.56.883">partial fix</a>
was suggested for this problem some years ago but, to the best of my knowledge,
a full fix (or, at least, a full understanding of a fix) remains an open problem.
As this might suggest, combining lexing and parsing is much harder than it
may first seem.


</p><h2>Summary</h2><p>

The most obvious reason for separating lexing and parsing is that it
allows us to move some things that are difficult
to express in a parser into a lexer, where they are easy to express. More
fundamentally, it allows us to use unambiguous parsing algorithms for
languages that would seem to be ambiguous. This can seem a contradiction in terms, or
disingenuous: does forcing lexing to remove ambiguity sully supposedly
unambiguous parsing algorithms like LR? In my opinion, this is a fun
philosophical debating point, but isn&#39;t very relevant in practise: having
separate lexers works rather well.

</p><p>However, as with nearly everything in parsing, there are trade-offs. <code>lex</code>-style lexers
are not closed under composition. In other words, because <code>lex</code> files
have ordered rules, combining two <code>lex</code> files together can have
surprising results, with some rules never matching. This is a terrible
shame, because computing would be <em>much</em> better if we could
reliably compose languages. Scannerless parsing, as we saw above, is closed
under composition, but causes other trade-offs.

</p><p>There are two other major approaches to language composition. First, is to
have some way to switch between different lexers. This almost always involves
some form of syntactic marker: <a href="https://www-users.cse.umn.edu/~evw/pubs/kaminski17oopsla/kaminski17oopsla.pdf">Copper
and Silver</a> are probably the best examples of such a system. However, when I
was working on <a href="https://convergepl.org/">Converge</a>, which allowed
something similar and also required syntactic markers, I eventually concluded
that I was rarely willing to pay the cost of the syntactic noise it required.
Second, is some form of tree-based editing. Our group (well, mostly <a href="https://diekmann.uk/">Lukas</a>!)
developed the <a href="https://tratt.net/laurie/blog/2014/an_editor_for_composed_programs.html">Eco</a> editor to
explore this, and I think we&#39;ve shown that it can <a href="https://soft-dev.org/pubs/html/diekmann_tratt__default_disambiguation/">work well in some surprising
situations</a>, but it&#39;s still not magic. Parsing continues to be a frustrating
topic — not only are there no free lunches, but
sometimes there is no lunch available, no matter how deep your pockets!

</p>



<h3>Footnotes</h3>
<p><a name="35930368">[1] If you&#39;re familiar with <code>lex</code>, you might notice that
<code>lrlex</code> uses a slightly different format.</a></p></div></div>
  </body>
</html>
