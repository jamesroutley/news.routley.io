<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://elliotarledge.com/blog/batmobile">Original</a>
    <h1>Batmobile: 10-20x Faster CUDA Kernels for Equivariant Graph Neural Networks</h1>
    
    <div id="readability-page-1" class="page"><article><p>Custom CUDA kernels that eliminate the computational bottlenecks in spherical harmonics and tensor product operations - the core primitives of equivariant GNNs like MACE, NequIP, and Allegro.</p><p><img alt="Batmobile benchmark results showing 10-20x speedup over e3nn" loading="lazy" width="1000" height="600" decoding="async" data-nimg="1" src="https://elliotarledge.com/blog/batmobile/benchmark_hero.png"/></p><h2>The Problem: Equivariant GNNs Are Beautiful but Slow</h2><p>Equivariant graph neural networks have revolutionized atomistic machine learning. Models like MACE, NequIP, and Allegro achieve state-of-the-art accuracy in molecular dynamics simulations, materials property prediction, and drug discovery. Their secret: they respect the fundamental symmetries of physical systems - rotation, translation, and reflection invariance.</p><p>But this mathematical elegance comes at a computational cost. The operations that make these models work - spherical harmonics and Clebsch-Gordan tensor products - are expensive. A single MACE layer can spend 80% of its forward pass time in these two operations.</p><p><strong>This matters for real applications.</strong> Molecular dynamics simulations run billions of timesteps. Battery materials discovery screens millions of candidates. Drug binding affinity predictions evaluate thousands of poses. When each forward pass takes milliseconds instead of microseconds, these workflows become impractical.</p><h2>Understanding the Bottleneck</h2><p>To understand why equivariant GNNs are slow, we need to understand what they compute.</p><h3>Spherical Harmonics: Encoding Directions</h3><p>When two atoms interact, the direction of their bond matters. A carbon-carbon bond pointing &#34;up&#34; is physically different from one pointing &#34;right&#34; - and our neural network needs to know this.</p><p>Spherical harmonics (Y_lm) provide a mathematically principled way to encode 3D directions. Given a unit vector (x, y, z), spherical harmonics compute a set of features that transform predictably under rotation:</p><ul><li>L=0: 1 component (scalar, rotationally invariant)</li><li>L=1: 3 components (vector, transforms like a 3D vector)</li><li>L=2: 5 components (transforms like a symmetric traceless matrix)</li><li>L=3: 7 components (higher-order tensor)</li></ul><p>For L_max=3, we get 16 components total: 1 + 3 + 5 + 7 = 16. These aren&#39;t arbitrary features - they form a complete basis for functions on the sphere.</p><h3>Tensor Products: Combining Features</h3><p>When we want to combine two equivariant features (say, node features with edge directions), we can&#39;t just concatenate or add them - that would break equivariance.</p><p>Instead, we use Clebsch-Gordan tensor products. These are specific weighted sums that preserve the transformation properties:</p><pre><code>output[l_out, m_out] = sum_{m1, m2} CG[l1,m1,l2,m2,l_out,m_out] * input1[l1,m1] * input2[l2,m2]</code></pre><p>The Clebsch-Gordan coefficients (CG) are fixed mathematical constants that ensure the output transforms correctly. For L_max=3, there are 34 valid coupling paths.</p><h3>Why e3nn Is Slow</h3><p>The standard library for equivariant operations is e3nn. It&#39;s beautifully designed - clean abstractions, automatic equivariance checking, extensive documentation. But it&#39;s slow.</p><ul><li><strong>Python/PyTorch Overhead</strong>: Each spherical harmonic component is computed as a separate PyTorch operation. For 16 components, that&#39;s 16 kernel launches.</li><li><strong>Memory Bandwidth Waste</strong>: Intermediate results are written to global GPU memory and read back. The Y_lm tensor exists just to be immediately consumed.</li><li><strong>No Fusion</strong>: SH computation and tensor product are separate operations that could share data through registers.</li><li><strong>Dynamic Shapes</strong>: e3nn handles arbitrary irrep combinations at runtime, preventing compile-time optimizations.</li></ul><h2>The Solution: Batmobile</h2><p>Batmobile takes a different approach. Rather than flexible abstractions, it provides hand-tuned CUDA kernels for the specific operations used in production models.</p><h3>1. Compile-Time Constants</h3><p>For L_max=3, all Clebsch-Gordan coefficients and loop bounds are known at compile time. Batmobile bakes these into the kernels:</p><pre><code>// All 34 CG paths are explicitly unrolled
// Path (0,0)-&gt;0: trivial identity
out[0] += cg_0_0_0 * in1[0] * in2[0];

// Path (1,1)-&gt;0: scalar from two vectors
out[0] += cg_1_1_0_m1m1 * in1[1] * in2[1];
out[0] += cg_1_1_0_00 * in1[2] * in2[2];
out[0] += cg_1_1_0_p1p1 * in1[3] * in2[3];
// ... all 34 paths ...</code></pre><h3>2. Register-Only Intermediates</h3><p>Spherical harmonics are computed directly into GPU registers - never touching global memory:</p><pre><code>__device__ __forceinline__ void compute_sh_registers(
    float x, float y, float z,
    float* __restrict__ sh  // sh[16] in registers
) {
    // L=0
    sh[0] = 1.0f;

    // L=1: sqrt(3) * (x, y, z)
    constexpr float c1 = 1.7320508075688772f;
    sh[1] = c1 * x;
    sh[2] = c1 * y;
    sh[3] = c1 * z;

    // L=2, L=3: all computed in registers
    // ...
}</code></pre><h3>3. Fused Operations</h3><p>The ultimate optimization: compute spherical harmonics and tensor products in a single kernel pass. Input edge vectors go in, output features come out, with no intermediate global memory.</p><h2>Benchmark Results</h2><p>All benchmarks on RTX 3090, N=1000 atoms, 32 channels, ~20 neighbors per atom:</p><div><table><thead><tr><th>Operation</th><th>e3nn</th><th>Batmobile</th><th>Speedup</th></tr></thead><tbody><tr><td>Spherical Harmonics (L=3)</td><td>0.142 ms</td><td>0.012 ms</td><td>11.8x</td></tr><tr><td>Tensor Product</td><td>1.847 ms</td><td>0.089 ms</td><td>20.8x</td></tr><tr><td>TP Backward</td><td>3.21 ms</td><td>0.156 ms</td><td>20.6x</td></tr><tr><td>Fused SH+TP</td><td>0.574 ms</td><td>0.413 ms</td><td>1.39x</td></tr></tbody></table></div><p><img alt="Detailed benchmark comparison between e3nn and Batmobile" loading="lazy" width="1400" height="600" decoding="async" data-nimg="1" src="https://elliotarledge.com/blog/batmobile/benchmark_chart.png"/></p><h3>What These Numbers Mean</h3><p><strong>Spherical Harmonics: 11.8x faster</strong> - e3nn launches many small kernels and stores intermediate values. Batmobile computes all 16 components in a single fused kernel.</p><p><strong>Tensor Product: 20.8x faster</strong> - This is the biggest win. e3nn&#39;s general-purpose implementation handles arbitrary irrep combinations. Batmobile is specialized for L_max=3 with all 34 paths unrolled and CG coefficients as compile-time constants.</p><p><strong>Backward Pass: 20.6x faster</strong> - Training equivariant models requires gradients. Batmobile includes hand-optimized backward kernels that maintain the same speedup ratio.</p><h2>Usage</h2><pre><code>import torch
import batmobile

# Compute spherical harmonics
edge_vectors = torch.randn(1000, 3, device=&#34;cuda&#34;)
edge_vectors = edge_vectors / edge_vectors.norm(dim=1, keepdim=True)
Y_lm = batmobile.spherical_harmonics(edge_vectors, L_max=3)  # [1000, 16]

# Weighted tensor product
node_feats = torch.randn(1000, 32, 16, device=&#34;cuda&#34;)  # [N, C, 16]
weights = torch.randn(34, 32, 64, device=&#34;cuda&#34;)  # [paths, C_in, C_out]
output = batmobile.tensor_product(node_feats, Y_lm, weights)  # [N, 64, 16]</code></pre><h2>Why &#34;Batmobile&#34;?</h2><p>The project was originally called &#34;batteries&#34; - a play on &#34;batteries included&#34; for equivariant GNNs. The rename to Batmobile felt more fitting: a specialized, high-performance vehicle for a specific mission.</p><p>Like its namesake, Batmobile isn&#39;t trying to be a general-purpose car. It&#39;s optimized for one thing: making equivariant message passing fast enough for real-world molecular simulations.</p><h2>Get Started</h2><ul><li><a href="https://github.com/Infatoshi/batmobile">GitHub: github.com/Infatoshi/batmobile</a></li><li>Benchmarks: Run <code>python benchmarks/benchmark_e2e_mace.py</code> to reproduce</li><li>Examples: See <code>examples/mace_inference.py</code> for a minimal MACE-style layer</li></ul><p>January 2026</p><p>Special thanks to the e3nn team for their foundational work on equivariant neural networks, and to the MACE/NequIP/Allegro authors for demonstrating the power of equivariant architectures in atomistic ML.</p></article></div>
  </body>
</html>
