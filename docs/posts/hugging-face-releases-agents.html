<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/docs/transformers/transformers_agents">Original</a>
    <h1>Hugging Face Releases Agents</h1>
    
    <div id="readability-page-1" class="page"><div><!-- HTML_TAG_START -->	
	
	
	
	
	
	
	
	 










<p>Transformers Agent is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.</p>
<p>Transformers version v4.29.0, building on the concept of <em>tools</em> and <em>agents</em>. You can play with in
<a href="https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj" rel="nofollow">this colab</a>.</p>
<p>In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an
agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools,
but we’ll show you how the system can be extended easily to use any tool developed by the community.</p>
<p>Let’s start with a few examples of what can be achieved with this new API. It is particularly powerful when it comes
to multimodal tasks, so let’s take it for a spin to generate images and read text out loud.</p>

	<div>
	<pre><!-- HTML_TAG_START -->agent.run(<span>&#34;Caption the following image&#34;</span>, image=image)<!-- HTML_TAG_END --></pre></div>
<table><thead><tr><th><strong>Input</strong></th>
<th><strong>Output</strong></th></tr></thead>
<tbody><tr><td><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beaver.png" width="200"/></td>
<td>A beaver is swimming in the water</td></tr></tbody></table>
<hr/>

	<div>
	<pre><!-- HTML_TAG_START -->agent.run(<span>&#34;Read the following text out loud&#34;</span>, text=text)<!-- HTML_TAG_END --></pre></div>
<table><thead><tr><th><strong>Input</strong></th>
<th><strong>Output</strong></th></tr></thead>
<tbody><tr><td>A beaver is swimming in the water</td>
<td><audio controls=""><source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tts_example.wav" type="audio/wav"/> your browser does not support the audio element. &lt;/audio&gt;</audio></td></tr></tbody></table>
<hr/>

	<div>
	<pre><!-- HTML_TAG_START -->agent.run(
    <span>&#34;In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?&#34;</span>,
    document=document,
)<!-- HTML_TAG_END --></pre></div>
<table><thead><tr><th><strong>Input</strong></th>
<th><strong>Output</strong></th></tr></thead>
<tbody><tr><td><img src="https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/0/image/image.jpg" width="200"/></td>
<td>ballroom foyer</td></tr></tbody></table>
<h2><a id="quickstart" href="#quickstart"><span><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Quickstart
	</span></h2>

<p>Before being able to use <code>agent.run</code>, you will need to instantiate an agent, which is a large language model (LLM).
We provide support for openAI models as well as opensource alternatives from BigCode and OpenAssistant. The openAI
models perform better (but require you to have an openAI API key, so cannot be used for free); Hugging Face is
providing free access to endpoints for BigCode and OpenAssistant models.</p>
<p>To use openAI models, you instantiate an <a href="https://huggingface.co/docs/transformers/v4.29.0/en/main_classes/agent#transformers.OpenAiAgent">OpenAiAgent</a>:</p>

	<div>
	<pre><!-- HTML_TAG_START --><span>from</span> transformers <span>import</span> OpenAiAgent

agent = OpenAiAgent(model=<span>&#34;text-davinci-003&#34;</span>, api_key=<span>&#34;&lt;your_api_key&gt;&#34;</span>)<!-- HTML_TAG_END --></pre></div>
<p>To use BigCode or OpenAssistant, start by logging in to have access to the Inference API:</p>

	<div>
	<pre><!-- HTML_TAG_START --><span>from</span> huggingface_hub <span>import</span> login

login(<span>&#34;&lt;YOUR_TOKEN&gt;&#34;</span>)<!-- HTML_TAG_END --></pre></div>
<p>Then, instantiate the agent</p>

	<div>
	<pre><!-- HTML_TAG_START --><span>from</span> transformers <span>import</span> HfAgent


agent = HfAgent(<span>&#34;https://api-inference.huggingface.co/models/bigcode/starcoder&#34;</span>)



<!-- HTML_TAG_END --></pre></div>
<p>This is using the inference API that Hugging Face provides for free at the moment. If you have your own inference
endpoint for this model (or another one) you can replace the URL above with your URL endpoint.</p>


<p>StarCoder and OpenAssistant are free to use and perform admirably well on simple tasks. However, the checkpoints
don’t hold up when handling more complex prompts. If you’re facing such an issue, we recommend trying out the OpenAI
model which, while sadly not open-source, performs better at this given time.</p>
<p>You’re now good to go! Let’s dive into the two APIs that you now have at your disposal.</p>
<h3><a id="single-execution-run" href="#single-execution-run"><span><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Single execution (run)
	</span></h3>

<p>The single execution method is when using the <a href="https://huggingface.co/docs/transformers/v4.29.0/en/main_classes/agent#transformers.Agent.run">run()</a> method of the agent:</p>

	<div>
	<pre><!-- HTML_TAG_START -->agent.run(<span>&#34;Draw me a picture of rivers and lakes.&#34;</span>)<!-- HTML_TAG_END --></pre></div>
<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png" width="200"/></p><p>It automatically selects the tool (or tools) appropriate for the task you want to perform and runs them appropriately. It
can perform one or several tasks in the same instruction (though the more complex your instruction, the more likely
the agent is to fail).</p>

	<div>
	<pre><!-- HTML_TAG_START -->agent.run(<span>&#34;Draw me a picture of the sea then transform the picture to add an island&#34;</span>)<!-- HTML_TAG_END --></pre></div>
<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sea_and_island.png" width="200"/></p><!-- HTML_TAG_END --></div></div>
  </body>
</html>
