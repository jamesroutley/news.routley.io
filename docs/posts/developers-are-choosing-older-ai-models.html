<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.augmentcode.com/blog/developers-are-choosing-older-ai-models-and-16b-tokens-of-data-explain-why">Original</a>
    <h1>Developers are choosing older AI models</h1>
    
    <div id="readability-page-1" class="page"><div><p>At Augment Code, we run multiple frontier models side by side in production. This gives us a unique vantage point into how different models behave in real coding workflows. Usage patterns suggest developers are no longer just chasing the newest model; they are matching models to specific task profiles.<br/></p><p>This post shares data from millions of live interactions and discusses what it may reveal about model adoption, behavioral differences, and system-level trade-offs.</p><h3 id="model-adoption-is-fragmenting">Model Adoption Is Fragmenting</h3><p>Over the first week of October 2025, Sonnet 4.5’s share of total requests declined from 66% → 52%, while Sonnet 4.0 rose from 23% → 37%. GPT-5 usage stayed steady at about 10–12%.</p><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Date</th><th data-slot="table-head">Sonnet 4.5</th><th data-slot="table-head">Sonnet 4.0</th><th data-slot="table-head">GPT-5</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">2025-09-30</td><td data-slot="table-cell">66.18%</td><td data-slot="table-cell">23.26%</td><td data-slot="table-cell">10.57%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-01</td><td data-slot="table-cell">59.39%</td><td data-slot="table-cell">30.28%</td><td data-slot="table-cell">10.33%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-02</td><td data-slot="table-cell">55.77%</td><td data-slot="table-cell">33.54%</td><td data-slot="table-cell">10.69%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-03</td><td data-slot="table-cell">54.16%</td><td data-slot="table-cell">35.36%</td><td data-slot="table-cell">10.48%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-04</td><td data-slot="table-cell">56.66%</td><td data-slot="table-cell">31.70%</td><td data-slot="table-cell">11.64%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-05</td><td data-slot="table-cell">56.54%</td><td data-slot="table-cell">31.02%</td><td data-slot="table-cell">12.44%</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2025-10-06</td><td data-slot="table-cell">52.29%</td><td data-slot="table-cell">37.38%</td><td data-slot="table-cell">10.33%</td></tr></tbody></table></div><p>At first glance this could look like short-term churn after a new release. But if developers were simply upgrading, Sonnet 4.5’s share would continue rising while 4.0’s declined. The opposite happened. Both models retained significant usage, suggesting that teams are choosing models based on the kind of task, not on version number. In other words, upgrades are beginning to behave like <strong>alternatives</strong> rather than <strong>successors</strong>. That shift marks the early stages of specialization in production environments.</p><h3 id="diverging-behaviors-reasoning-depth-vs-action-frequency">Diverging Behaviors: Reasoning Depth vs. Action Frequency</h3><p>Despite producing larger total outputs, Sonnet 4.5 makes fewer tool calls per user message than 4.0.</p><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Model</th><th data-slot="table-head">Avg Tool Calls / User Message</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.5</td><td data-slot="table-cell">12.33</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.0</td><td data-slot="table-cell">15.65</td></tr><tr data-slot="table-row"><td data-slot="table-cell">GPT-5</td><td data-slot="table-cell">11.58</td></tr></tbody></table></div><p>Higher verbosity combined with fewer actions suggests that Sonnet 4.5 performs more internal reasoning before deciding to act. By contrast, 4.0 issues more frequent tool calls, favoring quick task execution over extended deliberation. GPT-5 falls close to 4.5 in call frequency but tends to favor natural-language reasoning over tool use.</p><p>We are monitoring whether this behavioral difference aligns with prompt success rates. If higher internal reasoning correlates with improved completion, it would confirm that Sonnet 4.5’s “think more, act less” tendency leads to better outcomes.</p><h3 id="throughput-and-token-economy">Throughput and Token Economy</h3><p>Sonnet 4.5 generates more text and tool output per message—about <strong>7.5 k tokens</strong> on average compared with <strong>5.5 k</strong> for 4.0. That is a <strong>37 % increase in total output</strong> per interaction.</p><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Model</th><th data-slot="table-head">Text Output</th><th data-slot="table-head">Tool Output</th><th data-slot="table-head">Total Output</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.5</td><td data-slot="table-cell">2,497</td><td data-slot="table-cell">5,018</td><td data-slot="table-cell">7,517</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.0</td><td data-slot="table-cell">1,168</td><td data-slot="table-cell">3,948</td><td data-slot="table-cell">5,481</td></tr><tr data-slot="table-row"><td data-slot="table-cell">GPT-5</td><td data-slot="table-cell">3,740</td><td data-slot="table-cell">1,729</td><td data-slot="table-cell">5,469</td></tr></tbody></table></div><p>Richer reasoning leads to more contextual responses but introduces additional latency. We do not yet have per-request tokens-per-second data, but qualitative traces suggest throughput is slightly lower, consistent with the extra compute required for deeper reasoning chains.</p><h3 id="compute-footprint-and-cache-utilization">Compute Footprint and Cache Utilization</h3><p>To understand how reasoning depth affects system load, we sampled a small subset of production data covering several billion tokens and corresponding cache operations.</p><p>Sonnet 4.5 still accounts for the majority of processed volume, with roughly <strong>one-third more cache reads</strong> than Sonnet 4.0. GPT-5 shows a much lighter footprint overall.</p><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Model</th><th data-slot="table-head">Input Tokens</th><th data-slot="table-head">Text Output</th><th data-slot="table-head">Tool Output</th><th data-slot="table-head">Total Output</th><th data-slot="table-head">Cache Reads</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.5</td><td data-slot="table-cell">0.25 B</td><td data-slot="table-cell">0.75 B</td><td data-slot="table-cell">1.55 B</td><td data-slot="table-cell">2.30 B</td><td data-slot="table-cell">240.0 B</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.0</td><td data-slot="table-cell">0.13 B</td><td data-slot="table-cell">0.20 B</td><td data-slot="table-cell">0.72 B</td><td data-slot="table-cell">0.92 B</td><td data-slot="table-cell">135.0 B</td></tr><tr data-slot="table-row"><td data-slot="table-cell">GPT-5</td><td data-slot="table-cell">0.16 B</td><td data-slot="table-cell">0.22 B</td><td data-slot="table-cell">0.10 B</td><td data-slot="table-cell">0.32 B</td><td data-slot="table-cell">28.0 B</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Grand Total</td><td data-slot="table-cell">0.54 B</td><td data-slot="table-cell">1.17 B</td><td data-slot="table-cell">2.37 B</td><td data-slot="table-cell">3.54 B</td><td data-slot="table-cell">403.0 B</td></tr></tbody></table></div><p>The higher cache-read volume for Sonnet 4.5 likely comes from heavier use of retrieval-augmented workflows and longer context windows. This suggests a system-level shift: more compute is being spent on managing and reusing context rather than on token generation itself.</p><h3 id="emergent-specialization-where-each-model-excels">Emergent Specialization: Where Each Model Excels</h3><p>Even though developers can freely choose models, their behavior reveals clear preferences by task type. Usage data and qualitative feedback show early signs of specialization.</p><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Model</th><th data-slot="table-head">Observed Strengths</th><th data-slot="table-head">Typical Workflows</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.5</td><td data-slot="table-cell">Long-context reasoning, multi-file understanding, autonomous planning</td><td data-slot="table-cell">Refactoring agents, complex debugging, design synthesis</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Sonnet 4.0</td><td data-slot="table-cell">Deterministic completions, consistent formatting, tool-friendly outputs</td><td data-slot="table-cell">API generation, structured edits, rule-based transforms</td></tr><tr data-slot="table-row"><td data-slot="table-cell">GPT-5</td><td data-slot="table-cell">Explanatory fluency, general reasoning, hybrid coding + documentation</td><td data-slot="table-cell">Code walkthroughs, summarization, developer education</td></tr></tbody></table></div><p>Each model appears to emphasize a different balance between reasoning and execution. Rather than seeking one “best” system, developers are assembling <strong>model alloys</strong>—ensembles that select the cognitive style best suited to a task.</p><p>Community discussions of <strong>Sonnet 4.5, 4.0, and GPT-5</strong> align closely with the production data:</p><ul><li><strong>Sonnet 4.5:</strong> Users describe it as thoughtful and reliable for multi-file reasoning but occasionally verbose or slower for simple edits. It handles refactors and architectural planning effectively but can over-explain.</li><li><strong>Sonnet 4.0:</strong> Praised for tool integration stability and predictable formatting. It is quick and consistent, ideal for automation or rule-based coding tasks. Teams often select it as the “safe default” model.</li><li><strong>GPT-5:</strong> Recognized for fluency and clarity in explanations. It performs well in hybrid reasoning-plus-writing contexts such as code reviews and documentation but lags in heavy tool execution.</li></ul><div data-slot="table-container"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Theme</th><th data-slot="table-head">Sonnet 4.5</th><th data-slot="table-head">Sonnet 4.0</th><th data-slot="table-head">GPT-5</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">Reasoning Depth</td><td data-slot="table-cell">⭐⭐⭐⭐ — Deep planning, sometimes overthinks</td><td data-slot="table-cell">⭐⭐ — Direct, task-driven</td><td data-slot="table-cell">⭐⭐⭐⭐ — Analytical and expressive</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Latency / Responsiveness</td><td data-slot="table-cell">Slower</td><td data-slot="table-cell">Fast</td><td data-slot="table-cell">Moderate</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Output Determinism</td><td data-slot="table-cell">Medium</td><td data-slot="table-cell">High</td><td data-slot="table-cell">Medium</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Code Generation Quality</td><td data-slot="table-cell">Excellent for multi-file</td><td data-slot="table-cell">Strong for single-file</td><td data-slot="table-cell">Great for hybrid code + docs</td></tr><tr data-slot="table-row"><td data-slot="table-cell">Ideal Use Cases</td><td data-slot="table-cell">Refactors, architecture</td><td data-slot="table-cell">Automation, structured tasks</td><td data-slot="table-cell">Walkthroughs, learning, synthesis</td></tr></tbody></table></div><h3 id="takeaways-the-early-signals-of-behavioral-specialization">Takeaways: The Early Signals of Behavioral Specialization</h3><p>Three main insights emerge from this dataset:</p><ol><li><strong>Adoption is diversifying, not consolidating.</strong> Newer models are not always better for every workflow.</li><li><strong>Behavioral divergence is measurable.</strong> Sonnet 4.5 reasons more deeply, while 4.0 acts more frequently.</li><li><strong>System costs are shifting.</strong> Reasoning intensity and cache utilization are now central performance metrics.</li></ol><p>The story here is not about one model surpassing others but about each developing its own niche. As capabilities expand, behaviors diverge. The industry may be entering a stage where functional specialization replaces the race for a single “best” model—much like how databases evolved into SQL, NoSQL, and time-series systems optimized for different workloads. The same dynamic is beginning to appear in AI: success depends less on overall strength and more on the right cognitive style for the job.</p><p>As reasoning depth increases, these behavioral distinctions could define the next phase of AI tooling. The key question for builders is no longer “Which model is best?” but “Which model best fits this task?”</p></div></div>
  </body>
</html>
