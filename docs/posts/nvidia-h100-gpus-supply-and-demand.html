<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/">Original</a>
    <h1>Nvidia H100 GPUs: Supply and Demand</h1>
    
    <div id="readability-page-1" class="page"><article>
    <header>
      
      
      
      
        <p><img srcset="/nvidia-h100-gpus-supply-and-demand/feature_hu4b9dcc4d15a2fc6ab8bc2944a890614b_273016_330x0_resize_box_3.png 330w,/nvidia-h100-gpus-supply-and-demand/feature_hu4b9dcc4d15a2fc6ab8bc2944a890614b_273016_660x0_resize_box_3.png 660w,/nvidia-h100-gpus-supply-and-demand/feature_hu4b9dcc4d15a2fc6ab8bc2944a890614b_273016_1024x0_resize_box_3.png 1024w,/nvidia-h100-gpus-supply-and-demand/feature_hu4b9dcc4d15a2fc6ab8bc2944a890614b_273016_1320x0_resize_box_3.png 2x" alt="" loading="lazy"/>
          
        </p>
      
    </header>
    <section>
      
        
      
      <div>
        <!-- maybe add disqus -->
<!-- created: 2023-06-28T06:24:16-07:00 -->
<p><strong>This post is an exploration of the supply and demand of GPUs, particularly Nvidia H100s. We’re also releasing a song and music video on the same day as this post.</strong></p>
<h2 id="introduction">Introduction <span><a href="#introduction" aria-label="Anchor">#</a></span></h2>
<p>As of July 2023, it seems AI might be bottlenecked by the supply of GPUs.</p>
<blockquote>
<p>“One reason the AI boom is being underestimated is the GPU/TPU shortage. This shortage is causing all kinds of limits on product rollouts and model training but these are not visible. Instead all we see is Nvidia spiking in price. Things will accelerate once supply meets demand.”</p>
<p>— Adam D’Angelo, CEO of Quora, Poe.com, former Facebook CTO</p>
</blockquote>
<figure>
      <img srcset="/images/ai-leaders-small_hu52fb41bb95d450a1e9186789c5540494_485451_330x0_resize_q75_box.jpg 330w,/images/ai-leaders-small_hu52fb41bb95d450a1e9186789c5540494_485451_660x0_resize_q75_box.jpg 660w,/images/ai-leaders-small_hu52fb41bb95d450a1e9186789c5540494_485451_1024x0_resize_q75_box.jpg 1024w,/images/ai-leaders-small_hu52fb41bb95d450a1e9186789c5540494_485451_1320x0_resize_q75_box.jpg 2x" src="https://gpus.llm-utils.org/images/ai-leaders-small_hu52fb41bb95d450a1e9186789c5540494_485451_660x0_resize_q75_box.jpg" alt="AI Leaders" loading="lazy"/>
      <figcaption>These Are The CEOs And Companies That Are Most Important to GPU Supply and Demand - And To AI. <a href="https://gpus.llm-utils.org/static-images/ai-leaders-large.jpg">Larger version</a></figcaption>
    </figure>
  

<h3 id="is-there-really-a-bottleneck">Is There Really A Bottleneck? <span><a href="#is-there-really-a-bottleneck" aria-label="Anchor">#</a></span></h3>
<p>Elon Musk says that “GPUs are at this point considerably harder to get than drugs.”<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>Sam Altman says that OpenAI is GPU-limited and it’s delaying their short term plans (fine-tuning, dedicated capacity, 32k context windows, multimodality).<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<p>Capacity of large scale H100 clusters at small and large cloud providers is running out.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></p>
<blockquote>
<p>“Rn everybody wishes Nvidia could produce more A/H100”<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>— Message from an exec at a cloud provider</p>
</blockquote>
<blockquote>
<p>“We’re so short on GPUs the less people use our products the better”</p>
<p>“We’d love it if they use it less because we don’t have enough GPUs”</p>
<p>Sam Altman, CEO at OpenAI<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
</blockquote>
<p>It’s a good soundbite to remind the world how much users love your product, but it’s also true that OpenAI needs more GPUs.</p>
<blockquote>
<p>For Azure/Microsoft:</p>
<ol>
<li>They are rate limiting employees on GPUs internally. They have to queue up like it was a university mainframe in the 1970s. I think OpenAI is sucking up all of it right now.</li>
<li>The Coreweave deal is all about pasting on their GPU infrastructure.</li>
</ol>
<p>— Anonymous</p>
</blockquote>
<p>In short: Yes, there’s a supply shortage of H100 GPUs. I’m told that for companies seeking 100s or 1000s of H100s, Azure and GCP are effectively out of capacity, and AWS is close to being out.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></p>
<p>This “out of capacity” is based on the allocations that Nvidia gave them.</p>
<p><strong>What do we want to know about the bottleneck?</strong></p>
<ol>
<li>What’s causing it (how much demand, how much supply)</li>
<li>How long will it last</li>
<li>What’s going to help resolve it</li>
</ol>
<h2 id="the-gpu-song">The GPU Song <span><a href="#the-gpu-song" aria-label="Anchor">#</a></span></h2>
<p>Uh… We’re also releasing a song on the same day as we’re releasing this post. It’s fire.</p>
<p>If you haven’t heard The GPU Song yet, do yourself a favor and play it.</p>

<p>
  <iframe src="https://www.youtube.com/embed/YGpnXANXGUg" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>It’s on <a href="https://open.spotify.com/track/752PA6x8X3x0Gdft1khH61?si=429b9fd7233f4b2e" target="_blank" rel="noreferrer noopener">Spotify</a>, <a href="https://music.apple.com/us/album/the-gpu-song-gpus-are-fire/1699113597?i=1699113599" target="_blank" rel="noreferrer noopener">Apple Music</a> and <a href="https://www.youtube.com/watch?v=YGpnXANXGUg" target="_blank" rel="noreferrer noopener">YouTube</a>.</p>
<p>See more info on <a href="https://gpus.llm-utils.org/the-gpu-song-gpus-are-fire/">the song here</a>.</p>
<h2 id="table-of-contents">Table Of Contents <span><a href="#table-of-contents" aria-label="Anchor">#</a></span></h2>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#is-there-really-a-bottleneck">Is There Really A Bottleneck?</a></li>
</ul>
</li>
<li><a href="#table-of-contents">Table Of Contents</a></li>
<li><a href="#demand-for-h100-gpus">Demand For H100 GPUs</a>
<ul>
<li><a href="#who-needs-h100s">Who Needs H100s?</a>
<ul>
<li><a href="#who-needshas-1000-h100-or-a100s">Who Needs/Has 1,000+ H100 Or A100s</a></li>
<li><a href="#who-needshas-100-h100-or-a100s">Who Needs/Has 100+ H100 Or A100s</a></li>
<li><a href="#what-are-most-of-the-high-end-gpus-being-used-for">What Are Most Of The High End GPUs Being Used For?</a></li>
<li><a href="#are-the-big-ai-labs-more-constrained-on-inference-or-training">Are The Big AI Labs More Constrained On Inference Or Training?</a></li>
</ul>
</li>
<li><a href="#which-gpus-do-people-need">Which GPUs Do People Need?</a>
<ul>
<li><a href="#whats-the-most-common-need-from-llm-startups">What’s The Most Common Need From LLM Startups?</a></li>
<li><a href="#what-do-companies-want-for-llm-training-and-inference">What Do Companies Want For LLM Training And Inference?</a></li>
<li><a href="#what-is-important-for-llm-training">What Is Important For LLM Training?</a></li>
<li><a href="#what-are-the-other-costs-of-training-and-running-llms">What Are The Other Costs Of Training And Running LLMs?</a></li>
<li><a href="#what-about-gpudirect">What About GPUDirect?</a></li>
<li><a href="#what-stops-llm-companies-from-using-amd-gpus">What Stops LLM Companies From Using AMD GPUs?</a></li>
<li><a href="#h100-vs-a100-how-much-faster-are-h100s-than-a100s">H100 Vs A100: How Much Faster Are H100s Than A100s?</a></li>
<li><a href="#is-everyone-going-to-want-to-upgrade-from-a100s-to-h100s">Is Everyone Going To Want To Upgrade From A100s To H100s?</a></li>
<li><a href="#whats-the-difference-between-h100s-gh200s-dgx-gh200s-hgx-h100s-and-dgx-h100s">What’s The Difference Between H100s, GH200s, DGX GH200s, HGX H100s, And DGX H100s?</a>
<ul>
<li><a href="#which-of-those-will-be-most-popular">Which Of Those Will Be Most Popular?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#how-much-do-these-gpus-cost">How Much Do These GPUs Cost?</a></li>
<li><a href="#how-many-gpus-are-needed">How Many GPUs Are Needed?</a>
<ul>
<li><a href="#how-many-h100s-are-most-startups-ordering">How Many H100s Are Most Startups Ordering?</a></li>
<li><a href="#how-many-h100s-might-companies-be-wanting">How Many H100s Might Companies Be Wanting?</a></li>
</ul>
</li>
<li><a href="#summary-h100-demand">Summary: H100 Demand</a></li>
</ul>
</li>
<li><a href="#supply-of-h100-gpus">Supply Of H100 GPUs</a>
<ul>
<li><a href="#who-makes-the-h100s">Who Makes The H100s?</a>
<ul>
<li><a href="#can-nvidia-use-other-chip-fabs-for-h100-production">Can Nvidia Use Other Chip Fabs For H100 Production?</a></li>
<li><a href="#how-do-the-different-tsmc-nodes-relate">How Do The Different TSMC Nodes Relate?</a></li>
<li><a href="#which-tsmc-node-is-the-h100-made-on">Which TSMC Node Is The H100 Made On?</a>
<ul>
<li><a href="#who-else-uses-that-node">Who Else Uses That Node?</a></li>
</ul>
</li>
<li><a href="#which-tsmc-node-does-the-a100-use">Which TSMC Node Does The A100 Use?</a></li>
<li><a href="#how-long-in-advance-is-fab-capacity-normally-reserved">How Long In Advance Is Fab Capacity Normally Reserved?</a></li>
<li><a href="#how-long-does-production-take-production-packaging-testing">How Long Does Production Take (production, Packaging, Testing)?</a></li>
<li><a href="#where-are-the-bottlenecks">Where Are The Bottlenecks?</a></li>
</ul>
</li>
<li><a href="#h100-memory">H100 Memory</a>
<ul>
<li><a href="#what-impacts-memory-bandwidth-on-gpus">What Impacts Memory Bandwidth On GPUs?</a></li>
<li><a href="#what-memory-is-used-on-the-h100s">What Memory Is Used On The H100s?</a></li>
<li><a href="#who-makes-the-memory-on-the-h100s">Who Makes The Memory On The H100s?</a></li>
</ul>
</li>
<li><a href="#what-else-is-used-when-making-gpus">What Else Is Used When Making GPUs?</a></li>
</ul>
</li>
<li><a href="#outlook-and-predictions">Outlook And Predictions</a>
<ul>
<li><a href="#what-is-nvidia-saying">What Is Nvidia Saying?</a></li>
<li><a href="#whatll-happen-next">What’ll Happen Next?</a>
<ul>
<li><a href="#when-will-there-be-a-h100-successor">When Will There Be A H100 Successor?</a></li>
<li><a href="#will-there-be-higher-vram-h100s">Will There Be Higher VRAM H100s?</a></li>
<li><a href="#when-will-the-shortage-end">When Will The Shortage End?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sourcing-h100s">Sourcing H100s</a>
<ul>
<li><a href="#who-sells-h100s">Who Sells H100s?</a>
<ul>
<li><a href="#how-are-the-lead-times">How Are The Lead Times?</a></li>
<li><a href="#if-a-startup-places-an-order-today-when-would-they-have-ssh-access">If A Startup Places An Order Today, When Would They Have SSH Access?</a></li>
<li><a href="#do-startups-buy-from-oems-and-resellers">Do Startups Buy From OEMs And Resellers?</a></li>
<li><a href="#when-do-startups-build-their-own-datacenter-vs-doing-colocation">When Do Startups Build Their Own Datacenter Vs Doing Colocation?</a></li>
</ul>
</li>
<li><a href="#how-do-the-big-clouds-compare">How Do The Big Clouds Compare?</a>
<ul>
<li><a href="#which-big-cloud-has-the-best-networking">Which Big Cloud Has The Best Networking?</a></li>
<li><a href="#which-big-clouds-do-enterprises-use">Which Big Clouds Do Enterprises Use?</a></li>
<li><a href="#how-about-dgx-cloud-who-is-nvidia-working-with-for-that">How About DGX Cloud, Who Is Nvidia Working With For That?</a></li>
<li><a href="#when-did-the-big-clouds-launch-their-h100-previews">When Did The Big Clouds Launch Their H100 Previews?</a></li>
</ul>
</li>
<li><a href="#how-can-a-company-or-cloud-service-provider-get-more-gpus">How Can A Company Or Cloud Service Provider Get More GPUs?</a>
<ul>
<li><a href="#how-do-nvidia-allocations-work">How Do Nvidia Allocations Work?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#closing-thoughts">Closing Thoughts</a>
<ul>
<li><a href="#tracing-the-journey-of-gpu-supply-and-demand">Tracing The Journey Of GPU Supply And Demand</a></li>
<li><a href="#getting-in-touch">Getting In Touch</a></li>
<li><a href="#the-natural-next-question---what-about-nvidia-alternatives">The Natural Next Question - What About Nvidia Alternatives?</a></li>
</ul>
</li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>
<h2 id="demand-for-h100-gpus">Demand For H100 GPUs <span><a href="#demand-for-h100-gpus" aria-label="Anchor">#</a></span></h2>
<p><strong>What’s causing the bottleneck - Demand</strong></p>
<ol>
<li>Specifically, what do people want to buy that they can’t?</li>
<li>How many of those GPUs do they need?</li>
<li>Why can’t they use a different GPU?</li>
<li>What are the different product names?</li>
<li>Where do companies buy them and how much do they cost?</li>
</ol>
<h3 id="who-needs-h100s">Who Needs H100s? <span><a href="#who-needs-h100s" aria-label="Anchor">#</a></span></h3>
<blockquote>
<p>“It seems like everyone and their dog is buying GPUs at this point”<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></p>
<p>– Elon</p>
</blockquote>
<h4 id="who-needshas-1000-h100-or-a100s">Who Needs/Has 1,000+ H100 Or A100s <span><a href="#who-needshas-1000-h100-or-a100s" aria-label="Anchor">#</a></span></h4>
<ul>
<li>Startups training LLMs
<ul>
<li>OpenAI (through Azure), Anthropic, Inflection (through Azure<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> and CoreWeave<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>), Mistral AI</li>
</ul>
</li>
<li>CSPs (Cloud Service Providers)
<ul>
<li>The big 3: Azure, GCP, AWS</li>
<li>The other public cloud: Oracle</li>
<li>Larger private clouds like CoreWeave, Lambda</li>
</ul>
</li>
<li>Other large companies
<ul>
<li>Tesla<sup id="fnref1:7"><a href="#fn:7" role="doc-noteref">7</a></sup> <sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup></li>
</ul>
</li>
</ul>
<h4 id="who-needshas-100-h100-or-a100s">Who Needs/Has 100+ H100 Or A100s <span><a href="#who-needshas-100-h100-or-a100s" aria-label="Anchor">#</a></span></h4>
<p>Startups doing significant fine-tuning large open source models.</p>
<h4 id="what-are-most-of-the-high-end-gpus-being-used-for">What Are Most Of The High End GPUs Being Used For? <span><a href="#what-are-most-of-the-high-end-gpus-being-used-for" aria-label="Anchor">#</a></span></h4>
<p>For companies using private clouds (CoreWeave, Lambda), of companies with hundreds or thousands of H100s, it’s almost all LLMs, and some diffusion model work. Some of it is fine-tuning of existing models, but mostly it’s new startups that you may not yet know about that are building new models from scratch. They’re doing $10mm-50mm contracts done over 3 years, with a few hundred to a few thousand GPUs.</p>
<p>For companies using on-demand H100s with a handful of GPUs, it’s still probably &gt;50% LLM related usage.</p>
<p>Private clouds are now starting to see inbound demand from enterprises who would normally be going with their default big cloud provider, but everyone is out.</p>
<h4 id="are-the-big-ai-labs-more-constrained-on-inference-or-training">Are The Big AI Labs More Constrained On Inference Or Training? <span><a href="#are-the-big-ai-labs-more-constrained-on-inference-or-training" aria-label="Anchor">#</a></span></h4>
<p>Depends on how much product traction they have! Sam Altman says OpenAI would rather have more inference capacity if forced to choose, but OpenAI is still constrained on both.<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup></p>
<h3 id="which-gpus-do-people-need">Which GPUs Do People Need? <span><a href="#which-gpus-do-people-need" aria-label="Anchor">#</a></span></h3>
<p>Mostly H100s. Why? It’s the fastest both for inference and training for LLMs. (The H100 is often also the best price-performance ratio for inference, too)</p>
<p>Specifically: 8-GPU HGX H100 SXM servers.</p>
<blockquote>
<p>My analysis is it’s cheaper to run for the same work as well. The V100 a great deal if you could find them used, which you can’t</p>
<p>– Anonymous</p>
</blockquote>
<blockquote>
<p>honestly not sure about [it being the best price-performance ratio]? price/performance for training looks about the same for A100 as for H100. for inference, we find that A10Gs are more than enough and much cheaper.</p>
<p>– Private cloud exec</p>
</blockquote>
<blockquote>
<p>this [A10G’s being more than enough] was true for a while. but in the world of falcon 40b and llama2 70b, which we’re seeing a lot of usage for, it’s not true anymore. we need A100s for these</p>
<p>2xA100s to be exact. so the interconnect speed matters for inference.</p>
<p>– (Different) Private cloud exec</p>
</blockquote>
<!-- I'm planning to do a deep dive on specifically what stops companies who are spending 9-10 figures on GPUs from spending a portion of their budget on getting their training or inference to run on non-Nvidia hardware. If you're a reader and have insight there, let me know. -->
<h4 id="whats-the-most-common-need-from-llm-startups">What’s The Most Common Need From LLM Startups? <span><a href="#whats-the-most-common-need-from-llm-startups" aria-label="Anchor">#</a></span></h4>
<p>For training LLMs: H100s with 3.2Tb/s InfiniBand.</p>
<h4 id="what-do-companies-want-for-llm-training-and-inference">What Do Companies Want For LLM Training And Inference? <span><a href="#what-do-companies-want-for-llm-training-and-inference" aria-label="Anchor">#</a></span></h4>
<p>For training they tend to want H100s, for inference it’s much more about performance per dollar.</p>
<p>It’s still a performance per dollar question with H100s vs A100s, but H100s are generally favored as they can scale better with higher numbers of GPUs and give faster training times, and speed / compressing time to launch or train or improve models is critical for startups.</p>
<blockquote>
<p>“For multi-node training, all of them are asking for A100 or H100 with InfiniBand networking. Only non A/H100 request we see are for inference where workloads are single GPU or single node”</p>
<p>– Private cloud exec</p>
</blockquote>
<h4 id="what-is-important-for-llm-training">What Is Important For LLM Training? <span><a href="#what-is-important-for-llm-training" aria-label="Anchor">#</a></span></h4>
<ul>
<li>Memory bandwidth</li>
<li>FLOPS (tensor cores or equivalent matrix multiplication units)</li>
<li>Caches and cache latencies</li>
<li>Additional features like FP8 compute</li>
<li>Compute performance (related to number of cuda cores)</li>
<li>Interconnect speed (eg InfiniBand)</li>
</ul>
<p>The H100 is preferred over A100 partly because of things like lower cache latencies and FP8 compute.</p>
<blockquote>
<p>H100 is preferred because it is up to 3x more efficient, but the costs are only (1.5 - 2x). Combined with the overall system cost, H100 yields much more performance per dollar (if you look at system performance, probably 4-5x more performance per dollar).</p>
<p>— Deep learning researcher</p>
</blockquote>
<h4 id="what-are-the-other-costs-of-training-and-running-llms">What Are The Other Costs Of Training And Running LLMs? <span><a href="#what-are-the-other-costs-of-training-and-running-llms" aria-label="Anchor">#</a></span></h4>
<p>GPUs are the most expensive individual component, but there are other costs.</p>
<p>System RAM and NVMe SSDs are expensive.</p>
<p>InfiniBand networking is costly.</p>
<p>10-15% of total cost for running a cluster might go to power and hosting (electricity, cost of the datacenter building, cost of the land, staff) - roughly split between the two, can be 5-8% for power and 5-10% for other elements of hosting cost (land, building, staff).</p>
<blockquote>
<p>It’s mostly networking and reliable datacenters. AWS is difficult to work with because of network limitations and unreliable hardware</p>
<p>— Deep learning researcher</p>
</blockquote>
<h4 id="what-about-gpudirect">What About GPUDirect? <span><a href="#what-about-gpudirect" aria-label="Anchor">#</a></span></h4>
<p>GPUDirect is not a critical requirement, but can be helpful.</p>
<blockquote>
<p>I would not say it is supercritical, but it makes a difference in performance. I guess it depends on where your bottleneck is. For some architectures / software implementations, the bottleneck is not necessarily networking, but if it is GPUDirect can make a difference of 10-20%, and that are some pretty significant numbers for expensive training runs.</p>
<p>That being said, GPUDirect RDMA is now so ubiquitous that it goes almost without saying that it is supported. I think support is less strong for non-InfiniBand networking, but most GPU clusters optimized for neural network training have Infiniband networks / cards. A bigger factor for performance might be NVLink, since this is rarer than Infiniband, but it is also only critical if you have particular parallelization strategies.</p>
<p>So features like strong networking and GPUDirect allows you to be lazy and you can guarantee that naive software is better out of the box. But it is not a strict requirement if you care about cost or using infrastructure that you already have.</p>
<p>– Deep learning researcher</p>
</blockquote>
<h4 id="what-stops-llm-companies-from-using-amd-gpus">What Stops LLM Companies From Using AMD GPUs? <span><a href="#what-stops-llm-companies-from-using-amd-gpus" aria-label="Anchor">#</a></span></h4>
<blockquote>
<p>Theoretically a company can buy a bunch of AMD GPUs, but it just takes time to get everything to work. That dev time (even if just 2 months) might mean being later to market than a competitor. So CUDA is NVIDIA’s moat right now.</p>
<p>– Private cloud exec</p>
</blockquote>
<blockquote>
<p>I suspect 2 months is off by an order of magnitude, it’s probably not a meaningful difference, see <a href="https://www.mosaicml.com/blog/amd-mi250" target="_blank" rel="noreferrer noopener">https://www.mosaicml.com/blog/amd-mi250</a></p>
<p>– ML Engineer</p>
</blockquote>
<blockquote>
<p>Who is going to take the risk of deplying 10,000 AMD GPUs or 10,000 random startup silicon chips? That’s almost a $300 million investment.</p>
<p>– Private cloud exec</p>
</blockquote>
<blockquote>
<p>MosaicML/MI250 - Has anyone asked AMD about availability?  It doesn’t seem like AMD built many beyond what they needed for Frontier, and now TSMC CoWoS capacity is sucked up by Nvidia.  MI250 may be a viable alternative but unavailable.</p>
<p>– Retired semiconductor industry professional</p>
</blockquote>
<!-- There's a *lot* of activity going to reducing Nvidia's lock on GPUs, I'll be writing more about that.  -->
<!-- AMD, Intel, geohot's tiny corp, lots of efforts for CUDA porting, making CUDA irrelevant, Mojo, PyTorch, various other chip companies, and so on. -->
<h4 id="h100-vs-a100-how-much-faster-are-h100s-than-a100s">H100 Vs A100: How Much Faster Are H100s Than A100s? <span><a href="#h100-vs-a100-how-much-faster-are-h100s-than-a100s" aria-label="Anchor">#</a></span></h4>
<p>About 3.5x faster for 16-bit inference<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup> and about 2.3x faster for 16-bit training.<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p>
<figure>
      <img srcset="/images/a100-vs-h100-speed_hu4a71f10b529caba057f44f0c2d231f98_87340_330x0_resize_q75_box.jpeg 330w,/images/a100-vs-h100-speed_hu4a71f10b529caba057f44f0c2d231f98_87340_660x0_resize_q75_box.jpeg 660w,/images/a100-vs-h100-speed_hu4a71f10b529caba057f44f0c2d231f98_87340_1024x0_resize_q75_box.jpeg 1024w,/images/a100-vs-h100-speed_hu4a71f10b529caba057f44f0c2d231f98_87340_1320x0_resize_q75_box.jpeg 2x" src="https://gpus.llm-utils.org/images/a100-vs-h100-speed_hu4a71f10b529caba057f44f0c2d231f98_87340_660x0_resize_q75_box.jpeg" alt="A100 vs H100 speed" loading="lazy"/>
      <figcaption>A100 vs H100 Speed</figcaption>
    </figure>
  

<figure>
      <img src="https://gpus.llm-utils.org/images/h100-moe-vs-a100.webp" alt="H100 training MoE" loading="lazy"/>
      <figcaption>H100 Training MoE</figcaption>
    </figure>
  

<figure>
      <img srcset="/images/h100-speedup-at-scale_hud19a8968909652227c6b38ef1bbeae84_163135_330x0_resize_q75_box.jpeg 330w,/images/h100-speedup-at-scale_hud19a8968909652227c6b38ef1bbeae84_163135_660x0_resize_q75_box.jpeg 660w,/images/h100-speedup-at-scale_hud19a8968909652227c6b38ef1bbeae84_163135_1024x0_resize_q75_box.jpeg 1024w,/images/h100-speedup-at-scale_hud19a8968909652227c6b38ef1bbeae84_163135_1320x0_resize_q75_box.jpeg 2x" src="https://gpus.llm-utils.org/images/h100-speedup-at-scale_hud19a8968909652227c6b38ef1bbeae84_163135_660x0_resize_q75_box.jpeg" alt="H100 speedup at scale" loading="lazy"/>
      <figcaption>H100 Speedup At Scale</figcaption>
    </figure>
  

<p>Here’s some more reading for you: <a href="https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/" target="_blank" rel="noreferrer noopener">1</a> <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" target="_blank" rel="noreferrer noopener">2</a> <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" target="_blank" rel="noreferrer noopener">3</a>.</p>
<h4 id="is-everyone-going-to-want-to-upgrade-from-a100s-to-h100s">Is Everyone Going To Want To Upgrade From A100s To H100s? <span><a href="#is-everyone-going-to-want-to-upgrade-from-a100s-to-h100s" aria-label="Anchor">#</a></span></h4>
<p>Mostly people will want to buy H100s and use them for training and inference and switch their A100s to be used primarily for inference. But, some people might be hesitant to switch due to cost, capacity, the risk of using new hardware and setting it up, and their existing software being already optimized for A100s.</p>
<blockquote>
<p>Yes, A100s will become today’s V100s in a few years. I don’t know of anyone training LLMs on V100s right now because of performance constraints. But they are still used in inference and other workloads. Similarly, A100 pricing will come down as more AI companies shift workloads to H100s, but there will always be demand, especially for inference.</p>
<p>– Private cloud exec</p>
</blockquote>
<blockquote>
<p>think it’s also plausible some of the startups that raised huge rounds end up folding and then there’s a lot of A100s coming back on the market</p>
<p>– (Different) Private cloud exec</p>
</blockquote>
<p>Over time people will move and the A100s will be more used for inference.</p>
<p>What about V100s? Higher VRAM cards are better for large models, so cutting edge groups much prefer H100s or A100s.</p>
<blockquote>
<p>The main reason for not using V100 is the lack of brainfloat16 (bfloat16, BF16) data type. Without that, its very difficult to train models easily. The poor performance of OPT and BLOOM can be mostly attributed to not having this data type (OPT was trained in float16, BLOOM’s prototyping was mostly done in fp16, which did not yield data was generalized to the training run which was done in bf16)</p>
<p>— Deep learning researcher</p>
</blockquote>
<h4 id="whats-the-difference-between-h100s-gh200s-dgx-gh200s-hgx-h100s-and-dgx-h100s">What’s The Difference Between H100s, GH200s, DGX GH200s, HGX H100s, And DGX H100s? <span><a href="#whats-the-difference-between-h100s-gh200s-dgx-gh200s-hgx-h100s-and-dgx-h100s" aria-label="Anchor">#</a></span></h4>
<ul>
<li>H100 = 1x H100 GPU</li>
<li>HGX H100 = the Nvidia server reference platform that OEMs use to build 4-GPU or 8-GPU servers. Built by third-party OEMs like Supermicro.</li>
<li>DGX H100 = the Nvidia official H100 server with 8x H100s.<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> Nvidia is the sole vendor.</li>
<li>GH200 = 1x H100 GPU plus 1x Grace CPU.<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup></li>
<li>DGX GH200 = 256x GH200s,<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup> available toward the end of 2023.<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup> Likely only offered by Nvidia.</li>
</ul>
<p>There’s also MGX which is aimed at large cloud companies.</p>
<h5 id="which-of-those-will-be-most-popular">Which Of Those Will Be Most Popular? <span><a href="#which-of-those-will-be-most-popular" aria-label="Anchor">#</a></span></h5>
<p>Most companies will buy 8-GPU HGX H100s,<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup> rather than DGX H100s or 4-GPU HGX H100 servers.</p>
<h3 id="how-much-do-these-gpus-cost">How Much Do These GPUs Cost? <span><a href="#how-much-do-these-gpus-cost" aria-label="Anchor">#</a></span></h3>
<p>1x DGX H100 (SXM) with 8x H100 GPUs is $460k including the required support. $100k of the $460k is required support. The specs are below. Startups can get the Inception discount which is about $50k off, and can be used on up to 8x DGX H100 boxes for a total of 64 H100s.</p>
<figure>
      <img src="https://gpus.llm-utils.org/images/dgx-h100-specs.png" alt="DGX H100 Specs" loading="lazy"/>
      <figcaption>DGX H100 Specs</figcaption>
    </figure>
  

<p>1x HGX H100 (SXM) with 8x H100 GPUs is between $300k-380k, depending on the specs (networking, storage, ram, CPUs) and the margins of whoever is selling it and the level of support. The higher end of that range, $360k-380k including support, is what you might expect for identical specs to a DGX H100.</p>
<p>1x HGX H100 (PCIe) with 8x H100 GPUs is approx $300k including support, depending on specs.</p>
<p>PCIe cards are around $30k-32k market prices.</p>
<p>SXM cards aren’t really sold as single cards, so it’s tough to give pricing there. Generally only sold as 4-GPU and 8-GPU servers.</p>
<p>Around 70-80% of the demand is for SXM H100s, the rest is for PCIe H100s. And the SXM portion of the demand is trending upwards, because PCIe cards were the only ones available for the first few months. Given most companies buy 8-GPU HGX H100s (SXM), the approximate spend is $360k-380k per 8 H100s, including other server components.</p>
<p>The DGX GH200 (which as a reminder, contains 256x GH200s, and each GH200 contains 1x H100 GPU and 1x Grace CPU) might cost in the range of $15mm-25mm - though this is a guess, not based on a pricing sheet.<sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup></p>
<h3 id="how-many-gpus-are-needed">How Many GPUs Are Needed? <span><a href="#how-many-gpus-are-needed" aria-label="Anchor">#</a></span></h3>
<ul>
<li>GPT-4 was likely trained on somewhere between 10,000 to 25,000 A100s.<sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup></li>
<li>Meta has about 21,000 A100s, Tesla has about 7,000 A100s, and Stability AI has about 5,000 A100s.<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup></li>
<li>Falcon-40B was trained on 384 A100s.<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup></li>
<li>Inflection used 3,500 H100s for their GPT-3.5 equivalent model.<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup></li>
</ul>
<p>GPT-5 might need 30k-50k H100s according to Elon. Morgan Stanley said in Feb 2023 that GPT-5 would use 25,000 GPUs, but they also said it was already being trained as of Feb 2023 and Sam Altman said in May 2023 that it’s not yet being trained, so MS’s info may be outdated.</p>
<p>GCP has approx 25k H100s. Azure probably has 10k-40k H100s. Should be similar for Oracle. Most of Azure’s capacity is going to OpenAI.</p>
<p>CoreWeave is in the ballpark of 35k-40k H100s - not live, but based on bookings.</p>
<h4 id="how-many-h100s-are-most-startups-ordering">How Many H100s Are Most Startups Ordering? <span><a href="#how-many-h100s-are-most-startups-ordering" aria-label="Anchor">#</a></span></h4>
<p>For LLMs: For fine tuning, dozens or low hundreds. For training, thousands.</p>
<h4 id="how-many-h100s-might-companies-be-wanting">How Many H100s Might Companies Be Wanting? <span><a href="#how-many-h100s-might-companies-be-wanting" aria-label="Anchor">#</a></span></h4>
<p>OpenAI might want 50k. Inflection wants 22k.<sup id="fnref:24"><a href="#fn:24" role="doc-noteref">24</a></sup> Meta maybe 25k (I’m told actually Meta wants 100k or more). Big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral, Character, might want 10k each. Total ballparks and guessing, and some of that is double counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35k a piece, that’s about $15b worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu, and Tencent who will want a lot of H800s.</p>
<p>There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.</p>
<p>How does that compare to Nvidia’s data center revenue?</p>
<p>Feb-April 2023 was $4.28b data center revenue.<sup id="fnref:25"><a href="#fn:25" role="doc-noteref">25</a></sup> May-July 2023 might be around $8b data center revenue, assuming most of the higher guidance for that quarter is due to gain in data center revenue rather than other segments.</p>
<p>So might take a while for the supply shortage to go away. But also all my ballparks could be wildly overstated, and many of these companies aren’t going to go right out and buy the H100s today, they’ll upgrade over time. Plus, Nvidia is aggressively ramping production capacity.</p>
<blockquote>
<p>Seems possible. 400k H100s doesn’t sound out of reach, especially given how everyone is doing a massive 4 or 5-figure H100 deployment right now.</p>
<p>– Private cloud exec</p>
</blockquote>
<h3 id="summary-h100-demand">Summary: H100 Demand <span><a href="#summary-h100-demand" aria-label="Anchor">#</a></span></h3>
<p>The main things to keep in mind as you go onto the next section are that most of the big CSPs (Azure, AWS, GCP, and also Oracle) and private clouds (CoreWeave, Lambda, and various others) want more H100s than they can get access to. Most of the big AI product companies want more H100s than they can get access to, as well. Generally they want 8-GPU HGX H100 boxes with SXM cards, which cost approx $300k-400k per 8-GPU server depending on specs and support. There may be a few hundred thousand H100 GPUs worth of excess demand ($15b+ of GPUs). With a limited supply, Nvidia could purely raise prices to find a clearing price, and are doing that to some extent. But it’s important to know that ultimately H100 allocation comes down to who Nvidia prefers to give that allocation to.</p>
<h2 id="supply-of-h100-gpus">Supply Of H100 GPUs <span><a href="#supply-of-h100-gpus" aria-label="Anchor">#</a></span></h2>
<p><strong>What’s causing the bottleneck - Supply</strong></p>
<ol>
<li>What are the bottlenecks on the production side?</li>
<li>Which components?</li>
<li>Who produces them?</li>
</ol>
<h3 id="who-makes-the-h100s">Who Makes The H100s? <span><a href="#who-makes-the-h100s" aria-label="Anchor">#</a></span></h3>
<p>TSMC.</p>
<h4 id="can-nvidia-use-other-chip-fabs-for-h100-production">Can Nvidia Use Other Chip Fabs For H100 Production? <span><a href="#can-nvidia-use-other-chip-fabs-for-h100-production" aria-label="Anchor">#</a></span></h4>
<p>Not really, at least not yet. They’ve worked with Samsung in the past. But on the H100s and other 5nm GPUs they only use TSMC. Implication is that Samsung can’t yet meet their needs for cutting edge GPUs. They might work with Intel in the future, and Samsung again on cutting edge, but neither of those will be happening in the short term in a way that’d help the H100 supply crunch.</p>
<h4 id="how-do-the-different-tsmc-nodes-relate">How Do The Different TSMC Nodes Relate? <span><a href="#how-do-the-different-tsmc-nodes-relate" aria-label="Anchor">#</a></span></h4>
<p>TSMC 5nm family:</p>
<ul>
<li>N5<sup id="fnref:26"><a href="#fn:26" role="doc-noteref">26</a></sup>
<ul>
<li>4N either fits here as an enhanced version of N5, or below N5P</li>
<li>N5P
<ul>
<li>4N either fits here as an enhanced version of N5P, or below N5 as an enhanced version of N5</li>
</ul>
</li>
<li>N4</li>
<li>N4P</li>
</ul>
</li>
</ul>
<h4 id="which-tsmc-node-is-the-h100-made-on">Which TSMC Node Is The H100 Made On? <span><a href="#which-tsmc-node-is-the-h100-made-on" aria-label="Anchor">#</a></span></h4>
<p>TSMC 4N. This is a special node for Nvidia, it’s in the 5nm family and is enhanced 5nm though rather than truly 4nm.</p>
<h5 id="who-else-uses-that-node">Who Else Uses That Node? <span><a href="#who-else-uses-that-node" aria-label="Anchor">#</a></span></h5>
<p>It was Apple, but they’ve moved primarily to N3 and have reserved most of the N3 capacity. Qualcomm and AMD are the other big N5-family customers.</p>
<h4 id="which-tsmc-node-does-the-a100-use">Which TSMC Node Does The A100 Use? <span><a href="#which-tsmc-node-does-the-a100-use" aria-label="Anchor">#</a></span></h4>
<p>N7<sup id="fnref:27"><a href="#fn:27" role="doc-noteref">27</a></sup></p>
<h4 id="how-long-in-advance-is-fab-capacity-normally-reserved">How Long In Advance Is Fab Capacity Normally Reserved? <span><a href="#how-long-in-advance-is-fab-capacity-normally-reserved" aria-label="Anchor">#</a></span></h4>
<p>Not sure though maybe 12+ months.</p>
<blockquote>
<p>that applies to TSM and their big customers
They sort of plan it out together
Which is why TSM/NVDA may have underestimated what they need</p>
<p>– Anonymous</p>
</blockquote>
<h4 id="how-long-does-production-take-production-packaging-testing">How Long Does Production Take (Production, Packaging, Testing)? <span><a href="#how-long-does-production-take-production-packaging-testing" aria-label="Anchor">#</a></span></h4>
<p>6 months from production on a H100 starting to that H100 being ready to be sold to a customer (est from a conversation, would like to get a confirmation)</p>
<h4 id="where-are-the-bottlenecks">Where Are The Bottlenecks? <span><a href="#where-are-the-bottlenecks" aria-label="Anchor">#</a></span></h4>
<blockquote>
<p>Wafer starts are not the bottleneck at TSMC.  Mentioned earlier CoWoS (3D stacking) packaging is the gate at TSMC.</p>
<p>– Retired semiconductor industry professional</p>
</blockquote>
<h3 id="h100-memory">H100 Memory <span><a href="#h100-memory" aria-label="Anchor">#</a></span></h3>
<h4 id="what-impacts-memory-bandwidth-on-gpus">What Impacts Memory Bandwidth On GPUs? <span><a href="#what-impacts-memory-bandwidth-on-gpus" aria-label="Anchor">#</a></span></h4>
<p>Memory type, memory bus width, and memory clock speed.</p>
<blockquote>
<p>It’s mostly HBM. Manufacturing it is a nightmare. Supply is also mostly limited because HBM is so difficult to produce. Once you have HBM the design follows intuitively</p>
<p>— Deep learning researcher</p>
</blockquote>
<h4 id="what-memory-is-used-on-the-h100s">What Memory Is Used On The H100s? <span><a href="#what-memory-is-used-on-the-h100s" aria-label="Anchor">#</a></span></h4>
<p>On the H100 SXM, it’s HBM3.<sup id="fnref:28"><a href="#fn:28" role="doc-noteref">28</a></sup> On the H100 PCIe, it’s actually HBM2e.<sup id="fnref:29"><a href="#fn:29" role="doc-noteref">29</a></sup></p>
<h4 id="who-makes-the-memory-on-the-h100s">Who Makes The Memory On The H100s? <span><a href="#who-makes-the-memory-on-the-h100s" aria-label="Anchor">#</a></span></h4>
<p>The bus width and clock speed are designed by Nvidia as part of the GPU architecture.</p>
<p>For the HBM3 memory itself, I think Nvidia uses either all or mostly SK Hynix. Not sure if Nvidia uses any from Samsung for the H100s and I believe it’s nothing from Micron for the H100s.</p>
<p>In terms of HBM3 generally, SK Hynix makes the most, then Samsung not that far behind, then Micron far behind. Seems like SK Hynix is ramped up but Nvidia still wants them to make more, and Samsung and Micron haven’t successfully ramped up production yet.</p>
<h3 id="what-else-is-used-when-making-gpus">What Else Is Used When Making GPUs? <span><a href="#what-else-is-used-when-making-gpus" aria-label="Anchor">#</a></span></h3>
<p>Note that some of these pieces are significantly more bottlenecked than others.</p>
<ul>
<li>
<p><strong>Metal Elements</strong>: These are essential in the production of GPUs. They include:</p>
<ul>
<li>Copper: Used in the creation of electrical connections due to its high conductivity.</li>
<li>Tantalum: Often used in capacitors due to its ability to hold a high electrical charge.</li>
<li>Gold: Used in high-quality plating and connectors due to its resistance to corrosion.</li>
<li>Aluminum: Frequently used in the heatsink to help dissipate heat.</li>
<li>Nickel: Often used in the coating of connectors for its corrosion resistance.</li>
<li>Tin: Used in soldering components together.</li>
<li>Indium: Used in thermal interface materials for its good thermal conductivity.</li>
<li>Palladium: Used in certain types of capacitors and semiconductor devices.</li>
</ul>
</li>
<li>
<p><strong>Silicon (Metalloid)</strong>: This is the primary material used in the creation of semiconductor devices.</p>
</li>
<li>
<p><strong>Rare Earth Elements</strong>: These are used in various parts of the GPU for their unique properties.</p>
</li>
<li>
<p><strong>Other Metals and Chemicals</strong>: These are used in various stages of production, from creating the silicon wafers to the final assembly of the GPU.</p>
</li>
<li>
<p><strong>Substrates</strong>: These are the material on which the GPU components are mounted.</p>
</li>
<li>
<p><strong>Package Materials</strong>: These are used to house and protect the GPU chip.</p>
</li>
<li>
<p><strong>Solder Balls and Bonding Wires</strong>: These are used to connect the GPU chip to the substrate and other components.</p>
</li>
<li>
<p><strong>Passive Components</strong>: These include capacitors and resistors, which are essential for the operation of the GPU.</p>
</li>
<li>
<p><strong>Printed Circuit Board (PCB)</strong>: This is the board on which all the components of the GPU are mounted. It provides the electrical connections between the components.</p>
</li>
<li>
<p><strong>Thermal Compounds</strong>: These are used to improve heat conduction between the chip and the heatsink.</p>
</li>
<li>
<p><strong>Semiconductor Manufacturing Equipment</strong>: This includes photolithography machines, etching equipment, ion implantation equipment, etc.</p>
</li>
<li>
<p><strong>Clean Room Facilities</strong>: These are necessary for the production of GPUs to prevent contamination of the silicon wafers and other components.</p>
</li>
<li>
<p><strong>Testing and Quality Control Equipment</strong>: These are used to ensure that the GPUs meet the required performance and reliability standards.</p>
</li>
<li>
<p><strong>Software and Firmware</strong>: These are essential for controlling the operation of the GPU and for interfacing with the rest of the computer system.</p>
</li>
<li>
<p><strong>Packaging and Shipping Materials</strong>: These are necessary for delivering the final product to customers in good condition.</p>
</li>
<li>
<p><strong>Software Tools</strong>: Software tools for Computer-Aided Design (CAD) and simulations are crucial in designing the structure and testing functionality of the GPU.</p>
</li>
<li>
<p><strong>Energy Consumption</strong>: A significant amount of electricity is required in the manufacturing process of GPU chips due to the usage of high-precision machinery.</p>
</li>
<li>
<p><strong>Waste Management:</strong> The production of GPUs results in waste which has to be properly managed and disposed of, as many of the materials used can be harmful to the environment.</p>
</li>
<li>
<p><strong>Test capacity:</strong> Custom/specialty test equipment that verifies functionality and performance.</p>
</li>
<li>
<p><strong>Chip packaging:</strong> Assembling the silicon wafer into a component package that can be utilized in a larger system.</p>
</li>
</ul>
<h2 id="outlook-and-predictions">Outlook And Predictions <span><a href="#outlook-and-predictions" aria-label="Anchor">#</a></span></h2>
<h3 id="what-is-nvidia-saying">What Is Nvidia Saying? <span><a href="#what-is-nvidia-saying" aria-label="Anchor">#</a></span></h3>
<p>Nvidia has disclosed that they have more supply in the second half of the year, but beyond that they haven’t said much more, and nothing quantitative.</p>
<blockquote>
<p>“We are working on both supply today for this quarter, but we have also procured a substantial amount of supply for the second half”</p>
<p>“We believe that the supply that we will have for the second half of the year will be substantially larger than h1”</p>
<p>– Nvidia CFO Colette Kress during the earnings call for Feb-April 2023</p>
</blockquote>
<h3 id="whatll-happen-next">What’ll Happen Next? <span><a href="#whatll-happen-next" aria-label="Anchor">#</a></span></h3>
<blockquote>
<p>I think it’s possible we have a self-reinforcing cycle right now where scarcity causes GPU capacity to be perceived as a moat, which causes more GPU-hoarding, which exacerbates scarcity.</p>
<p>– Private cloud exec</p>
</blockquote>
<h4 id="when-will-there-be-a-h100-successor">When Will There Be A H100 Successor? <span><a href="#when-will-there-be-a-h100-successor" aria-label="Anchor">#</a></span></h4>
<p>Probably won’t be announced until late 2024 (mid 2024 to early 2025), based on historical Nvidia time between architectures.</p>
<p>The H100 will be the top of the line Nvidia GPU until then. (The GH200 and DGX GH200 don’t count, they’re not pure GPUs, they all use H100s as their GPU)</p>
<h4 id="will-there-be-higher-vram-h100s">Will There Be Higher VRAM H100s? <span><a href="#will-there-be-higher-vram-h100s" aria-label="Anchor">#</a></span></h4>
<p>Maybe liquid cooled 120GB H100s.</p>
<h4 id="when-will-the-shortage-end">When Will The Shortage End? <span><a href="#when-will-the-shortage-end" aria-label="Anchor">#</a></span></h4>
<p>One group I talked with mentioned they are effectively sold out until the end of 2023.</p>
<h2 id="sourcing-h100s">Sourcing H100s <span><a href="#sourcing-h100s" aria-label="Anchor">#</a></span></h2>
<h3 id="who-sells-h100s">Who Sells H100s? <span><a href="#who-sells-h100s" aria-label="Anchor">#</a></span></h3>
<p>OEMs like Dell, HPE, Lenovo, Supermicro and Quanta sell H100s and HGX H100s.<sup id="fnref:30"><a href="#fn:30" role="doc-noteref">30</a></sup></p>
<p>And when you need InfiniBand, you’ll need to speak directly to Mellanox at Nvidia.<sup id="fnref:31"><a href="#fn:31" role="doc-noteref">31</a></sup></p>
<p>So GPU clouds like CoreWeave and Lambda buy from OEMs and then rent to startups.</p>
<p>Hyperscalers (Azure, GCP, AWS, Oracle) work more directly with Nvidia but they are generally also working with the OEMs as well.</p>
<p>And even for DGX you’ll still buy through an OEM. You can talk to Nvidia, but you’ll buy through an OEM. You won’t do a purchase order directly to Nvidia.</p>
<h4 id="how-are-the-lead-times">How Are The Lead Times? <span><a href="#how-are-the-lead-times" aria-label="Anchor">#</a></span></h4>
<p>Lead times on 8-GPU HGX servers are terrible, lead times on 4-GPU HGX servers are good. Everyone wants the 8-GPU servers!</p>
<h4 id="if-a-startup-places-an-order-today-when-would-they-have-ssh-access">If A Startup Places An Order Today, When Would They Have SSH Access? <span><a href="#if-a-startup-places-an-order-today-when-would-they-have-ssh-access" aria-label="Anchor">#</a></span></h4>
<p>It’d be a staggered deployment. Say it was a 5,000 GPU order. They might get access to 2,000 or 4,000 in 4-5 months and then the remaining by around 6 months total.</p>
<h4 id="do-startups-buy-from-oems-and-resellers">Do Startups Buy From OEMs And Resellers? <span><a href="#do-startups-buy-from-oems-and-resellers" aria-label="Anchor">#</a></span></h4>
<p>Not really. Startups will generally go to big clouds like Oracle to rent access, or to private clouds like Lambda and CoreWeave, or to providers that work with OEMs and data centers like FluidStack.</p>
<h4 id="when-do-startups-build-their-own-datacenter-vs-doing-colocation">When Do Startups Build Their Own Datacenter Vs Doing Colocation? <span><a href="#when-do-startups-build-their-own-datacenter-vs-doing-colocation" aria-label="Anchor">#</a></span></h4>
<p>For building a datacenter, the considerations are the time to build the datacenter, whether you have the people and experience in hardware, and that it’s capex expensive.</p>
<blockquote>
<p>Much easier to rent &amp; colo servers. If you want to build your own DC, you literally have to run a dark fiber line out to your location to connect to the internet - $10k per km. Most of this infra was already built &amp; paid for during dot-com boom. Now you can just rent it, quite cheap</p>
<p>– Private cloud exec</p>
</blockquote>
<p>The spectrum from rent to own is: on-demand cloud (pure rental using cloud services), reserved cloud, colo (buy the servers, work with a provider to host and manage the servers), self-hosting (buy and host the servers yourself).</p>
<p>Most startups needing large H100 quantities will do either reserved cloud or colo.</p>
<h3 id="how-do-the-big-clouds-compare">How Do The Big Clouds Compare? <span><a href="#how-do-the-big-clouds-compare" aria-label="Anchor">#</a></span></h3>
<p>The sentiment is that Oracle infrastructure is less reliable than the big 3 clouds. In exchange, Oracle gives more tech support help and time.</p>
<blockquote>
<p>100%. a big feeder of unhappy customers lol</p>
<p>– Private cloud exec</p>
</blockquote>
<blockquote>
<p>i think [oracle has] better networking though</p>
<p>– (Different) Private cloud exec</p>
</blockquote>
<p>Generally startups will pick whoever offers the best blend of support, price, and capacity.</p>
<p>The main big differences at the large clouds are:</p>
<ul>
<li>Networking (AWS and Google Cloud have been slower to adopt InfiniBand because they have their own approaches, though most startups looking for large A100/H100 clusters are seeking InfiniBand)</li>
<li>Availability (Azure’s H100s are mostly going to OpenAI. GCP is struggling to get H100s.)</li>
</ul>
<p>Nvidia seems to tend to give better allocations to clouds that aren’t building competing machine learning chips. (This is all speculation, not hard facts.) All of the big 3 clouds are working on machine learning chips, but the Nvidia-alternative offerings from AWS and Google are already available and taking dollars that might’ve gone to Nvidia.</p>
<blockquote>
<p>also speculation but i agree that nvidia likes oracle for this reason</p>
<p>– Private cloud exec</p>
</blockquote>
<p>Some big clouds have better pricing than others. As one private cloud exec noted, “a100s are much more expensive on aws/azure than gcp for instance.”</p>
<blockquote>
<p>oracle told me they have “10s of thousands of H100s” coming online later this year. they boasted about their special relationship with nvidia.</p>
<p>but… when it came to pricing, they were way higher than anyone else. they didn’t give me H100 pricing but for A100 80gb they quoted me close to $4/hour, which is nearly 2x more than gcp’s quote for the same hw and same commit.</p>
<p>– Anonymous</p>
</blockquote>
<p>The smaller clouds are better for pricing, except in some instances where the one of the big clouds does a weird deal in exchange for equity.</p>
<p>It might be something like: Oracle &amp; Azure &gt; GCP &amp; AWS in terms of Nvidia relationship. But that’s speculation.</p>
<p>Oracle was the <a href="https://www.oracle.com/news/announcement/nvidia-a100-oracle-cloud-infrastructure-092220/" target="_blank" rel="noreferrer noopener">first to launch A100s</a>, and they worked with Nvidia to <a href="https://www.oracle.com/news/announcement/nvidia-chooses-oracle-cloud-infrastructure-for-ai-services-2023-03-21/" target="_blank" rel="noreferrer noopener">host an NVIDIA-based cluster</a>. Nvidia is also a customer of <a href="https://azure.microsoft.com/en-us/blog/azure-previews-powerful-and-scalable-virtual-machine-series-to-accelerate-generative-ai/" target="_blank" rel="noreferrer noopener">Azure</a>.</p>
<h4 id="which-big-cloud-has-the-best-networking">Which Big Cloud Has The Best Networking? <span><a href="#which-big-cloud-has-the-best-networking" aria-label="Anchor">#</a></span></h4>
<p>Azure, CoreWeave and Lambda all use InfiniBand. Oracle has good networking, it is 3200 Gbps, but it’s ethernet rather than InfiniBand, which may be around 15-20% slower than IB for use cases like high-parameter count LLM training. AWS and GCP’s networking isn’t as good.</p>
<h4 id="which-big-clouds-do-enterprises-use">Which Big Clouds Do Enterprises Use? <span><a href="#which-big-clouds-do-enterprises-use" aria-label="Anchor">#</a></span></h4>
<p>In one private datapoint of about 15 enterprises, all 15 were either AWS, GCP or Azure, zero Oracle.</p>
<p>Most enterprises will stick with their existing cloud. Desperate startups will go wherever the supply is.</p>
<h4 id="how-about-dgx-cloud-who-is-nvidia-working-with-for-that">How About DGX Cloud, Who Is Nvidia Working With For That? <span><a href="#how-about-dgx-cloud-who-is-nvidia-working-with-for-that" aria-label="Anchor">#</a></span></h4>
<p>“NVIDIA is partnering with leading cloud service providers to host DGX Cloud infrastructure, starting with Oracle Cloud Infrastructure (OCI)” - you deal with Nvidia sales but you rent it through an existing cloud provider (first launching with Oracle, then Azure, then Google Cloud, not launching with AWS)<sup id="fnref:32"><a href="#fn:32" role="doc-noteref">32</a></sup> <sup id="fnref:33"><a href="#fn:33" role="doc-noteref">33</a></sup></p>
<p>Jensen said on the last earnings call: “The ideal mix is something like 10% Nvidia DGX Cloud and 90% the CSPs clouds”</p>
<h4 id="when-did-the-big-clouds-launch-their-h100-previews">When Did The Big Clouds Launch Their H100 Previews? <span><a href="#when-did-the-big-clouds-launch-their-h100-previews" aria-label="Anchor">#</a></span></h4>
<p>CoreWeave was first.<sup id="fnref:34"><a href="#fn:34" role="doc-noteref">34</a></sup> Nvidia gave them an earlier allocation, presumably to help strengthen competition (and because Nvidia is an investor) amongst large clouds.</p>
<p>Azure on March 13 announced that H100s were available for preview.<sup id="fnref:35"><a href="#fn:35" role="doc-noteref">35</a></sup></p>
<p>Oracle on March 21 announced that H100s were available in limited availability.<sup id="fnref:36"><a href="#fn:36" role="doc-noteref">36</a></sup></p>
<p>Lambda Labs on March 21 announced that H100s would be added in early April.<sup id="fnref:37"><a href="#fn:37" role="doc-noteref">37</a></sup></p>
<p>AWS on March 21 announced that H100s would be available for preview starting in a few weeks.<sup id="fnref:38"><a href="#fn:38" role="doc-noteref">38</a></sup></p>
<p>Google Cloud on May 10 announced the start of a private preview for H100s.<sup id="fnref:39"><a href="#fn:39" role="doc-noteref">39</a></sup></p>
<h4 id="which-companies-use-which-clouds">Which Companies Use Which Clouds? <span><a href="#which-companies-use-which-clouds" aria-label="Anchor">#</a></span></h4>
<ul>
<li>OpenAI: Azure.</li>
<li>Inflection: Azure and CoreWeave.</li>
<li>Anthropic: AWS and Google Cloud.</li>
<li>Cohere: AWS.</li>
<li>Hugging Face: AWS.</li>
<li>Stability AI: CoreWeave and AWS.</li>
<li>Character.ai: Google Cloud.</li>
<li>X.ai: Oracle.</li>
<li>Nvidia: Azure.<sup id="fnref1:35"><a href="#fn:35" role="doc-noteref">35</a></sup></li>
</ul>
<h3 id="how-can-a-company-or-cloud-service-provider-get-more-gpus">How Can A Company Or Cloud Service Provider Get More GPUs? <span><a href="#how-can-a-company-or-cloud-service-provider-get-more-gpus" aria-label="Anchor">#</a></span></h3>
<p>The ultimate bottleneck is getting allocation from Nvidia.</p>
<h4 id="how-do-nvidia-allocations-work">How Do Nvidia Allocations Work? <span><a href="#how-do-nvidia-allocations-work" aria-label="Anchor">#</a></span></h4>
<p>They have an allocation they give per customer. But for example, Azure saying “hey we would like 10,000 H100s all to be used by Inflection” is different from Azure saying “hey we would like 10,000 H100s for Azure’s cloud” - Nvidia cares about who the end customer is, and so clouds might be able to get an extra allocation for a specific end customer if Nvidia is excited about the end customer. Nvidia also wants to know who that end customer is, as much as possible. And they prefer customers with nice brand names or startups with strong pedigrees.</p>
<blockquote>
<p>Yes, this seems to be the case. NVIDIA likes to guarantee GPU access to rising AI companies (many of which they have a close relationship with). See Inflection — an AI company they invested in — testing a huge H100 cluster on CoreWeave, which they also invested in</p>
<p>– Private cloud exec</p>
</blockquote>
<p>If a cloud brings Nvidia an end customer and says they’re ready to purchase xxxx H100s, if Nvidia is excited about that end customer they’ll generally give an allocation, which effectively boosts the total capacity allocated by Nvidia to that cloud - because it won’t count against the original allocation that Nvidia gave to that cloud.</p>
<p>It’s a unique situation in that Nvidia is giving large allocations to private clouds: CoreWeave has more H100s than GCP.</p>
<p>Nvidia would prefer not to give large allocations to companies that are attempting to compete directly with them (AWS Inferentia and Tranium, Google TPUs, Azure Project Athena).</p>
<p>But ultimately, if you put the purchase order and money in front of Nvidia, committing to a bigger deal and more money up front and show that you have a low risk profile, then you’ll get a larger allocation than others get.</p>
<!-- ## Open questions

When will we see dozens of enterprises (not the companies training the LLMs, I mean the companies using them) using LLMs in a way that gets them increased profits? When will we see dozens of enterprises using LLMs in a way that gets them increased profits, and more satisfied end users? How will those timelines differ for closed LLMs vs open source LLMs? -->
<h2 id="closing-thoughts">Closing Thoughts <span><a href="#closing-thoughts" aria-label="Anchor">#</a></span></h2>
<p>For now, we are GPU-limited. Even if we are at the “end of the era where it’s going to be these giant models” as Sam Altman has said.</p>
<p>It’s both bubble-ish and not-bubble-ish depending on where you look. Some companies like OpenAI have products like ChatGPT with intense product-market-fit, and can’t get enough GPUs. Other companies are buying or reserving GPU capacity so they’ll have access in the future, or to train LLMs that are much less likely to have product-market-fit.</p>
<p>Nvidia is the green king of the castle right now.</p>
<h3 id="tracing-the-journey-of-gpu-supply-and-demand">Tracing The Journey Of GPU Supply And Demand <span><a href="#tracing-the-journey-of-gpu-supply-and-demand" aria-label="Anchor">#</a></span></h3>
<p>The LLM product with the strongest product-market fit is ChatGPT. Here’s the story of GPU demand with respect to ChatGPT:</p>
<ol>
<li>Users love ChatGPT. It’s probably making $500mm++ annual recurring revenue.</li>
<li>ChatGPT runs on the GPT-4 and GPT-3.5 APIs.</li>
<li>The GPT-4 and GPT-3.5 APIs need GPUs to run. Lots of them. And OpenAI wants to release more features for ChatGPT and their APIs, but they can’t, because they don’t have access to enough GPUs.</li>
<li>They buy lots of Nvidia GPUs through Microsoft/Azure. Specifically the GPU they want most is the Nvidia H100 GPU.</li>
<li>To make H100 SXM GPUs, Nvidia uses TSMC for fabrication and uses TSMC’s CoWoS packaging tech and uses HBM3 primarily from SK Hynix.</li>
</ol>
<p>OpenAI isn’t the only company that wants GPUs (but they are the company with the strongest product-market-fit that wants GPUs). Other companies are also wanting to train large AI models. Some of these use cases will make sense, but some are more hype driven and unlikely to get product-market-fit. This is pushing up demand. Also, some companies are concerned about not being able to access GPUs in the future so they’re placing their orders now even when they don’t need them yet. So there’s a bit of “expectations of supply shortages create even more supply shortages” going on.</p>
<p>The other major contributor to GPU demand is from companies that want to create new LLMs. Here’s the story of GPU demand with respect to companies wanting to build new LLMs:</p>
<ol>
<li>A company executive or founder knows there’s big opportunities in the AI space. Maybe they’re an enterprise that wants to train an LLM on their own data and use it externally or sell access, or maybe they’re a startup that wants to build an LLM and sell access.</li>
<li>They know they need GPUs to train large models.</li>
<li>They talk with some set of people from the big clouds (Azure, Google Cloud, AWS) to try and get many H100s.</li>
<li>They find out that they can’t get a big allocation from the big clouds, and that some of the big clouds don’t have good networking setups. So they go and talk with other providers like CoreWeave, Oracle, Lambda, FluidStack. If they want to buy the GPUs themselves and own them, maybe they also talk with OEMs and Nvidia.</li>
<li>Eventually, they acquire a lot of GPUs.</li>
<li>Now, they try and get product-market-fit.</li>
<li>In case it’s not obvious, this pathway isn’t as good - remember that OpenAI got product-market-fit on much smaller models and then scaled them up. But, now to get product-market-fit you have to be better than OpenAI’s models for your users’ use-cases, so to start you will need more GPUs than OpenAI started with.</li>
</ol>
<p>Expect H100 shortages for multi-hundred or multi-thousand deployments through the end of 2023 at least. At the end of 2023 the picture will be clearer, but for now it looks like the shortages may persist through some of 2024 as well.</p>
<figure>
      <img srcset="/images/gpu-journey-small_hubc5d31a0ba9b346a37f48de655885693_361966_330x0_resize_q75_box.jpg 330w,/images/gpu-journey-small_hubc5d31a0ba9b346a37f48de655885693_361966_660x0_resize_q75_box.jpg 660w,/images/gpu-journey-small_hubc5d31a0ba9b346a37f48de655885693_361966_1024x0_resize_q75_box.jpg 1024w,/images/gpu-journey-small_hubc5d31a0ba9b346a37f48de655885693_361966_1320x0_resize_q75_box.jpg 2x" src="https://gpus.llm-utils.org/images/gpu-journey-small_hubc5d31a0ba9b346a37f48de655885693_361966_660x0_resize_q75_box.jpg" alt="The GPU Journey" loading="lazy"/>
      <figcaption>The Journey of GPU Supply and Demand. <a href="https://gpus.llm-utils.org/static-images/gpu-journey-large.jpg">Larger version</a></figcaption>
    </figure>
  

<h3 id="getting-in-touch">Getting In Touch <span><a href="#getting-in-touch" aria-label="Anchor">#</a></span></h3>
<!-- Contact info for questions, notes, new posts and Discord: -->
<!-- If you need a large amount of H100s, feel free to let [me](mailto:clay@studionumberzero.com) know and we may be able to make some helpful intros. -->
<p>Questions and notes can be sent in via <a href="https://gpus.llm-utils.org/cdn-cgi/l/email-protection#e98a858890a99a9d9c8d8086879c848b8c9b938c9b86c78a8684">email</a>. Also if you can offer helpful comments on any of these topics, please send me an email: the deal structures of large CSP investments in AI startups, the financing structures of large H100 purchases, and the economics at each layer of the stack (the ones discussed in this post, plus other layers including colo providers, invididual GPU hosts, electricity and so on). If you’d like to help, the most helpful thing would be to email and offer interesting conversations either with you or someone you could intro me to. I’d like to write more about interesting things related to GPUs, LLM startups, financing, public and private equities, colocation and so on.</p>
<!-- New posts: [get notified about new posts via email](https://airtable.com/shr411VWRbl9og1xb). -->
<!-- Someone I work with set up a discord server to chat all things GPUs - 'gpugang' - [here's the link](https://discord.gg/3zCM9meP). It's brand new and we plan to close it after around 20 users. -->
<h3 id="the-natural-next-question---what-about-nvidia-alternatives">The Natural Next Question - What About Nvidia Alternatives? <span><a href="#the-natural-next-question---what-about-nvidia-alternatives" aria-label="Anchor">#</a></span></h3>
<p>The natural next question is “ok, what about the competition and alternatives?” I’d like to do a post exploring hardware alternatives as well as software approaches. Submit things I should explore as alternatives to <a href="https://airtable.com/appOhBgC3S2sN8aE2/shrvsLIxFv7zdf1sT" target="_blank" rel="noreferrer noopener">this form</a>. For example, TPUs, Inferentia, LLM ASICs and others on the hardware side, and Mojo, Triton and others on the software side, and what it looks like to use AMD hardware and software. I’d like to explore things that are in development, but to emphasize hardware and software that is actually usable by customers today.</p>
<h2 id="acknowledgements">Acknowledgements <span><a href="#acknowledgements" aria-label="Anchor">#</a></span></h2>
<p><em>This article contains a decent amount of proprietary and previously unpublished information. When you see people wondering about GPU production capacity, please point them in the direction of this post.</em></p>
<p>Thanks to a handful of execs and founders at private GPU cloud companies, a few AI founders, an ML engineer, a deep learning researcher, a few other industry experts, and some non-industry readers, for providing helpful comments. Thanks to Hamid for illustrations.</p>


      </div>
    </section>
    
  </article></div>
  </body>
</html>
