<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/">Original</a>
    <h1>Linux Kernel vs. DPDK: HTTP Performance Showdown</h1>
    
    <div id="readability-page-1" class="page"><div data-v-d847899c=""><h2 id="overview"><a href="#overview" aria-hidden="true">#</a> Overview</h2><p><!-- start_excerpt -->In this post I will use a simple HTTP benchmark to do a head-to-head performance comparison between the Linux kernel&#39;s network stack, and a <i>kernel-bypass</i> stack powered by DPDK.<!-- end_excerpt --> I will run my tests using <a href="http://seastar.io/" target="_blank" rel="noopener noreferrer">Seastar</a>, a C++ framework for building high-performance server applications. Seastar has support for building apps that use either the Linux kernel or DPDK for networking, so it is the perfect framework for this comparison.</p><p>I will build on a lot of the ideas and techniques from <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/" target="_blank" rel="noopener noreferrer">my previous performance tuning post</a> so it might be worth it to at least read the <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#overview" target="_blank" rel="noopener noreferrer">overview section</a> before continuing.</p><h3 id="in-defense-of-the-kernel"><a href="#in-defense-of-the-kernel" aria-hidden="true">#</a> In Defense of the Kernel</h3><p>Bypassing the kernel can open up a whole new world of high-throughput and low latency. Depending on who you ask, you might hear that bypassing the kernel will result in a 3-5x performance improvement. However, most of those comparisons are done without much optimization on the kernel side.</p><p>The Linux kernel is designed to be fast, but it is also designed to be multi-purpose, so it isn&#39;t <em>perfectly</em> optimized for high-speed networking by default. On the other hand, kernel-bypass technologies like DPDK take a single-minded approach to networking performance. An entire network interface is dedicated to a single application, and aggressive busy polling is used to achieve high throughput and low latency. For this post I wanted to see what the performance gap would look like when a finely tuned kernel/application goes head to head with kernel-bypass in a no holds barred fight.</p><p>DPDK advocates suggest that bypassing the kernel is necessary because the kernel is &#34;slow&#34;, but in reality a lot of DPDK&#39;s performance advantages come <strong>not</strong> from bypassing the kernel, but from enforcing certain constraints. As it turns out, many of these advantages can be achieved while still using the kernel. By turning off some features, turning on others, and tuning the application accordingly, one can achieve performance that <em>approaches</em> kernel-bypass speeds.</p><p>Here are a few DPDK strategies that can also be accomplished using the kernel:</p><ul><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#busy-polling" target="_blank" rel="noopener noreferrer">Busy polling</a> (Interrupt moderation + net.core.busy_poll=1)</li><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_5-perfect-locality" target="_blank" rel="noopener noreferrer">Perfect locality</a> (RSS + XPS + SO_REUSEPORT_CBPF)</li><li>Simplified TCP/IP subsystem (Disable iptables/syscall auditing/AF_PACKET sockets)</li></ul><p>One advantage that kernel-bypass technologies still have, is that they avoid the syscall overheads that arise from transitioning (and copying data) back and forth between user-land and the kernel. So DPDK should still have the overall advantage, but the question is, how much of an advantage.</p><h3 id="disclaimer"><a href="#disclaimer" aria-hidden="true">#</a> Disclaimer</h3><p>Work on this exploratory project was sponsored by the folks over at <a href="https://www.scylladb.com/" target="_blank" rel="noopener noreferrer">ScyllaDB, Inc</a>, the primary stewards of the open-source Seastar framework, and organizers of <a href="https://www.p99conf.io/" target="_blank" rel="noopener noreferrer">P99 CONF</a>. After I spoke at P99 CONF last year, they contacted me to see if there were any areas of mutual interest that we could explore. My last experiment made me curious about doing a kernel vs DPDK showdown, and Seastar fit the bill perfectly, so this post is the result of that engagement. All technical discussions took place on their public Slack channel and mailing list.</p><h2 id="roadmap"><a href="#roadmap" aria-hidden="true">#</a> Roadmap</h2><p>This post is pretty long, so here is a high-level outline in case you want to jump to a particular area of interest.</p><h4 id="getting-started"><a href="#getting-started" aria-hidden="true">#</a> Getting Started</h4><ul><li><a href="#building-seastar">Building Seastar</a></li><li><a href="#benchmark-setup">Benchmark Setup</a></li></ul><h4 id="dpdk-setup-and-optimizations"><a href="#dpdk-setup-and-optimizations" aria-hidden="true">#</a> DPDK Setup and Optimizations</h4><ul><li><a href="#dpdk-on-aws">DPDK on AWS</a></li><li><a href="#dpdk-optimization">DPDK Optimization</a></li></ul><h4 id="kernel-stack-optimizations"><a href="#kernel-stack-optimizations" aria-hidden="true">#</a> Kernel Stack Optimizations</h4><ul><li><a href="#baseline-kernel-performance">Baseline Kernel Performance</a></li><li><a href="#os-level-optimizations">OS Level Optimizations</a></li><li><a href="#perfect-locality-and-busy-polling">Perfect Locality and Busy Polling</a> (It took several tries to get this working)</li><li><a href="#constant-context-switching">Constant Context Switching</a></li><li><a href="#it-is-better-to-recv">It is better to RECV</a></li><li><a href="#remember-to-flush">Remember to flush</a></li></ul><h4 id="results-caveats-and-curiosities"><a href="#results-caveats-and-curiosities" aria-hidden="true">#</a> Results, Caveats, and Curiosities</h4><ul><li><a href="#and-the-winner-is">And the winner is...</a></li><li><a href="#dpdk-caveats">DPDK Caveats</a></li><li><a href="#speculative-execution-mitigations">Speculative Execution Mitigations</a></li></ul><h4 id="the-end"><a href="#the-end" aria-hidden="true">#</a> The End</h4><ul><li><a href="#conclusion">Conclusion</a></li><li><a href="#appendix">Appendix</a></li></ul><p>Clicking the <strong>menu icon at the top right will open a table of contents</strong> so that you can easily jump to a specific section.</p><h2 id="building-seastar"><a href="#building-seastar" aria-hidden="true">#</a> Building Seastar</h2><p>I had a bit of a challenge getting Seastar built initially. I wanted to use Amazon Linux 2 since I am pretty familiar with it, but it became clear that I was fighting a losing battle with outdated dependencies. I switched to vanilla CentOS 8 and managed to get it running despite <a href="https://github.com/scylladb/seastar/issues/995" target="_blank" rel="noopener noreferrer">a few issues</a>, but I still didn&#39;t feel like I was on solid enough ground. After a brief dalliance with CentOS Stream 9, I asked for help in the public Slack channel, and I was pointed in the direction of Fedora 34 as the best OS for building the most recent version of the codebase.</p><p>I actually did most of my research and testing using Fedora 34 (kernel 5.15), but while Fedora may be great for its cutting-edge updates, sometimes the cutting edge becomes the bleeding edge. By the time I decided to start reproducing my results from scratch, I realized that the latest Fedora 34 updates were upgrading the kernel from 5.11 straight to version 5.16. Unfortunately kernel 5.16 triggered performance regression for my tests, so I needed an alternative.</p><p>As it turns out, <a href="https://aws.amazon.com/linux/amazon-linux-2022/" target="_blank" rel="noopener noreferrer">Amazon Linux 2022</a> is based on Fedora 34, but has a more conservative kernel update policy, opting to stick with the 5.15 LTS release, so I chose AL 2022 as the new base OS for these tests, and in a bit of revisionist history, for the rest of the post I will just pretend like I was using it the whole time.</p><h3 id="http-server"><a href="#http-server" aria-hidden="true">#</a> HTTP Server</h3><p>I started out using Seastar&#39;s built-in HTTP Server (<a href="https://github.com/talawahtech/seastar/tree/http-performance/apps/httpd" target="_blank" rel="noopener noreferrer">httpd</a>) for my tests but I decided to go a level down from httpd to using a bare-bones TCP server that just <strong>pretends</strong> to be an HTTP server. The server just sends back a fixed HTTP response without doing any parsing or routing. This simplifies my analysis and highlights the effect of each change that I make more clearly. In particular I wanted to eliminate Seastar&#39;s built-in HTTP parser from the equation. Before I removed it, performance would vary significantly just based on how many HTTP headers the client sent. So rather than go down the rabbit hole of figuring out what was going on there, I decided to just cheat by using my simple <a href="https://github.com/talawahtech/seastar/tree/http-performance/apps/tcp_httpd" target="_blank" rel="noopener noreferrer">tcp_httpd</a> server instead.</p><!--
`twrk -H 'Host: server.tfb' -H 'Accept: application/json,text/html;q=0.9,application/xhtml+xml;q=0.9,application/xml;q=0.8,*/*;q=0.7' -H 'Connection: keep-alive' --latency --pin-cpus "http://172.31.4.185:8080/" -t 16 -c 256 -D 1 -d 5`
```
    Running 5s test @ http://172.31.4.185:8080/
      16 threads and 256 connections
      Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
        Latency     0.95ms  225.98us    1.87ms   73.00us   71.19%
        Req/Sec    16.88k   138.24     17.35k    16.41k    67.98%
      Latency Distribution
      50.00%    0.99ms
      90.00%    1.20ms
      99.00%    1.40ms
      99.99%    1.58ms
      1343709 requests in 5.00s, 175.56MB read
    Requests/sec: 268737.98
```

`twrk --latency --pin-cpus "http://172.31.4.185:8080/" -t 16 -c 256 -D 1 -d 5`
```
    Running 5s test @ http://172.31.4.185:8080/
      16 threads and 256 connections
      Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
        Latency   544.81us  113.73us    1.28ms   77.00us   79.02%
        Req/Sec    29.26k   220.93     29.81k    28.59k    66.58%
      Latency Distribution
      50.00%  534.00us
      90.00%  716.00us
      99.00%  847.00us
      99.99%    1.07ms
      2328886 requests in 5.00s, 304.28MB read
    Requests/sec: 465770.03
```
--><h3 id="source-code"><a href="#source-code" aria-hidden="true">#</a> Source Code</h3><p>I opened a few PRs on the main Seastar repo based on the work I did for this project, but most of the changes aren&#39;t suitable for upstreaming given that they depend on epoll, and current development is now focused around aio and io_uring. All the patches used in this post are available in <a href="https://github.com/talawahtech/seastar/tree/http-performance/" target="_blank" rel="noopener noreferrer">my Seastar repo</a> on GitHub.</p><h2 id="benchmark-setup"><a href="#benchmark-setup" aria-hidden="true">#</a> Benchmark Setup</h2><p>This is a basic overview of the benchmark setup on AWS. I used the <a href="https://www.techempower.com/benchmarks/#section=intro" target="_blank" rel="noopener noreferrer">Techempower</a> <a href="https://github.com/TechEmpower/FrameworkBenchmarks/wiki/Project-Information-Framework-Tests-Overview#json-serialization" target="_blank" rel="noopener noreferrer">JSON Serialization test</a> as the reference benchmark for this experiment.</p><h3 id="hardware"><a href="#hardware" aria-hidden="true">#</a> Hardware</h3><ul><li><strong>Server</strong>: 4 vCPU c5n.xlarge instance</li><li><strong>Client</strong>: 16 vCPU c5n.4xlarge instance (the client becomes the bottleneck if I try to use a smaller instance size)</li><li><strong>Network</strong>: Server and client located in the same availability zone (use2-az1) in a <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" target="_blank" rel="noopener noreferrer">cluster placement group</a></li></ul><h3 id="software"><a href="#software" aria-hidden="true">#</a> Software</h3><ul><li><strong>Operating System</strong>: Amazon Linux 2022 (kernel 5.15)</li><li><strong>Server</strong>: My simple <a href="https://github.com/talawahtech/seastar/tree/http-performance/apps/tcp_httpd" target="_blank" rel="noopener noreferrer">tcp_httpd</a> server: <code>sudo ./tcp_httpd --reactor-backend epoll </code></li><li><strong>Client</strong>: I made a few modifications to <a href="https://github.com/wg/wrk" target="_blank" rel="noopener noreferrer">wrk</a>, the popular HTTP benchmarking tool, and nicknamed it <a href="https://github.com/talawahtech/wrk/commits/twrk" target="_blank" rel="noopener noreferrer">twrk</a>. twrk delivers more consistent results on short, low latency test runs. The standard version of wrk should yield similar numbers in terms of throughput, but twrk allows for improved <a href="https://engineering.linkedin.com/performance/who-moved-my-99th-percentile-latency" target="_blank" rel="noopener noreferrer">p99 latencies</a>, and adds support for displaying p99.99 latency.</li></ul><h3 id="benchmark-configuration"><a href="#benchmark-configuration" aria-hidden="true">#</a> Benchmark Configuration</h3><p>I ran twrk manually from the client using the following parameters:</p><ul><li>No pipelining</li><li>256 connections</li><li>16 threads (1 per vCPU), with each thread pinned to a vCPU</li><li>1 second warmup before stats collection starts, then the test runs for 5s</li></ul><div><pre><code>twrk --latency --pin-cpus -H &#39;Host: server.tld&#39; &#34;http://172.31.XX.XX:8080/json&#34; -t 16 -c 256 -D 1 -d 5
</code></pre></div><h2 id="dpdk-on-aws"><a href="#dpdk-on-aws" aria-hidden="true">#</a> DPDK on AWS</h2><p>Getting Seastar and DPDK working on AWS was no walk in the park. The <a href="https://github.com/amzn/amzn-drivers/tree/master/userspace/dpdk" target="_blank" rel="noopener noreferrer">DPDK documentation for the AWS ENA driver</a> has improved significantly in recent times, but it was a little bit rougher when I started, and it was difficult to find working examples of using Seastar with DPDK. Thankfully, between assistance on the Slack channel and my stubborn persistence I was able to get things running.</p><p>Here are some of the highlights for those looking to do the same:</p><ol><li><p>DPDK needs to be able to take over an entire network interface, so in addition to the primary interface for connecting to the instance via SSH (eth0/ens5), you will also need to attach a secondary interface dedicated to DPDK (eth1/ens6).</p></li><li><p>DPDK relies on one of two available kernel frameworks for exposing direct device access to user-land, <a href="https://www.kernel.org/doc/html/v5.15/driver-api/vfio.html" target="_blank" rel="noopener noreferrer">VFIO</a> or <a href="https://www.kernel.org/doc/html/v5.15/driver-api/uio-howto.html" target="_blank" rel="noopener noreferrer">UIO</a>. VFIO is the recommended choice, and it is available by default on recent kernels. By default, VFIO depends on hardware IOMMU support to ensure that direct memory access happens in a secure way, however IOMMU support is only available for *.metal EC2 instances. For non-metal instances, VFIO supports running without IOMMU by setting <code>enable_unsafe_noiommu_mode=1</code> when loading the kernel module.</p></li><li><p>Seastar uses DPDK 19.05, which is a little outdated at this point. The AWS ENA driver has a set of <a href="https://github.com/amzn/amzn-drivers/tree/ena_linux_2.6.1/userspace/dpdk/19.05" target="_blank" rel="noopener noreferrer">patches for DPDK 19.05</a> which must be applied to get Seastar running on AWS. I backported the patches to my <a href="https://github.com/talawahtech/dpdk/commits/http-performance" target="_blank" rel="noopener noreferrer">DPDK fork</a> for convenience.</p></li><li><p>Last but not least, I encountered a bug in the DPDK/ENA driver that resulted in the following error message: <code>runtime error: ena_queue_start(): Failed to populate rx ring</code>. This issue was <a href="https://github.com/DPDK/dpdk/commit/30a6c7ef4054" target="_blank" rel="noopener noreferrer">fixed in the DPDK codebase</a> last year so I <a href="https://github.com/talawahtech/dpdk/commit/0992a5c89a3f2052abaaa76b9e026c7edfbd1e2c" target="_blank" rel="noopener noreferrer">backported the change</a> to my DPDK fork.</p></li></ol><p>Using the tcp_httpd app, I ran my benchmark with DPDK as the underlying network stack: <code>sudo ./tcp_httpd --network-stack native --dpdk-pmd</code></p><div><pre><code>Running 5s test @ http://172.31.12.71:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   205.32us   36.57us    1.34ms   62.00us   69.36%
    Req/Sec    74.80k     1.81k    77.85k    69.06k    73.85%
  Latency Distribution
  50.00%  204.00us
  90.00%  252.00us
  99.00%  297.00us
  99.99%  403.00us
  5954189 requests in 5.00s, 0.86GB read
Requests/sec: 1190822.80
</code></pre></div><p>DPDK performance clocks in at an impressive 1.19M req/s right out of the gate.</p><h3 id="initial-flame-graph"><a href="#initial-flame-graph" aria-hidden="true">#</a> Initial Flame Graph</h3><p><a href="http://www.brendangregg.com/flamegraphs.html" target="_blank" rel="noopener noreferrer">Flame Graphs</a> provide a unique way to visualize CPU usage and identify your application&#39;s most frequently used code-paths. They are a powerful optimization tool, as they allow you to quickly identify and eliminate bottlenecks. Clicking the image below will open the original SVG file that was generated by the <a href="https://github.com/brendangregg/FlameGraph" target="_blank" rel="noopener noreferrer">Flamegraph tool</a>. <strong>These SVGs are interactive</strong>. You can click a segment to drill down for a more detailed view, or you can search (Ctrl + F or click the link at the top right) for a function name. Note that each <a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-initial-app-unoptimized-os.svg" target="_blank">complete flame graph</a> captures four near-identical stacks representing the 4 reactor threads (one per vCPU), but throughout the post we will mostly focus on analyzing the data for a single reactor/vCPU.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-dpdk-no-wc.svg?x=1198.8&amp;y=357" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-dpdk-no-wc.png" alt="Flame graph - DPDK without write combining" loading="lazy" width="800" height="211"/></a></p><h3 id="flame-graph-analysis"><a href="#flame-graph-analysis" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>A quick look at the flame graph is enough to see that the <code>eth_ena_xmit_pkts</code> function looks suspiciously large, weighing in at 53.1% of the total flame graph.</p><h2 id="dpdk-optimization"><a href="#dpdk-optimization" aria-hidden="true">#</a> DPDK Optimization</h2><p>On 5th+ generation instances the ENA hardware/driver supports a LLQ (Low Latency Queue) mode for improved performance. When using these instances, it is strongly recommended that you enable the write combining feature of the respective kernel module (VFIO or UIO), otherwise, performance will suffer due to slow PCI transactions.</p><p>The VFIO module doesn&#39;t support write combining by default, but the ENA team provides <a href="https://github.com/amzn/amzn-drivers/tree/master/userspace/dpdk#622-vfio-pci" target="_blank" rel="noopener noreferrer">a patch and a script</a> to automate the process of adding WC support to the kernel module. I originally had a <a href="https://github.com/amzn/amzn-drivers/issues/200" target="_blank" rel="noopener noreferrer">couple</a> <a href="https://github.com/amzn/amzn-drivers/issues/202" target="_blank" rel="noopener noreferrer">issues</a> getting it working with kernel 5.15 but the ENA team was pretty responsive about getting them fixed. The team also recently indicated they intend to <a href="https://github.com/amzn/amzn-drivers/issues/212#issuecomment-1110290752" target="_blank" rel="noopener noreferrer">upstream the VFIO patch</a> which will hopefully make things even more painless in the future.</p><div><pre><code>Running 5s test @ http://172.31.12.71:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   153.79us   31.63us    1.43ms   52.00us   68.70%
    Req/Sec    95.18k     2.31k   100.94k    89.75k    68.88%
  Latency Distribution
  50.00%  152.00us
  90.00%  195.00us
  99.00%  233.00us
  99.99%  352.00us
  7575198 requests in 5.00s, 1.09GB read
Requests/sec: 1515010.51
</code></pre></div><p>Enabling write combining brings performance from 1.19M req/s to 1.51M req/s, a 27% performance increase.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-dpdk-wc.svg?x=1200.4&amp;y=341" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-dpdk-wc.png" alt="Flame graph - DPDK with write combining" loading="lazy" width="800" height="203"/></a></p><h3 id="flame-graph-analysis-2"><a href="#flame-graph-analysis-2" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>Our flame graph now looks a lot more balanced and <code>eth_ena_xmit_pkts</code> has dropped from 53.1% of the flame graph to a mere 6.1%.</p><h3 id="a-tall-order"><a href="#a-tall-order" aria-hidden="true">#</a> A Tall Order</h3><p>DPDK has thrown down the gauntlet with an absolutely massive showing. <strong>1.51M</strong> requests per second on a <strong>4 vCPU</strong> instance is HUGE. Can the kernel even get close?</p><h2 id="baseline-kernel-performance"><a href="#baseline-kernel-performance" aria-hidden="true">#</a> Baseline Kernel Performance</h2><p>Starting with an unmodified AL 2022 AMI, <code>tcp_httpd</code> performance starts out at around 358k req/s. In absolute terms this is really, really fast, but it is underwhelming by comparison.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   711.06us   97.91us    1.65ms  108.00us   70.06%
    Req/Sec    22.48k   205.46     23.10k    21.83k    68.62%
  Latency Distribution
  50.00%  696.00us
  90.00%    0.85ms
  99.00%    0.96ms
  99.99%    1.10ms
  1789658 requests in 5.00s, 264.55MB read
Requests/sec: 357927.16
</code></pre></div><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-initial-app-unoptimized-os.svg?x=1179.9&amp;y=853" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-initial-app-unoptimized-os.png" alt="Flame graph - Initial" loading="lazy" width="800" height="459"/></a></p><h2 id="os-level-optimizations"><a href="#os-level-optimizations" aria-hidden="true">#</a> OS Level Optimizations</h2><p>I won&#39;t go into a lot of detail about the specific Linux changes that I made. At a high level, the changes are very similar in nature to the tweaks that I made to Amazon Linux 2/kernel 4.14 in my <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/" target="_blank" rel="noopener noreferrer">previous post</a>. That being said, there was actually a significant performance regression for this workload going from kernel 4.14 and 5.15, and it took quite a bit of work to get performance back up to par. But I want to stay focused on the kernel vs DPDK comparison for now, so I will save those details for another day, and another post. Here is a high-level overview of the OS optimizations used:</p><ul><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_2-speculative-execution-mitigations" target="_blank" rel="noopener noreferrer">Disable Speculative Execution Mitigations</a></li><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#receive-side-scaling-rss" target="_blank" rel="noopener noreferrer">Configure RSS and XPS for perfect locality</a></li><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_6-interrupt-optimizations" target="_blank" rel="noopener noreferrer">Interrupt Moderation and Busy Polling</a></li><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_7-the-case-of-the-nosy-neighbor" target="_blank" rel="noopener noreferrer">Disable Raw/Packet Sockets</a> (FYI it wasn&#39;t quite the same nosy neighbor this time around)</li><li><a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_9-this-goes-to-twelve" target="_blank" rel="noopener noreferrer">GRO, Congestion Control, and Static Interrupt Moderation</a></li><li>A handful of new optimizations</li></ul><p>Our OS optimizations took throughput from 358k req/s to a whopping 726k req/s. A solid 103% performance improvement.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   346.76us   86.26us    1.51ms   62.00us   72.62%
    Req/Sec    45.61k     0.88k    48.82k    42.50k    70.15%
  Latency Distribution
  50.00%  347.00us
  90.00%  455.00us
  99.00%  564.00us
  99.99%  758.00us
  3630818 requests in 5.00s, 536.71MB read
Requests/sec: 726153.58
</code></pre></div><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-initial-app-optimized-os.svg?x=1180.7&amp;y=837" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-initial-app-optimized-os.png" alt="Flame graph - OS Optimizations" loading="lazy" width="800" height="451"/></a></p><h2 id="perfect-locality-and-busy-polling"><a href="#perfect-locality-and-busy-polling" aria-hidden="true">#</a> Perfect Locality and Busy Polling</h2><p>The OS level changes to enable perfect locality/busy polling don&#39;t really have much effect until the application is properly configured as well. My next step was to add <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#so-attach-reuseport-cbpf" target="_blank" rel="noopener noreferrer">SO_ATTACH_REUSEPORT_CBPF</a> support <a href="https://github.com/talawahtech/seastar/commit/80a0ccaa4aaa87f5f7f87a7c56aa2605ef9f177c" target="_blank" rel="noopener noreferrer">to my Seastar fork</a> so that the perfect locality setup would be complete.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   338.93us   90.62us    1.56ms   61.00us   68.11%
    Req/Sec    46.57k     2.67k    54.00k    40.32k    64.29%
  Latency Distribution
  50.00%  330.00us
  90.00%  466.00us
  99.00%  562.00us
  99.99%  759.00us
  3706485 requests in 5.00s, 547.89MB read
Requests/sec: 741286.62
</code></pre></div><p>Throughput moved from 736k req/s to an underwhelming 741k req/s. A 2% performance bump was <strong>way</strong> below my expectations for this change.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-enable-so_attach_reuse_port_cbpf.svg?x=1185.7&amp;y=837" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-enable-so_attach_reuse_port_cbpf.png" alt="Flame graph - Enable SO_ATTACH_REUSEPORT_CBPF" loading="lazy" width="800" height="451"/></a></p><h3 id="flame-graph-analysis-3"><a href="#flame-graph-analysis-3" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>The flame graph shows zero evidence of busy polling. Perfect locality and busy polling work together in a virtuous cycle, so the lack of busy polling is a strong indicator that something is wrong with our setup. Perfect locality requires the OS and application to be configured so that once a network packet arrives on a given queue, all further processing is handled by the same vCPU/queue silo for both incoming and outgoing data. This means the order in which processes/threads are started, and the CPUs to which they are pinned must be controlled.</p><h3 id="perfect-locality-and-busy-polling-take-two"><a href="#perfect-locality-and-busy-polling-take-two" aria-hidden="true">#</a> Perfect Locality and Busy Polling: Take two</h3><p>I created a <a href="#">bftrace script</a> to take a closer look at what was actually going on. The script attaches kprobes to <a href="https://github.com/torvalds/linux/blob/v5.15/net/core/sock_reuseport.c#L105" target="_blank" rel="noopener noreferrer"><code>reuseport_alloc()</code></a> and <a href="https://github.com/torvalds/linux/blob/v5.15/net/core/sock_reuseport.c#L228" target="_blank" rel="noopener noreferrer"><code>reuseport_add_sock()</code></a> to track the process/thread startup order and cpu affinity. The results immediately showed the problem. Even though the reactor threads are started sequentially (tcp_httpd/reactor-0, reactor-1, reactor-2, reactor-3), the CPU pinning is out of order (0, 2, 1, 3).</p><div><pre><code>tcp_httpd, cpu=0, socket 0
reactor-1, cpu=2, socket 1
reactor-2, cpu=1, socket 2
reactor-3, cpu=3, socket 3
</code></pre></div><p>Further investigations revealed that Seastar uses <a href="https://linux.die.net/man/7/hwloc" target="_blank" rel="noopener noreferrer"><code>hwloc</code></a> to understand the hardware topology and optimize accordingly. But the default CPU allocation strategy is not optimal for our use case, so after <a href="https://groups.google.com/g/seastar-dev/c/-Wfj_KGFSgA/m/KKhcb0uLAgAJ" target="_blank" rel="noopener noreferrer">raising the issue on the mailing list</a>, I added a function to my fork that <a href="https://github.com/talawahtech/seastar/commit/503836269d1792a0fda1c6902afb275370c4d9ad" target="_blank" rel="noopener noreferrer">exposes the mapping between reactor shard ids and cpu ids</a> to apps that build on Seastar.</p><p>I <a href="https://github.com/talawahtech/seastar/commit/fb07e57ecec1a2b5700bcc998e14eecce8614c0b" target="_blank" rel="noopener noreferrer">modified tcp_httpd</a> to ensure that the cpu ids and socket ids matched. This resulted in the expected output from my bpftrace script.</p><div><pre><code>tcp_httpd, cpu=0, socket 0
reactor-2, cpu=1, socket 1
reactor-1, cpu=2, socket 2
reactor-3, cpu=3, socket 3
</code></pre></div><p>Performance improves slightly, but still leaves a lot to be desired.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   317.99us   74.65us    1.39ms   78.00us   76.29%
    Req/Sec    49.51k     2.01k    54.74k    44.35k    68.88%
  Latency Distribution
  50.00%  312.00us
  90.00%  405.00us
  99.00%  531.00us
  99.99%  749.00us
  3938893 requests in 5.00s, 582.25MB read
Requests/sec: 787768.20
</code></pre></div><p>A 6% performance improvement this time, but still well below expectations.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimize-reuse_port_cbpf.svg?x=1190.6&amp;y=789" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimize-reuse_port_cbpf.png" alt="Flame graph - Optimize SO_ATTACH_REUSEPORT_CBPF" loading="lazy" width="800" height="427"/></a></p><h3 id="flame-graph-analysis-4"><a href="#flame-graph-analysis-4" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>The flame graph doesn&#39;t show much in the way of change either, and there is still no busy polling happening, so something else is wrong. I dug into my bag of performance analysis tools to see if I could figure out what else was going on.</p><h3 id="impatiently-waiting"><a href="#impatiently-waiting" aria-hidden="true">#</a> Impatiently Waiting</h3><p>I was able to use <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#libreactor-vs-the-world" target="_blank" rel="noopener noreferrer">libreactor</a> as a point of reference for how a fully optimized epoll-based HTTP server should behave, and contrast that against tcp_httpd. Running a 10 second <a href="https://manpages.ubuntu.com/manpages/bionic/man8/syscount-bpfcc.8.html" target="_blank" rel="noopener noreferrer">syscount</a> trace (<code>syscount -d 10</code>) while the benchmark ran for both libreactor and tcp_httpd produced some enlightening results:</p><h4 id="libreactor"><a href="#libreactor" aria-hidden="true">#</a> libreactor</h4><div><pre><code>SYSCALL                   COUNT
recvfrom                9755167
sendto                  9353652
epoll_wait               754685
read                         94
bpf                          43
newfstatat                   18
ppoll                        11
pselect6                      7
futex                         5
write                         5
</code></pre></div><h4 id="tcp-httpd"><a href="#tcp-httpd" aria-hidden="true">#</a> tcp_httpd</h4><div><pre><code>SYSCALL                   COUNT
epoll_pwait             7525419
read                    7272935
sendto                  6926720
epoll_ctl                824992
poll                      76612
timerfd_settime           34276
rt_sigprocmask            11356
ioctl                      6447
membarrier                 5676
newfstatat                   18
</code></pre></div><p>For libreactor, the top two syscalls were <code>send</code>/<code>recv</code> with <code>epoll_wait</code> coming in at a distant third. Conversely with tcp_httpd, <code>epoll_pwait</code> was the number one syscall. This was a pretty good indicator that I needed to take a look at how <code>epoll_pwait</code> was called in the Seastar codebase.</p><p>The <code>epoll_pwait</code> syscall waits for events associated with file descriptors. In our case, we are dealing with socket file descriptors (representing a TCP connection) specifically, and each event indicates readiness to send or receive data.</p><p>The original <a href="https://man7.org/linux/man-pages/man2/epoll_wait.2.html" target="_blank" rel="noopener noreferrer"><code>epoll_(p)wait</code></a> syscall can be thought of as taking 3 types of values for the <code>timeout</code> parameter</p><ul><li>-1: The syscall waits indefinitely for an event</li><li>0: The syscall returns immediately whether or not any events are ready</li><li>n: The syscall waits until either a file descriptor delivers an event or n milliseconds have passed</li></ul><p>libreactor uses a relatively simple reactor engine built entirely around <code>epoll</code>, so it can afford to wait indefinitely for the next event. Seastar, on the other hand, is a bit more sophisticated. Seastar supports a number of different high-resolution timers, poll functions, and cross-reactor message queues; and it tries to enforce certain guarantees about how long tasks are expected to run. Within the main <code>do_run</code> loop, Seastar calls <code>epoll_pwait</code> with a timeout of 0 (it doesn&#39;t wait at all), which is why we are not seeing any busy polling happen. Calling <code>epoll_pwait</code> with an indefinite timeout is a non-starter for Seastar, and even calling it with the <code>epoll_pwait</code> minimum value of 1ms is probably a little too long. This is demonstrated by the fact that Seastar&#39;s default value for how long tasks should run in a single cycle (<code>task-quota-ms</code>) is 0.5 (500us).</p><p>In order to strike a balance between the framework&#39;s latency expectations and my performance goals, I decided to make use of the relatively new <code>epoll_pwait2</code> syscall. <code>epoll_pwait2</code> is equivalent to <code>epoll_pwait</code>, but the timeout argument can be specified with nanosecond resolution. I settled on a timeout value of 100us as a good balance between performance and latency guarantees. The new syscall is available as of kernel 5.11 but the corresponding glibc wrapper isn&#39;t available until glibc 2.35, and Amazon Linux 2022 ships with glibc 2.34. To work around that, I hacked up a wrapper function named <code>epoll_pwait_us</code>, and I <a href="https://github.com/talawahtech/seastar/commit/da68d1c12554eff13bddf29af9cd5e40cf8a7b59" target="_blank" rel="noopener noreferrer">updated my Seastar fork</a> to call it with a value of 100.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   273.38us   39.11us    1.37ms   79.00us   71.32%
    Req/Sec    57.48k   742.64     59.34k    55.62k    67.98%
  Latency Distribution
  50.00%  271.00us
  90.00%  322.00us
  99.00%  378.00us
  99.99%  613.00us
  4575332 requests in 5.00s, 676.32MB read
Requests/sec: 915053.04
</code></pre></div><p>Performance moves from 788k req/s to 915k req/s, a solid 16% jump. Now we&#39;re cooking!</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us.svg?x=1192.4&amp;y=773" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us.png" alt="Flame graph - epoll_wait 100us" loading="lazy" width="800" height="419"/></a></p><h3 id="flame-graph-analysis-5"><a href="#flame-graph-analysis-5" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>Looking at the flame graph you can clearly see that busy polling has finally kicked in, and looking at our syscall count we see the expected pattern emerge.</p><h4 id="tcp-httpd-2"><a href="#tcp-httpd-2" aria-hidden="true">#</a> tcp_httpd</h4><div><pre><code>SYSCALL                   COUNT
read                    8422317
sendto                  7964784
epoll_ctl                450827
epoll_pwait2             375947
poll                      79836
ioctl                       202
bpf                          49
newfstatat                   18
ppoll                        11
</code></pre></div><h2 id="constant-context-switching"><a href="#constant-context-switching" aria-hidden="true">#</a> Constant Context Switching</h2><p>I continued comparing tcp_httpd to libreactor using a few more perf tools to see if I could pick up any more anomalies. Sure enough, using <code>sar -w 1</code> to monitor context switches produced some eyebrow raising numbers for tcp_httpd.</p><h4 id="libreactor-2"><a href="#libreactor-2" aria-hidden="true">#</a> libreactor</h4><div><pre><code>01:13:50 AM    proc/s   cswch/s
01:13:57 AM      0.00    277.00
01:13:58 AM      0.00    229.00
01:13:59 AM      0.00    290.00
01:14:00 AM      0.00    340.00
</code></pre></div><h4 id="tcp-httpd-3"><a href="#tcp-httpd-3" aria-hidden="true">#</a> tcp_httpd</h4><div><pre><code>01:03:03 AM    proc/s   cswch/s
01:03:04 AM      0.00  17132.00
01:03:05 AM      0.00  17060.00
01:03:06 AM      0.00  17048.00
01:03:07 AM      0.00  17026.00
</code></pre></div><p>Looking at the <a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us.svg" target="_blank">flame graph without zooming in</a> I noticed that for each reactor thread, Seastar creates a matching timer thread named timer-0, timer-1, etc. At first I didn&#39;t pay much attention to them since I wasn&#39;t explicitly setting any timers, and they were barely visible on the flame graph, but in light of the context switching numbers I decided to take a closer look.</p><p>For each reactor/cpu core, <code>start_tick()</code> <a href="https://github.com/scylladb/seastar/blob/9ecc97a/src/core/reactor_backend.cc#L735" target="_blank" rel="noopener noreferrer">starts a thread</a> using the <a href="https://github.com/scylladb/seastar/blob/9ecc97a/src/core/reactor_backend.cc#L681" target="_blank" rel="noopener noreferrer"><code>task_quota_timer_thread_fn()</code></a> function. The function waits for the reactor&#39;s <code>_task_quota_timer</code> to expire and then interrupts the main thread by calling <code>request_preemption()</code>. This is done to make sure that the tasks on the main thread don&#39;t hog resources by running for more than X ms without preemption. But for our specific workload, it causes excessive context switching and tanks performance. What we want to do is set it just long enough so that <code>reactor::run_some_tasks()</code> can complete all tasks and reset preemption without ever being interrupted. It should be noted that Seastar&#39;s default aio backend seems to make use of some aio specific preempting functionality to handle the task quota, so this particular behavior is limited to the epoll backend.</p><p>Seastar allows users to pass in a value to set the <code>--task-quota-ms</code> via the command line. The default value is 0.5, but I found 10ms to be a more reasonable value for this workload.</p><h4 id="tcp-httpd-with-task-quota-ms-10"><a href="#tcp-httpd-with-task-quota-ms-10" aria-hidden="true">#</a> tcp_httpd with --task-quota-ms 10</h4><div><pre><code>01:04:58 AM    proc/s   cswch/s
01:04:59 AM      0.00   1327.00
01:05:00 AM      0.00   1303.00
01:05:01 AM      0.00   1339.00
01:05:02 AM      0.00   1296.00
</code></pre></div><p>The number of context switches per second dropped dramatically from 17k to 1.3k</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   259.14us   29.51us    1.51ms   77.00us   71.92%
    Req/Sec    60.55k   532.97     61.77k    58.82k    66.71%
  Latency Distribution
  50.00%  257.00us
  90.00%  296.00us
  99.00%  337.00us
  99.99%  557.00us
  4820680 requests in 5.00s, 712.59MB read
Requests/sec: 964121.54
</code></pre></div><p>Throughput moved from 915k req/s to 964k req/s, a 5.3% improvement.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us-task-quota-ms-10.svg?x=1198.8&amp;y=885" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us-task-quota-ms-10.png" alt="Flame graph - task-quota-ms 10" loading="lazy" width="800" height="475"/></a></p><h3 id="flame-graph-analysis-6"><a href="#flame-graph-analysis-6" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>The change in the flame graph is pretty subtle. If you zoom out to the &#34;all&#34; view and then search for <code>timer-</code> you will see a small section to the far right go from <a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us.svg?s=timer-" target="_blank">0.7% of the previous flame graph</a> to <a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-epoll_wait-100us-task-quota-ms-10.svg?s=timer-" target="_blank">0.1% of the current one</a>. Flame graphs are extremely useful but they don&#39;t always capture the performance impact in a proportional way. Sometimes you have to rummage around in your perf toolbox to find the right tool to pick up an anomaly.</p><h2 id="it-is-better-to-recv"><a href="#it-is-better-to-recv" aria-hidden="true">#</a> It is better to RECV</h2><p>This is a simple fix that I figured out back when I was optimizing libreactor. When working with sockets, it is a little more efficient to use Linux&#39;s <code>recv</code>/<code>send</code> functions, than the more general-purpose <code>read</code>/<code>write</code>. Generally the difference is negligible, however when you move beyond 50k req/s it starts to add up. Seastar was already using <code>send</code> for outgoing data, but it was using <code>read</code> for incoming requests, so I made the relatively simple change to <a href="https://github.com/talawahtech/seastar/commit/c7d3289aec701631549e89605a782979cd73828d" target="_blank" rel="noopener noreferrer">switch it to <code>recv</code></a> instead.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   253.53us   30.51us    1.21ms   93.00us   74.63%
    Req/Sec    61.72k   597.21     62.99k    58.46k    71.81%
  Latency Distribution
  50.00%  250.00us
  90.00%  291.00us
  99.00%  342.00us
  99.99%  652.00us
  4911503 requests in 5.00s, 726.02MB read
Requests/sec: 982287.44
</code></pre></div><p>Throughput moved from 964k req/s to 982k req/s, just under a 2% performance improvement.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-recv-syscall.svg?x=1198.2&amp;y=757" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-recv-syscall.png" alt="Flame graph - recv syscall" loading="lazy" width="800" height="411"/></a></p><h3 id="flame-graph-analysis-7"><a href="#flame-graph-analysis-7" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>If you look at the read/recv stack on the left side of the flame graph you will see that <code>__libc_recv</code> gets to the point a lot more directly than <code>__libc_read</code>.</p><h2 id="remember-to-flush"><a href="#remember-to-flush" aria-hidden="true">#</a> Remember to Flush</h2><p>I found the final optimization by roaming around the codebase and switching things on/off to see what they did. When using the epoll reactor backend, the <code>batch_flushes</code> option on <code>output_stream</code> defers calling <code>send()</code> right away when <code>flush()</code> is called. It is designed as an optimization for RPC workloads that may call <code>flush()</code> multiple times, but it doesn&#39;t provide any benefit for our simple request/response workload. As a matter of fact, it adds a little bit of overhead, so as a quick fix I just <a href="https://github.com/talawahtech/seastar/commit/58545f05d0250d8dc720ddf422f22cb557ebc365" target="_blank" rel="noopener noreferrer">disabled <code>batch_flushes</code></a>.</p><div><pre><code>Running 5s test @ http://172.31.XX.XX:8080/json
  16 threads and 256 connections
  Thread Stats   Avg     Stdev       Max       Min   +/- Stdev
    Latency   246.66us   34.32us    1.25ms   61.00us   74.07%
    Req/Sec    63.30k     0.88k    65.72k    61.63k    66.84%
  Latency Distribution
  50.00%  246.00us
  90.00%  288.00us
  99.00%  333.00us
  99.99%  436.00us
  5038933 requests in 5.00s, 744.85MB read
Requests/sec: 1007771.89
</code></pre></div><p>Throughput moved from 982k req/s to 1.0M req/s, a 2.2% performance boost.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimized.svg?x=1198.4&amp;y=837" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimized.png" alt="Flame graph - fully optimized" loading="lazy" width="800" height="451"/></a></p><h3 id="flame-graph-analysis-8"><a href="#flame-graph-analysis-8" aria-hidden="true">#</a> Flame Graph Analysis</h3><p>The flame graph shows that the send stack moved from <code>batch_flush_pollfn::poll</code> to <code>output_stream&lt;char&gt;::flush</code></p><p>Our optimization efforts have rewarded us with a psychologically satisfying base 10 number for our final figure: <strong>1.0M request/s</strong> using nothing more than the good old Linux kernel.</p><h2 id="and-the-winner-is"><a href="#and-the-winner-is" aria-hidden="true">#</a> And the winner is...</h2><p>In the end, DPDK still maintains a solid 51% percent performance lead over the kernel. Whether that is a lot or a little depends on your perspective. The way I look at it, when you compare the unoptimized and optimized versions of the kernel/application, we have narrowed DPDK&#39;s performance advantage from 4.2x to just 1.5x.</p><h2 id="dpdk-caveats"><a href="#dpdk-caveats" aria-hidden="true">#</a> DPDK Caveats</h2><p>DPDK&#39;s 51% advantage is nothing to scoff at, however I would be remiss if I sent you down a DPDK rabbit hole without adding some disclaimers about DPDK&#39;s challenges.</p><ol><li><p>To start, it is a bit of a niche technology, so finding articles and examples online (especially for use-cases outside established areas) can be challenging.</p></li><li><p>Bypassing the kernel means you also bypass its time-tested TCP stack. If your application uses a TCP based protocol like HTTP, you need to provide your own TCP networking stack in userspace. There are frameworks like Seastar and <a href="https://github.com/F-Stack/f-stack" target="_blank" rel="noopener noreferrer">F-Stack</a> that help, but migrating your application to them may be non-trivial.</p></li><li><p>Working with a custom framework might also mean that you are tied to the specific DPDK version that it supports, which might not be the version supported by your network driver or kernel.</p></li><li><p>In bypassing the kernel you also bypass a rich ecosystem of existing tools and features for securing, monitoring and configuring your network traffic. Many of the tools and techniques that you are accustomed to no longer work.</p></li><li><p>If you use poll-mode processing your CPU usage will <em>always</em> be 100%. In addition to not being energy efficient/environmentally friendly, it also makes it difficult to quickly assess/troubleshoot your workload using CPU usage as a gauge.</p></li><li><p>DPDK based applications take full control of the network interface, which means:</p><ul><li>You must have more than one interface.</li><li>If you want to modify device settings, you have to do it before startup, or through the application.</li><li>If you want to capture metrics, the application has to be configured to do it; it is much harder to troubleshoot on the fly.</li></ul></li></ol><p>That being said, there may be reasons to pursue a custom TCP/IP stack other than pure performance. An in-application TCP stack allows the application to precisely control memory allocation (avoiding contention for memory between the application and kernel) and scheduling (avoiding contention for CPU time). This can be important for applications that strive not only for maximum throughput, but also for excellent p99 latency.</p><p>At the end of the day it is about balancing priorities. As an example, even though the ScyllaDB team occasionally gets <a href="https://github.com/scylladb/scylla/issues/7882" target="_blank" rel="noopener noreferrer">reports of reactor stalls</a> related to the kernel network stack, they still choose to stick with the kernel for their flagship product, because switching to DPDK would be <a href="https://github.com/scylladb/scylla/issues/7882#issuecomment-866746310" target="_blank" rel="noopener noreferrer">far from a simple undertaking</a>.</p><h2 id="speculative-execution-mitigations"><a href="#speculative-execution-mitigations" aria-hidden="true">#</a> Speculative Execution Mitigations</h2><p>Early on in this post I glossed over the OS level optimizations that I made before I started optimizing the application. From a high level the changes were similar to my previous post, and for those who want to dig into more details, I plan to write up a kernel 4.14 vs 5.15 post &#34;at some point&#34;. Nevertheless there is one particular optimization that deserves further analysis in this kernel vs DPDK showdown: disabling speculative execution mitigations.</p><p>I won&#39;t rehash my opinion on these mitigations, you can <a href="https://talawah.io/blog/extreme-http-performance-tuning-one-point-two-million/#_2-speculative-execution-mitigations" target="_blank" rel="noopener noreferrer">read it here</a>. For the purposes of this post, I turned them off, but if you look at the graph below you will see that turning them back on shows some interesting results.</p><p>As you can see, while disabling mitigations yields a 33% performance improvement on the kernel side, it had <strong>zero impact</strong> on DPDK performance. This leads to two main takeaways:</p><ol><li><p>For environments where speculative execution mitigations are a must, DPDK represents an even bigger performance improvement over the kernel TCP stack.</p></li><li><p>Kernel technologies like io_uring that bypass the syscall interface for I/O have even greater potential for improving performance on the majority of workloads.</p></li></ol><p>Most people don&#39;t disable Spectre mitigations, so solutions that work with them enabled are important. I am not 100% sure that <strong>all</strong> of the mitigation overhead comes from syscalls, but it stands to reason that a lot of it arises from security hardening in user-to-kernel and kernel-to-user transitions. The impact is certainly visible in the syscall related functions on the flame graph.</p><p><a href="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimized-mitigations-on.svg?x=1197.4&amp;y=837" target="_blank"><img src="https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/tcp_httpd-final-optimized-mitigations-on.png" alt="Flame graph - Mitigations ON" loading="lazy" width="800" height="451"/></a></p><h2 id="conclusion"><a href="#conclusion" aria-hidden="true">#</a> Conclusion</h2><p>We have demonstrated that even when the OS and application are optimized to the extreme, DPDK still has a 51% performance lead over the kernel networking stack. Instead of seeing that difference as an insurmountable hurdle, I see the gap as unrealized potential on the kernel side. The gap simply raises the question: To what extent can the Linux kernel be further optimized for thread-per-core applications without compromising its general-purpose nature?</p><p>DPDK gives us an idea of what is possible under ideal circumstances, and serves as a target to strive towards. Even if the gap can&#39;t fully be closed, it quantifies the task and throws the obstacles into sharper focus.</p><p>One very obvious obstacle is the overhead of the syscall interface when doing millions of syscalls per second. Thankfully, io_uring seems to offer a potential solution for that particular challenge. I have been keeping a close eye on io_uring, as it is still under pretty heavy development. I am particularly excited to see the recent <a href="https://twitter.com/axboe/status/1504263869824266241" target="_blank" rel="noopener noreferrer">wave of networking focused optimizations</a> like <a href="https://twitter.com/axboe/status/1502002250557923337" target="_blank" rel="noopener noreferrer">busy poll support</a>, <a href="https://twitter.com/axboe/status/1519404690001305600" target="_blank" rel="noopener noreferrer">recv hints</a>, and even experimental explorations like <a href="https://twitter.com/axboe/status/1514013578629574656" target="_blank" rel="noopener noreferrer">lockless TCP support</a>. It remains very high on my list of things to test &#34;real soon&#34;.</p><h2 id="appendix"><a href="#appendix" aria-hidden="true">#</a> Appendix</h2><h3 id="special-thanks"><a href="#special-thanks" aria-hidden="true">#</a> Special Thanks</h3><p>Special thanks to my reviewers: Dor and Kenia, and to everyone on the Seastar Slack channel and mailing list, particularly Piotr, Avi and Max.</p><h3 id="c-c-primers"><a href="#c-c-primers" aria-hidden="true">#</a> C/C++ Primers</h3><p>I used my limited C knowledge, combined with basic pattern recognition to fumble around Seastar&#39;s C++ codebase for way longer than I probably should have, but when it came time to add the <code>get_cpu_to_shard_mapping()</code> function, I decided to stop fooling myself and learn a little C++. If you find yourself in a similar predicament, I recommend <a href="https://isocpp.org/tour" target="_blank" rel="noopener noreferrer">A Tour of C++</a> as a decent primer. If you need a quick C refresher as well, I recommend <a href="http://cslibrary.stanford.edu/101/EssentialC.pdf" target="_blank" rel="noopener noreferrer">Essential C</a> and <a href="http://cslibrary.stanford.edu/102/PointersAndMemory.pdf" target="_blank" rel="noopener noreferrer">Pointers and Memory</a>.</p></div></div>
  </body>
</html>
