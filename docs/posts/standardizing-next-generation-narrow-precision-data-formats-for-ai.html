<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.opencompute.org/blog/amd-arm-intel-meta-microsoft-nvidia-and-qualcomm-standardize-next-generation-narrow-precision-data-formats-for-ai">Original</a>
    <h1>Standardizing next-generation narrow precision data formats for AI</h1>
    
    <div id="readability-page-1" class="page"><article>
                <header>
                    <h2>AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm Standardize Next-Generation Narrow Precision Data Formats for AI</h2>
                    
                </header>
                <section>
                    <div>
                        <p dir="ltr">Realizing the full potential of next-generation deep learning requires highly efficient AI infrastructure. For a computing platform to be scalable and cost efficient, optimizing every layer of the AI stack, from algorithms to hardware, is essential. Advances in narrow-precision AI data formats and associated optimized algorithms have been pivotal to this journey, allowing the industry to transition from traditional 32-bit floating point precision to presently only 8 bits of precision (i.e. <a href="https://www.opencompute.org/documents/ocp-8-bit-floating-point-specification-ofp8-revision-1-0-2023-06-20-pdf">OCP FP8</a>).</p>

<p dir="ltr">Narrower formats allow silicon to execute more efficient AI calculations per clock cycle, which accelerates model training and inference times. AI models take up less space, which means they require fewer data fetches from memory, and can run with better performance and efficiency. Additionally, fewer bit transfers reduces data movement over the interconnect, which can enhance application performance or cut network costs.</p>

<h3 dir="ltr"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Bringing Together Key Industry Leaders to Set the Standard</strong></h3>

<p dir="ltr">Earlier this year, AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm Technologies, Inc. formed the Microscaling Formats (MX) Alliance with the goal of creating and standardizing next-generation 6- and 4-bit data types for AI training and inferencing.  The key enabling technology that enables sub 8-bit formats to work, referred to as microscaling, builds on a foundation of years of design space exploration and research.  MX enhances the robustness and ease-of-use of existing 8-bit formats such as FP8 and INT8, thus lowering the barrier for broader adoption of single digit bit training and inference.</p>

<p dir="ltr"><img alt="" src="https://146a55aca6f00848c565-a7635525d40ac1c70300198708936b4e.ssl.cf1.rackcdn.com/images/2d5022c0b8fbe7af031cd064edad4c1ac66ee2f4.png"/>The initial MX specification introduces four concrete floating point and integer-based data formats (MXFP8, MXFP6, MXFP4, and MXINT8) that are compatible with current AI stacks, support implementation flexibility across both hardware and software, and enable fine-grain microscaling at the hardware level. <a href="https://arxiv.org/abs/2302.08007">Extensive studies</a> demonstrate that MX formats can be easily deployed for many diverse real-world cases such as large language models, computer vision, and recommender systems. MX technology also enables LLM pre-training at 6- and 4-bit precisions without any modifications to conventional training recipes.</p>

<h3 dir="ltr"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Democratizing AI Capabilities</strong></h3>

<p dir="ltr">In the evolving landscape of AI, open standards are critical to foster innovation, collaboration, and widespread adoption. These standards offer a unifying framework that enables consistent toolchains, model development, and interoperability across the AI ecosystem.  This further empowers developers and organizations to harness the full potential of AI while mitigating the fragmentation and technology constraints that could otherwise stifle progress.  </p>

<p dir="ltr">In this spirit, the MX Alliance has released the <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">Microscaling Formats (MX) Specification v1.0</a> in an open, license-free format through the Open Compute Project Foundation (OCP) to enable and encourage broad industry adoption and provide the foundation for potential future narrow-format innovations. Additionally, <a href="https://arxiv.org/abs/2310.10537">a white paper</a> and <a href="https://github.com/microsoft/microxcaling">emulation libraries</a> have also been published to provide details on the data science approach and select results of MX in action. This inclusivity not only accelerates the pace of AI advancement but also promotes openness, accountability, and the responsible development of AI applications.</p>

<p dir="ltr">&#34;<em>AMD is pleased to be a founding member of the MX Alliance and has been a key contributor to the OCP MX Specification v1.0. This industry collaboration to standardize MX data formats provides an open and sustainable approach to continued AI innovations while providing the AI ecosystem time to prepare for the use of MX data formats in future hardware and software. AMD is committed to driving forward an open AI ecosystem and is happy to contribute our research results on MX data formats to the broader AI community.</em>”<strong>   </strong></p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Michael Schulte, Sr. Fellow, AMD</strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>As an industry we have a unique opportunity to collaborate and realize the benefits of AI technology, which will enable new use cases from cloud to edge to endpoint. This requires commitment to standardization for AI training and inference so that developers can focus on innovating where it really matters, and the release of the OCP MX specification is a significant milestone in this journey.</em>”</p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Ian Bratt, Fellow and Senior Director of Technology, Arm</strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>The OCP MX spec is the result of a fairly broad cross-industry collaboration and represents an important step forward in unifying and standardizing emerging sub-8bit data formats for AI applications. Portability and interoperability of AI models enabled by this should make AI developers very happy. Benefiting AI applications should see higher levels of performance and energy efficiency, with reduced memory needs.</em>”</p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Pradeep Dubey, Senior Fellow and Director of the Parallel Computing Lab, Intel</strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>To keep pace with the accelerating demands of AI, innovation must happen across every layer of the stack. The OCP MX effort is a significant leap forward in enabling more scalability and efficiency for the most advanced training and inferencing workloads. MX builds upon years of internal work, and now working together with our valued partners, has evolved into an open standard that will benefit the entire AI ecosystem and industry.</em>” </p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Brian Harry, Technical Fellow, Microsoft </strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>MX formats with a wide spectrum of sub-8-bit support provide efficient training and inference solutions that can be applied to AI models in various domains, from recommendation models with strict accuracy requirements, to the latest large language models that are latency-sensitive and compute intensive. We believe sharing these MX formats with the OCP and broader ML community will lead to more innovation in AI modeling.</em>”</p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Ajit Mathews, Senior Director of Engineering, Meta AI</strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>The OCP MX specification is a significant step towards accelerating AI training and inference workloads with sub-8-bit data formats. These formats accelerate applications by reducing memory footprint and bandwidth pressure, also allowing for innovation in math operation implementation. The open format specification enables platform interoperability, benefiting the entire industry.</em>”</p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Paulius Micikevicius, Senior Distinguished Engineer, NVIDIA</strong></p>
	</li>
</ul>

<p dir="ltr">&#34;<em>The new OCP MX specification will help accelerate the transition to lower-cost, lower-power server-based forms of AI inference. We are passionate about democratizing AI through lower-cost inference and we are glad to join this effort.</em>”</p>

<ul>
	<li aria-level="1" dir="ltr">
	<p dir="ltr" role="presentation"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">Colin Verrilli, Senior Director, Qualcomm Technologies, Inc</strong></p>
	</li>
</ul>



<h3 dir="ltr"><strong id="docs-internal-guid-5417e130-7fff-920b-08a2-3ed5a36615d3">About the Open Compute Project Foundation</strong></h3>

<p dir="ltr">The Open Compute Project (OCP) is a collaborative Community of hyperscale data center operators, telecom, colocation providers and enterprise IT users, working with the product and solution vendor ecosystem to develop open innovations deployable from the cloud to the edge. The OCP Foundation is responsible for fostering and serving the OCP Community to meet the market and shape the future, taking hyperscale-led innovations to everyone. Meeting the market is accomplished through addressing challenging market obstacles with open specifications, designs and emerging market programs that showcase OCP-recognized IT equipment and data center facility best practices. Shaping the future includes investing in strategic initiatives and programs that prepare the IT ecosystem for major technology changes, such as AI &amp; ML, optics, advanced cooling techniques, composable memory and silicon. OCP Community-developed open innovations strive to benefit all, optimized through the lens of impact, efficiency, scale and sustainability.</p>

<p dir="ltr">Learn more at:<a href="http://www.opencompute.org/"> www.opencompute.org</a>.</p>
                    </div>
                </section>
            </article></div>
  </body>
</html>
