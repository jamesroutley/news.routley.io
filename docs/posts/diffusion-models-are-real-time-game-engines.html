<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gamengen.github.io">Original</a>
    <h1>Diffusion Models Are Real-Time Game Engines</h1>
    
    <div id="readability-page-1" class="page">

  <section>
    
  </section>

  <section>
    <div>
      <div>
        <div>
          <div>
            <p>
              <iframe src="https://www.youtube.com/embed/O3616ZFGpqw?autoplay=1&amp;mute=1&amp;loop=1&amp;showinfo=0&amp;list=PL3ZfMho22LwDvJSEKVBiwxNsVEqUTUmhJ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
            </p>
          </div>
        </div>
        <p><span>Real-time</span> recordings of people playing the game <a href="https://en.wikipedia.org/wiki/Doom_(1993_video_game)" target="_blank">DOOM</a>
          <span><b>simulated entirely by the <i>GameNGen</i> neural model</b></span>.
        </p>
      </div>
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Abstract</h2>
          <p>
              We present <i>GameNGen</i>, the first game engine powered entirely by a neural model
              that enables real-time interaction with a complex environment over long trajectories at high quality.
              <i>GameNGen</i> can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU.
              Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression.
              Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation.
              <i>GameNGen</i> is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and
              (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions.
              Conditioning augmentations enable stable auto-regressive generation over long trajectories.
            </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    
  </section>

  <section>
    <div>
      <div>
        <div>
          <div>
            <h2>Architecture</h2>
            <div>
              
              <p><img src="https://gamengen.github.io/static/images/Architecture_08_27.png" alt="Architecture overview"/></p><p><b>Data Collection via Agent Play:</b>
                Since we cannot collect human gameplay at scale, as a first stage we train an automatic RL-agent to play
                the game,
                persisting it&#39;s training episodes of actions and observations, which become the training data for our
                generative model.
              </p>
              <p><b>Training the Generative Diffusion Model:</b> We re-purpose a small diffusion model, <a href="https://arxiv.org/abs/2112.10752" target="_blank">Stable Diffusion</a> v1.4,
                and condition it on a sequence of previous actions and observations (frames). To mitigate
                auto-regressive drift
                during inference, we corrupt context frames by adding Gaussian noise to encoded frames during training.
                This allows the network to correct information sampled in previous frames, and we found it to be
                critical
                for preserving visual stability over long time periods.
              </p>
              <p><b>Latent Decoder Fine-Tuning:</b> The pre-trained auto-encoder of <a href="https://arxiv.org/abs/2112.10752" target="_blank">Stable Diffusion</a> v1.4, which compresses
                8x8
                pixel patches into 4 latent channels, results in meaningful artifacts when predicting game frames, which
                affect
                small details and particularly the bottom bar HUD. To leverage the pre-trained knowledge while improving
                image quality, we train just the decoder of the latent auto-encoder using an MSE loss computed
                against the target frame pixels.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@misc{valevski2024diffusionmodelsrealtimegame,
        title={Diffusion Models Are Real-Time Game Engines}, 
        author={Dani Valevski and Yaniv Leviathan and Moab Arar and Shlomi Fruchter},
        year={2024},
        eprint={2408.14837},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2408.14837}, 
  }</code></pre>
    </div>
  </section>

  <section id="Acknowledgements">
    <div>
      <h2>Acknowledgements</h2><p>
      We&#39;d like to extend a huge thank you to Eyal Segalis, Eyal Molad, Matan Kalman, Nataniel Ruiz,
      Amir Hertz, Matan Cohen, Yossi Matias, Yael Pritch, Danny Lumen, Valerie Nygaard, the Theta Labs and Google
      Research teams, and our families for insightful feedback, ideas, suggestions, and support.
    </p></div>
  </section>

  

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</div>
  </body>
</html>
