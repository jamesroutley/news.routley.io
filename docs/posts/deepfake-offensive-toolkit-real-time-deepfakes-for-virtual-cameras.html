<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/sensity-ai/dot">Original</a>
    <h1>Deepfake Offensive Toolkit (real-time deepfakes for virtual cameras)</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://github.com/sensity-ai/dot/actions/workflows/build_dot.yaml"><img src="https://github.com/sensity-ai/dot/actions/workflows/build_dot.yaml/badge.svg" alt="build-dot"/></a> <a href="https://github.com/sensity-ai/dot/actions/workflows/code_check.yaml"><img src="https://github.com/sensity-ai/dot/actions/workflows/code_check.yaml/badge.svg" alt="code-check"/></a></p>
<p dir="auto"><em>dot</em> (aka Deepfake Offensive Toolkit) makes real-time, controllable deepfakes ready for virtual cameras injection. <em>dot</em> is created for performing penetration testing against e.g. identity verification and video conferencing systems, for the use by security analysts, Red Team members, and biometrics researchers.</p>
<p dir="auto">If you want to learn more about <em>dot</em> is used for penetration tests with deepfakes in the industry, read <a href="https://www.theverge.com/2022/5/18/23092964/deepfake-attack-facial-recognition-liveness-test-banks-sensity-report" rel="nofollow">this article by The Verge</a></p>
<p dir="auto">dot <em>is developed for research and demonstration purposes. As an end user, you have the responsibility to obey all applicable laws when using this program. Authors and contributing developers assume no liability and are not responsible for any misuse or damage caused by the use of this program.</em></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://blog.plover.com/sensity-ai/dot/blob/main/assets/dot_intro.gif"><img src="https://blog.plover.com/sensity-ai/dot/raw/main/assets/dot_intro.gif" width="500" data-animated-image=""/></a>
</p>
<h2 dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How it works</h2>
<p dir="auto">In a nutshell, <em>dot</em> works like this</p>
<div data-snippet-clipboard-copy-content="    __________________      _____________________________      __________________________
   | your webcam feed | -&gt; | suite of realtime deepfakes | -&gt; | virtual camera injection |
    ------------------      -----------------------------      --------------------------"><pre lang="text"><code>    __________________      _____________________________      __________________________
   | your webcam feed | -&gt; | suite of realtime deepfakes | -&gt; | virtual camera injection |
    ------------------      -----------------------------      --------------------------
</code></pre></div>
<p dir="auto">All deepfakes supported by <em>dot</em> do not require additional training. They can be used
in real-time on the fly on a photo that becomes the target of face impersonation.
Supported methods:</p>
<ul dir="auto">
<li>face swap (via <a href="https://github.com/neuralchen/SimSwap">SimSwap</a>), at resolutions <code>224</code> and <code>512</code>
<ul dir="auto">
<li>with the option of face superresolution (via <a href="https://github.com/yangxy/GPEN">GPen</a>) at resolutions <code>256</code> and <code>512</code></li>
</ul>
</li>
<li>lower quality face swap (via OpenCV)</li>
<li><a href="https://github.com/AliaksandrSiarohin/first-order-model">FOMM</a>, First Order Motion Model for image animation</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<h3 dir="auto"><a id="user-content-install-pre-requisites" aria-hidden="true" href="#install-pre-requisites"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install Pre-requisites</h3>
<ul dir="auto">
<li>
<p dir="auto">Linux</p>
<div data-snippet-clipboard-copy-content="sudo apt install ffmpeg cmake"><pre>sudo apt install ffmpeg cmake</pre></div>
</li>
<li>
<p dir="auto">MacOS</p>
<div data-snippet-clipboard-copy-content="brew install ffmpeg cmake"><pre>brew install ffmpeg cmake</pre></div>
</li>
</ul>
<h3 dir="auto"><a id="user-content-create-conda-environment" aria-hidden="true" href="#create-conda-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Create Conda Environment</h3>
<blockquote>
<p dir="auto">The instructions assumes that you have Miniconda installed on your machine. If you don&#39;t, you can refer to this <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html" rel="nofollow">link</a> for installation instructions.</p>
</blockquote>
<h4 dir="auto"><a id="user-content-with-gpu-support" aria-hidden="true" href="#with-gpu-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>With GPU Support</h4>
<div data-snippet-clipboard-copy-content="conda env create -f envs/environment-gpu.yaml
conda activate dot"><pre>conda env create -f envs/environment-gpu.yaml
conda activate dot</pre></div>
<p dir="auto">Install the <code>torch</code> and <code>torchvision</code> dependencies based on the CUDA version installed on your machine:</p>
<ul dir="auto">
<li>Install <code>cudatoolkit</code> from <code>conda</code>: <code>conda install cudatoolkit=&lt;cuda_version_no&gt;</code> (replace <code>&lt;cuda_version_no&gt;</code> with the version on your machine)</li>
<li>Install <code>torch</code> and <code>torchvision</code> dependencies: <code>pip install torch==1.9.0+&lt;cuda_tag&gt; torchvision==0.10.0+&lt;cuda_tag&gt; -f https://download.pytorch.org/whl/torch_stable.html</code>, where <code>&lt;cuda_tag&gt;</code> is the CUDA tag defined by Pytorch. For example, <code>pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html</code> for CUDA 11.1. See <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">here</a> for a list of all available <code>torch</code> and <code>torchvision</code> versions.</li>
</ul>
<p dir="auto">To check that <code>torch</code> and <code>torchvision</code> are installed correctly, run the following command: <code>python -c &#34;import torch; print(torch.cuda.is_available())&#34;</code>. If the output is <code>True</code>, the dependencies are installed with CUDA support.</p>
<h4 dir="auto"><a id="user-content-with-cpu-support-slow-not-recommended" aria-hidden="true" href="#with-cpu-support-slow-not-recommended"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>With CPU Support (slow, not recommended)</h4>
<div data-snippet-clipboard-copy-content="conda env create -f envs/environment-cpu.yaml
conda activate dot"><pre>conda env create -f envs/environment-cpu.yaml
conda activate dot</pre></div>
<h3 dir="auto"><a id="user-content-install-dot" aria-hidden="true" href="#install-dot"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install dot</h3>
<div data-snippet-clipboard-copy-content="pip install -e ."><pre>pip install -e <span>.</span></pre></div>
<h3 dir="auto"><a id="user-content-download-models" aria-hidden="true" href="#download-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Download Models</h3>
<p dir="auto">There are 2 options for downloading the model weights:</p>
<ul dir="auto">
<li>
<p dir="auto">GDrive: Download folder from <a href="https://drive.google.com/drive/folders/1FX1QoXragN4aKJZFo2DLiDE8fqKHeXEB?usp=sharing" rel="nofollow">here</a>, unzip and place the folder in the root directory of the repository.</p>
</li>
<li>
<p dir="auto"><code>gdown</code>: Run the following command:</p>
<div data-snippet-clipboard-copy-content="gdown https://drive.google.com/drive/folders/1FX1QoXragN4aKJZFo2DLiDE8fqKHeXEB -O ./saved_models --folder"><pre>gdown https://drive.google.com/drive/folders/1FX1QoXragN4aKJZFo2DLiDE8fqKHeXEB -O ./saved_models --folder</pre></div>
</li>
</ul>
<h2 dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<h3 dir="auto"><a id="user-content-running-dot" aria-hidden="true" href="#running-dot"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Running dot</h3>
<p dir="auto">Run <code>dot --help</code> to get a full list of available options.</p>
<ol dir="auto">
<li>
<p dir="auto">Simswap</p>
<div data-snippet-clipboard-copy-content="dot \
--swap_type simswap \
--target 0 \
--source &#34;./data&#34; \
--parsing_model_path ./saved_models/simswap/parsing_model/checkpoint/79999_iter.pth \
--arcface_model_path ./saved_models/simswap/arcface_model/arcface_checkpoint.tar \
--checkpoints_dir ./saved_models/simswap/checkpoints \
--show_fps \
--use_gpu"><pre>dot \
--swap_type simswap \
--target 0 \
--source <span><span>&#34;</span>./data<span>&#34;</span></span> \
--parsing_model_path ./saved_models/simswap/parsing_model/checkpoint/79999_iter.pth \
--arcface_model_path ./saved_models/simswap/arcface_model/arcface_checkpoint.tar \
--checkpoints_dir ./saved_models/simswap/checkpoints \
--show_fps \
--use_gpu</pre></div>
</li>
<li>
<p dir="auto">SimSwapHQ</p>
<div data-snippet-clipboard-copy-content="dot \
--swap_type simswap \
--target 0 \
--source &#34;./data&#34; \
--parsing_model_path ./saved_models/simswap/parsing_model/checkpoint/79999_iter.pth \
--arcface_model_path ./saved_models/simswap/arcface_model/arcface_checkpoint.tar \
--checkpoints_dir ./saved_models/simswap/checkpoints \
--crop_size 512 \
--show_fps \
--use_gpu"><pre>dot \
--swap_type simswap \
--target 0 \
--source <span><span>&#34;</span>./data<span>&#34;</span></span> \
--parsing_model_path ./saved_models/simswap/parsing_model/checkpoint/79999_iter.pth \
--arcface_model_path ./saved_models/simswap/arcface_model/arcface_checkpoint.tar \
--checkpoints_dir ./saved_models/simswap/checkpoints \
--crop_size 512 \
--show_fps \
--use_gpu</pre></div>
<p dir="auto">Additionally, to enable face superresolution, use the flag <code>--gpen_type gpen_256</code> or <code>--gpen_type gpen_512</code>.</p>
</li>
<li>
<p dir="auto">FOMM</p>
<div data-snippet-clipboard-copy-content="dot \
--swap_type fomm \
--target 0 \
--source &#34;./data&#34; \
--model_path ./saved_models/fomm/vox-adv-cpk.pth.tar \
--show_fps \
--use_gpu"><pre>dot \
--swap_type fomm \
--target 0 \
--source <span><span>&#34;</span>./data<span>&#34;</span></span> \
--model_path ./saved_models/fomm/vox-adv-cpk.pth.tar \
--show_fps \
--use_gpu</pre></div>
</li>
<li>
<p dir="auto">FaceSwap</p>
<div data-snippet-clipboard-copy-content="dot \
--swap_type faceswap_cv2 \
--target 0 \
--source &#34;./data&#34; \
--model_path ./saved_models/faceswap_cv/shape_predictor_68_face_landmarks.dat \
--show_fps \
--use_gpu"><pre>dot \
--swap_type faceswap_cv2 \
--target 0 \
--source <span><span>&#34;</span>./data<span>&#34;</span></span> \
--model_path ./saved_models/faceswap_cv/shape_predictor_68_face_landmarks.dat \
--show_fps \
--use_gpu</pre></div>
</li>
</ol>
<p dir="auto"><strong>Note</strong>: To use <em>dot</em> on CPU (not recommended), do not pass the <code>--use_gpu</code> flag.</p>
<h3 dir="auto"><a id="user-content-controlling-dot" aria-hidden="true" href="#controlling-dot"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Controlling dot</h3>
<blockquote>
<p dir="auto"><strong>Disclaimer</strong>: We use the <code>SimSwap</code> technique for the following demonstration</p>
</blockquote>
<p dir="auto">Running <em>dot</em> via any of the above methods generates real-time Deepfake on the input video feed using source images from the <code>./data</code> folder.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://blog.plover.com/sensity-ai/dot/blob/main/assets/dot_run.gif"><img src="https://blog.plover.com/sensity-ai/dot/raw/main/assets/dot_run.gif" width="500" data-animated-image=""/></a>
</p>
<p dir="auto">When running <em>dot</em> a list of available control options appear on the terminal window as shown above. You can toggle through and select different source images by pressing the associated control key.</p>
<p dir="auto">Watch the following demo video for better understanding of the control options:</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://blog.plover.com/sensity-ai/dot/blob/main/assets/dot_demo.gif"><img src="https://blog.plover.com/sensity-ai/dot/raw/main/assets/dot_demo.gif" width="480" data-animated-image=""/></a>
</p>
<h2 dir="auto"><a id="user-content-virtual-camera-injection" aria-hidden="true" href="#virtual-camera-injection"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Virtual Camera Injection</h2>
<p dir="auto">Instructions vary depending on your operating system.</p>
<h3 dir="auto"><a id="user-content-windows" aria-hidden="true" href="#windows"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Windows</h3>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://obsproject.com/" rel="nofollow">OBS Studio</a>.</p>
</li>
<li>
<p dir="auto">Install <a href="https://obsproject.com/forum/resources/obs-virtualcam.539/" rel="nofollow">VirtualCam plugin</a>.</p>
</li>
</ul>
<p dir="auto">Choose <code>Install and register only 1 virtual camera</code>.</p>
<ul dir="auto">
<li>
<p dir="auto">Run OBS Studio.</p>
</li>
<li>
<p dir="auto">In the Sources section, press on Add button (&#34;+&#34; sign),</p>
<p dir="auto">select Windows Capture and press OK. In the appeared window,
choose &#34;[python.exe]: fomm&#34; in Window drop-down menu and press OK.
Then select Edit -&gt; Transform -&gt; Fit to screen.</p>
</li>
<li>
<p dir="auto">In OBS Studio, go to Tools -&gt; VirtualCam. Check AutoStart,</p>
<p dir="auto">set Buffered Frames to 0 and press Start.</p>
</li>
<li>
<p dir="auto">Now <code>OBS-Camera</code> camera should be available in Zoom</p>
<p dir="auto">(or other videoconferencing software).</p>
</li>
</ul>
<h3 dir="auto"><a id="user-content-ubuntu" aria-hidden="true" href="#ubuntu"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Ubuntu</h3>
<div data-snippet-clipboard-copy-content="sudo apt update
sudo apt install v4l-utils v4l2loopback-dkms v4l2loopback-utils
sudo modprobe v4l2loopback devices=1 card_label=&#34;OBS Cam&#34; exclusive_caps=1
v4l2-ctl --list-devices
sudo add-apt-repository ppa:obsproject/obs-studio
sudo apt install obs-studio"><pre>sudo apt update
sudo apt install v4l-utils v4l2loopback-dkms v4l2loopback-utils
sudo modprobe v4l2loopback devices=1 card_label=<span><span>&#34;</span>OBS Cam<span>&#34;</span></span> exclusive_caps=1
v4l2-ctl --list-devices
sudo add-apt-repository ppa:obsproject/obs-studio
sudo apt install obs-studio</pre></div>
<p dir="auto">Open <code>OBS Studio</code> and check if <code>tools --&gt; v4l2sink</code> exists.
If it doesn&#39;t follow these instructions:</p>
<div data-snippet-clipboard-copy-content="mkdir -p ~/.config/obs-studio/plugins/v4l2sink/bin/64bit/
ln -s /usr/lib/obs-plugins/v4l2sink.so ~/.config/obs-studio/plugins/v4l2sink/bin/64bit/"><pre>mkdir -p <span>~</span>/.config/obs-studio/plugins/v4l2sink/bin/64bit/
ln -s /usr/lib/obs-plugins/v4l2sink.so <span>~</span>/.config/obs-studio/plugins/v4l2sink/bin/64bit/</pre></div>
<p dir="auto">Use the virtual camera with <code>OBS Studio</code>:</p>
<ul dir="auto">
<li>Open <code>OBS Studio</code></li>
<li>Go to <code>tools --&gt; v4l2sink</code></li>
<li>Select <code>/dev/video2</code> and <code>YUV420</code></li>
<li>Click on <code>start</code></li>
<li>Join a meeting and select <code>OBS Cam</code></li>
</ul>
<h3 dir="auto"><a id="user-content-macos" aria-hidden="true" href="#macos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MacOS</h3>
<ul dir="auto">
<li>Download and install OBS Studio for MacOS from <a href="https://obsproject.com/" rel="nofollow">here</a></li>
<li>Open OBS and follow the first-time setup (you might be required to enable certain permissions in <em>System Preferences</em>)</li>
<li>Run <em>dot</em> with <code>--use_cam</code> flag to enable camera feed</li>
<li>Click the &#34;+&#34; button in the sources section → select &#34;Windows Capture&#34;, create a new source and enter &#34;OK&#34; → select window with &#34;python&#34; included in the name and enter OK</li>
<li>Click &#34;Start Virtual Camera&#34; button in the controls section</li>
<li>Select &#34;OBS Cam&#34; as default camera in the video settings of the application target of the injection</li>
</ul>
<h2 dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p dir="auto"><em>This is not a commercial Sensity product, and it is distributed freely with no warranties</em></p>
<p dir="auto">The software is distributed under <a href="https://blog.plover.com/sensity-ai/dot/blob/main/LICENSE">BSD 3-Clause</a>.
<em>dot</em> utilizes several open source libraries. If you use <em>dot</em>, make sure you agree with their
licenses too. In particular, this codebase is built on top of the following research projects:</p>
<ul dir="auto">
<li><a href="https://github.com/AliaksandrSiarohin/first-order-model">https://github.com/AliaksandrSiarohin/first-order-model</a></li>
<li><a href="https://github.com/alievk/avatarify-python">https://github.com/alievk/avatarify-python</a></li>
<li><a href="https://github.com/neuralchen/SimSwap">https://github.com/neuralchen/SimSwap</a></li>
<li><a href="https://github.com/yangxy/GPEN">https://github.com/yangxy/GPEN</a></li>
</ul>
<h2 dir="auto"><a id="user-content-contributing" aria-hidden="true" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p dir="auto">This repository follows the <a href="https://google.github.io/styleguide/pyguide.html" rel="nofollow">Google Python Style Guide</a> for code formatting.</p>
<p dir="auto">If you have ideas for improving <em>dot</em>, feel free to open relevant Issues and PRs. Please read <a href="https://blog.plover.com/sensity-ai/dot/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> before contributing to the repository.</p>
<p dir="auto">If you are working on improving the speed of <em>dot</em>, please read first our guide on <a href="https://blog.plover.com/sensity-ai/dot/blob/main/docs/profiling.md">code profiling</a>.</p>
<h3 dir="auto"><a id="user-content-setup-dev-tools" aria-hidden="true" href="#setup-dev-tools"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setup Dev-Tools</h3>
<ol dir="auto">
<li>
<p dir="auto">Install Dev Requirements</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements-dev.txt"><pre>pip install -r requirements-dev.txt</pre></div>
</li>
<li>
<p dir="auto">Install Pre-Commit Hooks</p>
<div data-snippet-clipboard-copy-content="pre-commit install"><pre>pre-commit install</pre></div>
</li>
<li>
<p dir="auto">Run Unit Tests (with coverage)</p>
<div data-snippet-clipboard-copy-content="pytest --cov=dot --cov-report=term --cov-fail-under=10"><pre>pytest --cov=dot --cov-report=term --cov-fail-under=10</pre></div>
</li>
</ol>
<h3 dir="auto"><a id="user-content-list-of-maintainers" aria-hidden="true" href="#list-of-maintainers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>List of maintainers</h3>
<ul dir="auto">
<li><a href="https://github.com/ghassen1302">@ghassen1302</a></li>
<li><a href="https://github.com/vassilispapadop">@vassilispapadop</a></li>
<li><a href="https://github.com/giorgiop">@giorgiop</a></li>
<li><a href="https://github.com/AjinkyaIndulkar">@AjinkyaIndulkar</a></li>
<li><a href="https://github.com/kjod">@kjod</a></li>
</ul>
<h2 dir="auto"><a id="user-content-research" aria-hidden="true" href="#research"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Research</h2>
<ul dir="auto">
<li><a href="https://blog.plover.com/sensity-ai/dot/blob/main/docs/run_without_camera.md">Run <em>dot</em> on image and video files instead of camera feed</a></li>
</ul>
</article>
          </div></div>
  </body>
</html>
