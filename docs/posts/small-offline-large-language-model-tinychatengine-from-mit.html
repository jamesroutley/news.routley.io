<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://graphthinking.blogspot.com/2023/12/small-offline-large-language-model.html">Original</a>
    <h1>Small offline large language model – TinyChatEngine from MIT</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
  
  <p><a href="https://en.wikipedia.org/wiki/BLUF_(communication)">BLUF</a>: TinyChatEngine provides an off-line open-source <a href="https://en.wikipedia.org/wiki/Large_language_model">large language model (LLM)</a> that has been reduced in size. This post provides context and a containerized version for Docker or Podman.</p>

<h2>Who&#39;s who</h2>
<p>HuggingFace is a startup company the CEO describes as &#34;what GitHub has been for previous version of software, but for the AI world.&#34; [<a href="https://www.marketwatch.com/story/open-source-ai-amd-looks-to-hugging-face-and-meta-spinoff-pytorch-to-take-on-nvidia-e4738f87">source</a>]</p><p>Meta (formerly Facebook) is a company that built large language models (LLMs). There is an LLM called LLaMA. Meta&#39;s LLMs are hosted on HuggingFace; see <a href="https://huggingface.co/meta-llama">https://huggingface.co/meta-llama</a>

</p><p>The repo discussed in this post is <a href="https://github.com/mit-han-lab/TinyChatEngine">https://github.com/mit-han-lab/TinyChatEngine</a></p>

  <h2>Models</h2>
  
<p>There are additional floating point 32-bit models available (see <a href="https://github.com/mit-han-lab/TinyChatEngine?tab=readme-ov-file#download-and-deploy-models-from-our-model-zoo">the model zoo table in the README</a>), but for my scope I am looking at just the 4-bit integer models. 
</p>

<p>The following are sorted from newest (CodeLLaMA) to oldest (opt).</p>

<h3>CodeLLaMA</h3>
<p>CodeLLaMA extends LLaMA2 for software tasks; see <a href="https://huggingface.co/blog/codellama">this post</a>.
</p><pre>6.8G  CodeLLaMA_13B_Instruct_awq_int4.zip
3.6G  CodeLLaMA_7B_Instruct_awq_int4.zip
</pre>
<blockquote>&#34;Code Llama is a family of large language models for code based on Llama 2. ... for programming tasks.&#34;
  [<a href="https://github.com/facebookresearch/codellama?tab=readme-ov-file">source</a>]</blockquote>

<h3>LLaMA2</h3>
<p>LLaMA2 was released July 18, 2023 (see <a href="https://huggingface.co/blog/llama2">this post</a>). Llama 2 was trained on 40% more data than LLaMA. Llama 2 is open-source and freely available for commercial and research use. LLaMA2 is roughly equivalent to ChatGPT3.5.
</p><pre>6.6G  LLaMA2_13B_chat_awq_int4.zip
3.6G  LLaMA2_7B_chat_awq_int4.zip
</pre>
<blockquote>&#34;Llama 2 is a collection of pretrained and fine-tuned generative text models. ... Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases.&#34; 
  [<a href="https://huggingface.co/meta-llama">source</a>]</blockquote>

<p>&#34;Llama-2 likely costs $20M+ to train&#34; [<a href="https://twitter.com/DrJimFan/status/1681372700881854465">source</a>]

</p><h3>LLaMA</h3>
<p>LLaMA was released Feb 27, 2023 (see <a href="https://arxiv.org/abs/2302.13971">this paper</a>).
</p><pre>3.7G  LLaMA_7B_awq_int4.zip
</pre>


<h3>OPT</h3>
<p>OPT was released May 3, 2022. OPT has since been superseded by models such as LLaMA.</p>
<pre>152M  opt_125m_awq_int4.zip
903M  opt_1.3B_awq_int4.zip
3.8G  opt_6.7B_awq_int4.zip
</pre>
<blockquote>&#34;Open Pretrained Transformers (OPT) belongs to the same family of decoder-only models like GPT-3.&#34; [<a href="https://huggingface.co/facebook/opt-6.7b">source</a>]</blockquote>

<p>Fun fact: OPT &#34;requires 33 days to fully train ... with 1024 80GB A100s.&#34; If there are 8 GPUs per server, thats a 128 node cluster.  [<a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/10_percent_update.md">source</a>, which is linked from <a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md">this page</a>.]</p>

<p>For a comparison see <a href="https://sapling.ai/llm/llama-vs-opt">https://sapling.ai/llm/llama-vs-opt</a></p>


<h2>Containerized (in Docker)</h2>
<p>Should also work in Podman</p>
<fieldset><legend>Dockerfile</legend>
<pre># https://hub.docker.com/r/phusion/baseimage/tags
FROM phusion/baseimage:18.04-1.0.0

RUN apt-get update &amp;&amp; \
    apt-get install -y \
               python3 \
               python3-dev \
               python3-pip \
               git

WORKDIR /opt
#RUN git clone --recursive https://github.com/mit-han-lab/TinyChatEngine
# or, if you alread did that,
RUN mkdir TinyChatEngine
WORKDIR /opt/TinyChatEngine
COPY TinyChatEngine .

WORKDIR /opt/TinyChatEngine
RUN python3 -m pip install -r requirements.txt

WORKDIR /opt/TinyChatEngine/llm

# the following SED commands disable the deletion of the downloaded model .zip file
#RUN sed -i -e &#39;199 i \ \ \ \ \ \ \ \ pass&#39; tools/download_model.py
#RUN sed -i -e &#39;200,201d&#39; tools/download_model.py

# the list of available models is on https://github.com/mit-han-lab/TinyChatEngine/tree/main?tab=readme-ov-file#download-and-deploy-models-from-our-model-zoo
RUN python3 tools/download_model.py --model CodeLLaMA_13B_Instruct_awq_int4 --QM QM_x86
#RUN python3 tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86

RUN make chat -j

# ./chat
</pre>
</fieldset>
  
<fieldset><legend>Makefile</legend>
<pre>mytag=tinychatengine

docker: docker_build docker_run
docker_build:
	docker build -t $(mytag) .

docker_run:
	docker run -it --rm -v `pwd`:/scratch $(mytag) /bin/bash


download_repo:
	git clone --recursive https://github.com/mit-han-lab/TinyChatEngine

download_models:
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model LLaMA2_13B_chat_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model LLaMA_7B_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model CodeLLaMA_13B_Instruct_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model CodeLLaMA_7B_Instruct_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model opt_6.7B_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model opt_1.3B_awq_int4 --QM QM_x86
	python3 /opt/TinyChatEngine/llm/tools/download_model.py --model opt_125m_awq_int4 --QM QM_x86  
</pre>
</fieldset>

<p>Use:
</p><pre>docker run -it --rm -v `pwd`:/scratch $(mytag) /opt/TinyChatEngine/llm/chat &lt;model_name&gt; &lt;precision&gt; &lt;num_threads&gt;
</pre><p>
as per <a href="https://github.com/mit-han-lab/TinyChatEngine/issues/62">this issue</a></p>
</div></div>
  </body>
</html>
