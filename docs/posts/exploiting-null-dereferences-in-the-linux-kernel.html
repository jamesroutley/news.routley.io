<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://googleprojectzero.blogspot.com/2023/01/exploiting-null-dereferences-in-linux.html">Original</a>
    <h1>Exploiting null-dereferences in the Linux kernel</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-6288849578901159797" itemprop="description articleBody">

 <p><span>Posted by Seth Jenkins, Project Zero</span></p>
 
 <p><span>For a fair amount of time</span><span>, null-deref bugs were a highly exploitable kernel bug class. Back when the kernel was able to access userland memory without restriction, and userland programs were still able to map the zero page, there were many easy techniques for exploiting null-deref bugs. However with the introduction of modern exploit mitigations such as SMEP and SMAP, as well as </span><span>mmap_min_addr</span><span> preventing unprivileged programs from mmap’ing low addresses, null-deref bugs are generally not considered </span><span>a security issue</span><span> in modern kernel versions. </span><span>This blog post provides an exploit technique</span><span> demonstrating that treating these bugs as universally innocuous often leads to faulty evaluations of their relevance to security.</span></p>
 <h2 id="h.m1bt0m324efm"><span>Kernel oops overview</span></h2>
 <p><span>At present, when the Linux kernel triggers a null-deref from within a process context, it generates an </span><span><a href="https://en.wikipedia.org/wiki/Linux_kernel_oops">oops</a></span><span>, which is distinct from a kernel panic. A panic occurs when the kernel determines that there is no safe way to continue execution, and that therefore all execution must </span><span>cease</span><span>. However, the kernel does </span><span>not</span><span> stop all execution during an oops - instead the kernel tries to recover as best as it can and continue </span><span>execution</span><span>. In the case of a task, that involves throwing out the existing kernel stack and going directly to </span><span>make_task_dead</span><span> which calls </span><span>do_exit</span><span>. The kernel will also publish in dmesg a “crash” log and kernel backtrace depicting what state the kernel was in when the oops occurred. This may seem like an odd choice to make when memory corruption has clearly occurred - however the intention is to allow kernel bugs to more easily be detectable and loggable under the </span><span><a href="https://yarchive.net/comp/linux/BUG.html">philosophy that a working system is much easier to debug than a dead one.</a></span></p>
 
 <p><span>The unfortunate side effect of the oops recovery path is that the kernel is not able to perform any associated cleanup that it would normally perform on a typical syscall error recovery path. This means that any locks that were locked at the moment of the oops stay locked, any refcounts remain taken, any memory otherwise temporarily allocated remains allocated, etc. However, the process that generated the oops, its associated kernel stack, task struct and derivative members etc. can and often will be freed, meaning that depending on the precise circumstances of the oops, it’s possible that no memory is actually leaked. This becomes particularly important in regards to exploitation later.</span></p>
 <h2 id="h.eq2rlvs1528n"><span>Reference count mismanagement overview</span></h2>
 <p><span>Refcount mismanagement is a fairly well-known and exploitable issue. In the case where software improperly decrements a refcount, this can lead to a classic UAF primitive. The case where </span><span>software</span><span> improperly </span><span>doesn’t</span><span> decrement</span><span> a refcount (</span><span>leaking a </span><span>reference) is also often exploitable. If the attacker can cause a refcount to be repeatedly improperly incremented, it is possible that given enough effort the refcount may overflow, at which point the software no longer has any remotely sensible idea of how many refcounts are taken on an object.</span><span> In such a case, it is possible for an attacker to destroy the object</span><span> by incrementing and decrementing the refcount back to zero after overflowing, while still holding reachable references to the associated memory. </span><span>32-bit refcounts are particularly vulnerable to this sort of overflow</span><span>. It is important however, that each increment of the refcount allocates little or no </span><span>physical memory</span><span>. </span><span>Even a single byte allocation is quite expensive if it must be performed 2</span><span>32</span><span> times</span><span>.</span></p><h2 id="h.35si9vu7f0jz"><span>Example null-deref bug</span></h2>
 <p><span>When a kernel oops unceremoniously ends a task, any refcounts that the task was holding remain held, even though all memory associated with the task may be freed when the task exits. Let’s look at </span><span>an example</span><span> - an otherwise unrelated bug I coincidentally discovered in the very recent past:</span></p>
 <div colspan="1" rowspan="1">
 <p><span>static</span><span> </span><span>int</span><span> show_smaps_rollup</span><span>(</span><span>struct</span><span> seq_file </span><span>*</span><span>m</span><span>,</span><span> </span><span>void</span><span> </span><span>*</span><span>v</span><span>)</span></p>
 <p><span>{</span></p>
 <p><span>        </span><span>struct</span><span> proc_maps_private </span><span>*</span><span>priv </span><span>=</span><span> m</span><span>-&gt;</span><span>private</span><span>;</span></p>
 <p><span>        </span><span>struct</span><span> mem_size_stats mss</span><span>;</span></p>
 <p><span>        </span><span>struct</span><span> mm_struct </span><span>*</span><span>mm</span><span>;</span></p>
 <p><span>        </span><span>struct</span><span> vm_area_struct </span><span>*</span><span>vma</span><span>;</span></p>
 <p><span>        </span><span>unsigned</span><span> </span><span>long</span><span> last_vma_end </span><span>=</span><span> </span><span>0</span><span>;</span></p>
 <p><span>        </span><span>int</span><span> ret </span><span>=</span><span> </span><span>0</span><span>;</span></p>
 
 <p><span>        priv</span><span>-&gt;</span><span>task </span><span>=</span><span> get_proc_task</span><span>(</span><span>priv</span><span>-&gt;</span><span>inode</span><span>);</span><span> </span><span>//task reference taken</span></p>
 <p><span>        </span><span>if</span><span> </span><span>(!</span><span>priv</span><span>-&gt;</span><span>task</span><span>)</span></p>
 <p><span>                </span><span>return</span><span> </span><span>-</span><span>ESRCH</span><span>;</span></p>
 
 <p><span>        mm </span><span>=</span><span> priv</span><span>-&gt;</span><span>mm</span><span>;</span><span> </span><span>//With no vma&#39;s, mm-&gt;mmap is NULL</span></p>
 <p><span>        </span><span>if</span><span> </span><span>(!</span><span>mm </span><span>||</span><span> </span><span>!</span><span>mmget_not_zero</span><span>(</span><span>mm</span><span>))</span><span> </span><span>{</span><span> </span><span>//mm reference taken</span></p>
 <p><span>                ret </span><span>=</span><span> </span><span>-</span><span>ESRCH</span><span>;</span></p>
 <p><span>                </span><span>goto</span><span> out_put_task</span><span>;</span></p>
 <p><span>        </span><span>}</span></p>
 
 <p><span>        memset</span><span>(&amp;</span><span>mss</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>mss</span><span>));</span></p>
 
 <p><span>        ret </span><span>=</span><span> mmap_read_lock_killable</span><span>(</span><span>mm</span><span>);</span><span> </span><span>//mmap read lock taken</span></p>
 <p><span>        </span><span>if</span><span> </span><span>(</span><span>ret</span><span>)</span></p>
 <p><span>                </span><span>goto</span><span> out_put_mm</span><span>;</span></p>
 
 <p><span>        hold_task_mempolicy</span><span>(</span><span>priv</span><span>);</span></p>
 
 <p><span>        </span><span>for</span><span> </span><span>(</span><span>vma </span><span>=</span><span> priv</span><span>-&gt;</span><span>mm</span><span>-&gt;</span><span>mmap</span><span>;</span><span> vma</span><span>;</span><span> vma </span><span>=</span><span> vma</span><span>-&gt;</span><span>vm_next</span><span>)</span><span> </span><span>{</span></p>
 <p><span>                smap_gather_stats</span><span>(</span><span>vma</span><span>,</span><span> </span><span>&amp;</span><span>mss</span><span>);</span></p>
 <p><span>                last_vma_end </span><span>=</span><span> vma</span><span>-&gt;</span><span>vm_end</span><span>;</span></p>
 <p><span>        </span><span>}</span></p>
 
 <p><span>        show_vma_header_prefix</span><span>(</span><span>m</span><span>,</span><span> priv</span><span>-&gt;</span><span>mm</span><span>-&gt;</span><span>mmap</span><span>-&gt;</span><span>vm_start</span><span>,</span><span>last_vma_end</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0</span><span>);</span><span> </span><span>//the deref of mmap causes a kernel oops here</span></p>
 <p><span>        seq_pad</span><span>(</span><span>m</span><span>,</span><span> </span><span>&#39; &#39;</span><span>);</span></p>
 <p><span>        seq_puts</span><span>(</span><span>m</span><span>,</span><span> </span><span>&#34;[rollup]\n&#34;</span><span>);</span></p>
 
 <p><span>        __show_smap</span><span>(</span><span>m</span><span>,</span><span> </span><span>&amp;</span><span>mss</span><span>,</span><span> </span><span>true</span><span>);</span></p>
 
 <p><span>        release_task_mempolicy</span><span>(</span><span>priv</span><span>);</span></p>
 <p><span>        mmap_read_unlock</span><span>(</span><span>mm</span><span>);</span></p>
 
 <p><span>out_put_mm</span><span>:</span></p>
 <p><span>        mmput</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>out_put_task</span><span>:</span></p>
 <p><span>        put_task_struct</span><span>(</span><span>priv</span><span>-&gt;</span><span>task</span><span>);</span></p>
 <p><span>        priv</span><span>-&gt;</span><span>task </span><span>=</span><span> NULL</span><span>;</span></p>
 
 <p><span>        </span><span>return</span><span> ret</span><span>;</span></p>
 <p><span>}</span></p></div>
 
 <p><span>This file is intended simply to print a set of memory usage statistics for the respective process. Regardless, </span><span><a href="https://lore.kernel.org/lkml/20221003224531.1930646-1-sethjenkins@google.com/T/">this bug report</a></span><span> reveals a classic and otherwise innocuous null-deref bug within this function. In the case of a task that has no VMA’s mapped at all, the task’s </span><span>mm_struct mmap</span><span> member will be equal to NULL. Thus the </span><span>priv-&gt;mm-&gt;mmap-&gt;vm_start</span><span> access causes a null dereference and consequently a kernel oops. This bug can be triggered by simply </span><span>read</span><span>’ing </span><span>/proc/[pid]/smaps_rollup</span><span> on a task with no VMA’s (which itself can be stably created via </span><span>ptrace</span><span>):</span></p>
 
 <p><span><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW-7Gf0dWd5QJfxXYwRAdtMX7uoynsldEgf9C7saF7MLL_iwIUVaagMyOAa8r-e4_KDf33viQn5U2w1LirUQnx7Vk1fJgpACWUijAl8Pgtiu8ZrDTaiEzJYSo3pJUX_nh4Mh4EQ2GPclwSCHcA-OmGfexegm4WykTZyKnIq9ktGTiLcCJo0QW0VKx6/s1138/image1.png"><img alt="Kernel log showing the oops condition backtrace" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW-7Gf0dWd5QJfxXYwRAdtMX7uoynsldEgf9C7saF7MLL_iwIUVaagMyOAa8r-e4_KDf33viQn5U2w1LirUQnx7Vk1fJgpACWUijAl8Pgtiu8ZrDTaiEzJYSo3pJUX_nh4Mh4EQ2GPclwSCHcA-OmGfexegm4WykTZyKnIq9ktGTiLcCJo0QW0VKx6/s1138/image1.png" title="Kernel log showing the oops condition backtrace"/></a></span></p>
 
 <p><span>This kernel oops will mean that the following events occur:</span></p><ol start="1"><li><span>The associated </span><span>struct file </span><span>will have a refcount leaked if </span><span>fdget</span><span> took a refcount </span><span>(we’ll try and make sure this doesn’t happen later)</span></li><li><span>The associated </span><span>seq_file</span><span> within the </span><span>struct file</span><span> has a mutex that will forever be locked (any future reads/writes/lseeks etc. will hang forever).</span></li><li><span>The </span><span>task struct</span><span> associated with the </span><span>smaps_rollup</span><span> file will have a refcount leaked</span></li><li><span>The </span><span>mm_struct’s mm_users</span><span> refcount</span><span> associated with the task will be leaked</span></li><li><span>The </span><span>mm_struct’s</span><span> </span><span>mmap</span><span> lock will be permanently readlocked (any future write-lock attempts will hang forever)</span></li></ol>
 
 <p><span>Each of these conditions is an unintentional side-effect that leads to buggy behaviors, but not all of those behaviors are useful to an attacker. </span><span>The permanent locking of events 2 and 5 only makes exploitation more difficult. Condition 1 is unexploitable because we cannot leak the struct file refcount again without taking a mutex that will never be unlocked. Condition 3 is unexploitable because a task struct uses a safe saturating kernel </span><span>refcount_t</span><span> which prevents the overflow condition. This leaves condition 4. </span></p>
 <p><span><br/></span><span>The </span><span>mm_users</span><span> refcount still </span><span>uses an overflow-unsafe </span><span>atomic_t</span><span> and since we can take a readlock an indefinite number of times, the associated </span><span>mmap_read_lock</span><span> does not prevent us from incrementing the refcount again. There are a couple important roadblocks we need to avoid in order to repeatedly leak this refcount:</span></p>
 <ol start="1"><li><span>We cannot call this syscall from the task with the empty vma list itself - in other words, we can’t call </span><span>read</span><span> from </span><span>/proc/self/smaps_rollup</span><span>. Such a process cannot easily make repeated syscalls since it has no virtual memory mapped. We avoid this by reading </span><span>smaps_rollup</span><span> from another process.</span></li><li><span>We must re-open the </span><span>smaps_rollup</span><span> file every time because any future reads we perform on a </span><span>smaps_rollup</span><span> instance we already triggered the oops on will deadlock on the local </span><span>seq_file</span><span> mutex lock which is locked forever. We also need to destroy the resulting </span><span>struct file</span><span> (via </span><span>close</span><span>) after we generate the oops in order to prevent untenable memory usage.</span></li><li><span>If we access the </span><span>mm</span><span> through the same pid every time, we will run into the </span><span>task struct</span><span> </span><span>max refcount before we overflow the </span><span>mm_users</span><span> refcount. Thus we need to create two separate tasks that share the same </span><span>mm</span><span> and balance the oopses we generate across both tasks so the task refcounts grow half as quickly as the </span><span>mm_users</span><span> refcount. We do this via the </span><span>clone</span><span> flag </span><span>CLONE_VM</span></li><li><span>We must avoid opening/reading the </span><span>smaps_rollup</span><span> file from a task that has a shared file descriptor table, as otherwise a refcount will be leaked on the </span><span>struct file</span><span> itself. This isn’t difficult, just don’t </span><span>read</span><span> the file from a multi-threaded process.</span></li></ol>
 
 <p><span>Our final refcount leaking overflow strategy is as follows:<br/></span></p><ol start="1"><li><span>Process A forks a process B</span></li><li><span>Process B issues </span><span>PTRACE_TRACEME</span><span> so that when it segfaults upon return from </span><span>munmap</span><span> it won’t go away (but rather will enter tracing stop)</span></li><li><span>Proces B clones with </span><span>CLONE_VM | CLONE_PTRACE</span><span> another process C</span></li><li><span>Process B </span><span>munmap</span><span>’s its entire virtual memory address space - this also unmaps process C’s virtual memory address space.</span></li><li><span>Process A forks new children D and E which will access (B|C)’s </span><span>smaps_rollup</span><span> file respectively</span></li><li><span>(D|E) opens (B|C)’s </span><span>smaps_rollup</span><span> file and performs a read which will oops, causing (D|E) to die. </span><span>mm_users</span><span> will be refcount leaked/incremented once per oops</span></li><li><span>Process A goes back to step 5 ~2</span><span>32</span><span> times</span></li></ol>
 
 <p><span>The above strategy can be </span><span>rearchitected</span><span> to run in parallel (across </span><span>processes</span><span> not threads, because of roadblock 4) and improve performance.</span><span> On server setups that print kernel logging to a serial console, generating 2</span><span>32</span><span> kernel oopses takes over 2 years. However on a vanilla Kali Linux box using a graphical interface, a demonstrative proof-of-concept takes only about 8 days to complete! At the completion of execution, </span><span>the </span><span>mm_users</span><span> refcount will have </span><span>overflow</span><span>ed</span><span> and be set to zero, even though this </span><span>mm</span><span> is currently in use by</span><span> multiple processes and can still be referenced via the </span><span>proc</span><span> filesystem.</span></p><h2 id="h.6a9h8qcml5lm"><span>Exploitation</span></h2>
 <p><span>Once the </span><span>mm_users</span><span> refcount has been set to zero, triggering undefined behavior and memory corruption </span><span>should</span><span> be fairly easy. By triggering an </span><span>mmget</span><span> and an </span><span>mmput</span><span> (which we can very easily do by opening the </span><span>smaps_rollup</span><span> file once more) we should be able to free the entire </span><span>mm</span><span> and cause a UAF condition:</span></p>
 <div colspan="1" rowspan="1">
 <p><span>static</span><span> </span><span>inline</span><span> </span><span>void</span><span> __mmput</span><span>(</span><span>struct</span><span> mm_struct </span><span>*</span><span>mm</span><span>)</span></p>
 <p><span>{</span></p>
 <p><span>        VM_BUG_ON</span><span>(</span><span>atomic_read</span><span>(&amp;</span><span>mm</span><span>-&gt;</span><span>mm_users</span><span>));</span></p>
 
 <p><span>        uprobe_clear_state</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        exit_aio</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        ksm_exit</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        khugepaged_exit</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        exit_mmap</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        mm_put_huge_zero_page</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        set_mm_exe_file</span><span>(</span><span>mm</span><span>,</span><span> NULL</span><span>);</span></p>
 <p><span>        </span><span>if</span><span> </span><span>(!</span><span>list_empty</span><span>(&amp;</span><span>mm</span><span>-&gt;</span><span>mmlist</span><span>))</span><span> </span><span>{</span></p>
 <p><span>                spin_lock</span><span>(&amp;</span><span>mmlist_lock</span><span>);</span></p>
 <p><span>                list_del</span><span>(&amp;</span><span>mm</span><span>-&gt;</span><span>mmlist</span><span>);</span></p>
 <p><span>                spin_unlock</span><span>(&amp;</span><span>mmlist_lock</span><span>);</span></p>
 <p><span>        </span><span>}</span></p>
 <p><span>        </span><span>if</span><span> </span><span>(</span><span>mm</span><span>-&gt;</span><span>binfmt</span><span>)</span></p>
 <p><span>                module_put</span><span>(</span><span>mm</span><span>-&gt;</span><span>binfmt</span><span>-&gt;</span><span>module</span><span>);</span></p>
 <p><span>        lru_gen_del_mm</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>        mmdrop</span><span>(</span><span>mm</span><span>);</span></p>
 <p><span>}</span></p></div>
 
 <p><span>Unfortunately, since </span><span>64591e8605</span><span> (“mm: protect free_pgtables with mmap_lock write lock in exit_mmap”)</span><span>, </span><span>exit_mmap</span><span> unconditionally takes the </span><span>mmap lock</span><span> in write mode. Since this </span><span>mm’s</span><span> </span><span>mmap_lock</span><span> is permanently readlocked </span><span>many</span><span> times, any calls to </span><span>__mmput</span><span> will manifest as a permanent deadlock inside of </span><span>exit_mmap</span><span>.</span></p>
 
 <p><span>However, before the call permanently deadlocks, it will call several other functions:</span></p><ol start="1"><li><span>u</span><span>probe_clear_state</span></li><li><span>exit_aio</span></li><li><span>ksm_exit</span></li><li><span>khugepaged_exit</span></li></ol>
 <p><span>Additionally, we can call </span><span>__mmput</span><span> on this </span><span>mm</span><span> from several tasks simultaneously by having each of them trigger an </span><span>mmget/mmput</span><span> on the </span><span>mm</span><span>, generating irregular race conditions. Under normal execution, it should not be possible to trigger multiple </span><span>__mmput’s</span><span> on the same </span><span>mm</span><span> (much less concurrent ones) as </span><span>__mmput</span><span> should only be called on the last and only refcount decrement which sets the refcount to zero. However, after the refcount overflow, all </span><span>mmget</span><span>/</span><span>mmput’s</span><span> on the still-referenced </span><span>mm</span><span> will trigger an </span><span>__mmput</span><span>. This is because each </span><span>mmput</span><span> that decrements the refcount to zero </span><span>(despite the corresponding </span><span>mmget</span><span> being why the refcount was above zero in the first place)</span><span> believes that it is solely responsible for freeing the associated </span><span>mm</span><span>.</span></p>
 
 
 <p><span><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-cmLd84fS7B5_N1JRJKV3ChuYXr24FB8Yfn7ALBrmzk71O7MXZulCtaF6csA9A3021X9bEruV8iJ2aUBK2q16MfhkL5r1ofXrXEpImakAJPFNmMkeDRK1m3IAPYA3xYDOxP236HaRgDBqXsf_V9wcE9xu9sLSrNIh2hmEqbphe6CwOv7Ld9E0nv2c/s836/image3.png"><img alt="Flowchart showing how mmget/mmput on a 0 refcount leads to unsafe concurrent __mmput calls." src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-cmLd84fS7B5_N1JRJKV3ChuYXr24FB8Yfn7ALBrmzk71O7MXZulCtaF6csA9A3021X9bEruV8iJ2aUBK2q16MfhkL5r1ofXrXEpImakAJPFNmMkeDRK1m3IAPYA3xYDOxP236HaRgDBqXsf_V9wcE9xu9sLSrNIh2hmEqbphe6CwOv7Ld9E0nv2c/s836/image3.png" title="Flowchart showing how mmget/mmput on a 0 refcount leads to unsafe concurrent __mmput calls."/></a></span></p>
 
 <p><span>This racy </span><span>__mmput</span><span> primitive extends to its callees as well. </span><span>exit_aio</span><span> is a good candidate for taking advantage of this:</span></p>
 <div colspan="1" rowspan="1">
 <p><span>void</span><span> exit_aio</span><span>(</span><span>struct</span><span> mm_struct </span><span>*</span><span>mm</span><span>)</span></p>
 <p><span>{</span></p>
 <p><span>        </span><span>struct</span><span> kioctx_table </span><span>*</span><span>table </span><span>=</span><span> rcu_dereference_raw</span><span>(</span><span>mm</span><span>-&gt;</span><span>ioctx_table</span><span>);</span></p>
 <p><span>        </span><span>struct</span><span> ctx_rq_wait wait</span><span>;</span></p>
 <p><span>        </span><span>int</span><span> i</span><span>,</span><span> skipped</span><span>;</span></p>
 
 <p><span>        </span><span>if</span><span> </span><span>(!</span><span>table</span><span>)</span></p>
 <p><span>                </span><span>return</span><span>;</span></p>
 
 <p><span>        atomic_set</span><span>(&amp;</span><span>wait</span><span>.</span><span>count</span><span>,</span><span> table</span><span>-&gt;</span><span>nr</span><span>);</span></p>
 <p><span>        init_completion</span><span>(&amp;</span><span>wait</span><span>.</span><span>comp</span><span>);</span></p>
 
 <p><span>        skipped </span><span>=</span><span> </span><span>0</span><span>;</span></p>
 <p><span>        </span><span>for</span><span> </span><span>(</span><span>i </span><span>=</span><span> </span><span>0</span><span>;</span><span> i </span><span>&lt;</span><span> table</span><span>-&gt;</span><span>nr</span><span>;</span><span> </span><span>++</span><span>i</span><span>)</span><span> </span><span>{</span></p>
 <p><span>                </span><span>struct</span><span> kioctx </span><span>*</span><span>ctx </span><span>=</span></p>
 <p><span>                rcu_dereference_protected</span><span>(</span><span>table</span><span>-&gt;</span><span>table</span><span>[</span><span>i</span><span>],</span><span> </span><span>true</span><span>);</span></p>
 
 <p><span>                </span><span>if</span><span> </span><span>(!</span><span>ctx</span><span>)</span><span> </span><span>{</span></p>
 <p><span>                        skipped</span><span>++;</span></p>
 <p><span>                        </span><span>continue</span><span>;</span></p>
 <p><span>                </span><span>}</span></p>
 <p><span>                ctx</span><span>-&gt;</span><span>mmap_size </span><span>=</span><span> </span><span>0</span><span>;</span></p>
 <p><span>                kill_ioctx</span><span>(</span><span>mm</span><span>,</span><span> ctx</span><span>,</span><span> </span><span>&amp;</span><span>wait</span><span>);</span></p>
 <p><span>        </span><span>}</span></p>
 
 <p><span>        </span><span>if</span><span> </span><span>(!</span><span>atomic_sub_and_test</span><span>(</span><span>skipped</span><span>,</span><span> </span><span>&amp;</span><span>wait</span><span>.</span><span>count</span><span>))</span><span> </span><span>{</span></p>
 <p><span>                </span><span>/* Wait until all IO for the context are done. */</span></p>
 <p><span>                wait_for_completion</span><span>(&amp;</span><span>wait</span><span>.</span><span>comp</span><span>);</span></p>
 <p><span>        </span><span>}</span></p>
 
 <p><span>        RCU_INIT_POINTER</span><span>(</span><span>mm</span><span>-&gt;</span><span>ioctx_table</span><span>,</span><span> NULL</span><span>);</span></p>
 <p><span>        kfree</span><span>(</span><span>table</span><span>);</span></p>
 <p><span>}</span></p></div>
 
 <p><span>While the callee function </span><span>kill_ioctx</span><span> is written in such a way to prevent concurrent execution from causing memory corruption (part of the contract of </span><span><a href="https://man7.org/linux/man-pages/man7/aio.7.html">aio</a></span><span> allows for </span><span>kill_ioctx</span><span> to be called in a concurrent way), </span><span>exit_aio</span><span> itself makes no such guarantees. Two concurrent calls of </span><span>exit_aio</span><span> on the same </span><span>mm</span><span> </span><span>struct</span><span> can consequently</span><span> </span><span>induce</span><span> a double free of the </span><span>mm-&gt;ioctx_table</span><span> object, which is fetched at the beginning of the function, while only being freed at the very end. This race window can be widened substantially by creating many </span><span>aio</span><span> contexts in order to slow down </span><span>exit_aio’s</span><span> internal context freeing loop. Successful exploitation will trigger the following kernel </span><span>BUG</span><span> indicating that a double free has occurred:</span></p>
 
 
 <p><span><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyX2lmxdi_963UE1t88A8xenzG7zPNT8Ou2Hvspij4S2FIc_L-y1NsdEy5obGxFNaG3W8X0TpL1TubA64elOjSkJ8FAgPrrDp5B9e1dYtRiM53tlGD1eT8m_2Md9GFS_Hoye-VdLvpZfkII_Qh4eKYSaVDefbE4ttCmgrF-Hkft2ILgW2dTdcIUxgG/s1437/image2.png"><img alt="Kernel backtrace showing the double free condition detection" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyX2lmxdi_963UE1t88A8xenzG7zPNT8Ou2Hvspij4S2FIc_L-y1NsdEy5obGxFNaG3W8X0TpL1TubA64elOjSkJ8FAgPrrDp5B9e1dYtRiM53tlGD1eT8m_2Md9GFS_Hoye-VdLvpZfkII_Qh4eKYSaVDefbE4ttCmgrF-Hkft2ILgW2dTdcIUxgG/s1200/image2.png" title="Kernel backtrace showing the double free condition detection"/></a></span></p>
 
 <p><span>Note that as this </span><span>exit_aio</span><span> path is hit from </span><span>__mmput</span><span>, triggering this race will produce at least two permanently deadlocked processes when those processes later try to take the mmap write lock. However, from an exploitation perspective, this is irrelevant as the memory corruption primitive has already occurred before the deadlock occurs. Exploiting the resultant primitive would probably involve racing a reclaiming allocation in between the two frees of the </span><span>mm-&gt;ioctx_table</span><span> object, then taking advantage of the resulting UAF condition of the reclaimed allocation. It is </span><span>undoubtedly</span><span> possible, although I didn’t take this all the way to a completed PoC.</span></p><h2 id="h.hesv56ci2m4m"><span>Conclusion</span></h2>
 <p><span>While the null-dereference bug itself was </span><span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/fs/proc/task_mmu.c?h=v5.15.86&amp;id=33fc9e26b7cb39f0d4219c875a2451802249c225">fixed in October 2022</a></span><span>, the more important fix was the </span><span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.2-rc3&amp;id=48ea09cddae0b794cde2070f106ef676703dbcd3">introduction of an oops limit</a></span><span> which causes the kernel to panic if too many oopses occur. While this patch is already upstream, it is </span><span>important</span><span> that distributed kernels also inherit this oops limit and backport it to LTS releases if we want to avoid treating such null-dereference bugs as full-fledged security issues in the future. Even in that best-case scenario, it is nevertheless highly beneficial for security researchers to carefully evaluate the side-effects of bugs discovered in the future that are similarly “harmless” and ensure that the </span><span>abrupt</span><span> halt of kernel code execution caused by a kernel oops does not lead to other security-relevant primitives.</span></p>

</div></div>
  </body>
</html>
