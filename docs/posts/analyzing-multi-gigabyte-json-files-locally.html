<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thenybble.de/posts/json-analysis/">Original</a>
    <h1>Analyzing multi-gigabyte JSON files locally</h1>
    
    <div id="readability-page-1" class="page"><div>

<article>
   
  <div><p>I’ve had the pleasure of having had to analyse multi-gigabyte JSON dumps in a project context recently. <a href="https://www.json.org/json-en.html">JSON</a> itself is actually a rather pleasant format to consume, as it’s human-readable and there is a <em>lot</em> of tooling available for it. <a href="https://stedolan.github.io/jq/">JQ</a> allows expressing sophisticated processing steps in a single command line, and <a href="http://jupyter.org/">Jupyter</a> with Python and <a href="https://pandas.pydata.org/">Pandas</a> allow easy interactive analysis to quickly find what you’re looking for.</p>
<p>However, with multi-gigabyte files, analysis becomes quite a lot more difficult. Running a single
<code>jq</code> command will take a long time. When you’re ~trial-and-error~iteratively building <code>jq</code> commands
as I do, you’ll quickly grow tired of having to wait about a minute for your command to succeed,
only to find out that it didn’t in fact return what you were looking for. Interactive analysis is
similar. Reading all 20 gigabyte of JSON will take a fair amount of time. You might find out that
the data doesn’t fit into RAM (which it well might, JSON is a human-readable format after all), or
end up having to restart your Python kernel, which means you’ll have to endure the loading time
again.</p>
<p>Of course, there’s cloud-based offerings that are based on Apache <a href="https://beam.apache.org/">Beam</a>,
<a href="https://flink.apache.org/">Flink</a> and many others. However, customer data doesn’t go on cloud
services on my authority, so that’s out. Setting up an environment like Flink locally is doable, but a
lot of effort for a one-off analysis.</p>
<p>While trying to analyse files of this size, I’ve found two ways of doing efficient local processing of very large JSON files that I want to share. One is based on parallelizing the <code>jq</code> command line with <a href="https://www.gnu.org/software/parallel/">GNU parallel</a>, the other is based on Jupyter with the <a href="https://www.dask.org/">Dask</a> library.</p>
<h2 id="in-the-beginning-was-the-command-line-jq-and-parallel">In the Beginning was the Command Line: JQ and Parallel</h2>
<p>I try to find low-effort solutions to problems first, and most of the tasks I had for the JSON files were simple transformations that are easily expressible in <code>jq</code>’s language. Extracting nested values or searching for specific JSON objects is very easily accomplished. As an example, imagine having 20 Gigabytes of structures like this (I’ve inserted the newlines for readability, the input we’re actually reading is all on one line):</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>  <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>  <span>&#34;artCode&#34;</span>: <span>&#34;124546&#34;</span>,
</span></span><span><span>  <span>&#34;status&#34;</span>: <span>&#34;AVAILABLE&#34;</span>,
</span></span><span><span>  <span>&#34;description&#34;</span>: <span>&#34;A Windows XP sweater&#34;</span>,
</span></span><span><span>  <span>&#34;brandName&#34;</span>: <span>&#34;Microsoft&#34;</span>,
</span></span><span><span>  <span>&#34;subArts&#34;</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>      <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>      <span>&#34;subCode&#34;</span>: <span>&#34;123748&#34;</span>,
</span></span><span><span>      <span>&#34;color&#34;</span>: <span>&#34;green&#34;</span>,
</span></span><span><span>      <span>&#34;subSubArts&#34;</span>: [
</span></span><span><span>        {
</span></span><span><span>          <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;code&#34;</span>: <span>&#34;12876&#34;</span>,
</span></span><span><span>          <span>&#34;size&#34;</span>: <span>&#34;droopy&#34;</span>,
</span></span><span><span>          <span>&#34;currency&#34;</span>: <span>&#34;EUR&#34;</span>,
</span></span><span><span>          <span>&#34;currentPrice&#34;</span>: <span>35</span>
</span></span><span><span>        },
</span></span><span><span>        {
</span></span><span><span>          <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;code&#34;</span>: <span>&#34;12876&#34;</span>,
</span></span><span><span>          <span>&#34;size&#34;</span>: <span>&#34;snug&#34;</span>,
</span></span><span><span>          <span>&#34;currency&#34;</span>: <span>&#34;EUR&#34;</span>,
</span></span><span><span>          <span>&#34;currentPrice&#34;</span>: <span>30</span>
</span></span><span><span>        }
</span></span><span><span>      ]
</span></span><span><span>    },
</span></span><span><span>    {
</span></span><span><span>      <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>      <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>      <span>&#34;subCode&#34;</span>: <span>&#34;123749&#34;</span>,
</span></span><span><span>      <span>&#34;color&#34;</span>: <span>&#34;grey&#34;</span>,
</span></span><span><span>      <span>&#34;subSubArts&#34;</span>: [
</span></span><span><span>        {
</span></span><span><span>          <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;code&#34;</span>: <span>&#34;12879&#34;</span>,
</span></span><span><span>          <span>&#34;size&#34;</span>: <span>&#34;droopy&#34;</span>,
</span></span><span><span>          <span>&#34;currency&#34;</span>: <span>&#34;EUR&#34;</span>,
</span></span><span><span>          <span>&#34;currentPrice&#34;</span>: <span>40</span>
</span></span><span><span>        },
</span></span><span><span>        {
</span></span><span><span>          <span>&#34;created_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;modified_at&#34;</span>: <span>1678184483</span>,
</span></span><span><span>          <span>&#34;code&#34;</span>: <span>&#34;12876&#34;</span>,
</span></span><span><span>          <span>&#34;size&#34;</span>: <span>&#34;snug&#34;</span>,
</span></span><span><span>          <span>&#34;currency&#34;</span>: <span>&#34;EUR&#34;</span>,
</span></span><span><span>          <span>&#34;currentPrice&#34;</span>: <span>35</span>
</span></span><span><span>        }
</span></span><span><span>      ]
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>A <code>jq</code> query like <code>.subArts[]|select(.subSubArts[].size|contains(&#34;snug&#34;))</code> will give you all subarticles having a subsubarticle with a size of “snug”. Running a similar command on a 10-gigabyte JSON file took about three minutes, which isn’t great, especially when you’re impatient (like I happen to be).</p>
<p>Luckily, we can speed this up, if we have some information about the structure of the input file (we know the format is JSON, obviously). We’re using <code>jq</code> as a filter for single JSON objects, which means that we should be able to efficiently parallelize the search expression. Whenever I have to run shell commands in parallel, I reach for <a href="https://www.gnu.org/software/parallel/">GNU parallel</a>, which can handle shell commands, SSH access to remote servers for a DIY cluster, SQL insertion and lots more.</p>
<p>In this case, we know that our JSON objects in the file are delimited by a closing curly bracket
followed by a newline, one JSON object per line. This means that we can tell <code>parallel</code> to run <code>jq</code>
in parallel on these JSON objects with the <code>--recend</code> switch. Note that you could also tell parallel
to interpret <code>--recend</code> as a regular expression, which would allow you to correctly split the
pretty-printed example above with a <code>--recend</code> of <code>^}\n</code>. This is probably substantially slower, I
wouldn’t use a tool that spits out 10 gigabyte of <em>pretty-printed</em> JSON, and if necessary, I would
just use <code>jq -c</code> to collapse it again.</p>
<p>Spawning a single <code>jq</code> process for every JSON object would <em>not</em> lead to a speedup (because
executing new processes is expensive), which is why we tell <code>parallel</code> to collect complete objects
into blocks, and pass those to a <code>jq</code> process. The optimal block size will depend on the size of the
input file, the throughput of your disk, your number of processors, and others. I’ve had sufficient
speedup with a block size of 100 megabyte, but choosing a larger block size would probably not hurt.
<code>Parallel</code> can split up files in an efficient manner using the <code>--pipe-part</code> option (for the reasons
as to why this is more efficient, see
<a href="https://www.gnu.org/software/parallel/parallel_design.html#pipepart-vs-pipe">here</a>), so we can use this to provide input to our parallel <code>jq</code> processes.</p>
<p>Finally, the worst part of every parallel job: Ordering the results. <code>Parallel</code> has lots of options
for this. We want to keep our output in the original order, so we add the <code>--keep-order</code> argument.
The default configuration, <code>--group</code>, would buffer input for each job until it is finished.
Depending on your exact query, this will require buffering to disk if the query output can’t fit in
main memory. This is probably not the case, so using <code>--group</code> would be fine. However, we can do
slightly better with <code>--line-buffer</code>, which, in combination with <code>--keep-order</code>, starts output for
the first job immediately, and buffers output for other jobs. This should require slightly less disk
space or memory, at the cost of some CPU time. Both will be fine for “normal” queries, but do some
benchmarking if your query generates large amounts of output.</p>
<p>Finally, provide the input file with <code>--arg-file</code>. Putting it all together, we get our finished command line:</p>
<pre tabindex="0"><code>parallel -a &#39;&lt;file&gt;&#39; --pipepart --keep-order --line-buffer --block 100M --recend &#39;}\n&#39; &#34;jq &#39;&lt;query&gt;&#39;&#34;
</code></pre><p>This will run <code>jq</code> in parallel on your file on blocks of 100 megabyte, always containing complete JSON objects. You’ll get your query results in the original order, but much quicker than in the non-parallel case. Running on a 8-core/16-thread Ryzen processor, parallelizing the query from above leads to a run time of 30 seconds, which is a speedup of roughly 6<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. Not bad for some shell magic, eh? And here’s a <code>htop</code> screenshot showing glorious parallelization.</p>
<p><img loading="lazy" src="http://jimkang.com/weblog/articles/lets-have-fun-why-not/screenshot-jq.png" alt="screenshot"/>
</p>
<p>Also note that this approach generalizes to other text-based formats. If you have 10 gigabyte of CSV, you can use <a href="https://github.com/johnkerl/miller">Miller</a> for processing. For binary formats, you could use <a href="https://github.com/wader/fq">fq</a> if you can find a workable record separator.</p>
<h2 id="the-notebook-jupyter-and-dask">The Notebook: Jupyter and Dask</h2>
<p>Using GNU parallel is nifty, but for interactive analyses, I prefer Python and Jupyter notebooks. One way of using a notebook with such a large file would be preprocessing it with the <code>parallel</code> magic from the previous section. However, I prefer not having to switch environments while doing data analysis, and using your shell history as documentation is not a sustainable practice (ask me how I know).</p>
<p>Naively reading 9 gigabytes of JSON data with Pandas’ <code>read_json</code> quickly exhausts my 30 gigabytes
of RAM, so there is clearly need for some preprocessing. Again, doing this preprocessing in an
iterative fashion would be painful if we had to process the whole JSON file again to see our
results. We could write some code to only process the first <code>n</code> lines of the JSON file, but I was looking for a more general solution. I’ve mentioned Beam and Flink above, but had no success trying to get a local setup to work.</p>
<p><a href="https://www.dask.org/">Dask</a> does what we want: It can partition large datasets, process the
partitions in parallel, and merge them back together to get our final output. Let’s create a new Python environment with <code>pipenv</code>, install the necessary dependencies and launch a Jupyter notebook:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>pipenv lock
</span></span><span><span>pipenv install jupyterlab dask<span>[</span>distributed<span>]</span> bokeh pandas numpy seaborn
</span></span><span><span>pipenv run jupyter lab
</span></span></code></pre></div><p>If <code>pipenv</code> is not available, follow the <a href="https://pipenv.pypa.io/en/latest/install/">installation
instructions</a> to get it set up on your machine. Now, we can get started. We import necessary packages and start a local cluster.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> dask.bag <span>as</span> db
</span></span><span><span><span>import</span> json
</span></span><span><span><span>from</span> dask.distributed <span>import</span> Client
</span></span><span><span>client <span>=</span> Client()
</span></span><span><span>client<span>.</span>dashboard_link
</span></span></code></pre></div><p>The dashboard link provides a dashboard that shows the activity going on in your local cluster in detail. Every distributed operation we’ll run will use this client. It’s almost like magic!</p>
<p><img loading="lazy" src="http://jimkang.com/weblog/articles/lets-have-fun-why-not/screenshot-python.png" alt="screenshot"/>
</p>
<p>Now, we can use that local cluster to read our large JSON file into a bag. A bag is an unordered
structure, unlike a dataframe, which is ordered and partitioned by its index. It works well with
unstructured and nested data, which is why we’re using it here to preprocess our JSON. We can read a
text file into a partitioned bag with <code>dask.bag.read_text</code> and the <code>blocksize</code> argument. Note that we’re loading into JSON immediately, as we know the payload is valid JSON.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>bag <span>=</span> db<span>.</span>read_text(<span>&#34;&lt;file&gt;&#34;</span>, blocksize<span>=</span><span>100</span> <span>*</span> <span>1000</span> <span>*</span> <span>1000</span>)<span>.</span>map(json<span>.</span>loads)
</span></span><span><span>bag
</span></span></code></pre></div><p>You can get the first few items in the bag with <code>bag.take(5)</code>. This will allow you to look at the data and do preprocessing. You can interactively test the preprocessing by adding extra map steps:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>bag<span>.</span>map(<span>lambda</span> x: x[<span>&#34;artCode&#34;</span>])<span>.</span>take(<span>5</span>)
</span></span></code></pre></div><p>This will give you the first five article codes in the bag. Note that the function wasn’t called on every element on the bag, only the first five elements, just enough to give us our answer. This is the good thing about using Dask: You’re only running code as needed, which is very useful for finding suitable preprocessing steps.</p>
<p>Once you have a suitable pipeline, you can compute the full data with <code>bag.compute()</code> or turn it into a Dask dataframe with <code>bag.to_dataframe()</code>. Let’s say we wanted to extract the sizes and codes of our subsubarticles from the example above (it’s a very small file, but it’s an illustrative example only). Then, we’d do something like the following:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>result <span>=</span> db<span>.</span>read_text(<span>&#34;&lt;file&gt;&#34;</span>)<span>.</span>map(json<span>.</span>loads)<span>.</span>map(<span>lambda</span> x: [{<span>&#34;code&#34;</span>: z[<span>&#34;code&#34;</span>], <span>&#34;size&#34;</span>: z[<span>&#34;size&#34;</span>]} <span>for</span> y <span>in</span> x[<span>&#34;subArts&#34;</span>] <span>for</span> z <span>in</span> y[<span>&#34;subSubArts&#34;</span>]])
</span></span><span><span>result<span>.</span>flatten()<span>.</span>to_dataframe()<span>.</span>compute()
</span></span></code></pre></div><p>This will run the provided lambda function on each element of the bag, parallel for each partition.
<code>flatten</code> will split the list into distinct bag items to allow us to create a non-nested dataframe.
Finally, <code>to_dataframe()</code> will convert our data into a Dask dataframe. Calling <code>compute()</code> will
execute our pipeline for the whole dataset, which might take a while. However, due to the “laziness”
of Dask, you’re able to inspect the intermediate steps in the pipeline interactively (with <code>take()</code>
and <code>head()</code>). Additionally, Dask will take care of restarting workers and spilling data to disk if
memory is not sufficient. Once we have a Dask dataframe, we can dump it into a more efficient file
format like <a href="https://parquet.apache.org/">Parquet</a>, which we can then use in the rest of our Python
code, either in parallel or in “regular” Pandas.</p>
<p>For 9 gigabytes of JSON, my Laptop was able to execute a data processing pipeline similar to the one above in 50 seconds<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. Additionally, I was able to build the pipeline in “standard” Python interactively, similar to how I build my <code>jq</code> queries.</p>
<p>Dask has a whole bunch of extra functionality for parallel processing of data, but I hope you’ve gotten a basic understanding of how it works. Compared to <code>jq</code>, you have the full power of Python on your hands, which make it easier to combine data from different sources (files and a database, for example), which is where the shell-based solution starts to struggle.</p>
<h2 id="fin">Fin</h2>
<p>I hope you’ve seen that processing large files doesn’t necessarily have to take place in the cloud. A recent laptop or desktop machine is often good enough to run preprocessing and statistics tasks with a bit of tooling<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>. For me, that tooling consists of <code>jq</code> to answer quick questions during debugging and for deciding on ways to implement features, and Dask to do more involved exploratory data analysis.</p>



  </div>

  
</article>
    </div></div>
  </body>
</html>
