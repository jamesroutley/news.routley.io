<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/openai/openai-python/blob/main/chatml.md">Original</a>
    <h1>ChatML: ChatGPT API expects a structured format, called Chat Markup Language</h1>
    
    <div id="readability-page-1" class="page"><div id="readme">
    <article itemprop="text"><p dir="auto">(This document is a preview of the underlying format consumed by
ChatGPT models. As a developer, you can use our <a href="https://platform.openai.com/docs/guides/chat" rel="nofollow">higher-level
API</a> and won&#39;t need to
iteract directly with this format today — but expect to have the
option in the future!)</p>
<p dir="auto">Traditionally, GPT models consumed unstructured text. ChatGPT models
instead expect a structured format, called Chat Markup Language
(ChatML for short).
ChatML documents consists of a sequence of messages. Each message
contains a header (which today consists of who said it, but in the
future will contain other metadata) and contents (which today is a
text payload, but in the future will contain other datatypes).
We are still evolving ChatML, but the current version (ChatML v0) can
be represented with our upcoming &#34;list of dicts&#34; JSON format as
follows:</p>
<div data-snippet-clipboard-copy-content="[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;system\nYou are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\nKnowledge cutoff: 2021-09-01\nCurrent date: 2023-03-01&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nHow are you&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;assistant\nI am doing well!&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nHow are you now?&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;
]"><pre><code>[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;system\nYou are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\nKnowledge cutoff: 2021-09-01\nCurrent date: 2023-03-01&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nHow are you&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;assistant\nI am doing well!&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nHow are you now?&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;
]
</code></pre></div>
<p dir="auto">You could also represent it in the classic &#34;unsafe raw string&#34;
format. Note this format inherently allows injections from user input
containing special-token syntax, similar to a SQL injections:</p>
<div data-snippet-clipboard-copy-content="&lt;|im_start|&gt;system
You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.
Knowledge cutoff: 2021-09-01
Current date: 2023-03-01&lt;|im_end|&gt;
&lt;|im_start|&gt;user
How are you&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
I am doing well!&lt;|im_end|&gt;
&lt;|im_start|&gt;user
How are you now?&lt;|im_end|&gt;"><pre><code>&lt;|im_start|&gt;system
You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.
Knowledge cutoff: 2021-09-01
Current date: 2023-03-01&lt;|im_end|&gt;
&lt;|im_start|&gt;user
How are you&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
I am doing well!&lt;|im_end|&gt;
&lt;|im_start|&gt;user
How are you now?&lt;|im_end|&gt;
</code></pre></div>
<h2 dir="auto"><a id="user-content-non-chat-use-cases" aria-hidden="true" href="#non-chat-use-cases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Non-chat use-cases</h2>
<p dir="auto">ChatML can be applied to classic GPT use-cases that are not
traditionally thought of as chat. For example, instruction following
(where a user requests for the AI to complete an instruction) can be
implemented as a ChatML query like the following:</p>
<div data-snippet-clipboard-copy-content="[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nList off some good ideas:&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;assistant&#34;
]"><pre><code>[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nList off some good ideas:&#34;,
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;assistant&#34;
]
</code></pre></div>
<p dir="auto">We do not currently allow autocompleting of partial messages,</p>
<div data-snippet-clipboard-copy-content="[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;system\nPlease autocomplete the user&#39;s message.&#34;
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nThis morning I decided to eat a giant&#34;
]"><pre><code>[
 {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;system\nPlease autocomplete the user&#39;s message.&#34;
 {&#34;token&#34;: &#34;&lt;|im_end|&gt;&#34;}, &#34;\n&#34;, {&#34;token&#34;: &#34;&lt;|im_start|&gt;&#34;},
 &#34;user\nThis morning I decided to eat a giant&#34;
]
</code></pre></div>
<p dir="auto">Note that ChatML makes explicit to the model the source of each piece
of text, and particularly shows the boundary between human and AI
text. This gives an opportunity to mitigate and eventually solve
injections, as the model can tell which instructions come from the
developer, the user, or its own input.</p>
<h2 dir="auto"><a id="user-content-few-shot-prompting" aria-hidden="true" href="#few-shot-prompting"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Few-shot prompting</h2>
<p dir="auto">In general, we recommend adding few-shot examples using separate
<code>system</code> messages with a <code>name</code> field of <code>example_user</code> or
<code>example_assistant</code>. For example, here is a 1-shot prompt:</p>
<div data-snippet-clipboard-copy-content="&lt;|im_start|&gt;system
Translate from English to French
&lt;|im_end|&gt;
&lt;|im_start|&gt;system name=example_user
How are you?
&lt;|im_end|&gt;
&lt;|im_start|&gt;system name=example_assistant
Comment allez-vous?
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
{{user input here}}&lt;|im_end|&gt;"><pre><code>&lt;|im_start|&gt;system
Translate from English to French
&lt;|im_end|&gt;
&lt;|im_start|&gt;system name=example_user
How are you?
&lt;|im_end|&gt;
&lt;|im_start|&gt;system name=example_assistant
Comment allez-vous?
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
{{user input here}}&lt;|im_end|&gt;
</code></pre></div>
<p dir="auto">If adding instructions in the <code>system</code> message doesn&#39;t work, you can
also try putting them into a <code>user</code> message.  (In the near future, we
will train our models to be much more steerable via the system
message. But to date, we have trained only on a few system messages,
so the models pay much most attention to user examples.)</p>
</article>
  </div></div>
  </body>
</html>
