<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/">Original</a>
    <h1>My 2023 all-flash ZFS NAS (Network Storage) build</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
  
  <details>
    <summary>Table of contents</summary>
    <nav>
  <ul>
    <li><a href="#design-goals">Design Goals</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#hardware">Hardware</a>
      <ul>
        <li><a href="#base-components">Base Components</a></li>
        <li><a href="#16-tb-ssd-data-disks">16 TB SSD Data Disks</a></li>
        <li><a href="#power-usage">Power Usage</a></li>
      </ul>
    </li>
    <li><a href="#operating-system">Operating System</a>
      <ul>
        <li><a href="#previously-coreos">Previously: CoreOS</a></li>
        <li><a href="#now-ubuntu-server">Now: Ubuntu Server</a></li>
        <li><a href="#maybe-later-gokrazy">Maybe later? gokrazy</a></li>
      </ul>
    </li>
    <li><a href="#setup">Setup</a>
      <ul>
        <li><a href="#uefi">UEFI</a></li>
        <li><a href="#operating-system-1">Operating System</a></li>
        <li><a href="#network">Network</a></li>
        <li><a href="#encrypted-zfs">Encrypted ZFS</a></li>
        <li><a href="#backup">Backup</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>
  </details>
  <p>For over 10 years now, I run two self-built NAS (Network Storage) devices which serve media (currently via Jellyfin) and run daily backups of all my PCs and servers.</p>
<p>In this article, I describe my goals, which hardware I picked for my new build (and why) and how I set it up.</p>
<h2 id="design-goals">Design Goals</h2>
<p>I use my network storage devices primarily for archival (daily backups), and secondarily as a media server.</p>
<p>There are days when I don’t consume any media (TV series and movies) from my NAS, because I have my music collection mirrored to another server that’s running 24/7 anyway. In total, my NAS runs for a few hours in some evenings, and for about an hour (daily backups) in the mornings.</p>
<p>This usage pattern is distinctly different than, for example, running a NAS as a file server for collaborative video editing that needs to be available 24/7.</p>
<p>The goals of my NAS setup are:</p>
<ol>
<li>Save power: each NAS build only runs when needed.
<ul>
<li>They must support Wake-on-LAN or <a href="https://michael.stapelberg.ch/posts/2022-10-09-remote-power-button/">similar (ESP32 remote power button)</a>.</li>
<li>Scheduling of backups is done separately, on a Raspberry Pi with <a href="https://gokrazy.org/">gokrazy</a>.</li>
<li>Convenient <a href="https://github.com/stapelberg/regelwerk/commit/8b81d7a808b1d76a0e96bdb4ab43964623d133c4">power off (tied to our all-lights-out button)</a> and power on (with <a href="https://github.com/stapelberg/zkj-nas-tools/blob/master/webwake/webwake.go">webwake</a>).</li>
</ul>
</li>
<li>Use Off-the-shelf hardware and software.
<ul>
<li>When hardware breaks, I can get replacements from the local PC store the same day.</li>
<li>Even when only the data disk(s) survive, I should be able to access my data when booting a standard live Linux system.</li>
<li>Minimal application software risk: I want to minimize risk for manual screw-ups or software bugs, meaning I use the venerable rsync for my backup needs (not Borg, restic, or similar).</li>
<li>Minimal system software risk: I use reliable file systems with the minimal feature set — no LVM or btrfs snapshots, no ZFS replication, etc. To achieve redundancy, I don’t use a cluster file system with replication, instead I synchronize my two NAS builds using rsync, without the <code>--delete</code> flag.</li>
</ul>
</li>
<li>Minimal failure domains: when one NAS fails, the other one keeps working.
<ul>
<li>Having N+1 redundancy here takes the stress out of repairing your NAS.</li>
<li>I run each NAS in a separate room, so that accidents like fires or spilled drinks only affect one machine.</li>
</ul>
</li>
</ol>
<h4 id="file-system-zfs">File System: ZFS</h4>
<p>In this specific build, I am trying out <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a>. Because I have two NAS builds
running, it is easy to change one variable of the system (which file system to
use) in one build, without affecting the other build.</p>
<p>My main motivation for using ZFS instead of <a href="https://en.wikipedia.org/wiki/Ext4"><code>ext4</code></a> is that ZFS does data checksumming, whereas ext4 only checksums metadata and the journal, but not data at rest. With large enough datasets, the chance of bit flips increases significantly, and I would prefer to know about them so that I can restore the affected files from another copy.</p>
<h2 id="hardware">Hardware</h2>
<p>Each of the two storage builds has (almost) the same components. This makes it easy to diagnose one with the help of the other. When needed, I can swap out components of the second build to temporarily repair the first one, or vice versa.</p>















<p><a href="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/IMG_1974.jpg"><img srcset="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/IMG_1974_hubee9c905cc3b7237e7c96518cbb38b46_1156723_1200x0_resize_q75_box.jpg 2x,https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/IMG_1974_hubee9c905cc3b7237e7c96518cbb38b46_1156723_1800x0_resize_q75_box.jpg 3x" src="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/IMG_1974_hubee9c905cc3b7237e7c96518cbb38b46_1156723_600x0_resize_q75_box.jpg" alt="photo of the Network Storage PC from the side, showing the Noctua case fan and CPU cooler, data disks, PSU and cables" title="photo of the Network Storage PC from the side, showing the Noctua case fan and CPU cooler, data disks, PSU and cables" width="600" height="450" loading="lazy"/></a></p><h3 id="base-components">Base Components</h3>
<table>
<thead>
<tr>
<th>Price</th>
<th>Type</th>
<th>Article</th>
<th>Remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>114 CHF</td>
<td>mainboard</td>
<td><a href="https://www.digitec.ch/en/s1/product/asrock-b450-gaming-itxac-am4-amd-b450-mini-itx-motherboards-9385702">AsRock B450 Gaming ITX/ac</a></td>
<td>Mini ITX</td>
</tr>
<tr>
<td>80 CHF</td>
<td>cpu</td>
<td><a href="https://www.heise.de/preisvergleich/amd-athlon-3000g-yd3000c6m2ofh-yd3000c6fhmpk-a2174924.html?hloc=at&amp;hloc=de">AMD Athlon 3000G</a></td>
<td>35W TDP, GPU</td>
</tr>
<tr>
<td>65 CHF</td>
<td>cpu cooler</td>
<td><a href="https://www.digitec.ch/de/s1/product/noctua-nh-l12s-70-mm-cpu-kuehler-6817433">Noctua NH-L12S</a></td>
<td>silent!</td>
</tr>
<tr>
<td>58 CHF</td>
<td>power supply</td>
<td><a href="https://www.digitec.ch/en/s1/product/silverstone-power-supply-st30sf-300w-sfx-300-w-power-supply-pc-5988297">Silverstone ST30SF 300W SFX</a></td>
<td>SFX form factor</td>
</tr>
<tr>
<td>51 CHF</td>
<td>case</td>
<td><a href="https://www.digitec.ch/en/s1/product/silverstone-sst-sg05bb-lite-mini-itx-mini-dtx-pc-case-3525365">Silverstone SST-SG05BB-Lite</a></td>
<td>Mini ITX</td>
</tr>
<tr>
<td>48 CHF</td>
<td>system disk</td>
<td><a href="https://www.digitec.ch/en/s1/product/wd-red-sn700-250-gb-m2-2280-ssd-17688689">WD Red SN700 250GB</a></td>
<td>M.2 NVMe</td>
</tr>
<tr>
<td>32 CHF</td>
<td>case fan</td>
<td><a href="https://www.digitec.ch/en/s1/product/noctua-nf-s12a-uln-120mm-1x-pc-fans-2451401">Noctua NF-S12A ULN</a></td>
<td>silent 120mm</td>
</tr>
<tr>
<td>28 CHF</td>
<td>ram</td>
<td><a href="https://www.digitec.ch/en/s1/product/gskill-value-1-x-8gb-2400-mhz-ddr4-ram-dimm-ram-11056524">8 GB DDR4 Value RAM (F4-2400C15-8GNT)</a></td>
<td></td>
</tr>
</tbody>
</table>
<p>The total price of 476 CHF makes this not a cheap build.</p>
<p>But, I think each component is well worth its price. Here’s my thinking regarding the components:</p>
<ul>
<li>Why not a cheaper <strong>system disk</strong>? I wanted to use an M.2 NVMe disk so that I could mount it on the bottom of the mainboard instead of having to mount another SATA disk in the already-crowded case. Instead of chosing the cheapest M.2 disk I could find, I went with WD Red as a brand I recognize. While it’s not a lot of effort to re-install the system disk, it’s still annoying and something I want to avoid if possible. If spending 20 bucks saves me one disk swap + re-install, that’s well worth it for me!</li>
<li>Why not skip the <strong>system disk</strong> entirely and install on the data disks? That makes the system harder to (re-)install, and easier to make manual errors when recovering the system. I like to physically disconnect the data disks while re-installing a NAS, for example. (I’m a fan of simple precautions that prevent drastic mistakes!)</li>
<li>Why not a cheaper <strong>CPU cooler</strong>? In <a href="https://michael.stapelberg.ch/posts/2019-10-23-nas/">one of my earlier NAS builds</a>, I used a (cheaper) passive CPU fan, which was directly in the air stream of the Noctua 120mm case fan. This setup was spec’ed for the CPU I used, and yet said CPU died as the only CPU to die on me in many many years. I want a reliable CPU fan, but also an absolutely silent build, so I went with the Noctua CPU cooler.</li>
<li>Why not skip the <strong>case fan</strong>, or go with the Silverstone-supplied one? You might argue that the airflow of the CPU cooler is sufficient for this entire build. Maybe that’s true, but I don’t want to risk it. Also, there are 3 disks (two data disks and one system disk) that can benefit from additional airflow.</li>
<li>Regarding the <strong>CPU</strong>, I chose the cheapest AMD CPU for Socket AM4, with a 35W TDP and built-in graphics. The built-in graphics means I can connect an HDMI monitor for setup and troubleshooting, without having to use the mainboard’s valuable one and only PCIe slot.
</li>
<li>Regarding the <strong>mainboard</strong>, I went with the AsRock Mini ITX series, which have served me well over the years. I started with an <a href="https://www.asrock.com/mb/AMD/AM1H-itx/">AsRock AM1H-ITX</a> in 2016, then bought two <a href="https://www.digitec.ch/en/s1/product/asrock-ab350-gaming-itxac-am4-amd-b350-mini-itx-motherboards-7022839">AsRock AB350 Gaming ITX/ac</a> in 2019, and recently an <a href="https://www.digitec.ch/en/s1/product/asrock-b450-gaming-itxac-am4-amd-b450-mini-itx-motherboards-9385702">AsRock B450 Gaming ITX/ac</a>.</li>
</ul>
<p>As a disclaimer: the two builds I use are <em>very similar</em> to the component list above, with the following differences:</p>
<ol>
<li>On storage2, I use an old AMD Ryzen 5 5600X CPU instead of the listed Athlon 3000G. The extra performance isn’t needed, and the lack of integrated graphics is annoying. But, I had the CPU lying around and didn’t want it to go to waste.</li>
<li>On storage3, I use an old AMD Athlon 200GE CPU on an <a href="https://www.digitec.ch/en/s1/product/asrock-ab350-gaming-itxac-am4-amd-b350-mini-itx-motherboards-7022839">AsRock AB350</a> mainboard.</li>
</ol>
<p>I didn’t describe the <em>exact</em> builds I use because a component list is more useful if the components on it are actually available :-).</p>
<h3 id="16-tb-ssd-data-disks">16 TB SSD Data Disks</h3>
<p>It used to be that Solid State Drives (SSDs) were just way too expensive compared to spinning hard disks when talking about terabyte sizes, so I used to put the largest single disk drive I could find into each NAS build: I started with 8 TB disks, then upgraded to 16 TB disks later.</p>
<p>Luckily, the price of flash storage has come down quite a bit: the <a href="https://www.digitec.ch/en/s1/product/samsung-870-qvo-8000-gb-25-ssd-13388185">Samsung SSD 870 QVO (8 TB)</a> costs “only” 42 CHF per TB. For a total of 658 CHF, I can get 16 TB of flash storage in 2 drives:</p>















<p><a href="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-22-samsung-870qvo-featured.jpg"><img srcset="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-22-samsung-870qvo-featured_hu799c805b17c89c62874113693200acea_520811_1200x0_resize_q75_box.jpg 2x,https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-22-samsung-870qvo-featured_hu799c805b17c89c62874113693200acea_520811_1800x0_resize_q75_box.jpg 3x" src="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-22-samsung-870qvo-featured_hu799c805b17c89c62874113693200acea_520811_600x0_resize_q75_box.jpg" alt="two samsung 870 QVO disks" title="two samsung 870 QVO disks" width="600" height="338" loading="lazy"/></a></p><p>Of course, spinning hard disks are at 16 CHF per TB, so going all-flash is over 3x as expensive.</p>
<p>I decided to pay the premium to get a number of benefits:</p>
<ul>
<li>My NAS devices are quieter because there are no more spinning disks in them. This gives me more flexibility in where to physically locate each storage machine.</li>
<li>My daily backups run quicker, meaning each NAS needs to be powered on for less time. The effect was actually quite pronounced, because figuring out which files need backing up requires a lot of random disk access. My backups used to take about 1 hour, and now finish in less than 20 minutes.</li>
<li>The quick access times of SSDs solve the last remaining wrinkle in my backup scheme: deleting backups and measuring used disk space is finally fast!</li>
</ul>
<h3 id="power-usage">Power Usage</h3>
<p>The choice of CPU, Mainboard and Network Card all influence the total power usage of the system. Here are a couple of measurements to give you a rough idea of the power usage:</p>
<table>
<thead>
<tr>
<th>build</th>
<th>CPU</th>
<th>main board</th>
<th>network card</th>
<th>idle</th>
<th>load</th>
</tr>
</thead>
<tbody>
<tr>
<td>s2</td>
<td>5600X</td>
<td><a href="https://www.digitec.ch/en/s1/product/asrock-b450-gaming-itxac-am4-amd-b450-mini-itx-motherboards-9385702">B450</a></td>
<td>10G: Mellanox ConnectX-3</td>
<td>26W</td>
<td>60W</td>
</tr>
<tr>
<td>s3</td>
<td>200GE</td>
<td><a href="https://www.digitec.ch/en/s1/product/asrock-ab350-gaming-itxac-am4-amd-b350-mini-itx-motherboards-7022839">AB350</a></td>
<td>10G: <a href="https://www.fs.com/products/135978.html">FS Intel 82599</a></td>
<td>28W</td>
<td>50W</td>
</tr>
<tr>
<td>s3</td>
<td>200GE</td>
<td><a href="https://www.digitec.ch/en/s1/product/asrock-ab350-gaming-itxac-am4-amd-b350-mini-itx-motherboards-7022839">AB350</a></td>
<td>1G onboard</td>
<td>23W</td>
<td>40W</td>
</tr>
</tbody>
</table>
<p>These values were measured using a <a href="https://mystrom.ch/de/wifi-switch/">myStrom WiFi Switch</a>.</p>
<h2 id="operating-system">Operating System</h2>
<h3 id="previously-coreos">Previously: CoreOS</h3>
<p>Before this build, I ran my NAS using Docker containers on <a href="https://en.wikipedia.org/wiki/Container_Linux">CoreOS (later renamed to Container Linux)</a>, which was a light-weight Linux distribution focused on containers. There are two parts about CoreOS that I liked most.</p>
<p>The most important part was that CoreOS updated automatically, using an A/B updating scheme, just like I do in <a href="https://gokrazy.org/">gokrazy</a>. I want to run as many of my devices as possible with A/B updates.</p>
<p>The other bit I like is that the configuration is very clearly separated from the OS. I managed the configuration (a <a href="https://cloud-init.io/">cloud-init YAML file</a>) on my main PC, so when swapping out the NAS system disk with a blank disk, I could just plug my config file into the CoreOS installer, and be done.</p>
<p>When CoreOS was bought by Red Hat and merged into Project Atomic, there wasn’t a good migration path and cloud-init wasn’t supported anymore. As a short-term solution, I switched from CoreOS to Flatcar Linux, a spiritual successor.</p>
<h3 id="now-ubuntu-server">Now: Ubuntu Server</h3>
<p>For this build, I wanted to try out ZFS. I always got the impression that ZFS was a pain to run because its kernel modules are not included in the upstream Linux kernel source.</p>
<p>Then, in 2016, Ubuntu decided to include ZFS by default. There are a couple of other Linux distributions on which ZFS seems easy enough to run, like Gentoo, Arch Linux or NixOS.</p>
<p>I wanted to spend my “innovation tokens” on ZFS, and keep the rest boring and similar to what I already know and work with, so I chose Ubuntu Server over NixOS. It’s similar enough to Debian that I don’t need to re-learn.</p>
<p>Luckily, the migration path from Flatcar’s cloud-init config to Ubuntu Server is really easy: just copy over parts of the cloud-config until you’re through the entire thing. It’s like a checklist!</p>
<h3 id="maybe-later-gokrazy">Maybe later? gokrazy</h3>
<p>In the future, it might be interesting to build a NAS setup using <a href="https://gokrazy.org">gokrazy</a>. In particular since we now can <a href="https://gokrazy.org/packages/docker-containers/">run Docker containers on gokrazy</a>, which makes running Samba or Jellyfin quite easy!</p>
<p>Using gokrazy instead of Ubuntu Server would get rid of a lot of moving parts. The current blocker is that ZFS is not available on gokrazy. Unfortunately that’s not easy to change, in particular also from a licensing perspective.</p>
<h2 id="setup">Setup</h2>
<h3 id="uefi">UEFI</h3>
<p>I changed the following UEFI settings:</p>
<ul>
<li>
<p>Advanced → ACPI Configuration → PCIE Devices Power On: Enabled</p>
<ul>
<li>This setting is needed (but not sufficient) for Wake On LAN (WOL). You also need to enable WOL in your operating system.</li>
</ul>
</li>
<li>
<p>Advanced → Onboard Devices Configuration → Restore on AC/Power Loss: Power On</p>
<ul>
<li>This setting ensures the machine turns back on after a power loss. Without it, WOL might not work after a power loss.</li>
</ul>
</li>
</ul>
<h3 id="operating-system-1">Operating System</h3>
<h4 id="network-preparation">Network preparation</h4>
<p>I like to configure static IP addresses for devices that are a permanent part of my network.</p>
<p>I have come to prefer configuring static addresses as static DHCP leases in my router, because then the address remains the same no matter which operating system I boot — whether it’s the installed one, or a live USB stick for debugging.</p>
<h4 id="ubuntu-server">Ubuntu Server</h4>
<ol>
<li>
<p>Download Ubuntu Server from <a href="https://ubuntu.com/download/server">https://ubuntu.com/download/server</a></p>
<ul>
<li>I initially let the setup program install Docker, but that’s a mistake. The setup program will get you Docker from snap (not apt), which <a href="https://stackoverflow.com/questions/52526219/docker-mkdir-read-only-file-system">can’t work with the whole file system</a>.</li>
</ul>
</li>
<li>
<p>Disable swap:</p>
<ul>
<li><code>swapoff -a</code></li>
<li><code>$EDITOR /etc/fstab</code> # delete the swap line</li>
</ul>
</li>
<li>
<p>Automatically load the corresponding sensors kernel module for the mainboard so that the Prometheus node exporter picks up temperature values and fan speed values:</p>
<ul>
<li><code>echo nct6775 | sudo tee /etc/modules</code></li>
</ul>
</li>
<li>
<p>Enable <a href="https://help.ubuntu.com/community/AutomaticSecurityUpdates">unattended upgrades</a>:</p>
<ul>
<li>
<p><code>dpkg-reconfigure -plow unattended-upgrades</code></p>
</li>
<li>
<p>Edit <code>/etc/apt/apt.conf.d/50unattended-upgrades</code> — I like to make the following changes:</p>
<pre tabindex="0"><code>Unattended-Upgrade::MinimalSteps &#34;true&#34;;
Unattended-Upgrade::Mail &#34;<a href="https://michael.stapelberg.ch/cdn-cgi/l/email-protection" data-cfemail="6f02060c070e0a032f0a170e021f030a41010a1b">[email protected]</a>&#34;;
Unattended-Upgrade::MailReport &#34;only-on-error&#34;;
Unattended-Upgrade::Automatic-Reboot &#34;true&#34;;
Unattended-Upgrade::Automatic-Reboot-Time &#34;08:00&#34;;
Unattended-Upgrade::SyslogEnable &#34;true&#34;;
</code></pre></li>
</ul>
</li>
</ol>
<h3 id="network">Network</h3>
<h4 id="tailscale-mesh-vpn">Tailscale Mesh VPN</h4>
<p>I have come to like Tailscale. It’s a mesh VPN (data flows directly between the machines) that allows me access to and from my PCs, servers and storage machines from anywhere.</p>
<p>Specifically, I followed the <a href="https://tailscale.com/download/linux/ubuntu-2204">install Tailscale on Ubuntu 22.04 guide</a>.</p>
<h4 id="prometheus-node-exporter">Prometheus Node Exporter</h4>
<p>For monitoring, I have an existing Prometheus setup. To add a new machine to my setup, I need to configure it as a new target on my Prometheus server. In addition, I need to set up Prometheus on the new machine.</p>
<p>First, I installed the Prometheus node exporter using <code>apt install prometheus-node-exporter</code>.</p>
<p>Then, I modified <code>/etc/default/prometheus-node-exporter</code> to only listen on the Tailscale IP address:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span><span>ARGS</span><span>=</span><span>&#34;--web.listen-address=100.85.3.16:9100&#34;</span>
</span></span></code></pre></div><p>Lastly, I added a systemd override to ensure the node exporter keeps trying to start until tailscale is up: the command <code>systemctl edit prometheus-node-exporter</code> opens an editor, and I configured the override like so:</p>
<pre tabindex="0"><code># /etc/systemd/system/prometheus-node-exporter.service.d/override.conf
[Unit]
# Allow infinite restarts, even within a short time.
StartLimitIntervalSec=0

[Service]
RestartSec=1
</code></pre><h4 id="static-ipv6-address">Static IPv6 address</h4>
<p>Similar to the static IPv4 address, I like to give my NAS a static IPv6 address as well. This way, I don’t need to reconfigure remote systems when I (sometimes temporarily) switch my NAS to a different network card with a different MAC address. Of course, this point becomes moot if I ever switch all my backups to Tailscale.</p>
<p>Ubuntu Server comes with Netplan by default, but I don’t know Netplan and don’t want to use it.</p>
<p>To switch to <code>systemd-networkd</code>, I ran:</p>
<pre tabindex="0"><code>apt remove --purge netplan.io
</code></pre><p>Then, I created a <code>systemd-networkd</code> config file with a static IPv6 token, resulting in a predictable IPv6 address:</p>
<pre tabindex="0"><code>$EDITOR /etc/systemd/network/enp.network
</code></pre><p>My config file looks like this:</p>
<pre tabindex="0"><code>[Match]
Name=enp*

[Network]
DHCP=yes
IPv6Token=0:0:0:0:10::253
IPv6AcceptRouterAdvertisements=yes
</code></pre><h4 id="ipv6-firewall-setup">IPv6 firewall setup</h4>
<p>An easy way to configure Linux’s <code>netfilter</code> firewall is to <code>apt install iptables-persistent</code>. That package takes care of saving firewall rules on shutdown and restoring them on the next system boot.</p>
<p>My rule setup is very simple: allow ICMP (IPv6 needs it), then set up <code>ACCEPT</code> rules for the traffic I expect, and <code>DROP</code> the rest.</p>
<p>Here’s my resulting <code>/etc/iptables/rules.v6</code> from such a setup:</p>
<details>
<summary>
<code>/etc/iptables/rules.v6</code>
</summary>
<pre tabindex="0"><code># Generated by ip6tables-save v1.4.14 on Fri Aug 26 19:57:51 2016
*filter
:INPUT DROP [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -p ipv6-icmp -m comment --comment &#34;IPv6 needs ICMPv6 to work&#34; -j ACCEPT
-A INPUT -m state --state RELATED,ESTABLISHED -m comment --comment &#34;Allow packets for outgoing connections&#34; -j ACCEPT
-A INPUT -s fe80::/10 -d fe80::/10 -m comment --comment &#34;Allow link-local traffic&#34; -j ACCEPT
-A INPUT -s 2001:db8::/64 -m comment --comment &#34;local traffic&#34; -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -m comment --comment &#34;SSH&#34; -j ACCEPT
COMMIT
# Completed on Fri Aug 26 19:57:51 2016
</code></pre></details>
<h3 id="encrypted-zfs">Encrypted ZFS</h3>
<p>Before you can use ZFS, you need to install the ZFS tools using <code>apt install zfsutils-linux</code>.</p>
<p>Then, we create a zpool that spans both SSDs:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>zpool create <span>\
</span></span></span><span><span><span></span>  -o <span>ashift</span><span>=</span><span>12</span> <span>\
</span></span></span><span><span><span></span>  srv <span>\
</span></span></span><span><span><span></span>  /dev/disk/by-id/ata-Samsung_SSD_870_QVO_8TB_S5SSNF0TC06121Z <span>\
</span></span></span><span><span><span></span>  /dev/disk/by-id/ata-Samsung_SSD_870_QVO_8TB_S5SSNF0TC06787P
</span></span></code></pre></div><p>The <code>-o ashift=12</code> ensures <a href="https://wiki.archlinux.org/title/ZFS#Advanced_Format_disks">proper alignment</a> on disks with a sector size of either 512B or 4KB.</p>
<p>On that zpool, we now create our datasets:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span><span>(</span><span>echo</span> -n on-device-secret <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span> wget -qO - https://autounlock.zekjur.net:8443/nascrypto<span>)</span> | zfs create <span>\
</span></span></span><span><span><span></span>  -o <span>encryption</span><span>=</span>on <span>\
</span></span></span><span><span><span></span>  -o <span>compression</span><span>=</span>off <span>\
</span></span></span><span><span><span></span>  -o <span>atime</span><span>=</span>off <span>\
</span></span></span><span><span><span></span>  -o <span>keyformat</span><span>=</span>passphrase <span>\
</span></span></span><span><span><span></span>  -o <span>keylocation</span><span>=</span>file:///dev/stdin <span>\
</span></span></span><span><span><span></span>  srv/data
</span></span></code></pre></div><p>The key I’m piping into <code>zfs create</code> is constructed from two halves: the on-device secret and the remote secret, which is a setup I’m using to implement an automated crypto unlock that is remotely revokable. See the next section for the corresponding <code>unlock.service</code>.</p>
<p>I repeated this same command (adjusting the dataset name) for each dataset: I currently have one for <code>data</code> and one for <code>backup</code>, just so that the used disk space of each major use case is separately visible:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>df -h /srv /srv/backup /srv/data   
</span></span><span><span>Filesystem      Size  Used Avail Use% Mounted on
</span></span><span><span>srv             4,2T  128K  4,2T   1% /srv
</span></span><span><span>srv/backup      8,1T  3,9T  4,2T  49% /srv/backup
</span></span><span><span>srv/data         11T  6,4T  4,2T  61% /srv/data
</span></span></code></pre></div><h4 id="zfs-maintenance">ZFS maintenance</h4>
<p>To detect errors on your disks, ZFS has a feature called “scrubbing”. I don’t think I need to scrub more often than monthly, but <a href="https://wiki.archlinux.org/title/ZFS#Scrubbing">maybe your scrubbing requirements are different</a>.</p>
<p>I enabled monthly scrubbing on my zpool <code>srv</code>:</p>
<p>On this machine, a scrub takes a little over 4 hours and keeps the disks busy:</p>
<pre tabindex="0"><code>  scan: scrub in progress since Wed Oct 11 16:32:05 2023
	808G scanned at 909M/s, 735G issued at 827M/s, 10.2T total
	0B repaired, 7.01% done, 03:21:02 to go
</code></pre><p>We can confirm by looking at the Prometheus Node Exporter metrics:</p>















<p><a href="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-11-grafana-scrub.png"><img srcset="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-11-grafana-scrub_huab242c6c68f4e18b70199053338a4d4f_420340_1200x0_resize_box_3.png 2x,https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-11-grafana-scrub_huab242c6c68f4e18b70199053338a4d4f_420340_1800x0_resize_box_3.png 3x" src="https://michael.stapelberg.ch/posts/2023-10-25-my-all-flash-zfs-network-storage-build/2023-10-11-grafana-scrub_huab242c6c68f4e18b70199053338a4d4f_420340_600x0_resize_box_3.png" alt="screenshot of a Grafana dashboard showing Prometheus Node Exporter metrics" title="screenshot of a Grafana dashboard showing Prometheus Node Exporter metrics" width="600" height="226" loading="lazy"/></a></p><p>The other maintenance-related setting I changed is to enable automated TRIM:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>zpool <span>set</span> <span>autotrim</span><span>=</span>on srv
</span></span></code></pre></div><h4 id="auto-crypto-unlock">Auto Crypto Unlock</h4>
<p>To automatically unlock the encrypted datasets at boot, I’m using a custom <code>unlock.service</code> systemd service file.</p>
<p>My <code>unlock.service</code> constructs the crypto key from two halves: the on-device secret and the remote secret that’s downloaded over HTTPS.</p>
<p>This way, my NAS can boot up automatically, but in an emergency I can remotely stop this mechanism.</p>
<details>
<summary>
My unlock.service
</summary>
<div><pre tabindex="0"><code data-lang="systemd"><span><span><span>[Unit]</span>
</span></span><span><span><span>Description</span><span>=</span><span>unlock hard drive</span>
</span></span><span><span><span>Wants</span><span>=</span><span>network.target</span>
</span></span><span><span><span>After</span><span>=</span><span>systemd-networkd-wait-online.service</span>
</span></span><span><span><span>Before</span><span>=</span><span>samba.service</span>
</span></span><span><span>
</span></span><span><span><span>[Service]</span>
</span></span><span><span><span>Type</span><span>=</span><span>oneshot</span>
</span></span><span><span><span>RemainAfterExit</span><span>=</span><span>yes</span>
</span></span><span><span><span># Wait until the host is actually reachable.</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/bin/sh -c &#34;c=0; while [ $c -lt 5 ]; do /bin/ping6 -n -c 1 autounlock.zekjur.net &amp;&amp; break; c=$((c+1)); sleep 1; done&#34;</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/bin/sh -c &#34;(echo -n secret &amp;&amp; wget --retry-connrefused -qO - https://autounlock.zekjur.net:8443/nascrypto) | zfs load-key srv/data&#34;</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/bin/sh -c &#34;(echo -n secret &amp;&amp; wget --retry-connrefused -qO - https://autounlock.zekjur.net:8443/nascrypto) | zfs load-key srv/backup&#34;</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/bin/sh -c &#34;zfs mount srv/data&#34;</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/bin/sh -c &#34;zfs mount srv/backup&#34;</span>
</span></span><span><span>
</span></span><span><span><span>[Install]</span>
</span></span><span><span><span>WantedBy</span><span>=</span><span>multi-user.target</span>
</span></span></code></pre></div></details>
<h3 id="backup">Backup</h3>
<p>For the last 10 years, I have been doing my backups using <code>rsync</code>.</p>
<p>Each machine pushes an incremental backup of its entire root file system (and any mounted file systems that should be backed up, too) to the backup destination (storage2/3).</p>
<p>All the machines I’m backing up run Linux and the <code>ext4</code> file system. I verified that my backup destination file systems support all the features of the backup source file system that I care about, i.e. extended attributes and POSIX ACLs.</p>
<p>The scheduling of backups is done by “<a href="https://github.com/stapelberg/zkj-nas-tools/tree/master/dornroeschen">dornröschen</a>”, a Go program that wakes up the backup sources and destination machines and starts the backup by triggering a command via SSH.</p>
<h4 id="ssh-configuration">SSH configuration</h4>
<p>The backup scheduler establishes an SSH connection to the backup source.</p>
<p>On the backup source, I authorized the scheduler like so, meaning it will run <a href="https://github.com/stapelberg/zkj-nas-tools/blob/master/dornroeschen/backup-remote.pl"><code>/root/backup.pl</code></a> when connecting:</p>
<pre tabindex="0"><code>command=&#34;/root/backup.pl&#34;,no-port-forwarding,no-X11-forwarding ssh-ed25519 AAAAC3Nzainvalidkey backup-scheduler
</code></pre><p>backup.pl runs <code>rsync</code>, which establishes another SSH connection, this time from the backup source to the backup destination.</p>
<p>On the backup destination (storage2/3), I authorize the backup source’s SSH public key to run <a href="https://manpages.debian.org/rrsync.1"><code>rrsync(1)</code></a>
, a script that only permits running <code>rsync</code> in the specified directory:</p>
<pre tabindex="0"><code>command=&#34;/usr/bin/rrsync /srv/backup/server.zekjur.net&#34;,no-port-forwarding,no-X11-forwarding ssh-ed25519 AAAAC3Nzainvalidkey server.zekjur.net
</code></pre><h4 id="signaling-readiness-after-wake-up">Signaling Readiness after Wake-Up</h4>
<p>I found it easiest to signal readiness by starting an empty HTTP server gated on <code>After=unlock.service</code> in systemd:</p>
<details>
<summary><code>/etc/systemd/system/healthz.service</code></summary>
<div><pre tabindex="0"><code data-lang="systemd"><span><span><span>[Unit]</span>
</span></span><span><span><span>Description</span><span>=</span><span>nginx for /srv health check</span>
</span></span><span><span><span>Wants</span><span>=</span><span>network.target</span>
</span></span><span><span><span>After</span><span>=</span><span>unlock.service</span>
</span></span><span><span><span>Requires</span><span>=</span><span>unlock.service</span>
</span></span><span><span><span>StartLimitInterval</span><span>=</span><span>0</span>
</span></span><span><span>
</span></span><span><span><span>[Service]</span>
</span></span><span><span><span>Restart</span><span>=</span><span>always</span>
</span></span><span><span><span># https://itectec.com/unixlinux/restarting-systemd-service-on-dependency-failure/</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>/bin/sh -c &#39;systemctl is-active docker.service&#39;</span>
</span></span><span><span><span># Stay on the same major version in the hope that nginx never decides to break</span>
</span></span><span><span><span># the config file syntax (or features) without doing a major version bump.</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>/usr/bin/docker pull nginx:1</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>-/usr/bin/docker kill nginx-healthz</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>-/usr/bin/docker rm -f nginx-healthz</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/usr/bin/docker run </span>\
</span></span><span><span><span>  --name nginx-healthz </span>\
</span></span><span><span><span>  --publish 10.0.0.253:8200:80 </span>\
</span></span><span><span><span>  --log-driver=journald </span>\
</span></span><span><span><span>nginx:1</span>
</span></span><span><span>
</span></span><span><span><span>[Install]</span>
</span></span><span><span><span>WantedBy</span><span>=</span><span>multi-user.target</span>
</span></span></code></pre></div></details>
<p>My <a href="https://github.com/stapelberg/zkj-nas-tools/blob/master/wake/wake.go"><code>wake</code></a> program then polls that port and returns once the server is up, i.e. the file system has been unlocked and mounted.</p>
<h4 id="auto-shutdown">Auto Shutdown</h4>
<p>Instead of explicitly triggering a shutdown from the scheduler program, I run “dramaqueen”, which shuts down the machine after 10 minutes, but will be inhibited while a backup is running. Optionally, shutting down can be inhibited while there are active samba sessions.</p>
<details>
<summary><code>/etc/systemd/system/dramaqueen.service</code></summary>
<div><pre tabindex="0"><code data-lang="systemd"><span><span><span>[Unit]</span>
</span></span><span><span><span>Description</span><span>=</span><span>dramaqueen</span>
</span></span><span><span><span>After</span><span>=</span><span>docker.service</span>
</span></span><span><span><span>Requires</span><span>=</span><span>docker.service</span>
</span></span><span><span>
</span></span><span><span><span>[Service]</span>
</span></span><span><span><span>Restart</span><span>=</span><span>always</span>
</span></span><span><span><span>StartLimitInterval</span><span>=</span><span>0</span>
</span></span><span><span>
</span></span><span><span><span># Always pull the latest version (bleeding edge).</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>-/usr/bin/docker pull stapelberg/dramaqueen</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>-/usr/bin/docker rm -f dramaqueen</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>/usr/bin/docker create --name dramaqueen stapelberg/dramaqueen</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>/usr/bin/docker cp dramaqueen:/usr/bin/dramaqueen /tmp/</span>
</span></span><span><span><span>ExecStartPre</span><span>=</span><span>/usr/bin/docker rm -f dramaqueen</span>
</span></span><span><span><span>ExecStart</span><span>=</span><span>/tmp/dramaqueen -net_command=</span>
</span></span><span><span>
</span></span><span><span><span>[Install]</span>
</span></span><span><span><span>WantedBy</span><span>=</span><span>multi-user.target</span>
</span></span></code></pre></div></details>
<h4 id="enabling-wake-on-lan">Enabling Wake-on-LAN</h4>
<p>Luckily, the network driver of the onboard network card supports WOL by
default. If that’s not the case for your network card, see <a href="https://wiki.archlinux.org/title/Wake-on-LAN">the Arch wiki
Wake-on-LAN article</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I have been running a PC-based few-large-disk Network Storage setup for years at this point, and I am very happy with all the properties of the system. I expect to run a very similar setup for years to come.</p>
<p>The low-tech approach to backups of using rsync has worked well — without changes — for years, and I don’t see rsync going away anytime soon.</p>
<p>The upgrade to all-flash is really nice in terms of random access time (for incremental backups) and to eliminate one of the largest sources of noise from my builds.</p>
<p>ZFS seems to work fine so far and is well-integrated into Ubuntu Server.</p>

<p>There are solutions for almost everyone’s NAS needs. This build obviously hits my personal sweet spot, but your needs and preferences might be different!</p>
<p>Here are a couple of related solutions:</p>
<ul>
<li>If you would like a more integrated solution, you could take a look at <a href="https://www.heise.de/ratgeber/Einplatinencomputer-Odroid-H3-als-NAS-und-Heimserver-einrichten-7496088.html">the Odroid H3 (Celeron)</a>.</li>
<li>If you’re okay with less compute power, but want more power efficiency, you could use an ARM64-based Single Board Computer.</li>
<li>If you want to buy a commercial solution, buy a device from qnap and fill it with SSD disks.
<ul>
<li>There are even commercial M.2 flash storage devices like the <a href="https://www.jeffgeerling.com/blog/2023/first-look-asustors-new-12-bay-all-m2-nvme-ssd-nas">ASUSTOR Flashstor</a> becoming available! If not for the “off the shelf hardware” goal of my build, this would probably be the most interesting commercial alternative to me.</li>
</ul>
</li>
<li>If you want more compute power, consider a Thin Client (perhaps used) instead of a Single Board Computer.
<ul>
<li><a href="https://www.servethehome.com/">ServeTheHome</a> has a nice series called Project TinyMiniMicro (<a href="https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/">introduction</a>, <a href="https://www.servethehome.com/tag/tinyminimicro/">blog posts</a>)</li>
<li>If you’re a heise+ subscriber, <a href="https://www.heise.de/ratgeber/Schlank-guenstig-stromsparend-NAS-mit-Thin-Client-im-Eigenbau-7546763.html">they have a (German) article about building a NAS from a thin client</a>.</li>
</ul>
</li>
<li>Very similar to thin clients is the Intel NUC (“Next Unit of Computing”): <a href="https://www.golem.de/news/nuc-12-pro-test-mini-kraftpakete-fuers-buero-und-mediacenter-2303-172992.html">(German) article comparing different NUC 12 devices</a></li>
</ul>
<div id="bmc">
  <p>
    I run a blog since 2005, spreading knowledge and experience for almost 20 years! :)
  </p>
  <p>
    If you want to support my work, you
    can <a href="https://www.buymeacoffee.com/stapelberg">buy me a coffee</a>.
  </p>
  <p>
    Thank you for your support! ❤️
  </p>
</div>

</div></div>
  </body>
</html>
