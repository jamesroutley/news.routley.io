<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/attractivechaos/plb2">Original</a>
    <h1>Benchmarking 20 programming languages on N-queens and matrix multiplication</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><strong>TL;DR</strong>: see the figure below. Note that nqueen and matmul are implemented in
all languages but sudoku and bedcov are only implemented in some.
NB: nqueen and matmul in V have been updated in the <a href="#table">table</a>, but the
figure is not in sync yet. Now V is the fastest on nqueen+matmul.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/attractivechaos/plb2/blob/master/analysis/rst-m1.png"><img src="https://github.com/attractivechaos/plb2/raw/master/analysis/rst-m1.png"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-table-of-contents" aria-hidden="true" tabindex="-1" href="#table-of-contents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Table of Contents</h2>
<ul dir="auto">
<li><a href="#intro">Introduction</a></li>
<li><a href="#result">Results</a>
<ul dir="auto">
<li><a href="#overall">Overall impressions</a></li>
<li><a href="#caveat">Caveats</a>
<ul dir="auto">
<li><a href="#startup">Startup time</a></li>
<li><a href="#cputime">Elapsed time vs CPU time</a></li>
</ul>
</li>
<li><a href="#opt">Subtle optimizations</a>
<ul dir="auto">
<li><a href="#matmul">Optimizing inner loops</a></li>
<li><a href="#memlayout">Controlling memory layout</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#conclusion">Discussions</a></li>
<li><a href="#table">Appendix: Timing on Apple M1 Macbook Pro</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-introduction" aria-hidden="true" tabindex="-1" href="#introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-intro"></a>Introduction</h2>
<p dir="auto">Programming Language Benchmark v2 (plb2) evaluates the performance of 20
programming languages on four CPU-intensive tasks. It is a follow-up to
<a href="https://github.com/attractivechaos/plb">plb</a> conducted in 2011. In plb2, all implementations use the same
algorithm for each task and their performance bottlenecks do not fall in
library functions. We do not intend to compare different algorithms or the
quality of the standard libraries in these languages. Plb2 is supposed to
demonstrate the performance of a language when you have to implement a new
algorithm in the language, which may happen if you can&#39;t find the algorithm in
existing libraries.</p>
<p dir="auto">The four tasks in plb2 all take a few seconds for a fast implementation to
complete. The tasks are:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>nqueen</strong>: solving a <a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle" rel="nofollow">15-queens problem</a>. The algorithm was inspired
by the second C implementation <a href="https://rosettacode.org/wiki/N-queens_problem#C" rel="nofollow">from Rosetta Code</a>. It involves nested
loops and integer bit operations.</p>
</li>
<li>
<p dir="auto"><strong>matmul</strong>: multiplying two square matrices of 1500x1500 in size. The inner
loop resembles BLAS&#39; <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1" rel="nofollow">axpy</a> operation.</p>
</li>
<li>
<p dir="auto"><strong>sudoku</strong>: solving 4000 hard <a href="https://en.wikipedia.org/wiki/Sudoku" rel="nofollow">Sudokus</a> (20 puzzles repeated for 200
times) using the <a href="https://attractivechaos.github.io/plb/kudoku.html" rel="nofollow">kudoku algorithm</a>. This algorithm heavily uses
small fixed-sized arrays with a bit complex logic.</p>
</li>
<li>
<p dir="auto"><strong>bedcov</strong>: finding the overlaps between two arrays of 1,000,000 intervals
with <a href="https://academic.oup.com/bioinformatics/article/37/9/1315/5910546" rel="nofollow">implicit interval trees</a>. The algorithm involves frequent
array access in a pattern similar to binary searches.</p>
</li>
</ul>
<p dir="auto">Every language has nqueen and matmul implementations. Some languages do not
have sudoku or bedcov implementations. In addition, I implemented most
algorithms in plb2 and adapted a few contributed matmul and sudoku
implementations in plb. As I am mostly a C programmer, implementations in other
languages may be suboptimal and there are no implementations in functional
languages. <strong>Pull requests are welcomed!</strong></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-results" aria-hidden="true" tabindex="-1" href="#results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-result"></a>Results</h2>
<p dir="auto">The figure at the top of the page summarizes the elapsed time of each implementation
measured on an Apple M1 MacBook Pro. <a href="https://github.com/sharkdp/hyperfine">Hyperfine</a> was used for timing
except for a few slow implementations which were timed with the &#34;time&#34; bash
command without repetition. A plus sign &#34;+&#34; indicates an explicit compilation
step. Exact timing can be found in the <a href="#table">table below</a>. The figure was
programmatically generated from the table but may be outdated.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-overall-impression" aria-hidden="true" tabindex="-1" href="#overall-impression"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-overall"></a>Overall impression</h3>
<p dir="auto">Programming language implementations in plb2 can be classified into four groups
depending on how and when compilation is done:</p>
<ol dir="auto">
<li>
<p dir="auto">Purely interpretted with no compilation (Perl and <a href="https://en.wikipedia.org/wiki/CPython" rel="nofollow">CPython</a>, the
official Python implementation). Not surprisingly, these are the slowest
language implementations in this benchmark.</p>
</li>
<li>
<p dir="auto">JIT compiled without a separate compilation step (Dart, all JavaScript
runtimes, Julia, LuaJIT, PHP, PyPy and Ruby3 with <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT</a>). These
language implementations compile hot code on the fly and then execute. They
have to balance compilation time and running time to achieve the best
overall performance.</p>
<p dir="auto">In this group, although PHP and Ruby3 are faster than Perl and CPython, they
are still an order of magnitude slower than PyPy. The two JavaScript engines
(Bun and Node), Dart and Julia all perform well. They are about twice as
fast as PyPy.</p>
</li>
<li>
<p dir="auto">JIT compiled with a separate compilation step (Java and C#). With separate
compilation, Java and C# can afford to trade compilation time for running
time in theory, but in this benchmark, they are not obviously faster than
those in group 2.</p>
</li>
<li>
<p dir="auto"><a href="https://en.wikipedia.org/wiki/Ahead-of-time_compilation" rel="nofollow">Ahead-of-time compilation</a> (the rest). Optimizing binaries for
specific hardware, these compilers, except Swift, tend to generate the
fastest executables.</p>
</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-caveats" aria-hidden="true" tabindex="-1" href="#caveats"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-caveat"></a>Caveats</h3>
<h4 tabindex="-1" dir="auto"><a id="user-content-startup-time" aria-hidden="true" tabindex="-1" href="#startup-time"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-startup"></a>Startup time</h4>
<p dir="auto">Some JIT-based language runtimes take up to ~0.3 second to compile and warm-up.
We are not separating out this startup time. Nonetheless, because most
benchmarks run for several seconds, including the startup time does not greatly
affect the results.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-elapsed-time-vs-cpu-time" aria-hidden="true" tabindex="-1" href="#elapsed-time-vs-cpu-time"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-cputime"></a>Elapsed time vs CPU time</h4>
<p dir="auto">Although no implementations use multithreading, language runtimes may be doing
extra work, such as garbage collection, in a separate thread. In this case, the
CPU time (user plus system) may be longer than elapsed wall-clock time. Julia,
in particular, takes noticeably more CPU time than wall-clock time even for the
simplest nqueen benchmark. In plb2, we are measuring the elapsed wall-clock
time because that is the number users often see. The ranking of CPU time may be
slightly different.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-subtle-optimizations" aria-hidden="true" tabindex="-1" href="#subtle-optimizations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-opt"></a>Subtle optimizations</h3>
<h4 tabindex="-1" dir="auto"><a id="user-content-controlling-memory-layout" aria-hidden="true" tabindex="-1" href="#controlling-memory-layout"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-memlayout"></a>Controlling memory layout</h4>
<p dir="auto">When implementing bedcov in Julia, C and many compiled languages, it is
preferred to have an array of objects in a contiguous memory block such that
adjacent objects are close in memory. This helps cache efficiency. In most
scripting languages, unfortunately, we have to put references to objects in an
array at the cost of cache locality. The issue can be alleviated by cloning
objects to a new array. This doubles the speed of PyPy and Bun.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-optimizing-inner-loops" aria-hidden="true" tabindex="-1" href="#optimizing-inner-loops"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-matmul"></a>Optimizing inner loops</h4>
<p dir="auto">The bottleneck of matrix multiplication falls in the following nested loop:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (int i = 0; i &lt; n; ++i)
    for (int k = 0; k &lt; n; ++k)
        for (int j = 0; j &lt; n; ++j)
            c[i][j] += a[i][k] * b[k][j];"><pre><span>for</span> (<span>int</span> i = <span>0</span>; i &lt; n; ++i)
    <span>for</span> (<span>int</span> k = <span>0</span>; k &lt; n; ++k)
        <span>for</span> (<span>int</span> j = <span>0</span>; j &lt; n; ++j)
            c[i][j] += a[i][k] * b[k][j];</pre></div>
<p dir="auto">It is obvious that <code>c[i]</code>, <code>b[k]</code> and <code>a[i][k]</code> can be moved out of the inner
loop to reduce the frequency of matrix access. The Clang compiler can apply
this optimization. Manual optimization may actually hurt performance.</p>
<p dir="auto">However, <strong>most other languages cannot optimize this nested loop.</strong> If we
manually move <code>a[i][k]</code> to the loop above it, we can often improve their
performance. Some C/C++ programmers say compilers often optimize better than
human, but this might not be the case in other languages.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-discussions" aria-hidden="true" tabindex="-1" href="#discussions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-conclusion"></a>Discussions</h2>
<p dir="auto">The most well-known and the longest running language benchmark is the <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html" rel="nofollow">Computer
Language Benchmark Games</a>. Plb2 differs in that it includes more recent
languages (e.g. Nim and Crystal), more language runtimes (e.g. PyPy and
LuaJIT) and more tasks (all four tasks are new), and it comes with more uniform
implementations and focuses more on the performance of the language itself
without library functions. <strong>Plb2 complements the Computer Language Benchmark
Games.</strong></p>
<p dir="auto">One important area that plb2 does not evaluate is the performance of memory
allocation and/or garbage collection. This may contribute more to practical
performance than generating machine code. Nonetheless, it is challenging to
design a realistic micro-benchmark to evaluate memory allocation. If the
built-in allocator in a language implementation does not work well, we can
implement customized memory allocator just for the specific task but this, in
my view, would not represent typical use cases.</p>
<p dir="auto">When plb was conducted in 2011, half of the languages in the figure above were
not mature or even did not exist. It is exciting to see many of them have
reached the 1.0 milestone and are gaining popularity among modern programmers.
On the other hand, Python remains one of the two most used scripting languages
despite its poor performance. In my view, this is because PyPy would not be
officially endorsed while other JIT-based languages are not general or good
enough. Will there be a language to displace Python in the next decade? I am
not optimistic.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-appendix-timing-on-apple-m1-macbook-pro" aria-hidden="true" tabindex="-1" href="#appendix-timing-on-apple-m1-macbook-pro"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a name="user-content-table"></a>Appendix: Timing on Apple M1 Macbook Pro</h2>
<table>
<thead>
<tr>
<th>Label</th>
<th>Language</th>
<th>Runtime</th>
<th>Version</th>
<th>nqueen</th>
<th>matmul</th>
<th>sudoku</th>
<th>bedcov</th>
</tr>
</thead>
<tbody>
<tr>
<td>c:clang+</td>
<td>C</td>
<td>Clang</td>
<td>15.0.0</td>
<td>2.70</td>
<td>0.54</td>
<td>1.54</td>
<td>0.84</td>
</tr>
<tr>
<td>crystal+</td>
<td>Crystal</td>
<td></td>
<td>1.10.0</td>
<td>3.28</td>
<td>2.45</td>
<td></td>
<td>0.87</td>
</tr>
<tr>
<td>c#:.net+</td>
<td>C#</td>
<td>.NET</td>
<td>8.0.100</td>
<td>3.00</td>
<td>4.67</td>
<td>3.01</td>
<td></td>
</tr>
<tr>
<td>d:ldc2+</td>
<td>D</td>
<td>LDC2</td>
<td>2.105.2</td>
<td>2.68</td>
<td>0.57</td>
<td>1.60</td>
<td></td>
</tr>
<tr>
<td>dart</td>
<td>Dart</td>
<td>(JIT)</td>
<td>3.2.4</td>
<td>3.62</td>
<td>4.81</td>
<td>3.24</td>
<td></td>
</tr>
<tr>
<td>go+</td>
<td>Go</td>
<td></td>
<td>1.21.5</td>
<td>2.94</td>
<td>1.63</td>
<td>2.04</td>
<td></td>
</tr>
<tr>
<td>java+</td>
<td>Java</td>
<td>OpenJDK</td>
<td>20.0.1</td>
<td>3.92</td>
<td>1.14</td>
<td>3.20</td>
<td></td>
</tr>
<tr>
<td>js:bun</td>
<td>JavaScript</td>
<td>Bun</td>
<td>1.0.20</td>
<td>3.11</td>
<td>1.75</td>
<td>3.07</td>
<td>2.83</td>
</tr>
<tr>
<td>js:deno</td>
<td>JavaScript</td>
<td>Deno</td>
<td>1.39.1</td>
<td>4.00</td>
<td>3.06</td>
<td>4.04</td>
<td>3.87</td>
</tr>
<tr>
<td>js:k8</td>
<td>JavaScript</td>
<td>k8</td>
<td>1.0</td>
<td>3.79</td>
<td>2.99</td>
<td>3.76</td>
<td>4.02</td>
</tr>
<tr>
<td>js:node</td>
<td>JavaScript</td>
<td>Node</td>
<td>21.5.0</td>
<td>3.73</td>
<td>2.88</td>
<td>3.77</td>
<td>3.83</td>
</tr>
<tr>
<td>julia</td>
<td>Julia</td>
<td></td>
<td>1.10.0</td>
<td>3.75</td>
<td>0.76</td>
<td>2.72</td>
<td>2.47</td>
</tr>
<tr>
<td>luajit</td>
<td>Lua</td>
<td>LuaJIT</td>
<td>2.1</td>
<td>5.31</td>
<td>2.66</td>
<td>4.48</td>
<td>10.59</td>
</tr>
<tr>
<td>mojo+</td>
<td>Mojo</td>
<td></td>
<td>0.6.1</td>
<td>3.24</td>
<td>1.12</td>
<td></td>
<td></td>
</tr>
<tr>
<td>nim+</td>
<td>Nim</td>
<td></td>
<td>2.0.2</td>
<td>3.18</td>
<td>0.69</td>
<td></td>
<td>1.18</td>
</tr>
<tr>
<td>perl</td>
<td>Perl</td>
<td></td>
<td>5.34.1</td>
<td>158.34</td>
<td>158.01</td>
<td>90.78</td>
<td></td>
</tr>
<tr>
<td>php</td>
<td>PHP</td>
<td></td>
<td>8.3</td>
<td>48.15</td>
<td>71.20</td>
<td></td>
<td></td>
</tr>
<tr>
<td>py:pypy</td>
<td>Python</td>
<td>Pypy</td>
<td>7.3.14</td>
<td>6.91</td>
<td>4.95</td>
<td>8.82</td>
<td>6.27</td>
</tr>
<tr>
<td>py:cpy</td>
<td>Python</td>
<td>CPython</td>
<td>3.11.7</td>
<td>159.97</td>
<td>223.66</td>
<td>52.88</td>
<td>42.84</td>
</tr>
<tr>
<td>ruby</td>
<td>Ruby</td>
<td>(YJIT)</td>
<td>3.3.0</td>
<td>88.15</td>
<td>130.51</td>
<td>52.26</td>
<td></td>
</tr>
<tr>
<td>rust+</td>
<td>Rust</td>
<td></td>
<td>1.75.0</td>
<td>2.68</td>
<td>0.56</td>
<td>1.65</td>
<td></td>
</tr>
<tr>
<td>swift+</td>
<td>Swift</td>
<td></td>
<td>5.9.0</td>
<td>2.92</td>
<td>7.46</td>
<td>16.02</td>
<td></td>
</tr>
<tr>
<td>v+</td>
<td>V</td>
<td></td>
<td>0.4.3</td>
<td>2.57</td>
<td>0.56</td>
<td></td>
<td></td>
</tr>
<tr>
<td>zig+</td>
<td>Zig</td>
<td></td>
<td>0.11.0</td>
<td>2.74</td>
<td>0.56</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</article>
          </div></div>
  </body>
</html>
