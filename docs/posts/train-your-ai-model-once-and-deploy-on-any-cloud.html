<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/">Original</a>
    <h1>Train Your AI Model Once and Deploy on Any Cloud</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Organizations are increasingly adopting hybrid and multi-cloud strategies to access the latest compute resources, consistently support worldwide customers, and optimize cost. However, a major challenge that engineering teams face is operationalizing AI applications across different platforms as the stack changes. This requires MLOps teams to familiarize themselves with different environments and developers to customize applications to run across target platforms.</p>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. You can then deploy that AI application on any GPU-powered platform without code changes.<strong></strong></p>



<h2>Introducing the latest NVIDIA Virtual Machine Image</h2>



<p>The NVIDIA Cloud Native Stack Virtual Machine Image (VMI) is GPU-accelerated. It comes pre-installed with Cloud Native Stack, which is a reference architecture that includes upstream Kubernetes and the NVIDIA GPU Operator. NVIDIA Cloud Native Stack VMI enables you to build, test, and run GPU-accelerated containerized applications orchestrated by Kubernetes.</p>



<p>The NVIDIA GPU Operator automates the lifecycle management of the software required to expose GPUs on Kubernetes. It enables advanced functionality, including better GPU performance, utilization, and telemetry. Certified and validated for compatibility with industry-leading Kubernetes solutions, GPU Operator enables organizations to focus on building applications, rather than managing Kubernetes infrastructure.</p>



<p>NVIDIA Cloud Native Stack VMI is available on AWS, Azure, and GCP.</p>



<h2>Now Available: Enterprise support by NVIDIA</h2>



<p>For enterprise support for NVIDIA Cloud Native Stack VMI and GPU Operator,  purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA partner</a>.</p>



<p>Developing AI solutions from concept to deployment is not easy. Keep your AI projects on track with NVIDIA AI Enterprise Support Services. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options. Additional services, including training and AI workload onboarding, are available.<strong></strong></p>



<h2>Run:ai is now certified on NVIDIA AI Enterprise</h2>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on their Atlas platform. This additional certification enables enterprises to accelerate the data science pipeline. They can focus on streamlining the development and deployment of predictive AI models to automate essential processes and gain rapid insights from data.</p>



<p>Run:ai provides an AI Computing platform that simplifies the access, management, and utilization of GPUs in cloud and on-premises clusters. Smart scheduling and advanced fractional GPU capabilities ensure that you get the right amount of compute for the job.</p>



<p>Run:ai Atlas includes GPU Orchestration capabilities to help researchers consume GPUs more efficiently. They do this by automating the orchestration of AI workloads and the management and virtualization of hardware resources across teams and clusters.</p>



<p>Run:ai can be installed on any Kubernetes cluster, to provide efficient scheduling and monitoring capabilities to your AI infrastructure. With the NVIDIA Cloud Native Stack VMI, you can add cloud instances to a Kubernetes cluster so that they become GPU-powered worker nodes of the cluster.</p>



<p>Here’s testimony from one of our team members: “As an engineer, without the NVIDIA Cloud Native Stack VMI, there is a lot of manual work involved. With the Cloud Native Stack VMI, it was two clicks and took care of provisioning Kubernetes and Docker and the GPU Operator. It was easier and faster to get started on my work.”</p>



<h2>Set up a Cloud Native Stack VMI on AWS</h2>



<p>In the AWS marketplace, <a href="https://aws.amazon.com/marketplace/pp/prodview-cucpqpmvqkajy?sr=0-2&amp;ref_=beagle&amp;applicationId=AWSMPContessa" data-wpel-link="external" target="_blank" rel="follow external noopener">launch an NVIDIA Cloud Native Stack VMI</a> using the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launch-marketplace-console.html" data-wpel-link="external" target="_blank" rel="follow external noopener">Launch an AWS Marketplace instance</a> instructions.</p>



<p>Ensure that the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-prerequisites/" data-wpel-link="external" target="_blank" rel="follow external noopener">necessary prerequisites</a> have been met and install Run:ai using the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-install/" data-wpel-link="external" target="_blank" rel="follow external noopener">Cluster Install</a> instructions. After the installation, on the <strong>Overview</strong> dashboard, you should see that the metrics begin to populate. On the <strong>Clusters</strong> tab, you should also see the cluster as connected.</p>



<p>Next, add a few command components to the kube-apiserver.yaml file to enable user authentication on the Run:ai platform. For more information, see <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=researcher+api#administration-user-interface-setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Administration User Interface Setup</a>.</p>



<p>By default, you can find the kube-apiserver.yaml file in the following directory:</p>


<div><pre title="">/etc/kubernetes/manifests/kube-apiserver.yaml
</pre></div>


<p>You can <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=oidc#mandatory-kubernetes-configuration" data-wpel-link="external" target="_blank" rel="follow external noopener">validate that the oidc commands were successfully applied</a> by the kube-apiserver. Look for the <code>oidc</code> commands in the output.</p>



<pre><code>spec:
  containers:
  - command:
    - kube-apiserver
    - --oidc-client-id=runai
    - --oidc-issuer-url=https://app.run.ai/auth/realms/nvaie
    - --oidc-username-prefix=-</code></pre>



<p>Set up the <a href="https://docs.run.ai/admin/admin-ui-setup/overview/#setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Unified UI</a> and <a href="https://docs.run.ai/admin/admin-ui-setup/project-setup/?h=projects" data-wpel-link="external" target="_blank" rel="follow external noopener">create a new project</a>. Projects help to dictate GPU quota guarantees for data scientists and researchers who are using the Run:ai platform.</p>



<p>Name the new project and give the project at least one assigned GPU. For this post, I created one project with a two-GPU quota and another project with no GPU quota, labeled <code>nvaie-high-priority</code> and <code>nvaie-low-priority</code>, respectively After the project is created, you can <a href="https://docs.run.ai/admin/researcher-setup/cli-install/?h=researcher+command+line+interface#install-runai-cli" data-wpel-link="external" target="_blank" rel="follow external noopener">install the Run:ai CLI tool</a>, which enables you to submit workloads to the cluster.</p>



<p>The following commands use the runai CLI to submit a job (job1 or job2) leveraging a Docker image called quickstart. Quickstart contains TensorFlow, CUDA, a model, and data that feeds in and trains the model. It leverages one GPU for training (-g 1) and is submitted on behalf of the low-priority or high-priority project denoted by the <code>-p</code> parameter. </p>



<p>Deploy a few test jobs to show some of Run:ai’s orchestration functionality by running:</p>


<div><pre title="">runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority 
runai submit job2 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-low-priority
</pre></div>


<p>You can check the status of the jobs by running:</p>


<div><pre title="">runai describe job job1 -p nvaie-high-priority
runai describe job job2 -p nvaie-low-priority
</pre></div>


<p>Both workloads are now training on the GPUs, as you can see on the <strong>Overview</strong> dashboard.</p>



<p>You can submit an additional workload to highlight your job preemption capabilities. Currently, the <code>nvaie-high-priority</code> project is guaranteed access to both GPUs since their Assigned GPU quota is set to 2. You can submit an additional workload for the <code>nvaie-high-priority</code> project and observe that you are preempting the <code>nvaie-low-priority</code> job.</p>



<p>The job preemption enables you to look at the <a href="https://docs.run.ai/Researcher/best-practices/convert-to-unattended/?h=checkpoint#checkpoints" data-wpel-link="external" target="_blank" rel="follow external noopener">checkpointing process</a> for the training workloads, save the current progress at the checkpoint, and then preempt the workload to remove it from the GPU. Save the training progress and free up the GPU for a higher-priority workload to run.</p>


<div><pre title="">runai submit job3 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority
</pre></div>


<p>You can check the status of the job by running:</p>


<div><pre title="">runai describe job job3 -p nvaie-high-priority
</pre></div>


<p>If you go back to the overview dashboard, you’ll see the two jobs running for the <code>nvaie-high-priority</code> project and the workload from <code>nvaie-low-priority</code> preempted and placed back into the pending queue. The workload in the pending queue is automatically rescheduled when a GPU becomes available.</p>



<p>To clean up your jobs, run the following commands:</p>


<div><pre title="">runai delete job job1 -p nvaie-low-priority 
runai delete job job2 job3 -p nvaie-high-priority 
</pre></div>


<h2>Summary</h2>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. Developers and MLOps can then deploy that AI application on any GPU-powered platform without code change. </p>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on its Atlas platform. You can purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Partner</a> to obtain enterprise support for NVIDIA VMI and GPU Operator. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options.</p>



<p>For more information, see the following resources:</p>



<ul>
<li><a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA AI Enterprise</a></li>



<li><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/nvidia_vmi" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA VMI</a></li>



<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA GPU Operator</a> </li>



<li><a href="https://www.run.ai/" data-wpel-link="external" target="_blank" rel="follow external noopener">Run:ai solutions</a></li>
</ul>
</div></div>
  </body>
</html>
