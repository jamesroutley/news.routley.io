<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://renegadeotter.com/2023/09/10/death-by-a-thousand-microservices.html">Original</a>
    <h1>Death by a Thousand Microservices</h1>
    
    <div id="readability-page-1" class="page"><div id="postBody">
    <h3 id="the-church-of-complexity">The Church of Complexity</h3>

<p>There is a pretty well-known sketch in which an engineer is explaining to the project manager how an overly complicated maze of 
microservices works in order to get a user’s birthday - and fails to do so anyway. The scene accurately describes the 
absurdity of the state of the current tech culture. We laugh, and yet bringing this up in a 
serious conversation is tantamount to professional heresy, rendering you borderline un-hirable.</p>

<p>
    <iframe src="https://www.youtube.com/embed/y8OnoxKotPQ?si=7qBEqGaN7ATD-Gex" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; 
        picture-in-picture; web-share" allowfullscreen="">
    </iframe>
</p>

<p>How did we get here? How did our aim become not addressing the task at hand but instead setting a pile of 
cash on fire by <strong>solving problems we don’t have</strong>?</p>

<h3 id="the-perfect-storm">The perfect storm</h3>

<p>There are a few things in recent history that may have contributed to the current state of things. First, a whole army of 
developers 
writing Javascript for the browser started self-identifying as 
“full-stack”, diving into server development and asynchronous code. Javascript is Javascript, right? What difference does it make 
what you create using it - user interfaces, servers, games, or embedded systems. <em>Right</em>? 
Node was still kind of a <a href="https://www.youtube.com/watch?v=M3BM9TB-8yA" target="_blank">learning project of one person</a>, and 
Javascript back then was a deeply problematic choice for server development.
<a href="https://notes.ericjiang.com/posts/751" target="_blank">Pointing this out</a> to still green server-side developers usually 
resulted in a lot of huffing and puffing. This is all they knew, after all. The world outside of Node 
effectively did not exist, the Node way was the only known way, and so this was the genesis of the 
stubborn, dogmatic thinking 
that we deal with to this day.</p>

<photo-element source="complexity/theway.webp" aspect="landscape" alt="Performance"></photo-element>

<p><strong>And then</strong>, a steady stream of 
<a href="https://en.wikipedia.org/wiki/Big_Tech" target="_blank">FAANG</a> veterans started merging into the river of startups, 
mentoring the newly-minted and highly impressionable young Javascript server-side engineers. The apostles of the Church of Complexity would 
assertively claim that “how they did things over at Google” was unquestionable and correct - even if it made no sense under 
current context and size. What 
do you <em>mean</em> you don’t have a separate <em>User Preferences Service</em>? That just <em>will not scale, bro!</em></p>

<p>But, it’s easy to blame the veterans and the newcomers for all of this. What else was happening? Oh yeah - easy money.</p>

<p>What do you do when you are flush with venture capital? You don’t 
<a href="https://www.youtube.com/embed/BzAdXyPYKQo?si=3lOVi1rhtaPC-nv4" target="_blank">go for revenue</a>, surely! On more than one 
occasion I received an email from management, asking everyone to be in the office, tidy up their desks and look busy, as a 
clouder of Patagonia vests was about to be paraded through the office. Investors needed to see explosive growth, but not in 
profitability, 
no. They just needed to see how quickly the company could hire ultra-expensive software engineers to do … <em>something</em>.</p>

<p>And now that you have these developers, what do you do with them? Well, they could build a simpler system that is easier to 
grow and
maintain, or they could conjure up a monstrous constellation of “microservices” that no one really 
understands. Microservices 
- the new way of writing scalable software! Are we just going to pretend that the concept of “distributed systems” never existed?
(Let’s skip the whole parsing of the nuances that microservices are not real distributed systems).</p>

<p>Back in the days when the tech industry was not such a bloated farce, distributed systems were respected, feared, and generally 
avoided - reserved only as the weapon of last resort for particularly gnarly problems. Everything with a distributed system 
becomes more challenging and time-consuming - development, debugging, deployment, testing, resilience. But I don’t know - maybe 
it’s all 
super easy now because 
<em>toooollling</em>.</p>

<p>There is no standard tooling for microservices-based development - there is no common 
framework. Working on distributed systems has gotten only marginally easier in 2020s. The Dockers and the 
Kuberneteses of the world did not magically take away the inherent complexity of a distributed setup.</p>

<p>I love referring to this
<a href="https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/" target="_blank">summary of 5 years of startup audits</a>, 
as it is packed with common-sense conclusions based on hard evidence (and paid insights):</p>

<blockquote>
  <p>… the startups we audited that are now doing the best usually had an almost brazenly ‘Keep It Simple’ approach to
engineering. Cleverness for cleverness sake was abhorred. On the flip side, the companies where we were like ”woah,
these folks are smart as hell” for the most part kind of faded.</p>
</blockquote>

<p>Literally - “complexity kills”.</p>

<p>The 
<a href="https://podcasts.apple.com/mt/podcast/lessons-from-5-years-of-startup-code-audits/id341623264?i=1000567623452" target="_blank">audit</a>
revealed an interesting pattern, where many startups experienced a sort of collective imposter syndrome while building 
straight-forward, simple,
performant systems. There is a dogma attached to not starting out with microservices on day one - 
no matter the problem. “Everyone is doing microservices, yet we have a single Django monolith maintained by just a few engineers, 
and a MySQL instance - what are we doing wrong?”. The answer is almost always “nothing”.</p>

<p>Likewise, it’s common for seasoned engineers to experience 
hesitation and inadequacy in today’s tech world, and the good news is 
that, 
no - it’s probably not you. It’s common for teams to pretend like they are doing “webs cale”, hiding behind libraries, ORMs, and 
cache - 
confident in their expertise (they crushed that Leetcode!), yet they may not even be
<a href="https://www.reddit.com/r/programming/comments/f46f5a/comment/fhp26k8/?context=3" target="_blank">aware of database indexing basics</a>. 
You are operating in a sea of unjustified overconfidence, waste, and Dunning-Kruger, so who is really the imposter here?</p>

<h3 id="there-is-nothing-wrong-with-a-monolith">There is nothing wrong with a monolith</h3>

<p>The idea that you cannot grow without a system that looks like the infamous diagram of Afghanistan war 
strategy is largely a myth.</p>

<photo-element source="complexity/af.png" aspect="landscape"></photo-element>

<p>Dropbox, Twitter, Facebook, Instagram, Shopify, Stack Overflow - these companies and others started out as monolithic 
code bases. Many have a monolith at their core to this day. Stack Overflow makes it a 
<a href="https://stackexchange.com/performance" target="_blank">point of pride</a> how little hardware they need to run the massive site. 
Shopify is still a <a href="https://blog.quastor.org/p/shopify-ensures-consistent-reads">Rails monolith</a>, leveraging the tried and true 
<a href="https://twitter.com/ShopifyEng/status/1597983928018948096" target="_blank">Resque</a> to proces billions of tasks.</p>

<p>WhatsApp went supernova with their 
<a href="https://blog.quastor.org/p/whatsapp-scaled-1-billion-users-50-engineers" target="_blank">Erlang monolith and 50 engineers</a>.
How?</p>

<blockquote>
  <p>WhatsApp consciously keeps the engineering staff small to only about 50 engineers.</p>

  <p>Individual engineering teams are also small, consisting of 1 - 3 engineers and teams are each given a great deal of autonomy.</p>

  <p>In terms of servers, WhatsApp prefers to use a smaller number of servers and vertically scale each server to the highest 
extent possible.</p>
</blockquote>

<p>Instagram was acquired for billions - with a crew of 12.</p>

<p>And do you imagine Threads as an effort involving a whole Meta campus? Nope. They followed the
<a href="https://instagram-engineering.com/static-analysis-at-scale-an-instagram-story-8f498ab71a0c" target="_blank">Instagram model</a>, 
and this is the entire Threads team:</p>

<photo-element source="complexity/threads-team.webp" aspect="landscape" credit="Credit: Substack - The Pragmatic 
Engineer"></photo-element>

<p>Perhaps claiming that <em>your</em> particular problem domain requires a massively complicated distributed system and an open office 
stuffed to the gills with turbo-geniuses is just crossing over into 
arrogance rather than brilliance?</p>



<p>Try <a href="https://friendlyfire.tech" target="_blank">Friendly Fire</a> - our system for GitHub and Slack to
    streamline the code review process.
</p>

<h3 id="dont-solve-problems-you-dont-have">Don’t solve problems you don’t have</h3>

<p>It’s a simple question - what problem are you solving? Is it scale? How do you know how to break it all up for scale and performance? 
Do you have enough data to show what needs to be a separate service and why? Distributed systems are built for size and 
resilience. Can your system scale and be resilient at the same time? What happens if one of the services goes down or comes to a crawl? 
Just scale it up, yes? What about the <em>other</em> services that will get flooded with load? Did you war-game the endless permutations 
of things that can and will go wrong? Is there back pressure? Circuit breakers? Queues? Jitter? Sensible timeouts on every 
endpoint? Are there fool-proof guards to make sure a simple change does not bring everything down? 
The knobs you need to be aware of and tune are endless, and they are all specific to your system’s particular signature of 
usage and traffic.</p>

<p>The truth is that most companies will never reach the massive size that will actually require building a true distribute 
system. Your cos playing Amazon and Google - without 
their scale, expertise, and endless resources - is very likely just an egregious waste of money and time.</p>

<p><em>The only thing harder than a distributed system is a BAD distributed system</em>.</p>

<photo-element source="complexity/twitter3.png" aspect="landscape" alt="Performance"></photo-element>

<h3 id="but-each-teambut-separate-but-api">“But each team…but separate… but API”</h3>

<p>Trying to shove a distributed topology into your company’s structure is a noble effort, but it almost always backfires. It’s a 
common approach 
to break up a problem into smaller pieces and then solve those one by one. So, the thinking goes, if you break up one service 
into multiple ones, everything becomes easier, right?</p>

<p>The theory is sweet and elegant - each microservice is being maintained rigorously by a dedicated team, 
walled off behind a beautiful, backward-compatible, versioned API. In fact, this is all so steely that you rarely even have to 
communicate with that team - as if the microservice was maintained by a 3rd party vendor. It’s <em>simple</em>!</p>

<p>If that doesn’t sound familiar, that’s because this rarely happens. In reality, our Slack channels are <em>flooded</em> with 
messages from teams communicating about releases, bugs, configuration updates, breaking changes, and PSAs. Everyone 
needs to be on top of everything, all the time. And if that wasn’t great, it’s normal for one 
already-slammed team to half-ass multiple 
microservices instead of doing a great job on a single one, often changing ownership as people come and go.</p>

<p>In order to win the race, we don’t build <em>one</em> good race car - we build a fleet of shitty golf carts.</p>

<photo-element source="complexity/twitter2.png" aspect="landscape" alt="Performance"></photo-element>

<h3 id="what-you-lose">What you lose</h3>

<p>There are multiple pitfalls to building with microservices, and often that minefield is either not fully appreciated or simply 
ignored. Teams spend months writing highly customized tooling and learning lessons not 
related at all to the core product. Here are just some often overlooked aspects…</p>

<h4 id="say-goodbye-to-dry">Say goodbye to DRY</h4>

<p>After decades of teaching developers to write Don’t Repeat Yourself code, it seems we just stopped talking about it altogether.
Microservices by default are not DRY, with every service stuffed with redundant boilerplate. Very often the overhead of such 
“plumbing” is so heavy, and the size of the microservices is so small, that the average instance of a service has more “service” 
than 
“product”. So what about the common code that <em>can</em> be factored out?</p>

<ul>
  <li>Have a common library?</li>
  <li>How does the common library get updated? Keep different versions everywhere?</li>
  <li>Force updates regularly, creating dozens of pull requests across all repositories?</li>
  <li>Keep it all in a monorepo? That comes with its <em>own</em> set of problems.</li>
  <li>Allow for some code duplication?</li>
  <li>Forget it, each team gets to reinvent the wheel every time.</li>
</ul>

<p>Each company going this route faces these choices, and there are no good “ergonomic” options - you <em>have</em> to 
choose your version of the pain.</p>

<h4 id="developer-ergonomics-will-crater">Developer ergonomics will crater</h4>

<p>“Developer ergonomics” is the friction, the amount of effort a developer
must go through in order to get something done, be it working on a new feature or resolving a bug.</p>

<p>With microservices, an engineer has to have a mental map of the entire system in order to know what services  to bring up for any 
particular task, what teams to talk to, whom to talk to, and what about. The “you have to know everything before 
doing anything” 
principle. How 
do you keep on top of it? Spotify, a 
multi-billion dollar company, spent probably not negligible internal resources to build 
<a href="https://backstage.spotify.com/" target="_blank">Backstage</a>, software for cataloging its endless systems and services.</p>

<p>This should at least give you a clue that this game is not for everyone, and the price of the ride is <em>high</em>. So what about 
the <em>tooooling</em>? The Not Spotifies of the world are left with MacGyvering their own solutions, robustness and portability of 
which you can probably guess.</p>

<p>And how many teams actually streamline the process of starting a <em>YASS</em> - “yet another stupid service”? 
This includes:</p>

<ul>
  <li>Developer privileges in GitHub/GitLab</li>
  <li>Default environment variables and configuration</li>
  <li>CI/CD</li>
  <li>Code quality checkers</li>
  <li>Code review settings</li>
  <li>Branch rules and protections</li>
  <li>Monitoring and observability</li>
  <li>Test harness</li>
  <li>Infrastructure-as-code</li>
</ul>

<p>And of course, multiply this list by the number of programming languages used throughout the company. Maybe you have a 
usable 
template or a runbook? Maybe a frictionless, one-click system to 
launch a new service from scratch? It takes months to iron out all the kinks with this kind of automation. So, you can either 
work on your product, or you can be working on <em>toooooling</em>.</p>

<h4 id="integration-tests---lol">Integration tests - LOL</h4>

<p>As if the everyday microservices grind was not enough, you also forfeit the peace of mind offered by solid integration tests. 
Your single-service and unit tests are passing, but are your critical paths still intact after 
each commit? Who is in charge of the overall integration test suite, in Postman or wherever else? Is there one?</p>

<photo-element source="complexity/unit.gif" aspect="original-size" alt="Service tests"></photo-element>

<p>Integration testing a distributed setup is a nearly-impossible problem, so we pretty much gave up on that and replaced it with 
another one - Observability. Just like “microservices” are the new “distributed systems”, “observability” is the new 
“debugging in production”. Surely, you are not writing real software if you are not doing…. observability!</p>

<p>Observability has become its own sector, and you will pay in both pretty penny and in developer 
time for it. It doesn’t come as plug-and-pay either - you need to understand and implement canary releases, feature flags, etc. 
Who is doing that? One already 
<a href="https://renegadeotter.com/2023/07/26/i-am-not-your-cloud-person.html">overwhelmed</a> engineer?</p>

<p>As you can see, breaking up your problem does not make solving it easier - all you get is another set of <em>even 
harder problems</em>.</p>

<h3 id="what-about-just-services">What about just “services”?</h3>

<p>Why do your services need to be “micro”? What’s wrong with
<a href="https://leeatchison.com/app-architectures/moving-beyond-microservices-hype/" target="_blank">just services</a>? Some
startups have gone as far as create <em>a service for
each function</em>, and yes, “isn’t that just like Lambda” is a valid question. This 
gives you an idea of how far gone this unchecked cargo cult is.</p>

<p>So what do we do? 
<a href="https://www.fearofoblivion.com/build-a-modular-monolith-first" target="_blank">Starting with a monolith</a>
is one obvious choice. A pattern that could also  work in many instances is “trunk &amp; 
branches”, where the main “meat and potatoes” monolith is helped by “branch” services. A branch service can be a service that 
takes care of a clearly-identifiable and
separately-scalable load. A CPU-hungry <em>Image-Resizing Service</em> makes way more sense than a <em>User Registration Service</em>. Or do you
get so many registrations per second that it requires independent horizontal scaling?</p>

<photo-element source="complexity/really.gif" aspect="original-size" alt="Vertical"></photo-element>



<h3 id="the-pendulum-is-swinging-back">The pendulum is swinging back</h3>
<p>The hype, however, seems to be dying down. The VC cash faucet is tightening, and so the
businesses have been market-corrected into exercising common-sense decisions, recognizing that perhaps splurging on 
web-scale architectures when they don’t have web-scale problems is not sustainable.</p>

<photo-element source="complexity/twitter1.png" aspect="landscape" alt="Vertical"></photo-element>

<photo-element source="complexity/twitter4.png" aspect="landscape" alt="AWS"></photo-element>

<p>Ultimately, when faced with the need to travel from New York to Philadelphia, you have two options. 
You can either attempt to construct a highly intricate spaceship for an orbital descent to your destination, or you can simply 
purchase an Amtrak train ticket for a 90-minute ride. <em>That</em> is the problem at hand.</p>






</div></div>
  </body>
</html>
