<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ferd.ca/a-pipeline-made-of-airbags.html">Original</a>
    <h1>A Pipeline Made of Airbags</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
        
<p>At a former job, we used to deploy live systems by doing hot code loading. We had a thing that automatically pushed all the new code onto each server&#39;s disk, but would do nothing with it. Then an engineer would log onto one Erlang node, <a href="https://ferd.ca/repl-a-bit-more-and-less-than-that.html">in the REPL</a>. All Erlang nodes were always connected in a mesh, so one of them could talk to any other one.</p>

<p>There&#39;d be a little script that was copy/pastable in the REPL for each deploy where all the build steps were tried in development and then a staging environment ahead of time. We started from a small template that had 4 functions:</p>
<div><pre><span></span><span>UpgradeNode</span><span>()</span><span>           </span><span>% upgrade the current node; returns &#39;ok&#39; or errors out</span>
<span>NodeVersions</span><span>()</span><span>          </span><span>% return the version of all running nodes</span>
<span>NodesAt</span><span>(</span><span>Vsn</span><span>[,</span><span> </span><span>Count</span><span>])</span><span>   </span><span>% return names of all or Count nodes at a given version</span>
<span>RollingUpgrade</span><span>(</span><span>Nodes</span><span>)</span><span>   </span><span>% upgrade the nodes in the list in a rolling manner</span>
</pre></div>

<p>You&#39;d copy paste the script on the production instance you were on, call <code>UpgradeNode()</code>, see if it worked, then call <code>RollingUpgrade(...)</code> as aggressively or carefully as you thought was warranted. If you wanted, in a few milliseconds, dozens or hundreds of instances got live-deployed without losing a single connection. If you preferred, you could take it slow and do it in stages and carefully monitor things.</p>

<p>That system we&#39;d deploy to would handle over a hundred million messages per second. It was maintained and operated by one or two developers at any given time, and we could deploy it whenever without anybody even realizing it from any operational metrics. It was really neat, and fully stateful.</p>

<p>That system could, if we needed (or if we weren&#39;t confident in the safety of a given live upgrade), be slowly rolled in stages, because we were still running immutable infrastructure. But that was optional, and we were free to break down our changesets in order to be trivially live-deployable to do more of them, safely. It created new possibilities when designing code.</p>

<p>It eventually got bulldozed by multiple attempts of normalizing over Go practices, which essentially leave you on your own whenever it comes to operational aspects of the development cycle, and then containerization took over and the whole pipeline was voided of ways to do the nice stateful thing that saves everyone hours of rollouts and draining and reconnection storms with state losses.</p>

<p>At another job, we took roughly 3 months to figure out and implement in-place upgrades of signed packages for embedded surveillance equipment (such as cameras in an airport) where you could roll out updates to production devices without needing to shut down the whole infrastructure and interrupt live security. Any failure would in fact automatically crash and roll-back to the current version (in milliseconds) unless we marked a file on disk that greenlit the new version as being acceptable. It even had a small admin panel where you could do live upgrades across all versions for each device and see things change without interrupting a single data stream.</p>

<p>This also got cancelled; Docker was being seen as simpler to deal with for the whole cloud part of things, and nowadays nobody can really deploy this live. Product requirements were changed to play with the adopted tech. Scheduled downtime more or less became a necessity in most cases, because you can&#39;t afford to blank out security coverage while you roll things out in critical systems.</p>

<p>These tools still exist, but they&#39;re no longer seen as a good practice to adopt almost anywhere. I&#39;m still sour about the whole freaking docker-meets-kubernetes mandatorily-immutable ecosystem of modern day DevOps because it eagerly throws the baby with the bathwater, every time. Immutable infrastructure is good, but on its own it&#39;s also pretty underwhelming. We&#39;re now feeling network effects where the choice no longer really exists, because everything assumes you should just be stateless all the time.</p>
<p><a href="https://commons.wikimedia.org/wiki/File:Cadillac_Ranch.jpg" title="CC 2.0 Richie Diesterheft"><img src="https://ferd.ca/static/img/cadillac-ranch.jpg"/></a></p>
<p>There also exists a broad misconception that kubernetes (or any other cluster scheduler) replaces the concepts of supervision trees in Erlang/OTP. The fact is that they operate at different scopes. The &#34;just let it crash and restart&#34; for Erlang often works at a request-level and sometimes at an even finer granularity. You can still benefit from the cluster-level control plane, but you get something much richer if you can have <em>both</em>. The problem is that unless you&#39;ve tried both, you don&#39;t really have a good conception of what is possible, and it&#39;s easy to be locked to think inside the box.</p>

<p>To me, abandoning all these live upgrades to have only k8s is like someone is asking me to just get rid of all error and exceptions handling and reboot the computer each time a small thing goes wrong. There&#39;s a definite useful aspect to being able to do that, but losing the ability to work on things at a finer granularity is a huge loss.</p>

<p>The thing with &#34;let it crash&#34; and restarting is that it&#39;s a good default <em>backstop</em> mechanism for your failures. This is where you start, and if you make that scenario acceptable, then you&#39;re always in a manageable situation. This is really great. The unspoken bit about Erlang and Elixir is that from that point on, your system is a living thing that you refine by adjusting the supervision tree&#39;s structure, or by gradually handling more and more of your edge cases <em>if you know how to handle them</em>.</p>

<p>The thing that stateless containers and kubernetes do is handle that base case of &#34;when a thing is wrong, replace it and get back to a good state.&#34; The thing it does not easily let you do is &#34;and then start iterating to get better and better at not losing all your state and recuperating fast&#34;. The idea is that you <em>should</em> be able to plug in invariants and still bail out in bad cases, but also have the option of just keeping things running when they go right: no cache to warm, no synchronization to deal with, no sessions to re-negotiate, no reinstantiation, fewer feature flags to handle, and near-instant deploys rather than having them take a long time.</p>

<p>We&#39;re isolating ourselves from a whole class of worthwhile optimizations, of ways of structuring our workflows, of conceptualizing software and configuration changes.</p>

<p>Immutable infra took the backstop and forced everyone to <em>only</em> use the backstop. It&#39;s using the airbag on every single street corner with a red light you encounter, forever.</p>

        </div></div>
  </body>
</html>
