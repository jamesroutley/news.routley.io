<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kenschutte.com/gzip-knn-paper/">Original</a>
    <h1>Bad numbers in the “gzip beats BERT” paper?</h1>
    
    <div id="readability-page-1" class="page"><div><h3>Bad numbers in the &#34;gzip beats BERT&#34; paper?</h3><p><i>2023-07-17</i></p><p>The recent paper,<i>“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</i> by Jiang et al. [<a href="https://aclanthology.org/2023.findings-acl.426/">link</a>] recently received a lot of attention on twitter.</p><p>I recently checked out their <a href="https://github.com/bazingagin/npc_gzip">source code</a> to try to recreate their results and to try out  some related ideas.</p><p>
What I found (although I could be mistaken!)
is that there appears
to be a
bug (or at least an unexpected choice) in their kNN code
that makes all the accuracy numbers
for their method higher than
expected.
<b>[tldr: it is reporting a top-2 accuracy rather than a kNN(k=2) accuracy]</b>.</p><p>Table 5 from the paper was often included in tweets and shows the <code>gzip</code> method beating all these other neural-network-based methods:</p><p><a href="https://kenschutte.com/gzip-knn-paper/table5.png"><img src="https://kenschutte.com/gzip-knn-paper/table5.png"/></a></p><p>
I&#39;ll explain the details below, but my
calculations for these experiments
are (first 4 datasets,
only the &#34;Full&#34; column):
        </p><table id="tab1"><thead><tr><th></th><th>KinyarwandaNews</th><th>KirundiNews</th><th>DengueFilipino</th><th>SwahiliNews</th></tr></thead><tbody><tr><th>in paper</th><td>0.891</td><td>0.905</td><td>0.998</td><td>0.927</td></tr><tr><th>corrected (knn2d)</th><td>0.835</td><td>0.858</td><td>0.999</td><td>0.850</td></tr></tbody></table><p>(Fifth dataset SogouNews is large -- I haven&#39;t run it yet)</p><p>
These numbers would significantly change the take-away from
these experiments. For example,
for <code>KirundiNews</code>, the <code>gzip</code> method went from best-perfoming to worst-performing.</p><h4>kNN</h4><p>Their method uses a <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">kNN classifier</a> using k=2 (Appendix C says all experiments used k=2).</p><p>
k=2 is a bit of an odd choice for kNN classification.
For every test case, you search the training set
for the two &#34;closest&#34; examples. Looking at
the labels of these two, there are only two possibilities,
            </p><ul><li><i>The labels are equal.</i>
 So, this is clearly your hypothesized label.
Note that you would get the same answer for k=1.
</li><li><i>The labels are different.</i>
 We have a 1-1 tie that most be broken. There
are many ways to do a tie breaker, but one
reasonable one is take the label of the closer point.
In this case, you get the same answer for k=1.                    
</li></ul><p>

So, going from k=1 to k=2 doesn&#39;t add much information to your classifier. But, it can be different depending on the tie-breaking strategy.  
            </p><p>
It is in the case of ties that the source code
here is doing something unexpected, shown below.
            </p><h4>Code</h4><p>The issue is in <code>calc_acc</code> method in <code>experiments.py</code> [<a href="https://github.com/bazingagin/npc_gzip/blob/main/experiments.py#L88">here</a>].</p><p>Here is the relevant snippet, with my added comments, and branches dealing with <code>rand==True</code> removed for clarity:</p><div><pre><span></span><span># here, sorted_pred_lab[][] has the </span>
<span># labels and counts corresponding</span>
<span># to the top-k samples,</span>
<span>#  [[label,count],[label,count],...]</span>
<span># grouped-by label and sorted by count.</span>

<span>most_label</span> <span>=</span> <span>sorted_pred_lab</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span>
<span>most_count</span> <span>=</span> <span>sorted_pred_lab</span><span>[</span><span>0</span><span>][</span><span>1</span><span>]</span>

<span>if_right</span> <span>=</span> <span>0</span>
<span>for</span> <span>pair</span> <span>in</span> <span>sorted_pred_lab</span><span>:</span>
    <span># we loop until we drop below &#39;most_count&#39;, ie</span>
    <span># this for-loop iterates over those classes</span>
    <span># tied for highest count</span>
    <span>if</span> <span>pair</span><span>[</span><span>1</span><span>]</span> <span>&lt;</span> <span>most_count</span><span>:</span>
        <span>break</span>

    <span># this says if ANY of those</span>
    <span># in the tied-set are equal to</span>
    <span># the test label,</span>
    <span># it is marked correct (if_right=1)</span>
    <span>if</span> <span>pair</span><span>[</span><span>0</span><span>]</span> <span>==</span> <span>label</span><span>[</span><span>i</span><span>]:</span>
        <span>if_right</span> <span>=</span> <span>1</span>
        <span>most_label</span> <span>=</span> <span>pair</span><span>[</span><span>0</span><span>]</span>

<span># accumulate results:        </span>
<span>pred</span><span>.</span><span>append</span><span>(</span><span>most_label</span><span>)</span>
<span>correct</span><span>.</span><span>append</span><span>(</span><span>if_right</span><span>)</span>
        
</pre></div><p>So, if any of the tie-break labels is equal to the test label, it is marked as correct. For k=2, a tie simply means there was one vote for each of two different classes amongst the 2 closest training points. Therefore, the reported accuracies could be considered <b>top-2</b>, meaning that it&#39;s marked correct if either of the top  two choices is correct (you may have encountered top-k in ImageNet, where top-5 accuracy is often cited).</p><p>This method takes arbitrary <code>k</code> but note that it doesn&#39;t compute <code>top-k</code> for any <code>k</code>. Only in the special case of <code>k=2</code> do we have that when there is a tie, all the <code>k</code> examples are tied with the max value (1).</p><p>The <code>calc_acc</code> method has a <code>rand</code> flag that seems to be correct:  if <code>rand==True</code> it will correctly break the tie using <code>random.choice</code>. But it seems that this wasn&#39;t used for the paper results.</p><h4>Re-calc</h4><p>
I wrote a simple implementation with two
different tie-breaking strategies [<a href="https://gist.github.com/kts/46709a70c4efd167eff7ccfa62f30dd8">gist</a>]:</p><ul><li>[r] random selection</li><li>[d] decrement k until you are left with a case without ties.</li></ul><pre>Results        
           kinnews  kirnews  filipino swahili 
table5     0.891    0.905    0.998    0.927   value in paper
code       0.891    0.906    1.000    0.927   using npc_gzip repo
top2       0.891    0.906    1.000    0.927   top-2
knn1r      0.835    0.858    0.999    0.850   kNN,k=1,tie=random
knn1d      0.835    0.858    0.999    0.850   kNN,k=1,tie=decrement
knn2r      0.828    0.807    0.851    0.842   kNN,k=2,tie=random
knn3r      0.838    0.791    0.851    0.881   kNN,k=3,tie=random
knn2d      0.835    0.858    0.999    0.850   kNN,k=2,tie=decrement
knn3d      0.843    0.794    0.904    0.883   kNN,k=3,tie=decrement
</pre><p>Some sanity checks:</p><ul><li><code>code</code> always equals <code>top2</code></li><li><code>knn1r == knn1d</code>. There are never ties for k=1</li><li><code>knn2d == knn1d</code>. For k=2, ties go to first, so same as using k=1.</li><li><code>knn2r &lt; knn2d</code>. For k=2, on a 1-1 tie, random is just taking the further one 50% of the time. So, it makes sence that&#39;s worse than just taking the closest.</li><li><code>table5</code> very close to <code>code</code> (within 0.001 or 0.002): able to recreate numbers from paper</li></ul><p>todo:</p><ul><li>Why is filipino so high (1.0 in one case)?</li><li>Why is &#39;table5&#39; slightly different than &#39;code&#39; in two cases?</li></ul></div></div>
  </body>
</html>
