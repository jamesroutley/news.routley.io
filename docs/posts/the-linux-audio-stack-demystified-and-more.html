<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.rtrace.io/posts/the-linux-audio-stack-demystified/">Original</a>
    <h1>The Linux audio stack demystified (and more)</h1>
    
    <div id="readability-page-1" class="page"><div>
            
            <h2 id="Introduction"><a href="#Introduction" title="Introduction"></a>Introduction</h2>
            <p>Digital audio processing is a complex yet fascinating subject. Linux, known for its robust and flexible system architecture, runs a sophisticated audio stack that can be particularly daunting to understand. Grasping it properly requires knowledge in multiple technical disciplines, including physics, electrical/electronical engineering, and software engineering specific to the Linux ecosystem. This article aims to demystify the Linux audio stack by explaining the basics of sound, how humans perceive sound, and the workings of digital audio. We will then delve into the components that make up the Linux audio system and explore how these interact. Buckle up; it’s going to be a long and informative read.</p>
            <h2 id="Exclaimer"><a href="#Exclaimer" title="Exclaimer"></a>Exclaimer</h2>
            <p>In this article, I will simplify concepts and ideas to provide you with the basic knowledge needed to follow along. Each chapter could easily be expanded into a dedicated, lengthy blog post. However, for simplicity’s sake, I have refrained from doing so. If you would like further details about a specific chapter, please feel free to contact me or leave a comment. I’ll be happy to delve into more detail if needed.</p>
            <h2 id="Understanding-Sound-and-Sound-Waves"><a href="#Understanding-Sound-and-Sound-Waves" title="Understanding Sound and Sound Waves"></a>Understanding Sound and Sound Waves</h2>
            <p>Before delving into the technicalities of audio processing, it’s essential to understand what sound is. In a nutshell, sound is a vibration that propagates as an acoustic wave through a medium such as air. These waves are created by vibrating objects (such as speakers, headphones, piezo buzzers, etc.), causing fluctuations in air pressure that our ears can pick up and perceive as sound. Audio waves can have many shapes, the most common waveform is the sine wave. It is the fundamental building block for all other waves existing, and thus it’s important to learn about the sine wave first.</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/sine-wave-fundamentals.svg" alt="Sine-Wave Fundamentals"/></p>
            <p>All waveforms have 2 fundamental key properties:</p>
            <ul>
              <li><code>Period (duration)</code>: The time it takes the wave to perform a single oscillation.</li>
              <li><code>Amplitude</code>: This represents the wave’s strength or intensity, which we perceive as volume. Higher amplitudes generally correspond to louder sounds.</li>
            </ul>
            <p>We can derive the <code>Frequency</code> from the period duration of a wave form. The frequency is measured in Hertz (Hz), and it represents the number of oscillations (waves) per second. Higher frequencies correspond to higher-pitched sounds. In other words, the shorter the period duration, the higher the pitch. The higher the frequency the higher the pitch. Thus, the frequency is the inverse of the period duration, or in mathematical terms:</p>
            <p><code>f = 1 / (period duration[s]) = (period duration[s])⁻¹</code>.</p>
            <p>Amplitudes can be measured as the pressure variation in the air - the so called Sound Pressure Level (SPL). The standard unit for pressure is Pascal (Pa). In audio contexts, we often deal with very small pressures, so micropascals (µPa) are typically used. However, the most common way of describing amplitude is Decibels (dB). Decibels are a logarithmic unit used to describe the ratio of a particular sound level to a given reference level. Decibels are the preferred unit, because the human ear perceives sound pressure levels logarithmically. </p>
            <ul>
              <li><code>dB SPL</code>: Sound pressure level in decibels. In air, the reference pressure is typically 20 µPa (typically referred to as <code>P0</code>), the threshold of human hearing. It is calculated using the formula <code>SPL = 20 * log₁₀(P/P0)</code>, where <code>P</code> is the measured pressure and <code>P0</code>​ is the reference pressure (20 µPa). </li>
              <li><code>dBV</code> and <code>dBu</code>: When sound is converted into an electrical signal by a microphone, the amplitude can be measured in volts (V). This is common in audio equipment and recording. <code>dBV</code> and <code>dBU</code> are units for voltage measurements in audio systems, referencing 1 volt for dBV or 0.775 volts for dBu. Similarily to <code>db SPL</code>, <code>dbV and dBu</code> can be calculated using the same formula, just exchange the reference pressure with a reference voltage. dBV = 20 * log₁₀(V/V0)<code>, where </code>V<code>is the measured voltage and</code>V0`​ is the reference voltage.</li>
            </ul>
            <p>You might wonder how a sine wave actually sounds like? Here’s an example of a sine wave at a frequency of 440 Hz. Please check your volume before you play!</p>
            
            <h2 id="How-humans-process-sound-waves"><a href="#How-humans-process-sound-waves" title="How humans process sound waves"></a>How humans process sound waves</h2>
            <p>Sound waves are captured by the <code>pinna</code>, the visible part of the outer ear. The pinna helps direct sound waves into the ear canal. The sound waves travel through the ear canal and reach the eardrum (tympanic membrane). The sound waves cause the eardrum to vibrate (very similar to sound waves bring microphone membranes to vibrate). These vibrations correspond to the frequency and amplitude of the incoming sound waves. The vibrations from the eardrum are transmitted to the three tiny bones in the middle ear, known as the ossicles (malleus, incus, and stapes). The ossicles amplify the vibrations and transmit them to the oval window, a membrane-covered opening to the inner ear. The vibrations pass through the oval window and enter the cochlea, a spiral-shaped, fluid-filled structure in the inner ear. Inside the cochlea, the vibrations create waves in the cochlear fluid, causing the basilar membrane to move. The movement of the basilar membrane stimulates tiny hair cells located on it. These hair cells convert mechanical vibrations into electrical signals through a process called transduction. Different frequencies of sound stimulate different parts of the basilar membrane, allowing the ear to distinguish between various pitches. The hair cells release neurotransmitters in response to their movement, which generates electrical impulses in the auditory nerve. These electrical signals travel along the auditory nerve to the brain. The auditory signals reach the auditory cortex in the brain, where they are processed and interpreted as recognizable sounds, such as speech, music, or noise.</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/human-ear-anatomy.png" alt="Anatomy of the human ear"/></p>
            <p>Hard facts to better understand the dimensions of sound waves:</p>
            <ul>
              <li>The typical frequency range of human hearing is from ~20 Hz to ~20.000 Hz (20 kHz). This range can vary with age and exposure to loud sounds.</li>
              <li>The frequency of humans speaking typically falls within the range of ~300 Hz to ~3.400 Hz (3.4 kHz).</li>
              <li>Human ears are most sensitive to frequencies between 2,000 Hz and 5,000 Hz.</li>
              <li>The quietest sound that the average human ear can hear is around 0 dB SPL (sound pressure level), equivalent to 20 micropascals.</li>
              <li>The loudest sound that the average human ear can tolerate without pain is around 120-130 dB SPL. Sounds above this level can cause immediate hearing damage.</li>
              <li>The range between the threshold of hearing and the threshold of pain is known as the dynamic range of human hearing, which is approximately 120 dB.</li>
            </ul>
            <h2 id="How-Digital-Audio-Works"><a href="#How-Digital-Audio-Works" title="How Digital Audio Works"></a>How Digital Audio Works</h2>
            <p>Now, that we got a rudimentary understanding of sound waves in the analog world work, let’s continue with sound waves in a world of zeroes and ones.</p>
            <h3 id="Audio-Input-and-Sampling"><a href="#Audio-Input-and-Sampling" title="Audio-Input and Sampling"></a>Audio-Input and Sampling</h3>
            <p>Sampling to computers is what hearing is to humans. Sampling is the process of converting analog sound waves into digital data that a computer can process. This is done by taking regular measurements of the amplitude of the sound wave at fixed intervals. The rate at which these samples are taken is called the sampling rate and is measured in samples per second (Hz). Common sampling rates include 44.1 kHz (CD quality) and 48 kHz (professional audio).</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/sine-wave-sampling.svg" alt="Sampling rate on a sine wave"/></p>
            <p>Each lollipop on the sine wave represents a single discrete sample/measurmeent. The first oscillation has a higher sample rate, the second oscillation has a lower sample rate for demonstration purposes only. Typically, the sample rate is constant, and more than double of the highest frequency humans can hear.</p>
            <h4 id="Nyquist-Shannon"><a href="#Nyquist-Shannon" title="Nyquist-Shannon"></a>Nyquist-Shannon</h4>
            <p>The Nyquist-Shannon Sampling Theorem is a fundamental principle in the field of digital signal processing. It provides a criterion for determining the minimum sampling rate that allows a continuous signal to be perfectly reconstructed from its samples. A continuous-time signal that has been bandlimited to a maximum frequency fmax (meaning it contains no frequency components higher than fmax​) can be completely and perfectly reconstructed from its samples if the sampling rate fs is greater than twice the maximum frequency of the signal.</p>
            <p>Mathematically, this can be expressed as: <code>fs &gt; 2 * fmax</code></p>
            <p>This nicely explains why common sampling rate are typically a little bit more than double of the human hearing range (e.g. 44.1 kHz for CD quality).</p>
            <h4 id="Quantization"><a href="#Quantization" title="Quantization"></a>Quantization</h4>
            <p>In the analog world, signals have infinite precision. For example, if you zoom in on a sine wave, you can always find more detail — the waveform continues to be smooth and precise no matter how closely you examine it. However, in the digital world, precision is limited. To understand this limitation, consider this question: “How many numbers can you represent between 0 and 1?” If you use 0.1 as the smallest possible increment value, you can only represent 10 discrete values between 0 and 1. If you refine the smallest increment to 0.01, you get 100 values between 0 and 1. In the analog realm, there’s no smallest possible increment; values can be infinitely precise. But in digital systems, precision is constrained by the number of bits used to represent the signal. Digital audio uses a fixed number of bits to encode each sample of sound. For instance, in a common 16-bit audio system, there are 65,536 discrete levels available to represent the amplitude of each sample. This means that the smallest difference between two levels, or the smallest possible increment, is approximately 0.000030518.</p>
            <p>The process of approximating each sample’s amplitude to the nearest value within these discrete levels is known as quantization. The number of discrete levels depends on the bit depth of the audio. For example:</p>
            <ul>
              <li>16-bit audio provides 65.536 possible amplitude values.</li>
              <li>24-bit audio offers 16.777.216 possible amplitude values.</li>
            </ul>
            <p>Quantization is essential because it allows us to represent analog signals with a finite number of discrete values, making digital storage and processing feasible, even though we sacrifice some level of precision compared to the analog world. For all the audiophiles reading this: 24-bit resolution is enough! No, you don’t hear the difference between analog signals and 24-bit digital signals.</p>
            <h4 id="Storing-Digital-Audio-signals"><a href="#Storing-Digital-Audio-signals" title="Storing Digital Audio signals?"></a>Storing Digital Audio signals?</h4>
            <p>With an understanding of sampling and quantization, we can now explore how digital audio can be stored. For illustrative purposes, let’s consider designing a basic audio format to store digital audio, using the simplicity of the CSV (Comma-Separated Values) format as our foundation. Although this proposed format would be inefficient compared to established audio formats like MP3, WAV, or FLAC, it serves as a useful exercise in understanding the principles behind audio file storage. CSV files are a familiar format for many, used to store tabular data in a straightforward, readable manner. They consist of rows and columns separated by commas, making them an ideal starting point for conceptualizing audio storage. The key here is to recognize that a sampled audio signal can be represented similarly to a table in a spreadsheet, where each row corresponds to a discrete sample in time.</p>
            <p>Let’s define a CSV-based audio format with the following structure:</p>
            <ul>
              <li>Sample Index: Each row represents a discrete sample point. For example, in the case of a sine wave, each row corresponds to a specific point along the waveform. Typically the sample index is just a self-incrementing number. </li>
              <li>Quantized Value: The second column contains the quantized amplitude value for each sample.</li>
              <li>for simplicity sake, we assume that the sample rate is 44.1 kHz (this information is needed for later playback) and the bit depth is statically set to 16 bit.</li>
            </ul>
            <p>a simple CSV file could like this:</p>
            <pre data-language="csv"><code>Sample Index, Quantized Value
1, 0.32767
2, 0.32780
3, 0.32809
4, 0.32853
...</code></pre>
            <p>While CSV is of course not a practical choice for real-world audio storage due to its inefficiency and lack of support for metadata, compression, multiple channels, and other critical features, this exercise helps illustrate the fundamental concepts of how digital audio data can be organized and stored. Real-world formats like MP3, WAV, and FLAC utilize more sophisticated techniques to handle audio data efficiently, like</p>
            <ul>
              <li>Compression, for reducing file size while maintaining audio quality.</li>
              <li>Metadata, for storing additional information about the audio file, such as title, artist, and duration, sample rate, bit-depth</li>
              <li>Error Correction for ensuring data integrity during storage and playback.</li>
              <li>as well as support for multiple channels (e.g. for stereo signals)</li>
            </ul>
            <h4 id="Mono-and-Stereo"><a href="#Mono-and-Stereo" title="Mono and Stereo"></a>Mono and Stereo</h4>
            <p><code>Mono</code> (short for monaural) refers to audio that is recorded and played back through a single channel. In a mono audio setup, all sounds are combined into one channel, and this single channel is then played through one or more speakers or headphones. The primary advantage of mono audio is its simplicity and the uniformity it provides — sound is uniformly distributed regardless of how many speakers are used. This makes mono suitable for applications where spatial sound is not crucial, such as telephone conversations or certain types of spoken-word recordings. Even audio on bigger venues is typical in mono.</p>
            <p>On the other hand, <code>stereo</code> (short for stereophonic) involves two audio channels: left and right. Stereo recording captures sound from two distinct channels, creating a sense of spatial separation between sounds. This allows for a more immersive listening experience because different sounds can be directed to different speakers or headphones. For instance, in a stereo mix, you might hear a guitar predominantly through the left speaker and a vocal through the right speaker. This spatial separation mimics the way humans naturally hear sounds in real life, making stereo particularly effective for music, movies, and other applications where a realistic sound field enhances the experience.</p>
            <h3 id="Audio-Output"><a href="#Audio-Output" title="Audio-Output"></a>Audio-Output</h3>
            <p>We’ve learned a lot about audio waves, and how audio waves can be captured and stored. Now it’s time to move forward and think about how digitally stored audio is played back. As we’ve discussed in the previous article, digital audio data is typically stored as a sequence of discrete samples that represent the amplitude of a sound wave at various points. This data is then streamed to your computers sound card. A DAC (Digital-To-Analog) converter converts these digital values into a continuous analog signal. This conversion process typically involves 2 steps. First, the digital samples are mapped to discrete voltage levels. In case your sound card outputs line-level, the highest possible digital representable value would map to 0.447V (447 mV). To smooth out the discrete steps and reconstruct a continuous waveform, the DAC uses a low-pass filter. This filter removes high-frequency artifacts (often referred to as “sampling noise”) that result from the discrete nature of the digital data. Finally, the filtered analog signal is sent to the output stage, which drives the speakers or headphones.</p>
            <h2 id="The-Role-of-a-Sound-Card-x2F-Audio-Interface"><a href="#The-Role-of-a-Sound-Card-x2F-Audio-Interface" title="The Role of a Sound Card / Audio Interface"></a>The Role of a Sound Card / Audio Interface</h2>
            <p>A sound card is a important component in a computer’s audio system. It handles the input and output of audio signals, converting between analog and digital forms. Sound cards typically include ADCs for recording, DACs for playback, and additional circuitry for audio processing.</p>
            <p>Typical Components of a Sound Card:</p>
            <ul>
              <li>ADCs (Analog-to-Digital Converters): Converts analog signals from microphones or other input devices into digital data.</li>
              <li>DACs (Digital-to-Analog Converters): Converts digital audio data into analog signals for playback through speakers or headphones.</li>
              <li>Amplifiers: Boosts the audio signal to a level that can drive speakers or headphones.</li>
            </ul>
            <h3 id="What-makes-a-good-Sound-Card-x2F-Audio-Interface"><a href="#What-makes-a-good-Sound-Card-x2F-Audio-Interface" title="What makes a good Sound Card / Audio Interface?"></a>What makes a good Sound Card / Audio Interface?</h3>
            <p>Often times I get the question what audio interface is a good audio interface. Typically I don’t want to recommend a particular brand, but explain people what they should look for when comparing audio interfaces. Here’s a short and opinionated set of aspects you should look out for.</p>
            <ul>
              <li>Signal-to-Noise Ratio (SNR): A high SNR indicates that the sound card produces clear audio with minimal background noise. Look for a sound card with a high SNR, typically 100 dB or higher.</li>
              <li>Total Harmonic Distortion (THD): Low THD means that the sound card introduces minimal distortion to the audio signal. Good sound cards often have THD ratings below 0.01%.</li>
              <li>Dynamic Range: A wide dynamic range allows the sound card to accurately reproduce both very quiet and very loud sounds without distortion or noise.</li>
              <li>Sampling Rate: A higher sampling rate allows for more accurate representation of audio. Common high-quality sound cards support rates up to 192 kHz or higher.</li>
              <li>Bit Depth: A higher bit depth enables better resolution of audio details. Good sound cards support 24-bit depth, providing a broader range of amplitudes and finer detail.</li>
              <li>Low Latency: Good sound cards have low latency, which is essential for real-time applications like gaming, live recording, and professional audio production. Latency is the delay between input and output, and lower latency ensures more immediate audio responses.</li>
            </ul>
            <h3 id="The-Stairstep-Fallacy"><a href="#The-Stairstep-Fallacy" title="The Stairstep Fallacy"></a>The Stairstep Fallacy</h3>
            
            <p>
              <a target="_blank" rel="noopener" href="https://yt.artemislena.eu/watch?v=cIQ9IXSUzuM"><b>Source</b></a>: Invidious on <a target="_blank" rel="noopener" href="https://yt.artemislena.eu">yt.artemislena.eu</a> provided by <a target="_blank" rel="noopener" href="https://artemislena.eu">artemislena</a>
            </p>
            <h2 id="The-Linux-Audio-Stack"><a href="#The-Linux-Audio-Stack" title="The Linux Audio Stack"></a>The Linux Audio Stack</h2>
            <p>Now that we’ve covered the basics, let’s delve into how Linux manages audio. The Linux audio system employs a modular and layered architecture to handle audio processing, which provides both abstraction and flexibility. Layering in Linux audio allows for a structured approach to audio management. By separating concerns into different layers, the system abstracts the complexities of lower-level operations, such as direct hardware interactions. This separation means that user-space applications do not need to deal with the specifics of hardware configurations, streamlining development and improving compatibility. Modularity complements this layered approach by allowing individual components to be swapped or reconfigured without disrupting the entire system. This means that different layers or components can be replaced or updated as needed. For example, if you plug in a headset, the system can dynamically switch from speaker output to headset output thanks to its modular design. Similarly, if you change your sound card or audio driver, the modular system can adapt without requiring extensive reconfiguration.</p>
            <h3 id="ALSA-Advanced-Linux-Sound-Architecture"><a href="#ALSA-Advanced-Linux-Sound-Architecture" title="ALSA (Advanced Linux Sound Architecture)"></a>ALSA (Advanced Linux Sound Architecture)</h3>
            <p>Let’s start with the foundation of all Linux systems when it comes to audio. ALSA is the core layer of the Linux audio stack. It provides low-level audio hardware control, including drivers for sound cards and basic audio functionality. ALSA provides a standardized interface for audio hardware, allowing applications to interact with sound cards and other audio devices without needing to know the specifics of the hardware. It includes a set of drivers for different types of sound cards and audio interfaces. These drivers translate the high-level audio commands from applications into low-level instructions understood by the hardware.</p>
            <p>ALSA operates in the kernel space, meaning it interacts directly with the Linux kernel. This position allows it to manage hardware resources and perform low-level audio operations efficiently. To allow the user space from profiting from ALSA, ALSA provides libraries and tools in user space, such as <code>libasound</code> (the ALSA library), which applications use to interface with the ALSA kernel drivers. This library offers functions for audio playback, recording, and control.</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/alsamixer.png" alt="Alsa Mixer - an application to control ALSA from th user space"/></p>
            <p>ALSA supports multi-channel audio, enabling configurations like 5.1 or 7.1 surround sound. Additionally, it is designed to offer low-latency audio processing, which is essential for real-time applications like music production. Further, ALSA includes a mixer interface that allows users to control audio levels and settings, such as adjusting volume or muting channels.</p>
            <h3 id="JACK"><a href="#JACK" title="JACK"></a>JACK</h3>
            <p>JACK (the Jack Audio Connection Kit) is a professional-grade audio server designed for Unix-like operating systems, including Linux. It is tailored for real-time, low-latency audio processing and is widely used in professional audio production environments. JACK is designed to provide low-latency, real-time audio processing. This is essential for applications where timing and synchronization are critical, such as live music performance, recording, and audio production. It allows multiple audio applications to connect and communicate with each other. This means that the output of one application can be routed to the input of another, enabling complex audio processing chains.</p>
            <p>JACK excels in delivering minimal latency, which is crucial for real-time audio tasks. This ensures that audio signals are processed and transmitted with minimal delay, maintaining synchronization and responsiveness. It offers sample-accurate synchronization between audio streams, ensuring precise timing and phase alignment across different audio applications and devices. JACK uses a client-server model, where the JACK server manages audio data and clients (applications) connect to it for audio processing. This modular approach allows for flexibility and scalability in audio setups. JACK includes transport control features that allow users to synchronize playback and recording across multiple applications, making it easier to manage complex audio projects. JACK builds on top of ALSA and requires ALSA to be installed and configured correctly.</p>
            <h3 id="PulseAudio"><a href="#PulseAudio" title="PulseAudio"></a>PulseAudio</h3>
            <p>PulseAudio is a sound server for Linux and other Unix-like operating systems that sits on top of lower-level sound systems such as ALSA (Advanced Linux Sound Architecture). It provides a higher-level interface for audio management, offering additional features and flexibility beyond what is available directly through ALSA. PulseAudio acts as an intermediary between applications and the underlying sound hardware. It handles audio streams, mixing, and routing, allowing multiple applications to produce sound simultaneously and manage their audio independently. One of PulseAudio’s key features is its ability to stream audio over a network. This allows users to play audio from one machine on another machine or device connected to the same network.</p>
            <p>PulseAudio enables the mixing of multiple audio streams into a single output. This means you can listen to music while receiving notifications from other applications, all mixed seamlessly into your speakers or headphones. Additionally, it allows for independent volume control of each application. This means you can adjust the volume of your music player separately from your web browser or other applications. PulseAudio supports various audio effects and processing features, such as equalization, sound enhancements, and echo cancellation. Further, it manages audio devices and allows easy switching between them. For instance, you can switch audio output from speakers to headphones or to an external Bluetooth device without disrupting playback.</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/pulseaudio-architecture.png" alt="PulseAudio Architecture"/></p>
            <p>PulseAudio provides a more intuitive and user-friendly way to manage audio compared to working directly with ALSA. Its ability to handle various audio sources and outputs simultaneously, along with network capabilities, makes it versatile for different audio needs and setups. By providing advanced features such as audio mixing, device switching, and network streaming, PulseAudio contributes to a richer and more flexible audio experience on Linux systems - particularly the Linux Desktop. While it’s a little bit of a meme, PulseAudio made “the year of the Linux desktop” a lot more likely.</p>
            <h3 id="PipeWire"><a href="#PipeWire" title="PipeWire"></a>PipeWire</h3>
            <p>PipeWire is a modern multimedia framework designed for Linux systems that aims to unify and enhance the handling of both audio and video streams. Developed as a replacement for both PulseAudio and JACK, PipeWire provides a flexible and efficient infrastructure for managing complex multimedia tasks. PipeWire combines the capabilities of audio servers like PulseAudio and low-latency audio servers like JACK into a single framework. It also supports video processing and capture, making it a comprehensive solution for managing both audio and video streams. Like JACK, PipeWire offers low-latency audio processing, which is crucial for professional audio applications such as live music production and sound design.</p>
            <p>PipeWire is designed to be highly flexible, accommodating various use cases ranging from consumer audio and video to professional audio production and video conferencing. It provides a unified API that can handle both audio and video streams, simplifying development and integration for applications that need to process multimedia content.Further, PipeWire is engineered from the ground up to deliver low-latency audio and real-time performance, making it suitable for high-performance audio tasks and complex multimedia workflows. Finally, PipeWire includes features for improved security and sandboxing, allowing applications to access only the resources they need and protecting against potential security vulnerabilities.</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/pipewire-qpwgraph.png" alt="qpwgraph, a graphical utility to understand/display audio routing with Pipewire"/></p>
            <p>By combining audio and video management into a single framework, PipeWire provides a more integrated and cohesive approach to handling multimedia content. Its design incorporates modern requirements and use cases, including enhanced security, better real-time performance, and support for a wide range of multimedia applications. PipeWire also offers compatibility with existing audio and video servers, easing the transition from older systems and allowing for gradual adoption. To give an example, even if one of your applications is built to use PulseAudio, PipeWire supports a PulseAudio compatible input API, so that an application does not have to care if it’s PulseAudio or PipeWire. A similar input API exists also for JACK - PipeWire will emulate the Sound Server API of JACK. Ulike PuleAudio and JACK, PipeWire does not require ALSA on a system, in fact if ALSA is installed the output of ALSA is very likely pushed through PipeWire.</p>
            <h3 id="So-what-is-it-a-sound-server-does"><a href="#So-what-is-it-a-sound-server-does" title="So what is it a sound server does?"></a>So what is it a sound server does?</h3>
            <h4 id="Mixing-multiple-input-streams"><a href="#Mixing-multiple-input-streams" title="Mixing multiple input streams"></a>Mixing multiple input streams</h4>
            <p>In essence, a sound server acts as an intermediary that takes multiple audio input streams with varying sample rates and bit depths, mixes them into a single output signal, and sends it to the sound card. For example, imagine one application generating a sine wave and another producing a rectangular wave. In today’s world, it would be unimaginable not to be able to watch a video while simultaneously receiving notification sounds from your messenger (or more generally to receive audio from multiple input sources). This is where the sound server implementations such as PipeWire, JACK and PulseAudio come in — a sound server combines the two (or more) input signals into one output signal and outputs it to the sound card. The sound server achieves this by summing each sample of the input signals, effectively merging multiple inputs into a single output. The operation performed is sample-wise addition. In case a faster sampled signal needs to mixed with a lower-sampled signal, the audio server will also have to interpolate the values for the signal with the lower sample rate. All this is happening in real time with little to no latency introduced (depending on the buffer size of the mixer buffer).</p>
            <p><img src="https://blog.rtrace.io/images/linux-audio-stack-demystified/sum-of-2-input-signals-as-output-signal.png" alt="Mixing together 2 input signals to generate one output signal"/></p>
            <h4 id="Multiple-Output-Streams-x2F-Mixes"><a href="#Multiple-Output-Streams-x2F-Mixes" title="Multiple Output Streams/Mixes"></a>Multiple Output Streams/Mixes</h4>
            <p>If you own a laptop, there’s high chance you have 2 audio output devices. One Headphone output if you connect your headphones to the audio output jack. And one for the built-in speakers in your laptop. A sound server can also output audio to multiple outputs at the same time. Let’s say you prefer listening to music through your headphones, and a friend is in your room and you’d like to show them a cool track you enjoy. So you decide to activate output on both devices at the same time.</p>
            <h4 id="Volume-Control-per-input-stream"><a href="#Volume-Control-per-input-stream" title="Volume Control per input stream"></a>Volume Control per input stream</h4>
            <p>Typically each input stream comes from different applications on your Linux Desktop. And sometimes you’d like to individually control volume for each of your applications. Let’s say you’re watching a movie and you’d like to mute all messengers in the meanwhile. You can configure the sound server to reduce/mute the volume of the corresponding input streams.</p>
            <h4 id="Virtual-Outputs"><a href="#Virtual-Outputs" title="Virtual Outputs"></a>Virtual Outputs</h4>
            <p>Not every signal needs to be routed to an actual output. Think about your microphone. Most of the time the microphone is not outputted on your speakers/headphones, yet the input stream is provided to applications such as your favorite VOIP/conferencing tool. The concept of virtual outputs allows you to create an arbitrary number of output streams that applications can consume. </p>
            <h4 id="Sound-effects-for-streams"><a href="#Sound-effects-for-streams" title="Sound effects for streams"></a>Sound effects for streams</h4>
            <p>A sound server allows to apply various effects to individual input streams or output streams using effect plugins. These plugins, available in formats such as LADSPA, CLAP, VST2, VST3, AU, and RTAS, allow you to integrate software effects into your audio routing chain seamlessly.</p>
            <h5 id="Example-Using-a-Compressor"><a href="#Example-Using-a-Compressor" title="Example: Using a Compressor"></a>Example: Using a Compressor</h5>
            <p>One practical example of an effect plugin is a compressor. If you often find yourself adjusting the volume while watching movies—turning it down during loud action scenes and back up for quieter dialogue—a compressor can help. By adding a compressor to the input stream of your media player or browser, you can reduce the volume of the loud parts without affecting the quieter sections. Most compressors include a make-up gain feature to increase the overall volume after compression, ensuring a balanced listening experience.</p>
            <h5 id="Example-Adding-Reverb"><a href="#Example-Adding-Reverb" title="Example: Adding Reverb"></a>Example: Adding Reverb</h5>
            <p>Another use case is enhancing your microphone input with a bright reverb effect, making it sound as though you’re speaking in a large room. This can add a sense of space and presence to your voice.</p>
            <h5 id="Example-Applying-an-Equalizer"><a href="#Example-Applying-an-Equalizer" title="Example: Applying an Equalizer"></a>Example: Applying an Equalizer</h5>
            <p>You might also want to use an equalizer to adjust your output stream. For instance, if your speakers produce a muddy bass around 200Hz, you can use an equalizer to reduce the gain in that frequency range, resulting in clearer sound.</p>
            <h5 id="Example-Denoising"><a href="#Example-Denoising" title="Example: Denoising"></a>Example: Denoising</h5>
            <p>If your microphone has a lot of background noise, you can apply a denoising effect to clean up the signal before it reaches your VOIP tool. This ensures that your voice comes through clearly during calls.</p>
            <h5 id="Combining-Multiple-Effects"><a href="#Combining-Multiple-Effects" title="Combining Multiple Effects"></a>Combining Multiple Effects</h5>
            <p>For more advanced setups, you can combine multiple effects like reverbs, delays, compressors, and equalizers into your audio chain. Whether you need simple adjustments or complex processing, a robust sound server in the Linux audio stack provides the flexibility and control you need.</p>
            <p>The modular and layered design of the Linux audio stack, combined with the wide array of available plugins, allows you to customize your audio experience to suit your specific needs. Whether you’re looking to enhance your media playback, improve your microphone input, or create a unique sound environment, the Linux audio stack has the tools to make it happen.</p>
            <h4 id="Providing-an-API-for-application-developers"><a href="#Providing-an-API-for-application-developers" title="Providing an API for application developers"></a>Providing an API for application developers</h4>
            <p>A very important aspect of a sound server is to provide a convenient API for application developers to consume. The following Python script demonstrates how the ALSA API can be used to playback a *.wav file.</p>
            <pre data-language="python"><code>import audioop
import alsaaudio

# Initialize ALSA-Audio-Output
audio_output = alsaaudio.PCM(alsaaudio.PCM_PLAYBACK)
audio_output.setchannels(1) # mono
audio_output.setrate(44100) # 44.1 kHz
audio_output.setformat(alsaaudio.PCM_FORMAT_S32_LE)

with open(&#39;example-audio-file.wav&#39;, &#39;rb&#39;) as wav_file:
    try:
        audio_output.setperiodsize(160)
        data = wav_file.read(320)

        while data:
            # Write read bits of audio file to sound card
            audio_output.write(data)
            data = wav_file.read(320)

    except Exception as exc:
        print(exc)
</code></pre>
            <h3 id="Which-sound-server-is-better-for-me"><a href="#Which-sound-server-is-better-for-me" title="Which sound server is better for me?"></a>Which sound server is better for me?</h3>
            <p>To help you understand which sound server suits your needs the best, I tried to collect some use-cases and rated the corresponding implementations capability to fulfill these use-cases. Note, that the rating might be a very opinionated rating, and you might disagree. Let’s discuss in the comment section below!</p>
            <table>
              <thead>
                <tr>
                  <th>Use Case</th>
                  <th>JACK</th>
                  <th>PulseAudio</th>
                  <th>PipeWire</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Professional Audio Production</td>
                  <td>★★★★★</td>
                  <td>★★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Real-Time Audio Processing</td>
                  <td>★★★★★</td>
                  <td>★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Low-Latency Audio</td>
                  <td>★★★★★</td>
                  <td>★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Live Music Performance</td>
                  <td>★★★★★</td>
                  <td>★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>General Desktop Audio</td>
                  <td>★★</td>
                  <td>★★★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Network Audio Streaming</td>
                  <td>★★★★</td>
                  <td>★★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Bluetooth Audio Devices</td>
                  <td>★★</td>
                  <td>★★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Video Conferencing</td>
                  <td>-</td>
                  <td>-</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Screen Recording</td>
                  <td>-</td>
                  <td>-</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Audio Routing Between Apps</td>
                  <td>★★★★★</td>
                  <td>★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Multi-User Audio Management</td>
                  <td>★★★</td>
                  <td>★★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Security and Sandboxing</td>
                  <td>★★★</td>
                  <td>★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Ease of Configuration</td>
                  <td>★★</td>
                  <td>★★★★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Compatibility with Consumer Apps</td>
                  <td>★</td>
                  <td>★★★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Handling MIDI</td>
                  <td>★★★★★</td>
                  <td>★★★ (requires FluidSynth)</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>High-Performance Gaming Audio</td>
                  <td>★★</td>
                  <td>★★★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Podcasting and Broadcasting</td>
                  <td>★★★★☆</td>
                  <td>★★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Remote Audio Processing</td>
                  <td>★★★★☆</td>
                  <td>★★★</td>
                  <td>★★★★☆</td>
                </tr>
                <tr>
                  <td>Developing Audio Applications</td>
                  <td>★★★★★</td>
                  <td>★★★</td>
                  <td>★★★★★</td>
                </tr>
                <tr>
                  <td>Effect Support (LADSPA, etc. …)</td>
                  <td>★★★★☆</td>
                  <td>★★★</td>
                  <td>★★★★★</td>
                </tr>
              </tbody>
            </table>
            <p>PipeWire is emerging as a versatile and powerful option that integrates the best features of both JACK and PulseAudio, making it suitable for a wide range of use cases from general desktop audio to professional audio and video production. JACK remains the best choice for specialized professional audio tasks requiring the lowest latency and real-time performance. PulseAudio continues to excel in general desktop and consumer audio applications, offering ease of use and broad compatibility.</p>
            <p>The Linux distribution you’re using likely employs either PipeWire or PulseAudio. RHEL-based are on PipeWire already (e.g. Fedora), most Debian-based are still on PulseAudio, but PipeWire is installable. If you’re satisfied with your current audio stack, there’s no need to change it. However, if you seek lower-latency capabilities, consider upgrading to PipeWire. Generally speaking I’d not recommend transitioning to JACK unless you’re already familiar with its configuration and setup. PipeWire will eventually completely replace PulseAudio and JACK, as from an architectural and technical view it is a lot more capable.</p>
          </div></div>
  </body>
</html>
