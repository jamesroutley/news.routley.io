<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp">Original</a>
    <h1>Llama.cpp: Port of Facebook&#39;s LLaMA model in C/C&#43;&#43;, with Apple Silicon support</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Inference of <a href="https://github.com/facebookresearch/llama">Facebook&#39;s LLaMA</a> model in pure C/C++</p>
<p dir="auto"><strong>Hot topics</strong></p>
<ul dir="auto">
<li>Running on Windows: <a data-error-text="Failed to load title" data-id="1620140533" data-permission-text="Title is private" data-url="https://github.com/ggerganov/llama.cpp/issues/22" data-hovercard-type="issue" data-hovercard-url="/ggerganov/llama.cpp/issues/22/hovercard" href="https://github.com/ggerganov/llama.cpp/issues/22">#22</a></li>
<li>Fix Tokenizer / Unicode support: <a data-error-text="Failed to load title" data-id="1619969641" data-permission-text="Title is private" data-url="https://github.com/ggerganov/llama.cpp/issues/11" data-hovercard-type="issue" data-hovercard-url="/ggerganov/llama.cpp/issues/11/hovercard" href="https://github.com/ggerganov/llama.cpp/issues/11">#11</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-description" aria-hidden="true" href="#description"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Description</h2>
<p dir="auto">The main goal is to run the model using 4-bit quantization on a MacBook</p>
<ul dir="auto">
<li>Plain C/C++ implementation without dependencies</li>
<li>Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework</li>
<li>AVX2 support for x86 architectures</li>
<li>Mixed F16 / F32 precision</li>
<li>4-bit quantization support</li>
<li>Runs on the CPU</li>
</ul>
<p dir="auto">This was <a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022" data-hovercard-type="issue" data-hovercard-url="/ggerganov/llama.cpp/issues/33/hovercard">hacked in an evening</a> - I have no idea if it works correctly.
Please do not make conclusions about the models based on the results from this implementation.
For all I know, it can be completely wrong. This project is for educational purposes and is not going to be maintained properly.
New features will probably be added mostly through community contributions, if any.</p>
<p dir="auto">Supported platformst:</p>
<ul>
<li> Mac OS</li>
<li> Linux</li>
<li> Windows (soon)</li>
</ul>
<hr/>
<p dir="auto">Here is a typical run using LLaMA-7B:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make -j &amp;&amp; ./main -m ./models/7B/ggml-model-q4_0.bin -p &#34;Building a website can be done in 10 simple steps:&#34; -t 8 -n 512
I llama.cpp build info:
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.0 (clang-1400.0.29.202)
I CXX:      Apple clang version 14.0.0 (clang-1400.0.29.202)

make: Nothing to be done for `default&#39;.
main: seed = 1678486056
llama_model_load: loading model from &#39;./models/7B/ggml-model-q4_0.bin&#39; - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 512
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 2
llama_model_load: n_ff    = 11008
llama_model_load: ggml ctx size = 4529.34 MB
llama_model_load: memory_size =   512.00 MB, n_mem = 16384
llama_model_load: .................................... done
llama_model_load: model size =  4017.27 MB / num tensors = 291

main: prompt: &#39;Building a website can be done in 10 simple steps:&#39;
main: number of tokens in prompt = 15
     1 -&gt; &#39;&#39;
  8893 -&gt; &#39;Build&#39;
   292 -&gt; &#39;ing&#39;
   263 -&gt; &#39; a&#39;
  4700 -&gt; &#39; website&#39;
   508 -&gt; &#39; can&#39;
   367 -&gt; &#39; be&#39;
  2309 -&gt; &#39; done&#39;
   297 -&gt; &#39; in&#39;
 29871 -&gt; &#39; &#39;
 29896 -&gt; &#39;1&#39;
 29900 -&gt; &#39;0&#39;
  2560 -&gt; &#39; simple&#39;
  6576 -&gt; &#39; steps&#39;
 29901 -&gt; &#39;:&#39;

sampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000


Building a website can be done in 10 simple steps:
1) Select a domain name and web hosting plan
2) Complete a sitemap
3) List your products
4) Write product descriptions
5) Create a user account
6) Build the template
7) Start building the website
8) Advertise the website
9) Provide email support
10) Submit the website to search engines
A website is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.
The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user&#39;s browser.
The web pages are stored in a web server. The web server is also called a host. When the website is accessed, it is retrieved from the server and displayed on the user&#39;s computer.
A website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.
A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user&#39;s screen.
A website can also be viewed on different devices such as desktops, tablets and smartphones.
Hence, to have a website displayed on a browser, the website must be hosted.
A domain name is an address of a website. It is the name of the website.
The website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.
A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user’s screen.
A website can also be viewed on different devices such as desktops, tablets and smartphones. Hence, to have a website displayed on a browser, the website must be hosted.
A domain name is an address of a website. It is the name of the website.
A website is an address of a website. It is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.
The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user’s browser.
A website is known as a website when it is hosted

main: mem per token = 14434244 bytes
main:     load time =  1332.48 ms
main:   sample time =  1081.40 ms
main:  predict time = 31378.77 ms / 61.41 ms per token
main:    total time = 34036.74 ms"><pre><span>make</span> -<span>j</span> &amp;&amp; ./<span>main</span> -<span>m</span> ./<span>models</span>/<span>7</span><span>B</span>/<span>ggml</span>-<span>model</span>-<span>q4_0</span>.<span>bin</span> -<span>p</span> <span>&#34;Building a website can be done in 10 simple steps:&#34;</span> -<span>t</span> <span>8</span> -<span>n</span> <span>512</span>
<span>I</span> <span>llama</span>.<span>cpp</span> <span>build</span> <span>info</span>:
<span>I</span> <span>UNAME_S</span>:  <span>Darwin</span>
<span>I</span> <span>UNAME_P</span>:  <span>arm</span>
<span>I</span> <span>UNAME_M</span>:  <span>arm64</span>
<span>I</span> <span>CFLAGS</span>:   -<span>I</span>.              -<span>O3</span> -<span>DNDEBUG</span> -<span>std</span>=<span>c11</span>   -<span>fPIC</span> -<span>pthread</span> -<span>DGGML_USE_ACCELERATE</span>
<span>I</span> <span>CXXFLAGS</span>: -<span>I</span>. -<span>I</span>./<span>examples</span> -<span>O3</span> -<span>DNDEBUG</span> -<span>std</span>=<span>c</span>++<span>11</span> -<span>fPIC</span> -<span>pthread</span>
<span>I</span> <span>LDFLAGS</span>:   -<span>framework</span> <span>Accelerate</span>
<span>I</span> <span>CC</span>:       <span>Apple</span> <span>clang</span> <span>version</span> <span>14.0</span><span>.0</span> (<span>clang</span>-<span>1400.0</span>.<span>29.202</span>)
<span>I</span> <span>CXX</span>:      <span>Apple</span> <span>clang</span> <span>version</span> <span>14.0</span><span>.0</span> (<span>clang</span>-<span>1400.0</span>.<span>29.202</span>)

<span>make</span>: <span>Nothing</span> <span>to</span> <span>be</span> <span>done</span> <span>for</span> `<span>default</span>&#39;.
<span>main</span>: <span>seed</span> = <span>1678486056</span>
<span>llama_model_load</span>: <span>loading</span> <span>model</span> <span>from</span> <span>&#39;./models/7B/ggml-model-q4_0.bin&#39;</span> - <span>please</span> <span>wait</span> ...
<span>llama_model_load</span>: <span>n_vocab</span> = <span>32000</span>
<span>llama_model_load</span>: <span>n_ctx</span>   = <span>512</span>
<span>llama_model_load</span>: <span>n_embd</span>  = <span>4096</span>
<span>llama_model_load</span>: <span>n_mult</span>  = <span>256</span>
<span>llama_model_load</span>: <span>n_head</span>  = <span>32</span>
<span>llama_model_load</span>: <span>n_layer</span> = <span>32</span>
<span>llama_model_load</span>: <span>n_rot</span>   = <span>128</span>
<span>llama_model_load</span>: <span>f16</span>     = <span>2</span>
<span>llama_model_load</span>: <span>n_ff</span>    = <span>11008</span>
<span>llama_model_load</span>: <span>ggml</span> <span>ctx</span> <span>size</span> = <span>4529.34</span> <span>MB</span>
<span>llama_model_load</span>: <span>memory_size</span> =   <span>512.00</span> <span>MB</span>, <span>n_mem</span> = <span>16384</span>
<span>llama_model_load</span>: .................................... <span>done</span>
<span>llama_model_load</span>: <span>model</span> <span>size</span> =  <span>4017.27</span> <span>MB</span> / <span>num</span> <span>tensors</span> = <span>291</span>

<span>main</span>: <span>prompt</span>: <span>&#39;Building a website can be done in 10 simple steps:&#39;</span>
<span>main</span>: <span>number</span> <span>of</span> <span>tokens</span> <span>in</span> <span>prompt</span> = <span>15</span>
     <span>1</span> -&gt; &#39;&#39;
  <span>8893</span> -&gt; <span>&#39;Build&#39;</span>
   <span>292</span> -&gt; <span>&#39;ing&#39;</span>
   <span>263</span> -&gt; <span>&#39; a&#39;</span>
  <span>4700</span> -&gt; <span>&#39; website&#39;</span>
   <span>508</span> -&gt; <span>&#39; can&#39;</span>
   <span>367</span> -&gt; <span>&#39; be&#39;</span>
  <span>2309</span> -&gt; <span>&#39; done&#39;</span>
   <span>297</span> -&gt; <span>&#39; in&#39;</span>
 <span>29871</span> -&gt; <span>&#39; &#39;</span>
 <span>29896</span> -&gt; <span>&#39;1&#39;</span>
 <span>29900</span> -&gt; <span>&#39;0&#39;</span>
  <span>2560</span> -&gt; <span>&#39; simple&#39;</span>
  <span>6576</span> -&gt; <span>&#39; steps&#39;</span>
 <span>29901</span> -&gt; <span>&#39;:&#39;</span>

<span>sampling</span> <span>parameters</span>: <span>temp</span> = <span>0.800000</span>, <span>top_k</span> = <span>40</span>, <span>top_p</span> = <span>0.950000</span>


<span>Building</span> <span>a</span> <span>website</span> <span>can</span> <span>be</span> <span>done</span> <span>in</span> <span>10</span> <span>simple</span> <span>steps</span>:
<span>1</span>) <span>Select</span> <span>a</span> <span>domain</span> <span>name</span> <span>and</span> <span>web</span> <span>hosting</span> <span>plan</span>
<span>2</span>) <span>Complete</span> <span>a</span> <span>sitemap</span>
<span>3</span>) <span>List</span> <span>your</span> <span>products</span>
<span>4</span>) <span>Write</span> <span>product</span> <span>descriptions</span>
<span>5</span>) <span>Create</span> <span>a</span> <span>user</span> <span>account</span>
<span>6</span>) <span>Build</span> <span>the</span> <span>template</span>
<span>7</span>) <span>Start</span> <span>building</span> <span>the</span> <span>website</span>
<span>8</span>) <span>Advertise</span> <span>the</span> <span>website</span>
<span>9</span>) <span>Provide</span> <span>email</span> <span>support</span>
<span>10</span>) <span>Submit</span> <span>the</span> <span>website</span> <span>to</span> <span>search</span> <span>engines</span>
<span>A</span> <span>website</span> <span>is</span> <span>a</span> <span>collection</span> <span>of</span> <span>web</span> <span>pages</span> <span>that</span> <span>are</span> <span>formatted</span> <span>with</span> <span>HTML</span>. <span>HTML</span> <span>is</span> <span>the</span> <span>code</span> <span>that</span> <span>defines</span> <span>what</span> <span>the</span> <span>website</span> <span>looks</span> <span>like</span> <span>and</span> <span>how</span> <span>it</span> <span>behaves</span>.
<span>The</span> <span>HTML</span> <span>code</span> <span>is</span> <span>formatted</span> <span>into</span> <span>a</span> <span>template</span> <span>or</span> <span>a</span> <span>format</span>. <span>Once</span> <span>this</span> <span>is</span> <span>done</span>, <span>it</span> <span>is</span> <span>displayed</span> <span>on</span> <span>the</span> <span>user</span>&#39;s browser.
<span>The</span> <span>web</span> <span>pages</span> <span>are</span> <span>stored</span> <span>in</span> <span>a</span> <span>web</span> <span>server</span>. <span>The</span> <span>web</span> <span>server</span> <span>is</span> <span>also</span> <span>called</span> <span>a</span> <span>host</span>. <span>When</span> <span>the</span> <span>website</span> <span>is</span> <span>accessed</span>, <span>it</span> <span>is</span> <span>retrieved</span> <span>from</span> <span>the</span> <span>server</span> <span>and</span> <span>displayed</span> <span>on</span> <span>the</span> <span>user</span>&#39;s computer.
<span>A</span> <span>website</span> <span>is</span> <span>known</span> <span>as</span> <span>a</span> <span>website</span> <span>when</span> <span>it</span> <span>is</span> <span>hosted</span>. <span>This</span> <span>means</span> <span>that</span> <span>it</span> <span>is</span> <span>displayed</span> <span>on</span> <span>a</span> <span>host</span>. <span>The</span> <span>host</span> <span>is</span> <span>usually</span> <span>a</span> <span>web</span> <span>server</span>.
<span>A</span> <span>website</span> <span>can</span> <span>be</span> <span>displayed</span> <span>on</span> <span>different</span> <span>browsers</span>. <span>The</span> <span>browsers</span> <span>are</span> <span>basically</span> <span>the</span> <span>software</span> <span>that</span> <span>renders</span> <span>the</span> <span>website</span> <span>on</span> <span>the</span> <span>user</span>&#39;s screen.
<span>A</span> <span>website</span> <span>can</span> <span>also</span> <span>be</span> <span>viewed</span> <span>on</span> <span>different</span> <span>devices</span> <span>such</span> <span>as</span> <span>desktops</span>, <span>tablets</span> <span>and</span> <span>smartphones</span>.
<span>Hence</span>, <span>to</span> <span>have</span> <span>a</span> <span>website</span> <span>displayed</span> <span>on</span> <span>a</span> <span>browser</span>, <span>the</span> <span>website</span> <span>must</span> <span>be</span> <span>hosted</span>.
<span>A</span> <span>domain</span> <span>name</span> <span>is</span> <span>an</span> <span>address</span> <span>of</span> <span>a</span> <span>website</span>. <span>It</span> <span>is</span> <span>the</span> <span>name</span> <span>of</span> <span>the</span> <span>website</span>.
<span>The</span> <span>website</span> <span>is</span> <span>known</span> <span>as</span> <span>a</span> <span>website</span> <span>when</span> <span>it</span> <span>is</span> <span>hosted</span>. <span>This</span> <span>means</span> <span>that</span> <span>it</span> <span>is</span> <span>displayed</span> <span>on</span> <span>a</span> <span>host</span>. <span>The</span> <span>host</span> <span>is</span> <span>usually</span> <span>a</span> <span>web</span> <span>server</span>.
<span>A</span> <span>website</span> <span>can</span> <span>be</span> <span>displayed</span> <span>on</span> <span>different</span> <span>browsers</span>. <span>The</span> <span>browsers</span> <span>are</span> <span>basically</span> <span>the</span> <span>software</span> <span>that</span> <span>renders</span> <span>the</span> <span>website</span> <span>on</span> <span>the</span> <span>user</span>’<span>s</span> <span>screen</span>.
<span>A</span> <span>website</span> <span>can</span> <span>also</span> <span>be</span> <span>viewed</span> <span>on</span> <span>different</span> <span>devices</span> <span>such</span> <span>as</span> <span>desktops</span>, <span>tablets</span> <span>and</span> <span>smartphones</span>. <span>Hence</span>, <span>to</span> <span>have</span> <span>a</span> <span>website</span> <span>displayed</span> <span>on</span> <span>a</span> <span>browser</span>, <span>the</span> <span>website</span> <span>must</span> <span>be</span> <span>hosted</span>.
<span>A</span> <span>domain</span> <span>name</span> <span>is</span> <span>an</span> <span>address</span> <span>of</span> <span>a</span> <span>website</span>. <span>It</span> <span>is</span> <span>the</span> <span>name</span> <span>of</span> <span>the</span> <span>website</span>.
<span>A</span> <span>website</span> <span>is</span> <span>an</span> <span>address</span> <span>of</span> <span>a</span> <span>website</span>. <span>It</span> <span>is</span> <span>a</span> <span>collection</span> <span>of</span> <span>web</span> <span>pages</span> <span>that</span> <span>are</span> <span>formatted</span> <span>with</span> <span>HTML</span>. <span>HTML</span> <span>is</span> <span>the</span> <span>code</span> <span>that</span> <span>defines</span> <span>what</span> <span>the</span> <span>website</span> <span>looks</span> <span>like</span> <span>and</span> <span>how</span> <span>it</span> <span>behaves</span>.
<span>The</span> <span>HTML</span> <span>code</span> <span>is</span> <span>formatted</span> <span>into</span> <span>a</span> <span>template</span> <span>or</span> <span>a</span> <span>format</span>. <span>Once</span> <span>this</span> <span>is</span> <span>done</span>, <span>it</span> <span>is</span> <span>displayed</span> <span>on</span> <span>the</span> <span>user</span>’<span>s</span> <span>browser</span>.
<span>A</span> <span>website</span> <span>is</span> <span>known</span> <span>as</span> <span>a</span> <span>website</span> <span>when</span> <span>it</span> <span>is</span> <span>hosted</span>

<span>main</span>: <span>mem</span> <span>per</span> <span>token</span> = <span>14434244</span> <span>bytes</span>
<span>main</span>:     <span>load</span> <span>time</span> =  <span>1332.48</span> <span>ms</span>
<span>main</span>:   <span>sample</span> <span>time</span> =  <span>1081.40</span> <span>ms</span>
<span>main</span>:  <span>predict</span> <span>time</span> = <span>31378.77</span> <span>ms</span> / <span>61.41</span> <span>ms</span> <span>per</span> <span>token</span>
<span>main</span>:    <span>total</span> <span>time</span> = <span>34036.74</span> <span>ms</span></pre></div>
<p dir="auto">And here is another demo of running both LLaMA-7B and <a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a> on a single M1 Pro MacBook:</p>
<video src="https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4" data-canonical-src="https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4" controls="controls" muted="muted">

  </video>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Here are the step for the LLaMA-7B model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# build this repo
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# obtain the original LLaMA model weights and place them in ./models
ls ./models
65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model

# install Python dependencies
# preferred versions: python 3.10 (not 3.11), torch 1.13.1+
python3 -m pip install torch numpy sentencepiece

# convert the 7B model to ggml FP16 format
python3 convert-pth-to-ggml.py models/7B/ 1

# quantize the model to 4-bits
./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2

# run the inference
./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 128"><pre><span><span>#</span> build this repo</span>
git clone https://github.com/ggerganov/llama.cpp
<span>cd</span> llama.cpp
make

<span><span>#</span> obtain the original LLaMA model weights and place them in ./models</span>
ls ./models
65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model

<span><span>#</span> install Python dependencies</span>
<span><span>#</span> preferred versions: python 3.10 (not 3.11), torch 1.13.1+</span>
python3 -m pip install torch numpy sentencepiece

<span><span>#</span> convert the 7B model to ggml FP16 format</span>
python3 convert-pth-to-ggml.py models/7B/ 1

<span><span>#</span> quantize the model to 4-bits</span>
./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2

<span><span>#</span> run the inference</span>
./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 128</pre></div>
<p dir="auto">For the bigger models, there are a few extra quantization steps. For example, for LLaMA-13B, converting to FP16 format
will create 2 ggml files, instead of one:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ggml-model-f16.bin
ggml-model-f16.bin.1"><pre>ggml-model-f16.bin
ggml-model-f16.bin.1</pre></div>
<p dir="auto">You need to quantize each of them separately like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./quantize ./models/13B/ggml-model-f16.bin   ./models/13B/ggml-model-q4_0.bin 2
./quantize ./models/13B/ggml-model-f16.bin.1 ./models/13B/ggml-model-q4_0.bin.1 2"><pre>./quantize ./models/13B/ggml-model-f16.bin   ./models/13B/ggml-model-q4_0.bin 2
./quantize ./models/13B/ggml-model-f16.bin.1 ./models/13B/ggml-model-q4_0.bin.1 2</pre></div>
<p dir="auto">Everything else is the same. Simply run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./main -m ./models/13B/ggml-model-q4_0.bin -t 8 -n 128"><pre>./main -m ./models/13B/ggml-model-q4_0.bin -t 8 -n 128</pre></div>
<p dir="auto">The number of files generated for each model is as follows:</p>
<div data-snippet-clipboard-copy-content="7B  -&gt; 1 file
13B -&gt; 2 files
30B -&gt; 4 files
65B -&gt; 8 files"><pre><code>7B  -&gt; 1 file
13B -&gt; 2 files
30B -&gt; 4 files
65B -&gt; 8 files
</code></pre></div>
<p dir="auto">When running the larger models, make sure you have enough disk space to store all the intermediate files.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-limitations" aria-hidden="true" href="#limitations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Limitations</h2>
<ul dir="auto">
<li>Not sure if my tokenizer is correct. There are a few places where we might have a mistake:
<ul dir="auto">
<li><div>
  
  <div itemprop="text">
    <table data-tab-size="8" data-paste-markdown-skip="">

        <tbody><tr>
          <td id="L79" data-line-number="79"></td>
          <td id="LC79"> <span># Is this correct??</span> </td>
        </tr>

        <tr>
          <td id="L80" data-line-number="80"></td>
          <td id="LC80"> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>32000</span>): </td>
        </tr>

        <tr>
          <td id="L81" data-line-number="81"></td>
          <td id="LC81">     <span># TODO: this is probably wrong - not sure how this tokenizer works</span> </td>
        </tr>

        <tr>
          <td id="L82" data-line-number="82"></td>
          <td id="LC82">     <span>text</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>([<span>29889</span>, <span>i</span>]).<span>encode</span>(<span>&#39;utf-8&#39;</span>) </td>
        </tr>

        <tr>
          <td id="L83" data-line-number="83"></td>
          <td id="LC83">     <span># remove the first byte (it&#39;s always &#39;.&#39;)</span> </td>
        </tr>

        <tr>
          <td id="L84" data-line-number="84"></td>
          <td id="LC84">     <span>text</span> <span>=</span> <span>text</span>[<span>1</span>:] </td>
        </tr>

        <tr>
          <td id="L85" data-line-number="85"></td>
          <td id="LC85">     <span>fout</span>.<span>write</span>(<span>struct</span>.<span>pack</span>(<span>&#34;i&#34;</span>, <span>len</span>(<span>text</span>))) </td>
        </tr>

        <tr>
          <td id="L86" data-line-number="86"></td>
          <td id="LC86">     <span>fout</span>.<span>write</span>(<span>text</span>) </td>
        </tr>

        <tr>
          <td id="L87" data-line-number="87"></td>
          <td id="LC87">  </td>
        </tr>
    </tbody></table>
  </div>
</div>
</li>
<li><div>
  
  <div itemprop="text">
    <table data-tab-size="8" data-paste-markdown-skip="">

        <tbody><tr>
          <td id="L65" data-line-number="65"></td>
          <td id="LC65">  </td>
        </tr>

        <tr>
          <td id="L66" data-line-number="66"></td>
          <td id="LC66"> <span><span>//</span> TODO: this is probably wrong, but I cannot figure out how this tokenizer works ..</span> </td>
        </tr>

        <tr>
          <td id="L67" data-line-number="67"></td>
          <td id="LC67"> <span><span>//</span> ref: https://github.com/google/sentencepiece</span> </td>
        </tr>

        <tr>
          <td id="L68" data-line-number="68"></td>
          <td id="LC68"> std::vector&lt;gpt_vocab::id&gt; <span>llama_tokenize</span>(<span>const</span> gpt_vocab &amp; vocab, <span>const</span> std::string &amp; text, <span>bool</span> bos); </td>
        </tr>

        <tr>
          <td id="L69" data-line-number="69"></td>
          <td id="LC69">  </td>
        </tr>
    </tbody></table>
  </div>
</div>

In general, it seems to work, but I think it fails for unicode character support. Hopefully, someone can help with that</li>
</ul>
</li>
<li>I don&#39;t know yet how much the quantization affects the quality of the generated text</li>
<li>Probably the token sampling can be improved</li>
<li>The Accelerate framework is actually currently unused since I found that for tensor shapes typical for the Decoder,
there is no benefit compared to the ARM_NEON intrinsics implementation. Of course, it&#39;s possible that I simlpy don&#39;t
know how to utilize it properly. But in any case, you can even disable it with <code>LLAMA_NO_ACCELERATE=1 make</code> and the
performance will be the same, since no BLAS calls are invoked by the current implementation</li>
</ul>
</article>
          </div></div>
  </body>
</html>
