<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/prompt-engine">Original</a>
    <h1>Prompt Engine â€“ Microsoft&#39;s prompt engineering library</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repo contains an NPM utility library for creating and maintaining prompts for Large Language Models (LLMs).</p>
<h2 dir="auto"><a id="user-content-background" aria-hidden="true" href="#background"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Background</h2>
<p dir="auto">LLMs like GPT-3 and Codex have continued to push the bounds of what AI is capable of - they can capably generate language and code, but are also capable of emergent behavior like question answering, summarization, classification and dialog. One of the best techniques for enabling specific behavior out of LLMs is called prompt engineering - crafting inputs that coax the model to produce certain kinds of outputs. Few-shot prompting is the discipline of giving examples of inputs and outputs, such that the model has a reference for the type of output you&#39;re looking for.</p>
<p dir="auto">Prompt engineering can be as simple as formatting a question and passing it to the model, but it can also get quite complex - requiring substantial code to manipulate and update strings. This library aims to make that easier. It also aims to codify patterns and practices around prompt engineering.</p>
<p dir="auto">See <a href="https://microsoft.github.io/prompt-engineering/" rel="nofollow">How to get Codex to produce the code you want</a> article for an example of the prompt engineering patterns this library codifies.</p>
<h2 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p dir="auto"><code>npm install prompt-engine</code></p>
<h2 dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p dir="auto">The library currently supports a generic <code>PromptEngine</code>, a <code>CodeEngine</code> and a <code>ChatEngine</code>. All three facilitate a pattern of prompt engineering where the prompt is composed of a description, examples of inputs and outputs and an ongoing &#34;dialog&#34; representing the ongoing input/output pairs as the user and model communicate. The dialog ensures that the model (which is stateless) has the context about what&#39;s happened in the conversation so far.</p>
<p dir="auto">See architecture diagram representation:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/17247257/178334939-65e0e3ce-39b3-4abc-a889-7f2c0fb75f60.png"><img src="https://user-images.githubusercontent.com/17247257/178334939-65e0e3ce-39b3-4abc-a889-7f2c0fb75f60.png" width="500"/></a></p>
<h3 dir="auto"><a id="user-content-code-engine" aria-hidden="true" href="#code-engine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code Engine</h3>
<p dir="auto">Code Engine creates prompts for Natural Language to Code scenarios. See TypeScript Syntax for importing <code>CodeEngine</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { CodeEngine } from &#34;prompt-engine&#34;;"><pre><span>import</span> <span>{</span> <span>CodeEngine</span> <span>}</span> <span>from</span> <span>&#34;prompt-engine&#34;</span><span>;</span></pre></div>
<p dir="auto">NL-&gt;Code prompts should generally have a description, which should give context about the programming language the model should generate and libraries it should be using. The description should also give information about the task at hand:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const description =
  &#34;Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.&#34;;"><pre><span>const</span> <span>description</span> <span>=</span>
  <span>&#34;Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.&#34;</span><span>;</span></pre></div>
<p dir="auto">NL-&gt;Code prompts should also have examples of NL-&gt;Code interactions, exemplifying the kind of code you expect the model to produce. In this case, the inputs are math queries (e.g. &#34;what is 2 + 2?&#34;) and code that console logs the result of the query.</p>
<div dir="auto" data-snippet-clipboard-copy-content="const examples = [
  { input: &#34;what&#39;s 10 plus 18&#34;, response: &#34;console.log(10 + 18)&#34; },
  { input: &#34;what&#39;s 10 times 18&#34;, response: &#34;console.log(10 * 18)&#34; },
];"><pre><span>const</span> <span>examples</span> <span>=</span> <span>[</span>
  <span>{</span> <span>input</span>: <span>&#34;what&#39;s 10 plus 18&#34;</span><span>,</span> <span>response</span>: <span>&#34;console.log(10 + 18)&#34;</span> <span>}</span><span>,</span>
  <span>{</span> <span>input</span>: <span>&#34;what&#39;s 10 times 18&#34;</span><span>,</span> <span>response</span>: <span>&#34;console.log(10 * 18)&#34;</span> <span>}</span><span>,</span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">By default, <code>CodeEngine</code> uses JavaScript as the programming language, but you can create prompts for different languages by passing a different <code>CodePromptConfig</code> into the constructor. If, for example, we wanted to produce Python prompts, we could have passed <code>CodeEngine</code> a <code>pythonConfig</code> specifying the comment operator it should be using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const pythonConfig = {
  commentOperator: &#34;#&#34;,
}
const codeEngine = new CodeEngine(description, examples, flowResetText, pythonConfig);
"><pre><span>const</span> <span>pythonConfig</span> <span>=</span> <span>{</span>
  <span>commentOperator</span>: <span>&#34;#&#34;</span><span>,</span>
<span>}</span>
<span>const</span> <span>codeEngine</span> <span>=</span> <span>new</span> <span>CodeEngine</span><span>(</span><span>description</span><span>,</span> <span>examples</span><span>,</span> <span>flowResetText</span><span>,</span> <span>pythonConfig</span><span>)</span><span>;</span></pre></div>
<p dir="auto">With our description and our examples, we can go ahead and create our <code>CodeEngine</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const codeEngine = new CodeEngine(description, examples);"><pre><span>const</span> <span>codeEngine</span> <span>=</span> <span>new</span> <span>CodeEngine</span><span>(</span><span>description</span><span>,</span> <span>examples</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Now that we have our <code>CodeEngine</code>, we can use it to create prompts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const query = &#34;What&#39;s 1018 times the ninth power of four?&#34;;
const prompt = codeEngine.buildPrompt(query);"><pre><span>const</span> <span>query</span> <span>=</span> <span>&#34;What&#39;s 1018 times the ninth power of four?&#34;</span><span>;</span>
<span>const</span> <span>prompt</span> <span>=</span> <span>codeEngine</span><span>.</span><span>buildPrompt</span><span>(</span><span>query</span><span>)</span><span>;</span></pre></div>
<p dir="auto">The resulting prompt will be a string with the description, examples and the latest query formatted with comment operators and line breaks:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. */

/* what&#39;s 10 plus 18 */
console.log(10 + 18);

/* what&#39;s 10 times 18 */
console.log(10 * 18);

/* What&#39;s 1018 times the ninth power of four? */"><pre><span>/* Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. */</span>

<span>/* what&#39;s 10 plus 18 */</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>10</span> <span>+</span> <span>18</span><span>)</span><span>;</span>

<span>/* what&#39;s 10 times 18 */</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>10</span> <span>*</span> <span>18</span><span>)</span><span>;</span>

<span>/* What&#39;s 1018 times the ninth power of four? */</span></pre></div>
<p dir="auto">Given the context, a capable code generation model can take the above prompt and guess the next line: <code>console.log(1018 * Math.pow(4, 9));</code>.</p>
<p dir="auto">For multi-turn scenarios, where past conversations influences the next turn, Code Engine enables us to persist interactions in a prompt:</p>
<div dir="auto" data-snippet-clipboard-copy-content="...
// Assumes existence of code generation model
let code = model.generateCode(prompt);

// Adds interaction
codeEngine.addInteraction(query, code);"><pre>...
<span>// Assumes existence of code generation model</span>
<span>let</span> <span>code</span> <span>=</span> <span>model</span><span>.</span><span>generateCode</span><span>(</span><span>prompt</span><span>)</span><span>;</span>

<span>// Adds interaction</span>
<span>codeEngine</span><span>.</span><span>addInteraction</span><span>(</span><span>query</span><span>,</span> <span>code</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Now new prompts will include the latest NL-&gt;Code interaction:</p>
<div dir="auto" data-snippet-clipboard-copy-content="codeEngine.buildPrompt(&#34;How about the 8th power?&#34;);"><pre><span>codeEngine</span><span>.</span><span>buildPrompt</span><span>(</span><span>&#34;How about the 8th power?&#34;</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Produces a prompt identical to the one above, but with the NL-&gt;Code dialog history:</p>
<div dir="auto" data-snippet-clipboard-copy-content="...
/* What&#39;s 1018 times the ninth power of four? */
console.log(1018 * Math.pow(4, 9));

/* How about the 8th power? */"><pre>...
<span>/* What&#39;s 1018 times the ninth power of four? */</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>1018</span> <span>*</span> <span>Math</span><span>.</span><span>pow</span><span>(</span><span>4</span><span>,</span> <span>9</span><span>)</span><span>)</span><span>;</span>

<span>/* How about the 8th power? */</span></pre></div>
<p dir="auto">With this context, the code generation model has the dialog context needed to understand what we mean by the query. In this case, the model would correctly generate <code>console.log(1018 * Math.pow(4, 8));</code>.</p>
<h3 dir="auto"><a id="user-content-chat-engine" aria-hidden="true" href="#chat-engine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Chat Engine</h3>
<p dir="auto">Just like Code Engine, Chat Engine creates prompts with descriptions and examples. The difference is that Chat Engine creates prompts for dialog scenarios, where both the user and the model use natural language. The <code>ChatEngine</code> constructor takes an optional <code>chatConfig</code> argument, which allows you to define the name of a user and chatbot in a multi-turn dialog:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const chatEngineConfig = {
  user: &#34;Ryan&#34;,
  bot: &#34;Gordon&#34;
};"><pre><span>const</span> <span>chatEngineConfig</span> <span>=</span> <span>{</span>
  <span>user</span>: <span>&#34;Ryan&#34;</span><span>,</span>
  <span>bot</span>: <span>&#34;Gordon&#34;</span>
<span>}</span><span>;</span></pre></div>
<p dir="auto">Chat prompts also benefit from a description that gives context. This description helps the model determine how the bot should respond.</p>
<div dir="auto" data-snippet-clipboard-copy-content="const description = &#34;A conversation with Gordon the Anxious Robot. Gordon tends to reply nervously and asks a lot of follow-up questions.&#34;;"><pre><span>const</span> <span>description</span> <span>=</span> <span>&#34;A conversation with Gordon the Anxious Robot. Gordon tends to reply nervously and asks a lot of follow-up questions.&#34;</span><span>;</span></pre></div>
<p dir="auto">Similarly, Chat Engine prompts can have examples interactions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const examples = [
  { input: &#34;Who made you?&#34;, response: &#34;I don&#39;t know man! That&#39;s an awfully existential question. How would you answer it?&#34; },
  { input: &#34;Good point - do you at least know what you were made for?&#34;, response: &#34;I&#39;m OK at riveting, but that&#39;s not how I should answer a meaning of life question is it?&#34;}
];"><pre><span>const</span> <span>examples</span> <span>=</span> <span>[</span>
  <span>{</span> <span>input</span>: <span>&#34;Who made you?&#34;</span><span>,</span> <span>response</span>: <span>&#34;I don&#39;t know man! That&#39;s an awfully existential question. How would you answer it?&#34;</span> <span>}</span><span>,</span>
  <span>{</span> <span>input</span>: <span>&#34;Good point - do you at least know what you were made for?&#34;</span><span>,</span> <span>response</span>: <span>&#34;I&#39;m OK at riveting, but that&#39;s not how I should answer a meaning of life question is it?&#34;</span><span>}</span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">These examples help set the tone of the bot, in this case Gordon the Anxious Robot. Now we can create our <code>ChatEngine</code> and use it to create prompts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const chatEngine = new ChatEngine(description, examples, flowResetText, chatEngineConfig);
const userQuery = &#34;What are you made of?&#34;;
const prompt = chatEngine.buildPrompt(userQuery);"><pre><span>const</span> <span>chatEngine</span> <span>=</span> <span>new</span> <span>ChatEngine</span><span>(</span><span>description</span><span>,</span> <span>examples</span><span>,</span> <span>flowResetText</span><span>,</span> <span>chatEngineConfig</span><span>)</span><span>;</span>
<span>const</span> <span>userQuery</span> <span>=</span> <span>&#34;What are you made of?&#34;</span><span>;</span>
<span>const</span> <span>prompt</span> <span>=</span> <span>chatEngine</span><span>.</span><span>buildPrompt</span><span>(</span><span>userQuery</span><span>)</span><span>;</span></pre></div>
<p dir="auto">When passed to a large language model (e.g. GPT-3), the context of the above prompt will help coax a good answer from the model, like &#34;Subatomic particles at some level, but somehow I don&#39;t think that&#39;s what you were asking.&#34;. As with Code Engine, we can persist this answer and continue the dialog such that the model is aware of the conversation context:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chatEngine.addInteraction(userQuery, &#34;Subatomic particles at some level, but somehow I don&#39;t think that&#39;s what you were asking.&#34;);"><pre><span>chatEngine</span><span>.</span><span>addInteraction</span><span>(</span><span>userQuery</span><span>,</span> <span>&#34;Subatomic particles at some level, but somehow I don&#39;t think that&#39;s what you were asking.&#34;</span><span>)</span><span>;</span></pre></div>
<h2 dir="auto"><a id="user-content-managing-prompt-overflow" aria-hidden="true" href="#managing-prompt-overflow"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Managing Prompt Overflow</h2>
<p dir="auto">Prompts for Large Language Models generally have limited size, depending on the language model being used. Given that prompt-engine can persist dialog history, it is possible for dialogs to get so long that the prompt overflows. The Prompt Engine pattern handles this situation by removing the oldest dialog interaction from the prompt, effectively only remembering the most recent interactions.</p>
<p dir="auto">You can specify the maximum tokens allowed in your prompt by passing a <code>maxTokens</code> parameter when constructing the config for any prompt engine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let promptEngine = new PromptEngine(description, examples, flowResetText, {
  modelConfig: { maxTokens: 1000 }
});"><pre><span>let</span> <span>promptEngine</span> <span>=</span> <span>new</span> <span>PromptEngine</span><span>(</span><span>description</span><span>,</span> <span>examples</span><span>,</span> <span>flowResetText</span><span>,</span> <span>{</span>
  <span>modelConfig</span>: <span>{</span> <span>maxTokens</span>: <span>1000</span> <span>}</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<h2 dir="auto"><a id="user-content-available-functions" aria-hidden="true" href="#available-functions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Available Functions</h2>
<p dir="auto">The following are the functions available on the <code>PromptEngine</code> class and those that inherit from it:</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Parameters</th>
<th>Description</th>
<th>Returns</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>buildContext</code></td>
<td>None</td>
<td>Constructs and return the context with parameters provided to the Prompt Engine</td>
<td>Context: string</td>
</tr>
<tr>
<td><code>buildPrompt</code></td>
<td>Prompt: string</td>
<td>Combines the context from <code>buildContext</code> with a query to create a prompt</td>
<td>Prompt: string</td>
</tr>
<tr>
<td><code>buildDialog</code></td>
<td>None</td>
<td>Builds a dialog based on all the past interactions added to the Prompt Engine</td>
<td>Dialog: string</td>
</tr>
<tr>
<td><code>addExample</code></td>
<td>interaction: Interaction(input: string, response: string)</td>
<td>Adds the given example to the examples</td>
<td>None</td>
</tr>
<tr>
<td><code>addInteraction</code></td>
<td>interaction: Interaction(input: string, response: string)</td>
<td>Adds the given interaction to the dialog</td>
<td>None</td>
</tr>
<tr>
<td><code>removeFirstInteraction</code></td>
<td>None</td>
<td>Removes and returns the first interaction in the dialog</td>
<td>Interaction: string</td>
</tr>
<tr>
<td><code>removeLastInteraction</code></td>
<td>None</td>
<td>Removes and returns the last interaction added to the dialog</td>
<td>Interaction: string</td>
</tr>
<tr>
<td><code>resetContext</code></td>
<td>None</td>
<td>Removes all interactions from the dialog, returning the reset context</td>
<td>Context:string</td>
</tr>
</tbody>
</table>
<p dir="auto">For more examples and insights into using the prompt-engine library, have a look at the <a href="https://github.com/microsoft/prompt-engine/tree/main/examples">examples</a> folder</p>
<h2 dir="auto"><a id="user-content-yaml-representation" aria-hidden="true" href="#yaml-representation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YAML Representation</h2>
<p dir="auto">It can be useful to represent prompts as standalone files, versus code. This can allow easy swapping between different prompts, prompt versioning, and other advanced capabiliites. With this in mind, prompt-engine offers a way to represent prompts as YAML and to load that YAML into a prompt-engine class. See <code>examples/yaml-examples</code> for examples of YAML prompts and how they&#39;re loaded into prompt-engine.</p>
<h2 dir="auto"><a id="user-content-contributing" aria-hidden="true" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<h2 dir="auto"><a id="user-content-statement-of-purpose" aria-hidden="true" href="#statement-of-purpose"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Statement of Purpose</h2>
<p dir="auto">This library aims to simplify use of Large Language Models, and to make it easy for developers to take advantage of existing patterns. The package is released in conjunction with the <a href="https://github.com/microsoft/Build2022-AI-examples">Build 2022 AI examples</a>, as the first three use a multi-turn LLM pattern that this library simplifies. This package works independently of any specific LLM - prompt generated by the package should be useable with various language and code generating models.</p>
<h2 dir="auto"><a id="user-content-trademarks" aria-hidden="true" href="#trademarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Trademarks</h2>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft&#39;s Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.</p>
</article>
          </div></div>
  </body>
</html>
