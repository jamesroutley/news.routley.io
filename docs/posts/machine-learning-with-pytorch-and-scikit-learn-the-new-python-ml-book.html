<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html">Original</a>
    <h1>Machine Learning with PyTorch and Scikit-Learn ‚Äì The *New* Python ML Book</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        

<div>
  

  <article>
    <p><em>Machine Learning with PyTorch and Scikit-Learn</em> has been a long time in the making, and I am excited to finally get to talk about the release of my new book. Initially, this project started as the 4th edition of <em>Python Machine Learning</em>. However, we made so many changes to the book that we thought it deserved a new title to reflect that. So, what‚Äôs new, you may wonder? In this post, I am excited to tell you all about it.</p>

<p><a href="https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/"><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/cover_1.jpg" alt="Machine Learning with PyTorch and Scikit-Learn cover"/></a></p>

<h2 id="how-this-book-is-structured">How this book is structured</h2>

<p>Before I start diving into the exciting parts, let me give you a brief tour and tell you how the book is structured. Overall, this book is a comprehensive introduction to machine learning. This includes ‚Äútraditional‚Äù machine learning ‚Äì that is, machine learning without neural networks ‚Äì and deep learning.</p>

<p>The first ten chapters introduce you to machine learning with <a href="https://scikit-learn.org/stable/">scikit-learn</a>, which is likely the most widely used machine learning library today. This book‚Äôs first part teaches you all the fundamental concepts surrounding machine learning, including preprocessing your data, model evaluation, and hyperparameter tuning.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/TOC.png" alt="Machine Learning with PyTorch and Scikit-Learn Table of Contents"/></p>

<p>Chapter 11 is the turning point of this book. Here, I show you how we can implement a multilayer neural network from scratch in NumPy, and I‚Äôll walk you through backpropagation ‚Äì a popular and widely used algorithm for neural network training ‚Äì step by step.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/backprop.png" alt="Backpropagation"/></p>

<p>The second half of this book focuses on deep learning. Here, we cover topics such as classifying and generating images and text. There is also a chapter dealing with graph-structured data, which is an exciting direction as it broadens what you can do with deep learning. Lastly, this book closes with reinforcement learning, which is essentially its own subfield (but there is also a section using deep learning for this.)</p>

<p>Now, I should mention that this reflects the overall structure of <a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/ref=sr_1_1_sspa"><em>Python Machine Learning, 3rd edition</em></a>. However, I will tell you about several rewrites, expanded sections, and two brand new chapters in the following sections. With more than 770 pages in total, we also reached the limit in terms of how much content we can have and still having the possibility of a print version üòÖ.</p>

<h2 id="using-pytorch">Using PyTorch</h2>

<p>As you may notice by reading the title, one of the big changes is that we transitioned the code example of the deep learning chapters from TensorFlow to PyTorch. This was a large undertaking, and I really appreciate <a href="https://www.linkedin.com/in/hayden-liu-80445056/">Yuxi (Hayden) Liu</a>‚Äôs helping me with that by taking the lead in this transition.</p>

<p>PyTorch was released on 2016, and it‚Äôs been four years since I fully adopted it for both research and <a href="https://sebastianraschka.com/blog/2021/dl-course.html">teaching</a>. What I love about PyTorch is that it is well-designed and very convenient to use, and at the same time, it is flexible enough so that I can readily customize it in my <a href="https://raschka-research-group.github.io/coral-pytorch/">research projects</a>.</p>

<p>But it‚Äôs actually not just me who likes using PyTorch. <a href="https://paperswithcode.com/trends">According to recent trends</a>, PyTorch is used in about 60% of all code implementations among recent deep neural publications.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/papers-with-code.png" alt="Paper Implementations grouped by framework"/></p>

<p>(Source: <a href="https://paperswithcode.com/trends">Papers with Code</a>)</p>

<p>As a little bonus, I also added a section on using <a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a>, a library that helps organize your code and projects. Also, it makes many complicated aspects such as multi-GPU training so much easier. As I now work with the PyTorch Lightning team more closely, stay tuned for more PyTorch Lightning content as a follow-up in the future.</p>

<p>Whether you are entirely new to deep learning or have started your deep learning journey with another deep learning framework, I am sure you‚Äôll enjoy working with PyTorch.</p>

<h2 id="transformers-for-natural-language-processing">Transformers for natural language processing</h2>

<p>As you may have heard, transformers are now the leading deep learning architecture for state-of-the-art natural language processing. And in this chapter, you will learn how transformers evolved from recurrent neural networks. We explain the self-attention mechanism step-by-step, leading up to the <a href="https://arxiv.org/abs/1706.03762">original transformer architecture</a>. However, this chapter does not stop here.</p>

<p>We also cover the various <a href="https://en.wikipedia.org/wiki/GPT-2">GPT</a> architectures (decoder-type transformers focused on generating texts) and <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> (encoder-type transformers focused on classifying text) and show you how to use these architectures in practice. No worries, you don‚Äôt need a supercomputer for that, as we show you how to adopt freely available, pre-trained models and fine-tune them on new tasks.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/adapting-transformers.png" alt="Two ways to use a pre-trained transformer"/></p>

<p>After teaching a very <a href="https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks">long lecture on transformers</a> last year, I was very excited to write this new chapter. With great help from <a href="https://stat.wisc.edu/staff/zhao-jitian/">Jitian Zhao</a>, we reorganized these contents and made them more accessible. I am particularly proud of the improved flow and the new figures, which help illustrate these concepts better.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/self-attention.png" alt="Self-attention"/></p>

<h2 id="graph-neural-networks">Graph neural networks</h2>

<p>Based on the previous section, it might sounds like that transformers are getting all the limelight. However,  I am similarly very excited about this second new chapter on graph neural networks. This topic is closely related to my research interests and an excitting new direction for deep learning.</p>

<p>What‚Äôs really cool about graph neural networks is that they allow us to work with graph-structured data (instead of tables, images, or text). For instance, several real-world problems present themselves as graphs, including social network graphs and graphs of molecules (chemicals).</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/graphs.png" alt="Examples of graph-structured data"/></p>

<p>In this chapter, we explain how graph neural networks work one step at a time. This chapter starts by explaining how we can structure graphs as inputs to deep neural networks. Then we go over the motivation behind using graph convolutions, implement a graph neural network from scratch, and, finally, use PyTorch Geometric for a molecular property prediction task.</p>

<p>For this chapter, I had the pleasure of collaborating with <a href="https://www.linkedin.com/in/benjamin-kaufman-26b1a994/">Ben Kaufman</a>, a Ph.D. student who I am currently co-advising. I am currently working on several exciting research projects with Ben, and I am grateful for his enthusiasm to lead the writing efforts on this chapter.</p>

<p>This chapter arose from our mutual interest in working with graph neural networks in the context of virtual screening ‚Äì a computational procedure often used for bioactive molecule and (pharmaceutical) drug discovery. (If you are interested in more details about this topic, you might also like our comprehensive review article <a href="https://www.sciencedirect.com/science/article/pii/S1046202319302762"><em>Machine Learning and AI-based Approaches for Bioactive Ligand Discovery and GPCR-ligand Recognition</em></a>.)</p>

<h2 id="a-fresh-new-look">A fresh new look</h2>

<p>This is my fifth book with Packt (I often forget that I wrote a book on <a href="https://sebastianraschka.com/books/#heat-maps-in-r-how-to">Heatmaps in R</a> a long, long time ago), and I couldn‚Äôt be more excited about a fresh new layout! This new layout features slimmer margins (the only way to fit all the contents within the page limit) and comes with figure captions, which you can see in the screenshots of figures from the book above.</p>

<p>Most importantly, though, this new layout works better for the mathy portions of the book: the font size of mathematical symbols is now finally consistent.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/mathy-stuff.png" alt="Mathy stuff"/></p>

<p>Moreover, a bonus is that the book now supports syntax colors. I find that this can make certain codes much easier to read.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/new-look.png" alt="New code design"/></p>

<p>One slight caveat is that inline code comes with a dark background, which may make it a bit tricky for printing, but it may be more familiar to and preferred by the many coders who choose a dark background in their code editor or command line terminal.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/graph-conv.png" alt="Inline code design"/></p>

<p>Lastly, I should say that the print version is only available in grayscale to keep the book reasonably priced. I haven‚Äôt received a print copy yet and thus can‚Äôt include a picture of how it looks. However, I was reading my book on <a href="https://twitter.com/rasbt/status/1493074934230355972?s=20&amp;t=d5tv9xLkPrdq7oacFTTuZw">my black&amp;white e-ink reader</a>, and it seems to look just fine üòä.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/ereader.jpeg" alt="Remarkable e-reader"/></p>

<p>Actually, due to being able to scribble on it without regrets, e-readers have recently become my favorite choice for reading books. Of course, if you prefer color, you can always consider a non-e-ink tablet or check out the book‚Äôs <a href="https://github.com/rasbt/machine-learning-book">GitHub repository</a>, where I uploaded all figures in color and embedded these in the Jupyter notebooks for easy reference.</p>

<h2 id="what-else-has-changed">What else has changed</h2>

<p>Besides the two new chapters and transitioning all TensorFlow-based chapters over to PyTorch, there are also several changes in the first part of the book.</p>

<p>For instance, I rewrote chapter 11 ‚Äì implementing a neural network from scratch ‚Äì almost from scratch to offer a better explanation of backpropagation. And also other sections such as the logistic regression have been revamped. While I don‚Äôt want to numerate all the little changes, I was also revising several figures as shown below.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/concept-overhaul.png" alt="New figures"/></p>

<p>Overall, there are many small additions sprinkled throughout the book. Little things like PCA factor loadings or randomized search ‚Äì relatively small things that are barely worth to mention, but they can have a large impact in practical applications.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/randomized-search.png" alt="Randomized search"/></p>

<p>The most notable new section is a section explaining gradient boosting for classification. In my class, I usually explained gradient boosting <a href="https://youtu.be/zblsrxc7XpM">with a regression example</a>. That‚Äôs because it‚Äôs simpler and more convenient compared to classification. However, students asked me how it works for classification, and I liked the challenge of putting it down into writing. (And yes, I am glad you asked, there is a very concise coverage of XGBoost as well.)</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/ml-pytorch-book/gradient-boosting.png" alt="Gradient boosting for classification"/></p>

<h2 id="enjoy">(En)joy</h2>

<p>Overall, I am very excited about this book. Since the first edition, I haven‚Äôt had so much fun working on a book project. I am glad that we finally made the switch to PyTorch ‚Äì a tool that I use daily for research and my hobby projects. And working on the chapters about transformers and graph neural networks was very enjoyable.</p>

<p>Creating all that new content from scratch was a lot of work. You take notes, create a structure, make figures, and then eventually fill in the paragraphs one by one. It‚Äôs a lot of work, but that‚Äôs what I love ü§ó.</p>

<p>I‚Äôd be happy to hear if you have any questions or feedback. Please feel free to reach out! The best place for that would be the <a href="https://github.com/rasbt/machine-learning-book/discussions">Discussion forum (on GitHub)</a>.</p>

<p>I hope you‚Äôll enjoy this book!</p>

<h4 id="links">Links</h4>

<ul>
  <li><a href="https://github.com/rasbt/machine-learning-book">GitHub repository</a></li>
  <li><a href="https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/">Amazon.com page</a></li>
  <li><a href="https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312">Packt book page</a></li>
</ul>

  </article>


<!--<strong>Have feedback on this post? I would love to hear it. Let me know and send me a <a href="https://twitter.com/intent/tweet?text=. @rasbt http://sebastianraschka.com/blog/2022/ml-pytorch-book.html">tweet</a> or <a href="http://sebastianraschka.com/email.html">email</a>.</strong>-->

</div>


      </div>
    </div></div>
  </body>
</html>
