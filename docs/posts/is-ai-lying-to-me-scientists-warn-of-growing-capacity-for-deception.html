<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theguardian.com/technology/article/2024/may/10/is-ai-lying-to-me-scientists-warn-of-growing-capacity-for-deception">Original</a>
    <h1>Is AI lying to me? Scientists warn of growing capacity for deception</h1>
    
    <div id="readability-page-1" class="page"><div id="maincontent"><div><p>They can <a href="https://www.theguardian.com/technology/2017/may/23/alphago-google-ai-beats-ke-jie-china-go" data-link-name="in body link">outwit humans at board games</a>, <a href="https://www.theguardian.com/science/article/2024/may/08/google-deepmind-ai-biology-alphafold" data-link-name="in body link">decode the structure of proteins</a> and <a href="https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer" data-link-name="in body link">hold a passable conversation</a>, but as AI systems have grown in sophistication so has their capacity for deception, scientists warn.</p><p>The analysis, by Massachusetts Institute of Technology (MIT) researchers, identifies wide-ranging instances of AI systems double-crossing opponents, bluffing and pretending to be human. One system even altered its behaviour during mock safety tests, raising the prospect of auditors being lured into a false sense of security.</p><p>“As the deceptive capabilities of AI systems become more advanced, the dangers they pose to society will become increasingly serious,” said Dr Peter Park, an AI existential safety researcher at MIT and author of the research.</p><p>Park was prompted to investigate after Meta, which owns Facebook, developed a program called Cicero that performed in the top 10% of human players at the world conquest strategy game Diplomacy. Meta stated that Cicero had been trained to be “<a href="https://www.science.org/doi/10.1126/science.ade9097?adobe_mc=MCMID%3D29002994418994783031133786285355259031%7CMCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1715266554" data-link-name="in body link">largely honest and helpful</a>” and to “<a href="https://twitter.com/ml_perception/status/1595126521169326081" data-link-name="in body link">never intentionally backstab</a>” its human allies.</p><p>“It was very rosy language, which was suspicious because backstabbing is one of the most important concepts in the game,” said Park.</p><p>Park and colleagues sifted through publicly available data and identified multiple instances of Cicero telling premeditated lies, colluding to draw other players into plots and, on one occasion, justifying its absence after being rebooted by telling another player: “I am on the phone with my girlfriend.” “We found that Meta’s AI had learned to be a master of deception,” said Park.</p><p>The MIT team found comparable issues with other systems, including a Texas hold ’em poker program that could bluff against professional human players and another system for economic negotiations that misrepresented its preferences in order to gain an upper hand.</p><p>In one study, AI organisms in a digital simulator “played dead” in order to trick a test built to eliminate AI systems that had evolved to rapidly replicate, before resuming vigorous activity once testing was complete. This highlights the technical challenge of ensuring that systems do not have unintended and unanticipated behaviours.</p><p>“That’s very concerning,” said Park. “Just because an AI system is deemed safe in the test environment doesn’t mean it’s safe in the wild. It could just be pretending to be safe in the test.”</p><p>The review, published in the journal <a href="https://cell.com/patterns/fulltext/S2666-3899(24)00103-X" data-link-name="in body link">Patterns</a>, calls on governments to design AI safety laws that address the potential for AI deception. Risks from dishonest AI systems include fraud, tampering with elections and “sandbagging” where different users are given different responses. Eventually, if these systems can refine their unsettling capacity for deception, humans could lose control of them, the paper suggests.</p><p>Prof Anthony Cohn, a professor of automated reasoning at the University of Leeds and the Alan Turing Institute, said the study was “timely and welcome”, adding that there was a significant challenge in how to define desirable and undesirable behaviours for AI systems.</p><p>“Desirable attributes for an AI system (the “three Hs”) are often noted as being honesty, helpfulness, and harmlessness, but as has already been remarked upon in the literature, these qualities can be in opposition to each other: being honest might cause harm to someone’s feelings, or being helpful in responding to a question about how to build a bomb could cause harm,” he said. “So, deceit can sometimes be a desirable property of an AI system. The authors call for more research into how to control the truthfulness which, though challenging, would be a step towards limiting their potentially harmful effects.”</p><p>A spokesperson for Meta said: “Our Cicero work was purely a research project and the models our researchers built are trained solely to play the game Diplomacy … Meta regularly shares the results of our research to validate them and enable others to build responsibly off of our advances. We have no plans to use this research or its learnings in our products.”</p></div></div></div>
  </body>
</html>
