<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/">Original</a>
    <h1>Turing Machines Are Recurrent Neural Networks (1996)</h1>
    
    <div id="readability-page-1" class="page">
<meta name="description" value=" Turing Machines  are  Recurrent Neural Networks"/>
<meta name="keywords" value="HH1"/>
<meta name="resource-type" value="document"/>
<meta name="distribution" value="global"/>

<hr/>
<SPAN size="2">
Proceedings of STeP&#39;96. Jarmo Alander, Timo Honkela and Matti Jakobsson (eds.),</SPAN>
<hr/>

<center>

<p><strong>
<a name="tex2html2" href="http://www.hut.fi/TKK/Yksikot/Osastot/T/Saatotekniikka/homepage/heikki_h.htm">Heikki Hyötyniemi</a> </strong></p></center>
<h3>Abstract:</h3>
<em>Any algebraically computable function can be expressed as a recurrent
neural network structure consisting of identical computing elements 
(or, equivalently, as a nonlinear discrete-time
system of the form <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img1.gif"/>, where <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img2.gif"/> is a simple `cut&#39; 
function).  A constructive proof is presented in this paper. 
 </em><p>
<em> A PostScript version of this paper is available also at </em>
</p>
<h2><a name="SECTION00011000000000000000">1.1 Neural networks in classification</a></h2>
<p>
Neural networks can be used, for example, for classification
tasks, telling whether an input pattern belongs to a specific class or
not.  It has been known for a long time that one-layer feedforward
networks can be used to classify only patterns that are linearly
separable---the more there are successive layers, the more complex the
distribution of the classes can be.
</p><p>
When <em> feedback</em> is introduced in the network structure, so that
perceptron output values are recycled, the number of
successive layers becomes, in principle, infinite. Is the
computational power enhanced qualitatively? The answer is <em>
yes.</em> For example, it is possible to construct a classifier for telling
whether the input integer is a prime number or not.  It turns out that
the size of the network for this purpose can be finite, even if the
input integer size is not limited, the number of primes that can be
correctly classified being infinite.
</p><p>
In this paper, it is shown that a recurrent network structure consisting
of identical computing elements can be used to accomplish <em> any
(algorithmically) computable function.</em>
</p><h2><a name="SECTION00012000000000000000">1.2 About computability</a></h2>
<p>
According to the basic axiom of computability theory, computable 
functions can be implemented using a <em> Turing machine.</em>
There are various ways to realize a Turing machine.
</p><p>
Define the program language <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img3.gif"/>.  There are four basic operations in
this language:
 </p><pre><tt> 
		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img4.gif"/> 		 No operation: 		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img5.gif"/>
<p>
		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img6.gif"/> 		 Increment: 		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img7.gif"/>
</p><p>
		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img8.gif"/> 		 Decrement: 		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img9.gif"/>
</p><p>
		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img10.gif"/> 		 Conditional branch: 		 <tt> IF <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img11.gif"/> GOTO <b>j</b></tt>.
</p></tt></pre>
 Here, <b>V</b> stands for any variable having positive integer values, and
<b>j</b> stands for any line number.  It can be shown that if a function is
Turing computable, it can be coded using this simple language
(for details, see [<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Computa">1</a>]).

<h2><a name="SECTION00021000000000000000">2.1 Recurrent neural network structure</a></h2>
<p>
The neural network to be studied now consists of <em> perceptrons,</em>
all having identical structure.  The operation of the perceptron number
<b>q</b> can be defined as
 </p><p><a name="eqPerc"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img12.gif"/></p><p> 
 where the perceptron output at the current time instant, denoted by
<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img13.gif"/>, is calculated using the <b>n</b> inputs <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img14.gif"/>, <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img15.gif"/>. The nonlinear function <b>f</b> is now defined (somewhat exceptionally) as
 </p><p><a name="eqF"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img16.gif"/></p><p>
 so that the function simply `cuts off&#39; the negative values. 
The recurrence in the perceptron network means that perceptrons can be
combined in complex ways.  In Fig. <a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#figRecurre">1</a>, the recurrent
structure is shown in a common framework: now, <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img17.gif"/>, and
<b>n</b> is the number of perceptrons.  The connection from perceptron <b>p</b> to
perceptron <b>q</b> is represented by the scalar weigth <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img18.gif"/> in
(<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqPerc">1</a>).
</p><p>
 Given some initial state, the network state is iterated until no more
changes take place. The result can be read in that steady state, or
`fixed point&#39; of the network.
</p><p><a name="329"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img19.gif"/>
</p><h2><a name="SECTION00022000000000000000">2.2 Construction of the network</a></h2>
 <a name="secDef"> </a>
<p>
 It is now shown how the program <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img20.gif"/> can be implemented in a perceptron
network. The network consists of the following nodes (or perceptrons):
 </p>
 A realization of a program in language <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img25.gif"/> consists of the following
changes in the perceptron network:
 <ul><li>
 For each variable <b>V</b> in the program, 
augment the network with the following link: 
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img26.gif"/></p></li><li>
 If there is no operation (<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img27.gif"/>) on row <b>i</b> of the program
code then augment the network with the following link (assuming that node
<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img28.gif"/> exists): 
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img29.gif"/></p></li><li>
 If there is an increment operation (<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img30.gif"/>) on row <b>i</b> then
augment the network as follows: 
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img31.gif"/></p></li><li>
 If there is a decrement operation (<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img32.gif"/>) on 
row <b>i</b> then augment the network as follows: 
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img33.gif"/></p></li><li>
 If there is a conditional branch (<tt> IF <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img34.gif"/> GOTO <b>j</b></tt>) on row
<b>i</b> then augment the network as follows:
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img35.gif"/></p></li></ul><h2><a name="SECTION00023000000000000000">2.3 Proof of equivalence</a></h2>
<p>
It needs to be shown that the internal state of the network, or the
contents of the network nodes, can be identified with the program
states, and the succession of the network states corresponds to the
program flow.
</p><p>
Define a `legal state&#39; of the network as follows: 
 </p>
 If all of the instruction nodes have zero outputs, the state is <em>
final.</em> A legal state of the network can directly be interpreted as a
program `snapshot&#39;---if <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img41.gif"/>, the program counter is on the line <b>i</b>,
and the corresponding variable values are stored in the variable nodes.
<p>
 The changes in the network state are activated by the non-zero nodes. 
First, concentrating on the variable nodes, it turns out that they
behave as <em> integrators,</em> the previous contents of the node being
circulated back to the same node.  The only connections from variable
nodes to other nodes have negative weights---that is why, nodes
containing zeros will not be changed, because of the nonlinearity
(<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqF">2</a>).
</p><p>
Next, instruction nodes are elaborated on.  Assume that the only
non-zero instruction node is <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img42.gif"/> at time <b>k</b>---this corresponds to the
program counter being on row <b>i</b> in the program code.
</p><p>
If the row <b>i</b> in the program is <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img43.gif"/>, the one step ahead
behavior of the network can be expressed as (only showing the nodes that
are affected)
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img44.gif"/></p><p>
 It turns out that the new network state is again legal.  Comparing to
the program code, this corresponds to the program counter being
transferred to row <b>i+1</b>.
</p><p>
 On the other hand, if the row <b>i</b> in the
program is <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img45.gif"/>, the one step ahead behavior is
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img46.gif"/></p><p>
 so that, in addition to transferring the program counter to the next
line, the value of the variable <b>V</b> is decremented.  The operation of
the network, if the row <b>i</b> is <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img47.gif"/>, will be the same,
with the exception that the value of the variable <b>V</b> is incremented.
</p><p>
 The conditional branch operation (<tt> IF <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img48.gif"/> GOTO <b>j</b></tt>) on row
<b>i</b> activates a more complex sequence of actions:
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img49.gif"/></p><p>
 and, finally, 
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img50.gif"/></p><p>
 It turns out that after these steps the network state can again be
interpreted as another program snapshot.  The variable values have been
changed and the token has been transferred to a new location just as if
the corresponding program line were executed.  If the token disappears,
the network state does no more change---this can happen only if the
program counter goes `outside&#39; the program code, meaning that the
program terminates.  The operation of the network is also similar to the
operation of the corresponding program, and the proof is completed. 
 
</p>
<h2><a name="SECTION00031000000000000000">3.1 Extensions</a></h2>
<p>
 It is easy to define additional streamlined instructions that can make the
programming easier, and the resulting programs more readable and faster
to execute. For example, 
 </p><ul><li>
 An unconditional branch (<tt> GOTO <b>j</b></tt>) on row <b>i</b> 
can be realized as
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img51.gif"/></p></li><li>
 Addition of a constant <b>c</b> to a variable (<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img52.gif"/>) on row <b>i</b> 
can be realized as
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img53.gif"/></p></li><li>
 A different kind of conditional branch (<tt> IF <b>V=0</b> GOTO <b>j</b></tt>) on row <b>i</b> 
can be realized as
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img54.gif"/></p></li><li>
 Additionally, various increment/decrement instructions can be evaluated
simultaneously. Assume that the following operations are to be executed:
<img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img55.gif"/>. Only one
node <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img56.gif"/> is needed:   
 <p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img57.gif"/></p></li></ul>
 The above approach is by no means the only way to realize the Turing
machine. It is a straightforward implementation that is not necessarily
optimal in applications.
<h2><a name="SECTION00032000000000000000">3.2 Matrix formulation</a></h2>
<p>
 The above construction can also be implemented in a matrix form.  The
basic idea is to store the variable values and the `program counter&#39; in
the process state <b>s</b>, and let the state transition matrix <b>A</b> stand for
the links between the nodes.  The operation of the matrix structure can
be defined as a discrete-time dynamic process
 </p><p><a name="eqIter"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img58.gif"/></p><p> 
 where the nonlinear vector-valued function <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img59.gif"/> is now defined
elementwise as in (<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqF">2</a>). The contents of the state transition
matrix <b>A</b> are easily decoded from the network formulation---the matrix
elements are the weights between the nodes<a name="tex2html6" href="#330"><img alt="gif" src="http://www.hut.fi/WWW/icon/latex2html//foot_motif.gif"/></a>.
</p><p>
This matrix formulation resembles the `concept
matrix&#39; framework that was presented in [<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Aly">3</a>].
</p>
<p>
Assume that a simple function <b>y=x</b> is to be realized, that means, the
value of the input variable <b>x</b> should be transferred to the output variable
<b>y</b>. Using language <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img61.gif"/> this can be coded as (letting the `entry point&#39;
now be not the first but the third row):
 </p><pre><tt> 
		 <tt> back</tt>  		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img62.gif"/> 		 <em> row 1</em>
<p>
		             		 <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img63.gif"/> 		 <em> row 2</em>
</p><p>
		 <tt> start</tt> 		 <tt> IF <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img64.gif"/> GOTO back</tt> 		 <em> row 3</em>
</p></tt></pre>
 The resulting perceptron network is presented in
Fig. <a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#figNetwork">2</a>.  Solid links represent positive connection
(weights being 1), and dashed links represent negative connection
(weights <b>-1</b>).  Compared to Fig. <a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#figRecurre">1</a>, the network
structure is redrawn, and it is simplified by integrating the delay elements in
the nodes. 
<p><a name="331"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img65.gif"/>
</p><p>
In the matrix form the above program looks like
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img66.gif"/></p><p>
 the first two rows/columns in the matrix <b>A</b> corresponding to the links
connected to the nodes that stand for the two variables <b>Y</b> and <b>X</b>, the
next three standing for the three program lines (1, 2, and 3), and the
last two standing for the additional nodes (3&#39; and 3&#39;&#39;) needed for the
branch instruction.
</p><p>
 The initial (before iteration) and final (after iteration, when
the fixed point is found) states are then
 </p><p><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img67.gif"/></p><p>
 If the values of the variable nodes would remain strictly between 0 and
1, the operation of the dynamic system (<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqIter">3</a>) would be linear,
the function <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img68.gif"/> having no effect at all.  In principle, linear
systems theory could then be utilized in the analysis.  For example, in
Fig. <a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#figEigenv">3</a>, the eigenvalues of the state transition matrix
<b>A</b> are shown.  Even if there are eigenvalues outside the unit circle in
the above example, the nonlinearity makes the iteration always stable. 
It turns out that the iteration always converges after <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img69.gif"/>
steps, where <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img70.gif"/>. 
</p><p><a name="332"> </a><img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img71.gif"/>
</p>
<h2><a name="SECTION00051000000000000000">5.1 Theoretical aspects</a></h2>
<p>
It was shown that a Turing machine can be coded as a perceptron network.
By definition, all computable functions are Turing computable---in the
framework of computability theory, there cannot exist a more powerful
computing system. That is why, it can be concluded that
 </p><blockquote> <em> 
The recurrent perceptron network (as presented above) is (yet another)
form of a Turing machine. </em>
 </blockquote>
 The nice thing about this equivalence is that results of computability
theory are readily available---for example, given a network and an
initial state, it is not possible to tell whether the process will
eventually stop or not.
<p>
The above theoretical equivalence does not tell anything about the
efficiency of the computations.  As compared to traditional Turing
machine implementations (today&#39;s computers, actually), the different
mechanisms that take place in the network can make some functions better
realizable in this framework.  At least in some cases, for example, the
network realization of an algorithm can be <em> parallelized</em> by
allowing <em> multiple `program counters&#39;</em> in the snapshot vector. 
The operation of the network is strictly local rather than global. An
interesting question arises, for example, whether the class of <em>
NP-complete</em> problems could be attacked more efficiently in the
network environment!
</p><p>
Compared to the language <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img72.gif"/>, the network realization has the following
`extensions&#39;:
 </p><ul><li>
 The variables can be continuous, not only integer valued. 
Actually, the (theoretical) capability of presenting real numbers makes
the network realization more powerful than the language <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img73.gif"/> is, 
all numbers presented in the language being only rational. 
 </li><li>
 There can exist various `program counters&#39; simultaneously, and the
transfer of control may be `fuzzy&#39;, meaning that program counter values
presented by the instruction nodes 
may be non-integers. 
 </li><li>
 A minor extension is the freely definable program entry point. This may
facilitate simpler programs---for example, the copying of a variable was
done above in three program lines, while the nominal solution (see
[<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Computa">1</a>, p. 17,]) takes seven lines and an additional local variable. 
 </li></ul>
 Compared to the original program code, the matrix formulation is a
clearly more `continuous&#39; information representation formalism than the
program code is---the parameters can (often) be modified without the
iteration result abruptly changing.  This `redundancy&#39; can perhaps be
utilized in some applications.  For example, when using Genetic
Algorithms (GA) for structural optimization, the stochastic search
strategy that is used in the genetic algorithms can be made more
efficient: after a change in the system structure, the local minimum of
the continuous cost function can be searched using some traditional
technique (see [<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#GA">4</a>]).
<p>
Learning a finite state machine structure by examples, as presented in
[<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Automato">5</a>], suggests an approach to iteratively enhancing the
network structure also in this more complex case.
 
</p><p>
 It is not only the neural networks theory that may benefit of the above
result---looking merely at the dynamic system formulation
(<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqIter">3</a>), it becomes evident that all of the phenomena found in
the field of computability theory are also present in simple-looking
nonlinear dynamic processes. The undecidability of the halting problem,
for example, is an interesting contribution in the field of systems
theory: for any decision procedure, represented as a turing machine,
there exist dynamic systems of the form (<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqIter">3</a>) that defies this 
procedure---for example, no general purpose stability analysis algorithms
can be constructed.
</p><h2><a name="SECTION00052000000000000000">5.2 Related work</a></h2>
<p>
 There are some similarities between the presented network structure and
the recurrent Hopfield neural network paradigm (for example, see
[<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Haykin">2</a>]).  In both cases, the `input&#39; is coded as the initial
state in the network, and the `output&#39; is read from the final state of
the network after iteration.  The fixed points of the Hopfield network
are the preprogrammed pattern models, and the inputs are `noisy&#39;
patterns---the network can be used to enhance the corrupted patterns. 
The outlook of the nonlinear function <img alt="" src="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/img74.gif"/> in (<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqF">2</a>) makes the
number of possible states in the above `Turing network&#39; infinite.  As
compared with Hopfield networks, where the unit outputs are always -1 or
1, it can be seen that, theoretically, these network structures are very
different.  For example, while the set of stable points in a Hopfield
network is finite, a program, represented by the Turing network,
typically has an unbounded number of possible outcomes. The
computational power of Hopfield networks is discussed in [<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Orponen">6</a>].
</p><p>
<em> Petri nets</em> are powerful tools in modeling of event-based and
concurrent systems [<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Petri">7</a>].  Petri nets consist of <em> places</em>
and <em> transitions,</em> and <em> arcs</em> connecting them.  Each of the
places may contain an arbitrary number of <em> tokens,</em> the
distribution of tokens being called a <em> marking</em> of a Petri net. 
If all input places of a transition are occupied by tokens, the
transition may <em> fire,</em> one token being deleted from each of the
input places and one token being added to each of its output places. 
It can be shown that an <em> extended Petri net</em> with additional <em>
inhibitory arcs</em> also has the power of a Turing machine (see
[<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#Petri">7</a>]).  The main difference between the above Turing networks
and the Petri nets is that the Petri net framework is more complex, with
specially tailored constructs, and it cannot be expressed in the simple
general form (<a href="http://users.ics.aalto.fi/tho/stes/step96/hyotyniemi1/HH1.html#eqIter">3</a>).
</p>
<p>
This research has been financed by the Academy of Finland, and this
support is gratefully acknowledged.
</p><h2><a name="SECTIONREF">References</a></h2><dl compact="">
<dt><a name="Computa"><strong>1</strong></a></dt><dd>
 Davis, M. and Weyuker, E.:
<em> Computability, Complexity, and Languages---Fundamentals of
Theoretical Computer Science.</em>
Academic Press, New York, 1983.
</dd><dt><a name="Haykin"><strong>2</strong></a></dt><dd> 
Haykin, S.: <em> Neural Networks.  A Comprehensive Foundation.</em>
Macmillan College Publishing, New York, 1994.
</dd><dt><a name="Aly"><strong>3</strong></a></dt><dd>
 Hyötyniemi, H.: 
Correlations---Building Blocks of Intelligence?
 
In <em> Älyn ulottuvuudet ja oppihistoria (History and dimensions
of intelligence),</em> Finnish Artificial Intelligence
Society, 1995, pp. 199--226.
</dd><dt><a name="GA"><strong>4</strong></a></dt><dd>
 Hyötyniemi, H. and Koivo, H.: 
 Genes, Codes, and Dynamic Systems.
 In <em> Proceedings of the Second Nordic Workshop on Genetic 
Algorithms (NWGA&#39;96),</em>
 Vaasa, Finland, August 19--23, 1996.
</dd><dt><a name="Automato"><strong>5</strong></a></dt><dd>
 Manolios, P. and Fanelli, R.:
 First-Order Recurrent Neural Networks and Deterministic Finite State
Automata. 
<em> Neural Computation</em> <b> 6</b>,
1994, pp. 1155--1173.
</dd><dt><a name="Orponen"><strong>6</strong></a></dt><dd>
 Orponen, P.:
 The Computational Power of Discrete Hopfield Nets with Hidden Units.
<em> Neural Computation</em> <b> 8</b>,
1996, pp. 403--415.
</dd><dt><a name="Petri"><strong>7</strong></a></dt><dd>
 Peterson, J.L.: <em> Petri Net Theory and the Modeling of Systems.</em>
Prentice--Hall, Englewood Cliffs, New Jersey, 1981.
</dd></dl>
<p>
 <strong> Turing Machines  are  Recurrent Neural Networks</strong></p><p>
This document was generated using the <a href="http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/latex2html.html"><strong>LaTeX</strong>2<tt>HTML</tt></a> translator Version 95.1 (Fri Jan 20 1995) Copyright © 1993, 1994,  <a href="http://cbl.leeds.ac.uk/nikos/personal.html">Nikos Drakos</a>, Computer Based Learning Unit, University of Leeds. </p><p> The command line arguments were: </p><p>The translation was initiated by Heikki Hy|tyniemi on Mon Aug 26 12:40:58 EET DST 1996</p><dl> <a name="5"><dt>...Networks </dt></a><dd><a name="5">
This paper was presented at the Symposium on Artificial Networks
(Finnish Artificial Intelligence Conference), August 19--23, in Vaasa,
Finland, organized by the Finnish Artificial Intelligence Society and
University of Vaasa.  Published in &#34;STeP&#39;96---Genes, Nets and
Symbols&#34;, edited by Alander, J., Honkela, T., and Jakobsson, M.,
Finnish Artificial Intelligence Society,
pp. 13--24.  </a>
<pre></pre><a name="330"></a></dd><dt><a name="330">...nodes </a></dt><dd><a name="330">A simple `compiler&#39;
between the language 60#60 and the matrix <b>A</b>, written in <tt> Matlab</tt>, is
available from the author</a>
<pre></pre> </dd></dl>
</div>
  </body>
</html>
