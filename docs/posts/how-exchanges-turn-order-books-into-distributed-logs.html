<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://quant.engineering/exchange-order-book-distributed-logs.html">Original</a>
    <h1>How exchanges turn order books into distributed logs</h1>
    
    <div id="readability-page-1" class="page"><div>
    <hr/>
<h2>1. The Parallel Between Exchanges and Databases</h2>
<p>Let&#39;s think about the scale of exchanges for a moment: thousands of orders hitting the system every millisecond, yet every participant, from HFT firms in New York to pension funds in Singapore, sees the exact same sequence of events.</p>
<p>This is distributed systems engineering at its finest, operating under one of the most demanding real-time constraints in computing. High-frequency chaos must be transformed into a single deterministic timeline. </p>
<p>How do exchanges guarantee that when trader A&#39;s order arrives at <code>09:30:00.123456789</code> and trader B&#39;s arrives at <code>09:30:00.123456790</code>, everyone agrees on which came first (even when those orders traverse different network paths, different gateways, different continents)?</p>
<p>The answer: <strong>order books are distributed logs of market events</strong>. This architecture guarantees fairness through deterministic ordering.</p>
<hr/>
<h2>2. The Problem: Ordering Chaos</h2>
<p><img alt="High-speed stream of red and blue data flowing from an exchange matching engine" src="https://quant.engineering/images/exchange-order-book-distributed-logs/ordering_chaos.png"/></p>
<p>Physical reality is messy: orders don&#39;t arrive at exchanges in a neat, orderly stream.
If they did, this article would be one paragraph long.</p>
<p>Instead they pour in from different gateways, different data centers, different continents. Each packet might take a unique path through the internet&#39;s topology. A trader in London might route through Frankfurt. A firm in Chicago might have direct fiber. Another might bounce through three ISPs.</p>
<p><strong>The core problem</strong>: turning concurrent events into a single, globally-agreed sequence.</p>
<p>The stakes are very high: <em>price-time priority</em> (the principle that earlier orders at the same price get filled first) requires perfect ordering. Market integrity depends on participants trusting that the game isn&#39;t rigged, that the sequence is fair and deterministic.</p>
<p>A tempting idea is to simply timestamp orders on arrival. The problem: distributed clocks lie. Even with PTP (<em>Precision Time Protocol</em>), microsecond-level drifts happen. And with NTP (<em>Network Time Protocol</em>), it&#39;s orders of magnitude worse. And at a deeper level, there&#39;s no global <em>&#34;now&#34;</em> in a distributed system: two orders hitting different gateways at the same instant have no natural ordering.</p>
<p>Timestamps aren&#39;t enough. Exchanges need a stronger ordering primitive.</p>
<hr/>
<h2>3. The Solution: Event Sourcing at Nanosecond Scale</h2>
<h3>Architecture Overview</h3>
<p>Modern exchanges solve the ordering problem with a deceptively simple pipeline:</p>
<div><pre><span></span><code>Gateway → Sequencer → Matching Engine.
</code></pre></div>

<p>Orders hit the exchange through multiple gateways. They handle basic validation and sanity checks, but they never decide ordering. Everything gets funneled straight to <strong>the sequencer</strong>.</p>
<p><img alt="A glowing circular gateway channels chaotic neon light trails on the left into perfectly aligned vertical bars on the right symbolizing the transformation of a linear event log into a structured order book with organized price levels." src="https://quant.engineering/images/exchange-order-book-distributed-logs/event_log_to_order_book.png"/></p>
<p>The sequencer is a single logical component (replicated for fault tolerance) that assigns a monotonically increasing sequence number to every event:</p>
<div><pre><span></span><code>Order from gateway #3? → seq=1000
Cancel from gateway #1? → seq=1001
Execution report? → seq=1002
</code></pre></div>

<p>This creates a total order, something that can&#39;t be achieved with timestamps alone.</p>
<p>Once an event has a sequence number, it flows to the matching engine.</p>
<p>The matching engine maintains the in-memory order book and applies events in the exact sequence they were assigned. It&#39;s fully deterministic: replaying the same stream yields the same outcome.</p>
<p>That determinism is the key. It turns the order book into a distributed log: append-only, replayable, auditable, and reconstructable.</p>
<h3>Log Structure</h3>
<p>Once the exchange is treated as an event-sourced system, the structure of the log becomes obvious. It&#39;s minimal by design. Every state transition fits into a single event shape:</p>
<p><code>[seq_num, timestamp, order_id, event_type, price, quantity, metadata]</code></p>
<p>Events are never overwritten or removed: state transitions are recorded through appends.</p>
<p>A cancel event does not delete an order, it simply represents an explicit cancellation request and is appended to the log. The append-only contract is what makes replay deterministic: feeding the raw log back into a clean instance of the matching engine yields the same book state.</p>
<p>A simple lifecycle illustrates the idea:</p>
<div><pre><span></span><code><span>seq</span><span>=</span><span>1000</span><span>:</span><span> </span><span>NEW_ORDER</span><span>  </span><span>order_id</span><span>=</span><span>ABC123</span><span> </span><span>BUY</span><span> </span><span>100</span><span> </span><span>AAPL</span><span> </span><span>@150.00</span>
<span>seq</span><span>=</span><span>1001</span><span>:</span><span> </span><span>NEW_ORDER</span><span>  </span><span>order_id</span><span>=</span><span>XYZ789</span><span> </span><span>SELL</span><span> </span><span>50</span><span> </span><span>AAPL</span><span> </span><span>@150.00</span>
<span>seq</span><span>=</span><span>1002</span><span>:</span><span> </span><span>TRADE</span><span>      </span><span>buy</span><span>=</span><span>ABC123</span><span> </span><span>sell</span><span>=</span><span>XYZ789</span><span> </span><span>qty</span><span>=</span><span>50</span><span> </span><span>price</span><span>=</span><span>150.00</span>
</code></pre></div>

<p>One buyer, one seller, one partial execution captured as three immutable events.</p>
<p>The log is the truth; the order book is just a real-time projection of this sequence.</p>
<h3>From Log to Book: The Reduction Operation</h3>
<p>The log is linear: a single global sequence of events.
But the order book is hierarchical: price levels, each holding a FIFO queue of resting orders.</p>
<p>Bridging the two is a reduction step: a deterministic function that consumes the event stream and produces the current book state.</p>
<p><strong>Reduction rules</strong>:</p>
<ul>
<li><strong>NEW_ORDER</strong> → append to the queue at the price level</li>
<li><strong>CANCEL</strong> → remove from the queue</li>
<li><strong>TRADE</strong> → pop from the front of the queue (FIFO per price level)</li>
<li><strong>MODIFY</strong> → remove the old entry, insert the updated one</li>
</ul>
<p>Each price level behaves like its own per-key append log, and the full order book is a merged materialization of all those per-key logs, kept in price–time priority order.</p>
<div><pre><span></span><code>Log:
  seq=1: BUY 100 @ 150
  seq=2: BUY 200 @ 150
  seq=3: BUY 150 @ 151
  seq=4: SELL 60 @ 151
  seq=5: SELL 120 @ 152

Book state after seq=5:
  Asks:
    152: [order_5: 120]
  Bids:
    151: [order_3: 90]
    150: [order_1: 100, order_2: 200]
</code></pre></div>

<p>The elegance of this model is that the in-memory book is just cached state.</p>
<p>If the matching engine restarts, replaying the log restores the book exactly as it was.</p>
<h3>The Anatomy of an Order Book as a Log</h3>
<p>The matching engine keeps the book in-memory: nanosecond access, tight data structures, no syscalls in the hot path.</p>
<p>The log on disk is the source of truth. Exchanges write every event to replicated storage designed to absorb millions of appends per second.</p>
<p>Recovery is simple: replay the log and rebuild the book.
Load the last snapshot, apply the remaining events in order, and the in-memory structure reappears exactly as it was.</p>
<p>This works because matching is a pure function of the log.</p>
<h3>Why the Log Model Wins</h3>
<p>The log model is the only architecture that scales technically and economically.</p>
<ul>
<li><strong>Fairness</strong>: Price–time priority requires a total order. At the same price level, sequence numbers decide who gets filled first.</li>
<li><strong>Determinism</strong>: Given the same log, every engine produces the same fills. Determinism makes the system predictable under load.</li>
<li><strong>Auditability</strong>: Regulators replay the log to verify behavior.</li>
<li><strong>Simplicity</strong>: Everything reduces to append. New orders, cancels, modifies, trades: only one primitive, one path, one mental model.</li>
<li><strong>Recovery</strong>: Matching engines can crash; the log cannot. Rebuild by replaying events.</li>
<li><strong>Materialized Views</strong>: The order book is one projection. Risk systems, surveillance engines, analytics pipelines: all derive their own views directly from the same event stream.</li>
<li><strong>Testing</strong>: Deterministic logs produce deterministic simulations. Entire markets can be replayed for debugging or scenario analysis.</li>
<li><strong>Analytics</strong>: Market behavior becomes a data-engineering problem. The log is a fact table with perfect temporal ordering.</li>
<li><strong>Cross-system consistency</strong>: Every downstream system integrates through the log. It&#39;s the universal interface.</li>
</ul>
<p>This is why modern exchanges behave more like ultra-low-latency log processors than traditional databases.</p>
<p>The book is fast; the log is truth.</p>
<hr/>
<h2>4. The Performance Cost of Determinism</h2>
<p>Deterministic ordering keeps markets fair, but it comes with a real cost: every event, no matter its origin, must pass through the same chokepoint.</p>
<p>A total order forces serialization: no parallelism in the ordering path.</p>
<p>Determinism locks the system into a single timeline, rendering it expensive.</p>
<h3>The Sequencer Bottleneck</h3>
<p><img alt="A glowing cyberpunk crystal network radiates from a central data core, each translucent shard displaying vertical sequences of numbers in cyan. The crystals form a hexagonal pattern, emitting synchronized magenta and aqua light bursts across a dark blue backdrop, symbolizing replication, durability, and deterministic data replay." src="https://quant.engineering/images/exchange-order-book-distributed-logs/sequencer_bottleneck.png"/></p>
<p>Every modern exchange has a single logical sequencer.
No matter how many gateways feed the system, all events flow into one component whose job is to assign the next sequence number. That integer defines the global timeline.</p>
<p>The sequencer is the first latency chokepoint:</p>
<ul>
<li><strong>Throughput limits</strong>: how many events per second can be stamped with a sequence number ... millions? tens of millions?</li>
<li><strong>Propagation delay</strong>: once sequenced, the event must reach the matching engine and every replica immediately.</li>
<li><strong>Coordination cost</strong>: replicas must apply events in the same order, adding nanoseconds to microseconds of agreement overhead.</li>
</ul>
<p>Exchanges can scale horizontally almost everywhere else.
The sequencer is the exception: it&#39;s vertical scale only.</p>
<h3>How Exchanges Hit the Nanosecond Budget</h3>
<p>The only way to scale is to make the fast path very efficient.</p>
<p>Matching engines run with latencies measured in tens of nanoseconds: every instruction matters and every cache miss hurts.</p>
<p>Exchanges hit these budgets through a stack of low-level engineering techniques:</p>
<ul>
<li><strong>Kernel bypass</strong>: network frames are pushed straight into user memory, bypassing the OS network stack.</li>
<li><strong>Batching</strong>: events are processed in small bursts to amortize fixed costs.</li>
<li><strong>Cache locality</strong>: data stays hot in the CPU&#39;s L1/L2 caches, avoiding slow random memory access.</li>
<li><strong>NUMA pinning</strong>: threads run on a specific CPU socket and use its local RAM to avoid cross-socket latency.</li>
<li><strong>Zero-copy design</strong>: sequencer, matcher, and downstream feeds operate on shared buffers with no unnecessary copies.</li>
</ul>
<blockquote>
<p>Several of these techniques appear in the <a href="https://quant.engineering/series/low-latency-fundamentals.html" rel="noopener noreferrer" target="_blank">Low-Latency Fundamentals series</a>.</p>
</blockquote>
<h3>Why Eventual Consistency Is Impossible in Finance</h3>
<p>Eventual consistency works for systems that can tolerate temporary divergence between replicas.
Markets cannot: price–time priority requires strict ordering.</p>
<p>If two orders compete at the same price, every participant must agree on which one arrived first immediately, not eventually. Any disagreement produces a different winner and that&#39;s a market integrity violation.</p>
<p>This is CAP in its harshest form: trading systems choose <strong>Consistency</strong> and <strong>Partition Tolerance</strong>.
They cannot choose <strong>Availability</strong> in the CAP sense.
If a gateway cannot reach the sequencer, it must reject new orders. Accepting them without a sequence number would violate fairness.</p>
<hr/>
<h2>5. Replication: Making the Log Fault-Tolerant</h2>
<p>The log is the source of truth, so it must survive hardware failures, process crashes, and network partitions. The challenge is doing this without breaking the nanosecond-level fast path that matching engines rely on.</p>
<p>Replication solves this, but only if it preserves ordering and avoids adding unnecessary latency.</p>
<h3>Replication Strategies Without Latency Spikes</h3>
<p><img alt="A glowing cyberpunk crystal network radiates from a central data core, each translucent shard displaying vertical sequences of numbers in cyan. The crystals form a hexagonal pattern, emitting synchronized magenta and aqua light bursts across a dark blue backdrop, symbolizing replication, durability, and deterministic data replay." src="https://quant.engineering/images/exchange-order-book-distributed-logs/deterministic_data_replay.png"/></p>
<p>Replicating the log sounds expensive, but exchanges can&#39;t afford to slow down the sequencing path. The sequencer must stamp events with minimal delay, then push them to replicas without blocking matching.</p>
<p>Exchanges use a combination of techniques:</p>
<ul>
<li><strong>Pipelined replication</strong>: the sequencer assigns a sequence number immediately and ships the event to replicas in parallel. Matching doesn&#39;t wait for the replicas to acknowledge.</li>
<li><strong>Quorum strategies</strong>: some systems require a subset of replicas (a quorum) to confirm durability before an event is considered safe, balancing latency against failure tolerance.</li>
<li><strong>Asynchronous disk writes</strong>: events land in memory and are flushed to storage in batches, off the hot path.</li>
<li><strong>NIC-level fan-out</strong>: modern exchanges use hardware multicast or kernel-bypass NICs to distribute events to replicas with minimal CPU involvement.</li>
</ul>
<p>Sequencing must stay fast, so durability happens in the background.</p>
<h3>Continuity Guarantees: No Gaps, No Duplicates, No Reordering</h3>
<p>Replication only works if replicas can prove the log is continuous. A missing event, a duplicated event, or a reorder breaks determinism and invalidates every downstream view of the market.</p>
<p>Replicas enforce strict invariants:</p>
<ul>
<li><strong>No gaps</strong>: if a replica receives <code>seq=5001</code> without having seen <code>seq=5000</code>, it halts and requests the missing event before proceeding.</li>
<li><strong>No duplicates</strong>: receiving the same sequence twice is a signal of upstream retry or network duplication. It must be detected and ignored.</li>
<li><strong>No reordering</strong>: <code>seq=5002</code> must never be applied before <code>seq=5001</code>, even if the network delivers it first.</li>
</ul>
<p>If any invariant is violated, the replica stops applying events until the timeline is repaired.
This guarantees that every replica maintains an identical log.</p>
<h3>Snapshots: Making Replay Practical at Scale</h3>
<p>Replaying from genesis isn&#39;t practical once logs reach millions or billions of events. Snapshots solve that problem.</p>
<p>A snapshot is a point-in-time dump of the in-memory book. Snapshots are written periodically and stored alongside the log.</p>
<p>On restart:</p>
<ol>
<li>Load the latest snapshot.</li>
<li>Apply the log entries recorded after it.</li>
<li>The book is restored exactly as it should be.</li>
</ol>
<hr/>
<h2>6. Conclusion</h2>
<p>Modern exchanges behave like ultra-low-latency log processors. Everything flows from one idea: a total order of events. The sequencer defines the timeline, the matching engine reduces that timeline into a book, and replication keeps the log durable without slowing the fast path.</p>
  </div><p>
    AI tools are used for drafting and editing. All technical reasoning, system design, and conclusions are human-driven.
</p></div>
  </body>
</html>
