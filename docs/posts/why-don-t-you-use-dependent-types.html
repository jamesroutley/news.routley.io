<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html">Original</a>
    <h1>Why don&#39;t you use dependent types?</h1>
    
    <div id="readability-page-1" class="page"><section>

      
<p>02 Nov 2025</p>

<span>[
  
    
    <a href="https://lawrencecpaulson.github.io/tag/memories"><code><nobr>memories</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/AUTOMATH"><code><nobr>AUTOMATH</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/LCF"><code><nobr>LCF</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/type_theory"><code><nobr>type theory</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/Martin-L%C3%B6f_type_theory"><code><nobr>Martin-Löf type theory</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/NG_de_Bruijn"><code><nobr>NG de Bruijn</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/ALEXANDRIA"><code><nobr>ALEXANDRIA</nobr></code> </a>
  
]</span>

<p>To be fair, nobody asks me this exact question.
But people have regularly asked why Isabelle dispenses with proof objects.
The two questions are essentially the same, 
because proof objects are intrinsic to all the usual type theories.
They are also completely unnecessary and a huge waste of space.
As described in an <a href="https://lawrencecpaulson.github.io/2022/01/05/LCF.html">earlier post</a>,
type checking in the <em>implementation language</em> (rather than in the logic)
can ensure that only legitimate proof steps are executed.
Robin Milner had this fundamental insight 50 years ago,
giving us the LCF architecture with its proof kernel.
But the best answer to the original question is simply this: 
I did use dependent types, for years.</p>

<h3 id="my-time-with-automath">My time with AUTOMATH</h3>

<p>I was lucky enough to get some personal time with N G de Bruijn
when he came to Caltech in 1977 to lecture about
<a href="https://lawrencecpaulson.github.io/2021/11/03/AUTOMATH.html">AUTOMATH</a>.
I never actually got to use this system.
Back then, researchers used the nascent Internet (the ARPAnet)
not to download software so much as 
to run software directly on the host computer, 
since most software was not portable.
But Eindhoven University was not on the ARPAnet,
and AUTOMATH was configured to run on 
<a href="https://automath.win.tue.nl/archive/pdf/aut034.pdf">a computer we did not have</a>:</p>

<blockquote>
  <p>Until September 1973, the computer was the Electrologica X8, after that
Burroughs 6700. In both cases the available multiprogranming systems
required the use of ALGOL 60.</p>
</blockquote>

<p>I did however read many of the research reports, including 
the <a href="https://automath.win.tue.nl/archive/pdf/aut046.pdf">PhD dissertation</a> by LS Jutting,
where he presents his translation 
of Landau’s text <em>Grundlagen der Analysis</em> (described <a href="https://lawrencecpaulson.github.io/2025/10/15/Proofs-trivial.html">last time</a>)
from German into AUTOMATH.
It is no coincidence that many of my papers, from the
<a href="https://doi.org/10.1016/0167-6423(85)90009-7">earliest</a>
to the <a href="https://doi.org/10.4230/LIPIcs.ITP.2025.18">latest</a>,
copied the idea of formalising a text
and attempting to be faithful to it, if possible line by line.</p>

<p>As an aside, note that while AUTOMATH was a system of dependent types,
it did not embody the 
<a href="https://lawrencecpaulson.github.io/2023/08/23/Propositions_as_Types.html">Curry–Howard correspondence</a>
(sometimes wrongly called the Curry–Howard–de Bruijn correspondence).
That correspondence involves having a type theory strong enough
to represent the predicate calculus directly in the form of types.
In AUTOMATH you had to introduce the symbols and inference rules 
of your desired calculus in the form of axioms, much as you do with Isabelle.
In short, AUTOMATH was a <a href="https://pure.tue.nl/ws/files/1892191/597622.pdf">logical framework</a>:</p>

<blockquote>
  <p>like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants</p>
</blockquote>

<p>De Bruijn
<a href="https://pure.tue.nl/ws/portalfiles/portal/4428179/597611.pdf">did not approve</a> 
of the increasingly powerful type theories being developed in the 1990s.
AUTOMATH was a weak language, 
a form of λ-calculus including a general product construction just
powerful enough to express the inference rules of a variety of formalisms
and to make simple definitions, again clearly the inspiration for Isabelle.
Isabelle aims to be <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-130.html"><em>generic</em></a>, like the big AUTOMATH restaurant.
Only these days everybody prefers the same cuisine,
higher-order logic, so Isabelle/HOL has become dominant.
Unfortunately, I last spoke to Dick (as he was known to friends)
when I was putting all my effort into Isabelle/ZF.
He simply loathed set theory and saw mathematics as essentially typed.
He never lived to see the enormous amount of advanced mathematics
that would be formalised using types in Isabelle/HOL.</p>

<p>I annoyed him in another way. I kept asking,
AUTOMATH looks natural, but how do we know that it is right?
He eventually sent me a 300 page volume entitled
<a href="https://automath.win.tue.nl/archive/pdf/aut073.pdf">The Language Theory of Automath</a>.
It describes AUTOMATH’s formal properties such as 
strong normalisation and Church–Rosser properties,
but this was not the answer I wanted at all.
I got that answer for a quite different type theory.</p>

<h3 id="martin-löf-type-theory">Martin-Löf type theory</h3>

<p>In response to kind invitations from Bengt Nordström and Kent Petersson,
I paid a number of visits to Chalmers University in Gothenburg
to learn about Martin-Löf type theory.
I was particularly impressed by its promise 
of a systematic and formal approach to program synthesis.
I had already encountered <a href="https://lawrencecpaulson.github.io/2021/11/24/Intuitionism.html">intuitionism</a>
through a course on the philosophy of mathematics at Stanford University,
as I recall taught by <a href="https://www.pet.cam.ac.uk/news/professor-ian-macdougall-hacking-1936-2023">Ian Hacking</a>.
The “rightness” of Martin-Löf type theory was obvious, 
because it directly embodied the principles of intuition truth
as outlined by Heyting: for example, that
a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.</p>

<p>I devoted several years of research to Martin-Löf type theory.
This included a whole year of intricate hand derivations to produce a 
<a href="https://doi.org/10.1016/S0747-7171(86)80002-5">paper</a> 
that I once thought would be important,
and the <a href="https://lawrencecpaulson.github.io/2022/07/13/Isabelle_influences.html">very first version</a> 
of Isabelle.
Yes: Isabelle began as an implementation of Martin-Löf type theory,
which is <a href="https://lawrencecpaulson.github.io/2022/11/30/CTT_in_Isabelle-II.html">still included</a> 
in the distribution even today as Isabelle/CTT.
But eventually I tired of what seemed to me a doctrinaire attitude
bordering on a cult of personality around Per Martin-Löf.
The sudden switch to intensional equality 
(everyone was expected to adopt the new approach) wrecked most of my work.
Screw that.</p>

<p>You might ask, what about the calculus of constructions,
which arose during that time and eventually gave us Rocq and Lean?
(Not to mention <a href="https://www.lfcs.inf.ed.ac.uk/reports/92/ECS-LFCS-92-211/">LEGO</a>.)
To me they raised, and continue to raise, the same question I had put to de Bruijn.
Gérard Huet said something like “it is nothing but function application”,
which did not convince me.
It’s clear that I am being fussy,<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>
because thousands of people find these formalisms perfectly natural and believable.
But it is also true that the calculus of constructions 
underwent numerous changes over the past four decades.
There seem to be several optional axioms that people sometimes adopt
while attempting to minimise their use, 
like dieters enjoying an occasional croissant.</p>

<h3 id="decisions-decisions">Decisions, decisions</h3>

<p>We can see all this as an example of the choices we make in research.
People were developing new formalisms. This specific fact was the impetus
for making Isabelle generic in the first place.
But we have to choose whether to spend our time developing formalisms 
or instead to choose a fixed formalism and see how far you can push it.
Both are legitimate research goals.</p>

<p>For example, already in 1985, Mike Gordon 
was using higher-order logic <a href="https://doi.org/10.48456/tr-77">to verify hardware</a>.
He was not distracted by the idea that some dependent type theory might work better
for <em>n</em>-bit words and the like.
The formalism that he implemented was essentially the same as the 
<a href="https://plato.stanford.edu/entries/type-theory-church/">simple theory of types</a> 
outlined by Alonzo Church in 1940.
He made verification history using this venerable formalism, 
and John Harrison later found
a clever way to <a href="https://doi.org/10.1007/11541868_8">encode the dimension</a>
of vector types including words.
Isabelle/HOL also implements Church’s simple type theory,
with one extension: <a href="https://lawrencecpaulson.github.io/2022/03/02/Type_classes.html">axiomatic type classes</a>.
Isabella users also derive much power from the <a href="https://doi.org/10.1007/s10817-019-09537-9">locale concept</a>, 
a kind of module sysstem that lies outside any particular logic.</p>

<p>During all this time, both Martin-Löf type theory and the calculus of constructions
went through several stages of evolution. It’s remarkable how the Lean community,
by running with a certain version of the calculus,
quickly formalised a <a href="https://leanprover-community.github.io/mathlib-overview.html">vast amount of mathematics</a>.</p>

<h3 id="pushing-higher-order-logic-to-its-limit">Pushing higher-order logic to its limit</h3>

<p>I felt exceptionally lucky to win 
<a href="https://cordis.europa.eu/project/id/742178">funding from the European Research Council</a>
for the advanced grant <a href="https://lawrencecpaulson.github.io/2021/12/08/ALEXANDRIA.html">ALEXANDRIA</a>.
When I applied, homotopy type theory was still all the rage,
so <a href="https://www.cl.cam.ac.uk/~lp15/Grants/Alexandria/Part-B2.pdf">the proposal</a>  emphasised Isabelle’s specific advantages: its automation,
its huge libraries and the legibility of its proofs.</p>

<p>The team started work with enthusiasm.
Nevertheless, I fully expected that we would hit a wall, 
reaching mathematical material
that could not easily be formalised in higher-order logic.
Too much of Isabelle’s analysis library identified topological spaces
with types.
Isabelle’s abstract algebra library was old and crufty.
A number of my research colleagues were convinced
that higher-logic was not adequate for serious mathematics.
But Anthony Bordg took up the challenge, leading a subproject
to <a href="https://doi.org/10.1080/10586458.2022.2062073">formalise Grothendieck schemes</a>.</p>

<p>For some reason I had a particular fear of the field extension $F[a]$,
which extends the field $F$ with some $a$ postulated to be 
a root of some polynomial over $F$.
(For example, the field of complex numbers is precisely $\mathbb{R}[i]$, 
where $i$ is postulated to be a root of $x^2+1=0$.)
And yet an early outcome of ALEXANDRIA was<a href="https://rdcu.be/cIK3W"> a proof</a>,
by Paulo Emílio de Vilhena and Martin Baillon,
that every field admits an algebraically closed extension. 
This was the first proof of that theorem in any proof assistant, 
and its proof involves an infinite series of field extensions.</p>

<p>We never hit any wall.
As our group went on to formalise 
<a href="https://www.cl.cam.ac.uk/~lp15/Grants/Alexandria/">more and more advanced results</a>,
such as the <a href="https://doi.org/10.1145/3573105.3575680">Balog–Szemerédi–Gowers theorem</a>,
people stopped saying “you can’t formalise mathematics without dependent types”
and switched to saying “dependent types give you nicer proofs”.
But they never proved this claim.</p>

<p>Now that dependent type theory has attained maturity 
and has an excellent tool in the form of Lean, shall I go back to dependent types?
I am not tempted. The only aspects of Lean that I envy are its huge community and
the <a href="https://github.com/PatrickMassot/leanblueprint">Blueprint tool</a>.
I hear too many complaints about Lean’s performance.
I’ve heard of too many cases where dependent types played badly 
with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. 
Quite a few people have told me that 
the secret of dependent types is knowing when <strong>not</strong> to use them.
And so, to me, they have too much in common 
with Tesla’s <a href="https://electrek.co/2025/10/29/tesla-full-self-driving-v14-disappoints-with-hallucinations-brake-stabbing-speeding/">Full Self-Driving</a>.</p>

<p><em>Addendum</em>: somebody commented on Hacker News that higher-order logic is too weak 
(in terms of proof-theoretic strength) to formalise post-WWII mathematics.
This is not quite right.
It is true that higher-order logic is much, much weaker than ZF set theory.
But one of the most striking findings of ALEXANDRIA is that this is no obstacle
to doing advanced mathematics, say to formalise Grothendieck schemes.
Such elaborate towers of definitions do not seem to ascend especially high
in the set-theoretic hierarchy. I can only recall a couple of proofs
(<a href="https://doi.org/10.1080/10586458.2021.1980464">this one</a>, 
and <a href="https://rdcu.be/eNVb6">that one</a>) 
that required strengthening higher-order logic with the ZF axioms 
(which is easily done). 
These were theorems that referred to ZF entities in their very statements.</p>







      


    </section></div>
  </body>
</html>
