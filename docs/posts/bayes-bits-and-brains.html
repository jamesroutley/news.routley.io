<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bayesbitsbrains.github.io/">Original</a>
    <h1>Bayes, Bits and Brains</h1>
    
    <div id="readability-page-1" class="page"><div><main><article>
<p>This site is about probability and information theory. We&#39;ll see how they help us understand machine learning and the world around us.</p>
<h2>A few riddles <a id="introduction"></a></h2>
<p>More about the content, prerequisites, and logistics later. I hope you get a feel for what this is about by checking out the following riddles. I hope some of them <a href="https://xkcd.com/356/">nerd-snipe</a> you! üòâ You will understand all of them at the end of this minicourse.</p>
<div id="intelligence"><div><p>Test your intelligence with the following widget! You will be given a bunch of text snippets cut from Wikipedia at a random place. Your job: predict the next letter! Try at least five snippets and compare your performance with some neural nets (GPT-2 and Llama 4).</p><p>Don&#39;t feel bad if a machine beats you; they&#39;ve been studying for this test their entire lives! But why? And why did Claude Shannon - the information theory <a href="https://www.merriam-webster.com/dictionary/goat">GOAT</a> - make this experiment in the 1940s?</p><p>Hide ‚ñ≤</p></div></div>

<div id="wikipedia"><div><p>üåê How much information is on Wikipedia?</p></div></div>


<!-- -->
<div id="xkcd"><div><p>ü§ì Explaining XKCD jokes</p></div></div>
<!-- -->
<h3>Onboarding</h3>
<p>As we go through the mini-course, we&#39;ll revisit each puzzle and understand what&#39;s going on. But more importantly, we will understand some important pieces of mathematics and get solid theoretical background behind machine learning.</p>
<p>Here are some questions we will explore.</p>
<ul>
<li>What&#39;s KL divergence, entropy and cross-entropy? What&#39;s the intuition behind them? (chapters 1-3)</li>
<li>Where do the machine-learning principles of maximum likelihood &amp; maximum entropy come from? (chapters 4-5)</li>
<li>Why do we use logits, softmax, and Gaussian all the time? (chapter 5)</li>
<li>How to set up loss functions? (chapter 6)</li>
<li>How compression works and what intuitions it gives about LLMs? (chapter 7)</li>
</ul>


<h2>What&#39;s next?</h2>
<p>This is your last chance. You can go on with your life and believe whatever you want to believe about KL divergence. Or you go to the <a href="https://bayesbitsbrains.github.io/01-kl_intro">first chapter</a> and see how far the rabbit-hole goes.</p>
<p><img src="https://bayesbitsbrains.github.io/fig/pills.png" alt="pills" title="Image source: https://www.pngegg.com/en/png-nllqe/"/><a aria-label="Take the red pill - Go to first chapter" href="https://bayesbitsbrains.github.io/01-kl_intro"></a></p>
<!-- -->
</article></main></div></div>
  </body>
</html>
