<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd">Original</a>
    <h1>Optimizing the Lichess Tablebase Server</h1>
    
    <div id="readability-page-1" class="page"><div><p>Recently our <a href="https://lichess.org/@/lichess/blog/7-piece-syzygy-tablebases-are-complete/W3WeMyQA">our 7 piece Syzygy tablebase server</a> was struggling to complete its periodic RAID integrity check while being hammered with tablebase requests. We decided to try a new approach, using <a href="https://man7.org/linux/man-pages/man7/lvmraid.7.html#DATA_INTEGRITY">dm-integrity on LVM</a>. Now, instead of periodically checking every data block, we passively check blocks whenever they are read.</p>
<p>17 TiB of tablebases are unwieldy, so to do this migration without hours of downtime, we set up a second server with the new approach. This also allowed us to run controlled benchmarks on the full set of tablebases, before finally doing the switch and retiring the old server.</p>
<p>We&#39;re trying to get the most out of the following new hardware:</p>
<ul>
<li>32 GiB RAM unchanged</li>
<li>2 x 201 GiB NVMe, where the previous server didn&#39;t have any SSD space. The rest of the 476 GiB disks is reserved for OS and working space</li>
<li>6 x 5.46 TiB HDD, where the previous server had only 5 disks</li>
</ul>
<p>The current operating system is Debian bookworm with default I/O schedulers:</p>
<pre><code>root@bwrdd:~# uname -a
Linux bwrdd 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux
root@bwrdd:~# cat /sys/class/block/nvme0n1/queue/scheduler
[none] mq-deadline
root@bwrdd:~# cat /sys/class/block/sda/queue/scheduler
[mq-deadline] none
</code></pre>
<h2>Monitoring important as ever</h2>
<p>RAID 5 is a good fit here, allowing recovery from any single disk failure, and distributing random reads across all disks. My first attempt was:</p>
<pre><code>$ lvcreate --type raid5 --raidintegrity y --raidintegrityblocksize 512 --name tables --size 21T vg-hdd # Oops
</code></pre>
<p>Peformance numbers in initial tests were decent, but we would have left a lot on the table if we didn&#39;t have monitoring to catch that not all disks were indeed participating equally.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:0yd54XyjtR2R:VUTWHQqX.png&amp;w=800&amp;sig=3fc8b923017ef07ac43342d38a06c1665b34d092" alt="Reads not distributed evenly"/></p>
<p>That&#39;s because omitting <code>--stripes</code> does <em>not</em> default to use all physical volumes.</p>
<h2>Benchmark results (overview)</h2>
<p>In normal conditions the server receives between 10 and 35 requests per second. We record 1 million requests in the production environment to replay them in a controlled benchmark. In the chosen scenario, 12 parallel clients each sequentially submit requests from the production log.</p>
<p>Tables are lazily opened, and application and OS caches are lazily populated. So the first 800k response times are ignored as a warmup. We analyse the response times for the remaining 200k requests.</p>
<p>On average, response times are plenty fast, but tail latencies are high. So this is our focus for any optimizations. We&#39;ll unpack the results in a moment, but here are the empirical distribution functions (ECDFs) with 30ms added to each response time for an overview.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:KfbJv0yt3hyH:W2HFO1ha.png&amp;w=800&amp;sig=324cebd8631a951e7a67b9ec74831391dfcbca1e" alt="ECDFs with 30ms offset"/></p>
<p>For a given response time on the x axis (log scale!) you can see which proportion of requests is faster. Or for a given proportion on the y axis (think percentile), you can read off the corresponding response time on the x axis.</p>
<p>The added constant seems artificial, but it&#39;s just viewing the results from the point of view of a client with 30ms ping time. Otherwise the log scaled x-axis would overemphasize the importance of a few milliseconds at the low end.</p>
<h2>mmap with higher tail latencies than pread</h2>
<p>Our Syzygy tablebase implementation <a href="https://github.com/niklasf/shakmaty-syzygy">shakmaty-syzygy</a> now offers an interface to plug in different ways of opening and reading from table files. The main contenders are:</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man2/mmap.2.html">Map</a> table files into memory. After the file has been mapped, disk reads happen transparently when accessing the respective memory region, so no further system calls are needed. Unfortunately, that also means reads look just as infallible as normal memory accesses, so that errors can only be handled out of band, via signals.</li>
<li><a href="https://man7.org/linux/man-pages/man2/pwrite.2.html">pread(2)</a>, one system call per read, with read error reporting via the return value.</li>
</ul>
<p>More robust error handling would probably be enough to justify using <code>pread</code> for a server implementation, but surprisingly, the diagram above shows that <code>pread</code> also peforms <em>better</em> in the scenario we care about. Perhaps that is because sometimes transparently reading a single memory-mapped data block across page boundaries may end up issuing two disk reads</p>
<pre><code>while (...)
{
    uint8_t d = *ptr++;
}
</code></pre>
<p>whereas</p>
<pre><code>uint8_t buf[MAX_BLOCK_SIZE];
ssize_t res = pread(fd, buf, block_size, offset);
</code></pre>
<p>immediately reveals how much data will be read.</p>
<p>Now, before you change your chess engine to use <code>pread</code>: Tablebases in engine matches are typically used only if enough fast storage for all WDL tables is available. The typical range of response times is not even visible in the graph above. Here, the saved syscall overhead is significant, so that memory mapping performs better.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:TLBrF0isfFgq:FbA5f9fq.png&amp;w=800&amp;sig=8aa103c6da4a742e53ed5e78949542a0231afb79" alt="ECDFs without offset"/></p>
<h2><code>MADV_RANDOM</code> / <code>POSIX_FADV_RANDOM</code> counter-productive</h2>
<p>The next surprise looking at the results above, is that <a href="https://man7.org/linux/man-pages/man2/posix_fadvise.2.html"><code>posix_fadvise(fd, 0, 0, POSIX_FADV_RANDOM)</code></a> or its equivalent for memory maps are actually mostly counter-productive. <code>POSIX_FADV_RANDOM</code> is intended to alleviate pressure on the page cache, by hinting to the operating system that file accesses are going to be random and automatic read-ahead is likely pointless.</p>
<p>Perhaps tablebase access patterns when people are analysing endgames are not so random afterall.</p>
<p>Again, this may differ for chess engines, where probes may be more likely to be scattered across different possible endgames.</p>
<h2>Table prefixes on limited SSD space</h2>
<p>To decide how to use the limited SSD space, let&#39;s have a look at the anatomy of a single table probe. The position will be encoded as an integer index, based on encoding information from the table header. Then we need to find the compressed data block that contains the result for the particular index. Syzygy provides a &#34;sparse&#34; block length list, which points close to the correct entry in the block length list, which is then used to find the relevant data block.</p>
<div>
<table>
<thead>
<tr><th>Table section sizes</th><th>WDL</th><th>DTZ</th><th>Total</th></tr>
</thead>
<tbody>
<tr><td>Headers and sparse block length lists</td><td>38 GiB</td><td>9 GiB</td><td>47 GiB</td></tr>
<tr><td>Block length lists</td><td>274 GiB</td><td>64 GiB</td><td>339 GiB</td></tr>
<tr><td>Compressed data blocks</td><td>8433 GiB</td><td>8458 GiB</td><td>16891 GiB</td></tr>
</tbody>
</table>
</div>
<p>We could certainly use the SSD space for an additional layer of adaptive caching, to cache hot list entries and data blocks. But since we&#39;re trying to improve tail latencies in particular, it makes sense to think about the worst case. By putting the sparse block length lists and the block length lists on SSD storage, hot or cold, we can guarantee a maximum of 1 slow disk read per table probe.</p>
<p>In our case that doesn&#39;t quite fit when using the SSD space in RAID 1 (mirrored), but since this optimization is optional, we can give up redundancy and use RAID 0.</p>
<h2>Parallelizing reads</h2>
<p>In chess engines, a typical tablebase request will be for a single WDL value. But for the user interface we instead want to display DTZ values for all moves.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:JGE5DJVrlUwR:VQu8XPZu.png&amp;w=800&amp;sig=66061784df25478d0e120a193664ce6dbd2ee481" alt="Screenshot of tablebase response"/></p>
<p>That, together with Syzygy&#39;s internal resolution of captures, will cause the average request to issue 23 WDL probes and 70 DTZ probes. In the initial implementation handling of requests was parallelized, but probes within each request were executed sequentially.</p>
<p>In the benchmark results we can see that using more fine grained parallelism has some overhead at the low end, but significantly reduces tail latencies. Of course the disks can not really physically handle that many parallel reads, but now the I/O scheduler is more likely to plan them in a way that will finish each request as soon as possible, and can better plan the order of all involved disk accesses (minimizing time until the disk&#39;s read head is at the next requested sector).</p>
<h2>Performance in production</h2>
<p>Finally, it&#39;s good to confirm that optimizations in the benchmark scenario actually help in production. Here are response time charts sliced together.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:hil5PjrHZMoy:Hc1Y8NLW.png&amp;w=800&amp;sig=fd4b864738e2060844341e192d20cfd51fa86efd" alt="Standard chess response times significantly reduced in production"/></p>
<hr/>
<p>Raw data at <a href="https://github.com/niklasf/lila-tablebase-bench-tool">https://github.com/niklasf/lila-tablebase-bench-tool</a></p>
</div></div>
  </body>
</html>
