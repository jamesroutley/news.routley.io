<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kuai-lab.github.io/soundini-gallery/">Original</a>
    <h1>Soundini: Sound-Guided Diffusion for Natural Video Editing</h1>
    
    <div id="readability-page-1" class="page">

  <section>
    <div>
      <div>
        <div>
          <div>
            
            <p>
              <span>
                Sieun Kim <sup>1</sup>,
              </span>
              <span>
                Innfarn Yoo<sup>2</sup>,
              </span>
              <span>
                Feng Yang<sup>2</sup>,
              </span>
              <span>
                Donghyeon Cho<sup>1</sup>,
              </span>
               <span>
                Youngseo Kim<sup>1</sup>,
              </span>
              
               <span>
                Huiwen Chang<sup>2</sup>,
              </span>
              
               <span>
                Jinkyu Kim<sup>3,*</sup>,
              </span>
              
               <span>
                Sangpil Kim<sup>1,*</sup>
              </span>
             
            </p>
            
            <!--
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Department of Artificial Intelligence and</span><span class="author-block"><sup>3</sup>CSE, Korea University</span>
              <span class="author-block"><sup>2</sup>Google Research</span>
            </div>
            -->

            
      </div>
    </div>
  </div>
</div>
</section>

<!-- main carousel -->
<section>
      
</section>
<!-- End main carousel -->


<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            We propose a method for adding sound-guided visual effects to specific regions of videos with a zero-shot setting. Animating the appearance of the visual effect is challenging because each frame of the edited video should have visual changes while maintaining temporal consistency. Moreover, existing video editing solutions focus on temporal consistency across frames, ignoring the visual style variations over time, e.g., thunderstorm, wave, fire crackling. To overcome this limitation, we utilize temporal sound features for the dynamic style. Specifically, we guide denoising diffusion probabilistic models with an audio latent representation in the audio-visual latent space. To the best of our knowledge, our work is the first to explore sound-guided natural video editing from various sound sources with sound-specialized properties, such as intensity, timbre, and volume. Additionally, we design optical flow-based guidance to generate temporally consistent video frames, capturing the pixel-wise relationship between adjacent frames. Experimental results show that our method outperforms existing video editing techniques, producing more realistic visual effects that reflect the properties of sound. 
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Comparison with the extension of image editing works</h2>
          <center>
            <img src="https://kr.object.ncloudstorage.com/soundini/video6.gif" alt="wave" loof="infinite"/>
          </center>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Context adaptive video editing</h2>
          <center>
            <img src="https://kr.object.ncloudstorage.com/soundini/video7.gif" alt="wave" loof="infinite"/>
          </center>
          
        </div>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Sound-guided video global editing</h2>
          <center>
            <img src="https://kr.object.ncloudstorage.com/soundini/video3.gif" alt="underwater_bubbling" loof="infinite"/>
          </center>
          
        </div>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Overview of Soundini</h2>
          <center>
          <img src="https://kuai-lab.github.io/soundini-gallery/static/images/figure2_15-1.png" alt="Overview of Soundini"/>
          </center>
          <p>
              Soundini consists of two gradient-based guidance for diffusion: (a) Local Sound guidance and (b) Optical flow guidance. In (a), we match the appearance of the sampled frame with the sound in the mask region using the loss $\mathcal{L}_\text{SG}$ and $\mathcal{L}_\text{DSG}$. First, we minimize the cosine distance loss $\mathcal{L}_\text{SG}$ between the denoised frame and the sound segments in the audio-visual latent space. Additionally, the loss term $\mathcal{L}_\text{DSG}$ aligns the variation of frame latent representation and that of sound, capturing the transition of sound semantic and reaction. In (b), our optical flow guidance $\mathcal{L}_\text{flow}$ allows the sampled video to maintain temporal consistency by measuring the mean squared error between warped and sampled frames using optical flow. In particular, gradually increasing the influence on the loss  $\mathcal{L}_\text{flow}$ helps to stably sample the temporally consistent video. Background reconstruction loss $\mathcal{L}_\text{back}$ allows the background to be close to the source video.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Illustration of optical flow guidance for diffusion</h2>
          <center>
          <img src="https://kuai-lab.github.io/soundini-gallery/static/images/figure3_9-1.png" alt="Illustration of optical flow guidance for diffusion"/>
          </center>
          <p>
              The frozen optical flow estimator produces bi-directional optical flows between the sampled frame $i$ and frame $i+1$. Then, we apply forward and backward warp to match each pixel and minimize the mean squared error between the adjacent and the corresponding warped frames.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>




<!--BibTex citation -->
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@misc{lee2023soundini,
      title={Soundini: Sound-Guided Diffusion for Natural Video Editing}, 
      author={Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang, Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu Kim, Sangpil Kim},
      year={2023},
      eprint={2304.06818},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>
<!-- End BibTex citation -->

<!--Acknowledgements -->
<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
      We thank. 
  </div>
</section> -->
<!--End Acknowledgements -->
  


<!-- Statcounter tracking code -->






<!-- End of Statcounter Code -->



</div>
  </body>
</html>
