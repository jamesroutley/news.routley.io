<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/">Original</a>
    <h1>LLMs are cheap</h1>
    
    <div id="readability-page-1" class="page"><div><div>
<p>This post is making a point - generative AI is relatively cheap -
that might seem so obvious it doesn&#39;t need making. I&#39;m mostly writing it because I&#39;ve
repeatedly had the same discussion in the past six months where people claim the opposite.
Not only is the misconception still around, but it&#39;s not even getting less frequent. This
is mainly written to have a document I can point people at, the next time it repeats.</p>

<p>It seems to be a common, if not a majority, belief that Large
Language Models (in the colloquial sense of &#34;things that are like
ChatGPT&#34;) are very expensive to operate. This then leads to a ton
of innumerate analyses about how AI companies must be obviously
doomed, as well as a myopic view on how consumer AI businesses can/will
be monetized.</p>
<p>It&#39;s an understandable mistake, since inference was indeed very
expensive at the start of the AI boom, and those costs were talked about
a lot. But inference has gotten cheaper even faster
than models have gotten better, and nobody has an intuition for
something becoming 1000x cheaper in two years. It just doesn&#39;t happen.
It doesn&#39;t help that the common pricing model (&#34;$ per million tokens&#34;)
is very hard to visualize.</p>
<p>So let&#39;s compare LLMs to web search. I&#39;m choosing search as the
comparison since it&#39;s in the same vicinity and since it&#39;s
something everyone uses and nobody pays for, not because I&#39;m suggesting that
ungrounded generative AI is a good substitute for search.</p>

<read-more></read-more>

<p>(It should also go without saying that these are just my personal
opinions.)</p>
<h3 id="what-does-a-web-search-cost">What is the price of a web search?</h3>
<p>Here&#39;s the public API pricing for some companies operating their own
web search infrastructure, retrieved on 2025-05-02:</p>
<ul>
<li>The <a href="https://ai.google.dev/gemini-api/docs/pricing">Gemini API pricing</a>
lists a &#34;Grounding with Google Search&#34; feature at $35/1k queries. I
believe that&#39;s the best number we can get for Google, they don&#39;t publish
prices for a &#34;raw&#34; search result API.<br/>
</li>
<li>The <a href="https://www.microsoft.com/en-us/bing/apis/pricing">Bing Search API</a>
is priced at $15/1k queries at the cheapest tier.<br/>
</li>
<li><a href="https://brave.com/search/api/">Brave</a>
has a price of $5/1k searches at the cheapest tier. Though there&#39;s something very
strange about their pricing structure, with the unit pricing increasing as
the quota increases, which is the opposite of what you&#39;d expect. The
tier with real quota is priced at $9/1k searches.</li>
</ul>
<p>So there&#39;s a range of prices, but not a horribly wide one, and with the
engines you&#39;d expect to be of higher quality also having higher prices.</p>
<h3 id="what-does-equivalent-llm-usage-cost">What is the price of LLMs in a similar domain?</h3>
<p>To make a reasonable comparison between those search prices and LLM
prices, we need two numbers:</p>
<ul>
<li>How many tokens are output per query?<br/>
</li>
<li>What&#39;s the price per token?</li>
</ul>
<p>I picked a few arbitrary queries from my search history, and phrased
them as questions, and ran them on Gemini 2.5 Flash (thinking mode off)
in AI Studio:</p>
<ul>
<li>[When was the term LLM first used?] -&gt; 361 tokens, 2.5
seconds<br/>
</li>
<li>[What are the top javascript game engines?] -&gt; 1145 tokens, 7.6
seconds<br/>
</li>
<li>[What are the typical carry-on bag size limits in europe?] -&gt; 506
tokens, 3.4 seconds<br/>
</li>
<li>[List the 10 largest power outages in history] -&gt; 583 tokens, 3.7
seconds</li>
</ul>
<p>Note that I&#39;m not judging the quality of the answers here. The
purpose is just to get rough numbers for how large typical responses
are. A 500-1000 token range seems like a reasonable estimate.</p>
<p>What&#39;s the price of a token? The pricing is sometimes different for input
and output tokens. Input tokens tend to be cheaper, and our inputs are
very short compared to the outputs, so for simplicity let&#39;s consider all
the tokens to be outputs. Here&#39;s the pricing of some relevant models,
retrieved on 2025-05-02:</p>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Model</th>
<th>Price / 1M tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemma 3 27B</td>
<td>$0.20 (<a href="https://openrouter.ai/google/gemma-3-27b-it">source</a>)</td>
</tr>
<tr>
<td>Qwen3 30B A3B</td>
<td>$0.30 (<a href="https://openrouter.ai/qwen/qwen3-30b-a3b">source</a>)</td>
</tr>
<tr>
<td>Gemini 2.0 Flash</td>
<td>$0.40 (<a href="https://ai.google.dev/gemini-api/docs/pricing">source</a>)</td>
</tr>
<tr>
<td>GPT-4.1 nano</td>
<td>$0.40 (<a href="https://openai.com/api/pricing/">source</a>)</td>
</tr>
<tr>
<td>Gemini 2.5 Flash Preview</td>
<td>$0.60 (<a href="https://ai.google.dev/gemini-api/docs/pricing">source</a>)</td>
</tr>
<tr>
<td>Deepseek V3</td>
<td>$1.10 (<a href="https://api-docs.deepseek.com/quick_start/pricing">source</a>)</td>
</tr>
<tr>
<td>GPT-4.1 mini</td>
<td>$1.60 (<a href="https://openai.com/api/pricing/">source</a>)</td>
</tr>
<tr>
<td>Deepseek R1</td>
<td>$2.19 (<a href="https://api-docs.deepseek.com/quick_start/pricing">source</a>)</td>
</tr>
<tr>
<td>Claude 3.5 Haiku</td>
<td>$4.00 (<a href="https://www.anthropic.com/pricing#anthropic-api">source</a>)</td>
</tr>
<tr>
<td>GPT-4.1</td>
<td>$8.00 (<a href="https://openai.com/api/pricing/">source</a>)</td>
</tr>
<tr>
<td>Gemini 2.5 Pro Preview</td>
<td>$10.00 (<a href="https://ai.google.dev/gemini-api/docs/pricing">source</a>)</td>
</tr>
<tr>
<td>Claude 3.7 Sonnet</td>
<td>$15.00 (<a href="https://www.anthropic.com/pricing#anthropic-api">source</a>)</td>
</tr>
<tr>
<td>o3</td>
<td>$40.00 (<a href="https://openai.com/api/pricing/">source</a>)</td>
</tr>
</tbody>
</table>

<p>If we assume the average query uses 1k tokens, these prices would be
directly comparable to the prices per 1k search queries. That&#39;s convenient.</p>
<p>The low end of that spectrum is at least an order of magnitude
cheaper than even the cheapest search API, and even the models at the
low end are pretty capable. The high end is about on par with the
highest end of search pricing. To compare a midrange pair on quality,
the Bing Search vs. a Gemini 2.5 Flash comparison shows the LLM being
1/25th the price.</p>
<p>Note that many of the above models have cheaper pricing in exchange
for more flexible scheduling (Anthropic, Google and OpenAI give a 50%
discount for batch requests, Deepseek is 50%-75% cheaper during off-peak
hours). I&#39;ve not included those cheaper options in the table to keep
things comparable, but the presence of those cheaper tiers is worth
keeping in mind when thinking about the next section...</p>
<h3 id="objection">Objection!</h3>
<p>I know some people are going to have objections to this
back-of-the-envelope calculation, and a lot of them will be totally
legit concerns. I&#39;ll try to address some of them preemptively. Slightly
different assumptions can easily lead to clawing back 10% here and 50%
there. But I don&#39;t see how to bridge a 25x gap just for breaking even,
let alone making the AI significantly more expensive. If you want to play
around with different assumptions, there&#39;s a little calculator widget
below.</p>
<p><strong>Surely the typical LLM response is longer than that</strong>
- I already picked the upper end of what the (very light) testing
suggested as a reasonable range for the type of question that I&#39;d use
web search for. There&#39;s a lot of use cases where the inputs and outputs
are going to be much longer (e.g. coding), but then you&#39;d need to also
switch the comparison to something in that same domain as well.</p>
<p><strong>The LLM API prices must be subsidized to grab market
share -- i.e. the prices might be low, but the costs are high</strong> -
I don&#39;t think they are, for a few reasons. I&#39;d instead assume
APIs are typically profitable on a unit basis. I have not found any
credible analysis suggesting otherwise.</p>
<p>First, there&#39;s not that much motive to gain API market share
with unsustainably cheap prices. Any gains would be temporary, since
there&#39;s no long-term lock-in, and better models are released weekly.
Data from paid API queries will also typically not be used for training
or tuning the models, so getting access to more data wouldn&#39;t explain
it. Note that it&#39;s not just that you&#39;d be losing money on each of
these queries for no benefit, you&#39;re losing the compute that could
be spent on training, research, or more useful types of inference.</p>
<p>Second, some of those models have been released with open weights and
API access is also available from third-party providers who would have
no motive to subsidize inference. (Or the number in the table isn&#39;t even
first party hosting -- I sure can&#39;t figure out what the Vertex AI
pricing for Gemma 3 is). The pricing of those third-party hosted APIs
appears competitive with first-party hosted APIs. For example, the
<a href="https://artificialanalysis.ai/models/deepseek-r1/providers">Artificial Analysis
summary on Deepseek R1 hosting</a>.</p>
<p>Third, Deepseek released <a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md">actual
numbers</a> on their inference efficiency in February. Those numbers
suggest that their normal R1 API pricing has about 80% margins
when considering the GPU costs, though not any other serving costs.</p>
<p>Fourth, there are a bunch of first-principles analyses on the cost structure
of models with various architectures should be. Those are of course mathematical
models, but those costs line up pretty well with the observed end-user
pricing of models whose architecture is known. See the references section for
links.</p>
<p><strong>The search API prices amortize building and updating the
search index, LLM inference is based on just the cost of
inference</strong> - This seems pretty likely to be true, actually? But
the effect can&#39;t really be <em>that</em> large for a popular model:
e.g. the allegedly leaked OpenAI financials claimed $4B/year spent on
inference vs. $3B/year on training. Given the crazy growth of
inference volumes (e.g. Google recently claimed a <a href="https://blog.google/technology/ai/io-2025-keynote/">
50x increase in token volumes in the last year</a>) the training costs
are getting amortized much more effectively.</p>
<p><strong>The search API prices must have higher margins than LLM
inference</strong> - It&#39;s possible. I certainly don&#39;t know what the
margins of any Search API providers are, though it seems fair to assume
they&#39;re pretty robust. But, well, see the point above about Deepseek&#39;s
releasd numbers on the R1 profit margins.</p>
<p>Also, it seems quite plausible that some Search providers would
accept lower margins, since at least Microsoft execs have testified
under oath that they&#39;d be willing to pay more for the iOS query stream
than their revenue, just to get more usage data.</p>
<p><strong>Web search returns results 20x-100x faster than an LLM
finishes the query, how could it be more expensive?</strong> - Search
latency can be improved by parallelizing the problem, while LLM
inference is (for now) serial in nature. The task of predicting a single token can
be parallelized, but the you can&#39;t predict all the output tokens at
once.</p>

<p><strong>But OpenAI made a loss, and they don&#39;t expect to make profit for years!</strong> -
That&#39;s because a huge proportion of their usage is not monetized at all,
despite the usage pattern being ideal for it. OpenAI reportedly made a
loss of $5B in 2024. They also reportedly have 500M MAUs. To reach break-even,
they&#39;d just need to monetize (e.g. with ads) those free users for an average of $10/year,
or $1/month. A $1 ARPU for a service like this would be pitifully low.
</p>

<p>If the reported numbers are true, OpenAI doesn&#39;t actually have
high costs for a consumer service that popular, which is what you&#39;d
expect to see if the high cost of inference was the problem. They just
have a very low per-user revenue, by choice.</p>

<h3 id="widget">Sensitivity analysis</h3>

<p>
  If you want to play around with different assumptions, here&#39;s
  a calculator:
</p>

<p><a href="https://www.snellman.net/blog/stc/files/llm-cost-widget.html" target="_blank">Open in new tab</a></p>

<h3 id="why-does-this-matter">Why does this matter?</h3>
<p>I mean, you&#39;re right to ask that. Nothing really matters and
eventually we&#39;ll all be dead.</p>
<p>But it is interesting how many people have built their mental
model for the near future on a premise that was true for only a brief
moment. Some things that will come as a surprise to them even assuming
all progress stops right now:</p>
<p>There&#39;s an argument advanced by some people about how low
prices mean it&#39;ll be impossible for AI companies to ever recoup
model training costs. The thinking seems to be that it&#39;s just
the prices that have been going down, but not the costs, and
the low prices must be an unprofitable race to the bottom for
what little demand there is. What&#39;s happening and will continue
to happen instead is that as costs go down, the prices go down too,
and demand increases as new uses become viable. For an example,
look at the <a href="https://openrouter.ai/rankings">OpenRouter
API traffic volumes</a>, both in aggregate and in the relative
share of cheaper models.
</p>
<p>This post was mainly about APIs, but consumer usage will have
exactly the same cost structure, just a different monetization
structure. And given how low the unit costs must be, advertising isn&#39;t merely
viable but lucrative.</p>
<p>From this it follows that the financials of frontier AI labs are a
lot better than some innumerate pundits would have you believe. They&#39;re
making a loss because they&#39;re not under pressure to be profitable, and
aren&#39;t actively trying to monetize consumer traffic yet. This
could well be a land grab unlike APIs, since unpaid consumer queries
may be used for training while paid API queries typically are not.
Even the subscription pricing might be there
mainly for demand management rather than trying to run a profit.</p>
<p>The real cost problem isn&#39;t going to be with the LLMs themselves,
it&#39;s with all the backend services that AI agents will want to access
if even a rudimentary form of the agentic vision actually materializes.
Running the AI is already cheap, will keep getting cheaper, and will always
have a monetization model of some sort since it&#39;s what the end user is
interacting with. Neither of those is true for the end-user services
that have been turned into AI backends without their consent. An AI
trying to, I don&#39;t know, book concert tickets whenever a band I like
plays in my town will probably be phenomenally expensive to its
third-party backends (e.g. scraping ticket sites). Those sites will be
uncompensated for the expense while also removing their actual revenue
streams.</p>
<p>I don&#39;t really know how that plays out.</p>
<p>Obviously many service owners will try to make unauthorized scraping
harder, but that&#39;s a very hard problem to solve on the web. Maybe some
of them give up on the web entirely, and move to mobile where they can
at least get device attestations. Some might just give up on the open
web, and require all usage to be signed in, with account creation being
gated on something scarce. Some might become unviable and close up shop
entirely.</p>
<p>If/when that happens, what&#39;s the play on the AI agent side? Will they
choose an escalating adversarial arms race with increasingly dodgy
tactics, or will they eventually decide that it&#39;s better to pay for the
services they use? The former seems unsustainable. If the latter, then
it feels like the core engineering challenge becomes one of building data
provider backends optimized specifically for AI use, with the goal of scaling to
massive volumes and cheaper unit prices, with the trade-off being higher latency, lower
reliability and lower quality.
That could be quite interesting from a systems perspective. (Yes, I&#39;m aware
of <a href="https://www.anthropic.com/news/model-context-protocol">MCP</a>,
but it&#39;s a solution to an orthogonal issue.)
</p>
<p>But one thing I&#39;m confident won&#39;t be happening is that it&#39;s the AIs that
turn out to be too expensive to run.</p>

<h3>Additional reading</h3>

<p>
Below are some additional references that were not worked into the main
narrative (this article was long-winded enough already).
</p>

<ul>
<li><a href="https://arxiv.org/html/2506.04645v1">Inference economics of language models</a> (2025) - A mathematical model for estimating the cost structure, latency/cost tradeoffs, optimal cluster size, and optimal batching based on the LLM architecture.

</li><li><a href="https://www.tensoreconomics.com/p/llm-inference-economics-from-first">LLM Inference Economics from First Principles</a> - (2025) A very detailed cost-per-token computation on the cost structure of one specific model, LLama 3.3 70B.

</li><li><a href="https://www.lesswrong.com/posts/mRKd4ArA5fYhd2BPb/observations-about-llm-inference-pricing">Observations About LLM Inference Pricing</a> - (2025) Analysis of the economics driven by pricing data rather than first-principles cost structure; concludes that proprietary models have very significant markups.

</li><li><a href="https://semianalysis.com/2023/02/13/peeling-the-onions-layers-large-language/">Large Language Models Search Architecture And Cost</a> - (2023) Analysis on the cost of integrating LLMs into search; the LLM cost data is no longer very relevant due to the age of the article (GPT-3.5) but it uses a different way of estimating the search cost structure.
</li></ul>
</div></div></div>
  </body>
</html>
